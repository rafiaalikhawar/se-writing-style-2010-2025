Empirical Studyof Transformers for SourceCode
Nadezhda Chirkova
HSE University
Moscow, Russia
nchirkova@hse.ruSergeyTroshin
HSE University
Moscow, Russia
stroshin@hse.ru
ABSTRACT
Initiallydevelopedfornaturallanguageprocessing(NLP),Trans-
formers are now widely used for source code processing, due to
the format similarity betweensource code and text. In contrast to
naturallanguage,sourcecodeisstrictlystructured,i.e.,itfollows
the syntax of the programming language. Several recent works
developTransformermodificationsforcapturingsyntacticinfor-
mationinsourcecode.Thedrawbackoftheseworksisthatthey
donotcomparetoeachotherandconsiderdifferenttasks.Inthis
work, we conduct a thorough empirical study of the capabilities of
Transformers to utilize syntactic information in different tasks. We
consider three tasks (code completion, function naming and bug
fixing)andre-implement different syntax-capturingmodifications
in a unified framework. We show that Transformers are able to
makemeaningfulpredictionsbasedpurelyonsyntacticinformation
andunderlinethebestpracticesoftakingthesyntacticinformation
intoaccount forimprovingthe performance ofthe model.
CCSCONCEPTS
Â·Computing methodologies â†’Neural networks .
KEYWORDS
neural networks, transformer, variable misuse detection, function
naming,code completion
ACM ReferenceFormat:
Nadezhda Chirkova and Sergey Troshin. 2021. Empirical Study of Trans-
formers for Source Code. In Proceedings of the 29th ACM Joint European
Software Engineering Conference and Symposium on the Foundations of Soft-
wareEngineering(ESEC/FSEâ€™21),August 23Å›28,2021,Athens,Greece. ACM,
NewYork, NY, USA, 13pages.https://doi.org/10.1145/3468264.3468611
1 INTRODUCTION
Transformer[ 37]iscurrentlyastate-of-the-artarchitectureinalot
of source code processing tasks, including code completion [ 20],
code translation [ 21,32], and bug fixing [ 14]. Particularly, Trans-
formers were shown to outperform classic deep learning architec-
tures, e.g., recurrent (RNNs), recursive and convolutional neural
networks in the mentioned tasks. These architectures focus on
localconnections between input elements, while Transformer pro-
cessesallinputelementsinparallelandfocusesoncapturing global
ESEC/FSE â€™21,August23Å›28, 2021, Athens, Greece
Â©2021 Copyright held bytheowner/author(s).
ACM ISBN978-1-4503-8562-6/21/08.
https://doi.org/10.1145/3468264.3468611dependencies in data, producing more meaningful code representa-
tions[14].Thisparallelismalso speedsuptrainingandprediction.
Transformer is often applied to source code directly, treating
code as a sequence of language keywords, punctuation marks, and
identifiers.Inthiscase,aneuralnetworkmostlyreliesonidentifiers,
e.g.variable names, to make predictions [ 1,22]. High-quality vari-
ablenamescanbearichsourceofinformationaboutthesemantics
ofthecode;however,thisisonlyanindirect,secondarysourceof
information.Theprimary source ofinformationof whatthe code
implements is its syntactic structure.
Transformer architecture relies on the self-attention mechanism
that is not aware of the order or structure of input elements and
treatstheinputasanunordered bagofelements.Toaccountforthe
particularstructureoftheinput,additionalmechanismsareusually
used,e.g.positionalencodingforprocessingsequentialstructure.
In recent years, a line of research has developed mechanisms for
utilizingtreestructureofcodeinTransformer[ 14,20,32].However,
the mosteffective wayofutilizing syntacticinformationin Trans-
formerisstillunclearforthreereasons.First,themechanismswere
developedconcurrently,sotheywerenotcomparedtoeachotherby
their authors. Moreover,different workstest the proposed mecha-
nismsondifferentcodeprocessingtasks,makingithardtoalignthe
empiricalresultsreportedinthepapers. Secondly,the mentioned
worksusedstandardTransformerwithpositionalencodingsasa
baseline, while modern practice uses more advanced modifications
of Transformer, e.g., equipping it with relative attention [ 31]. As
a result,it is unclearwhether usingsophisticated mechanisms for
utilizing syntactic information is needed at all. Thirdly, most of the
worksfocusonutilizingtreestructureinTransformeranddonot
investigatethe effectof processingothersyntax components,e. g.
the syntactic units ofthe programminglanguage.
Inthiswork,weconductanempiricalstudyofusingTransformer
for processing source code. Firstly, we would like to answer the
question, what is the best way of utilizing syntactic information
in Transformer, and to provide practical recommendations for the
use ofTransformersinsoftwareengineering tasks.Secondly,we
aim at understanding whether Transformer is generally suitable
for capturing code syntax, to ground the future development of
Transformers forcode.Ourcontributions are as follows:
â€¢Were-implementseveralapproachesforcapturingsyntactic
structure in Transformerand investigate their effectiveness
inthreecodeprocessingtasksontwodatasets.Weunderline
the importance of evaluating code processing models on
severaldifferenttasks andbelieve that our workwill help to
establish standardbenchmarks in neuralcode processing.
â€¢Weintroduceananonymizedsettinginwhichalluser-defined
identifiers are replaced with placeholders, and show that
Transformer is capable of making meaningful predictions
based purely on syntactic information, in all three tasks. We
This work is licensed under a Creative Commons Attribution-
NoDerivatives 4.0 InternationalLicense.
703
ESEC/FSE â€™21, August 23â€“28, 2021,Athens,Greece N.Chirkova andS.Troshin
elem = lst[idx]<latexit sha1_base64="lhnYvmnSt74IBXifJNQJlIQEMV0=">AAAB/3icbVBNS8NAEN34WetXVPDiZbEInkrSi16EohePFewHpKFsNpN26WYTdjdiSXvwr3jxoIhX/4Y3/43bNgdtfTDweG+GmXlBypnSjvNtrayurW9slrbK2zu7e/v2wWFLJZmk0KQJT2QnIAo4E9DUTHPopBJIHHBoB8Obqd9+AKlYIu71KAU/Jn3BIkaJNlLPPu4aNxgDhxhfYa60x8JHf9yzK07VmQEvE7cgFVSg0bO/umFCsxiEppwo5blOqv2cSM0oh0m5mylICR2SPniGChKD8vPZ/RN8ZpQQR4k0JTSeqb8nchIrNYoD0xkTPVCL3lT8z/MyHV36ORNppkHQ+aIo41gneBoGDpkEqvnIEEIlM7diOiCSUG0iK5sQ3MWXl0mrVnWdqntXq9SvizhK6ASdonPkogtUR7eogZqIojF6Rq/ozXqyXqx362PeumIVM0foD6zPH9h3lfg=</latexit><latexit sha1_base64="lhnYvmnSt74IBXifJNQJlIQEMV0=">AAAB/3icbVBNS8NAEN34WetXVPDiZbEInkrSi16EohePFewHpKFsNpN26WYTdjdiSXvwr3jxoIhX/4Y3/43bNgdtfTDweG+GmXlBypnSjvNtrayurW9slrbK2zu7e/v2wWFLJZmk0KQJT2QnIAo4E9DUTHPopBJIHHBoB8Obqd9+AKlYIu71KAU/Jn3BIkaJNlLPPu4aNxgDhxhfYa60x8JHf9yzK07VmQEvE7cgFVSg0bO/umFCsxiEppwo5blOqv2cSM0oh0m5mylICR2SPniGChKD8vPZ/RN8ZpQQR4k0JTSeqb8nchIrNYoD0xkTPVCL3lT8z/MyHV36ORNppkHQ+aIo41gneBoGDpkEqvnIEEIlM7diOiCSUG0iK5sQ3MWXl0mrVnWdqntXq9SvizhK6ASdonPkogtUR7eogZqIojF6Rq/ozXqyXqx362PeumIVM0foD6zPH9h3lfg=</latexit><latexit sha1_base64="lhnYvmnSt74IBXifJNQJlIQEMV0=">AAAB/3icbVBNS8NAEN34WetXVPDiZbEInkrSi16EohePFewHpKFsNpN26WYTdjdiSXvwr3jxoIhX/4Y3/43bNgdtfTDweG+GmXlBypnSjvNtrayurW9slrbK2zu7e/v2wWFLJZmk0KQJT2QnIAo4E9DUTHPopBJIHHBoB8Obqd9+AKlYIu71KAU/Jn3BIkaJNlLPPu4aNxgDhxhfYa60x8JHf9yzK07VmQEvE7cgFVSg0bO/umFCsxiEppwo5blOqv2cSM0oh0m5mylICR2SPniGChKD8vPZ/RN8ZpQQR4k0JTSeqb8nchIrNYoD0xkTPVCL3lT8z/MyHV36ORNppkHQ+aIo41gneBoGDpkEqvnIEEIlM7diOiCSUG0iK5sQ3MWXl0mrVnWdqntXq9SvizhK6ASdonPkogtUR7eogZqIojF6Rq/ozXqyXqx362PeumIVM0foD6zPH9h3lfg=</latexit><latexit sha1_base64="lhnYvmnSt74IBXifJNQJlIQEMV0=">AAAB/3icbVBNS8NAEN34WetXVPDiZbEInkrSi16EohePFewHpKFsNpN26WYTdjdiSXvwr3jxoIhX/4Y3/43bNgdtfTDweG+GmXlBypnSjvNtrayurW9slrbK2zu7e/v2wWFLJZmk0KQJT2QnIAo4E9DUTHPopBJIHHBoB8Obqd9+AKlYIu71KAU/Jn3BIkaJNlLPPu4aNxgDhxhfYa60x8JHf9yzK07VmQEvE7cgFVSg0bO/umFCsxiEppwo5blOqv2cSM0oh0m5mylICR2SPniGChKD8vPZ/RN8ZpQQR4k0JTSeqb8nchIrNYoD0xkTPVCL3lT8z/MyHV36ORNppkHQ+aIo41gneBoGDpkEqvnIEEIlM7diOiCSUG0iK5sQ3MWXl0mrVnWdqntXq9SvizhK6ASdonPkogtUR7eogZqIojF6Rq/ozXqyXqx362PeumIVM0foD6zPH9h3lfg=</latexit>
Assign
NameStore
elemSubscriptLoad
NameLoad
lstIndex
NameLoad
idx1
2 3
45
6U D
D4
6123456
//1/2/2/1/2/2/2/2/1
<latexit sha1_base64="qQjnmsSQddqO32tpXszJQIf3oiw=">AAACbHicbVHJTsMwEHXCHrayHEAVkkVFxalNynpEcOEIEgWkpqocd9paOE5kO0hV6Ik/5MYncOEbcNoUlZaxnvVm5o3HHgcxZ0q77qdlz80vLC4trzira+sbm4Wt7UcVJZJCnUY8ks8BUcCZgLpmmsNzLIGEAYen4OUmyz+9glQsEg+6H0MzJF3BOowSbUKtwrsfQJeJVJMg4UQO0jc6sQbY72UnO9jDZVwzODE4NTgzOMe+PxZUjV/NRNXacMv52Mt8Z0Lug2j/9mwVSm7FHRqeJV5OSii3u1bhw29HNAlBaMqJUg3PjXUzJVIzymHg+ImCmNAX0oWGoYKEoJrpcFgDfGQibdyJpIHQeBidrEhJqFQ/DIwyJLqnpnNZ8L9cI9Gdy2bKRJxoEHTUqJNwrCOcTR63mQSqed8QQiUzd8W0RySh2vyPY4bgTT95ljzWKp5b8e5rpavrfBzLqIgO0THy0AW6QrfoDtURRV/WprVn7Vvf9q5dtA9GUtvKa3bQH7PLP8S8r7g=</latexit><latexit sha1_base64="qQjnmsSQddqO32tpXszJQIf3oiw=">AAACbHicbVHJTsMwEHXCHrayHEAVkkVFxalNynpEcOEIEgWkpqocd9paOE5kO0hV6Ik/5MYncOEbcNoUlZaxnvVm5o3HHgcxZ0q77qdlz80vLC4trzira+sbm4Wt7UcVJZJCnUY8ks8BUcCZgLpmmsNzLIGEAYen4OUmyz+9glQsEg+6H0MzJF3BOowSbUKtwrsfQJeJVJMg4UQO0jc6sQbY72UnO9jDZVwzODE4NTgzOMe+PxZUjV/NRNXacMv52Mt8Z0Lug2j/9mwVSm7FHRqeJV5OSii3u1bhw29HNAlBaMqJUg3PjXUzJVIzymHg+ImCmNAX0oWGoYKEoJrpcFgDfGQibdyJpIHQeBidrEhJqFQ/DIwyJLqnpnNZ8L9cI9Gdy2bKRJxoEHTUqJNwrCOcTR63mQSqed8QQiUzd8W0RySh2vyPY4bgTT95ljzWKp5b8e5rpavrfBzLqIgO0THy0AW6QrfoDtURRV/WprVn7Vvf9q5dtA9GUtvKa3bQH7PLP8S8r7g=</latexit><latexit sha1_base64="qQjnmsSQddqO32tpXszJQIf3oiw=">AAACbHicbVHJTsMwEHXCHrayHEAVkkVFxalNynpEcOEIEgWkpqocd9paOE5kO0hV6Ik/5MYncOEbcNoUlZaxnvVm5o3HHgcxZ0q77qdlz80vLC4trzira+sbm4Wt7UcVJZJCnUY8ks8BUcCZgLpmmsNzLIGEAYen4OUmyz+9glQsEg+6H0MzJF3BOowSbUKtwrsfQJeJVJMg4UQO0jc6sQbY72UnO9jDZVwzODE4NTgzOMe+PxZUjV/NRNXacMv52Mt8Z0Lug2j/9mwVSm7FHRqeJV5OSii3u1bhw29HNAlBaMqJUg3PjXUzJVIzymHg+ImCmNAX0oWGoYKEoJrpcFgDfGQibdyJpIHQeBidrEhJqFQ/DIwyJLqnpnNZ8L9cI9Gdy2bKRJxoEHTUqJNwrCOcTR63mQSqed8QQiUzd8W0RySh2vyPY4bgTT95ljzWKp5b8e5rpavrfBzLqIgO0THy0AW6QrfoDtURRV/WprVn7Vvf9q5dtA9GUtvKa3bQH7PLP8S8r7g=</latexit><latexit sha1_base64="qQjnmsSQddqO32tpXszJQIf3oiw=">AAACbHicbVHJTsMwEHXCHrayHEAVkkVFxalNynpEcOEIEgWkpqocd9paOE5kO0hV6Ik/5MYncOEbcNoUlZaxnvVm5o3HHgcxZ0q77qdlz80vLC4trzira+sbm4Wt7UcVJZJCnUY8ks8BUcCZgLpmmsNzLIGEAYen4OUmyz+9glQsEg+6H0MzJF3BOowSbUKtwrsfQJeJVJMg4UQO0jc6sQbY72UnO9jDZVwzODE4NTgzOMe+PxZUjV/NRNXacMv52Mt8Z0Lug2j/9mwVSm7FHRqeJV5OSii3u1bhw29HNAlBaMqJUg3PjXUzJVIzymHg+ImCmNAX0oWGoYKEoJrpcFgDfGQibdyJpIHQeBidrEhJqFQ/DIwyJLqnpnNZ8L9cI9Gdy2bKRJxoEHTUqJNwrCOcTR63mQSqed8QQiUzd8W0RySh2vyPY4bgTT95ljzWKp5b8e5rpavrfBzLqIgO0THy0AW6QrfoDtURRV/WprVn7Vvf9q5dtA9GUtvKa3bQH7PLP8S8r7g=</latexit>/2
/2
/1
6
Assign NameStore SubscriptLoad ...
hemptyielem hemptyi...
<latexit sha1_base64="lkPxNop1jfhIUekugx0O76isz9E=">AAAClXichVFLb9QwEHbCq4TXUg4cuFhsQZyipBd6QWoLQhwQFJVtK61Xq4kzm7Xq2JE9QaxC/hG/hhv/Bm+6QtAiMZblz9+8Z4pGK09Z9jOKr12/cfPW1u3kzt179x+MHm6feNs6iRNptXVnBXjUyuCEFGk8axxCXWg8Lc5fr/WnX9B5Zc1nWjU4q6EyaqEkUKDmo++iwEqZjqBoNbi+k9+G0/PkwHtVGf6cf4Aaj8k6DPi4Lbx0qqH3FsrwT9NUCC6W6/zJjtBgKo2cC8Kv1GHd0Krnwg3sTjBHjXV4/m8Y4vIQOBFoyt/FzUfjLM0G4VdBvgFjtpGj+eiHKK1sazQkNXg/zbOGZh04UlJjn4jWYwPyHCqcBmhCo37WDVPt+bPAlHxhXbiG+MD+6dFB7f2qLoJlDbT0l3Vr8l+6aUuLvVmnTNMSGnmRaNFqTpavV8RL5VCSXgUAYdahVi6X4EBSWGQShpBfbvkqONlN8yzNP+2O9w8349hiT9hT9oLl7CXbZ+/YEZswGW1He9FBdBg/jl/Fb+K3F6ZxtPF5xP6S+OMvTLbGCw==</latexit><latexit sha1_base64="lkPxNop1jfhIUekugx0O76isz9E=">AAAClXichVFLb9QwEHbCq4TXUg4cuFhsQZyipBd6QWoLQhwQFJVtK61Xq4kzm7Xq2JE9QaxC/hG/hhv/Bm+6QtAiMZblz9+8Z4pGK09Z9jOKr12/cfPW1u3kzt179x+MHm6feNs6iRNptXVnBXjUyuCEFGk8axxCXWg8Lc5fr/WnX9B5Zc1nWjU4q6EyaqEkUKDmo++iwEqZjqBoNbi+k9+G0/PkwHtVGf6cf4Aaj8k6DPi4Lbx0qqH3FsrwT9NUCC6W6/zJjtBgKo2cC8Kv1GHd0Krnwg3sTjBHjXV4/m8Y4vIQOBFoyt/FzUfjLM0G4VdBvgFjtpGj+eiHKK1sazQkNXg/zbOGZh04UlJjn4jWYwPyHCqcBmhCo37WDVPt+bPAlHxhXbiG+MD+6dFB7f2qLoJlDbT0l3Vr8l+6aUuLvVmnTNMSGnmRaNFqTpavV8RL5VCSXgUAYdahVi6X4EBSWGQShpBfbvkqONlN8yzNP+2O9w8349hiT9hT9oLl7CXbZ+/YEZswGW1He9FBdBg/jl/Fb+K3F6ZxtPF5xP6S+OMvTLbGCw==</latexit><latexit sha1_base64="lkPxNop1jfhIUekugx0O76isz9E=">AAAClXichVFLb9QwEHbCq4TXUg4cuFhsQZyipBd6QWoLQhwQFJVtK61Xq4kzm7Xq2JE9QaxC/hG/hhv/Bm+6QtAiMZblz9+8Z4pGK09Z9jOKr12/cfPW1u3kzt179x+MHm6feNs6iRNptXVnBXjUyuCEFGk8axxCXWg8Lc5fr/WnX9B5Zc1nWjU4q6EyaqEkUKDmo++iwEqZjqBoNbi+k9+G0/PkwHtVGf6cf4Aaj8k6DPi4Lbx0qqH3FsrwT9NUCC6W6/zJjtBgKo2cC8Kv1GHd0Krnwg3sTjBHjXV4/m8Y4vIQOBFoyt/FzUfjLM0G4VdBvgFjtpGj+eiHKK1sazQkNXg/zbOGZh04UlJjn4jWYwPyHCqcBmhCo37WDVPt+bPAlHxhXbiG+MD+6dFB7f2qLoJlDbT0l3Vr8l+6aUuLvVmnTNMSGnmRaNFqTpavV8RL5VCSXgUAYdahVi6X4EBSWGQShpBfbvkqONlN8yzNP+2O9w8349hiT9hT9oLl7CXbZ+/YEZswGW1He9FBdBg/jl/Fb+K3F6ZxtPF5xP6S+OMvTLbGCw==</latexit><latexit sha1_base64="lkPxNop1jfhIUekugx0O76isz9E=">AAAClXichVFLb9QwEHbCq4TXUg4cuFhsQZyipBd6QWoLQhwQFJVtK61Xq4kzm7Xq2JE9QaxC/hG/hhv/Bm+6QtAiMZblz9+8Z4pGK09Z9jOKr12/cfPW1u3kzt179x+MHm6feNs6iRNptXVnBXjUyuCEFGk8axxCXWg8Lc5fr/WnX9B5Zc1nWjU4q6EyaqEkUKDmo++iwEqZjqBoNbi+k9+G0/PkwHtVGf6cf4Aaj8k6DPi4Lbx0qqH3FsrwT9NUCC6W6/zJjtBgKo2cC8Kv1GHd0Krnwg3sTjBHjXV4/m8Y4vIQOBFoyt/FzUfjLM0G4VdBvgFjtpGj+eiHKK1sazQkNXg/zbOGZh04UlJjn4jWYwPyHCqcBmhCo37WDVPt+bPAlHxhXbiG+MD+6dFB7f2qLoJlDbT0l3Vr8l+6aUuLvVmnTNMSGnmRaNFqTpavV8RL5VCSXgUAYdahVi6X4EBSWGQShpBfbvkqONlN8yzNP+2O9w8349hiT9hT9oLl7CXbZ+/YEZswGW1He9FBdBg/jl/Fb+K3F6ZxtPF5xP6S+OMvTLbGCw==</latexit>
...NameLoad Index NameLoad
... lst hemptyiidx
<latexit sha1_base64="e0vvaUzxo4RrExdh3y+3Kueu0aI=">AAACc3icbVFNb9QwEHXCVxs+uoDEpQes7oK4ECUICY5VuYBUoSKxbaX1ajVxZnetOk5kT9CuQv5Afx43/gUX7jjZFSotY9l6evOeZjyTVVo5SpKfQXjr9p2793Z2o/sPHj7aGzx+curK2kocy1KX9jwDh1oZHJMijeeVRSgyjWfZxYcuf/YNrVOl+UrrCqcFLIyaKwnkqdngUmS4UKYhyGoNtm3k9/60PIrjmL/kn6HA4xJyDz+ZHFdXKSG4WHaVI74Ra0f+HQkNZqGRc0G4ogaLitYtF7ZnR16h8lVnjgSa/G/p2WCYxEkf/CZIt2DItnEyG/wQeSnrAg1JDc5N0qSiaQOWlNTYRqJ2WIG8gAVOPDS+bTdt+pm1/IVncj4vrb+GeM9edTRQOLcuMq8sgJbueq4j/5eb1DR/P22UqWpCIzeF5rXmVPJuATxXFiXptQcgrfK9crkEC5L8miI/hPT6l2+C0zdxmsTpl7fDw6PtOHbYPjtgr1jK3rFD9pGdsDGT7FfwLHge8OB3uB8ehKONNAy2nqfsnwhf/wH0Frkg</latexit><latexit sha1_base64="e0vvaUzxo4RrExdh3y+3Kueu0aI=">AAACc3icbVFNb9QwEHXCVxs+uoDEpQes7oK4ECUICY5VuYBUoSKxbaX1ajVxZnetOk5kT9CuQv5Afx43/gUX7jjZFSotY9l6evOeZjyTVVo5SpKfQXjr9p2793Z2o/sPHj7aGzx+curK2kocy1KX9jwDh1oZHJMijeeVRSgyjWfZxYcuf/YNrVOl+UrrCqcFLIyaKwnkqdngUmS4UKYhyGoNtm3k9/60PIrjmL/kn6HA4xJyDz+ZHFdXKSG4WHaVI74Ra0f+HQkNZqGRc0G4ogaLitYtF7ZnR16h8lVnjgSa/G/p2WCYxEkf/CZIt2DItnEyG/wQeSnrAg1JDc5N0qSiaQOWlNTYRqJ2WIG8gAVOPDS+bTdt+pm1/IVncj4vrb+GeM9edTRQOLcuMq8sgJbueq4j/5eb1DR/P22UqWpCIzeF5rXmVPJuATxXFiXptQcgrfK9crkEC5L8miI/hPT6l2+C0zdxmsTpl7fDw6PtOHbYPjtgr1jK3rFD9pGdsDGT7FfwLHge8OB3uB8ehKONNAy2nqfsnwhf/wH0Frkg</latexit><latexit sha1_base64="e0vvaUzxo4RrExdh3y+3Kueu0aI=">AAACc3icbVFNb9QwEHXCVxs+uoDEpQes7oK4ECUICY5VuYBUoSKxbaX1ajVxZnetOk5kT9CuQv5Afx43/gUX7jjZFSotY9l6evOeZjyTVVo5SpKfQXjr9p2793Z2o/sPHj7aGzx+curK2kocy1KX9jwDh1oZHJMijeeVRSgyjWfZxYcuf/YNrVOl+UrrCqcFLIyaKwnkqdngUmS4UKYhyGoNtm3k9/60PIrjmL/kn6HA4xJyDz+ZHFdXKSG4WHaVI74Ra0f+HQkNZqGRc0G4ogaLitYtF7ZnR16h8lVnjgSa/G/p2WCYxEkf/CZIt2DItnEyG/wQeSnrAg1JDc5N0qSiaQOWlNTYRqJ2WIG8gAVOPDS+bTdt+pm1/IVncj4vrb+GeM9edTRQOLcuMq8sgJbueq4j/5eb1DR/P22UqWpCIzeF5rXmVPJuATxXFiXptQcgrfK9crkEC5L8miI/hPT6l2+C0zdxmsTpl7fDw6PtOHbYPjtgr1jK3rFD9pGdsDGT7FfwLHge8OB3uB8ehKONNAy2nqfsnwhf/wH0Frkg</latexit><latexit sha1_base64="e0vvaUzxo4RrExdh3y+3Kueu0aI=">AAACc3icbVFNb9QwEHXCVxs+uoDEpQes7oK4ECUICY5VuYBUoSKxbaX1ajVxZnetOk5kT9CuQv5Afx43/gUX7jjZFSotY9l6evOeZjyTVVo5SpKfQXjr9p2793Z2o/sPHj7aGzx+curK2kocy1KX9jwDh1oZHJMijeeVRSgyjWfZxYcuf/YNrVOl+UrrCqcFLIyaKwnkqdngUmS4UKYhyGoNtm3k9/60PIrjmL/kn6HA4xJyDz+ZHFdXKSG4WHaVI74Ra0f+HQkNZqGRc0G4ogaLitYtF7ZnR16h8lVnjgSa/G/p2WCYxEkf/CZIt2DItnEyG/wQeSnrAg1JDc5N0qSiaQOWlNTYRqJ2WIG8gAVOPDS+bTdt+pm1/IVncj4vrb+GeM9edTRQOLcuMq8sgJbueq4j/5eb1DR/P22UqWpCIzeF5rXmVPJuATxXFiXptQcgrfK9crkEC5L8miI/hPT6l2+C0zdxmsTpl7fDw6PtOHbYPjtgr1jK3rFD9pGdsDGT7FfwLHge8OB3uB8ehKONNAy2nqfsnwhf/wH0Frkg</latexit>1 2345 6
1IDDDDDDDDD
2U IUDUDDUDDUDDD
3UUD IDDDD
4UUUUD UIUDUDD
5UUUUD UUD ID
6UUUUUUD UUUUD U I
<latexit sha1_base64="/jMC2ZujAyrKvdeVqpyvnIEKocg=">AAADOnicbZI9b9swEIYppUlTt0mcZuxC1EjRyZDy1Y5B6yHdHCBKAliGQdFnmzBFCSRVwFD9u7rkV3TLkCVDiqJrfkCoj9ayHArv4XD38BV1YhBzprTj3Fr22ov1jZebrxqv32xt7zR3316qKJEUPBrxSF4HRAFnAjzNNIfrWAIJAw5XwfRr1r/6DlKxSFzoWQz9kIwFGzFKtCkNdq2uH8CYiVSTIOFEztMfdOmZN/xJZt7AH7BrdGB0aHRkdGx0gn0fl0jW/2bU+adK6FS4zMMrWa+Th6VYZQ9LNudq5gsqO42XY4Vf3XxBHq+SFesFd5IjBViQdfsF64MY/h/goNly2k6+8GrilkkLlas7aP7yhxFNQhCacqJUz3Vi3U+J1IxyMPNPFMSETskYeiYVJATVT/NfP8f7pjLEo0gaCY3zanVHSkKlZmFgyJDoiar3suJzvV6iR5/7KRNxokHQ4kWjhGMd4ewe4SGTQDWfmYRQycxZMZ0QSag2t61hhuDWP3k1uTxou07bPT9qnX4px7GJ3qH36CNy0Sd0is5QF3mIWj+tO+vB+m3f2Pf2H/tvgdpWuWcPLS378Qn/5eUL</latexit><latexit sha1_base64="/jMC2ZujAyrKvdeVqpyvnIEKocg=">AAADOnicbZI9b9swEIYppUlTt0mcZuxC1EjRyZDy1Y5B6yHdHCBKAliGQdFnmzBFCSRVwFD9u7rkV3TLkCVDiqJrfkCoj9ayHArv4XD38BV1YhBzprTj3Fr22ov1jZebrxqv32xt7zR3316qKJEUPBrxSF4HRAFnAjzNNIfrWAIJAw5XwfRr1r/6DlKxSFzoWQz9kIwFGzFKtCkNdq2uH8CYiVSTIOFEztMfdOmZN/xJZt7AH7BrdGB0aHRkdGx0gn0fl0jW/2bU+adK6FS4zMMrWa+Th6VYZQ9LNudq5gsqO42XY4Vf3XxBHq+SFesFd5IjBViQdfsF64MY/h/goNly2k6+8GrilkkLlas7aP7yhxFNQhCacqJUz3Vi3U+J1IxyMPNPFMSETskYeiYVJATVT/NfP8f7pjLEo0gaCY3zanVHSkKlZmFgyJDoiar3suJzvV6iR5/7KRNxokHQ4kWjhGMd4ewe4SGTQDWfmYRQycxZMZ0QSag2t61hhuDWP3k1uTxou07bPT9qnX4px7GJ3qH36CNy0Sd0is5QF3mIWj+tO+vB+m3f2Pf2H/tvgdpWuWcPLS378Qn/5eUL</latexit><latexit sha1_base64="/jMC2ZujAyrKvdeVqpyvnIEKocg=">AAADOnicbZI9b9swEIYppUlTt0mcZuxC1EjRyZDy1Y5B6yHdHCBKAliGQdFnmzBFCSRVwFD9u7rkV3TLkCVDiqJrfkCoj9ayHArv4XD38BV1YhBzprTj3Fr22ov1jZebrxqv32xt7zR3316qKJEUPBrxSF4HRAFnAjzNNIfrWAIJAw5XwfRr1r/6DlKxSFzoWQz9kIwFGzFKtCkNdq2uH8CYiVSTIOFEztMfdOmZN/xJZt7AH7BrdGB0aHRkdGx0gn0fl0jW/2bU+adK6FS4zMMrWa+Th6VYZQ9LNudq5gsqO42XY4Vf3XxBHq+SFesFd5IjBViQdfsF64MY/h/goNly2k6+8GrilkkLlas7aP7yhxFNQhCacqJUz3Vi3U+J1IxyMPNPFMSETskYeiYVJATVT/NfP8f7pjLEo0gaCY3zanVHSkKlZmFgyJDoiar3suJzvV6iR5/7KRNxokHQ4kWjhGMd4ewe4SGTQDWfmYRQycxZMZ0QSag2t61hhuDWP3k1uTxou07bPT9qnX4px7GJ3qH36CNy0Sd0is5QF3mIWj+tO+vB+m3f2Pf2H/tvgdpWuWcPLS378Qn/5eUL</latexit><latexit sha1_base64="/jMC2ZujAyrKvdeVqpyvnIEKocg=">AAADOnicbZI9b9swEIYppUlTt0mcZuxC1EjRyZDy1Y5B6yHdHCBKAliGQdFnmzBFCSRVwFD9u7rkV3TLkCVDiqJrfkCoj9ayHArv4XD38BV1YhBzprTj3Fr22ov1jZebrxqv32xt7zR3316qKJEUPBrxSF4HRAFnAjzNNIfrWAIJAw5XwfRr1r/6DlKxSFzoWQz9kIwFGzFKtCkNdq2uH8CYiVSTIOFEztMfdOmZN/xJZt7AH7BrdGB0aHRkdGx0gn0fl0jW/2bU+adK6FS4zMMrWa+Th6VYZQ9LNudq5gsqO42XY4Vf3XxBHq+SFesFd5IjBViQdfsF64MY/h/goNly2k6+8GrilkkLlas7aP7yhxFNQhCacqJUz3Vi3U+J1IxyMPNPFMSETskYeiYVJATVT/NfP8f7pjLEo0gaCY3zanVHSkKlZmFgyJDoiar3suJzvV6iR5/7KRNxokHQ4kWjhGMd4ewe4SGTQDWfmYRQycxZMZ0QSag2t61hhuDWP3k1uTxou07bPT9qnX4px7GJ3qH36CNy0Sd0is5QF3mIWj+tO+vB+m3f2Pf2H/tvgdpWuWcPLS378Qn/5eUL</latexit>(a) Code:
(b) Abstract syntax
     tree (AST):
(c) AST depth-ï¬rst traversal:(d) Tree positional encodings:
(e) Tree relative attention:
(f) GGNN Sandwich:
 Types of edges:
â€¢Parent (P)
â€¢Child (C)â€¢Left (L)
â€¢Right (R)123456
1 P, LP
2C, R L
3CR P, LP
4 C, R L
5 CR P, L
6 C, R
<latexit sha1_base64="vZu5ft0fe9mvZRkgNrvwUk74f9k=">AAADFXicbVJNb9NAEF27fJTwlcKxl1UjEBJVZJe0cKzIpYce0oq0leIoWm8myarrtbW7RopM/kQv/Su9cAAhrkjc+DfMJlZMEsb75OeZN8/r8caZFMYGwR/P37p3/8HD7Ue1x0+ePnte33lxYdJcc+jyVKb6KmYGpFDQtcJKuMo0sCSWcBlft1398jNoI1L1yU4z6CdsrMRIcGYxNdjx3kYxjIUqLItzyfSs+MJXrlktmjjzGn1NQ8QB4h2ihThEHNEooqXE1XF19umpuy2e3Kokrr29T88XhdOlpFI48zbifNOsErWWjWtmleSwkqy5VZqjaosrZpUiAjVczmZQbwTNYB50k4QlaZAyOoP672iY8jwBZblkxvTCILP9gmkruAQcbW4gY/yajaGHVLEETL+Y/9UZfYWZIR2lGqEsnWf/7ShYYsw0iVGZMDsx6zWX/F+tl9vRh34hVJZbUHzxolEuqU2pOyJ0KDRwK6dIGNcC90r5hGnGLR6kGg4hXP/kTXJx0AyDZnjWahx/LMexTXbJHnlDQvKeHJMT0iFdwr0b78775n33b/2v/g//50Lqe2XPS7IS/q+/d0zW+Q==</latexit><latexit sha1_base64="vZu5ft0fe9mvZRkgNrvwUk74f9k=">AAADFXicbVJNb9NAEF27fJTwlcKxl1UjEBJVZJe0cKzIpYce0oq0leIoWm8myarrtbW7RopM/kQv/Su9cAAhrkjc+DfMJlZMEsb75OeZN8/r8caZFMYGwR/P37p3/8HD7Ue1x0+ePnte33lxYdJcc+jyVKb6KmYGpFDQtcJKuMo0sCSWcBlft1398jNoI1L1yU4z6CdsrMRIcGYxNdjx3kYxjIUqLItzyfSs+MJXrlktmjjzGn1NQ8QB4h2ihThEHNEooqXE1XF19umpuy2e3Kokrr29T88XhdOlpFI48zbifNOsErWWjWtmleSwkqy5VZqjaosrZpUiAjVczmZQbwTNYB50k4QlaZAyOoP672iY8jwBZblkxvTCILP9gmkruAQcbW4gY/yajaGHVLEETL+Y/9UZfYWZIR2lGqEsnWf/7ShYYsw0iVGZMDsx6zWX/F+tl9vRh34hVJZbUHzxolEuqU2pOyJ0KDRwK6dIGNcC90r5hGnGLR6kGg4hXP/kTXJx0AyDZnjWahx/LMexTXbJHnlDQvKeHJMT0iFdwr0b78775n33b/2v/g//50Lqe2XPS7IS/q+/d0zW+Q==</latexit><latexit sha1_base64="vZu5ft0fe9mvZRkgNrvwUk74f9k=">AAADFXicbVJNb9NAEF27fJTwlcKxl1UjEBJVZJe0cKzIpYce0oq0leIoWm8myarrtbW7RopM/kQv/Su9cAAhrkjc+DfMJlZMEsb75OeZN8/r8caZFMYGwR/P37p3/8HD7Ue1x0+ePnte33lxYdJcc+jyVKb6KmYGpFDQtcJKuMo0sCSWcBlft1398jNoI1L1yU4z6CdsrMRIcGYxNdjx3kYxjIUqLItzyfSs+MJXrlktmjjzGn1NQ8QB4h2ihThEHNEooqXE1XF19umpuy2e3Kokrr29T88XhdOlpFI48zbifNOsErWWjWtmleSwkqy5VZqjaosrZpUiAjVczmZQbwTNYB50k4QlaZAyOoP672iY8jwBZblkxvTCILP9gmkruAQcbW4gY/yajaGHVLEETL+Y/9UZfYWZIR2lGqEsnWf/7ShYYsw0iVGZMDsx6zWX/F+tl9vRh34hVJZbUHzxolEuqU2pOyJ0KDRwK6dIGNcC90r5hGnGLR6kGg4hXP/kTXJx0AyDZnjWahx/LMexTXbJHnlDQvKeHJMT0iFdwr0b78775n33b/2v/g//50Lqe2XPS7IS/q+/d0zW+Q==</latexit><latexit sha1_base64="vZu5ft0fe9mvZRkgNrvwUk74f9k=">AAADFXicbVJNb9NAEF27fJTwlcKxl1UjEBJVZJe0cKzIpYce0oq0leIoWm8myarrtbW7RopM/kQv/Su9cAAhrkjc+DfMJlZMEsb75OeZN8/r8caZFMYGwR/P37p3/8HD7Ue1x0+ePnte33lxYdJcc+jyVKb6KmYGpFDQtcJKuMo0sCSWcBlft1398jNoI1L1yU4z6CdsrMRIcGYxNdjx3kYxjIUqLItzyfSs+MJXrlktmjjzGn1NQ8QB4h2ihThEHNEooqXE1XF19umpuy2e3Kokrr29T88XhdOlpFI48zbifNOsErWWjWtmleSwkqy5VZqjaosrZpUiAjVczmZQbwTNYB50k4QlaZAyOoP672iY8jwBZblkxvTCILP9gmkruAQcbW4gY/yajaGHVLEETL+Y/9UZfYWZIR2lGqEsnWf/7ShYYsw0iVGZMDsx6zWX/F+tl9vRh34hVJZbUHzxolEuqU2pOyJ0KDRwK6dIGNcC90r5hGnGLR6kGg4hXP/kTXJx0AyDZnjWahx/LMexTXbJHnlDQvKeHJMT0iFdwr0b78775n33b/2v/g//50Lqe2XPS7IS/q+/d0zW+Q==</latexit>1: 000 000 000
2: 100 000 000
3: 010 000 000
4: 100 010 000
5: 010 010 000
6: 100 010 010
TransGGNN
GGNN
Trans
INPUTOUTPUTstack-like enc.
S
S
S
S
S
Sâ€¢Self (S)
Figure 1:Illustrationofmechanisms forprocessing ASTstructure inTransformer.
also show that using the proposed anonymization can im-
prove the quality ofthemodel, either a singleTransformer
oran ensemble ofTransformers.
â€¢We conduct an ablation study of different syntax-capturing
components in Transformer, underlining which ones are
essentialforachievinghighqualityandanalysingtheresults
obtainedfor the anonymizedsetting.
Our source code is available at https://github.com/bayesgroup/
code_transformers .
The rest of the work is organized as follows. In Section 2we
review theexistingapproachesforutilizingsyntacticinformation
inTransformer.InSection 3wedescribe ourmethodologyforthe
empiricalevaluationofTransformercapabilitiestoutilizesyntactic
information.ThefollowingSections 6Å›8describeourempiricalfind-
ings. In Section 10,we give thereview of the literatureconnected
to our research. Finally, Section 11discusses threats to validity and
Section12concludes the work.
2 REVIEW OFTRANSFORMERS FORSOURCE
CODE
Abstract syntax tree. A syntactic structure of code is usually
representedintheformofanabstractsyntaxtree(AST).Eachnode
ofthetreecontainsatype,whichrepresentsthesyntacticunitofthe
programming language (e.g. "Assign", "Index", "NameLoad"), some
nodes also contain a value (e.g. "idx", "elem"). Values store user-
defined variable names, reserved language names, integers, strings
etc. An example AST for acode snippetisshowninFigure 1(b).
2.1 Transformer Architectureand
Self-AttentionMechanism
We describe Transformer architecture for a task of mapping input
sequence ğ‘1,...ğ‘ğ¿,ğ‘ğ‘–âˆˆ {1,...,ğ‘€}(ğ‘€is a vocabulary size) to a
sequenceof ğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™-dimensionalrepresentations ğ‘¦1,...,ğ‘¦ğ¿thatcan
beusedformakingtask-specificpredictionsinvarioustasks.BeforebeingpassedtotheTransformerblocks,theinputsequenceisfirstly
mappedintoasequenceof embeddings ğ‘¥1,...,ğ‘¥ğ¿,ğ‘¥ğ‘–âˆˆRğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™.
A key ingredient of a Transformer block is a self-attention layer
thatmapsinputsequence ğ‘¥1,...ğ‘¥ğ¿,ğ‘¥ğ‘–âˆˆRğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™toasequenceof
the same length: ğ‘§1,...,ğ‘§ğ¿,ğ‘§ğ‘–âˆˆRğ‘‘ğ‘§.Self-attention firstcomputes
key, query,andvalue vectorsfromeachinputvector: ğ‘¥ğ‘˜
ğ‘—=ğ‘¥ğ‘—ğ‘Šğ¾,
ğ‘¥ğ‘
ğ‘—=ğ‘¥ğ‘—ğ‘Šğ‘„andğ‘¥ğ‘£
ğ‘—=ğ‘¥ğ‘—ğ‘Šğ‘‰. Each output ğ‘§ğ‘–is computed as a
weightedcombination ofinputs:
ğ‘§ğ‘–=/summationdisplay.1
ğ‘—Ëœğ›¼ğ‘–ğ‘—ğ‘¥ğ‘£
ğ‘—,Ëœğ›¼ğ‘–ğ‘—=exp(ğ‘ğ‘–ğ‘—)/summationtext.1
ğ‘—exp(ğ‘ğ‘–ğ‘—), ğ‘ğ‘–ğ‘—=ğ‘¥ğ‘
ğ‘–ğ‘¥ğ‘˜
ğ‘—ğ‘‡
âˆšğ‘‘ğ‘§(1)
Attention weights Ëœğ›¼ğ‘–ğ‘—â©¾0,/summationtext.1ğ¿
ğ‘—=1ğ›¼ğ‘–ğ‘—=1 are computed based
onquery-keysimilarities.Severalattentionlayers(heads)areap-
pliedinparallelwithdifferentprojectionmatrices ğ‘Šğ‘‰
â„,ğ‘Šğ‘„
â„,ğ‘Šğ¾
â„,
â„=1,...,ğ». The outputs are concatenated and projected to ob-
tainË†ğ‘¥ğ‘–=[ğ‘§1
ğ‘–,...,ğ‘§ğ»
ğ‘–]ğ‘Šğ‘‚,ğ‘Šğ‘‚âˆˆRğ»ğ‘‘ğ‘§Ã—ğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™. A Transformer
blockincludesthedescribedmulti-headattention,aresidualcon-
nection, a layernormalization,and a position-wise fully-connected
layer. The overall Transformer architecture is composed by the
consequentstackingofthedescribedblocks.WhenapplyingTrans-
formertogenerationtasks,futureelements( ğ‘–>ğ‘—)aremaskedin
self-attention (Transformer decoder). Without this masking, the
stackofthelayersiscalledTransformer encoder.Inthesequence-
to-sequence task, when both encoder and decoder are used, the
attention from decoder to encoder is also incorporated into the
model.
2.2 PassingASTs to Transformers
Forourstudy,weselecttwocommonlyusedNLPapproachesforuti-
lizing sequential structure and three approaches developed specifi-
cally for utilizing sourcecode structure inTransformer.
Sequential positional encodings and embeddings. Transformers
were initially developed for NLP and therefore were augmented
704Empirical Studyof TransformersforSource Code ESEC/FSE â€™21, August 23â€“28, 2021,Athens,Greece
with sequence-capturing mechanisms to account for sequential
input structure. As a result, the simplest way of applying Trans-
formers to AST is to traverseASTin someorder, e.g., indepth-first
order (see Figure 1(c)), and use standard sequence-capturing mech-
anisms.
Toaccountforthesequentialnatureoftheinput,standardTrans-
formerisaugmentedwithpositionalencodingsorpositionalem-
beddings. Namely, the input embeddings ğ‘¥ğ‘–âˆˆğ‘…ğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™are summed
up with positional representations ğ‘ğ‘–âˆˆğ‘…ğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™:Ë†ğ‘¥ğ‘–=ğ‘¥ğ‘–+ğ‘ğ‘–. For
example,positionalembeddingsimplylearningtheembeddingvec-
tor of each position ğ‘–âˆˆ1...ğ¿:ğ‘ğ‘–=ğ‘’ğ‘–, ğ‘’ğ‘–âˆˆğ‘…ğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™. Positional
encoding implies computing ğ‘ğ‘–based on sine and cosine functions.
We include positional embeddings in our comparisons and add the
prefix Å‚sequentialÅ¾ to the title of this mechanism. This approach
wasusedas abaselineinseveral ofworks [ 14,32].
Sequentialrelativeattention. Shawetal .[31]proposedrelativeat-
tentionforcapturingtheorderoftheinputelements.Theyaugment
self-attentionwithrelative embeddings:
ğ‘§ğ‘–=/summationdisplay.1
ğ‘—Ëœğ›¼ğ‘–ğ‘—(ğ‘¥ğ‘£
ğ‘—+ğ‘’ğ‘£
ğ‘–âˆ’ğ‘—),Ëœğ›¼ğ‘–ğ‘—=exp(ğ‘ğ‘–ğ‘—)/summationtext.1
ğ‘—exp(ğ‘ğ‘–ğ‘—), ğ‘ğ‘–ğ‘—=ğ‘¥ğ‘
ğ‘–(ğ‘¥ğ‘˜
ğ‘—+ğ‘’ğ‘˜
ğ‘–âˆ’ğ‘—)ğ‘‡
âˆšğ‘‘ğ‘§,
(2)
whereğ‘’ğ‘£
ğ‘–âˆ’ğ‘—,ğ‘’ğ‘˜
ğ‘–âˆ’ğ‘—âˆˆRğ‘‘ğ‘§are learned embeddings for each relative
positionğ‘–âˆ’ğ‘—, e.g.one token is located two tokens to the left from
another token. This mechanism, that we call sequential relative
attention,wasshowntosubstantiallyoutperformsequentialposi-
tionalembeddingsandencodingsinsequence-basedtextprocessing
tasks. Ahmad et al . [1]reach the same conclusion evaluating se-
quential relative attention in a task of code summarization, i.e.,
generatingnaturallanguagesummariesfor code snippets.
Treepositionalencodings. Inspiredbypreviouslydiscussedworks,
severalauthorsdevelopedmechanismsforprocessingtrees.Shiv
and Quirk [32]develop positional encodings for tree-structured
data,assumingthatthemaximumnumber ğ‘›ğ‘¤ofnodechildrenand
the maximum depth ğ‘›ğ‘‘of the tree are relatively small. Example
encodingsaregiveninFigure 1(d).Thepositionofeachnodeina
tree is defined by its path from the root, and each child number
inthepathisencodedusing ğ‘›ğ‘¤-sizedone-hotvector.Theoverall
representationofanodeisobtainedbyconcatenatingtheseone-hot
vectorsinreverseorderandpaddingshortpathswithzerosfrom
theright.Theauthorsalsointroducethelearnableparametersof
theencoding,theirnumberequals ğ‘‘model/(ğ‘›ğ‘¤Â·ğ‘›ğ‘‘).Pathslonger
thanğ‘›ğ‘‘are clipped (the root node is clipped first). The authors
binarize ASTs to achieve ğ‘›ğ‘¤=2. To avoid this binarization, we
replaceallchildnumbersgreaterthan ğ‘›ğ‘¤withğ‘›ğ‘¤,andselectthe
besthyperparameters ğ‘›ğ‘¤andğ‘›ğ‘‘usinggridsearch,seedetailsin
section4.
Shiv and Quirk [32]tested the approach on the task of code
translation (code-to-code) and semantic parsing (text-to-code). The
Transformerwithtreepositionalencodingsoutperformedstandard
TransformerwithsequentialpositionalencodingsandTreeLSTM
[34].
Tree relative attention. An extension of sequential relative atten-
tion for trees was proposed by Kim et al . [20]. In a sequence, the
distancebetweentwoinputpositionsisdefinedasthenumberofpositions between them. Similarly, in a tree, the distance between
two nodes can be defined as the shortest path between nodes, con-
sisting of ğ‘›ğ‘ˆâ©¾0 stepsupandğ‘›ğ·â©¾0 stepsdown, see example
in Figure 1(e). Now, similarly to sequential relative attention, we
can learn embeddings for the described distances and plug them
into self-attention. Learning multidimensional embeddings for the
tree input requires muchmore memory than for sequentialinput,
sincedistancesinthetreeareobject-specific,whiledistancesinthe
sequencearethesameforallobjectsinamini-batch.Asaresult,
the authors use scalar embedding ğ‘Ÿğ‘–ğ‘—âˆˆRfor the distance between
nodesğ‘–andğ‘—andplug itintotheattentionmechanismasfollows
(otherformulas staythe same):
Ëœğ›¼ğ‘–ğ‘—=exp(ğ‘ğ‘–ğ‘—Â·ğ‘Ÿğ‘–ğ‘—)/summationtext.1
ğ‘—exp(ğ‘ğ‘–ğ‘—Â·ğ‘Ÿğ‘–ğ‘—). (3)
Ourpreliminaryexperimentssuggestedthatusingsummation ğ›¼ğ‘–ğ‘—+
ğ‘Ÿğ‘–ğ‘—insteadofmultiplicationleadstoahigherfinalscore.Theauthors
tested the approach on the task of code completion, i.e., predicting
thenexttoken,andshowedthatusingmodifiedattentionimproves
qualitywhenapplyingTransformertoASTtraversalandtocode
as text.
GGNN Sandwich. Due to the graph nature of AST, source codes
areoftenprocessedusinggraphgatedneuralnetworks(GGNN)[ 4].
Toaddmoreinductivebias,ASTisaugmentedwithedgesofsev-
eraladditionaltypes,e.g.,reflectingdata-andcontrol-flowinthe
program. Such a model captures localdependencies in data well
but lacks a globalview of the input program, that is the Trans-
formerâ€™sforte.Inspiredbythisreasoning,Hellendoornetal .[14]
proposealternatingTransformerandGGNNlayersasillustratedin
Figure1(f), tocombine thestrengths ofboth models. GGNNlayer
reliesonpassingmessagesthroughedgesforafixednumberofiter-
ations (number of passes). The model is called GGNN Sandwich by
the authors, and the details can be found in [ 14]. GGNN Sandwich
was shown to be effective in the variable misuse detection task,
i.e., predicting the location of a bug and the location used to fix
thebug(copyvariable).GGNNSandwichoutperformedstandard
Transformer withsequentialpositional encodings.
Our work focuses of processing syntactic information in Trans-
former,thuswedonotusedata-andcontrol-flowedges.Data-or
control-flowedgesarehardtoincorporateinothermechanismsex-
cept GGNN Sandwich. In our GGNN Sandwich, we use AST edges,
edgesconnectingtheneighbouringnodesintheASTdepth-firsttra-
versal, andedges connecting nodes to themselves, see illustration
infig.1(f).
Hellendoornetal .[14]alsoproposeamodelcalledGREATthatis
inspired by relative attention and incorporates 1-dimensional edge
embeddingsintotheattentionmechanism.Thismodelisconcep-
tually similarto the tree relative attention, thus we donot include
GREAT inour comparison.
3 LIMITATIONSOFEXISTING APPROACHES
AND METHODOLOGYOFTHE WORK
AsshowninSection 2severalapproachesforprocessingASTsin
Transformershavebeenproposed.However,itisstillunclearwhich
approaches perform better than others and what mechanisms to
useinpractice.First,alltheworksdiscussedinSection 2conduct
705ESEC/FSE â€™21, August 23â€“28, 2021,Athens,Greece N.Chirkova andS.Troshin
experimentswithdifferenttasksmakingithardtoaligntheresults.
Moreover,almostallthelistedworkscomparetheirapproacheswith
thevanillaTransformer,i.e.,Transformerwithsequentialpositional
encodings or embeddings, while modern practices use advanced
mechanisms, like sequential relative attention, by default. Even
worksthatproposetree-processingapproachesinspiredbysequen-
tialrelativeattentiondonotincludethismechanismasabaseline.
Thatis,itisunclearwhetherusingadvancedtree-processingmech-
anisms isbeneficialatall. Secondly, theexisting approachesfocus
oncapturingtree structure anddonotinvestigatetheinfluenceof
othercomponentsofAST,i.e.,types andvalues.
Inthiswork,weconductathoroughempiricalstudyonutilizing
ASTinTransformers.Weconsiderthreecodeprocessingtasks:vari-
ablemisuse(VM)detection,functionnaming(FN),andcodecom-
pletion(CC),andtwosourcecodedatasets:Python150k[ 38]and
JavaScript150k[ 30].Weselectedtasksthatareoftenusedasbench-
marksintheliteratureandonwhichthecomparedapproacheswere
tested by their authors. Our selection also covers various Trans-
formerconfigurations,i.e.,encoderonly(VM),decoderonly(CC)
and encoder-decoder (FN). We selected the Python150k dataset be-
cause it is often used in the literature, and JavaScript150k because
itisdistributedbythe same authorsandhas the same format.
We re-implement all mechanisms described in Section 2in a
unified framework and investigate the most effective approach for
processing ASTs in Transformer in different tasks. We answer the
following researchquestions:
â€¢What is the most effective approach for utilizing AST struc-
tureinTransformer?
â€¢Is Transformer generally capable of utilizing syntactic infor-
mation representedviaAST?
â€¢Whatcomponents of AST(structure, node types and values)
does Transformer use indifferenttasks?
ThereisnocommonpracticeofpreprocessingASTs,particularly,
processing values. Each node in AST is associated with a type, but
notallnodeshaveassociatedvalues.Kimetal . [20]andShivand
Quirk[32]attach values as separate child nodes so that each node
storesonlyoneitem(typeorvalue),whileHellendoornetal .[14]
proposeomittingtypes.Theformerapproachincreasesinputlength
and thus makes code processing significantly slower, while the
latterapproachlosestypeinformation.Wechooseanin-between
strategyinspiredbytheapproachofLietal .[23]usedforRNNs:we
associatethe <empty>valuewithnodesthatdonotehavevalues,so
thateachnode ğ‘–inASThasbothtype ğ‘¡ğ‘–andvalue ğ‘£ğ‘–,seeFigure 1(c).
ThissetuppreservestheinitialASTstructureandallowsustoeasily
ablate types or values, leaving the other item in each node present.
Some works show that splitting values based on snake_case or
CamelCase , or using splitting techniques such as byte-pairencod-
ing may improve the quality [ 6,19]. We do not use splitting into
subtokensfortworeasons.Firstly,splittingmakessequencesmuch
longer, resultingin a substantial slow down of training procedure
becauseofquadraticTransformercomplexityw.r.t.theinputlength.
Secondly, splitting breaks the one-to-one correspondence between
AST nodes and values, i.e., severalvalues belong to one AST node.
TherearedifferentwaysofadaptingAST-basedTransformersto
thedescribedproblem:oneoptionistoaverageembeddingsover
subtokens [ 14], another option is to assign a chain of subtokensas a child of a node and then directly apply tree-processing mech-
anisms. A third option is to modify tree-processing mechanisms,
e.g.,duplicatepathsforallsubtokensintreepositionalencodingor
duplicatetree relationsfor allpairsof subtokensof twotokensin
treerelativeattention.Asaresult,thequestionofhowsplittinginto
subtokensaffectssyntax-capturingmechanismsrequiresaseparate
study whichwe leave for the future work.
An important part of our methodology is conducting experi-
mentsintwosettings,namely anonymized andfull-data.Thefull-
data setting corresponds to the conventional training of Trans-
formeronASTsparsedfromcode.Inthiscase,Transformerhastwo
sourcesofinformationaboutinputcodesnippets:syntacticinforma-
tion and user-defined identifiers (stored in node values). Identifiers
usuallygivemuch additional informationaboutthesemanticsof
the code, however, their presence is not necessary for correct code
execution: renaming all user-defined identifiers with placeholders
var1,var2,var3etc.willleadtothesameresultofcodeexecution
and will not change the semantics of the algorithm the code im-
plements. Here we mean that all occurrences of an identifier are
replacedwiththesameplaceholder,thus,importantinformation
aboutidentifierrepetitionissaved.Wecallthisrenamingidentifiers
withplaceholdersas anonymization .Intheanonymizedsetting,the
inputcodeisrepresentedpurelywithsyntaxstructureandtheonly
way Transformer can make meaningful predictions is to capture
informationfromAST.Inthisway,usingtheanonymizedsetting
allows a better understanding of the capabilities of Transformer to
utilizesyntacticinformation.Moredetailsontheanonymization
procedure are given inSupplementary materials.
Another important part of our methodology is a thoughtful
splittingofthedatasetintotrainingandtestingsets,whichincludes
splitting by repository and removing code duplicates. Alon et al .
[6], LeClair et al . [22]notice that code files inside one repository
usually share variable names and code patterns, thus splitting files
from one repository between training and testing sets simplifies
predictions for the testing set and leads to a data leak. To avoid
this,oneshouldputallfilesfromonerepositoryintooneset,either
trainingortesting(thisstrategyiscalledsplittingbyrepository).
Evenusingthisstrategy,duplicatecodecanstilloccurinthetesting
set, since developers often copy code from other projects or fork
other repositories. Allamanis [3]underline that in commonly used
datasetsupto20%oftestingobjectscanberepeatedinthetraining
set, biasing evaluation results. As a result, the deduplication step is
neededafter data splitting.
4 EXPERIMENTALSETUP
Data.Inalltasks,weusethePython150k(PY)dataset[ 28](redis-
tributableversion)andJavaScript150k(JS)dataset[ 30]downloaded
fromtheofficialrepositoryat https://eth-sri.github.io .Bothdatasets
consistofcodefilesdownloadedfromGithubandarecommonly
usedto evaluate code processing models.
Most research use the train-test split provided by the authors
of the dataset, however, this split does not follow best practices
described in Section 3and produce biased results [ 3], so we release
a new split of the dataset. We remove duplicate files from both
datasets using the list of duplicates provided by Allamanis [3]. We
also filter out absolutely identical code files, and when selecting
706Empirical Studyof TransformersforSource Code ESEC/FSE â€™21, August 23â€“28, 2021,Athens,Greece
functions from code files, we additionally filter out absolutely iden-
tical functions. We split data into training / validation / testing sets
in proportion 60% / 6.7% / 33.3% based on Github usernames (each
repository isassignedto one username).
Preprocessingdetailsforeachtaskaregivenbelow.Werelease
ourdatasplitandoursourcecodeincludingscriptsfordownloading
data, deterministic code for data preprocessing, models, training
etc. We largely rely on the implementations of other research, and
compare the quality of our baseline models to the results reported
inotherpapers, when possible; see details inSection 9.
Variable misuse task (VM). For the variable misuse task, we use
the setup and evaluation strategy of Hellendoorn et al .[14]. Given
thecodeofafunction,thetaskistoidentifytwopositions(usingtwo
pointers):oneinwhichpositionawrongvariableisused,andonein
whichpositionacorrectvariablecanbecopiedfrom(anysuchposi-
tion is accepted). If a snippet is non-buggy, the first pointer should
select a special no-bug position. We obtain two pointers, by apply-
ing two position-wise fully-connected layers, and softmax over po-
sitionsontopofTransformeroutputs.Forexample,thefirstpointer
selects position as argmax1â©½ğ‘–â©½ğ¿softmax([ğ‘¢ğ‘‡ğ‘¦1,...,ğ‘¢ğ‘‡ğ‘¦ğ¿,ğ‘]),ğ‘¦ğ‘–âˆˆ
Rğ‘‘model, ğ‘¢âˆˆRğ‘‘model,ğ‘âˆˆR(ğ‘is a learnable scalar corresponding
totheno-bugposition), [...]denotestheconcatenationoftheel-
ements into a vector of scalars. The second pointer is computed
in a similar way but without ğ‘. The model is trained using the
cross-entropy loss.
Toprocess theoriginaldataset for thevariable misusetask,we
select all top-level functions, including functions inside classes,
from all (filtered) 150K files, and filter out functions: longer than
250nodes(toavoidverylongfunctions);andfunctionswithless
than three positions containing user-defined variables or less than
three distinct user-defined variables (to avoid trivial bug fixes). We
select a function with a root node type FunctionDef for PY, and
FunctionDeclaration orFunctionExpression forJS.Theresult-
ingtraining / validation / testingsetconsists of417K / 48K/ 231K
functions for PY and 202K / 29K / 108K for JS. One function may
occur in the dataset up to 6 times, 3 times with a synthetically gen-
eratedbugand3timeswithoutabug.Following[ 14],weusethis
strategytoavoidbiasingtowardslongfunctionswithalotofdiffer-
entvariables.Thebuggyexamplesaregeneratedsyntheticallyby
choosingrandombugandfixpositionsfrompositionscontaining
user-definedvariables.
Weusethejointlocalizationandrepairaccuracymetricof[ 14]to
assessthequalityofthemodel.Thismetricestimatestheportionof
buggy samples for which the model correctly localizes and repairs
thebug.Wealsomeasuredlocalizationaccuracyandrepairaccuracy
independently and foundthat all three metricscorrelate well with
eachother.
Function naming task (FN). In this task, given the code of a func-
tion,thetaskistopredictthenameofthefunction.Tosolvethistask,
we use the classic sequence-to-sequence Transformer architecture
that outputs the function name word by word. A particular imple-
mentationisborrowedfrom[ 1](thepaperusedanotherdataset).
Firstly,wepassfunctioncodetotheTransformerencodertoobtain
code representations ğ‘¦1,...,ğ‘¦ğ¿. Then, the Transformer decoder
generatesthemethodnamewordbyword,andduringeachwordgeneration,decoderattendsto ğ‘¦1,...,ğ‘¦ğ¿(usingencoder-decoder
attention) and to previously generated tokens (using masked de-
coderattention).Toaccountforthesequentialorderofthefunction
name,weusesequentialpositionalembeddingsintheTransformer
decoder.Intheencoder,weconsiderdifferentstructure-capturing
mechanisms.Weusegreedydecoding.Wetrainthewholeencoder-
decoder modelend-to-end, optimizing the cross-entropy loss.
Toobtaintheprocesseddatasetforthefunctionnametask,we
select all top-level functions, including functions inside classes,
from all (filtered) 150K files, and filter out functions longer than
250 AST nodes (to avoid very long functions), functions for which
thenamecouldnotbeextracted(alotof FunctionExpression sin
JS are anonymous), and functions with names consisting of only
underscore characters and names containing rare words (less than
5/3occurrencesinthetrainingsetforPY/JS).Toextractfunctions,
weusethesamerootnodetypesasintheVMtask.Theresulting
dataset consists of 523K / 56K / 264K training/validation/testing
functions for PY and 186K / 23K / 93K for JS. We replace func-
tion name in the AST with a special <fun_name> token. To ex-
tract target function names, we remove extra underscores and
spliteachfunctionnamebasedon CamelCase orsnake_case ,e.g.,
name_get_feature_names becomes [ get,feature,names]. A
meanÂ±stdlengthoffunctionnameis2.42 Â±1.46wordsforPYand
2.22Â±1.23for JS.
We assess the quality of the generated function namesusing the
F1-metric. If ğ‘”ğ‘¡ğ‘›is a set of words in ground-truth function name
andğ‘ğ‘›is a set of words in predicted function name, the F1-metric
is computed as 2 ğ‘ƒğ‘…/(ğ‘ƒ+ğ‘…) âˆˆ [0,1], whereğ‘ƒ=|ğ‘”ğ‘¡ğ‘›âˆ©ğ‘ğ‘›|/|ğ‘ğ‘›|,
ğ‘…=|ğ‘”ğ‘¡ğ‘›âˆ©ğ‘ğ‘›|/|ğ‘”ğ‘¡ğ‘›|,| Â· |denotes the number of elements. F1 is
averagedoverfunctions.WechoosetheF1metricfollowingAlon
et al.[6]who solved a similar task with another dataset and model.
Codecompletiontask(CC). Forthetaskofcodecompletion,we
use the setup, metrics and Transformer implementation of Kim
et al.[20]. The task is to predict the next node (ğ‘¡ğ‘–,ğ‘£ğ‘–)in the depth-
firsttraversalofAST [(ğ‘¡1,ğ‘£1),...,(ğ‘¡ğ‘–âˆ’1,ğ‘£ğ‘–âˆ’1)].Wepredicttype ğ‘¡ğ‘–
and value ğ‘£ğ‘–using two fully connected layers with softmax on
top of the prefix representation ğ‘¦ğ‘–:ğ‘ƒ(ğ‘¡ğ‘–)=softmax(ğ‘Šğ‘¡ğ‘¦ğ‘–),ğ‘Šğ‘¡âˆˆ
R#typesÃ—ğ‘‘model,ğ‘ƒ(ğ‘£ğ‘–)=softmax(ğ‘Šğ‘£ğ‘¦ğ‘–),ğ‘Šğ‘£âˆˆR#valuesÃ—dmodel.
Toobtainthedatasetforthecodecompletiontask,weusefull
ASTsfrom(filtered)150kfiles,removingsequenceswithlengthless
than2.IfthenumberofASTnodesislargerthan ğ‘›ğ‘ğ‘¡ğ‘¥=500,wesplit
ASTintooverlappingchunksoflength ğ‘›ğ‘ğ‘¡ğ‘¥withashift1
2ğ‘›ğ‘ğ‘¡ğ‘¥.The
overlapprovidesacontextforthemodel.Forexample,ifthelength
ofASTis800,weselectthefollowingsamples: ğ´ğ‘†ğ‘‡[:500),ğ´ğ‘†ğ‘‡[250:
750),ğ´ğ‘†ğ‘‡[300:800]. We do not calculate loss or metrics over the
intersection twice. For the previous example, the quality of predic-
tionsismeasuredonlyon ğ´ğ‘†ğ‘‡[:500),ğ´ğ‘†ğ‘‡[500:750),ğ´ğ‘†ğ‘‡[750:800].
Theoverlappingsplittingprocedureisborrowedfrom[ 20]andis
neededsinceprocessingextremelylongsequencesistooslowin
Transformer because of its quadratic complexity w.r.t. input length.
Theresulting datasetconsistsof186K/20K/ 100Ktraining /vali-
dation/ testingchunksfor PYand270K / 32K/ 220K for JS.
Wemostlyfocusonvalueprediction,sinceitisthemorecomplex
task,andpresentresultsfortypepredictionwheretheysignificantly
differfromothertasks.Weoptimizethesumofcross-entropylosses
for types andvalues.
707ESEC/FSE â€™21, August 23â€“28, 2021,Athens,Greece N.Chirkova andS.Troshin
Table 1: Selected hyperparameters for different structure-
capturing mechanisms, tasks and datasets. The details on
selecting hyperparameters are given in Supplementary ma-
terials.
Model(hypers.) Lang. VM FN CC
Seq.rel. attn. PY 8 250 32
(max. dist.) JS 8 250 32
Tree pos.enc. PY8, 16 16, 8 16, 8
(max width, depth) JS4, 8 2, 64 16, 32
Tree rel. attn. PY100 600 1000
(rel. vocab.size) JS600 100 1000
GGNNSandwich PY12, 3, N 6, 2, Y N/A
(num. layers, JS12, 3, N 6, 2, Y N/A
num. edge types,
isGGNNfirst?)
Weusemeanreciprocalrank(MRR)tomeasurethemodelquality
sinceitreflectsthepracticalapplicationofcodecompletion: MRR=
1
ğ‘/summationtext.1ğ‘
ğ‘–=11/ğ‘Ÿğ‘ğ‘›ğ‘˜ğ‘–, whereğ‘Ÿğ‘ğ‘›ğ‘˜ğ‘–is a position of the ğ‘–-th true token
inthemodelranking, ğ‘isthetotalnumberoftargettokensina
dataset,excluding <padding> and<empty>tokens.Asin[ 20],we
assign zero score if the true token is out of top 10 predicted tokens.
Hyperparameters. Welistgeneralhyperparametersforthe vari-
ablemisuse/functionnaming/codecompletiontasksusingslashes.
OurTransformermodelshave6layers,6/6/8heads, ğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™=512.
We limit vocabulary sizes for values up to 50K / 50K / 100K tokens
and preserve all types. As discussed in Section 3, we do not split
values into subtokens. We train all Transformers using Adam with
a starting learning rate of 0.00001 / 0.0001 / 0.0001 and a batch size
of32for25/15/20epochsforPYand40/25/20epochsforJS(the
number of functions in the JS dataset is smaller than in PY dataset,
thus more epochs are needed). In the code completion task, we use
the cosine learning rate schedule [ 25] with 2000 warm-up steps
and a zero minimal learning rate, and a gradient clipping of 0.2. In
the variable misuse task and in the function naming task for JS, we
use a constant learning rate. In the function naming task for PY,
we decay the learning rate by 0.9 after each epoch. In the function
naming task, we also use a gradient clipping of 5. We use resid-
ual,embeddingandattentiondropoutwith ğ‘=0.2/0.2/0.1.All
modelsweretrainedthreetimes,toestimatethestandarddeviation
ofthequality(excepthyperparametertuning).Inallexperiments
wereportthequalityonthetestset,excepthyperparametertun-
ingwhere we report the qualityonthevalidation set.We trainall
models onone GPU(NVIDIA Tesla P40 orV100).
Thehyperparametersfordifferentstructure-capturingmecha-
nisms were tuned using grid search, based on the quality on the
thevalidationset,foreachdatasetÅ›taskcombinationindividually.
For sequential relative attention, we tune the maximum relative
distance between the elements of the sequence. For tree positional
encoding,wetunethemaximumpathwidthandthemaximumpath
depth.Fortreerelativeattention,wetunethesizeoftherelationvo-
cabulary. For the GGNN Sandwich model, we consider 6-layer and
12-layerconfigurationsofalternatingTransformer(T)andGGNN
(G) layers, we also consider placing both types of layers first i.e.,[T,G,T,G,T,G]or[G,T,G,T,G,T](andsimilarlyfor12layers).
GGNN layers include 4 message passes. We also consider omitting
edgesof types LeftandRight. Sequential positionalembeddings
do not have hyperparameters. The number of parameters in all
AST-based modifications of Transformer are approximately the
same,exceptGGNNSandwiches:12-layerSandwichincorporates
slightlymoreparametersthanvanillaTransformer,while6-layer
incorporates slightly fewer parameters. The details and the Tables
onhyperparametersearcharegiveninSupplementarymaterials,
the resultinghyperparameters are listedinTable 1.
5 COMPARISON OFAPPROACHESFOR
UTILIZING SYNTACTICSTRUCTUREIN
TRANSFORMER
We begin with investigating which of the mechanisms for utilizing
AST structure in Transformer is the most effective one. We obtain
thetreesstoringa(type,value)pairineachnodeusingtheapproach
describedinSection 3andpassthesetreestoTransformer,equipped
withoneofthemechanismsdescribedinSection 2.GGNNSand-
wichisnotapplicabletocodecompletionbecausemessage-passing
involves allnodes andprohibitsusing maskinginthe decoder.
The resultsare presented inFigure 2. Infunctionnaming,most
structure-capturingmechanismsperformsimilarly.InSection 7,we
show that in this task quality is not affected much even if we com-
pletely ablate structure information, i.e., do not use any structure-
capturing mechanism and treat input as a setof (type, value) pairs.
Thatis,Transformerhardlyutilizessyntacticstructurewhenpre-
dictingfunctionnames.However,inothertasks,thisisnotthecase
and there is more variability in different mechanisms performance.
Utilizingstructureinformationintheinputembeddingsisnot
effective: sequential positional embeddings and tree positional en-
codingsdonotachievehighestscoreinanytask,exceptfunction
namingwheretreepositionalencodingsperformonparwithother
mechanisms.
Utilizingstructureintheself-attentionmechanismismuchmore
effective: in all tasks, at least one of sequential relative attention
andtreerelativeattentionisthebestperformingmodel.Sequential
relative attention achieves the highest score in variable misuse and
value prediction tasks, while tree relative attention outperforms
others by a high margin in type prediction task (this model was
developedforcodecompletiontask).Thelastresultisinterpretable
since tree relative attention helps to find relatives in AST tree, e.g.,
parent and siblings, which is important in type prediction. The
advantage of sequential relative attention is that it can use the
multidimensional embeddings of relations: the sequential relations
are shared acrossobjects,leadingto affordable3-dimensionalem-
beddingtensorsofshape(length,length,embeddingdimension).
In contrast, tree relative attention can only afford one-dimensional
embeddingsofrelations,becausetree-basedrelationsarenotshared
between objects in a mini-batch, and extracting them for a mini-
batch would already lead to a 3-dimensional tensor: (batch, length,
length).
GGNN Sandwich achieves high results in the variable misuse
task for which this model was developed. The reason is that in
variable misuse detection,the goal is to choosetwo variables, and
localmessagepassinginformseachvariableofitsroleinaprogram
708Empirical Studyof TransformersforSource Code ESEC/FSE â€™21, August 23â€“28, 2021,Athens,Greece
Python: VariableMisuse Function Naming Code Completion(values) Code Completion(types)
60
 70
 80
Joint accuracy
Seq. pos. emb.
Seq. rel. attn.
Tree pos. enc.
Tree rel. attn.
GGNN Sandwich
32
 33
 34
 35
 36
F1
JavaScript: VariableMisuse Function Naming Code Completion(values) Code Completion(types)
40
 50
 60
 70
 80
Joint accuracy
Seq. pos. emb.
Seq. rel. attn.
Tree pos. enc.
Tree rel. attn.
GGNN Sandwich
22
 23
 24
 25
F1
Figure 2: A comparison of different mechanisms for processing AST structure in Transformer, in the full-data setting. The
numeric data forbarplotsisgiven inSupplementary materials.
Table 2: Time- and storage-consumption of different
structure-capturing mechanisms for the variable misuse
task on thePython dataset.
Train time Preprocess Add. train
Model (h/epoch) time (ms/func.) data (GB)
Seq.pos.emb. 2.3 0 0
Seq.rel. att. 2.7 0 0
Tree pos.enc. 2.5 0.4 0.3
Tree rel. attn. 3.9 16.7 18
GGNNSandwich 7.2 0.3 0.35
and makes variable representations more meaningful. The original
work on GGNN Sandwiches also uses additional types of edges
whichwouldimprovetheperformanceofthismodelfurther.Using
these types of edges is out of scope of this work, since we focus
onutilizing syntactic information,thusweonlyusesyntax-based
edges.
InSupplementarymaterials,wevisualizetheprogressoftestmet-
ricsduringtraining,fordifferentstructure-capturingmechanisms
inthefull-datasetting.Supplementarymaterialsalsopresentthe
comparison of structure-capturing mechanisms in the anonymized
settingthatisdescribedinSection 3andimpliesreplacingvalues
in ASTs with unique placeholders. The leading mechanisms are
the same in all tasks as in the full data setting, considered above.
An interested reader may also find examples of attention maps for
differentmechanismsinSupplementary materials.
In Table2, we list training time and the size of auxiliary data
needed for different structure-capturing mechanisms. GGNN Sand-
wich model requires twice the time for training (and prediction)
compared to other models, because of the time-consuming mes-
sage passing mechanism. Tree relative attention requires dozens
ofgigabytesforstoringpairwiserelationmatricesforalltraining
objects that could be replaced with slow on-the-fly relation matrix
generation.TreepositionalencodingsandGGNNSandwichmodelsTable 3: Comparison of combinations of sequential relative
attention (SRA) with other structure-capturing approaches.
All numbers in percent, standard deviations: VM: 0.5%, FN:
0.4%, CC: 0.1%. Bold emphasizes combinations that signifi-
cantly outperform SRA. *In the VM task, SRA+GGNN Sand-
wich significantly outperforms SRA during the first half of
epochs, but loses superiority at the last epochs, for both
datasets. On the Python dataset, SRA+GGNN Sandwich out-
performs SRAby onestandard deviationat the lastepoch.
Model VM FN CC (val.)
PY SRA 81.42 35.73 54.53
SRA+Seq.pos.emb. 80.77 33.99 54.37
PYSRA+Tree pos.enc. 81.73 34.71 54.63
SRA+Tree rel. attn. 81.58 35.41 54.91
SRA+GGNNsand. 82.00* 33.39 N/A
JS SRA 76.52 24.62 64.11
SRA+Seq.pos.emb. 73.17 23.09 63.97
SRA+Tree pos.enc. 74.73 23.70 64.49
JSSRA+Tree rel. attn. 76.34 24.71 64.79
SRA+GGNNsand. 75.33* 21.44 N/A
alsorequireadditionaldiskspaceforstoringpreprocessedgraph
representations,but thesizes ofthesefilesarerelativelysmall.Se-
quential positional embeddings and relative attention are the most
efficient models,inboth time- anddisk-consumptionaspects.
To sum up, we emphasise sequential relative attention as the most
effective and efficient approach for capturing AST structure in Trans-
former.
Combining structure-capturing mechanisms. In Table3, we show
thatusing sophisticated structure-capturing mechanisms may be use-
ful for further improving sequential relative attention if we com-
bine two mechanisms . We find that tree relative attention (for both
datasets) and tree positional encoding (for JS) improve the score in
709ESEC/FSE â€™21, August 23â€“28, 2021,Athens,Greece N.Chirkova andS.Troshin
Table4:Illustrationofdifferentkindsofmodelsusedinthe
experiments.ThecodesnippetanditsASTusedintheillus-
trationmay be foundinFigure 1(a, b).
Model Inputrepresentation
Syntax+Text [(Assign,<empty>), (NameStore ,elem), ...
..., (Index,<empty>), (NameLoad ,idx)]
Syntax [(Assign,<empty>), (NameStore ,<var1>), ...
..., (Index,<empty>), (NameLoad ,<var3>)]
Text [elem,lst,idx]
Constant Predicts the mostfrequent target for any input
the value prediction task, while GGNN Sandwich may improve the
score inthe variable misuse task,especiallyat earlierepochs.
6 CAPABILITYOFTRANSFORMER TO
UTILIZESYNTACTICINFORMATION
When developing approaches for utilizing syntactic information
in Transformer, the majority of works mostly focus on the tree
structure.We discussed the utilizationofstructure intheprevious
section, but we also wouldliketo investigate the influence of other
AST components, namely typesandvalues. In the next section, we
conductanablationstudyofthementionedcomponents,andinthis
section,weinvestigatewhetherTransformerisgenerallycapable
ofutilizingthesyntactic information insourcecode,i.e.does pro-
cessing AST components improve performance. This conceptual
experimentteststheoverallsuitabilityofTransformerforsource
codeprocessing.Weformalizethespecifiedquestioninfull-data
andanonymizedsettings as follows:
â€¢Syntax+Text vs.Text: First,we testwhether using syntactic
information in addition to the textual information is benefi-
cial,comparedtousingpuretextualinformation.Todoso,
we compare thequality of Transformer trained on fullAST
data(Syntax+Text )withthequalityofTransformertrained
on a sequence of non-empty values ( Text), see Table 4for
illustrations. The Textmodel relies only on textual informa-
tionstoredinvalues anddoes not have access toanyother
kindofinformation.
â€¢Syntaxvs.Constant: Secondly, we test whether Transformer
is able to make meaningful predictions given onlysyntac-
tic information, without textual information. To do this,
wetestwhetherthequalityofTransformertrainedonthe
anonymized ASTdata( Syntax)isbetterthanthequalityofa
simpleconstantbaseline( Constant).Anonymizationremoves
identifiers,i.e.,textualinformation,butpreservesinforma-
tion about identifiersâ€™ repetition, which may be essential for
understandingtheprogram.The Constant modeloutputsthe
most frequent target, e.g., no-bug in VM and name initfor
PY andexportsfor JS in FN. Since anonymized AST data
contains only syntactic information and does not contain
textual information, the only way the Transformer can out-
performthe Constant baselineonthisdataistocapturesome
information from AST.All the described models are trained with sequential relative atten-
tion.Deduplicatingthedatasetisveryimportantinthisexperiment
to avoid overestimating the Syntaxbaseline.
The results for three tasks are presented in Figure 3. In all cases,
Syntax+Text outperforms Text,andSyntaxoutperforms Constant.In
Figure4, we present example predictions for code completion and
function naming tasks for Python language with all four models.
Incodecompletion,syntax-basedmodelscapturefrequentcoding
patterns well, for instance, in example (b), Syntaxmodel correctly
chooses (anonymized) value argmnents and notargvorparse
becauseargmnents goes before assignment. In example (a), Syn-
tax+Text correctlypredicts create_bucket becauseitgoesinside
the if-statement checking whether the bucket exists, while Text
modeloutputs frequent tokens associatedwithbuckets.
In the function naming task, the Textmodel captures code se-
mantics based on variable names and sometimes selects wrong
Å‚anchorÅ¾variables,e.g., splitinexample(c),whilethe Syntax+Text
modelutilizesASTinformationandoutputscorrectword dict.The
Syntaxmodeldoesnothaveaccesstovariablenamesasitprocesses
datawithplaceholders,andoutputsoverlygeneralpredictions,e.g.,
get all in example (c). Nevertheless, the Syntaxmodel is capable
ofdistinguishinggeneralcodepurpose,e.g.,modelusesword getin
example (c) and word readin example (d). To sum up, Transformer
is indeed capable ofutilizingsyntacticinformation .
Interestingly,incodecompletion(valueprediction,PY)and vari-
ablemisusedetectiontasks(JS),the Syntaxmodel,trainedonthe
anonymizeddataoutperformsthe Syntax+Text modeltrainedon
full data, though the latter uses more data than the former. The
reasonisthatthevaluesvocabularyonfulldataislimited,soap-
prox. 25% of values are replaced with the UNKtoken and cannot
be predicted correctly. On the other hand, this is not a complica-
tion for the Syntaxmodel, which anonymizes both frequent and
rare identifiers in the same way. For example, in Figure 4(b), the
Syntaxmodelcorrectlypredictsthemisspelledtoken argmnents
whileforthe Syntax+Text model,thistokenisout-of-vocabulary
and so model outputs frequently used strings for printing. One
moreadvantageofthe Syntaxmodelinthevaluepredictiontask
is that it is twice faster in training because of the small output
softmax dimension. In the function naming task, the Syntaxmodel
performssubstantiallyworsethan Syntax+Text ,becausevariable
namesprovidemuch naturallanguage informationneededtomake
natural language predictions. To sum up, anonymizationmay lead
to ahigherqualitythanusing full data .
7 ABLATION STUDYOFMECHANISMS FOR
UTILIZING SYNTACTICINFORMATION IN
TRANSFORMER
Inthissection,weinvestigatetheeffectofablatingdifferentAST
components on the performance in three tasks. This ablation study
is important for both providing practical recommendations and un-
derstanding what mechanisms are essential for making reasonable
predictions in the anonymized setting, discussed in the previous
section.WeconsiderthreeASTcomponents(commentsinitems
regard the usual scenario without ablation): ( types): the types of
nodes are passed as one of the Transformer inputs; ( values): the
(anonymized)valuesarepassedasoneoftheTransformerinputs;
710Empirical Studyof TransformersforSource Code ESEC/FSE â€™21, August 23â€“28, 2021,Athens,Greece
VariableMisuse Å›Python Function Naming Å›Python Code Completion(values) Å›Python
0
 5
 10
 15
 20
 25
Epoch
0
20
40
60
80Joint accuracy
Syntax+Text
Text
Syntax
Constant
0
 2
 4
 6
 8
 10
 12
 14
Epoch
10
15
20
25
30
35F1
 Syntax+Text
Text
Syntax
Constant
0.0
 2.5
 5.0
 7.5
10.0
 12.5
 15.0
 17.5
Epoch
10
20
30
40
50
60MRR
Syntax+Text
Text
Syntax
Dummy
VariableMisuse Å›JavaScript Function Naming Å›JavaScript Code Completion(values) Å›JavaScript
0
 5
10
 15
 20
 25
 30
 35
 40
Epoch
0
10
20
30
40
50
60
70
80Joint accuracy
Syntax+Text
Text
Syntax
Constant
0
 5
 10
 15
 20
 25
Epoch
5
10
15
20
25F1
Syntax+Text
Text
Syntax
Constant
0.0
 2.5
 5.0
 7.5
10.0
 12.5
 15.0
 17.5
Epoch
10
20
30
40
50
60MRR
Syntax+Text
Text
Syntax
Dummy
Figure 3:Comparisonofsyntax-based Transformer models with text-onlyand constant baselines.
defget_or_create_bucket (s3_connection):
bucket=s3_connection .get_bucket(
settings .S3_BUCKET_NAME
)
ifbucketisNone:
bucket=s3_connection .importsys
fromutils.parsing importparse
if__name__ =='__main__' :
argmnents =parse(
sys.argv[1:]
)
print(def<fun_name >(seqs):
dt={}
forseqinseqs:
forwordinseq.split():
ifnotwordindt:
dt[word] = 1
else:
dt[word] += 1
returndtdef<fun_name >(filename):
with open (filename) asfin:
return len (
fin.read()
.split("\n")
)
(a)S+T: [create bucket get bucket get key] (b) S+T: [Usage: done. \n] (c) S+T: get dict (d) S+T: read ï¬le
S: [getbucket S3 BUCKET NAME var107] S: [argmnents var440 var289] S: get all S: read
T: [bucket get bucket settings] T: [sys outfn len] T: get split T: read ï¬le
Dummy : [self 0 None] Dummy : [self 0 None] Dummy : init Dummy : init
Gold: createbucket Gold: argmnents Gold: get dictionary Gold: count lines
Figure 4:Example predictionsincodecompletion(a, b; top 3 predictions) andfunction naming(c,d)tasks. S: Syntax,T:Text.
(structure):ASTstructureisprocessedusingoneofthemechanisms
discussedinSection 2.
As in Section 6, we consider both anonymized and full-data set-
tings.WeablateASTcomponentsonebyoneinbothmodels Syntax
andSyntax+Text and check whether the quality drops. Ablating
typesforSyntax+Text was in fact performed in Section 6, but we
repeat the results in this experimentâ€™s table and also report this ab-
lationfortheanonymizedsetting.Ablating structuremeansturning
offallsyntax-capturingmechanismssothattheinputofthe Syn-
tax+Text/Syntaxmodelwillbeviewedasan unorderedset of(type,
value) / (type, anonymized value) pairs. Ablating valuesmeans us-
ingonlytypesastheinputtothemodelÃthenumbersarethesame
forbothfull-dataandanonymizedsettings.Weskipthisablationin
thecodecompletiontask,since,inthiscase,(anonymized)values
are the target ofthe model.
The results are presented in Table 5.In variable misuse and code
completion,allASTcomponentsareessentialforachievinghighqual-
ityresults .Particularly,invariablemisuse,allablationsresultina
largequalitydrop,inbothsettings,andincodecompletion,ablating
types results in a large quality drop and ablating structure Ã in
substantialdrop.Interestingly,anonymizationplaysanimportantrole in achieving high quality in the variable misuse task, with
absent valuesfrom the data.
However, the observations differ for function naming. In this
task, (1) ablating typesresults in a substantial quality drop in both
settings, (2) ablating structure results in a small (but significant)
quality drop in both settings, (3) ablating valuesin the full-data
settingresultsinthelargequalitydrop,and(4)ablating anonymized
valuesdoes not affecttheperformancein theanonymizedsetting.
The first and the third observations underline the importance of
using both types and values in practice. The second and the fourth
observations show that Transformer is now far from utilizing all the
information stored in AST when predicting function names . Partic-
ularly, in the anonymized setting, Transformer predicts function
namesmostlybasedon types.Ithardlyusessyntactic structure,and
does not use information about value repetition which is stored in
anonymizedvalues andisessentialforunderstandingthealgorithm
thatthecodeimplements.Overcomingthisissueisaninteresting
direction for future research.
711ESEC/FSE â€™21, August 23â€“28, 2021,Athens,Greece N.Chirkova andS.Troshin
Table 5: Ablation study of processing different AST components in Transformer. Bold emphasises best models and ablations
that do not hurt the performance. AST w/o struct.: Transformer treats input as a bag without structure; AST w/o types: only
values or anonymized values are passed to Transformer; AST w/o an.val.: only types are passed to Transformer. N/A Å› not
applicable.
Fulldata Anonymizeddata
Var.misuse Fun. naming Comp.(val.) Var.misuse Fun. naming Comp.(val.)
FullAST 81.59Â±0.50%35.73Â±0.19%54.59Â±0.2%81.71Â±0.41%25.26Â±0.15%58.76Â±0.2%
Python AST w/ostruct. 26.81Â±0.47% 34.80 Â±0.24% 53.1 Â±0.1%12.41Â±0.58% 23.29 Â±0.18% 57.75 Â±0.05%
AST w/otypes 71.55Â±0.28% 33.60 Â±0.23% 42.01 Â±0.05%58.55Â±0.51% 12.50 Â±1.5% 41.26 Â±0.05%
AST w/oan.val. 32.44Â±0.35% 25.25 Â±0.06% N/A 32.44Â±0.35%25.25Â±0.06% N/A
FullAST 75.60Â±0.15%24.62Â±0.14% 64.2Â±0.0578.47Â±0.26%13.66Â±0.30%60.82Â±0.07%
JavaScript AST w/ostruct. 17.25Â±0.83% 23.40 Â±0.12% 61.53 Â±0.15%5.37Â±0.97% 11.25 Â±0.08% 58.59 Â±0.1%
AST w/otypes 60.33Â±0.50% 23.09 Â±0.09% 53.4 Â±0.1 %43.53Â±0.92% 8.10 Â±1.4% 42.91 Â±0.1%
AST w/oan.val. 42.56Â±0.24% 13.64 Â±0.07% N/A 42.56Â±0.24%13.64Â±0.07% N/A
Table 6: Comparison of ensembles. Notation: ST Å› Syn-
tax+Text , S Å›Syntax, & denotes ensembling. All models are
trained with sequential relative attention. All numbers in
percent, standard deviations: VM:0.5%,FN: 0.4%,CC: 0.1%.
Models VM FN CC(types) CC(values)
ST81.4235.73 89.22 54.53
ST &ST 82.8035.61 89.39 56.35
PYS81.83 25.26 88.42 58.6
S&S 82.57 25.46 88.65 59.29
ST &S 86.7232.15 89.49 61.84
ST76.5224.62 90.14 64.11
ST &ST 77.2524.53 90.56 65.68
JSS78.53 13.66 88.22 60.71
S&S 79.65 13.19 88.52 61.42
ST &S 82.2919.33 90.32 68.33
8 ENSEMBLINGOFSYNTAX-BASEDMODELS
As was shown in Figure 4,Syntax+Text andSyntaxmodels capture
dependencies of different nature and are orthogonal in a sense
of handlingmissingvaluesand first time seen tokens. This allows
hypothesizingthat ensembling twomentionedmodelscanboostthe
performance of theTransformer. We usethe standard ensembling
approach that implies training networks from different random
initializationsandaveragingtheirpredictionsaftersoftmax[ 8].We
use sequentialrelative attention inthis experiment.
In Table6, we compare an ensemble of Syntax+Text andSyntax
modelswithensemblesoftwo Syntax+Text andoftwo Syntaxmod-
els. We observe that in variable misuse and value prediction tasks,
ensemblingmodelsthatviewinputdataintwocompletelydiffer-
ent formats is much more effective than ensembling two similar
models.Thisisthewayhowusinganonymizeddatamayboostthe
Transformerâ€™s performance.
9 VALIDATING OURIMPLEMENTATIONS
AND COMPARINGTO OTHER WORKS
Weensurethevalidityofourresultsintwoways:byrelyingonthe
codeofalreadypublishedworks,andbycomparingournumbersachievedforthecommonlyuseddatasplittothenumbersinthe
correspondingpapers.Particularly,weusethemodel/loss/metrics
/ overlapping chunks code of Kim et al .[20]as the baseline for the
CC task, we rewrite (line by line) the main parts of the model /
loss / metrics code of Hellendoorn et al . [14]in PyTorch, as the
baselinefortheVMtask,andweusethemodel/loss/metricscode
ofAhmadetal.[ 1]as the baselinefor the FN task.
ForVM,thevanillaTransformerofHellendoornetal .[14]achieve
67.7% joint accuracy andwe achieve 64.4%: theresults are close to
each other. Here the performance is given for our model, closest to
the model of Hellendoorn et al .[14]: theTextmodel of similar size
andwithsimilarnumberoftrainingupdates;usingour Syntax+Text
model achieves higher quality. The performance of GGNN Sand-
wich is high on VM, as in [ 14]. For CC, with tree relative attention,
we achieve 59.79 / 91.65 MRR (values / types) while Kim et al .[20]
achieved 58.8 / 91.9 (their Å‚TravTrans variantÅ¾); and for standard
Transformer(nostructureinformation),weachieve59.66/89.16
MRRwhileKimetal .[20]achieved58.0/87.3(theirÅ‚TravTransÅ¾)
respectively,againtheresultsareclose.ForFN,weusedthecode
ofAhmadetal .[1]withourcustomdataandtargets,sotheresults
arenotcomparable,butwecheckedthattheircodeproducesthe
same numbers ontheirdata as inthe paper.
The results given in our paper are for our custom data split and
thusarenotdirectlycomparabletothenumbersinotherworks.We
arguethatdataresplittingiscrucialforachievingcorrectresults,see
details in Section 3.Atthe same time,the remaining experimental
setup(e.g.architecture,metrics)isthe same as inrecent works.
10 RELATED WORK
Variable misuse. The field of automated program repair includes
alotofdifferenttasks,see[ 27]forareview,wefocusonaparticular
variablemisusedetectiontask.ThistaskwasintroducedbyAlla-
manis et al . [4]who proposed using GGNN with different types of
edges to predict the true variable name for each name placeholder.
Vasicetal .[36]enhancestheVMtaskbylearningtojointlyclassify,
localizebugandrepairthecodesnippet.TheyuseanRNNequipped
with two pointers that locate and fix the bug. Hellendoorn et al .
[14]improvedtheperformanceonVMtask,usingTransformers,
GREAT model,andGGNNSandwich model.
712Empirical Studyof TransformersforSource Code ESEC/FSE â€™21, August 23â€“28, 2021,Athens,Greece
Code summarization. The task of code summarization is formal-
ized in literature in different ways: given a code snippet, predict
the docstring [ 16], the function name [ 5], or the accompanying
comment [ 17]. Allamanis et al . [5]propose using convolutional
neural networks for generating human readable function names,
while Iyer et al . [17]proposed using LSTM [ 15] with attention
togeneratenaturallanguagesummaries.Alonetal .[7]proposed
sampling random AST paths and encoding them with bidirectional
LSTMtoproducenaturalmethodnamesandsummariesofthecode.
Fernandesetal .[11]proposedcombiningRNNs/Transformerswith
GGNN.Ahmadetal .[1]empiricallyinvestigateTransformersfor
codesummarization,showingthatTransformerwithsequentialrel-
ative attention outperforms Transformer equipped with positional
encodings as well as awide range ofothermodels,e,g.RNNs.
Code completion. Early works on code generation built proba-
bilistic models over the grammar rules. Maddison and Tarlow [26]
learned Markov Decision Process over free contextgrammars,uti-
lizingAST.Raychevetal .[29]learneddecisiontreespredictingAST
nodes. Sun et al .[33]generated code by expanding AST nodes,us-
ingnaturallanguagecommentsasadditionalsourceofinformation.
Li et al.[24]used LSTM with a pointer, this model either generates
thenexttokenfromvocabularyorcopiesthetokenfromaprevi-
ouslyseenposition.Kimetal .[20]proposedusingTransformers
for code generation, enhanced them with tree relative attention
and showed that the resulting model significantly outperforms
RNN-basedmodels as well as othermodels [ 29].
Recent advances in neural source code processing. The recent line
of work is dedicated to learning contextual embeddings for code
onthebasisofBidirectionalEncoderRepresentationsfromTrans-
formers (BERT) [ 9]. Such models are firstly pretrained on large
datasets providing high-quality embeddings of code, and then fine-
tunedonsmalldatasetsfordownstreamtasks[ 10,12,18].Allthese
Transformer-based models treat code as text and can potentially
benefit from the further utilization of the syntactic information.
Anotherline of researchregards makingTransformersmore time-
andmemory-efficient[ 35].Investigatingtheapplicabilityofsuch
methods to syntax-based Transformersis an interesting direction
for future research.
Investigatingneuralnetworksforcodewithomittedvariablenames.
A few of previous works considered training neural networks with
omittedvariablenames:Guptaetal .[13],Xuetal.[39]trainedRNNs
onthedatawithanonymizedvariables,Ahmedetal .[2]replaced
variables with their types. LeClair et al . [22]investigated the effect
of replacing all values in the AST traversal with <unk>value in
the code summarization task, and concluded that the quality of
anRNNtrainedonsuchdataisextremelylow.Theirresultaligns
with ours, while we consider a more general procedure of value
anonymization (that saves information about value repetition) and
investigatetheeffectofusinganonymizationinawidersetoftasks
for aTransformer architecture.
11 THREATS TO VALIDITY
We did our best to make out comparison of different AST pro-
cessing mechanisms as fair as possible. However, the following
factors could potentially affect the validity of our results: using thesame training hyperparameters for all models, not using subtok-
enization, and not using data- and control-flow edges in GGNN
Sandwich.Thedecisionnottousesubtokenizationwasexplained
in Section 3. Moreover, we underline that sequential relative atten-
tion, our best performing mechanism, allows for easy combination
with any subtokenization technique which will result in further
qualityimprovement.Onthecontrary,thisisnotthecaseformore
complexconsideredAST-processingmechanisms.Thedecisionnot
use control- and data-flow edges was explained in Section 2. We
notethat adding data-andcontrol-flow edges to theGGNNSand-
wich equipped with sequential relative attention would increase
thequalityofthiscombinedmodelevenfurther.Asforthetraining
hyperparameters,tuning them for each model individuallywould
beveryexpensivegivenourlimitedcomputationalresources.How-
ever, we note that our models differ only in the AST-processing
mechanism that is a relatively small change to the architecture.
Thus we assume that using the same training hyperparameters for
differentmodels ispermissibleinour work.
12 CONCLUSION
In this work, we investigated the capabilities of Transformer to
utilizesyntacticinformation insourcecodeprocessing. Ourstudy
underlinedthe following practical conclusions:
â€¢sequential relative attention is a simple, fast and not con-
sidered as the baseline in previous works mechanism that
performs best in 3 out of 4 tasks (in some cases, similarly to
otherslower mechanisms);
â€¢combining sequential relative attention with GGNN Sand-
wichinthevariablemisusetask andwithtreerelativeatten-
tion or tree positional encoding in the code completion task
mayfurther improve quality;
â€¢omittingtypes, valuesoredges inASTshurts performance;
â€¢ensemblingTransformertrainedonthefull-datawithTrans-
former trained on the anonymized data outperforms the
ensemble of Transformers trained on the same kind of data.
Further, our study highlighted two conceptual insights. On the
one hand, Transformers are generally capable of utilizing syntactic
informationinsourcecode,despitetheywereinitiallydevelopedfor
NLP,i.e.processingsequences.Ontheotherhand,Transformers
utilizesyntacticinformationfullynotinalltasks:invariablemisuse
and code completion, Transformer uses all AST components, while
infunctionnaming,Transformermostlyreliesonasetoftypesand
valuesusedinthe program, hardlyutilizing syntactic structure.
ACKNOWLEDGMENTS
We would like to thank Ildus Sadrtdinov, Ivan Rubachev and the
anonymous reviewers for the valuable feedback. The results pre-
sentedinSections 5and8weresupportedbytheRussianScience
Foundationgrant â„–19-71-30020.TheresultspresentedinSections 6
and7were supported by Samsung Research, Samsung Electronics.
The research was supported in part through the computational
resourcesofHPCfacilities at NRU HSE.
713ESEC/FSE â€™21, August 23â€“28, 2021,Athens,Greece N.Chirkova andS.Troshin
REFERENCES
[1]WasiUddinAhmad,SaikatChakraborty,BaishakhiRay,andKai-WeiChang.2020.
A Transformer-basedApproachfor SourceCode Summarization.In Proceedings
ofthe58thAnnualMeetingoftheAssociationforComputationalLinguistics(ACL) .
[2]Umair Z. Ahmed, Pawan Kumar, Amey Karkare, Purushottam Kar, and Sumit
Gulwani.2018. CompilationErrorRepair:FortheStudentPrograms,fromthe
Student Programs. In Proceedings of the 40th International Conference on Software
Engineering: Software Engineering Education and Training (Gothenburg, Sweden)
(ICSE-SEET â€™18) . Association for Computing Machinery, New York, NY, USA,
78Å›87.https://doi.org/10.1145/3183377.3183383
[3]Miltiadis Allamanis. 2019. The adverse effects of code duplication in machine
learning models of code. Proceedings of the 2019 ACM SIGPLAN International
SymposiumonNewIdeas,NewParadigms,andReflectionsonProgrammingand
Software(2019).
[4]MiltiadisAllamanis,MarcBrockschmidt,andMahmoudKhademi.2018. Learning
to Represent Programs with Graphs. In 6th International Conference on Learning
Representations,ICLR2018,Vancouver,BC,Canada,April30-May3,2018,Con-
ferenceTrackProceedings .OpenReview.net. https://openreview.net/forum?id=
BJOFETxR-
[5]Miltiadis Allamanis, Hao Peng, and Charles Sutton. 2016. A Convolutional
Attention Networkfor Extreme Summarization of Source Code.In International
Conference onMachineLearning (ICML) .
[6]UriAlon,ShakedBrody,OmerLevy,andEranYahav.2019. code2seq:Generating
Sequences from Structured Representations of Code. In 7th International Con-
ferenceonLearningRepresentations,ICLR2019,NewOrleans,LA,USA,May6-9,
2019. OpenReview.net. https://openreview.net/forum?id=H1gKYo09tX
[7]UriAlon,ShakedBrody,OmerLevy,andEranYahav.2019. code2seq:Generating
Sequences from Structured Representations of Code. In International Conference
onLearning Representations .https://openreview.net/forum?id=H1gKYo09tX
[8]Arsenii Ashukha, Alexander Lyzhov, Dmitry Molchanov, and Dmitry Vetrov.
2020. Pitfalls of In-Domain Uncertainty Estimation and Ensembling in Deep
Learning. In International Conference on Learning Representations, ICLR 2020 .
https://openreview.net/forum?id=BJxI5gHKDr
[9]JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2019. BERT:
Pre-trainingofDeepBidirectionalTransformersforLanguageUnderstanding.In
Proceedingsofthe2019ConferenceoftheNorthAmericanChapteroftheAssociation
forComputationalLinguistics:HumanLanguageTechnologies,Volume1(Longand
ShortPapers) .AssociationforComputationalLinguistics,Minneapolis,Minnesota,
4171Å›4186. https://doi.org/10.18653/v1/N19-1423
[10]Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong,
Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, and Ming Zhou. 2020. CodeBERT:
APre-TrainedModelforProgrammingandNaturalLanguages.In Findingsofthe
AssociationforComputationalLinguistics:EMNLP2020 .AssociationforComputa-
tional Linguistics, Online, 1536Å›1547. https://doi.org/10.18653/v1/2020.findings-
emnlp.139
[11]PatrickFernandes,MiltiadisAllamanis,andMarcBrockschmidt.2019. Structured
NeuralSummarization.In InternationalConferenceonLearningRepresentations .
https://openreview.net/forum?id=H1ersoRqtm
[12]Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie LIU, Long
Zhou, Nan Duan, Alexey Svyatkovskiy,Shengyu Fu,Michele Tufano, Shao Kun
Deng, Colin Clement, Dawn Drain, Neel Sundaresan, Jian Yin, Daxin Jiang,
and Ming Zhou. 2021. GraphCodeBERT: Pre-training Code Representations
withDataFlow.In InternationalConferenceonLearningRepresentations .https:
//openreview.net/forum?id=jLoC4ez43PZ
[13]Rahul Gupta, Soham Pal, Aditya Kanade, and Shirish Shevade. 2017. DeepFix:
Fixing Common C Language Errors by Deep Learning. In Proceedings of the
Thirty-First AAAI Conference on Artificial Intelligence (San Francisco, California,
USA)(AAAIâ€™17) . AAAI Press,1345Å›1351.
[14]Vincent J. Hellendoorn, Charles Sutton, Rishabh Singh, Petros Maniatis, and
David Bieber. 2020. Global Relational Models of Source Code. In International
Conference on Learning Representations, ICLR 2020 .https://openreview.net/
forum?id=B1lnbRNtwr
[15]Sepp Hochreiter and JÃƒÅ’rgen Schmidhuber. 1997. Long Short-term Memory.
Neuralcomputation 9(121997),1735Å›80. https://doi.org/10.1162/neco.1997.9.8.
1735
[16]Xing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. 2018. Deep Code Comment
Generation. In Proceedings of the 26th Conference on Program Comprehension
(Gothenburg,Sweden) (ICPCâ€™18) .AssociationforComputingMachinery,New
York, NY, USA,200Å›210. https://doi.org/10.1145/3196321.3196334
[17]Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2016.
Summarizing Source Code using a Neural Attention Model. In Proceedings of
the 54th Annual Meeting of the Association for Computational Linguistics (Volume
1: Long Papers) . Association for Computational Linguistics, Berlin, Germany,
2073Å›2083. https://doi.org/10.18653/v1/P16-1195
[18]Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, and Kensen Shi. 2020.
Learning and evaluating contextual embedding of source code. In Proceedings
ofthe37thInternationalConferenceonMachineLearning,ICML2020,12-18July
2020 (ProceedingsofMachineLearning Research) . PMLR.[19]Rafael-Michael Karampatsis, Hlib Babii, Romain Robbes, Charles Sutton, and
AndreaJanes.2020. BigCode!=BigVocabulary:Open-VocabularyModelsfor
SourceCode.In ProceedingsoftheACM/IEEE42ndInternationalConferenceon
Software Engineering (Seoul, South Korea) (ICSE 20). Association for Computing
Machinery, New York, NY, USA, 1073Å›1085. https://doi.org/10.1145/3377811.
3380342
[20]Seohyun Kim, Jinman Zhao, Yuchi Tian, and Satish Chandra. 2020. Code Predic-
tionby Feeding Treesto Transformers. arXiv: 2003.13848 [cs.SE]
[21]Marie-AnneLachaux,BaptisteRoziere,LowikChanussot,andGuillaumeLample.
2020. UnsupervisedTranslationofProgrammingLanguages.In arXivpreprint
arXiv:2006.03511 . arXiv:2006.03511 [cs.CL]
[22]Alexander LeClair, Siyuan Jiang, and Collin McMillan. 2019. A Neural Model for
GeneratingNaturalLanguageSummariesofProgramSubroutines.In Proceedings
ofthe41stInternationalConferenceonSoftwareEngineering (Montreal,Quebec,
Canada)(ICSEâ€™19) .IEEEPress,795Å›806. https://doi.org/10.1109/ICSE.2019.00087
[23]Jian Li, Yue Wang, Michael R. Lyu, and Irwin King. 2018. Code Completion with
NeuralAttentionandPointerNetworks.In Proceedingsofthe27thInternational
Joint Conference on Artificial Intelligence (Stockholm, Sweden) (IJCAIâ€™18) . AAAI
Press,4159Å›25.
[24]Jian Li, Yue Wang, Michael R. Lyu, and Irwin King. 2018. Code Completion with
Neural Attention and Pointer Networks. In Proceedings of the Twenty-Seventh
International Joint Conference on Artificial Intelligence, IJCAI-18 . International
Joint Conferences on Artificial Intelligence Organization, 4159Å›4165. https:
//doi.org/10.24963/ijcai.2018/578
[25]I.LoshchilovandF.Hutter.2017. SGDR:StochasticGradientDescentwithWarm
Restarts.In ICLR.
[26]Chris Maddison and Daniel Tarlow. 2014. Structured Generative Models of Natu-
ralSourceCode.In Proceedingsofthe31stInternationalConferenceonMachine
Learning (Proceedings of Machine Learning Research, Vol. 32) , Eric P. Xing and
Tony Jebara (Eds.). PMLR, Bejing, China, 649Å›657. http://proceedings.mlr.press/
v32/maddison14.html
[27]Martin Monperrus. 2020. The Living Review on Automated Program Repair.
(Dec. 2020). https://hal.archives-ouvertes.fr/hal-01956501 working paper or
preprint.
[28]Veselin Raychev, Pavol Bielik, and Martin Vechev. 2016. Probabilistic Model for
CodewithDecisionTrees.In Proceedingsofthe2016ACMSIGPLANInternational
ConferenceonObject-OrientedProgramming,Systems,Languages,andApplications
(Amsterdam,Netherlands) (OOPSLA2016) .AssociationforComputingMachinery,
NewYork, NY, USA,731Å›747. https://doi.org/10.1145/2983990.2984041
[29]Veselin Raychev, Pavol Bielik, and Martin Vechev. 2016. Probabilistic Model for
Code with Decision Trees. SIGPLAN Not. 51, 10 (Oct. 2016), 731Å›747. https:
//doi.org/10.1145/3022671.2984041
[30]VeselinRaychev,PavolBielik,MartinVechev,andAndreasKrause.2016.Learning
Programs from Noisy Data. In Proceedings of the 43rd Annual ACM SIGPLAN-
SIGACT Symposium on Principles of Programming Languages (St. Petersburg, FL,
USA)(POPL â€™16) . Association for Computing Machinery, New York, NY, USA,
761Å›774. https://doi.org/10.1145/2837614.2837671
[31]Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-Attention with
Relative Position Representations. In Proceedings of the 2018 Conference of the
North American Chapter of the Association for Computational Linguistics: Human
LanguageTechnologies,Volume2(ShortPapers) .AssociationforComputational
Linguistics, New Orleans, Louisiana, 464Å›468. https://doi.org/10.18653/v1/N18-
2074
[32]Vighnesh Shiv and Chris Quirk. 2019. Novel positional encodings to enable
tree-basedtransformers. In AdvancesinNeuralInformationProcessingSystems32 ,
H. Wallach, H. Larochelle, A. Beygelzimer, F. d 'AlchÃ©-Buc, E. Fox, and R. Garnett
(Eds.).CurranAssociates,Inc.,12081Å›12091. http://papers.nips.cc/paper/9376-
novel-positional-encodings-to-enable-tree-based-transformers.pdf
[33]ZeyuSun,QihaoZhu,YingfeiXiong,YicanSun,LiliMou,andLuZhang.2020.
TreeGen: A Tree-Based Transformer Architecture for Code Generation. Proceed-
ingsoftheAAAIConferenceonArtificialIntelligence 34,05(Apr.2020),8984Å›8991.
https://doi.org/10.1609/aaai.v34i05.6430
[34]Kai Sheng Tai, Richard Socher, and Christopher D. Manning. 2015. Improved
Semantic Representations From Tree-Structured Long Short-Term Memory Net-
works. In Proceedings of the 53rd Annual Meeting of the Association for Computa-
tional Linguistics and the 7th International Joint Conference on Natural Language
Processing(Volume1:LongPapers) .AssociationforComputationalLinguistics,
Beijing, China, 1556Å›1566. https://doi.org/10.3115/v1/P15-1150
[35]Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. 2020. Efficient
Transformers:ASurvey. arXiv: 2009.06732 [cs.LG]
[36]Marko Vasic, Aditya Kanade, Petros Maniatis, David Bieber, and Rishabh Singh.
2019. NeuralProgramRepairbyJointlyLearningtoLocalizeandRepair.In 7th
International Conference on Learning Representations, ICLR 2019, New Orleans,
LA, USA, May 6-9, 2019 . OpenReview.net. https://openreview.net/forum?id=
ByloJ20qtm
[37]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Å ukasz Kaiser, and Illia Polosukhin. 2017. Attention is All
youNeed. In AdvancesinNeuralInformationProcessingSystems30 ,I.Guyon,U.V.
714Empirical Studyof TransformersforSource Code ESEC/FSE â€™21, August 23â€“28, 2021,Athens,Greece
Luxburg,S.Bengio,H.Wallach,R.Fergus,S.Vishwanathan,andR.Garnett(Eds.).
Curran Associates, Inc.,5998Å›6008. http://papers.nips.cc/paper/7181-attention-
is-all-you-need.pdf
[38]Yao Wan, Zhou Zhao, Min Yang, Guandong Xu, Haochao Ying, Jian Wu, and
Philip S. Yu. 2018. Improving Automatic Source Code Summarization via
Deep Reinforcement Learning. In Proceedings of the 33rd ACM/IEEE Interna-
tional Conference on Automated Software Engineering (Montpellier, France) (ASE2018). Association for Computing Machinery, New York, NY, USA, 397Å›407.
https://doi.org/10.1145/3238147.3238206
[39]ShengbinXu,YuanYao,FengXu,TianxiaoGu,HanghangTong,andJianLu.2019.
Commit Message Generation for Source Code Changes. In Proceedings of the
Twenty-EighthInternationalJointConferenceonArtificialIntelligence,IJCAI-19 .
International Joint Conferences on Artificial Intelligence Organization, 3975Å›
3981.https://doi.org/10.24963/ijcai.2019/552
715