Problems and Opportunities in Training Deep Learning
Software Systems: An Analysis of Variance
Hung Viet Pham
University of Waterloo
Waterloo, ON, Canada
hvpham@uwaterloo.caShangshu Qian
Purdue University
West Lafayette, IN, USA
qian151@purdue.eduJiannan Wang
Purdue University
West Lafayette, IN, USA
wang4524@purdue.eduThibaud Lutellier
University of Waterloo
Waterloo, ON, Canada
tlutelli@uwaterloo.ca
Jonathan Rosenthal
Purdue University
West Lafayette, IN, USA
rosenth0@purdue.eduLin Tan
Purdue University
West Lafayette, IN, USA
lintan@purdue.eduYaoliang Yu
University of Waterloo
Waterloo, ON, Canada
yaoliang.yu@uwaterloo.caNachiappan Nagappan
Microsoft Research
Redmond, WA, USA
nachin@microsoft.com
ABSTRACT
Deep learning (DL) training algorithms utilize nondeterminism to
improve models’ accuracy and training efficiency. Hence, multi-
pleidenticaltrainingruns(e.g.,identicaltrainingdata,algorithm,
and network) produce different models with different accuracies
andtrainingtimes.Inadditiontothesealgorithmicfactors,DLli-
braries(e.g.,TensorFlowandcuDNN)introduceadditionalvariance
(referredtoasimplementation-levelvariance)duetoparallelism,
optimization, and floating-point computation.
This work is the first to study the variance of DL systems and
theawarenessofthisvarianceamongresearchersandpractition-
ers.Ourexperimentsonthreedatasetswithsixpopularnetworks
showlargeoverallaccuracydifferencesamongidenticaltraining
runs. Even after excluding weak models, the accuracy difference
is10.8%.Inaddition, implementation-level factorsalonecausethe
accuracydifference acrossidenticaltraining runstobeup to2.9%,
theper-classaccuracydifferencetobeupto52.4%,andthetraining
timedifferencetobeupto145.3%.Allcorelibraries(TensorFlow,CNTK,andTheano)andlow-levellibraries(e.g.,cuDNN)exhibit
implementation-level variance across all evaluated versions.
Our researcherand practitioner survey showsthat 83.8% ofthe
901participantsareunawareoforunsureaboutanyimplementa-
tion-level variance. In addition, our literature survey shows thatonly 19.5
±3% of papers in recent top software engineering (SE),
artificial intelligence (AI), and systems conferences use multipleidentical training runs to quantify the variance of their DL ap-
proaches.ThispaperraisesawarenessofDLvarianceanddirects
SE researchers to challenging tasks such as creating determinis-
tic DL implementations to facilitate debugging and improving the
reproducibility of DL software and results.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
onthe firstpage.Copyrights forcomponentsof thisworkowned byothersthan the
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/or a fee. Request permissions from permissions@acm.org.
ASE ’20, September 21–25, 2020, Virtual Event, Australia
© 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-6768-4/20/09...$15.00
https://doi.org/10.1145/3324884.3416545CCS CONCEPTS
•Softwareanditsengineering →Empiricalsoftwarevalida-
tion;•General and reference →Empirical studies .
KEYWORDS
deep learning, variance, nondeterminism
ACM Reference Format:
HungVietPham,ShangshuQian,JiannanWang,ThibaudLutellier,Jonathan
Rosenthal,LinTan,YaoliangYu,andNachiappanNagappan.2020.Problems
and Opportunities in Training Deep Learning Software Systems: An Analy-
sisofVariance.In 35thIEEE/ACMInternationalConferenceonAutomated
SoftwareEngineering(ASE’20),September21–25,2020,VirtualEvent,Aus-
tralia.ACM, New York, NY, USA, 13 pages. https://doi.org/10.1145/3324884.
3416545
1 INTRODUCTION
Deep learning is widely used in many fields including autonomous
drivingcars[ 19],diabeticbloodglucoseprediction[ 78],andsoft-
ware engineering [ 18,20,21,52,63,87,114,117,119]. DL training
algorithmsutilizenondeterminismtoimprovetrainingefficiency
and model accuracy, e.g., using shuffled batch ordering of training
data to prevent overfitting andspeed up training [49].
Thesenondeterminism-introducing (NI)-factors cause multiple
identical training runs, i.e., training runs with the samesettings
(e.g., identical training data, identical algorithm, and identical net-
work),toproducedifferentDLmodelswithsignificantlydifferent
accuracies and training times [57, 88, 101, 104].
Forexample,ourexperimentsshowthatfor16identicaltraining
runs of a popular DL network, LeNet5 [ 65], the accuracy of the
resulting16modelsrangesfrom8.6%to99.0%—alargeaccuracy
differenceof90.4%.Fouroftheseidenticaltrainingrunsresultedin
weakmodels (accuracybelow20%).Evenifweexcludesuchmodels,
theaccuracydifferenceisstillupto10.8%withLeNet1betweenthe
most accurate run (98.6%) and the least accurate run (87.8%).
One can eliminate the variance introduced by algorithmic NI-
factors(e.g., shuffled batch ordering) using fixed random seeds. For
example, with a fixed seed for batch ordering, multiple identical
training runs will have the same batch ordering. We refer to these
runs asfixed-seed identical training runs.
In addition to these algorithmic NI-factors, DL libraries (e.g.,
TensorFlow [ 13] and cuDNN [ 23]) introduce additional variance.
7712020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE)
Forexample,bydefault, coreDLlibraries (e.g.,TensorFloworPy-
Torch [82])performdatapreprocessing inparallelforspeed,which
changestheorderoftrainingdata,evenifalgorithmicallythebatch
orderisfixed.Inaddition,coreDLlibraries,bydefault,leverage au-
totuneto automatically benchmark several modes of operation (i.e.,
underlying algorithms for computations such as addition) for each
primitivecuDNNfunction(e.g.,poolingandnormalization)inits
firstcall.ThefastestmodeofoperationisthenusedforthatcuDNN
function in subsequent calls. Different identical runs sometimes
use different modes of operation, introducing variance.
Ourexperiments(Section 5.2)showthatthese implementation-
level NI-factors alone cause an overall accuracy difference of up
to 2.9%. Specifically, we train the popular WideResNet-28-10 [ 112]
(or WRN-28-10forshort) DLnetworkfor imageclassification16
times using the same default training configuration (e.g., same
CIFAR100[ 61]trainingdata,batchsize,optimizer,andlearningrate
schedule),identicalmodelselectioncriteria(i.e.,selectingmodels
with the lowest loss on a validation set), the same DL libraries
(i.e., Keras 2.2.2, TensorFlow 1.14.0, CUDA 10.0, and cuDNN 7.6),
andidenticalhardware(i.e.,thesameNVIDIARTX2080TiGPU),
while disabling all algorithmic NI-factors (i.e., using fixed random
seedstoensureidenticalinitialweights,identicalbatchordering,
deterministicdropoutlayers,anddeterministicdataaugmentation).
This processgenerates 16models. Sinceall algorithmicNI-factors
are disabled, one may expect little variance across the 16 runs.However, we found that the accuracies of these 16 models vary
between 77.3% and 80.2% (a 2.9% difference).
These implementation-level NI-factors and differences are often
characteristicsandconsequencesoftheDL softwareimplementa-
tions,whichcreateuniquechallengesforSEresearchersandpracti-
tioners (Section 8), while DL researchers have paid little attention
toimplementation-levelNI-factors(thefocusisontheoreticalanal-
yses of DL training [26, 27, 35, 58, 68, 90]).
To see whether such difference is known, we conduct a sur-
vey (Section 6), which surprisingly shows, that 83.8% of the 901
responded researchers and practitioners with DL experience areunaware of (63.4%) or unsure about (20.4%) any implementation-level variance! Of the 901 respondents, only 10.4% expect 2% or
more accuracy difference across fixed-seed identical training runs.
Wealsoperformaliteraturesurveyof454papers randomly sam-
pled from recent top SE,AI, andsystemsconferences to understand
the awareness and practices of handling DL system variance in
research papers. Of 225 papers that train and evaluate DL systems,
only19.5 ±3%usemultipleidenticaltrainingrunstoquantifythe
variance of their DL approaches.
The per-class accuracy exhibits a much larger difference among
the 16 fixed-seed identical training runs. For example, in the previ-
ouslydescribed WRN-28-10runs,forthe “camel"class(allimages
withtheground-truthlabelof“camel”),themodels’accuracyvaries
from 38.1% to 90.5% (a 52.4% difference).
In addition, there are large differences in convergence time. For
example, the time to convergence of the 16 fixed-seed identicaltrainingrunsofanotherpopularnetwork,ResNet56[
51],ranges
from2,986to7,324seconds(aonehourand12minutesdifference)—
a 145.3% relative time difference. We also observe a discrepancybetween the empirical per-class accuracy and convergence timeand the corresponding estimates from the surveyed researchers
and practitioners.
Thus,it isimportant tostudy andquantify thevarianceof DL
systems, especially the implementation-level variance. On the one
hand, some practitioners, whose primary goal is to obtain the best
model,maybeabletotakeadvantageofthevariancebyrunning
multiple identical runs to achieve their goal.
Ontheotherhand,SE,AI,andsystemsresearcherswhopropose
new DL architectures and models that outperform existing ones
may need toexecute multiple identical runs to ensurethe validityof their experiments.
For example, a recent research paper [ 64] proposed a new ap-
proach with a reported 0.8% accuracy improvement over the stan-
dard WRN-28-10. Our experiments show that this network’s ac-curacy can vary by up to 2.9%. Therefore, the reported accuracy
improvement may not be statistically significant when considering
theaforementionedNI-factorsinthetrainingalgorithmandtheim-plementation.Inotherwords,ifonerunsthetwoapproachesagain,
the resulting mode of [ 64] may not outperform the WRN-28-10
model.Atbest,thecomparisonresultsstillhold,butthecurrentex-perimentsfailtoprovideevidencetodemonstratetheimprovement
given the possible variance.
There are existing theoretical analyses of DL training [ 26,27,
35,58,68,90] that study how well optimizers find good local op-
tima given algorithmic NI-factors. Such work fails to study the
nondeterminism in the underlying DL implementation.
Tofillthisgap,first,wesystematicallystudyandquantifythe
accuracy and time variance of DL systems across identical runs
using6widely-usednetworksand3datasets.Second,weconductasurveyto ascertainwhetherDL varianceandtheir implicationsare
known to researchers and practitioners with DL experience. Third,
we conduct a literature review of the most recent editions of top
SE, AI, and systems conferences to understand the awareness andpractices of coping with DL variance in research papers.
In this paper, we make the following contributions:
/diamondsolidFinding 0
: A list of implementation-level NI-factors (parallel
process,auto-selectionofprimitiveoperations,andscheduling
and floating-point precision) and algorithmic NI-factors (nonde-
terministicDLlayers,weightinitializationapproach,dataaug-
mentationapproach,andbatchordering),andtechniquesthat
controltheseNI-factorstoremoveorreducevariance(Section2).
/diamondsolidA DL variance study of 4,838 hours (over 6.5 months) of GPU
time on 3 widely-used datasets (MNIST, CIFAR10, CIFAR100)
with 6 popular models (LeNet1, LetNet4, LetNet5, ResNet38,
ResNet56, and WideResNet-28-10) on three core DL libraries
(TensorFlow, CNTK, and Theano):
•Finding 1 : The accuracy of models from 16 identical training
runs varies by as much as 10.8%, even after removing weak
models.
•Finding 2 : With algorithmic NI-factors disabled, DL model
accuracy varies by as much as 2.9%—an accuracy difference
caused solely by implementation-level nondeterminism.
•Finding 3 : Implementation-level NI-factors cause a per-class
accuracy difference up to 52.4%, while the per-class difference
is upto 100% with defaultsettings (i.e., withalgorithmic and
implementation-level nondeterminism).
772•Finding4 :Trainingtimevariesbyasmuchas145.3%(1hour
and 12 minutes) among fixed-seed identical training runs,
whilethetrainingtimedifferenceisupto4,014.8%withdefault
settings.
/diamondsolidA researcher and practitioner survey, with 901 valid replies,
reveals that:
•Finding5 : A large percentage of respondents are unaware of
(31.9%)orunsureabout(21.8%) anyvariance ofDLsystems;
there is no correlation between DL experience and awareness
of DL variance.
•Finding 6 : Even more researchers and practitioners (83.8%)
are unaware or uncertain of implementation-level nondeter-
minism in DL systems.
•Finding 7 : Only 10.4% of respondents expect 2% or more
accuracy difference across fixed-seed identical training runs.
•Finding8 :Most(77.7%)participantsestimatetheconvergence
timedifferencestobelessthan10%acrossidenticaltraining
runs, and the majority of respondents (84.5%) estimates a sim-
ilar 10%or lessconvergence timedifferenceamong fixed-seed
identical training runs.
/diamondsolidFinding9 :Aliteraturesurveyofarandomsampleof454papers
fromthemostrecenteditionsoftopSE(ICSE[ 7],FSE[4],and
ASE [1]), AI(NeurIPS/NIPS [ 9], ICLR[11], ICML[6], CVPR[ 3],
and ICCV [ 5]), and systems (ASPLOS [ 2], SOSP [ 10], and ML-
Sys[8])conferences,showsthat225paperstrainandevaluateDL
systems,only19.5 ±3%of whichusemultipleidenticaltraining
runs to evaluate their approaches.
/diamondsolidImplications and suggestions for researchers and practition-
ers, and raising awareness of DL variance (Section 8).
Code and data are available in a GitHub repository1.
2 NONDETERMINISM-INTRODUCING
(NI)-FACTORS
Many factors affect DL systems’ results and training time. The
first set of factors is the inputto a system. Such input includes the
trainingdataandhyper-parameters(e.g.,numberoflayers,dropout
rate, optimizer, learning rate, batch size, and data augmentation
method and settings). It is expected that models with different
inputs perform differently, and there is a flurry of work on how to
select the best input (e.g., hyper-parameter tuning) [17, 54, 111].
However, several factors (e.g., shuffled batch ordering) indepen-
dentofthesystem’sinputaffectthetrainingandaccuracyofthe
final models. We call these factors NI-factors. We divide NI-factors
into two categories: (1) algorithmic NI-factors, which are intro-
ducedtoimprovetheeffectivenessofthetrainingalgorithm,and
(2) implementation-level NI-factors, which are the byproduct of
optimizations to improve DL implementations’ efficiency.
2.1 Definitions
In this study, we define a DL system as the composition of a DL
algorithm and aDL implementation. DL algorithm is the theory
portion of DL and consists of model definition, hyper-parameters,
and theoretical training process. DL implementation consists of
1https://github.com/lin-tan/dl-variancehigh-level DL libraries (e.g., Keras), core DL libraries (e.g., Tensor-
FlowandPyTorch), low-levelcomputationlibraries (e.g.,cuDNNand
CUDA), and hardware (e.g., GPU, CPU, and TPU). The DL training
process spans across the DL algorithm and DL implementation.
2.2 Algorithmic NI-factors
ThemostcommonalgorithmicNI-factorsincludenondeterministic
DLlayers(e.g.,dropoutlayer),weightinitialization,dataaugmen-
tation, and batch ordering.
NondeterministicDL Layers: DL architectures can contain non-
deterministiclayers.Forexample,dropoutlayers[ 99]arecommonly
usedtopreventoverfitting.Theyrandomlysetpartsoftheinput
tensor to zero during training and guide each neuron to be trained
withdifferentportionsofthetrainingdata. Dropouttensorsarecho-
senrandomlyon-the-flyduringtraining,whichmeanstwoidentical
runs could produce two different models with different accuracies
and different training times.
Weight Initialization: Weightinitialization [ 42,50,66,90]i sa n
important step in DL training [ 101,104]. Random weight initial-
ization samples the initial DL model weights from a predefined
distribution. Goodfellow et al. [ 45] state that random initialization
“breaksthesymmetry”acrossalltheweighttensors.Thisprocess
helpssimilarlystructuredneuronslearndifferentfunctionsinstead
of repeating each other. Thus, they learn different aspects of the
trainingdata andhelp toincreasethe model’sgeneralization. How-
ever,differentinitialweightsmayresultinconvergencetodifferent
local minima [ 36,40,67]. Therefore, random initialization can lead
to variance in model accuracy across identical runs.
Data Augmentation: DL training algorithms also utilize the ran-
domnessin dataaugmentation toimprove theireffectiveness (i.e.,
producemoreaccuratemodels).Dataaugmentation[ 96]isaninex-
pensivemethodthatrandomlytransformstheinputtoincreasethe
input’s diversity. It has been shownto improve the generalization
ofthefinaltrainedmodel.Randomlytransformingthetrainingdata
will result in nondeterministic identical training runs.
Batch Ordering: Random batch ordering also improves the gen-
eralization of DL models. It breaks up the order of the training
datatopreventthemodelfromquicklyoverfittingtoaparticular
label [66]. Reordering training batches at each epoch results in
nondeterministic identical training runs.
2.3 Implementation-level NI-factors
Implementation-levelNI-factorsarecausedbylibraries(e.g.,Tensor-
Flow,CUDA,andcuDNN).Themostcommonimplementation-level
NI-factors are parallel computation, nondeterministic primitive op-
erations, and rounding errors due to scheduling.
Parallel processes: Core DL libraries (e.g., TensorFlow and Py-
torch) provide options to use multiple processes to improve the
efficiencyofDLsystems.Forexample,thecorelibraries,bydefault,
run the data preprocessing task in parallel to prepare the training
datafaster.However,duetotherandomcompletionorderofpar-
alleltasks,theorderoftrainingdatamaychangeandimpactthe
optimizationpathofthetrainingprocess,resultinginvarianceeven
if data preprocessing itself is deterministic.
773Figure 1: Overview of the experimental method
Auto-selection of primitive operations: Core DL libraries im-
plementDLalgorithmsbyleveragingtheGPU-optimizedDLprimi-
tivesprovidedbylow-levellibraries(e.g.,cuDNNandCUDA).When
usedwith NVIDIA GPUs, cuDNNprovides hardware-accelerated
primitives for common DL operations such as forward and back-
ward convolution, pooling, normalization, and activation layers.
cuDNN provides several modes of operation (i.e., different compu-
tational algorithms) for primitive functions.
By default, core DL libraries enable autotune, which automat-
ically benchmarks several modes of operation for each primitivecuDNN function in its first call. The fastest mode of operation is
thenusedforthatcuDNNfunctioninsubsequentcalls.Theexact
mode of operation for each primitive used in each run changes de-
pending on the dynamic benchmark result. Different identical runs
sometimes use different modes of operation, introducing variance.
Furthermore,somemodesofoperationarenondeterministicdue
to rounding errors introduced by scheduling (see below).
Scheduling and floating-point precision: GPU programming
useswarpasaunitofcomputation.Awarpconsistsof32parallel
threads with concurrent memory access. Due to the limited preci-
sion(32-bits)ofDLmodels,roundingerrorsareintroducedatevery
step of floating-point calculation. In the GPU model, concurrent
accesses to a single memory address must be serialized to prevent
race conditions with no guaranteed order of access. Therefore, the
roundingerrorintroducedineachwarpmayvaryacrossfixed-seed
identical training runs due to the different accessing orders.
For example, matrix reduction operations such as atomicAdd
(used in depthwise convolution layers) are affected by the varying
serialization order. Since floating-point operations are not associa-
tiveduetoroundingerrors[ 43],varyingordersofadditionsmay
produce nondeterministic output ( A+B+C/nequalB+C+A).
2.4 Controlling NI-factors for determinism
Removing algorithmic NI-factors enables us to study the nondeter-
minismintroducedbyimplementation-levelNI-factors.Inaddition,
deterministic training may be desirable for debugging and other
purposes. Thus, we identify techniques that control algorithmic
and implementation-level NI-factors to remove or reduce variance.
Controllingalgorithmic NI-factors: AllalgorithmicNI-factors
arecontrolledbypseudo-randomnumbergenerators.Thus,wecancontroltheseNI-factorsbyfixingtherandomseedsatthebeginningofeachruntoachievealgorithmicdeterminismacrossidenticalruns
while still maintaining the pseudo-random characteristic within a
single run. We defined this as fixed-seed identical training runs.Controllingimplementation-levelNI-factors: Tocontrolthese
NI-factors,weneedtotakeseveralsteps.First,theDLsystemshould
not use multiple processes that cannot guarantee data order. Forexample, using more than one worker in the data generator to
feed training data would shuffle the batch ordering even with fixed
random seeds.
Second, the autotune mechanism should choose deterministic
implementations of primitive operations only. For example, in Ten-
sorFlow1.14.0,iftheenvironmentflag TF_CUDNN_DETERMINISTIC is
setto1,theautotunemechanismwillnotconsiderthenondeter-
ministic modes for cuDNN primitive functions.
Third, since some operations (e.g., atomicAdd ) are nondetermin-
istic when used on a GPU due to nondeterministic serialization,
theinputoftheseoperationsshouldbeserializedafterallparallel
executions (i.e., to ensure a deterministic ordering of input). Then,
the operations should be executed on a single CPU thread.
Finally, one solution to achieve complete deterministic training
is forcing the DL system to run completely in a serial manner (i.e.,
runningonasingleCPUthread).However,thisoptionpreventsDL
systemstoutilizethehardwareefficientlyandmaybeunrealistic,
as many models would take months or years to train on a single
CPUthread.Asfuturework,deterministicmultithreading[ 32]may
be promising for more realistic deterministic DL systems.
Amajorgoalofthispaperistoquantifythevarianceintroduced
by implementation-level NI-factors.
3 EXPERIMENTAL METHOD
First,weextractthe defaultinput (i.e.,trainingdata,hyper-parame-
ters,andoptimizers)ofaDLsystemfromexistingwork(Section4).
Figure 1 shows an overview of our experimental method. We gen-
eratedifferent environments thatcombine differentversions ofDL
libraries (e.g., high-level libraries, core libraries, and low-level li-
braries). For all environments, the hardware is the same (details in
Section4).Forexample,onesuchenvironmentincludesKeras2.2.2,
TensorFlow 1.14.0, cuDNN 7.6, and CUDA 10.0. Each network is
coupled with its default input. For example, the CIFAR 100 dataset
(including training, validation, and test data) is used to train the
WRN-28-10networkwithstochasticgradientdescent(SGD)opti-
mizer in 50 epochs. Table 1 shows the corresponding default input
foreachnetwork.Eachnetwork(includingitsdefaultinput)com-
binedwithone nondeterminismsetting (detailsbelow)isdefinedasa
setofsetting.Forexample,onesetofsettingsthatweuseistraining
theWRN-28-10networkwithallalgorithmicNI-factorsdisabled
(i.e.,fixed-seednondeterminismsetting).Foreachenvironmentand
774setting combination, we perform an experimental set and measure
the accuracy and time variance across nidentical training runs.
To ensure a valid study, we address one main challenge: to mea-
surerealisticvariance,theexperimentsneedtoreflecttherealusage
ofDLsystems.Wepicktrainingfrom-scratchasourscenariofor
thetrainingphasebecauseitisacommonandfundamentaltraining
scenario[ 14,51,94,98,103,112,122],i.e.,wetrainanewDLmodel
from the beginning, starting from randomly initialized weights.
Inaddition,wefocusonstudyingthevarianceofmodels’overall
accuracy,per-classaccuracy,andtrainingtime,asthesearecommon
metricsthatDLresearchersandpractitionersuse,andtherehave
beenmanytechniques[ 14,22,38,45,103,118]proposedtoimprove
on these metrics.
Further, to make sure the accuracy and time that we observe
are valid, we check that our results are equivalent to the ones
reported in the original papers. Training inputs (e.g., training data,
prepossessing methods, and optimizer) are chosen to match, asclosely as possible, those reported or used in the authors’ code.
Whileacompletereproductionisoftenimpossible,wereproduce
previousworkasfaithfullyaspossiblebyusingreportedsettings
and ensuring at least one of our runs can reach almost identical
accuracy to that of the original work.
Finally,weperformtwostatisticaltests(Levene’stestforvari-
anceandMann WhitneyU-testformean) toensurethatwe draw
statistically significant conclusions.
3.1 Experimentalsetsofidenticaltrainingruns
Thetrainingphaseisaniterativeprocessandaftereachiteration
(i.e., epoch) a checkpoint of the model is stored. After the training
finishes,thebestcheckpointisselectedbasedonafinal modelselec-
tioncriterion. We focus on two common (77.5% of the respondents
in our survey use one of these criteria) model selection criteria:
•Best-loss selection criterion: The final model is the check-
point with the best (i.e., lowest) validation loss.
•Best-accuracyselectioncriterion: Thefinalmodelisthecheck-
point with the best (i.e., highest) validation accuracy.
Validation loss and accuracy are calculated on the validation set
(i.e., unseen data, different from the training data, and used to tune
the model). We report the test accuracy of the selected best model
whichiscalculatedonthetestdata(i.e.,unseendata,differentfrom
the training and validation data).
Inpractice,thetrainingrunswouldendiftheselectionmetric
(i.e.,validationlossoraccuracy)didnotimproveafterasetnumber
ofepochs(i.e.,patience).Inthisstudy,weinsteadrunthetrainingtoamaximumnumberofepochswhilestoringthemodelcheckpoints.Oncethetrainingisdone,weselectthemodelbasedontheselection
criterion and then compute the training time as if the training had
stoppedatthebestcheckpoint.Thisisanestimationoftrainingtime
without running a separate set of experiments for each criterion.
We define identical training runs as training runs executed with
thesame environment (i.e., hardware and DL libraries), the same
network architecture, and the same inputs (i.e., training data, hyper-
parameters,andoptimizers).Eachidenticaltrainingrunisfollowedbyaninferencerunonthetestdatatocomputethemodelaccuracy.
Anexperimentalset isagroupofidenticaltrainingruns.Wemake
sure to avoid measurement bias [ 79] as much as we could by usingthesame machinealongwith Dockerenvironments thatarebuilt
fromthesamebaseimage. Theonlychangesacrossexperimental
setsaretheDLlibrarycombinationsandthesetofsettings(i.e.,the
nondeterminism-level, the network, and its default input). In each
experimental set, we perform n=16runs.
3.2 Nondeterminism-level settings
We perform two categories of experiments with different non-
determinism-levels: the default and fixed-seed settings.
Default identical training runs are experiments that do not en-
forcedeterminism(i.e.,noneoftheNI-factorsarecontrolled).Theseareidenticaltrainingrunswiththedefaultinput(trainingdataand
hyper-parameters).
Fixed-seedidenticaltrainingruns areexperimentsforwhichal-
gorithmic NI-factors are disabled, i.e., we use the same randomgenerator and the same seed. For example, with the TensorFlowcorelibrary,wesettheglobalPythonrandomseed,Pythonhash
seed, Numpy random seed, and the TensorFlow random seed to be
identical. Initializing all random number generators with identical
seed disables all algorithmic NI-factors (i.e.,dropout layers, initial
weights, data augmentation, and batch ordering).
3.3 Metrics and measurements
Tomeasurethevarianceacrossidenticaltrainingruns,wemeasure
amodel’soverallandper-classaccuracyonthetestset.The overall
accuracymeasurestheportionofcorrectclassificationsthatamodel
makes on test samples. The per-class accuracy splits the overall
accuracy into separate classes based on the ground-truth class
labels(i.e.,theaccuracyofthemodelforeachclass).Forexample,
the MNIST dataset has 10 classes, so an MNIST model would have
10per-classaccuracyvalues(oneforeachdigit).Forallidentical
training runs, we measure the total training time as well as thenumber of epochs until convergence (i.e., until the checkpoints
specified by the selection criterion). For each experimental set, the
maximumdifferenceshowsthemostextremegapofmodelaccuracy
and training time between the best and the worst runs.
3.4 Statistical tests
Levene’stest is a statistical test to assess the equality of variance
oftwosamples.Specifically,whentestingaccuracyvariance,thenull hypothesis is that the accuracy variance of set A is equal to
the accuracyvariance ofset B. Ifwe findthat p-value <0.05 then
wecanconfirmwith95%confidencetheaccuracyvarianceofset
A is different from set B. Thus, if the accuracy variance of set A is
smaller, then runs in set A are more stable than in B.
Mann Whitney U-test is astatistical testto assessthe similarity
of sample distributions. We run the U-test instead of the T-test
because the U-test does not assume normality while the T-testdoes. For example, when comparing two sets of runs (A and B),
the null hypothesis is that the accuracies of set A are similar to set
B. If we find that p-value <0.05, then we can confirm with 95%
confidence that our alternative hypothesis is true which means set
A hasstatistically differentaccuracies than setB. Wecompute the
effect size as the Cohen’s d[28] to check if the difference has a
meaningful effect ( d=0: no effect and d=2: huge effect [89]).
775Table 1: Datasets, networks, and training settings
Dataset#samples Network Settings
Train Val Test Name #parameters #epochs Optimizer
MNIST 60,000 7,500 2,500LeNet1 7,206
50 SGD LeNet4 69,362
LeNet5 107,786
CIFAR10 50,000 7,500 2,500ResNet38 565,386200 AdamResNet56 857,706
CIFAR100 50,000 7,500 2,500 WRN-28-10 36,536,884 200 SGD
4 EXPERIMENTAL SETTINGS
Datasets and models: We perform our experiments using three
popular datasets: MNIST [ 65], CIFAR10 [ 61], and CIFAR100 [ 61].
We choose image classification architectures as they are often used
as test subjects in recent SE papers that test [ 41,71,73,85,100,
105,106,109,115,120],verify[46,83],andimprove[ 39,56,74,110,
113,116] DL models and libraries. Table 1 shows each dataset with
thecorrespondingnumbersofinstancesineachsubset(training,
validation, and test). The training set is used to train the model.
Following common practice [ 14,24,45,51,62,97,102], we use
the validation set to select the best model. The test set is used to
evaluate the final model.
WechoosetoexperimentonLeNet[ 65],ResNet[ 51],andWideRes-
Net [112] architectures as they are popular networks for image
classification.Inourliteraturereview,ofthe225relevantpapers,
64% of papers use or compare to one of these architectures. Table 1
alsoshowsthenumberoftrainableparametersforeachnetwork.
Our networks are diverse in size, from 7,206 (LeNet1) to 36,536,884
parameters (WRN-28-10).
Wereproducepreviousworkasfaithfullyaspossiblebyusing
networks and settings recorded by previous work and ensuring
some of our runs have a similar (within 1%) accuracy as that in the
originalwork.Wealsouse(whenavailable)theoriginalimplementa-
tionoftheapproachfromtheauthorandtheKerasimplementation
(if available) to reduce the risk of introducing new bugs.
Welistthetrainingconfigurationsusedintable1.Toensurethat
the model will converge (i.e., training loss stops improving) within
the maximum number of epochs (Table 1), we empirically choose a
maximumnumberofepochslargerthanthenumberofepochsto
convergence using both selection criteria.
Wecannotusenetworksfrompriorwork[ 84,85,105]sincethey
onlyprovidepre-trainedmodelsanddonotprovideenoughdetails
for us to reproduce the training runs.
DL libraries: We use Keras version 2.2.2 [ 25] as our high-level
library since it provides us with the ability to transparently switch
between three DL core libraries (TensorFlow [ 13], CNTK [ 16], and
Theano[91]).Thisensuresthatthecomparisonacrosscorelibraries
is fair and the least affected by our code. We perform our experi-
mentswiththeofficialTensorFlowversions(includingthelatest)
(1.10,1.12,and1.14),CNTKversion(2.7),andTheanoversion(1.0.4).
We pair each version of the core libraries with the officially sup-
ported low-level cuDNN and CUDA versions. For example, Ten-
sorFlow 1.12 supports cuDNN 7.3 to 7.6 coupled with CUDA 9.0,
while TensorFlow 1.14 supports only cuDNN 7.4 to 7.6 coupledwithCUDA10.0.SinceitisnotpracticaltoperformexperimentsTable 2: Maximum differences of overallandper-class accu-
racy among defaultandfixed-seed identical training runs
Setting NetworkOverall(%) Per-class(%)
Diff SDev (SDevCI) Diff SDev (SDevCI)DefaultLeNet1 10.8 2.6 (2.0-3.8) 99.6 24.5 (19.0-35.2)
LeNet4 10.6 2.6 (2.0-3.7) 100.0 24.7 (19.1-35.5)
LeNet5 90.4 38.7 (30.0-55.6)*100.0 44.5 (34.4-63.9)
ResNet38 1.9 0.5 (0.4-0.7) 11.7 2.8 (2.2-4.1)
ResNet56 2.1 0.6 (0.4-0.8) 11.9 2.8 (2.2-4.0)WRN-28-10 2.8 0.8 (0.6-1.1) 50.0 13.3 (10.3-19.1)Fixed-seedLeNet1 0.1 <0.1 (<0.1) 0.8 0.3 (0.2-0.4)
LeNet4 0.5 0.1 (0.1-0.2) 1.9 0.6 (0.5-0.9)LeNet5 1.2 0.3 (0.2-0.4) 4.8 1.3 (1.0-1.9)ResNet38 2.7 0.6 (0.5-0.8) 12.2 3.3 (2.6-4.8)ResNet56 1.9 0.5 (0.4-0.7) 10.6 2.3 (1.8-3.3)WRN-28-10 2.9 0.7 (0.6-1.0) 52.4 16.3 (12.6-23.4)
*4/16 runs produce weak models that have lower than 20% accuracy
Figure2:Boxplotsoftheoverallaccuracyfordefaultidenti-
cal runs with the largest overall accuracy difference
on all library combinations, we use 11 library combinations for
TensorFlow, one combination each for CNTK and Theano.
Infrastructure: Wecarryoutallexperimentsonamachinewith
56 cores, 384GB of RAM, and RTX 2080Ti graphic cards each with
11GBmemory.Toaccommodatemultiplecombinationsoflibraries,
we use Anaconda (4.4.10) with Python (3.6) and Docker (19.03).
5 RESULTS AND FINDINGS
We perform 2,304 identical training runs (144 experimental sets
with16runseach)ofsixnetworksonthreedatasets,withtwolevels
of nondeterminism, using three core libraries (TensorFlow, CNTK,
and Theano) which is 4,838 hours ( over6.5months ) of GPU time.
5.1 RQ1: How much accuracy variance do
NI-factors introduce?
To investigate the variance caused by NI-factors, we run 16 default
identical training runs for each of the 66 experimental sets (i.e.,combinations of 6 networks and 11 environments). Recall that
defaultidentical trainingrunsaredefined astrainingruns withthe
samedefaultinputs wherenoNI-factorsare disabled(Section3.1).
Toestimatethe extremecase,wecomputethemaximumdiffer-
ence of accuracy (overall and per-class) between the least accurate
andthemostaccuratedefaultidenticaltrainingrunsofanexperi-
mental set while the standard deviation estimates the average case.
Table 2 (Default ) shows results for RQ1. Columns Diffshow the
maximumdifferencesofaccuracywhilecolumns SDevand(SDe-
vCI)shows the standard deviationof accuracy among 16 identical
training runs and corresponding confidence interval (i.e., with 90%
confidence,theconfidenceintervalwouldcontainthepopulation
standard deviation). We only show the larger accuracy differences
whenusingeitherselectioncriterion(best-lossorbest-accuracy)
astheresultsaresimilarbetweenthecriteria.Figure2showsthe
776boxplots of the overall accuracy of each network. The triangles
represent the mean accuracy and the orange line is the median.
Dots outside of the whiskers are outliers.
Acrossdefaultidenticaltrainingruns,theaccuracydifferenceis
as big as 10.8%, even after removing weak models. ( Finding 1 ).
Specifically,intheLeNet5defaulttrainingexperimentalset(with
TensorFlow 1.14.0, CUDA 10.0, cuDNN 7.5, and loss selection crite-
rion), the most and least accurate runs have an overall accuracy of
99.0% and 8.6% respectively (a 90.4% difference). The worst model’saccuracyislowerthanrandomguesses(i.e.,10%becausetheMNIST
dataset has 10 classes). This large accuracy difference is caused by
therandominitializationoftheweights[ 101,104].Particularly,four
runs do not improve much after training—with the final models’
accuraciesbeing8.6%,9.9%,10.6%,and19.7%(theoutliersshownascirclesinFigure2).Whilethefourrunsproduceweakmodels,they
arefaithfulreproductionsoftrainingwithwidely-usednetworks
and algorithms using realistic data and settings. The fact that 4
outof16runsfailtoimprovesignificantly,showsthe importance
ofreportingthevariance betweenmultipleidenticaltrainingruns
so thatthe DL approaches can be evaluated on not just their best
accuracy, but also on how stable the training process is.
If we exclude networks with such weak models, we still see
an accuracy difference up to 10.8% with LeNet1 (the difference
between87.8%and98.6%).ForWRN-28-10,thelargestdifference
is 2.8% (between 78.2% and 81.0%) respectively. Although these
differences may seem small, researchers [ 64] report improvements
of0.8%whencomparingagainstWRN-28-10withoutaccounting
forNI-factors.Atbest,thecomparisonconclusionsstillhold,but
the papers fail to provide evidence for that.
The per-class accuracy differences are even larger compared to
theoverallaccuracydifferences(Table2,column Default:Overall
versus column Default: Per-Class ). On the least accurate run of
LeNet5,thetrainedmodelfailscompletelyonasingleclass(i.e.,the
prediction accuracy for the class digit “0” is 0%), while, for otherruns, the highest prediction accuracy for the same class is 100%.
Digit“0”has261testimages(allclasseshavesimilarnumbers)so
such single-class failures are not due to insufficient instances or
biasdistributionofthatclass.Asimilarsingle-classfailurehappens
for LeNet1 and LeNet4 training runs. The standard deviation is
smallerforthesenetworks(24.5%and24.7%comparingto44.5%)
because only one run completely fails.
As another example, WRN-28-10 default identical training runs
(usinglibrarycombinationTensorFlow1.12.0,CUDA9.0,cuDNN
7.6, and best-accuracy selection criteria) incur a maximum overall
accuracydifferenceof2.8%.Withthesamesettings,theper-class
accuracy difference is 50.0% (dropping from 72.7% to 22.7%) for
theclass“bee”(with22testsamples).Per-classaccuracyvariance
can be problematic for applications where the accuracy of specific
classes is critical. For example, the accuracy variance of the pedes-
trian class of a self-driving car’s object classification system could
vary pedestrian prediction reliability. This, in turn, could endanger
pedestrians, even if the overall variance of the model is small.
NI-factors cause a complete single-class failure, where thebiggest per-class accuracy difference is 100% with a standard
deviation of 44.5% (Finding 3(a)).Figure 3: Boxplots of the overall accuracy for fixed-seedidentical runs with the largest overall accuracy difference
5.2 RQ2: How much accuracy variance do
implementation-level NI-factors cause?
Accuracy variance: We analyze nondeterminism introduced by
implementation-level NI-factors by performing 66 experimentalsets (i.e., combinations of 6 networks with 11 environments) of
fixed-seedidenticaltrainingruns(eachwith16runs). Recallthat
fixed-seedidenticaltrainingrunsaredefaultidenticaltrainingruns
withalgorithmicNI-factorsdisabledusingfixedrandomseedini-
tialization (Section 3.2).
Table 2 (Fixed-seed ) shows the largest accuracy differences of
the overall and per-class accuracy of all models (for any library
combinationsandselectioncriteria)withdisabledalgorithmicNI-
factors (i.e., among fixed-seed identical training runs).
Implementation-levelNI-factorscauseaccuracydifferencesas
large as 2.9% ( Finding 2 ), while per-class accuracy differences
are up to 52.4% (Finding 3 (b)).
Among the fixed-seed identical training runs of WRN-28-10
(withTensorFlow1.14.0,CUDA10,cuDNN7.6,andlossselection
criterion), the most and the least accurate runs have an overall
accuracyof 80.2%and 77.3%respectively. Inthe same experimental
set, the implementation-level NI-factors cause a per-class accuracy
difference of 52.4% (the “camel” class–with 21 test samples– has
90.5% and 38.1% accuracies in the most and least accurate run). All
other classes have similar numbers of test samples so the large
per-class accuracy difference is not due to insufficient instances or
bias distribution classes.
Thelackofcompletefailurecausedbytherandomweightini-
tialization(analgorithmicNI-factors)inLeNettraining(Figure3)
indicatesthattrainingismorestablewithoutalgorithmicNI-factors.
When comparing the results of setting DefaultandFixed-seed in
Table 2, LeNet and ResNet56 have smaller overall and per-class ac-
curacy differences among default identical training runs. While for
ResNet38andWRN-28-10,theaccuracydifferencesamongfixed-
seed identical training runs are smaller. Levene’s test cannot statis-
ticallyconfirmthesignificance(p-value >0.05)ofthesedifferences
in variance for all networks except for LeNet5 (where there are
complete failures in identical training runs with default setting).
Table 2 (Fixed-seed ) shows that except for ResNet38, the more
complexanetworkis(i.e.,moretrainableparameters),thelarger
the accuracy (overall and per-class) variance exists across fixed-
seed identical training runs. For more complex networks, the error
introduced by nondeterminism might propagate further.
To demonstrate the importance of performing identical training
runswhen comparingdifferent DLapproaches, weconsider asce-
nariowhereResNet56isabaselineapproachtotheCIFAR10image
classification problem and ResNet38 is the proposed improvement.
777Figure4:Boxplotsoftheoverallaccuracyoffixed-seediden-
tical training runs with different core libraries
Figure5:Boxplotsoftheoverallaccuracydifferenceoffixed-seedidenticaltrainingrunswith11low-levellibraryversioncombinations for each network
Among 16 fixed-seed identical training runs, ResNet56 averages
91.2%intestaccuracywhileResNet38averages90.3%.TheU-test
confirms(withp-value <0.01)thatResNet56 has0.9%highertest
accuracy than ResNet38 with an effect size (Cohen’s d) of 1.7 (very
large effect). Hence, there is no improvement from the proposedtechnique (ResNet38) over the baseline (ResNet56). However, if
each approach only runs once, in the most extreme case, ResNet56
accuracy is reported with its worse run (90.4%) and ResNet38 with
its best run (91.4%), the researchers might have come to an invalid
conclusionthatResNet38has1%higheraccuracythanResNet56.Re-searchersandpractitionersshouldbeawareofDLsystemvariance,evenwithonlyimplementation-levelNI-factors,sotheywouldper-
form multiple identical training runs when comparing approaches.
Differentcorelibraries: Weinvestigateifswitchingcorelibraries
leads to different accuracy variance among fixed-seed identical
trainingruns.Sinceitisprohibitivelyexpensivetorunallcombina-
tions of core and low-level library versions (our experiments’ GPU
timeare alreadyover 6.5months), wecompare thelatest versions
of core and low-level libraries at the time of the experiment (i.e.,
inaddition,werun12moreexperimentsets–combinationsof6
models with 2 environments).
Figure4showstheboxplotsoftheoverallaccuracyoffixed-seed
identical training runs for the experimental set of each network
with the best-loss selection criterion across three different core
libraries. The accuracy variance is similar across different core
libraries.Forexample,forResNet56theaccuracydifferencewith
CNTKis1.5%(between91.8%and90.3%)and1.9%withTensorFlow.
All core libraries are affected similarly by implementation-level
NI-factors,asLevene’stestcannotrejectthenullhypothesisthat
each core library has a different accuracy variance (p-value >0.1).
Different low-level libraries versions: We analyze the over-
all accuracy differences of the 11 low-level library combinations
(cuDNN and CUDA) with TensorFlow to see if there is still vari-
ancewhenswitchingversionsofthelow-levellibraries.Figure5shows the boxplots of the overall accuracy differences of fixed-
seed identical training runs when training each network with each
of the 11 library combinations. All training runs are affected byTable3:Runningtimetoconvergencedifferencesamong de-
faultandfixed seed identical training runs
Setting NetworkTime Loss (seconds) Time Acc (seconds)
Diff RelDiff RelSDev Diff RelDiff RelSDevDefaultLeNet1 27 24.5% 6.5% 41 47.2% 8.5%
LeNet4 22 17.4% 4.7% 25 19.9% 4.0%LeNet5 155 3,940.7%*54.2% 158 4,014.8%*55.1%
ResNet38 434 21.4% 5.3% 2,953 133.2% 18.7%ResNet56 699 23.9% 6.0% 3,813 116.5% 17.7%WRN-28-10 2,333 12.9% 3.2% 6,316 46.0% 8.8%Fixed-seedLeNet1 17 14.3% 3.8% 18 14.4% 3.8%LeNet4 17 13.3% 3.6% 25 20.1% 6.2%LeNet5 31 25.8% 5.6% 37 30.0% 6.0%ResNet38 415 20.4% 4.4% 2,782 115.5% 15.9%ResNet56 467 16.4% 3.6% 4,338 145.3% 22.5%
WRN-28-10 2,197 12.2% 2.9% 5,625 38.3% 10.1%
*3/16 runs stuck at the first epoch
implementation-level NI-factors, independently from the low-level
libraries used. For example, with WRN-28-10, the largest overall
accuracydifferenceis2.9%(reportedinTable2).Onaverage,across
11 experimental sets, the accuracy difference for this network is
over 2% while the smallest accuracy difference is 1.6%.
5.3 RQ3: How much training-time variance do
NI-factors introduce?
Westudythevarianceinoveralltrainingtimetoconvergenceof
default identical training runs and fixed-seed identical training
runs which is often the primary variance that researchers and
practitioners care about. We measure training time to convergence
with respect to best-loss and best-accuracy selection criteria.
Table3showstheanalysisoftherunningtimetoconvergencefor
defaultidenticaltrainingrunsandfixed-seedidenticaltrainingruns.
Time LossandTime Accdenotethetrainingtimeusingtwopopular
model selectioncriteria—best-loss and best-accuracy,respectively.
For each selection criterion, the table shows the time differencebetween the slowest and the fastest runs (columns Diff). Since
the running time is very different across networks, we computethe relative time difference (columns RelDiff) —the ratio of the
timedifferenceovertherunningtimeofthefastest.Togivesome
indicationofanaveragecase,columns RelSDevshowtherelative
standard deviation (i.e., coefficient of variation [ 37]) of the 16 runs.
Among default identical training runs, LeNet5 has the largest
relativetrainingtimedifferenceof4,014.9%usingthebest-accuracy
selection criterion. As discussed in RQ1, three runs fail to improve
after the first epoch (3.9 seconds for the fastest), creating such a
largetimedifference.However,sinceonlythreerunsgotstuckat
the first epoch, the relative standard deviation is 55.1%.
Thelargesttrainingtimedifferenceamongdefaultidenticaltrain-
ingrunsis6,316seconds(1hourand40minutes)fortheWRN-28-10networkwiththebest-accuracyselectioncriterion(relativetraining
time difference of 46.0%). Given how expensive DL training can be,
46.0% of training time difference could mean days or longer.
Amongfixed-seedidenticaltrainingruns,ResNet56incursthe
largest relative training time difference (145.3%) when using thebest-accuracy selection criterion. This means that the deviationcaused by random computation errors can lead to significantly
different optimization paths, hence different convergence time.
778Finding4: Trainingtimevariesbyasmuchas145.3%(1hour
and12minutes)amongfixed-seedidenticaltrainingruns,while
the training time difference is up to 4,014.8% with default iden-
tical training runs.
6 RESEARCHER AND PRACTITIONER
SURVEY
We conducta survey tounderstand ifresearchersand practitioners
are (1) aware of the NI-factors and (2) if they correctly estimate
how much impact NI-factors have on DL experiments.
6.1 Survey design and deployment
We conduct an anonymous online survey over a period of two
weeksinFebruary2020.WetargetGitHubuserswhocommitted
code to popular public DL projects under the topics TensorFlow ,
PyTorch,CNTK,Theano,deeplearning ,and neural-network .Wesend
19,333 emails using Qualtrics services and receive 1,051 responses
(5.4% response rate), 901 of which are valid. Many of the email
addresses are from industry (364 from Microsoft, 833 from Google,
and80fromNVidia),andfromacademia(797fromU.S.universities).
Wetakethefollowingstepstoensureoursurveyof29questions
is valid and not biased. First, we conduct three rounds of in-person
pilotstudieswithtengraduatestudentswhohaveworkedonDL
projects and use their feedback to remove ambiguity and biases in
our initial design. The pilot studies’ participants do not participate
in the actual survey.
Second, to ensure participant’s understanding, we define impor-
tantterms(e.g.,deeplearning,determinism,identicaltrainingruns,
and fixed-seed identical training runs) in the context of our survey
before the questions. For example, we give a clear definition of
a deterministic DL system before survey questions: “We define a
systemasdeterministicifthesystemhasidenticalaccuracyorsimilar
running time between multiple identical runs. In the case of a DL
system, identical trainingruns havethe sametraining dataset,data
preprocessing method(e.g.,same transformationoperations), weight
initializer(i.e.,drawnfromthesamerandomdistribution),network
structure, loss function, optimizer, lower libraries, and hardware.”
All questions and definitions are included in the GitHub reposi-
tory whose link is provided in Section 1.
6.2 Survey results and findings
Participant Experience and Statistics: Of the 901 responses,
472 work in industry and 342 work in academia. Participants have
an average work experience of 6.3 years and a maximum of 47years. The average DL experience is 3.0 years. Over 68.6% learn
AI formally(e.g.,undergraduate andgraduate school) and32 are
involved with 5 or more AI projects.
AwarenessofNI-factorsinDLsystems: WeaskQuestion20: “In
youropinion,areDLsystemsdeterministic?” togaugetheawareness
that participants have of the NI-factors (results in Figure 6).
Many respondentsare unaware(31.9%) or uncertain(21.8%) of
anyvarianceofDLsystems;andthereisnocorrelationbetween
DL experience and awareness of DL variance (Finding 5).Figure 6: Distribution of responses to Question 20 (Defaultidentical runs ) and Question 26 (Fixed-seed identical runs )
Figure7:Estimationof overallandper-class accuracydiffer-
ence across default and fixed-seed identical training runs
To measure the correlation between different factors, we use
the Pearson correlation coefficient ( r), a statistical indicator oflin-
ear correlation between two variables ( |r|=0 means no correlation,
while|r|=1suggestsastrongcorrelation).Thereisnocorrelation
between awareness of DL variance and DL experience ( r=0.03), DL
educational background ( r=0.04), or job position ( r=0.02). These
resultssuggestlimitedawarenessofvarianceinDLsystemsregard-
less of experience and educational background.
Awarenessofimplementation-levelNI-factorsinDLsystems:
WedesignQuestions26: “Doyouexpectfixed-seedidenticalDLtrain-
ingrunsto bedeterministic?” tostudyhow awarerespondentsare
with implementation-level NI-factors (results in Figure 6).
Most (83.8%, 755 out of 901) of our surveyed researchers and
practitionersareunawareoforunsureaboutimplementation-
level NI-factors (Finding 6).
Thereisnocorrelationbetweenawarenessofimplementation-
levelNI-factorsandDLexperience( r=0.03),DLeducationalback-
ground ( r=-0.01), or job position ( r=0.06).
Estimate of accuracy difference: We ask participants who an-
swered “Yes” or “Maybe” to Question 20 to answer Question 21:
“From your experience, in the worst case, by how much would you
expectthefinaloverallaccuracy(e.g.,inclassificationtask)tovaryin
terms of absolute value between identical training runs? ”. Also, after
Question26,weaskparticipantsasimilarQuestion27regarding
fixed-seed identical training runs. Those who answer “Maybe” (i.e.,
unsure about DL system variance), we still ask them to estimate
themagnitudeofthevariance.“Other”isanoptiontospecifyan
explanation if no estimate is given.
Figure7showsparticipants’estimationsoftheoverallandper-
class accuracy differences across default identical training runs
(Default Overall andDefault Per-Class ) and fixed-seed identical
training runs (Fixed-seed Overall andFixed-seed Per-Class ).No vari-
anceindicatesparticipantsthatareunawareofthenondeterminism
ofDLsystems.Someparticipantschoose“Others”andstatethatthe
accuracy difference depends on the task and network architecture.
Researchersandpractitionersunderestimatethemagnitudeof
accuracy differences. Most (80.7%)responses estimate an accuracy
difference across default identical training runs to be less than 5%.
779Finding2indicatesthattheaccuracydifferenceisupto2.9%with
implementation-level NI-factors alone. However, only 10.4% of
respondentsexpect2%ormoreaccuracydifferenceacrossfixed-
seed identical training runs, and they estimate similarly for
per-class accuracy differences (Finding 7).
Estimateoftrainingtimedifference: Weaskparticipantstoes-
timatehowmuchthe runningtimeto convergencevaries across
default and fixed-seed identical training runs to see if their esti-mation matches the results from RQ3 (i.e., the convergence time
differencesareupto4,014.8%amongdefaultidenticaltrainingruns
and up to 145.3% among fixed-seed identical training runs).
Most (77.7%) participants estimate the convergence time differ-
ences to be less than 10% across default identical training runs,
and the majority of (84.5%)respondents estimate a similar 10%
or less convergence time difference among fixed-seed identical
training runs (Finding 8).
7 DL-TRAINING PAPER SURVEY
Weconductaliteraturesurveytostudytheawarenessofandthe
practice of handling DL variance in research papers.
Paper selection criteria and study approach: We extract re-
searcharticlesfromthemostrecenttopSE(ICSE’19,FSE’19,ASE’19),
machinelearning(NeurIPS/NIPS’19,ICLR’20,andICML’19),com-
puter vision (CVPR’19 and ICCV’19), and systems (SOSP’19, AS-
PLOS’19, MLSys’19) conferences. We focus on articles that were
acceptedfororalpresentations(i.e.,weexcludepostersandspot-
light articles), to keep the amount of manual examination realistic,
consideringthatover1,000papersareacceptedperyearforconfer-
ences such as NeurIPS/NIPS. In total, 1,152 articles meet the above
criterion. We split conferences into SE-systems-focused (SE and
systems)andAI-focused(machinelearningandcomputervision)
conferences to investigate whether AI papers are more likely to
consider this variance in their evaluation.
Two authors independently check each of the 454 randomly
sampled papers to see if it is relevant, i.e., papers that train DL
models(89.3%ofagreement).With95%confidence,28outof202
papersfromSE-systemsconferences(13.9 ±3%),versus197outof
252 papers from AI conferences (78.1±4%) are relevant.
Paper survey results: We present the survey result as follows.
Of the 225 relevant papers, only 19.5 ±3% use multiple identical
trainingrunstoevaluatetheirapproaches( Finding9 ):25.0±4%
for SE-systems conferences and 18.7±3% for AI conferences.
These results corroborate our online survey findings, indicat-
ingthatresearchersrarelyconsider(orhavenoclearsolutionsto
measure) the impact of NI-factors. In addition, 33 papers in our
sample use the same models we evaluated and report an accuracy
improvementlowerthanthevariancethatweobservedacrossmul-
tiple fixed-seed identical training runs (2.9%). Most (23) of these
studies do not report validation using multiple identical training
runs.Thus,theconclusionsofthese23studiesarelikelyaffected
by the variance in multiple identical training runs. This is a conser-
vativeestimateasweusetheimplementation-levelonlyvariance
(2.9%) instead of the overall variance (10.8%) as the criterion.8 IMPLICATIONS, SUGGESTIONS, AND
FUTURE WORK
Improving the stability of training implementations: Practi-
tioners may need to control NI-factors or replay DL training deter-
ministically to facilitate debugging, which are challenging tasks.
As discussed in Section 2.4, algorithmic NI-factors are generally
straightforwardtocontrolastheyareintroducedexplicitlyusing
pseudo-random number generators which can be seeded beforeeach run. Practitioners may benefit from new methods (e.g., de-
terministicGPU[ 55])tocontrolimplementation-levelNI-factors,
whicharemuchhardertocontrolbecausetheyareoftenthebyprod-
uct of optimization.
Research reproducibilityand validity: Variance introduced by
NI-factorsreducesthereproducibilityofDL-relatedexperiments.
Researchers should check if multiple identical training runs are
needed to ensure the validity of their experiments and comparison.
Itisnontrivialtodeterminethenumberofidenticalrunsneeded,
whichdependsontheapproachesandthebaselines.Onesolutionis
iteratively performing more replication runs when comparing to a
selected baseline. The replication process can stop when statistical
tests(e.g.,U-test)confirmthesignificance(e.g.,p-value<0.05)of
the difference between the new approach and the baseline. If after
a large number of replication runs (e.g., more than 30 runs[ 15])
theimprovementisnotstatisticallysignificant,thenthevariance
mightbelargeenoughsuchthatastatisticallysignificantconclusion
about the difference between the two techniques might not be
possible.Wearedevelopingsuchatechniquetohelpresearchers
and practitionerswith thisprocess, toreduce themanual effortto
conduct valid experiments and replicate experiments.
ApproachestoimprovereproducibilitysuggestedbytheSEcom-
munity[44,75,107]needtoalsoconsidertrainingvariance.New
approaches such as efficient checkpointing may be desirable.
Transparencyisimportantinmakingsurethatresearchisrepro-
ducible and valid. Recently, the DL research community promoted
sharing artifacts and results transparently[ 12,34]. Since DL sys-
temsarenondeterministic,itisimportanttosharethedatafromthe
replication runs as well. One solution is maintaining a centralized
trusted database that stores these replication runs and provides au-
thorsofnewapproacheswithbaselineresultsthattheycandirectly
compare to without rerunning the baseline approaches. We are
developing a tool that helps users to measure the variance of their
approachesandfacilitatesthecomparisonacrossapproaches.Userscanuploadtheirreplicationpackagesandresultstoadatabase,that
is provided by our tool, to be curated for comparison.
Producing better models: When a DL model is the contribution
(e.g., defect prediction [ 70] or program repair [ 69]), practitioners
could leverage variance to obtain a more accurate model.Less expensive training and variance estimation:
Since DL
training is expensive, an important research direction could be
less-expensivevarianceestimationandtrainingapproachessuch
as software support for incremental training [47].
9 THREATS TO VALIDITY
External validity: Observed results might be different for other
networks.Weuse6verypopularnetworkswithdiversecomplexity
780(from 7,206 to 36,536,884 parameters) to mitigate this issue. We
encourageothers(andourselves)toreplicateandenrichourstud-
ies with different DL networks, DL training approaches, and DL
librariestobuildanempiricalbodyofknowledgeaboutvariancein
DL systems. We are building a replication tool to help the research
communitytoshareandreplicateresearchexperimentsandresults.
New algorithmic NI-factors can be added (e.g., new nondeter-
ministiclayers) soourlistofalgorithmic NI-factorscouldbecome
incomplete in the future. For fixed-seed identical training runs,
we ensure that all algorithmic NI-factors of the DL networks we
evaluate are disabled.
Internal validity: Our implementation or the libraries we used
mighthavebugs.Thisshouldbealleviatedbythat(1)allourcodeis
reviewed by most authors, (2) our results show that variance exists
for allversions of alllibraries we evaluate,thus unlikely thatthey
arecausedbylibrarybugs,(3)wefocusonofficialreleasesofDL
libraries,soourrunsshouldstillberepresentativeofrealDLusage,
and (4) we analyze all results especially outliers to ensure there
were no implementation bugs.
Multiple identical training runs might produce different models
(i.e., models with different weights) with identical accuracy and
running time. Our study focuses on accuracy and time variance,
since these are the end results that users care about.
Construct validity: Weensuretotargetrelevantparticipantsin
oursurveybyspecificallyinvitingcodecontributorstoDLprojects
and asking them to confirm that they work with DL. Respondents
might not have wanted to show any perceived ignorance which
could have biased their responses. However, the strength of the
responses,83.8%beingunawareoruncertainaboutimplementation-
level nondeterminism in DL systems helps alleviate this issue.
10 RELATED WORK
Our paper is unique because we study and quantify DL variance
caused by NI-factors and conduct surveys to check its awareness.
Variancestudyofreinforcementlearning(RL): Closesttoour
workisthestudy[ 80]thatmeasurestheimpactofsomeimplemen-
tation-level NI-factors on RL experiments. We study the general
DL variance, while they focus on RL variance only. In addition, we
measure the awareness by conducting a survey and a literaturereview. Furthermore, while they focus on one network for one
task (i.e., playing the Atari game Breakout) and one version of one
corelibrary(PyTorch),westudytheimpactofNI-factorsusing6
networks trained on 3 datasets and multiple versions of three core
libraries. Papers [ 29,53] that investigates the impact of random
seedsonRLaredifferentfromours,sincetheyonlyconsiderthe
impact of random seeds (i.e., algorithmic NI-factors), while we also
study the variance caused by implementation-level NI-factors.
Awareness of the impact of nondeterminism: Arecentlitera-
ture survey [ 81] on 30 papers on the topic of text mining confirms
the results of our literature survey. None of the 30 investigated
papersreportusingdifferentrandomseeds.Ourliteraturesurvey
investigates DL training in general (not just text mining papers)
and examines 225 papers. Furthermore, our overall contribution is
differentsincewealsoquantifythedifferencesintrainingaccuracyandtimeacrossidenticaltrainingruns.Anotherpaper[ 79]states
thatsmallchangesintheexperimentalsetupcangeneratemeasure-mentbias.ItfocusesonstandardCPUcomputationbenchmark[
31]
and does not study the nonderterminism of DL systems.
AnecdotalevidenceofNI-factors: Somestudies[ 59,76,77,108]
quantifythevarianceintheirresultscausedbyNI-factors.However,
these are only anecdotal evidence and none attempts to system-atically study the variance introduced by algorithmic and imple-mentation-level NI-factors. In addition, our surveys show that
awareness is still very low in the research community.
Nondeterminisminstochasticgradientdescent(SGD): Much
workinvestigatesvariancecausedbySGD[ 26,33,35,58,68].While
these papers quantify nondeterminism caused by SGD, they ignore
all other sources of nondeterminism described in Section 2.Impact of weight initialization:
Prior work [ 57,88,101,104]
measure the impact of different initial weights on models’ training
time.Whilesuchanissueisknown,implementation-levelNI-factors
have not been studied and our surveys show that they are often
not considered when evaluating DL systems.Controllingimplementation-levelnondeterminism:
Jooybar
etal.[55]proposeahardwaremechanismtointroducedeterminism
in GPU-based algorithms. In our work, we do not focus on the
hardware itself and measure the variance caused by NI-factors
using popular GPUs without special hardware modifications.
DL System Benchmarking: Muchworkfocusesonbenchmark-
ing DL systems. However, their target is finding the best net-
works[30,86,121],hardware[ 92,121],hyper-parameters[ 72],and
framework[ 48,60,72,93,95,121].Suchapproachesdonotconsider
the impact of NI-factors on multiple identical training runs.
11 CONCLUSIONS
Thisworkstudiesthevarianceintroducedbynondeterminismin
DL systems and the awareness of this variance among researchers
and practitioners. We perform experiments on three datasets with
six popular networks and find up to 10.8% accuracy differences
among identical training runs when excluding weak models. Even
with fixed seeds, the accuracy differences are as large as 2.9%. Our
surveys show that 83.8% of surveyed researchers and practitioners
are unaware of or unsure about implementation-level variance
and only 19.5 ±3% of papers in recent relevant top conferences use
multiple identical training runs to quantify the variance of their
DLapproaches.Thus,weraisetheawarenessofDLvariance,for
better research validity and reproducibility, more accurate models,
deterministicdebugging,newresearchontrainingstability,efficient
training, and fast variance estimation.
ACKNOWLEDGMENTS
Theauthors thankEricHorvitz,Ben Zorn,andMadan Musuvathi
for theirinsightful commentsand discussions.They thankYitong
Li for examining some research papers for the literature survey.
TheresearchispartiallysupportedbyNSF2006688andaFacebook
research award. Any opinions, findings, and conclusions in this
paperarethoseoftheauthorsonlyanddonotnecessarilyreflect
the views of the sponsors.
781REFERENCES
[1]2019.ASE ’19: Proceedings of the 34th ACM/IEEE International Conference on
Automated Software Engineering.
[2]2019.ASPLOS ’19: Proceedings of the Twenty-Fourth International Conference on
Architectural Support for Programming Languages and Operating Systems.
[3]2019.CVPR’19: Proceedings of The IEEE Conference on Computer Vision and
Pattern Recognition.
[4]2019.ESEC/FSE ’19: Proceedings of the 27th ACM Joint Meeting on European
Software Engineering Conference and Symposium on the Foundations of Software
Engineering.
[5]2019.ICCV’19: Proceedings of The IEEE International Conference on Computer
Vision. IEEE Press.
[6]2019.ICML’19:ProceedingsofTheInternationalConferenceonMachineLearning.
[7]2019.ICSE ’19: Proceedings of the 41st International Conference on Software
Engineering. IEEE Press.
[8] 2019. MLSys’19: Proceedings of Machine Learning and Systems.
[9]2019.NIPS’19:Proceedingsofthe33rdConferenceonNeuralInformationProcessing
Systems.
[10]2019.SOSP’19:Proceedingsofthe27thACMSymposiumonOperatingSystems
Principles.
[11]2020.ICLR’20: Proceedings of The International Conference on Learning Represen-
tations.
[12]2020.Reproducibility Challenge @ NeurIPS 2019. https://reproducibility-
challenge.github.io/neurips2019/
[13] MartínAbadi,PaulBarham,JianminChen,ZhifengChen,AndyDavis,Jeffrey
Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al .
2016. TensorFlow: a System for Large-Scale Machine Learning.. In OSDI.
[14] Charu C Aggarwal. 2018. Neural networks and deep learning. In Springer.
[15]AndreaArcuriandLionelBriand.2011. Apracticalguideforusingstatistical
tests to assess randomized algorithms in software engineering. In ICSE.
[16]James Bergstra, Frédéric Bastien, Olivier Breuleux, Pascal Lamblin, Razvan
Pascanu, Olivier Delalleau, Guillaume Desjardins, David Warde-Farley, Ian
Goodfellow,ArnaudBergeron,etal .2011. Theano:DeeplearningonGPUswith
Python. In NIPS.
[17]James Bergstra and Yoshua Bengio. 2012. Randomsearch for hyper-parameter
optimization. In JMLR.
[18]Nghi D. Q. Bui, Yijun Yu, and Lingxiao Jiang. 2019. AutoFocus: Interpreting
Attention-Based Neural Networks by Code Perturbation. In ASE.
[19]ChenyiChen,AriSeff,AlainKornhauser,andJianxiongXiao.2015. Deepdriving:
Learning Affordance for Direct Perception in Autonomous Driving. In ICCV.
[20]Jieshan Chen, Chunyang Chen, Zhenchang Xing, Xiwei Xu, Liming Zhu, Guo-
qiang Li, and Jinshui Wang. 2020. Unblind Your Apps: Predicting Natural-
Language Labels for Mobile GUI Components by Deep Learning. In ICSE.
[21]Jinyin Chen, Keke Hu, Yue Yu, Zhuangzhi Chen, Qi Xuan, Yi Liu, and Vladimir
Filkov.2020. SoftwareVisualization andDeepTransferLearningforEffective
Software Defect Prediction. In ICSE.
[22]Liang-ChiehChen,MaxwellCollins,YukunZhu,GeorgePapandreou,Barret
Zoph, Florian Schroff, Hartwig Adam, and Jon Shlens. 2018. Searching for
efficient multi-scale architectures for dense image prediction. In NIPS.
[23]SharanChetlur,CliffWoolley,PhilippeVandermersch,JonathanCohen,John
Tran, Bryan Catanzaro, and Evan Shelhamer. 2014. cuDNN: Efficient Primitives
for Deep Learning.
[24]François Chollet. 2017. Xception: Deep learning with depthwise separable
convolutions. In CVPR.
[25] François Chollet et al. 2015. Keras. https://keras.io.[26]
AnnaChoromanska,MikaelHenaff,MichaelMathieu,GérardBenArous,and
Yann LeCun. 2015. The loss surfaces of multilayer networks. In AISTATS.
[27]Anna Choromanska, Yann LeCun, and Gérard Ben Arous. 2015. Open Problem:
The landscape of the loss surfaces of multilayer networks. In COLR.
[28] J. Cohen. 1988. Statistical Power Analysis for the Behavioral Sciences.
[29]CédricColas,OlivierSigaud,andPierre-YvesOudeyer.2018.HowManyRandomSeeds?StatisticalPowerAnalysisinDeepReinforcementLearningExperiments.
InCoRR.
[30]Cody Coleman, Deepak Narayanan, Daniel Kang, Tian Zhao, Jian Zhang, Luigi
Nardi, Peter Bailis, Kunle Olukotun, Chris Ré, and Matei Zaharia. 2017. Dawn-
bench: An end-to-end deep learning benchmark and competition. In Training.
[31]Standard Performance Evaluation Corporation. 2006. SPEC CPU2006 Bench-
marks. http://www.spec.org/cpu2006/.
[32]Heming Cui, Jiri Simsa, Yi-Hong Lin, Hao Li, Ben Blum, Xinan Xu, Junfeng
Yang, Garth A. Gibson, and Randal E. Bryant. 2013. Parrot: a Practical Runtime
for Deterministic, Stable, and Reliable Threads. In SOSP.
[33]Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya
Ganguli,andYoshuaBengio.2014. Identifyingandattackingthesaddlepoint
problem in high-dimensional non-convex optimization. In NIPS.
[34]Jesse Dodge, Suchin Gururangan, Dallas Card, Roy Schwartz, and Noah A.
Smith. 2019. Show Your Work: Improved Reporting of Experimental Results. In
EMNLP-IJCNLP.[35]FelixDraxler,KambisVeschgini,ManfredSalmhofer,andFredHamprecht.2018.
Essentially No Barriers in Neural Network Energy Landscape. In ICML.
[36]Simon S Du, Jason D Lee, Yuandong Tian, Barnabas Poczos, and Aarti Singh.
2017. Gradientdescentlearnsone-hidden-layercnn:Don’tbeafraidofspurious
local minima. In arXiv preprint arXiv:1712.00779.
[37] Brian Everitt. 2002. The Cambridge dictionary of statistics.
[38]Hao-Shu Fang, Guansong Lu, Xiaolin Fang, Jianwen Xie, Yu-Wing Tai, andCewu Lu. 2018. Weakly and Semi Supervised Human Body Part Parsing via
Pose-Guided Knowledge Transfer. In CVPR.
[39]XiangGao,RiponSaha,MukulPrasad,andAbhikRoychoudhury.2020. Fuzz
TestingbasedDataAugmentationtoImproveRobustnessofDeepNeuralNet-
works. In ICSE.
[40]RongGe,FurongHuang,ChiJin,andYangYuan.2015. Escapingfromsaddle
points—online stochastic gradient for tensor decomposition. In COLT.
[41]Simos Gerasimou, Hasan Ferit-Eniser, Alper Sen, and Alper Çakan. 2020.
Importance-Driven Deep Learning System Testing. In ICSE.
[42]XavierGlorotandYoshuaBengio.2010. Understandingthedifficultyoftraining
deep feedforward neural networks. In AISTATS.
[43]David Goldberg. 1991. What Every Computer Scientist Should Know about
Floating-Point Arithmetic. ACM Comput. Surv. (1991).
[44]Jesús M González-Barahona and Gregorio Robles. 2012. On the reproducibil-ity of empirical software engineering studies based on data retrieved from
development repositories. In ESE.
[45]Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. Deep learning.M I T
press.
[46]DivyaGopinath,HayesConverse,CorinaS.Păsăreanu,andAnkurTaly.2019.
Property Inference for Deep Neural Networks. In ASE.
[47]HuiGuan,XipengShen,andSeung-HwanLim.2019. Wootz:ACompiler-Based
Framework for Fast CNN Pruning via Composability. In PLDI.
[48]Qianyu Guo, Sen Chen, Xiaofei Xie, Lei Ma, Qiang Hu, Hongtao Liu, Yang Liu,
JianjunZhao,andXiaohongLi.2019. Anempiricalstudytowardscharacterizing
deeplearningdevelopmentanddeploymentacrossdifferentframeworksand
platforms. In ASE.
[49]Jeff Haochen and Suvrit Sra. 2019. Random Shuffling Beats SGD after Finite
Epochs. In ICML.
[50]Kaiming He,Xiangyu Zhang, Shaoqing Ren,and Jian Sun.2015. Delving deep
into rectifiers: Surpassing human-level performance on imagenet classification.
InICCV.
[51]Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual
learning for image recognition. In CVPR.
[52]Vincent J. Hellendoorn, Christian Bird, Earl T. Barr, and Miltiadis Allamanis.
2018. Deep Learning Type Inference. In ESEC/FSE.
[53]PeterHenderson, RiashatIslam,Philip Bachman,JoellePineau,Doina Precup,
and David Meger. 2018. Deep Reinforcement Learning that Matters. In AAAI.
[54]Ilija Ilievski, Taimoor Akhtar, Jiashi Feng, and Christine Annette Shoemaker.
2017. Efficienthyperparameteroptimizationfordeeplearningalgorithmsusing
deterministic rbf surrogates. In AAAI.
[55]Hadi Jooybar, Wilson WL Fung, Mike O’Connor, Joseph Devietti, and Tor M
Aamodt. 2013. GPUDet: a deterministic GPU architecture. In ASPLOS.
[56]JinhanKim,RobertFeldt,andShinYoo.2019. GuidingDeepLearningSystem
Testing Using Surprise Adequacy. In ICSE.
[57]YK Kim and JB Ra. 1991. Weight value initialization for improving training
speedinthe backpropagation network. In IJCNN.
[58]BobbyKleinberg,YuanzhiLi,andYangYuan.2018. AnAlternativeView:When
Does SGD Escape Local Minima?. In ICML.
[59]Yuriy Kochura, Sergii Stirenko, Oleg Alienin, Michail Novotarskiy, and YuriGordienko. 2018. Performance Analysis of Open Source Machine LearningFrameworks for Various Parameters in Single-Threaded and Multi-threaded
Modes. In CSIT.
[60]VassiliKovalev,AlexanderKalinovsky,andSergeyKovalev.2016. Deeplearning
with theano, torch, caffe, tensorflow, and deeplearning4j: Which one is the best
inspeedand accuracy?
[61]Alex Krizhevsky. 2009. Learning multiple layers of features from tiny images.
[62]AlexKrizhevsky,IlyaSutskever,andGeoffreyEHinton.2012. Imagenetclassifi-
cation with deep convolutional neural networks. In NIPS.
[63]Jeremy Lacomis, Pengcheng Yin, Edward J. Schwartz, Miltiadis Allamanis,
Claire Le Goues, Graham Neubig, and Bogdan Vasilescu. 2019. DIRE: A Neural
Approach to Decompiled Identifier Naming. In ASE.
[64]Alex Lamb, Jonathan Binas, Anirudh Goyal, Sandeep Subramanian, Ioannis
Mitliagkas, Denis Kazakov, Yoshua Bengio, and Michael C. Mozer. 2019. State-
Reification Networks: Improving Generalization by Modeling the Distribution
of Hidden Representations. In ICML.
[65]YannLeCun,LéonBottou,YoshuaBengio,PatrickHaffner,etal .1998. Gradient-
based learning applied to document recognition. In Proceedings of the IEEE.
[66]YannALeCun,LéonBottou,GenevieveBOrr,andKlaus-RobertMüller.2012.
Efficient backprop. In Neural networks: Tricks of the trade.
[67]Jason D Lee, Max Simchowitz, Michael I Jordan, and Benjamin Recht. 2016.
Gradient descent converges to minimizers. In arXiv preprint arXiv:1602.04915.
782[68]Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. 2018.
Visualizing the Loss Landscape of Neural Nets. In NIPS.
[69]YiLi,WangShaohua,andTienN.Nguyen.2020. DLFix:Context-basedCode
Transformation Learning for Automated Program Repair. In ICSE.
[70]YiLi,ShaohuaWang,TienNNguyen,andSonVanNguyen.2019.Improvingbug
detectionviacontext-basedcoderepresentationlearningandattention-based
neural networks. In PACMPL.
[71]Zenan Li, Xiaoxing Ma, Chang Xu, Chun Cao, Jingwei Xu, and Jian Lü.
2019. Boosting Operational DNN Testing Efficiency through Conditioning.
InESEC/FSE.
[72]Ling Liu, Yanzhao Wu, Wenqi Wei, Wenqi Cao, Semih Sahin, and Qi Zhang.
2018. Benchmarking deep learning frameworks: Design considerations, metrics
and beyond. In ICDCS.
[73]LeiMa,FelixJuefei-Xu,FuyuanZhang,JiyuanSun,MinhuiXue,BoLi,Chun-
yang Chen, Ting Su, Li Li, Yang Liu, Jianjun Zhao, and Yadong Wang. 2018.
DeepGauge: Multi-Granularity Testing Criteria for Deep Learning Systems. In
ASE.
[74]Shiqing Ma,Yingqi Liu,Wen-ChuanLee, XiangyuZhang, and AnanthGrama.
2018. MODE: Automated Neural Network Model Debugging via State Differen-
tial Analysis and Input Selection. In ESEC/FSE.
[75]Zaheed Mahmood, David Bowes, Tracy Hall, Peter CR Lane, and Jean Petrić.
2018. Reproducibilityandreplicabilityofsoftwaredefectpredictionstudies. ISE
(2018).
[76]Andrii Maksai and Pascal Fua. 2019. Eliminating exposure bias and metric
mismatch in multiple object tracking. In CVPR.
[77]Luke Metz, Niru Maheswaranathan, Jeremy Nixon, C Daniel Freeman, and
JaschaSohl-Dickstein. 2019. Understanding andcorrecting pathologiesinthe
training of learned optimizers. In ICML.
[78]HrushikeshN.Mhaskar,SergeiV.Pereverzyev,andMariaD.vanderWalt.2017.
ADeepLearningApproachtoDiabeticBloodGlucosePrediction.In Front.Appl.
Math. Stat.
[79]ToddMytkowicz,AmerDiwan, MatthiasHauswirth,andPeterSweeney.2009.
Producing wrong data without doing anything obviously wrong!. In ACM
SIGARCH Computer Architecture News.
[80]Prabhat Nagarajan, Garrett Warnell, and Peter Stone. 2019. Deterministic
Implementations for Reproducibility in Deep Reinforcement Learning. In AAAI
Workshop on Reproducible AI.
[81]BabatundeKOlorisade,PearlBrereton,andPeterAndras.2017. ReproducibilityinmachineLearning-Basedstudies:Anexampleoftextmining.In Reproducibil-
ity in Machine Learning Workshop, ICML.
[82]AdamPaszke,SamGross,FranciscoMassa,AdamLerer,JamesBradbury,Gre-
gory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga,
Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison,
Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and
SoumithChintala.2019. PyTorch:AnImperativeStyle,High-PerformanceDeep
Learning Library. In NIPS.
[83]Brandon Paulsen, Jingbo Wang, and Chao Wang. 2020. ReluDiff: Differential
Verification of Deep Neural Networks. In ICSE.
[84]Kexin Pei, Yinzhi Cao, Junfeng Yang, and Suman Jana. 2019. DeepXplore:
Automated Whitebox Testing of Deep Learning Systems. In Commun. ACM.
[85]Hung Viet Pham, Thibaud Lutellier, Weizhen Qi, and Lin Tan. 2019. CRA-
DLE: Cross-Backend Validation to Detect and Localize Bugs in Deep Learning
Libraries. In ICSE.
[86]Sanjay Purushotham, Chuizheng Meng, Zhengping Che, and Yan Liu. 2018.
Benchmarking deep learning models on large healthcare datasets. In J. Biomed.
Inform.
[87]SubhajitRoy,AwanishPandey,BrendanDolan-Gavitt,andYuHu.2018. Bug
Synthesis: Challenging Bug-Finding Tools with Deep Faults. In ESEC/FSE.
[88]Tim Salimans and Durk P Kingma. 2016. Weight normalization: A simple
reparameterization to accelerate training of deep neural networks. In NIPS.
[89]Shlomo Sawilowsky, Jack Sawilowsky, and Robert J. Grissom. 2011. Effect Size.
[90]AndrewM.Saxe,JamesL.Mcclelland,andSuryaGanguli.2014. Exactsolutions
to the nonlineardynamics of learning in deeplinear neural network. In ICLR.
[91]FrankSeideandAmitAgarwal.2016. CNTK:Microsoft’sOpen-SourceDeep-
Learning Toolkit. In KDD.
[92]ShayanShams,RichardPlatania,KisungLee,andSeung-JongPark.2017. Evalu-
ation of deep learning frameworks over different HPC architectures. In ICDCS.
[93]AliShatnawi,GhadeerAl-Bdour,RaffiAl-Qurran,andMahmoudAl-Ayyoub.
2018. A comparative study of open source deep learning frameworks. In ICICS.
[94]Zhiqiang Shen, Zhuang Liu, Jianguo Li, Yu-Gang Jiang, Yurong Chen, and
Xiangyang Xue. 2017. Dsod: Learning deeply supervised object detectors from
scratch. In ICCV.
[95]ShaohuaiShi,QiangWang,PengfeiXu,andXiaowenChu.2016. Benchmarking
state-of-the-art deep learning software tools. In CCBD.[96]Connor Shorten and Taghi M. Khoshgoftaar. 2019. A survey on Image Data
Augmentation for Deep Learning. In Journal of Big Data.
[97]KarenSimonyanandAndrewZisserman.2014. Verydeepconvolutionalnet-
works for large-scale image recognition. In arXiv preprint arXiv:1409.1556.
[98]XinhangSong,LuisHerranz,andShuqiangJiang.2017. Depthcnnsforrgb-d
scenerecognition:Learningfromscratchbetterthantransferringfromrgb-cnns.
InAAAI.
[99]Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
Salakhutdinov.2014. Dropout:asimplewaytopreventneuralnetworksfrom
overfitting. In JMLR.
[100]Youcheng Sun, Min Wu, Wenjie Ruan, Xiaowei Huang, Marta Kwiatkowska,and Daniel Kroening. 2018. Concolic Testing for Deep Neural Networks. In
ASE.
[101]IlyaSutskever,JamesMartens,GeorgeDahl,andGeoffreyHinton.2013. Onthe
importance of initialization and momentum in deep learning. In ICML.
[102]Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew
Wojna. 2016. Rethinking the inception architecture for computer vision. In
CVPR.
[103]NimaTajbakhsh,JaeYShin,SuryakanthRGurudu,RToddHurst,ChristopherB
Kendall, Michael B Gotway, andJianming Liang. 2016. Convolutionalneural
networksformedicalimageanalysis:Fulltrainingorfinetuning?.In IEEETrans.
Med. Imag.
[104]Kok Keong Teo, Lipo Wang, and Zhiping Lin. 2001. Wavelet packet multi-layer
perceptron for chaotic time series prediction: effects of weight initialization. In
ICCS.
[105]YuchiTian,KexinPei,SumanJana,andBaishakhiRay.2018. DeepTest:Auto-
mated Testing of Deep-Neural-Network-Driven Autonomous Cars. In ICSE.
[106]Yuchi Tian, Ziyuan Zhong, Vicente Ordonez, Gail Kaiser, and Baishakhi Ray.
2020. Testing DNN Image Classifier for Confusion & Bias Errors. In ICSE.
[107]FabianTrautsch,SteffenHerbold,PhilipMakedonski,andJensGrabowski.2018.
Addressingproblemswithreplicabilityandvalidityofrepositoryminingstudies
through a smart data platform. In ESE.
[108]Nam Vo, Lu Jiang, Chen Sun, Kevin Murphy, Li-Jia Li, Li Fei-Fei, and James
Hays.2019. Composingtextandimageforimageretrieval-anempiricalodyssey.
InCVPR.
[109]HuiyanWang,JingweiuXu,ChangXu,XiaoxingMa,andJianLu.2020. DIS-
SECTOR:InputValidationforDeepLearningApplicationsbyCrossing-layer
Dissection. In ICSE.
[110]JingyiWang,GuoliangDong,JunSun,XinyuWang,andPeixinZhang.2019. Ad-
versarial Sample Detection for Deep Neural Network through Model Mutation
Testing. In ICSE.
[111]StevenRYoung,DerekCRose,ThomasPKarnowski,Seung-HwanLim,and
Robert M Patton.2015. Optimizing deep learninghyper-parameters through an
evolutionary algorithm. In MLHPC.
[112]Sergey Zagoruyko and Nikos Komodakis. 2016. Wide Residual Networks. In
BMVC.
[113]Hao Zhang and W. K. Chan. 2019. Apricot: A Weight-Adaptation Approach to
Fixing Deep Learning Models. In ASE.
[114]Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, and Xudong Liu. 2020.
Retrieval-based Neural Source Code Summarization. In ICSE.
[115]MengshiZhang,YuqunZhang,LingmingZhang,CongLiu,andSarfrazKhur-
shid. 2018. DeepRoad: GAN-Based MetamorphicTestingand Input Validation
Framework for Autonomous Driving Systems. In ASE.
[116]XiyueZhang,XiaofeiXie,LeiMa,XiaoningDu,QiangHu,YangLiu,Jianjun
Zhao,andMengSun.2020. TowardsCharacterizingAdversarialDefectsofDeep
Learning Software from the Lens of Uncertainty. In ICSE.
[117]Gang Zhao and Jeff Huang. 2018. DeepSim: Deep Learning Code Functional
Similarity. In ESEC/FSE.
[118]Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia.
2017. Pyramid scene parsing network. In CVPR.
[119]YanZheng,XiaofeiXie,TingSu,LeiMa,JianyeHao,ZhaopengMeng,YangLiu,
Ruimin Shen, Yingfeng Chen, and Changjie Fan. 2019. Wuji: Automatic Online
Combat Game Testing Using Evolutionary Deep Reinforcement Learning. In
ASE.
[120]Husheng Zhou, Wei Li, Zelun Kong, Junfeng Guo, Yuqun Zhang, Lingming
Zhang, Bei Yu, and Cong Liu. 2020. DeepBillboard: Systematic Physical-World
Testing of Autonomous Driving Systems. In ICSE.
[121]H. Zhu, M. Akrout, B. Zheng, A. Pelegris, A. Jayarajan, A. Phanishayee, B.Schroeder, and G. Pekhimenko. 2018. Benchmarking and Analyzing Deep
Neural Network Training. In IISWC.
[122]RuiZhu,ShifengZhang,XiaoboWang,LongyinWen,HailinShi,LiefengBo,
and Tao Mei. 2019. ScratchDet: Training single-shot object detectors from
scratch. In CVPR.
783