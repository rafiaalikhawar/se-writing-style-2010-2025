ComboDroid: Generating High-Quality Test Inputs for Android
Apps via Use Case Combinations
Jue Wang
State Key Lab for Novel Software
Technology and Department of
Computer Science and Technology,
Nanjing University, Nanjing, China
juewang591@gmail.comYanyan Jiang
State Key Lab for Novel Software
Technology and Department of
Computer Science and Technology,
Nanjing University, Nanjing, China
jyy@nju.edu.cnChang Xu
State Key Lab for Novel Software
Technology and Department of
Computer Science and Technology,
Nanjing University, Nanjing, China
changxu@nju.edu.cn
Chun Cao
State Key Lab for Novel Software
Technology and Department of
Computer Science and Technology,
Nanjing University, Nanjing, China
caochun@nju.edu.cnXiaoxing Ma
State Key Lab for Novel Software
Technology and Department of
Computer Science and Technology,
Nanjing University, Nanjing, China
xxm@nju.edu.cnJian Lu
State Key Lab for Novel Software
Technology and Department of
Computer Science and Technology,
Nanjing University, Nanjing, China
lj@nju.edu.cn
ABSTRACT
Androidappsdemandhigh-qualitytestinputs,whosegenerationre-
mainsanopenchallenge.Existingtechniquesfallshortonexploring
complex app functionalitiesreachable only by along, meaningful,
andeffectivetestinput.Observingthatsuchtestinputscanusually
be decomposed into relatively independent short use cases, this pa-
perpresentsComboDroid,afundamentallydifferentAndroidapp
testing framework. ComboDroid obtains use cases for manifesting
a specific app functionality (either manually provided or automati-
callyextracted),andsystematicallyenumeratesthecombinations
of use cases, yielding high-quality test inputs.
The evaluation results of ComboDroid on real-world apps are
encouraging. Our fully automatic variant outperformed the bestexisting technique APE by covering 4.6% more code (APE only
outperformedMonkeyby2.1%),andrevealedfourpreviouslyun-
known bugs in extensively tested subjects. Our semi-automatic
variant boosts the manual use cases obtained with little manual
labor, achieving a comparable coverage (only 3.2% less) with a
white-box human testing expert.
KEYWORDS
Software testing, mobile apps
ACM Reference Format:
JueWang,YanyanJiang,ChangXu,ChunCao,XiaoxingMa,andJianLu.
2020. ComboDroid: Generating High-Quality Test Inputs for Android Apps
via Use Case Combinations. In 42nd International Conference on Software
Engineering(ICSE’20),May23–29,2020,Seoul,RepublicofKorea. ACM,New
York, NY, USA, 12pages.https://doi.org/10.1145/3377811.3380382
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ICSE ’20, May 23–29, 2020, Seoul, Republic of Korea
© 2020 Association for Computing Machinery.
ACM ISBN 978-1-4503-7121-6/20/05...$15.00
https://doi.org/10.1145/3377811.33803821 INTRODUCTION
Android apps are oftentimes inadequately tested due to the lack
ofhigh-quality test inputs1to thoroughly exercise an app’s func-
tionalities and manifest potential bugs [ 11]. Existing automatic
testing techniques fall short on exploring complex app function-
alities that are only reachable by long and “meaningful” event
sequences [ 33,59]. Random or heuristic test input generation tech-
niques [5,6,10,28,41–43,56] can quickly cover superficial app
functionalities, but have difficulty in reaching deeper app statesto cover complex ones. Systematic input space exploration tech-niques [
7,47,48,61,65] have severe scalability issues. Manual
testing iseffective and thorough,but also tedious,labor-intensive,
and time-consuming, and usually hinders the rapid release of an
app.
To generate high-quality test inputs to thoroughly explore an
app’sfunctionalities,weobservethatalongandmeaningfultestinput can usually be decomposed into relatively independent use
cases. A use case is a short event sequence for manifesting a desig-
natedapp’sfunctionality,e.g., 1togglingasetting, 2switching
to an activity, or 3downloading a Web content. 1→2→3
isalong(andmeaningful)testinput,andisparticularlyusefulin
manifestingdiverseappbehaviorswhentheapp’sbehaviorin 3
varies on different settings in 1.
Conversely, we can solve the problem of generating long and
meaningful test inputs by a fundamentally different two-phase
approach, which we call it the ComboDroid framework:
(1)Collecthigh-qualityusecases thatcoverasmanybasicapp
functionalities as possible.
(2)Concatenate a number of use cases to form a test input for
covering complex functionalities.
Use cases can be either manually provided (e.g., by an app’s
developer) or automatically extracted from execution traces. Since
developersclearlyknowhowtherequirementsareimplemented,
they can easily provide high-quality use cases with little manuallabor. To extract use cases automatically from execution traces,
1In the context of testing Android apps, a test input is a sequence of the Android
system’s atomic input events (touching, swiping, etc.).
4692020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE)
we leverage the insight that use cases, by their definitions, almost
beginandendat quiescent appstates,usuallywithastableGUI.We
accordingly designedan algorithmto automaticallyidentify such
GUI states and extract use cases from long event sequences.
To efficiently generate high-quality use case combinations (or
combosforshort)astestinputs,wedeviseanalgorithmtotriage
combosforamaximizedtestingdiversity.Particularly,wedefinethe
aligns-with relation, which determines whether two use cases con-
nected at the same quiescent state, to prune likely invalid combos.
We also define the depends-on relation, which determines whether
a use case can affect the behavior of another. We generate only
alignedcomboswithsufficientdata-flowdiversitiesforaneffective
test input generation.
We implemented these ideas as the ComboDroid tool, including
the fully automatic ComboDroidαand semi-automatic Combo-
Droidβ.TheevaluationresultsareencouragingthatComboDroid
is effective in both testing scenarios:
(1)The fully automatic ComboDroidαcovered 4.6% and 6.7%
more code on average compared with the most effective
existingtechniqueAPE[ 28]andmostwidelyusedtechnique
Monkey [ 26], respectively. ComboDroidαalso revealed four
previouslyunknownbugsinextensivelytestedsubjects[ 51–
54].
(2)The semi-automatic ComboDroidβboosted the coverage of
manually provided use cases by 13.2%, achieving a competi-
tive code coverage (the gap is only 3.2%) compared with a
human testing expert, but with much less manual labor.
The rest of this paper is organized as follows. Section 2presents
anoverviewofourapproachwithanillustrativeexample.Details
of our approach are discussed in Section 3. Section 4introduces
theComboDroidimplementationandourextensiveevaluationis
conductedinSection 5.Section 6surveysrelatedwork,andSection 7
concludes this paper.
2 OVERVIEW
Figure1displaystheComboDroidworkflow.ComboDroidtakesan
app under test Pand repeats the two-phase testing procedure con-
sistingofobtainingusecases(theleftbox)andenumeratingusecase
combos(therightbox).WeexplaintheworkflowofComboDroid
using a motivating example, a previously unknown bug2found by
ComboDroidαin Aard2 (a popular dictionary app). This bug re-
quiresalong(andmeaningful)testinput 1→3→2→4→5
to trigger.
Obtainingusecases . Wefirstobservethatameaningfulusecase
(event sequence) usually begins and ends at quiescent app states,
in which the app is idle (completes handling of all received events)
onastableGUI.Quiescentstatesnaturallyindicatethatahuman
can perform the next step of an action in the computer-human
interaction.InAard2,usefulusecasesincludeadding/deletinga
dictionary, searching for a word, view a word’s detail explanations,
etc.
Usecasescanbeprovidedbyahumandeveloper(notedCDβ).
ComboDroid contains an auxiliary tool to help developers col-
lectusescasesbyrecordingeventsequences(bothUIandsystem
2Aard2hasbeenextensivelyevaluatedintheexistingstudies[ 43,56,57].However,
ComboDroidαis the first to uncover this bug.events[46])ataspecifiedtimeinterval.ComboDroidautomatically
identifies quiescent states, and collects execution traces and GUI
snapshots along with the use cases. In Aard2, the app’s developer
would have no difficulty in providing meaningful use cases like 1,
2,...,5, and⋆.
Usecasescanalsobeextractedbyanautomaticanalysisofan
app’sexistingexecutiontraces(notedCDα).ComboDroidminesan
extended labeledtransition system (ELTS) [31] atruntime basedon
the GUI transitions using an existing algorithm [ 10]. Similar stable
GUIsareclusteredasasinglestateintheELTS.Eachinputevent
between a pair of stable GUIs in the execution traces is added as a
transition (labeled with that event) in the ELTS.
To bootstrap CDα(as there is no trace at first), we implemented
a baseline DFS-alike state space exploration tool [ 5] to generate
initial testing traces. Unique acyclic transitional paths on the ELTS
areextractedaslikelyusecases.InAard2,automaticallygenerated
use cases are not as readable as manual ones, but share similar
features (e.g., starting from and ending at quiescent app states).
Nevertheless, ComboDroidαsuccessfully identified different pages
(e.g.,thedictionary,search,anddetailpage)asdistinctstatesinthe
ELTS,andthegeneratedusecasescoverallfunctionalitiesin 1,2,
...,5, and⋆.
Enumerating use case combos . Either way, ComboDroid enu-
merates the combinations (combos) of use cases to obtain high
quality test inputs. A combo is a sequence of use cases
u1→u2→...→un
whereu1startsfromtheapp’sinitialstate.Tomakecomboseffective
in testing, a combo should additionally satisfy:
(1)Deliverability : for all 1 ≤i<n,uialigns with ui+1. Foruto
be aligned with v, the last GUI layout in ushould be similar
to the first one of v(such that it is sane to deliver vto the
appimmediatelyafter u).Similarityischaracterizedbyan
editing-distance based measurement.
(2)Dataflow diversity : there exists at least kdistinct pairs of
(ui,uj)whereuidepends on ujandi>j. Foruto be de-
pendenton v,thereshouldbesomesharedprogramstates
used inuand modified in v. Thereby we filter out loosely
connected use case combos.
ThesystematicenumerationinComboDroidfirstsearchesfor
data-dependent pairs for a maximized data flow diversity, and then
addsrandomtransitionaluse casestosatisfythedeliverability.In
Aard2, 1→2and4→5aredata-dependent3.Then,Combo-
Droidgenerates 1→2→4→5asaskeleton,whichisfilled
with transitional use cases ( 3and⋆s) to yield the bug-triggering
combo in Figure 1(a combo of n=8,k=2).
The feedback loop . Generatedcombosaredeliveredtotheapp
withexecutiontracesbeingcollected.Afterthedelivery,Combo-
Droid terminates if there is no newly explored quiescent app state
otherthanthoseidentifiedduringtheusecasegeneration.Other-
wise,ComboDroidrestartsthefirstphasetoeitheraskahumanfor
additional effective use cases concerning these states (e.g., visiting
themduringtheexecution),orextractmorepotentiallyprofitable
3Use case delete a dictionary (2) overwrites the dictionary object referred in add
a dictionary (1), and thus 2depends on 1. For a similar reason on the shared
WebViewobject,5depends on 4.
470濄濅濇濈
澽
濄濅濆
澽濇
澽濈
濔濿濼濺瀁瀆激瀊濼瀇濻澳瀅濸濿濴瀇濼瀂瀁瀆濄濅 濆
濇濈
濗濸瀃濸瀁濷瀆激瀂瀁澳瀅濸濿濴瀇濼瀂瀁瀆
濄濆濅濇濈 澽澽澽濄濔濷濷澳濴澳濷濼濶瀇濼瀂瀁濴瀅瀌
濅濗濸濿濸瀇濸澳濴澳濷濼濶瀇濼瀂瀁濴瀅瀌
濆濦濸濴瀅濶濻澳濹瀂瀅澳濴澳瀊瀂瀅濷澳
濇濥濸瀉濼濸瀊澳濴澳濷濸瀇濴濼濿
濈濭瀂瀂瀀澳濼瀁
澽濣濴濺濸澳瀇瀅濴瀁瀆濼瀇濼瀂瀁瀆
濨瀆濸澳濶濴瀆濸瀆 濥濸濿濴瀇濼瀂瀁瀆 濖瀂瀀濵瀂瀆濗濙濦澳
濸瀋瀃濿瀂瀅濴瀇濼瀂瀁濘濟濧濦濛瀈瀀濴瀁濢濵瀇濴濼瀁濼瀁濺澳瀈瀆濸澳濶濴瀆濸瀆澳澻濦濸濶瀇濼瀂瀁瀆澳濆濁濄澳濴瀁濷澳濆濁濅澼 濘瀁瀈瀀濸瀅濴瀇濼瀁濺澳瀈瀆濸澳濶濴瀆濸澳濶瀂瀀濵瀂瀆澳澻濦濸濶瀇濼瀂瀁澳濆濁濆澼
濦濻瀂瀈濿濷澳瀇濸瀅瀀濼瀁濴瀇濸濒濡瀂 濬濸瀆
濖濗ߙ濖濗ߚ濠濴瀁瀈濴濿濿瀌
濸瀋瀇瀅濴濶瀇濒
濡瀂濬濸瀆
濧濻濸澳濹濸濸濷濵濴濶濾澳濿瀂瀂瀃澳澻濦濸濶瀇濼瀂瀁澳濆濁濇澼
Figure 1: ComboDroid overview and a motivating bug example
usecasesfromtheELTSrefinedbythenewlycollectedexecution
traces.
Manifestationofthebug . ThecomboinFigure 1crashestheapp.
Afterdeletingadictionary,allofitsdetailwordexplanationsare
removed.However,the“detail”pageofapreviouslysearchedword
isstillcachedintheapp.Returningtosuchadetailpagedisplays
a null (blank) WebView. A subsequent zoom-in triggers the crash
by a NullPointerException . All eight use cases (12 events) are
necessary to trigger the bug, and such a long event sequence is not
likelyto begenerated byexisting techniques,which indeedfailed
to do so in our evaluation.
3 APPROACH
3.1 Notations and Definitions
GivenanAndroidapp P,ourgoalistogeneratehigh-qualitytest
inputs viause casecombinations. Androidapps areGUI-centered
and event-driven. The runtime GUI layout (snapshot) /lscriptis a tree in
whicheachnode w∈/lscriptisaGUIwidget(e.g.,abuttonoratextfield
object). We use w.typeto refer to w’s widget type (e.g., a button or
atextfield).When Pisinactive(closedorpausedtobackground),
there is no GUI layout and /lscript=⊥.
Anevent e=/angbracketleftt,r,z/angbracketrightisarecordinwhich e.t,e.r,ande.zdenote
e’seventtype,receiverwidget,andassociateddata,respectively.An
event can be either a UI event or a system event, and examples of t
are“ui-click”,“ui-swipe”,or“sys-pause”.ForaUIevent,thereceiver
r(/lscript)=wdenotes that ecan be delivered to w∈/lscriptat runtime.
r(/lscript)=⊥indicates that this event cannot be delivered. A system
event’sreceiverisalwaysthe“system”widget.Otherevent-specific
information is stored in z, e.g., texts entered in a text field or the
content of an added file.
Executing Pwithasequenceofevents E=[e1,e2,...,en]yields
anexecution trace τ=Execute P(E)=/angbracketleftL,M,T/angbracketright.As defined in
Algorithm 1,L4,M,andTdenotethedumpedGUIlayouts,method
invocationtrace,andeachevent’scorrespondingmethodinvoca-
tions, respectively.
4ForL=[/lscript1,/lscript2,...,/lscriptn+1],/lscriptiis the GUI layout dump (at a quiescent state) after
the firsti−1events inEare sent to the app.Algorithm 1: Execution of a sequence of events
1Function ExecuteP(E)
2/lscript←GetGUI(); L←[/lscript];M←∅;T←∅;
3foreache∈Edo
4 ifr(/lscript)/nequal⊥then//ecan be sent on /lscript
5 M/prime←SendEventToApp( e.t,e.r(/lscript),e.z);// send
eventetoP, wait for a quiescent state, and return the
corresponding method invocation sequence
6 M←M::M/prime;T←T∪{/angbracketlefte,M/prime/angbracketright};
7 /lscript←GetGUI(); L←L::[/lscript];
8 else
9 return⊥;
10return/angbracketleftL,M,T/angbracketright;
A use case u=[e1,e2,...,e|u|]is also an event sequence. It
is straightforward for a human developer to manually provide
use cases in either way: (1) annotating use cases as substrings inan execution trace
τ, or (2) feeding τto the following automatic
extraction algorithm.
3.2 Use Case Extraction
Usecasesareextracteduponaminedextendedlabeledtransition
system [31] (ELTS). Furthermore, in the fully automatic settings in
whichnotraceisprovided,weuseastandarddepth-firstexploration
to obtain a bootstrapping trace.
Mining an Automaton . Given an execution trace τ=/angbracketleftL,M,T/angbracketright
fromexecutingeventsequence E,itscorrespondingELTSisathree-
tupleG=/angbracketleftS,E,δ/angbracketright, in which Sis a set of abstract states ( {s|s∈S}
is a partition of the GUI layouts L) andδ:S×E→Scontains the
state transitions.
We adopt the existing algorithm in SwiftHand [ 10] for min-
ingaminimalELTSthatgroupssimilarGUIlayoutstogether,i.e.,
equivalent(/lscript1,/lscript2)5holds for all GUI layouts /lscript1,/lscript2in the same
5We use the Lv.4 GUI Comparison Criteria (GUICC) of AMOLA [ 6] to measure the
similarity between GUIs, i.e., GUI layouts /lscript1and/lscript2are equivalent if and only if
∀e∈E.e.r(/lscript1)/nequal⊥↔e.r(/lscript2)/nequal⊥.
471Algorithm 2: ELTS Mining
1Function
MineELTS(/angbracketleft L=[/lscript1,/lscript2,...,/lscriptn+1],M,T/angbracketright,E=[e1,e2,...,en])
2S←{ {/lscript}|/lscript∈L};// initially, no state is merged
3δ←{/angbracketleft/lscripti,ei,/lscripti+1/angbracketright|1≤i≤n};
4foreach(si,sj)∈S×Sandsi/nequalsjdo// in the BlueFringe
ordering [35]
5 /angbracketleftS/prime,δ/prime/angbracketright← merge-recursive( si,sj,S,δ);
6 if/angbracketleftS/prime,δ/prime/angbracketright/nequal⊥then
7 /angbracketleftS,δ/angbracketright←/angbracketleftS/prime,δ/prime/angbracketright;// update merged states
8return/angbracketleftS,E,δ/angbracketright;
9Function merge-recursive( s,t,S,δ)
10if∀/lscript1∈s,/lscript2∈t.equivalent( /lscript1,/lscript2)then
11 S/prime←S\{s,t}∪{s∪t};δ/prime←δ[s/t];
12 foreach/angbracketlefts,e,t1/angbracketright,/angbracketlefts,e,t2/angbracketright∈δwheret1/nequalt2do
13 /angbracketleftS/prime,δ/prime/angbracketright← merge-recursive( t1,t2,S/prime,δ/prime);
14 if/angbracketleftS/prime,δ/prime/angbracketright=⊥then
15 break;
16 return/angbracketleftS/prime,δ/prime/angbracketright;
17return⊥;// merging failed
states. Such an algorithm (Algorithm 2) is originally used in the
dynamic model extraction of Android apps.
Extractingusecases . Avalidpath p=[s0,s1,...,sm]onG(S,E,δ)
whereδ(si−1,ei)=sifor all 1≤i≤mnaturally corresponds to
the sequence of events
u=[e1,e2,...,em]
as a likely use case. Therefore, the automatic use case extraction
algorithm enumerates all acyclic paths in Gand produces a use
case for each of them.
Notethatourautomaticalgorithmextracts likelyusecasesfrom
the ELTS. In such a manner, we can maximize the chance of ex-
hausting all possible use cases. Moreover, most likely use cases can
be real use cases, while others share similar features with them
(e.g., starting from and endingat quiescentapp states)and can also
be effective exploring the app’s behavior.
Bootstrapping the use casegeneration . Inthefully automatic
setting of ComboDroid, theuse case extraction is bootstrapped by
a standard DFS-alike state space exploration strategy similar to the
A3E algorithm [5].
Starting from the initial state, we take the GUI layout snapshot
/lscript, analyze all widgets w∈/lscriptfor all possible actions on w. For each
action (e.g., clicking a button, or entering a random text from a
predefineddictionarytoatextfield[ 43]),wecreateanevent e6and
addittoEui.Wethensequentiallyexecute(sendtheeventtotheapp
andwaitforaquiescentstate)alleventsin Eui∪Esys,whereEsysis
a set of predefined system events. If executing an event reaches an
unexploredGUI/lscript/prime,theexplorationisrecursivelyconductedon /lscript/prime;if
all events are exercised or reaching an explored GUI, backtracking
is performed (thus this is a depth-first exploration). The depth-first
exploration yields a sequence of events Edfs.
6Fore=/angbracketleftt,r,z/angbracketright,e.tande.zare straightforward to determine. The receiver e.ris
determined by an editing-distance based algorithm described later in Section 3.3.3.3 Enumerating Use Case Combos
Suppose that use cases U={u1,u2,...,un}are extracted from
executiontrace τ=/angbracketleftL,M,T/angbracketrightbyexecutingeventsequence E.Ause
case combination (orcombo) isa sequence ofuse cases denoted by
[ui1→ui2→...→uik]. Sequentially concatenating the events
in the use cases of a combo yields a runnable test input.
Unfortunately,randomlygeneratedcombosusuallystopearly
inanexecutionbecausetherewilllikelyexistanevent ethathas
no receiver on the deliver-time GUI /lscript, i.e.,e.r(/lscript)=⊥. Consider the
combo2→5inthemotivatingexample(Figure 1).The“zooming
in” event has no receiver after deleting a dictionary because the
current GUI does not contain a ListView menu containing the
ZoomInbutton.
Togeneratehigh-qualityusecasecombos,weleveragethefol-
lowing two use case relations:
Aligns-with . For two use cases u=[e1,e2,...,en]andv=
[e/prime
1,e/prime
2,...,e/primem],wesaythat ualignswith v,oru/leadstov,ifwehave
witnessed once that e/prime
1can be successfully delivered after en.I n
otherwords, u/leadstovife/prime
1.r(/lscriptn)/nequal⊥where/lscriptnistheGUIlayoutafter
the execution of en∈Ein the trace τ.
Another issue in the use case alignment (and replaying an event
sequence)istodeterminehowtodeliveraUIevent etoaparticular
GUI layout. For/lscript={w1,w2,...,w|/lscript|}being the GUI layouts right
beforeewas sent in τ, and an arbitrary /lscript/prime={w/prime
1,w/prime
2,...,w/prime
|/lscript/prime|},
we know that there exists 1 ≤i≤|/lscript|such that e.r(/lscript)=wi∈/lscript
becausewiise’sreceiverwidgetin τ.Therefore,thewidget w/prime
j∈/lscript/prime
that is “most similar” to wishould be the receiver of eon/lscript/prime, i.e.,
e.r(/lscript/prime)=w/prime
j.
To measure the similarity between GUI layouts, we compute
theeditingdistance between/lscriptand/lscript/primeusingthealgorithminRep-
Droid[68].Wefindtheshortesteditingoperationsequence(each
editing operation is either inserting or removing a widget) thattransforms
/lscriptto/lscript/prime.I fwiis not removed during the transforma-
tion, it must have a unique correspondence w/prime
j∈/lscript/prime. We thus let
e.r(/lscript/prime)=w/prime
j; otherwise wiis removed and e.r(/lscript/prime)=⊥.
Depends-on . Forusecases uandv,wesaythat vdependson u,
oru/dashedarrowrightv,ifthetwousecasesarepotentiallydata-dependent.Data
dependencyismeasuredatamethodlevel.Consideringthemethod
invocation trace in τ, if there exists a method m∈T(e)fore∈u
andm/prime∈T(e/prime)fore/prime∈vsuch that m/primedata-dependson m, wesay
thatu/dashedarrowrightv.Datadependenciesbetweenmethodsaredetermined
byalightweightstaticanalysis. m/primedata-dependson mifthereisan
(abstract) object or resource write-accessed in mand read-accessed
inm/prime.
Combogeneration .Aligns-with anddepends-on relationsguide
our use case combination (combo) generation. To maximize the
diversity of generated combos, we enforce each combo c=[u1→
u2→...→u|c|]to satisfy:
(1)Eachcomboisanindependenttestcase :e1.r(/lscript0)/nequal⊥for/lscript0∈L
beingtheapp’sinitialGUIlayoutand e1beingthefirstevent
inu1;
(2)Consecutiveusecasesinthecomboarealigned :ui/leadstoui+1for
all 1≤i<|c|; and
472Algorithm 3: Combo Generation
1Function RandomCombo( U,/lscript0,k)
2G(V,E)←randomDAG(2 k);// random DAG of |E|=2k
3F←{ (v,randomChoice( U)) |v∈V};// randomly assign each
v∈Va use case in U
4if|{e|e=/angbracketleftv1,v2/angbracketright∈E∧F(v1)/dashedarrowrightF(v2)}| ≥kthen
5 foreach linear extension [v1,v2,...,v|V|]ofGdo
6 foru0=[e1,e2,...,em]∈U∧e1.r(/lscript0)/nequal⊥do
7 c←connect(u0,F(v1),U,0)::
connect(F(v1),F(v2),U,0)::...::
connect(F(vn−1),F(v|V|),U,0)::[F(v|V|)];
// add paddings such that consecutive use cases are
aligned
8 if⊥/nelementcthen
9 returnc;
10return⊥;
11Function connect(u,dst,U,depth)
12ifu/leadstodstthen
13 return[u];
14ifdepth>MAX_DEPTH then
15 return⊥;
16foru/prime∈U∧u/leadstou/primedo
17 seq←conncet(u/prime,dst,U,depth+1);
18 ifseq/nequal⊥then
19 return[u]::seq;
20return⊥;
(3)Use casesin acombo exhibit k-data-flow diversity, i.e.,there
existskdistinct pairs of (ui,uj)(1≤i<j≤|c|) such that
ui/dashedarrowrightuj.
The algorithm for generating a combo is presented in Algo-
rithm3. Given a set of use cases U, the app’s initial GUI layout /lscript0,
andadata-flowdiversitymetric k,arandom skeletonisfirstsam-
pled. A skeleton is a directed acyclic graph G(V,E)where|E|=2k.
Ifthedata-flowdiversityof Gislessthan k(Line4),thegeneration
should be restarted. Otherwise, each vertex v∈Vis assigned with
a random use case F(v)inU(Lines 2–3);
Alinerextensionoftheskeleton Gcorrespondstoasequence
of use cases: [F(v1),F(v2),...,F(v|V|)].W et r yt opad use cases
F(vi)andF(vi+1)(1≤i<|V|) with more use cases to obtain a
combocsuchthatconsecutiveusecasesin carealigned(Line7).
The padding use cases are depth-first searched with a maximum
length limit MAX_DEPTH (Lines 11–20).
We also add paddings before the first use case in c(Line 6) such
that the resulting combo can be delivered to the initial app state
(andthus ccanbeusedasanindependenttestcase).Ifallaforemen-
tioned paddings exist7, we successfully obtained a use case combo
satisfying our requirements (Lines 8–9). Such a combo is sent to
the app for testing.
7A transition between each pair of GUIs naturally exist for a well-designed app.
Therefore, it is highly like that all aforementioned paddings exist.3.4 Feedback Loop of ComboDroid
As Figure 1shows, there can be multiple iterations of use case gen-
eration and combo enumeration. When enumerated combos are
senttotheappanddiscoveredpreviouslyunknownstates,anew
iterationshouldbeinitiated.Beforethenextiterationstarts,adevel-
oper can manually inspect the testing report and provide/annotate
more use cases.
Supposethatweconcatenatetheexecutiontracesinallprevious
iterations of use case generation and combo enumeration. Concep-
tually, this can be regarded as adding an extra “restart” event after
sending all events in a combo8. Such a merged trace is used for the
ELTSminingandusecaseextractioninthenextroundofiteration.
4 IMPLEMENTATION
The ComboDroid framework is implemented using Kotlin and
Java. ComboDroid consists of a fully automatic variant Combo-
Droidαand a semi-automatic variant ComboDroidβ. We exten-
sivelyusedopen-sourcetoolsinthe implementation,andCombo-
Droid is also open-source available9: GUI events are recorded by
Getevent [25];GUIandsystemeventsaredeliveredusingAndroid
Debug Bridge (ADB) [ 22]; GUI layouts are dumped by Android UI
Automator [24];methodtracesarecollectedbyprograminstrumen-
tation with Soot [ 13]. The implementation follows the descriptions
inSection 3.Wefollowthecommonpracticeofexistingstate-of-
the-arttechniques[ 6,28,43,56]andidentifyquiescentappstates
bystabilizedGUIs.Specifically,wetakethesameimplementation
as APE [28] by dumping GUI layouts every 200ms until it is stable
(using the Lv.4 GUICC) with a 1000ms upper-bound.
For ComboDroidα, we implement the DFS-alike exploration
tool,andfollowthesameimplementationasAPE[ 28]byreplaying
previous execution traces for backtracking. For ComboDroidβ,w e
analyze the GUI layouts, where the human tester sends each event,
togetherwiththecorrespondingeventtodetermineeachevent’s
receiver.
Inthelightweightstaticanalysistodeterminethedepends-on
relation, we also model the Android 6.0 APIs (API level 23) [ 23]
todetermineread/writeaccessestoresources,e.g.,wedetermine
whether an SQL command in SQLiteDatabase.execSQL is a read
orwritetothedatabase.Moreover,foran(abstract)object,ifany
method whose name matches the regular expression
(get|is|read)(.+)|on(.+)changed
iscalled,weconsideritaread;Similarly,callinganymethodwhose
name matches
(set|write|change|modify)(.+)
is considered a write.
The depth-first exploration of ComboDroidαwas set with a
time limit of 30 minutes in each iteration. In the combo generation
(bothComboDroidαandComboDroidβ),wesetdata-flowdiversity
k=2 and MAX_DEPTH =5. We generate d2random combos (by
RandomCombo) if there are ddepends-on edges.
8A restart event is also added after ELTS mining.
9https://github.com/skull591/ComboDroid-Artifact.
4735 EVALUATION
ThissectionpresentsourevaluationofComboDroid.Theexperi-
mental subjects and setup are described in Section 5.1, followed by
evaluation results of ComboDroidα(the fully automatic variant)
andComboDroidβ(thehumanaidedvariant)inSections 5.2and5.3,
respectively.Discussionsincludingthreatstovalidityarepresented
in Section 5.4.
5.1 Experimental Subjects and Setup
The first column of Table 1lists the 17 evaluation subjects. The
appsareselectedusingthefollowingrules:First,weselectedthe
threelargest (inLoC)apps evaluatedin existingwork[ 28,43,56]:
WordPress, K-9 Mail,and MyExpense.Second, werandomly se-
lected nine apps with at least 10K Downloads evaluated in the
existingwork[ 28,43,56]:Wikipedia,AnkiDroid,AmazeFileM-
anager,AnyMemo,HackerNewsReader,CallMeter,Aard2,
WorldClock,andAlogcat.Additionally,werandomlyselected
five popular (at least 100 stars by 2018) open-source apps from
Github: AntennaPod, PocketHub, SimpleTask, Simple Draw,
and CoolClock.
If an app’s major functionalities cannot be accessed without a
proper initial setup (e.g., user login), we provide the app a script
to complete the setup. All evaluated techniques receive exactly the
samescript(andthescriptrunsautomaticallyoncetheinitialsetup
GUI is reached) to ensure a fair comparison. We did not mock any
further functionality other than the initial setup script.
Weusetwometricstomeasurethetestingthoroughness.The
firstisbytecodeinstructioncoveragecollectedbyJaCoCo[ 32],as
ahighercodecoveragestronglycorrelatestoabetterexerciseof
an app’s functionalities. Second, we study whether the techniques
can manifest (and reproduce) previously known or unknown bugs
by examining the Android system’s logs.
To evaluate ComboDroidα, we compare it with the state-of-the-
artautomatedtechniques:Monkey[ 26],Sapienz[ 43],andAPE[ 28]10.
For each subject, we ran each automatic testing technique for 12
hoursto simulateanightly continuousintegration build-and-test
cycle.WeranComboDroidαforterminationor12hoursatmost.
For each subject, we ran each techniques three times and reported
the average results. Test coverage and bug manifestation results
are then studied.
To evaluate ComboDroidβ, we compare it with a human expert.
For manual use case generation of ComboDroidβ,w eg a v ear e -
cruitedtesteroneworkday(~8workhours)foreachtestsubjectand
thenraneachsubjectfor12hours.Meanwhile,weaskedanother
independently recruited Android testing expert (a post-graduatestudent who had published a few research papers on testing and
analysis of Android apps) to cover as much code as possible given
atimelimitofthreeworkdays(~24workhours).Thehumanexpert
had access to an app’s source code and was told to use coveragefeedback to maximize code coverage. Since manual labor is notscalable, we only evaluated the subjects of top 10 LoC, as shownin Table3. To reduce distractions (as confirming and diagnosing
10Since APE [ 28] significantly outperforms Stoat [ 56] and other related work, we did
not show results of other techniques in this paper.bugs aretime-consuming), weasked thehuman expertnot topro-
vide any bug report. Therefore, only test coverage is studied in the
evaluation of ComboDroidβ.
Allexperiments wereconducted onanocta-core Inteli7-4790U
PCwith16GiBRAMrunningUbuntu16.04LTSandanAndroid
6.0 Emulator.
5.2 Evaluation Results: ComboDroidα
The 12-hour coverage results and manifested bugs are listed inTable1and Table 2, respectively. The detailed coverage trends
areplottedinFigure 2.Theseresultsareconsistentwitharecent
empirical study [ 59]: automated techniques by that time barely
outperformthesimplestMonkey.Thebestexistingtechnique,APE,
marginally outperforms Monkey by covering 2.1% more code.
Encouragingly, ComboDroidαconsistently outperforms exist-
ing techniques in nearly allsubjects11. For Alogcat, CoolClock,
and CallMeter, ComboDroidαterminated within 12 hours, while
for other subjects it ran until the time limit exceeded. Compared
with the best existing technique APE, ComboDroidαcovered 4.6%
more code on average. This improvement is even 2 ×as much as
theimprovementofAPEoverMonkey.ConsideringthattheAPE
implementationgenerates~1 .5×moreeventsin12hours(~120Kfor
ComboDroidαv.s.~300KforAPE),ComboDroidαisconsiderably
more effective in exploiting each event’s merits.
The progressive coverage in Figure 2shows that ComboDroidα
usually begins to outperform existing techniques after six hours.Consider that the current ComboDroid
αimplementation emits
events at ~1 /2 speed, the result is also promising. Furthermore,
codecoveragegainofexistingtechniquesisusuallymarginal(or
zero) in the last hours. In contrast, ComboDroidαis consistently
exploring useful use case combinations to cover more code.
Thebugmanifestationevaluationresults(Table 2)arealsoen-
couraging.Wemanuallyexaminedthetestlogsofalltechniques
andfound12reproduciblebugswithanexplicitrootcause.Exclud-
ing the bug in Simple Draw (ComboDroidαmissed it due to an
implementationlimitations),ComboDroidαmanifested all11previ-
ously known or unknown bugs, where the best existing technique,
APE,manifested7(64%).Wealsoreportedfour previouslyunknown
bugs (APE can discover only two of them) to the developers. All of
themwereconfirmedandtwoofwhichhavebeenfixed.Further-
more, the two previously unknown bugs uniquely discovered by
ComboDroidαaredeepbugswhichrequirealong(andmeaningful)
input sequence to trigger. The motivating example (Figure 1)i s
such a case.
Therefore, we hold strong evidence that ComboDroidαis more
effective in automatically generating high-quality test inputs for
Android apps compared with existing techniques.
5.3 Evaluation Results: ComboDroidβ
The evaluation results of ComboDroidβare displayed in Table 3.
For all subjects, ComboDroidβran until the time limit exceeded. It
isexpectedthatthehumantestingexpertsignificantlyoutperforms
11APEandComboDroidαcoveredlesscodeforSimpleDrawcomparedwithMonkey
becausetheimplementationsdonotidentifycanvaswidgetsandthusnotsenddragging
events. We consider this an implementation limitation.
474020406080100WordPress AntennaPod K-9 Mail
020406080100MyExpenses Wikipedia AnkiDroid
020406080100AmazeFileManager PocketHub AnyMemo
020406080100Hacker News Reader CallMeter SimpleTask
020406080100Simple Draw Aard2 World Clock
020406080100CoolClock Alogcat ComboDroidα(fully automatic)
Monkey
Sapienz
APE
ComboDroidβ(semi-automatic)
Human expert
Figure 2: Progressive coverage report of evaluated techniques (averaged over three runs). The xaxis is the time spent (0–12
hours). The yaxis indicates the percentage of code covered thus far.
475Table 1: Evaluation results of ComboDroidα: test coverage
Subject (Category, Downloads; LoC) Monkey Sapienz APE ComboDroidαCoverage trend
WordPress, WP (Social, 5M–10M; 327,845) 24.4% 24.3%24.1%36.1%(+11.7%)
AntennaPod, AP (Video, 100K–500K; 262,460) 57.5% 61.3% 65.5% 69.8% (+4.3%)
K-9 Mail, K9 (Communication, 5M–10M; 159,708) 19.1% 20.4%26.3%32.5%(+6.2%)
MyExpenses, ME (Finance, 500K–1M; 104,306) 43.8% 40.2% 48.6% 56.3% (+7.7%)
Wikipedia, Wiki (Books, 10M–50M; 93,404) 37.2% 39.3%44.3%45.1%(+0.8%)
AnkiDroid, AD (Education, 1M–5M; 66,513) 50.6% 49.0% 50.6% 54.3% (+3.7%)
AmazeFileManager, AFM (Tools, 100K–500K; 66,126) 39.6% 42.5%45.0%55.2%(+10.2%)
PocketHub, PH (Tools, 100K–500K; 47,946) 22.1% 19.1% 27.2% 31.4% (+4.2%)
AnyMemo, AM (Education, 100K–500K; 40,503) 57.5% 51.7%64.3%66.8%(+2.5%)
Hacker News Reader, HNR (News, 50K–100K; 38,315) 69.9% 66.2% 65.5% 71.2% (+1.3%)
CallMeter, CM (Tools, 1M–5M; 21,973) 54.0% 49.1%58.5%60.4%(+1.9%)
SimpleTask, ST (Productivity, 10K–50K; 20,980) 57.2% 57.2% 62.8% 70.2% (+7.4%)
Simple Draw, SD (Tools, 10K–50K; 18,685) 50.0% 51.3%22.8%26.8%(-24.5%)
Aard2, Aard (Books, 10K–50K; 9,622) 68.0% 64.3% 73.8% 77.6% (+3.8%)
World Clock, WC (Bussiness, 1M–5M; 7,181) 50.2% 50.8%55.1%58.0%(+2.9%)
CoolClock, CC (Tools, 10K–50K; 2,762) 75.4% 73.2% 78.0% 79.6% (+1.6%)
Alogcat, ALC (Tools, 100K–500K; 846) 49.1% 48.8%49.1%49.1%(0.0%)
Average 48.6% 47.6% 50.7% 55.3% (+4.6%)
1ColumnCoverage trend plots the coverage trend of each tool. The red solid lines denote ComboDroidα, and dashed lines are
existingtechniques.ThedetailedcoveragetrendsaredisplayedinFigure 2.Numberinabracketisthecoveragedifferencesbetween
ComboDroidαand the best existing technique (Monkey, Sapienz, and APE).
Table2:EvaluationresultsofComboDroidα:bugmanifesta-
tion
Bug ID Cause Discovered by
WP-10147 Infinite recursion APE, CDα
AP-1234 Atomicity violation CDα
AP-3195 Null pointer dereference all
K9-3308 Mismatched mime type Sapienz, CDα
AFM-1351 Null pointer dereference all
AFM-1402 Lifecycleeventmishandling APE, CDα
AM-480⋆Lifecycleeventmishandling CDα
AM-503⋆Null pointer dereference APE, CDα
CM-128⋆Text input mishandling APE, CDα
SD-49 Miss-used local variables Monkey
Aard-90⋆Null pointer dereference CDα
Aard-7 Null pointer dereference all
Monkey: 4 (33%); Sapienz: 4 (33%); APE: 7 (58%); CDα: 11 (92%)
1Bug IDis theissue IDinthe project’sGitHub repository.A starred
Bug ID⋆denotes a previously unknown bug.
automatedtechniques.Eventhethebestautomatedtechniqueso
far, ComboDroidα, covered 12.0% less code.
However, this gap is reduced to 3.2% when human knowledge is
integrated into our framework: use case combinations additionally
covered13.2%morecodethanmanualusecasesonly.ComboDroidβ
greatly amplified the use cases (covering 47.5% code, which is even
4.4% less than ComboDroidα) to achieve a result nearly as goodTable 3: Evaluation results of ComboDroidβ: test coverage
Subject UC CDαComboDroidβExpert
WP(328K)40.1%36.1%48.3% (+8.2%/+12.2%) 53.3% (+5.0%)
AP(262K)65.4% 69.8% 75.2% (+9.8%/+5.4%) 78.6% (+3.4%)
K9(160K)38.5%32.5%50.3% (+11.8%/+17.8%) 53.7% (+3.4%)
ME(104K)53.1% 56.3% 66.8% (+13.7%/+10.5%) 69.7% (+2.9%)
Wiki(93K)37.3%45.1%46.0% (+8.7%/+0.9%) 49.6% (+3.6%)
AD(67K)50.3% 54.3% 66.8% (+16.5%/+12.5%) 71.4% (+4.6%)
AFM(66K)43.3%55.2%66.2% (+22.9%/+11%) 67.3% (+1.1%)
PH(48K)31.5% 31.4% 39.2% (+7.7%/+7.8%) 45.3% (+6.1%)
AM(41K)62.1%66.8%71.3% (+9.2%/+4.5%) 74.0% (+2.7%)
HNR(38K)53.4% 71.2% 76.5% (+23.1%/+5.3%) 76.3% (-0.2%)
Average 47.5% 51.9% 60.7% (+13.2%/+8.8%) 63.9% (+3.2%)
1The number in Column Subjectis the app’s LoC. Columns UC,CDα,
ComboDroidβ, andExpertdisplay the code coverage of manual use
cases,ComboDroidα,ComboDroidβ,andthehumanexpert,respec-
tively.ThenumbersinthebracketsofColumn ComboDroidβindicate
thecoveragedifferencesbetweenComboDroidβandmanualusecases
and ComboDroidα, respectively. The numbers in the brackets of Col-
umnExpertindicatethedifferencesbetweenthehumanexpertand
ComboDroidβ.
as the human expert. Surprisingly, the ComboDroidβeven outper-
formedthehumanexpertinHackerNewsReader.Afteranalyzing
the code and coverage data, we found that Hacker News Reader
can enable data-preload of news articles in the settings. When
itisenabled,openinganarticleinanapplication-internalformat
476濖瀈瀅瀅濸瀁瀇
瀆瀇濴瀇濸濣瀅瀂濹濼瀇濴濵濿濸
瀆瀇濴瀇濸濇
濄濅濆濧濸瀆瀇濼瀁濺澳濻濼瀆瀇瀂瀅瀌
濈濉
濗濸濸瀀澳濇澳濴瀁濷澳濈澳濸瀄瀈濼瀉濴濿濸瀁瀇濍
濿瀂瀆濸澳瀃瀅瀂濹濼瀇濴濵濿濸澳瀆瀇濴瀇濸濇濂濈
濄濅濆濧濸瀆瀇濼瀁濺澳濻濼瀆瀇瀂瀅瀌
濉濥濴瀁濷瀂瀀濿瀌
瀆濸濿濸濶瀇
濄濅濆
濆濇濄濔濿濼濺瀁濸濷澳瀈瀆濸澳濶濴瀆濸瀆
瀊濼瀇濻澳濷濸瀃濸瀁濷濸瀁濶瀌濖瀂瀀濵濼瀁濸
濄濅濆濈濉濥濸濴濶濻澳瀃瀅瀂濹濼瀇濴濵濿濸
瀆瀇濴瀇濸濄濅濆濇
濴濵濶濷濠瀂瀇濼濹澳
瀆濸瀄瀈濸瀁濶濸瀆
濠瀂瀇濼濹澳
瀆濸瀄瀈濸瀁濶濸瀆濄濅濆濘瀋濸濶瀈瀇濼瀂瀁澳濹濴濼濿濍
݁.ݎ݈=٣
濴濵濶濇濥濴瀁濷瀂瀀濿瀌
濶瀅瀂瀆瀆瀂瀉濸瀅
濡瀂澳瀁濸瀊澳瀆瀇濴瀇濸
濸瀋瀃濿瀂瀅濸濷濁濣濢濟濙濭澣澵濄澹
濇濕濤濝濙濢濮
澷濣濡濖濣澸濦濣濝濘
Figure 3: Qualitative illustration of the state space explo-
ration strategies in evaluated techniques.
invokesadditionalcodetoprocesspre-loadeddata.Suchasubtlede-pendencyismissedbythehumanexpert;ontheotherhand,though
also not covered by any single manual use case, ComboDroidβ
correctly pinpointed such a data dependency (in the depends-on
relation) and accordingly generated the use case combination.
Though in a preliminary stage, ComboDroidβdemonstrates the
potential of automatically leveraging human insight in comple-menting and boosting automated techniques in testing Android
apps.
5.4 Discussions
5.4.1 Towards Thorough Automatic Testing of Android Apps. Fig-
ure3illustratesthesearchstrategiesoftheevaluatedtechniques,
for giving a qualitatively explanation of why ComboDroidαout-
performed existing techniques.
Random-based techniques Monkey [ 26] and APE [ 28] at each
timedeliversexactlyoneeventtotheapp,andthereforearecom-
pletely unaware of the remaining state space. Their limitationsare obvious: the search strategies are purely based on the noisyexploration history. Such strategies may easily lose a deep (and
profitable)appstateonrandomtries(e.g.,pressingabuttonreturns
to the app’s main menu).
Sapienz[43],thoughexploitsmotifsequencesinaguidedsearch,
failstoeffectivelyassemblingthem.First,thereisnorationaleor
quality guarantee of the motif sequences—they are more or lessrandom event sequences. Second, mutation and crossover opera-
tionsinthegeneticsearchareinefficientincreatingusefulmotif
sequence combos: randomly concatenating two event sequences
willmostlyresultinauselesscombo.ItisnotsurprisethatSapienz
even covered less code than Monkey in the long run. This result is
consistent with the existing studies [59].Incontrast,ComboDroidαgeneratedbothhigh-qualityusecases
and their combos, and thus is highly effective in covering app
functionalities even if it delivers 60% less events.
Compared with manual testing, automatic testing is still far less
satisfactory:thehumanexpertcovered12.0%morecodeonaverage
than ComboDroidα. The evaluation results of ComboDroidβshow
that this gap is mainly due to the quality of use cases. Our usecase extraction algorithm simply cannot “understand” the app’s
functions andsemantics, however, meaningful usecases are quite
naturalevenforanappuser.Machinelearningoverlarge-scaleapp
usage data set may be a promising direction to address this issue.
5.4.2 Leveraging Human Insights in Semi-Automatic Testing of An-
droid Apps. ComboDroidβsuccessfully “amplified” the manual use
casestoachieveacompetitivecoveragecomparedwithahuman
expert: adding a little more human aid boosts the testing thorough-ness.Thispartiallyvalidatedourintuitionthathumansaregoodat
sketchingthemajorfunctionalitiesoftheapp;oncesuchinsights
are extracted (as use cases), tedious and repetitive work can be
offloaded to machines.
Therefore,ComboDroidβ,asaconcept-provingprototype,opens
anewresearchdirectiontowardsthehuman-machinecollaborativetestingofAndroidapps.Automaticallygeneratingmeaningful(and
handy)suggestions (eitherbyprogram analysisormachine learn-
ing) to help manual testers, developers, or even users to provide
better use cases is a rewarding future direction.
5.4.3 Threats to Validity. Biasintheselectedsubjects . Therep-
resentativenessofselectedtestsubjectscanaffectthefidelityofourconclusions.Tomitigatethisthreat,weselectedevaluationsubjects
from various sources: popular benchmarks evaluated in existing
workplusrandomonesfromGitHub.Thesesubjectsare(1)largein
size (around 76 KLoC on average), (2) well-maintained (containing
thousands of revisions and hundreds of issues on average), (3) pop-
ular (all have 10K+ downloads), and (4) diverse in categories. Since
ComboDroidconsistentlyandsignificantlyoutperformsexisting
techniquesinallthesebenchmarks(exceptforSimpleDrawdueto
the implementation limitation), the conclusion that ComboDroidα
is more effective than existing techniques is evident.
Randomnessandnon-determinism . Theevaluatedtechniques
(including ComboDroid) involve randomness, and subjects may be
non-deterministic. Therefore, for each subject and technique we
reporttheaverageresultofthreeindependentrunsunderthesame
settings(theexperimentscostover2,400CPUhours)toalleviate
this issue.Human factors
. The performance of human testers vary form
persontoperson.Therefore,theevaluationresultsofComboDroidβ
onlyapplytothathumantestingexpert.Sincethepost-graduate
Androidtesting/analysisexpertknewusinadvance,wearecertain
that he/she tried the best to cover as much code as possible.
6 RELATED WORK
Many technologies have been proposed for input generation for
Android app testing, including both fully automatic ones and semi-
automaticones.Moreover,sometechnologiesgeneratingtestinputs
for GUI/web testing also share similarities with ComboDroid.
477Fully automatic test input generation for Android apps .A
majority of existing technologies aim to fully automatically gener-
atetestinputsforAndroidapps.Manyofthemgeneratetestinputs
for general testing purposes.
Randomtestingisalightweightandpracticalapproachinwhich
alargenumberofrandomeventsarequicklyfedtoanapp,including
Monkey [ 26], DynoDroid [ 41], DroidFuzzer [ 64], IntentFuzzer [ 63],
etc.
Usinga GUImodel (eitherpredefined ormined)may guidethe
explorationofanapp’sstatespace.Representativeworkincludes
MobiGUITAR [ 3], SwiftHand [ 60], AMOLA [ 6], and the state-of-
the-artAPE[ 28].Suchstatespaceexplorationisusuallydoneby
a depth(breadth)-first search, e.g., A3E[5], GAT [61], and EHB-
Droid [55]. However, even if with a model, existing techniques fall
short on generating long (and meaningful) test inputs.
Search-based software engineering techniques can also be ap-
plied,suchasEvoDroid[ 42]andSapienz[ 43],whichemploygenetic
programming to evolve generated test inputs, or Stoat [ 56], which
constructs a stochastic model and uses MCMC [ 8] to guide the
generation. Moreover, some researchers propose to utilize machine
learning to guide the input generation [ 12,27,34]. Furthermore,
somepiecesof workutilizesymbolicor concolicexecutiontosys-
tematicallygeneratetestinputsformaximizingbranchcoverage,
including SIG-Droid [ 47], the technology proposed by Jensen et
al. [30], SynthesiSe [ 17], and DroidPF [ 7]. Existing search-based
techniques barely scale to large apps.
Finally,ComboDroidisnotthefirsttointroducetheideaofcom-
binationinAndroidapptesting.However,existingcombinatorial-
based strategies [ 1,48] concern only combinations of single events
and thus unable to generate long (and meaningful) test inputs.
In conclusion, all existing technologies fall short on generating
long (and meaningful) test inputs for practical apps, which are
essentialinmanifestingdeepappstatesandrevealingmanynon-
trivial bugs. The limitation of existing techniques motivated the
design of ComboDroid.
Semi-automatictestinputgenerationforAndroidapps . Some
technologies are proposed to utilize human intelligence to improve
the quality of generated test inputs. For instance, Polariz [ 44]
extracts common event sequences from crowd-based testing to
enhance SAPIENZ. AppFlow [ 29] records short event sequences
provided by human, and utilizes machine learning to synthesize
longeventsequences.Moreover,UGA[ 38]extendsmanualevent
sequences exploring the skeleton of the app’s state space. Though
capable of utilizing human intelligence, Polariz and UGA have no
controloverthequalityofextractedmanualeventsequences.On
the other hand, AppFlow lacks an effective mechanism for reusing
the event sequences in testing.
Domain-specifictestinputgenerationforAndroidapps . Some
technologiesaimtogeneratetestinputsforcertaintestingdomains
orformanifestingcertainkindofbugs.Forinstance,EOEDroid[ 62]
utilizes symbolic execution to generate inputs to testing WebViews
ofanapp,whileSnowDrop[ 69]aimstotestbackgroundservicesof
an app. APEChecker [ 16] and AATT+ [ 36,57] generate test inputs
to manifest potential concurrency bugs in Android apps. Moreover,sometechnologiesareproposedtodetectenergyinefficiencyinAn-
droid apps, Such as GreenDroid [ 40] and its extensions [ 37,39,58]ThesetechniquesaregenerallyorthogonaltoComboDroid.They
can be benefited by the high-quality test inputs generated by Com-
boDroid.
TestinputgenerationforGUI/webtesting . Sometechnologies
utilize iterative GUI exploration or program analysis to generate
test inputs for GUI/web testing. Some pieces of work [ 2,18–21,45,
66,67] iteratively observes the execution of existing test inputs,
extractsadditionalknowledge(e.g.,arefinedmodel),andderives
newtestinputs.Forinstance,Nguyenetal.[ 49]proposestheOEM*
paradigmthatautomaticallyidentifiesnewtestinputsduringthe
execution of existing ones, expands the current incomplete GUI
event model, and generates additional test inputs based on current
execution traces. Such iterative process resembles ComboDroid.However, the knowledge extracted by these technologies mostlycomes from observations of GUI transitions, and other relations
between test inputs such as data dependency are often neglected.
On the other hand, some technologies utilize static analysis
on program code to find data dependencies between events, and
thus generate effective test inputs [ 4,9,14,15,50]. However, these
technologiescannotbedirectlyappliedfortestingAndroidapps,
sinceAndroidappsarecomponent-basedwithbrokencontrol-/data-
flow, and often invoke Android-specific APIs to access shared data,
e.g.SharedPreference.getBoolean.
Incontrast,ComboDroidextractsknowledgeoftheappunder
test from both GUI transitions and data dependencies, and uti-
lizelightweightstaticanalysisonexecutiontraceswithAndroid-
specificAPImodelingtoinferdepends-onrelationsbetweeninputs.
7 CONCLUSION AND FUTURE WORK
Leveraging the insight that long, meaningful, and effective test
inputsareusuallytheconcatenationofshorteventsequencesfor
manifesting a specific app functionality, this paper presents the
ComboDroid framework in which the Android app test input gen-
eration problem is decomposed into a feedback loop of use casegeneration and use case combination. The evaluation results are
encouraging.ThefullyautomaticComboDroidαcovered on aver-
age4.6%morecodethanthebestexistingtechniqueandrevealed
four previously unknown bugs. With little human aid, the semi-
automatic ComboDroidβachieved a comparable coverage (only
3.2% less on average) with a human testing expert.
ComboDroid sheds light on a new research direction for obtain-
ing high-quality test inputs, either fully automatic or with human
aid.Based onthisproof-of-concept prototype,a diverserange of
technologies can be applied in the future enhancement of Combo-
Droid. Promising research includes exploiting machine learning in
usecasemining,crowd-sourcedusecasesacquisition,andmodel
checking combos.
ACKNOWLEDGMENTS
ThisworkwassupportedinpartbyNationalNaturalScienceFoun-
dation(Grants#61690204,#61932021,and#61802165)ofChina.The
authorswouldalsoliketothankthesupportfromtheCollaborativeInnovationCenterofNovelSoftwareTechnologyandIndustrializa-tion,Jiangsu,China.YanyanJiang(jyy@nju.edu.cn)andChangXu
(changxu@nju.edu.cn) are the corresponding authors.
478REFERENCES
[1]DavidAdamo,DmitryNurmuradov,ShraddhaPiparia,andRenéeBryce.2018.
Combinatorial-basedeventsequencetestingofAndroidapplications. Information
and Software Technology 99 (2018), 98–117.
[2] Pekka Aho,Matias Suarez,TeemuKanstrén, andAtifM Memon.2014. Murphy
tools: Utilizing extracted gui models for industrial software testing. In 2014 IEEE
SeventhInternationalConferenceonSoftwareTesting,VerificationandValidation
Workshops. IEEE, 343–348.
[3]DomenicoAmalfitano,AnnaRitaFasolino,PorfirioTramontana,BryanDzung
Ta, and AtifM Memon. 2014. MobiGUITAR: Automatedmodel-based testing of
mobile apps. IEEE software 32, 5 (2014), 53–59.
[4]StephanArlt,AndreasPodelski,CristianoBertolini,MartinSchäf,IshanBanerjee,
andAtifMMemon.2012. LightweightstaticanalysisforGUItesting.In 2012IEEE
23rdInternationalSymposiumonSoftwareReliabilityEngineering.IEEE,301–310.
[5]TanzirulAzimandIulianNeamtiu.2013. Targetedanddepth-firstexploration
for systematic testing of android apps. In Proceedings of the 2013 ACM SIGPLAN
international conference on Object oriented programming systems languages &
applications. 641–660.
[6]Young-Min Baek and Doo-Hwan Bae. 2016. Automated model-based Android
GUI testing using multi-level GUI comparison criteria. In Proceedings of the 31st
IEEE/ACMInternationalConferenceonAutomatedSoftwareEngineering.238–249.
[7]Guangdong Bai, Quanqi Ye, Yongzheng Wu, Heila Botha, Jun Sun, Yang Liu,
Jin Song Dong, and Willem Visser. 2017. Towards model checking android
applications. IEEE Transactions on Software Engineering 44, 6 (2017), 595–612.
[8]Steve Brooks, Andrew Gelman, Galin Jones, and Xiao-Li Meng. 2011. Handbook
of markov chain monte carlo. CRC press.
[9]Lin Cheng, Zijiang Yang, and Chao Wang. 2017. Systematic reduction of GUItestsequences.In 201732ndIEEE/ACMInternationalConferenceonAutomated
Software Engineering. IEEE, 849–860.
[10]Wontae Choi, George Necula, and Koushik Sen. 2013. Guided gui testing ofandroid apps with minimal restart and approximate learning. Acm Sigplan
Notices48, 10 (2013), 623–640.
[11]Shauvik Roy Choudhary, Alessandra Gorla, and Alessandro Orso. 2015. Au-tomated test input generation for android: Are we there yet?(e). In 2015 30th
IEEE/ACM International Conference on Automated Software Engineering. IEEE,
429–440.
[12]Christian Degott, Nataniel P Borges Jr, and Andreas Zeller. 2019. Learninguser interface element interactions. In Proceedings of the 28th ACM SIGSOFT
International Symposium on Software Testing and Analysis. 296–306.
[13]SootDevelopers.2019. Soot. RetrievedJune29,2019from https://github.com/
Sable/soot
[14]BernhardDorninger,JosefPichler,andAlbinKern.2015. Usingstaticanalysisfor
knowledge extraction from industrial User Interfaces. In 2015 IEEE International
Conference on Software Maintenance and Evolution. IEEE, 497–500.
[15]SebastianElbaum, SrikanthKarre,andGreggRothermel.2003. Improvingweb
application testing with user session data. In 25th International Conference on
Software Engineering, 2003. Proceedings. IEEE, 49–59.
[16]LinglingFan,TingSu,SenChen,GuozhuMeng,YangLiu,LihuaXu,andGeguang
Pu.2018. Efficientlymanifestingasynchronousprogrammingerrorsinandroid
apps. InProceedings of the 33rd ACM/IEEE International Conference on Automated
Software Engineering. 486–497.
[17]XiangGao,ShinHweiTan,ZhenDong,andAbhikRoychoudhury.2018. Android
testing via synthetic symbolic execution. In Proceedings of the 33rd ACM/IEEE
International Conference on Automated Software Engineering. 419–429.
[18]ZebaoGao,ZhenyuChen,YunxiaoZou,andAtifMMemon.2015. Sitar:Guitest
script repair. Ieee transactions on software engineering 42, 2 (2015), 170–186.
[19]Zebao Gao, Chunrong Fang, and Atif M Memon. 2015. Pushing the limits on
automation in GUI regression testing. In 2015 IEEE 26th international symposium
on software reliability engineering. IEEE, 565–575.
[20]CerenŞahinGebizli,AbdulhadiKırkıcı,andHasanSözer.2018. Increasingtest
efficiencybyrisk-drivenmodel-basedtesting. JournalofSystemsandSoftware
144 (2018), 356–365.
[21]CerenSahinGebizli,HasanSözer,andAliÖzerErcan.2016.Successiverefinement
ofmodelsfor model-basedtestingtoincreasesystemtest effectiveness.In 2016
IEEENinthInternationalConferenceonSoftwareTesting,VerificationandValidation
Workshops. IEEE, 263–268.
[22]Google.2019. AndroidDebugBridge(adb). RetrievedJune29,2019from https:
//developer.android.com/studio/command-line/adb
[23]Google. 2019. Android Documentation. Retrieved June 29, 2019 from https:
//developer.android.com/docs/
[24]Google. 2019. Android UI Automator. Retrieved June 29, 2019 from https:
//developer.android.com/training/testing/#UIAutomator
[25]Google. 2019. Getevent. Retrieved June 29, 2019 from https://source.android.
com/devices/input/getevent
[26]Google. 2019. UI/Application Exerciser Monkey. Retrieved June 29, 2019 from
https://developer.android.com/studio/test/monkey[27]TianxiaoGu,ChunCao,TianchiLiu,ChengnianSun,JingDeng,XiaoxingMa,
andJianLü. 2017. Aimdroid: Activity-insulatedmulti-levelautomatedtesting
for android applications. In 2017 IEEE International Conference on Software Main-
tenance and Evolution. IEEE, 103–114.
[28]Tianxiao Gu, Chengnian Sun, Xiaoxing Ma, Chun Cao, Chang Xu, Yuan Yao,Qirun Zhang, Jian Lu, and Zhendong Su. 2019. Practical GUI testing of An-
droid applications via model abstraction and refinement. In 2019 IEEE/ACM 41st
International Conference on Software Engineering. IEEE, 269–280.
[29]Gang Hu, Linjie Zhu, and Junfeng Yang. 2018. AppFlow: using machine learning
to synthesize robust, reusable UI tests. In Proceedings of the 2018 26th ACM Joint
Meeting on European Software Engineering Conference and Symposium on the
Foundations of Software Engineering. 269–282.
[30]CasperSJensen,MukulRPrasad,andAndersMøller.2013. Automatedtesting
with targeted event sequence generation. In Proceedings of the 2013 International
Symposium on Software Testing and Analysis. 67–77.
[31]Robert M Keller. 1976. Formal verification of parallel programs. Commun. ACM
19, 7 (1976), 371–384.
[32]MountainmindsGmbH&Co.KGandContributors.2019. JaCoCo-JavaCode
CoverageLibrary. RetrievedAugest7,2019from https://www.jacoco.org/jacoco/
trunk/index.html
[33]PingfanKong,LiLi,JunGao,KuiLiu,TegawendéFBissyandé,andJacquesKlein.
2018. Automatedtestingofandroidapps:Asystematicliteraturereview. IEEE
Transactions on Reliability 68, 1 (2018), 45–66.
[34]YavuzKoroglu,AlperSen,OzlemMuslu,YunusMete,CeydaUlker,TolgaTan-
riverdi, and Yunus Donmez. 2018. QBE: QLearning-based exploration of android
applications. In 2018 IEEE 11th International Conference on Software Testing, Veri-
fication and Validation. IEEE, 105–115.
[35]Kevin J Lang, Barak A Pearlmutter, and Rodney A Price. 1998. Results of the ab-
badingooneDFAlearningcompetitionandanewevidence-drivenstatemergingalgorithm.In InternationalColloquiumonGrammaticalInference.Springer,1–12.
[36]Qiwei Li, Yanyan Jiang, Tianxiao Gu, Chang Xu, Jun Ma, Xiaoxing Ma, and Jian
Lu. 2016. Effectively manifesting concurrency bugs in android apps. In 2016 23rd
Asia-Pacific Software Engineering Conference. IEEE, 209–216.
[37]Qiwei Li, Chang Xu, Yepang Liu, Chun Cao, Xiaoxing Ma, and Jian Lü. 2017.
CyanDroid: stable and effective energy inefficiency diagnosis for Android apps.
Science China Information Sciences 60, 1 (2017), 012104.
[38]XiujiangLi,YanyanJiang,YepangLiu,ChangXu,XiaoxingMa,andJianLu.2014.Userguidedautomationfortestingmobileapps.In 201421stAsia-PacificSoftware
Engineering Conference, Vol. 1. IEEE, 27–34.
[39]Yi Liu, Jue Wang, Chang Xu, Xiaoxing Ma, and Jian Lü. 2018. NavyDroid: an
efficient toolof energy inefficiencyproblem diagnosis forAndroid applications.
Science China Information Sciences 61, 5 (2018), 050103.
[40]YepangLiu,ChangXu,Shing-ChiCheung,andJianLü.2014. Greendroid:Au-
tomated diagnosis of energy inefficiency for smartphone applications. IEEE
Transactions on Software Engineering 40, 9 (2014), 911–940.
[41]Aravind Machiry, Rohan Tahiliani, and Mayur Naik. 2013. Dynodroid: An input
generation system for android apps. In Proceedings of the 2013 9th Joint Meeting
on Foundations of Software Engineering. 224–234.
[42]RiyadhMahmood,NarimanMirzaei,andSamMalek.2014. Evodroid:Segmented
evolutionarytestingofandroidapps.In Proceedingsofthe22ndACMSIGSOFT
International Symposium on Foundations of Software Engineering. 599–609.
[43]KeMao,MarkHarman,andYueJia.2016.Sapienz:Multi-objectiveautomatedtest-
ing for Android applications. In Proceedings of the 25th International Symposium
on Software Testing and Analysis. 94–105.
[44]KeMao,MarkHarman,andYueJia.2017.Crowd intelligenceenhancesautomated
mobile testing. In 2017 32nd IEEE/ACM International Conference on Automated
Software Engineering. IEEE, 16–26.
[45]AtifMemon,Ishan Banerjee, andAdithyaNagarajan.2003. GUIripping: Reverse
engineering of graphical user interfaces for testing. In 10th Working Conference
on Reverse Engineering, 2003. WCRE 2003. Proceedings. Citeseer, 260–269.
[46]Guozhu Meng, Yinxing Xue, Chandramohan Mahinthan, Annamalai Narayanan,
Yang Liu, Jie Zhang, and Tieming Chen. 2016. Mystique: Evolving android
malware for auditing anti-malware tools. In Proceedings of the 11th ACM on Asia
conference on computer and communications security. 365–376.
[47]Nariman Mirzaei, Hamid Bagheri, Riyadh Mahmood, and Sam Malek. 2015. Sig-
droid: Automated system input generation for android applications. In 2015 IEEE
26thInternationalSymposiumonSoftwareReliabilityEngineering.IEEE,461–471.
[48]NarimanMirzaei,JoshuaGarcia,HamidBagheri,AlirezaSadeghi,andSamMalek.
2016. Reducing combinatorics in GUI testing of android applications. In 2016
IEEE/ACM 38th International Conference on Software Engineering. IEEE, 559–570.
[49]Bao N Nguyen and Atif M Memon. 2014. An observe-model-exercise* paradigm
to test event-driven systems with undetermined input spaces. IEEE Transactions
on Software Engineering 40, 3 (2014), 216–234.
[50]Jacinto Reis and Alexandre Mota. 2018. Aiding exploratory testing with pruned
GUI models. Inform. Process. Lett. 133 (2018), 49–55.
[51]Issue report. 2019. Issue 128 of CallMeter. Retrieved June 29, 2019 from
https://github.com/felixb/callmeter/issues/128
479[52]Issuereport.2019. Issue480ofAnyMemo. RetrievedJune29,2019from https:
//github.com/helloworld1/AnyMemo/issues/480
[53]Issuereport.2019. Issue503ofAnyMemo. RetrievedJune29,2019from https:
//github.com/helloworld1/AnyMemo/issues/503
[54]Issue report. 2019. Issue 90 of Aard2. Retrieved June 29, 2019 from https:
//github.com/itkach/aard2-android/issues/90
[55]WeiSong,XiangxingQian,andJeffHuang.2017. EHBDroid:beyondGUItesting
for Android applications. In 2017 32nd IEEE/ACM International Conference on
Automated Software Engineering. IEEE, 27–37.
[56]Ting Su, Guozhu Meng, Yuting Chen, Ke Wu, Weiming Yang, Yao Yao, Geguang
Pu,YangLiu,andZhendongSu.2017. Guided,stochasticmodel-basedGUItesting
ofAndroidapps.In Proceedingsofthe201711thJointMeetingonFoundationsof
Software Engineering. 245–256.
[57]JueWang,YanyanJiang,ChangXu,QiweiLi,TianxiaoGu,JunMa,XiaoxingMa,
and Jian Lu. 2018. AATT+: Effectively manifesting concurrency bugs in Android
apps.Science of Computer Programming 163 (2018), 1–18.
[58]JueWang,YepangLiu,ChangXu,XiaoxingMa,andJianLu.2016. E-greenDroid:
effective energy inefficiency analysis for android applications. In Proceedings of
the 8th Asia-Pacific Symposium on Internetware. 71–80.
[59]Wenyu Wang, Dengfeng Li, Wei Yang, Yurui Cao, Zhenwen Zhang, Yuetang
Deng, and Tao Xie. 2018. An empirical study of android test generation tools in
industrial cases. In Proceedings of the 33rd ACM/IEEE International Conference on
Automated Software Engineering. 738–748.
[60]RenatoWerneck,JoaoSetubal,andArlindodaConceicao.2000.Findingminimum
congestion spanning trees. Journal of Experimental Algorithmics 5 (2000), 11–es.
[61]Xiangyu Wu, Yanyan Jiang, Chang Xu, Chun Cao, Xiaoxing Ma, and Jian Lu.2016. Testing android apps via guided gesture event generation. In 2016 23rd
Asia-Pacific Software Engineering Conference. IEEE, 201–208.[62]Guangliang Yang and Jeff Huang. 2018. Automated generation of event-oriented
exploitsinandroidhybridapps.In Proc.oftheNetworkandDistributedSystem
Security Symposium.
[63]Kun Yang, Jianwei Zhuge, Yongke Wang, Lujue Zhou, and Haixin Duan. 2014.
IntentFuzzer: detecting capability leaks of android applications. In Proceedings of
the9thACMsymposiumonInformation,computerandcommunicationssecurity.
531–536.
[64]Hui Ye, Shaoyin Cheng, Lanbo Zhang, and Fan Jiang. 2013. Droidfuzzer: Fuzzing
the android apps with intent-filter tag. In Proceedings of International Conference
on Advances in Mobile Computing & Multimedia. 68–74.
[65]Chao Chun Yeh, Han Lin Lu, Chun Yen Chen, Kee Kiat Khor, and Shih Kun
Huang.2014. Craxdroid:Automaticandroidsystemtestingbyselectivesymbolic
execution. In 2014 IEEE Eighth International Conference on Software Security and
Reliability-Companion. IEEE, 140–148.
[66]Xun Yuan and Atif M Memon. 2008. Alternating GUI test generation and execu-
tion.InTesting:Academic&IndustrialConference-PracticeandResearchTechniques
(taic part 2008). IEEE, 23–32.
[67]Xun Yuan and Atif M Memon. 2009. Generating event sequence-based test cases
using GUI runtime state feedback. IEEE Transactions on Software Engineering 36,
1 (2009), 81–95.
[68]Shengtao Yue, Weizan Feng, Jun Ma, Yanyan Jiang, Xianping Tao, Chang Xu,
and Jian Lu. 2017. RepDroid: an automated tool for Android application repack-
aging detection. In 2017 IEEE/ACM 25th International Conference on Program
Comprehension. IEEE, 132–142.
[69]Li Lyna Zhang, Chieh-Jan Mike Liang, Yunxin Liu, and Enhong Chen. 2017.
Systematicallytestingbackgroundservicesofmobileapps.In 201732ndIEEE/ACM
International Conference on Automated Software Engineering. IEEE, 4–15.
480