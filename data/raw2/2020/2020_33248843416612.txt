Defect Prediction Guided Search-Based Software Testing
Anjana Perera
Anjana.Perera@monash.edu
Faculty of Information Technology
Monash University
Melbourne, AustraliaAldeida Aleti
Aldeida.Aleti@monash.edu
Faculty of Information Technology
Monash University
Melbourne, Australia
Marcel BÃ¶hme
marcel.boehme@acm.org
Faculty of Information Technology
Monash University
Melbourne, AustraliaBurak Turhan
Burak.Turhan@monash.edu
Faculty of Information Technology
Monash University
Melbourne, Australia
ABSTRACT
Today,mostautomatedtestgenerators,suchassearch-basedsoft-
waretesting(SBST)techniquesfocusonachievinghighcodecov-
erage.However,highcodecoverageisnotsufficienttomaximise
the number of bugs found, especially when given a limited testing
budget.Inthispaper,weproposeanautomatedtestgenerationtech-
nique that isalso guided bythe estimated degree ofdefectiveness
ofthesourcecode.Partsofthecodethatarelikelytobemorede-
fectivereceivemoretestingbudgetthanthelessdefectiveparts.To
measure the degree of defectiveness, we leverage Schwa, a notable
defect prediction technique.
We implement our approach into EvoSuite, a state of the art
SBSTtoolforJava.OurexperimentsontheDefects4Jbenchmark
demonstrate the improved efficiency of defect prediction guided
test generation and confirm our hypothesis that spending more
time budget on likely defective parts increases the number of bugs
found in the same time budget.
CCS CONCEPTS
â€¢Software and its engineering â†’Software testing and de-
bugging; Search-based software engineering.
KEYWORDS
search-based software testing, automated test generation, defect
prediction
ACM Reference Format:
Anjana Perera, Aldeida Aleti, Marcel BÃ¶hme, and Burak Turhan. 2020. De-
fectPredictionGuidedSearch-BasedSoftwareTesting.In 35thIEEE/ACM
International Conference on Automated Software Engineering (ASE â€™20), Sep-
tember 21â€“25, 2020, Virtual Event, Australia. ACM, New York, NY, USA,
13 pages. https://doi.org/10.1145/3324884.3416612
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
onthe firstpage.Copyrights forcomponentsof thisworkowned byothersthan the
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/or a fee. Request permissions from permissions@acm.org.
ASE â€™20, September 21â€“25, 2020, Virtual Event, Australia
Â© 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-6768-4/20/09...$15.00
https://doi.org/10.1145/3324884.34166121 INTRODUCTION
Software testing is a crucial step in improving software quality.
Findingeffectivetestcases,however,isacomplextask,whichisbe-
comingevenmoredifficultwiththeincreasingsizeandcomplexityofsoftwaresystems.Automatedsoftwaretestingmakesthislabour
intensivetaskeasierforhumansbyautomaticallygeneratingtest
cases for a software system. In particular, search based software
testing(SBST)techniques[ 36]havebeenverysuccessfulinauto-
matically generating test cases, and are widely used not only in
academia, but also in the industry (e.g., Facebook [4, 44]).
SBSTtechniquesusesearchmethodssuchasgeneticalgorithms[ 1,
50] to find high quality test cases for a particular system. These
methods focus on code coverage, and research shows that SBST
methodsareveryeffectiveatachievinghighcoverage[ 2,49,51,52].
They can even cover more code than the manually written test
cases[28,58].However,atestsuitewithhighcodecoveragedoes
notnecessarilyimplyeffectivebugdetectionbythetestsuite.In-
deed,previousstudiesshowthatSBSTmethodsarenotaseffective
in finding real bugs [ 3,59]. Even EvoSuite [ 19] â€“ a state of the
art SBST tool â€“ could only find on average 23% of the bugs fromthe Defects4J dataset [
39], which contains 357 bugs from 5 java
projects [ 59]. Ideally, SBST techniques should aim at generating
test cases that reveal bugs, however this is a difficult task since
duringthesearchfortestcasesitisnotpossibletoassessifatest
case has found a bug (e.g., semantic bugs). In this paper, we aimto enhance SBST techniques by incorporating information fromadefectpredictionalgorithmtoinformthesearchprocessofthe
areasinthesoftwaresystemthatarelikelytobedefective.Thus,
the SBST technique, while it cannot tell whether the test cases itproducesareindeedfindingbugs,itisabletogeneratemoretest
casesforthedefectiveareas,andasaresult,increasesthelikelihood
of finding the bugs.
Defect prediction algorithms [ 42] estimate the likelihood that
a file [13,14,42], class [8] or method/function [ 10,32,37]i na
softwaresystemisdefective.Thesemethodsareveryeffectiveat
identifyingthelocationofbugsinsoftware[ 10,47,54].Asaresultof
their efficacy, defect prediction models are used to help developers
focustheir limitedtesting effortoncomponents thatare themost
likelytobedefective[ 13].Inaddition,defectpredictionhasbeen
used to inform a test case prioritisation strategy, G-clef [ 54], of the
classesthatarelikelytobebuggy,andfounditispromising.Our
4482020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE)
workisthefirsttousedefectpredictionforimprovingautomated
test case generation.
We introduce defect prediction guided SBST (SBST ğ·ğ‘ƒğº), which
usesinformationfromadefectpredictortofocusthesearchtowards
thedefectiveareasinsoftware ratherthanspendingtheavailable
computational resources (i.e.,time budget) to cover non-defective
areas.Weemploy Schwa[ 14,54] asthe defectprediction approach,
whichcalculatesthedefectscoresbasedonthechangehistoryof
the Java classes. Next, a budget allocation algorithm, called Budget
AllocationBasedonDefectScores(BADS)allocatesthetimebudget
foreachclassbasedonthepredictionsgivenbythedefectpredictor.
At a high level, it follows the basic and intuitive rule; highly likely
tobedefectiveclassesgetahighertimebudgetallocatedandless
likely to be defective classes get a lower time budget. Finally, we
useğ·ğ‘¦ğ‘›ğ‘ğ‘€ğ‘‚ğ‘†ğ´ [52],astateoftheartSBSTalgorithm,togenerate
test suites for each class in the project by spending the allocated
time budgets.
Real-worldprojectsareusuallyverylargeandtherecanbeeven
more than 1,000 classes in a project. Hence, it takes significant
amountofresources(e.g.,time)torunthesetestgenerationtools
foreachclassintheproject.Atthesametime,theavailablecompu-
tational resources are often limited in practice [ 11]. Therefore, it
isnecessarytooptimallyutilisetheavailableresources(e.g.,time
budget)togeneratetestsuitesfortheprojectswithmaximalbug
detection. Existing SBST approaches allocate the available time
budget equally for each class in the project [ 23,24]. Usually, most
classesareclean,hencewearguethatthisisasub-optimalstrategy.
Ourproposedapproachaddressesthisbyallocatingtheavailable
time budget to each class in the project based on the class level
defect prediction information.
We evaluate how our approach performs in terms of the effi-
ciencyinfindingrealbugscomparedtothestateoftheart.Second,
we examine if our approach finds more unique bugs. This is partic-
ularlyimportanttoinvestigate,asitwillrevealifusinginformation
fromthedefectpredictioncanhelpSBSTrevealbugsthatcannot
be found otherwise. We evaluate SBST ğ·ğ‘ƒğºon 434 reported real
bugsfrom6opensourcejavaprojectsintheDefects4Jdataset.Our
empiricalevaluationdemonstratesthatinaresourceconstrained
environment,whengivenatighttimebudget,SBST ğ·ğ‘ƒğºissignif-
icantly more efficient than the state of the art SBST with a large
effect size. In particular, SBST ğ·ğ‘ƒğºfinds up to 13.1% more bugs
on average compared to the baseline approach. In addition, our
approach is also able to expose more unique bugs which cannot be
found by the state-of-the-art approach.
Insummary,thecontributionofthispaperisanovelapproach
thatcombinesdefectpredictionandSBSTtoimprovethebugde-
tectioncapabilityofSBSTbyfocusingthesearchmoretowardsthe
defective areas in software. In addition, we present an empirical
evaluationinvolving434realbugsfrom6opensourcejavaprojects
(which took roughly 34,600 hours) that demonstrates the efficiency
of our proposed solution. Finally, the source code of our proposed
techniqueandthescriptsforpostprocessingtheresultsarepublicly
available here: https://github.com/SBST-DPG2 RELATED WORK
2.1 Search Based Software Testing
Search based software testing (SBST) is an effective strategy for
achieving high code coverage [ 51â€“53]. Shamshiri et al. [ 59] and
Almasi et al. [ 3] studied the bug detection performance of SBST on
opensourceandindustrialsoftwarerespectively.WhileEvoSuite
[19],whichweconsiderasthestateoftheartSBSTtoolgivenits
maturity, found more bugs than the other techniques used in their
studies, overall the results show that the bug detection is still a
significantchallengeforSBST.Particularly,EvoSuitefoundonlyan
average of 23% bugs from the Defects4J dataset [ 59]. It is clear that
usingonlythe100%branchcoveragecriterionwasnotsufficient
to search for test cases that can detect the bugs. In contrast, we
usedefectpredictioninformationtofocusthesearchtoextensively
explore the search space for test cases in defective areas.
Gay[31]studiedtheeffectofcombiningcoveragecriteriaonthe
bug detection performance of SBST, and found that multiple cover-
agecriteriaoutperformasinglecriterion.However,theauthorsdid
not recommenda generalstrategy toselect whichcriteria tocom-
bine,sincetheirselectionstrategiesalsoproducedmanyineffective
combinations. Our work is the first approach that focuses on in-
formingSBSTofthedefectiveareastospendmoresearchresources
to such areas. Thus, we believe our approach will further improve
the bug detection capability of the single criterion or combination
of criteria.
2.2 Defect Prediction
Previousworkondefectpredictionhaveconsideredawiderangeofmetricssuchascodesize[
45],codecomplexity[ 61],object-oriented
[8], organisational [ 47] and change history [ 46] to predict future
defects in a software project. Graves et al. [ 34] showed that the
number of changes and particularly the recent changes to the code
areeffectiveindicatorsoffuturedefects.Kimetal.[ 40]followedthe
observationthatbugsoccurinsoftwarechangehistoryasbursts,
hence they argue that recent changes to the code and recent faults
in the code are likely to introduce bugs in the future.
Rahmanetal.[ 56]proposedasimpleapproach,whichwaseven-
tually implemented by the Google Engineering team [ 42,43], that
ordersfilesbythenumberofbugfixcommitsinafile,andfoundoutthat its performance is quite similar to the more complex approach
FixCache[ 40].Furthermore,theyshowedthatthefilesthathave
been recently involved in a bug fix are likely to contain further
bugs.Patersonetal.[ 54]usedanenhancedversionofthisapproach
as the defect predictor to inform a test case prioritisation strategy
of the classes that are likely to be buggy, and found it is promising.
Inparticular,theyusedSchwa[ 14],whichpredictsdefectsinpro-
gramsbyusingthreemetrics;recentchanges,recentbugfixes,and
recent new authors [47] to the code.
2.3 Budget Allocation Problem
Search based software test generation tools like EvoSuite generate
test suites for each class in the project separately. This is doneby running a search method such as genetic algorithm (GA) foreach class to maximise statement, branch, and method coverage,
oracombinationofthethree.Oneofthecrucialparametersthat
449hastobe tuned isthetimebudgetfor eachclass,whichisusedas
a stopping criterion for the GA. Allocating a higher time budget
allowsthesearchmethodtoextensivelyexplorethesearchspace
of possible test inputs, thus increasing the probability of finding
the optimum.
Forsmallprojects,itisfeasibletorunautomatedtestgeneration
individually for each class in the project. Real-world projects, how-
ever, are usually very large, e.g., a modern car has millions of lines
of code and thousands of classes [ 9], and they require a significant
amount of resources (e.g., time) to run the test generation toolsfor each class in the project. Even in an open source project like
Apache Commons Math [
16], there are around 800 classes. In a
projectlikethis,itwouldtakeatleast13-14hourstorunautomatedtestgenerationwithspendingjustoneminutepereachclass.Atthe
same time, the available computational resources are often limited
inpractice[ 11].Thereforethisraisesthequestion,â€˜Howshouldwe
optimallyutilisetheavailablecomputationalresources(e.g.,time
budget) to generate test suites for the whole project with maximal
bug detection?â€™.
Previous work on bug detection performance of SBST [ 3,31,59]
allocated a fix time budget to test generation for each buggy class.
Since the buggy classes are not known prior to running tests, in
practice all the classes in the project have to be allocated the same
timebudget.Usually,mostclassesarenotbuggy,henceweargue
thatthisisasub-optimalstrategy.Ourapproachsolvesthisproblembyallocatingtimebudgettoclassesbasedontheinformationgiven
by a defect predictor.
Campos et al. [ 11] proposed a budget allocation based on the
complexityoftheclassesinordertomaximisethebranchcoverage.In particular, they used number of branches in a class as a proxy to
thecomplexityoftheclass.Incontrast,thescopeofthisresearch
is to maximise the number of bugs detection.
Contrary to the previous works [ 3,11,31,59] that considered
test suite generation for a regression testing scenario, we focus on
generating tests to find bugs not only limited to regressions, but
also the bugs that are introduced to the system at various times.
3 DEFECT PREDICTION GUIDED
SEARCH-BASED SOFTWARE TESTING
DefectPredictionGuidedSBST(SBST ğ·ğ‘ƒğº)(seeFigure1)usesdefect
scores of each class produced by a defect predictor to focus thesearch towards the defective areas of a program. Existing SBST
approachesallocatetheavailabletimebudgetequallyforeachclass
intheproject[ 23,24,52,59].Usually,mostclassesarenotbuggy,
hence we argue that this is a sub-optimal strategy. Ideally, valuable
resources should be spent in testing classes that are likely to be
buggy,henceweemployadefectpredictor,knownasSchwa[ 14],
to calculate the likelihood that a class in a project is defective.
Our approach has three main modules: i) Defect Predictor (DP), ii)
BudgetAllocationBasedon DefectScores(BADS),andiii)Search-
Based Software Testing (SBST).
3.1 Defect Predictor
The defect predictor gives a probability of defectiveness (defect
score) for each class in the project. The vector srepresents this
output.InourimplementationofSBST ğ·ğ‘ƒğº,wechoose(a)thelevelofgranularityofthedefectpredictortobetheclasslevel,and(b)
the Schwa [14] as the defect predictor module.
Paterson et al. [ 54] successfully applied Schwa as the defect
predictorinG-cleftoinformatestcaseprioritisationstrategyoftheclassesthatarelikelytobebuggy.Certainlyotherdefectprediction
approaches proposed in the literature (e.g., FixCache [ 40], Change
Bursts [48]) would also be suitable for the task at hand. A strength
of Schwa is its simplicity, and that it does not require traininga classifier which makes it easy to apply to an industrial setting
where training data is not always available (like the one we study).
In addition, Schwa can be considered as an enhancement of a tool
implemented by the Google Engineering team [42, 43].
Schwausesthefollowingthreemeasureswhichhavebeenshown
to be effective at producing defect predictions in the literature (see
Section2.2);i) Revisions -timestampsof revisions(recentchanges
are likely to introduce faults), ii) Fixes- timestamps of bug fix
commits(recentbugfixesarelikelytointroducenewfaults),and
iii)Authors-timestampsofcommitsbynewauthors(recentchanges
by the new authors are likely to introduce faults). The Schwa tool
extracts this information through mining a version control system
such as Git [ 33]. The tool is readily available to use as a python
packageatPypi[ 29].Therefore,giventherobustnessofthistool
anditsapproach,wedecidetouseitasthedefectpredictormodule
in our approach.
Schwa [30] starts with extracting the three metrics; Revisions
(ğ‘…ğ‘),Fixes(ğ¹ğ‘), andAuthors(ğ´ğ‘) for all classes ğ‘âˆˆğ¶in the project.
Foreach timestamp,it calculatesa timeweightedrisk (TWR)[42]using the Equation (1).
ğ‘‡ğ‘Šğ‘…(ğ‘¡
ğ‘–)=1
1+exp(âˆ’12ğ‘¡ğ‘–+2+(1âˆ’ğ‘‡ğ‘…)âˆ—10)(1)
The quantity ğ‘¡ğ‘–is the timestamp normalised between 0 and 1,
where0isthenormalisedtimestampoftheoldestcommitunder
consideration and 1 is the normalised timestamp of the latest com-
mit. The number of commits that Schwa tracks back in versionhistory of the project (
ğ‘›) is a configurable parameter and it can
take values from one commit to all the commits. The parameter
ğ‘‡ğ‘…âˆˆ[0,1]is called the Time Range and it allows to change the
importance given to the older commits. The time weighted risk
formulascoresrecenttimestampshigherthantheolderones(see
Figure 2).
Once Schwa calculated the TWRs, it aggregates these TWRs per
each metric,and calculatesa weightedsum ğ‘ ğ‘for eachclass ğ‘âˆˆğ¶
in the project as in Equation (2).
ğ‘ ğ‘=ğ‘¤ğ‘Ÿâˆ—/summationdisplay.1
ğ‘¡ğ‘–âˆˆğ‘…ğ‘ğ‘‡ğ‘Šğ‘…(ğ‘¡ğ‘–)+ğ‘¤ğ‘“âˆ—/summationdisplay.1
ğ‘¡ğ‘–âˆˆğ¹ğ‘ğ‘‡ğ‘Šğ‘…(ğ‘¡ğ‘–)
+ğ‘¤ğ‘âˆ—/summationdisplay.1
ğ‘¡ğ‘–âˆˆğ´ğ‘ğ‘‡ğ‘Šğ‘…(ğ‘¡ğ‘–)(2)
The sum/summationtext.1
ğ‘¡ğ‘–âˆˆğ‘…ğ‘ğ‘‡ğ‘Šğ‘…(ğ‘¡ğ‘–)is the total of the time weighted risks
of theRevisions metric for class ğ‘. Similarly,/summationtext.1
ğ‘¡ğ‘–âˆˆğ¹ğ‘ğ‘‡ğ‘Šğ‘…(ğ‘¡ğ‘–)and/summationtext.1
ğ‘¡ğ‘–âˆˆğ´ğ‘ğ‘‡ğ‘Šğ‘…(ğ‘¡ğ‘–)arethesumsoftheTWRsofthe FixesandAuthors
metrics for class ğ‘âˆˆğ¶. The quantities ğ‘¤ğ‘Ÿ,ğ‘¤ğ‘“, andğ‘¤ğ‘are weights
that modify the TWR sum of each metric and their sum is equal to
1. The weighted sum, ğ‘ ğ‘, is called the score of class ğ‘âˆˆğ¶.
450Project
Schwa: Defect
PredictorDefect
ScoresTime
Budgets
EvoSuite:
SBST Test SuiteBudget Allocation 
Based on 
Defect Scores
(BADS)
Figure 1: Defect Prediction Guided SBST Overview
                 
1RUPDOLVHG7LPHVWDPS7LPH:HLJKWHG5LVN
Figure 2: Time Weighted Risk ( ğ‘‡ğ‘…= 0.4)
Finally,theSchwatoolestimatestheprobability ğ‘(ğ‘)ofthata
classğ‘is defective as given in Equation (3).
ğ‘(ğ‘)=1âˆ’exp(âˆ’ğ‘ ğ‘) (3)
Inthispaper,werefertothisprobabilityofdefectiveness ğ‘(ğ‘)
as the defect score of class ğ‘âˆˆğ¶.
3.2 Budget Allocation Based on Defect Scores
Budget Allocation Based on Defect Scores (BADS) takes the defect
scores( s={ğ‘(ğ‘)|ğ‘âˆˆğ¶})asinputanddecidesonhowtoallocatethe
availabletimebudgettoeachclassbasedonthesescores,producing
a vectortas output. Ideally, all the defective classes in the project
should get more time budget while non-defective classes can be
left out from test generation. However, the defect predictor only
givesanestimationoftheprobabilityofdefectiveness.Therefore,
BADSallocatesmoretimebudgettothehighlylikelytobedefective
classesthantothelesslikelytobedefectiveclasses.Thiswaywe
expectSBSTtogethighertimebudgettoextensivelyexplorefor
test cases in defective classes rather than in non-defective ones.
3.2.1 Exponential Time Budget Allocation Based on Defect Scores.
Algorithm1illustratestheproposedtimebudgetallocationalgo-
rithm of BADS, where sis the set of defect scores of the classes, ğ‘‡
isthetotaltimebudgetfortheproject, ğ‘¡ministheminimumtime
budgettobeallocatedforeachclass, ğ‘‡ğ·ğ‘ƒisthetimespentbythe
defect predictor module, and ğ‘’ğ‘,ğ‘’ğ‘, andğ‘’ğ‘are parameters of the
exponentialfunctionthatdefinetheshapeoftheexponentialcurve.
tis the set of time budgets allocated for the classes.Algorithm 1: ExponentialTime Budget AllocationBased
on Defect Scores
Input :The set of all the classes ğ¶, where ğ‘=|ğ¶|
:s={ğ‘ 1,ğ‘ 2,...,ğ‘  ğ‘}
:ğ‘‡,ğ‘¡ğ‘šğ‘–ğ‘›,ğ‘‡ğ·ğ‘ƒ
:ğ‘’ğ‘,ğ‘’ğ‘,ğ‘’ğ‘
Output:t ={ğ‘¡1,ğ‘¡2,...,ğ‘¡ ğ‘}
1râ†Assign-Rank(s)
2r/primeâ†Normalise-Rank(r)
3w/primeâ†âˆ…
4forallğ‘ğ‘–âˆˆğ¶do
5ğ‘¤/prime
ğ‘–â†ğ‘’ğ‘+ğ‘’ğ‘âˆ—exp(ğ‘’ğ‘âˆ—ğ‘Ÿ/prime
ğ‘–)
6wâ†Normalise-Weight(w/prime)
7tâ†âˆ…
8forallğ‘ğ‘–âˆˆğ¶do
9ğ‘¡ğ‘–â†ğ‘¤ğ‘–âˆ—(ğ‘‡âˆ’ğ‘âˆ—ğ‘¡minâˆ’ğ‘‡ğ·ğ‘ƒ)+ğ‘¡min
10
ThedefectscoresassignmentinFigure3isagoodexampleof
the usual defect score distribution by a defect predictor. Usually,
thereareonlyafewclasseswhichareactuallybuggy.Allocating
higher time budgets for these classes would help maximise the bug
detection of the test generation tool. Following this observationand the results of our pilot runs, we use an exponential function
(line 5 in Algorithm 1) to highly favour the budget allocation for
the few highly likely to be defective classes.
Moreover, there is relatively higher number of classes which are
moderately likely to be defective (e.g., 0.5 < defect score < 0.8). It is
alsoimportanttoensurethereissufficienttimebudgetallocatedfor these classes. Otherwise, neglecting test generation for these
classes could negatively affect bug detection of the test generation
tool. We introduce a minimum time budget, ğ‘¡min, to all the classes
because we want to ensure that every class gets a budget allocated
regardless of the defectiveness predicted by the defect predictor.
The exponential function in Algorithm 1 together with ğ‘¡minallow
an adequate time budget allocation for the moderately likely to be
defective classes.
Upon receiving the defect scores ( s), BADS assigns ranks ( r) for
all the classes according to the defect scores. Next, the Normalise-
Rankfunctionnormalisestheranksintherange[0,1],wherethe
rankofthemostlikelytobedefectiveclassis0andtheleastlikelyto
451Defect ScoreFrequency
0.0 0.2 0.4 0.6 0.8 1.00 50 100 150
Figure3:DistributionofthedefectscoresassignedbySchwa
for the classes in Chart-9 bug from Defects4J.
be defective class is 1. Then, each class gets a weight ( ğ‘¤/prime
ğ‘–) assigned
based on its normalised rank by the exponential function. The
amount of time budget allocated to class ğ‘ğ‘–is proportional to ğ‘¤/prime
ğ‘–.
The parameters ğ‘’ğ‘,ğ‘’ğ‘, andğ‘’ğ‘have to be carefully selected such
that the weights are almost equal and significantly small for the
lower-ranked classes, and the difference between the weights ofadjacently ranked classes rapidly increases towards the highly-
rankedclasses.TheNormalise-Weightfunctionnormalisesthe
weights to the range [0,1], ensuring the summation is equal to
1, and produces the normalised weights vector w. Finally, BADS
allocatestimebudgetforeachclassfromtheremainingavailable
time budget, ğ‘‡âˆ’ğ‘âˆ—ğ‘¡minâˆ’ğ‘‡ğ·ğ‘ƒ, based on its normalised weight
(line 9 in Algorithm 1) .
3.2.2 The 2-Tier Approach. According to the defect predictor out-
come,almostalltheclassesintheprojectgetnon-zerodefectscoresattachedtothem.Thisgivestheimpressionthatalltheseclassescan
bedefectivewithatleastaslightprobability.However,inreality,
thisdoesnotholdtrue.Foragivenprojectversion,thereareonly
afewdefectiveclasses.Adefectpredictorislikelytopredictthat
clean classesare alsodefective witha non-zero probability.While
the exponential function disfavours the budget allocation for these
less likely to be defective classes, ğ‘¡minguarantees a minimum time
budget allocated to them. If we decrease ğ‘¡minin order to make the
budgetallocationnegligibleforthe likelytobecleanclasses,then
it would risk a sufficient time budget allocation for the moderately
likely to be defective classes.
Weproposethe2-Tierapproachwhichdividestheprojectinto
two tiers following the intuition that only a set of classes are de-fective in a project. BADS sorts the classes into two tiers before
the weights assignment, such that the highly likely to be defective
classes are in the first tierand the less likely to be defective classes
are in the second tier. This allowsto further discriminate the less
likely to be defective classes, and favour the highly likely to bedefective classes by simply allocating only a smaller fraction ofthe total time budget to the second tier and allocating the rest to
thefirsttier.Section4.4.3providesmoredetailsontheparameter
selection of the 2-Tier approach.3.3 Search Based Software Testing
WeuseEvoSuite[ 19]asthesearch-basedsoftwaretesting(SBST)
moduleinourdefectpredictionguidedSBSTapproach.EvoSuite
is an automated test generation framework that generates JUnittest suites for Java classes. It was first proposed by Fraser and
Arcuri [
19] in 2011, since then it has gained growing interest in
theSBSTcommunity[ 6,51,52,57,59].Itseffectivenesshasbeen
evaluatedonopensourceandaswellasindustrialsoftwareprojects
in terms of the code coverage [ 23,51â€“53,57] and bug detection
[3,59]. Furthermore, EvoSuite won 6 out of 7 of the SBST unit
testing tool competitions [ 12,21,22,25â€“27]. To date, EvoSuite is
actively maintained, and its source code and releases are readilyavailable to use at GitHub [
15] and their website [ 18]. Therefore,
given the maturity of EvoSuite, we decide to use it as the SBST
module in our approach.
Morerecently,Panichellaetal.[ 52]developedanewsearchal-
gorithm, ğ·ğ‘¦ğ‘›ğ‘ğ‘€ğ‘‚ğ‘†ğ´ (DynamicMany-ObjectiveSortingAlgorithm ),
as an extension to EvoSuite, which stands as the current state of
the art. It has been shown to be effective at achieving high branch,
statement and strong mutation coverage than the previous ver-sions of EvoSuite ([
20,51,57]) [52]. Moreover, ğ·ğ‘¦ğ‘›ğ‘ğ‘€ğ‘‚ğ‘†ğ´ was
thesearchalgorithmofEvoSuite,whichwontheunittestingtool
competitionatSBST2019[ 12].Therefore,weuse ğ·ğ‘¦ğ‘›ğ‘ğ‘€ğ‘‚ğ‘†ğ´ as
the search algorithm in EvoSuite.
4 DESIGN OF EXPERIMENTS
We evaluate our approach in terms of its efficiency in finding bugs,
andtheeffectivenessinrevealinguniquebugs,i.e.,bugsthatcannot
befoundbythebenchmarkapproach.Ourfirstresearchquestion
is:
RQ1:IsSBST ğ·ğ‘ƒğºmoreefficientinfindingbugscomparedtothestate
of the art?
Toanswerthisresearchquestion,werunasetofexperimentswhere
wecompareourapproachagainstthebaselinemethoddiscussed
in Section 4.3. All methods are employed to generate test casesfor Defects4J [
38], which is a well-studied benchmark of buggy
programsdescribedinSection4.2.Oncethetestcasesaregenerated,
we check if they find the bugs in the programs, and report the
resultsasthemeanandmedianover20runs.Tocheckforstatistical
significanceofthedifferencesandtheeffectsize,weemploytwo-
tailed non-parametric Mann-Whitney U-Test with the significance
level(ğ›¼)0.05[5]andVarghaandDelaneyâ€™s /hatwideğ´12statistic[ 60].We
also plot the results as boxplots to visualise their distribution.
Inaddition,toanalysetheeffectivenessoftheproposedapproach,
we seek to answer the following research question:
RQ2: Does SBST ğ·ğ‘ƒğºfind more unique bugs?
To answer this research question, we analyse the results from
theexperimentsinmoredetail.Whilethefirstresearchquestion
focusesontheoverallefficiency,inthesecondresearchquestionweaim to understand if SBST
ğ·ğ‘ƒğºis capable of revealing more unique
bugs which can not be exposed by the baseline method. Part of the
efficiency of our proposed method, however, could be due to its
robustness,whichismeasuredbythesuccessrate,hencewealso
report how often a bug is found over 20 runs.
4524.1 Time Budget
In real world scenario, total time budget reserved for test gener-
ation for a project depends on how it is used in the industry. For
example,aprojecthavinghundredsofclassesandrunningSBST1-2minutesperclasstakesseveralhourstofinishtestgeneration.Ifan
organisation wants to adaptSBST in their continuous integration
(CI)system[ 17],thenithastosharetheresourcesandschedules
withtheprocessesalreadyinthesystem;regressiontesting,code
quality checks, project builds etc. In such case, it is important that
SBST uses minimal resources possible, such that it does not idle
other jobs in the system due to resource limitations.
Panichella et al. [ 52] showed that DynaMOSA is capable of con-
verging to the final branch coverage quickly, sometimes with a
lowertimebudgetlike20seconds.Thisisparticularlyimportant
since faster test generation allows more frequent runs and thereby
itmakesSBSTsuitabletofitintotheCI/CDpipeline.Therefore,we
decide that 30 seconds per class is an adequate time budget for test
generationand15secondsperclassisatighttimebudgetinausual
resource constrainedenvironment. Weconduct experimentsfor 2
cases of total time budgets ( ğ‘‡); 15âˆ—ğ‘and 30âˆ—ğ‘seconds.
4.2 Experimental Subjects
WeusetheDefects4Jdataset[ 38,39]asourbenchmark.Itcontains
434realbugsfrom6real-worldopensourceJavaprojects.Were-
move4bugsfromtheoriginaldataset[ 38]sincetheyarenotrepro-
ducibleunderJava8,whichisrequiredbyEvoSuite.Theprojects
are JFreeChart (26 bugs), Closure Compiler (174 bugs), Apache
commons-lang(64bugs),Apachecommons-math(106bugs),Mock-
ito (38bugs) andJoda-Time (26 bugs).For eachbug, the Defects4J
benchmark gives a buggy version and a fixed version of the pro-
gram. The difference between these two versions of the program isthe applied patch to fix the bug, which indicates the location of the
bug. The Defects4J benchmark also provides a high-level interface
to perform tasks like running the generated tests against the other
version ofthe program (buggy/fixed)to check ifthe tests areable
to find the bug, fixing the flaky test suites etc. [38].
Defects4J is widely used for research on automated unit test
generation [ 59], automated program repair [ 41], fault localisation
[55],testcaseprioritisation[ 54],etc.ThismakesDefects4Jasuitable
benchmark for evaluating our approach, as it allows us to compare
our results to existing work.
4.3 Baseline Selection
WeusethecurrentstateoftheartSBSTalgorithm, ğ·ğ‘¦ğ‘›ğ‘ğ‘€ğ‘‚ğ‘†ğ´ [52],
with equal time budget allocation, SBST ğ‘›ğ‘œğ·ğ‘ƒğº, as our baseline for
comparison. Previous work on bug detection capability of SBST
allocatedanequaltimebudgetforalltheclasses[ 3,31,59].Even
though, Campos et al. [ 11] proposed a budget allocation targeting
themaximumbranchcoverage,wedonotconsiderthisasabaseline
in our work as we focus on bug detection instead. Our intended
applicationscenarioisgeneratingteststofindbugsnotonlylimited
to regressions, but also the bugs that are introduced to the system
in different times. Hence, we consider generating tests to all of the
classesintheprojectregardlessofwhethertheyhavebeenchanged
ornot.Therefore,inequalbudgetallocation,totaltimebudgetis
equally allocated to all the classes in a project.4.4 Parameter Settings
There are 3 modules in our approach. Each module has various
parameters to be configured, and the following subsections outline
the parameters and their chosen values in our experiments.
4.4.1 Schwa. Schwa has 5 parameters to be configured; ğ‘¤ğ‘Ÿ,ğ‘¤ğ‘“,
ğ‘¤ğ‘,ğ‘‡ğ‘…, andğ‘›. We choose the default parameter values used in
Schwa [30] as follows: ğ‘¤ğ‘Ÿ=0.25,ğ‘¤ğ‘“=0.5,ğ‘¤ğ‘=0.25, and ğ‘‡ğ‘…=
0.4. Our preliminary experiments with ğ‘›=50,100,500,1000 and
ğ‘ğ‘™ğ‘™ ğ‘ğ‘œğ‘šğ‘šğ‘–ğ‘¡ğ‘  suggest that ğ‘›=500 gives most accurate predictions.
4.4.2 EvoSuite. Arcuri and Fraser [ 7] showed that parameter tun-
ing of search algorithms is an expensive and long process, and the
default values give reasonable results when compared to tuned
parameters.Therefore,weusethedefaultparametervaluesusedin
EvoSuite in previous work [ 20,52] except for the following param-
eters.
Coveragecriteria: Weusebranchcoveragesinceitperformsbetter
among the other single criteria [ 31]. Gay [31] found some multiple
criteriacombinationstobeeffectiveonbugdetectionthansingle
criterion. However, they did not recommend a strategy to com-
binemultiplecriteriaastheirstrategiesalsoproducedineffective
combinations. Therefore, we decide to use only single criterion.
Assertion strategy: As Shamshiri et al. [ 59] mentioned, mutation-
basedassertionfilteringcanbecomputationallyexpensiveandlead
to timeouts sometimes. Therefore, we use all possible assertions as
the assertion strategy.
Givenacoveragecriterion(e.g.,branchcoverage), ğ·ğ‘¦ğ‘›ğ‘ğ‘€ğ‘‚ğ‘†ğ´
exploresthesearchspaceofpossibletestinputsuntilitfindstest
casesthatcoverallofthetargets(e.g.,branches)orthetimeruns
out(i.e.,timebudget).Theseareknownasstoppingcriteria.This
way,ifthesearchachieves100%coveragebeforethetimeout,anyre-
mainingtimebudgetwillbewasted.Atthesametime, ğ·ğ‘¦ğ‘›ğ‘ğ‘€ğ‘‚ğ‘†ğ´
aims at generating only one test case to cover each target in the
system under test (SUT), since its objective is to maximise the cov-
eragecriteriongiven.Thisalsohelpsinminimisingthetestsuite
produced. However, when it comes to finding bugs in the SUT, justcoveringthebugdoesnotnecessarilyimplythattheparticulartestcasecandiscoverthebug.Hence,wefindthatusing100%coverage
as a stopping criterion and aiming at finding only one test case for
each target deteriorate the bug detection capability of ğ·ğ‘¦ğ‘›ğ‘ğ‘€ğ‘‚ğ‘†ğ´ .
Therefore,inourapproach,weconfigure ğ·ğ‘¦ğ‘›ğ‘ğ‘€ğ‘‚ğ‘†ğ´ togenerate
more than one test case for each target in the SUT, retain all these
test cases,disable testsuite minimisationand remove100% cover-
agefromthestoppingcriteria.Bydoingthis,wecompromisethe
testsuitesizeinordertoincreasethebugdetectioncapabilityof
SBST.
4.4.3 BADS. Following the results of our pilot runs, we use the
default threshold of 0.5 to allocate the classes into the two tiers. In
particular,thetophalfoftheclasses(rankedindescendingorder
according to defect scores) are allocated in the first tier(ğ‘1) and
the rest are in the second tier (ğ‘2).ğ‘1andğ‘2are the number of
classes in the firstandsecond tiers respectively.
Ourpreliminaryresultsalsosuggestthatallocating90%and10%
ofthetotaltimebudget( ğ‘‡)tothefirsttier (ğ‘‡1)andthe secondtier
(ğ‘‡2) sufficiently favours the highly likely to be defective classes,
whilenotleavingoutthelesslikelytobedefectiveclassesfromtest
453generation.Inparticular,wechoose ğ‘‡1=27âˆ—ğ‘1andğ‘‡2=3âˆ—ğ‘2
secondsat ğ‘‡=15âˆ—ğ‘andğ‘‡1=54âˆ—ğ‘1andğ‘‡2=6âˆ—ğ‘2secondsat ğ‘‡=
30âˆ—ğ‘.Wechoose15and30secondsas ğ‘¡ğ‘šğ‘–ğ‘›forthefirsttier(ğ‘¡min1)
atğ‘‡=15âˆ—ğ‘andğ‘‡=30âˆ—ğ‘respectively. The rationale behind
choosingthesevaluesfor ğ‘¡min1isthatitguaranteestheclassesin
thefirsttieratleastgetatimebudgetoftheequalbudgetallocation
(i.e.,budgetallocationwithoutdefectpredictionguidance).For ğ‘¡ğ‘šğ‘–ğ‘›
ofthesecondtier (ğ‘¡min2),weassign3and6secondsat ğ‘‡=15âˆ—ğ‘
andğ‘‡=30âˆ—ğ‘because we believe ğ‘‡2is not enough to go for an
exponential allocation.
Theparametersfortheexponentialfunctionareasfollows: ğ‘’ğ‘=
0.02393705, ğ‘’ğ‘=0.9731946, and ğ‘’ğ‘=âˆ’10.47408. The rationale
behind choosing the parameter values for the exponential function
is as follows. The exponential curve is almost flat and equal to 0
for the values in the ğ‘¥axis from 0.5 to 1 (see Figure 4). Then, after
ğ‘¥=0.5, it starts increasing towards ğ‘¥=0. Finally, at ğ‘¥=0, the
output is equal to 1.
          
1RUPDOLVHG5DQN:HLJKWEHIRUH1RUPDOLVDWLRQ
Figure4:ExponentialFunctionofBADS. ğ‘’ğ‘=0.02393705,ğ‘’ğ‘=
0.9731946, and ğ‘’ğ‘=âˆ’10.47408
4.5 Prototype
We implement the Defect Prediction Guided SBST approach in a
prototypetoolinordertoempiricallyevaluateit.Theprototyped
tool is available to download from here: https://github.com/SBST-
DPG/sbst-dpg
4.6 Experimental Protocol
As we mentioned earlier, to answer RQ1andRQ2, we conduct
experiments for ğ‘‡=15âˆ—ğ‘and 30âˆ—ğ‘seconds.
In SBST ğ·ğ‘ƒğº, Schwa uses current versions of the repositories
oftheprojects.Foreachbug,Schwapredictsthedefectivenessof
the classes at the commit just before the bug fixing commit. For
each bug in Defects4J, there is a buggy version and a fixed version
of the project. We take each buggy version of the projects, and
thengeneratetestsuitesonlyforthebuggyclass(es)ofthatproject
versionusingthetwoapproaches.TotaketherandomnessofSBST
intoaccount,werepeateachtestgenerationrun20times,andcarry
out statistical tests when necessary. Consequently, we have to run
a total of 2 (approaches) âˆ—511 (buggy classes) âˆ—20 (repetitions) âˆ—2
(timebudgets) =40,880testgenerations.Wecollectthegeneratedtestsuitesaftereachtestgenerationrun.Next,weusethe fixtest
suiteinterface in Defects4J to remove the flaky tests from each test
suite [59]. Then, we execute each resulting test suite against the
respectivebuggyandfixedversionstocheckifit findsthebugor
not using the run bug detection interface. If the test suite is not
compilable or there is at least one failing test case when the test
suite is run against the buggy version, then it is marked as Broken.
If not, it will be run against the fixed version. Then, if at least one
testcasefails,thetestsuiteismarkedas Fail(i.e.,testsuitefinds
thebug).Ifallthetestcasespass,thenthetestsuiteismarkedas
Pass(i.e., test suite does not find the bug).
5 RESULTS
We present the results for each research question following the
methoddescribedinSection4.Whilethemainaimistoevaluate
ifourapproachismoreefficientthanthestateoftheart,wealso
focus on explaining its strengths and weaknesses.
Table 1: Mean and median number of bugs found by the 2
approaches against different total time budgets.
Mean Medianp-value /hatwideğ´12 T( s )SBST ğ·ğ‘ƒğºSBST ğ‘›ğ‘œğ·ğ‘ƒğº SBST ğ·ğ‘ƒğºSBST ğ‘›ğ‘œğ·ğ‘ƒğº
15âˆ—ğ‘151.45 133.95 150.5 134.0 <0.0001 0.94
30âˆ—ğ‘171.45 166.9 170 167.5 0.0671 0.67
RQ1. Is SBST ğ·ğ‘ƒğºefficient in finding bugs?
As described in Section 4, we perform 20 runs for each SBST ap-
proachandeachbuggyprograminDefects4Jandreporttheresults
asboxplotsinFigure5.Aswecansee,overall,ourproposedmethod
SBST ğ·ğ‘ƒğºfindsmorebugsthanthebaselineapproachforboth15
and 30 seconds time budgets.
We also report the means, medians and the results from the sta-
tisticalanalysisinTable1.SBST ğ‘›ğ‘œğ·ğ‘ƒğºfinds133.95bugsonaverage
at total time budget of 15 seconds per class. SBST ğ·ğ‘ƒğºoutperforms
SBST ğ‘›ğ‘œğ·ğ‘ƒğº,andfinds151.45bugsonaverage,whichisanaverage
improvementof17.5(+13.1%)morebugsthan SBST ğ‘›ğ‘œğ·ğ‘ƒğº.Thedif-
ference of the number of bugs found by SBST ğ·ğ‘ƒğºand SBST ğ‘›ğ‘œğ·ğ‘ƒğº
isstatisticallysignificantaccordingtotheMann-WhitneyU-Test
(p-value<0.0001)withalargeeffectsize( /hatwideğ´12=0.94).Thus,wecan
conclude that SBST ğ·ğ‘ƒğºis more efficient than SBST ğ‘›ğ‘œğ·ğ‘ƒğº.
Attotaltimebudgetof30secondsperclass,SBST ğ·ğ‘ƒğºfindsmore
bugsthantheSBST ğ‘›ğ‘œğ·ğ‘ƒğº.AccordingtotheMann-WhitneyU-Test,
thedifferencebetweenSBST ğ·ğ‘ƒğºandSBST ğ‘›ğ‘œğ·ğ‘ƒğºisnotstatistically
significant, with a p-value of 0.067. However the effect size of 0.67
suggeststhatSBST ğ·ğ‘ƒğºfindsmorebugsthanSBST ğ‘›ğ‘œğ·ğ‘ƒğº67%of
the time, which is significant given how difficult it is to find failing
test cases [35].
In summary, defect prediction guided SBST (SBST ğ·ğ‘ƒğº)
is significantly more efficient than SBST without defect
predictionguidance(SBST ğ‘›ğ‘œğ·ğ‘ƒğº)whentheyaregivena
tight time budget in a usual resource constrained scenario.
When there is sufficient time budget SBST ğ·ğ‘ƒğºis more
effective than SBST ğ‘›ğ‘œğ·ğ‘ƒğº67% of the time.
454â—
â—
â—
â—â—
â—â—
â—
â—
â—â—â—â—
â—
â—â—
â—â—â—
â— â—â—
â—
â—
â—
â—â—
â—â—â— â—â—
â—â—
â—â—
â—â—
â—â—
â—â—
â—â—
â—â—â—
â—â—â—
â—â—
â—â—â—
â—â—â—
â—
â—â—
â—â—
â—
â—
â—â—
â—â—
â—â—
â—
â—
â—â—
â—â—
â—
â—â—
â—

 
7RWDO7LPH%XGJHW[1VHFRQGV1XPEHURI%XJV)RXQG$SSURDFK
6%67 '3*
6%67 QR'3*
Figure 5: The number of bugs found by the 2 approaches
against different total time budgets
â—
â—
â—
â—â—
â—â—â—â—â—
Ã­ Ã­ Ã­ Ã­ Ã­ Ã­ Ã­ Ã­ Ã­ Ã­
5HODWLYH5DQNLQJ3RVLWLRQRIWKH&ODVVHV1XPEHURIFODVVHVZKHUHDEXJZDVIRXQG$SSURDFK
â—6%67'3*
6%67QR'3*
Figure 6: The number of classes where a bug was found by
the2approaches,groupedbytherelativerankingpositions
(%) of the classes in the project at T = 15âˆ—ğ‘seconds
To further analysethe differences between the two approaches,
Figure6reports thedistributionofthe numberofclasseswherea
bug was found across 20 runs for the 2 approaches grouped by the
relativerankingpositionproducedbySchwaattotaltimebudget
of 15 seconds per class. Relative ranking position is the normalised
rank of the respective class as described in Algorithm 1.Table2:Summaryofthebugfindingresultsgroupedbythe
relative ranking position (%) of the classes in the project at
T=15âˆ—ğ‘seconds.
Rank
(%)# Buggy
ClassesAvg.
Time
BudgetMean number of classes
where a bug was found
SBST ğ·ğ‘ƒğºSBST ğ‘›ğ‘œğ·ğ‘ƒğº
0-1 0 26666.61 96.25 69.45
1 0-2 0 7220.76 28.40 26.40
2 0-3 0 6316.43 20.00 19.00
3 0-4 0 2516.00 7.00 6.75
4 0-5 0 2616.00 8.00 8.00
5 0-6 0 132.00 1.35 2.35
6 0-7 0 162.00 2.85 4.70
7 0-8 0 122.00 3.30 5.95
8 0-9 0 132.00 1.30 4.10
90 - 100 52.00 0.25 1.45
We observe that when the buggy classes are correctly ranked at
the top by Schwa, and allocated more time by BADS, the perfor-
manceofSBST ğ·ğ‘ƒğºissignificantlybetterthanthebaselinemethod.
More than half of the buggy classes (52%) are ranked in the top
10% of the project by Schwa, as shown in Table 2, and allocated
66.61 seconds of time budget on average by BADS. Around 36% of
thebuggyclassesarerankedinthe10-50%oftheprojects.BADS
employsanexponentialfunctiontolargelyfavourasmallernumber
of highly likely to be defective classes and allocates an adequate
amount of time to the moderately defective classes.
Only 12% of the buggy classes are ranked below the first half of
the project. BADS assumes not all classes in a project are defective
andfollowsthe2-Tierapproachtooptimisethebudgetallocation
fortheproject.Thus,alltheclassesinthe secondtier whichcontains
theclassesthatarerankedaslesslikelytobebuggy,getaverysmall
timebudget(2seconds).Unsurprisingly,SBST ğ‘›ğ‘œğ·ğ‘ƒğºfoundmore
bugsoutofthese59buggyclassesthanSBST ğ·ğ‘ƒğº.Thisindicates
thatthedefectpredictorâ€™saccuracyiskeytothebetterperformanceofSBST
ğ·ğ‘ƒğºandthereispotentialtoimp roveourapproachfurther.
Forcompleteness,wealsomeasureandpresentthenumberof
truepositives,falsenegatives,andrecallofSchwa.Basedonthe0.5threshold,i.e.,ifthedefectscoreisgreaterthanorequalto0.5thentheclassisbuggyanditisnon-buggyifthedefectscoreislessthan
0.5,Schwalabels436buggyclassescorrectly(truepositives)and
mislabels75buggyclasses(falsenegatives).Hence,Schwaachieves
a recall of 0.85.
The defect predictor (i.e., Schwa) and BADS modules add an
overhead to SBST ğ·ğ‘ƒğº. While this overhead is accounted in the
time budget allocation in SBST ğ·ğ‘ƒğº, we also report the time spent
by the defect predictor and BADS modules together. Schwa and
BADS spent 0.68 seconds per class on average (standard deviation
=0.4seconds),whichtranslatestoa4.53%and2.27%overheadin15
and 30 seconds per class time budgets respectively. Therefore, this
showstheoverheadintroducedbySchwaandBADSinSBST ğ·ğ‘ƒğº
is very small and negligible.
455Table3:Successrateforeachmethodat 15âˆ—ğ‘totaltimebudget.BugIDsthatwerefoundbyonlyoneapproacharehighlighted
with different colours; SBST ğ·ğ‘ƒğºandSBST ğ‘›ğ‘œğ·ğ‘ƒğº.
BugIDSBST ğ·ğ‘ƒğºSBST ğ‘›ğ‘œğ·ğ‘ƒğº
Lang-1 1 0.45
Lang-4 0.9 1
Lang-5 0 0.2
Lang-7 1 1
Lang-8 0.1 0.1
Lang-9 0.95 1
Lang-10 0.95 0.8
Lang-11 0.8 0.95
Lang-12 0.2 0.8
Lang-14 0.05 0
Lang-17 0.05 0
Lang-18 0.5 0.3
Lang-19 0.05 0.7
Lang-20 0.8 0.4
Lang-21 0.1 0.1
Lang-22 0.55 0.8
Lang-23 1 0.95
Lang-27 0.8 0.75
Lang-28 0.05 0.05
Lang-32 1 1
Lang-33 1 1
Lang-34 1 0.9
Lang-35 1 0.3
Lang-36 1 1
Lang-37 0.65 0.2
Lang-39 1 0.95
Lang-41 0.7 1
Lang-44 0.85 0.65
Lang-45 1 1
Lang-46 0.5 1
Lang-47 0.95 0.9
Lang-49 0.55 0.4
Lang-50 0.3 0.3
Lang-51 0.1 0.05
Lang-52 1 1
Lang-53 0.3 0.15
Lang-54 0.05 0.05
Lang-55 0.05 0
Lang-57 1 1
Lang-58 0 0.05
Lang-59 1 0.95
Lang-60 0.75 0.3
Lang-61 1 0.25
Lang-65 1 0.95
Math-1 1 1
Math-2 0 0.1
Math-3 0.55 1
Math-4 1 1
Math-5 0.45 0.95
Math-6 1 1BugIDSBST ğ·ğ‘ƒğºSBST ğ‘›ğ‘œğ·ğ‘ƒğº
Math-9 0.7 0.6
Math-10 0.1 0
Math-11 0.95 1
Math-14 1 1
Math-16 0 0.05
Math-21 0.05 0.45
Math-22 1 1
Math-23 0.95 0.8
Math-24 0.9 0.85
Math-25 0.1 0
Math-26 1 1
Math-27 0.6 0.65
Math-28 0.05 0
Math-29 0.9 1
Math-32 1 1
Math-33 0.45 0.35
Math-35 1 1
Math-36 0.2 0.1
Math-37 1 1
Math-40 1 0.95
Math-41 0.25 0.4
Math-42 0.95 0.95
Math-43 0.45 0.55
Math-45 0 0.3
Math-46 1 1
Math-47 1 0.95
Math-48 0.65 0.75
Math-49 0.8 0.75
Math-50 0.75 0.3
Math-51 0.35 0.25
Math-52 0.65 0.6
Math-53 1 1
Math-55 1 1
Math-56 1 0.9
Math-59 1 1
Math-60 0.95 0.95
Math-61 1 1
Math-63 1 0.4
Math-64 0.05 0
Math-65 0.25 0.25
Math-66 1 1
Math-67 1 1
Math-68 1 1
Math-70 1 1
Math-71 0.6 0.35
Math-72 0.5 0.45
Math-73 0.75 1
Math-75 1 0.9
Math-76 0.15 0.05
Math-77 1 1BugIDSBST ğ·ğ‘ƒğºSBST ğ‘›ğ‘œğ·ğ‘ƒğº
Math-78 0.6 0.6
Math-79 0.15 0.05
Math-80 0.3 0
Math-81 0.15 0
Math-83 0.9 1
Math-84 0.15 0
Math-85 1 1
Math-86 0.95 0.85
Math-87 0.95 1
Math-88 0.75 0.7
Math-89 1 1
Math-90 1 1
Math-92 1 1
Math-93 0.35 0.25
Math-94 0.35 0
Math-95 1 1
Math-96 1 1
Math-97 1 1
Math-98 1 0.85
Math-100 1 1
Math-101 0.2 1
Math-102 0.75 0.5
Math-103 1 1
Math-104 0.5 0.4
Math-105 1 1
Math-106 0.15 0
Time-1 1 1
Time-2 0.85 1
Time-3 0.15 0.05
Time-4 0 0.3
Time-5 1 1
Time-6 1 0.8
Time-7 0.15 0
Time-8 1 0.7
Time-9 1 1
Time-10 0.1 0.1
Time-11 1 1
Time-12 1 0.55
Time-13 0.5 0.05
Time-14 0 0.95
Time-15 0.4 0.3
Time-16 0.15 0
Time-17 1 0.55
Time-22 0 0.25
Time-23 0 0.2
Time-24 0 0.45
Time-26 0.1 0.05
Time-27 0.15 0.5
Chart-1 0.2 0.05
Chart-2 0.05 0BugID SBST ğ·ğ‘ƒğºSBST ğ‘›ğ‘œğ·ğ‘ƒğº
Chart-3 0.90.15
Chart-4 0.85 0.3
Chart-5 0.35 1
Chart-6 0.8 1
Chart-7 0.30.25
Chart-8 1 1
Chart-10 1 1
Chart-11 0.2 1
Chart-12 0.90.5
Chart-13 0.90.2
Chart-14 1 1
Chart-15 10.9
Chart-16 1 1
Chart-17 1 1
Chart-18 1 1
Chart-19 10.15
Chart-20 0.50.1
Chart-21 0.550.05
Chart-22 1 1
Chart-23 1 1
Chart-24 0 1
Mockito-2 1 1
Mockito-17 1 1
Mockito-29 0.850.95
Mockito-35 1 1
Closure-6 0.05 0
Closure-7 0.35 0.1
Closure-9 0.60.15
Closure-12 0.30.1
Closure-19 00.1
Closure-21 0.90.35
Closure-22 0.50.5
Closure-26 0.50.4
Closure-27 0.25 0.1
Closure-28 1 1
Closure-30 10.95
Closure-33 10.5
Closure-34 0.05 0
Closure-39 10.6
Closure-41 0.1 0
Closure-43 0.05 0
Closure-46 1 1
Closure-48 0.1 0
Closure-49 0.45 0.5
Closure-52 0.40.1
Closure-54 10.8
Closure-56 0.95 1
Closure-60 0.1 0
Closure-65 0.90.45
Closure-72 0.20.3BugID SBST ğ·ğ‘ƒğºSBST ğ‘›ğ‘œğ·ğ‘ƒğº
Closure-73 1 1
Closure-77 0.7 0.25
Closure-78 0.05 0
Closure-79 1 0.85
Closure-80 0.2 0
Closure-81 0.35 0
Closure-82 1 1
Closure-86 0.15 0
Closure-89 0.05 0
Closure-91 0.15 0
Closure-94 0.25 0
Closure-104 0.95 0.5
Closure-106 1 0.95
Closure-108 0.8 0.2
Closure-110 0.95 1
Closure-112 0.1 0
Closure-113 0.25 0.05
Closure-114 0 0.1
Closure-115 0.3 0.25
Closure-116 0.2 0.1
Closure-117 0.4 0.05
Closure-119 0.25 0
Closure-120 0.2 0.1
Closure-121 0.55 0.2
Closure-122 0.05 0
Closure-123 0.15 0.1
Closure-125 0.45 0
Closure-128 0.15 0.1
Closure-129 0.2 0.05
Closure-131 0.15 0.9
Closure-137 0.95 1
Closure-139 0.15 0.05
Closure-140 0.85 0.25
Closure-141 0.3 0
Closure-144 0.3 0.1
Closure-146 0.15 0
Closure-150 0.45 0.1
Closure-151 1 1
Closure-160 0.55 0.05
Closure-164 0.35 0.45
Closure-165 0.95 0.8
Closure-167 0.35 0
Closure-169 0 0.05
Closure-170 0.2 0.2
Closure-171 0.9 0.05
Closure-172 0.65 0.15
Closure-173 1 0.5
Closure-174 1 1
Closure-175 0.75 0.15
Closure-176 0.1 0.1
RQ2. Does SBST ğ·ğ‘ƒğºfind more unique bugs?
To investigate how our approach performs against each bug, we
present an overview of thesuccess rates for each SBST method at
total time budget of 15 seconds per class in Table 3. Success rate is
theratioofrunswherethebugwasdetected.Duetospacelimita-
tion, we omit the entries for bugs where none of the approaches
were able to find the bug. We also highlight the bugs that were
detectedbyonlyoneapproach.AscanbeseenfromTable3,our
approach outperforms the benchmark in terms of the success rates
for most of the bugs.
This observation can be confirmed with the summary of the
results which we report in Table 4. What is particularly interesting
to observe from the more granular representation of the results
in Table 3 is the high number of bugs where our approach has
100% success rate, which means that SBST ğ·ğ‘ƒğºfinds the respectiveTable 4: Summary of the bug finding results at T = 15âˆ—ğ‘.
Bugs
foundUnique
bugsBugs found
in every runBugs found
more often
SBST ğ·ğ‘ƒğº 236 35 84 127
SBST ğ‘›ğ‘œğ·ğ‘ƒğº 215 14 76 47
bugsinalltheruns.Thisisanindicationoftherobustnessofour
approach.
Certainbugsarehardertofindthanothers.Outofthe20runs
for each SBST approach, if a bug is only detected by one of the
approaches,wecallitauniquebug.Thereasonwhywepayspecial
attention to unique bugs is because they are an indication of the
456ability of the testing technique to discover what cannot be discov-
ered otherwise in the given time budget, which is an important
strength[ 35].SBST ğ·ğ‘ƒğºfound236bugsaltogether,whichis54.38%
ofthetotalbugs,whereasSBST ğ‘›ğ‘œğ·ğ‘ƒğºfoundonly215(49.54%)bugs.
SBST ğ·ğ‘ƒğºfound 35 unique bugs that SBST ğ‘›ğ‘œğ·ğ‘ƒğºcould not find in
anyoftheruns.Ontheotherhand,SBST ğ‘›ğ‘œğ·ğ‘ƒğºfoundonly14such
unique bugs. 30 out of these 35 bugs have buggy classes ranked in
thetop10%oftheprojectbySchwa,andtheother5bugsin10-50%
of the project. We observe similar results at total time budget of 30
secondsperclassaswell,whereSBST ğ·ğ‘ƒğºfound32uniquebugs,
while SBST ğ‘›ğ‘œğ·ğ‘ƒğºwas only able to find 13 unique bugs.
SBST ğ·ğ‘ƒğºfound 127 bugs more times than SBST ğ‘›ğ‘œğ·ğ‘ƒğº, while
forSBST ğ‘›ğ‘œğ·ğ‘ƒğº,thisisonly47.92outofthese127bugshavebuggy
classesrankedinthetop10%oftheprojectandtheother35bugs
in 10-50% of the project.
Ifweconsiderabugasfoundonlyifalltherunsbyanapproach
findthebug(successrate=1.00),thenthenumberofbugsfound
bySBST ğ·ğ‘ƒğºandSBST ğ‘›ğ‘œğ·ğ‘ƒğºbecome84and76.Thereare27bugs
which only SBST ğ·ğ‘ƒğºdetected them in all of the runs.
In summary, SBST ğ·ğ‘ƒğºfinds 35 more unique bugs com-
pared to the benchmark approach. Furthermore, it finds
a large number of bugs more frequently than the base-
line. Thus, this suggests that the superior performance of
SBST ğ·ğ‘ƒğºis supported by both its capability of finding
newbugswhicharenotexposedbythebaselineandthe
robustness of the approach.
We pick Math-94 and Time-8 bugs and investigate the tests
generated by the 2 approaches. Figure 7a shows the buggy codesnippet of
MathUtils class from Math-94. The ifcondition at
line412isplacedtocheckifeither uorviszero.Thisisaclassic
exampleofabugduetoanintegeroverflow.Assumethemethod
is called with the following inputs MathUtils. ğ‘”ğ‘ğ‘‘(1073741824,
1032).Then,the ifconditionatline412isexpectedtobeevaluated
tofalsesince both u(1073741824) andv(1032) are non-zeros.
However, the multiplication of uandvcauses an integer overflow
tozero,andthe ifconditionatline412isevaluatedto true.Figure
7bshowsthesamecodesnippetof MathUtils classafterthepatch
is applied. To detect this bug, a test should not only cover the true
branchofthe ifconditionatline412,butalsopassthenon-zero
arguments uandvsuch that their multiplication causes an integer
overflow to zero.
Thefitnessfunctionforthe truebranchofthe ifconditionat
line412is ğ‘¢âˆ—ğ‘£/(ğ‘¢âˆ—ğ‘£+1),andittendstorewardthetestinputs
uandvwhosemultiplicationisclosertozeromorethantheones
whosemultiplicationisclosertocausinganintegeroverflowtozero.
For anexample, suppose we havetwo individuals ğ‘¢=2,ğ‘£=3 and
ğ‘¢=12085,ğ‘£=1241inthecurrentgeneration.Thefitnessofthefirst
andsecond individuals willbe6/(6+1)and14997485 /(14997485+1).
Thus,the first individual isconsidered fitterwhencomparedwith
the second one, while the second one is closer to detect the bug
thanthefirstone.Thereforeinasituationlikethis,wecanincrease
the chances of detecting the bug by allowing the search method
to extensively explore the search space of possible test inputs and
generate more than one test case (test inputs) for such branches.SBST ğ‘›ğ‘œğ·ğ‘ƒğºgenerated30.75testcasesonaveragethatcoverthe
truebranchofthe ifconditionatline412,yetitwasnotableto
detectthebuginanyoftheruns.SchwarankedMath-94buginthe
top 10% of the project and BADS allocated 37 seconds time budget
to the search. Then, SBST ğ·ğ‘ƒğºgenerated 49.8 test cases on average
thatcoverthesaidbranch.Asaresult,itwasabletofindthebug
in7runsoutof20.Allocatingahighertimebudgetincreasesthe
likelihoodofdetectingthebugsinceitallowsthesearchmethodtoexplorethesearchspaceextensivelytofindthetestinputsthatcan
detect the bug.
411public static int gcd(int u,intv) {
412if(u * v == 0) {
413 return(Math.abs(u) + Math.abs(v));
414}
415...
416}
(a) Buggy code411public static int gcd(int u,intv) {
412if((u == 0) || (v == 0)) {
413 return(Math.abs(u) + Math.abs(v));
414}
415...
416}
(b) Fixed code
Figure 7: MathUtils class from Math-94
Figure 8a shows the buggy code snippet of DateTimeZone class
fromTime-8.The forOffsetHoursMinutes methodtakestwoin-
teger inputs hoursOffset andminutesOffset , and returns the
DateTimeZone object for the offset specified by the two inputs.
If the method forOffsetHoursMinutes is called with the inputs
hoursOffset=0 andminutesOffset=-15 , then it is expected to
return aDateTimeZone object for the offset âˆ’00 : 15. However, the
ifconditionatline279isevaluatedto trueandthemethodthrows
anIllegalArgumentException instead.Figure8bshowsthesame
code snippet after the patch is applied. To detect this bug, a test
casehastoexecutethe ifconditionsatlines273and276to false;
that ishoursOffset â‰ 0o rminutesOffset â‰ 0 andhoursOffset
âˆˆ[ âˆ’23,23], and then it has to execute the ifcondition at line 279
totruewith aminutesOffset âˆˆ[ âˆ’59,âˆ’1]. Moreover, there is a
newconditionintroducedatline282inthefixedcodetocheckif
thehoursOffset is positive when the minutesOffset is negative
(seeFigure8b).Thus,thisaddsanotherconstrainttothepossible
test inputs that can detect the bug, which is hoursOffset â‰¤0.
Therefore, it is evident that it is hard not only to find the right test
inputstodetectthebug,butalsotofindtestinputstoatleastcover
the buggy code.
AsitwasthecaseinMath-94,justcoveringthebuggycode( true
branch of the ifcondition at line 279) is not sufficient to detect
theTime-8bug.For anexample,testinputs hoursOffset=-4 and
minutesOffset=-150 cover the buggy code, however they cannot
detect the bug. Therefore, the search method needs more resources
togeneratemoretestcasesthatcoverthebuggycodesuchthatit
eventually finds the right test cases that can detect the bug.
Ourinvestigationintothetestsgeneratedbythe2approaches
shows that the baseline, SBST ğ‘›ğ‘œğ·ğ‘ƒğº, covered the buggy code in
90% of the runs. SBST ğ‘›ğ‘œğ·ğ‘ƒğºgenerated 25.78 test cases on average
that cover the buggy code and it was ableto detect the bug in 14
runsoutof20.Whereas,SBST ğ·ğ‘ƒğºallocated75secondstimebudget
tothesearchasSchwarankedthebuginthetop10%oftheproject
and generated 109.8 test cases on average that cover the buggycode. As a result, it was able to detect the bug in all of the runs
(success rate = 1.00). Therefore, this again confirms the importance
457272public static DateTimeZone forOffsetHoursMinutes( inthoursOffset, int
minutesOffset) throws IllegalArgumentException {
273if(hoursOffset == 0 && minutesOffset == 0) {
274 return DateTimeZone.UTC;
275}
276if(hoursOffset < -23 || hoursOffset > 23) {
277 throw new IllegalArgumentException( "Hours out of range: " + hoursOffset);
278}
279if(minutesOffset < 0 || minutesOffset > 59) {
280 throw new IllegalArgumentException( "Minutes out of range: " + minutesOffset);
281}
282intoffset = 0;
283...
284}
(a) Buggy code
272public static DateTimeZone forOffsetHoursMinutes( inthoursOffset, int
minutesOffset) throws IllegalArgumentException {
273if(hoursOffset == 0 && minutesOffset == 0) {
274 return DateTimeZone.UTC;
275}
276if(hoursOffset < -23 || hoursOffset > 23) {
277 throw new IllegalArgumentException( "Hours out of range: " + hoursOffset);
278}
279if(minutesOffset < -59 || minutesOffset > 59) {
280 throw new IllegalArgumentException( "Minutes out of range: " + minutesOffset);
281}
282if(hoursOffset > 0 && minutesOffset < 0) {
283 throw new IllegalArgumentException( "Positive hours must not have negative
minutes: " + minutesOffset);
284}
285intoffset = 0;
286...
287}
(b) Fixed code
Figure 8: DateTimeZone class from Time-8
of focusing the search more into the buggy classes to increase the
likelihood of detecting the bug.
6 THREATS TO VALIDITY
InternalValidity. AsoutlinedinSection4.4.2,weconfigure ğ·ğ‘¦ğ‘›ğ‘-
ğ‘€ğ‘‚ğ‘†ğ´to generate more than one test case for each target in the
SUT,retainallthese testcasesanddisabletestsuiteminimisation.
By doing this, we expect to compromise the test suite size in order
to maximise the bug detection of SBST. To investigate the bene-fit of configuring
ğ·ğ‘¦ğ‘›ğ‘ğ‘€ğ‘‚ğ‘†ğ´ in this way, we also run the same
set of experiments using ğ·ğ‘¦ğ‘›ğ‘ğ‘€ğ‘‚ğ‘†ğ´ with test suite minimisation
and equal budget allocation, SBST ğ‘‚. We compare its performance
against SBST ğ‘›ğ‘œğ·ğ‘ƒğº. SBST ğ‘‚finds 85.75 and 93.45 bugs on average
attotaltimebudgetof15and30secondsperclass.SBST ğ‘›ğ‘œğ·ğ‘ƒğºout-
performsSBST ğ‘‚withanaverageimprovementof48.2(+56.2%)and
73.45(+78.6%)morebugsineachcase,whicharestatisticallysig-
nificantaccording totheMann-WhitneyU-Test (p-value<0.0001)
with a large effect size ( /hatwideğ´12= 1.00). However, this huge improve-
mentcomeswithaprice,i.e.,SBST ğ‘›ğ‘œğ·ğ‘ƒğºproduceslargetestsuites.
This can be problematic if the developers have to insert the test
oraclesmanuallytothegeneratedtests.Thus,weidentifythisas
a potential threatto internal validity and future worksneed to be
doneonadaptingappropriatetestsuiteminimisationtechniques
to SBST ğ·ğ‘ƒğº.
ToencountertherandomisednatureofGAusedin ğ·ğ‘¦ğ‘›ğ‘ğ‘€ğ‘‚ğ‘†ğ´ ,
weruntheexperimentsfor20timesandcarryoutsoundstatistical
tests; two-tailed non-parametric Mann-Whitney U-Test [ 5] and
Vargha and Delaneyâ€™s /hatwideğ´12statistic [60].The parameter configurations for Schwa and BADS are either
the default values or based on the results of the pilot runs. We
believetheperformanceofSBST ğ·ğ‘ƒğºcanbefurtherimprovedby
fine-tuning the parameters of Schwa and BADS.
We employ an exponential function to allocate time budgets for
classes based on the defect scores. As opposed to an exponentialallocation,adirectmapping(i.e.,linearbudgetallocation)would
havebeensimpleandstraight-forward.However,asdescribedin
Section 3.2.1, there are only a few number of classes which are
actually buggy (i.e., highly likely to be defective) and they need to
beallocatedmoretimebudgettomaximisethebugdetectionofthe
test generation tool. Thus, we believe a linear allocation approach
is not able to largely favourthese small number of classes like the
exponential allocation approach does.
ExternalValidity. Weuse434realbugsfromDefects4Jdataset
that are drawn from 6 open source projects. These projects maynotrepresentallprogramcharacteristics;especiallyinindustrial
projects.Although,Defects4Jhasbeenwidelyusedintheliterature
[41,54,55,59]asabenchmark.Futureworkneedstobedoneon
applying SBST ğ·ğ‘ƒğºon other bugs datasets.
EvoSuite generates JUnit test suites for Java programs. Thus, we
maynotbeabletogeneralisetheconclusionstootherprogramming
languages. However, the concept we introduced in this research is
not language dependent and can be applied to other programming
languages as well.
7 CONCLUSION
WeintroducedefectpredictionguidedSBST(SBST ğ·ğ‘ƒğº)thatcom-
bines class level defect prediction and Search-Based Software Test-
ing to efficiently find bugs in a resource constrained environment.
SBST ğ·ğ‘ƒğºemploysabudgetallocationalgorithm,BudgetAllocation
BasedonDefectScores(BADS),toallocatetimebudgetsforclassesbasedontheirlikelihoodofdefectiveness.Wevalidateourapproach
against 434 real bugs from Defects4J dataset. Our empirical eval-
uationdemonstratesthatinaresourceconstrainedenvironment,
when given a tight time budget, SBST ğ·ğ‘ƒğºis significantly more
efficientthanthestateoftheartapproachwithalargeeffectsize.
Inparticular,SBST ğ·ğ‘ƒğºfinds13.1%morebugsonaveragecompared
tothestateoftheartSBSTapproachwhentheyaregivenatight
time budget of 15 seconds per class. Further analysis of the re-
sults finds that the superior performance of SBST ğ·ğ‘ƒğºis supported
by its ability to find more unique bugs which otherwise remain
undetected.
We aim to extend our work in the following directions as future
work; i) employ a defect predictor which uses different features to
producepredictions,ii)adaptanappropriatetestsuiteminimisation
strategy to address the generation of larger test suites, and iii)
validate SBST ğ·ğ‘ƒğºagainst other bugs datasets.
ACKNOWLEDGEMENTS
Thisworkwaspartiallyfunded bytheAustralianResearchCoun-
cil (ARC) through a Discovery Early Career Researcher Award
(DE190100046).
458REFERENCES
[1]Aldeida Aleti and Lars Grunske. 2015. Test data generation with a Kalman filter-
based adaptive genetic algorithm. Journal of Systems and Software 103 (2015),
343â€“352.
[2]Aldeida Aleti,Irene Moser, andLars Grunske. 2017. Analysing the fitnessland-
scapeofsearch-basedsoftwaretestingproblems. AutomatedSoftwareEngineering
24, 3 (2017), 603â€“621.
[3]M Moein Almasi, Hadi Hemmati, Gordon Fraser, Andrea Arcuri, and J Â¯anis Bene-
felds.2017. Anindustrialevaluationofunittestgeneration:Findingrealfaults
inafinancialapplication.In Proceedingsofthe39thInternationalConferenceon
SoftwareEngineering:SoftwareEngineeringinPracticeTrack.IEEEPress,263â€“272.
[4] NadiaAlshahwan,XinboGao,MarkHarman,YueJia,KeMao,AlexanderMols,
TaijinTei,andIlyaZorin.2018. Deployingsearchbasedsoftwareengineering
withSapienz atFacebook.In InternationalSymposiumon SearchBasedSoftware
Engineering. Springer, 3â€“45.
[5]AndreaArcuriandLionelBriand.2014. AHitchhikerâ€™sguidetostatisticaltests
forassessingrandomizedalgorithmsinsoftwareengineering. SoftwareTesting,
Verification and Reliability 24, 3 (2014), 219â€“250.
[6]Andrea Arcuri, JosÃ© Campos, and Gordon Fraser. 2016. Unit test generation
duringsoftwaredevelopment:Evosuitepluginsformaven,intellijandjenkins.In
2016IEEEInternationalConferenceonSoftwareTesting,VerificationandValidation
(ICST). IEEE, 401â€“408.
[7]Andrea Arcuri and Gordon Fraser. 2013. Parameter tuning or default values? An
empiricalinvestigationinsearch-basedsoftwareengineering. EmpiricalSoftware
Engineering 18, 3 (2013), 594â€“623.
[8]Victor R Basili, Lionel C. Briand, and WalcÃ©lio L Melo. 1996. A validation of
object-orienteddesignmetricsasqualityindicators. IEEETransactionsonsoftware
engineering 22, 10 (1996), 751â€“761.
[9]Manfred Broy, Ingolf H Kruger, Alexander Pretschner, and ChristianSalzmann.
2007. Engineering automotive software. Proc. IEEE 95, 2 (2007), 356â€“373.
[10]Bora Caglayan, Burak Turhan, Ayse Bener, Mayy Habayeb, Andriy Miransky,and Enzo Cialini. 2015. Merits of organizational metrics in defect prediction:
anindustrialreplication.In Proceedingsofthe37thInternationalConferenceon
Software Engineering-Volume 2. IEEE Press, 89â€“98.
[11]JosÃ©Campos,AndreaArcuri,GordonFraser,andRuiAbreu.2014.Continuoustestgeneration:enhancingcontinuousintegrationwithautomatedtestgeneration.In
Proceedings of the 29th ACM/IEEE international conference on Automated software
engineering. ACM, 55â€“66.
[12]JosÃ© Campos, Annibale Panichella, and Gordon Fraser. 2019. EvoSuiTE at the
SBST 2019 tool competition. In Proceedings of the 12th International Workshop on
Search-Based Software Testing. IEEE Press, 29â€“32.
[13]HoaKhanhDam,TrangPham,ShienWeeNg,TruyenTran,JohnGrundy,Aditya
Ghose, Taeksu Kim, and Chul-Joo Kim. 2019. Lessons learned from using a deep
tree-based model for software defect prediction in practice. In Proceedings of the
16th International Conference on Mining Software Repositories. IEEE Press, 46â€“57.
[14]Paulo AndrÃ© Faria de Freitas. 2015. Software Repository Mining Analytics to
Estimate Software Component Reliability. (2015).
[15]EvoSuite. 2019. EvoSuite - automated generation of JUnit test suites for Java
classes. https://github.com/EvoSuite/evosuite Last accessed on: 29/11/2019.
[16]The Apache Software Foundation. 2019. Apache Commons Math. https:
//github.com/apache/commons-math Last accessed on: 19/09/2019.
[17] Martin Fowler and Matthew Foemmel. 2006. Continuous integration.[18]
Gordon Fraser. 2018. EvoSuite - Automatic Test Suite Generation for Java. http:
//www.evosuite.org/ Last accessed on: 19/09/2019.
[19] Gordon Fraser and Andrea Arcuri. 2011. Evolutionary generation of whole test
suites. In 2011 11th International Conference on Quality Software. IEEE, 31â€“40.
[20]Gordon Fraser and Andrea Arcuri. 2012. Whole test suite generation. IEEE
Transactions on Software Engineering 39, 2 (2012), 276â€“291.
[21]G. Fraser and A. Arcuri. 2013. EvoSuite at the SBST 2013 Tool Competition.
In2013IEEESixthInternationalConferenceonSoftwareTesting,Verificationand
Validation Workshops. 406â€“409. https://doi.org/10.1109/ICSTW.2013.53
[22]Gordon Fraser and Andrea Arcuri. 2014. EvoSuite at the Second Unit Testing
Tool Competition. In Future Internet Testing , Tanja E.J.Vos, Kiran Lakhotia, and
Sebastian Bauersfeld (Eds.). Springer International Publishing, Cham, 95â€“100.
[23]Gordon Fraser and Andrea Arcuri. 2014. A large-scale evaluation of automated
unittestgenerationusingevosuite. ACMTransactionsonSoftwareEngineering
and Methodology (TOSEM) 24, 2 (2014), 8.
[24]GordonFraserandAndreaArcuri.2015. 1600faultsin100projects:automatically
findingfaultswhileachievinghighcoveragewithevosuite. EmpiricalSoftware
Engineering 20, 3 (2015), 611â€“639.
[25]Gordon Fraser and Andrea Arcuri. 2016. EvoSuite at the SBST 2016 tool compe-
tition. In 2016 IEEE/ACM 9th International Workshop on Search-Based Software
Testing (SBST). IEEE, 33â€“36.
[26]Gordon Fraser, JosÃ© Miguel Rojas, and Andrea Arcuri. 2018. Evosuite at the
SBST2018ToolCompetition.In Proceedingsofthe11thInternationalWorkshop
onSearch-BasedSoftwareTesting (SBSTâ€™18).ACM,NewYork,NY,USA,34â€“37.
https://doi.org/10.1145/3194718.3194729[27]GordonFraser,JosÃ©MiguelRojas,JosÃ©Campos,andAndreaArcuri.2017. Evo-
Suite at the SBST 2017 Tool Competition. In Proceedings of the 10th International
WorkshoponSearch-BasedSoftwareTesting (SBSTâ€™17).IEEEPress,Piscataway,
NJ, USA, 39â€“41. https://doi.org/10.1109/SBST.2017..6
[28]Gordon Fraser, Matt Staats, Phil McMinn, Andrea Arcuri, and Frank Padberg.
2013. Does automated white-box test generation really help software testers?. In
Proceedings of the 2013 International Symposium on Software Testing and Analysis.
ACM, 291â€“301.
[29]AndreFreitas.2015. Schwa. https://pypi.org/project/Schwa Lastaccessedon
16/09/2019.
[30]AndrÃ©Freitas.2015. schwa. https://github.com/andrefreitas/schwa Lastaccessed
on 16/09/2019.
[31]GregoryGay.2017.Generatingeffectivetestsuitesbycombiningcoveragecriteria.
InInternationalSymposiumonSearchBasedSoftwareEngineering .Springer,65â€“
82.
[32]Emanuel Giger, Marco Dâ€™Ambros, Martin Pinzger, and Harald C Gall. 2012.
Method-level bug prediction. In Proceedings of the 2012 ACM-IEEE International
Symposium on Empirical Software Engineering and Measurement. IEEE, 171â€“180.
[33] Git. 2019. Git. https://git-scm.com Last accessed on: 19/09/2019.[34]
ToddLGraves,AlanFKarr,JamesSMarron,andHarveySiy.2000. Predicting
fault incidence using software change history. IEEE Transactions on software
engineering 26, 7 (2000), 653â€“661.
[35]AndrewHabibandMichaelPradel.2018. Howmanyofallbugsdowefind?a
studyof staticbug detectors.In Proceedingsof the33rd ACM/IEEEInternational
Conference on Automated Software Engineering. 317â€“328.
[36]MarkHarman,YueJia,andYuanyuanZhang.2015. Achievements,openproblems
andchallengesforsearchbasedsoftwaretesting.In 2015IEEE8thInternational
Conference on Software Testing, Verification and Validation (ICST). IEEE, 1â€“12.
[37]Hideaki Hata, Osamu Mizuno, and Tohru Kikuno. 2012. Bug prediction based on
fine-grainedmodulehistories.In 201234thinternationalconferenceonsoftware
engineering (ICSE). IEEE, 200â€“210.
[38]ReneJust.2019. Defects4J-ADatabaseofRealFaultsandanExperimentalIn-
frastructuretoEnableControlledExperimentsinSoftwareEngineeringResearch.
https://github.com/rjust/defects4j Last accessed on: 02/10/2019.
[39]RenÃ©Just,DarioushJalali,andMichaelDErnst.2014. Defects4J:Adatabaseof
existing faults to enable controlled testing studies for Java programs. In Proceed-
ings of the 2014 International Symposium on Software Testing and Analysis. ACM,
437â€“440.
[40]Sunghun Kim, Thomas Zimmermann, E James Whitehead Jr, and Andreas Zeller.
2007. Predictingfaultsfromcachedhistory.In Proceedingsofthe29thinternational
conference on Software Engineering. IEEE Computer Society, 489â€“498.
[41]XuanBachDLe,DavidLo,andClaireLeGoues.2016. Historydrivenprogram
repair. In 2016 IEEE 23rd International Conference on Software Analysis, Evolution,
and Reengineering (SANER), Vol. 1. IEEE, 213â€“224.
[42]Chris Lewis, Zhongpeng Lin, Caitlin Sadowski, Xiaoyan Zhu, Rong Ou, andE James Whitehead Jr. 2013. Does bug prediction support human developers?
findings from a google case study. In Proceedings of the 2013 International Confer-
ence on Software Engineering. IEEE Press, 372â€“381.
[43]Chris Lewis and Rong Ou. 2011. Bug Prediction at Google. http://google-
engtools.blogspot.com/2011/12/ Last accessed on: 16/09/2019.
[44]KeMao,MarkHarman,andYueJia.2016.Sapienz:Multi-objectiveautomatedtest-
ing for Android applications. In Proceedings of the 25th International Symposium
on Software Testing and Analysis. 94â€“105.
[45]Tim Menzies, JeremyGreenwald, and ArtFrank. 2006. Data miningstatic code
attributes to learn defect predictors. IEEE transactions on software engineering 33,
1 (2006), 2â€“13.
[46]NachiappanNagappanand ThomasBall.2005. Use ofrelativecodechurnmea-
sures to predict system defect density. In Proceedings of the 27th international
conference on Software engineering. ACM, 284â€“292.
[47]NachiappanNagappan,BrendanMurphy,andVictorBasili.2008. Theinfluenceoforganizationalstructureonsoftwarequality.In 2008ACM/IEEE30thInternational
Conference on Software Engineering. IEEE, 521â€“530.
[48]Nachiappan Nagappan, Andreas Zeller, Thomas Zimmermann, Kim Herzig, and
Brendan Murphy. 2010. Change bursts as defect predictors. In 2010 IEEE 21st
International Symposium on Software Reliability Engineering. IEEE, 309â€“318.
[49]Carlos Oliveira,Aldeida Aleti,Lars Grunske,and KateSmith-Miles. 2018. Map-
ping the effectiveness of automated test suite generation techniques. IEEE Trans-
actions on Reliability 67, 3 (2018), 771â€“785.
[50]Carlos Oliveira, Aldeida Aleti, Yuan-Fang Li, and Mohamed Abdelrazek. 2019.
Footprints of Fitness Functions in Search-Based Software Testing. In Proceedings
oftheGeneticandEvolutionaryComputationConference(GECCOâ€™19).Association
for Computing Machinery, 1399â€“1407. https://doi.org/10.1145/3321707.3321880
[51]Annibale Panichella, Fitsum Meshesha Kifetew, and Paolo Tonella. 2015. Refor-
mulating branch coverage as a many-objective optimization problem. In 2015
IEEE8thinternationalconferenceonsoftwaretesting,verificationandvalidation
(ICST). IEEE, 1â€“10.
[52]Annibale Panichella, Fitsum Meshesha Kifetew, and Paolo Tonella. 2017. Au-tomated test case generation as a many-objective optimisation problem with
459dynamic selection of the targets. IEEE Transactions on Software Engineering 44, 2
(2017), 122â€“158.
[53]Annibale Panichella, Fitsum Meshesha Kifetew, and Paolo Tonella. 2018. A large
scale empirical comparison of state-of-the-art search-based test case generators.
Information and Software Technology 104 (2018), 236â€“256.
[54]DavidPaterson,JoseCampos,RuiAbreu,GregoryMKapfhammer,GordonFraser,
and Phil McMinn. 2019. An Empirical Study on the Use of Defect Prediction
for Test Case Prioritization. In 2019 12th IEEE Conference on Software Testing,
Validation and Verification (ICST). IEEE, 346â€“357.
[55]Spencer Pearson, JosÃ© Campos, RenÃ© Just, Gordon Fraser, Rui Abreu, Michael D
Ernst, Deric Pang, and Benjamin Keller. 2017. Evaluating and improving fault
localization. In Proceedings of the 39th International Conference on Software Engi-
neering. IEEE Press, 609â€“620.
[56]Foyzur Rahman, Daryl Posnett, Abram Hindle, Earl Barr, and Premkumar De-
vanbu.2011. BugCacheforinspections:hitormiss?.In Proceedingsofthe19th
ACM SIGSOFT symposium and the 13th European conference on Foundations of
software engineering. ACM, 322â€“331.[57]JosÃ©MiguelRojas,MattiaVivanti,AndreaArcuri,andGordonFraser.2017. A
detailedinvestigationoftheeffectivenessofwholetestsuitegeneration. Empirical
Software Engineering 22, 2 (2017), 852â€“893.
[58]Urko Rueda, Tanja EJ Vos, and ISWB Prasetya. 2015. Unit Testing ToolCompetitionâ€“Round Three. In 2015 IEEE/ACM 8th International Workshop on
Search-Based Software Testing. IEEE, 19â€“24.
[59]SinaShamshiri,ReneJust,JoseMiguelRojas,GordonFraser,PhilMcMinn,and
Andrea Arcuri. 2015. Do automatically generated unit tests find real faults?
an empirical study of effectiveness and challenges (t). In 2015 30th IEEE/ACM
InternationalConferenceonAutomatedSoftwareEngineering(ASE).IEEE,201â€“211.
[60]AndrÃ¡s Vargha and Harold D Delaney. 2000. A critique and improvement of
the CL common language effect size statistics of McGraw and Wong. Journal of
Educational and Behavioral Statistics 25, 2 (2000), 101â€“132.
[61]Thomas Zimmermann, Rahul Premraj, and Andreas Zeller. 2007. Predicting
defectsforeclipse.In ThirdInternationalWorkshoponPredictorModelsinSoftware
Engineering (PROMISEâ€™07: ICSE Workshops 2007). IEEE, 9â€“9.
460