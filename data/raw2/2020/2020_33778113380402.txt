Time-travel Testing of Android Apps
Zhen Dong
National University of Singapore
zhen.dong@comp.nus.edu.sgMarcel B√∂hme
Monash University, Australia
marcel.boehme@monash.edu
Lucia Cojocaru
Politehnica University of Bucharest
lucia.cojocaru@stud.acs.upb.roAbhik Roychoudhury
National University of Singapore
abhik@comp.nus.edu.sg
ABSTRACT
Androidtestingtoolsgeneratesequencesofinputeventstoexer-
cise the state space of the app-under-test. Existing search-based
techniquessystematicallyevolveapopulationofeventsequences
so as to achieve certain objectives such as maximal code coverage.
The hope is that the mutation of fit event sequences leads to the
generationofevenfittersequences.Ho wever,theev olutionofevent
sequences may be ineffective. Our key insight is that pertinent app
stateswhich contributed to the original sequence‚Äôs fitness may not
bereachedbyamutatedeventsequence.Theoriginalpaththrough
the state space is truncated at the point of mutation.
Inthispaper,weproposeinsteadtoevolveapopulationofstates
which can be captured upon discovery and resumed when needed.
Thehopeisthatgeneratingeventsonafitprogramstateleadsto
the transition to even fitter states. For instance, we can quickly
deprioritize testing the main screen state which is visited by most
eventsequences,andinsteadfocusourlimitedresourcesontesting
more interesting states that are otherwise difficult to reach.
Wecallourapproach time-traveltesting becauseofthisability
totravelbacktoanystatethathasbeenobservedinthepast.We
implementedtime-traveltestingintoTimeMachine,atime-travel
enabled versionof thesuccessful, automated Android testingtool
Monkey.Inourexperimentsonalargenumberofopen-andclosed
sourceAndroidapps,TimeMachineoutperformsthestate-of-the-
art search-based/model-based Android testing tools Sapienz and
Stoat, both in terms of coverage achieved and crashes found.
1 INTRODUCTION
Android app testing has been gaining in importance. In 2020, there
is a smart phone for every third person (2.9 billionusers) while
app revenue will double from 2016 (US$ 88 to 189 billion).1The
number of bugs and vulnerabilities in mobile apps are growing. In
2016,24.7%ofmobileappscontainedatleastonehigh-risksecurity
flaw[1].TheAndroidtestingmarketisalsoexpectedtodoublein
five years from US$ 3.21 billionin 2016 to US$ 6.35 billionin 2021.2
1https://www.statista.com/statistics/269025/worldwide-mobile-app-revenue-forecast/2https://www.businesswire.com/news/home/20170217005501/en/
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ICSE ‚Äô20, May 23‚Äì29, 2020, Seoul, Republic of Korea
¬© 2020 Association for Computing Machinery.
ACM ISBN 978-1-4503-7121-6/20/05...$15.00
https://doi.org/10.1145/3377811.3380402
(a) Excerpt of the map of Maridia
where pink marks explored rooms.
(b) Samus discovering the Spazer
weapon
Figure 1: Super Metroid on an Android Emulator
ToillustratethechallengesofexistingAndroidtestingtools,take
forexampleSuperMetroid(Fig.1),oneofthebestgamesforthe
NES gaming console,now available for Android. SuperMetroid is
playedonalargemapofroomsthatcanbeexploredinanyorder.
By pushing the right buttons on the controller, the main character
Samusmovesfromoneroomtothenext,findingsecretsandgaining
in strength by fighting enemies. Today, Android app testing is like
playing a game of Super Metroid, albeit withoutthe ability to save
after important milestones and to travel back in time when facing
the consequences of a wrong decision.
One possible approach is to generate a single, very long sequence
of events in a random fashion [ 3]. However, the testing tool may
ultimatelygetstuckindeadends.Forinstance,Samusmayfallinto
pits or get lost in a particularly complex part of the labyrinth. This
problemisovercome onlypartially byrestartingthe Androidapp
because (i) we must start from the beginning, (ii) there is no clean
slate, e.g., database entries remain, and (iii) how to detect when we
are stuck isstill an open question. ForAndroid testing, the ability
tosaveandtravelbacktothemostinterestingstatesgoesalong
way towards a more systematic exploration of the state space.
AnotherAndroidapptestingapproach[ 36]istoevolveapopula-
tion of event sequences in a search-based manner. In each iteration,
the fittest event sequences are chosen for mutation to generate the
next generation of event sequences. An event sequence is mutated
by adding, modifying, or removing arbitrary events. However, this
approach does not allow for systematic state space exploration by
traversingthevariousenabledeventsfromastate.If eiinthese-
quenceE=/angbracketlefte1,...,ei,...en/angbracketrightismutated,thenthesuffixstarting
inei+1maynolongerbeenabled.Forinstance,whenSamusstands
next to an enemy or a ledge after event ei‚àí1and the event eiis
turnedfromapressofthe [‚áê]-buttontoapressofthe [‚áí]-button,
Samus may be killed or get stuck. The remaining events starting
fromei+1becomeimmaterial;roomsthatwerereachedby Emay
not be reached by its mutant offspring.
4812020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE)
In this paper, we propose instead to evolve a population of states
which can be captured upon discovery and resumed when needed.
By capturing and resuming an app‚Äôs states, we seek to achieve a
systematicstatespaceexploration(withoutgoingtotheextentof
exhaustive exploration as in formal verification). Due to the ability
totravelbacktoanypaststate,wecallthisastime-traveltesting.
Our novel time-travel testing approach systematically resets the
entire system‚Äîthe Android app and all of its environment‚Äîto the
mostprogressivestatesthatwereobservedinthepast.A progressive
stateis one which allows us to discover new states when different
input events are executed. Once the tool gets stuck, it goes back in
time and resumes a progressive state to execute different events.
Weimplementtime-traveltestingforAndroidappsinto Time-
Machine3a time-travel-enabled variant of the automated Android
testing tool Monkey [ 3]. In our example, one can think of Time-
Machine as an automatic player that explores the map of Super
Metroidthroughveryfastrandomactions,automaticallysavesafter
important milestones, and once it gets stuck or dies, it travels back
to secret passages and less visited rooms seen before in order to
maximizethe coverageofthe map.Comparedto toolsthatevolve
event sequences, such as Sapienz [ 36],TimeMachine does not mu-
tate thesequence prefix which is required to reach the fittest, most
progressive state, and instead generates only the sequence suffix
startingfromthatstate.Comparedtotoolsthatgenerateasingle,
very long event sequence, such as Monkey [ 3] or Stoat [ 40],Time-
Machine automatically detects when it gets stuck (i.e., there is a
lackofprogress)andresumesthatstateforfurthertestingwhichis
mostpromisingforfindingerrors.InourexperimentswithSapienz,Stoat,andMonkeyonbothopen-sourceandclosed-sourceAndroid
apps TimeMachine substantially outperformed the state-of-the-art
in terms of both, coverage achieved and errors found.
TimeMachine canbeseededwithasetofinitialeventsequences.
Atthebeginningofatestingsession, TimeMachine takesasnapshot
ofthestartingstate.Duringtestexecution, TimeMachine takesa
snapshotofeveryinterestingstateandaddsittothe statecorpus,
travels back to the interesting state and executes the next test.Foreachtransitionfromonestatetoanother, TimeMachine also
recordstheshortesteventsequence.Ifnoinitialtestsetisprovided,
TimeMachine only adds the starting state to the state corpus.
TimeMachine isanautomatictime-travelling-enabledtestgener-
ator for Android apps that implements several heuristics to choose
the most progressive state from the state corpus to explore next.
Intuitively,a statereachingwhichcovered newcodeandthathas
beendifficulttoreachhasmorepotentialtotriggernewprogrambe-havior. TimeMachine dynamicallycollectssuchfeedbacktoidentify
themostprogressivestate. TimeMachine identifiesaprogressive
state as one which itself was infrequently visited andtheknearest
neighbors4were visited relatively infrequently.
Ourexperimentsdemonstrateasubstantialperformanceincrease
over our baseline test generation tool‚ÄîMonkey extended withsystem-level event generator of Stoat [
40]. Given the 68 apps in
the AndroTest benchmark [ 23], our time-travel strategy enables
the baseline tool to achieve 1.15 times more statement coverageand to discover 1.73 times more unique crashes. Given 37 apps
3Named after the celebrated fictional work by H.G. Wells more than a century ago.
4Theknearest neighbors are states reachable along at most kedges.in the benchmark of industrial apps, around 900 more methods
are covered on average and 1.5 times more unique crashes are dis-
covered. Our time-travel strategy makes TimeMachine so efficient
thatitoutperformsthestate-of-the-arttestgeneratorsSapienz[ 36]
and Stoat [ 40] both in terms of coverage as well as errors found,
detecting around 1.5 times more unique crashes than the next best
test generator. TimeMachine tested the Top-100 most popular apps
from Google Play and found 137 unique crashes.
In summary, our work makes the following contributions:
‚Ä¢We propose time-travel testing for Android which resumes
the most progressive states observed in the past so as to
maximize efficiency during the exploration of an app‚Äôs state
space.Theapproachidentifiesandcapturesinterestingstates
as save points, detects when there is a lack of progress, and
resumesthe mostprogressivestates forfurther testing.For
instance, it can quickly deprioritize the main screen state
whichisvisitedbymostsequences,andresume/testdifficult-
to-reach states. We propose several heuristics that guide
execution to a progressive state when progress is slow.
‚Ä¢We implement the time-travel testing framework and an
automated,feedback-guided,time-travel-enabledstatespace
explorationtechniqueforAndroidapps.Theframeworkand
testing technique are evaluated on both open-source and
closed-sourceAndroidappbenchmarks,aswellastop-100
popular apps from Google Play. We have made our time-
travel Android app testing tool TimeMachine publicly avail-
able on Github: https://github.com/DroidTest/TimeMachine
2 TIME-TRAVEL FRAMEWORK
We design a general time-travel framework for Android testing,
which allows us to save a particular discovered state on the fly
andrestoreitwhenneeded.Figure2showsthetime-travelinfra-
structure. The Android app can be launched either by a human
developeroranautomatedtestgenerator.Whentheappisinter-
acted with, the state observer module records state transitions and
monitors the change of code coverage. States satisfying a prede-
finedcriteriaaremarkedas interesting,andaresavedbytakinga
snapshot of the entire simulated Android device. Meanwhile the
framework observes the app execution to identify when there is a
lack of progress, that is, when the testing tool is unable to discover
any new program behavior over the course of a large number of
state transitions. When a ‚Äúlack of progress‚Äù is detected, the frame-
work terminates the current execution, selects, and restores the
most progressive one among previously recorded states. A more
progressivestate isonethatallowsustodiscovermorestatesquickly.
When we travelback to the progressive state,an alternative event
sequence is launched to quickly discover new program behaviors.
Theframeworkisdesignedaseasy-to-useandhighly-configurable.
Existing testingtechniques canbe deployedon the frameworkby
implementing the following strategies:‚Ä¢
Specifyingcriteria whichconstitute an‚Äúinteresting‚Äù state,e.g.,
increases code coverage. Only those states will be saved.
‚Ä¢Specifyingcriteriawhichconstitute‚Äúlackofprogress‚Äù,e.g.,whentesting techniques traverse the same sequence of states in a loop.
‚Ä¢Providinganalgorithmtoselectthemostprogressivestatefor
time-travelling when a lack of progress is detected.
482Automated test 
generators, e.g. MonkeyState 
identificationCoverage
monitorState 
recorder
State
manager Snapshot
creator
Lack of progress
detection
Snapshot
restorerSnapshot PoolInteresting state 
detection
Progressive 
state selectionState transition
State graphA snapshot
Android OS
State observer
A snapshot
Developer
Figure 2: Time travel framework. Modules in grey are configurable, allowing users to adjust strategy according to scenarios.
2.1 Taking Control of State
Stateidentification.Inordertoidentifywhatconstitutesastate,our
frameworkcomputesanabstractionofthecurrentprogramstate.
AprogramstateinAndroidappisabstractedasanapppagewhich
isrepresentedasawidgethierarchytree(non-leafnodesindicate
layout widgets and leaf nodes denote executable or displaying wid-
gets such as buttons and text-views). A state is uniquely identified
bycomputingahashoveritswidgethierarchytree.Inotherwords,
when a page‚Äôs structure changes, a new state is generated.
Tomitigate thestate spaceexplosion problem, weabstract away
values of text-boxes when computing the hash over a widget hier-
archy tree. By the above definition, a state comprises of all widgets
(andtheirattributes)inanapppage.Anydifferenceinthosewidgets
orattributevaluesleadstoadifferentstate.Someattributessuchas
text-boxvaluesmayhavehugeorinfinitenumberofpossiblevalues
that can be generated during testing, which causes a state space
explosion issue. To find a balance between accurate expressiveness
ofa stateand statespace explosion,we ignoretext-box values for
stateidentification.OurpracticethataGUIstateisdefinedwithoutconsideringtext-boxvaluesisadoptedbypreviousAndroidtesting
works as well [21, 22].
State saving& restoring. We leverage virtualizationto save and
restoreastate.Ourframeworkworksontopofavirtualmachine
where Android apps can be tested. A virtual machine (VM) is a
softwarethatrunsafullsimulationofaphysicalmachine,including
the operating system and the application itself. For instance, a VM
withanAndroidimageallowsustorunAndroidappsonadesktop
machinewhererelatedhardwaresuchastheGPSmodulecanbe
simulated. App states can be saved and restored with VM.
Our framework records a program state by snapshotting the
entire virtual machine state including software and emulated hard-
ware inside. States of the involved files, databases, third-party li-
braries, and sensors on the virtual device are kept in the snapshot
so that the state can be fully resumed by restoring the snapshot.
This overcomes the challenge that a state may not be reached from
the initial state by replaying the recorded event sequence due to
state change of background services.2.2 Collecting State-Level Feedback
Toidentifywhetherastateis‚Äúinteresting‚Äù,ourframeworkmonitors
thechangeincodecoverage.Wheneveranewstateisgenerated,
code coverage is re-computed to identify whether the state haspotential to cover new code via the execution of enabled events.
Ourframework supportsbothopen-source andclose-sourceapps.
For open-source apps, we collect statement coverage using theEmma coverage tool [
9]. For closed-source, industrial apps, we
collectmethodcoverageusingtheEllacoveragetool[ 8].Forclosed-
source apps, statement coverage is difficult to obtain.
Our framework uses a directed graph to represent state tran-
sitions, where a node indicates a discovered state and an edge
representsastatetransition.Eachnodemaintainssomeinforma-
tion about the state:whether there is a snapshot (only states with
snapshotscanberestored),howoftenithasbeenvisited,howoften
ithasbeenrestored,andsoon.Thisinformationcanbeprovided
totestingtoolsorhumantesterstoevaluatehowwellastatehas
been tested and to guide execution.
3 METHODOLOGY
We develop the first time-travel-enabled test generator Time-
Machine forAndroidappsbyenhancingAndroidMonkey[ 3]with
our framework. TimeMachine‚Äôs procedure is presented in Algo-
rithm 1. TimeMachine‚Äôs objective is to maximize state and code
coverage. TimeMachine startswithasnapshotoftheinitialstate
(lines 1-4). For each event that Monkey generates, the new state
is computed and the state transition graph updated (lines 5-9). Ifthe state isInteresting (Sec. 3.1), a snapshot of the VM is taken
andassociatedwiththatstate(lines10-13).IfMonkeyisStuckand
no more progess is made (Sec. 3.2), TimeMachine finds the most
progressivestate(selectFittestState;Sec.3.3)andrestoresthe
associated VM snapshot (lines 14-17). Otherwise, a new event is
generated and loop begins anew (lines 5-18).
3.1 Identifying Interesting States
TimeMachine identifies an interesting state based on changes in
GUIorcodecoverage(Line10inAlgorithm1).ThefunctionisIn-
teresting( state)returnstrueif(1) stateisvisitedforthefirsttime,
and (2) when statewas first reached new code was executed.
483Algorithm 1: Time-travel testing (TimeMachine).
Input:AndroidApp, Sequence generator Monkey
1:StatecurState ‚Üêlaunch(App)
2:Save VM snapshot of curState
3:Interesting states states‚Üê{curState}
4:State Transition Graph stateGraph ‚ÜêinitGraph( curState)
5:for each EventeinMonkey.generateEvent() do
6:iftimeout reached then break; end if
7:prevState ‚ÜêcurState
8:curState ‚ÜêexecuteEvent( App,e)
9:stateGraph ‚ÜêupdateGraph( prevState ,curState)
10:ifisInteresting( curState ,stateGraph) then
11:Save VM snapshot of curState
12:states‚Üêstates‚à™{curState}
13:end if
14:ifisStuck(curState ,stateGraph) then
15:curState ‚ÜêselectFittestState( states,stateGraph)
16:Restore VM snapshot of curState
17:end if
18:end for
Output: State Transition Graph stateGraph
The intuition behind our definition of ‚Äúinteresting" states is that
theexecutionofnewcodeprovidestheevidencethatafunctionality
thathasnotbeentestedbeforeisenabledinthediscoveredstate.
Morenewcode relatedtothefunctionalitymightbeexecutedbyex-
ploringthisstate.Forinstance,supposeclickingabuttononscreen
S1leads to a new screen S2, from where a new widget is displayed
(increasing code coverage). The new widget comes with its own
event handlers that have not been executed. These event handlers
canbecoveredbyfurtherexploringscreen S2.Thisheuristicnot
onlyaccuratelyidentifiesaninterestingstate( S2inthiscase)but
also significantly reduces the total number of saved states (since
only interesting states are saved during testing).
3.2 Identifying Lack of Progress
Thetestingprocesscanstayunprogressivewithoutdiscoveringany
new program behavior for quite some time. As reasonsfor Monkey
getting stuck, we identified loops and dead ends.
Loops.Aloopisobservedwhenthesamefew(high-frequency)
states are visited again and again. To easily perform routine activ-
ities, app pages are typically organized under common patterns,
e.g.,fromthemainpageonecanreachmostotherpages.Thisde-
signleadstoaphenomenonwhererandomeventstendtotrigger
transitions to app pages which are easy to trigger. Moreover, apps
often browse nested data structures, it is difficult to jump out from
them without human knowledge. For example, let us consider the
AnyMemo [ 7] app, a flashcard learning app we tested. Monkey
clicks a button to load a CSVfile and arrives at an app page that
browsessystemdirectories.Itkeepsonexploringdirectoriesand
cannotleavethisapppageuntilitfindsa CSVfiletoload(orbypress-
ing the ‚ÄúBack‚Äù button many times in a row). In our experiments,
Monkey could not jump out of the loop within 5000 events.Algorithm 2: Detecting loops and dead-ends (isStuck).
Input:Queue length l
Input:Lack-of-progress threshold maxNoPro–¥ress
Input:Max. top (Œ±¬∑100)% most frequently visited states
Input:Max. proportion Œ≤of repeated plus frequent states
1:FIFOQueue‚Üêempty queue of length l
2:noPro–¥ress =0// #events since last state transition
3:
4:procedure isStuck(State curState, GraphstateGraph){
5:prevStateID =Queue.top()
6:ifprevStateID ==curState .IDthen
7:noPro–¥ress ‚ÜênoPro–¥ress +1
8:else
9:Queue.push(curState .ID)
10:noPro–¥ress =0
11:end if
12:ifnoPro–¥ress >maxNoPro–¥ress then
13:return true // detect dead ends
14:end if
15:ifQueue.length == lthen
16:nRepeated ‚ÜêcountMaxRepeatedStates( Queue)
17:nFrequent ‚ÜêcountFreqStates( Queue,stateGraph ,Œ±)
18:if(nRepeated +nFrequent )/l>Œ≤then
19: return true // detect loops
20:end if
21:end if
22:return false
23:}
Deadends. Adeadendisastatewhichisdifficulttoexit.Some
pagesrequirespecificinputswhichareveryunlikelytoberandomly
generated.Monkeycanbetrappedbythemandcankeepongener-
ating events without making any ‚Äúprogress". For instance, consider
anapppageinAnyMemo[ 7]whereaformneedstobefilledand
submitted. Yet, the ‚ÄúSubmit‚Äù button is located at the bottom of the
page, and does not even appear on screen. Monkey would need to
correctly fill in certain parameters, scroll all the way to the bottom,andthengeneratea‚ÄúClick‚Äùeventonthebuttontotransitiontoexit
the page. This is quite unlikely. Monkey gets stuck in a dead end.
When TimeMachine gets stuck, the most progressive state is
traveled back to (lines 14-17 in Algorithm 1). The function isStuckissketchedinAlgorithm2andrealizesaslidingwindowalgorithm.
Firstly, four parameters must be specified, which are explained
later.Therearetwoglobalvariables,aqueueof specifiedlengthl
and a counter which keeps track how often the same state has
been observed (lines 1-3). Given the current app state and the state
transition graph, if the current state is the same as the previous
statetheno-progresscounterisincremented(lines4-7).Otherwise,
the counter is reset (lines 8-11). If the counter exceeds the specified
maximum ( maxNoPro–¥ress ), then a dead end is detected (lines 12-
14).Ifthefixed-lengthqueueis filledandtheproportionof‚Äúeasy‚Äù
states in the queue surpasses the specifiedthreshold Œ≤, then a loop
isdetected.Twokindsofstatesinthequeueareconsidered easy:
states occurring multiple times in the queue, and states among the
topŒ±percentage of the most frequently visited states.
484Algorithm 3: Selecting the next state
(selectFittestState)
Input:Path length k
1:procedure selectFittestState( states,stateGraph){
2:bestFitness ‚Üê0
3:for each stateinstatesdo
4:stateFitness ‚Üê0
5:paths‚Üêall paths in stateGraph of length kfromstate
6:for each pathinpathsdo
7: for each Nodesinpathdo
8: stateFitness ‚ÜêstateFitness +f(s)// see Eq. (1)
9: end for
10:end for
11:stateFitness ‚ÜêstateFitness
|paths|
12:ifstateFitness >bestFitness then
13: bestState =state
14: bestFitness =stateFitness
15:end if
16:end for
17:returnbestState
18:}
3.3 Progressive State Selection
In order to select a state to travel back to once Monkey isStuck,
we assign a fitness to each state which evaluates its potential to
trigger new program behavior (lines 14-17 in Alg. 1). The fitness
f(s)of a state sis determined by the number of times the state has
been visited and the number of ‚Äúinteresting‚Äù states generated from
it. Concretely, the fitness function is defined as:
f(s)=f0‚àó(1+r)w(s)‚àó(1‚àíp)v(s)‚àíw(s)(1)
wherev(s)isthenumberoftimesstate sisvisitedand w(s)isthe
number of ‚Äúinteresting states‚Äù generated from state s;ris a reward
of finding an interesting state and pis a penalty of transiting to
astatethathasalreadybeendiscovered; f0istheinitialvalue.In
TimeMachine, the initial value of an interesting state is set as 6
timesofthatofanuninterestingstate,and raswellas paresetas0 .1.
Whenastateisrepeatedlybeingvisitedandnointerestingstates
are discovered, its fitness keeps on being reduced due to penalty p
so that other state will be selected and restored eventually.
Maximizing benefit of time travel. The definition of state
fitness in Equation (1) does not account for the fact that events
executedonthatstatemayquicklytriggeradeparturefromthat
state, again advancing throughunprogressive states. To maximize
benefit of time-travel, we develop an algorithm that selects the
statewithahigh-fitnees‚Äúneighborhood‚Äù,i.e.,thestatewhichhas
neighboring states which also have a high fitness.
Algorithm3outlinestheprocessofselectingthemostprogres-
sive state for time travel. It takes as input the interesting states
thathaveanassociatedVMsnapshotandthestatetransitiongraph
thatismaintainedbyourtime-travelframework.Thenumberof
transitions kwhichdeterminesastate‚Äôs‚Äúneighborhood‚Äùmustbe
specified by the user. In our experiments, we let k=3. For each
interesting state,TimeMachine computestheaveragefitnessofa
State 
IdentificationADB Daemon
ADB Server Cov. Data CollectorGuided Event Generator
Virtualbox ManagerMonkey UIautomator
Sys Event 
GeneratorCoverage
MonitorTimeMachineAndroid Virtual Machine  (Android OS)
Docker container (Host OS)
State Corpus
VM Controller
Figure 3: Architecture of TimeMachine implementation.
stateinthe k-neighborhoodofthestate.Thestatewiththemaxi-
mumaveragestatefitnessinits k-neighborhoodisreturned.The
k-neighborhood ofstateareallstates sinstateGraph thatarereach-
able from statealong at most ktransitions. The fitnessf(s)of a
statesis computed according to Equation (1). With this algorithm,
Monkeynotonlytravelsintimetothestatewiththehighestfitness
value but also continues to explore states with high fitness values
withinktransitions, which maximizes the benefit of time travel.
4 IMPLEMENTATION
Our time travel framework is implemented as a fully automatedapp testing platform, which uses or extends the following tools:
VirtualBox [
4], the Python library pyvbox [ 11] for running and
controllingtheAndroid-x86OS[ 6],AndroidUIAutomator[ 10]for
observingstatetransitions,andAndroidDebugBridge(ADB)[ 5]for
interacting with the app under test. Figure 3 gives an architectural
overview of our platform. Components in grey are implemented
by us while others are existing tools that we used or modified.
Forcoveragecollection,ourframeworkinstrumentsopen-source
apps using Emma [ 9] (statement coverage) and closed-source apps
using Ella [ 8] (method coverage). Ella uses a client-server model
sendingcoveragedatafromtheAndroidOStotheVMhostviaa
socket connection. Unfortunately, this connection is broken every
time a snapshot is restored. To solve this issue, we modified Ella to
save coverage data on the Android OS to actively pull as needed.
Ontopofthetimetravelframework,weimplement TimeMachine.
To facilitate the analysis of all benchmarks, we integrated Time-
Machine withtwoAndroidversions. TimeMachine workswiththe
mostwidely-usedversion,AndroidNougatwithAPI25(Android
7.1). However, to perform end-to-end comparison on AndroTest
benchmark [ 23], we also implement TimeMachine on Android
KitKat version with API 19 (Android 4.4). The publicly available
version of Sapienz [ 36] (a state-of-the-art/practice baseline for our
experiments)islimitedtoAndroidAPI19andcannotrunonAn-
droid 7.1. To collect state-level feedback, we modified AndroidMonkeyandUIAutomatortomonitorstatetransitionaftereach
eventexecution. TimeMachine alsoincludes asystem-level event
generatortakenfromStoat[ 40]tosupportsystemeventssuchas
phone calls and SMSs.
5 EMPIRICAL EVALUATION
Inourexperimentalevaluation,weseektoanswerthefollowing
research questions.
485RQ1Howeffectiveisourtime-travelstrategyintermsofachieving
more code coverage and finding more crashes? We compare
TimeMachine tothebaselineintowhichitwasimplemented.
RQ2Howdoestime-traveltesting(i.e., TimeMachine)compareto
state-of-the-arttechniquesintermsofachievedcodecover-
age and found crashes?
RQ3Howdoestime-traveltesting(i.e., TimeMachine)performon
larger, real-world apps, such as industrial apps and Top-100
apps from Google Play?
5.1 Experimental Setup
To answer these research questions, we conducted three empirical
studies on both open-source and closed-source Android apps.
Study1. ToanswerRQ1,weevaluate TimeMachine andbaseline
toolsonAndroTest[ 23]andinvestigatehowachievedcodecoverage
andfoundfaultsareimprovedbyusingthetime-travelstrategy.We
choseAndroTestappsassubjectsbecauseAndroTesthasbecome
a standard testing benchmark for Android and has been used to
evaluate a large number of Android testing tools [ 16,20,23,34‚Äì
37,40,44].Itwascreatedin2015bycollectingAndroidappsthat
have been used in evaluations of 14 Android testing tools.
TimeMachine applies time-travel strategy to a baseline tool; the
baseline tool is Monkey extended with Stoat‚Äôs system-level event gen-
erator. To accurately evaluate effectiveness of time-travel strategy,
we set Monkey extended with the system-level event generator
fromStoatasbaseline(called MS).WechoseMSinsteadofMonkey
asabaselinetooltomakesurethattheimprovementachievedby
TimeMachine completelycomesfromtime-travelstrategy,notfrom
system event generation.
We also implement another variant of Monkey as baseline to
evaluate effectiveness of ‚Äúheavy components" such as state saving
andrestoringonenhancingatesttechnique.Thisvariantapplies
onlythelackofprogressdetectioncomponentofourtime-travel
strategywithoutstatesavingandrestoringcomponents.Whenlack
ofprogressisdetected,itsimplyrestartstestingfromscratch,i.e.,
re-launching app under test without resuming states (called MR).
InTimeMachine,parameters l,maxNoPro–¥ress ,Œ±,Œ≤forisStuck
inAlg.2aresetto10 ,200,0.2,and0.8,respectively.Thesevalues
were fixed during initial experiments of two authors with three
appsfromAndroTest(Anymemo,Bites,aCal).Weexecutedthese
apps with Monkey for many rounds and recorded relevant data
such as the number of state transitions when a loop was observed
andthenumberofexecutedeventswhenMonkeyjumpedoutfrom
a dead end. Based on observed data and authors‚Äô heuristics, we
cameupwithseveralgroupsofvaluesandevaluatedthemonthese
threeapps,andeventuallychoseabovedataasdefaultparameter
values. In the evaluation, TimeMachine used the default values
for all the three studies. Baseline tool MS and MR use the same
parameter values as in TimeMachine.
Study 2. ToanswerRQ2,weevaluate TimeMachine andstate-
of-the-art app testing tools on AndroTest and compare them interms of achieved code coverage and found crashes. For state-of-the-art tools, we chose Monkey [
3],Sapienz[36], andStoat[40].
Monkey is an automatic random event sequence generator fortesting Android apps and has been reported to achieve the best
performance in two works [ 23,42]. Sapienz and Stoat are the mostrecenttechniquesforAndroidtesting.Thesetestingtoolshavealso
beenadequatelytestedandarestandardbaselinesintheAndroid
testingliterature.Tohaveafaircomparison,alltechniquesusetheir
default configuration.
Study 3. To answer RQ3, we evaluate TimeMachine, baseline
toolsandallstate-of-the-arttechniquesonlargereal-worldAndroid
apps, and investigate whether they have a consistent performance
on both closed-source and open-source Androidapps. In thiseval-
uation, we use IndustrialApps [42] as subject apps. IndustrialApps
wasabenchmarksuitecreatedin2018toevaluatetheeffectiveness
ofAndroidtestingtools onreal-worldapps.Theauthorssampled
68appsfromtop-recommendedappsineachcategoryonGoogle
Play,andsuccessfullyinstrumented41appswithamodifiedver-sion of Ella [
8]. In our experiment, we chose to use the original
version of Ella and successfully instrumented 37 apps in Industrial
app-suite. On this benchmark, we could not compare with Sapienz
because the publicly available version of Sapienzis limited to an
older version of Android (API 19).
Tofurtherinvestigatetheusabilityof TimeMachine,weevaluate
TimeMachine on Top-100 popular Android apps from Google Play
andinvestigatewhether TimeMachine caneffectivelydetectcrashes
in online apps, i.e., those available for download from Google Play
at the time of writing. Following the practice adopted by some
previous authors [ 36,40] of applying the technique to top popular
apps on Google Play, we focus on analyzing detected crashes by
TimeMachine and do not compare TimeMachine with state-of-the-
arttechniquesonthisdataset.Top-100popularappswerecollected
bydownloadingthemosthighlyrankedappson GooglePlayand
instrumenting them withour coverage tool Ella until we obtained
100 apps that could be successfully instrumented by Ella.
Procedure. Tomitigateexperimenterbiasandtoscaleourex-
periments,wechosetoprovidenomanualassistanceduringtesting
in all studies. For all test generators, the Android testing is fully
automatic. None of the test generators is seeded with an initial set
of event sequences. The testing process is automatically started af-
ter installation. All data are generated and processed automatically.
We neither provide any input files, nor create any fake accounts.
Each experiment is conducted for six (6) hours and repeated five
(5) times totalling 35580 CPU hours ( ‚âà4.1 year). To mitigate the
impact of random variations during the experiments, we repeated
each experiment five times and report the average. In comparison,
the authors of Sapienz report one repetition of one hour while the
authors of Stoat report on five repetitions of three hours. We chose
atimebudgetofsixhoursbecausewefoundthattheasymptotic
coverage was far from reached after three hours in many apps (i.e.,
no saturation had occurred).
Coverage&Crashes .Wemeasurecodecoverageachievedand
errorsdiscoveredwithinsixhours.To measurestatementormethod
coverage,weuseEmmaandElla,thesamecoveragetoolsthatare
usedinSapienzandStoat.To measurethenumberofuniquecrashes
detected, we parse the output of Logcat,5an ADB tool that dumps
alogofsystemmessages.Weusethefollowingprotocoltoidentify
a unique crash from the error stack (taken from Su et al. [40]):
‚Ä¢Removeallunrelatedcrashesbyretainingonlyexceptions
containing the app‚Äôs package name (and filtering others).
5https://developer.android.com/studio/command-line/logcat
486Table 1: Results from AndroTest (68 open-source apps).
SubjectsTimeMachine Baselines State-of-the-art
%Cov #Cra #State %Coverage #Crashes %Coverage #Crashes
--- MS MR MS MR ST SA MO ST SA MO
A2DP 46 1 222 42 35 0 0 47 44 39 1 40
aagtl 19 5 34 16 16 3 3 1719 17 3 63
Aarddict 19 2 31 18 14 1 0 37 15 13 500
aCal 299 178 28 26 7 1 22 27 18 7 5 3
addi 19 2 63 19 19 3 41420 19 3 1 4
adsdroid 39 2 23 32 36 42 31 36 30 2 1 1
aGrep 6437 7 55 57 2 337 59 46 1 2 1
aka 83 1 166 62 77 2 1 8183 65 1 81
alarmclock 65 4 41 69 65 50 6575 70 3 55
aLogCat 81 0 114 74 65 0 0 -- 6 3--0
Amazed 67 1 25 62 40 0 1 57 66 36 0 21
anycut 68 0 37 63 67 0 0 59 65 63 0 0 0
anymemo 4712 311 40 36 5 2 3653 32 7 7 2
autoanswer 2334 5 19 17 1 0 20 16 13 2 0 0
baterrydog 6723 2 6370 1 1 54 67 64 1 1 0
battery 83 13 58 76 80 16 5 75 78 75 1 18 0
bites 4986 8 37 40 5 0 38 41 37 2 1 1
blockish 73 0 71 50 49 0 0 36 52 59 0 20
bomber 83 0 32 80 80 1 0 57 75 76 0 0 0
Book-Cat 307 109 28 29 0 1 1432 29 3 2 1
CDT 81 0 49 79 66 0 0 79 62 77 0 0 0
dalvik-exp 7376 5 69 72 73 70 72 68 6 2 2
dialer2 4734 7 38 51 0 0 3347 38 300
DAC 88 2 39 85 88 0 0 53 83 86 0 50
fileexplorer 59 0 32 41 55 0 0 41 49 41 0 0 0
fbubble 81 0 15 8181 0 0 50 76 8 1000
gestures 55 0 29 3655 0 0 32 52 36 0 0 0
hndroid 2063 9 10 8 2 1 9 1 58121
hotdeath 7626 1 81 69 1 0 60 75 76 1 21
importcont 4315 1 41 40 0 0 62 39 40 0 0 1
Jamendo 588 107 57 57 6 1 4463 55 7 3 0
k9mail 91 5 5 0 88 16 1 87 7 16 2 1
LNM -- - -- - ------
LPG 80 0 101 76 77 0 0 68 79 76 0 0 0
LBuilder 311 102 28 28 0 0 25 27 27 0 0 0
manpages 751 189 69 72 0 0 63 73 39 100
mileage 5221 136 46 43 14 2 39 49 39 13 9 3
MNV 466 292 39 42 0 2 4563 40 3 0 3
Mirrored 6698 8 45 46 1 0 51 59 59 6 8 5
multisms 6616 8 58 59 0 0 45 61 33 100
MunchLife 77 0 47 67 75 0 0 65 72 69 0 0 0
MyExp 541 109 51 49 0 0 4860 42 110
myLock 473 118 45 29 2 0 44 31 27 2 0 0
nectroid 6235 3 36 34 0 0 6466 33 310
netcounter 6336 0 57 58 1 0 7070 42 2 1 0
PWMG 58 4 56 50 43 1 3 62 58 53 643
PWM 18 0 80 12 7 0 0 687000
Photos 3822 9 29 24 1 230 34 30 211
QSettings 51 0 256 48 45 0 0 4252 51 0 10
RMP 6213 4 52 58 0 0 70 58 53 100
ringdroid 5236 3 23 50 0 1 -3 8 2 2 - 1 0
sanity 333 407 35 28 2 327 21 27 2 32
soundboard 63 0 27 42 61 0 0 42 51 42 0 0 0
SMT 79 0 34 80 38 0 0 8080 18 0 0 0
SpriteText 61 0 23 59 60 0 0 59 60 59 0 0 0
swiftp 14 0 42 1314 0 0 1314 13 0 0 0
SyncMyPix 2516 6 25 20 0 0 25 21 20 110
tippytipper 81 0 112 77 80 0 0 7783 79 0 0 0
tomdroid 56 0 68 55 50 0 0 5456 48 110
Translate 51 0 41 37 49 0 0 44 49 48 0 0 0
Triangle -- - -- - - ------
wchart 71 0 92 69 63 0 0 4773 64 2 60
WHAMS 7116 8 61 69 0 0 7277 64 100
wikipedia 36 0 204 33 35 0 0 30 32 34 0 0 0
Wordpress 812 52 84 4 2 86 4 12 2 0
worldclock 9219 9 90 91 0 0 92 91 90 100
yahtzee 6024 6 48 33 0 0 60 58 52 1 0 2
zooborns 38 1 30 35 37 1 0 33 36 35 200
Ave/Sum 54 199 85 47 47 115 45 45 51 44 140 121 48
‚Ä¢Giventhe relatedcrash information,extractonly thecrash
stackandfilteroutallinformationthatisnotdirectlyrele-
vant (e.g., the message ‚Äúinvalid text...‚Äù).
‚Ä¢Computeahashoverthesanitizedstacktraceofthecrash
toidentifyuniquecrashes.Differentcrashesshouldhavea
different stack trace and thus a different hash.0102030405060
Time in minutesStatement coverageTM
MS
MR
Figure 4: Progressive statement coverage for TimeMachine
(TM)andbaselinetoolson68benchmarkapps.MSindicates
Monkey extended with Stoat‚Äôs system-level generator and
MRindicatesMonkeywiththeabilitytorestartfromscratch
when lack of progress is detected.
Executionenvironment. Theexperimentswereconductedon
two physical machines with 64 GB of main memory, running a64-bit Ubuntu 16.04 operating system. One machine is powered
byanIntel(R)Xeon(R)CPUE5-2660v4@2.00GHzwith56cores
while the other features an Intel(R) Xeon(R) CPU E5-2660 v3 @
2.60GHzwith40cores.Toallowforparallelexecutions,werunour
systeminDocker(v1.13)containers.EachDockercontainerrunsaVirtualBox(v5.0.18)VMconfiguredwith2GBRAMand2cores
for the Android 4.4 and 2 cores and 4GB RAM for Android 7.1. We
madesurethateachevaluatedtechniqueistestedunderthesame
workload by running all evaluated techniques for the same app on
the same machine.
5.2 Experimental Results
5.2.1 Study 1: Effectiveness of Time-travel Strategy.
Table 1 shows achieved coverages and found faults by each
technique on 68 Android apps. The highest coverage and mostfound crashes are highlighted with the grey color for each app.
The results of TimeMachine and baseline techniques are shown in
column‚ÄúTimeMachine "and"Baselines".RecallthatMSindicates
MonkeyextendedwithStoat‚Äôssystem-leveleventgenerator,and
MRindicatesMonkeywiththeabilitytorestarttestingfromscratch
when lack of progress is detected.
Comparisonbetween TimeMachine andMS. TimeMachine
achieves54%statementcoverageonaverageanddetects199unique
crashes for 68 benchmark apps. MS achieves 47% statement cov-erage on average and detects 115 unique crashes. TimeMachine
covers1.15timesstatementsandreveals1.73timescrashesmore
than MS. To further investigate these results, Figure 4 presents
achieved code coverage over execution time for all 68 apps. As we
cansee, TimeMachine hasachievedhighercoveragefromaround
the 20th minute onwards, finally achieving 7% more statement cov-
erage at the end of execution time. Figure 5 presents the box-plots
ofthefinalcoverageresultsforappsgroupedbysize-of-app,where
"x" indicates the mean for each box-plot. We see that coverage
improvement is substantial for all four app size groups.
487TM MS MR TM MS MR TM MS MR TM MS MR
(a) <1k (33 Apps) (b) 1k ~ 3k  (20 Apps) (c) > 3k  (15 Apps) (d) All 68 Apps
TM
 MS
 M
() < 1 k ( 3 3 A )
MR
 TM
 MS
M
(b) 1k ~ 3k (20 A
MR
)
TM
 MS
 M
)
() k( )
MR
A
R
 TM
 MS
MR
()
R
Figure 5: Statement coverage achieved by TimeMachine
(TM), MS and MR.
(a) <1k (33 Apps) (b) 1k ~ 3k  (20 Apps) (c) > 3k  (15 Apps) (d) All 68 AppsTM ST SA MO
() < 1 k ( 3 3 A
TM
ST
 SA
 MO
(b) 1k ~ 3k (20 A )
 )
O
)
 )
 (d) All 68 A
TM ST SA MO TM ST SA MO TM ST SA MO
Figure 6: Statement coverage achieved by TimeMachine
(TM), Stoat (ST), Sapienz (SA) and Monkey (MO).
Ourtime-travelstrategyeffectivelyenhancestheexistingtesting
technique (MS) by achieving 1.15 times statement coverage and
detecting 1.73 times crashes on 68 benchmark apps.
Comparison between TimeMachine and MR. MR achieves
47%statementcoverageonaverageanddetects45uniquecrashes
for68benchmarkapps. TimeMachine achieves1.15timesstatement
coverage and 4.4 times unique crashes more than MR. Similarly,
Figure4andFigure5shows TimeMachine coversmorecodeina
short time and substantially improves statement coverage for all
fourappsizegroupscomparedtoMR.Thisshowsthatitisnotsuffi-
cienttosimplyrestartanappfromscratchwhenlackofprogressis
detected, though MR improves Monkey by 3% statement coverage
(Monkey‚Äôs statement coverage is shown in the third subcolumn of
column "State-of-the-art" of Table 1).
State saving and restoring as well as other components substan-
tially contribute to enhancing testing techniques, it is not suffi-
cienttosimplyrestartappfromscratchwhenlackofprogress
is detected.
5.2.2 Study 2: Testing Effectiveness.
Theresultsofstate-of-the-arttechniquesareshownincolumn
‚ÄúState-of-the-art"ofTable1(ST,SA,andMOindicateStoat,Sapienz
and Monkey, respectively). As can be seen, TimeMachine achieves
thehigheststatementcoverageonaverage(54%)andisfollowedby
Sapienz(51%),Stoat(45%) andMonkey(44%).Figure6also shows
that TimeMachine achieves the highest statement coverage for all15199
140
050100150200250
TM ST199
121
050100150200250
TM SA1129199
48
050100150200250
TM MO
28140121
050100150200250
ST SA9140
48
050100150200250
ST MO8121
48
050100150200250
SA MO
Figure7:Comparisonoftotalnumberofuniquecrashesfor
AndroTestapps.Thedarkgreyareasindicatetheproportion
of crashes found by both techniques.
four app size groups. TimeMachine detects the most crashes (199)
as well, followed by Stoat (140), Sapienz (121) and Monkey (48).
Thebetterresultsfrom TimeMachine canbeexplainedasfollows:
state-level feedback accurately identifies which parts in app are
inadequatelyexplored.Moreoveraninadequatelyexploredstatecan
bearbitrarilyanddeterministicallylaunchedforfurtherexploration
via restoring a snapshot. Existing techniques typically observe
programbehavioroveraneventsequencethatoftenisverylong
and goes through many states. Coverage feedback of an individual
state is unavailable. So our time travel framework enhances app
testing by providing fine-grained state-level coverage feedback.
TimeMachine achievesthehigheststatementcoverageandde-
tectsthemostcrasheson68benchmarkappscomparedtostate-
of-the-arttechniques.Promisingly,ourtime-travelframework
hasapotentialtoenhancestate-of-the-artapptestingtoolsto
achieve better results.
Tostudyperformanceacrossapps,foreachtechniqueundereval-
uation, we compute the number of apps on which the techniqueachieves the best performance. In terms of statement coverage,
TimeMachine achieves the best performance on 45 apps, followed
bySapienz(19apps),Stoat(11apps)andMonkey(1app).Forde-
tected crashes, TimeMachine achieves the best performance on
32 apps. For Stoat,Sapienz, and Monkey, there are 16, 15, and 4apps, respectively. We further perform a pairwise comparison of
detectedcrashesamongevaluatedtechniques.AsshowninFigure7,thereislessthantenpercentoverlapbetweenthecrashesfoundby
TimeMachine andStoat,or TimeMachine andSapienz,respectively.
The overlap with Monkey is reasonably high. About 60% of unique
crashes found by Monkey are also found by TimeMachine;h o w -
ever TimeMachine foundmanynewcrasheswhicharenotfound
by Monkey. This analysis shows that TimeMachine can be used
together with other state-of-the-art Android testing techniques to
jointly cover more code and discover more crashes.
TimeMachine complements state-of-the-art Android testing
techniques interms ofthe abilityto discovermore crashesand
cover more code.
488Table 2: Results from 37 closed-source industrial apps.
Subject %Coverage #Crashes #State
Name #Method TMST MO MS MR TMST MO MS MR TM
AutoScout24 49772 3425 29 31 29 180 1 12 0 915
BestHairstyles 28227 142013 15 14 100 1034
Crackle 487022291 92222210 8 10 8 905
Duolingo 463822613 22 23 22 120080 384
ESFile Explorer 47273 2815 18 24 21 90080 594
Evernote 458801110 7 11800000 45
Excel 488491991 41 41 4 20000 201
FiltersFor Selfie 17145 913898 00000 43
Flipboard 415633017 24 28 25 00000 308
Floor Plan Creator 23303 293023 26 26 00000 394
Fox News 425692813 21 20 17 50140 635
G.P. Newsstand 50225 76565 00000 14
GOLauncher Z 45751 1391 11 11 2 00000 81
GoodRx 482222419 21 22 21 170 0 11 0 468
ibisPaint X 47721161610 12 12 0100 1655
LINECamera 47295 1715 14 15 15 221 1 19 1 413
Marvel Comics 43578 181815 17 15 00000 133
Match 504361511 11 151100000 106
Merriam- Webster 50436 2521 18 24 23 60232 1018
Mirror 36662 8888800000 23
Mybaby Piano 20975 7777700000 4
OfficeSuite 458761711 16 16 16 30010 479
OneNote 501001111111011230492 181
Pinterest 460712110 16 15 8 00000 382
Quizlet 483693322 30 3331230 0 17 0 548
Singing 46521106586 140 0 12 0 77
Speedometer 47773 1611 13 13 15 00000 51
Spotify 44556131410 11 10 00000 36
TripAdvisor 46617 2622 21 23 22 11000 1279
trivago 50879169 8 14 10 70060 139
WatchESPN 43639 2622 23 25 24 00000 395
Wattpad 440692514 13 22 13 00000 327
WEBTOON 47465 2117 13 19 16 120081 487
Wish 4820716151715 14 400 4055
Word 499591691 41 41 4 00000 146
Yelp 469032416 17 20 17 690 0 37 0 395
Zedge 467992423 22 23 21 1203 130911
Ave/Sum 441821914 15 17 15 2813 20 183 15 358
5.2.3 Study 3: Closed-source Apps.
Table 2 shows results of 37 closed-source benchmark apps. It is
clear that TimeMachine achieves the highest method coverage 19%
and the most found crashes 281 among all evaluated techniques.
ComparedtobaselineMSandMR, TimeMachine improvesmethod
coverage to 19% from 17% and 15%, respectively. Note that the
improvement of 2% to 4% is considerable since each app has 44182
methods on average and around 900 to 1800 more methods are
coveredforeachapp.Intermsofnumberofcrashesfound, Time-
Machine detects 1.5 times and 18.7 times crashes more than MS
and MR, respectively. MS detects 183 crashes and MR detects 15
crashes.
Compared to state-of-the-art techniques, TimeMachine substan-
tially outperforms them on both method coverage and the number
of found crashes. Stoat achieves 14% method coverage and detects
3crashes.Monkeyachieves15%methodcoverageand20crashes.
Unexpectedly, Stoat demonstrated the worst results, worse than
Monkey.Acloserinspectionrevealedthatthesereal-worldappsuse
complexUIcontainers(e.g.,animations),whichposedifficultiesfor
Stoat to build a model. Certain app pages might be missed entirely
becausetheeventhandlersassociatedwiththosewidgetscannotbetriggered.However,both TimeMachine andMonkeyovercomethis
issueduetotheir independence fromanyUImodels.Wereportedthis matter to the authors of Stoat who confirmed our insight.$#

 "	"#(!%&"#!##!"$#!"24
88652211
00
6152012
 1/06 1/07 1/080145
08
/*3 3)/*3)4 3)5*4)/
+,"#!$#
"#$ ##+,#!""#!$#
+,#!("#!$# ' #(  $!
0 $#!' # 87
1 "#"	!!! 0/2 !$#' # 8
3 $#
!(!!! 7
4 ##' # 65 '$#$"' # 1
6 "$  !# !#' # 07 $!"!'$#$"' # 0
8 #' # 0
+,"#!$## ##!""
Figure 8: Statistics of tested 87 apps from Google Play.
Ourtime-travelstrategysubstantiallyimprovetheexistingtech-
nique(i.e.,MS)bycoveringaround900moremethodsanddis-
covering1.5timesmorecrashes. TimeMachine alsooutperforms
state-of-the-art techniques (Stoat and Monkey) in terms of both
method coverage and the number of found crashes.
OutofTop-100instrumentedappsfromGooglePlay,wesuccess-
fully tested 87 apps. The remaining 13 apps kept crashing due to a
self-protection-mechanism(thoughtheyweresuccessfullyinstru-
mented). As shown in Figure 8, the tested apps are quite diverse
beingselectedfrommorethan10categories.Itcomesasnosurprise
thatthemajorityofthemarerankedwithover4stars,andarebeing
actively maintained.
Inthe87apps,wefound137uniquecrashes.Theseareallnon-
native crashes, i.e., their stack traces explicitly point to the source
lineofthepotentialfaultsinthetestedapp.Thedetected137crashes
were caused by 9 kinds of exceptions shown in Figure 8. The most
common type is NullPointerException.
Intotal, TimeMachine detects137uniquecrashesin25ofTop-
100 Google Play apps.
5.2.4 Analysis on State Identification and Time-travel.
State identification. The evaluation shows GUI layout is ap-
propriate to identify an app state. This state abstraction generates
acceptablenumberofstatesforanapp,atthesametimesufficiently
capturesfeaturesofastate.Asweseecolumn"#State"inTable1
and Table 2, more states are discovered in apps with rich function-
alityandlessstatesarediscoveredinsimpleapps.Forinstance,app
AnyMemo with plentiful activities has 311 found states and app
Frozenbubblewithfewactivitieshas15foundstates.Similarresults
can be found for industrial apps. Note that no login is providedin the evaluation, such that TimeMachine might identify a small
number of states for some large scale apps, like K9Mail.
GUI layout sufficiently captures features of an app state. On
average,85statesarefoundinanopensourcebenchmarkapp
and 358 states are found in an industrial benchmark app.
Frequency. An automatic Android testing tool that generates a
verylongeventsequence,likeMonkey,maygetstuckinloopsor
dead ends. We measure the number of times, our time-travelling
infrastructureisemployedtounderstandhowoftenMonkeygets
stuck. In six hours over all runs and apps, Monkey gets stuck with
a mean of 305.6 and a median of 308.5 times. As we can see inthe box plots of Figure 9, there are generally less restores as the
4891k ~ 3k  (20 Apps) <1k (33 Apps) > 3k  (15 Apps) All 68 Apps#Restores
Figure 9: Boxplots. Depending on app size how often does
TimeMachine travel back in time?
apps get bigger. This is due to an increasing state space. A greater
proportion of random actions lead to yet undiscovered states such
thatprogressismaintained. Smallerappshaveasmallstatespace
andMonkeymayrunmorequicklyintoloopsofstatesthathave
already been observed.
On average, TimeMachine travels about 51 times per hour back
to more progressive states that were observed in the past‚Äî
because Monkey gets stuck in a loop or dead end.
Cost.Thecostoftime-travelisacceptable. TimeMachine spends
around 10 seconds taking a snapshot and 9 seconds restoring a
snapshot on an Android7.1 virtual machine with 4 GB memory. A
snapshottakesaround1GBdiskspace.Forlargescaleindustrial
appsintheevaluation,asessiontypicallygenerateslessthan100
snapshots. So TimeMachine is able to run on a desktop in term
ofconsumedstoragespace.Besides,sinceonesnapshotisstored
foreach"interesting"state,storagecanbepotentiallyreducedby
re-configuring the definition of "interesting".
Thisisareasonablecostforreachingaparticularstateinade-
terministic way for Android testing, especially for large scale apps.
To reach a deep state, a human tester may need to perform dozens
of events and repeat them many timesdue to non-determinism of
Androidapps.Thisisevenmoredifficultforanautomatedtestgen-
eratorsbecauseittypicallyrequiresgeneratingaverylongevent
sequenceautomatically.Thesetoolsthusspendmoretimereaching
hard-to-reach states than TimeMachine, which makes reachability
easier by recording and restoring snapshots of ‚Äúinteresting" states.
Thecostoftime-travellingisacceptable,andalsoreasonablefor
testing stateful programs.
6 THREATS TO VALIDITY
We adopted several strategies to enhance internal validity of our
results. To mitigate risks of selection bias, we chose apps in a stan-
dardtestingbenchmarkwhichhasbeenusedinpreviousstudies
[16,20,23,34‚Äì37,40,44]. In order to put no testing tool at a disad-
vantage,weuseddefaultconfigurations,providedtheexactsame
starting condition, and executed each tool several times and under
the same workload. To identify unique crashes, we followed the
Stoatprotocol[ 40]andalsomanuallycheckedthecrashesfound.
To measure coverage, we used a standard coverage tool.We realise that our results on Stoat versus Sapienz and those
reported in the Stoat paper [ 40] are vastly different. We checked
withtheauthorsofStoat[ 40]onthismatter.TheauthorsofStoat
explain the disparity (i) by additional files they provided to the
Android Device via SDCard, and (ii) running of experiments at
their end on a different machine (Intel Xeon(R) CPU @ 3.50GhZ,
12 cores, 32GB RAM) with hardware acceleration.
Additionally,wetooktwomeasurementstoruleoutcrashesthat
mightbecausedbyourtechniqueitself(i.e.,artificialcrashes).First,
TimeMachine inserted a delay of 200 ms between two events to
avoidcrashesduetoMonkeygeneratingmanyeventsinaextremely
short time. Second, we manually checked stack traces to filter out
crashes due to state restoring issues such as inconsistent states.
Finally,ourtechniquetestsappsinavirtualmachineinstalled
withAndroid-x86OSanddoesnotsupportphysicaldevicesyet.For
appsinteractingwitharemoteserver,ourtechniquesaves/restores
only app states without considering remote server states.
7 RELATED WORK
The stream of works most closely related to ours is that of time-
travel debugging [17,28,29,32,41]. Time-travel debugging allows
the user to step back in time, and to change the course of events.
Theusercannowaskquestions,suchas:‚ÄúWhatifthisvariablehad
a different value earlier in the execution‚Äù? Now, time-travel testing
has a similar motivation. The tester can test the state-ful app for
various, alternative sequences of events, starting in any state.
This work was originally inspired by existing work on coverage-
based greybox fuzzers (CGF)[2,18,19,39]. A CGF, started with a
seed corpus of initial inputs, generates further inputs by fuzzing. If
ageneratedinputincreasescoverage,itisaddedtotheseedcorpus.
Similar to CGF, our time-travel-enabled test generator maintains a
statecorpus with states that can be restored and fuzzed as needed.
Search-based. The most closely related automatic Android test
generationtechniquesemploysearch-basedmethods.Maoetal.de-
velopedamulti-objectiveautomatedtestingtechniqueSapienz[ 36].
Sapienz adopts genetic algorithms to optimize randomly generated
teststomaximizecodecoveragewhileminimizingtestlengths.Evo-
Droid[35],thefirstsearch-basedframeworkforAndroidtesting,
extractstheinterfacemodelandacallgraphfromappundertest
andusesthisinformationtoguidethecomputationalsearchprocess.
Search-basedapproachesareeasytodeployandhaveattracteda
lot of attention from industry, e.g., Sapienz has been used to test
differentkindsofmobileappsatFacebook.Ourwork TimeMachine
takes a search-based approach as well, but instead of a population
of input sequences it evolves a population of app states. Our work
proposes a new perspective of app testing as state exploration, and
providesafeedback-guidedalgorithmtoefficientlyexploreanapp‚Äôs
state space.
Random. OneofthemostefficientapproachesfortestingAn-
droid apps is the random generation of event sequences [ 23]. Apps
are exercised by injecting arbitrary or contextual events. Mon-
key[3]isGoogle‚ÄôsofficialtestingtoolforAndroidapps,whichis
built into the Android platforms and widely used by developers.
Monkeygeneratesrandomusereventssuchasclicks,touches,or
gestures,aswellasanumberofsystem-levelevents.Dynodroid[ 34]
employs a feedback-directed random testing approach with two
490strategies: BIASEDRANDOM favors events related to the current
context, and FREQUENCY is biased towards less frequently used
events. Although random testing has gained popularity because
of its ease of use, it suffers from an early saturation effect, i.e., it
quickly stops making progress, e.g., no new code is executed after
certainnumberofeventexecutions.Fromthispointofview,our
work powers random testing with the ability to jump to a progres-
sive state observed in the past when there is no progress. Thus, an
early saturation can be avoided.
Model-based. Another popular approach of Android apps test-
ing is model-based testing. App event sequences are generated
according to models which are manually constructed, or extracted
from project artefacts such as source code, XML configuration
files and UI runtime state. Ape [ 26] leverages runtime information
to evolve an initial GUI model to achieve more precise models.
Stoat[40]assignswidgetsinaGUImodelwithdifferentprobabil-
ities of being selected during testing and adjusts them based on
feedback such as code coverage to explore uncovered models. An-
droidRipper[ 13]usesadepth-firstsearchovertheuserinterfaceto
buildamodel.A3E[15]exploresappswithtwostrategies: Targeted
Exploration whichprioritizesexplorationofactivitiesthatarereach-
ablefromtheinitialactivityonastaticactivitytransitiongraph,and
Depth-first Exploration which systematically exercises user inter-
faceindepth-firstorder.Droidbot[ 30],ORBIT[ 44]andPUMA[ 27]
use static and dynamic analysis to build basic GUI models fromapp under test, on top of which different exploration strategies
canbedeveloped.Model-basedapproacheshaveattractedagreat
dealofattentioninthisfieldbecausetheyallowtorepresentapp
behavior as a model on which various exploration strategies can
be applied. However, complex widgets (e.g, animation) commonly-
usedinmodernappsposedifficultiesonmodelconstruction,leading
to an incomplete model. Combining model-based approaches with
othertechniquessuchasrandomtestingcanbeapromisingoption
for Android apps testing.
Learning-based. Adifferentlineofworkusesmachinelearn-
ingtotestAndroidapps.Liuetal.[ 33]useamodellearnedfrom
amanuallycrafteddataset(includingmanualtextinputsandas-
sociated contexts) to produce text inputs that are relevantto the
current context during app testing. For instance, it would use a
nameforanexistingcitywhengeneratinganinputforasearchbox
ifthereisanearbyitemlabeled Weather.Wuji[45]employsevo-
lutionaryalgorithmsanddeepreinforcementlearningtoexplore
the state space of a game under test. SwiftHand [ 20] uses machine
learning to learn a model of the user interface, and uses this model
todiscoverunexploredstates.ThetechniquebyDegottetal.[ 24]
leverages reinforcement learning to identify valid interactions for
a GUI element (e.g., a button allows to beclicked but not dragged)
and uses this information to guide execution. Humanoid [ 31] takes
manualeventsequencesandtheircorrespondingUIscreenstolearn
a model and uses the model to predict human-like interactions for
an app screen. Machine learning is typically applied to resolve spe-
cificchallengeintheeventsequencegeneration,suchasgenerating
contextual text inputs or identifying possible types of input events
thatcanbeexecuteduponaGUIelement.Incontrast,ourworkfea-
turesafullyautomatedAndroideventsequencegenerator.Some
components in TimeMachine such as identifying an interesting
state might benefit from machine learning since learning-basedapproacheshaveshowntobeeffectiveforsimilarissues.Itisworth
exploring this direction in future work.
Program analysis-based. Several existing approaches employ
program analysis when testing Android apps. ACTEve [ 14] uses
symbolic execution to compute enabled input events in a given
app stateand systematically exploresthe stateby triggering these
events. SynthesiSE [ 25] leverages concolic execution and program
synthesistoautomaticallygeneratemodelsforAndroidlibrarycalls.
CrashScope [ 38] combines static and dynamic analysis to generate
an event sequence that is used to reproduce a crash. Similarly,IntelliDriod [
43] uses static and dynamic analysis to generate an
event sequence thatleads execution to aspecified API invocation.
Thor[12]executesanexistingtestsuiteunderadverseconditionsto
discoverunexpected appbehavior.Such programanalysisprovides
detailed information about the app, which can help to guide testsequence generation. At the same time, intrinsic limitations ofprogram analysis such as poor scalability create an impedimentto easy and widely-applicable automation. In contrast, our work
TimeMachine requireslittleinformationfromtheappundertest
and is widely applicable to variety of Android apps as shown by
our experiments.
8 CONCLUSION
Androidapptestingisawell-studiedtopic.Inthispaper,wedevelop
time-travelapptestingwhichleveragesvirtualizationtechnology
toenhancetestingtechniqueswiththeabilitytocapturesnapshots
of the system state‚Äî state of the app and all of its environment.
Capturingsnapshotsfacilitatesourtime-travel-enabledtestingtool
to visit arbitrary states discovered earlier that have the potential
totriggernewprogrambehavior.State-levelfeedbackallowsour
technique to accurately identify progressive states and travel to
them for maximizing progress in terms of code coverage and state
space exploration.
Our time-travel strategy enhances a testing technique (Monkey
extended with Stoat‚Äôs system-level generator) by achieving 1.15times more statement coverage and discovering 1.7 times morecrashes on the AndroTest benchmark. Moreover, TimeMachine
outperformsotherstate-of-the-arttechniques(SapienzandStoat)
byachievingthehighestcoverageonaverageandthemostfound
crashes in total. On large, industrial apps, TimeMachine covers
around 900 more methods on average and discover 1.5 times more
unique crashes over the baseline tool, at the same time outper-
formsstate-of-the-arttechniquesaswell.Ourtool TimeMachine
also reveals a large number of crashes, owing to a wide varietyof exceptions (nine different kinds of exceptions), in real-life top
popular apps from Google Play.
ACKNOWLEDGMENTS
ThisworkwaspartiallysupportedbytheNationalSatelliteofExcel-
lence in Trustworthy Software Systems, funded by NRF Singapore
underNationalCybersecurityR&D(NCR)programme,andanAcRF
Tier1 project T1 251RES1708 from Singapore. This research was
partially funded by the Australian Government through an Aus-
tralianResearchCouncilDiscoveryEarlyCareerResearcherAward
(DE190100046).
491REFERENCES
[1]2018. 2016NowSecureMobileSecurityReport. (2018). https://info.nowsecure.
com/rs/201-XEW-873/images/2016-NowSecure-mobile-security-report.pdf
[2] 2018. American Fuzzy Lop Fuzzer. (2018). http://lcamtuf.coredump.cx/afl/
[3] 2018. Monkey. (2018). https://developer.android.com/studio/test/monkey[4] 2018. VMWare VirtualBox. (2018). https://www.virtualbox.org/[5]
2019. Android Debug Bridge. (2019). https://developer.android.com/studio/
command-line/adb
[6] 2019. Android-x86. (2019). http://www.android-x86.org/[7] 2019. Anymemo. (2019). https://anymemo.org/[8]
2019. ELLA: A Tool for Binary Instrumentation of Android Apps. (2019). https:
//github.com/saswatanand/ella
[9]2019.EMMA:afreeJavacodecoveragetool.(2019). http://emma.sourceforge.net/
[10]2019. Google UI Automator. (2019). https://developer.android.com/training/
testing/ui-automator
[11]2019. A python library for VirtualBox. (2019). https://pypi.org/project/pyvbox/
[12]Christoffer Quist Adamsen, Gianluca Mezzetti, and Anders M√∏ller. 2015. System-
atic execution of Android test suites in adverse conditions. In Proceedings of the
2015 International Symposium on Software Testing and Analysis, ISSTA 2015, Balti-
more,MD,USA,July12-17,2015.ACM,83‚Äì93. https://doi.org/10.1145/2771783.
2771786
[13]DomenicoAmalfitano,AnnaRitaFasolino,PorfirioTramontana,SalvatoreDe
Carmine, and Atif M. Memon. 2012. Using GUI ripping for automated testingof Android applications. In IEEE/ACM International Conference on Automated
SoftwareEngineering,ASE‚Äô12,Essen,Germany,September3-7,2012.ACM,258‚Äì261.
https://doi.org/10.1145/2351676.2351717
[14]SaswatAnand,MayurNaik,MaryJeanHarrold,andHongseokYang.2012. Auto-matedConcolicTestingofSmartphoneApps.In ProceedingsoftheACMSIGSOFT
20th International Symposium on the Foundations of Software Engineering (FSE
‚Äô12). 59:1‚Äì59:11.
[15]TanzirulAzimandIulianNeamtiu.2013. Targetedanddepth-firstexploration
for systematic testing of android apps. In Proceedings of the 2013 ACM SIGPLAN
InternationalConferenceonObjectOrientedProgrammingSystemsLanguages&
Applications,OOPSLA2013,partofSPLASH2013,Indianapolis,IN,USA,October
26-31, 2013. ACM, 641‚Äì660. https://doi.org/10.1145/2509136.2509549
[16]Young-MinBaekandDoo-HwanBae.2016. AutomatedModel-basedAndroidGUI Testing Using Multi-level GUI Comparison Criteria. In Proceedings of the
31st IEEE/ACM International Conference on Automated Software Engineering (ASE
2016). 238‚Äì249.
[17]Earl T. Barr, Mark Marron, Ed Maurer, Dan Moseley, and Gaurav Seth. 2016.
Time-travel Debugging for JavaScript/Node.Js. In Proceedings of the 2016 24th
ACMSIGSOFTInternationalSymposiumonFoundationsofSoftwareEngineering
(FSE 2016). 1003‚Äì1007.
[18]MarcelB√∂hme,Van-ThuanPham,Manh-DungNguyen,andAbhikRoychoudhury.
2017. DirectedGreyboxFuzzing.In Proceedingsofthe24thACMConferenceon
Computer and Communications Security (CCS). 1‚Äì16.
[19]Marcel B√∂hme, Van-Thuan Pham, and Abhik Roychoudhury. 2018. Coverage-based Greybox Fuzzing as Markov Chain. IEEE Transactions on Software Engi-
neering(2018), 1‚Äì18.
[20]Wontae Choi, George Necula, and Koushik Sen. 2013. Guided GUI Testing of
AndroidAppswithMinimalRestartandApproximateLearning.In Proceedingsof
the2013ACMSIGPLANInternationalConferenceonObjectOrientedProgramming
Systems Languages &#38; Applications (OOPSLA ‚Äô13). 623‚Äì640.
[21]Wontae Choi, George C.Necula, and Koushik Sen. 2013. GuidedGUI testing of
androidappswithminimalrestartandapproximatelearning.In Proceedingsofthe
2013ACMSIGPLANInternationalConferenceonObjectOrientedProgrammingSys-
tems Languages & Applications, OOPSLA 2013, part of SPLASH 2013, Indianapolis,
IN, USA, October 26-31, 2013. 623‚Äì640. https://doi.org/10.1145/2509136.2509552
[22]Wontae Choi, Koushik Sen, George Necula, and Wenyu Wang. 2018. DetReduce:
Minimizing Android GUI Test Suites for Regression Testing. In Proceedings of
the40thInternationalConferenceonSoftwareEngineering (ICSE‚Äô18).ACM,New
York, NY, USA, 445‚Äì455. https://doi.org/10.1145/3180155.3180173
[23]Shauvik Roy Choudhary, Alessandra Gorla, and Alessandro Orso. 2015. Auto-
mated Test Input Generation for Android: Are We There Yet? (E). In Proceedings
of the 2015 30th IEEE/ACM International Conference on Automated Software Engi-
neering (ASE) (ASE ‚Äô15). 429‚Äì440. https://doi.org/10.1109/ASE.2015.89
[24]Christian Degott, Nataniel P. Borges Jr., and Andreas Zeller. 2019. Learninguser interface element interactions. In Proceedings of the 28th ACM SIGSOFT
International Symposium on Software Testing and Analysis, ISSTA 2019, Beijing,
China, July 15-19, 2019. ACM, 296‚Äì306. https://doi.org/10.1145/3293882.3330569
[25]XiangGao,ShinHweiTan,ZhenDong,andAbhikRoychoudhury.2018. Android
TestingviaSyntheticSymbolicExecution.In Proceedingsofthe33rdACM/IEEE
InternationalConferenceonAutomatedSoftwareEngineering (ASE2018).ACM,
New York, NY, USA, 419‚Äì429. https://doi.org/10.1145/3238147.3238225
[26]Tianxiao Gu, Chengnian Sun, Xiaoxing Ma, Chun Cao, Chang Xu, Yuan Yao,
Qirun Zhang, Jian Lu, and Zhendong Su. 2019. Practical GUI testing of Android
applications via model abstraction and refinement. In Proceedings of the 41stInternationalConferenceonSoftwareEngineering,ICSE2019,Montreal,QC,Canada,
May 25-31, 2019. IEEE / ACM, 269‚Äì280. https://doi.org/10.1109/ICSE.2019.00042
[27]Shuai Hao, Bin Liu, Suman Nath, William G. J. Halfond, and Ramesh Govindan.
2014. PUMA: programmable UI-automation for large-scale dynamic analysis
ofmobileapps.In The12thAnnualInternationalConferenceonMobileSystems,
Applications, and Services, MobiSys‚Äô14, Bretton Woods, NH, USA, June 16-19, 2014.
ACM, 204‚Äì217. https://doi.org/10.1145/2594368.2594390
[28]Yit Phang Khoo, Jeffrey S. Foster, and Michael Hicks. 2013. Expositor: Script-able Time-travel Debugging with First-class Traces. In Proceedings of the 2013
International Conference on Software Engineering (ICSE ‚Äô13). 352‚Äì361.
[29]SamuelT.King, GeorgeW.Dunlap,andPeter M.Chen.2005. Debugging Operat-
ing Systems with Time-traveling Virtual Machines. In Proceedings of the Annual
Conference on USENIX Annual Technical Conference (ATEC ‚Äô05). 1‚Äì1.
[30]Yuanchun Li, Ziyue Yang, Yao Guo, and Xiangqun Chen. 2017. DroidBot: alightweight UI-guided test input generator for Android. In Proceedings of the
39thInternationalConferenceonSoftwareEngineering,ICSE2017,BuenosAires,
Argentina, May 20-28, 2017 - Companion Volume. IEEE Computer Society, 23‚Äì26.
https://doi.org/10.1109/ICSE-C.2017.8
[31]Y. Li, Z. Yang, Y. Guo, and X. Chen. 2019. Humanoid: A Deep Learning-Based
Approach to Automated Black-box Android App Testing. In 2019 34th IEEE/ACM
InternationalConferenceonAutomatedSoftwareEngineering(ASE).1070‚Äì1073.
https://doi.org/10.1109/ASE.2019.00104
[32]Yun Lin, Jun Sun, Yinxing Xue, Yang Liu, and Jin Song Dong. 2017. Feedback-
based debugging. In Proceedings of the 39th International Conference on Software
Engineering, ICSE 2017, Buenos Aires, Argentina, May 20-28, 2017 . IEEE / ACM,
393‚Äì403. https://doi.org/10.1109/ICSE.2017.43
[33]Peng Liu, Xiangyu Zhang, Marco Pistoia, Yunhui Zheng, Manoel Marques,and Lingfei Zeng. 2017. Automatic text input generation for mobile testing.InProceedings of the 39th International Conference on Software Engineering,
ICSE 2017, Buenos Aires, Argentina, May 20-28, 2017. IEEE / ACM, 643‚Äì653.
https://doi.org/10.1109/ICSE.2017.65
[34]Aravind Machiry, Rohan Tahiliani, and Mayur Naik. 2013. Dynodroid: An Input
GenerationSystemforAndroidApps.In Proceedingsofthe20139thJointMeeting
on Foundations of Software Engineering (ESEC/FSE 2013). 224‚Äì234.
[35]RiyadhMahmood,NarimanMirzaei,andSamMalek.2014. EvoDroid:Segmented
Evolutionary Testing of Android Apps. In Proceedings of the 22Nd ACM SIGSOFT
International Symposium on Foundations of Software Engineering (FSE 2014). 599‚Äì
609.
[36]KeMao,MarkHarman,andYueJia.2016. Sapienz:Multi-objectiveAutomated
Testing for Android Applications. In Proceedings of the 25th International Sympo-
siumonSoftwareTestingandAnalysis (ISSTA2016) .ACM,NewYork,NY,USA,
94‚Äì105.
[37]NarimanMirzaei,JoshuaGarcia,HamidBagheri,AlirezaSadeghi,andSamMalek.
2016. ReducingCombinatoricsinGUITestingofAndroidApplications.In Pro-
ceedingsofthe38thInternationalConferenceonSoftwareEngineering (ICSE‚Äô16).
559‚Äì570.
[38]Kevin Moran, Mario Linares V√°squez, Carlos Bernal-C√°rdenas, Christopher Ven-
dome, and Denys Poshyvanyk. 2016. Automatically Discovering, Reporting and
Reproducing Android Application Crashes. In 2016 IEEE International Conference
on Software Testing, Verification and Validation, ICST 2016, Chicago, IL, USA, April
11-15, 2016. IEEE Computer Society, 33‚Äì44. https://doi.org/10.1109/ICST.2016.34
[39]Van-ThuanPham,MarcelB√∂hme,AndrewE.Santosa,AlexandruR.CƒÉciulescu,
andAbhikRoychoudhury.2019. SmartGreyboxFuzzing. IEEETransactionson
Software Engineering (2019), 1‚Äì17.
[40]TingSu,GuozhuMeng,YutingChen,KeWu,WeimingYang,YaoYao,GeguangPu,
YangLiu,and ZhendongSu.2017. Guided,StochasticModel-basedGUITesting
of Android Apps. In Proceedings of the 2017 11th Joint Meeting on Foundations of
Software Engineering (ESEC/FSE 2017). ACM, New York, NY, USA, 245‚Äì256.
[41]Nicolas Viennot, Siddharth Nair, and Jason Nieh. 2013. Transparent MutableReplay for Multicore Debugging and Patch Validation. In Proceedings of the
Eighteenth International Conference on Architectural Support for Programming
Languages and Operating Systems (ASPLOS ‚Äô13). 127‚Äì138.
[42]WenyuWang,DengfengLi,WeiYang,YuruiCao,ZhenwenZhang,YuetangDeng,
and Tao Xie. 2018. An Empirical Study of Android Test Generation Tools in
Industrial Cases. In Proceedings of the 33rd ACM/IEEE International Conference on
Automated Software Engineering (ASE 2018). ACM, New York, NY, USA, 738‚Äì748.
https://doi.org/10.1145/3238147.3240465
[43]Michelle Y. Wong and David Lie. 2016. IntelliDroid: A Targeted Input Generator
for the Dynamic Analysis of Android Malware. In NDSS. The Internet Society.
[44]Wei Yang, Mukul R. Prasad, and Tao Xie. 2013. A Grey-box Approach for Au-tomated GUI-model Generation of Mobile Applications. In Proceedings of the
16thInternationalConferenceonFundamentalApproachestoSoftwareEngineering
(FASE‚Äô13). 250‚Äì265.
[45]Y.Zheng, X.Xie,T. Su,L. Ma,J.Hao, Z.Meng,Y.Liu, R.Shen,Y. Chen,andC.
Fan.2019. Wuji:AutomaticOnlineCombatGameTestingUsingEvolutionary
Deep ReinforcementLearning. In 2019 34thIEEE/ACM International Conference
on Automated Software Engineering (ASE). 772‚Äì784. https://doi.org/10.1109/ASE.
2019.00077
492