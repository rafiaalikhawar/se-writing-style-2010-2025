Adapting Requirements Models to Varying Environments
Dalal Alrajeh
Department of Computing, Imperial
College London
UK
dalal.alrajeh@ic.ac.ukAntoine Cailliau
ICTEAM, UCLouvain
Belgium
antoine.cailliau@uclouvain.beAxel van Lamsweerde
ICTEAM, UCLouvain
Belgium
axel.vanlamsweerde@uclouvain.be
ABSTRACT
The engineering of high-quality software requirements generally
relies on properties and assumptions about the environment in
which the software-to-be has to operate. Such properties and as-
sumptions, referred to as environment conditions in this paper, are
highly subject to change over time or from one software variant
to another. As a consequence, the requirements engineered for a
specific set of environment conditions may no longer be adequate,
complete and consistent for another set.
The paper addresses this problem through a tool-supported re-
quirements adaptation technique. A goal-oriented requirements
modelling framework is considered to make requirements’ refine-
ments and dependencies on environment conditions explicit. When
environment conditions change, an adapted goal model is computed
that is correct with respect to the new environment conditions. The
space of possible adaptations is not fixed a priori; the required
changes are expected to meet one or more environment-independent
goal(s) to be satisfied in any version of the system. The adapted goal
model is generated using a new counterexample-guided learning
procedure that ensures the correctness of the updated goal model,
and prefers more local adaptations and more similar goal models.
CCS CONCEPTS
•Software and its engineering →Requirements analysis ;
Software evolution ; Model-driven software engineering.
KEYWORDS
Requirements adaptation, requirements evolution, context-dependent
requirements, formal verification, logic-based learning
ACM Reference Format:
Dalal Alrajeh, Antoine Cailliau, and Axel van Lamsweerde. 2020. Adapting
Requirements Models to Varying Environments. In 42nd International Con-
ference on Software Engineering (ICSE ’20), May 23–29, 2020, Seoul, Republic of
Korea. ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/3377811.
3380927
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
ICSE ’20, May 23–29, 2020, Seoul, Republic of Korea
©2020 Association for Computing Machinery.
ACM ISBN 978-1-4503-7121-6/20/05. . .$15.00
https://doi.org/10.1145/3377811.33809271 INTRODUCTION
The engineering of adequate, consistent and complete software re-
quirements is known to rely on properties and assumptions about
the environment in which the software will operate [ 29,40]. In
particular, lower-level requirements conjoined with such proper-
ties and assumptions must entail the higher-level requirements to
which they contribute [ 49]. The documentation of requirements
satisfaction arguments showing this is an important aspect of the
requirements engineering (RE) process [ 29,36], notably for trace-
ability management and requirements verification purposes [48].
Properties and assumptions about the environment, called envi-
ronment conditions hereafter, are subject to change over time and
space—as the system evolves over time or as system variants are
being considered over space. The originally engineered require-
ments may no longer be adequate, consistent nor complete when
those properties and assumptions change. As a consequence, the
original requirements must be adapted to ensure their adequacy,
consistency and completeness in the new environment.
This paper uses a goal-oriented RE framework in order to make
requirements refinements and dependencies on environment con-
ditions fully explicit [ 48]. To illustrate the problem addressed in the
paper, consider an example of an urban traffic control system aimed
at enforcing low emission zones. Air pollution is estimated to be re-
sponsible for 310,000 premature deaths in Europe each year, causing
more of these than road accidents [ 2]. Low Emission Zones (LEZs)
are urban areas where the most polluting vehicles are regulated.
Consider a software house developing LEZ enforcement software
for cities across Europe [ 44]. A high-level goal for such system is
that “urban traffic death toll shall be reduced ”. To achieve this, the
goal is refined by prescribing the subgoal “G: polluting vehicles shall
be penalized ”, among others. When developing the London product,
the latter goal is refined into subgoal “SG: polluting vehicles shall be
charged when entering designated LEZ ”, among others. The latter
goal depends on a number of environment conditions, e.g., being
in London where polluting vehicles are admitted within such zone,
but must pay a charge of £22. For cities like Brussels, Frankfurt and
Stockholm, the higher-level goal Gremains the same in spite of
different environment conditions. For Brussels, in particular, the
London environment conditions do not hold since polluting vehi-
cles are not admitted in LEZ in Brussels; the lower-level London
goal SGthus no longer makes sense; it must be adapted to match
the Brussels’ regulations (e.g., by issuing fines). The problem for the
software RE team gets exacerbated as regulations are not only vary-
ing over space (across different countries and cities) but also over
time as regulations within the same city evolve in unpredictable
502020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE)
ICSE ’20, May 23–29, 2020, Seoul, Republic of Korea Dalal Alrajeh, Antoine Cailliau, and Axel van Lamsweerde
ways (in Brussels, they changed once over the last two years). Ad-
vanced vehicle technologies (e.g., electric cars) are also expected to
call for other types of system evolutions.
Assume a LEZ enforcement software product is developed for
London. Options for delivering this product to other cities world-
wide would currently include: (a)adapt the software implementa-
tion directly regardless of the London requirements—the resulting
implementation would then be requirements-free, or inconsistent
with the original requirements, making it impossible to enable
requirements-based testing, traceability management, and so forth;
(b)develop a set of business rules, or a feature diagram along a
product line approach [ 17], for deciding which requirements to con-
sider in which city; this appears highly unfeasible given the huge
variation space, its expansion over time, and the unpredictability of
evolving city regulations; (c)manually adapt the London require-
ments to match the specifics of each city—a costly and error-prone
task given for similar reasons and the interdependency among re-
quirements; or(d)provide automated support for the requirements
adaptation process—as we propose in this paper.
The requirements adaptation problem can be more precisely de-
fined in terms of the well-known RE reference framework [ 49]
transposed to goal models [ 48].GIVEN :(i)a set of system goals
G; a set of lower-level subgoals SGcontributing to G; one or more
common, environment-independent goals CG, to be found in any
system variant or evolution, to which Gcontributes; a set of environ-
ment conditions Esuch that{SG,E}|=Gand{G,E}|=CG(where
|=denotes logical entailment), (ii ) an environment variation E′ofE
compatible with the preservation of the original high-level common
goal(s) CG;FIND adapted goals G′and lower-level subgoals SG′
where{SG′,E′}|=G′and{G′,E′}|=CG. The new environment
conditions should thus allow one or more common high-level goals
to remain stable through all system evolutions or variants; it would
not make much sense to consider ones leading to an adapted system
having no common purpose with the original.
Little research attention has been paid to provide automated sup-
port for this adaptation problem. Much of RE work on adaptation so
far focuses on reconfiguration within an adaptation space known
in advance [ 3,4,9,14,31,34]. The preservation of requirements
satisfaction arguments under environment changes is never ad-
dressed there. Work on requirements variants in static or dynamic
product lines also assumes the space of all possible variations to
be known a priori (e.g., in a feature diagram) [ 26,27]. A significant
amount of work on requirements evolution has been devoted to
requirements traceability and requirements organization for easier
change management [ 30]. The work closest to ours is [ 8] where a
learning technique for revising goal models is also described. The
problem in [ 8] is, however, different and the solution there is not
adequate for the adaptation problem considered here. In [ 8], envi-
ronment conditions are not changing and assumed to be correct;
new conditions may be added but none removed. Preservation of
goal model correctness w.r.t. higher-level goals is not considered.
In contrast, our aim here is to generate adaptations that preserve
model correctness across varying environments. Additionally, the
learning technique there can only modify goal specifications; the
goal model structure remains fixed. Our adaptation problem may
require other types of changes (such as changing the refinementstructure). Finally, the learning technique in [ 8] does not yield re-
visions guaranteed to be local and similar to the original model.
In [14], predefined sets of countermeasures to goals obstructions
are dynamically selected at system runtime to reduce the mon-
itored obstruction rates. Note that our requirements adaptation
problem might sometimes be addressed at system runtime, runtime
adaptations are, however, not discussed here.
This paper provides a formal, tool-supported approach to address
the adaptation problem defined above at system development time.
For this purpose, we use the KAOS goal-oriented RE framework
[48] to model requirements and reason about their adaptations.
This framework is chosen because: (i ) requirements dependencies
on other requirements and on environment conditions is made fully
explicit; (ii ) multiple abstraction levels are supported; (iii ) require-
ments can be formally specified, enabling the use of formal analysis
tools; and (iv ) the modelling framework has been used in industry
[19], e.g., for Huawei smartphones [ 41]. Thus given a source goal
model, the proposed approach computes model adaptations for a
different target environment where:
–The adaptations may cover model evolution over time or model
variants over space;
–The space of variations is not explicitly known in advance; it
is, however, constrained by the preservation of environment-
independent goals to be commonly met across adaptations;
–The approach preserves model correctness; it is driven by satis-
faction arguments of the form {SG′,E′}|=G′and{G′,E′}|=CG,
given the original model where {SG,E}|=Gand{G,E}|=CG.
At the heart of our solution is a novel counterexample-guided
learning technique for adapting requirements expressed in metric
linear temporal logic (MLTL [ 32]). In brief, given a correct goal
model where the source, original environment conditions are re-
placed with the target, new ones, our procedure verifies the correct-
ness of the refinements of each goal in the model. If the verification
finds an incorrect refinement, counterexamples demonstrating this
are automatically generated. These traces are then used as examples
by a logic-based learning procedure to revise goals appearing in the
refinements so as to eliminate this violation. The procedure iterates
until an updated goal model is obtained that fulfils the satisfaction
arguments for every goal refinement in the model.
A key aspect of our technique is its ability to search for adap-
tations of an existing goal model that best meet some preference
criteria. Our technique generates adaptations to goals and their
refinements that are syntactically as close as possible to the origi-
nal ones, with the fewest goals being adapted, whilst ensuring the
model’s correctness. The paper makes the following contributions:
–a dedicated notion of context and context-dependent property
for reasoning about dependencies on changing environment con-
ditions in the RE process;
–a novel adaptation method for formal goal models at development
time that is guaranteed to produce “minimal” adaptations;
–a software tool that integrates model checking and logic-based
learning with preferences; our implementation supports multi-
ple degrees of automation (from fully automated mode to semi-
automated mode allowing for human intervention to guide the
search towards relevant adaptations);
51Adapting Requirements Models to Varying Environments ICSE ’20, May 23–29, 2020, Seoul, Republic of Korea
–an evaluation of the correctness and the applicability of our
approach on real adaptation problems. We particularly show
how adaptations to goal specifications, goal refinement tactics
and goal refinement depths are effectively computed.
The paper is organized as follows. Section 2 recalls some required
material on goal modelling and logic-based learning. Section 3
reviews the sources of variability in goal models. Section 4 defines
the requirements adaptation problem. Section 5 gives an overview
of our solution. Section 6 details the adaptation procedure. Section 7
discusses our tool implementation. Section 8 evaluates the approach
with respect to correctness and applicability. We discuss aspects of
scalability and human intervention in Section 9. Section 10 reviews
related work while Section 11 concludes our work.
2 BACKGROUND
2.1 Goal-oriented system modelling
In KAOS [ 48], agoal is a prescriptive statement of intent to be
satisfied by cooperation of the agents forming the system. The word
system refers to both the software and its environment, including
people, legacy software, devices like sensors and actuators, etc.
Anagent is an active system component having responsibilities in
goal satisfaction and capabilities in terms of conditions the agent
can monitor or control. A environment property is a descriptive
statement about the system, e.g., a physical law or a regulation.
We focuse on behavioural goals whose satisfaction is determined
in a clear-cut sense [ 48]. A behavioural goal defines a maximal set
of behaviours declaratively. A behaviour violates a goal if it is not
among the behaviours prescribed by it. To enable formal analysis,
behavioural goals are specified in a metric linear temporal logic
(MLTL). The syntax of MLTL formulae is defined over a finite non-
empty set of propositional variables, the logical constants trueand
false, the standard Boolean connectives and the temporal operators
⃝(next state), ^(sometime in the future), ^≤d(sometime in the
future before deadline d),□(always in the future), and □≤d(always
in the future up to deadline d). The formulae pand¬p, where p∈P,
are called positive and negative literals, respectively.
A behavioural goal Gis of type Achieve orMaintain. The specifi-
cation pattern for Achieve goals is □(Current→ΘTarget)orCurrent⇒
ΘTarget for short (as in [ 37]), where Current - and Target -conditions
are Boolean formulae, and Θ∈{⃝ ,^≤d,^<d}. The pattern for
Maintain goals is □(Current→Target), shortened to Current⇒Target .
We refer to behavioural goals written in MLTL as goal specifica-
tions. Their semantics is defined over infinite sequences of states in
the standard way [ 33]. Each state siis uniquely identified by the
valuation of variables. We consider lasso-shaped traces of the form
σ=w1(w2)ω, where w1=s1s2...andw2=sksk+1...are finite
sequences [12]. We use σ(j)to denote the jth state ofσ.
Agoal model is an AND/OR graph showing how goals are re-
fined, and how their responsibilities are assigned to system agents.
AnAND-refinement defines a set of subgoals and environment
conditions that together entail the parent goal. An OR-refinement
captures an alternative AND-refinement. Leaf goals of the AND/OR
graph are assigned as responsibilities to single agents. A goal as-
signed to a software agent is a software requirement whereas a goal
assigned to an environment agent is an environment assumption.
As a precondition for assigning a leaf goal to an agent, the goal
UrbanTrafficDeathTollReduced
InputSpeedMonitoredEnteringVehicleMonitoredSpeedLimitEnforcedLEZEnforced
ANPR camera
Speed detector
Column widthSpeedingVehiclesFinedPollutingVehiclePenalised
ChargePaidWhenPolluting-
VehicleAdmittedVehiclePlateRecognised
PollutionClassDetermined
FineIssuedForPollutingVehicles
VehicleTypeChargedPollutingVehiclesAdmitted
PollutingByVehicleType
TwentyTwoPaidWhenChargePaid
FineSetByVehicleAndOffenceType Electronic fee collector
 Electronic fee collectorLEZ checker
 Electronic fee collector
FineIssuedByOffenceTypePenalisedVehiclesIssued-
FineOrChagePaidFigure 1: Goal model for London’s low emission zone.
must be realizable by the agent, that is, it must have finite bounds
in case of an Achieve goal, and the propositional variables in the
Target -part of the goal specification must be controllable by the
agent whereas the propositional variables in the Current -part must
be monitorable by it [ 48]. An agent’s capabilities are defined by the
sets of propositional variables it can monitor and control.
A goal is defined in terms of a name, type, its refinement, a formal
specification and an agent assignment for leaf goals. An environ-
ment condition is defined by a name and a formal specification. The
notation GX, where X∈{Type ,RefinedBy ,Spec,Resp}, refers to
specific components of a goal definition, and similarly for environ-
ment conditions. G\{Xi}means the goal definition excluding the
components in{Xi}. A goal model is a collection of goal and envi-
ronment condition definitions. It is captured diagrammatically with
goals, environment conditions and agents represented by parallelo-
grams, trapezes and hexagons, respectively. Edges connected by a
white circle capture an AND-refinement link. Directed edges from
hexagons capture agent assignments. Fig. 1 shows a goal model
fragment for the LEZ example. The root goal is AND-refined into
two subgoals which are in turn further AND-refined. The leaf goal
VehicleTypeCharged is assigned to the ElectronicFeeCollector agent.
A goal model is correct if all its AND-refinements are com-
plete, consistent and minimal. An AND-refinement of a parent
goal PGinto subgoals{SG1, ...,SGn}iscomplete if the subgoals
{SGi}and environment conditions Eare sufficient to satisfy PG:
{SG1, ...,SGn,E}|=PG(complete refinement) (1)
The refinement is consistent if the subgoals are consistent with the
environment conditions:
{SG1, ...,SGn,E}̸|=false(consistent refinement) (2)
The refinement is minimal if it contains a minimal set of subgoals,
{SGi}, necessary to satisfy the parent goal PG:
{Ó
k,jSGk,E}̸|=PG(minimal refinement) (3)
where we represent a conjunction of conjuncts as a set of conjuncts
for convenience.
52ICSE ’20, May 23–29, 2020, Seoul, Republic of Korea Dalal Alrajeh, Antoine Cailliau, and Axel van Lamsweerde
2.2 Logic-based Learning
Logic-based learning is a symbolic machine learning technique con-
cerned with inductively inferring general logical theories that, with
a given background knowledge, explain a given set of examples [ 38].
State-of-the-art tools, referred to as non-monotonic logic learning
systems, are capable of learning generalizations from examples, and
revisions of existing logical theories [ 11,18] expressed as normal
logic programs (i.e., logic programs with negation as failure) [35].
Arevision task is a tuple⟨B,O+,O−,K⟩where Bis acorrect logical
theory (a set of clauses), called the background theory, O+andO−
are sets of atomic formulae called positive andnegative observations
(a.k.a. examples) respectively and Kis a possibly incorrect logical
theory, called the revisable theory, such that BandKdo not cover all
examples in O+and/or do not exclude all examples in O−; formally
O+⊈ M(B∧K), where M(B∧K)denotes the semantic model of
B∧K, and O−∩ M(B∧K),∅. Typically the semantic model M(ϕ)
is the set of atomic formulae that follow from ϕ.
Aninductive solution to such task is a revised theory eKthat,
together with B, covers all examples in O+, that is, O+⊆ M(B∧eK)
and excludes all negative ones O−, i.e., O−∩ M(B∧eK)=∅. The
computation of eKis typically achieved through a transformation
function that applies change operations (adding/deleting literals
or clauses) to K. To make the problem tractable, the search for re-
visions is generally performed within the scope of a search space
defined by a mode bias I, that reduces the set of candidate solutions
by restricting the set of literals that can be added to or removed
from the revisable theory K. Further restrictions can be imposed
by declaring structural constraints Don the set of logical expres-
sions that may be included within the solution space (see [ 10] for
details). In this setting, a revision task is now defined as a tuple
⟨B,O+,O−,K,I,D⟩. A logical theory eKis an acceptable revision if
and only if(a)it is an inductive solution; (b)it is is within the scope
of the search space defined by the mode bias I; and(c)it satisfies
all structural-constraints D. Recent state-of-the-art learning sys-
tems (as the one used in this paper) implement a revision task as a
satisfiability problem for which efficient solvers exist [22].
Typically, logic-based learning algorithms generate minimal so-
lutions, i.e., ones with the fewest number of clauses and literals.
When learning revisions, minimal solutions are ones whose gen-
eration involves the smallest number of change operations. This
enables the revision task to search for solutions that, for instance,
minimize the sum of weights of the derivable consequences in the
highest priority level, and then those in the next lower level, and so
forth. A solution is minimal if the sum of weights of atomic formu-
lae that hold is maximal or minimal, as required by the statement,
among all semantic models of B∧eK.
3 ENVIRONMENT CONDITIONS IN
CHANGING CONTEXTS
As introduced in Section 1, environment conditions are properties
and assumptions about the environment in which the software is
intended to operate. They can take one of three forms: (i )descriptive
properties of the environment: these are the environment properties
introduced in Section 2 (e.g., the trapeze PollutingVehiclesAdmit-
tedin Fig. 1); (ii )prescriptive assumptions about the environment:
these are the goals assigned to environment agents as introduced inSection 2 (e.g., the leaf goal VehiclePlateRecognized assigned to the
ANPR camera agent in Fig. 1); and (iii )descriptive assumptions about
the environment: these are often implicitly assumed and are needed
for refinements to be correct (e.g., Vehicle⇔Car∨Lorry∨Bus).
Environment conditions may hold in specific “contexts” only,
e.g., PollutingVehiclesAdmitted holds only in the London context
but not in the Brussels one. A context is defined as a set of moni-
torable facts that are subject to change over time or space, and that
restrict the validity of an environment condition. A context has
a name, called context label, (e.g., London) and a context predicate
(e.g., InLondon ). In the general case, a context may be specified in
disjunctive normal form where each literal corresponds to a moni-
torable fact, e.g., InLondon∨InBrussels . Contexts form a Boolean
lattice under∨−and∧−operators with false andtrue as bottom
and top, respectively. We can thus consider environment conditions
whose validity spans over unions or intersections of contexts.
Acontext-dependent environment condition holds only in a re-
stricted (set of) context(s). It is written as C:ψfor “environment con-
ditionψholds only in context labelled C”, e.g., London :Polluting⇒
Admitted and Brussels :Polluting⇒¬Admitted.
Auniversal environment condition holds in all contexts. It cor-
responds to True :ψwhere True represents the set of all possible
context labels. A context-dependent environment condition C:ψ
can be made universal through an implication PC⇒ψ, where PC
is the context predicate specifying the context labelled by C. In
practice, we may wish to avoid this for a simpler formulation. An
environment condition valid in a set of contexts is valid in each
member context: if C1, ...,Cn:ψthen Ci:ψ. We write EC⊆Efor
the subset of environment conditions in Edependent on C.
An AND/OR graph may integrate context-specific goal model
adaptations as alternative OR-refinements. This enables moving
back to previously considered contexts. It is, therefore, convenient
toannotate goal refinements involving a context-dependent en-
vironment condition with the label of the context in which this
condition is valid. Context-dependent refinements are thereby cap-
tured, as in [ 5,48]. In Fig. 1, the three bottom AND-refinements
are context-dependent and should be annotated with the context
label London. This label propagates up to a common OR-node to be
created when an alternative context labelled Brussels is introduced.
4 THE REQUIREMENTS ADAPTATION
PROBLEM
Changing contexts may invalidate the context-dependent environ-
ment conditions within a goal model. Hence a new goal model has
to be computed that accounts for the environment conditions of
the new context while preserving model correctness. There might
be several model adaptations whose “qualities" differ. We identify
here two qualities for desirable adaptations.
Similarity . It appears highly desirable to favour model adaptations
that maintain similar functionalities across different contexts. We
measure similarity between a goal Gand its adaptation eGin terms
of their respective types, refinements, formal specifications and
agent assignments. More precisely, the similarity measure for these
two goals is determined by summing up the number of definitions
that are common to the two, and then deducting the number of
definitions that are specific to Gand those that are specific to eG,
53Adapting Requirements Models to Varying Environments ICSE ’20, May 23–29, 2020, Seoul, Republic of Korea
as commonly done in analogical reasoning [ 47]. The similarity
measure for two goal models is the sum of the similarity measures
of their corresponding goals.
Locality. The impact of an adaptation can range from being local
(e.g., affecting goals assigned to a single agent) to system-wide (e.g.,
affecting goals involving multiple agents). Adaptations brought
about by changes that are localized to parts of a goal model are
preferred over adaptations made of disperse changes across the
whole goal model. This principle is inspired by the software evolu-
tion literature where disperse changes risk introducing new errors
[13]. In our case, disperse changes to a goal model may lead to
new incorrect refinements. To measure locality of an adaptation we
introduce the notion of distance between connected goals in a goal
model as the total number of refinement links between them. We
say an adaptation gM1ofMis more local than gM2ofMif the sum
of the distances of all adapted goals in gM1is smaller than in gM2.
To constrain the space of admissible adaptations to local adapta-
tions and similar goal models, we introduce the concept of anchor
goals [ 15]. Here, an anchor goal in a goal model is a non-leaf goal
that should, together with its ancestors, remain unchanged in any
adapted version of the model. In the LEZ example, for instance, an
anchor goal should be PollutingVehiclesPenalized since, in any
foreseeable context for LEZ enforcement, vehicles categorized as
polluting are expected to be penalized even though how this goal is
realized might vary from one context to another. To favour similar-
ity among models and locality of adaptations, anchor goals should
be selected at lowest possible levels in the model to be adapted
whilst being compatible with the new context. (An anchor goal
whose refinement is correct in context C1is said to be compatible
with a context C2if it has a correct refinement in context C2.)
Our adaptation task can be described more precisely as follows:
GIVEN an adaptation problem ⟨E,C,M,C′,A,N⟩where Eare the
environment conditions Cis a source context; Mis a source goal
model that is correct under Caccording to properties (1)–(3) in
Section 2.1; C′is a target context; Ais a set of anchor goals in M;
andNare agents’ capabilities specifications;
FIND a target goal model eMadapted from Msuch that: eMis correct
under the target context C′; each anchor goal AG∈A is unchanged
ineM; every leaf goal fLGineMis realizable; eMis a maximally local
adaptation of M; andeMis maximally similar to M.
5 GOAL MODEL ADAPTATION: OVERVIEW
Fig. 2 shows the basic workflow of our approach, called RESPIRE,
for REquirementS adaPtation In vaRying Environments. Thick-
bordered input boxes are mandatory. At development time, the
analyst provides: (i ) an AND/OR graph M, (ii) a setAof anchor
goals1, (iii) a source context C, (iv) a target context C′and its envi-
ronment conditions EC′, and (v ) the agents’ capabilities specifica-
tion Nfor the target context2. We assume context names correctly
label the corresponding context-dependent refinements in M, each
refinement is correct and the source context Cis indicated inM.
Step 1in Fig. 2, Acquire contextual changes, checks if Malready
contains a model variant, through its OR-branches, that is labelled
1If empty, RESPIRE assumes every root goal in Mto be an anchor goal.
2If no agents’ capabilities specification is provided, then the realizability condition is
dropped from the task specification.
Goal 
AND/OR-graphAgent specification
(1) Acquire 
contextual changes 
(4) Learn model  
adaptation (3a) Check model 
completeness and 
consistencyExisting
adaptation
found?
NOYES
Column widthAnchor goals
YESAdapted goal
model
(6) Consolidate 
AND/OR graph Updated goal
AND/OR-graph
Model 
found?NO(2) Extract reference 
model
Complete & 
consistent?
Minimal?
(5) Update model
adaptationNONOYES
More 
anchors?YES NOYESTarget context/
environment conditions
(3b) Check minimal 
refinementsFigure 2: RESPIRE workflow.
with the target context C′. If found, no adaptation is required and
the model variant eMis returned as the target goal model.
If the target model is not in M,Extract reference model (Step 2in
Fig. 2) computes, from Mand the source context, a reference goal
model to be used for the adaptation process. This model comprises
all AND-refinements that were labelled with the source context C
but in which all environment conditions of the source context are
replaced by those from the target context. This step defines a new
adaptation problem as described in Section 4. RESPIRE then first
a goal AG∈A (according to some assumed selection rule, in our
case, the left most anchor goal appearing in the graph) and uses it
to guide a two-stage adaptation process described below; the two
stages are sequentially applied to each anchor goal in this model.
The first stage involves a counterexample-guided learning loop
(enclosed in dashed lines in Fig. 2). The following steps are repeat-
edly called, starting from the reference goal model:
–Step 3a:Check model completeness and consistency under the tar-
get context. This involves: (i ) constructing completeness and
consistency properties given the selected anchor goal AG; (ii)
generating counterexamples that violate the completeness and
consistency properties under the target context; and (iii ) gener-
ating witness examples to the satisfiability of the completeness
and consistency properties under the target context;
–Step 4: Learn goal model adaptation of the reference goal model
so that refinements in the adapted model are violated in the
counterexamples, but hold in the witness examples;
– Go to step 3a.
The loop repeats until all refinements of the selected anchor goal
AGare complete and consistent in the adapted goal model, or the
learner fails to find such a model. (The latter may occur owing to
incompatibilities between the anchor goal and some new environ-
ment conditions.) If the refinements are complete and consistent
with respect to the anchor goal AG, then the second stage of the
adaptation is triggered.
This second stage involves checking that the refinements of AG
in the model generated from the first stage satisfy the minimality
54ICSE ’20, May 23–29, 2020, Seoul, Republic of Korea Dalal Alrajeh, Antoine Cailliau, and Axel van Lamsweerde
property in the Check minimal refinements process (Step 3bin Fig. 2).
If not satisfied, then Update model adaptation (Step 5in Fig. 2) removes
refinement links until all refinements satisfy it.
If there remain anchor goals in Athat have not been checked for
correctness, RESPIRE chooses one and instigates another iterative
adaptation process as described above (Steps 3a,4,3band 5). The
iteration terminates once the refinements of all anchor goals are
correct under the target environment conditions considered, or no
correct goal model adaptation is found by the learner.
If successful in finding a correct model adaptation, the proce-
dure proceeds to Consolidate AND/OR-graph (Step 6in Fig. 2), where
the original graph AND/OR Mis updated with an OR-refinement
representing the computed target model. The output of RESPIRE is
the newly computed target model and the updated Mgraph.
RESPIRE can run with varying degrees of human intervention
within the dashed lines in Fig. 2, e.g., for leaf goal selection or for
acceptance, rejection or modification of generated adaptation traces
(see discussions in Sections 7 and 8).
6 THE RESPIRE PROCEDURE
This section details the core steps 3a,4,3band 5in Fig. 2. We start
with a reference model, M0
C′, in which all context-dependent en-
vironment conditions are now those of C′, and anchor goals are
labelled as anchors. In our running example, the specifications of
theLondon-dependent environment conditions PenalizedVehicle-
sIssuedFineOrChargePaid, PollutingVehiclesAdmitted, FineIssued-
ByOffenceType and PollutingByVehicleType are replaced by the
following Brussels-dependent environment conditions, respectively:
Brussels :Penalized⇔FineIssued
Brussels :Polluting⇒¬Admitted
Brussels :Polluting⇔((Diesel∧Euro≤3)∨(Petrol∧Euro≤1)) (4)
Brussels :FineIssued⇔((FirstTime→OneHundredFiftyFine)
∧(¬FirstTime→TwoHundredFiftyFine))
whilst the London environment condition TwentyPaidWhenCharge-
Paid is deleted. The obvious anchor goal here is PollutingVehicle-
sPenalized since any LEZ enforcement system has to meet this.
6.1 Checking Model Correctness
This step takes a goal model eMC′—at start, this is the reference
model M0
C′—and checks if all its AND-refinements are correct under
the target context C′(steps 3aand 3bin Fig. 2). The outcome is a
confirmation that either (i ) the input model is correct; (ii ) the input
model is complete and consistent but not minimal; or (iii ) the input
model is incomplete or inconsistent, and a counterexample trace.
Given an anchor goal AGinA, RESPIRE constructs satisfaction
arguments for the completeness and consistency of its refinements
first, and verifies these to generate counterexamples and witness
examples if these arguments do not hold. An obvious option would
be to construct these arguments between the anchor goal (as conclu-
sion) and its “immediate" subgoals (in the premises) first, check the
satisfaction of these, and learn an adaptation before recursively pro-
ceeding to the construction of arguments for descendent subgoals
in a top-down fashion. This option would deem some computations
wasteful as the satisfaction argument for a parent goal may become
invalidated by adaptations to its subgoals. In contrast, RESPIREconstructs satisfaction arguments for refinement completeness/con-
sistency between leaf goals and their ancestral anchor goals. This
provides several benefits: (i ) the traces generated by the checks and
used by the logic-based learning algorithm are guaranteed to be
consistent with the agents’ capabilities specification—this in turn
guarantees that the adapted goals are realizable; (ii ) the generated
traces are more concrete and provide the learning algorithm with
finer-grained examples highlighting which parts of the goal speci-
fications need to be adapted; and (iii ) unnecessary re-computations
of adaptations to parents are avoided as changes to the leaf goals
automatically produce changes to their parent goals, and ensure
the progress of all ancestral refinements towards correct ones. The
construction of satisfaction arguments for checking minimality is
discussed in Section 6.1.3.
The three main checks RESPIRE conducts for verifying the adapted
model’s correctness are now further detailed.
6.1.1 Checking Completeness. RESPIRE first verifies the complete-
ness of refinements of the anchor goal AGin the input model under
the target context C′. In what follows, eMi
C′denotes the goal model
generated in the ith iteration of the counterexample-guided learn-
ing loop, thus representing the ith adaptation of the reference goal
model M0
C′. Initially, i=0.
The verification is done by model checking that all leaf goals
satisfy their anchor goal:
{Û
leaves(AG ,eMi
C′),PC′,EC′}|=AG (5)
where leaves denotes the set of leaf goals descending from AGin
eMi
C′; a counterexample to the argument (5) is a witness to the
incompleteness of AG’s refinement.
In our example, the reference model M0
Brusselscontains the an-
chor goal PollutingVehiclesPenalized specified as Polluting⇒
Penalized . Its refinement is incomplete since there is a trace in
which the leaf goals FineSetByVehicleAndOffenceType and Vehicle-
TypeCharged, specified as
(((Lorry∨Bus)∧ Euro≤3)∨( Car∧((Diesel∧Euro≤3)∨
(Petrol∧Euro≤5)))∧¬TwentyTwoPaid)⇒ (6)
((FirstTime→SixtyFiveFine)∧(¬FirstTime →OneHundredNinetyFiveFine))
((Lorry∨Bus)∧ Euro≤3)∨(Car∧((Diesel∧Euro≤3)∨
(Petrol∧Euro≤5)))⇒ TwentyTwoPaid (7)
respectively, and the environment conditions of the target context,
specified in (4), are satisfied but PollutingVehiclesPenalized is not.
The problem is demonstrated by the trace s1(s2)ωwith:
s1={Polluting ,¬Admitted ,¬ChargePaid ,Lorry ,TwentyTwoPaid ,Diesel ,
¬Petrol ,Euro≤3,Euro≤5,¬Euro≤1,FirstTime ,¬SixtyFiveFine ,
OneHundredNinetyFiveFine ,OneHundredFiftyFine ,¬TwoHundredFiftyFine ,
FineIssued ,Penalized ,¬Car ,¬Bus}
s2={¬FineIssued ,¬Penalized ,¬OneHundredFiftyFine }
The truth values for all variables are shown for the initial state,
denoted s1, followed by their values in consecutive states only if
changed from the previous state. Otherwise, their truth values are
assumed to persist. This counterexample violates PollutingVehi-
clesPenalized ins2where Polluting is true but Penalized is false.
If a counterexample to (5) is found, it is added to the set Σ−
which accumulates all (negative) traces demonstrating incomplete
55Adapting Requirements Models to Varying Environments ICSE ’20, May 23–29, 2020, Seoul, Republic of Korea
refinements. This set is then used by the learner in the next step to
prune out adaptations that falsify the anchor goals.
On the other hand, to guide the search towards correct adap-
tations, RESPIRE seeks to find (positive) traces that exhibit how
anchor goals may be achieved in the target context. To this end,
RESPIRE selects a leaf goal LGinleaves(AG ,eMi
C′), at random or ac-
cording to some ordering provided by the analyst. It then constructs
and model-checks the argument
{Û
anchors(LG ,eMi
C′),PC′,EC′}|=LG (8)
where anchors denotes the set of anchor goals that are ancestral
to the leaf goal LGineMi
C′. Traces violating the above argument
are called adaptation traces. They demonstrate situations in which
the anchor goal and the pre-adapted leaf goal are not satisfied to-
gether under the target context (hence requiring an adaptation).
In the LEZ example, we may consider the leaf goal FineSetByVe-
hicleAndOffenceType whose payment of pollution charges is ir-
relevant in Brussels. (Relevance may be assessed from the target
environment conditions; for instance, a goal is relevant if the propo-
sitional variables appearing in its goal specification also appear
in at least one environment condition of the target context.) To
generate an adaptation trace, RESPIRE checks the argument (8),
where the specifications of FineSetByVehicleAndOffenceType as
LG, and PollutingVehiclesPenalized as its single ancestral anchor,
respectively. A counterexample to this is the trace s1s2(s3)ω:
s1={Polluting ,¬Admitted ,¬ChargePaid ,Lorry ,Diesel ,
Euro≤3,TwentyTwoPaid ,FirstTime ,¬SixtyFiveFine ,
¬OneHundredNinetyFiveFine ,OneHundredFiftyFine ,
FineIssued ,Penalized ,¬TwoHundredFiftyFine }
s2={¬TwentyTwoPaid} s3={TwentyTwoPaid}
The original leaf goal is violated above as a lorry whose euro cate-
gory is less than 3 and which did not pay the charge is not set the
correct fine of SixtyFiveFine ins1. This is acceptable behaviour in
the target context where the correct fine of OneHundredFiftyFine for
a first time offender is set. Note that as TwentyTwoPaid is irrelevant
in the Brussels context, its truth value does not affect the acceptabil-
ity of the trace since—we will see in Section 6.2—adaptations are
restricted to the language relevant to the target context. Traces vio-
lating the argument (8) are added to the set Σ+. A counterexample
to the satisfaction of a leaf goal could be deemed unacceptable by
the analyst. (Section 7 shows how our implementation of RESPIRE
supports human-in-the-loop to assess the acceptability of and mod-
ify the counterexample before it is added to Σ+.)
The output of a failed completeness check is two sets of traces:
Σ−, containing traces violating the anchor goal in the target context
C′, andΣ+, containing traces that satisfy it in C′.
6.1.2 Checking Consistency. To ensure that refinements in the
adapted goal model are consistent, RESPIRE constructs and model-
checks the following argument.
{AG,PC′,EC′}|=¬Û
leaves(AG ,eMi
C′) (9)
A violation to (9) is a demonstration of the leaf goals’ consis-
tency. The satisfaction of this argument, on the other hand, shows
that at least one of the goals in leaves( AG,eMi
C′)cannot be satis-
fied together with the other leaf goals under the target context;
thus requiring adaptation. In this case, RESPIRE selects a leaf goalLG∈leaves(AG ,eMi
C′), searches for an adaptation trace violating
LGas per argument (8) and adds this trace to the set Σ+.3
Consistency checks are performed only once for each anchor
goal. The reason is that the subsequent learning steps guarantee,
by construction, that all adapted goals are true in each adaptation
trace. Inconsistency can only arise between goals of the reference
goal model eM0
C′and the target environment conditions, leading to
the vacuous satisfaction of the completeness property. The output
of this check is an updated set Σ+of adaptation traces.
6.1.3 Checking Minimality. If the AND-refinements of anchor goal
AGare complete and consistent, then RESPIRE proceeds to Step
3bin Fig. 2, where every such refinement in eMi
C′is checked for its
satisfaction of the minimality property (3) of Section 2.1.
In contrast with completeness checks, minimality checks are
conducted for every refinement starting from the anchor goal and
its immediate subgoals, and recursively in a top-down fashion.
Minimality checking considers the direct refinement of a parent
goal, dropping a subgoal and checking for completeness of the
remaining subgoals with respect to the parent goal. This top-down
strategy avoids unnecessary checks for refinements that would
be eliminated in Step 5by the removal of ancestral refinements
violating the minimality property. If a refinement is found to be
not minimal, the procedure moves to Step 5in Fig. 2.
6.2 Learning Model Adaptations
Step 4in Fig. 2 takes as input the anchor goal AG, reference goal
model M0
C′, target context C′and environment conditions EC′,
agents’ capabilities specifications N, and sets Σ+andΣ−produced
by the completeness and consistency checks above. The aim of this
step at the ith iterated adaptation of the reference model is to find
a model adaptation eMi+1
C′where:
(a) for every σ∈Σ+,eG∈eMi+1
C′, we haveσ|=eG;
(b) for every σ∈Σ−, we haveσ̸|=Óleaves(AG ,eMi+1
C′);
(c) each anchor goal AG∈A is unchanged in eMi+1
C′;
(d) every leaf goal eGineMi+1
C′is realizable;
(e)eMi+1
C′is a maximally local adaptation of M0
C′;
(f)eMi+1
C′is maximally similar to M0
C′.
The second requirement above ensures the correctness of the
adapted model; since every trace in Σ−violates an anchor goal, it
should also violate at least one goal in its refinement.
This step deals with a restricted form of the adaptation task
defined in Section 4; it considers correctness of the adapted model
w.r.t. traces in Σ−andΣ+. We call this a trace-driven adaptation task .
We thus introduce a weaker notion of completeness and consistency.
Given a trace σ, parent goal PGand subgoals{SGi}⊆PGRefinedBy ,
{SGi}is atrace-complete refinement w.r.t. σ, iff
σ|={SG1, ...,SGn,E}impliesσ|=PG (10)
It is said to be a trace-consistent refinement w.r.t. σiff
σ|={SG1, ...,SGn,E} (11)
A goal model is trace-consistent and -complete w.r.t. σif every
refinement is trace-consistent and -complete w.r.t. σ, respectively.
3The selection of leaf goals can be done at random automatically or user-driven. This
is discussed in Section 7.
56ICSE ’20, May 23–29, 2020, Seoul, Republic of Korea Dalal Alrajeh, Antoine Cailliau, and Axel van Lamsweerde
To compute such adaptations, the learning step comprises four
main sub-steps: (i ) encode the trace-driven adaptation task into
a learning-based revision task; (ii ) define the space of preferred
adaptations; (iii ) compute an adaptation as a solution to the revision
task; and (iv ) encode the solution to the revision task back into an
adapted goal model. The first three sub-steps are described below;
the last is simply the inverse of the first.
6.2.1 Encoding the adaptation problem. Our learning algorithm
relies on a canonical representation of goal models, on the semantics
of correct refinements, and on the syntax and semantics of the class
of MLTL formulas expressible as normal logic programs. We thus
develop an encoding (JK)that extends the one presented in [ 8] with
the rules below. The symbol lϕ(resp. lσ) corresponds to a constant
uniquely representing the formula ϕ(resp. trace σ);lαis a constant
representing agent α’s name. Monα⊆P (resp. Ctrlα⊆P) denotes
the set of monitorable (resp. controllable) propositional variables
for this agent. The main property of this encoding is that, for any
given trace σand MLTL safety property ϕ, we haveσ,i|=ϕiff
true(lϕ,i,lσ)⊂ M(JϕK∧JσK).
JGResp Kdef=assigned_to(G Name ,GResp)
JGAnchor Kdef=anchor(G Name) if G Anchor =true else empty
JCKdef=context( PC)
JNKdef=Ó{monitorable( lα,x)|x∈Mon αandα∈N}∧Ó{controllable( lα,x)|x∈Ctrlαandα∈N}
JΣKdef=Ó{true(x,i,lσ)|x∈σ(i)andσ∈Σ}∧Ó{false( x,i,lσ)|x<σ(i)andσ∈Σ}
JGKencodes G’s definition. The background theory Bcomprises
the encoding of anchor goals’ definitions in M0
C′(excluding their
refinement links), the target context and its environment conditions,
agent capabilities’ specification and traces in Σ+andΣ−:
Ó{JAG\RefinedBy K|AG∈anchors(LG ,M0
C′)}∧
∧JC′K∧JEC′K∧JNK∧JΣ+K∧JΣ−K
In addition, Balso comprises a declarative representation of the
semantics of trace-complete refinements, for all PG∈M0
C′:
Ó{true(SG Name ,lΣ)|SG∈M0
C′and SG∈PGRefinedBy}
→true(PG Name ,lΣ)
false(PG Name ,lΣ)→
¬Ó{true(SG Name ,lΣ)|SG∈M0
C′and SG∈PGRefinedBy}
where lΣis a universally quantified variable over Σ+∪Σ−. The first
is a direct encoding of (10). The latter encodes the requirement (b)
described above. The trace-consistent refinement property (11) is
encoded implicitly in the examples as discussed below. To learn
model adaptations whose refinements comprise new goals, Bmay
be extended with placeholders for goal definitions.
The revisable theory K, instead, includes an encoding of defini-
tions of all goals descending from an anchor goal that is an ancestor
to the selected leaf goal, and an encoding of the refinement relations
between anchor goals and their subgoals as follows.
Ó{JAGRefinedByGK|AG∈anchors(LG ,M0
C′)}∧Ó{JGK|G∈desc(AG ,M0
C′)and AG∈anchors(LG ,M0
C′)}
The examples O+∪O−capture the trace-consistent refinements’
requirement—it necessitates all adapted goal specifications in eKtohold in every trace in Σ+and not in Σ−:
O+={true(G Name ,lσ)|G∈M0
C′andσ∈Σ+}and
O−={false(AG Name ,lσ)|AG∈M0
C′andσ∈Σ−}.
To guide the learning procedure towards adaptations yielding
correct refinements and realizable leaf goals, RESPIRE specifies: (i )
a mode bias Ithat restricts the propositional variables allowed to
appear in goal specifications to those “relevant” to the new context;
and (ii ) structural constraints Dover the set of specifications and re-
finements relations to which any candidate goal model must adhere.
The interested reader is referred to the extended version of this
paper in [ 43] for details of the mode bias Iand structural constraints
D. Note that the encoding for all the above definitions is generated
once for every selected anchor goal. Only new counterexamples
and adaptation traces are encoded in subsequent iterations.
6.2.2 Defining preferred adaptations. One key novelty in our learn-
ing procedure is the use of soft preferences to guide the revision
task towards adaptations that meet the similarity andlocality re-
quirements defined in Section 4. RESPIRE implements these notions
by defining an adaptation cost and adaptation impact.
Adaptation cost. When computing adaptations, RESPIRE applies a
series of change operations to goal definitions encoded in K. These
include the following operations: modify a type, add/delete a refine-
ment relation, add/delete a literal to/from a specification or delete
a responsibility assignment. The application of these operations
defines an adaptation mapping between goal definitions. Given
the definition of two goals G1and G2, an adaptation mapping is
the smallest set of change operations that can be applied to G1to
obtain G2. The costof a goal adaptation is given by the size of the
adaptation mapping between G1andG2. We assume a uniform cost
of one for change operations. The total cost of an adaptation of
Mis the sum of all goal adaptations’ costs. The learning aims at
finding adaptations that minimize this cost.
The adaptation cost implicitly captures a measure of similarity
between two sets γ1andγ2based on Tversky’s [47]:
similar(γ1,γ2)=|{x|x∈γ1∩γ2}|−
|{x|x∈γ1andx<γ2}|−|{ x|x<γ1andx∈γ2}|(12)
(Whenγ1,γ2are elements, then similar(γ1,γ2)=1ifγ1=γ2;
0otherwise.) Similarity between goals is thereby defined as the
similarity between their types, specification, refinements and re-
sponsibility assignments: similar( G,eG)=similar( GType ,eGType)+
similar( GSpec ,eGSpec)+similar( GRefinedBy ,eGRefinedBy)+similar(
GResp ,eGResp), where similar( GSpec ,eGSpec)is the sum of the simi-
larity measures between their respective Current - and Target -condition
and temporal operators.4Forsimilar(Current G,Current eG), for in-
stance, the first term of Equation (12) corresponds to literals that
are unchanged in the Current -condition of GineK. The second cor-
responds to those deleted from the Current -condition of GinKand
the third corresponds to those added to it. (Goals not included in
any refinement link in eMare considered to be deleted from M.)
The similarity measure between goal models is defined as the
sum of the similarity measures between every goal GinMand its
adaptation eGineM. The cost of the adaptation, denoted cost(M,gM1),
is the inverse of the similarity measure. We say that an adaptation
4Note that we consider here syntactic similarity. We reserve discussion on semantic
similarity for Section 9.
57Adapting Requirements Models to Varying Environments ICSE ’20, May 23–29, 2020, Seoul, Republic of Korea
M1ofMis less costly than gM2iffsimilar( M,gM1)>similar( M,gM2).
The learning procedure aims to minimize the adaptation cost (i.e.,
maximize the similarity between a goal model and its adaptation).
Adaptation impact . Impact measures how local the changes are
in the reference goal model M0
C′. To measure the impact of an
adaptation, we define a notion of distance between goals in a goal
model. The distance between two connected goals G1and G2in
model M, denoted by dist(G1,G2,M), is the minimum number of
refinement links connecting them. RESPIRE computes this measure
by keeping track of all leaf goals selected in Step 3a. We denote
this set as selected( M). The locality of an adaptation eMtoMis
calculated by summing the distances between every leaf goal LG∈
selected( M)and goal XinMwhere both have been modified in eM
and, in the case where a new goal Xis added to eM, the distances
between the selected LGand between the goal Xand some common
ancestral anchor appearing in both MandeMas given below.
local( M,eM,LG)=Í
x∈I1(LG)x+Í
y∈I2(LG)y+Í
z∈I3(LG)z
I1(LG)def={dist(LG ,X,M)|X∈MandeX∈eMand
XName =eXName and X .eX}
I2(LG)def={dist(LG ,X,M)|X∈Mand X <eM}
I3(LG)def={dist(LG ,G∗,M)+dist(G∗,X,eM)|G∗∈M∩eM
and G∗is the closest ancestor to LG in M
G∗∈ances(X ,eM)and X <Mand X∈eM}(13)
The impact of an adaptation eMofM, denoted impact( M,eM), is
the sum of the locality measures w.r.t. all LG∈selected( M). Thus
an adaptation eM1has less impact on (i.e., more local in) Mthan
an adaptation eM2iffimpact( M,eM1)<impact( M,eM2). The learning
procedure aims to minimize the impact of an adaptation.
6.2.3 Computing adaptations. The learning algorithm attempts to
find an inductive solution that optimizes the preference criteria
described above. Conflicts may arise when attempting to minimize
both the adaptation cost and impact. To resolve these, we deploy
a pre-emptive optimization strategy in which cost minimization
is prioritized over impact minimization. If a solution exists, the
adapted goal specifications are guaranteed to hold in all traces
ofΣ+but in none of Σ−—thus meeting the trace-consistent and
-complete refinement properties. For our LEZ example, the learning
step returns for instance the following adaptation of FineSetByVe-
hicleAndOffenceType, compactly specified as:
(Euro≤3∨(Diesel∧Euro≤3)∨ Petrol)⇒
((FirstTime→OneHundredFiftyFine)∧
(¬FirstTime→TwoHundredFiftyFine))
Since the leaf goals (6) and (7), both of which appear in eM, were mod-
ified, we have impact( M,eM1
Brussels)=4as there are 4refinement
links connecting them (i.e., I1(LG) =4andI2(LG) =I3(LG) =0).
cost(M,eM1
Brussels)=14for the deletion of the literals Lorry, Bus,
Car, TwentyTwoPaid and Euro≤5(which appear in both), and
SixtyFiveFine andOneHundredNinetyFiveFine from (6) and and the
addition of OneHundredFiftyFine andTwoHundredFiftyFine to (6).
Once a solution is generated, the adapted model eMi+1
C′is checked
for complete refinements w.r.t. AGas per Section 6.1.1.6.3 Updating adaptations towards minimality
As noted in Section 6.1.3, once all refinements of an anchor goal
in the adapted goal model are complete and consistent, RESPIRE
conducts a minimality check for every parent goal in the adapted
model (Step 3bin Fig. 2). The aim of this step is to remove subgoals
that violate the minimality property of (3). Starting at the anchor
goal, if removing a direct subgoal maintains completeness, the
subgoal and its descendants are removed. Finding a minimal set
of subgoals satisfying (3) is NP-hard. However, it is assumed that
parent goals in well-structured models have a small number of
immediate children [48] making this computation tractable.
Column widthPollutingVehiclePenalised
FineIssuedForPollutingVehicles
PollutingVehiclesNotAdmitted PollutingByFuelType
FineSetByVehicleAndOffenceType
 Electronic fee collectorFineIssuedByOffenceTypePenalisedVehiclesIssuedFine
Figure 3: Adapted goal model for Brussels LEZ.
In our LEZ example, two more iterations over the anchor goal
PollutingVehiclesPenalized result in a complete refinement. The
minimality check finds that dropping the subgoal ChargePaidWhen-
PollutingVehicleAdmitted maintains completeness of the goal Pol-
lutingVehiclesPenalized’s refinement. Therefore, this subgoal and
its subtree are removed. The updated goal model is shown in Fig. 3.
7 IMPLEMENTATION
We developed a prototype toolset (in Python) implementing the
adaptation procedure detailed in Section 6. The toolset integrates a
number of third-party libraries and software. We use the Python
Boolean library [ 1] to rewrite the Current-conditions of specifica-
tions into a standard form. We use the NuSMV2 2.6 symbolic model
checker [ 16] to(i)check the correctness of goal models; and (ii)gen-
erate counterexamples. We built upon the RASPAL learning-based
revision tool [ 10] for our learning engine. We built an automated
translator to convert goal specifications into NuSMV and RASPAL
syntax, and vice versa.
The tool can run in fully automated mode or in interactive mode.
The automated mode forms the base setting in which: (i ) all propo-
sitional variables appearing in the environment conditions of the
target context, agents’ capabilities specifications and anchor goals
are considered when computing adaptation; (ii ) leaf goals of the an-
chor goal are selected at random when verifying argument (8); (iii )
all generated adaptation traces are accepted (without modification);
and finally (iv ) the size of the adapted goal model is fixed.
The interactive mode supports various degrees of human control
including: (i ) selecting the variables that may appear in the target
model; (ii ) selecting the leaf goal to be checked; (iii ) confirming
the suitability of the automatically generated adaptation trace; (iv )
providing an alternative adaptation trace if not; and finally (v ) en-
larging the size of the search space by allowing RESPIRE to increase
the number of goals in the target model or its refinement depth,
or to explore alternative refinement tactics. Each of these features
can be enabled when RESPIRE is launched. They are introduced to
speed up the adaptation process towards relevant models.
58ICSE ’20, May 23–29, 2020, Seoul, Republic of Korea Dalal Alrajeh, Antoine Cailliau, and Axel van Lamsweerde
8 EVALUATION
In this section, we evaluate our approach in terms of its correctness
and applicability to real-world problems.
8.1 Correctness
Theorem 1 (Termination). The RESPIRE adaptation procedure
is guaranteed to terminate.
Proof sketch. There are three loops in the adaptation procedure.
The loop over anchor goals terminates as the set of anchor goals is
finite. The counterexample-guided learning loop within the dashed
lines in Fig. 2 is guaranteed to terminate since: (i ) counterexamples
and witness examples are accumulated in the sets Σ+andΣ−thus
ensuring that a new goal model is learned in every iteration that is
semantically different from previous ones; and (ii ) the adaptation
space is finite since the sets of change operations and of proposi-
tional variables are finite. The loop for minimality checking and
model updating also terminates as the set of subgoals is finite.
Theorem 2 (Soundness and completeness). LetΣ+andΣ−
be sets of traces generated by RESPIRE for the adaptation problem
⟨E,C,M,C′,A,N⟩. If RESPIRE returns a goal model eMC′, theneMC′
is correct under the target context. If RESPIRE returns ⊥, then no adap-
tation exists in the constrained search space, that is trace-consistent
and -complete w.r.t. Σ+andΣ−under C′.
Proof sketch. Soundness is guaranteed since RESPIRE only con-
structs well-formed goal models that satisfy the correctness prop-
erties (1–3) of Section 2. Moreover, the learning procedure fails
in two cases: (i ) the sets of examples provided by the analyst are
not well-separated (i.e., a trace can appear as both a positive and
negative); or (ii ) no solution exists within the search space.
Theorem 3 (Similarity and locality). Let⟨E,C,M,C′,A,N⟩
be an adaptation problem, Σ+andΣ−the traces generated by RESPIRE
andeMC′an adaptation solution. Then eMC′is a maximally similar
adaptation to M0
C. It is a maximally local adaptation, within the space
of a maximally similar adaptations.
This follows from the definition of the reward function in the
learning step (see Section 6.2). RESPIRE supports the search for
similar goal models by providing the learning step with the refer-
ence goal model M0
C′as input. As the search for a solution is driven
by the sample of traces used, a local “minimal” solution to the
adaptation problem, rather than a global minimal one is ensured.
8.2 Applicability
We briefly report on the application of RESPIRE to two real computer-
aided ambulance dispatch (CAD) systems: one for Belgium [ 42] and
one for Wales [ 45]. Three common anchor goals were considered,
yielding three sub-models to be adapted from Belgium to Wales
(B-W) and three vice versa (W-B). A description of the number of
goals and environment conditions in the sub-models rooted on the
same anchor goal across the two contexts is given in Table 1.
Anchor ID #EB #GB #EW #GW
1 4 2 2 3
2 14 4 7 3
3 3 2 5 4
Table 1: Description of source/target sub-models considered.Our RESPIRE tool was applied in two modes: the fully automated
mode and the interactive mode. In the interactive mode, the analyst
could: (i ) assess and manually change the automatically generated
adaptation traces to suit the target context; and (ii ) increase the the
size of any adapted model to be computed. The first author acted as
the analyst. All other steps were done automatically. Experiments
were conducted using a 2.8 GHz, Intel Core i7 processor.
The fully automated mode often terminated unsuccessfully in
at most two iterations owing to either the automatically generated
adaptation traces being inadequate for the target context, the need
to increase size of the adapted model or missing environment prop-
erties. Consider the anchor goal “an ambulance shall arrive at the
scene of a reported incident on time ". Checking its completeness in
the Wales context results in an adaptation trace for the leaf goal
MaxInterventionTimeIs12Minutes where an ambulance requires
more than 14 minutes to arrive (without any problem along the
way): this scenario is inadequate in the Wales context. By mod-
ifying the ambulance’s arrival time in the trace, as indicated by
EW, RESPIRE finds the correct solution. Hence, we focus in our
discussion on the interactive mode experiments.
RESPIRE terminated successfully for all six adaptation prob-
lems. It effectively covered different types of adaptations requiring
modifications to: goal specifications (GS); refinement links (RL);
refinement tactics (RT) by transforming a case-driven goal refine-
ment into a milestone-driven one [ 48] (and vice versa); and the goal
model’s depth (RD). Table 2 gives a summary of the results.
# Lrn G T # Min. G T #Iter. Type Avg. cost Avg. reward
B-W-1 2 2 2 GS, RL 4 1
B-W-2 3 2 2 GS, RT 12 2
B-W-3 3 3 1 GS, RD, RL 14 4
W-B-1 2 1 1 GS 10 2
W-B-2 3 3 6 GS, RT, RL 12.5 3
W-B-3 3 1 1 GS, RD 14 3
Table 2: Summary of second experiment results.
#Lrn. G Tgives the number of learned goals in the adapted sub-
model of the target context. #Min. G Tis the number of goals in the
updated goal model after the minimality check and model updat-
ing.#Iter. is the number of completeness checks performed until
RESPIRE terminates. The column Type gives the form of adaptation
involved. The last two columns give the average costs and impact
of all adapted models computed until RESPIRE terminates.
For instance, we know from Table 1 that the source sub-model
rooted on the anchor goal with ID 1contained four environment
conditions which were replaced by two environment conditions for
the Wales context. The original Belgium sub-model contained two
goals (including the anchor goal), whilst the Wales one contained
three. From Table 2, we see that RESPIRE learned two goals in solv-
ing the adaptation problem B-W for the sub-models rooted on the
anchor goal with ID 1(i.e., B-W-1); both were maintained after the
minimality check. Two iterations were needed to termination. The
adaptation involved changes to the goal specification and addition
of a refinement link (of distance 2). The average cost of the adapted
models computed over the iterations is 4and the average impact is
1. We notice that RD adaptations are typically more costly as they
require learning the full specification of the new leaf goals.
Fig. 4 plots the time in seconds (y- axis) spent by the model
checking (blue) and the learning (red) components in an application
of RESPIRE to each adaptation problem (x- axis). The variance in
59Adapting Requirements Models to Varying Environments ICSE ’20, May 23–29, 2020, Seoul, Republic of Korea
each box is over time spent in iterations of the counterexample-
guided learning loop. Overall, the time spent by the two components
did not exceed 1.6 sand the time spent by each component increased
linearly as the number of iterations increased.
B-W-1 W-B-1 B-W-2 W-B-2 B-W-3 W-B-300.20.40.60.811.21.41.61.82
Model checking Learning
Figure 4: Performance of RESPIRE in interactive mode.
9 DISCUSSION
Scalability. RESPIRE’s scalability relies on that of the model check-
ing and learning procedures. We use NuSMV, an efficient BDD-
based checker heavily deployed in practice. The learning engine is
implemented using the answer set solver Clingo [ 23] whose scal-
ability is comparable to SAT solvers. A crucial point is the size of
the search space. The interactive mode, therefore, also allows the
analyst to restrict the search space to a practical scope.
Human control. Having analysts in the loop is important in pro-
ducing adequate models for unseen contexts. This was particularly
apparent in the choice of adaptation traces as these guide the out-
come of the learning procedure and subsequently generated coun-
terexamples. We argue that RESPIRE’s benefits are not hindered by
this; modifying trace examples is often considered less error-prone
than modifying the specifications [ 25]. In our experience, trace
modification typically involved changing the truth values of one or
two propositional variables in the automatically produced trace at
most. We also found that allowing the analyst to assess the adapta-
tion traces often highlighted implicit environment conditions, e.g.,
Critical⇒¬HighlyCritical inW-B-2. Though the fully automated
mode terminated unsuccessfully initially for W-B-2, two goals were
correctly computed. Increasing the number of goals in the adapted
model in interactive mode, here by 1, lead to successful termination.
Semantic similarity. RESPIRE searches for syntactically similar
goal models which may yield semantically different models (i.e., in
terms of the set of behaviour they prescribe). We can leverage for
this by extending the step 3awith an additional query to the model
checker, namely,Ódesc(AG ,M0
C′)∧PC′∧EC′|=¬AG . Counterex-
amples are then added to Σ+. We will treat this in future work.
10 RELATED WORK
Much of RE work on adaptation so far has focused on reconfigura-
tion within a fixed, predefined adaptation space [ 3,4,9,28,31,34].
None is driven by satisfaction arguments. In [ 31], a general problem
definition framework is proposed for requirements-based runtime
adaptations as an alternative to [ 49] which this work builds on.
The work in [ 4] also presents a notion of contextual goal models
where goals and refinements are decorated with context conditions
that are monitored at run-time for switching to more appropriate
alternative refinements. The work in [ 24] addresses the selection
of most appropriate configurations for a dynamic software productline. The requirements there are assumed to be known at RE time
and are fixed. Our technique might be seen as complementary by
learning modified requirements that could be taken as input.
The techniques proposed in [20] and [39] compute new specifi-
cations that satisfy modified requirements. In both techniques, the
computed specifications are evaluated in terms of: (a)the number of
elements in common with the current specifications; (b)the change
effort required to modify the current specifications; and (c)the
reuse of previous specifications. Our technique might complement
these approaches by providing the modified requirements as input.
Learning-based solutions include [ 8,46]. In [ 46], the authors
define a probabilistic logic-based learning approach to revise be-
haviour models for reactive planning at run-time. Their focus is
restricted to revising probabilities over environment assumptions
(expressed as condition-event invariants). The work of [ 8], as noted
in Section 1, differs substantially, including in: (a)purpose and prob-
lem addressed (finding countermeasures to obstacles in a single
environment vs. finding model adaptations across varying envi-
ronments);(b)method (type of model-checking and learning tasks,
when/how to perform these); (c)degrees of automation; (d)types
and properties of model revisions (e.g., refinement minimality); and
(e)specifications’ semantics (event-based vs. state-based—the latter
being better suited for reasoning about goal models [ 48]). As in [ 8],
however, goal models are represented as normal logic programs and
our inductive learning solutions can be classified as oracle-guided.
RESPIRE shares its essence with counterexample-guided learning
methodologies, e.g., for invariant discovery [ 21]. The closest work to
our learning methodology are [ 6,7]. Unlike ours, however, (a)their
learning approaches are based on incremental refinement, which
reduces the prescribed behaviour monotonically; (b)preferences
are not specified over the solution space; and (c)both approaches
assume an event-driven asynchronous semantics of goal models.
11 CONCLUSION AND FUTURE WORK
The adequacy, completeness and consistency of software require-
ments depend on the software environment. Environment condi-
tions are involved in satisfaction arguments, an important aspect
of the RE process [ 49]. In varying environments, such arguments
may be invalidated. A tool-supported approach was presented for
adapting a given goal model automatically to new environment
conditions. This approach combines verification-learning cycles
constrained by preserving higher-level goals and by minimizing
the adaptation cost and impact. Unlike other approaches to re-
quirements adaptation, the space of possible changes needs not
be explicit in advance. The application of RESPIRE to documented
variations of a real ambulance system revealed that our approach
goes far beyond mere renaming of variables in goal specifications.
This work builds a theoretical foundation for and assesses the fea-
sibility of learning requirements adaptations. Further experiments
into factors affecting the performance of RESPIRE, such as leaf
goal selection and its usability is part of our future work. Common
challenges of using formal methods at RE time remains. In partic-
ular, the environment knowledge used in satisfaction arguments
has to be made explicit; complementary means are thus needed for
handling the common tacit knowledge problem in RE. In addition,
we intend to expand RESPIRE for runtime adaptations, e.g., using a
runtime verifier.
60ICSE ’20, May 23–29, 2020, Seoul, Republic of Korea Dalal Alrajeh, Antoine Cailliau, and Axel van Lamsweerde
REFERENCES
[1] [n. d.]. ([n. d.]). http://formal.cs.utah.edu:8080/pbl/PBL.php.
[2]European Environment Agency. 2019. Low Emission Zones. (2019). http:
//urbanaccessregulations.eu/low-emission-zones-main/.
[3]Germán H Alférez and Vicente Pelechano. 2012. Dynamic Evolution of Context-
Aware Systems with Models at Runtime.. In MoDELS. Springer, 70–86.
[4]Raian Ali, Fabiano Dalpiaz, and Paolo Giorgini. 2010. A goal-based framework
for contextual requirements modeling and analysis. Requirements Engineering
15, 4 (2010), 439–458.
[5]Raian Ali, Fabiano Dalpiaz, Paolo Giorgini, and Vítor E. Silva Souza. 2011. Re-
quirements Evolution: From Assumptions to Reality. In Proceedings of the 12th
International Conference Enterprise, Business-Process and Information Systems
Modeling. 372–382.
[6]Dalal Alrajeh, Jeff Kramer, Alessandra Russo, and Sebastián Uchitel. 2009. Learn-
ing operational requirements from goal models. In Proceedings of the 31st Interna-
tional Conference on Software Engineering, ICSE 2009, May 16-24, 2009, Vancouver,
Canada, Proceedings. 265–275.
[7]Dalal Alrajeh, Jeff Kramer, Axel Van Lamsweerde, Alessandra Russo, and Se-
bastián Uchitel. 2012. Generating obstacle conditions for requirements complete-
ness. In Software Engineering (ICSE), 2012 34th International Conference on. IEEE,
705–715.
[8]Dalal Alrajeh, Axel van Lamsweerde, Jeff Kramer, Alessandra Russo, and Se-
bastián Uchitel. 2016. Risk-driven revision of requirements models. In Proceedings
of the 38th International Conference on Software Engineering. 855–865.
[9]Konstantinos Angelopoulos, Vítor E. Silva Souza, and John Mylopoulos. 2015.
Capturing Variability in Adaptation Spaces: A Three-Peaks Approach. In Pro-
ceedings of the 34th International Conference Conceptual Modeling. 384–398.
[10] Duangtida Athakravi, Dalal Alrajeh, Krysia Broda, Alessandra Russo, and Ken
Satoh. 2014. Inductive Learning Using Constraint-Driven Bias. In Inductive Logic
Programming - 24th International Conference, ILP 2014, Nancy, France, September
14-16, 2014, Revised Selected Papers. 16–32.
[11] Duangtida Athakravi, Domenico Corapi, Krysia Broda, and Alessandra Russo.
2013. Learning Through Hypothesis Refinement Using Answer Set Programming.
InInductive Logic Programming - 23rd International Conference (ILP). 31–46.
[12] Armin Biere, Alessandro Cimatti, Edmund Clarke, and Yunshan Zhu. 1999. Sym-
bolic Model Checking without BDDs. In Tools and Algorithms for the Construction
and Analysis of Systems, W. Rance Cleaveland (Ed.). Springer Berlin Heidelberg,
193–207.
[13] Jim Buckley, Tom Mens, Matthias Zenger, Awais Rashid, and Günter Kniesel.
2005. Towards a Taxonomy of Software Change. Journal of Software Maintenance
and Evolution: Research and Practice 17, 5 (2005), 309–332.
[14] Antoine Cailliau and Axel Van Lamsweerde. 2019. Runtime Monitoring and
Resolution of Probabilistic Obstacles to System Goals. ACM Trans. Auton. Adapt.
Syst. 14, 1 (2019), 3:1–3:40.
[15] Antoine Cailliau and Axel van Lamsweerde. 2014. Integrating exception handling
in goal models. In Requirements Engineering Conference (RE), 2014 IEEE 22nd
International. IEEE, 43–52.
[16] Alessandro Cimatti, Edmund M. Clarke, Enrico Giunchiglia, Fausto Giunchiglia,
Marco Pistore, Marco Roveri, Roberto Sebastiani, and Armando Tacchella. 2002.
NuSMV 2: An OpenSource Tool for Symbolic Model Checking. In Proceedings
of the 14th International Conference on Computer Aided Verification (CAV ’02).
Springer-Verlag, 359–364.
[17] Paul Clements and Linda Northrop. 2001. Software Product Lines: Practices and
Patterns. Addison-Wesley Professional.
[18] Domenico Corapi. 2011. Nonmonotonic Inductive Logic Programming as Abductive
Search. Ph.D. Dissertation. Imperial College London.
[19] Robert Darimont. 2014. GORE/KAOS in the industry: Lessons learnt. (2014).
https://refsq.org/2014/2014/files/Robert-Darimont-GORE.pdf.
[20] Neil A. Ernst, Alexander Borgida, and Ivan Jureta. 2011. Finding incremental
solutions for evolving requirements. In Proceedings of the IEEE 19th International
Requirements Engineering Conference. 15–24.
[21] Pranav Garg, Christof Löding, P. Madhusudan, and Daniel Neider. 2014. ICE: A
Robust Framework for Learning Invariants. In Proceedings of the 26th International
Conference on Computer Aided Verification. 69–87.
[22] Martin Gebser, Roland Kaminski, Benjamin Kaufmann, and Torsten Schaub. 2014.
Clingo = ASP + Control: Preliminary Report. CoRR abs/1405.3694 (2014).
[23] Martin Gebser, Benjamin Kaufmann, Roland Kaminski, Max Ostrowski, Torsten
Schaub, and Marius Schneider. 2011. Potassco: The Potsdam Answer Set Solving
Collection. AI Commun. 24, 2 (2011), 107–124.
[24] Carlo Ghezzi and Amir Molzam Sharifloo. 2013. Dealing with non-functional re-
quirements for adaptive systems via dynamic software product-lines. In Software
Engineering for Self-Adaptive Systems II. Springer, 191–213.
[25] Joel Greenyer, Daniel Gritzner, David Harel, and Assaf Marron. 2018. Towards
Automated Defect Analysis Using Execution Traces of Scenario-Based Models.
InProceedings of the 6th International Conference on Model-Driven Engineering
and Software Development MODELSWARD. 335–354.[26] Jilles Van Gurp, Jan Bosch, and Mikael Svahnberg. 2001. On the Notion of Variabil-
ity in Software Product Lines. In Proceedings of the Working IEEE/IFIP Conference
on Software Architecture (WICSA ’01). IEEE Computer Society, Washington, DC,
USA, 45–.
[27] Mike Hinchey, Sooyong Park, and Klaus Schmid. 2012. Building Dynamic Soft-
ware Product Lines. Computer 45, 10 (Oct. 2012), 22–26.
[28] Paola Inverardi and Marco Mori. 2011. Requirements models at run-time to sup-
port consistent system evolutions. In Requirements@ Run. Time (RE@ RunTime),
2011 2nd International Workshop on. IEEE, 1–8.
[29] Michael Jackson. 1995. Software Requirements and Specifications: A Lexicon of
Practice, Principles and Prejudices. ACM Press/Addison-Wesley Publishing Co.,
New York, NY, USA.
[30] Shalinka Jayatilleke and Richard Lai. 2018. A systematic review of requirements
change management. Information and Software Technology 93 (2018), 163 – 185.
[31] Ivan J. Jureta, Alexander Borgida, Neil A. Ernst, and John Mylopoulos. 2014. The
Requirements Problem for Adaptive Systems. ACM Trans. Manage. Inf. Syst. 5, 3
(2014), 17:1–17:33.
[32] Ron Koymans. 1990. Specifying Real-time Properties with Metric Temporal Logic.
Real-Time Syst. 2, 4 (1990), 255–299.
[33] Ron Koymans. 1992. Specifying message passing and time-critical systems with
temporal logic. Vol. 651. Springer Science & Business Media.
[34] Alexei Lapouchnian and John Mylopoulos. 2009. Modeling Domain Variability in
Requirements Engineering with Contexts. In Proceedings of the 28th International
Conference on Conceptual Modeling. 115–130.
[35] John W. Lloyd. 1987. Foundations of Logic Programming. Springer-Verlag.
[36] James Lockerbie, Neil Arthur Maiden, Jorgen Engmann, Debbie Randall, Sean
Jones, and David Bush. 2012. Exploring the Impact of Software Requirements
on System-wide Goals: A Method Using Satisfaction Arguments and I* Goal
Modelling. Requir. Eng. 17, 3 (Sept. 2012), 227–254.
[37] Zohar Manna and Amir Pnueli. 1992. The Temporal Logic of Reactive and Concur-
rent Systems. Springer-Verlag.
[38] Stephen H. Muggleton and Christopher H. Bryant. 2000. Theory Completion
Using Inverse Entailment. In Proceedings of 10th International Conference on
Inductive Logic Programming. LNCS, Vol. 1866. 130–146.
[39] Chi Mai Nguyen, Roberto Sebastiani, Paolo Giorgini, and John Mylopoulos.
2016. Requirements evolution and evolution requirements with constrained
goal models. In Proceedings of the 35th International Conference Conceptual Mod-
eling. Springer, 544–552.
[40] Bashar Nuseibeh and Steve Easterbrook. 2000. Requirements Engineering: A
Roadmap. In Proceedings of the Conference on The Future of Software Engineering
(ICSE ’00). ACM, New York, NY, USA, 35–46.
[41] Christophe Ponsard and Robert Darimont. 2017. Improving Requirements Engi-
neering through Goal-oriented Models and Tools: Feedback from a Large Indus-
trial Deployment. In Proceedings of the 12th International Conference on Software
Technologies, ICSOFT. 372–381.
[42] SPF Santé Publique. 2013. Manuel belge de la régulation médicale. (2013).
https://www.health.belgium.be/fr/manuel-belge-de-la-regulation-medicale
[43] RESPIRE. [n. d.]. ([n. d.]). https://www.dropbox.com/sh/wdew1sbcoqhhfe0/
AACBqCRj7epEyRX9teKYZ3bpa?dl=0.
[44] Siemens Products & Services. 2018. Low Emission Zones - Reducing air pollution
where it matters. (2018). https://new.siemens.com/global/en/products/mobility/
road-solutions/enforcement-and-tolling-solutions/low-emission-zone.html.
[45] Lightfoot Solutions, Lis Nixon Associates, and Baker Tilly. 2009. Effi-
ciency review of the Welsh Ambulance Services NHS Trust. Technical Report.
http://www.wales.nhs.uk/document/171903.
[46] Daniel Sykes, Domenico Corapi, Jeff Magee, Jeff Kramer, Alessandra Russo, and
Katsumi Inoue. 2013. Learning Revised Models for Planning in Adaptive Systems.
InProceedings of the 2013 International Conference on Software Engineering. IEEE
Press, 63–71.
[47] Amos Tversky. 1977. Features of similarity. Psychological Review 84, 4 (1977),
327—-35.
[48] Axel van Lamsweerde. 2009. Requirements Engineering: From System Goals to
UML Models to Software Specifications (1st ed.). Wiley Publishing.
[49] Pamela Zave and Michael Jackson. 1997. Four Dark Corners of Requirements
Engineering. ACM Transactions on Software Engineering and Methodology 6, 1
(Jan. 1997), 1–30.
61