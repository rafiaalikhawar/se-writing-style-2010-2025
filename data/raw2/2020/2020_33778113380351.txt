Towards the Use of the Readily Available Tests from the Release
Pipeline as Performance Tests. Are We There Yet?
Zishuo Ding, Jinfu Chen, and Weiyi Shang
Department of Computer Science and Software Engineering
Concordia University, Montreal, Canada
{zi_ding,fu_chen,shang}@encs.concordia.ca
ABSTRACT
Performance is one of the important aspects of software quality.
Performanceissuesexistwidelyinsoftwaresystems,andthepro-
cess of fixing the performance issues is an essential step in the
release cycle of software systems. Although performance testing is
widelyadoptedinpractice,itisstillexpensiveandtime-consuming.
Inparticular,theperformancetestingisusuallyconductedafterthe
systemisbuiltinadedicatedtestingenvironment.ThechallengesofperformancetestingmakeitdifficulttofitintothecommonDevOps
process in software development. On the other hand, there exist a
large number of tests readily available, that are executed regularly
within the release pipeline during software development. In this
paper,weperformanexploratorystudytodeterminewhethersuch
readily available tests are capableof serving as performance tests.
Inparticular,wewouldliketoseewhethertheperformanceofthese
testscandemonstrateperformanceimprovementsobtainedfrom
fixing real-life performance issues. We collect 127 performance
issuesfrom HadoopandCassandra,andevaluatetheperformance
of the readily available tests from the commits before and after the
performance issue fixes. We find that most of the improvements
from the fixes to performance issues can be demonstrated using
thereadilyavailabletestsinthereleasepipeline.However,onlya
verysmallportionofthetestscanbeusedfordemonstratingthe
improvements. By manually examining the tests, we identify eight
reasons that a test cannot demonstrate performance improvements
eventhoughitcoversthechangedsourcecodeoftheissuefix.Fi-
nally, we build random forest classifiers determining the important
metrics influencing the readily available tests (not) being able to
demonstrate performance improvements from issue fixes. We find
that the test code itself and the source code covered by the test are
importantfactors, whilethe factorsrelated to the code changes in
the performance issues fixes have a low importance. Practitioners
may focus on designing and improving the tests, instead of fine-
tuning tests for different performance issues fixes. Our findings
canbeusedasaguidelineforpractitionerstoreducetheamount
of effort spent on leveraging and designing tests that run in the
release pipeline for performance assurance activities.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ICSE ’20, May 23–29,2020, Seoul, Republic of Korea
© 2020 Association for Computing Machinery.
ACM ISBN 978-1-4503-7121-6/20/05...$15.00
https://doi.org/10.1145/3377811.3380351KEYWORDS
Performancetesting,Performanceissues,Software performance
ACM Reference Format:
Zishuo Ding, JinfuChen, and Weiyi Shang. 2020. Towards the Use ofthe
ReadilyAvailableTestsfromtheReleasePipelineasPerformanceTests.Are
We There Yet?. In 42nd International Conference on Software Engineering
(ICSE’20),May23–29,2020,Seoul,RepublicofKorea. ACM,NewYork,NY,
USA, 12 pages. https://doi.org/10.1145/3377811.3380351
1 INTRODUCTION
Performance is one of the most important aspects of software qual-
ity. Performance can directly affect the user experience of large-
scale systems, such as Amazon, Ebay, and Google [ 33]. A Prior
studyfindsthatfieldissuesreportedinsuchsystemsaremoreas-
sociated with the performance of the system, instead of functional
issues[50].
Performanceissuesexistwidelyinsoftwaresystems[ 26],andare
difficulttoavoidduringthesoftwaredevelopmentprocesses[ 38].
Theperformanceissueshavevariouseffectsonthesystem.Some
leadtohighresource(likeCPUormemory)utilization,andsome
cancausealongresponsetimetouserrequests.Anexampleperfor-manceissueexcerptfrom Hadoopissuetrackingsystem
1describes
thatwhen NetworkTopology callsadd()orremove(),itcalls toString()
forLOG.debug() whichrequiresextraresources.Asindicatedinthe
issue report, the toString() method is used for logging messages,
which can lead to the unnecessary slowdown of the operation and
extra resource utilization.
Performance testing is challenging. It is often an expensive and
time-consumingprocess[ 3,25].Performancetestsoftenneedtorun
with carefully designed sophisticated test plans, on top of the sup-
portofspecialsoftware(likeJMeter[ 1])andareexecutedforalong
period of time (days) [ 25]. On the other hand, such performance
teststypicallyexercisetheentiresystemasawholeinsteadofan
optimized“TargetedTherapy”.Inparticular,suchlong-runningand
un-targeted performance testing is difficult to fit into the widely
adoptedDevOpsprocess,whenreleasesarefrequentandcontain
smallerchangesbetween two releases.
Ontheotherhand,thereexistalargenumberofteststhataretyp-
ically executed regularly during every build in the release pipeline
ofsoftwaredevelopment[ 49].Forinstance,inarecentreleaseof
Cassandra, more than 500 tests are executed by default in a regular
build process during the release pipeline; while more than 4,000
tests are executed in a recent release of Hadoop2. Prior studies find
thatsuchtestsareoftencomplex,coveringvariousscenariosofthe
1https://issues.apache.org/jira/browse/HADOOP-14369
2https://github.com/apache/hadoop/releases/tag/rel/release-3.1.2
14352020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE)
ICSE ’20, May 23–29, 2020, Seoul, Republic of Korea Zishuo Ding, Jinfu Chen, and Weiyi Shang
usageofthesoftware[ 4,5,40].Moreimportantly,thesetestsare
readily available and are executed by default on a regular basis.
Due to the expensive performance testing as well as the wide
availability and maturity of tests that run in the release pipeline,
recent research has been advocating the use of such tests in per-
formance assurance activities[ 11,22,23,45]. However, there exists
littleknowledgeabouttowhatextentcanthetestsintherelease
pipeline behave as performance tests. Therefore, in this paper, we
studytheuseofthereadilyavailabletestsinthereleasepipeline
of two open-source projects, i.e., HadoopandCassandra, as per-
formancetests.Weidentify127performanceissuesthatare fixed
in the two subject systems and the snapshots of the source codebeforeandafterthefixofeachperformanceissue.Byevaluating
the performance of the tests with the snapshots of the source code,
we aim to answer the following research questions3:
RQ 1:Can the readily available tests from the release pipeline
demonstrate performance improvements from performance
issuesfixes?
Most of the performance improvements after an issue fix
can be demonstrated by at least one test. However, for each
performanceissue,onlyaverysmall(9.2%and20.6%)portion
ofthetestscandemonstratetheperformance improvements.
RQ 2:Whatarethereasonsthatsometestsinthereleasepipeline
cannotbe used as performance tests?
Weidentifyeightreasonsthatatestfromthereleasepipeline
cannot demonstrate performance improvements from a per-
formance issue fixes. The reasons can be used as a guideline
for practitionersto design micro-performance tests.
RQ 3:What are the important factors for a test to be useful as a
performance test?
Webuildclassifierstomodelwhetheratestcandemonstrate
the performance improvements of a particular performance
issue.Byexploringtheimportantfactorsinourclassifiers,
we find that the factors related to the test itself and the cov-
eredsourcecodeofthetestareimportantintheclassifiers.
Ontheotherhand,thefactorsrelatedtothecodechanges
intheperformanceissuefixeshavealowimportance.Our
results imply that practitioners may focus on designing and
selecting tests, instead of optimizing tests especially for dif-
ferentperformance issues.
Ourfindingsdemonstratethecapabilityandthechallengesof
using the readily available tests from the release pipeline in perfor-
mance assurance activities. Our paper calls for future research that
assists in designing and selecting tests that can be used in various
(e.g., functional and non-functional) scenarios for the development
of software systems.
Paper organization. The restof this paperis organized asfol-
lows: Section 2 presents the prior research that is related to thispaper. Section 3 presents our approach for collecting the perfor-mance datafrom the readily available tests and manual labelling
with test the performance metrics. Section 4 presents our three
research questions and our results to answer the three research
questions.Section5presentsthethreatstothevalidityofourstudy.
Finally, Section 6 concludes this paper.
3Thedatafromourstudyissharedathttps://github.com/senseconcordia/ICSE2020-
Performance2 RELATED WORK
Inthis section,wediscussthe priorresearchthat isrelatedtothis
paper.
Empiricalstudieson performance issues
Empiricalstudiesareconductedinordertogainadeepunder-
standing of the nature of performance issues. Jin et al. [ 26] con-
ducted an empirical study on 109 real-world performance issues
that are collected from five representative software projects. Za-man et al. [
54] study a random sample of 400 performance and
non-performanceissuesfrom MozillaFirefox andGoogleChrome.
Huang et al. [ 24] study 100 randomly selected real-world perfor-
mance regression issues from three open source systems. Based
on the study results, prior research found that that it is difficult to
reproduceperformanceissuesandmoretimeisspentondiscussing
performanceissuesthanother kindsofissues[ 54].Therefore,au-
tomated approaches are designed in order to assist in detecting
performance issues [ 26] and prioritizing performance tests [ 24]
basedonthestudyresults.Priorresearchillustratestheimportance
of addressing performance issues in practice. Our work can be
adoptedbypracticesintandemwiththepriorresearchonthetopic
of performance issues.Performanceissues detection
Priorresearchbuildspredictivemodelsinordertopredictper-
formance issues [ 30,52]. Lim et al. [ 30] formulate the performance
issueidentificationasaHiddenMarkovRandomFieldbasedcluster-
ingproblem. Xiongetal. [ 52]leverage statisticalmodelsto model
thesystemperformanceinthecloud.Luoetal.[ 31]proposearecom-
mendationsystem,calledPerfImpacttoidentifycodechangesthat
maypotentiallycauseperformanceregressions.Suchapproaches
are appliedwith anew versionof the softwarein orderto detect
performanceissue.However,suchpriorresearchonperformanceis-suemodelingdependsonalargeamountofperformancedatawith
complex modeling techniques. Such approaches, although proven
tobeeffective,aredifficulttoadoptinpractice[ 6],duetotheirextra
overheadandtherequiredresources.Moreover,suchapproaches
are often conducted at the last stage of the release. Leveraging
these approaches to detect every performance issue is difficult and
impractical. Therefore, our findings in this paper may complement
existingapproachesinordertodetectperformanceissuefixesmore
frequently during the rapid development processes.Micro-scaleperformance tests
Extensive prior research has proposed automated techniques to
design,executeandanalyzelarge-scaleperformancetesting[ 25].
Duetothecomplexityandtheresourcesneededforsuchlarge-scale
performance testing, in recent years, research has been conducted
inordertostudyanddesignperformancetestinginasmallscale
(micro-scaleperformance test).
Leitner et al. [ 29] conduct a study on 111 open-source java
projects to understand the state of art of performance testing. Sim-
ilarly, Stefan et al. [ 45] conduct a study on the practices of using
performance unit testing frameworks, including Caliper,ContiPerf,
Japex,JMH,JunitPerf.Bothstudiesshowthatmostoftheperfor-
mance tests are smoke tests and the projects often use JUnitto
testtheperformancecombinedwithfunctionaltest;whileonlyfew
open source projects use any performance unit testing framework.
1436Towards the Use of the Readily Available Tests from the Release Pipeline as Performance Tests. Are We There Yet? ICSE ’20, May 23–29, 2020, Seoul, Republic of Korea
Subject systemsCollecting
performance
issues
Identifying the
issue- ﬁxing
commit
Identifying
associated
testsTests execution
&Performance
data collection Performance-
¬-issue- ﬁxing
commitIssues reports
Labelling the
performance issue with
performance metrics
Associated
testsLabelled
performance
issuePerformance
data
Version
control
repository¬Issue
tracking
system
Commit before
performance issue
ﬁxesFor each issue
Figure 1: An overview of our case study setup and performance data collection.
Thesepriorpapersmotivateourworkinordertosupportamore
flexible and low-friction performance testing practice.
Approachesaredesignedtoimprovetheexistingmicro-performance
testing. Bulej et al. [ 11] present a statistic approach to express per-
formancerequirementsonunittesting.Inaddition,Horkýetal.[ 23]
propose an approach to use performance unit tests to increase per-
formanceawareness.
The priorresearch on micro-performance testing motivates the
needofknowingtheeffectivenessofthereadilyavailabletestsin
performance assurance scenarios. Our findings can complement
prior research in order to advance the practice of testing system
performance in a targeted manner.
3 CASE STUDY SETUP
Inthissection,wefirstpresentthesubjectsystemsofourstudyandthecollectionofperformanceissuesfromthesubjectsystems.Then
we present our approach and experiment to collect performance
dataandwealsopresenttheexperimentalenvironment.Figure1
shows an overview of these steps.
3.1 Subject systems
Webaseourstudyontwoopen-sourceprojects, HadoopandCas-
sandra.Hadoopis a distributed data processing system. Cassandra
isafreeandopen-sourcedistributedNoSQLdatabasemanagement
system. We choose HadoopandCassandra since they are highly
concernedwiththeirperformanceandhavebeenstudiedinprior
research in mining performance data [15, 46].
3.2 Collecting performance issues
We first collect the performance issues in the two subject systems.
We follow an approach similar to the one used in prior studies [ 54]
for performance issues collection. In order to ensure that thereexists a performance improvement after the issue fixes, we only
focusontheissuereportsthathavethetype Bugandarelabeled
asResolvedorFixed.
Weusekeywordsastheheuristicstoidentifyperformanceissue
reports. We start by using the keywords that are used in priorresearch [ 26,54]. In order to avoid missing performance issues, we
expandour listofkeywordsbyusing wordembedding.Weadopt
a word2vec model trained over 15GB of textual data from Stack
Overflow posts [19] to identify the words that are semantically
relatedtotheexistinglistofkeywords.Examplesoftheuncommon
words that related to performance issues include “sluggish”, and
“laggy”, which may not be used in previous research, but can help
collect performance issue reports.
By expanding the list of keywords, we gathered a total of 953
and 966 issue reports in HadoopandCassandra, respectively4.
Intuitively, not all issue reports are indeed related to performance
issues.Therefore,thefirstandlastauthorsmanuallyexamineeveryissuereportindependentlytoconfirmthattheissuereportisrelated
to a performance issue. The two authors achieve an agreement
of73.9%.Afterwards,thetwoauthorsdiscusseachdisagreement
to reach consensus. When the consensus cannot be reached, a
third author examines the issue report and makes a final decision.
Finally, we collect 88 and 121 performance-related issue reports in
HadoopandCassandra,respectively.Theamountofissuereportsis
comparableto prior study on performance issues [24, 26, 54].
3.3 Labelling performance issues with
performance metrics
Eachperformanceissuehasitscorrespondingperformancemetrics
that can be measured and used to demonstrate the symptom of the
performanceissueandtheimprovementafterfixes.Forexample,
issueHADOOP-6502,hasadescriptionof“ ...DistributedFileSys-
tem#listStatusisveryslowwhenlistingadirectorywithasizeof
1300 ...”.Basedonthedescription,weknowthattheperformance
issue can be observed by measuring elapsed time of the execution
andtheelapsedtimeshoulddecreaseaftertheissueisfixed.The
first two authors manually label all of the collected performance
issues with their correspondingperformance metrics. In total, we
identify five performance metrics in our labelling of the perfor-
mance issues in our subject systems, i.e., elapsed time, CPU usage,
4Thetimeperiod ofthedata collectionis fromthestart dateof eachprojectto theday
we collected the issues (17, September 2018).
1437ICSE ’20, May 23–29, 2020, Seoul, Republic of Korea Zishuo Ding, Jinfu Chen, and Weiyi Shang
memory usage, I/O read and I/O write. For Hadoop, 70, 19, 17, 6,
and 4 issues are labeled with elapsed time, CPU usage, memoryusage, I/O read and I/O write, respectively. 77, 32, 29, 33, and 29
issues from Cassandra are labeled with elapsed time, CPU usage,
memoryusage,I/OreadandI/Owrite,respectively.Notethatanis-
sue report can have performance issues with multiple performance
metrics. The two authors have an agreement of 89.0% on the la-belling and a similar approach as the last step is followed when
labelling disagreement occurs.
3.4 Evaluating the fixes of performance issue
Inthissubsection,wepresenthowdowestudytheuseofthereadily
availabletestsfromthereleasepipelinetoevaluateperformance.
We first identify the performance issue fixing commits, in order to
identify the two snapshots of the source code, i.e., before and after
fixingeachperformanceissue.Wethenpresenttheselectionand
execution of the associated tests that cover the issue fixing source
code. Finally, we present theperformance evaluation for each test
inordertostudywhethereachtestcandemonstrateaperformance
improvement for the performance issue fixes.
3.4.1 Identifying performance issue fixing commits. Weclonethe
gitversion controlrepositories ofour subject systems,and use git
logtoextractallthecodecommitstogetherwiththecorresponding
commit messages. The commit messages typically contain an issue
ID,indicatingtheissuethateachcommitaddresses.Withthisin-
formation, we collect all the associated commits for each collected
performance issue.
We note that there may exist multiple commits for fixing one
issue. One reason is that an issue may be too complex to fix in one
commit.Therefore,developersmaydividethefixofanissueinto
severalcommits.Inaddition,developersmighthavethoughtthat
theissueisfixed,whileactuallyisfoundnotfixed,reopened[ 51]
and fixed in a later commit. In these cases, we consider the chrono-
logicallastcommitsastheissuefixingcommits.Wealsoexclude
the commits that do not have any code changes. Finally, if an issue
IDisnotcontainedinanycommitmessage,weremovetheissue
from our study.
Afterthisstep,46issuesarefilteredout.Andthen,wecancollect
two snapshotsof source codefor eachperformance issue,i.e., one
before issue fixing, and one after issue fixing. We checkout both
snapshotsof the source code for each performance issue.
3.4.2 Executing associated tests. Both of our subject systems have
alargenumberofteststhatareavailableinthereleasepipeline.We
firstsearch forall testsbasedon theirbuildfiles. Hadoophasfour
differentsub-modules.Weselectthetestsbyeachsub-moduleto
minimizethelargeamountofirrelevantteststosavecomputational
resources. For Cassandra, we include all the retrieved tests.
Intuitively, not all tests execute the source code that is changed
by the performance issue fixes. Hence, for each performance issue,
weidentifytheteststhatexecutethesourcecodethatischanged
by the fixes (impacted tests) and the tests that do not (un-impacted
tests). We leverage code coverage tools to identify the executed
lines in the source code for each test. Different code coverage tools
areusedinthesubjectsystems.Inparticular, Cobertura andJaCoCo
are used for Cassandra. Hadoopdepends on Atlassian Clover tocalculate code coverage. Since Atlassian Clover needs licenses to
execute, and all support was discontinued at April 11, 2018, we
turn toOpenClover, which is an open-sourced version of Atlassian
Clover, to measurethe code coverage in Hadoop. If atest executes
theaddedormodifiedlinesinthesourcecodebetweentwoversions
(beforeandaftertheperformanceissuesfixes),weconsiderthetest
impacted.Inaddition,fordeletedlinesofcode, weconsideratest
covering the code if the test executes the lines before and after the
deleted lines. By doing this, we identify 127 issues that have the
impacted tests.
Afterwards, we run every test (both impacted and un-impacted)
individuallytoevaluateperformancethatisassociatedwitheach
test.Inparticular,thetestsforeachperformanceissueareexecutedononevirtualmachinewith8GBmemoryand16coresCPUhosted
by Google Compute Engine (GCE)5. Each test is independently
executedwith30repetitionstominimizenoise.Priorresearchstud-
iestheuseofcloudenvironmentonperformanceevaluationand
showsthesuccessfuluseofsuchanumberofrepetitions[ 28].Note
that we also exclude the commits and the issues where the project
fails to build and run. In total, we spent more than 11,642 machine
hoursforexecutingall thetestsforthe127performanceissuesin
our subject systems.
3.4.3 Evaluating the performance of each test. To evaluate the per-
formancethatisassociatedwitheachtest,wecollectthefiveper-
formance metrics, including the elapsed time, CPU usage, memory
usage, I/O read and I/O write, as the labelling of performance is-
sues. We use psutil(python system and process utilities) [ 41] for
monitoring the CPU usage, memory usage, I/O read, and I/O write
oftheprocessthatexecutesthetests. Psutilhasbeenusedwidely
in prior research on software performance [ 13,53]. We use test
summaryreportsgeneratedvia Ant/Maven andJunittomeasure
the elapsed time of each test. After this step, we have collected
performance data for all the tests (both impacted and un-impacted)
that are associated with two versions of source code (before and
after each performance issue fix) of each performance issue. We
thenuse thisdatato answer our research questions.
4 CASE STUDY RESULTS
In this section, we aim to answer the following research questions:
RQ1: Can the readily available tests from the
release pipeline demonstrate performance
improvements from performance issues fixes?
Motivation. Performance issue reports are often used as a great
source of knowledge in system performance assurance activities
in prior research [ 24,26]. The certainty of having performance im-
provements,thedescriptionofthereportsandtheavailablepatches
make performance issues a great subject for prior research on soft-
ware performance. This research question concerns whether the
performance of the readily available tests from the release pipeline
can demonstrate performance improvements from performance
issuefixes.Ifnot,thereadilyavailabletestswouldnotbecapable
ofservingasperformancetestsforotherperformanceassurance
activitieswitheven higher difficulty.
5https://cloud.google.com/compute/
1438Towards the Use of the Readily Available Tests from the Release Pipeline as Performance Tests. Are We There Yet? ICSE ’20, May 23–29, 2020, Seoul, Republic of Korea
Approach. Analyzingperformanceevaluationresults. Foreach
test, we leverage statistical tests on the performance evaluation re-
sultstodeterminewhethertheperformanceofthetesthaschanged
after fixing the performance issue. In particular, for each perfor-
mance issue, we first select only the tests that are impacted by
the performance issue fixes. Afterwards, we check the label of the
performancemetrics(e.g.,elapsedtime)(seeSection3.3)thatare
associatedwiththesymptomsoftheperformanceissues.Wewould
like to determine whether the corresponding performance metrics
have different statistical significance values before and after the
performance issues fixes.
Duetothenon-normalityoftheperformancedata,weuse Mann-
WhitneyUtest,asdoespriorwork[ 14,55].Ournullhypothesis and
alternative hypothesis are given below,
H0:The two performance result (i.e., test and control group –
thesametestbeforeandafterperformanceissuefixes)are
equal.
H1:The two compared tests do not have the same perfor-
mance.
and we run the test at the 5% level of significance (i.e., α=0.05).
That is,if the P-valueof thetest is notgreater than 0.05(i.e., P−
value≤0.05), we would reject the null hypothesis in favour of the
alternative hypothesis. In other words, there exists a statistically
significant performance change between the performance metrics,
and thechangeis unlikely by chance.
However,astatisticalsignificancetestdoesnotcontainthein-
formation about the size of the effect [17], and when the perfor-
mance data points under study are formed by a great number of
items,thestatisticallysignificantdifferencesaremorefrequently
observed[ 12,28].Therefore,wefurtheradopttheeffectsizeasa
complementofthestatisticalsignificancetest.Consideringthenon-
normalityofourdatapoints,weutilize Cliff’sDelta [16],whichdoes
not require any assumptions about the shape or spread of the two
distributions[ 28].Theeffectsizeisassessedusingthethresholds
provided in prior research [42],
Filteringfalse-positiveresults. ToavoidtheFalsePositives,and
eliminate the influence of the negligible or small changes of the
performance, we only consider the performance changes that have
alargeeffectsize.Inshort,iftheperformancemetricofanimpacted
testischanged,inparticularimproved(e.g.,lowerCPUusage),after
the performance issue fixes, with statistically significant difference
and large effect size, and the performance metric is also labelled
for the performance issue, we consider the test to be capable of
verifying the performance issues fixes.
In order to further avoid false positive results, we would like
to understand the patterns of false-positive results and use such
patternstofilteroutourdata.Inordertoidentifythemostobvious
false-positives,wecheckthelargesttenperformancechanges(in
effectsizes,c.f.,Section4)intheun-impactedtests(nomodification
committed on the source code covered by the tests) in each subject
system.Wemanualstudyonthepossiblecausesofthefalsepositivechangesthatresideinthesourcecode.Wefindtworeasons:1)some
functionaltestscontainrandomoperations, whichcan lead tothe
unstable performance and 2) frequent I/O operations. Therefore,
we do not consider the results of a test if the test is corresponding
to either of these two reasons.Finally, we manually examine all the cases of each performance
issue (c.f.,Section 4)to ensurethat thetests indeeddemonstrate a
performance improvement after a performance issue fix.
Results.Mostperformancefixes’improvementscanbedemon-
strated by at least one readily available test. We find that for
56outof60oftheperformanceissuesin Hadoopand46outof67
performance issues in Cassandra, at least one test from the release
pipeline can be used to demonstrate performance improvements
withalltheirassociatedperformancemetrics.Inaddition,forseven
additional performance issues in Cassandra, performance improve-
ments withpart ofthe performance metrics canbedemonstrated.
Forexample,thecommit #9afc209fixestheissue CASSANDRA-7401,
whichdescribesanendlessloopinthesourcecode.Basedonthe
report, there should be improvements on both elapsed time and
CPUusagefromtheissuefix.Amongalltheimpactedtests,elapsed
timeandCPUusageareindeedimprovedsignificantlywithlarge
effect size in three tests. Such results show the potential capability
ofthereadilyavailabletestsfromthereleasepipelinetoserveas
performance tests.
Onlyasmallportionofthetestsfromthereleasepipeline
canbeusedtodemonstrateperformanceimprovements. Fig-
ure2showsthepercentageofteststhatcanorcannotbeusedto
demonstrate the improvements from performance issues fixes. The
resultsshowthatitwouldbechallengingforpractitionertodirectly
use the readily available test in the release pipeline as performance
tests. In particular, on average, only 9.2% and 20.6% of the tests
inCassandra andHadoop, respectively, can demonstrate perfor-
mance improvements for all associated performance metrics. 13.9%
and 5.1% of the tests in Cassandra andHadoop, respectively, can
demonstrateperformanceimprovementswithpartoftheassociated
performance metrics. On the other hand, 76.9% and 74.3% of thetestsinCassandra andHadoop,respectively,cannotdemonstrate
any performance improvement, even though these tests all executethechangedsourcecodefortheissuefixes.Forexample,tofixissueCASSANDRA-3344,25testsareimpactedbythecodechange;while
onlytwotestscandemonstratetheperformanceimprovementfrom
the issue fix. Due to the large number of total available tests in the
releasepipeline,practitionersmaybeoverwhelmedbytheinflux
of performance results from the tests in the release pipeline and
thedifficultyof selecting the useful ones.

 
Ononehand,mostofperformanceimprovementsfromperfor-
manceissuefixescanbedemonstratedusingthereadilyavailable
tests in the release pipeline. On the other hand, it is challenging
to use these tests in practice since only a very small portion of
thetestscan demonstratetheimprovements.
RQ2: What are the reasons that some tests in the
release pipeline cannot be used as performance
tests?
Motivation. In the last research question, we find that many of the
readily available tests in the release pipeline cannot demonstrate a
performance improvement from the performance issue fixes, even
thoughthechangedsourcecodefortheissuefixesisexecutedby
thesetests.Therefore,inthisresearchquestion,wewouldliketo
understandthereasonthatthesetestscannotserveasperformance
1439ICSE ’20, May 23–29, 2020, Seoul, Republic of Korea Zishuo Ding, Jinfu Chen, and Weiyi Shang
Figure 2: The percentage of tests that can or cannot be used
todemonstrateperformanceimprovementsfromissuefixes
foreach issue.
tests.Thefindingsofthisresearchquestioncanassistpractition-
ers in avoiding the use of certain tests in performance assurance
activitiesand in improving tests to serve as performance tests.
Approach. We follow a four-step open coding approach to analyze
the reasons that can cause a test to not be able to demonstrate
performanceimprovements,eventhoughthetestisimpactedby
the issuefix.
Based on the results from RQ1, we collect all the impacted tests
for the performance issues, i.e., the tests that cover the changed
source code of the corresponding issue fix, but do not demonstrate
performanceimprovementsontheperformancemetricsoftheissue.
Two authors independently examine each test to uncover reasons
of not being able to demonstrate performance improvements. In
particular,theauthorsexaminethefollowinginformationthatis
associatedwitheachtest:1)theperformanceissuereport,which
containsthehigh-levelinformationfortheissues’description,2)
the test code, which contains the low-level information of the tests
andthechangedpartsofthecommittedfilesand3)thesourcecode
covered by the test, which tells us which lines have been executed
by the tests.
Step 1.The first two authors independently generate categories
of reasons that a test cannot demonstrate performance improve-
ments.Inparticular,eachauthoriterativelyinvestigatesallthetests
to identify the reasons, until no more new reasons can be found.
Theoutcomeofthefirststepisthedifferentcategoryofreasonsby
eachof the two authors.
Step2.Intuitively,thetwoauthorswouldnotgenerateidentical
categories.Hence,thetwoauthorsmeetanddiscusstheircategories.
The goal is to generate final categories of reasons that both of
the two authors agree on. The two authors discuss each of their
generatedcategoriesofreasonsandreachconsensusonthefinal
categories.
Step 3.The two authors use the agreed categories from the
second step. The two authors independently put each test into one
category.
Step 4.Finally,thetwoauthorsexaminetheresultswherethe
twoauthorsdonotagree.Thetwoauthorsdiscusstheirrationaletotrytoreachconsensus.Ifconsensuscannotbemade,thethirdau-
thorwillexaminethecorrespondingtesttomakethefinaldecision.
The two authors have an agreement of 71.1%.
Results.Weidentifyeightpossiblereasonsthatatestcannot
be used to demonstrate performanceimprovements. We dis-
cuss each reason in detail with examples in the rest of this RQ.
Too light workload (185 tests). We find that some perfor-
mance issues can only be triggered with a rather large data size.However, functional tests may not be written with such a large
data size as input, making it impossible to demonstrate the issue
fixes.Forexample,theissuereportedin CASSANDRA-581,canbe
triggered with a very large number of sstables.I ti sfi x e di nt h e
commit#2b62df2. However, the impacted tests do not have a large
enoughamount of sstablesasinput toreproduce theperformance
issue.
Not enough repetition (9 tests). Some performance issues
havearathersmalleffect,whilebecomingimpactfulwithalarge
number of repetitions. For such performance issues, the tests of-ten can detect the performance improvements but only with asmallormediumeffect size, which are not considered in our ex-
periments to minimize noise. However, with more repetitions, the
effectcanincrease.Forexample,inthereportofperformanceissue
CASSANDRA-581, developers mention that the method convert-
FromDiskFormat usingsplitis slow only when being tested with
morethan1,000keys.Althoughatest RandomPartitionerTest covers
the code changed by the issue fix, the method convertFromDiskFor-
matiscalledonlyonceinthetestandtheelapsedtimeisslightly
improvedwithasmalleffectsize.Basedonthedescriptionofthe
issuereport,ifthereweremorerepetitionsaroundthismethod,the
performance improvement would be demonstrated by the test.
Race conditions (2 tests). The race condition related perfor-
mance issuescan onlyhappen when given a certain set of circum-
stances. For example, the commit #6158c64fixed the deadlock issue
in the streaming code. With the description provided in the re-
port,CASSANDRA-5699, we find that we need a specific execution
conditionto trigger the deadlock.
Limited line coverage of the performance related codes
(24tests). Wenoticethatdevelopersmaychangealargeamount
ofsourcecodetofixperformanceissues,butthetestonlycovers
a small portion of the committed changes. In this situation, the
performance of the test can be misleading since it does not tell the
full picture of the issue fixes. For example, the commit #67ccdab
fixedaperformanceissueinthestreamingcode.Byusingthe gitdiff
command,weknowthatthereare10fileschangedwith437line
additionsand243linedeletions.However,amongthesechanges,
onlyonelineiscoveredbythetest SessionInfoTest.Moreover,the
covered lineisa refactoringoperation (RenameVariable), andthe
performance sensitive operations are never performed by the tests
to demonstrate the performance improvement.
Partial branch coverage (34 tests) . If the performance issue
iscausedbythecodeinsidethe ifstatement,andwithoutthe100%
coverage of the conditions, the code snippets cannot be tested, and
thus, the tests cannot demonstrate the fix to the performance issue.
A representative example can be found in the fixing process of
issueCASSANDRA-3234.Theperformanceissueiscausedbythe
1440Towards the Use of the Readily Available Tests from the Release Pipeline as Performance Tests. Are We There Yet? ICSE ’20, May 23–29, 2020, Seoul, Republic of Korea
echoedRow function, while this function cannot be invoked as it
lies insidethe ifstatementwithouta 100% branch coverage.
Indirectperformanceinfluence(1test). Inthissituation,the
behaviorofperformanceissuerelatedcodeisbasedonthereturn
value of another function. Therefore, covering the fix locations
of the issue may not be useful to demonstrate the fix to the per-
formance issue. For example, in the fixing process of the issue
CASSANDRA-8550,whilebenchmarking CQL3secondaryindexes,
developers noticed substantial performance degradation as the vol-
ume of indexed data increases. The issue is caused by the page size
selection,whichisreturnedbyanotherfunction.Wenoticethatthe
testscancovertheuseofthereturnvaluewhilemissingitscaller.
Therefore, the tests cannot demonstrate the performance changes
as expected.
Frequent access of external resources (31 tests). Frequent
accessoperationsofexternalresourcesmayintroducenoiseintothe
performance evaluation of the tests. We find tests that may have 1)
frequentI/Ooperations,includingtables’creation,deletion,update
anddatainsertionandselection,or2)frequentmemoryoperations,
liketheflushoperations.Forexample,test DefsTestcoversthefix
in commit #3ad3e73 for theissue CASSANDRA-3234. However, the
test cannot demonstrate the improvement due to the noise from its
largenumber of flush operations.
Idle during execution (6 tests). Some tests may proactively
wait for a period of time, introducing an idle time that is much
longer than the actual execution time, which reduces the observed
performanceimprovementafterissuefixes.Forexample,inthecom-
mit#3ad3e73 that fixes issue CASSANDRA-3234, test CleanupTest
contains a 10-second Thread.sleep operation with a total 11.685s
elapsed test time. In this case, the elapsed time is dominated by the
sleep time, hiding the performance improvement after the issue
fixes.
 
Weidentifyeightpossiblereasonsthatatestinareleasepipeline
cannot serve as a performance test. The reasons can be used as a
guidelineforpractitionerstoavoidandimprovetheuseofcertain
testsfrom the release pipeline.
RQ3: What are theimportant factors for a test to
be useful as a performance test?
Motivation. Priorresearchhasstudiedtheuseofmicro-scaleper-
formancetestsinperformanceevaluation[ 11,22,23,45].However,
thefindingsinourpriorresearchquestionsillustratethechallenges
and show the reasons why we cannot directly adopt those tests in
performanceevaluation.Onthe otherhand,thereexisttestsfrom
the release pipeline that successfully demonstrate performance
improvements.Byunderstandingthecharacteristicsofteststhat
areabletodemonstrateperformanceimprovements,wemaygain
a better understanding of these tests and thus can provide more
generalguidancetoadeveloperforwritingnewteststhatrunin
the release pipeline for performance assurance activities.
Approach. To answer this research question, we adopt random
forest, an ensemble learning method [ 8], as it is one of the most
used machine learning algorithms for its performance and has
been adopted in various software engineering research [ 48]. We
build a binary classifier to identify whether a test can be used to
demonstrateperformance improvements.Step 1: Raw data collection. In RQ1, we have identified the
impacted tests of each performance issue, and whether the test
can demonstrate performance improvements. However, the ability
of a test to serve as a performance test may vary among differ-
entperformancemetrics.Forexample,atestthatcansuccessfully
demonstrate memory usage improvement may not be able to show
the improvement with elapsed time. Therefore, in this step, we
separate the data based on each performance metric, i.e., we build
one classifier for each performance metric. For example, to collect
the raw data of elapsed time for project Cassandra, we first only
take all the performance issues that are manually labelled withelapsed time. Then we collect the impacted tests of each perfor-
manceissue.Foreachimpactedtest,weusetheresultsshownin
RQ1 to determine whether the test can demonstrate a performance
improvement. The results in RQ1 are considered the ground truthdatafor our classifier.
Step2:Metricsextraction.
Tobuildclassifiers,weextractmet-
ricsfortherawdatacollectedfromthepreviousstep.Theeffective-
ness of a test can be associated with many metrics. In this work,
we extract metrics from three aspects of the tests:
•test code, which contains the information about the test
itself.
•source code covered by the test, where we canfind the test
coverage rate and the characteristics of covered source code.
•sourcecodeimpactedbytheissuefix,whichmeasuresthe
characteristics of committed changes of the source code
whilefixingtheperformance issue.
Theintuitionbehindtheselectionofthethreeaspectsisstraight-
forward, as we are running the test to evaluate the performance of
the covered source code and the performance improvements from
issuesfixesshould be caused by the committed changes.
Inspired by the work on defect prediction [ 27,34,36,37], and
theprior findingson performanceissues andperformance regres-
sions[2,13,18,24,26,44],weextractmetricsfromeachofthethree
aspects. Some metrics exists in multiple aspects. The details of the
metricsare shown in Table 1.
Step 3: Training and testing random forest classifiers. In
this step, we build random forest classifiers to model whether a
test can demonstrate the performance improvements or not. In
particular,webuildfiveclassifiers,eachpredictingforoneperfor-
mance metric (i.e., elapsed time, CPU usage, memory usage, I/O
read and I/O write). For each classifier, we use a 10 ×10-fold cross-
validationimplementationin scikit-learn6withrandomshuffle[ 39].
Wefitaclassifier onthetrainingdata,andusethevalidation data
totesttheclassifier.Forourbinaryclassificationproblem,weuse
the area under the receiver operating characteristic (ROC) curve
(AUC) as a performance measurement [ 7]. AUC ranges in value
from0to1,showingthecapacityoftheclassifierondistinguishing
between classes. A higher AUC means a better classifier at pre-
dicting.Finally,wehave10 ×10modelsandcorrespondingAUC
values.Inthisstudy,weusetherandomforestimplementation7
6https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.
StratifiedKFold.html
7https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.
RandomForestClassifier.html
1441ICSE ’20, May 23–29, 2020, Seoul, Republic of Korea Zishuo Ding, Jinfu Chen, and Weiyi Shang
Table 1: An overview of our extracted metrics to build random forest classifiers.
TSF Category Metrics Level Description
•••
Complexity and sizeFanOut Method Number of unique methods that are called by the code snippet.
••• FanIn Method Number of unique methods that call the methods of the code snippet.
••• CyclomaticComplexity Method McCabe Cyclomatic complexity of the code snippet.
• SLOC File Number of source code lines in the code snippet.
• CodeElementsSize Method Code elements divided by Size.
•Diffusion Entropy Commit Distribution of modified code across files in one commit.
•••
HistoryDeveloperCount Commit Number of developers that changed the code snippet.
••• TimeInterval File Average time interval between the last and the current change of the code snippet.
••
Human factorDeveloperCommitCount File Average number of commits of the developers who modified the code snippet.
•• RecentDeveloperCommitCount File Average number ofcommits made inlast 12 months ofthe developers who modifiedthe
code snippet.
•••
Code elementsCondition Method Number of condition statements of the code snippet.
••• Loop Method Number of loop statements of the code snippet.
••• ExceptionHandling Method Number of try-catch statements of the code snippet.
••• Synchronization Method Number of synchronization statements of the code snippet.
••• FinalStatic Method Number of final or static statements of the code snippet.
••• ExpensiveVariableParameter Method Number of expensive parameters/variables of the code snippet.
••• ExternalCall Method Number of external function call of the code snippet.
••• Control Method Number of control statements of the code snippet.
••
Code changeCodeChurn File Total sum of lines added into and deleted from the code snippet across all the commit
history.
•• LineAdded File Total sum of lines added into the the code snippet across all the commit history.
•• LineDeleted File Total sum of lines deleted from the code snippet across all the commit history.
•
Coverage criteriaLineCoverage File Line coverage ratio of the test.
• BrahchCoverage File Branchcoverage ratio of the test.
Note: T, S and F in the heading are abbreviations for the three aspect of metrics: test code, source code covered by the test and source code impacted by the issue fix. •means that
the metricis calculated for the corresponding aspect.
androc_auc_score8functionin scikit-learn [39]totrainandeval-
uate our classifiers. Note that for I/O read of project Hadoop,w e
onlyhave13and 345functional teststhat canand cannotdemon-
strate performance improvements. The dataset is small for training
aclassifier,resultinginthemisleadingconclusions.Therefore,we
do not train our classifier for I/O read with Hadoop.
Step4:Determiningimportanceofeachgroupofmetrics.
Inthisstep,weexaminetheimportanceofeachgroupofmetrics.
Inparticular,weextractthreegroupsofmetrics,i.e.,fiximpacted
source code, test code, and test covered source code. We removeeach group of metrics from our data and rebuild the classifiers.
Afterwards, we measure the AUC values of each classifier and
compare with the AUC values of the original classifiers with all
metrics. The more the AUC values decrease, the more important
the group of metrics are.
Step 5: Determining the importance of each metric. To
evaluate the importance of each metric on our random forest clas-
sifiers, we adopt the Mean Decrease Impurity (MDI) (also called
Gini importance) [ 9,10]. In a tree algorithm, it calculates each
metric’simportanceasthesumoverthenumberofsplitsthatin-
clude themetric, proportionally to the number of samples it splits.
For our random forest, the importance is averaged over all trees
oftheensemble.Weusethefunction feature_importances_ ofthe
scikit-learn9[39] in Python to compute the metrics importance
values.
Afterwerepeatthe10-foldcross-validationfor10times,each
metric has 100 importance scores. We then perform Scott-KnottEffect Size Difference (ESD) test [
43] on the metrics importance.
8https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.
html
9https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.
RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.feature_
importances_The Scott-Knott ESD test uses hierarchical clustering analysis to
partitiondifferent metricsinto distinctgroups. Withthisanalysis,
each metric has a rank. In this study, we use the sk_esdfunction of
theScottKnottESDpackage10in R [47].
Finally, to examine the direction of the relationship between
each metric and the likelihood of a test being successful on demon-
stratingperformanceimprovements,wemeasurethecorrelation
betweeneachmetricandthetargets/classesusingaSpearmanrank
correlation( rho).ApositiveSpearmanrankcorrelationindicates
thatthemetricsharesapositiverelationshipwiththelikelihoodofa
testbeingsuccessfulondemonstratingperformanceimprovements,
whereas a negative correlation indicates an inverse relationship.
Results.Ourrandomforest classifiersachieve high AUC val-
ues,considerablyoutperformingarandomclassifier. Forproject
Cassandra,Table2showsthat,ourrandomforestclassifiersachieve
anaverageAUCof0.86,0.59,0.69,0.72,and0.73forelapsedtime,
CPU usage, memory usage, I/O read and I/O write, respectively.Similarly, for project Hadoop, our classifiers achieve an average
AUCof 0.90,0.68, 0.66,0.79 forelapsedtime, CPUusage, memory
usage,andI/Owrite,respectively.Theseresultsindicatethatourrandomforestclassifiersoutperformrandomclassifierswhende-
terminewhetheratestcanbeusedfordemonstratingperformance
improvements. By analyzing the results, we find that the higher
AUC value of elapsed time than the CPU usage, Memory usage,
I/O read and I/O write classifiers may be due to the larger number
of available tests that can be used to demonstrate improvements
in elapsed time over other performance metrics. In addition, we
findthattheAUCvaluesofalltheclassifiersarestable,especially
the models from the elapsed time. The stable AUC values of our
classifierssuggestthatourclassifiersachievestableperformancein
10https://github.com/klainfo/ScottKnottESD
1442Towards the Use of the Readily Available Tests from the Release Pipeline as Performance Tests. Are We There Yet? ICSE ’20, May 23–29, 2020, Seoul, Republic of Korea
Table 2: An average of AUC, and AUC changes after removing some metrics. −,+, and 0 means there is a decrease, increase
andno change of AUC.
All
MetricsMetricswithout
source code
impacted by the
issuefixMetricswithout
test codeMetricswithout
source code
covered by the
testCassandraAUC AUC Change AUC Change AUC Change
Elapsed time 0.86 0.79 -0.07 0.85 -0.01 0.8 -0.06
CPU usage 0.59 0.58 -0.01 0.57 -0.02 0.56 -0.03
Memory usage 0.69 0.67 -0.02 0.67 -0.02 0.64 -0.05
I/O read 0.72 0.68 -0.04 0.68 -0.04 0.68 -0.04
I/O write 0.73 0.73 0 0.67 -0.06 0.7 -0.03HadoopElapsed time 0.90 0.90 0 0.87 -0.03 0.86 -0.04
CPU usage 0.68 0.68 0 0.59 -0.09 0.68 0
Memory usage 0.66 0.66 0 0.61 -0.05 0.67 0.01
I/O write 0.79 0.79 0 0.74 -0.05 0.8 0.01
determining the effectiveness of using these readily available tests
in the release pipeline in performance assurance activities.
The metrics extracted from the source code covered by the
testplayanimportantroleintheusefulnessofatest. Table2
showsthatfor Cassandra,themetricsfromthesourcecodecovered
by the tests always have a strong influence on the AUC values
among the classifiers for all performance metrics. Table 3 presents
thetopthreemostimportantmetricstotheclassifiers.Tohavea
betterunderstandingofthesemetrics,wealsopresenttheirmetrics
importancemeasuredusingMDI,thedirection(i.e.,thesignof ρ)of
the relationship between these metrics and the likelihood of a test
being successful on demonstrating performance improvements. By
examiningTable3,wefindthatfor Cassandra,themetricsfromthe
source code covered by the test always have the largest MDI for all
classifiers. The LineCoverage andBranchCoverage metrics lie in the
toptworanksacrossalltheclassifiers.Theresultsalsoshowthat
thesetwometricshaveapositiveimpactontheunitusage,I/Oread,
and I/O writeperformance metrics. It indicates that a test tends to
successfully demonstrate a performance improvement from a per-
formance issue fix, if the test has a relatively higher line or branch
coverage. These findings confirm the results in our preliminarymanual study in RQ2, i.e., the tests with a lower line or branchcoverage have difficulty triggering the performance issues, thus
cannotdemonstratetheimprovementsfromtheperformanceissues
fixes. Thisfinding suggests the importanceof coverage criteriain
developing performance tests.
The metrics of the test itself play an important role in the
usefulness of a test. Shown in Table 2, for Hadoop, the metrics
related to the test code have a large influence on all the classifiers.
Byexaminingthetopthreemostimportantmetricstotheclassifiers
(see Table 3), the SizeandTimeInterval metrics from test code and
are also important on whether a test can demonstrating perfor-mance improvements. For project Cassandra, Table 3 shows that
SLOCmetricofthetestcoderanksfirstintheI/Owriteclassifier.
ThisSLOCmetric is also one of the top three important metrics in
theelapsedtime,CPUusage,memoryusage,andI/Oreadclassifiers.
TheSLOCmetric has a positive impact in all the five performance
metrics.It indicatesthat atest tendstosuccessfully demonstrate
performanceimprov ements,ifithasarelativelyhighersourcelines
of code. Meanwhile, for project Hadoop, the metric TimeIntervalalso lie in the top three most important metrics. The negative sign
indicatesthatif atest codeis updatedlong timeago, itmay result
in a low likelihood demonstrating the performance improvements.
Finally,for Hadoop,theimportanceofthemetric RelativeExpensive-
VariableParameter, from test code, indicates that a readily available
test tends to successfully demonstrate the performance improve-
mentsfromperformanceissues,especiallymemoryissues,ifithas
a relatively higher call of expensive variables in the test.
Themetricsofthechangedsourcecodebyaperformanceis-
suefixdonotoftenplayanimportantroleintheusefulnessof a test.
We find that for Hadoopthe average AUC our random
forest classifiers do not change when the metrics extracted from
the source code impacted by the issue fix category (see Table 2). In
addition, none of the metrics that are related to the source codeimpacted by the issue fix lies in the top three important metrics
oftheclassifiers.Thesefindingssuggestthatdevelopersmaypay
more attention to the test code and the source code covered bythetest.Somepractitionersmayliketofinetunethetestsforev-
eryperformance issuefix. However, ourresults suggestthat such
fine-tuning may not be cost-effective since the characteristics from
the changed source code of a performance issue do not typicallyplay an important role in whether the test can demonstrate the
performance improvements from performance issue fixes.

 
Metricsrelatedtothetestitselfandthesourcecodecoveredby
thetestareimportantintheclassifiers.Ontheotherhand,the
metricsrelatedtothecodechangesintheperformanceissuesfixes
havealowimportance.Practitionersshouldfocusondesigning
and improving the tests, instead of optimizing tests for different
performance issue fixes.
5 THREATS TO VALIDITY
Thissection discusses the threats to the validity of our study.
Externalvalidity. Duetothelargeamountoftimeandcomputing
resources for execution to identify performance improvements and
thecodecoverageoftests,ourevaluationisconductedontwoopen-
source software systems, i.e., HadoopandCassandra. Although our
studyonlyfocuseson127performanceissues,thescaleofourstudy
iscomparabletopriorresearchonperformanceissues[ 26,54].Our
findingsmightnotbegeneralizabletoothersystems.Futurestudies
1443ICSE ’20, May 23–29, 2020, Seoul, Republic of Korea Zishuo Ding, Jinfu Chen, and Weiyi Shang
Table 3: Average rank of the top three influential metrics
and the Spearman rank correlation ( ρ). Note: A +(or−)
sign ofρindicates a positive (or an inverse) relationship of
the metric with the likelihood that a functional being able
to demonstrate the performance improvements. The largerMDIthata metric has, the more influentialthemetricis.
Cassandra
Rank Aspect::Metrics MDI±SD ρ
Elapsed time
1 S::LineCoverage 0.068±0.001 +
2 S::BrahchCoverage 0.068±0.001 +
3 T::RelativeExceptionHandling 0.044±0.001 +
CPU usage
1 S::LineCoverage 0.059±0.002 +
2 S::BrahchCoverage 0.059±0.002 +
3 T::TimeInterval 0.046±0.002 +
Memory
1 S::BrahchCoverage 0.051±0.001 −
2 S::LineCoverage 0.051±0.001 −
3 T::TimeInterval 0.045±0.001 −
I/O read
1 S::BranchCoverage 0.060±0.001 +
2 S::LineCoverage 0.059±0.001 +
3 T::TimeInterval 0.048±0.001 +
I/O write
1 S::BrahchCoverage 0.049±0.002 +
S::LineCoverage 0.049±0.002 +
T::SLOC 0.049±0.002 +
2 T::RelativeExpensiveVariableParameter 0.040±0.001 −
3 T::TimeInterval 0.038±0.001 +
Hadoop
Rank Category::Metrics MDI±SD ρ
Elapsed time
1 S::LineAdded 0.038±0.001 +
2 S::TimeInterval 0.037±0.001 −
3 S::LineDeleted 0.033±0.001 +
CPU usage
1 T::TimeInterval 0.036±0.001 −
2 T::RelativeExceptionHandling 0.033±0.001 +
3 T:RelativeExpensiveVariableParameter 0.032±0.001 +
Memory
1 T::RelativeExpensiveVariableParameter 0.035±0.001 +
2 S::TimeInterval 0.034±0.001 −
3 T::TimeInterval 0.033±0.001 −
I/O write
1 S::LineAdded 0.040±0.002 +
2 T::TimeInterval 0.030±0.001 −
3 T::RelativeExpensiveVariableParameter 0.030±0.001 +
Note: T and S in the aspects are abbreviations for the two aspect of metrics:
testcode and source code covered by the test.
can apply our approach on other systems, such as commercial
closed source systems.Internalvalidity.
Our issue report selection in the JIRAtracking
system may be biased by the keyword definition. Although we use
a manual identification process to verify whether the filtered issue
reports are related to performance, we may still miss performance
issuethatdonotcontainanyofourlistedkeywords.Ourapproach
requires performance metrics to measure performance of available
tests.Inparticular,weonlystudyfiveperformancemetricswhile
there may be others if other people study and label other perfor-
mance issues. Future studies can include more performance issues
and metricsto complement the findings of our study. The manuallabellingandmanualstudyresultsmaybesubjectivetothetwoau-
thors. More user studies and surveys on practitioners may address
thisthreat.
We use software metrics based on the findings from prior re-
searchandalsoextractnewmetricshighlyrelatedtotest.Wechoose
ourpredictionmodel(RandomForest),basedonitswidespreaduse
inpriorsoftwareengineeringresearch[ 20],andsinceittypically
providesahighaccuracyinthemodeling.Theremayexistother
metrics and machine learning models that can be leveraged in our
study,wherefutureresearchcanexploretocomplementourfind-
ings.Construct validity.
Thereexistotherperformanceassuranceac-
tivities,suchaperformanceregressiondetection[ 21,32].Ourstudy
choosestouseperformanceissuesbecauseoftheknowledgeand
quality of issue reports and the certainty in performance improve-
ments. Future research can complement our study by using readily
availabletests in other performance assuranceactivities as perfor-
mancetests.
Therealwaysexistsnoisewhenmonitoringperformance[ 35].
In order to minimize such noise, for each readily available test, we
repeattheexecution30timesindependently.Thenweuseastatisti-
cally rigorous approach to measuring performance improvements.
Further studies may opt to increase the number of repeated execu-
tionstofurtherminimizethethreatbasedontheirtimeandresource
budget.Ourapproachisbasedonthesystemperformancethatis
recorded by Psutil[14]. Further studies may evaluate our approach
by varying such performance monitoring tools, i.e., pidStat.
In our context, we evaluate the performance of tests in a Google
Cloud Platform performance evaluation environment. Although
weminimizethenoiseintheenvironmenttoavoidbias,suchan
environment is not exactly the same as in-field environment of the
users.Tominimizethethreat,weonlyconsidertheperformance
improvements that have large effect size. In addition, with the
advancing of DevOps, more operational data will become available
for future mining software repository research. Research based on
field datafrom the real users can address this threat.
6 CONCLUSION
In this paper, we evaluate the performance of readily available
tests in the release pipeline, and then examine whether these tests
can be used as performance tests, in particular, to demonstrate
the performance improvements from performance issues fixes. By
performing an exploratory study on a total of 127 performance
issues in two open-source projects, i.e., HadoopandCassandra,w e
find that most of improvements from performance issues can be
demonstratedusingthereadilyavailabletestsinthereleasepipeline.
Moreover, through a manualstudy,we identify eight reasons that
may lead a test not being able to demonstrate the performance
improvements.Finally,webuildrandomforestclassifierstoidentify
themostimportantmetricsthatinfluencethetests’capabilityon
demonstratingperformance improvements.
To summarize, this paper makes the following contributions:
•Tothebest ofourknowledge,our workisthefirst tostudy
theuseofreadilyavailabletestsinperformanceassurance
activities.
1444Towards the Use of the Readily Available Tests from the Release Pipeline as Performance Tests. Are We There Yet? ICSE ’20, May 23–29, 2020, Seoul, Republic of Korea
•Weuncovereightreasonswhyareadilyavailabletestcannot
be used as a performance test.
•We find that a test itself and the source code covered by the
test are the important factors for tests to be able to serve as
performance tests.
Our findings shed light on the opportunities and challenges in
leveraging the readily available tests in performance assurance
activities. Practitioners can use our uncovered reasons and factors
as guidelines to design and improve tests that run in the release
pipeline forperformance assurance activities.
REFERENCES
[1][n.d.]. ApacheJMeter-ApacheJMeter. https://jmeter.apache.org/. (Accessed
on 03/29/2019).
[2]Mohammad Mejbah Ul Alam, Tongping Liu, Guangming Zeng, and Abdul-
lah Muzahid. 2017. SyncPerf: Categorizing, Detecting, and Diagnosing Syn-
chronization Performance Bugs. In Proceedings of the Twelfth European Con-
ference on Computer Systems, EuroSys 2017, Belgrade, Serbia, April 23-26, 2017,Gustavo Alonso, Ricardo Bianchini, and Marko Vukolic (Eds.). ACM, 298–313.
https://doi.org/10.1145/3064176.3064186
[3]Hammam M. Alghmadi, Mark D. Syer, Weiyi Shang, and Ahmed E. Hassan. 2016.
AnAutomatedApproachforRecommendingWhentoStopPerformanceTests.
In2016 IEEE International Conference on Software Maintenance and Evolution,
ICSME2016,Raleigh,NC,USA,October2-7,2016.IEEEComputerSociety,279–289.
https://doi.org/10.1109/ICSME.2016.46
[4]DimitriosAthanasiou,AriadiNugroho,JoostVisser,andAndyZaidman.2014.
TestCodeQualityandItsRelationtoIssueHandlingPerformance. IEEETrans.
SoftwareEng. 40,11(2014),1100–1125. https://doi.org/10.1109/TSE.2014.2342227
[5]MoritzBeller,GeorgiosGousios,andAndyZaidman.2015.How(Much)DoDevel-
opers Test?.In 37th IEEE/ACM InternationalConference on Software Engineering,
ICSE 2015, Florence, Italy, May 16-24, 2015, Volume 2, Antonia Bertolino, Ger-
ardo Canfora, and Sebastian G. Elbaum (Eds.). IEEE Computer Society, 559–562.
https://doi.org/10.1109/ICSE.2015.193
[6]Cor-PaulBezemer,SimonEismann,VincenzoFerme,JohannesGrohmann,Robert
Heinrich,PooyanJamshidi,WeiyiShang,AndrévanHoorn,MonicaVillavicencio,
Jürgen Walter, and Felix Willnecker. 2019. How is Performance Addressed
in DevOps?. In Proceedings of the 2019 ACM/SPEC International Conference on
Performance Engineering (ICPE ’19) . ACM, New York, NY, USA, 45–50. https:
//doi.org/10.1145/3297663.3309672
[7]Andrew P. Bradley. 1997. The use of the area under the ROC curve in the
evaluation of machine learning algorithms. Pattern Recognition 30, 7 (1997),
1145–1159. https://doi.org/10.1016/S0031-3203(96)00142-2
[8]Leo Breiman. 2001. Random Forests. Machine Learning 45, 1 (01 Oct 2001), 5–32.
https://doi.org/10.1023/A:1010933404324
[9] Leo Breiman. 2001. Random forests. Machine learning 45, 1 (2001), 5–32.
[10]Leo Breiman. 2002. Manual on setting up, using, and understanding random
forests v3. 1. Statistics Department University of California Berkeley, CA, USA
(2002).
[11]Lubomír Bulej, Tomás Bures, Vojtech Horký, Jaroslav Kotrc, Lukás Marek,
Tomás Trojánek, and Petr Tuma. 2017. Unit testing performance with Sto-
chastic Performance Logic. Autom. Softw. Eng. 24, 1 (2017), 139–187. https:
//doi.org/10.1007/s10515-015-0188-0
[12]Ruth Cano-Corres, Javier Sánchez-Álvarez, and Xavier Fuentes-Arderiu. 2012.
The effect size: beyond statistical significance. EJIFCC23, 1 (2012), 19.
[13]Jinfu Chen and Weiyi Shang. 2017. An Exploratory Study of Performance Re-gression Introducing Code Changes. In 2017 IEEE International Conference on
Software Maintenance and Evolution, ICSME 2017, Shanghai, China, September
17-22, 2017. 341–352. https://doi.org/10.1109/ICSME.2017.13
[14]Tse-HsunChen,WeiyiShang,AhmedE.Hassan,MohamedN.Nasser,andPar-
minder Flora. 2016. CacheOptimizer: helping developers configure caching
frameworksforhibernate-baseddatabase-centricwebapplications.In Proceed-
ingsofthe24thACMSIGSOFTInternationalSymposiumonFoundationsofSoft-
ware Engineering, FSE 2016, Seattle, WA, USA, November 13-18, 2016. 666–677.
https://doi.org/10.1145/2950290.2950303
[15]Tse-Hsun Chen, Weiyi Shang, Zhen Ming Jiang, Ahmed E. Hassan, Mohamed N.
Nasser, and Parminder Flora. 2014. Detecting performance anti-patterns forapplications developed using object-relational mapping. In 36th International
ConferenceonSoftwareEngineering,ICSE’14,Hyderabad,India-May31-June
07, 2014. 1001–1012. https://doi.org/10.1145/2568225.2568259
[16] Norman Cliff. 1996. Ordinal methods for behavioral data analysis. (1996).[17]
Robert Coe. 2002. It’s the effect size, stupid: What effect size is and why it is
important. (2002).[18]DiegoCosta,ArturAndrzejak,JanosSeboek,andDavidLo.2017. EmpiricalStudyofUsageandPerformanceofJavaCollections.In Proceedingsofthe8thACM/SPEC
onInternationalConferenceonPerformanceEngineering,ICPE2017,L’Aquila,Italy,
April 22-26, 2017, Walter Binder, Vittorio Cortellessa, Anne Koziolek, Evgenia
Smirni, and Meikel Poess (Eds.). ACM, 389–400. https://doi.org/10.1145/3030207.
3030221
[19]Vasiliki Efstathiou, Christos Chatzilenas, and Diomidis Spinellis. 2018. Wordembeddings for the software engineering domain. In Proceedings of the 15th
International Conference on Mining Software Repositories, MSR 2018, Gothenburg,
Sweden, May 28-29, 2018, Andy Zaidman, Yasutaka Kamei, and Emily Hill (Eds.).
ACM, 38–41. https://doi.org/10.1145/3196398.3196448
[20]BaljinderGhotra,ShaneMcIntosh,andAhmedE.Hassan.2015. Revisitingthe
Impact of Classification Techniques on the Performance of Defect Prediction
Models.In Proceedingsofthe37thInternationalConferenceonSoftwareEngineering
-Volume1(ICSE’15) .IEEEPress,Piscataway,NJ,USA,789–800. http://dl.acm.
org/citation.cfm?id=2818754.2818850
[21]Christoph Heger, Jens Happe, and Roozbeh Farahbod. 2013. Automated Root
CauseIsolationofPerformanceRegressionsDuringSoftwareDevelopment.In
Proceedingsofthe4thACM/SPECInternationalConferenceonPerformanceEngi-
neering (ICPE’13).ACM,NewYork,NY,USA,27–38. https://doi.org/10.1145/
2479871.2479879
[22]Vojtěch Horký, František Haas, Jaroslav Kotrč, Martin Lacina, and Petr Tůma.
2013. PerformanceRegressionUnitTesting:ACaseStudy.In ComputerPerfor-
manceEngineering-10thEuropeanWorkshop,EPEW2013,Venice,Italy,September
16-17, 2013. Proceedings (Lecture Notes in Computer Science), Maria Simonetta
Balsamo,WilliamJ.Knottenbelt,andAndreaMarin(Eds.),Vol.8168.Springer,
149–163. https://doi.org/10.1007/978-3-642-40725-3_12
[23]VojtechHorký,PeterLibic,LukásMarek,AntonínSteinhauser,andPetrTuma.
2015. Utilizing Performance Unit Tests To Increase Performance Awareness.InProceedings of the 6th ACM/SPEC International Conference on Performance
Engineering, Austin, TX, USA, January 31 - February 4, 2015. 289–300. https:
//doi.org/10.1145/2668930.2688051
[24]PengHuang,XiaoMa,DongcaiShen,andYuanyuanZhou.2014. Performance
regression testing target prioritization via performance risk analysis. In 36th
International Conference on Software Engineering, ICSE ’14, Hyderabad, India -
May 31 - June 07, 2014. 60–71. https://doi.org/10.1145/2568225.2568232
[25]Zhen Ming Jiang and Ahmed E. Hassan. 2015. A Survey on Load Testing of
Large-ScaleSoftwareSystems. IEEETrans.SoftwareEng. 41,11(2015),1091–1118.
https://doi.org/10.1109/TSE.2015.2445340
[26]Guoliang Jin, Linhai Song, Xiaoming Shi, Joel Scherpelz, and Shan Lu. 2012.Understanding and detecting real-world performance bugs. In ACM SIGPLAN
Conference on Programming Language Design and Implementation, PLDI ’12, Bei-
jing, China - June 11 - 16, 2012. 77–88. https://doi.org/10.1145/2254064.2254075
[27]Yasutaka Kamei, Emad Shihab, Bram Adams, Ahmed E. Hassan, Audris Mockus,
Anand Sinha, and Naoyasu Ubayashi. 2013. A Large-Scale Empirical Study of
Just-in-Time QualityAssurance. IEEE Trans. Software Eng. 39, 6 (2013), 757–773.
https://doi.org/10.1109/TSE.2012.70
[28]ChristophLaaber,JoelScheuner,andPhilippLeitner.2019. SoftwareMicrobench-
marking in the Cloud. How Bad is it Really? Empirical Software Engineering
(2019), 1–46.
[29]Philipp Leitner and Cor-Paul Bezemer. 2017. An Exploratory Study of the
State of Practice of Performance Testing in Java-Based Open Source Projects. In
Proceedings of the 8th ACM/SPEC on International Conference on Performance
Engineering, ICPE 2017, L’Aquila, Italy, April 22-26, 2017 . 373–384. https:
//doi.org/10.1145/3030207.3030213
[30]Meng-Hui Lim, Jian-Guang Lou, Hongyu Zhang, Qiang Fu, Andrew Beng Jin
Teoh,QingweiLin, RuiDing,andDongmeiZhang. 2014. IdentifyingRecurrent
and Unknown Performance Issues. In 2014 IEEE International Conference on Data
Mining, ICDM 2014, Shenzhen, China, December 14-17, 2014 . 320–329. https:
//doi.org/10.1109/ICDM.2014.96
[31]Qi Luo, Denys Poshyvanyk, and Mark Grechanik. 2016. Mining performance
regressioninducingcodechangesinevolvingsoftware.In Proceedingsofthe13th
InternationalConferenceonMiningSoftwareRepositories,MSR2016,Austin,TX,
USA, May14-22, 2016. 25–36. https://doi.org/10.1145/2901739.2901765
[32]Qi Luo, Denys Poshyvanyk, and Mark Grechanik. 2016. Mining Performance
RegressionInducingCodeChangesinEvolvingSoftware.In Proceedingsofthe
13th International Conference on Mining Software Repositories (MSR ’16) . ACM,
New York, NY, USA, 25–36. https://doi.org/10.1145/2901739.2901765
[33]Haroon Malik, Hadi Hemmati, and Ahmed E. Hassan. 2013. Automatic detection
of performance deviations in the load testing of large scale systems. In 35th
International Conference on Software Engineering, ICSE ’13, San Francisco, CA,
USA, May18-26, 2013. 1012–1021. https://doi.org/10.1109/ICSE.2013.6606651
[34]AudrisMockusandDavidM.Weiss.2000. Predictingriskofsoftwarechanges.
Bell Labs Technical Journal 5, 2 (2000), 169–180. https://doi.org/10.1002/bltj.2229
[35]Todd Mytkowicz, Amer Diwan, Matthias Hauswirth, and Peter F. Sweeney. 2009.
Producing wrong data without doing anything obviously wrong!. In Proceedings
of the 14th International Conference on Architectural Support for Programming
Languages and Operating Systems, ASPLOS 2009, Washington, DC, USA, March
1445ICSE ’20, May 23–29, 2020, Seoul, Republic of Korea Zishuo Ding, Jinfu Chen, and Weiyi Shang
7-11, 2009. 265–276. https://doi.org/10.1145/1508244.1508275
[36]NachiappanNagappanandThomasBall.2007. UsingSoftwareDependenciesand
ChurnMetricstoPredictFieldFailures:AnEmpiricalCaseStudy.In ESEM’07:
ProceedingsoftheFirstInternationalSymposiumonEmpiricalSoftwareEngineering
andMeasurement.IEEEComputerSociety,Washington,DC,USA,364–373. https:
//doi.org/10.1109/ESEM.2007.87
[37]Nachiappan Nagappan, Thomas Ball, and Andreas Zeller. 2006. Mining MetricstoPredict ComponentFailures. In ICSE’06: Proceedingsof the28th International
Conference on Software Engineering. ACM, New York, NY, USA, 452–461. https:
//doi.org/10.1145/1134285.1134349
[38]AdrianNistor,LinhaiSong,DarkoMarinov,andShanLu.2013. Toddler:detecting
performance problems via similar memory-access patterns. In 35th International
Conference on Software Engineering, ICSE ’13, San Francisco, CA, USA, May 18-26,
2013. 562–571. https://doi.org/10.1109/ICSE.2013.6606602
[39]F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M.
Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cour-
napeau,M.Brucher,M.Perrot,andE.Duchesnay.2011. Scikit-learn:Machine
Learning in Python. Journal of Machine Learning Research 12 (2011), 2825–2830.
[40]StuartReid.2005. TheArtofSoftwareTesting,Secondedition.GlenfordJ.Myers.
Revised and updated by Tom Badgett and Todd M. Thomas, with Corey Sandler.
JohnWileyandSons,NewJersey,USA,2004,ISBN0-471-46912-2. Softw.Test.,
Verif. Reliab. 15,2 (2005), 136–137. https://doi.org/10.1002/stvr.322
[41]Giampaolo Rodola.2016. Psutil package:a cross-platformlibrary forretrieving
informationonrunningprocessesandsystemutilization. https://github.com/
giampaolo/psutil
[42]JeanineRomano,JeffreyDKromrey,JesseCoraggio,andJeffSkowronek.2006.
Appropriate statistics for ordinal level data: Should we really be using t-test and
Cohen’sd for evaluating group differences on the NSSE and other surveys. In
annual meeting of the Florida Association of Institutional Research. 1–33.
[43]AJ Scott and M Knott. 1974. A cluster analysis method for grouping means in
the analysis of variance. Biometrics (1974), 507–512.
[44]LinhaiSongandShanLu.2017. Performancediagnosisforinefficientloops.In
Proceedings of the 39th International Conference on Software Engineering, ICSE2017, Buenos Aires, Argentina, May 20-28, 2017, Sebastián Uchitel, AlessandroOrso,andMartinP.Robillard(Eds.).IEEE/ACM,370–380. https://doi.org/10.
1109/ICSE.2017.41
[45]PetrStefan,VojtechHorký,LubomírBulej,andPetrTuma.2017. UnitTestingPer-formanceinJavaProjects:AreWeThereYet?.In Proceedingsofthe8thACM/SPEC
onInternationalConferenceonPerformanceEngineering,ICPE2017,L’Aquila,Italy,April 22-26, 2017. 401–412. https://doi.org/10.1145/3030207.3030226
[46]Mark D. Syer, Weiyi Shang, Zhen Ming Jiang, and Ahmed E. Hassan. 2017. Con-
tinuousvalidationofperformancetestworkloads. Autom.Softw.Eng. 24,1(2017),
189–231. https://doi.org/10.1007/s10515-016-0196-8
[47]Chakkrit Tantithamthavorn, Shane McIntosh, Ahmed E. Hassan, and Kenichi
Matsumoto. 2017. An Empirical Comparison of Model Validation Techniques for
Defect Prediction Models. IEEE Trans. Software Eng. 43, 1 (2017), 1–18. https:
//doi.org/10.1109/TSE.2016.2584050
[48]Patanamon Thongtanunam, Weiyi Shang, and Ahmed E. Hassan. 2019. Will this
clone be short-lived? Towards a better understanding of the characteristics of
short-livedclones. EmpiricalSoftwareEngineering 24,2(2019),937–972. https:
//doi.org/10.1007/s10664-018-9645-2
[49]NikolaiTillmannandWolframSchulte.2006. UnitTestsReloaded:Parameterized
Unit Testing with Symbolic Execution. IEEE Software 23, 4 (2006), 38–47. https:
//doi.org/10.1109/MS.2006.117
[50]ElaineJ.WeyukerandFilipposI.Vokolos.2000. ExperiencewithPerformance
TestingofSoftwareSystems:Issues,anApproach,andCaseStudy. IEEETrans.
Software Eng. 26, 12 (2000), 1147–1156. https://doi.org/10.1109/32.888628
[51]XinXia,DavidLo,EmadShihab,XinyuWang,andBoZhou.2015. Automatic,
high accuracy prediction of reopened bugs. Autom. Softw. Eng. 22, 1 (2015),
75–109. https://doi.org/10.1007/s10515-014-0162-2
[52]PengChengXiong,CaltonPu,XiaoyunZhu,andReanGriffith.2013. vPerfGuard:
anautomatedmodel-drivenframeworkforapplicationperformancediagnosis
inconsolidatedcloudenvironments.In ACM/SPECInternationalConferenceon
Performance Engineering, ICPE’13, Prague, Czech Republic - April 21 - 24, 2013.
271–282. https://doi.org/10.1145/2479871.2479909
[53]Kundi Yao, Guilherme B. de Pádua, Weiyi Shang, Steve Sporea, Andrei Toma,
and Sarah Sajedi.2018. Log4Perf: Suggesting LoggingLocations for Web-based
Systems’ Performance Monitoring. In Proceedings of the 2018 ACM/SPEC Interna-
tionalConferenceon PerformanceEngineering,ICPE2018,Berlin,Germany,April
09-13,2018,KatinkaWolter,WilliamJ.Knottenbelt,AndrévanHoorn,andManoj
Nambiar(Eds.). ACM, 127–138. https://doi.org/10.1145/3184407.3184416
[54]ShahedZaman,BramAdams,andAhmedE.Hassan.2012. Aqualitativestudyon
performancebugs.In 9thIEEEWorkingConferenceofMiningSoftwareRepositories,
MSR2012, June 2-3, 2012, Zurich, Switzerland. 199–208. https://doi.org/10.1109/
MSR.2012.6224281
[55]H. Zhang, S. Wang, T. P. Chen, Y. Zou, and A. E. Hassan. 2019. An EmpiricalStudy of Obsolete Answers on Stack Overflow. IEEE Transactions on Software
Engineering (2019), 1–1. https://doi.org/10.1109/TSE.2019.2906315
1446