Mastering Uncertainty in Performance Estimations of
Configurable Software Systems
Johannes Dorn
Leipzig University
johannes.dorn@uni-leipzig.deSven Apel
Saarland University
Saarland Informatics Campus
apel@cs.uni-saarland.deNorbert Siegmund
Leipzig University
norbert.siegmund@uni-leipzig.de
ABSTRACT
Understanding the influence of configuration options on perfor-
mance is key for finding optimal system configurations, systemunderstanding, and performance debugging. In prior research, a
number of performance-influence modeling approaches have been
proposed, which model a configuration option’s influence and a
configuration’sperformanceasascalarvalue.However,thesepoint
estimates falsely imply a certainty regarding an option’s influencethat neglects several sources of uncertainty within the assessment
process,suchas(1)measurementbias,(2)modelrepresentationandlearningprocess,and(3)incompletedata.Thisleadstothesituation
thatdifferentapproachesandevendifferentlearningrunsassign
different scalar performance values to options and interactions
amongthem.Thetrueinfluenceisuncertain,though.Thereisno
way to quantify this uncertainty with state-of-the-art performance
modeling approaches.We propose a novelapproach, P4, basedon
probabilisticprogrammingthatexplicitlymodelsuncertaintyfor
option influences and consequently provides a confidence interval
for each prediction of a configuration’s performance alongside a
scalar. This way, we can explain, for the first time, why predictions
may cause errors and which option’s influences may be unreliable.
An evaluation on 12 real-world subject systems shows that P4’s
accuracy is in line with the state of the art while providing reliable
confidence intervals, in addition to scalar predictions.
CCS CONCEPTS
•Software and its engineering →Software performance ;•
Computing methodologies →Uncertainty quantification;
KEYWORDS
Probabilistic Programming, Performance-Influence Modeling, Con-
figurable Software Systems, P4
ACM Reference Format:
Johannes Dorn, Sven Apel, and Norbert Siegmund. 2020. Mastering Uncer-
tainty in Performance Estimations of Configurable Software Systems. In
35th IEEE/ACM International Conference on Automated Software Engineering
(ASE’20),September21–25,2020,VirtualEvent,Australia. ACM,NewYork,
NY, USA, 13 pages. https://doi.org/10.1145/3324884.3416620
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ASE ’20, September 21–25, 2020, Virtual Event, Australia
© 2020 Association for Computing Machinery.
ACM ISBN 978-1-4503-6768-4/20/09...$15.00
https://doi.org/10.1145/3324884.3416620Figure 1: An exemplary option’s performance influence
modeled by different scalar regression models (bars) con-trasted by P4’s probability density prediction (blue curve).
1 INTRODUCTION
Modern software systems are often configurable. They offer sev-
eralconfigurationoptionsthataffectthesystems’functionaland
non-functionalproperties.Energyconsumption,responsetime,and
throughputareexamplesofnon-functionalproperties,whichare
commonlysubsumedbytheterm performance.Understandingan
option’s influence on performance and predicting performance for
particularconfigurationsiskeywhenitcomestofindingoptimal
systemconfigurations.Findinganoptimalconfigurationisanessen-
tial task because (1) many systems are shipped with a sub-optimal
defaultconfiguration[ 10,40],(2)manuallyexploringconfigurations
does not scale [ 45], and (3) fine-grained tuning can improve perfor-
manceuptoseveralordersofmagnitude[ 15,47].Todeterminethe
influence of individual configuration options and their interactions
onperformance,anumberofmachinelearningapproacheshave
beenproposed,relyingonrule-baseddecisiontrees[ 5],symbolic
regression [35], and artificial neural networks [7].
To model the influence of options on performance, a set of con-
figurations must be sampled and measured from a system’s config-
uration space. These data are fed into a learning algorithm to fit a
model which then allows users to estimate a scalar performance
valueforagiven configuration.However, this scalarvaluefalsely
impliesacertaintythatneglectsseveralsourcesofuncertaintyin
the modeling process: (1) measurement bias, (2) model represen-tation and learning process, and (3) incomplete data (e.g., due to
sampling bias) [36].
Without a proper uncertainty measure, application engineers
may be led to wrong decisions as there is no available information
about how certain a learned influence or estimated performanceis. Let us assume that we have trained a model using one of thepreviously mentioned learning approaches. In the ideal case, we
6842020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE)
obtainaninfluenceperconfigurationoptionandinteractionstat-
ing theexpected contributionto theoverallsystem performance
when(de-)selectinganoptionorcombinationsofoptions.Figure
1illustratesthisinfluenceasascalarnumberusingaverticalbar.
Each bar represents a different learning approach to determine the
scalar representing the option’s influence. As we can see, different
learning approaches lead to different scalars, and even a single
approachcanproducesubstantiallydifferentvaluesarisingfrom
differentrunsanddifferenthyper-parametersettings.Lookingat
Figure 1, it is unclear which actual effect an option has on the sys-
tem’sperformance,andthereisnowaytoquantifythisuncertainty
with state-of-the-art performance modeling approaches for con-
figurable systems. Likewise, applying an optimization approach
toautomaticallyfindanoptimalconfigurationusingonlyscalars
will yield a single optimal (possibly incorrect) configuration, while
there may be other better configurations that the model misjudged
due to unconsidered uncertainties.
Our key contribution is the following: We account for uncer-
tainty about an option’s and interaction’s true influence on per-
formance that may arise from measurement bias, the learning pro-
cedure, and incompleteness of data [ 39]. By making uncertainty
explicitacrossthemodelingprocessusinga Bayesian ratherthan
a frequentist approach, we foster model understanding for perfor-
mance engineers, provide clear expectation boundaries for perfor-
manceestimatesofsoftwareconfigurations,andofferameansto
quantify when and where a learned model is inaccurate. All these
pieces of information are absent in current approaches, which can
harm trust in the models and transfer into practice.
To illustrate our approach, we compare the probability distri-
butiondescribingthepossibleperformancevalueofanoption(in
blue)inFigure1withthescalarsproducedbythedifferentlearning
approaches.Consideringthe distributionasawhole, wecanstate
howlikelytheinfluenceofanoptionorinteractionfallsintoavalue
range.Thespreadofthedistributionisanimportantindicatorfor
the certainty of estimation and whether additional data for thisoption might be necessary. It also gives confidence intervals for
predictions and performance optimizations.
Framing the problem of performance modeling in a Bayesian
setting can be done via probabilistic programming [ 31]. It requires
the specification of three key components: likelihood, prior, and
observations.The likelihood expressesagenerativemodelofhow
theobservations (i.e., measured configurations) are distributed. The
priorencodes the belief (or expectation) about each option’s and
interaction’s influence on performance. This is usually stated by
a distribution for a specific value range (e.g., uniform distribution
between 3 and 5 seconds). Specifying this distribution requires
domainknowledge,whichisnotalwaysavailable.Akeyelementofourapproachisan automatedpriorestimationalgorithm,whichcan
be used to learn accurate Bayesian performance-influence models
without domain knowledge.
Insummary,weproposeanapproachforperformance-influence
modeling that incorporates and quantifies the uncertainty of influ-
ences of configuration options and interactions on performance.
Akeyingredientisanautomaticpriorestimationalgorithmthat
takestheburdenofguessingpriorsfromtheuser.Weconductan
evaluation of the reliability of the uncertainty estimates of inferredmodelsandcomparetheaccuracyofourapproachtoastate-of-the-
art point estimate model.
We make the following contributions:
•aprobabilisticmodelingapproachforperformanceinfluence
modeling of configurable software systems,
•a data preprocessing pipeline to avoid inference failures and
to improve model interpretability,
•the tool called P4, which is an open-source implementation
of our approach,
•an evaluation of P4’s prediction accuracy, and
•anevaluationofthereliabilityoftheuncertaintymeasures
of models inferred with P4.
Withourapproach,weaddtotheimportanttrendonexplainability
andinterpretabilityofmachine-learningmodels.Webelievethat
this is especially important in domainssuch as software engineer-
ing, in which machine-learning models must provide insights and
explanations to help improving the field.
2 MODELING UNCERTAINTY
Performance-influencemodelingentailsdifferentkindsofuncer-
tainty, of which we consider aleatoric and epistemic uncertainty
inourwork,similarto[ 19,20].Aleatoricuncertainty resultsfrom
errorsinherenttothemeasurementsofthetrainingset, epistemic
uncertainty expressesdoubtinthemodel’sparameters.Bothcan
be integrated into a Bayesian performance model, for which we
explain the basics in Section 2.3.
2.1 Aleatoric Uncertainty
Performance-influence models describe a system’s performance in
termsofinfluencesofitsconfigurationoptionsandinteractions[ 33].
Aconfigurationisasetofassignmentstoallavailableoptionsfroma
certaindomain(e.g.,binaryornumeric),thatis c={o1,o2,...,on},
wherenis the number of options and oiis the value assigned to
the i-th option.
We measure the performance of a configuration by configuring
a software system, and executing a workload. Formally, we denote
a configuration’s performance Πas a function that maps a configu-
rationcfrom the set of valid configurations Cto its corresponding
scalar performance value: Π:C/mapsto→R. For a DBMS, we could
chooseenergyconsumptionasperformance,runabenchmark,and
query an external power meter to determine the energy needed.
However, there are two notable sources of error arising from mea-
surement, which introduce uncertainty: measurement error and
representation error.
2.1.1 Measurement Error. Typically,themeasurementprocess
hasaninherenterror ε,whichistypicallyeitherabsoluteorrela-
tive [37]. Absolute errors εabsaffect all measurements equally:
ˆΠ(c)∈/bracketleftBig
Π(c)−εabs,Π(c)+εabs/bracketrightBig
(1)
By contrast, relative errors εrelare given in percent and affect
higher values more severely:
ˆΠ(c)∈/bracketleftbigg
Π(c)×100−εrel
100,Π(c)×100+εrel
100/bracketrightbigg
(2)
Notethat,dependingonthecontext,thisinterval,called confi-
denceinterval,canbedefinedtospanallpossiblemeasurementsfor
685ˆΠor,altenatively,tocontain ˆΠonlyinafractionofcases(e.g.95%).
Unfortunately, this information is rarely available to the user.
The confidence interval of the measurement error constitutes
anuncertaintythatcanbereducedbyaggregatingrepeatedmea-
surements, but it is fixed at modeling time (i.e., the time when we
fit the model). Moreover, absolute and relative errors are examples
forhomoscedastic andheteroscedastic aleatoric uncertainty, respec-
tively.Thismeansthat,inthecaseofrelativemeasurementerror,
the variance of uncertainty depends on the individual sample (het-
eroscedastic), whereas it is constant for the absolute measurement
error (homoscedastic).
2.1.2 Representation Error. Representationofmeasurementdata
requires discretization for storage and processing. We assume a de-
cimalrepresentationforsimplicity,astheprecisionoffloating-point
representationsismorecomplicated1.Discretizationcanhappen
onthesensorsidebeforewestorethedata.Forexample,anenergy
meter returning only integer Watt-hour (Wh) values may cause
a representation error of +/-0.5Wh, while storing the execution
time of a benchmark in seconds with two decimals may yield a
representation error of +/-5ms.
¯Π(c)∈/bracketleftbigˆΠ(c)−u,ˆΠ(c)+u/bracketrightbig
(3)
That is, in the general case, the performance value at modeling
timeliesaroundthemeasuredperformance ˆΠ(c)within+/- u,the
unit length of the discretization. Depending on the use case, the
representation error can induce substantial uncertainty.
2.2 Epistemic Uncertainty
Models,ingeneral,andperformance-influencemodels,inparticular,
nevermatchrealityperfectly.While,inourcase,aleatoricuncer-
tainty arises from the training data samples, epistemic uncertainty
stems from the model chosen and the amount of data provided.
Let us assume a linear performance model Π(c)for a configurable
software system with noptions:
Π(c)=β0+β1×c(o1)+···+βn×c(on) (4)
Here,c(oi)returns the value for the i-th option of configuration
c; these values are multiplied with the model parameters β, where
β0is the base performance of the system. However, we can assign
different values to βto model Πas a one-point estimate.
Atypicalusecaseinpracticearelinearregressionmodels,which
canbefittedtominimizedifferentobjectivefunctions.Lasso[ 38]
andRidge[ 12]regressionarealternativestoOrdinaryLeastSquares
regression, whichcanbe combinedinto anElastic Net[ 48].Their
objectivesdifferintheirwayofcomputingthelearningerror(L1
andL2normalization).AtuningparameterchangesElasticNet’s
errorcomputationfunctionsuchthatthereisnosinglerightwaytofit a linear model. As Figure 1 shows, we obtain different values for
the samecoefficient βiwhen applyingLasso, Ridge, andOrdinary
LeastSquares.Hence,thefittedvaluefor βiisuncertain,astheblue
curve in Figure 1 illustrates.
Another reason why βcan take different values lies in the train-
ing data used. Different samples of configurations — sampled ac-
cordingtodifferentsamplingstrategies[ 17]—leadtodifferent β
1see 754-2019 - IEEE Standard for Floating-Point Arithmetic for precisionvalues, even with the same error function, as the literature on sam-
plingapproacheshasdemonstrated[ 9,18,35].Yet,evendifferent
hyperparameter settings can result in different coefficients depend-
ingonhowstrongwepenalizethelearningerror.Inaddition,unlessatrainingsetcontainsallvalidsamples,weareuncertainwhether
β
is a good fit, since increasing the training set size usually improves
thepredictionaccuracyofaregressionmodelbyrefining βandalso
reducesuncertaintyabout β.Notethatalthoughaddingsamplesto
the training set reduces epistemic uncertainty, each sample itself is
still subject to aleatoric uncertainty.
Insteadofspecifyingthemodel’sweightsasareal-valuedvec-
torβ∈Rn, we can formally incorporate uncertainty into βby
changing it to a probability vectorˆβ. This way, each model weight
becomes a probability density function that specifies which values
forβare more probable than others representing the best fit. Thus,
for Gaussian-distributed uncertainty, we can specify
ˆβ∼N(μ,σ) (5)
as a probability vector, with μ,σ∈Rn. We do not know, though,
whetheruncertaintyisGaussian-distributedforreal-valuedconfi-
gurablesystemsandwhatarethesettingsfor μ,σ.Todetermine
this distribution, we need probabilistic programming.
2.3 Probabilistic Programming
FramingtheproblemofperformancemodelinginaBayesianset-
tingcanbedonevia probabilisticprogramming [31].Usersofthis
paradigmmustspecifythreekeycomponentswitha probabilistic
programminglanguage (PPL):likelihood,prior,andobservations.
Withthese,thePPLtakes careofBayesianinferenceaccordingto
Bayes’ theorem:
Posterior
/bracehtipdownleft/bracehext/bracehext/bracehtipupright/bracehtipupleft /bracehext/bracehext/bracehtipdownright
P(A|B)=Likelihood
/bracehtipdownleft/bracehext/bracehext/bracehtipupright/bracehtipupleft /bracehext/bracehext/bracehtipdownright
P(B|A)·Prior
/bracehtipdownleft/bracehtipupright/bracehtipupleft/bracehtipdownright
P(A)
P(B)(6)
We refrain from explaining Bayesian statistics from scratch, but
explain in what follows the necessary components for inference. If
weassumethat AandBaredistinctevents,then P(·)mapsanevent
to its probability to occur, P(·|·)gives the conditional probability
ofanevent Agiventhatanotherevent Boccurs.Inthecontextof
probabilisticprogramming, Aisavectorof randomvariables that
representsmodelparameters,whereas Brepresentsobservations.
Arobability ensity unction (PDF) is a function of a random variable
whose integral over an interval represents the probability of the
random variable’s value to lie within this interval. Accordingly,
P(·)maps a random variable to its PDF, and P(·|·)returns the
conditionalPDFofarandomvariablegiventhatanotherrandom
variable has a certain PDF. With these definitions, we next explain
thecomponentsofBayes’theoremthatarerelevantforprobabilistic
programming.
Likelihood/parenleftbigP(B|A)/parenrightbig.The likelihood specifies the distribution of
observations BassumingthatthePDFsformodelparameters Aare
true. With probabilistic programming, the likelihood is typically
specified as a generative model that incorporates random variables.
Imagine an example in which we repeatedly toss a coin to find out
whether and how it is biased. We can represent the probabilities of
thepossibleoutcomes,headsandtails,withaBernoullidistribution
686B(·), whose parameter p∈[0,1]defines the probability of heads.
Formally, we first let Abe a Bernoulli-distributed random variable
and then define the likelihood P(B|A)to be determined by A:
A∼B(p)
B|A∼A
While this model has only one random variable, more complex
modelsarepossible;ho wever,the inferencemaynotbeanalytically
solvable, requiring approximations such as Monte Carlo Markov
Chain(MCMC)sampling[ 26].Suchagenerativemodelcanmake
predictions that are PDFs (i.e., posterior distributions) themselves.
Prior/parenleftbigP(A)/parenrightbig.Priors define our belief about the distribution of
our random variables before seeing any training data. Choosing
priorsnaturallyrequiresdomainknowledgeandiscomparableto
selecting a optimization starting point. An uninformed prior for
the coin-toss example is
A∼B(0.5),
which assumes that both heads and tails are equally probable.
Posterior/parenleftbigP(A|B)/parenrightbigfromobservations B:Givenalikelihood,we
can finally update our prior beliefs with observations. From a
machine-learning stance, observations form the training set. In
caseofthecoin-tossexample,runningBayesianinferencewith5
observedheadswillyieldanupdatedgenerativemodel,theposte-
rior, which will give heads a higher probability.
3 BAYESIAN PERFORMANCE MODELING
Inthissection,wedescribeourapproachofincorporatinguncer-
tainty into performance-influence models. Figure 2 provides an
overviewofallstepsinvolved.Inanutshell,weperformthefollow-
ingtasks:First,wepreprocessagivensetofmeasuredconfigura-
tions(i.e.,thetrainingset)toensurethatinference(i)doesnotbreakand(ii)finishesinareasonabletime.Second,weapplyprobabilistic
programmingto buildaBayesian modelfor aselectionof options
and interactionsthereof. Itis key forscalability thatthis selection
comprises the actual set of influencing options and interactions.
Third,weestimatethepriorsforthe model’srandomvariables(i.e.,
optionsandinteractions)andcomputeafittedmodelwithBayesian
inference.
3.1 Data Preprocessing
Our approach relies on a training set consisting of a number of
sampled configurations that are attributed with their performance.
Thus,ourapproachcanbecombinedwithanysamplingstrategy,
suchasfeature-wise,t-wise[ 16],orrandomsampling[ 4].However,
it is important to process the sample set to avoid inference failures
and to promote interpretability, as we explain next.
SimilartoOrdinaryLeastSquares,Bayesianinferenceisprone
tofailureif multicollinearity existsinthetrainingset,whichoccurs
whenthevaluesofindependentvariablesareintercorrelated[ 3,11].
Letusconsiderthefollowingtrainingsetforanexemplarysoftware
system with options X, Y, Z, and M, illustrating multicollinearity:BXYZM Π(·)
11001 10
10101 20
10011 30
OptionBismandatory.Itrepresentsthebasefunctionalityofthe
system, which results from configuration-independent parts of the
code.Options X,Y,andZformanalternativegroup,thatis,thesys-
tem’s constraints enforce that exactly one of them is active in each
configuration. An important insight is that an alternative group
introduces multicollinearity to a training set because the selection
of any single option is determined by the remaining options, for
example: Z=1−X−Y.Multicollinearitynotonlyhindersinference,
but also interpretability. Consideringthe training set above, we seethat the followingperformance-influencemodelsare accurate with
respect to the measurements, but assigning different contributions
of individual options:
Π(c)=0×c(B)+10×c(X)+20×c(Y)+30×c(Z)
Π(c)=5×c(B)+5×c(X)+15×c(Y)+25×c(Z)
Π(c)=10×c(B)+0×c(X)+10×c(Y)+20×c(Z)
Becauseexactlyoneoptionofthealternativegroupisactivein
each configuration, the base performance of a software system can
beattributedtothebasefunctionalityBandtheoptionsofanalter-
native with any ratio. For example, option Xcan have an influence
of 10, 5, or none, depending on how we assign the performance to
thesystem’sbasefunctionality.Therefore,performanceinfluence
modelsforsuchsystemsaredifficulttocompareandinterpret.Here,
we do not even know whether an option (e.g., X) is influential at
all. This is a problem that related approaches share [5, 34].
Choosingadefaultconfiguration providesremedyformulticol-
linearityinferencefailuresandinterpretabilityproblems.Thatis,
we select a default option for each alternative group using domain
knowledgeoratrandom.Wethenremovetheseoptionsfromthe
training set to achieve the following effects:
•Default options’ performance influences are set to 0.
•Multicollinearity arising from alternative groups is reduced,
sincetheselectionofasingleremainingoptionofanalter-
native group cannot be determined without the removed
defaultoption(i.e., Z=1−X−Ydoesnotholdanymoreif
any of these options is removed from the training set).
Mandatory options, which must be selected in each configuration,
introduceaspecialcaseofmulticollinearity.Option Mismandatory
andthereforepresentineachconfigurationandindistinguishable
from the base influence. Similar to alternative groups, a model can
splitthebaseinfluencebetweenmandatoryoptionsandthebase
influence with any ratio. Moreover, we can see that such an option
does not contribute any information to the model by computing
the Shannon information entropy [32]:
H(o)=−1/summationdisplay.1
x=0Po(x)·log2/parenleftbigPo(x)/parenrightbig(7)
AsMisselectedineachconfiguration,itsonlyselectionvalue
is 1, with selection probability PM(1)=1. We see that, therefore,
687Figure 2: Workflow of P4: 1) preprocess data; 2) compose model from options and interactions; 3) estimate priors for random
variables; 4) infer Bayesian performance-influence model.
the information entropy of Mis 0:
H(M)=−PM(1)·log2/parenleftbigPM(1)/parenrightbig−PM(0)·log2/parenleftbigPM(0)/parenrightbig
=−1·log21−0·log20=0(8)
For that reason, we can safely remove mandatory options from
thetrainingset.Thesameappliesfordeadoptions,whicharenever
active.
Notethatoptionsmayonlyappeartobedeadormandatoryasan
artifactofthesamplingprocess.Thatis,itisinsufficienttoquery
only the system’s variability model for its constraints to detect
mandatory or dead options. Hence, we perform constraint mining
onthesamplesetratherthanthewholesystemtoovercomethis
problem.WeusetheShannoninformationentropyinEquation7
asameanstodeterminedeadoptionsandscanthesetofoptions
for combinations that appear to be alternative groups.
3.2 Model Composition
TobuildaBayesianmodelwithprobabilisticprogramming,wefirst
needtospecifywhichoptionsandinteractionsarepresentinthe
model. Subsequently, we create random variables from this model
structure to account for epistemic and aleatoric uncertainty.
3.2.1 Option and Interaction Filtering. Composingamodelfrom
all options and all potential interactions, whose number is expo-
nential in the number of options, is impractical for large software
systems, because models with high numbers of parameters are dif-
ficult to interpret and, more importantly, inference may become
computationallyinfeasible[ 14].Therefore,weapply modelselection
toconstrainthenumberofparameters.Inparticular,weusea subset
selectionapproach [22],becauseityieldsasubsetofunalteredop-
tions from a parent set, which is not the case for other approaches,
suchasdimensionalityreduction[ 41].Webuildtheparentsetof
available options Sfrom all options Oof the system in question as
well as all pair-wise interactions IwithS=O∪I. We map each
pair-wiseoption itoavirtualoptionwithrespecttoitsconstituting
optionsorandos:
c(on+p)=c(or)×c(os)with 1≤p≤| I |∧ r/nequals(9)
Comparedtohigher-orderinteractions,pair-wiseinteractions
havebeenfoundtofrequentlyinfluenceperformance[ 34]andtobe
the most common kind of interaction [ 21]. However, we acknowl-
edgethatconsideringhigher-orderinteractionsmayimprovethe
accuracy of our approach [ 34], at the cost of possibly leading to
computationally intractable models.
Subset selection approaches define a filter function F:S /mapsto→
{0,1}, which yield 1 if an option or interaction of the parent set Sshould be considered by the model, and 0, otherwise. The result of
subset selection consists of filtered options and interactions:
V=/braceleftbig
s|∀s∈SandF(s)=1/bracerightbig
(10)
Similar to previous work [ 8], we apply Lasso regression [ 38]
on the preprocessed training set. As a result, Lasso assigns zero
performance influence to less- and non-influential options andinteractions, and it distributes the performance influence amongthe remaining elements in
V. Our Lasso filter selects vl∈V,
whose performance influence IΠLasso(vl)is non-zero according to
Lasso regression:
FLasso(vl)=/braceleftBigg
0IΠLasso(vl)=0
1IΠLasso(vl)/nequal0(11)
3.2.2 Applied Probabilistic Programming. We follow related ap-
proachesforperformancemodelingofconfigurablesoftwaresys-
temsandchosean additivemodel tomaketheuncertaintyofthe
options’ and interactions’ performance influence explicit. We start
withamodelthattakestheformofEquation4(whichrepresents
the state of the art) with two differences:
(1)Instead of scalar influences β∈Rn, we use a probability
vectorˆβ, whose elements each have a PDF and form the
coefficients as explained in Section 2.2.
(2)We use the filtered options and interactions Vfrom Sec-
tion3.2.1andthusenableourmodeltocapturenon-linear
performance influence:
Πep(c)=ˆβ0+ˆβ1×c(o1)+···+ˆβn×c(on)
+ˆβn+1×c(on+1)+···+ˆβn+|I|×c(on+|I|)(12)
To infer the distribution of an option, we need to specify a prior
distributionfortheprobabilityvectorˆβ.Thisdistributionshould
be continuous (i.e., defined over all β∈R) and have non-zero
mass for any β∈R, not to exclude certain values entirely. For
performance modeling, we choose the normal distribution N(μ,σ).
It has a mode that, other than the uniform distribution, lets usencode an influence area of high probability. That is, an option’s
or interaction’s influence has a normally distributed probability to
fall into an interval to be inferred by probabilistic programming.Note that, even if a normal distribution is not the best fit for all
randomvariables,Bayesianinferencecanadjustthem.Wedescribe
how to determine the parameters for chosen prior distributions,such as the mean
μand the standard deviation σfor the normal
distribution N(μ,σ), in Section 3.3.
688Atthispoint,wehaveconstructed Πep,amodelthatincorporates
epistemicuncertaintyinˆβ.Toaccountforaleatoricuncertainty(i.e.,
theuncertaintyinthetrainingset),weusetwodifferentmodels,one
for homoscedastic (constant variance) and one for heteroscedastic
(variance depending on true performance) aleatoric uncertainty.
Thesemodelsbuildon Πep.Weadoptthecommonpriorofanormal
distribution for both models.
HomoscedasticModel. Ifweassumethatthevarianceofuncer-
tainty is equal for all training set samples, we can complete our
Bayesian model with a normal distribution around Πep(c):
Πho(c)=N/parenleftbigΠep(c),σ/parenrightbig(13)
This normal distribution is modeled as an additional random
variable,whose σparametercapturesthevarianceofabsoluteerrors
in training set samples.
HeteroscedasticModel. Toaccountforerrorsinthetrainingset
thatarerelativetothetrainingsetsampleperformance,weintro-
duceσrel, a random variable that captures uncertainty about the
errorratio.Asanerrorratioisin R>0(i.e.,acontinuous,positive
variable), wechoose the Gammadistribution as priorfor σrel. The
Gammadistributionwithashape aandaspreadparameter bcan
take a (possibly skewed) bell shape with non-negative values:
σrel=G(a,b) (14)
Similar to the homoscedastic model, we define the heteroscedas-
tic model as a normal distribution around Πep(c), but with the
productoftheepistemicperformancepredictionandtherelative
error ratio σrelas standard deviation:
Πhe(c)=N/parenleftbigΠep(c),Πep(c)×σrel/parenrightbig(15)
3.3 Prior Estimation
RegularBayesianinferencerequirestheusertoestimatepriordistri-
butions for the model’s random variables from domain knowledge
or personal experience. Distributions that are too uninformative
(i.e., very wide) can lead to a holdof the inference, whereas distri-
butions thatare too informativewill alsoslowdown inference if
theyareimprecise[ 31].Ourapproachautomaticallychooseswhich
optionsandinteractionsaremodeledasrandomvariables,suchthat
theuserdoesnotneedtoknowwhichrandomvariablesneedpriors
beforehand. For that reason, we employ an automatic prior esti-
mationfollowingthe empiricalBayesapproach [29],whichdiffers
fromtheregularBayesianapproachinthatitestimatespriorsfrom
the training data. As a result, every aspect of Bayesian modeling is
automated for the user.
3.3.1 Epistemic Uncertainty Priors. We capture epistemic un-
certaintyinourBayesianmodelinrandomvariablesforthebase
influence and the influences for options and interactions, whose
assumed normally distributed priors rely on means μand standard
deviations σ.
We propose a prior estimation algorithm that uses the influence
valuesofotheradditivemodelstoestimatepriors.Asmodels,we
use instances of Elastic Net [ 48] withrevenly distributed ratios of
l1∈[0,1]. Forl1=1, Elastic Net behaves like Lasso, for l1=0i t
behaveslikeRidgeregressionanditinterpolatestheerrorfunctionsof both approaches for 0 <l1<1. We fit 50 Elastic Nets evenly
distributed with l1on the training set. This way, we obtain a set
of 50 models Mwith different performance influences I(·)for the
previouslyselectedoptionsandinteractions.Next,wedeterminetheempiricaldistributionofinfluencesforeachoptionandinteraction:
ˆIM(vl)=/braceleftbig
Im(vl)|∀m∈M/bracerightbig
(16)
We could use the mean and standard deviation of ˆIMas priorμ
andσforeachoptionandinteraction.However,notallmodelsin
Mwillfitthetrainingdatawell.Toreducetheinfluenceforunfit
models, we weigh each model according to its average error on the
training set ¯ε(·):
w=⎧⎪⎪ ⎨
⎪⎪⎩−¯ε(mi)
/summationtext.1|M|
j=1−¯ε(mj)/barex/barex/barex∀m
i∈M⎫⎪⎪ ⎬
⎪⎪⎭(17)
Wecomputetheweightedmean μw(t)andweightedstandard
deviation σw(t)for a specific option or interaction tas follows:
μw(t)=1
/summationtext.1|w|
j=1wγ
j/summationdisplay.1
∀i∈ˆIM(t)wγ
i(18)
σw(t)=/radicaltp/radicalbt1
/summationtext.1|w|
j=1wγ
j/summationdisplay.1
∀i∈ˆIM(t)wγ
i/parenleftbigμw(t)−i/parenrightbig2(19)
We added the tuning parameter γto enable polynomial weighting.
That is, the influence of models with the lowest average error ¯ε
is increased for γ>1. In a pre-study, we empirically evaluated
different values for γand found that γ=3 yields accurate priors.
3.3.2 Aleatoric Uncertainty Priors. We model aleatoric uncer-
tainty (i.e., uncertainty in each training set sample) as a normal
distribution for the homoscedastic model Πhoand as a gamma dis-
tributionastherelativeuncertaintyintheheteroscedasticmodel
Πhe. We build the set of all absolute prediction errors for all mo-
delsm∈Mover the samples in the training set and fit a normal
distribution using maximum likelihood estimation to estimate a
prior for the aleatoric uncertainty in Πho. Likewise, we estimate a
priorforthegammadistributionin Πho,butwecomputerelative
prediction errors, instead, to model the error ratio (cf. Equation 15).
3.4 Bayesian Inference and Prediction
As discussed in Section 2.3, Bayesian inference uses prior assump-
tionsonPDFsofrandomvariablesthatformagenerativemodel,
called likelihood, to compute a posterior, that is, an updated belief
aboutthe randomvariable’sPDFs.Unfortunately,theposteriorto
many Bayesian inference problems cannot be computed directly,
sorecentresearchinthisfieldhasdevelopedalgorithmsthatcan
estimate the posterior approximately. Two notable classes of infer-
ence algorithms are variational inference and Markov chain Monte
Carlo [23].
Variational inference algorithms tune the prior distribution’s
parameterswithoutchangingthetypesofthedistributions(i.e.,a
prior normal distribution stays a normal distribution) [ 30]. This
method is preferred for quick results that do not need to be precise.
Markov chain Monte Carlo (MCMC) algorithms draw samples
fromtheposteriordistributionsandareabletoestimatearbitrary
posteriordistributionsintheory(apriornormaldistributionmay
689bytransformedtoaskeweddistribution).MCMCalgorithmsare
consideredmoreprecise,butalsoslowerthanvariationalinference.
Wefollowacombinedapproachbyfirstestimatinganapprox-
imate solution with variational inference and subsequently fine-
tunewiththe No-U-TurnSampler(NUTS) [13],anMCMCalgorithm.
We allow 200 .000 iterations for variational inference, but abort on
convergence.NUTSusestheintermediateresultofvariationalin-
ference and draws 5000 samples from the posterior distributions
in total, from which 1000 are reserved for internal tuning. In the
idealcase,weobtain4000samplesofeachrandomvariable’sposte-
rior distribution, which enables analysis of uncertainty at a high
resolution.
Prediction. Topredicttheperformanceofaconfiguration c,w e
insertc’soptionselectionvaluesinto c(o1),...,c(on)anddetermine
active interactions according to Equation 9. We can now draw a
numberofposteriorsamplestoapproximatethedistributionforthe
prediction. Increasing the number of posterior samples makes the
approximation more accurate, but also slows down prediction. We
draw1000posteriorsamplestoyieldagoodapproximation.With
this approximation, we can make different kinds of predictions, for
which we introduce individual notations. The most informative
kindofpredictionisthesampledapproximationitself( /tildewideΠ).Using
/tildewideΠ,wecancomputeaconfidenceintervalforadesiredconfidence
αci∈[0%,100%](¯Πα). This yields the interval around the mode of
predictionoverwhichthepredicteddistributionintegratesto αci.
We use¯Πto indicate the 95% confidence interval by default. We
can also use mode of the approximation as a single-point estimate
prediction ( /dotaccΠ).
4 EVALUATION
To evaluate our approach, we state three research questions that
are in line with related work and are also concerned with the new
possibilitiesofobtainingaconfidenceintervalforperformancepre-
dictions. Specifically, we answer the following research questions:
RQ1:Canweaccuratelypredictperformanceasascalarvalue
with probabilistic programming?
Thisresearchquestionplacesourapproachinrelationtoastate-of-
the-artapproachthatresortsonlytoascalarvalue.Althoughthis
is not the main usage scenario, we evaluate whether our approach
has a comparable accuracy.
RQ2:Can we accurately predict performance in terms of a
confidence interval with probabilistic programming?
RQ2referstotheabilitythatuserscanspecifyaconfidenceinterval
ofpredictions.Thiscansubstantiallyeffectpredictionaccuracyand
evaluates the strength of our approach.
RQ3:How reliable are predicted confidence intervals?
The third research questions aims at providing a deeper under-
standing of confidence intervals and incorporated uncertainties in
our approach. We evaluate whether the confidence intervals trulycapture the uncertainty in the predictions.Table1:Overviewofthesubjectsystemswithdomain,num-
ber of valid configurations |C|, number of options |O|, and
the kind of performance for prediction.
Domain |C| |O| Performance
7z File archive utility 68 640 44 Compression time
BDB-C Embedded database 2 560 18 Response timeSune Multigrid solver 2 304 32 Solving timeHIPA
ccImage processing 13 485 54 Solving time
HSQLDB Java-based database 864 21 Energy consumptionJavaGC Garbage collector 193 536 39 TimeLLVM Compiler infrastructure 1 024 11 Compilation timelrzip File archive utility 432 19 Compression timePolly Code optimizer 60 000 40 RuntimePSQL Database system 864 14 Energy consumption
VP9 Video encoder 216 000 42 Encoding time
Energy consumption
x264 Video encoder 1 152 16 Encoding time
Energy consumption
4.1 Subject Systems
Forourexperiments,weuse12real-worldconfigurablesoftware
systemsthathavebeenusedintheliterature,aspresentedinTable1.
Weusemeasuredexecutiontimeasperformancefor10subjectsys-
tems from Kaltenecker et al .[18]. For VPXENC and x264, we have
additionally measured energy consumption with a different work-
load. In addition, we consider energy consumption for two further
subjectsystems:PostgreSQL(shortPSQL)andHSQLDB[ 43].A
further description of the systems including the used benchmarks
is given at our supplementary Web site2.
Weadopttheprocedureofextractingtrainingandtestsetsfrom
each system’s measurement data from Kaltenecker et al .[18]. That
is, we apply t-wise sampling with t∈{1,2,3}to obtain three
training test sets, T1,T2,T3, of different sizes. Each system’s whole
population (i.e., all measurements) form its test set.
4.2 Setup
We implement our approach with the PyMC3[31] framework.
PyMC3offersimplementationsforMCMC,variationalinference,
as well asconfidence intervalcomputation for model parameters β
andpredictions.Formaximumlikelihoodpriorestimations,werely
onSciPy[42]. The result is a performance prediction tool based on
probabilistic programming, P4for short.
Toanswerourresearchquestions,weinferBayesianmodelswith
absolute and relative error with P4 for the chosen subject systems
using three training sets T1,T2,T3on a cluster of machines with
Intel Xeon E5-2690v2 CPU and 64GB memory. For the ten subject
systemsbyKalteneckeretal .[18],weusethetrainingsetsprovided
attheirsupplementaryWebsite.Fortheremainingsubjectsystems,
we sample new training sets with SPL Conqeror [35].
Fort=1,t-wise sampling is equal to option-wise sampling,
which yields n=|O|samples. Since we want to evaluate our ap-
proachalsoforlearninginteractionsamongoptions,creating n+|I|
random variables leads to a modeling problem with more variables
2https://git.io/JUfjyoranarchivedversionathttps://archive.softwareheritage.org/swh:
1:dir:5a525f45ec77dbe982081e7f8159e9541391725e/
690020406080Error in %o: 26
i: 0o: 26
i: 0o: 26
i: 0o: 26
i: 0o: 7
i: 0o: 7
i: 0o: 7
i: 0o: 7
i: 0o: 16
i: 35o: 16
i: 35o: 16
i: 35o: 16
i: 357z
0255075100125150o: 1
i: 0o: 1
i: 0o: 1
i: 0o: 1
i: 0o: 4
i: 4o: 4
i: 4o: 4
i: 4o: 4
i: 4o: 6
i: 11o: 6
i: 11o: 6
i: 11o: 6
i: 11BerkeleyDBC
05101520o: 3
i: 0o: 3
i: 0o: 3
i: 0o: 3
i: 0o: 18
i: 23o: 18
i: 23o: 18
i: 23o: 18
i: 23o: 21
i: 127o: 21
i: 127o: 21
i: 127o: 21
i: 127Dune
0102030405060o: 4
i: 0o: 4
i: 0o: 4
i: 0o: 4
i: 0o: 39
i: 89o: 39
i: 89o: 39
i: 89o: 39
i: 89o: 40
i: 185o: 40
i: 185o: 40
i: 185o: 40
i: 185Hipacc
0102030405060o: 8
i: 0o: 8
i: 0o: 8
i: 0o: 8
i: 0o: 20
i: 66o: 20
i: 66o: 20
i: 66o: 20
i: 66o: 23
i: 178o: 23
i: 178o: 23
i: 178o: 23
i: 178JavaGC
123
t02468Error in %o: 1
i: 0o: 1
i: 0o: 1
i: 0o: 1
i: 0o: 9
i: 3o: 9
i: 3o: 9
i: 3o: 9
i: 3o: 8
i: 3o: 8
i: 3o: 8
i: 3o: 8
i: 3LLVM
123
t0204060o: 4
i: 0o: 4
i: 0o: 4
i: 0o: 4
i: 0o: 9
i: 15o: 9
i: 15o: 9
i: 15o: 9
i: 15o: 12
i: 41o: 12
i: 41o: 12
i: 41o: 12
i: 41lrzip
123
t0102030o: 4
i: 0o: 4
i: 0o: 4
i: 0o: 4
i: 0o: 11
i: 79o: 11
i: 79o: 11
i: 79o: 11
i: 79o: 13
i: 97o: 13
i: 97o: 13
i: 97o: 13
i: 97Polly
123
t0100200300o: 7
i: 0o: 7
i: 0o: 7
i: 0o: 7
i: 0o: 14
i: 99o: 14
i: 99o: 14
i: 99o: 14
i: 99o: 24
i: 172o: 24
i: 172o: 24
i: 172o: 24
i: 172VP9
123
t0510152025o: 10
i: 0o: 10
i: 0o: 10
i: 0o: 10
i: 0o: 5
i: 0o: 5
i: 0o: 5
i: 0o: 5
i: 0o: 9
i: 23o: 9
i: 23o: 9
i: 23o: 9
i: 23x264Model (line color): P4 with he(relative error) SPL Conqueror Error Type (line style): MAPE 50% MAPE CI 95% MAPE CI
Figure3:ScalarMeanAbsolutePercentageError(MAPE)of SPLConqerorcomparedtotheMAPEandintervalpredictions
MAPECIforconfidencelevels 50%and95%of P4,withabsoluteerror( Πhoandrelativeerror( Πhe)fort-wisesampledtraining
sets. For each subject system’s training set, we specify P4’s model size in terms of the number of modeled options (o) andinteractions (i) below the system’s name.
than observations. We avoid this situation by excluding interac-
tions from our model for T1. This might affect prediction accuracy
especially compared to other approaches that do not exclude inter-
actions. We will discuss this in RQ 1.
Toaccount forstochasticelements inMCMC,we runtheinfer-
enceforeachsystem’strainingsetwith5repetitions.Theinference
took 418s, on average, with the times ranging from 98 sfor the
smallest models and 4 hfor the largest. Among all inferences, two
failed despite our training set preprocessing in Section 3.1.
4.3 RQ 1: Accuracy of Scalar Predictions
4.3.1 Setup. WechoseSPLConqerorforcomparisonbecause
itsharestheadditivemodelstructurewithourapproachandisused
as baseline in the literature [ 18,28]. For comparison, we rely on
accuraciesof SPLConqerorasreportedbyKalteneckeretal .[18].
That is, we consider for RQ 1the ten subject systems that the orig-
inal authors have used. Another benefit is that Kaltenecker et al .
provided raw measurements of the whole population, so we have a
reliable ground truth.
Weusetheinferredperformance-influencemodelstopredictthe
performance of the whole populations of our subject systems. We
adoptthe MeanAbsolutePercentageError (MAPE)fromprevious
work[18]toquantifypredictionaccuracy.Thatis,wefirstcompute
the absolute percentage error (APE) for each configuration c∈
Cwith the measured performance Πtrue(c)and predicted scalar
performance /dotaccΠ(c)for our models ΠhoandΠhe:
APE(c)=/barex/barexΠtrue(c)−/dotaccΠ(c)/barex/barex
Πtrue(c)(20)We then compute the MAPE as the average over all APEs:
MAPE(C) =/summationtext.1
c∈CAPE(c)
|C|(21)
4.3.2 Results. AsFigure3shows,P4achievesMAPEscorescom-
parabletoSPLConqeror.Table2providesamorefine-grained
view. We see that the overall accuracy is higher when using SPL
Conqeror, which is to be expected, as only the mode is taken
fromtheperformancedistributionprovidedaspredictionsbyour
approach.Nevertheless,weobservethat,formanysystems,espe-
cially when using the relative error Πhe, we obtain a similar or
even better prediction indicated by underscored values. The mean
error is, thus, distorted by some larger outliers, such as for HIPAcc
and VP9. These systems have many alternative options, so there is
a larger uncertainty involved. Since we are not using the provided
confidence interval, we deprive our approach of its strength.
Interestingly,comparedto T1,forsomesubjectsystemsP4per-
forms worse on T2. The reason is that the increased number of
randomvariablesinP4,duetotheadditionalmodelingofinterac-
tions, requires more measurements as provided by T2to effectively
inferperformancedistributions.Moreover,weseeacleartrendthat,
withanincreasingnumberofmeasurements, P4closesthegap in
prediction accuracy with SPL Conqeror and even outperforms it
forT3andΠhefor 7 out of 10 systems.
ToanswerRQ 1,ourapproachachievestheaccuracyofstate-
of-the-art scalar predictions when a sufficient number of mea-
surements is provided. In the case of fewer measurements, the
overhead of learning probability distributions leads to more
inaccurate predictions.
691Table2:ScalarMeanAbsolutePercentageError(MAPE)ofSPLConqeror(shortSPLC)comparedtotheMAPEandconfidence
intervalpredictionsMAPE(MAPE CI)of P4,withabsoluteerror( Πho)andrelativeerror( Πhe)fort-wisesampledtrainingsets.
Best scalar MAPE values for each training set are shaded light gray, best overall MAPE values are shaded dark gray.
SPLC MAPE ΠhoMAPE ΠheMAPE Πho-MAPECI(95%) ΠheMAPECI(95%)
t=1t=2t=3t=1t=2t=3t=1t=2t=3t=1t=2t=3t=1t=2t=3
7z 51.233.8 22.6 70.8 87.5 45.7 61.3 66.2 9.37.14.8 0 23.8 9.6 1.7
BDB-C 122.9 2926.5 123.9 58.8 31.3 121.8 69.2 60.9 89.5 2.7 028.31.7 3.1
Dune 15.512.5 11.4 17.2 13.8 9.2 17.1 12.8 7.71.80.40.1 3 1.5 0.5
Hipacc 26.2 20.5 20.5 53 17.8 9.4 52.8 17 830.55.60.321.6 7.8 1.7
JavaGC 36.732.1 23.7 40.9 65.1 33.4 40.8 52.2 14.324.616.60.7 27.4 28.8 3.1
LLVM 6.2 6.2 5.8 6.9 5.82.8 6.9 5.82.80.21.30.2 1.4 1.9 0.4
Polly 19.7 12.7 7.3 31 11.5 11.1 31.3 12.4 11.1 6.91.30.8 11.2 4.3 2.4
VP9 100.396.345.3 160.3 109.5 88.5 269.4 157.2 108.7 1.90.60.1 3.5 3.3 2.7
lrzip 27.228.2 13.4 45.8 141.7 153.2 32.4 60.1 58.3 00 12.1 4.6 0.2
x264 20.9 11.9 10.9 9.8 16.5 4.7 7.79.61.50.1 00 7.2 2.2 0.6
Mean 42.728.318.7 56 52.8 38.9 64.2 46.3 22.9 17.13.30.214 6.6 1.6
4.4 RQ 2: Accuracy of Confidence Intervals
4.4.1 Setup. Confidenceintervalswithconfidence αCI∈[0%,
100%]specify a range in which a given PDF integrates to αCI.
For predictions, a 95% confidence interval specifies a performance
range for which the model is 95% confident that it contains the
true performance value of the corresponding configuration. Conse-
quently, we can expect the true performance to lie outside the 95%
confidenceintervalin 5%ofpredictions.Although wecanexpect
to always capture the true performance with a 100% confidence
interval,suchanintervalwilllikelyapproach [−∞,+∞]forPDFs
that are defined over R.
Similar to RQ 1, we use a relative error metric to answer RQ 2.
However, for RQ 2, we use P4 to predict confidence intervals as
prediction, which is the actual strength and novel feature of our
approach. Instead of using the APEof a scalar prediction, we com-
pute the confidence interval’s APECIwith relation to the closest
endpoint of the confidence interval ¯Παfor an outlying true per-
formance; we define APECI=0 for anαconfidence interval that
includes the measured performance:
APECI(c)=min∀π∈¯Πα(c)/barex/barexΠtrue(c)−π/barex/barex
Πtrue(c)(22)
Hence, the MAPE CIis the average over all APECI, similar to
Equation21.Forourmodels ΠhoandΠhe,wereporttheMAPE CIfor
predictedconfidenceintervalswith αCI=95%forhighlyconfident
predictionsand αCI=50%forlessconfidentpredictions,forwhich
we expect a narrower interval and, consequently, a higher error.
4.4.2 Results. Thedotted linesinFigure3 illustrateasubstan-
tialdecreaseinpredictionerrorwhenusingaconfidenceinterval
rather than a scalar prediction. Note that we report in Figure 3
onlyMAPE CI’sforΠhe;weprovidesimilarresultsfor Πhoatour
supplementaryWebsite.Table2providesfurtherdatafor Πho.It
revealsthatthepredictedconfidenceintervalsfor7z,BDB-C,lrzip,
and x264 contain all measuredperformance values when training
the absolute model ΠhoonT3.Table3:Fivemostuncertainfeaturesmeasuredbythemean
relative confidence interval β?
jaccording to Equation 25, of
models trained on T1. Values for the variance inflation fac-
tor(VIF)larger10areindarkgray(highlyproblematic)and
values between 5 and 10 are in light gray (moderately prob-
lematic). Files_30 &BlockSize_1024 were removed from T3.
T1 T3
System Option β?VIFβ?VIF
VP8(E)threads_4 24.6410.50 0.15 3.24
VP9(T)bitRate_1500 16.9310.58 2.84 2.90
7z(T)Files_30 11.86 7.13 – 1.67
7z(T)BlockSize_1024 11.44 7.13 – 1.63
VP9(T)variableBitrate 4.9116.90 0.86 1.99
We illustrate how more training samples allow P4 to decrease
uncertainty in internal parameters to achieve better prediction
accuracy using the variance inflation factor (VIF). The VIF is an
indicator for multicollinearity, which can be computed for the acti-
vationvaluesofanoption ojinthetrainingset T.Itisbasedonthe
coefficientofdetermination R2.Todetermine R2foranoption oj,
we fit a linear regression function fjto predict whether ojis active
in a configuration c\ojwith the remaining options as predictors.
We compute the VIF as follows:
VIFj=1
1−R2
j(23)
R2
j=1−/summationtext.1
∀c∈T/parenleftbig
c(oj)−¯c(oj)/parenrightbig2
/summationtext.1
∀c∈T/parenleftbig
c(oj)−fj(c\oj)/parenrightbig2 (24)
AVIFof0indicatesanoptionwithnomulticollinearityinthe
training set, while higher values mark increasingly problematicmulticollinearity. We adopt the thresholds of 5 and 10 [
27,44]t o
indicate moderate and highly problematic multicollinearity, respec-
tively.
6920.00 0.25 0.50 0.75 1.00
Model confidence0.00.20.40.60.81.0Observed confidencet = 1
0.00 0.25 0.50 0.75 1.00
Model confidencet = 2
0.00 0.25 0.50 0.75 1.00
Model confidencet = 3
0102030405060
0102030405060
0102030405060
CI MAPE in %Model (line color): P4 with he(relative error) P4 with ho(absolute error) Metric (line type): CI MAPE in % Observed confidence
Figure 4: MAPE CIdepending on model confidence (solid) versus uncertainty calibration (dashed) for t-wise training sets, ag-
gregated over all subject systems. Gray dashed line indicates ideal calibration.
Although we could use the VIF as a filter for feature selection
(cf. Section 3.3) to remove options with high multicollinearity in
thetrainingset,thecomputationaleffortrequiredtocalculateall
R2
jmakes it infeasible in practice. Hence, we compute the VIF only
for the 5 most uncertain options in T1to analyze whether multi-
collinearityisapossiblecauseforuncertaintyofoptions’influences.
To compute the uncertainty of an option influence β?
j, we use its
confidenceinterval ¯βjandpointestimate /dotaccβj.Toremovetheinflu-
ence of differing influence scales between software systems, we
determine the relative confidence interval width as the ratio of the
absolute confidence interval width |¯βj|and the point estimate:
β?
j=|¯βj|
/dotaccβj(25)
Looking at Table 3, we see that all five options exhibit either a
moderate or even a high VIF for the training set T1. This points to
a situation in which the learning procedure cannot safely assign a
performanceratiotothespecificoption.Investigatingthiscloser,
four options are part of an alternative group despite our efforts
toavoidmulticollinearitybyremovingonealternativefromeach
alternativegroup.Foroption threads_4,wefoundthatitwasactive
inalmosteveryconfiguration(13outof16),reducingthecontained
information according to Equation 7.
Tofurther confirm ourhypothesis that multicollinearitycan be
a possible cause, we show in Table 3 the uncertainty β?
jand the
VIF for these five options using the larger training set T3.W es eea
substantialreductioninuncertaintyforthreeoptionsinlinewith
the reduction of the VIF. This strongly indicates that a reduced
multicollinearityreducesalsotheuncertaintyofanoption’sinflu-
enceonperformance.Options Files_30andBlockSize_1024 have
no uncertainty as they were chosen by P4 to be removed from the
alternative group in T3.
Overall, Πhoyields better results than Πhein most cases, but
both approaches always show substantially lower relative errors
than scalarpredictions. Of course, itwould be easyfor a modelto
predict all performance values correctly with a sufficiently large
confidence interval. However, our findings for RQ 3demonstrate
that P4’s prediction confidence intervals are reliable, as we will
discuss in Section 4.5.ToanswerRQ 2,usingconfidenceintervalsto frametheconfi-
dence of predictions substantially reduces the prediction error.
Thatis, ourapproachisableto modeltheuncertaintyas well
as the true performance distributions accurately.
4.5 RQ 3: Reliability of Confidence Intervals
4.5.1 Setup. As predictions, our approach can yield confidence
intervals with any given confidence level αCI∈[0%,100%].W e
call a model’s predicted confidence intervals reliable if predictions
withanαCIconfidenceintervalcontainthemeasuredperformance
with a similar observed frequency αobs(i.e.,αobs(αCI)≈αCI). To
compute the observed frequency αobs(αCI)for anαCIconfidence
interval, we first define the function within, which returns 1 if
themeasuredperformance Πtrue(c)liesinapredictedconfidence
interval¯Π(c), and 0, otherwise:
within/parenleftbigΠtrue(c),¯Π(c)/parenrightbig=/braceleftBigg
1Πtrue(c)∈¯Π(c)
0 else(26)
Second, the observed frequency is computed as the average of
within over all configurations of a subject system and their mea-
sured performances Πtrue(c):
αobs(αCI)=/summationtext.1
c∈Cwithin/parenleftbigΠtrue(c),¯Π(c)/parenrightbig
|C|(27)
IfαCI/greatermuchαobs(αCI), the predicted confidence interval is inac-
curatemoreoftenthanweexpectandshouldhavebeenbroader;
conversely,thepredictedconfidenceintervalshouldbemorenar-
row and thus more informative if αCI/lessmuchαobs(αCI). Since using
confidence intervals for performance predictions is novel, we have
no baseline to which we can compare. Hence, we report the ob-served frequencies for confidence levels
αCIfrom 5% to 95% in
steps of 5% as well as the average error in percentage to answer
RQ3.Inaddition,wereporttheMAPE CIforallconfidenceintervals.
4.5.2 Results. Figure4showsacalibrationplotthatcompares
αCIwithαobsusing dashed lines. A model with αCI=αobsfor
allαCIwould yield values alongthe dashed gray diagonal. Values
abovethediagonalindicatetoobroadconfidenceintervals(i.e.,our
predictionsaremoreaccuratethantheyshouldbe),valuesbelow
itsignalconfidenceintervalsthataretoonarrow.Thesolidlines
693in Figure 4 show the mean MAPE CIover all subject systems for
boththerelativeandtheabsolutemodel.Theshadedareaaround
it constitutes a 95% confidence interval.
Whenanalyzingthedashedlines,weseethatusingtheabsolute
errorΠhoyieldsintervalsthatareclosertothediagonalthanwhen
usingtherelativeerror Πhe.Moreover,thereisacleartrendthat,
whenusingmoremeasurements,theintervalsbecomeeithernearly
perfectly aligned or are underestimating the models prediction
accuracy. Hence, we see a picture that resembles the picture when
usingthemodeforscalarperformanceprediction:Theapproach
requires a certain number of measurements to become accurate,
but then works robustly.
We can make a further interesting observation when compar-
ing the confidence intervals (dashed lines) with the MAPE CI(solid
lines). First and most importantly, we see that using confidence
intervalsof varyingsizes hasa clearmonotonic relationshipwith
theprediction error. Thatis,increasing theinterval decreases the
error.Second,theerrorsfallrapidly,especiallyfor T2andT3,already
whenusinganarrowinterval,suchas25%.Thisisgoodnewsas
thisclearlyindicatesthatnarrowconfidenceintervalsyieldaccu-
rate predictions. Third, we observe that (for the solid lines) the
uncertaintyishigherwithfewermeasurements,asindicatedbythe
colored area. That is, the model is aware that the measurements
are insufficient to actually make trustworthy predictions. This is a
featuremissinginallscalarpredictionapproaches.Forexample,for
SPL Conqeror, we have no clue whether the model is confident
with a certain prediction. With P4, we have a means to quantify
this confidence.
To answer RQ 3, with enough measurements, our approach
yieldsconfidenceintervalsthatcontainthetruevaluewitha
frequency that matches the specified confidence. Even with
oursmallesttrainingset T1,confidenceintervalswithhigher
specified confidence contain the true value more often.
5 THREATS TO VALIDITY
Threatstointernalvalidityarisefrommeasurementbias.Wereusea
measurement set from a recent paper whose authors controlled for
thisbiasbyrepeatingthemeasurementsseveraltimes[ 18].Athreat
toconstructvaliditymayarisefromthemodelconstructionprocess
in PyMC3. We selected probability distributions for the random
variables based on typical least squares error distributions and best
practices for regression modeling in probabilistic programming.
Externalvalidityreferstothegeneralizabilityofourapproach.Our
data set comprises 12 different subject systems of varying domains
andsizes.Moreover,weassesseddifferentproperties,suchasenergy
consumption and response time. We made similar observations for
all systems such that we are convinced that our approach worksonalargeandpracticallyrelevantclassofconfigurablesoftware
systems.
6 RELATED WORK
Thereisanumberofapproachesinthefieldofperformancemodel-
ing.CART[ 5]anditsimprovedversionDECART[ 6]userule-based
models to accuratelylearn performance models with a smallnum-
ber of samples. FLASH [ 25] is a sequential model-based methodthatreliesonactivelearningtofitCART[ 5]modelsmoreefficiently.
DeepPerf[ 7]isadeeplearning-basedapproach,whichusessparse
neural networks for performance estimation. Zhang et al .propose
a framework to model performance influence with Fourier approx-
imation [ 46], whereas Nair et al .employ spectral learning with
dimensionalityreduction [ 24].SPLConqeror learnsan additive
model with step-wise selection of new terms [ 33]. None of the
proposedapproachesconsidersuncertaintyinpredictionsandin
the internal representation of influences, producing only scalar
estimates.
Notably, the need for incorporating uncertainty in performance
modelinghasbeenarguedbeforebyTrubianiandApel[ 39].While
there are alreadyconsiderations in other fieldsfor both epistemic
andaleatoricuncertainty,suchasforcomputervision[ 19],forsoft-
ware engineering, there are only approaches that model some kind
of epistemic uncertainty. Antonelli et al .have incorporated uncer-
taintybyallowingtwoparametersofaperformanceindexforcloudcomputingsystemstobeuncertainandthusadapttochanginghard-
ware[1].AnotherapproachbyArcainietal .transformsafeature
modelintotwoQueueingNetworks—oneeachforthetwovariants
with minimal and maximal performance — and thereby represents
uncertaintyinperformance [ 2].To thebestofourknowledge, we
arethefirsttofollowTrubianiandApel’scalltoincorporateboth
epistemic and aleatoric uncertainty in performance modeling of
configurable software systems.
7 SUMMARY
Existing approaches forperformance-influence modeling provide
onlyscalarpredictionsbasedonmodelinginfluencesofoptionsandinteractions with scalar values. We argue that these approaches ne-
glect uncertainty arising from the modeling and measurement pro-
cess.Weproposeanovelperformance-influencemodelingapproach
thatincorporatesuncertaintyexplicitlyandyieldsconfidencein-
tervalsalongsidescalarpoint-estimatepredictions.Thisway,we
provide not only a singular number as a performance estimate, but
also a posterior distribution and a confidence in which range a per-
formance value lies. Our experiments with 12 real-world software
systems show that our implementation, P4, yields scalar prediction
accuracies that match the state of the art when provided with a
sufficient number of measurements. Further evaluation shows that
theconfidenceintervalsprovidedarereliableand,whenusedfor
prediction, achieve competitive accuracies.
The analysis of our trained models indicates that options that
are selected in almost every configuration can reduce the amount
ofinformationcontainedinatrainingset,renderingtheoption’s
influenceuncertain.Thisobservationcallsforashiftincurrentsam-
pling strategies by taking the information gain more into account,
ascomparedtocoverageoruniformness.P4showeditspotential
especially with pairwise and triple-wise sampled training sets. Im-
proving P4 for small training sets, hence, remains an open issue. A
possibleremedyareP4’soptioninfluenceuncertainties,whichmay
be facilitated in an active learning setupto learn more efficiently.
ACKNOWLEDGMENT
Siegmund’sandApel’sworkhasbeenfundedbytheGermanRe-
search Foundation (SI 2171/3-1, SI 2171/2, and AP 206/11-1).
694REFERENCES
[1]FabioAntonelli,VittorioCortellessa,MarcoGribaudo,RiccardoPinciroli,KishorS.
Trivedi,andCatiaTrubiani.2020. AnalyticalModelingofPerformanceIndices
under Epistemic Uncertainty Applied to Cloud Computing Systems. Future
GenerationComputerSystems 102(2020),746–761. https://doi.org/10.1016/j.future.
2019.09.006
[2]Paolo Arcaini, Omar Inverso, and Catia Trubiani. 2020. Automated Model-Based
Performance Analysisof Software Product Linesunder Uncertainty. Journal of
Information and Software Technology (IST) 127 (2020), 106371. https://doi.org/10.
1016/j.infsof.2020.106371
[3]Donald E. Farrar and Robert R. Glauber. 1967. Multicollinearity in Regression
Analysis:TheProblemRevisited. TheReviewofEconomicsandStatistics 49(1967),
92–107. https://doi.org/10.2307/1937887
[4]VibhavGogateandRinaDechter.2006. ANewAlgorithmforSamplingCSPSolu-
tions Uniformly atRandom. In Principles andPractice of Constraint Programming
- CP 2006. Springer, 711–715. https://doi.org/10.1007/11889205_56
[5]JianmeiGuo,KrzysztofCzarnecki,SvenApel,NorbertSiegmund,andAndrzej
Wasowski.2013.Variability-AwarePerformancePrediction:AStatisticalLearning
Approach.In ProceedingsoftheInternationalConferenceonAutomatedSoftware
Engineering (ASE). IEEE, 301–311. https://doi.org/10.1109/ASE.2013.6693089
[6]Jianmei Guo, Dingyu Yang, Norbert Siegmund, Sven Apel, Atrisha Sarkar, Pavel
Valov, Krzysztof Czarnecki, Andrzej Wasowski, and Huiqun Yu. 2018. Data-
Efficient Performance Learning for Configurable Systems. Empirical Software
Engineering 23 (2018), 1826–1867.
[7]Huong Ha and Hongyu Zhang. 2019. DeepPerf: Performance Prediction forConfigurable Software with Deep Sparse Neural Network. In Proceedings of
the International Conference on Software Engineering (ICSE). IEEE, 1095–1106.
https://doi.org/10.1109/ICSE.2019.00113
[8]HuongHaandHongyuZhang.2019. Performance-InfluenceModelforHighly
ConfigurableSoftwarewithFourierLearningandLassoRegression.In Proceedings
oftheInternationalConferenceonSoftwareMaintenanceandEvolution(ICSME).
IEEE, 470–480. https://doi.org/10.1109/ICSME.2019.00080
[9]ChristopherHenard,MikePapadakis,MarkHarman,andYvesLeTraon.2015.
CombiningMulti-ObjectiveSearchandConstraintSolvingforConfiguringLargeSoftwareProductLines.In ProceedingsoftheInternationalConferenceonSoftware
Engineering (ICSE). IEEE/ACM, 517–528. https://doi.org/10.1109/ICSE.2015.69
[10]Herodotos Herodotou, Harold Lim, Gang Luo, Nedyalko Borisov, Liang Dong,
Fatma Bilgen Cetin, and Shivnath Babu. 2011. Starfish: A Self-Tuning System for
Big Data Analytics. In Proceedings of the Conference on Innovative Data Systems
Research (CIDR). www.cidrdb.org, 261–272.
[11]R.CarterHillandLeeC.Adkins.2007. Collinearity. In ACompaniontoTheoretical
Econometrics.JohnWiley&Sons,Ltd,Chapter12,256–278. https://doi.org/10.
1002/9780470996249.ch13
[12]Arthur E. Hoerl and Robert W. Kennard. 1970. Ridge Regression: Biased Es-timation for Nonorthogonal Problems. Technometrics 12 (1970), 55–67. https:
//doi.org/10.1080/00401706.1970.10488634
[13]Matthew D. Hoffman and Andrew Gelman. 2014. The No-U-Turn Sampler:
Adaptively Setting Path Lengths in Hamiltonian Monte Carlo. The Journal of
Machine Learning Research 15 (2014), 1593–1623.
[14]Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013.An Introduction to Statistical Learning. Springer. https://doi.org/10.1007/
978-1-4614-7138-7
[15]PooyanJamshidiandGiulianoCasale.2016. AnUncertainty-AwareApproach
to Optimal Configuration of Stream Processing Systems. In Proceedings of the
InternationalSymposiumon Modeling,AnalysisandSimulationof Computerand
Telecommunication Systems (MASCOTS) . IEEE, 39–48. https://doi.org/10.1109/
MASCOTS.2016.17
[16]Martin Fagereng Johansen, Øystein Haugen, and Franck Fleurey. 2012. An
AlgorithmforGeneratingT-WiseCoveringArraysfromLargeFeatureModels.
InProceedings of the International Software Product Line Conference (SPLC). ACM,
46. https://doi.org/10.1145/2362536.2362547
[17]Christian Kaltenecker, Alexander Grebhahn, Norbert Siegmund, and Sven Apel.
2020. TheInterplayofSamplingandMachineLearningforSoftwarePerformance
Prediction. IEEESoftware 37,4(2020),58–66. https://doi.org/10.1109/MS.2020.
2987024
[18]Christian Kaltenecker, Alexander Grebhahn, Norbert Siegmund, Jianmei Guo,
andSvenApel.2019. Distance-BasedSamplingofSoftwareConfigurationSpaces.
InProceedings of the International Conference on Software Engineering (ICSE).
IEEE, 1084–1094. https://doi.org/10.1109/ICSE.2019.00112
[19]Alex Kendall and Yarin Gal. 2017. What Uncertainties Do We Need in Bayesian
Deep Learning for Computer Vision?. In Proceedings of the International Con-
ferenceonNeural InformationProcessingSystems(NIPS).CurranAssociates Inc.,
5580–5590.
[20]Armen Der Kiureghian and Ove Ditlevsen. 2009. Aleatory or Epistemic? Does It
Matter?StructuralSafety 31(2009),105–112. https://doi.org/10.1016/j.strusafe.
2008.06.020[21]SergiyKolesnikov,JudithRoth,andSvenApel.2014. OntheRelationbetween
Internal and External Feature Interactions in Feature-Oriented Product Lines: A
Case Study. In Proceedings ofthe InternationalWorkshopon Feature-OrientedSoft-
ware Development (FOSD). ACM, 1–8. https://doi.org/10.1145/2660190.2660191
[22]AlanMiller.2002. SubsetSelectioninRegression. CRCPress. https://doi.org/10.
1201/9781420035933
[23]KevinP.Murphy.2012. MachineLearning:AProbabilisticPerspective. MITPress.
[24]VivekNair,TimMenzies,NorbertSiegmund,andSvenApel.2017. FasterDiscov-eryofFasterSystemConfigurationswithSpectralLearning. AutomatedSoftware
Engineering 25 (2017), 247–277. https://doi.org/10.1007/s10515-017-0225-2
[25]VivekNair,ZheYu,TimMenzies,NorbertSiegmund,andSvenApel.2020.Finding
Faster Configurations Using FLASH. Transactions on Software Engineering 46
(2020), 794–811.
[26]RadfordMNeal.1993. ProbabilisticInferenceUsingMarkovChainMonteCarlo
Methods. Department of Computer Science, University of Toronto.
[27]Robert M. O’Brien. 2007. A Caution Regarding Rules of Thumb for Variance
InflationFactors. Quality&Quantity 41(2007),673–690. https://doi.org/10.1007/
s11135-006-9018-6
[28]JehoOh,DonBatory,MargaretMyers,andNorbertSiegmund.2017.FindingNear-
OptimalConfigurationsinProductLinesbyRandomSampling.In Proceedings
of the Joint Meeting of the European Software Engineering Conference and the
ACMSIGSOFTSymposiumontheFoundationsofSoftwareEngineering(ESEC/FSE).
ACM, 61–71. https://doi.org/10.1145/3106237.3106273
[29]Herbert E. Robbins. 1956. An Empirical Bayes Approach to Statistics. In Proceed-
ingsoftheThirdBerkeleySymposiumonMathematicalStatisticsandProbability,
Volume 1: Contributions to the Theory of Statistics. The Regents of the University
of California. https://doi.org/10.1007/978-1-4612-0919-5_26
[30]GeoffreyRoeder,YuhuaiWu,andDavidKDuvenaud.2017. StickingtheLanding:
Simple, Lower-Variance Gradient Estimators for Variational Inference. In Pro-
ceedings of the International Conference on Neural Information Processing Systems
(NIPS). Curran Associates Inc., 6928–6937.
[31]John Salvatier, Thomas V. Wiecki, and Christopher Fonnesbeck. 2016. Probabilis-
tic Programming in Python Using PyMC3. PeerJ Computer Science 2 (2016), e55.
https://doi.org/10.7717/peerj-cs.55
[32]Claude E.Shannon. 1948. A Mathematical Theoryof Communication. The Bell
System Technical Journal 27 (1948), 379–423. https://doi.org/10.1002/j.1538-7305.
1948.tb01338.x
[33]NorbertSiegmund,AlexanderGrebhahn,SvenApel,andChristianKästner.2015.
Performance-Influence Models for Highly Configurable Systems. In Proceedings
of the Joint Meeting of the European Software Engineering Conference and the
ACMSIGSOFTSymposiumontheFoundationsofSoftwareEngineering(ESEC/FSE).
ACM, 284–294. https://doi.org/10.1145/2786805.2786845
[34]Norbert Siegmund, Sergiy S. Kolesnikov, Christian Kastner, Sven Apel, DonBatory, Marko Rosenmuller, and Gunter Saake. 2012. Predicting Performance
via Automated Feature-Interaction Detection. In Proceedings of the International
Conference on Software Engineering (ICSE). IEEE, 167–177. https://doi.org/10.
1109/ICSE.2012.6227196
[35]NorbertSiegmund,MarkoRosenmüller,MartinKuhlemann,ChristianKästner,
Sven Apel, and Gunter Saake. 2012. SPL Conqueror: Toward Optimization of
Non-Functional Properties in Software Product Lines. Software Quality Journal
20 (2012), 487–517. https://doi.org/10.1007/s11219-011-9152-9
[36]Ralph Smith. 2013. Uncertainty Quantification: Theory, Implementation, and
Applications. Society for Industrial and Applied Mathematics.
[37]John R. Taylor. 1997. An Introduction to Error Analysis: The Study of Uncertainties
in Physical Measurements (2nd ed.). University Science Books. https://doi.org/10.
1063/1.882103
[38]RobertTibshirani.1996.RegressionShrinkageandSelectionviatheLasso. Journal
of the Royal Statistical Society. Series B (Methodological) 58 (1996), 267–288.
[39]CatiaTrubianiandSvenApel.2019. PLUS:PerformanceLearningforUncertaintyofSoftware.In ProceedingsoftheInternationalConferenceonSoftwareEngineering:
New Ideas and Emerging Results. IEEE, 77–80. https://doi.org/10.1109/ICSE-NIER.
2019.00028
[40]Dana Van Aken, Andrew Pavlo, Geoffrey J. Gordon, and Bohan Zhang. 2017.
Automatic Database Management System Tuning Through Large-Scale Machine
Learning. In Proceedings of the International Conference on Management of Data
(SIGMOD). ACM, 1009–1024. https://doi.org/10.1145/3035918.3064029
[41]LaurensVanDerMaaten,EricPostma,andJaapVandenHerik.2009. Dimension-
alityReduction:AComparativeReview. JournalofMachineLearningResearch
10 (2009).
[42]Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler
Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser,
Jonathan Bright, Stéfan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jar-
rod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern,
Eric Larson, CJ Carey, İlhan Polat, Yu Feng, Eric W. Moore, Jake Vand erPlas,
DenisLaxalde,JosefPerktold,RobertCimrman,IanHenriksen,E.A.Quintero,
Charles R Harris, Anne M. Archibald, Antônio H. Ribeiro, Fabian Pedregosa,
695Paul van Mulbregt, and SciPy 1. 0 Contributors. 2020. SciPy 1.0: Fundamen-
tal Algorithms for Scientific Computing in Python. Nature Methods 17 (2020),
261–272.
[43]Niklas Werner. 2019. Energy and Performance Evolution of Configurable Systems:
Case Studies and Experiments. Master Thesis. University of Passau.
[44]JeffreyWooldridge. 2012. IntroductoryEconometrics:A ModernApproach (5ed.).
South-Western College Pub.
[45]TianyinXu,LongJin,XuepengFan,YuanyuanZhou,ShankarPasupathy,and
Rukma Talwadker. 2015. Hey, You Have given Me Too Many Knobs!: Under-
standing and Dealing with over-Designed Configuration in System Software. In
Proceedings of the Joint Meeting of the European Software Engineering Conference
and the ACM SIGSOFT Symposium on the Foundations of Software Engineering
(ESEC/FSE). ACM, 307–319. https://doi.org/10.1145/2786805.2786852
[46]YiZhang, JianmeiGuo, EricBlais, andKrzysztof Czarnecki.2015. Performance
Prediction of Configurable Software Systems by Fourier Learning. In Proceedings
oftheInternationalConferenceonAutomatedSoftwareEngineering(ASE).IEEE,
365–373. https://doi.org/10.1109/ASE.2015.15
[47]Yuqing Zhu, Jianxun Liu, Mengying Guo, Yungang Bao, Wenlong Ma, Zhuoyue
Liu, Kunpeng Song, and Yingchun Yang. 2017. BestConfig: Tapping the Per-formance Potential of Systems via Automatic Configuration Tuning. In Pro-
ceedings of the Symposium on Cloud Computing (SoCC) . ACM, 338–350. https:
//doi.org/10.1145/3127479.3128605
[48]HuiZouandTrevorHastie.2017. RegularizationandVariableSelectionviathe
ElasticNet. JournaloftheRoyalStatisticalSociety:SeriesB(StatisticalMethodology)
67 (2017), 301–320. https://doi.org/10.1111/j.1467-9868.2005.00503.x
696