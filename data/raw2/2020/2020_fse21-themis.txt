Benchmarking Automated GUI Testing forAndroid against
Real-WorldBugs
TingSu
ShanghaiKey Laboratory of
TrustworthyComputing,
EastChina NormalUniversity
China
tsu@sei.ecnu.edu.cnJueWang
StateKey LabforNovel Software
Tech.and Dept. of ComputerSci.and
Tech.,Nanjing University
China
juewang591@gmail.comZhendongSu
Departmentof ComputerScience,
ETHZurich
Switzerland
zhendong.su@inf.ethz.ch
ABSTRACT
ForensuringthereliabilityofAndroidapps,therehasbeentremen-
dous, continuous progress on improving automated GUI testing in
thepastdecade.Specifically,dozensoftestingtechniquesandtools
have been developed and demonstrated to be effective in detecting
crash bugs and outperform their respective prior work in the num-
ber of detected crashes.However,an overarching question ≈Ç How
effectively and thoroughly can these tools find crash bugs in prac-
tice?≈æhasnotbeenwell-explored,whichrequiresaground-truth
benchmark with real-world bugs. Since prior studies focus on tool
comparisons w.r.t.someselectedapps,they cannotprovidedirect,
in-depthanswers to this question.
To complement existing work and tackle the above question,
this paper offers the first ground-truth empirical evaluation of
automated GUI testing for Android. To this end, we devote sub-
stantialmanualeffort toset upthe Themisbenchmarkset, includ-
ing (1) a carefully constructed dataset with 52 real, reproducible
crashbugs(takingtwoperson-monthsforitscollectionandvali-
dation),and (2) aunified,extensibleinfrastructure withsix recent
state-of-the-art testing tools. The whole evaluation has taken over
10,920CPUhours.Wefindaconsiderablegapinthesetoolsfind-
ing the collected real bugs √ê 18 bugs cannot be detected by any
tool. Our systematic analysis further identifies five major common
challenges that these tools face, and reveals additional findings
such as factors affecting these tools in bug finding and opportu-
nities for tool improvements. Overall, this work offers new con-
crete insights, most of which are previously unknown/unstated
anddifficulttoobtain.Ourstudypresentsanew,complementary
perspective from prior studies to understand and analyze the effec-
tiveness of existing testing tools, as well as a benchmark for future
researchonthistopic.The Themisbenchmarkispubliclyavailable
athttps://github.com/the-themis-benchmarks/home .
CCS CONCEPTS
¬∑Software and its engineering ‚ÜíSoftware testing and de-
bugging.
Permissionto make digitalor hard copies of allorpart ofthis work for personalor
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACM
mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,
topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ESEC/FSE ‚Äô21, August 23≈õ28,2021, Athens,Greece
¬©2021 Associationfor Computing Machinery.
ACM ISBN 978-1-4503-8562-6/21/08...$15.00
https://doi.org/10.1145/3468264.3468620KEYWORDS
GUI testing,Android apps, Crash bugs,Benchmarking.
ACMReference Format:
Ting Su, Jue Wang, and Zhendong Su. 2021. Benchmarking Automated GUI
Testing for Android against Real-World Bugs. In Proceedings of the 29th
ACM Joint European Software Engineering Conference and Symposium on
theFoundations ofSoftware Engineering(ESEC/FSE‚Äô21), August 23≈õ28, 2021,
Athens,Greece. ACM,NewYork,NY,USA, 12pages.https://doi.org/10.1145/
3468264.3468620
1 INTRODUCTION
Androidappstypicallyrunincomplexend-userenvironmentspost-
deployment. Ensuringtheir reliability and correctness√ê avoiding
fatal crashes in particular √ê is thus a top priority of any app devel-
opmentteam.SincethefirsteffortbyHuandNeamtiu[ 13]in2011,
tremendous and continuous efforts have been made to improve
automatedGUItestingforAndroid[ 19,47,49],whichcomplement
thecommonly-adoptedmanualtestinginthisfield[ 18,48].Specifi-
cally,dozensofautomatedGUItestingorfuzzingtools( e.g.,[5,9,
12,15,24,25,29,31,37])havebeendevelopedanddemonstratedto
beeffectiveindetectingcrashbugsandoutperformtheirrespective
prior work inthe number of detectedcrashes.
Thus,anoverarchingquestionis≈Ç Howeffectivelyandthoroughly
canthesetoolsfindcrashbugsinpractice? ≈æ.Toanswerthisquestion,
an ideal approach is to directly assess these tools against a ground-
truth benchmark with real-world bugs, and check how many bugs
are found or missed by a given tool. Indeed, such a benchmark-
ing approach is well-justified and widely-adopted in practice for
evaluating software testing or analysis tools [ 17],e.g., LAVA [8],
Defects4J [ 16] and DeCapo [ 4]. It has two key benefits : (1)enabling
many direct, in-depth analyses (e.g., analyzing the false negatives
and common weaknesses of tools), and (2) consolidating the evalua-
tionvalidity (e.g.,avoidingsuchfalsepositivesasbugovercounting
duetotheimprecisionofbugde-duplicationstrategies).Incontrast,
evaluating testing tools against onlyapps (without known bugs) is
difficult to obtain such benefitsif not impossible.
Ontheotherhand,noeffortexistsintheliteratureyettoanswer
theaforementionedquestionagainstground-truth.Forexample,by
investigating the recent literature reviews [ 19,47,49] and relevant
publications in this field, we identified 32 research papers that
proposeautomated testingtechniquesfordetecting crashbugs in
Androidapps.However,noneevaluatedtheproposedtechniques
against real-world bugs, and all compare tools w.r.t.some selected
appsalone.Similarly,allpriorrelevantempiricalstudies[ 3,7,32,
50,52] in this field also use only apps (without known bugs) to
119ESEC/FSE ‚Äô21, August 23‚Äì28, 2021,Athens,Greece Ting Su,Jue Wang,Zhendong Su
Table 1: Key differences between the prior relevant studies and ours in evaluating automated testing techniques for Android
(≈Ç‚úì≈æ,≈Ç‚úó≈æand≈Ç?≈ædenotethatthestudycan ,cannotorcan onlypartially giveanswers,respectively,and≈ÇN/A≈æisnotapplicable).
Studies VenueEvaluationBasis AnalysisBasis NewStudyInsights
#Tools BasisHas the
ground-truth?BasisArecrashes
known and
reproducible?Examine tool
implementations?Discuss/confirm
withtool authors?Quantifying bug
findingabilities
(RQ1)Common
challenges
(RQ2)Factorsand
Opportunities
(RQ3)
Choudhary etal.[7]ASE‚Äô15 multiple apps ‚úó #crashes, coverage ‚úó ‚úó ‚úó ‚úó ? ‚úó
Wangetal.[50] ASE‚Äô18 multiple apps ‚úó #crashes, coverage ‚úó ‚úó ‚úó ‚úó ‚úó ‚úó
Zhengetal.[52]ICSE‚Äô17-SEIP one one app ‚úó coverage N/A ‚úó ‚úó ‚úó ? ‚úó
Behrangetal.[3]ASE‚Äô20 one apps ‚úó coverage N/A ‚úó ‚úó ‚úó ? ‚úó
Our study ESEC/FSE‚Äô21 multiple realbugs‚úì real-world bugs ‚úì ‚úì ‚úì ‚úì‚úì‚úì
evaluate testing tools. Therefore, we still do not have direct, in-
depthanswers to the aforementionedquestion.
Ourworkaimstofillthisgapbyprovidingthefirstground-truth
evaluation of existing testing techniques for Android. To clearly
present the necessity and novelty of our study, Table 1summarizes
the key differences between the prior relevant studies [ 3,7,50,52]
and ours. These differences show that our study presents a new,
complementary perspective from prior studies.
Specifically,thestudiesbyChoudhary et al.[7]andWang et al.[50]
compare one tool to another based on some selected apps in terms
ofthe numbers of found unique crashes and achieved code coverage.
However,duetothelackofground-truth,wecannotanalyzethe
falsenegativesofthesetoolsonacommonbasis.Asaresult,quanti-
fying the degree of effectiveness of these tools becomes difficult. In
practice, these two studies face two additional challenges. First, ex-
istingtestingtoolsforAndroidcannotreliablyprovidereproducible
tests for found crashes (see Section 5 in [ 7] and Section 3.5 in [ 36])
due to the open technical challenges like GUI flakiness [ 27,46]
andlengthytests[ 6].Asaresult,itisdifficulttoanalyzethebug
features,thusunabletoofferfine-grainedanalysesonthetools‚Äôbug
finding abilities. Second, existing tools heuristically de-duplicate
crashes by hashing stack traces, which is difficult to make reliable,
thus likely incurringbug-overcounting[ 17].
ThestudiesbyZheng et al.[52]andBehrang et al.[3]investigate
thetoollimitationsbyanalyzingtheuncoveredcodeofoneormore
apps,respectively.Butthesetwostudiescannotgivedirectanswers
totheraisedquestion,becausetheyonlyfocusoncodecoverage,
whichisaproxyindicatorofbugfindingabilitiesandthecorrelation
could be weak [ 14] (our study also observes this in Section 4.2).
Moreover, they only evaluate one tool, Monkey [ 29]. Thus, the
generabilityofthe identifiedtoollimitationsisunclear.
Toachieveourstudy,oneimportantstepistosetupaground-
truthbenchmarkwithreal-worldbugsbasedonanagreed-uponcri-
terion.Tothisend,weresorttotheindustrialpractitionersforgain-
ing insights. Specifically, we contact 8 senior app testing managers
and engineers (with 3 ‚àº10 years‚Äô working experience) from five
well-knowncompanies, i.e.,Google,Facebook,Tencent,ByteDance
andTestin(amajormobileapptestingserviceproviderinChina)
withinournetworks.Theirteams areresponsiblefor testingtheir
own apps (like Google Pay, Messenger, WeChat and TikTok which
have billions of monthly active users worldwide) or the apps from
differentvendors.Weconductindependenton-lineinterviewswith
thempercompany,andaskthem5preparedandsomefollow-up
questionsto fully understandtheirtestingpractice.
Finally, all the interviewees respond that in practice they assign
priority labels to the bugs reported by in-house testing or app
users,andtheyprioritize criticalbugs (namelyimportantbugs )√ê
the bugs that break the major app functionalities and affect the
largerpercentageofappusers(inpractice,realtimecrashreportingplatformsareusedtotrackcrashissuesfromend-users).Inother
words,criticalbugsaremorelikelytoaffectmoreusersinreality.
Alltheintervieweesindicateandagreethattheabilityoffinding
critical bugs isan objective metric tomeasure the effectiveness of
testing tools in practice. Thus, we decide to choose critical bugs
astheagreed-uponcriteriontosettingupthebenchmark.Infact,
such ability has already been strongly and widely advocated for
evaluating testingtoolsinboth industry andacademia [ 26,34].
To this end, we take three steps to approach this study. First,
wechooseopen-sourceappsasthetargetstocollectcriticalbugs
because their issuerepositories are public. Specifically, we desig-
natetheimportanceofbugs w.r.t.theirissuelabelsassignedbyapp
developersthemselves.Wecollectthebugswithcriticalissuelabels
likehigh-priority ,blocking-release ,P1-urgent .Wefinallyconstruct
a dataset of 52 real bugs from 20 open-source Android apps by
crawling the issue repositories of 1,829 Android apps. This pro-
cesstookussubstantialmanualeffort(nearly twoperson-months )
thatcouldnotbeautomated.Itinvolvesmanuallyreviewingbug
reports,locatingbuggycodeversions,buildingappbinaries,and
reproducing andvalidating bugs.Section 3.1details this step.
Second,werigorouslysetupaunified,extensibleexperimental
infrastructure,andintegrate Monkey [29],thestate-of-the-practice
testingtool,andfivemostrecentstate-of-the-artonesforthorough
evaluation, namely Ape[12],Humanoid [21],ComboDroid [15],
TimeMachine [9] andQ-testing [31]. Specifically, we run these
toolsonthecollectedbugs,andprofiledifferentmetrics:thenumber
of bugs they can find ( i.e.,effectiveness ), how many times theycan
trigger a bug givena number of runs ( i.e.,stability), and howlong
theytaketotriggerabug( i.e.,efficiency).Section 3.2detailsthisstep.
We name our dataset and infrastructure as the Themisbenchmark,
whichaims for an objective evaluation w.r.t.ground-truth.
Finally,wegivethedetailedquantitativeandqualitativeanalysis
onthe testingresults ofthesetoolsbyreviewing the bugfeatures,
examiningthesetools‚Äôimplementations,anddiscussing/confirming
with the tool authors. We identify the common challenges that ex-
isting tools face and the factors that affect bug finding, which have
notbeenwell-identifiedbythepriorstudies.Inparticular,weinves-
tigatedthe following research questions(answeredinSection 4):
‚Ä¢RQ1:Howeffectivelyandthoroughlycanthesetestingtoolsfind
thecollected real-world bugs?
‚Ä¢RQ2:Are there any common challenges that allthese tools face in
finding thesebugs(byanalyzing the common false negatives)?
‚Ä¢RQ3:Arethereanyfactorsaffectthesetoolsinfindingthesebugs
(bypair-wisely analyzing the testing results of these tools)? What
aretheopportunitiesfor improvingthe state-of-the-arts?
Summary of main findings . Out of 52 bugs, 18 ( ‚âà34.6%) bugs
cannotbe detected byany testing tool,whichindicates thata con-
siderable gap exists between the existing tools and the collected
120BenchmarkingAutomatedGUI TestingforAndroid against Real-WorldBugs ESEC/FSE ‚Äô21, August 23‚Äì28, 2021,Athens,Greece
Table 2:Summary oftheselected automated GUItestingtools forAndroid inour study.
Tool Name Venue Open Source MainTechniqueNeedApp
Source Code?NeedApp
Instrumentation?Supported
SDKsTool VersionImplementation
Basis
Monkey [29] -‚úì Random testing ‚úó ‚úó Any default -
Ape[12] ICSE‚Äô19‚úì Model-based ‚úó ‚úó 6.0/ 7.1 a53e98c Monkey-based
Humanoid [21]ASE‚Äô19‚úì Deeplearning-based ‚úó ‚úó Any c494c7d DroidBot -based
ComboDroid [15]ICSE‚Äô20‚úì Model-based ‚úó‚úì 6.0/ 7.1 567b3f6 Monkey-based
TimeMachine [9]ICSE‚Äô20‚úì State-based ‚úó‚úì 4.4/ 7.1 e79beb5 Monkey-based
Q-testing [31]ISSTA‚Äô20 ‚úó Reinforcementlearning-based ‚úó ‚úó 4.4/ 7.1/ 9.0 045825a -
real-world bugs. Specifically, these 18 bugs impose five common
major challenges blocking any tool, e.g.,deep use case scenarios ,
changes of system/app settings , andspecific user interaction patterns .
It indicates that continuous, long-term research effort is needed to
tackle these challenges (Section 4.2). On the other hand, the gap
is larger when these tools are applied individually √ê they miss a
large portion (53.8 ‚àº71.2%) of bugs, although we indeed observe
their unique advantages in finding specific bugs (Section 4.1). Also,
we find these tools have obvious randomness in triggering bugs,
and no one can absolutely outperform the others in bug finding.
By pairwise comparisons, we find that their testing results are
largelyaffectedbythe GUIexplorationstrategies ,stateabstraction
criteria,andsmallheuristics ,whicharetheopportunitiesfortool
improvement in the short-term (Section 4.3). Table6in Section 4.4
summarizestheconcretenewinsightsweobtainedfromthisstudy,
mostofwhichare unknown/unstatedanddifficult to obtain.
Contributions .Our study makesseveral contributions:
‚Ä¢Ittakesthe firststeptoconductan empiricalstudy againstreal-
world bugs to evaluate GUI testing tools for Android, which
presents anew,complementary perspective from prior studies.
‚Ä¢It carefully setups the Themisbenchmark, including the first
ground-truth dataset of 52 real, reproducible crash bugs and a
unified,extensibleinfrastructure,to achieve this study.
‚Ä¢It gives in-depth quantitative and qualitative analysis on the
testingresults.Itobtainsnewconcretefindings,mostofwhich
wereunknown/statedbefore.Italsomotivatesthefutureresearch
onthis topic withabenchmark(discussedinSection 4.4).
2 TESTINGTOOLSFOROURSTUDY
Table2liststheselectedtoolsforourstudy.Notethatweusethe
latest versionsofthesetoolsat the time ofour study.
Monkey .Monkey [29]isapurerandomtestingtool.Inprinciple,
Monkeyemitspseudo-randomstreamsofUIevents( e.g.,touch,ges-
tures, randomtexts)andsomesystemevents ( e.g., volume controls,
navigation). Monkey iswidely-usedinindustryforstress-testing
because it is easy-to-use and compatible with any Android version.
Itisapopular baselineto evaluate newtestingtechniques.
Ape.Ape[12] is a novel model-based GUI testing tool. Different
frompriormodel-basedtestingtoolslikeStoatwhichusestaticGUI
abstractioncriteria, Apeusestheruntimeinformationtodynami-
cally evolve its abstraction criterion via a decision tree, which can
effectively balance the size and precision of the model. Specifically,
with this dynamically refined model, Apegenerates UI events via a
randomandgreedydepth-firststateexplorationstrategy.Moreover,
Apealso internally utilizes Monkey to occasionally emits random
UIeventsandsystemeventsto avoid stucking at local states.
Humanoid .Humanoid [21]isthefirstdeeplearning-basedtest-
ing tool. The core is a deep neural network model that predicts
whichUIelementsonthecurrentGUIpagearemorelikelytobeinteracted with by users and how to interact with it. The model
was trained upon a large-scale crowd-sourced human interactions
dataset.Humanoid is expected to drive the GUI exploration to-
wardsimportantstatesfasterasitprioritizesUIelementsaccording
to their importance andmeaningfulness like a human. Humanoid
isbuiltonDroidBot[ 20],alightweight,model-basedGUItesting
tool, which received 500+ stars on GitHub at the time of our study.
ComboDroid .ComboDroid [15]isanovelmodel-basedtesting
tool. Its core idea is to generate long and meaningful event se-
quences by combining a number of short, independent use cases,
to explore deep app states. ComboDroid obtains such use cases
either from humans or automatically generates from a GUI model
constructed by GUI exploration. It then analyzes the data-flow and
GUI-transition relations between obtained use cases, and combines
them (i.e.,concatenatingusecases inspecificorders) to generate
final tests. Moreover, it works in a feedback loop, i.e., generating
additionaluse caseswhen prior tests reachednewapp states.
TimeMachine .TimeMachine [9] is a novel state-based testing
tool.Differentfrompriortoolslike Sapienz[25]andStoat[37]that
evolve event sequences to maximize code coverage, TimeMachine
instead evolves a population of states which can be captured upon
discoveryandresumedwhenneededforfindingdeeperrors.During
testexecution,itscoreistotakeasnapshotofevery interestingstate
and add into the state corpus, and travel back to a most progressive
stateand execute next test when the current exploration cannot
reachnewinterestingstates.Itsuniquenessistheabilitytosnapshot
and resume specific app state for further testing via the underlying
Android-basedvirtual machine.
Q-testing .Q-testing [31]isareinforcementlearning-basedtest-
ingtool.ItusesatrainedneuralnetworktocompareGUIpages.Ifa
page is similar to any of prior explored GUI pages, the comparator
will give a small reward. Otherwise, the comparator will give a
large reward. These rewards are used and iteratively updated to
guide the testingto cover more functionalities of apps.
Sapienz andStoat. We also evaluated SapienzandStoat, al-
though the tools in Table 2outperform them. Sapienzuses genetic
algorithms, while Stoatuses the stochastic model learned from
an app to optimize test suite generation. Despite Sapienzis closed-
source and only compatible with Android 4.4, we still include it
because itiswell-knownandits techniqueisunique.
3 EXPERIMENTALSETUP
3.1Themis‚ÄôsDataset
Collect open-source apps . We chose the open-source Android
appsonGitHubasthemainsourceofcollectingreal-worldbugs.To
include as many candidate apps as possible, we use two strategies:
(1) We crawled all the apps from F-Droid [ 42], the largest open-
source app market, because most of these apps are maintained on
121ESEC/FSE ‚Äô21, August 23‚Äì28, 2021,Athens,Greece Ting Su,Jue Wang,Zhendong Su
GitHub. (2) We used the keyword ≈ÇAndroid≈æ and AndroidMani-
fest.xml,theuniquefileofanyAndroidproject,tocollectmissing
Android apps that are only maintained on GitHub but not released
onF-Droid.We finally got 1,829 unique Android apps onGitHub.
Filter apps with critical issues . We designate the importance
of bugsw.r.t.the issue labels assigned by developers. To collect
as many critical issues as possible, we built a GitHub API [ 43]
based crawler to collect all the issue labels from 1,829 candidate
apps, and manually identified 111 different labels denoting criti-
cal issues.Then,weextracted 12 shorten forms ofkeywordsfrom
these 111 labels for matching concrete issue labels. For example,
we use ≈Çblock≈æ to match ≈Çblocking-release≈æ, ≈Çblocked≈æ; ≈Ç sever≈æ to
match ≈Çseverity-high≈æ, ≈Çseverity: crash≈æ; ≈Ç pri≈æ to match ≈Çhigh prior-
ity≈æ,≈ÇPriority-Critical≈æ,≈ÇMajorpriority≈æ;≈Ç urgen≈ætomatch≈Çurgency:
HIGH≈æ,≈Çp1-urgent≈æ;≈Ç importan≈ætomatch≈Çimportant!≈æ,≈ÇP2:Very
Important≈æ, ≈ÇBUG: High Importance≈æ etc. We find these shorten
formsofkeywordscaneffectivelyreducethefalsenegativesofcrit-
icalissues.Byfilteringthoseappswhoseissuelabelscontainone
ofthese12 shortenforms ofkeywords,200valid apps remained.
Fromtheaboveresults,wefindmanyappsdonothavecritical
issuelabels.To furtheravoidmissingcriticalissues,wecontinued
to scan the remaining 1,629 apps by checking whether any issue
whosetitle,bodyorcommentscontainthekeywordssuchas≈Çblock≈æ,
≈Çsevere≈æ, ≈Çcritical≈æ, ≈Çmajor≈æ, ≈Çurgent≈æ, ≈Çimportant≈æ, ≈Çheavy≈æ (derived
fromthe12shortenformsofkeywords).Wegot209validappswith
such issues.Thus, we obtained409(=200+209) valid apps intotal.
Collect raw data of critical issues . Based on the previous data,
we manually inspected each issue of the 200 apps with explicit crit-
icalissuelabels,andtheissuesofthe209appswhichhavematched
keywords.Specifically,acandidateissueforourstudyshouldsat-
isfythesecriteria:(1)wasacrashbugwiththekeywords≈Çcrash≈æor
≈Çexception≈æ;(2) haveexplicitreproducingsteps; (3)wassubmitted
after 1st Jan, 2017 to avoid apps that could have outdated depen-
dencies.Wefinallygot228criticalissuesfrom51apps. Manyissues
were excluded because the bug reports do not have clear reproducing
steps and thecorrespondingappwasoutdated foralong time.
Validateandarchive criticalissues .Wemanuallycheckedand
validatedeachofthese228criticalissues.Thetypicalprocessis:(1)
reviewingandunderstandingthebugreport,(2)locatingthebuggy
codeversion,(3)buildingandinstrumentingthebuggyappversion,
(4)reproducingthebug,and(5)archivingthebugdata.Notethatin
practice we often have to iterate between step (2) and (4). Because
many bug reports are not well-formatted ( e.g., missing buggy code
version or code fixing commits), we have to manually locate the
rightversionbytrialanderroruntilwecanreproducethedescribed
bug.Moreover, buildingappsis verytime-consumingbecausewe
usuallyhavetoresolveoutdatedormissingdependenciesandsetup
necessary building environments ( e.g., local servers). Reproducing
bugsalsotakestimebecausewehavetolinkthestepstoreproduce
in text with the app functionalities in GUIs. Many bug reports are
not well-written; and many apps do not have clear documentation.
Duringthis process, anissue wouldbe excludedif(1)we cannot
fullyunderstandthebugreport;(2)thebuggyappversioncannotbe
located;(3)thebuggyappversioncannotbebuiltintoanexecutable
APK;(4)theissuecannotbefaithfullyreproducedonAndroid7.1
(the version supported by the selected tools), e.g., the backendserverwasobsoleted,thebugswereconcurrencyorcompatibility
issues; and (5) the issue is deadly simple ( e.g., start-up crashes).
In addition, we excluded an issue if its corresponding app is not
≈Çself-contained≈æ, i.e., testing such an app requires the non-trivial
collaborationswithhumansorotherdevices.Forexample,aGitHub
client app was excluded because none of existing GUI testing tools
can automatically test it without any appropriate, complicated app
datapreparation( e.g.,manuallycreatingasampleprojectrepository
withproper code commits, issues,branches andotherinfo).
Inourexperience,itusuallytook1 ‚àº4hourstovalidateoneissue
without the guarantee of success. We spent nearly two person-
monthsonvalidatingthe228issues,andobtained52validcriticalis-
suesfrom20apps.Foreachsuccessfullyvalidatedissue,wearchived
itscorrespondingbugdata,whichincludes(1)anexecutableAPK
file (Jacoco-instrumented), (2) a bug-reproducing video, (3) the ex-
ception stack trace, and (4) other necessary information ( e.g., login
script). Table 3lists these 52 critical crash bugs. It gives the app
name,issueid,appfeature,codeversion,numberofstarsonGitHub,
linesofcode(LOC),numberofstepstoreproduce(#STR)andother
bug information ( e.g., whether it needs network access, account
loginorsystemsettingchangesforreproducing).Notethat#STR
denotes the number of shorteststeps observed by us, and does not
include the steps to login orchangeexternal systemsettings.
Discussion . Note that the 20 apps in Table 3have diverse features
and many of them are highly-starred. Thus, these apps could serve
a good basis for evaluation. On the other hand, all these 52 bugs
canbedeterministicallyreproducedbyaGUItestinourevaluation
setting,i.e., an ideal testing tool could find each of them. Thus,
these bugs provide a fair basis for all testing tools. We note that
somepriorwork[ 11,36,39,51]providescrashbugdataset.Butwe
didnot reusethosedatasets.Because thosebugsareselected only
based onwhether thebugreports describe bug-reproducingsteps
rather thanthe agreed-uponcriterion of critical bugsin our study.
3.2Themis‚ÄôsInfrastructure
We built a unified, extensible infrastructure for our study. Any
testing tool can be integrated into this infrastructure and deployed
onagiven machine withone lineof command:
Themis:themis --avd avd_name -n dev_cnt --apk apk_name
-o output_dir --time testing_time --repeat run_cnt
--tool tool_name [--login login_script] [--gui]
[--check_crash] [--coverage]
Onecanspecifythetargetdevice( avd_name ),sizeofdevicepool
(dev_cnt), target app ( apk_name ), testing time ( testing_time ),
number of runs ( run_cnt), the target testing tool ( tool_name ), au-
tomaticlogin(via UiAutomator -basedscripts[ 45]),showingGUI
screens, checking crashesanddumpingcoverageat runtime.
Efforts under the hood . To build this infrastructure, we took
considerable time to coordinate with the authors of the selected
tools to assure correct and rigorous setup. We tried our best efforts
tominimize thebias andensure that eachtoolis at≈Çits best state≈æ
inbugfinding. We detailour effortsoneachtoolas follows.
Ape.Wespentslighteffortstosetup Ape,butaroundtwoweeks
tocoordinatewiththetoolauthorstoensureitsusability.Forex-
ample,weobserve Apefrequentlythrows OutOfMemory andNo
DiskSpace errorswhengivenalongrunningtime.Toresolvethese
122BenchmarkingAutomatedGUI TestingforAndroid against Real-WorldBugs ESEC/FSE ‚Äô21, August 23‚Äì28, 2021,Athens,Greece
Table3:Listof52real-world,reproduciblecrashbugs.≈Ç#STR≈ædenotesthenumberofshorteststepstoreproduce.≈ÇN≈æ,≈ÇL≈æand
≈ÇS≈æ denote whether reproducingthebugrequiresnetworkaccess,account loginandsystem settingchanges, respectively.
AppName Issue Id AppFeatureCode
Version#GitHub
StarsLOC #STR NLS‚àÖMAHCTQSaSt
ActivityDiary #118 personal diary 1.1.8 58 2,011 9 ‚úó -
ActivityDiary #285 personal diary 1.4.0 58 4,250 8 ‚òÖ -‚òÖ
AmazeFileManager #1232 filemanager 3.2.1 3.2K 16,969 8 ‚úó -
AmazeFileManager #1558 filemanager 3.3.2 3.2K 19,456 6 ‚úó -
AmazeFileManager #1796 filemanager 3.3.2 3.2K 19,457 9 ‚òÖ‚òÖ‚òÖ‚òÖ ‚òÖ
AmazeFileManager #1837 filemanager 3.4.2 3.2K 21,070 3 ‚òÖ‚òÖ -
and-bible #261 biblestudy 3.0.286 259 16,162 17‚úì ‚úó -
and-bible #375 biblestudy 3.1.309 259 16,611 25‚úì ‚òÖ
and-bible #480 biblestudy 3.2.327 259 17,205 14‚úì ‚òÖ ‚òÖ
and-bible #697 biblestudy 3.2.369 259 20,225 16‚úì ‚òÖ -
and-bible #703 biblestudy 3.3.377 259 20,301 10‚úì ‚òÖ -
AnkiDroid #4707 flashcardlearning 2.9 3.4K 29,390 5 ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ -‚òÖ
AnkiDroid #4200 flashcardlearning 2.6 3.4K 28,572 10 ‚úó
AnkiDroid #5638 flashcardlearning 2.9.1 3.4K 29,298 4 ‚úó -
AnkiDroid #4451 flashcardlearning 2.7 3.4K 28,673 17 ‚òÖ‚òÖ ‚òÖ
AnkiDroid #6145 flashcardlearning 2.10 3.4K 29,874 17 ‚úì ‚úó
AnkiDroid #5756 flashcardlearning 2.9.4 3.4K 29,657 15 ‚òÖ‚òÖ‚òÖ‚òÖ ‚òÖ‚òÖ‚òÖ
AnkiDroid #4977 flashcardlearning 2.9 3.4K 29,775 2 ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ ‚òÖ
APhotoManager #116 photo manager 0.6.4 148 8,969 2 ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ
collect #3222 onlineform 1.23.0 528 25,946 7‚úì ‚òÖ‚òÖ‚òÖ ‚òÖ ‚òÖ
commons #3244 wiki media 2.11.0 661 20,069 8‚úì‚úì ‚úó -
commons #2123 wiki media 2.9.0 661 15,540 5‚úì‚úì ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ-‚òÖ
commons #1391 wiki media 2.6.7 661 9,182 10‚úì‚úì ‚úó -
commons #1385 wiki media 2.6.7 661 9,221 6‚úì‚úì ‚úó -
commons #1581 wiki media 2.7.1 661 9,287 6‚úì‚úì‚úì ‚úó -
FirefoxLite #4881 webbrowser 2.1.12 251 13,626 7‚úì ‚òÖ‚òÖ ‚òÖ‚òÖ
FirefoxLite #5085 webbrowser 2.1.20 251 12,060 5‚úì ‚òÖ‚òÖ‚òÖ‚òÖ ‚òÖ
FirefoxLite #4942 webbrowser 2.1.16 251 13,661 5‚úì ‚òÖ ‚òÖ ‚òÖ
Frost-for-Facebook #1323 facebookwrapper 2.2.1 377 7,749 5‚úì‚úì‚úì ‚òÖ -
geohashdroid #73 geohashingapp 0.9.4 13 5,275 2‚úì ‚òÖ‚òÖ‚òÖ‚òÖ ‚òÖ ‚òÖ
MaterialFBook #224 facebookclient 4.0.2 122 1,740 1‚úì‚úì ‚òÖ ‚òÖ -
nextcloud #5173 file-sharingapp 3.10.0 2.3K 36,589 5‚úì‚úì ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ-‚òÖ
nextcloud #4026 file-sharingapp 3.6.1 2.3K 32,798 3‚úì‚úì ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ-‚òÖ
nextcloud #4792 file-sharingapp 3.9.2 2.3K 35,302 9‚úì‚úì ‚úó
nextcloud #1918 file-sharingapp 2.0.0 2.3K 28,505 3‚úì‚úì ‚òÖ‚òÖ‚òÖ‚òÖ -‚òÖ
Omni-Notes #745 notebookapp 6.1.0 2.1K 8,882 10 ‚òÖ‚òÖ‚òÖ ‚òÖ
open-event-attendee #2198 open event app 0.5 1.5K 6,624 7‚úì ‚òÖ‚òÖ‚òÖ -‚òÖ
openlauncher #67 home screen app 0.3.1 256 5,591 2‚úì ‚òÖ‚òÖ ‚òÖ‚òÖ -
osmeditor4android #729 mapeditor 11.0.0.8 196 36,887 11‚úì ‚òÖ
osmeditor4android #637 mapeditor 0.9.10 196 32,594 30‚úì ‚úó -
Phonograph #112 musicplayer 0.15.0 2.4K 10,800 1 ‚úó
Scarlet-Notes #114 notebookapp 6.9.5 300 2,860 12 ‚òÖ
sunflower #239 gallery app 0.1.6 12K 1,687 4 ‚òÖ ‚òÖ ‚òÖ
WordPress #8659 blog manager 11.3 2.5K 68,171 10‚úì‚úì ‚úó -
WordPress #7182 blog manager 9.2 2.4K 59,571 2‚úì‚úì ‚úó
WordPress #6530 blog manager 8.1 2.4K 54,211 24‚úì‚úì ‚úó
WordPress #11992 blog manager 14.9 2.4K 67,784 6‚úì‚úì ‚òÖ‚òÖ -
WordPress #11135 blog manager 13.6 2.4K 64,499 9‚úì‚úì ‚òÖ‚òÖ‚òÖ‚òÖ -‚òÖ
WordPress #10876 blog manager 13.7 2.4K 64,564 5‚úì‚úì ‚úó -
WordPress #10547 blog manager 13.3 2.4K 64,795 10‚úì‚úì ‚òÖ -
WordPress #10363 blog manager 13.1 2.4K 72,387 3‚úì‚úì ‚òÖ‚òÖ ‚òÖ-‚òÖ
WordPress #10302 blog manager 12.9 2.4K 72,202 2‚úì‚úì ‚òÖ‚òÖ ‚òÖ‚òÖ‚òÖ-‚òÖ
#Total 52 18222418211510319
issues, we discussed with the tool authors, and finally reached the
consensus thatallocating 2GBRAM, 1 GBinternal storage and 1
GB external SDCard storage for the Android devices could greatly
mitigate this issue. The reason is that Apemaintains all GUI states
in memory and dumps large output files and logs. Thus, we also
assigned the similar hardware setup for other tools under study to
ensure a fair basis. In addition, during the early stage of our study,
Apefrequentlycrashedonanumberofappsinourdataset.Thus,
wereportedalltheencounteredissues;andthetoolauthorsfixed
allthoseissuesbefore our deployment.
Humanoid . We spent around three days to setup Humanoid .
The main effort goes to setting up the compatible TensorFlow
versionandresolving outdated librarydependencies. Othereffort
includes fixing some obvious implementation bugs in DroidBot
(whichHumanoid wasbuilton) that affectedthe usability.
ComboDroid .Wespentaroundoneweektocoordinatewith
the tool authors to adapt ComboDroid into our infrastructure.For example, to meet our requirements, the tool authors modi-
fiedComboDroid to(1)supportrunningmultipletoolinstancesin
parallel,and (2) provide separate tool modules to support our login
scripts. During the early stage of our study, we reported some tool
crash issues because ComboDroid may fail to instrument some
apps bySoot.They fixedallthe issuesbefore our deployment.
TimeMachine .Wespentaroundtwoweekstoadaptthetool
into our infrastructure. We made three major modifications, which
werelaterverifiedbythetoolauthorsbeforeourdeployment.(1)
TimeMachine requirescodecoverageinformationforrunning.But
its original EMMAbased coverage collection module cannot work
on recent Android apps (created after 2015) which only support
Jacoco[44]basedcoverageprofiling.Thus,wereplace EMMAwith
Jacoco.(2)Because TimeMachine usesVirtualBoxtosnapshotand
resumeemulatorstates,allthetime-sensitiveinformationwillbe
lostorimprecise.Thus,weaddedanadditionalmoduletoenable
profiling time-sensitive information ( e.g., the time duration to trig-
geracrashbug).(3)Weenhanced TimeMachine tosupportparallel
123ESEC/FSE ‚Äô21, August 23‚Äì28, 2021,Athens,Greece Ting Su,Jue Wang,Zhendong Su
Table 4: Results of bug finding for the selected tools.
≈Ç#Found Bugs≈æ denotes the total number of bugs found by
eachtool.≈Çn/5≈æ(0 ‚â§ùëõ‚â§5)denotesthebreakdown,i.e.,which
bugswerefoundin ùëõrunsoutofthefiveindependentruns.
MAHCTQSaSt
#FoundBugs 222418211510319
5 / 5 610744224
4 / 5 32143302
3 / 5 34240213
2 / 5 52451104
1 / 5 56447206
0 / 5 3028343137424933
runningonservermachines,automaticloginscripts,Googleser-
vice apps (required by some apps in our dataset), and fixed several
obvious implementationissuesto improve its usability.
Other tools . It is easy to setup Monkey andQ-testing . We
spent around one week to setup SapienzandStoatby supporting
parallelrunning andresolvingsomeusabilityissues.
3.3 ExperimentalSetup
We deployed our experiment on a 64-bit Ubuntu 18.04 machine
(64 cores, AMD 2990WX CPU, and 128GB RAM). We evaluated all
theselectedtoolsonGoogleAndroid7.1emulators(APIlevel25).
Each emulator is configured with 2GB RAM, 1GB SDCard, 1GB
internal storage, and X86 ABI image. Different types of external
files (including PNGs/MP3s/PDFs/TXTs/DOCXs) are stored on the
SDCardtofacilitatefileaccessfromapps.Weregisteredseparate
accountsforeachbugthatrequiresloginandwrotetheloginscripts,
and during testing reset the account data before each run to avoid
possibleinterference.Notethatsince Sapienzisonlycompatible
withAndroid4.4,wewereunabletorun Sapienzonallthe52bugs
but only 19 bugs (verified to be reproducible on Android 4.4). The
symbol≈Ç-≈æincolumn≈ÇSa≈æinTable 3denotesthatthecorresponding
bug is not reproducible on Android 4.4. For Stoat, we allocated
one hour for modellearningandfive hoursfor modelmutation.
We allocated one device (i.e., one emulator) for each bug/tool in
one run (one run required 6 hours), and repeated 5 independent
runs for each bug/tool. This time setting was decided based on the
setupofthesetools intheiroriginalpapers( Apeuses1hour&5
runs,Humanoid uses1hour&3runs, ComboDroid uses12hours
&3runs, TimeMachine uses6hours&5runs,and Q-testing uses
1hour&4runs)andtwopriorstudies(Choudhary et al.[7]use1
hour and 10 runs; Wang et al.[50]use3 hours and 3runs).Thus,
ourtimesettingislargeenough.Thewholeevaluationtookover
52√ó5√ó6√ó7=10,920machinehours(notincluding Sapienz).Dueto
Android‚Äôslimitation,wecanonlyrun16emulatorsinparallelon
onephysicalmachine.Thus,theevaluationtookusaround28days,
inadditionto around one weekfor deploymentpreparation.
4 EXPERIMENTALRESULTS AND ANALYSIS
4.1 RQ1: QuantifyingBug FindingAbilities
The ultimate goal of testing tools is to find bugs. We measured
thebugfindingabilitiesoftheselectedtoolsfromthreedifferent
perspectives:(1) effectiveness :howmanybugscanbefoundbythese
tools?Arethereanydifferencesbetweenthebugsfoundbythese
tools? (2) stability: can these tools stably (deterministically) trigger
these bugs across the five runs? (3) efficiency: how many resources
(e.g.,time) are requiredbythesetoolsto trigger thesebugs?Effectiveness .InTable 3,thelasteightcolumnsgivethetestingre-
sults ofMonkey (≈ÇM≈æ),Ape(≈ÇA≈æ),Humanoid (≈ÇH≈æ),ComboDroid
(≈ÇC≈æ),TimeMachine (≈ÇT≈æ),Q-testing (≈ÇQ≈æ),Sapienz(≈ÇSa≈æ), and
Stoat(≈ÇSt≈æ) on each bug, respectively. The symbol ‚òÖdenotes that
thetoolfoundthecorrespondingbug.Incolumn≈Ç ‚àÖ≈æ,thesymbol
≈Ç‚úó≈ædenotesnoneofthetoolscandetectthecorrespondingbug.We
can see, outof the 52 bugs, 18 ( ‚âà34.6%) cannot be detected by any
tool.Itindicatesaconsiderablegapexistsbetweenallthetestingtools
and thecollected real-world bugs.We will look intothe gap in RQ2.
Table4summarizesthebugfindingresultsofindividualtools.
Therow≈Ç#FoundBugs≈ædenotesthetotalnumberofbugsthatwere
found by individual tools across the five runs. We can see Ape,
Monkey,ComboDroid , respectively, found 24, 22, and 21 bugs,
whileHumanoid ,TimeMachine ,Q-testing found18,15,and10
bugs,respectively.Theformerthreetoolsfoundafewmorebugs
than the latter three ones. Stoatfound 19 bugs, while Sapienz
only found 3 bugs out of the 19 bugs which it targets. We find Ape,
the most effective one among these tools, only found nearly half of
all bugs.Monkey,Ape,Humanoid ,ComboDroid ,TimeMachine ,
Q-testing andStoatmissed30( ‚âà57.7%),28( ‚âà53.8%),34( ‚âà65.4%),
31 (‚âà59.6%), 37 ( ‚âà71.2%), 41 ( ‚âà78.8%), and 33 ( ‚âà63.5%) bugs, respec-
tively.Itindicatesthegapbecomeslarger, i.e.,morebugsweremissed
when thesetoolswereapplied individually.
Totakeacloselook,Fig. 1(a)(thebottom-leftsection)reportsthe
pairwisecomparisonbetweenthetoolsontheirfoundbugs.The
comparisonreportswhichbugswerefoundbybothtools(reported
ingray),andwhichbugswerefoundby onlyoneofthetwotools.
Thisprovidesusacloserlookatthebugfindingabilitiesofthese
tools. We can clearly see these tools have obvious differences in
thebugsthattheyfound.Forexample,although Monkey,Ape,and
ComboDroid arecloseinthenumbersoffoundbugs,eachofthem
can still find some bugs that the others cannot. This phenomenon
also applies to those tools that have obvious differences in the
number of found bugs, e.g.,ApeandTimeMachine .It indicates
that noone canabsolutelyoutperform the others in findingbugs,and
instead they do complement each other by finding different bugs. We
willanalyzewhichfactorsaffectingthesetoolsinbugfindinginRQ3. .
Stability .Table4givesthebreakdownofwhichbugsweresuccess-
fullyfoundinhowmanyruns,whichindicatesthestabilityofthese
toolsinbugfinding.Row ùëõ/5(0‚â§ùëõ‚â§5)denoteswhichbugsweretrig-
geredinùëõrunsoutofthefiveruns.Forexample,row≈Ç1/5≈æandcol-
umn≈ÇM≈æmeansthereare5bugsof Monkey weretriggeredinonly
onerunoutoffiveruns.Thisisanotherimportantmetrictoconsider
whenadoptingatestingtool,whichindicateshowrandomaGUI
testingtoolcouldbeindetectingbugs.However,thismetrichasnot
beenreportedbythepriorstudies[ 7,50]orbytheauthorsofthese
tools. We can see a non-negligible number of bugs were only found
in one run but missed in the other four runs (see row ≈Ç1/5≈æ). For
example, TimeMachine andApefound7 and6 bugs,respectively,
inonlyonerun.Indetail, Monkey,Ape,Humanoid ,ComboDroid ,
TimeMachine andQ-testing have22.7%,25%,22.2%,19%,46%and
18.2%bugs, respectively,which were detectedinonly one run. It
indicates that existing tools have obvious randomness in bug finding,
andanon-negligiblenumberofbugswereactuallydetectedbychance.
Efficiency .Fig.1(b)givesthebugdetectiontimeofindividualtools
ontheirfoundbugs.Wecansee Ape,ComboDroid andQ-testing
124BenchmarkingAutomatedGUI TestingforAndroid against Real-WorldBugs ESEC/FSE ‚Äô21, August 23‚Äì28, 2021,Athens,Greece
TimeM . Combo.p-value = 0.0076
d = 0.33 
TimeM .   Apep-value = 0.00017 
d = 0.4
TimeM . Monkey
 TimeM . Human.
0102030
Combo. TimeM.
0102030
Human. TimeM.
0102030
Ape TimeM.0102030
Human. Combo.
0102030
Ape Combo.0102030
Ape Human.
0102030
Monkey TimeM.0102030
MonkeyCombo.0102030
Monkey Human.0102030
Monkey Ape
Combo. Human.p-value = 0.0069
d = 0.29 
Combo.  Ape
 Combo. Monkeyp-value = 0.028 
d = 0.2 
Human.    Apep-value = 7.35e -7
d = 0.47 
Human. Monkey
Ape    Monkeyp-value = 0.0068
d = 0.3 
p-value = 0.032
d= 0.2 1110
4
108
5
1447
12
1233
186
168
2
1111
498
1310
6
1257
17Monkey Ape Humanoid ComboDroid TimeMachine
TimeMachine ComboDroid Humanoid Ape MonkeyQ-testing
TimeM .   Q-test.p-value = 0.000024
d = 0.59 
Combo.   Q -test.
Human.   Q -test.p-value = 0.021
d = 0.3 
Ape      Q -test.p-value = 0.0042
d = 0.33 
Monkey    Q -test.
0102030
Q-test. TimeM.649
0102030
Q-test. Combo.813
2
0102030
Q-test. Human.810
2
0102030
Q-test. Ape1014
0
0102030
Q-test. Monkey913
1
Q-testing
(a) Pairwise comparison between pairs of tools
TimeM .
Combo.
Human.
Ape
Monkey
Q-test.
(b) Bug detection time
of individual tools
Figure1:(a)Pairwisecomparisonofthetoolsintermsoffoundbugsandbugdetectiontime(inminutes).Theplotsonbottom-
left section report the differences of found bugs between the pairs of tools (the grey bars shows the common bugs found by
bothtools),whiletheplotsontop-rightsectionreportthebugdetectiontimesbetweenthepairoftoolsonthecommonbugs
(thep-valueandthestandardizedeffectsize ùëëarereportedontop-rightwithineachplotifthecomparisonresultisstatistically
significant). (b) The bug detection time (in minutes) of individual tools on their found bugs. We did not include Sapienz and
StoatinFigure 1 because comparing therecenttools listed inTable 2 are our mainfocus.
arerelativelyfasterthantheothertoolsinbugfinding.Specifically,
Ape,ComboDroid andQ-testing detect20/24,19/21and9/10bugs
within the first one hour respectively, while Monkey,Humanoid ,
andTimeMachine detect 14/22, 14/18, and 10/15bugs respectively.
Fig.1(a) (the top-right section) reports a pairwise comparison
betweenthesetoolsinboxplotsonthebugdetectiontime.Notethat
(1)Thecomparisonreportstherunningtimesonthebugsfound
byboth tools . We did not consider the bugs found by only one tool
because that is unfair.(2) The detection time isthe offsetbetween
thefirstbugtriggeringtimeandtheexactstartrunningtimeofa
tool.Forexample, TimeMachine takesaround10minutestocreate
and setup the VM image before it actually starts the testing. We
excludedsuchpreparationtimeforanytool.Thus,thebugdetection
timewemeasuredishead-to-head.Wecansee thedetectiontimes
ofthesetoolshaveobviousdifferences .Tovalidatethesignificance
of these differences, we used Mann-Whitney U test [ 1], a non-
parametricstatisticalhypothesistestforindependentsamples,to
comparethedetectiontimesbetweentwotools.Wereportthe p-
valueandstandardized effect size at the top-right corner for any
pairwise comparison which is statistically significant. Here, the
significancelevel ùõºissetas0.05( i.e.,ifp-value <0.05,thedifferenceisbigenoughtobestatisticallysignificant).Thestandardizedeffect
sizeùëëindicates the magnitude of the difference ( ùëë<0.3 is small,
0.3‚â§ùëë<0.5 is medium, ùëë>0.5 is large). From the results, we can
seeApeis more efficient than all the other tools in finding bugs.
ComboDroid ismoreefficientthan Humanoid andTimeMachine ,
whileMonkey is more efficient than TimeMachine . The major
reason of such results is due to the differences of testing strategies
andtoolimplementations.
4.2 RQ2: Common Challenges andWeaknesses
Thissectionaimstoidentifythecommonchallengesforexisting
GUI testingtechniques andtoolsinfindingthe collectedbugs.
AnalysisMethods . To achieve this analysis, we focus on the 18
bugs (listed in Table 5) which have not been found by any tool.
Specifically,weusedthefollowinganalysismethodstoidentifythe
challenges. First,we carefullyreviewed the18 bugstounderstand
their features from both the GUI and code levels. Second, we exam-
ined the implementations of these tools to understand their testing
strategies. Third,weconductedtheonlinediscussionswiththetool
authors: we show the bug videos, discuss the possible reasons why
theirtoolsmiss thesebugs,andconfirmour observations.
125ESEC/FSE ‚Äô21, August 23‚Äì28, 2021,Athens,Greece Ting Su,Jue Wang,Zhendong Su
Table 5: Characteristics of the 18 bugs missed by all tools.
≈Ç#STR≈æisthenumberofshorteststepsto reproduce.
#IssueId #STR#Distinct
Transit.Text
InputsSetting
ChangesInteract.
PatternsExter.
Interact.
#118 9 6 ‚úì‚úì
#261 17 17
#4200 10 7 ‚úì
#5638 4 3‚úì
#6145 17 17 ‚úì ‚úì
#1232 8 8 ‚úì
#1558 6 3 ‚úì
#3244 8 8 ‚úì
#1581 6 5 ‚úì‚úì‚úì
#1391 10 9 ‚úì‚úì
#1385 6 6 ‚úì
#4792 9 8
#637 30 27‚úì ‚úì
#112 1 1 ‚úì
#8659 10 9‚úì ‚úì
#7182 2 2 ‚úì
#6530 24 10 ‚úì
#10876 5 4‚úì ‚úì
AnalysisResults .Table5summarizesthecharacteristicsofthe18
bugsviaouranalysismethods.Wedistilledfivemajorchallenges:(1)
deepusecasescenarios ,(2)specifictextinputs ,(3)changingsystemor
appsettings ,(4)specificuserinteractionpatterns ,and(5)externalapp
interactions . Note that one bug may impose multiple challenges at
thesametime,anyofwhichcouldblockatestingtool.Weillustrate
thesechallenges as follows.
C1 (event trace ): hard to reach deep use case scenarios . Ta-
ble5‚Äôscolumn≈Ç#DistinctTransit.≈ædenotesthenumberof distinct
GUIpagetransitions alongthebugtriggeringtrace.Thisnumber
approximates how deep a bug resides in the app . We can see that 12
outof18bugs( ‚âà66.6%)canonlybereachedafterbypassingmore
than 5 distinct page transitions. Specifically, nextcloud ‚Äôs #4792 and
and-bible ‚Äôs#261arethetwobugsthatposethissolechallengefor
the selected tools. For example, nextcloud ‚Äôs #4792 has 8 distinct
page transitions, and its search space of event traces is at least
16√ó12√ó2√ó1√ó7√ó5√ó2√ó3=80,642(eachnumberdenotesthenumber
of executable events on one distinct GUI page). This big number
blocksany toolfrom findingthe bugwithin six hours.
Insight:Itremainsanopenchallengeforexistingtoolstoreach
deepusecasescenarios,althoughsometoolslike ComboDroid and
TimeMachine were designed to reach deep app states; Humanoid
was designed to act like humans to cover more app functionalities.
C2 (text inputs ): no careful design of text input generation .
Textinputsareimportanttotriggersomebugsinadditiontothe
GUIactions.InTable 5,4bugsoutofthe18bugsrequiretextinputs,
and3outofthese4bugs( ‚âà75%)requirecorner-case(orinvalid)text
inputsratherthanmeaningful(orvalid)ones.Indetail, AnkiDroid ‚Äôs
#5638 requires to input the backslash codes ( e.g., ≈Ç&bsol;≈æ, ≈Ç&#92;≈æ);
osmeditor ‚Äôs #637 requires to fill two invalid, 1-length characters ≈Ç*≈æ
and≈Ç0≈æintothetextfieldsof valueandage,respectively; WordPress ‚Äôs
#10876requiresthatthecontentofapostunderwritingisleftas
empty; onlyWordPress ‚Äôs#8659 requires to input a valid text (not
necessarilymeaningful)thatcanobtainnon-emptysearchentries.
However,existingtoolsusuallygeneratepurerandomtextswith-
out careful designs, and thus hard to detect these bugs. For exam-
ple,ComboDroid andTimeMachine simplyinherit Monkey‚Äôstext
generationstrategy,whichgeneratesrandomtextsofdigits,letters,
or other symbols; Apeoptimizes Monkey by additionally generat-
ing random integer/float numbers and time/date formatted strings.
Humanoid randomly picks textsfrom the training data.Insight:Testing tools should improve the text input generation
strategiesforbugfinding .Inadditiontogeneratemeaningfultext
inputs [22], they should also stress test apps with corner-case or
invalid text inputs by analyzing app code or the meaning of text
fields, or defining a list of risky text inputs [ 28].Note that the prior
studies [3,7,52] only suggest generating valid text inputs because
theyaimforimprovingcode coverage rather thanbug finding.
C3(system/appsettings ):nodedicatedconsiderationofchang-
ing system/app settings . Changing system or app settings are
common user behaviors [ 23]. However, we find none of the se-
lected tools dedicatedly considers the necessity of such changes in
bugfinding,especiallyforsystemsettings(becausechangingsys-
temsettingusuallyrequiresinteractingwithsystemapp Settings ).
Thisleadstotheincapabilityofdetectingsuchbugs.InTable 5,3
bugs out of the 18 bugs involve setting changes, and 2 out of these
3bugs(‚âà66.6%)involvesystemsettings.Specifically, AnkiDroid ‚Äôs
#6145 requires changing the default system language from English
to another language andturning on one app preference option;
commons‚Äôs#1581requiresthatthesystemlocationserviceisturned
offbeforeenteringintothe Nearbypageandthenisturnedonto
use GPS for location; and commons‚Äôs #1391 requires turning on the
app‚Äôs ≈Çnight mode≈æ theme in the middle of a specific event trace.
None ofthe toolscan detectthesebugs.
Insight:Thekeychallenge ofconsideringsystemorappsettings
duringGUItestingisthelargespaceofpossibleGUItestscausedby
twomajorreasons. Onereasonisthediversityofsettingoptions.For
example,Android7.1provides9maincategoriesofsystemsettings
with over 50 concrete setting options [ 40,41], all of which could
affectappbehaviors.Butonlylimitedtypesofsystemsettingswere
considered before [ 23,35]. Another reason is the interleavings be-
tween the setting changes and the GUI events. Prior work [ 23,35]
only changes settings before an app starts and does not change set-
tings at runtime. However, all the 3 bugs require changing settings
at specific points at runtime. Note that the prior studies [ 3,52] have
not systematically observed this challenge. Because they analyze
the main app code ( i.e., Java code) coverage but we observe not all
settingchanges(especiallyforsystemsettings)willleadtoobvious
coverage changes in Java code ( e.g., changing system languages
mainly involves an app‚Äôs XML resource code). In addition, the im-
plicationfrompriorstudies(seeTableIIIin[ 52])togeneratesystem
events (i.e., sending broadcast intents) cannot work on changing
system settings ( e.g., security-related settings like permissions and
location cannotbe changedbysendingintents) orapp settings.
C4 (interaction patterns ): no explicit consideration of spe-
cific userinteraction patterns . Anothermajor challenge which
blocksthesetoolsfromfindingbugsisthelackofgeneratingspe-
cificuserinteractionpatternstoposeadverseconditions.Wecan
see that 12 out of the 18 bugs ( ‚âà66.6%) pose this challenge. For
example, WordPress ‚Äôs #6530 requires uploading a number of pic-
tures(making the uploading takes some time) to publish a post
and then deleting the post when the uploading is still in progress ;
osmeditor4android ‚Äôs#637requires removingallentriesbutthelast
onefromitspageofvalidatorpreference; commons‚Äôs#1385requires
a rotation action at one specific page ;WordPress ‚Äôs #8659 requires
scrollingdownandback thesitespage(revokingthepageloading
of new items) and select some specific items. AnkiDroid ‚Äôs #4200
requires putting one specific activity in the background for a while
126BenchmarkingAutomatedGUI TestingforAndroid against Real-WorldBugs ESEC/FSE ‚Äô21, August 23‚Äì28, 2021,Athens,Greece
andreturningbacktoit(makingtheAndroidsystemdestroyand
recreate the activity). Despite these bugs seem corner cases, the
corresponding userinteraction patterns are common inreality.
Insight: We carefully examined the relevant covered code of
these bugs. It reveals that manifesting these bugs requires exercising
specific sequences of callback interactions . For example, WordPress ‚Äôs
#6530involvestheinteractionsbetweenthecallbacksofGUIevents
(for deleting the post) and those of the background thread (for
uploading the pictures); commons‚Äôs #1385 involves the interactions
between the lifecycle callbacks of an activity. However, existing
tools only focus on maximizing line or activity coverage, which
ishardtostresstestdifferentcallbackinteractions.Oneplausible
wayistodesignspecificcoveragecriteria( e.g.,callbacksequence
coverage[ 33])ormutationoperators[ 30]toguidetesting. Notethat
thisinsightcannotbeobtainedbypriorstudies[ 3,52]becausesuch
bugswill not show differences in termsoflineoractivity coverage.
C5(externalinteractions):seldomconsidertheinteractions
withotherapps .5 outofthe18 bugs( ‚âà27.8%)requireinteracting
with other apps on the device to obtain the desired data (e.g., a
picture file) to enable testing the follow-up functionalities. How-
ever, most tools do not explicitly consider the necessity of these
interactionsinbugfinding,andinsteadtheyconstrainthetesting
effortswithintheappundertest.Forexample, Humanoid willsim-
plyrestartthetestedappaftercertainstepsofexplorationifitis
stillexploringthe otherapps. C5maybe relatedwithC1andC3.
Insight: It is much desirable for these testing tools to construct
external intents provided with desired data or files to simulate the
purpose ofexternal app interactions.
4.3 RQ3: Factors andOpportunities
This section discusses the factors we observed that affect bug find-
ing on the collected real-world bugs and the opportunities for tool
improvements.Specifically,weconducttheanalysisbasedonthe
testing results of each tool in Table 3and the pairwise comparison
resultsinFigure 1.WefollowthesameanalysismethodsinRQ2,
andsummarize our majorfindingsinthe following aspects.
GUI exploration (testing) strategies affect bug finding . The
toolswestudiedemploydifferentGUIexplorationstrategies. Indeed,
thesestrategiesshowtheiruniqueadvantagesinfindingspecificbugs.
Forexample, Monkey,Ape,ComboDroid ,TimeMachine found4,
1, 2, and1bugs,respectively,whichthe othertoolscannotfind.
Butwealsoobservethattheexplorationstrategieswithmoredi-
rectandfine-grainedguidanceseemmoreeffectiveinfindingbugs.
Forexample,inTable 4,Ape,ComboDroid andStoatdetectmore
bugs than Humanoid ,TimeMachine andQ-testing . Specifically,
bothHumanoid andQ-testing usetraineddeepneuralnetworkto
guideexploration: Humanoid explores towardshuman-preferred
pages,while Q-testing prefersexploringpageswithdifferentus-
agescenarios. TimeMachine heuristicallydeprioritizesthosepages
that have been visited more times (see Section 3.3 in [ 9]). Basically,
thesethreetoolsareonlyguidedtocovermoreGUIpages.How-
ever,thismaynotbedirectlylinkedwithbugfinding.Incontrast,
Apedifferentiates andexplores distinctapp states by dynamically
refiningstateabstraction, ComboDroid stress-teststhedata-flow
relations at the app code level, while Stoatoptimizes different
event compositions in GUI tests via the stochastic model. Thesethree tools are informed by more fine-grained analysis, and thus
are likely to detectmore bugs.
Opportunities :Integrating fine-grained(program)analysis re-
sultsintoGUI explorationcould be beneficialfor bugfinding.
Stateabstractiongranularityaffectsbugfinding .GUI layouts
are usually used to abstractly represent concrete app states during
testing.DuetothelargesearchspaceofGUIpages,GUIstateab-
straction strategies (or GUI comparison criteria [ 2]) are commonly
adopted by testing tools to improve testing scalability. We observe
thatthebugfindingabilitiescouldbeaffectedbythestateabstraction
granularity, which unfortunately has not been well-recognized by
existingtools.Specifically,weobservethatthetoolswithmorefine-
grainedabstractioncoulddetectmorebugs,whichcorroboratesthe
preliminaryfindingsof[ 2](see Section6.3) .
For example, we observe that TimeMachine andQ-testing
missedsometrivialbugslike WordPress ‚Äôs#11135and nextcloud ‚Äôs
#1918. Thetool authorsof TimeMachine explained to us thatone
majorreasoncouldbe TimeMachine ‚Äôsstateabstractioncriterion
istoocoarse.Inpractice, TimeMachine usesavariantoftheC-Lv3
abstractioncriterion[ 2](whichonlyuseslayoutwidgetstoabstract
GUI states) to decide whether a given state is a (new) interesting
state.However,thisabstractioncriterioncouldbetoocoarse,and
TimeMachine thus fails to identify and snapshot some ≈Çcritical≈æ
states (which are the preconditions of the bugs) into its state pool.
Asaresult,itmaymissthechancetotriggerthebug. Q-testing
usesamorecoarse-grainedabstractioncriterion(betweenC-Lv2
and C-Lv3 [ 2]), which only differentiates two GUI pages if they are
from two different app usage scenarios. In fact, TimeMachine and
Q-testing findtheleastnumbersofbugs,comparedtoothertools.
Meanwhile,alltheaforementionedthreebugscanbedetected
byApe,Humanoid andComboDroid . Because ComboDroid and
Humanoid usethefine-grainedC-Lv4criterion(whichusesboth
thelayoutandexecutablewidgetstoabstractstates),while Apeded-
icatedlyproposesadynamicallyrefinedstateabstractionstrategy
to achieve betterbalancebetween state precision andscalability.
On the other hand, Monkey is pure black-box and does not
do any abstraction. It treats everyGUI page as uniqueand emits
GUI events at any random screen coordinates, and thus sometimes
suffers from scalability issues. For example, Monkey cannot detect
FirefoxLite ‚Äôs#5085,whichonlyrequires5GUIevents.Thereason
isthatthisbugrequiresclickingasmallwidgetatthebottom-right
corner ofthe first GUI page, and then clickingone specific setting
option among many others on the next page. As a result, Monkey
has very lowchance to bypassthesetwopagesto trigger the bug.
Opportunities : Defining appropriate state abstraction criterion
isimportantforbugfindingbutstillanopenproblem.Onepossible
solutionistodefinespecificgranularityforspecifictypesofapps
or functionalities to reduce the chance of missing important states.
Small heuristics affect bug finding . We find some tools imple-
mentedsmallheuristics. Despitetheseheuristicsarenotthefunda-
mentaladvantagesofthecoretestingtechniques,theydoimprovethe
bug finding abilities .
Forexample, Monkey bydefaultdoesnotsupport long-touch ,so
it cannot detect AmazeFileManager ‚Äôs #1796, which requires a long-
touchevent.Butothermonkey-basedtools,i.e., Ape,ComboDroid ,
TimeMachine founditbecause they implemented long-touch .
127ESEC/FSE ‚Äô21, August 23‚Äì28, 2021,Athens,Greece Ting Su,Jue Wang,Zhendong Su
Table6:Concretenewinsightsobtainedfromourstudyandthecomparisonwithpriorstudiesonthesenewinsights(≈Ç ‚úì≈æ,≈Ç‚úó≈æ
and≈Ç?≈ædenote that thecorresponding studydoes ,doesnot oronlypartially does obtainthe new insight).
StudiesRQ1 (bug findingabilities) RQ2 (commonchallengesinfindingbugs) RQ3 (factorsand opportunities)
False
NegativesTesting
StabilityTesting
EfficiencyC1
(event
trace)C2
(text
inputs)C3
(system/app
settings)C4
(interaction
patterns)C5
(external
interactions)Testing
StrategiesState
AbstractionSmall
Heuristics
Choudhary etal.[7] ‚úó ‚úó ‚úó ‚úó ? ‚úó ‚úó ‚úó ‚úó ‚úó ‚úó
Wangetal.[50] ‚úó ‚úó ‚úó ‚úó ‚úó ‚úó ‚úó ‚úó ‚úó ‚úó ‚úó
Zhengetal.[52] ‚úó ‚úó ‚úó‚úì ? ? ‚úó‚úì ‚úó ‚úó ‚úó
Behrangetal.[3] ‚úó ‚úó ‚úó ‚úó ? ‚úó ‚úó‚úì ‚úó ‚úó ‚úó
Ourstudy ‚úì‚úì‚úì‚úì‚úì‚úì‚úì‚úì‚úì‚úì‚úì
Inaddition, ApeandComboDroid implementsaspecialstrategy
to input texts (one of the common user behaviors to input text): (1)
longtouchthetargettextfieldtoselecttheoriginaltext,(2)clearthe
wholecontent,andthen(3)inputthenewrandomtext.Duetothis
heuristic, only ApeandComboDroid foundMaterialFBook ‚Äôs #224,
which requires a long-touch to invoke the copy-paste operation.
Alltheothertoolscannotfindthisbugbecausetheyinputtextsvia
directlyoverwriting the originaltext.
Sometoolsinternallycomplementtheircoretestingtechnique
with some heuristics to improve testing effectiveness. For example,
ApeandComboDroid occasionally invoke the default Monkey to
do random testing. As a result, they can trigger some bugs that are
only likely to be triggered by Monkey. For example, Monkey may
slidedownthenotificationbarbyrandomswipesandchangesome
settings therein by random touches. As a result, all Monkey-based
toolscandetect openlauncher ‚Äôs#67,whichrequiresopeningthe≈Çdo
not disturb≈æ setting. Humanoid andQ-testing cannotdetect this
bug due to the lack of any Monkey-like random testing strategies.
Opportunities : Designing and integrating small heuristics by
simulatinghuman-appinteractionpatterns( e.g.,specificUIactions,
text input styles, putting apps in the background and returning
back to it) can improve bugfinding.
4.4 Discussion
Newinsightsobtainedfromourstudy .Table6summarizesthe
concretenewinsightsobtainedfromourstudy.Wecanseethatmost
of the new insights have not been identified by the prior studies [ 3,
7,50,52].Specifically,duetothelackofaground-truthbenchmark,
the studies [ 7,50] are difficult to do the in-depth analysis like
RQ1‚àºRQ3, while the studies [ 3,7,52] can only identify some or
partial insights in RQ2 because they identify the tool limitations in
achievinghighcodecoverageratherthanbugfinding.Wenotethat
thepriorstudies[ 3,7,52]identifiedsomeothertoollimitationslike
requiringaccount-loginandcollaborationwith otherdevices.We
excludedsuchlimitationsintheevaluationsetup, e.g.,byproviding
auto-loginscriptsandfocusingon≈Çself-contained≈æapps,because
theseare not the limitationsofthe core testingtechniques.
Applications of our study . Our study can have three major ap-
plications. First,thedetailedanalysisinRQ1 ‚àºRQ3distilledmany
important findings, which can help enhance, optimize and extend
existing testing tools. It also pointed out some open research prob-
lems,e.g.,howtoefficientlyfindsystemsettingrelatedcrashes[ 38]
and betterbalance betweendifferent GUIabstractioncriteria. Sec-
ond,theThemisbenchmarkcanbeusedtoquantitativelyandquali-
tativelyevaluatenewtestingtechniquesforAndroidinacontrolled,
rigorousenvironmentlikeDefects4JforJava.Forexample,anew
testingtechniquecouldcompareitselfwiththeresultsofselectedtoolstovalidateitseffectiveness,andchallengeitselfwiththe18
critical bugs (which no tool can find) to prove its advancement.
Third,theinfrastructurecanbeusedtofacilitateotherresearchlike
bugreproducing[ 51],faultlocalization[ 10]andprogramrepairfor
Android.
Threats to validity The validity of our study may be subject to
somethreats. Onethreat istherepresentativenessofourbugdataset
andthegenerabilityofourfindings.Toreducethisthreat,wein-
terviewed the industrial practitioners to obtain the agreed-upon
selectioncriterionofbugsthatconformstorealindustrialpractices.
The data collection is based on a large set of Android apps, and
all the issues with critical labels are assigned by developers. We
carefullyinspectedeachissueandcollectvalidoneswithoutany
bias(seeSection 3.1).Table3showstheappsarediverse,andthe
analysisinRQ2/RQ3alsoshowsthebugshavedifferentfeatures.
Moreover, the interviewees observe that critical bugsdo not have
obvious differences from other less important ones in bug mani-
festation( e.g.,thedifficultyofbug-triggeringandthetestlength).
Thus,ourstudyfindingsbasedoncriticalbugscouldbegeneralized
to real-world bugs. In the future, we could incorporate more bugs
tofurthermitigatethisthreat. Anotherthreat isthecorrectnessof
evaluation and result analysis. To counter this, we made consid-
erableefforttosetuparigorousexperimentalinfrastructure,and
resolvedmanytoolissuesbefore the deployment(seeSection 3.2).
We carefully examined tool implementations and discussed with
thetoolauthorstoanalyzetoolabilitiesandvalidateourobserva-
tions. The experimental data and results were cross-checked by
the two co-authors. We also made the Themisbenchmark publicly
available for replication.
5 CONCLUSION
In this paper, we take the first step to empirically evaluate auto-
matedGUItestingforAndroidagainstreal-worldbugs.Weevaluate
severaltestingtoolsonthe52real,reproduciblebugs,andreveal
many new findings. We find a considerable gap in these tools find-
ing the collected bugs. We identify five common major challenges
thatfutureworkshouldaddress,andthefactorsthataffectthese
tools in bug finding. Our study provides a new, complementary
perspective from prior studiesto analyze existing testingtools.
ACKNOWLEDGMENTS
WethanktheanonymousESEC/FSEreviewersfortheirvaluable
feedback and the generous helps from the studied tools‚Äô authors.
ThisworkispartiallysupportedbyNSFCProjectNo.62072178and
No. 61690204. Ting Su and Zhendong Su were partially supported
by a Google Faculty Research Award. Ting Su was also partially
supportedbyaSwissNSF Spark AwardCRSK-2_190302.
128BenchmarkingAutomatedGUI TestingforAndroid against Real-WorldBugs ESEC/FSE ‚Äô21, August 23‚Äì28, 2021,Athens,Greece
REFERENCES
[1]Andrea Arcuri and Lionel C. Briand. 2014. A Hitchhiker‚Äôs guide to statistical
testsforassessingrandomized algorithmsin softwareengineering. Softw. Test.,
Verif. Reliab. 24,3 (2014), 219≈õ250.
[2]Young Min Baek and Doo-Hwan Bae. 2016. Automated model-based Android
GUI testingusing multi-level GUI comparison criteria. In Proceedings ofthe 31st
IEEE/ACM International Conference on Automated Software Engineering (ASE) .
238≈õ249.
[3]FarnazBehrangandAlessandroOrso.2020. SevenReasonsWhy:AnIn-Depth
Study oftheLimitationsofRandom Test InputGenerationforAndroid.In 35th
IEEE/ACM International Conference on Automated Software Engineering (ASE) .
1066≈õ1077.
[4]StephenM.Blackburn,RobinGarner,ChrisHoffmann,AsjadM.Khan,KathrynS.
McKinley, Rotem Bentzur, Amer Diwan, Daniel Feinberg, Daniel Frampton,
Samuel Z. Guyer, Martin Hirzel, Antony L. Hosking, Maria Jump, Han Bok
Lee, J. Eliot B. Moss, Aashish Phansalkar, Darko Stefanovic, Thomas VanDrunen,
Daniel vonDincklage, and Ben Wiedermann.2006. The DaCapobenchmarks:
java benchmarking development and analysis. In Proceedings of the 21th Annual
ACMSIGPLANConferenceon Object-Oriented Programming, Systems, Languages,
and Applications(OOPSLA) . 169≈õ190.
[5]Wontae Choi, GeorgeC.Necula, and Koushik Sen.2013. GuidedGUI testingof
Androidappswithminimalrestartandapproximatelearning.In Proceedingsof
the2013ACMSIGPLANInternationalConferenceonObjectOrientedProgramming
SystemsLanguages & Applications(OOPSLA) . 623≈õ640.
[6]WontaeChoi,KoushikSen,GeorgeC.Necula,andWenyuWang.2018.DetReduce:
minimizingAndroidGUItestsuitesforregressiontesting.In Proceedingsofthe
40thInternationalConference onSoftwareEngineering (ICSE) . 445≈õ455.
[7]Shauvik Roy Choudhary, Alessandra Gorla, and Alessandro Orso. 2015. Au-
tomated Test Input Generation for Android: Are We There Yet? (E). In 30th
IEEE/ACM International Conference on Automated Software Engineering (ASE) .
429≈õ440.
[8]Brendan Dolan-Gavitt, Patrick Hulin, Engin Kirda, Tim Leek, Andrea Mambretti,
WilliamK.Robertson,FrederickUlrich,andRyanWhelan.2016. LAVA:Large-
Scale Automated Vulnerability Addition. In IEEE Symposium on Security and
Privacy (SP) . 110≈õ121.
[9]Zhen Dong, Marcel B√∂hme, Lucia Cojocaru, and Abhik Roychoudhury. 2020.
Time-travel Testing of Android Apps. In Proceedings of the 42nd International
Conference onSoftwareEngineering (ICSE) . 481≈õ492.
[10]LinglingFan,TingSu,SenChen,GuozhuMeng,YangLiu,LihuaXu,GeguangPu,
andZhendong Su.2018. Large-scaleanalysisofframework-specificexceptions
inAndroidapps. In Proceedingsofthe 40thInternationalConferenceonSoftware
Engineering (ICSE) . 408≈õ419.
[11]Mattia Fazzini, Martin Prammer, Marcelo d‚ÄôAmorim, and Alessandro Orso. 2018.
Automatically translating bug reports into test cases for mobile apps. In Proceed-
ingsofthe27thACMSIGSOFTInternationalSymposiumonSoftwareTestingand
Analysis(ISSTA) . 141≈õ152.
[12]Tianxiao Gu, Chengnian Sun, Xiaoxing Ma, Chun Cao, Chang Xu, Yuan Yao,
Qirun Zhang,Jian Lu,and ZhendongSu. 2019. Practical GUI testing of Android
applications via model abstraction and refinement. In Proceedings of the 41st
InternationalConference onSoftwareEngineering (ICSE) . 269≈õ280.
[13]Cuixiong Hu and Iulian Neamtiu. 2011. Automating GUI testing for Android
applications.In Proceedingsofthe6thInternationalWorkshoponAutomationof
SoftwareTest(AST) . 77≈õ83.
[14]LauraInozemtsevaandReidHolmes.2014. Coverageisnotstronglycorrelated
withtestsuiteeffectiveness.In 36thInternationalConferenceonSoftwareEngi-
neering(ICSE) ,PankajJalote,LionelC.Briand,andAndr√©vanderHoek(Eds.).
435≈õ445.
[15]WangJue, JiangYanyan, XuChang, CaoChun,Ma Xiaoxing,and LuJian.2020.
ComboDroid: Generating High-Quality Test Inputs for Android Apps via Use
CaseCombinations.In Proceedingsofthe42ndInternationalConferenceonSoftware
Engineering (ICSE) . 469≈õ480.
[16]Ren√©Just,DarioushJalali,andMichaelD.Ernst.2014. Defects4J:adatabaseofex-
istingfaultstoenablecontrolledtestingstudiesforJavaprograms.In International
SymposiumonSoftwareTestingand Analysis(ISSTA) . 437≈õ440.
[17]GeorgeKlees,AndrewRuef,BenjiCooper,ShiyiWei,andMichaelHicks.2018.
EvaluatingFuzzTesting.In Proceedingsofthe2018ACMSIGSACConferenceon
Computer and Communications Security(CCS) . 2123≈õ2138.
[18]Pavneet Singh Kochhar, Ferdian Thung, Nachiappan Nagappan, Thomas Zim-
mermann,andDavidLo.2015. Understandingthetestautomationcultureofappdevelopers.In 8thInternationalConferenceonSoftwareTesting,Verificationand
Validation (ICST) . IEEE,1≈õ10.
[19]PingfanKong,LiLi,JunGao,KuiLiu,Tegawend√©F.Bissyand√©,andJacquesKlein.
2019. AutomatedTestingofAndroidApps:ASystematicLiteratureReview. IEEE
Trans. Reliability 68,1 (2019), 45≈õ66.
[20]Yuanchun Li, Ziyue Yang, Yao Guo, and Xiangqun Chen. 2017. DroidBot: a
lightweightUI-guidedtestinputgeneratorforAndroid.In Proceedingsofthe39th
InternationalConference onSoftwareEngineering (ICSE) . 23≈õ26.
[21]Yuanchun Li, Ziyue Yang, Yao Guo, and Xiangqun Chen. 2019. Humanoid: A
DeepLearning-BasedApproachtoAutomatedBlack-boxAndroidAppTesting.
In34thIEEE/ACMInternationalConferenceonAutomatedSoftwareEngineering
(ASE). 1070≈õ1073.
[22]PengLiu,XiangyuZhang,MarcoPistoia,YunhuiZheng,ManoelMarques,and
LingfeiZeng.2017. Automatictextinputgenerationformobiletesting.In Proceed-
ingsofthe39thInternationalConferenceonSoftwareEngineering(ICSE) .643≈õ653.
[23]YifeiLu,MinxuePan,JuanZhai,TianZhang,andXuandongLi.2019. Preference-
wisetestingforAndroidapplications.In ProceedingsoftheACMJointMeetingon
EuropeanSoftware Engineering Conference and Symposium onthe Foundations of
SoftwareEngineering (ESEC/FSE) . 268≈õ278.
[24]Aravind Machiry, Rohan Tahiliani, and Mayur Naik. 2013. Dynodroid: an input
generationsystemforAndroidapps.In JointMeetingoftheEuropeanSoftware
Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of
SoftwareEngineering (ESEC/FSE) . 224≈õ234.
[25]KeMao,MarkHarman,andYueJia.2016. Sapienz:Multi-objectiveAutomated
TestingforAndroidApplications.In InternationalSymposiumonSoftwareTesting
and Analysis(ISSTA) . 94≈õ105.
[26]Micha√´l Marcozzi, Qiyi Tang, Alastair F. Donaldson, and Cristian Cadar. 2019.
Compilerfuzzing:howmuchdoesitmatter? Proc.ACMProgram.Lang. OOPSLA
(2019), 155:1≈õ155:29.
[27]Atif M. Memon and Myra B. Cohen. 2013. Automated testing of GUI applica-
tions:models,tools,andcontrollingflakiness.In 35thInternationalConferenceon
SoftwareEngineering(ICSE) ,DavidNotkin,BettyH.C.Cheng,andKlausPohl
(Eds.).1479≈õ1480.
[28]Daniel Miessler. 2021. Big list of naughty strings . Retrieved 2021-1
fromhttps://github.com/danielmiessler/SecLists/blob/master/Fuzzing/big-list-of-
naughty-strings.txt
[29]Monkey. 2020. Monkey. Retrieved 2020-5 from http://developer.android.com/
tools/help/monkey.html
[30]Ana C. R. Paiva, Joao M. E. P. Gouveia, Jean-David Elizabeth, and M√°rcio E.
Delamaro. 2019. Testing When Mobile Apps Go to Background and Come
Backto Foreground.In 2019IEEE International ConferenceonSoftware Testing,
Verification and Validation Workshops (ICSTW) . 102≈õ111.
[31]Minxue Pan, An Huang, Guoxin Wang, Tian Zhang, and Xuandong Li. 2020.
Reinforcementlearningbasedcuriosity-driventestingofAndroidapplications.
In29th ACM SIGSOFT International Symposium on Software Testing and Analysis
(ISSTA). 153≈õ164.
[32]PriyamPatel,GokulSrinivasan,SydurRahaman,andIulianNeamtiu.2018.Onthe
effectivenessofrandomtestingforAndroid:orhowilearnedtostopworryingand
lovethemonkey.In Proceedingsofthe13thInternationalWorkshoponAutomation
ofSoftwareTest(AST) . 34≈õ37.
[33]Danilo Dominguez Perez and Wei Le. 2019. Testing Criteria for Mobile Apps
Based onCallbackSequences. CoRRabs/1911.09201 (2019).
[34]JohnRegehr. 2021. Responsibleand Effective Bugfinding . Retrieved 2021-1 from
https://blog.regehr.org/archives/2037
[35]Alireza Sadeghi, Reyhaneh Jabbarvand, and Sam Malek. 2017. Patdroid:
permission-aware gui testing of android. In Proceedings of the 2017 11th Joint
MeetingonFoundationsofSoftwareEngineering (FSE) . 220≈õ232.
[36]TingSu,LinglingFan,SenChen,YangLiu,LihuaXu,GeguangPu,andZhendong
Su. 2020. Why My App Crashes Understanding andBenchmarking Framework-
specific Exceptions of Android apps. IEEE Transactions on Software Engineering
(2020).
[37]TingSu,GuozhuMeng,YutingChen,KeWu,WeimingYang,YaoYao,GeguangPu,
YangLiu,andZhendongSu.2017. Guided,StochasticModel-basedGUITesting
of Android Apps. In The joint meeting of the European Software Engineering
Conference and the ACM SIGSOFT Symposium on the Foundations of Software
Engineering (ESEC/FSE) . 245≈õ256.
[38]Jingling Sun, Ting Su, Junxin Li, Zhen Dong, Geguang Pu, Tao Xie, and Zhen-
dong Su. 2021. Understanding and Finding System Setting-Related Defects in
Android Apps. In Proceedings of the 30th ACM SIGSOFT International Symposium
onSoftwareTestingand Analysis(ISSTA) . to appear.
129ESEC/FSE ‚Äô21, August 23‚Äì28, 2021,Athens,Greece Ting Su,Jue Wang,Zhendong Su
[39]ShinHweiTan,ZhenDong,XiangGao,andAbhikRoychoudhury.2018.Repairing
crashesinAndroidapps.In Proceedingsofthe40thInternationalConferenceon
SoftwareEngineering (ICSE2018) . 187≈õ198.
[40]Android team. 2021. Android Developers Documentation . Retrieved 2021-1 from
https://developer.android.com/
[41]Androidteam.2021. AndroidHelp . Retrieved2021-1from https://support.google.
com/android/
[42] F-Droid Team. 2021. F-Droid. Retrieved 2021-1 from https://f-droid.org/
[43]GitHubTeam.2021. GitHubRESTAPI . Retrieved2021-1from https://docs.github.
com/en/rest/
[44]JacocoTeam.2021. Jacoco. Retrieved 2021-1from https://www.eclemma.org/
jacoco/
[45]uiautomator2Team.2021. uiautomator2 . Retrieved2021-1from https://github.
com/openatx/uiautomator2/
[46]Swapna Thorve, Chandani Sreshtha, and Na Meng. 2018. An Empirical Study
of Flaky Tests in Android Apps. In IEEE International Conference on Software
Maintenance and Evolution(ICSME) . 534≈õ538.
[47]Porfirio Tramontana, Domenico Amalfitano, Nicola Amatucci, and Anna Rita
Fasolino.2019. Automatedfunctionaltestingofmobileapplications:asystematicmappingstudy. SoftwareQuality Journal 27,1 (2019), 149≈õ201.
[48]MarioLinaresV√°squez,CarlosBernal-C√°rdenas,KevinMoran,andDenysPoshy-
vanyk.2017. Howdo DevelopersTest AndroidApplications?. In International
Conference onSoftwareMaintenance and Evolution(ICSME) . 613≈õ622.
[49]MarioLinaresV√°squez,KevinMoran,andDenysPoshyvanyk.2017. Continu-
ous, Evolutionary and Large-Scale: A New Perspective for Automated Mobile
AppTesting.In InternationalConference onSoftwareMaintenanceandEvolution
(ICSME). 399≈õ410.
[50]Wenyu Wang, Dengfeng Li, Wei Yang, Yurui Cao, Zhenwen Zhang, Yuetang
Deng, and Tao Xie. 2018. An empirical study of Android test generation tools in
industrial cases. In Proceedings of the 33rd ACM/IEEE International Conference on
AutomatedSoftwareEngineering (ASE) . 738≈õ748.
[51]YuZhao,TingtingYu,TingSu,YangLiu,WeiZheng,JingzhiZhang,andWilliam
G.J.Halfond.2019. ReCDroid:automaticallyreproducingAndroidapplication
crashesfrombugreports.In Proceedingsofthe41stInternationalConferenceon
SoftwareEngineering (ICSE) . 128≈õ139.
[52]Haibing Zheng, Dengfeng Li, Beihai Liang, Xia Zeng, Wujie Zheng, Yuetang
Deng,WingLam,WeiYang,andTaoXie.2017. AutomatedTestInputGeneration
for Android: Towards Getting There in an Industrial Case. In 39th IEEE/ACM
InternationalConferenceonSoftwareEngineering:SoftwareEngineeringinPractice
Track (ICSE-SEIP) . 253≈õ262.
130