Marble: Model-based Robustness Analysis of Stateful Deep
Learning Systems
Xiaoning Du
Nanyang Technological University
SingaporeYi Li
Nanyang Technological University
SingaporeXiaofei Xie∗
Nanyang Technological University
Singapore
Lei Ma
Kyushu University
JapanYang Liu
Nanyang Technological University
SingaporeJianjun Zhao
Kyushu University
Japan
ABSTRACT
State-of-the-artdeeplearning(DL)systemsarevulnerabletoadver-
sarialexamples,whichhinderstheirpotentialadoptioninsafety-
andsecurity-criticalscenarios.Whilesomerecentprogresshasbeen
madein analyzingtherobustnessoffeed-forwardneuralnetworks,
therobustnessanalysisforstatefulDLsystems,suchasrecurrent
neural networks (RNNs), still remains largely uncharted. In thispaper, we propose Marble, a model-based approach for quanti-tative robustness analysis of real-world RNN-based DL systems.
Marble builds a probabilistic model to compactly characterize the
robustness of RNNs through abstraction. Furthermore, we propose
an iterative refinement algorithm to derive a precise abstraction,
whichenablesaccuratequantificationoftherobustnessmeasure-
ment. We evaluate the effectiveness of Marble on both LSTM and
GRUmodelstrainedseparatelywiththreepopularnaturallanguagedatasets.Theresultsdemonstratethat(1)ourrefinementalgorithmismoreefficientinderivinganaccurateabstractionthantherandom
strategy,and(2)Marbleenablesquantitativerobustnessanalysis,
in rendering better efficiency, accuracy, and scalability than the
state-of-the-art techniques.
ACM Reference Format:
XiaoningDu,YiLi,XiaofeiXie,LeiMa,YangLiu,andJianjunZhao.2020.
Marble:Model-basedRobustnessAnalysisofStatefulDeepLearningSys-
tems. In35th IEEE/ACM International Conference on Automated Software
Engineering (ASE ’20), September 21–25, 2020, Virtual Event, Australia. ACM,
New York, NY, USA, 13 pages. https://doi.org/10.1145/3324884.3416564
1 INTRODUCTION
With the booming of big data and hardware acceleration, deep
learning (DL) has achieved tremendous success in many applica-tions such as image processing [
14], speech recognition [ 3,29],
∗Xiaofei Xie (xfxie@ntu.edu.sg) is the corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ASE ’20, September 21–25, 2020, Virtual Event, Australia
© 2020 Association for Computing Machinery.
ACM ISBN 978-1-4503-6768-4/20/09...$15.00
https://doi.org/10.1145/3324884.3416564video and board games [ 41,47]. In spite of achieving high accu-
racy, deep neural networks (DNNs) are still vulnerable to adversar-
ial attacks [ 8,11,12]. For example, an image classifier can easily
be fooled by pixel-level perturbations [ 52] or inevitable noise in
physical-world situations [ 22]. Hence, quality and reliability as-
surance of DL systems are urgently needed, especially for those
applied in safety- and security-critical scenarios.
Robustnessanalysisaimstoestimatethecapabilityofaneural
networkintoleratinginputperturbations,whichnaturallyoccurin
the physical environment or are intentionally applied by malicious
parties (e.g ., adversarial attacks). A neural network is said to be
robust,ifitspredictionresultscouldnotbemisledbyanysmallper-
turbations(e.g .,imperceptiblebyhuman).Infact,ithasbeenshown
thatmostneuralnetworksarevulnerabletosmallmanipulationsto
theinputs[ 8,51].Byfar,therearetwotypesofanalysismethods
to quantify robustness: robustness verification [6,24] that provides
theoretical guarantees on the level of perturbations a network is
immuneto,and robustnessquantification [10,38]whichestimates
the robustness score of a neural network with the difficulty of find-
inginputsthatleadtoincorrectpredictionresults.Intuitively,the
harder it is to generate such inputs, the more robust the model is.
Moreover, the quantification methods focus on either gener-
ating human imperceptible adversarial input perturbations [ 27]
to manipulate a DNN’s decision (e.g ., gradient-based attacks), or
producingsystematictests[ 38,43]tosimulaterealisticnoiseand
transformations that might occur in physical environment. Gener-
allyspeaking,thequantificationmethodsareoftenmorescalable
thantheverification-basedtechniques,andtheycanbegeneralized
to differentnetwork architecturesmore easily.However,it isnon-
trivialtoimprovemodelrobustness,evenwithsamplesexplicitly
exposingtheweaknesses.Lackingrobustnessoveraninputsample
indicatesthattheresultproducedbytheDNNispotentiallyunre-
liable.Iftherobustnessanalysiscanbeperformedinreal-time,adecision-making system is able to fallback to alternative backup
plans whenever the DNN reveals poor robustness.
Existingmethodsfacechallengesintermsofefficiencyandap-
plicability, especially in handling large models. In particular, the
testing- and attack-based techniques often require a large number
ofinputstobegeneratedandtested,whichisexpensiveandhardlyapplicabletoreal-timeapplications.Inaddition,thevastmajorityofexistingrobustnessanalysistechniquesonlyfocusonfeed-forward
neural networks (FNNs), leaving other types of networks, such as
recurrentneuralnetworks(RNNs),largelyuntouched.RNNsarede-
signed to process sequential inputs (e.g ., natural languages, audios,
4232020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE)
andsignals),andalsovulnerabletoadversarialattacks.Forinstance,
theRNN-basedtoxiccontents(e.g .,insultsandviolence)detector
in online discussions can be circumvented by replacing or modify-
ingasingleword[ 35].Toourbestknowledge,POPQORN[ 33]is
currentlytheonlyworkaddressingtherobustnessverificationof
RNNs.Itassumescontinuousinputdomainsandreliesonexpen-
siveboundcalculationspropagatedthroughlayers(i.e .,onunrolled
RNNs). As a result, it is rather imprecise when handling discrete
domaininputs,suchasnaturallanguagetexts,andfacesscalabil-
ity and efficiency issues. The problem of efficiently evaluating the
robustness of RNNs in processing sequential inputs remains an
openchallenge.Solvingthisproblemisanessentialsteptowards
improvingthereliabilityofreal-timeapplications,suchasmachine
translationforsimultaneousinterpretation, speechrecognition,and
perception tasks in cyber-physical systems.
Toaddresstheaftermentionedissues,weproposeamodel-based
approach,named Marble,for evaluatingtherobustnessofRNNs
effectively. The aim is to build an abstract model offline and enable
light-weightrobustnessestimationonlineforreal-timeapplications.
Toovercometheperformancechallengesandavoidmakingmany
perturbationsatruntime,weconstructarobustness-awareabstract
model for RNNs to capture the behaviors of an RNN given various
inputs in a compact form. Due to the stateful nature of RNNs,the output produced at each step is affected by both the input
elementandthecontextofit.Forexample,insentimentanalysis,
thepositive/negativeopinionspredictedatacertainwordwithin
asentenceareaffectedbytheworditselfandalsowordsbeforeit.
Ourabstractmodelmaintainspreviouslyseenstatesandtransitions
amongstates,whichareessentialincharacterizinghowdifferent
inputs are handled under various contexts.
Thecontextualinformationcanalsobeusefulin estimatingro-
bustness.Intuitively,underasimilarcontext,therobustnessover
the same input element is expected to be similar as well. For in-
stance,whenprocessingtwomoviereviews,“Ilovethismovie”and
“Ilovethatmovie”,anRNNoftenmaintainsverysimilarcontexts
before processing the final word “movie”. With this insight, we
computerobustnessmeasuresfortraininginputsandencodethis
informationintotheabstractmodel,whichisthenusedtoestimate
the robustness of new (previously unseen) test inputs. This makes
real-timeidentificationofunreliablepredictionspossibleandcould
improve the overall quality of the decision making process.
In particular, we build an abstract model based on Markov Deci-
sionProcess(MDP)foragivenRNNmodel,andproposearefine-
ment algorithm to iteratively improve the precision of the abstrac-
tioninquantifyingrobustness.Therefinementprocesscontinues
until the robustness estimation error (i.e ., the difference of the
robustness estimation from MDP and the dynamic input perturba-
tions)isreducedunderathreshold.WeappliedMarbletoquantifythe robustness of six RNNs which are trained for news title classifi-cation,toxiccommentdetectionandsentimentanalysis.Theresults
show that Marble is more effective in refining the abstract model
thanrandomstrategiesandproduces(atleast2times)smallerac-
curate MDPs. Marble is more efficient than the state-of-the-arttechnique – the average time taken by Marble per input is 0.03seconds while POPQORN requires 52.8 minutes. Marble is alsomoreprecise–itachieveshigherattacksuccessrateontheleastrobust inputs and lower attack success rate on the most robust
inputs, compared with POPQORN.
The main contributions of this paper are three-fold.
•We proposed to abstract RNN as an MDP model in order to
perform a quantitative robustness analysis of an RNN.
•Weintroducedamutation-basedrefinementtechniquetocontin-
uously refine the MDP for more accurate robustness estimation
of an RNN.
•Weperformedanin-depthevaluationontwopopulardatasets,
todemonstratetheeffectivenessoftherefinementalgorithmand
the estimation precision of Marble.
2 PROBLEM FORMULATION
2.1 Formalization of RNN and Traces
Definition 2.1 (Recurrent Neural Network (RNN)). An RNN is a
tupleR=(X,S,O,f)suchthat X,S,andOdenotethedomains
of the inputs, hidden states, and outputs, respectively, and fis a
differentiable parameterized function. Rtakes as input a sequence
x∈Xnof length n, maintains a sequence of hidden states s∈
Sn+1of length n+1, and applies the function fon each state and
input pair (si,xi)to produce an output sequence y∈Onsuch that
(si+1,yi)=f(si,xi),wherexi,si,andyidenotethe i-thelementof
the respective sequences.
Following[ 46],weformalizeanRNNabstractlyasabove.The
hidden states S⊆Rmarem-dimensional real number vectors and
the initial state s0is a vector of mzeros.sd
iis used to denote the
d-th dimension of the state vector si, whered∈[0,m).
Given an RNN, each input sequence xinduces a finite sequence
ofstatetransitions,whichformsa tracedenotedby t(x).Thei-th
elementofatrace,denotedby ti(x),isthetransitionfrom sitosi+1
after accepting an input element xiand producing an output yi.
ThetracesofanRNNcanberepresentedcompactlyasaFiniteState
Transducer (FST) [25], which is formally defined as follows.
Definition 2.2 (Finite State Transducer (FST)). Given an RNN R=
(X,S,O,f),wedefineitstracesinducedbyasetofinputs D∈2Xn
asafinitestatetransducer, TR(D)=(X,S,O,s0,F,δ),whereSis
a non-empty finite set of states, Xis the input alphabet, Ois the
output alphabet, s0∈Sis the initial state, F⊆Sis the set of final
states, and δ⊆S×X×O×S is the transition relation.
Example2.3. Fig.1givesanexampleofhowtorepresenttheRNN
behaviors as an FST, with transitions represented by solid black
arrows.TheRNN, R,isassumedtoworkonsentimentanalysisand
is trained to classify movie reviews into two categories, namely,
negative and positive. Note that 0 is used to represent negative
opinions, and 1 is used for positive ones.
Given a set of movie reviews, D={[“i”,“really”,“like”,“this”,
“movie”],[“i”,“like”,“this”,“movie”],[“we”,“like”,“this”,“movie”]},
three traces are induced. They share the same initial state s0and
consistoffive,fourandfourtransitions,respectively.Transitions
arerepresentedbyarrows,originatingfromonestateandtransit
to another state, labeling with its input and output pair. For the
transition from s0tos1, the label “ i: 0” denotes that this transition
is induced by the input element iand emits the output 0.
FortheFSTdefinedoverthesethreetraces,thesetofinputalpha-
betisX={“i”,“we”,“really”,“like”,“the”,“that”,“this”,“movie”},
424݅ǣͲ
݁ݓǣͲ݄݁ݐǣͳ
ݏ݄݅ݐǣͳ ݁݅ݒ݋݉ǣͳ
݁݅ݒ݋݉ǣͳ݁݅ݒ݋݉ǣͳݏ଴ݏଵ
ݏଽݏଶݏଷ
ݏସ
ݏହݏ଺
ݏ଻ݏଵ଴
ݏଵଵ
ݏ଼ݏଵଶݏଵସݏଵଷݏ݄݅ݐǣͳ
݄݁ݐǣͲ݄ݐǣͳ
Figure 1: FST Example.EŽZĞĨŝŶĞŵĞŶƚ
DƵƚĂƚĞ
WƌŽĨŝůŝŶŐ ďƐƚƌĂĐƚŝŽŶKƵƚƉƵƚ
ďƐƚƌĂĐƚŝŽŶ^ƚĂƚĞ
ďƐƚƌĂĐƚŝŽŶ
/ŶƉƵƚ
ďƐƚƌĂĐƚŝŽŶ
DW
^ĞůĞĐƚĂ
dƌĂŶƐŝƚŝŽŶфɅ͍zĞƐZĞƚƵƌŶ
DW
ZĞĨŝŶĞ^ŽƵƌĐĞ
^ƚĂƚĞhƉĚĂƚĞ^ƚĂƚĞ
ďƐƚƌĂĐƚŝŽŶZĞďƵŝůĚ
DW
ƐĂŵƉůĞƐ
Figure 2: The overview of Marble.
thesetofstatesis S={s0,...,s12},thesetofoutputsis O={0,1},
the initial state is s0, the set of final states is F={s5,s8,s12},
andthetransitionrelation δdefinesthesetof12transitions, e.g.,
(s0,“i”,0,s1), and(s1,“really”,0,s2).
2.2 Pointwise Robustness of an RNN
ThequantificationofFNNrobustnesshasbeenwidelystudiedin
the literature [ 9,54], whereas little work has been done to address
therobustnessofRNNmodels.UnlikeFNNthattakesaninputasa
whole, RNN consumes a sequential input element-by-element, and
perturbationsoneachinputelementmaycausedifferentlevelsof
impacttothemodeloutputs[ 18,30].Theoretically,thechangeat
asolitaryinputframecouldmakealong-lastinginfluenceonthe
process of upcoming frames. Also due to the complicated training
and working mechanism of RNNs, even well-trained RNNs tend to
reveal poor robustness to perturbations at a single input frame. In
linewiththeassumptionsinPOPQORN,westudythepointwisero-
bustnessofRNNs,insteadofallowingmutationsoverawiderangeofinputframes.Suchastrictconstraintcanbemoreinstructivefor
the quality assurance of RNNs at current stage.
To measure the robustness of an RNN model w.r.t. an input,
we allow the input to be perturbed within a certain range, and
observe their impact on the induced traces. Instead of searchingfor the lower bound of perturbations that would alter the RNN
decision,welookintohowlikelyperturbationswithintherange
could make a difference. Because the lowerbound measure overdiscrete data, e.g., natural language, is highly influenced by the
sparsityofthedata.Topreciselyquantifythepointwiserobustness,
wefirstspecifyhowinputelementsaretobeperturbed,formally
definedbya pointwisemutationfunction.Notethat,foragivenset Q,
theprobabilitydistributionover Qisafunction d:Q/mapsto→[0,1]such
thatΣq∈Qd(q)=1.Wedenotethesetofallprobabilitydistributions
overQasDist(Q).
Definition 2.4 (Pointwise Mutation Function). A pointwise muta-
tionfunction μ⊆X×Dist(X)mapsaninputelement x∈Xtoa
probabilitydistributionofthepossiblemutants x/prime∈X,suchthat
μ(x,x/prime)is the probability of mutating xintox/prime.
Thepointwisemutationfunctioncanbeusedtodescribevarious
typesofperturbationstotheinputs.Forexample, p-normball[ 7,33]
isoftenusedintheliteraturetodescribeperturbationshappened
uniformly within a sphere around the given input. Ingeneral, the
robustnessofamodelcanbeexaminedagainstdifferentreal-world
attack patterns by simulating the ranges and frequencies of the
potential mutations appeared in practice.
We define the 0-step pointwise robustness of an RNN at an input
elementxiasthelikelihoodthattheimmediatenexttransition tiinducedby xistaysunchangedaftersome perturbation isapplied
toxi. This definition can be further generalized to observe the
impactoftheperturbationonlyafteracertainnumberoftransitions,
instead of the immediate next one.
Definition 2.5 ( k-Step Pointwise Robustness). LetRbe an RNN
andx∈Xnbe an input of length n, respectively. Let xibe the
input element at the i-th position of x, wherei<n. Letμbe a
pointwise mutation function generating a new input x[xi→x/prime
i]
by mutating xi. Thek-step pointwise robustness of Rw.r.t.xat
thei-th position against perturbations defined by μ, denoted by
γk(R,x,i,μ),isgivenastheprobabilitythatthetraceinducedby
x/primeproduces the same output at the (i+k)-th transition, where
(i+k)≤n. More formally,
γk(R,x,i,μ)=/summationdisplay.1
x/prime
i∈X·ti+k(x)o=ti+k(x[xi→x/prime
i])μ(xi,x/prime
i),
whereo=denotes the equality of outputs for both sides.
Thepointwiserobustnessdefinitionisparameterizedbythenum-
berofsteps,afterwhichtheimpactoftheperturbationtothemodel
outputare observed.Specifically, t(x)andt(x[xi→x/prime
i])represent
the two traces induced by x, before and after xiis mutated, respec-
tively. The k-step pointwise robustness only observes the outputs
producedatthe (i+k)-thtransition,denotedby ti+k(x),andignores
any intermediate states and transitions. In particular, for the 0-step
pointwise robustness, we observe how the transition tiinduced by
xiisaffectedbyperturbationsover xi.Insteadofparameterizing
pointwiserobustnessbytheinputsequenceandpositionindices,
we sometimes use an equivalent notation γ0(R,si,xi,μ)such that,
γ0(R,si,xi,μ)=/summationdisplay.1
x/prime
i∈X·f(si,xi)o=f(xi,x/prime
i)μ(xi,x/prime
i).
Example 2.6. Following Fig. 1, we briefly illustrate how the 0-
step pointwise robustness is calculated with a simple example.Here we compute the robustness of
Rover the input sequence
[“i”,“really”,“like”,“that”,“movie”]atitsfourthinputelement, i.e.,
“that”.Forthemutationfunction μ,weassumethat“ that”canbe
mutated into any element in {“the”,“that”,“this”}with an equal
probability, i.e.,1
3.Theperturbationof“ that”wouldaffectthetran-
sitionrightafterthestate s3,andweassumethetwonewtransitions
induced by the replacements are (s3,“this”,1,s13),(s3,“the”,0,s14),
respectively. For the replacement to itself, the transition keeps still
as(s3,“that”,1,s4).Obviously, among all these three transitions,
two out of them emit the same output 1 as originally. Therefore,the 0-step robustness of
Rover the input sequence [“i”,“really”,
“like”,“that”,“movie”]at “that”i s2
3.
4252.3 Overview
FollowingDefinition2.5,thepointwiserobustnessofanRNNcan
be estimated through a random sampling from mutations. One can
randomly generate mutants according to the probabilistic distribu-
tiongivenbythemutationfunction,andobservetheirimpacton
the model outputs directly. Yet, this sampling approach likely does
not scale in practice. To get precise robustness measurements w.r.t.
even a single input, the number of mutants required can be huge if
the RNN model is non-trivial.
To overcome this challenge, we propose a model-based analysis
technique,Marble,toefficientlycomputethepointwiserobustness
of a given RNN with respect to an input element. Figure 2 sum-
marizestheworkflowof Marble.Wefirstconstructa robustness-
awareabstractmodelbyobservinghowtheRNN’sbehaviorsare
affected by pointwise mutations. Specifically, Marble profiles the
RNN’s behaviors on the training data and constructs an FST. Then,
foreachtransitionwithintheFST,Marbleexaminesits(0-step)ro-
bustness with input replacements sampled following the mutation
probabilistic distribution. Themutation function is specific tothe
application domains (e.g ., audio and image) and pre-defined by the
users. We then build an abstract model, in the form of a Markov
DecisionProcess(MDP),byabstractingthestatesandtransitions
intheFST,andannotatetheMDPwithrobustnessmeasurements
adapted with the abstraction.
Givenaninput,wecanthencomputearobustnessscore,regard-
ingeachinputframe,basedontheMDPmodel.Iftheobtainedscore
is inconsistentwith the resultof the dynamic mutation sampling
(i.e., their difference exceeds a threshold), we iteratively refine the
abstract modelso thatthe robustnesscan bemore preciselyquan-
tified.Finally,weobtainapreciseabstractmodelaftertheresults
ofalltrainingdatafromtheabstractmodelisconsistentwiththe
mutation sampling. Based on the abstract model, we can analyze
the robustness of the RNN against new inputs that follow a similar
distribution of training data efficiently.
3 RNN ABSTRACTION USING LABELED MDP
As shown in Definition 2.5, the robustness of an RNN is defined
with respect to a concrete input sequence at a certain position. Yet,
thenumberofstatesandtracesenabledduringthetrainingstage
of an RNN can be huge. It can be impractical, during runtime, to
compute the robustness for each trace individually, with either
classicalmathematicalestimationmethods[ 33]ormutationtesting
techniques [23]. To make the computationmore efficient, we nowintroduce an RNN abstraction model based on labeled MDPs.
3.1 Aggregated Pointwise Robustness
Thekeyideainderivingasuitableabstractionistogroupthetraces
withsimilarrobustnessmeasurestogether,whichcanbeusedto
estimate the robustness of an RNN for various inputs at various
contexts in a more cost-effective manner. Given a set of states ˜S,
from which transitions are induced by an input frame x,w ec a n
calculate the 0-step robustness for each such transition, against its
respective output. We define the aggregated pointwise robustness at
˜Storeflecttheoverallrobustnessofasetofinput-inducedtraces
against perturbations. The collection of transitions from ˜Smay
emitdifferentoutputs.Toeasethecomputationoftheprobabilityof
keepingtheoriginaloutputscollectively,werecordtheprobabilitiesemitting different outputs as a distribution over the output domain.
Specifically, we denote the probability of emitting yats∈˜Swhen
mutating input xwithμas:
γ0(R,s,x,μ)[y]=/summationdisplay.1
x/prime∈X·f(s,x/prime)o=(_ ,y)μ(x,x/prime).
Weusethenotation γ0(R,s,x,μ)todenotetheprobabilitydistri-
butionovertheoutputdomain,andomittheparameters Randμ
when they are clear from the context. Given that it is a categori-
cal distribution [ 4], themeanof this distribution is a vector p,o f
length|O|, wherepi=γ0(R,s,x,μ)[yi]for anyyi∈O. Further,
γ0(R,s,x,μ)issometimesalsoabusedtorepresentthe meanvector.
For an instance originally yielding y, the robustness is obtained
via taking the corresponding probability γ0(R,s,x,μ)[y].N o w ,t h e
aggregatedrobustnessdefinitionrequiresintegratingasetofdistri-
butionsobservedatdifferentstates.Acommonpracticetoreconcile
different probability distributions is to take the average of their
probabilitydensities[ 28].Hence,wedefinetheaggregatedpoint-
wise robustness as follows.
Definition 3.1 (Aggregated Pointwise Robustness). LetRbe an
RNNand TR(D)=(X,S,O,s0,F,δ)beitstracesinducedbyaset
ofinputsD.Letxbeaninputelementand μbeapointwisemutation
function. Let ˜S⊆Sbe a set of states such that for all s∈˜S,f(s,x)
emitsthesameoutput.Theaggregatedpointwiserobustnessof Rat
˜Sw.r.t.xagainstperturbationsdefinedby μ,denotedby Γ(R,˜S,x,μ),
isgivenastheaverageofthe0-steppointwiserobustnessof Rw.r.t.
the states in ˜S. More formally,
Γ(R,˜S,x,μ)=1
|˜S|/summationdisplay.1
s∈˜Sγ0(R,s,x,μ). (1)
The aggregated pointwise robustness is essentially the average
value of all pointwise robustness observations for a set of states. In
particular, for an output y,w eh a v e ,
Γ(R,˜S,x,μ)[y]=1
|˜S|/summationdisplay.1
s∈˜S/parenleftBig
γ0(R,s,x,μ)[y]/parenrightBig
,
whichtakestheaverageoftheprobabilitiesofemitting yindifferent
distributions. With Eq. (1), we are able to represent the overall
robustness at a set of states obeying proper statistical rules.
3.2 Abstraction as Labeled MDP
Abstractiononstatesisessentiallydividingthestatespacesuchthat
stateswithsimilarrobustnessmeasuresareclustered.Aftersuch
clustering,anabstractstatemayhavemultiplenon-deterministicoutgoingtransitionsevenafterreceivingthesameinput.Markov
DecisionProcess(MDP)[ 45]iswidelyusedtodescribeprobabilistic
systems, and to characterize the probabilistic state transitions trig-
geredbydifferentinputelements.Here,weusethelabeledMDPtoformalizetheabstractionofRNNtraces,andthedetaileddefinition
is given in Definition 3.2. The labeling function is used to record
the robustness measure of each state and input element pair.
Definition 3.2 (Labeled Markov Decision Process). A labeled MDP,
M,isatuple (ˆX,ˆS,ˆO,ˆs0,ˆδ,η)consistingofafiniteset ˆSofstates,a
finiteset ˆXofinputalphabet,aninitialstate ˆs0,afiniteprobabilistic
transition relation ˆδ⊆ˆS×ˆX×Dist(ˆS), where states and inputs
are in relation with distributions of successor states, and a labeling
426functionη⊆ˆS×ˆX×Dist(O)whichmapsthestateandinputpairs
to distributions of outputs.
Now we demonstrate how to derive a labeled MDP from a set
of observed traces represented as an FST. In order to represent
the behaviors of an RNN in a more compact manner, we allow
abstractions over the state domain, as well as the input and output
domains,andmoredetailsabouttheabstractionfunctionswillbe
discussed in Section 4.1. Given an FST TR(D)=(X,S,O,s0,F,δ)
of an RNN Ron the input set D, an input abstraction function
λI:X /mapsto→ˆX, a state abstraction function λS:S /mapsto→ˆSand an
output abstraction function λO:O /mapsto→ˆO, we can establish a la-
beledMDP M=(ˆX,ˆS,ˆO,ˆs0,ˆδ,η)forTR(D).Theinput,stateand
output domains in Mare abstracted from those in TR(D)with the
abstraction functions. Specifically, ˆs0=λS(s0), and for each tran-
sition(s,x,s/prime,y)∈δ, it is abstracted as (ˆs=λS(s),ˆx=λI(x),ˆs/prime=
λS(s/prime),ˆy=λO(y))and included as an abstract transition in M.
With these abstractions, the probabilistic transition relation ˆδand
thelabelingfunction ηcanbederivedaccordingly. ˆδ(ˆs,ˆx,ˆs/prime)denotes
the conditional probability of visiting ˆs/primegiven the current abstract
stateˆsandanabstractinputelement ˆx,andΣˆs/prime∈ˆSˆδ(ˆs,ˆx,ˆs/prime)=1.The
transition probability is calculated as the number of concrete tran-
sitions from ˆstoˆs/primeviaˆxover the number of all outgoing concrete
transitions from ˆsaccepting abstract input element ˆx,i.e.,
ˆδ(ˆs,ˆx,ˆs/prime)=|{(s,x,s/prime,_)∈δ|s∈ˆs∧x∈ˆx∧s/prime∈ˆs/prime}|
|{(s,x,_,_)∈δ|s∈ˆs∧x∈ˆx}|.
The labeling function ηrecords the 0-step aggregated pointwise ro-
bustnessof Runderamutationfunction μ.Notethattherobustness
definitioninDefinition3.1ispresentedundertheconcreteinput
and output domains, and here we adjust it to reflect their abstract
counterparts.SimilartoDefinition3.1,givenanabstractstate ˆsand
anabstractinput ˆx,theoutputsofthecorrespondingconcretestate
and input pairscan vary. Therefore,wealso label the robustness
under abstract states and abstract inputs as a distribution over the
output domain. Formally, we define 0-step aggregated pointwise
robustness at ˆsoverˆx, when emitting the abstract output ˆy,as,
η(ˆs,ˆx,ˆy)=1
|Sˆx|/summationdisplay.1
(s,x)∈Sˆx/parenlefttpA/parenleftexA
/parenleftbtA/summationdisplay.1
y∈ˆyγ0(R,s,x,μ)[y]/parenrighttpA/parenrightexA
/parenrightbtA,(2)
whereSˆx={(s,x)|∃ (s,x,_,_)∈δfors∈ˆs,x∈ˆx}is the set of
state-and-input pairs which source from concrete states in ˆsand
acceptconcreteinputelementsin ˆx.Theinnersumcalculatesthe
robustnessat soverx,whilerelaxingtheconstraintontheexpected
outputtobewithintheabstractoutput ˆy.Then,wetaketheaverage
value for robustness observations over all pairs inside Sˆx.
Example3.3. ContinuingwiththeexampleinFig.1,wedrawthe
abstractMDPmodelderivedfromitinFig.3(a)andFig.3(b).States
andtransitionsinducedbymutationsover“the”,“that”,and“this”
arehighlightedasreddotsandreddashedarrows,respectively.We
assumethatthesethreewordssharethesamemutationprobability
distribution, i.e., each could be mutated to others (including itself)
withequalprobability, i.e.,1
3.Thecorrespondingoutputs,either0or
1,areannotatedalongtheredarrows.Forinstance,bothtransitions
froms10tos16and from s10tos17emit output 0, and they are
inducedbyreplacing“this”with“the”and“that”,respectively.Inputelementsareomittedtokeeptheillustrationconcise.Fig.3(b)shows
the MDP abstraction of the concrete traces in Fig. 3(a). Now weelaborate on the input/output abstraction and thestate/transition
abstraction carried out during the MDP construction.
In this example, we assume an identity abstraction function for
theoutputspace.Thefullinputabstractionfunctionispresented
inthedashedboxatFig.3(a).Forinstance,“the”,“that”and“this”
are abstracted as ˆx4. The state space are abstracted via the grids
drawnindashedlinesinFig.3(a),resultinginfiveabstractstates,
i.e.,ˆs0,ˆs1,ˆs2,ˆs3,andˆs4,eachofwhichismappedtoasetofconcrete
statesinsidethecorrespondinggrid.Foreachabstractstate ˆs,w e
can then compute the transition function by deciding the set of
abstractinputsacceptedat ˆsandtheprobabilisticdistributionsof
thesuccessorstatesundereachabstractinput.Forexample,there
arethreeconcretetransitionsoriginatedfrom ˆs1,whichleadtotwo
abstractsuccessorstates: ˆs2andˆs3.Theabstractstate ˆs1acceptsonly
oneabstractinput, ˆx4,inducingadistributionofthesuccessorstates,
i.e.,{ˆδ(ˆs1,ˆx4,ˆs2)=1
3,ˆδ(ˆs1,ˆx4,ˆs3)=2
3}.Thetransitionprobabilities
are marked over the transitions between abstract states in Fig. 3(b)
as abstract input and probability pairs.
Finally, we show how the labeling function is computed, which
signifies the 0-step aggregated pointwise robustness. In Fig. 3(b),
thispartofinformationishighlightedwithblueboxesasideeach
abstract state. According to Eq. (2), the 0-step aggregated point-
wiserobustnessiscalculatedbytakingtheaverageofrobustness
observed concretely. We take the abstract state ˆs1and the abstract
inputˆx4asexample.FromtheconcretestatetransitionsinFig.3(a),
we can see that there are three concrete states within ˆs1, and all
of them accept concrete input elements in ˆx4.Specifically, at s3,
according to the mutation function, the probability to yield output
0is1
3,andtoyield1is2
3.Theprobabilitydistributionovertheout-
putsare markedfor s3,s6ands10,whenaccepting inputelements
belong to ˆx4.Finally, we can calculate the 0-step aggregated point-
wiserobustnessat ˆs1overˆx4,i.e.,η(ˆs1,ˆx4,0)=(1
3+1
3+2
3)/3=4
9
andη(ˆs1,ˆx4,1)=(2
3+2
3+1
3)/3=5
9. Hence, we label the output
probability distribution of ˆx4atˆs1as(4
9,5
9).The concrete distribu-
tionannotationsareomittedinFig.3(a)forotherconcretestates
and transitions, since they can be easily computed when no muta-
tions are applied to them. For the concrete transition (s0,“i”,0,s1),
theoutputprobabilitydistributionis (1,0),indicatingoutput0is
yielded with probability 1. Since there is only one concrete state
inˆs0accepting ˆx0,the aggregated robustness is directly (1,0).Sim-
ilarly, we can calculate the output distribution for each abstract
state over its accepting abstract inputs.
3.3k-step Aggregated Pointwise Robustness
Based on the abstracted MDP model, we define the k-step aggre-
gated pointwise robustness. Firstly, we define traces over the MDP
inducedbyanabstractinputsequencestartingfromadesignated
abstractstate.GivenanMDP M=(ˆX,ˆS,ˆO,ˆs,ˆδ,η),astartabstract
stateˆs0∈ˆSandanabstractinput ˆx∈ˆXnoflengthn,wedenotethe
setoftracesover Mtriggeredby ˆxandstartingfrom ˆs0asΠ(ˆx,ˆs0).
Thei-th element in a trace π∈Π(ˆx,ˆs0)is the transition from ˆsito
ˆsi+1viaˆxi.Inparticular,the0-thtransitionistriggered by ˆx0and
transits from ˆs0toˆs1. Moreover, we use ρ(π)to denote the trace
probability of π, which is the product of transit probabilities for
427ݏଵସƸ ݏ଴ Ƹ ݏଵ Ƹ ݏଶ
Ƹ ݏଷ Ƹ ݏସͳͳͳ
ͳͳ
ͲͲͲͲ
ݏ଴ݏଵ
ݏଽݏଶݏଷ
ݏସ
ݏହݏ଺
ݏ଻
ݏ଼ݏଵ଴
ݏଵଵ
ݏଵଶ ݏଵ଻ݏଵ଺ݏଵହݏଵଷ݅ǣͲ
݁ݓǣͲ
݁݅ݒ݋݉ǣͳ
݁݅ݒ݋݉ǣͳ݁݅ݒ݋݉ǣͳሺଵ
ଷǡଶଷሻ
ሺଵଷǡଶଷሻ
ሺଶଷǡଵଷሻ
(a) Abstraction over the state space.Ƹ ݏ଴
ො ݔ଴ǣሼሽ
ො ݔଵǣሼ݁ݓሽ
ො ݔଶǣሼݕ݈݈ܽ݁ݎሽො ݔ଴ǡଵ଺
ො ݔଵǡଵ଺
ො ݔଶǡଵ଺ො ݔସǡଵଷ
ො ݔସǡଶଷො ݔଷǡଵଶ
ො ݔହǡଵଶො ݔହǡଵଶො ݔହǡͳො ݔ଴ǣሺͳǡͲሻ
ො ݔଵǣሺͳǡͲሻ
ො ݔଶǣሺͳǡͲሻ
ො ݔଷǣሺͲǡͳሻ
ො ݔସǣሺସ
ଽǡହ
ଽሻො ݔହǣሺͲǡͳሻ
ො ݔହǣሺͲǡͳሻ
ො ݔହǣሺͲǡͳሻ
ො ݔଷǣሼ݈݁݇݅ሽ
ො ݔସǣሼ݄݁ݐǡݐ݄ܽݐǡݏ݄݅ݐሽ
ො ݔହǣሼ݁݅ݒ݋݉ሽƸ ݏଵƸ ݏଶ
Ƹ ݏସ
Ƹ ݏଷ
(b) MDP abstraction and modeling.ݏଵସƸ ݏ଴ Ƹ ݏହ Ƹ ݏଶ
Ƹ ݏଷ Ƹ ݏସͳͳͳ
ͳͳ
ͲͲͲͲ
ݏ଴ݏଵ
ݏଽݏଶݏଷ
ݏସ
ݏହݏ଺
ݏ଻
ݏ଼ݏଵ଴
ݏଵଵ
ݏଵଶ ݏଵ଻ݏଵ଺ݏଵହݏଵଷ݅ǣͲ
݁ݓǣͲ
݁݅ݒ݋݉ǣͳ
݁݅ݒ݋݉ǣͳ݁݅ݒ݋݉ǣͳሺଵଷǡଶଷሻ
ሺଵଷǡଶଷሻ
ሺଶଷǡଵଷሻƸ ݏ଺
(c) Refinement of the State Abstraction.
Figure 3: MDP construction and refinement.
transitionsin π,formally, ρ(π)=/producttext.1n−1
i=0ˆδ(ˆsi,ˆxi,ˆsi+1).Specially,we
defineϵas the emptyabstract input element, and forany abstract
state, the self-transition via ϵhappens with probability 1. Hence,
givenϵastheinputsequenceoflength1,wehave Π(ϵ,ˆs0)contains
theonlytrace with a single self-transition of ˆs0.To facilitate the
notations,weuse x[i:j]todenotethesubsequenceofasequence
x, which starts from its i-th element and ends at the ( j-1)-th ele-
ment;ifi≥j,thesubsequenceis ϵ.Also,weuse πrtodenotethe
reached state of its last transition. We give the definition of k-step
aggregated pointwise robustness in Definition 3.4.
Definition 3.4 ( k-step Aggregated Pointwise Robustness). Given
anRNNR,letM=(ˆX,ˆS,ˆO,ˆs0,ˆδ,η)bealabeledMDPestablished
for it. Let x∈Xnbe an input of length n, and the corresponding
abstractinputis ˆxwithˆxi=λI(xi)fori<n.Thek-stepaggregated
pointwiserobustnessof Rw.r.t.xatthei-th(i<n)positioncanbe
calculated over Mas,
Γk(M,x,i)=/summationdisplay.1
π∈Πρ(π)η(πr,ˆxi+k,ˆy),
wherethesourcestateof ti(x)issi,whichmapstoabstractstate ˆsi,
the output of ti+k(x)maps to the abstract output ˆy, and Π(ˆx[i:i+
k],ˆsi)(simplynotedas Π)isthesetoftracesover Minitiatingfrom
ˆsiand triggered by the abstract input sub-sequence ˆx[i:i+k].
The above definition is consistent with our 0-step aggregated
pointwise robustness defined under the abstract domain in Eq. (2).
Inthatcase,thesub-sequence ˆx[i:i+k]becomes ϵ,thus Π(ϵ,ˆsi)
contains only onetrace which consists of a single self-transition of
ˆsiviaϵ, and we have Γk(M,x,i)=η(ˆsi,ˆxi,ˆy).
4 MODEL REFINEMENT STRATEGY
WithRNNabstractedasalabeledMDP,wehopetocalculatethe
robustnessovertheabstractionforbetterefficiency.Toderivean
abstraction which allows for accurate estimation of the robustness
as obtained from mutation testing (see Definition 3.1), we design a
refinement process to reduce the estimation errors iteratively. The
aimistoreducetheestimationerrorsfromthe k-stepaggregated
pointwiserobustnessandgraduallyapproachestheground-truthro-
bustnessreflectedbymutationtesting.Inthefollowing,weassume
appropriate input and output abstraction functions are predefined,
andpresenttherefinementalgorithmfocusingontherefinement
of the state abstraction function in Section 4.2.
4.1 Input and Output Abstraction
Input abstraction aims to gather similar input elements, ideally the
ones which are able to reveal a similar level of robustness at thesame states. As a heuristic, we group inputs with identical or simi-
lar mutation probability distributions together to form an abstract
input.Thisway,theinputabstractionisalignedwiththepointwise
mutationfunction(Definition2.4)andinputswithsimilarseman-
tics and mutants are grouped and treated equivalently in the MDP
abstract model. The mutation functions are designed according to
thespecificpropertiesofinputdomains,consideringfactorssuchas,
whether the domain is continuous (e.g ., speech audios) or discrete
(e.g.,naturallanguages)andwhichtypesofdistancemeasures(e.g .,
lpnormdistanceorcosinedistance)aremoresuitableinrestricting
themutationmagnitude.Takingthenaturallanguageprocessing
(NLP)applicationasanexample,itisnaturaltoconsidersynonyms
as mutants of a word, which maintains semantic similarities. Prac-
tically, words with cosine distances within a predefined threshold
canbeidentifiedassynonyms.Weassumethemutationprobability
of a word to all its synonyms uniformly distributed. Hence, dif-
ferentwordssharingthesameorsimilarsetofsynonymscanbe
mappedintothesameabstractinput.Thebenefitofthischoiceis
thattherobustnessmeasurescomputedforoneinstancewithinthe
synonyms can be more easily generalized for other instances.
Fortheoutputabstractionfunction,itshouldbedesignedspecifi-
callyfordifferenttypesofdeeplearningtasks.Similarly,theabstrac-
tion on the output domain aims to gather similar outputs together,
based on how tolerant the users are about the output variations.
Forclassificationtasks,identityfunctioncanbeusedfortheoutputabstraction,sinceanyclassificationresultotherthanthetruthlabelisregardedasafailure.Fortasksattemptingtopredictavaluefrom
continuous domain, e.g., the steering angle of autonomous cars, an
abstraction function can be designed to map the outputs to a finite
discrete domain with techniques such as predicate abstraction [ 53].
The predictions of such tasks are deemed as correct as long as they
are within a certain range of the truth label. In general, the output
abstraction functions can be derived for different domains.
Similarity analysis of inputs/outputs can be a fundamental task
when conducting robustness analysis, and several metrics have
beenproposedandwidelyused, e.g.,p-normandcosinedistance.
For different DL applications, we could select suitable similaritymetrics. The quality of the input/output abstraction could makeasignificantdifferenceontheaccuracyof Marble.Forexample,a coarse input abstraction grouping inputs that are dramatically
differentishardtoberefinedtobeaccurateforeachinputelement.
In this work, we take the widely used metric (e.g ., cosine distance)
toperform theinput/output abstractionon theselectedtasks, and
leave the more comprehensive study as future work.
4284.2 Refinement of State Abstraction
We propose an algorithm for refining the state abstraction, which
aims to reduce the gap between the k-step aggregated pointwise
robustness (Definition 3.4) and the individual k-step pointwise
robustness (Definition 2.5) within an abstract state. From Defini-
tion 3.4, we see that the k-step aggregated pointwise robustness
can be deduced from the robustness labeling function.
Definition 4.1 gives the mean squared error (MSE) [34] of the
estimation calculated using an MDP abstraction compared with
the pointwise robustness obtained by mutating individual concrete
inputs. The accuracy of the MDP abstraction in measuring robust-
nessisdeterminedbyhowconcretestatesareclustered,because
errors arise when the aggregated robustness deviates from the ro-
bustnessobserved concretely.Ideally,we wouldliketodesign the
stateabstraction functionsuchthatstates withsimilarrobustness
distributionsareclusteredintoanabstractstate.MaximumMean
Discrepancy(MMD)[ 49,50]iswidelyusedtomeasurethedistance
between distributions, and defined as,
MMD(d1,d2)=/bardblex/bardblexEX∼d1[X]−EY∼d2[Y]/bardblex/bardblex
H,
whered1andd2are two distributions and the MMD takes the dis-
tancebetweenthemeanofthetwodistributionsintheReproducing
Kernel Hilbert Space (RKHS) H. Here we take the Euclidean dis-
tance with Euclidean space being a widely-used member of RKHS.
In Definition 4.1, we sum up the squared MMD distance (in Eu-
clidean space) between all aggregated robustness and concrete
robustness pairsto representthe errorsintroduced by theabstrac-
tion.
Definition 4.1 (Mean Squared Error). Given an RNN R, a set of
samplesD,letTR(D)=(X,S,O,s0,F,δ)andM=(ˆX,ˆS,ˆO,ˆs0,ˆδ,
η)be the FST and the labeled MDP established accordingly. The
mean squared error is calculated as,
1
|δ|/summationdisplay.1
ˆs∈ˆS/parenlefttpA/parenleftexA
/parenleftbtA/summationdisplay.1
(ˆs,ˆx,_)∈ˆδ/parenlefttpA/parenleftexA
/parenleftbtA/summationdisplay.1
s∈ˆs∧x∈ˆx∧(s,x,_,_)∈δ/bardblex/bardblexγ0(s,x)−η(ˆs,ˆx)/bardblex/bardblex2/parenrighttpA/parenrightexA
/parenrightbtA/parenrighttpA/parenrightexA
/parenrightbtA,
wheres∈ˆsandx∈ˆx.
Next, we elaborate on how to refine a state abstraction function
foragivenMSEthreshold θ,andsketchtheprocessinAlgorithm1.
GivenanRNNmodel R,amutationfunction μ,athreshold θand
a set of samples D, the algorithm produces a labeled MDP, with
which the estimated 0-step aggregated pointwise robustness of
samples in Dachieves an estimation error less than θ. Here, the
pointwise robustnesscomputed by mutation testingwith the mu-
tation function μis used as the ground truth. We assume that λI
andλOarepredefinedbyusers;and λSisinitializedsuchthatall
concrete states are mapped to a single abstract state.
Firstly, we profile the RNN model Rwith samples in Dand
represent the traces as an FST, TR(D)(Line 3). We then conduct
mutationsamplingbyreplacingtheinputofeachtransitionwith
Nmutants(Lines4to9).Themutantsaresampledaccordingtothe
probabilisticdistributiondefinedby μ.Viacountingthefrequencies
ofemittingdifferentoutputsafterapplyingthereplacements,we
obtainthe0-steprobustness(aprobabilisticdistributionoverthe
output domain) for each transition.
Lines 10 to 26 show the core refinement steps. In each iteration,
we attempt to improve the accuracy of the abstraction functionAlgorithm 1: Refinement algorithm for a labeled MDP.
input :R=(X,S,O,f): RNN,μ: mutation function, θ: threshold, D:
samples
output:M: labeled MDP
1Prepare input/output/state abstraction functions λI,λO,λS;
2Mutation sampling count N;
3TR(D)=(X,S,O,s0,F,δ); // finite state transducer of R
4for(s,x,y,s/prime)∈δdo // mutation sampling for robustness
estimation
5γ0(s,x)←[0]|O|;
6forjin[0,N)do
7 x/prime
i←selectMutation (μ,xi);
8 (_,y/prime)←f(s,x/prime
i);
9 γ0(s,x)[y/prime]+=1
N
10do
11M=(ˆX,ˆS,ˆO,ˆs0,ˆδ,η)←build _mdp(TR(D),λI,λO,λS);
12refinable ←False;
13forˆs∈ˆSdo // refine each abstract state
14 for(ˆs,ˆx,_)∈ˆδdo // refine the first refinable
transition
15 Spw,Bpw←∅ ,∅for(s,x,_)∈(ˆs,ˆx,_)do
16 b←[0]|ˆO|;
17 forˆy∈ˆOdo // pointwise robustness
18 b[ˆy]=/summationtext.1
y∈ˆyγ0(s,x)[y];
19 Spw,Bpw←Spw∪s,Bpw∪b;
20 if1
|Bpw|/summationtext.1
b∈Bpw/bardblη(ˆs,ˆx)−b/bardbl2>θthen // to refine
21 K←fitKmeansCluster (Bpw,clusters =2);
22 C←fitSVMClassifier (Spw,K.labels);
23 λS←addSubAbstracter (λS,ˆs,C);
24 refinable ←True;
25 break
26whilerefinable;
27returnM;
for each abstract state in M. As the first step, a labeled MDP is
computedfrom TR(D)withtheinput,output,andstateabstraction
functions.Foreachabstractinputelement ˆx,triggeringatransition
sourcingfrom ˆs(Line14),weexaminewhethertheestimationerror
ofthispairisbelow θ(Line20).Wecalculatethemutation-based
robustness related to ˆsandˆxwith Lines 15 to 19, and store the
resultsin Bpw.Wealsokeeptheconcretestatevectorsin Spw,one-
to-one mapped to those robustness in Bpw. The 0-step aggregated
pointwiserobustnessestimatedforthe( ˆs,ˆx)pairisη(ˆs,ˆx),which
equalstotheaverageofthevaluesin Bpw.IftheMSEofthissubset
ofobservationsisgreaterthan θ(Line20),wemakearefinement
onˆssuch that the overall MSE on Dis reduced (Theorem 4.3).
Wedesignatwo-steprefinementstrategytonarrowthedistance
betweentheestimatedaggregatedrobustnessandeverycorrespond-
ing mutation testing robustness. We aim to cluster the concrete
statesinSpwaccordingtotheirmutation-basedrobustness(Line21),
suchthatstateswithsimilarrobustnesswouldbegatheredtogether.
In the first step, we leverage k-means to separate the robustness
distributionsin Bpwintotwoclusters.OnLine21, Kdenotesthe
fittedk-meansclassifier,and K.labelsisusedtorepresentthelistof
obtainedclusterlabels.Clusteringovertherobustnessdistributions
offersimplicationonhowtodividethestatespace.Inthesecond
step, we treat the cluster index as the label and employ SVC classi-
fierCtoapproximatethedecisionboundaryoverthestatespace
(Line 22). Finally, we append Cas a sub-abstraction function of
stateˆsand update the state abstraction function λS(Line 23). After
429examining all abstract state and input element pairs, if there exists
anypairtoberefined,were-buildtheMDP Mwiththeupdated
λSand continue the iteration. Otherwise, if neither of the abstract
states requires further refinement, the procedure terminates and
returns the refined MDP model.
Example 4.2. Following Fig. 3(b), we illustrate how the refine-
ment algorithm works to reduce the estimation error. We assume a
thresholdof1
27ontheestimationerror.Whentheloopiteratesat
(ˆs1,ˆx4, _) at Line 14, the list of concrete states is Spw=[s3,s6,s10],
and the corresponding list of pointwise robustness observations is
Bpw=[(1
3,2
3),(1
3,2
3),(2
3,1
3)].Asweknowfromthelabelingfunc-
tion, the 0-step aggregated robustness at ˆs1overˆx4isη(ˆs1,ˆx4)=
(4
9,5
9).The local estimation error yields as,
1
3/parenleftbig/bardblex/bardblex/bardblex/bardblex/parenleftbig1
3,2
3/parenrightbig−/parenleftbig4
9,5
9/parenrightbig/bardblex/bardblex/bardblex/bardblex2
+/bardblex/bardblex/bardblex/bardblex/parenleftbig1
3,2
3/parenrightbig−/parenleftbig4
9,5
9/parenrightbig/bardblex/bardblex/bardblex/bardblex2
+/bardblex/bardblex/bardblex/bardblex/parenleftbig2
3,1
3/parenrightbig−/parenleftbig4
9,5
9/parenrightbig/bardblex/bardblex/bardblex/bardblex2/parenrightbig=4
81>1
27,
indicating a further refinement is required.
First,weapply k-meansover Bpw,andsettheexpectedcluster
number to two. Since k-means aims to put closer points into the
samecluster,itislikelytoreturntwoclusters C0=/bracketleftBig/parenleftBig
1
3,2
3/parenrightBig
,/parenleftBig
1
3,2
3/parenrightBig/bracketrightBig
andC1=/bracketleftBig/parenleftBig
2
3,1
3/parenrightBig/bracketrightBig
. Hence the labels for the list of states are
K.labels=[C0,C0,C1].Then,wecanfittheSVCwhichsplitsthe
abstract state ˆs1into two. As demonstrated in Fig. 3(c), we assume
thereddashedcurvetobetheboundarydeterminedbytheSVC.
Thus, the original abstract state ˆs1is refined into two new abstract
statesˆs5andˆs6. Now, the state abstraction is refined, and the MDP
model is to be reconstructed.
Theorem 4.3 (Error Reduction). Given an RNN R, a set of
samplesD, letMandM/primebe labeled MDPs obtained from Rand
D, before and after the execution of the refinement step (Lines 21
to 25 in Algorithm 1). We have MSEM/prime≤MSEM, whereMSEM
andMSEM/primeare the mean squared errors of MandM/prime, respectively.
The proof of Theorem 4.3 is available on our website [ 19]. With
Theorem4.3,theMSEof Misreducedaftereachrefinementstep.
Since there is only a finite number of concrete states in S, and the
number of abstract states ˆSis guaranteed to increase after each
iteration, MSEMis eventually approaching zero. Therefore the
threshold will be reached after finite number of iterations.
5 EVALUATION
WeimplementedMarbleinPythonbasedonthePyTorch(1.2.0)
frameworkandconductedevaluationonsixreal-worldRNNsubject
models to evaluate the refinement strategies and the robustness
quantification method. Specifically, our experiments are designed
to answer the following research questions:
•RQ1:How effective is the refinement algorithm in generating
MDPabstractions?Howaretheabstractionsintermsofsuccinct-
ness and generalization?
•RQ2:How is the scalability and efficiency of the robustness
quantification by Marble?
•RQ3:Does Marble provide a better quantification of RNN ro-
bustness than state-of-the-art approaches?Subject Datasets and RNN Models We selected three popular
NLPdatasetsandtrainedthecorrespondingRNNmodels.Inparticu-
lar,weleveragedthepre-trainedwordembeddingvectorsGloVe[ 44]
toacceleratethetrainingprocessandachievecompetitiveaccuracy.
(1) The CogComp QC Dataset (abbrev. QC) [ 37] includes news
titles labeled with different types of topics. There are 20K samples
fortrainingand8Ksamplesfortesting,andeachsamplecontains9.2
words onaverage. We followedthe same configurations as in[ 33]
totrainanLSTMmodelwithatestaccuracyof83.3%andaGRU
model with a test accuracy of 83.0%, respectively.
(2)TheJigsawToxicCommentDataset (abbrev.Toxic)[ 1]used
inKagglechallengeincludesasetofcommentsfromWikipedia’s
talkpageeditsandislabeledastoxicornot,withanaveragesamplelengthof54.8.Theoriginaldatasetisdesignedtobeseverelyimbal-
anced forthe challenge usage. Herein order toobtain an accurate
model, we use 25k non-toxic samples and 25k toxic samples forthe model training (80%) and testing (20%). We trained an LSTM
model (with 92.7% test accuracy) and a GRU model (with 93.1% test
accuracy), with 128 hidden nodes respectively.
(3) The Sentiment Analysis Dataset (abbrev. IMDb) [ 39] con-
tainsIMDbmoviereviewslabeledwithbinary(positiveornegative)
sentimental classifications. There are 25K training data and 25K
test data, where each sample contains 255.8 words on average. We
trained an LSTM model and a GRU model, each with 300 hidden
nodes, which achieve 90.7% and 90.9% test accuracy respectively.
StatesPreprocessandInputAbstraction. Tohandlehigh-dimen-
sionalstatevectors,weappliedPrincipalComponentAnalysis[ 56]
to reduce the data dimension to ωmajor components as in [ 20].
To build the mutation function for NLP applications, we use co-
sinesimilarity tomeasurethesemanticdistancesbetweenwords.
Based on the GloVe embedding vector, we performed the input
abstractionbymakingsynonymgroups,amongwhichthecosine
similaritiesareaboveathreshold.Inourevaluation,wesetthea
higherthreshold(0.75)toguaranteeaconservativemetamorphic
relation and obtained 3,572 synonym groups. During the mutation,
we allowed a uniform probability distribution on words within the
same synonym group.
5.1 RQ1: Effectiveness of the Refinement
We first examined the performance of our refinement strategy un-
derdifferentconfigurationinstances (ω,θ),whereωistheretained
dimensionofthestatevectorsafterPCAand θistheMSEthreshold.
As a baseline for comparison, we also implemented a random-split
basedstrategyforabstractstaterefinement.Therandom-splitstrat-
egy differs from Marble at only two steps (cf. Line 21 and Line 22
in Algorithm 1), which refines an abstract state by randomly sepa-
ratingthestateinstancesintotwogroups,andfittheSVCclassifier.
Our evaluation was conducted on the six RNN models (both
LSTM and GRU) of the three datasets. For each RNN model, we
experimented on six (ω,θ)configurations. Table 1 summarizes the
results of refinement obtained from the LSTM models of the three
datasets, under three configurations. The full results of all models
are on our website.1Column “Config.” shows the evaluated config-
uration instances (i.e .,(ω,θ)), which are used in both of Marble
andtherandomstrategy.Theothercolumnsincludethenumber
1https://sites.google.com/view/marble-rnn
430Table 1: Measures of MDP Models Refinement.
Config. Strategy #Iter. Time (s) #State #Tran. MSEtMiss (%)QC(16, 0.3)Marble 22 60.4 1158 136100 0 .04 45 .31
Random 43 49.8 3149 168552 0 .06 56 .68
(16, 0.6)Marble 20 89.4 864 129049 0 .05 41 .07
Random 31 54.1 2414 164461 0 .06 55 .59
(16, 1.0)Marble 16 112.6 447 110649 0 .07 33 .34
Random 25 56.1 1228 157208 0 .09 50 .47Toxic(16, 0.3)Marble 23 947.1 3577 859443 0 .01 13 .73
Random 162 649.6 12997 1147488 0 .01 22 .68
(16, 0.6)Marble 19 969.8 2083 729806 0 .01 11 .23
Random 105 763.9 10021 1118550 0 .01 20 .36
(16, 1.0)Marble 23 834.5 973 604829 0 .01 8 .34
Random 88 645.4 4782 1028458 0 .02 17 .44IMDB(32, 0.3)Marble 31 2849.7 13494 4075323 0 .02 25 .20
Random 127 805.1 67162 6472855 0 .06 49 .90
(32, 0.6)Marble 30 2028.7 8344 3900624 0 .03 21 .89
Random 42 812 53392 6426118 0 .06 46 .88
(32, 1.0)Marble 26 2385.7 3541 3437900 0 .03 16 .80
Random 38 812 23510 6032349 0 .08 38 .88
of iterations taken by the refinement (“#Iter”), the refinement time
(“Time (s)”), the number of states (“#State.”), the number of tran-
sitions (“#Tran.”) in the refined MDP. Further, we examined the
generalizationabilityofthelabeledMDPwhenappliedtothero-
bustnessestimationontheneverseentestdata,andreportedthe
percentageofmissedtransitionsincolumn“Miss(%)”aswellasthe
MSEmeasureincolumn“ MSEt”.Werefertoatransitionasmissed
ifitisnotincludedintheMDPmodel. MSEiscalculatedinasimilar
wayas whenbuildingthe MDPmodel,where weprofiledthe test
data, obtained the mutation-based robustness, and compared them
with the estimation by the labeled MDP.
Theresultsconfirmtheadvantagesoftherefinementstrategyof
Marblecomparedwitharandom-splitapproachinthat: 1)Marble
terminateswithlessiterationsacrossconfigurationsanddatasets.
Forthethreedatasets,theaveragenumberofiterationstakenby
Marbleis19.3,21.7and29.0,whilethatbyrandomstrategyis53.5,
118.3and69.0. 2)MarblealwaysderivesasmallerMDPmodelwith
less states and transitions through the refinement. For example,
the average number of states for the 3 datasets is 823.0, 2211.0and8459.7,respectively,whilethatbyrandomstrategyis2263.7,9266.7 and 48021.3, which are 1.8, 3.2 and 4.7 times larger thanMarble.
3)Marble always derives a better MDP model, which
demonstratesnotonlyalesstransitionmissratewhendealingwith
new samples from the test dataset, but also a smaller estimation
error. For instance, the average miss rate on the 3 datasets is 39.9%,
11.1% and 21.3%, while the random strategy causes larger miss rate
withrespectively54.2%,20.2%and45.2%.However,asatrade-off,
Marbletakeslongertimetocompletetherefinement.Forexample,
the average time used is 87.5s, 917.1s and 2421.4s, which are larger
than that by the random strategy, with respectively 53.3s, 686.3sand 809.7s. This is due to the application of the
k-means, which
better refines the abstract state with robustness estimation.
Finally, we summarize implications on applying Marble to
datasetofdifferentscalesandwithvariousconfigurations.Basically,
under thesame configuration,it takesmore time anditerations to
complete the refinement of larger dataset and more complicated
RNN models, and the resulted MDP is also larger. Under the sameconfiguration of ω, the larger threshold θis, the less effort it takes
to complete the refinement, resulting in a less complicated MDP,
butwithslightlyhigherestimationerror.AcoarseMDPoftenen-
dowsbettergeneralizationcapabilitywhenprocessingnewsamples,
leading to less missed transitions.
Answer to RQ1: Marbleissuperiorthantherandom-split
refinement strategy, and can deliver a more accurate abstract
MDPmodelwithsmallersizeandbettergeneralizationability.Marbletakeslessnumberofiterationsbutcostsslightlymore
time to accomplish the abstraction.
5.2 RQ2: Performance Efficiency
In this experiment, we evaluate the time taken to quantify the
robustness over every element of a sample, the efficiency of which
canbehighlydesirableforreal-timemonitoringapplications.We
present the results of three LSTM models, and full results can be
found on website [5].
ForeachoftherefinedMDP,weapplyittoestimatethe0-step
robustness of samples in the test data, and report the average time
usedpersampleinTable2.Notethatwetakethetotaltimeused
togetthe0-steprobustnessforeveryelementinasample,which
indicates that the longer the sample, the more time would be used.
InTable2,thefirstcolumnshowsthedatasetandthesecondcol-
umn“Config.”presentsthe (ω,θ)configurationsusedtoobtained
theMDP,andthetimeusedbytherobustnessestimationisgivenincolumn “Time (s)”. Overall, the robustnessof an input canbecalcu-
lated efficiently by Marble, with the time magnitude of thousands
or dozens of millisecond. On average, the three datasets take 0.03s,
0.24s and 1.07s to estimate the robustness of an individual sample.Larger dataset tends to take more time to finish the estimation.
Wecomparethescalabilityandefficiencyof Marblewiththe
state-of-the-art robustness quantification approach forRNNs, i.e.,
POPQORN.Ho wever,wecannot comapreMarblewithPOPQORN
inallmodels fortwo reasons: 1)scalabilityisamajorlimitationfor
POPQORN,whichfailstocompleteonlargerRNNstrainedfrom
ToxicandIMDbdatasetsand2)POPQORNismodel-dependentand
the current released version does not support GRU. Thus, we only
run POPQORN on the LSTM model of QC dataset.
We ran POPQORN on all sentences from the test dataset to
calculate their robustness scores and set a timeout, i.e., 12 hours.
Finally, only the robustness score of 42 sentences are calculated
successfully.Onaverage,POPQORNtakes52.8minutestocomplete
the estimation of a single input.
Answer toRQ2: Compared with POPQORN, Marble offers
betterscalabilityforhandlinglargeandcomplicatedmodels
for accepting longer inputs, and is also efficient in robust-
ness calculation, thus can be applied for real-time robustness
monitoring of larger RNN applications.
5.3 RQ3: Accuracy of the Robustness Measures
Due to the absence of the ground truth on the robustness measure-
ment,itisnotpossibletogaugetheabsoluteestimationaccuracy.
431Table 2: Attack Success Rate and Estimation Efficiency.
Dataset Config.ASRl(%) ASRm(%)Time (s)
Marble Vs Random Vs POPQORN Marble Vs Random Vs POPQORN
QC(16, 0.3) 20 .24 72.41 35.78 5 .76−50.91 -42.11 0 .02
(16, 0.6) 20 .00 70.39 34.19 4 .40−62.47 -55.74 0 .02
(16, 1.0) 22 .71 93.51 52.40 4 .24−63.89 -57.42 0 .02
(32, 0.3) 19 .83 68.97 33.07 2 .67−77.28 -73.21 0 .03
(32, 0.6) 20 .10 71.20 34.82 2 .57−78.09 -74.16 0 .05
(32, 1.0) 17 .74 51.12 19.01 5 .40−53.96 -45.69 0 .06
Avg. 20 .10 71.26 34.88 4 .17−64.44 -58.05 0 .03
Toxic(16, 0.3) 7 .27143.52 -2 .00−33.11 -0.22
(16, 0.6) 8 .76193.59 -2 .29−23.23 -0.24
(16, 1.0) 10 .24243.12 -2 .29−23.36 -0.21
(32, 0.3) 7 .80161.28 -2 .10−29.77 -0.27
(32, 0.6) 10 .46250.47 -2 .24−25.10 -0.25
(32, 1.0) 11 .74293.46 -2 .20−26.30 -0.23
Avg. 9 .38214.24 -2 .18−26.81 -0.24
IMDB(32, 0.3) 4 .76176.51 -1 .11−35.35 -1.09
(32, 0.6) 3 .60109.53 -1 .17−32.09 -1.04
(32, 1.0) 4 .01133.26 -1 .06−38.37 -0.84
(64, 0.3) 4 .04134.88 -1 .06−38.37 -1.25
(64, 0.6) 3 .59108.60 -1 .01−41.40 -1.16
(64, 1.0) 2 .79 62.33 -1 .16−32.79 -1.05
Avg. 3 .80120.85 -1 .09−36.40 -1.07
* The ASR differences between Marble and other approaches are examined with Mann-Whitney U test, and the displayed results are
statistically significant with p<0.05.
Instead,weinspectedwhetherthelocationswitnessingworsero-
bustness is more vulnerable to attack, i.e., to examine relatively
ordertherobustnessestimations.Worserobustnessindicatesthat
perturbationsoverthatlocationcaneasilymakeadversarialattacks.
Weevaluatedtheeffectivenessoftheobtainedrobustnessmeasures
by examining their helpfulness in launching adversarial attacks.
According to the robustness reverted by the quantification tools,
attackingtheleastrobustlocationsleadstothemostefficientattack.
To measure the attack efficiency, we assigned a fixed number of
attack chances to each sample, and calculated the attack success
rate(ASR)overasetofsamples, i.e.,theportionofsamplesthatare
successfully manipulated into adversarial. Our evaluation was con-
ducted on all the refined MDPs of the LSTM and GRU models, and
withthetestdataset.Weselecttwobaselines:1)randomstrategy
thatrandomlyselectsalocationtomutateand2)robustness-guided
attack based on the measurements by POPQORN.
For the QC dataset, since POPQORN is only successfully exe-
cutedon42samples,weusedthesamesetofsamplestoevaluate
the MDPs refined from the QC LSTM model to make a fair com-
parison.Specifically,foreachsentence,weperformedtwosetsof
experiments to attack the least robust word and the most robust
word, respectively. We replaced the selected word to one of its
synonymsinarandommannerandtheattackwasclaimedtobe
successful whenever the prediction result became different.
We repeated the experiments 100 times and report the aver-
ageASR.Anoverallcomparisonamongthesethreeapproachesis
demonstratedasabarplotinFig.4.Foreachdataset,itdisplaysthe
averageperformanceof Marblecalculatedoverallabstractions,
theperformanceofPOPQORNaswellastherandomattackstrategy.We also present the detailed results of Marble under various con-
figurations in Table 2. The three columns under “ ASRl(%)” reports
the ASR when attacking the least robust locations and the increase
rateofourapproachwhenrespectivelycomparedwiththerandom
strategy and POPQORN, while the results of attacking the most
robust locations are under “ ASRm(%)”. Here the ASR values are
averagedoverthe100-timeexecutions.Theperformancedifference
is also statistically significant (i.e ., confirmed by Mann-Whitney U
test [40] at p<0.05 confidence level).
From Fig. 4, we can see that the random attack strategy makes a
successrateof11.7%,3.0%and1.7%,respectivelyonthe3dataset.
Compared with random strategy, Marble achieves higher ASR
(i.e.,onaverage20.1%,9.4%and3.8%)whenattackingattheleast
robust locations and lower success rate (i.e ., on average 4.2%, 2,2%,
1.1%)whenattackingthemostrobustlocations.Inthebestcases,as
shown in Table 2, Marble outperforms the random strategy with
anASR increaseof93.51%, 293.46%and176.51%,when attacking
the least robust locations. For attacks on the most robust locations,
theASRbyMarblecanbe77.28%,33.11%and41.40%lessthanthat
of the random strategy. On the QC dataset, POPQORN achieves
anASRof14.9%whenattackingtheleastrobustlocations,which
isonly27.4%higherthantherandomstrategyandworsethanall
the resultsby Marble. Whenattacking themost robust locations,
POPQORNstillraisesahighASRof10%,whileMarbleonlymakes
4.2%.
Answer to RQ3: In our evaluation, Marble calculates ro-
bustnessmoreaccuratelythan POPQORN.Marblecanalso
achieve up to 2 times higher attack success rate than the ran-
dom strategy.
432(a) QC dataset. (b) Tosic dataset. (c) IMDb dataset.
Figure4:Attacksuccessratesbasedontherobustnessscores
calculated by different methods.
5.4 Threats to Validity
First,sixsubjectRNNmodelsandalimitsetofconfigurationswere
examinedforthestateabstractionrefinement.Wehavetriedour
besttocoverawiderangeofMSEthresholds,whichisanimpor-
tant and direct factor influencing the accuracy of the robustness
estimation.Evenwiththis,conclusionsdrawnonthelimitedgroup
of subjects may not generalize to other models and configurations.
Second,fortheattackingexperimentsaimingtoevaluatethemodel
accuracy,thesetofsamplestobeattackedisofarelativelysmall
size. For QC dataset, to accommodate the scalability of POPQORN,
only42 sampleswere investigated.Hence, thecomparisonresults
withPOPQORNmaylackstatisticalsignificance.Finally,random-
ness is hard to be avoided for both the refinement process and the
attacking experiments. The initial cluster centers of k-means were
randomly selected, thus the state abstraction achieved may vary
between executions. Moreover, mutations were randomly selected
(accordingtothemutationprobabilitydistribution)duringtheat-
tack procedure. Given a fixed number of chances, whether we can
construct an adversarial sample is non-deterministic. In order tooffset the randomness (to a certain extend), we repeated all the
experiments for five times and reported the average values.
6 RELATED WORK
Adversarial Attacks on RNNs NLP and automatic speech recog-
nition are the two typical domains where existing RNN robustness
attackapplies.ToattackNLPmodels,paper[ 42]isanearlywork
to launch adversarial attacks on text classification task with Fast
GradientSignMethod.Paper[ 36]providedatechniquetolocate
importantandsensitivewordswithreinforcementlearning.Follow-
ingthesimilarline,paper[ 31]proposedtoscoreeachwordwith
an importance metric and leverage word manipulations, including
swap,substitution,deletionandinsertion,togenerateadversarial
examplesforreadingcomprehensionsystems.Fortextclassificationmodels,therehavealsobeenworks[
13,21]ongeneratingadversar-
ial examples to fool general-purpose sequence-to-sequence models.
For attacks of speech recognition models, research work [ 26]
presented an approach to generate untargeted audio attacks, while
paper[15]proposedatechniquetogeneratephoneticallysimilar
phrasesandmadeitpossibletogeneratetargetedattacks.Paper[ 11]furtheradvancedtheadversarialaudiogenerationtoproduceimper-
ceptibleattacksforanygiventargetsandevaluatetheirapproach
onpopularmodelDeepSpeech[ 2].Theafter-mentionedapproaches
all reply on gradient-based algorithms, which require the white-box information on the full parameters of the subject models. A
GAN-basedblack-boxattackgenerationisdesignedin[ 57]withthe
demonstrations on textual entailment and machine translation for
untargetedattacks.AnalyzingthedifficultiesofleveragingexistingattackstoRNNthrougheitherdirectapplicationortransferattacks
enables to estimate robustness of an RNN against an input.RobustnessAnalysisofDNN
Insteadofansweringwhetherthere
exists an attack to compromise the DNN, robustness verificationaims to identify the minimum adversarial distortion bound such
that no attack exists with distortion under this lower bound. Exist-
ing works along this direction make some progress to verify the
robustnessofFNNs.Theminimumadversarialdistortioncalcula-
tion of ReLU networks is shown to be NP-hard [ 32]. Therefore,
further works attempt to compute a non-trivial certified lower
bound[9,48,54]ortofindanestimationoftheminimumadversar-
ialdistortion[ 7,55].Bastani etal.[7]proposedtoencodetheDNN
as a linear programming problem, and defined a robustness metric
based on the adversarial examples discovered.This metric depicts
anupperboundoftheminimumdistortionandalsodependentson
specific attack algorithms. In contrast, the CLEVER score [ 55]p r o -
vides an attack-agnostic estimation of the lower bound. Paper [ 24]
developed the first sound analyzer for DNN with abstract interpre-
tation [16, 17], to automatically prove robustness properties.
To our best knowledge, the only research on the robustness
verificationofRNNisPOPQORN[ 33],whichproposesarobustness
quantification framework to develop a guaranteed lower bound of
theminimumdistortioninRNNattacks.However,itsuffersfrom
scalabilityissues,whichcannothandlehundredsofhiddenlayers
as demonstrated in our empirical evaluation.
7 CONCLUSION
ThispaperproposedMarble,amodel-basedtechniqueforquantita-
tive robustness analysis of RNN-based DL systems. Our evaluation
Marbleenablesmoreaccurateandefficientquantificationofthe
robustnessofRNNs.Inthefuture,weplantoconductstudieson
theeffectparameters k(inthek-steptrace-basedrobustness)onthe
robustnessquantificationandapplyMarbleondiversepractical
applications, such as image classification and automatic speech
recognition.
ACKNOWLEDGMENTS
This work was supported by Singapore Ministry of Education Aca-
demicResearchFundTier1(AwardNo.2018-T1-002-069),theNa-
tionalResearch Foundation,PrimeMinisters Office,Singaporeun-
deritsNationalCybersecurityR&DProgram(AwardNo.NRF2018NCR-
NCR005-0001), the Singapore National Research Foundation un-der NCR Award Number NSOE003-0001, NRF Investigatorship
NRFI06-2020-0022andNTUGAPfunding(NGF-2019-06-024).Itwas
also supported by JSPS KAKENHI Grant No. 20H04168, 19K24348,
19H04086,andJST-MiraiProgramGrantNo.JPMJMI18BB,Japan.
We also gratefully acknowledge the support of NVIDIA AI Tech
Center (NVAITC) to our research.
433REFERENCES
[1]2018. Jigsaw Toxic Comment Classification Challenge. https://www.kaggle.
com/c/jigsaw-toxic-comment-classification-challenge
[2] 2018. Mozilla’sDeepSpeech. https://github.com/mozilla/DeepSpeech.
[3] 2020. Amazon Alexa. https://developer.amazon.com/alexa[4]
2020. Categorical Distribution. https://en.wikipedia.org/wiki/Categorical_
distribution
[5] 2020. MARBLE home page. https://sites.google.com/view/marble-rnn/home[6]
MislavBalunovic,MaximilianBaader,GagandeepSingh,TimonGehr,andMartin
Vechev.2019. CertifyingGeometricRobustnessofNeuralNetworks. In Advances
in Neural Information Processing Systems 32.
[7]Osbert Bastani, Yani Ioannou, Leonidas Lampropoulos, Dimitrios Vytiniotis,
Aditya V. Nori, and Antonio Criminisi. 2016. Measuring Neural Net Robustness
withConstraints.In Proceedingsofthe30thInternationalConferenceonNeural
InformationProcessingSystems (Barcelona,Spain) (NIPS’16).CurranAssociates
Inc., USA, 2621–2629. http://dl.acm.org/citation.cfm?id=3157382.3157391
[8]Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim Šrndić,Pavel Laskov, Giorgio Giacinto, and Fabio Roli. 2013. Evasion attacks against
machine learning at test time. In Joint European conference on machine learning
and knowledge discovery in databases. Springer, 387–402.
[9]AkhilanBoopathy,Tsui-WeiWeng,Pin-YuChen,SijiaLiu,andLucaDaniel.2019.
Cnn-cert: An efficient framework for certifying robustness of convolutional
neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence,
Vol. 33. 3240–3247.
[10]Nicholas Carlini and David Wagner. 2017. Towards evaluating the robustness of
neural networks. In Security and Privacy (SP), IEEE Symposium on. 39–57.
[11]NicholasCarliniandDavidWagner.2018. AudioAdversarialExamples:Targeted
Attacks on Speech-to-Text. (jan 2018). arXiv:1801.01944 http://arxiv.org/abs/
1801.01944
[12]GuangkeChen,SenChen,LinglingFan,XiaoningDu,ZheZhao,FuSong,and
YangLiu.2019. WhoisRealBob?AdversarialAttacksonSpeakerRecognition
Systems. arXiv preprint arXiv:1911.01840 (2019).
[13]Minhao Cheng, Jinfeng Yi, Huan Zhang, Pin-Yu Chen, and Cho-Jui Hsieh. 2018.
Seq2sick: Evaluating the robustness of sequence-to-sequence models with adver-
sarial examples. arXiv preprint arXiv:1803.01128 (2018).
[14]Dan Ciregan, Ueli Meier, and Jürgen Schmidhuber. 2012. Multi-column deep
neural networks for image classification. In CVPR. 3642–3649.
[15]MoustaphaCisse,YossiAdi,NataliaNeverova,andJosephKeshet.2017. Houdini:
FoolingDeepStructuredPredictionModels. (jul2017). arXiv:1707.05373 http:
//arxiv.org/abs/1707.05373
[16]Patrick Cousot and Radhia Cousot. 1977. Abstract interpretation: a unified
latticemodelforstaticanalysisofprogramsbyconstructionorapproximation
of fixpoints. In Proceedings of the 4th ACM SIGACT-SIGPLAN symposium on
Principles of programming languages. ACM, 238–252.
[17]Patrick Cousot and Radhia Cousot. 1992. Abstract interpretation frameworks.
Journal of logic and computation 2, 4 (1992), 511–547.
[18]XiaoningDu.2019. Towardssecureandrobuststatefuldeeplearningsystems
with model-based analysis. (2019).
[19]XiaoningDu,YiLi,XiaofeiXie,LeiMa,YangLiu,andJianjunZhao.2020. Sup-
plementary Materials for ’Marble: Model-based Robustness Analysis of Stateful
DeepLearning Systems’. https://doi.org/10.21979/N9/TTTSFK
[20]Xiaoning Du, Xiaofei Xie, Yi Li, Lei Ma, Yang Liu, and Jianjun Zhao. 2019. Deep-
Stellar:Model-BasedQuantitativeAnalysisofStatefulDeepLearningSystems.
InFSE. 477–487.
[21]JavidEbrahimi,AnyiRao,DanielLowd,andDejingDou.2017. Hotflip:White-
boxadversarialexamplesfortextclassification. arXivpreprintarXiv:1712.06751
(2017).
[22]KevinEykholt,IvanEvtimov,EarlenceFernandes,BoLi,AmirRahmati,Chaowei
Xiao,AtulPrakash,TadayoshiKohno,andDawnSong.2018. RobustPhysical-
WorldAttacksonDeepLearningVisualClassification.In 2018IEEEConference
on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA,
June 18-22, 2018. 1625–1634.
[23]Ji Gao, Jack Lanchantin, Mary Lou Soffa, and Yanjun Qi. 2018. Black-box genera-tion of adversarial text sequences to evade deep learning classifiers. In 2018 IEEE
Security and Privacy Workshops (SPW). IEEE, 50–56.
[24]Timon Gehr, MatthewMirman, DanaDrachsler-Cohen, PetarTsankov, Swarat
Chaudhuri, and Martin Vechev. 2018. Ai2: Safety and robustness certification of
neuralnetworkswithabstractinterpretation.In 2018IEEESymposiumonSecurity
and Privacy (SP). IEEE, 3–18.
[25]ArthurGill.1962. IntroductiontotheTheoryofFinite-StateMachines.McGraw-Hill.
https://books.google.com.sg/books?id=2IzQAAAAMAAJ
[26]Y. Gong and C. Poellabauer. 2017. Crafting Adversarial Examples For Speech
Paralinguistics Applications. ArXiv e-prints (Nov. 2017). arXiv:1711.03280
[27]IanGoodfellow,JonathonShlens,andChristianSzegedy.2015. Explainingand
Harnessing Adversarial Examples. In International Conference on Learning Repre-
sentations. http://arxiv.org/abs/1412.6572[28]TheodoreP.HillandJackJ.Miller.2011. HowtoCombineIndependentDataSets
fortheSameQuantity. Chaos:AnInterdisciplinaryJournalofNonlinearScience
21, 3 (2011).
[29]GeoffreyHinton,LiDeng,DongYu,GeorgeEDahl,Abdel-rahmanMohamed,
Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara NSainath, et al
.2012. Deep Neural Networks for Acoustic Modeling in Speech
Recognition: The Shared Views of Four Research Groups. IEEE Signal Processing
Magazine 29, 6 (2012), 82–97.
[30]Qiang Hu, Lei Ma, Xiaofei Xie, Bing Yu, Yang Liu, and Jianjun Zhao. 2019. Deep-
Mutation++: A Mutation Testing Framework for Deep Learning Systems. In 2019
34thIEEE/ACMInternationalConferenceonAutomatedSoftwareEngineering(ASE).
IEEE, 1158–1161.
[31]RobinJiaandPercyLiang.2017. AdversarialExamplesforEvaluatingReading
Comprehension Systems. In Proceedings of the 2017 Conference on Empirical
Methods in Natural Language Processing. 2021–2031.
[32]Guy Katz, Clark Barrett, David L Dill, Kyle Julian, and Mykel J Kochenderfer.
2017. Reluplex:AnefficientSMTsolverforverifyingdeepneuralnetworks.In
International Conference on Computer Aided Verification. Springer, 97–117.
[33]Ching-Yun Ko, Zhaoyang Lyu, Lily Weng, Luca Daniel, Ngai Wong, and Dahua
Lin.2019. POPQORN:QuantifyingRobustnessofRecurrentNeuralNetworks.InProceedingsofthe36thInternationalConferenceonMachineLearning (Proceedings
ofMachineLearningResearch),KamalikaChaudhuriandRuslanSalakhutdinov
(Eds.),Vol.97.PMLR,LongBeach,California,USA,3468–3477. http://proceedings.
mlr.press/v97/ko19a.html
[34]Erich L Lehmann and George Casella. 2006. Theory of point estimation. Springer
Science & Business Media.
[35]Jinfeng Li, Shouling Ji, Tianyu Du, Bo Li, and Ting Wang. 2018. Textbug-
ger:Generatingadversarialtextagainstreal-worldapplications. arXivpreprint
arXiv:1812.05271 (2018).
[36]JiweiLi,WillMonroe,andDanJurafsky.2016. Understandingneuralnetworks
through representation erasure. arXiv preprint arXiv:1612.08220 (2016).
[37]XinLiandDanRoth.2002. Learningquestionclassifiers.In Proceedingsofthe
19th internationalconference on Computationallinguistics-Volume 1. Association
for Computational Linguistics, 1–7.
[38]Lei Ma, Felix Juefei-Xu, Fuyuan Zhang, Jiyuan Sun, Minhui Xue, Bo Li, Chun-
yangChen,TingSu,LiLi,YangLiu,etal .2018. Deepgauge:Multi-granularity
testingcriteriafordeeplearningsystems.In Proceedingsofthe33rdACM/IEEE
International Conference on Automated Software Engineering. ACM, 120–131.
[39]Andrew L Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng,and Christopher Potts. 2011. Learning word vectors for sentiment analysis.InProceedings of the 49th annual meeting of the association for computational
linguistics:Humanlanguagetechnologies-volume1.AssociationforComputational
Linguistics, 142–150.
[40]Henry B Mann and Donald R Whitney. 1947. On a test of whether one oftwo random variables is stochastically larger than the other. The annals of
mathematical statistics (1947), 50–60.
[41]Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness,
Marc GBellemare, Alex Graves, MartinRiedmiller, Andreas K Fidjeland,Georg
Ostrovski,etal .2015. Human-levelcontrolthroughdeepreinforcementlearning.
Nature518, 7540 (2015), 529.
[42]Nicolas Papernot, Patrick Drew McDaniel, Ananthram Swami, and Richard Ha-
rang.2016. Craftingadversarial inputsequencesfor recurrentneuralnetworks.
In35th IEEE Military Communications Conference, MILCOM 2016. Institute of
Electrical and Electronics Engineers Inc., 49–54.
[43]KexinPei,YinzhiCao,JunfengYang,andSumanJana.2017. Deepxplore:Auto-
mated whitebox testing of deep learning systems. In SOSP. 1–18.
[44]JeffreyPennington,RichardSocher,andChristopherDManning.2014. Glove:
Globalvectorsforwordrepresentation.In Proceedingsofthe2014conferenceon
empirical methods in natural language processing (EMNLP). 1532–1543.
[45]MartinL.Puterman.1994. MarkovDecisionProcesses:DiscreteStochasticDynamic
Programming (1st ed.). John Wiley & Sons, Inc., New York, NY, USA.
[46]PushpendreRastogi,RyanCotterell,andJasonEisner.2016. WeightingFinite-
StateTransductionswithNeuralContext.In Proceedingsofthe2016Conference
oftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:
Human Language Technologies. 623–633.
[47]DavidSilver,AjaHuang, ChrisJMaddison,ArthurGuez,LaurentSifre,George
Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershel-
vam, Marc Lanctot, et al .2016. Mastering the game of Go with deep neural
networks and tree search. Nature529, 7587 (2016), 484.
[48]GagandeepSingh,TimonGehr,MatthewMirman,MarkusPüschel,andMartin
Vechev.2018. Fastandeffectiverobustnesscertification.In AdvancesinNeural
Information Processing Systems. 10802–10813.
[49]Bharath K. Sriperumbudur, Kenji Fukumizu, and Gert R. G. Lanckriet. 2011.
Universality, Characteristic Kernels and RKHS Embedding of Measures. J. Mach.
Learn. Res. 12, null (July 2011), 2389–2410.
[50]BharathK.Sriperumbudur,ArthurGretton,KenjiFukumizu,BernhardSchölkopf,
and Gert R. G. Lanckriet. 2009. Hilbert Space Embeddings and Metrics on Proba-
bility Measures. J. Mach. Learn. Res. 11 (2009), 1517–1561.
434[51]ChristianSzegedy,WojciechZaremba,IlyaSutskever,JoanBruna,DumitruErhan,
Ian Goodfellow, and Rob Fergus. 2013. Intriguing properties of neural networks.
arXiv preprint arXiv:1312.6199 (2013).
[52]The BBC. 2016. AI image recognition fooled by single pixel change. https:
//www.bbc.com/news/technology-41845878
[53]BjornWachter,LijunZhang,andHolgerHermanns.2007. Probabilisticmodel
checking modulo theories. In fourth international conference on the quantitative
evaluation of systems (QEST 2007). IEEE, 129–140.
[54]Tsui-WeiWeng,HuanZhang,HonggeChen,ZhaoSong,Cho-JuiHsieh,Duane
Boning, Inderjit S Dhillon, and Luca Daniel. 2018. Towards fast computation ofcertified robustness for relu networks. arXiv preprint arXiv:1804.09699 (2018).
[55]Tsui-Wei Weng, Huan Zhang, Pin-Yu Chen, Jinfeng Yi, Dong Su, Yupeng Gao,
Cho-Jui Hsieh, and Luca Daniel. 2018. Evaluating the robustness of neural
networks:Anextremevaluetheoryapproach. arXivpreprintarXiv:1801.10578
(2018).
[56]SvanteWold,KimEsbensen,andPaulGeladi.1987. Principalcomponentanalysis.
Chemometrics and intelligent laboratory systems 2, 1-3 (1987), 37–52.
[57]ZhengliZhao,DheeruDua, andSameer Singh.2017. Generatingnatural adver-
sarial examples. arXiv preprint arXiv:1710.11342 (2017).
435