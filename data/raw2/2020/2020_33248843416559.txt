Hybrid Deep Neural Networks to Infer State Models of
Black-Box Systems
Mohammad Jafar Mashhadi
University of Calgary
Calgary, Canada
mohammadjafar.mashha@ucalgary.caHadi Hemmati
University of Calgary
Calgary, Canada
hadi.hemmati@ucalgary.ca
ABSTRACT
Inferring behavior model of a running software system is quite
usefulforseveralautomatedsoftwareengineeringtasks,suchasprogram comprehension, anomaly detection, and testing. Mostexisting dynamic model inference techniques are white-box, i.e.,
they require source code to be instrumented to get run-time traces.
However,inmanysystems,instrumentingtheentiresourcecode
is not possible (e.g., when using black-box third-party libraries) or
mightbeverycostly.Unfortunately,mostblack-boxtechniquesthat
detect states over time are either univariate, or make assumptions
on the data distribution, or have limited power for learning overa long period of past behavior. To overcome the above issues, in
thispaper,weproposeahybriddeepneuralnetworkthataccepts
as input a set of time series, one per input/output signal of the
system,andappliesasetofconvolutionalandrecurrentlayersto
learn the non-linear correlations between signals and the patterns,
over time. We have applied our approach on a real UAV auto-pilot
solution from our industry partner with half a million lines of C
code.Weran888randomrecentsystem-leveltestcasesandinferred
states, over time. Our comparison with several traditional time
serieschangepointdetectiontechniquesshowedthatourapproachimprovestheirperformancebyupto102%,intermsoffindingstatechangepoints,measuredbyF1score.Wealsoshowedthatourstateclassificationalgorithmprovidesonaverage90.45%F1score,which
improves traditional classification algorithms by up to 17%.
CCS CONCEPTS
•Computingmethodologies →Neuralnetworks ;•Software
anditsengineering →Softwarereverseengineering ;Require-
ments analysis.
KEYWORDS
Recurrent Neural Network; Convolutional Neural Network; Deep
Learning;SpecificationMining;Black-boxModelInference;Time
series;
ACM Reference Format:
Mohammad Jafar Mashhadi and Hadi Hemmati. 2020. Hybrid Deep Neural
Networks to Infer State Models of Black-Box Systems. In 35th IEEE/ACM
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
onthe firstpage.Copyrights forcomponentsof thisworkowned byothersthan the
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/or a fee. Request permissions from permissions@acm.org.
ASE ’20, September21–25, 2020, Virtual Event, Australia
© 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-6768-4/20/09...$15.00
https://doi.org/10.1145/3324884.3416559International Conference on Automated Software Engineering (ASE ’20), Sep-
tember 21–25, 2020, Virtual Event, Australia. ACM, New York, NY, USA,
13 pages. https://doi.org/10.1145/3324884.3416559
1 INTRODUCTION
Automated specification mining or model inference [ 47] is the pro-
cess of automatically reverse engineering a model of an existing
software system. Behavioral models (e.g., state machines) are typi-
cally inferred from a running system by abstracting the execution
traces.Theinferredmodelsareusefulartifactsinmanyusecases
where the actual behavior (abstracted as the inferred model) of
the system is needed for analysis, such as debugging [ 2,51,68],
testing [18,57,66,75], anomalous behavior detection [ 74], and
requirements engineering [20].
Inferring a behavior model of a system in a black-box manner is
particularly interesting. In many real-world applications, the large-
scalesystemisbuiltbyintegratingmanyoff-the-shelflibrariesthat
are only available as binaries (no source code access). Thus, from a
system’s pointof view, knowing the exactbehavior of thesystem
including all the interactions between black-box units are needed
for most run-time analysis.
Mostcurrentbehavioralmodelinferencetechniquesaredynamic
analysismethods(usuallyaremoreaccuratethanstaticanalysisfor
run-timebehaviorinference)thatrequiresourcecodeinstrumen-
tationtocollectexecutiontraces[ 47].Thesemethodsareusually
helpfulinunit-levelanalysiswheretheinstrumentationisnotex-
pensive and access to the code is allowedfor the unit under study.
However, in the system-level, thorough instrumentation is more
expensive (not limited to one unit) and might not be even possi-ble for some units (black-box libraries). Therefore, for use cases
such assystem-level anomaly detection, testing, anddebugging a
black-boxbehaviormodelinferencethatworksonreadilyavailable
input/outputs of the system is crucial.
Inthispaper,weproposeadynamicanalysismethodtodetect
the internal state and the state changes in a black-box software
system usingdeeplearning. Wecollectedthe numericalvalues of
theinputsandoutputsofthesystem,inregulartimeintervalsto
create a multivariate time-series. A hybrid deep learning model
(includingconvolutionandrecurrentlayers)wasthentrainedon
these time-series to predict the state of the system at each point
intime.Thedeeplearningmodelautomaticallyperformsfeature
extractionmakingitwaymoreeffectiveandflexiblecomparedto
traditional methods.In addition, wedo not makeany assumption
about statistical properties of the data which makes it applicable to
a wide range of subjects.
We applied and evaluated this method on an auto-pilot software
(AutoPilot) used in an Unmanned Aerial Vehicle (UAV) system
2992020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE)
developed by our industry partner Winnipeg based Micropilot Inc.
Micropilotistheworld-leaderinprofessionalUAVauto-pilotwhich
developsbothhardware andsoftwarefor1000+clients (including
NASA,Raytheon,andNorthropGrumman)in85+countriesduring
thepast20+years.Weevaluatedthemethodfromtwoperspectives:howwellthemodelcandetectthepointintimewhenastatechangehappens?(RQ1:ChangePointDetection(CPD)),andhowaccurately
itcanpredictwhichstatethesystemisin,duringtheexecution?
(RQ2:StateClassification).Inaddition,inRQ3,weexploredsome
simplervariationsofourmethod(non-hybridvariations)toroughly
see how each part contributes to the overall model performance.
Comparing our approach with state-of-the-art alternatives, the
resultsshowthatourapproachperformsbetterinbothchangepointandstatedetection.Weobserved88.00%to102.20%improvementintheF1scoreofourCPD,comparedtotraditionalCPDtechniques.In
addition, we saw a 7.35% to 16.83% improvement in the F1 score of
ourstatedetection,comparedtotraditionalclassificationalgorithms
on a sliding window, over the data.
The contributions of this paper can be summarised as:
•Introducing the first (to the best of our knowledge) deeplearningarchitecturetoinferbehaviormodelsfromblack-
box software systems.
•Empiricallyevaluatingthemodelandachievingveryhigh
accuracycomparedtobaselinesusingareal-worldandlarge-
scale case study on a UAV auto-pilot system developed by
our industry partner.
Note that we have made all our source code, models, and execu-
tionscriptsavailableonline1,however,due toconfidentiality,we
can not make our dataset public.
The rest of this paper is organized as follows: In section 2 we
explain further how and in which contexts this research can be
beneficial.Theninsection3webrieflyexplainsomebackground
material this work is based on. In section 4, we explain how ourproposed model is designed. The way it was evaluated and the
results are explained in section 5. Finally, related work reviewed in
section 6, and some final remarks about the future work are made
in section 7.
2 MOTIVATION
Black-box components are ubiquitous in software development.
Reusing high-quality black-box units generally offers a better over-
all system quality and a higher productivity [ 23]. The black-box
units can be as small as a reusable library, or as large as a frame-
work (such as .NET before going open-source), or a complete piece
ofsoftwaresuchasaremotelyhostedwebservice.Therearealso
scenarios where the unit’s source code is not black-box in general,
but not accessible to a specific team that wishes to perform the
dynamic analysis.
One particular interesting use case of system-level black-box
analysis is inferring run-time state model of a control software sys-
tems, where inputs/outputs are signals to/from the system. These
inputs/outputs are typically multivariate time series, which are
already logged in such systems (no overhead for instrumentation).
The goal is automatically detecting the high-level system state and
its changes, over time.
1https://github.com/sea-lab/hybrid-netAs discussed in section 1, we partnered with an auto-pilot man-
ufacturer and performed this study on their auto-pilot software
(calledAutoPilotinthispaper).Thegoalwastodeterminethestate
of AutoPilot from its input/output signals, over time. In this sce-
nario,theinputsarethesensorreadingsgoingintoAutoPilotand
theoutputsarecommandsignalssenttocontrollermotorsofthe
aircraft,showingAutoPilot’sreactiontoeachinputateachstate.A
stateinthisexampleisthehigh-levelstageofaflightandastate
change happens when the current input values in the current state
trigger a constraint in the implementation that changes the way
the output signals are generated.
In this example, the training set will consist of input and output
values recorded during one execution of the system, as a multivari-
ate time series, along with state ids (as labels) per time stamp. One
executionoftheAutoPilotwillbethewholeflightprocessthatmay
go through a “take-off” until a successful “landing”. Depending on
theflightplan,AutoPilotgoesthroughstatessuchas“acceleration”,
“take-off”, “climbing”, “turning”, “descending”, etc.
During a flight, the AutoPilot monitors changes in the input
values and makes adjustments to its outputs in order to hold some
invariants (predefined rules). For example, if AutoPilot is in the
“holdaltitude”mode,itmonitorsthealtimeter’sreadingsandwhen
itgoesoutoftheacceptablerange,proportionateadjustmentstothe
throttle or the nose pitch will be made to get it back to the desired
altitude. This is basically how a typical feedback loop controller,such as PID or its variations work [
61]. When AutoPilot’s state
changes from “hold altitude” to “descend to X ft” state, the set of
invariantsthatAutoPilotistryingtoholdarechanged.Itmeansits
reactionstovariationsininputswillbedifferent.Inthisexample,
a decreasing altimeter reading will not trigger an increase in the
throttle anymore.
Lookingatthetimeseries,adomainexpertcanidentifywhatthe
stateofAutoPilotis,ateachpointintime;thelabelingprocess.Now
the goal is to automate this task on a test set (in practice, future
flights), assuming a training set is labeled by the experts (they only
need to identify the state change time stamps, during a flight).
Thisproblemcan betackledintwoways. Thefirstsolution isto
identify the time stamp that the state change happens (i.e., Change
Point Detection: RQ1); The more advanced solution is to predict
the exactstate pertime stamp(i.e., StateClassification: RQ2).The
classic CPD techniques on time series [ 73] are mainly applicable
onunivariatedataorputassumptionsontheinput/outputdistribu-
tions, thus not applicable inour case with multivariate inputs and
no assumptionsor knowledgeabout the states’distribution. The
classicstateclassificationtechniquesintimeseriesarealsoweakin
thattheyfailtobalancebetweenconsideringlong-termrelations
oracting locally.Theonesthat useasliding window,forexample,
donothavealong-termmemory.Theonesthatactonthewhole
dataontheotherhandaretoocoarse-grainedandinaccuratefor
this task.
Therefore,themotivationforthisstudyistoprovideablack-box
techniquethat canbeapplicable onboth CPDandstate classifica-
tionproblems,andovercomethelimitationsoftheexistingtech-niques,intermsofcapturingthenon-linearcorrelationbetween
multivariate inputs and outputs as well as learning patterns over a
longperiodoftime.Ourproposal,whichwillbeexplainedindetail
in section4, leveragesthe power ofa deep neuralnetwork (DNN)
300withtwotypesoflayersthatareparticularlyusefulforthisproblem:
a)convolutionallayerswhichdiscoverlatentfeaturesfromthedata
effectively through parameter sharing and b) recurrent layers that
play a significant role in problems dealing with time series as they
can learn long-term dependencies and seasonalities in the data.
Though our motivational example, as well as our case study,
are from the UAV auto-pilot domain, our proposed method can
be adapted to be applied to similar black-box control software
systems in domains such as IoT, intelligent video surveillance, and
self-driving cars.
3 BACKGROUND
Unlikethenumeroustechniquesintheliteratureforbehaviormodel
inference [ 19,40,48,76] which abstract a set of execution traces
intostates,ourapproachrequiresconsumingamultivariatetime-
seriesanddetectthestatechangesacrosstimeandpredicttheexact
statelabels.Thus,inthissection,webrieflyexplainthetwomain
setsofrelevantexistingtechniquesfor“ChangePointDetection”
and “State Prediction” in time-series that can serve as background
for our approach.
3.1 Change Point Detection
A fundamental tool in time-series data analysis is Change Point
Detection (CPD). It refers to the task of finding points of abruptchange in the underlying statistical model or its parameters that
could be a result ofa state transition [ 3]. There are plenty ofCPD
algorithms; many of which perform effectively on a subset of CPD
problems with some assumptions. The assumptions can be of vari-
oustypes.Forexample,onemayassumethetimeserieshasonly
one input variable (univariate) [ 26], there is only one changing
point [6], or the number of change points is known beforehand
[42], or they might assume some statistical properties on the data
[15].Thesearelimitingfactors,sincemanyoftheseassumptions
donotnecessarilyholdinourcase.CPDtechniquesarecategorized
intotwomaingroups:a)onlinemethodsthatprocessthedatain
real-time and b) offline methods that start processing the data after
receiving all the values [ 73]. Since our model inference use case of
CPD can afford waiting to collect all historical training data, we
only considered offline techniques.
In general, CPD algorithms consist of two major components: a)
thesearchmethodandb)thecostfunction[ 73].Searchmethodsare
either exact or approximate. For instance, Pelt is the most efficient
exact search method in the CPD literature, which uses pruning
[37]. Approximate methods include window-based [ 8], bottom-up
[33], binary segmentation [ 67], and more. In the window-based
segmentation a sliding window is rolled over the data and then
sumofcostsofleftandrighthalf-windowsissubtractedfromthe
costofthewholewindow.Whenthedifferencegetssignificantly
highit meansthat thediscrepancy betweenleftand righthalf of
thewindowishighandthereforeachangepointprobablyliesright
inthemiddleofthewindow.Inthebottom-upmethod,theinput
signal is split into multiple smaller parts, then using a similarity
measureadjacentsegmentsaremergeduntilnomoremergesare
feasible.Thebinarysegmentationmethodfindsonechangepoint
and splits the input into two parts around that point and then
recursively applies the same method on each part.Thecostfunctionsarealsoquitevarious,fromsimplysubtracting
each point from the mean to much more complex metrics, such as
auto-regressive cost functions [ 4], and kernel-based cost functions.
Kernel-basedcostscanhaveawidevariety,sincethekernelfunction
canbealmost arbitrary,howeverahandful ofthemsuchas linear
and Gaussian kernels are among the most popular ones [73].
In the context of our paper, we need a CPD method with no
assumptionondatadistribution,numberofchangepoints,etc.In
addition,ourCPDmethodshouldworkonmultivariatedata,andbe
abletocapturenon-linearrelationsbetweensignals.Italsoneeds
to be resilient to time lags between an input signal change andits effect on the output signal (and the systems state). There is
notraditionalCPDalgorithmsthatcoversalltheserequirements.
Therefore, we propose a novel CPD techniques that is based on
HybridDNNsandcompareitwithseveralexistingCPDtechniques
as our baselines, which are explained in details in section 5.
3.2 Convolutional and Recurrent Neural
Networks
Inbothourproblems(CPDandstateclassification),wecanseethatthechangesinsignalsaremoreinformativethantheirabsoluteval-
ues. Therefore, applying a derivation operation (or more generally
agradient)seemslikenecessary,atsomepointintheprocessing.
FaridandSimoncellilistedsomediscretederivationkernelsintheirstudy[
25],buttohaveamoregeneralizedandmoreflexiblenotion
ofdiscretederivatives,convolutionsseemslikeabetterchoiceto
apply.Nowadays,applyingconvolutionalfiltersonsignalsispretty
much a standard process in signal processing studies that leverage
deeplearning[ 53,82,85].Convolutionalneuralnetworks(CNNs)
can learn to find features in a multidimensional input while being
lesssensitivetotheexactlocationofthefeatureintheinput[ 43].
In the forward pass of a convolutional layer, multiple filters are
applied to the input. It means that in a trained neural net, multiple
features can be leaned in one single convolutional layer.
Recurrentneuralnetworks(RNN)haveshowngreatperformance
in analysing sequential data such as machine translation, time-
seriesprediction,andtime-seriesclassification[ 16,54,56,79,82,86].
RNNscancapturelong-termtemporaldependencieswhichisquite
useful for solving our problem. [ 14] For example, they might learn
that “climb” state in a UAV auto-pilot usually follows “take off”.
Therefore,whileitisoutputting“takeoff”itanticipateswhatthe
next state will probably be and as soon as its input features start
shifting,itdetectstheonsetofastatechange.Itwillhelpthemodel
to better predict the system’s behavior and be quicker to detect
statechangesinawaythatcouldhardlybeachievedwithclassic
methods.Therefore,inthispaper,wecombinetheCNNsandRNNs
to create what is known as a hybrid deep neural network [ 79]t o
use forboth CPDand state classificationproblems, in ourcontext.
4 HYBRID NEURAL NETWORK FOR STATE
INFERENCE
Inthissection,wedescribeourproposeddeeplearningapproach
for the black-box state inference task, in details.
301SoftmaxConvolutional Layers All Signals Fully Connected LayersSeq2Seq
Recurrent Layers
States over time
Black-Box
System
Inputs Outputs
Figure1:Theinputandoutputsignalsoftheblack-boxsystemarecapturedasamultivariatetimeseries;theyareprocessedin
adeepneuralnetworkthatconsistsof3sections:convolutional,recurrent,anddense(fullyconnected)topredictthesystem’sinternal state and its changes over time.
4.1 The Model Architecture
The goal of this study is to infer the states of a running software
system, over time. Given that our assumption is we don’t have
accesstothesourcecode(orpartofit),weonlyleveragethevalues
ofinputsandoutputsofthesystem,overtime.Ascanbeseenin
figure 1, we capture all the inputs and outputs of the system asa time series and then process it in a DNN. The architecture of
our proposed modelis a hybrid DNNwhich is inspired bymodels
proposed inthe fieldof HumanActivity Recognition(HAR). This
taskisquitesimilartothesubjectofourpaperinthesensethatthey
both take in a multivariate time series-data (from sensor readings)
andoutputthestateofthesystemthatgeneratedthosereadings(see
section 6.3 for more details on HAR papers). This DNN is made of
threepartsinsequence:1)Convolutional,2)Recurrent,and3)Fully
connectedlayers.Thisarchitectureaddressestheaforementioned
traditionalmethods’challenges;eachpartservesadifferentpurpose
in this process, as follows.
Convolutions, beingmore generalizedthan simplesliding win-
dows, can discover patterns and features in the signals, both in
temporal and in spatial (how signals affect each other) dimensions
[79].Theconvolutionallayers’flexibilityallowsthemtolearnsome
typicalpreprocessingoperations.Forexampleamovingaverageor
a discrete derivative can be learned as simple convolutional filters.
Theyalsohelpthemodeltobemoreresilienttovaryingtimedelays
between noticing a deviation in input signals and the reaction that
will appear in the output signals. Applying convolutional layers
in sequence has been shown to result in each layer learning more
complex features than the previous layers [ 84]. The number of lay-
ers, filters, and the kernel size are hyper-parameters that should beselected based on the size of data and the complexity of the system
beingmodeled.Usingasequenceofconvolutionswitha)increas-
ing number of filters and the same kernel size, b) same number
offiltersandincreasingkernelsize,andc)decreasingfilterswith
increasing kernel sizesare all different approachesthat have been
used in the literature by well-known architectures such as VGG
and U-net [ 64,69]. We will discuss more details of our CNN layers
in Section 4.3.
Convolutionsarequitepowerfulindiscoveringlocalfeatures.To
capturelong-termfeatures,recurrentlayerswhichlearnsequences
of data are leveraged. For example, in our case, they can learn that
“accelerate” and “take off” states only happen in the start of the
statessequence,andeach“takeoff”stateisusuallyfollowedbya“climb” state. The type of recurrent cell to use (LSTM, GRU, etc.),
howmanycellstounravelinthelayer,andthenumberoflayers
are also hyper-parameters that need to be tuned depending on the
size and complexity of system under study.
Finally, one or more dense (fully connected) layers in the end
areacommon wayofreducingthedimensions tomatchexpected
outputdimensions.Ifthereareonlytwostates,thelastlayercan
have a sigmoid activation function and be of shape L(the length of
theinput),otherwise,tomatchtheone-hotencodingoflabels,an
outputofshape L×Nswithsoftmaxactivationalongthesecond
axis (Ns) is required ( Nsbeing the number of possible states).
In termsof loss functionto optimize inthe training process,a
good choice is a dice overlap loss function, which is used in image
semanticsegmentationtasksaswell.Animportantpropertyofthis
loss function is not getting negatively affected by class imbalances
[52, 71].
4.2 Data Encoding
Theinput/outputvaluesoftheblack-boxsystemcreateamultivari-
atetime-series( Tk),whichcanbedefinedasasetof nunivariate
timeseries( Vi)ofthesamelength lk.EachVicorrespondstothe
recorded values for one of the inputs or outputs of the system:
Tk={V1k,V2k,...,Vnk} (1)
|V1k|=|V2k|=...=|Vnk|=lk (2)
Note that, as figure 1 shows, we take both inputs and outputs as
partofthetime-seriesdatatobefedasinputintoourdeeplearning
models. This is to make sure we can model state-based behavior of
thesystem,wherethecurrentstatedependsnotonlyontheinputs,
but also on the last state(s) (captured as previous outputs) of the
system. As an example, from our case study, if the outputs are not
taken into account a mid-flight “descend” state and the “approach”
state right before landing are indistinguishable, using the sensor
readings (inputs) alone.
Having such a time-series, the only remaining pieces from a
training set arethe labels. Unlike the input/output values (the fea-
tures in the data set) the labels are not usually given. Our method
to infer the labels is a supervised approach. Thus, we need the
domainexpert tomanually labeleachindividual timestamp witha
state name/ID. In practice, what they would do is to identify the
approximate time that a state change happens and assign the new
statetooneofthepreviousstateslabelsordefineanewlabelfor
this new state. Thus we encode the states information over time as
302a setof tuplesin theform of (ts,s)wheretsdenotes thetimestamp
wherethesystementeredstate s.Weshowthesetofallpossible
states with S(s∈S) and define Nsas the cardinality of this set.
CPk=/braceleftbig
(ts1,s1),(ts2,s2),...,(tsl,sl)/bracerightbig
,si∈S
Ns=|S|(3)
So in summary, the dataset consists of Npairs of the I/O values
as features and their state information as labels/braceleftbig
(X=Tk,y=
CPk)|1≤k≤N/bracerightbig
.
4.2.1 Data Preprocessing. Beforebeingfedintothemodel F(as
defined below), the inputs and labels need some preprocessing.
F(δ(T),m):RL×n×RL→SL. (4)
Torunmoreefficiently,TensorFlowexpectsallthe inputstohave
the same length. To do that, the shorter Tks should be zero-padded
tolengthL=max{lk}.Thepaddingfunction δdoesthat.Therefore,
eventually,theinputtothemodelwillbe Tksthatarerearranged
toformatensorofshape n×Lalongwithapaddingmask(denoted
withm).Themasktellsthemodelwherethetailstartssothemodel
can ignore all the zeros from there on.
ˆO=/angbracketleftˆoi∈S/angbracketrightL
i=1=F/parenleftbig/bracketleftbig
δ(V1)/intercalδ(V2)/intercal...δ(Vn)/intercal/bracketrightbig
,m/parenrightbig
m=δ(/vec1l)i.e./angbracketleftmj/angbracketrightl
j=1=1,/angbracketleftmj/angbracketrightL
j=l+1=0(5)
Hereldenotes the length of the input before padding. It is equal to
lkfor thekth training data ( Tk).
As defined in (3),CPks are tuples of (t,s)which indicate the
system have gone into state sat timet. To train the model, CPk
needstobeexpandedintoavectoroflength Ldenotedby Owhere
each element otholds the state at time t. To define it formally, the
elements can be derived from CPkusing the following formula:
O=/angbracketleft∀t∈NL:si|(tsi,si)∈CPk∧
tsi=max{tsj|(tsj,sj)∈CPk∧tsj≤t}/angbracketright(6)
For example: Suppose L=10 andCP={(0,a),(3,b),(5,c),(8,a)}
thenO=/angbracketleftaaabbcccaa /angbracketright. If there are more than two possible
states (Ns>2),Oneeds to be one-hot encoded, at this stage.
4.3 The Model Implementation
The first few layers of the model are convolutional layers. We have
used 5 convolutional layers with 64 filters each and a growing
kernelsize.The intuitionbehindthis designis thatstarting witha small kernel guides the training in a way that the first layers
learnsimplermorelocalfeaturesthatfitsintheirwindow(kernel
size). Kernel sizes started with 3 since it is a common number inthe literature for kernel sizes, then we used multiples of 5 from5 to 20. The rationale behind choosing 5 is because the samplingfrequency is 5, so each layer with a kernel size of 5
nprocesses a
wholensecondsworthofsimulationdata,ineachstep.Stoppingat
kernelsizeof20wasacompromisebetweengeneralizabilityand
modelsize.Generally,alargermodelhasmorelearningcapacity,
butitisalsomorepronetoover-fitting.Thecurrentmodelsarethe
smallest we could make the models (to avoid over-fitting), without
compromising the performance.
Same compromise was made in the second section of the model
(Recurrentlayers),thesweetspotforhyper-parametersherewas
to use two GRU layers with 128 cells each. Their output was fedintoafullyconnectedlayerwith128neuronswithaleakyReLU
(α=0.3) activationfunction [ 50] andfinally toa denselayer with
Ns=25 units with softmax activation. We used Adam optimizer
[38]thatcouldconvergein60-80epochs,i.e.validationaccuracy
plateaued. The full architecture can be seen in figure 2.
5 EMPIRICAL EVALUATION
Inthissection,weexplainourempiricalevaluationoftheproposed
approach through a case study.
5.1 The Study Objectives
The goals of this study is to evaluate our proposed method in
terms of change point detection and state inference, in comparison
to traditional techniques in this domain. Therefore, our research
questions are as follows:
5.1.1 RQ 1) How does our proposed technique perform in detect-ing the state changes? ThegoalofthisRQistoseehowclosethe
predicted state-change times are to the real state-change times.In other words, in RQ1, we do not predict the exact state labelsand are only interested in predicting the change. To answer this
question, we comparethe performance of our proposedapproach
withseveraltraditionalbaselines(see5.3.1),intermsofmodified
precision, recall, and F1 scores that are introduced in section 5.2.1.
5.1.2 RQ 2) How well does our proposed technique predict the inter-
nal state of the system? In RQ1, we are only interested in detecting
the time a state-change happens (binary classification), but here in
RQ2, we extend that and are also interested in predicting the label
ofthenewstatethatthesystemisgoinginto(multi-classclassifi-
cation). Therefore, to answer this RQ, we change the labels from a
Boolean (changed/not changed) to the actual collected labels.
NotethatforbothRQs,inourempiricalstudy,toevaluateour
approach,weusethesourcecodetocollecttheexacttimeastate-
changehappensandtheactualstatelabels(groundtruth).However,
in practice, labeling the training set is supposed to be done by the
domain expert in a black-box manner. This is not an infeasible task
or extra overhead. Monitoring the logs and identifying the current
systemstateisinfactpartofthedevelopers/testersregularpracticeduringinspectionanddebugging.Allweprovidehereisatoolthat
given a partial labeling (only on the training set), automatically
predictthestatelabelsandthestate-changetimes,forfutureflights.
Also note that even though we use the source code to label the
training set, we still look at the test set as a black-box and don’t
leak any information.
5.1.3 RQ 3) How much does the proposed model owe its perfor-
mance to being a hybrid model? Theproposedmodelarchitecture,
inspired by the related work, combines the power of convolutional
andrecurrentlayers.Ithasbeenshowninthosecontextsthatus-
ing this combination is beneficial over using a fully-convolutional
architecture or a recurrent architecture without any help from con-
volutions. To answer this question we trained two other models
onewithouttheconvolutionalpartandonewithouttheGRUlayers.
We compare these two with the proposed model.
303GRU
GRU
Leaky ReLu
SoftmaxConv1D 3 Conv1D 10 Conv1D 5 Conv1D 15 Conv1D 20 Seq2Seq GRU units Fully Connected Layers
Figure2:Modelarchitectureinanutshell.Tandemconvolutionallayerswithincreasingkernelsizefedintotwosequence-to-
sequencerecurrentlayerswith128GRUcellseach,whichisthenfedintodenselayerstooutputthepredictedsystemstate,as
alistofone-hotencodedstates. ˆOwillbetheresultofapplyingargmaxoperationonthelastlayer’soutput. L=18000,Ns=25
5.2 Evaluation Metrics
5.2.1 RQ1 (CPD) Performance Metrics. Given that in RQ1 there
isaninherentclassimbalance(therearefarmorepointswherea
change has nothappened compared to points with a state-change
positive label), we avoid using accuracy and report both precision
andrecall.However, theoriginalprecision/recallmetricsrequire
some modifications due to the difficulty of predicting the exact
timestampthatastate-changehappened.Tohandlethis,similarto
related work [ 73], we use a tolerancemargin τ. If a detected state-
change (∈ˆCPk) is within ±τof a true change ( ∈CPk) , we call the
predictionaTruePositive,otherwiseitisaFalsePositive.Similar
adjustment to definition is applied for True Negative and False
Negative.Formallyspeaking,wedefinepredictedchangepointsfor
k-th sample as:
ˆCPk=/braceleftbig
(t,ˆot)|ˆot/nequalˆot−1/bracerightbig
(7)
Please note that in (7),ˆotrefers tot-th element of output vector
ˆO, as previously defined in (5). Based on that the confusion matrix
elements are calculated as:
TP=/barex/barex/barex/braceleftbig
(ˆt,ˆs
t)∈ˆCPk/barex/barex∃(t,s
t)∈CPks.t.|t−ˆt|<τ/bracerightbig/barex/barex/barex
FP=/barex/barex/barex/braceleftbig
(ˆt,ˆs
t)∈ˆCPk/barex/barex∄(t,s
t)∈CPks.t.|t−ˆt|<τ/bracerightbig/barex/barex/barex
FN=/barex/barex/barex/braceleftbig
(t,s
t)∈CPk/barex/barex∄(ˆt,ˆs
t)∈ˆCPks.t.|t−ˆt|<τ/bracerightbig/barex/barex/barex(8)
With these in mind, we measure precision, recall, and their har-
monicmeanF1Scorewiththreevaluesfor τ:1,3,and5seconds.
The smaller the tolerance is the stricter the definitions become and
the lower the numbers are.
5.2.2 RQ2 (State detection) metrics. In RQ2, we have a multi-class
classification problemand thusmultiple precisions/recallswill be
calculated, one per class (state label). We then report the mean
value across all classes.
Ps=/braceleftbigˆst∈ˆOk/barex/barexˆs
t=s/bracerightbig
Ts=/braceleftbig
st∈Ok/barex/barexs
t=s/bracerightbig
TPs=/braceleftbigˆst∈Ps/barex/barexˆs
t=st∈Ok/bracerightbig(9)
Precision =1
NsNs/summationdisplay.1
s=1|TPs|
|Ps|,Recall=1
NsNs/summationdisplay.1
s=1|TPs|
|Ts|
5.2.3 RQ3. We used the same metrics as RQ1 and RQ2.5.3 Comparison Baselines
5.3.1 RQ1 (CPD) baselines. Weused ‘ruptures’librarydeveloped
byauthorsofarecentCPDsurveystudy[ 73].Itprovidesamodular
framework forapplying several CPD algorithmsto univariate and
multivariate data. As mentioned earlier two main elements of a
CPD algorithm in their survey are the search method and the cost
function.
We used Pelt [ 37] as the most efficient exact search method. As
examplesofapproximatesearchmethods,weappliedbottom-up
segmentation and window-based methods using a default window
size of 100 [ 33]. However, after trying to run Pelt algorithm, we
realizedthatittakesprohibitivelylongertoruncomparedtothe
approximate methods without providing much better results, so
we only use the bottom-up and the window-based segmentation
methods, as our CPD baselines.
Forthecostfunction,wetried“LeastAbsoluteDeviation”,“Least
Squared Deviation”, “Gaussian Process Change”, “Kernelized Mean
Change”, “Linear Model Change”, “Rank-based Cost Function”, and
“Auto-regressive model change” as defined in the library. Their
parameters were left as default. To optimize the number of change
points a penalty value (linearly proportionate to the number of
detected change points) is added to the cost function, which limits
thenumberofdetectedchangepoints,thehigherthepenaltythe
fewerreportedchangepoints.Wetriedthreedifferentratios(100,
500, and 1000) for the penalty.
5.3.2 RQ2 (Multi-class classification) baselines. Weusedasliding
windowofwidth woverthe10time-seriesvaluesandthenflattened
it to make a vector of size 10 was the features. For the labels, we
used one-hot encoded state of the system. The window sizes were
chosen as same as the sizes of convolutional layers’ kernel sizes
(3, 5, 10, 15, 20), to make the baselines better comparable with our
method.WeusedScikit-learn’simplementationoftheclassification
algorithms:Aridgeclassifier(LogisticregressionwithL2regular-
ization)andthreedecisiontrees.Theridgeclassifierwasconfigured
tousethebuilt-incrossvalidation toautomaticallychosethebest
regularization hyper-parameter αin the range of 10−6to 106. Each
decision tree was regularized by setting “maximum number of fea-
tures”and“maximumdepth”.For“maximumnumberoffeatures”
we tried no limits,√
10w, andlog210w. To find best “maximum
depth” we first tried having no upper bound and observed how
deep the tree grows; then we tried multiple numbers less than the
maximum, until a drop in performance was observed.
304Table 1: The n=10collected I/Os of AutoPilot. The inputs
are sensor readings and the outputs are the servo position
update commands. All these I/Os over time are used as theinputs of the state prediction model.
Inputs
Pitch Theanglethataircraft’snosemakeswiththehori-
zon around lateral axis
Roll Theangleofaircraft’swingsmakewiththehorizon
around longitudinal axis
Yaw The rotation angle of aircraft around the vertical
axis
Altitude AGL3Altitude
Airspeed Speedoftheair craft relative to the air
Outputs
Elevator Control surfaces that control the PitchAileron Control surfaces that control the RollRudder Control surface that controls the YawThrottle Controller of engine’s power, ranges from 0 to 1Flaps
Surfaces of back of the wings that provide extra lift
at low speeds, usually used during the landing
5.3.3 RQ3 Hybrid vs. Homogeneous baselines. Wecomparedtwo
versions of the model, one fully convolutional and one fully re-
current,withthefullhybridmodeltoseeifcombiningRNNsand
CNNs has an added value or the same results could be achieved
using only one type of layers.
5.4 Dataset and the Data Collection Process
We ran 948 existing test cases from MicroPilot’s test repository
using a software simulator2and collected the logged flight data,
over time. The test cases are system-level tests. Each test caseincludes a flight scenario for various supported aircraft. A flight
scenariogoesthroughdifferentphasesinaflightsuchas“takeoff”,
“climb”,“cruise”,“hittingwaypoints”,and“landing”.Wesampled
input and output values (listed in Table 1) at 5 Hz rate, which isthe rate that AutoPilot reads the sensor values and performs the
calculations required to update its output values at. Out of the 948
flight logs, we omitted 60 that were either too short or too long
(shorter than 200 samples or longer than 20k samples). Figure 3
showsthedistributionoftheremainingloglengths.Themaximum
length (L) was 18,000 samples.
The dataset was randomly split into three chunks of 90%, 5%,
and 5% for training, validation, and testing, where each sample
corresponds to one test execution. Note that separate test and vali-
dationsetsareneededtofacilitateproperhyper-parameterstuning,
without leaking information.
2It isdeveloped byMicroPilot Incand providesan accuratesimulation ofthe aerody-
namic forces on the aircraft, the physical environment irregularities (e.g. unexpected
wind gusts), and noises in sensor readings
3Above Ground Level
Figure 3: Distribution of flight log lengths for the N=888
logs(outoftheoriginal948availablelogs)whichwerekeptin the dataset (200 ≤l
k≤20,000)
5.5 Experiment Execution Environment
Trainingandevaluationofthedeeplearningmodelwasdoneon
a single node running Ubuntu 18.04 LTS (Linux 5.3.0) equipped
withIntelCore i7-9700CPU,32gigabytesof mainmemory,and8
gigabytesofGPUmemoryonaNVIDIAGeForceRTX2080graphics
card. The code was implemented using keras on TensorFlow 2.0.
Thebaselinemodelscouldnotfitonthatmachine,sotwonodes
on Compute Canada’s Beluga cluster, one with 6 CPUs and 75GiB
ofmemoryandonewith16CPUsand64GiBofmemory,wereused
to train and evaluate them.
5.6 Results
Inthissection,wepresenttheresultsoftheexperimentsandanswer
the two research questions.
5.6.1 RQ1 Results: CPD Performance. Table 2 shows the results of
runningCPDalgorithmsforvariousconfigurations(asdescribed
in5.3.1).Foreachsearchmethodandcostfunctionpaironlyone
of the penalty values which resulted in the highest F1 scores for all
τvalues is reported.
The first observation from the results is that as values of τin-
creasesthescoresgetbetter.Thiswasexpected,sincelargervalues
relax the constraints onwhich detected change pointsareconsid-
ered asa true positive. Anotherobservation is thatthe bottom-up
segmentation consistently outperforms the window-based segmen-
tationmethod.Wecanalsoseethatthelinearcostfunctionbeats
all the other ones, in terms of precision. The Gaussian cost func-tion achieves much higher recall values costing it a huge loss in
precision.Itmeansthiscostfunctionresultsindetectingnumerouschangepointsspreadacrossthetimeaxis,sothereisagoodchance
of having at least one change point predicted close to each true
changepoint(hencethehighrecall),butalsotherearealotoffalse
positives, which leads to a low precision.
Measuring the same metrics on how our model performs on the
test data shows better scores, almost twice the F1 score of the bestperforming baseline (see Table 3). Please note that unlike machine
learningalgorithms(suchasours),CPDalgorithmsdonothavea
separate training and testing phases. This fact works in their favor
(by using the entire dataset for prediction and not just the training
set), but still our model outperforms them.
305Table2:Changepointdetectionprecision,recall,andF1-scorecalculatedforthebaselinemethodsusingthreevaluesoftoler-
ance (τ) for multiple configurations.
Cost Function Search Method Penalty Prec. Recall F1 Prec. Recall F1 Prec. Recall F1
τ=1s τ=3s τ=5s
Autoregressive ModelBottom Up 1000 10.43% 75.44% 18.33% 21.21% 80.32% 33.55% 28.94% 81.22% 42.68%
Window Based 100 2.94% 3.98% 3.38% 8.53% 11.41% 9.76% 12.89% 17.54% 14.86%
Least Absolute DeviationBottom Up 500 7.32% 52.54% 12.85% 17.52% 87.73% 29.20% 25.02% 88.95% 39.05%Window Based 500 5.24% 8.31% 6.42% 15.20% 24.03% 18.62% 21.79% 38.25% 27.76%
Least Squared DeviationBottom Up 1000 7.44% 85.09% 13.68% 16.40% 89.81% 27.74% 24.16% 90.47% 38.14%Window Based 500 3.59% 6.79% 4.70% 10.27% 16.51% 12.66% 16.18% 26.84% 20.19%
Linear Model ChangeBottom Up 100 37.59% 28.98% 32.73% 45.20% 38.39% 41.52% 48.07% 41.36% 44.46%
Window Based 500 6.70% 4.14% 5.12% 20.50% 13.05% 15.95% 38.78% 26.77% 31.67%
Gaussian Process ChangeBottom Up 100 3.77% 92.23% 7.25% 8.99% 92.23% 16.39% 13.53% 92.23% 23.60%
Window Based 100 2.94% 3.95% 3.37% 8.69% 11.50% 9.90% 13.64% 18.30% 15.63%
Rank-based Cost FunctionBottom Up 100 13.45% 60.19% 21.98% 19.49% 80.10% 31.35% 22.98% 87.23% 36.38%Window Based 100 8.10% 13.70% 10.18% 15.72% 30.73% 20.80% 21.38% 46.64% 29.32%
Kernelized Mean ChangeBottom Up 100 4.13% 3.24% 3.63% 12.22% 8.14% 9.77% 15.38% 10.58% 12.54%Window Based 100 2.82% 3.00% 2.91% 10.14% 8.40% 9.19% 13.64% 12.61% 13.10%
Table 3: Change point detection precision, recall, and F1-
score calculated, on the test data, for our proposed model,usingthreevaluesoftolerance( τ)comparedwiththerespec-
tiveτ’s best F1 score among baseline methods
τPrec. Recall F1 score BaselineF1
1s 56.77% 79.32% 66.18% 32.73%3s 69.58% 88.88% 78.06% 41.52%5s 79.82% 91.87% 85.42% 44.46%
In terms of execution cost, running all 42 different settings of
CPDalgorithmsonthewholedatasettookabitover12hoursinthecloudusing16CPUsand64GBofmainmemory.Thedeeplearning
model on the other hand takes about an hour to train (which only
needstobe doneonce),on asmallermachine (seesection5.5). It
made predictions on the whole dataset in less than a minute. So to
answerRQ1,ourmethodhasshown (66.18/32.73)−1=102.20%
improvementinF1scorewith τ=1s,(78.06/41.52)−1=88.00%
withτ=3s, and(85.42/44.46)−1=92.13% with τ=5s; almost
doubling the score compared to the baselines.
Our model, which requires less memory compared to tra-
ditionalCPDalgorithms,improvedtheirbestperformance
by up to 102%, measured by F1 score, in less execution
time.
5.6.2 RQ2 Results: Multi-class Classification Performance. Toan-
swer RQ2, we first compare different configurations of the baselinemethodsusingtheF1score(harmonicmeanofprecisionandrecall)
on the test data. The results are presented in Table 4.
Comparing the baseline methods with our proposed method
(the last row) in Table 4 shows that our model outperforms all
baselines. Comparing it with the model with the best F1-scoreTable 4: Precision, recall, and F1 score of ridge classifiers
(linear classifiers with L2 regularization) and decision tree
classifiers(DT)withdifferentslidingwindowwidths( w).For
eachalgorithmoneach wseveralhyper-parameterswereap-
plied producing 152 different models. In this table, we onlyshowtheresultsofthebestperformingmodelineachgroup.
wClassifierMax
DepthMax
FeaturesPrec. Recall F1
3Ridge - - 71.39% 20.73% 32.13%
3 DT - - 69.21% 82.36% 75.21%
5Ridge - - 69.15% 21.89% 33.26%
5 DT 100 - 68.37% 83.16% 75.04%
10Ridge - - 71.97% 24.02% 36.02%
10 DT 260 - 67.94% 79.14% 73.12%
15Ridge - - 76.87 25.90% 38.75%
15 DT -√
10w69.06% 80.76% 74.45%
20Ridge - - 80.38% 26.50% 39.86%
20 DT 175√
10w73.21% 82.16% 77.42%
OurProposed Method 86.29% 95.04% 90.45%
shows a(86.29/73.21)−1=17.87% improvement in precision as
well as a (95.04/82.16)−1=15.68% improvement in recall that
means(90.45/77.42)−1=16.83%overallimprovementinF1-score.
To have a feeling of how good our predictions are in practice,
Figure4showstheoutputofourmodelsidebysidewiththegroundtruth.ThehorizontalaxisshowssampleID(time)andthestatesare
color coded. As it is seen, our algorithm performs better when the
state changes are farther apart. Also there are some state changes
that happen quite briefly which are not detected. That is not toa great surprise since it takes some time for state changes to be
reflected in the outputs and those might not have got any chance.
306The classical models only see one window of the data at a time,
convolutionallayersontheotherhandaremoregeneralizedand
flexible since each filter in each layer is comparable to a sliding
window.AswesawinTable4,alargerwindowsizemeansahigher
performance.However,itgetssignificantlymoredifficulttotrain
a model with large window sizes. In addition, convolutions can
automatically learn preprocessing steps that could be beneficialsuch as a moving average. Each convolutional filter can learn a
linear combination of its inputs. So when the convolutional layers
are stacked oneach other, withnon-linearactivation functionsin
between, the hypothesis space they can learn becomes quite large,
probablymuchlargerthanmostoftheclassicalMLalgorithmshere.
Also,theyarestillquiteefficient(moreefficientthanbaselines)due
to parameter sharing and their high parallelizability.
Thefactthattheperformanceimprovesasthewindowsizein-
creasesindicatesthepositiveeffectofbeingabletoseelonger-term
relations in detecting the system’s state. Recurrent cells (such as
GRU) can capture long-term dependencies (that do not necessarily
fallintoonewindow)andlearnsequences.Thisisoneofthemajor
differences between an RNN model and others, such as decision
trees,whichdonothavesuchanotionofa“long-termmemory”as
LSTM/GRU neural networks do. All a decision tree could see is the
values in a sliding window.
In terms of the training complexity (time and memory), our
methodissuperioraswell.Thatcanlargelybeattributedtotheuse
ofdeeplearning.Inbaselinemodels,asthewindowsize wgrows
the training and evaluation complexity grows, up to a point that
theyranoutofmemory–consumingallthe47GBofmainmemoryandswaparea.Thisforcedustotraintheminthecloud.Meanwhile,
as mentionedearlier, the deep learningmodel could betrained on
a8GBGPUinroughlyanhour.(seesection5.5forthemachines’
specs).Also,thedecisiontreetrainingwasnotparallelizedusing
only one core of the CPU, while virtually all deep learning models
can be heavily parallelized on a GPU/TPU.
Our model, which requires less than half as many CPUs
and70%asmuchmemorycomparedtothebestperforming
classicalMLmodel,improvedtheirbestperformanceby
up to 17%, measured by F1 score, in less execution time.
5.6.3 RQ3 Results: Hybrid vs. Homogeneous model. Astheresultsin
table5suggest,usingahybridarchitectureinthisproblemdelivers
more thansum of its parts,outperforming the fullyconvolutional
andfullyrecurrentbaselines.WecanalsoseehowtheRNNbaselinegotcloserresultstothefullmodel,suggestingtheimportantroleitplays in capturing long-term relations in the data and inferring the
system’s internal state.
You might also notice that the results in the last column differ a
little(around1%inabsolutevalue)fromtheircorrespondingresultsintables4and3.Thatisduetorandomizationsinsplittingthedata
into training, testing, and validation sets.Table5:Comparingthehybridmodel’sperformanceinboth
regards with the the its homogeneous sub-models.
τ RNN only CNN only Full Model
Precision 1s 45.31% 38.12% 53.84%Recall 1s 60.56% 58.50% 67.94%F1 1s 51.84% 46.16% 60.06%
Precision 3s 56.06% 50.97% 72.00%Recall 3s 78.00% 69.12% 88.56%F1 3s 65.25% 58.69% 79.44%
Precision 5s 68.75% 67.38% 78.56%Recall 5s 81.81% 73.75% 93.75%F1 5s 74.69% 70.44% 85.50%
Classification Prec. 81.56% 71.56% 88.44%Classification Recall 91.88% 86.88% 94.50%Classification F1 86.44% 78.44% 91.38%
Thehybridarchitectureperformsbetterthanacomparable
RNN model or fully convolutional model, however the re-
currentsectionplaysamoreimportantroleinthemodel’s
performance.
5.7 Limitations and Threats to Validity
One of the limitations of our approach is that it might miss an
input-outputinvariantcorrelation.Itcanhappenwhentheinput
remainsconstant oritchanges toolittletoreveal itsrelationwith
certainoutputs,thereforeremainingunobserved.Howeverthisisa
sharedshortcomingofdynamicanalysisapproaches.Weassume
that during the data collection, sampling happens in regular inter-
vals;ourapproachprobablywillhaveahardtimeachievinghigh
performances, working on unevenly spaced time-series data.
Interms ofconstructvalidity, weareusing standardmetricsto
evaluatetheresults.However,theuseoftolerancemarginshould
be taken with caution since it is a domain-dependant variable and
can change the final results. To alleviate this threats, we have used
multiplemarginsandreportedallresults.Intermsofinternalva-
lidity threats, we reduced the threat by not implementing the CPD
baselinesbyourselvesandratherreusingexistinglibraries.Interms
of conclusion validity threats, we have used many (888) real test
casesfromMicroPilot’stestrepositoryandprovidedapropertrain-
validation-test split for training, tuning, and evaluation. Finally,
intermsofexternalvaliditythreats,ourstudysuffersfrombeing
limited to only one case study. However, the study is a large-scale
real-worldstudywithmanytestcases.Weplantoextendthiswork
with more case studies from other domains, to increase its general-
izability.
307Figure 4: Evaluation of the model on 30 random test data. Each graph shows the states in one run of the system. The colors
show the states. The top-half of each plot depicts model’s prediction of the system states ( ˆO) and the bottom-half shows the
true labels( O). Since the output is one-hot encoded, the item with the most probability is used as the predicted label at each
point in time. X-axis is the time axis. Only the first 600 samples (2 minute of simulation) are shown to improve legibility.
6 RELATED WORK
6.1 Time Series Change Point Detection
Change point detection is a well-studied subject due to its wide
rangeof applications[ 8].Several statisticalandalgorithmic meth-
ods have been tried to tackle several variations of this problem
[15,17,28,30,41,44,55,60,63,65,78,80,81]. The models vary
based on: whether the whole data is available at once (offline) or
itisbeinggeneratedonthego(online),whethertherearestatisti-
calassumptionsaboutthedatadistribution[ 31,72],whetherthe
number of change points is known [ 73], or whether we are dealing
with a univariate or a multivariate time series, etc.
Ives and Dakos utilized locally linear models and used statistical
significance test to determine at which point the changes in model
parameters are large enough to signal a change in the state [ 32].
Blythe et al., used subspace analysis to reduce data dimensionality
to keep the most non-stationary dimensions. This process helps
detectingchangepointsmoreeffectively[ 13].Severaltechniques
haveusedpenaltyfunctionstofindmodelsthatbestfiteachseg-
mentofthesignal[ 34,36,41,42,58].Desobryetal.,andHidoetal.,
proposedmethodstoindirectlyuseclassifierssuchasSVMtodetect
changepoints[ 21,29,35].Weappliedtheirapproachonourdata
in early stages of the research but it could not perform as others.Lee et al., trained deep auto encoder networks that learns latent
features in the data to detect change points [ 45]. Ebrahimzadeh et
al., proposed what they call a pyramid recurrent neural networkarchitecture, which is resilient to missing to detect patterns that
are warped in time [22].
TherearealsoafamilyofmethodsbasedonBayesianmodelsthat
focus on finding changes in parameters of underlying distributions
of the data [1, 5, 7, 24, 45, 62].
Makingassumptionsaboutthedatasuchasitsdistributionorthe
distributionofchangepointsacrossthetimeandrelyingonbasic
statisticalpropertiesarethetwomajorshortcomingsoftraditional
CPD methods [45], which our proposed approach has overcome.
6.2 State Model Inference
Roughly speaking, dynamic EFSM4inference algorithms generally
takeatraceof“events”(alongwithperhapssomevariablevalues)astheirinput[
76]toinferageneralizedfinitestatemachine.Theyuse
the events to find the state transitions and the values for detecting
invariantsandgeneratingtheguardconditionsonthetransitions.
kTails,GkTail,EDSM,andMINTareexamplesofthesealgorithms,
each improving upon the previous one [12, 40, 49, 76].
Walkinshawet al.,proposed analgorithmand developeda tool
for state model inference [ 76]. Their work is based on previous
endeavorsonstatemergingalgorithmssuchasgk-tailandk-tails
[12,49]. These methods require an execution trace of the program
4ExtendedFiniteStateMachines,arespecialkindofstatemachinesthathavecondi-
tionalexpressionscalled“transitionguards”ontheirtransitions[ 49].Astatetransition
can only happen if the transition guard evaluates as true.
308consistingofalistof“events”thatoccurredduringprogramexe-
cution, such as function calls, system calls, transmitted network
data,etc.Krkaetal.,performedanempiricalstudyon4different
categories of model inference algorithms to figure out what makes
each group of methods more effective [ 39]. Beschastnikh et al.,
proposeda methodto mineinvariantsfrom partiallyorderedlogs
fromconcurrent/distributedsystems[ 10].Invariantscanbeused
to augment state models [ 9,11]. Groz et al., use machine learn-
ingtoheuristicallyinferstatemachinemodelsofaun-resettableblack-box system [
27], however a significant difference between
ourmethodandtheirsisthattheirmethodstillreliesondiscrete
events (such as HTTP request and responses) while our methoddoes not assume that the input and outputs contain any kind of
“events” happeningatcertain times.Ourmethod aimstosearch for
sucheventsaschangepointsinacontinuousstreamofdataastime
series.
6.3 Using Deep Learning on Time Series Data
Human activity recognition (HAR) is a well researched task which
is quite relevant to the problem of black-box model inference. InHAR, just like in our context, a multivariate time series data is
createdfromvarioussensorsonahumanbody.Thegoalistofigure
our what was the activity that human was performing in different
time intervals. The sensors can be body worn accelerometers, or
more generic sensors such as the ones found in a smart watch or a
smartphone. Murad et al., [ 54] have shown deep RNNs outperform
fullyconvolutionalnetworksanddeepbeliefnetworksinHARtask.Hybridmodelsarethecombinationofsomedeeparchitectures[
77],
suchasaCNN+RNNoraCNN+afullyconnectednet.Morales
et al., have shown the former preforms better than the latter inHAR [
53]. Yao et al., [ 83] introduced a CNN + RNN architecture
that outperforms the state of the art both in classification and in
regressiontasks.Similarresultshavebeenshowninotherworks
such as [56, 70, 87], as well.
Another related topic here is the time series classification. How-
ever, time series classification techniques often output only onelabel classifying entire data, thus not applicable in our context.
Whatismorerelatedtoourproblemiscalled“segmentation”,us-
ing the computer vision terminology (not be confused with timeseries segmentation, such as [
46]). U-net is one of the promising
auto-encoder architectures for image segmentation [ 64]. Perslev et
al., developed a similar idea for time-series to capture long-term
dependenciesandcalleditU-time[ 59].Itisfullyconvolutionaland
doesnotusememorycells(recurrentcells).Afullyconvolutional
model can perform very well, since convolutions operate locally
and image segments are large chunks of pixels in the 2D space and
capturinglocalfeaturesusingneighbouringpixelsisquiteuseful.
However, it cannot necessarily be as powerful on a more limited
1D data of time-series with different characteristics from an image.
Thisstudy’sdesignisoptimizedforthetaskofsleepphasedetec-
tion,whichdoesnothaveveryclearboundariesbetweenstatesand
alsothestatechangesarenotveryfrequent.Therefore,thesame
methoddoesnotnecessarilygeneralizetotaskssuchasours,where
we cannot make assumptions about frequency of state changes.7 SUMMARY AND FUTURE WORK
In this paper, we introduced a hybrid CNN-RNN model that can be
usedfor bothCPDandstate classificationproblemsinmultivariatetimeseries.Theproposedapproachcanbeusedasablack-boxstatemodel inference for variety of use cases such as testing, debugging,andanomalydetectionincontrolsoftwaresystems,wherethereare
several input signals that control output states. We have evaluated
our approach on a case study of a UAV auto-pilot software fromour industry partner with 888 test cases and showed significant
improvementinbothchangepointdetectionandstateclassification.
In the future, we are planning to extend this research with more
casestudiesfromopensourceauto-pilots.Inaddition,bettertuning
of hyper-parameters will be explored. Finally, we plan to examine
the use of transfer learning to reduce the labeling overhead.
ACKNOWLEDGMENTS
We would like to thank the anonymous reviewers for their con-
structivecomments.WeacknowledgethesupportoftheNatural
Sciences and Engineering Research Council of Canada (NSERC)
[funding reference number CRDPJ/515254-2017].
REFERENCES
[1]RyanPrescottAdamsandDavidJCMacKay.2007. BayesianOnlineChangepoint
Detection. stat1050 (2007), 19.
[2]Ziad A Al-Sharif. 2009. An Extensible Debugging Architecture Based on a Hybrid
Debugging Framework. Ph.D. Dissertation. University of Idaho.
[3]SamanehAminikhanghahiandDianeJCook.2017. Asurveyofmethodsfortime
serieschangepointdetection. Knowledgeandinformationsystems 51,2(2017),
339–367.
[4]DanieleAngelosanteandGeorgiosBGiannakis.2012. Grouplassoingchange-
points in piecewise-constant AR processes. EURASIP Journal on Advances in
Signal Processing 2012, 1 (2012), 70.
[5]Jushan Bai. 1997. Estimation of a change point in multiple regression models.
Review of Economics and Statistics 79, 4 (1997), 551–563.
[6]Jushan Bai, Robin L Lumsdaine, and James H Stock. 1998. Testing for and dating
common breaks in multivariate time series. The Review of Economic Studies 65, 3
(1998), 395–432.
[7]DanielBarryandJohnAHartigan.1993. ABayesiananalysisforchangepoint
problems. J. Amer. Statist. Assoc. 88, 421 (1993), 309–319.
[8]Michèle Basseville, Igor V Nikiforov, et al .1993.Detection of abrupt changes:
theory and application. Vol. 104. prentice Hall Englewood Cliffs.
[9]Ivan Beschastnikh, Yuriy Brun, Michael D Ernst, and Arvind Krishnamurthy.
2014. Inferringmodelsofconcurrentsystemsfromlogsoftheirbehaviorwith
CSight.In Proceedingsofthe36thInternationalConferenceonSoftwareEngineering.
468–479.
[10]Ivan Beschastnikh, Yuriy Brun, Michael D Ernst, Arvind Krishnamurthy, and
ThomasEAnderson.2011. Miningtemporalinvariantsfrompartiallyordered
logs. InManaging Large-scale Systems via the Analysis of System Logs and the
Application of Machine Learning Techniques. 1–10.
[11]Ivan Beschastnikh, Yuriy Brun, Sigurd Schneider, Michael Sloan, and Michael D
Ernst.2011. Leveragingexistinginstrumentationtoautomaticallyinferinvariant-
constrained models. In Proceedings of the 19th ACM SIGSOFT symposium and the
13th European conference on Foundations of software engineering. 267–277.
[12]Alan W Biermann and Jerome A Feldman. 1972. On the synthesis of finite-state
machines from samples of their behavior. IEEE transactions on Computers 100, 6
(1972), 592–597.
[13]DuncanA.J.Blythe,PaulVonBunau,FrankC.Meinecke,andKlausRobertMuller.
2012. Featureextractionforchange-pointdetectionusingstationarysubspace
analysis. IEEETransactionsonNeuralNetworksandLearningSystems 23,4(apr
2012), 631–643. http://ieeexplore.ieee.org/document/6151166/
[14]ZhengpingChe,SanjayPurushotham,KyunghyunCho,DavidSontag,andYan
Liu. 2018. Recurrent Neural Networks for Multivariate Time Series with Missing
Values.Scientific Reports 8, 1 (dec 2018), 1–12. arXiv:1606.01865
[15]JieChenandArjunKGupta.2011. Parametricstatisticalchangepointanalysis:
with applications to genetics, medicine, and finance. Springer Science & Business
Media.
[16]Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau,Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase
309representations using RNN encoder-decoder for statistical machine translation.
arXiv preprint arXiv:1406.1078 (2014).
[17]Md Foezur Rahman Chowdhury, S-A Selouani, and D O’Shaughnessy. 2012.
Bayesian on-line spectral change point detection: a soft computing approach for
on-line ASR. International Journal of SpeechTechnology 15, 1 (2012), 5–23.
[18]Valentin Dallmeier, Nikolai Knopp, Christoph Mallon, Gordon Fraser, Sebas-
tian Hack, and Andreas Zeller. 2011. Automatically generating test cases forspecification mining. IEEE Transactions on Software Engineering 38, 2 (2011),
243–257.
[19]Valentin Dallmeier, Christian Lindig, Andrzej Wasylkowski, and Andreas Zeller.
2006.MiningobjectbehaviorwithADABU.In Proceedingsofthe2006international
workshop on Dynamic systems analysis. 17–24.
[20]ChristopheDamas,BernardLambeau,PierreDupont,andAxelVanLamsweerde.
2005. Generating annotated behavior models from end-user scenarios. IEEE
Transactions on Software Engineering 31, 12 (2005), 1056–1073.
[21]Frédéric Desobry, Manuel Davy, and Christian Doncarli. 2005. An online kernel
change detection algorithm. IEEE Transactions onSignal Processing 53, 8(2005),
2961–2974.
[22]ZahraEbrahimzadeh,MinZheng,SelcukKarakas,andSamanthaKleinberg.2019.
Deep Learning for Multi-Scale Changepoint Detection in Multivariate Time Series.
Technical Report. arXiv:1905.06913v1
[23]StephenHEdwards.2001.Aframeworkforpractical,automatedblack-boxtesting
of component-based software. Software Testing, Verification and Reliability 11, 2
(2001), 97–111.
[24]Chandra Erdman and John W Emerson. 2008. A fast Bayesian change pointanalysisforthesegmentationofmicroarraydata. Bioinformatics 24,19(2008),
2143–2148.
[25]Hany Farid and Eero P Simoncelli. 2004. Differentiation of Discrete Multidi-
mensionalSignals. IEEETRANSACTIONS ONIMAGEPROCESSING 13,4(2004),
496–508.
[26]PiotrFryzlewiczetal .2014. Wildbinarysegmentationformultiplechange-point
detection. The Annals of Statistics 42, 6 (2014), 2243–2281.
[27]RolandGroz,AdenilsoSimao,NicolasBremond,andCatherineOriat.2018. Re-
visiting AI and testing methods to infer FSM models of black-box systems. In
2018IEEE/ACM13thInternationalWorkshoponAutomationofSoftwareTest(AST) .
IEEE, 16–19.
[28]Abeer Hasan, Wei Ning, and Arjun K Gupta. 2014. An information-based ap-
proachtothechange-pointproblemofthenoncentralskewtdistributionwith
applications to stock market data. Sequential Analysis 33, 4 (2014), 458–474.
[29]Shohei Hido, Tsuyoshi Idé, Hisashi Kashima, Harunobu Kubo, and HirofumiMatsuzawa. 2008. Unsupervised change analysis using supervised learning.InPacific-Asia Conference on Knowledge Discovery and Data Mining. Springer,
148–159.
[30]DA Hsu. 1982. A Bayesian robust detection of shift in the risk structure of stock
market returns. J. Amer. Statist. Assoc. 77, 377 (1982), 29–39.
[31]TsuyoshiIdéandKojiTsuda.2007. Change-pointdetectionusingkrylovsubspacelearning.In Proceedingsofthe2007SIAMInternationalConferenceonDataMining.
SIAM, 515–520.
[32]AnthonyR.IvesandVasilisDakos.2012. Detectingdynamicalchangesinnonlin-
ear time series using locally linear state-space models. Ecosphere 3, 6 (jun 2012),
art58. http://doi.wiley.com/10.1890/ES11-00347.1
[33]EamonnKeogh,SelinaChu,DavidHart,andMichaelPazzani.2001. Anonline
algorithm for segmenting time series. In Proceedings 2001 IEEE international
conference on data mining. IEEE, 289–296.
[34]HosseinKeshavarz,ClaytonScott,andXuanLongNguyen.2018. OptimalchangepointdetectioninGaussianprocesses. JournalofStatisticalPlanningandInference
193 (2018), 151–178.
[35]Haidar Khan. 2019. Predicting Change Points in Multivariate Time Series Data.
Ph.D. Dissertation. Rensselaer Polytechnic Institute.
[36]HaidarKhan,LaraMarcuse,andBülentYener.2019.Deepdensityratioestimation
for change point detection. (2019). arXiv:1905.09876 [cs.LG]
[37]Rebecca Killick, Paul Fearnhead, and IdrisA Eckley. 2012. Optimal detection of
changepointswithalinearcomputationalcost. J.Amer.Statist.Assoc. 107,500
(2012), 1590–1598.
[38]Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-
mization. arXiv preprint arXiv:1412.6980 (2014).
[39]Ivo Krka, Yuriy Brun, and Nenad Medvidovic. 2014. Automatic mining of specifi-
cations from invocation traces and method invariants. In Proceedings of the 22nd
ACM SIGSOFT International Symposium on Foundations of Software Engineering.
178–189.
[40]Kevin J Lang, Barak A Pearlmutter, and Rodney A Price. 1998. Results of the ab-
badingooneDFAlearningcompetitionandanewevidence-drivenstatemergingalgorithm.In InternationalColloquiumonGrammaticalInference .Springer,1–12.
[41]MarcLavielle.1999. Detectionofmultiplechangesinasequenceofdependent
variables. Stochastic Processes and their Applications 83, 1 (sep 1999), 79–102.
https://www.sciencedirect.com/science/article/pii/S030441499900023X
[42]Marc Lavielle. 2005. Using penalized contrasts for the change-point problem.
Signal processing 85, 8 (2005), 1501–1510.[43] YannLeCun,YoshuaBengio,andGeoffreyHinton.2015. Deeplearning. nature
521, 7553 (2015), 436–444.
[44]Wei-Han Lee and Ruby B Lee. 2017. Implicit smartphone user authentication
with sensors and contextual machine learning. In 2017 47th Annual IEEE/IFIP
International Conference on Dependable Systems and Networks (DSN). IEEE, 297–
308.
[45]Wei-Han Lee, Jorge Ortiz, Bongjun Ko, and Ruby Lee. 2018. Time Series Segmen-
tationthroughAutomaticFeatureLearning. TechnicalReport. arXiv:1801.05394v2
[46]Daniel Lemire. 2007. A better alternative to piecewise linear time series segmen-
tation. In Proceedings of the 2007 SIAM International Conference on Data Mining.
SIAM, 545–550.
[47]DavidLo,Siau-ChengKhoo,JiaweiHan,andChaoLiu.2011. Miningsoftware
specifications: methodologies and applications. CRC Press.
[48]David Lo, Shahar Maoz, and Siau-Cheng Khoo. 2007. Mining Modal Scenario-
BasedSpecificationsfromExecutionTracesofReactive Systems.In Proceedings
of the Twenty-Second IEEE/ACM International Conference on Automated Software
Engineering (Atlanta,Georgia,USA) (ASE’07).AssociationforComputingMa-
chinery, New York, NY, USA, 465–468. https://doi.org/10.1145/1321631.1321710
[49]Davide Lorenzoli, Leonardo Mariani, and Mauro Pezzè. 2008. Automatic gen-eration of software behavioral models. In Proceedings of the 30th international
conference on Software engineering. 501–510.
[50]AndrewLMaas,AwniYHannun,andAndrewYNg.2013. Rectifiernonlinearities
improve neural network acoustic models. In Proc. icml, Vol. 30. 3.
[51]Mohammad Jafar Mashhadi, Taha R Siddiqui, Hadi Hemmati, and Howard
Loewen. 2019. Interactive Semi-automated Specification Mining for Debugging:
An Experience Report. arXiv preprint arXiv:1905.02245 (2019).
[52]Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi. 2016. V-net: Fully
convolutionalneuralnetworksforvolumetricmedicalimagesegmentation.In
2016 Fourth International Conference on 3D Vision (3DV). IEEE, 565–571.
[53]Francisco Javier Ordóñez Morales and Daniel Roggen. 2016. Deep convolutional
featuretransferacrossmobileactivityrecognitiondomains,sensormodalitiesand
locations. In Proceedings of the 2016ACMInternational Symposium on Wearable
Computers. 92–99.
[54]Abdulmajid Murad and Jae-Young Pyun. 2017. Deep recurrent neural networks
for human activity recognition. Sensors17, 11 (2017), 2556.
[55]Kyong Joo Oh and Kyoung-jae Kim. 2002. Analyzing stock market tick data
usingpiecewise nonlinearmodel. ExpertSystems withApplications 22,3 (2002),
249–255.
[56]Francisco Ordóñez and Daniel Roggen. 2016. Deep Convolutional and LSTM Re-
currentNeuralNetworksforMultimodalWearableActivityRecognition. Sensors
16, 1 (jan 2016), 115. http://www.mdpi.com/1424-8220/16/1/115
[57]Petros Papadopoulos and Neil Walkinshaw. 2015. Black-box test generation
frominferredmodels.In Proceedings-4thInternationalWorkshoponRealizing
Artificial Intelligence Synergies in Software Engineering, RAISE 2015. IEEE, 19–24.
http://ieeexplore.ieee.org/document/7168327/
[58]FlorianPein,HannesSieling,andAxelMunk.2017. Heterogeneouschangepointinference. JournaloftheRoyalStatisticalSociety:SeriesB(StatisticalMethodology)
79, 4 (2017), 1207–1227.
[59]Mathias Perslev, Michael Jensen, Sune Darkner, Poul Jørgen Jennum, and Chris-
tian Igel. 2019. U-Time: A Fully Convolutional Network for Time Series Segmen-
tation Applied to Sleep Staging. In Advances in Neural Information Processing
Systems. 4417–4428.
[60]Rychelly Glenneson da S Ramos, Paulo Ribeiro, and José Vinícius de M Cardoso.
2016. AnomaliesDetectioninWirelessSensorNetworksUsingBayesianChange-
points.In 2016IEEE13thInternationalConferenceonMobileAdHocandSensor
Systems (MASS). IEEE, 384–385.
[61]Karl J (Karl Johan) Åström. 2008. Feedback systems : an introduction for scientists
and engineers. Princeton University Press, Princeton.
[62]Bonnie K Ray and Ruey S Tsay. 2002. Bayesian methods for change-point de-
tection in long-range dependent processes. Journal of Time Series Analysis 23, 6
(2002), 687–705.
[63]Jaxk Reeves, Jien Chen, Xiaolan L Wang, Robert Lund, and Qi Qi Lu. 2007. Areview and comparison of changepoint detection techniques for climate data.
Journal of applied meteorology and climatology 46, 6 (2007), 900–915.
[64]OlafRonneberger,PhilippFischer,andThomasBrox.2015. U-net:Convolutional
networks for biomedical image segmentation. In International Conference on
Medical image computing and computer-assisted intervention. Springer, 234–241.
[65]David Rosenfield, Enlu Zhou, Frank H Wilhelm, Ansgar Conrad, Walton T Roth,
and Alicia E Meuret. 2010. Change point analysis for longitudinal physiological
data: detection of cardio-respiratory changes preceding panic attacks. Biological
psychology 84, 1 (2010), 112–120.
[66]I. Schieferdecker. 2012. Model-Based Testing. IEEE Software 29, 1 (Jan 2012),
14–18. https://doi.org/10.1109/MS.2012.13
[67]AndrewJhonScottandMKnott.1974. Aclusteranalysismethodforgrouping
means in the analysis of variance. Biometrics (1974), 507–512.
310[68]Weiyi Shang, Zhen Ming Jiang, Hadi Hemmati, Bram Adams, Ahmed E Hassan,
and Patrick Martin. 2013. Assisting developers of big data analytics applica-
tions when deploying on hadoop clouds. In Proceedings of the 2013 International
Conference on Software Engineering. IEEE Press, 402–411.
[69]KarenSimonyanandAndrewZisserman.2014.Verydeepconvolutionalnetworks
for large-scale image recognition. arXiv preprint arXiv:1409.1556 (2014).
[70]Monit Shah Singh, Vinaychandran Pondenkandath, Bo Zhou, Paul Lukowicz,
andMarcusLiwickit.2017. Transformingsensordatatotheimagedomainfor
deeplearning—Anapplicationtofootstepdetection.In 2017InternationalJoint
Conference on Neural Networks (IJCNN). IEEE, 2665–2672.
[71]Carole H Sudre, Wenqi Li, Tom Vercauteren, Sebastien Ourselin, and M Jorge
Cardoso. 2017. Generalised dice overlap as a deep learning loss function forhighly unbalanced segmentations. In Deep learning in medical image analysis
and multimodal learning for clinical decision support. Springer, 240–248.
[72]Jun-ichiTakeuchiandKenjiYamanishi.2006. Aunifyingframeworkfordetecting
outliers and change points from time series. IEEE transactions on Knowledge and
Data Engineering 18, 4 (2006), 482–492.
[73]Charles Truong, Laurent Oudre, and Nicolas Vayatis. 2018. Selective reviewof offline change point detection methods. (jan 2018). arXiv:1801.00718 http:
//arxiv.org/abs/1801.00718
[74]Alfonso Valdes and Keith Skinner. 2000. Adaptive, model-based monitoring for
cyber attack detection. In International Workshop on Recent Advances in Intrusion
Detection. Springer, 80–93.
[75]Neil Walkinshaw. 2018. Testing Functional Black-Box Programs Without a Specifi-
cation. Springer International Publishing, Cham, 101–120. https://doi.org/10.
1007/978-3-319-96562-8_4
[76]Neil Walkinshaw, Ramsay Taylor, and John Derrick. 2016. Inferring extended
finitestatemachinemodelsfromsoftwareexecutions. EmpiricalSoftwareEngi-
neering21, 3 (2016), 811–853.
[77]JindongWang,YiqiangChen,ShujiHao,XiaohuiPeng,andLishaHu.2019. Deep
learning for sensor-based activity recognition: A survey. Pattern RecognitionLetters119 (2019), 3–11.
[78]Yao Wang, Chunguo Wu, Zhaohua Ji, Binghong Wang, and Yanchun Liang. 2011.
Non-parametric change-point method for differential gene expression detection.
PloS one6, 5 (2011).
[79]Zhiguang Wang, Weizhong Yan, and Tim Oates. 2017. Time series classification
from scratch with deep neural networks: A strong baseline. In 2017 International
joint conference on neural networks (IJCNN). IEEE, 1578–1585.
[80]Yao Xie and David Siegmund. 2013. Sequential multi-sensor change-point detec-
tion. In2013 Information Theory and Applications Workshop (ITA). IEEE, 1–20.
[81]Kenji Yamanishi, Jun-Ichi Takeuchi, Graham Williams, and Peter Milne. 2004.
On-lineunsupervisedoutlierdetectionusingfinitemixtureswithdiscounting
learning algorithms. Data Mining and Knowledge Discovery 8, 3 (2004), 275–300.
[82]Jianbo Yang, Minh Nhut Nguyen, Phyo Phyo San, Xiao Li Li, and Shonali Krish-
naswamy.2015. Deepconvolutionalneuralnetworksonmultichanneltimeseries
for human activity recognition. In Twenty-Fourth International Joint Conference
on Artificial Intelligence.
[83]Shuochao Yao, Shaohan Hu, Yiran Zhao, Aston Zhang, and Tarek Abdelzaher.
2017. DeepSense:aUnifiedDeepLearningFrameworkforTime-SeriesMobile
Sensing Data Processing. (2017). http://dx.doi.org/10.1145/3038912.3052577
[84]MatthewDZeilerandRobFergus.2014. Visualizingandunderstandingconvolu-
tional networks. In European conference on computer vision. Springer, 818–833.
[85]Ming Zeng, Le T Nguyen, Bo Yu, Ole J Mengshoel, Jiang Zhu, Pang Wu, and Joy
Zhang.2014.Convolutionalneuralnetworksforhumanactivityrecognitionusingmobilesensors.In 6thInternationalConferenceonMobileComputing,Applications
and Services. IEEE, 197–205.
[86]Jia-Shu Zhang and Xian-Ci Xiao. 2000. Predicting chaotic time series using
recurrent neural network. Chinese Physics Letters 17, 2 (2000), 88.
[87]Yi Zheng, Qi Liu, Enhong Chen, Yong Ge, and J Leon Zhao. 2016. Exploiting
multi-channels deep convolutional neural networks for multivariate time series
classification. Frontiers of Computer Science 10, 1 (2016), 96–112.
311