On Decomposinga Deep NeuralNetwork into Modules
RangeetPan
rangeet@iastate.edu
Dept. of ComputerScience, IowaStateUniversity
226 AtanasoffHall, Ames, IA,USAHridesh Rajan
hridesh@iastate.edu
Dept. of ComputerScience, IowaStateUniversity
226 AtanasoffHall, Ames, IA,USA
ABSTRACT
Deep learning is being incorporated in many modern software sys-
tems.Deeplearningapproachestrainadeepneuralnetwork(DNN)
modelusingtrainingexamples,andthenusetheDNNmodelfor
prediction. While the structure of a DNN model as layers is observ-
able,themodelistreatedinitsentiretyasamonolithiccomponent.
To change the logic implemented by the model, e.g. to add/remove
logic that recognizes inputs belonging to a certain class, or to re-
placethelogicwithanalternative,thetrainingexamplesneedto
bechangedandtheDNNneedstoberetrainedusingthenewsetof
examples. We argue that decomposing a DNN into DNN modulesÐ
akintodecomposingamonolithicsoftwarecodeintomodulesÐcan
bring the benefits of modularity to deep learning. In this work,
we develop a methodology for decomposing DNNs for multi-class
problems into DNN modules. For four canonical problems, namely
MNIST, EMNIST, FMNIST, and KMNIST, we demonstrate that such
decompositionenablesreuseofDNNmodulestocreatedifferent
DNNs, enables replacement of one DNN module in a DNN with
another without needing to retrain. The DNN models formed by
composing DNN modules are at least as good as traditional mono-
lithicDNNs interms oftest accuracyfor our problems.
CCS CONCEPTS
·Computingmethodologies →Machinelearning ;·Software
andits engineering →Abstractionandmodularity .
KEYWORDS
deepneuralnetworks, decomposing, modules, modularity
ACMReference Format:
Rangeet Pan and Hridesh Rajan. 2020. On Decomposing a Deep Neural
Network into Modules. In Proceedings of the 28th ACM Joint European Soft-
ware Engineering Conference and Symposium on the Foundations of Software
Engineering (ESEC/FSE ’20), November 8ś13, 2020, Virtual Event, USA. ACM,
NewYork, NY, USA, 12pages.https://doi.org/10.1145/3368089.3409668
1 INTRODUCTION
A class of machine learning algorithms known as deep learning has
receivedmuchattentioninbothacademiaandindustry.Thesealgo-
rithmsusemultiplelayersoftransformationfunctionstoconvert
inputs to outputs, each layer learning successively higher-level of
Permissionto make digitalor hard copies of allorpart ofthis work for personalor
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACM
mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,
topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ESEC/FSE ’20, November 8ś13, 2020, Virtual Event, USA
©2020 Associationfor Computing Machinery.
ACM ISBN 978-1-4503-7043-1/20/11.
https://doi.org/10.1145/3368089.3409668abstractions inthe data. The availabilityof large datasets hasmade
itfeasibletotrain(adjusttheweightsof)thesemultiplelayers.Since
theselayersareorganizedintheformofanetwork,thismachine
learning model is also referred to as a deep neural network (DNN).
While the jury is still out on the impact of deep learning on overall
understanding of software’s behavior, a significant uptick in its
usage and applications in wide-ranging areas and safety-critical
systems,e.g.,autonomousdriving,aviationsystem,medicalanal-
ysis, etc, combine to warrant research on software engineering
practices inthe presenceof deep learning.
AsoftwarecomponentandaDNNmodelaresimilarinspiritÐ
bothencodelogicandrepresentsignificantinvestments.Theformer
isaninvestmentofthedeveloper’seffortstoencodedesiredlogicin
theformofsoftwarecode,whereasthelatterisaninvestmentofthe
modeler’s efforts, an effort to label training data, and computation
time to create a trained DNN model. The similarity ends there,
however.While independentdevelopmentofsoftwarecomponents
and a software developer’s ability to (re)use software parts has
led to the rapid software-driven advances we enjoy today; the
ability to (re)use parts of DNN models has not been, tothe best of
our knowledge, attempted before. The closest approach, transfer
learning [ 16], attempts to reuse the entire DNN model for another
problem.Could we decompose andreuse parts of aDNN model?
Tothatend,weintroducethenovelideaof decomposingatrained
DNN model into DNN modules. Once the model has been decom-
posed, the modules of that model might be reused to create a com-
pletelydifferentDNNmodel,forinstance,aDNNmodelthatneeds
logic present in two different existing models can be created by
composing DNN modules from thosetwo models,without having
toretrain.DNNdecompositionalsoenablesreplacement.ADNN
modulecanbereplacedbyanothermodulewithouthavingtore-
train the DNN. The replacement could be needed for performance
improvement orfor replacing afunctionalitywithadifferentone.
To introduce our notion of DNN decomposition, we have fo-
cusedondecomposingDNNmodelsformulti-labelclassification
problems. We propose a series of techniques for decomposing a
DNN model for n-label classification problem into nDNN modules,
one for eachlabel in the original model. We considereach label as
aconcern, and view this decomposition as a separation of concerns
problem[ 4,15,23].EachDNNmoduleiscreated duetoitsability
to hide one concern [ 15]. As expected, a concern is tangledwith
other concerns [ 1,6,17,19,20,22] and we have proposed initial
strategies to identifyandeliminateconcerninteraction.
Evaluation. WehaveevaluatedourDNNdecompositionapproach
using16differentmodelsforfourcanonicaldatasets(MNIST[ 8],
FashionMNIST[ 26],EMNIST[ 3],andKuzushijiMNIST[ 2]).These
models use the feedforward fully connected neural networks, ReLU
astheactivationfunctionsforhiddenlayers,and Softmaxasthe
889
ESEC/FSE ’20, November8–13,2020,VirtualEvent, USA Rangeet Pan andHrideshRajan
activation function for the output layer. We have experimented
with six approaches for decomposition, each successively refining
the former. Our evaluation shows that for the majority of the DNN
models (9 out of 16), decomposing a DNN model into modules
and then composing the DNN modules together to form a DNN
model that is functionally equivalent to the original model, but
moremodular,doesnotleadtoanylossofperformanceinterms
of model accuracy. We also examine intra- and inter-dataset reuse,
where DNN modules are used to solve a different problem using
the same training dataset or entirely different problem using an
entirelydifferentdataset.
Results.Our results show DNN models trained by reusing DNN
modulesareatleastasgoodasDNNmodelstrainedfromscratchfor
MNIST (+0.30%), FMNIST (+0.00%), EMNIST (+0.62%), and KMNIST
(+0.05%),Wehavealsoevaluatedreplacement,whereaDNNmodule
isreplacedbyanotherandsee similarlyencouragingresults.
Outline.In the rest of this paper, we describe our initial steps to-
wardachievingbettermodularityfordeepneuralnetworksstarting
with a motivating example in ğ 2, related work in ğ 3, methodol-
ogy in ğ4and results in ğ 5. ğ7concludes. The code is also publicly
accessible at [ 13].
2 WHYDECOMPOSEADNNINTOMODULES?
Achieving decomposition of a DNN into modules has the potential
to bring many benefits of modular programming and associated
practicestodeeplearning.Tomotivateourobjectives,considerFig-
ure1thatusesDNNmodelsforrecognizingdigits[ 8]andletters[ 3]
toillustrate.Thetop-halfofthefigureshowstworeusescenarios.
(1) If we have a DNN model to recognize 0-9, can we extract the
logic to recognize 0-1 and build a DNN model for binary digits? (2)
If we have two DNN models for recognizing 0-9 and A-J, can we
buildaDNNmodelforrecognizing0-9AB,essentiallyaduodeci-
malclassifier?Thebottom-halfofthefigureshowsareplacement
scenario. (3) If we have aDNN model thatis doing a satisfactory
jobofclassifying0-4and6-9,butcouldimproveitsperformance
for5,couldwetakethelogicforclassifying5fromanotherDNN
modelandreplace the faulty5withanewpart.
0 1 2 3 4 5 6 7 8 9
DNN Model with English letters (A-J)
A B C D E F G H IDNN Model with English digits
J
0 10 1 2 3 4 5 6 7 8 9
0 1 2 3 4 5 6 7 8 9 A B
Build a subproblem: Use 0 and 1 to 
build  a binary digit classifierMerge two problems: Use 0-9 and A-B to build a 
duodecimal number classifier
0 1 2 3 4 5 6 7 8 9
 0 1 2 3 4 5 6 7 8 9
Remove the faulty part Identify a better DNN model
Replace the faulty partDNN Model A DNN Model B
Figure1:ExamplesofFine-grainedReuseandReplacement
Ifthelogictorecognize0-9andA-Jwereimplementedassource
code, modern SE practices might consider these reuse and replace-
ment scenarios trivial. These scenarios would be far from trivial in
thecurrentstate-of-the-artindeeplearning.Torealizethereusescenarios, first the developer will build a model structure for bi-
nary digitsfor the firstscenario,andduodecimal classifierforthe
second scenario. Then, the developer will take the training dataset
for 0-9 and partition it to extract labeled training samples for 0
and 1 for the first scenario and A and B for the second scenario.
Then,thesenewtrainingdatasetswillbeusedtoretrainthenew
model structures. Realizing the replacement scenario is more com-
plicated, however. The developer might need to change the model
structure of model A to match the structure of model B, which
alsohasthepotentialtochangemodelA’seffectivenessfor0-4and
6-9. Then, they can replace the training samples for 5 used to train
model A with those used to train model B. Finally, the modified
modelAwouldbetrainedwiththemodifiedtrainingdata.Evenfor
thesesimplescenarios,bothreuseandreplacementarecomplicated.
Coarse-grained reuse of the entire DNN model as a black-box is
becoming common. As modern software continues to integrate
DNNmodelsascomponents,itisimportanttounderstandwhether
fine-grained reuse/replacement of DNN models canbe improved.
3 RELATED IDEAS
We are inspired by the vast body of seminal work on decomposing
softwareintomodules[ 4,6,15,17,19,20,22,23].Webelievethat
there are ample opportunities to consider a number of these ideas
in the context of DNN, but focus primarily on decomposition in
this work.
The decomposition of a DNN into modules has not been at-
tempted before, but coarse-grained reuse (at the level of the entire
DNN) has been. An approach for achieving reusability in deep
learningistransferlearning[ 16].Inthistechnique,aDNNmodel
is leveraged for a different setting by changing the output layer
and input shape of a pre-trained network. Transfer learning can
be either heterogeneous [ 18,21] or homogeneous [ 14,25] based
ontheproblemdomain.Zhou[ 27]proposedaspecificationbased
framework that uniquely identifies the goal of the model. These
models can be advertised in the marketplace so that other develop-
erscantakethismodelasinputandutilizethemtobuildotherDNN
modelsontopofthose.Li etal.[9]proposedthatreusabilitycanbe
achieved by addingdifferent AUCmetrics as tags that canhelpto
choose the appropriate model with different parameters. Kirsch et
al.[7] have utilized modular architecture to create a modular layer,
where nodes are the modules and experiments have shown that
using a subset of all modules can outperform the prior results and
resultsinlessnoisytraining.Comparedtothecoarse-grainedreuse
ofDNN models,the focusof thisworkis onfine-grainedreuseand
replacementofDNN modules.
4 DECOMPOSINGADNN INTO MODULES
Ourapproach,illustratedinFigure 2,decomposesatrainedDNN
model into modules. In this work, we focus on DNN models for
multi-classclassificationproblems.Wewillrefertothesingle,black-
box, DNN model for all classes as the monolithic model . Our ap-
proachdecomposessuchmodelsinto DNNmodules ,oneforeach
label in the original monolithic model. A DNN module accepts the
same inputas the monolithic model, but acts as abinary classifier.
In our example in Figure 2, a multi-class classifier that classifies
890On Decomposing aDeep Neural Network intoModules ESEC/FSE ’20, November8–13,2020,VirtualEvent, USA
CI: Concern identification
TI: Tangling identification
CM: Concern modularizationConcern 0
Tangled Concerns0 1 2 0 1 2Concern 1 Concern 2
Functional Concerns0 1 2Concern 0 with tangling
Identify Tangling...
0 ¬0Concern 0 after concern 
channeling
...0
1
2DNN Model 
with digit 0-2Module Decomposition
Graph Based Representation
0
1
2Modules
TI CMCI
Figure 2:Overview oftheapproachto decompose aDNN model.
input into 0-2 (far-left) is decomposed into three DNN modules
(far-right)that classifywhether an inputis0or1or2.
DNNdecompositionhasthreesteps: concernidentification (CI),
tangling identification (TI), and concern modularization (CM). By
concernhere we mean a specific functionality, e.g. an ability to
determinewhetheraninputis0.Thismeaningisconsistentwith
thepriorworkonmodularity[ 4,6,15,17,19,20,22,23].Concern
identification (CI)istheprocessofidentifyingpartsofthemono-
lithicmodelthatareresponsibleforthatconcern.Clearly,asFigure
2shows,concernsaretangled(mixedtogether)withinthemono-
lithic model and parts of the model might contribute to more than
one concerns. Once concern identification is complete and parts of
the monolithic model that contribute to a concern are identified,
tanglingidentification (TI)istheprocessofidentifyelementsamong
those parts that are also contributing to other concerns. Finally,
concern modularization (CM) is the process of separating the parts
of the monolithic model belonging to a concern into its own DNN
module. CM also involves concern channeling where the effects
of the non-dominant concerns within the module are channeled
appropriately.
4.1 ConcernIdentification (CI)
Concernidentificationessentiallyidentifiesthosepartsofthemono-
lithic model that contribute to specific functionality or concern. To
paraphraseTarr etal.[23],inordertoachieveabetterDNNquality
andimprovethereusability,theconcernsofDNNneedtobesep-
aratedinsuchafashionthateachconcerncanperformaspecific
functionalitywithouttheinterventionoftheothers.Tountangle
theconcernoftheoutputlabel,onecouldobtainapieceoftheDNN
that can perform a certain task, e.g., prediction for a single class,
andcanhidethenon-dominantconcern(s)toseparatethatconcern.
To illustrate, consider Figure 2. Here, the monolithic model has
threetangledconcernsfor0,1,and2.Thegoaloftheconcerniden-
tificationstepistoidentifypartsoftheDNNthatareresponsible
forclassifyinganimageinto0,1,and2.Onceweidentifypartsof
the DNN related to a concern, those parts still might contribute to
otherconcernsaswell.Wecallthoseotherconcernsnon-dominant
concern(s). For example, for the concern 0, concerns 1 and 2 are
non-dominantconcernsinFigure 2.BeforewedecomposetheDNN
intomodules,weneedtoidentifytheconcernsinamonolithicDNN
andseparatethemtobuildsub-networksresponsibleforindividual
concerns.
OuralgorithmforconcernidentificationisshowninAlgorithm 1.
We monitor the behavior of the nodes by studying the trainingAlgorithm1 ConcernIdentification(CI).
1:procedure CI (model,input,indicator ,D,b)
2:X=input
3:W,B=extractWB (model) ▷Extractweightand bias.
4:X0=X
5:foreachi∈L−1do
6:Xi=Xi.Wi
7:Xi=Xi+Bi ▷Computethe valueof the nodes.
8:XL=XL−1.WL+BL
9:XL=softmax (XL)
10:foreachi∈L−1do
11: forj= 0toj=|Xi|do
12: ifXi[j]≤0then
13: Di[:,j] = 0▷Updatethevalueoftheedgesto0forinactivenodes.
14: ifi!=L-1then
15: Di+1[j,:] = 0
16: bi[j] = 0
17: else
18: ifindicator ==Truethen
19: Di[:,j] =Wi[:,j]▷Nochange to edge if indicator is set.
20: bi[j] =Bi[j]
21: else
22: fork= 0tok=|Wi[:,j]|do▷Updatethe common edges.
23: ifWi[j,k]<0then
24: Di[j,k] =max(Di[j,k],Wi[j,k])
25: ifDi[j,k]<0then
26: Di[j,k] = 0
27: else
28: Di[j,k] =min(Di[j,k],Wi[j,k])
29: bi[k] =Bi[k]
30:forj= 0toj=|XL|do
31: ifindicator ==Truethen
32: DL[:,j] =WL[:,j]
33: bL[j] =BL[j]
34: else
35: fork= 0tok=|WL[:,j]|do▷Updatethe output layer edges.
36: ifWL[j,k]<0then
37: DL[j,k] =max(DL[j,k],WL[j,k])
38: else
39: DL[j,k] =min(DL[j,k],WL[j,k])
40: bL[k] =BL[k]
41:returnD, b
examplesforthatconcern(e.g.,trainingexamplesfor0),andadd,
update,removetheedges.Thisalgorithmformsasub-graphthat
can identifythe common edges for asingleoutputlabel. First, the
weightandbiasoftheDNNmodelarestored.Inordertoidentifythe
edges that are responsible for a particular output label, we observe
thebehaviorwiththeexamplesthatbelongtothatlabelfromthe
trainingdataset.Witheachexample,weperformtheweightand
bias update operation. In the update operation, we provide the
DNNmodel( model),inputexample( input),theupdatedweight( D),
bias(b),andanindicator( indicator ).Theweight Dandbiasbare
891ESEC/FSE ’20, November8–13,2020,VirtualEvent, USA Rangeet Pan andHrideshRajan
initialized with the weight and bias of the monolithic DNN model
outside the Algo. 1. With everyexample, Dandbare updated and
they are returned as an output. For the next example, the modified
Dandbwillbe taken as an input. If for some inputs,the common
edges for all the examples that belong to a single concern are very
low, then the graph can become very sparsely connected. To avoid
such circumstances,the algorithm stops removingedges from the
graphonceitreachesathreshold.Wecomputethenon-negative
edgesoutsidetheAlgo. 1andsetthethe indicator variable.Here,
we evaluatethe variableand modifies the edges basedonthe value.
The detailed discussion is in in the Algo . 3. The edge update is
carriedoutinsuchafashionthatremoving,updatingtheedgescan
helptoidentifyasingleconcernwhileremovingtheotherconcerns
(outputlabels).First,weidentifythevalueofthenodesateachlayer
by applying the dense operation in the line 5-9. Here,Ldenotes
the total number of hidden and output layers. For nodes belong to
thehiddenlayer,wemonitorthevaluebeforeapplyingtheReLU
operation. However, for the output layer, the value computed after
applying the softmaxactivation function, has been analyzed. At
line10,weiteratethrougheachhiddenlayerandmonitorthevalue
ofthenodes.Fromline 12-16,weremovetheincomingedgestothe
nodesthat have value <=0byapplyingthe ReLUoperation. Based
on the definition of ReLU, any negative value will be updated to
zero. We update the bias of the node to be zero as there will not be
anydataflowthroughthatnode.Duetothesamereason,weremove
theoutgoingedgesfromthosenodestothenextlayerexceptthe
edges connectingthe last hiddenlayer andtheoutputlayer.Ifthe
valuecorrespondingtothenodeisapositiveone,wevalidatethe
indicator. If the indicator is false, the edges are updated by the
minimum value of the DNN model edge ( Wi[j,k]) and the updated
edge (Di[j,k]) ifthe value of the edge is a positive one. If the value
of the edgeis negative, we performthe maximum operation. Both
the operation are carried out to store the semantics of the edges
andtheirimpactontheprediction.Atline 25,theupdatededges
are validated, and if the value is negative, they are updated as zero
basedontheactivationlogic.Fromline 31-40,weperformasimilar
operation ontheoutput layer and returnthe updatedweight( D)
andthe bias( b).
4.2 Tangling Identification (CI)
Tanglingidentificationrecognizesthepartsresponsibleforother
concerns. While concern identification can separate the part of the
networkthatcontributestoaconcern,itmaynotbeabletomakethe
separatedpartsfunctional.Usingconcernidentification,weidentify
the edges that are responsible for a particularconcern. However,
the remaining network can only classify a single concern as all the
edges correspond to the other concerns are removed or updated,
and the model predicts the dominant concern irrespective of the
input. Thus, the resulting network becomes a single-class classifier.
Thisisakintoremovingaconditionalandabranchfromaprogram
that results in a subprogram that performs the functionality of
the remaining branch but does so unconditionally. For example, in
Figure1the concerns for 0, 1, and 2 cannot be used as they cannot
distinguishbetween differentinputs.
To solve this problem, our insight is to identify some edges and
nodesbacktotheconcernthathelpsusidentifyinputsthatdon’tneedclassificationbythedominantconcern.Inourapproach,wedo
so by adding parts of the non-dominant concerns. By doing so, the
problem of classification becomes akin to the one against all (OAA)
classification problem. In OAA, a classifier is built from the scratch
thatpredictsaparticularoutputlabel(positiveexample)andcan
stilldetectanynegativeones.Thereareafewtechniquesthathave
beenproposedbypriorworks[ 10,28]thatincludesintroducingan
imbalance betweenthe positiveand negative examples, punishing
the negative example,assign higherpriority to the negative exam-
pleswhilemodifyingtheedges.Weproposeanothertechniquethat
keeps the most relevant edges related to the negative examples.
Beforedescribingthesetechniques,wediscusstheapproachtoadd
edges relatedto non-dominantconcernsinAlgorithm 2.
Algorithm2 Tangling Identification(TI).
1:procedure TI(model,input,indicator ,D,b)
2:X=input
3:W,B=extractWB (model)
4:X0=X
5:foreachi∈L−1do
6:Xi=Xi.Wi ▷Computethe valueof the nodes.
7:Xi=Xi+Bi
8:XL=XL−1.WL+BL
9:XL=softmax (XL)
10:foreachi∈L−1do
11: forj= 0toj=|Xi|do ▷Updatethe negativeedges.
12: ifXi[j]≤0then
13: Di[:,j] =Di[:,j]
14: else
15: Di[:,j] =Wi[:,j]
16: bi[j] =Bi[j]
17:forj= 0toj=|XL|do ▷Updatethe output layer edges.
18: ifXL[j]>0.0001then
19: DL[:,j] =WL[:,j]
20: bL[j] =BL[j]
21:returnD, b
Algorithm 2works as follows. First, the value of the nodes is
computedsimilarlytotheAlgorithm 1.Afterthat,theedgesincident
to the nodes with positive value (at lines 12-16), are added. For the
output layer, we reintroduce the edges responsible for the negative
outputlabel classification (at lines 17-20).
Next, we discuss four different approaches for non-dominant
edge additiontechniques.
TanglingIdentification:Imbalance(TI-I). Recallthatconcerniden-
tificationworksusingtrainingexamplesforaparticularconcern.In
thisapproach,animbalanceisintroducedwhileaddingthenumber
ofexamplesfromthepositiveandthenegativeoutputlabels[ 12].
IntheAlgorithm 3,wediscussthestepsfollowedtoincludethepos-
itive and negative examples and carry the edge update operations.
First, weight and the bias of the DNN model are extracted at line 2.
Then, training examples belonging to the positive output label are
filtered. To identify the concern, the positive examples are used
to find the common edges that correspond to them based on the
approachdepictedintheAlgorithm 1Ourconcernidentification
algorithmreliesonanindicatortohalttheprocessofeliminating
edges.Atline 7,suchanindicatorhasbeenutilizedthatissetwhen
thetotalnumberofnonzeroedges(active)atthelastlayerisless
than10%ofthetotaledges.Then,negativeexamplesareadded(we
perform a floor operation to remove the floating-point value) at
lines8-11using the Algorithm 2. Finally, the edges corresponding
892On Decomposing aDeep Neural Network intoModules ESEC/FSE ’20, November8–13,2020,VirtualEvent, USA
Algorithm3 Tangling Identification: Imbalance(TI-I).
1:procedure TII(model,Xtrain,Ytrain,class,neдative class)
2:W,B=extractWB (model) ▷Extractweightand bias.
3:D=W,b=B,indicator =False
4:Xclass=Xtrain[Ytrain==class] ▷Filterpositiveexamples.
5:fori= 0toi=|Xclass[0 : 1000] |do
6:D,b=CI(model,Xclass[i],indicator ,D,b)▷Updatethe positive
edges.
7: ifcountnonzero (DL)≤.1∗ |DL|then
8: indicator =False ▷Stop the updateif graphisverysparse.
9:foreachi∈neдative classdo ▷Updatenegativeedges.
10: Xneдative =Xtrain[Ytrain==i]
11: fori= 0toi=|Xneдative [0 :⌊10/|neдative class|]⌋|do
12: D,b=TI(model,Xneдative [i],indicator ,D,b)
13:returnD, b
tothenon-dominantconcernsareupdatedoraddedbasedonthe
approach discussedinthe Algorithm 2.
TanglingIdentification:PunishNegativeExamples(TI-PN). Sim-
ilar to the previous approach of introducing imbalance to tackle
the OAA problem, here we punish the negative output labels by
letting the positive output label update the edges first [ 11]. The
process is similar to Algorithm 3. However, the input examples
are balanced. In thisapproach, if 100examples are utilized for the
positiveoutputlabel,thenasamenumberofexamplesforthenega-
tive labels are taken e.g., for an MNIST classification problem with
ten output labels, if we want to decompose a module for label 0,
then we choose the ratio being 100:99 (11 from each negative la-
bel). To stop the negative edges being introduced in a module, the
approachforidentifyingthecommonedgeshasbeenupdated.In
Algorithm 1,theedgesbetweentheoutputlayerandthelasthidden
layerareupdatedbasedonthevalueoftheweight.Inadditionto
that,validationhasbeenintroducedbeforeline 31,wheretheedges
that incident to the nodes at the output layer having a value at
least0.0001(0.01%asthelastlayerrepresentstheprobabilityvalue)
have been added. This helps to remove the edges responsible for
thenegativeexamplesleavingmoreedgesrelatedtothepositive
outputlabel.
Tangling Identification: HigherPriority to NegativeExamples (TI-
HP).Inthisapproach,thenegativeoutputlabelsareassignedmore
priorityoverthedominantconcernbyswappingtheorderofthe
edge update. For this, we update the edges correspond to the domi-
nantconcernfirst(lines 9-12fromAlgorithm 3)andthenupdate
theedgesresponsibleforthenon-dominantconcerns(lines 5-8).To
keeptheratioofpositiveandnegativeexamplesthesame,thetotal
negativeexamplesareequallydistributedtothenumberofnegative
output labels and floor operation has been performed to avoid any
floating-point number (at line 12from Algorithm 3, we replace the
number10with 1000 that is same as the positive examples). Other
thanthat, everythinghas been kept the same.
Tangling Identification: Strong Negative Edges (TI-SNE). In this
approach, the edges related to the positive label are updated, then
the negative labels. Furthermore, the strong negative edges are
added by introducing a validation to the Algorithm 2at line18by
updatingthecheckimposedontheoutputnodevalueassociated
withthenegativeexamples.Todoso,thevalueoftheprobabilityhas been changed from 0.01% to 50% while keeping the other parts
ofthe algorithm unchanged.
4.3 Concern Modularization (CM)
Concern modularization partitions the concerns into parts and
builds DNN modulesfor eachconcern.
Algorithm4 ConcernModularization:Channeling (CM-C).
1:procedure CMC (D,b,class)
2:fori= 0toi=|DL|do
3:temp=DL[i,:]
4: ifclass= 0then ▷Assignthe 2nd nodeas the negative.
5: temp[1] =mean(temp[1,:])
6: temp[2,:] = 0
7: dL[1] =mean(dL[1,:])▷Computethe meanof negativeedges.
8: dL[2,:] = 0 ▷Updatealledgesto othernegativenodes.
9: else ▷Assignthe 1st nodeas the negative.
10: tempW,tempB= [],k= 0
11: forj= 0toj=|bL|do▷|bL|represents the size of the output layer.
12: ifj!=classthen ▷Perform for allnegativenodes.
13: tempW[k] =temp[j]
14: tempB[k] =bL[j]
15: k=k+1
16: temp[0] =mean(tempW)▷Computethe meanof negativeedges.
17: dL[0] =mean(dL[1,:]) ▷Assignthe meanvalue.
18: forj= 1toj=|bL|do▷Updatealledgesto othernegativenodes.
19: ifj!=classthen
20: temp[j] = 0
21: dL[2,:] = 0
22: DL[i,:] =temp
23:returnD, b
Concern Modularization: Channeling (CM-C). Concern modu-
larizationincludestheabstractionofthenon-dominantconcerns.
In this approach, we propose an approach to abstract the non-
dominantconcernsbycombiningthenon-dominantnodesatthe
output layer. In the Algorithm 4, we discuss the methodology to
channel the output nodes into one for all the non-dominant nodes.
Atlines2-8,theedgesbetweenthelasthiddenlayerandthepositive
node at the output layer are updated. The input classindicates the
concerned output label. The position of the non-dominant node at
the output label after the concern channeling depends on the posi-
tionofthedominantnode.Ifthedominantnodeisthefirstnode,
thenweassignthe2ndnodeattheoutputlabelasthenon-dominant
node(lines 5-8).Ifnot,wechoosethe1stnodeasthenon-dominant
node and keep the position of the dominant node as it is (lines
12-21). All the edges incident on the pre-channeling non-dominant
nodes are replaced by edges directed towards the non-dominant
nodeafterchannelingwiththevaluebeingtheaverageoftheedges.
Theremainingedges(incidentonthenon-dominantnodesother
thanthe channelizedone)are updatedto be 0.
ConcernModularization:RemoveIrrelevantEdges(CM-RIE). Be-
fore applying the concern channeling, we remove the irrelevant
nodesatthelasthiddenlayerthatonlycontributestonon-dominant
concerns.Theedgesthatareconnectedwiththenegativeoutput
nodes (before channeling the output nodes) and not connected
(weight is zero) with the positive output node are combined into
onenode.Theoutgoingedge(s)fromthecombinednodetoapar-
ticular negative output node, is updated with the average of the all
theedgesincidenttothatnegativeoutputnodefromnodesthatare
connectedonlywiththenegativeoutputlabels.IntheAlgorithm 5,
893ESEC/FSE ’20, November8–13,2020,VirtualEvent, USA Rangeet Pan andHrideshRajan
Algorithm 5 ConcernModularization:RemoveIrrelevantEdges
(CM-RIE).
1:procedure CM-RIE(D,b,class)
2: tempD=[],tempCount=0
3:fori= 0toi=|DL|do
4: temp1, temp2=[]
5: ifDL[i,class]==0then
6: forj= 0to|dL|do
7: ifj!=classthen
8: ifDL[i,j]!= 0then ▷Identify the irrelevant nodes.
9: temp1.add(DL[i,j]);temp2.add(j)
10: ▷Computethemeanofthenegativeedgesfortheirrelevantnodes.
11: ifclass== 0then
12: DL[i,1] =mean(temp1)
13: else
14: DL[i,0] =mean(temp1)
15: fork∈temp2do
16: DL[i,k] = 0
17: tempD.add(i)
18:ifclass== 0then
19: neдative node= 1
20:else
21: neдative node= 0
22:iflen(tempD)>1then▷Merge the edges if more than 1 irrelevant nodes.
23: fori∈tempDdo
24: ifclass== 0then
25: tempCount + =DL[i,1]
26: else
27: tempCount + =DL[i,0]
28: DL[tempD[0],neдative node] =mean(tempCount )
29: forx∈tempD[1 :count(tempD)]do
30: DL[x,neдative node] = 0 ▷Updatethe removed edges.
31: fori= 0to|DL−1|do ▷Compensate the flow.
32: tempDL−1= []
33: forj∈tempDdo
34: tempDL−1.add(DL−1[i,j]) ▷Updatethe merged edges.
35: DL−1[i,tempD[0]] =mean(tempDL−1)
36: forx∈tempD[1 :count(tempD)]do
37: DL−1[i,x] = 0 ▷Updatethe removed edges.
38:D,b=CMC(D,b,class)▷Apply the concernchanneling approach.
39:returnD, b
we discussthesteps carried out toremove the edges picked based
onthefilteringcriteria.Atlines 4-18,theedgesfromthelasthidden
tooutputlayerareupdatedifanode nfromthelastlayerisonly
connected to the any of the negative output nodes. These edges
are replaced by a single edge with the weight and bias value as the
averageoftheedges.Ifthenumberofsuchnodesismorethanone,
we replace all the nodes by one node by removing all the edges
from those nodes with one edge with the average weight and bias.
Weperformthisoperationatlines 22-28.Updatingtheedgesand
removing the connection to some nodes is similar to changing the
flowofthedata.Inthisprocess,thepaththatdataflowshasbeen
updated, not the amount of the flow. Since the flow with the edges
fromthelasthiddenlayerandtheoutputlayerhasbeenupdated,
the same flow needs to be adjusted at the preceding layer. This
update operations is shown at lines 31-37. The edges from the pre-
cedinglayer tothe lasthidden layerthatincident onthe removed
nodes are removed and updated the edges incident to a replaced
node at the last hiddenlayer.
In Figure 3, an example of such an operation is shown, where
two nodes are connected with the negative output nodes, not with
the positive output label. First, edges from the last hidden layer to
theoutputlayerarereplacedwithasingleedgeforeachnode.Then,
all such nodes are replaced with a single node with the updated
1  0Module 1 with negative 
nodesRemove edge at the last layer
 2 1  0  2 1  0  2Remove irrelevant node(s)Figure3:ConcernModularization:RemoveIrrelevantEdges
value associated with the edges. Furthermore, to compensate for
theflow,alltheedgesthatareincidentontheremovednodesare
removed and the value of the edges to the replaced node at the
lasthiddenlayerisupdated.Afterthisoperation,wechannelthe
behavior of the negative output label using the approach described
inthe Algorithm 4.
5 EVALUATION
In this section, we first discuss the experimental setup and then
answer three research questions. We discuss how decomposing
the DNN models into smaller components or modules perform
compared to the monolithic models. Furthermore, we compare the
performance of reusing the modules from the same dataset and
differentdatasetswiththemodelbuiltforasimilarconcern.Wealso
answer how replacing a module by another module with the same
concernanddifferentconcernperforms.Finally,wesummarizethe
key findings.
5.1 ExperimentalSetup
In this section, we discuss the datasets, models, and the evaluation
metricsutilizedtoevaluateourapproach.Forconcernidentification
algorithm,wetakethethresholdvaluetobe10%ofthetotalnumber
of edges from the last hidden layer to the output layer, i.e. we stop
removingedgesatthatpointtopreventnetworkfrombecomingtoo
sparse.Fortanglingidentificationimbalance(TI-I)technique,we
choosetheimbalancetobe100:1,wherefora10-classclassification
problem,if1000examplesweretakenfromthepositiveoutputlabel,
1 example from each of the other output labels (remaining nine
labels)are taken for modifyingthe edges of the neuralnetwork.
5.1.1 Datasets. MNIST [ 8].This dataset comprises of various ex-
amples of handwritten digits (0-9). It is divided into the training
and testing section. There are 60,000 training and 10,000 testing
examples, andeachoutputlabel has an equal number of data.
ExtendedMNIST(EMNIST)[ 3].SimilartotheMNIST,thisdataset
has two-dimensional images from the English alphabets (A-Z). As
ourapproachisbasedonthedensehiddenlayer,trainingaDNN
modelwithonlydenselayersdoesnotachievehightestingaccuracy.
To remedy that problem and fixing the number of output labels for
all the datasets under experiment, A-J letters (10) are taken from
thedataset.Trainingandtestingdatasetcontains48000and8000
examples ofA-Jletters, respectively.
894On Decomposing aDeep Neural Network intoModules ESEC/FSE ’20, November8–13,2020,VirtualEvent, USA
Fashion MNIST (FMNIST) [ 26].This dataset is similar to the
structure of the MNIST in terms of the training, testing example
division, and the number of output labels. However, this dataset
has2Dimagesfromdifferentclothes-T-shirt/top,Trouser,Pullover,
Dress, Coat, Sandal, Shirt,Sneaker,Bag,andAnkleboot.
Kuzushiji MNIST(KMNIST)[ 2].The structure ofthedatasetis
similar to the MNIST.Itcontains imagesfrom Japanese digits.
MNIST and FMNIST have been taken from Keras [ 5] and the
othertwodatasets are extractedfrom Tensorflow[ 24].
5.1.2 Models. To evaluate our approach, we build 16 different
models.Thesemodelsaretrainedwithcorrespondingdatasetswith
50epochs,andtheyhave1,2,3,and4hiddenlayers(sizeofeach
layer is 49), respectively. The name of the DNN model has been
representedas <datasetś#ofhiddenlayers> ,e.g.,fortheKMNIST
dataset with 2 hidden layers, the model is referred as KMNIST-2.
For the activation function, we used ReLU for the hidden layer
andSoftmaxforthe outputlayer. For hyperparameters otherthan
epoch(setto50)anddropout(nodropoutused),weusedthedefault
settings of the Keras library. The testing accuracy of each model is
given inTable 1.
5.1.3 EvaluationMetrics.
Accuracy .FormeasuringtheperformanceoftheDNNmodel,
we usetheaccuracymetrics. However,in thecase of thecomposi-
tion of the decomposed modules, we use the same metrics based
onvoting.Weexecute Ndecomposedmodulesfora Nclassifica-
tion problem in parallel and measure the output of each module.
Ifonlyonemodulevotesfortheinput,weassignthelabelofthe
input based on the dominant output label of the module, e.g., if we
execute10modulesforanMNISTproblemandforinput,module
0onlyvotesforthepositiveoutputlabel,thenwelabelthatinput
as0.Ifmorethanoneornomodulevotesforthepositiveoutput
label,thenwechoosethemostconfidentonebypickingthemodule
with the highest probability for the positive output label. Based on
this, we compute the accuracy over the test dataset. We refer to
the composed accuracy of the decomposed modules as MAand the
accuracyofthe trainedmodelas TMA.
Jaccard Index (JI). We use the Jaccard Index to measure the
similarities between the decomposed modules. First, we transform
theweightandbiasfromall thelayers intoa1-dimensionalarray
and compare the similarities between two modules. Finally, we
computetheaveragevalueofthemetricandreportinTable 1.If
theJaccard Index is 0,then thereis nocommonalitybetweentwo
comparedobjects,andif itis1, then they are exactly the same.
5.2 Results
In this section, we validate our techniques to decompose DNN into
modulesandanswer our researchquestions.
5.2.1 Howefficientarethedecomposedmodules? Toanswerthisre-
searchquestion,weevaluatethefourdifferentproposedtechniques
toidentifytanglingconcern.Weutilizedthebestapproachbased
on the accuracy and the Jaccard index and used that technique
to apply concern modularization to build modules. To do so, we
decomposetheDNNmodelsintomodulesbeforechannelingthe
non-dominant concerns, and run them in parallel and compute theaccuracy.Finally,wecomparetheaccuracyamongdifferenttech-
niques and the pre-trained DNN model. Furthermore, we compute
the average Jaccard index (JI) over all the decomposed modules for
eachtechnique.The results have been depictedinTable 1.
We measure the Jaccard index to identify tangling concerns
amongmodules.WefoundthatutilizingtheTI-HPtechnique,where
higherpriority hasbeen giventothe negative examples toupdate
the edges,the lowestJaccard indexcan be achieved.This suggests
that the decomposed modules have the least overlap among them-
selves and are significantly different in structure. However, the
composed accuracy of the decomposed modules is very low (aver-
ageaccuracyis22.93%).Tounderstandthereason,weinvestigate
thestructureofthemodulesandfindthattheedgesresponsiblefor
predictingthenegativeoutputlabelsareupdatedbythepositive
ones. As the edges related to the negative examples are updated
first, then the positive ones, the edges that are responsible for neg-
ative output labels, are removed or modified by the edges from the
positive output labels. Therefore, the network can only classify the
dominant output label correctly not the non-dominant ones (7 out
16 scenarios have accuracy ∼10% that explains the decomposed
modules are not able to classify the rest of the 9 output labels). For
otherscenarios,especiallyforMNIST-3andEMNIST-3,whereall
theedgesrelatedtothenegativeoutputlabelsarenotupdatedor
removed,that results inhigher accuracy.
UtilizingTI-PN,wherewepunishthenegativeexamplesmore
than the positive examples by removing edgesrelated to the nega-
tiveoutputlabels,achieves alower accuracyfor mostof thecases
(10 out 16 scenarios have accuracy <50%). In comparison to the
TI-HPtechnique,weletthenegativeexamplestoaddorupdatethe
edgesresponsibleforthenegativeoutputlabelsafterthepositive
examples identify the common edges. Here, we can see that the
JaccardindexishigherthantheTI-HPthatindicatesthatthereis
ahighercommonalityamongthemodules,whicharemostlythe
negative edges. However, we found that letting the negative exam-
plesaddorupdatealltherelevantedges,thecomposedaccuracy
of the decomposed modules is significantly lower than the DNN
model(averagelossofaccuracyis44.15%).
This problemhasbeenpartially remedied byallowing onlythe
edges that are strongly coupled with the prediction of the negative
examples. If a negative example for a module (e.g., for MNIST, any
input digit other than 0 for a module responsible for 0) has been
taken as input to the system, that particular model can process the
inputandpredictasoneofthenon-dominantclassasthestrong
edgesstillremainintactinthemodules.However,thistechnique
increases the Jaccard index, which depicts that the modules are be-
coming similar to each other. This results in increasing the overlap
betweentheconcerns.However,incomparisontotheotherthree
techniques, adding the strong negative edges performs the best in
termsoftheaccuracy(averageloss-0.29%andmedianloss+0.00%).
We move forward with this technique and select this approach for
utilizing our concern channeling and remedy the tangling of the
concerns. To do so, we update, add, and remove edges (CM-C). The
concernchannelizationachievesthebestaccuracyandbetterJac-
cardindexfor37.5%and25%scenarios,respectively.However,to
identifymore tangledconcerns,we utilizedthe CM-RIEapproach,
whereweremoveirrelevantnodesonlyconnectedwiththenega-
tive output nodes and update the edges to reflect the change at the
895ESEC/FSE ’20, November8–13,2020,VirtualEvent, USA Rangeet Pan andHrideshRajan
Table 1:AccuracyandSimilarityofDecomposed Modules;Acc: Accuracy,JI: Mean Jaccard index
ModelTI-I TI-PN TI-HP TI-SNE CM-C CM-RIE
Model Accuracy Acc JIAcc JIAcc JIAcc JIAcc JIAcc JI
MNIST-1 94.91% 94.91% 0.479.79%0.4623.09% 0.0494.91% 0.4794.91% 0.4594.90% 0.43
MNIST-2 98.39% 96.83% 0.646.19%0.6413.09% 0.0296.83% 0.6596.83% 0.6396.82% 0.63
MNIST-3 98.47% 69.19% 0.449.01%0.4496.27% 0.4596.30% 0.4596.30% 0.4496.33% 0.43
MNIST-4 96.79% 96.44% 0.537.50%0.549.80%0.0196.79% 0.5596.79% 0.5496.75% 0.53
FMNIST-1 85.82% 85.82% 0.7869.21% 0.7810.72% 0.0985.82% 0.7885.82% 0.7685.85% 0.75
FMNIST-2 87.58% 87.58% 0.649.28%0.6418.69% 0.1487.58% 0.6587.58% 0.6387.56% 0.63
FMNIST-3 87.09% 77.55% 0.5261.95% 0.529.96%0.0587.09% 0.5385.64% 0.587.10% 0.51
FMNIST-4 87.79% 87.51% 0.5681.11% 0.57 10%0.0187.79% 0.5787.79% 0.5687.95% 0.55
KMNIST-1 76.02% 64.29% 0.5176.32% 0.5123.17% 0.0376.32% 0.5176.02% 0.5076.03% 0.48
KMNIST-2 83.29% 83.29% 0.7270.63% 0.7210.00% 0.0282.09% 0.7383.29% 0.7283.29% 0.7
KMNIST-3 83.02% 83.02% 0.4842.70% 0.4711.41% 0.0183.02% 0.4983.02% 0.4782.97% 0.47
KMNIST-4 83.63% 82.89% 0.5439.50% 0.5610.50% 0.0183.63% 0.5783.63% 0.5683.63% 0.54
EMNIST-1 89.00% 89.00% 0.4381.06% 0.4113.85% 0.0489.00% 0.4389.00% 0.4189.01% 0.4
EMNIST-2 92.27% 92.10% 0.6176.89% 0.6110.07% 0.0292.27% 0.6292.27% 0.6192.28% 0.61
EMNIST-3 92.20% 62.48% 0.5337.90% 0.5383.84% 0.5492.20% 0.5592.20% 0.5492.16% 0.53
EMNIST-4 91.88% 84.33% 0.5342.74% 0.5412.38% 0.0191.89% 0.5591.89% 0.5495.33% 0.53
Table 2:Intra-Dataset Reuse.All results in%.
MNIST and Fashion MNIST.MN:MNIST,FM: FMNIST.
MN 1 2 3 4 5 6 7 8 9
FMMNMATMA MATMA MATMA MATMA MATMA MATMA MATMA MATMA MATMA
T-shirt/top 010099.999.1 99.499.6 99.899.6 99.699.4 98.898.7 99.099.2 99.699.4 99.297.6 99.2
Trouser 198.2 98.599.7 99.499.8 99.699.9 99.910099.799.6 99.599.5 99.099.5 99.299.8 99.5
Pullover 293.7 95.299.3 98.999.0 97.699.4 98.599.9 98.599.5 99.098.9 97.498.9 99.199.6 99.2
Dress 392.2 91.598.4 97.597.1 97.399.9 99.498.6 97.799.8 99.599.3 98.698.8 97.599.2 98.6
Coat 497.2 98.399.3 99.588.0 86.396.0 95.199.8 99.099.2 99.399.5 98.699.5 99.298.2 97.7
Sandal 597.3 99.799.7 100100 10010099.910099.899.0 98.199.6 99.399.1 97.699.1 98.7
Shirt 652.4 84.799.1 98.488.0 86.195.4 93.690.3 87.310099.999.7 99.799.2 99.199.5 99.7
Sneaker 797.4 99.799.7 100100 100100 100100 10098.0 96.1100 10099.6 98.698.6 98.4
Bag 897.2 98.199.4 99.799.1 98.499.1 99.199.2 99.099.6 99.698.2 97.799.6 99.099.2 97.9
Ankle boot 997.4 10099.6 10099.9 99.999.9 99.9100 10098.5 98.0100 10096.5 96.499.8 99.6
FM T-Shirt/top Trouser Pullover Dress Coat Sandal Shirt Sneaker Bag
Blue represents the intramodulecombinationsfor F-MNISTand Yellow represents the the intramodulecombinationsfor MNIST.
ExtendedMNIST (EMNIST)and KuzushijiMNIST (KMNIST).EM: EMNIST,KM: KMNIST.
EM A B C D E F G H I
KMEMMATMA MATMA MATMA MATMA MATMA MATMA MATMA MATMA MATMA
Japanese 0 A97.8 97.996.6 97.894.8 94.995.1 97.498.4 98.495.3 92.994.2 96.598.5 98.897.8 98.6
Japanese 1 B97.5 98.298.9 98.097.6 96.898.6 98.498.9 98.896.1 95.296.8 97.099.4 98.298.9 97.9
Japanese 2 C98.3 97.994.2 94.198.9 98.495.5 93.198.8 98.898.4 97.899.3 97.899.1 99.099.4 99.4
Japanese 3 D97.8 97.098.4 97.293.5 93.899.3 98.099.0 98.397.9 95.297.8 97.399.3 98.197.4 97.2
Japanese 4 E87.7 94.393.4 93.495.3 95.197.2 97.198.2 98.198.5 98.199.2 98.199.1 98.899.5 98.5
Japanese 5 F93.8 96.495.2 94.591.5 91.397.3 97.095.9 96.197.7 97.198.9 97.898.4 96.698.8 97.1
Japanese 6 G97.6 99.093.6 91.695.1 96.397.9 97.096.3 95.096.0 95.598.6 97.898.9 97.097.3 97.9
Japanese 7 H93.1 94.395.4 94.295.7 93.997.8 98.092.7 89.797.1 96.394.8 96.899.3 97.999.3 97.3
Japanese 8 I92.8 96.994.6 94.893.1 95.796.9 96.096.2 95.695.0 96.397.6 97.795.2 96.296.0 94.5
Japanese 9 J97.3 97.693.3 95.292.2 94.696.9 97.295.0 94.995.6 97.096.4 94.495.1 92.795.7 96.7
KM Japanese 0 Japanese 1 Japanese 2 Japanese 3 Japanese 4 Japanese 5 Japanese 6 Japanese 7 Japanese 8
Blue represents the intramodulecombinationsfor EMNISTand Yellow represents the the intramodulecombinationsfor KMNIST.
MA: Decomposed ModuleAccuracy, TMA:Trained ModelAccuracy.
lasthiddenlayer.Weapplythistechniquebeforechannelizingnon-
dominantconcerns.FromTable 1,wecanvalidatethattheCM-RIE
decreases the Jaccard index from the prior techniques (CM-C), and
this approach can produce modules that perform the best in terms
ofthe accuracyfor 56.25%(9outof16) ofthe cases.
With the accuracy achieved using the CM-RIE technique, we
found that the accuracy after decomposing loses 0.01% on average
(medianis0.00%).Also,in9outof16cases,wewereabletoincrease
orabletogetthesameaccuracyasthetrainedmodel.Tovalidate
whether there is a significant difference between the DNN modelsand decomposed modules, the average number of edges with value
zero(inactive)have beencomputedforeach scenario.Thismetric
validates how the edge removal technique to decompose the DNN
modelintomodulesperformsinpractice.Wefoundthatforcases
wheredecomposingaDNNmodelintomoduleseithergainaccu-
racy or remain the same, there are, on average, 33.79% of the edges
are inactive. This result shows that the modules generated are not
the same as the DNN model.
896On Decomposing aDeep Neural Network intoModules ESEC/FSE ’20, November8–13,2020,VirtualEvent, USA
Table 3:Inter DatasetReuse.All results are in%
MNIST vs ExtendedMNIST (EMNIST). MN:MNIST,EM: EMNIST
EM A B C D E F G H I J
MNMATMAMATMAMATMAMATMAMATMAMATMAMATMAMATMAMATMAMATMA
074.899.395.799.697.299.789.798.998.899.199.299.997.698.599.099.799.499.693.799.4
198.999.797.699.999.7100.099.499.899.799.799.599.899.499.899.399.857.096.398.699.6
282.098.598.599.485.299.394.798.698.699.198.599.196.998.798.399.296.398.197.897.2
390.798.797.199.597.199.854.899.598.599.498.699.488.799.899.399.197.399.687.598.5
488.898.999.799.999.499.899.399.386.399.099.099.899.299.796.599.298.999.699.899.7
593.799.295.699.196.599.298.699.596.599.066.899.888.799.098.199.398.099.292.898.5
697.099.795.499.898.099.698.499.898.799.787.999.881.899.593.699.698.499.198.999.5
796.999.799.299.799.399.798.999.699.399.699.399.398.699.778.099.698.199.396.399.6
889.099.493.999.398.999.797.599.995.299.293.799.890.999.496.899.656.998.798.099.4
985.499.299.599.999.499.599.499.999.299.799.299.995.399.098.799.898.899.661.899.9
MNIST vs KuzushijiMNIST (KMNIST).MN:MNIST,KM: KMNIST
KMJapanese 0 Japanese 1 Japanese 2 Japanese 3 Japanese 4 Japanese 5 Japanese 6 Japanese 7 Japanese 8 Japanese 9
MNMATMAMATMAMATMAMATMAMATMAMATMAMATMAMATMAMATMAMATMA
086.099.993.699.898.499.997.899.995.699.796.999.997.699.991.599.593.099.998.799.9
191.8100.066.0100.098.5100.097.199.898.8100.098.4100.097.399.999.6100.098.5100.098.7100.0
273.499.793.9100.077.899.793.099.886.099.893.899.987.1100.088.999.895.0100.093.3100.0
380.399.971.799.978.1100.082.199.988.099.887.599.974.099.992.9100.077.1100.084.499.1
495.699.999.699.899.399.999.799.973.199.799.399.899.6100.099.199.999.699.999.3100.0
582.799.991.099.885.899.685.899.890.199.878.599.990.999.790.399.593.999.888.299.9
695.5100.098.199.997.099.998.899.894.199.997.899.879.799.995.899.898.999.997.799.9
787.899.996.699.997.6100.097.499.996.799.898.399.997.299.984.198.497.1100.095.1100.0
892.399.990.199.981.0100.098.199.993.899.894.399.890.399.991.599.671.4100.087.399.9
995.8100.094.899.994.6100.096.6100.098.099.795.8100.095.5100.096.499.891.6100.073.7100.0
MA: Decomposed ModuleAccuracy, TMA:Trained ModelAccuracy.
However,ourdecompositiontechniquecanbemodifiedinthe
futuretoincreasetheinactivenodesandremovethetangledcon-
cerns more efficiently. Also, to verifywhether changing the hidden
layers’ width affects the performance, we altered the width of each
hidden layer to 70 from 49 for all the 16 models. We computed the
accuracy of the decomposed modules using the CM-RIE approach.
WefoundthatthechangeofaccuracyforMNIST,FMNIST,KMNIST,
and EMNIST are on average, -0.01%, +0.01%, -0.01%, and -0.12%,
respectively.For furtherRQs,we evaluateourproposedapproach
with the decomposed modules build with the CM-RIE technique
using models having 49 nodes per hidden layer (as discussed in
ğ5.1.2).
5.2.2 Does modularity in DNN enable reuse? In this research ques-
tion, we validate whether fine-grained reuse can be achieved by
utilizing the decomposed modules. To validate the reusability and
answerthisRQ,weevaluatedecomposedmodulesfrom16different
DNN models andreuse themintwodifferentsettings.
Intra-DatasetReuse. Inthisscenario,westudytwomodules
decomposedfromthesameDNNmodelandexecutetheminparallel
to build a smaller problem and validate against a DNN model built
with the dominant examples for the picked modules. In Figure 1,
wedescribe asimilarexample,wherewetake moduleresponsible
for the digits 0 and 1 from a pre-trained DNN model and reuse
them to build a binary classifier. To validate such scenarios, we
train a DNN model with the same examples (based on the example,
digit 0 and 1) and the same structure as the DNN model that has
beendecomposedtoobtainthemodules.Finally,wecomparethe
composed accuracy of the modules and the accuracy of the trained
DNN model. In Table 2, we show various scenarios of intra dataset
reuse. While performing this evaluation, we use the modules build
from the model with the 4 hidden layers (MNIST-4, EMNIST-4,FMNIST-4,andKMNIST-4).Asthetotalnumberoftheoutputlabels
in each dataset is ten, choosing two modules responsible for one
output label can have 45 scenarios (/parenleftbig10
2/parenrightbig). In Table 2(upper half),
we combine the results from MNIST and FMNIST datasets. The
cellswithblueandyellowhavebeentakenfromtheFMNISTand
MNIST datasets, respectively. We perform similar operations on
KMNIST and EMNIST and depict the results in Table 2(bottom
half). Our results suggests that for 80% (36 out of 45), 68.89% (31
outof45),80%(36outof45),and51.11%(23outof45)scenariosfor
MNIST,FMNIST,EMNIST,andKMNISTrespectively,thecomposed
accuracyusingmodulesismoreorthesamecomparedtothetrained
DNNmodels.Ourresultsuggeststhatthereisnosignificantchange
ofaccuracyconsideringallthecases(4datasets).Averagegainof
accuracyis0.03% (+0.22% median).
Inter-Dataset Reuse. While the prior experiments were done
onthesamedatasetandmodel,theseexperimentsarecarriedon
differentdatasetswithmodelsbuildwithsimilararchitecture(same
number of hidden layers). We evaluate the MNIST vs. EMNIST and
MNIST vs. KMNIST with the same choice of the models. Similar to
thepriorexperimentalsetup,weevaluatethecomposedaccuracy
of two decomposed modules taken from two different datasets and
modelsandexecutethemonthedominantexamplesofthemodules,
e.g.,we takeone modulefromMNIST(MNIST-4 model) thatcan
classify English 0 and one module from KMNIST (KNIST-4 model)
that isresponsible for Japanese 2.
In this example, we validate the accuracy against the inputs and
test with the example taken from the test dataset where the output
label is either English 0 or Japanese 2. Also, we train a model with
the same number of hidden layers (four) with the same output
labels (English 0 and Japanese 2) from the training dataset and
compare them. In Table 3(upper half), we take two modules for
eachexperimentandcomparethetrainedDNNmodelbuildfrom
897ESEC/FSE ’20, November8–13,2020,VirtualEvent, USA Rangeet Pan andHrideshRajan
Table 4:Intra DatasetReplacement.RM:Replaced module.
DataSet TMA Prior MA RM0 RM1 RM2 RM3 RM4 RM5 RM6 RM7 RM8 RM9
MNIST 94.91% 94.90% 94.91%94.59% 94.83% 92.26% 94.40% 93.68% 95.11% 94.46% 93.72% 93.29%
FMNIST 85.82% 85.93% 85.36% 85.84%85.41% 85.82%84.91% 85.87%84.10% 85.88%85.98%85.78%
KMNIST 76.02% 76.03% 76.32% 74.18% 74.43% 75.64% 73.74% 73.77% 75.54% 76.90% 74.44% 76.53%
EMNIST 89.00% 89.01% 87.81% 87.6%87.89% 88.40% 86.42% 89.36%88.73% 88.51% 86.23% 85.57%
Table 5:Inter-Dataset Replacement.All results are in%
MNIST vs ExtendedMNIST (EMNIST). MN:MNIST,EM: EMNIST
EM A B C D E F G H I J
MNMATMAMATMAMATMAMATMAMATMAMATMAMATMAMATMAMATMAMATMA
094.094.893.794.294.795.594.495.094.495.394.394.993.994.894.195.494.495.194.695.7
194.394.494.094.294.594.594.795.794.495.894.994.794.795.294.495.695.295.894.696.0
295.595.294.096.194.594.594.895.594.495.394.995.194.495.294.495.694.795.894.796.0
394.894.593.494.794.595.894.495.393.895.594.095.493.794.793.895.194.394.994.195.9
495.194.794.994.395.494.995.595.395.395.795.795.295.193.995.396.096.196.395.595.4
592.694.992.595.393.095.293.095.192.995.894.095.594.491.592.996.093.796.193.296.6
691.895.791.494.792.195.792.895.191.995.492.995.691.895.491.994.192.896.092.496.0
794.694.994.493.895.094.994.995.395.395.594.996.095.495.294.995.595.395.395.195.4
886.893.894.994.387.594.687.593.987.193.987.494.787.094.787.294.287.594.287.394.5
993.294.893.294.693.795.095.195.493.295.794.095.693.194.993.695.293.595.693.495.0
MNIST vs KuzushijiMNIST (KMNIST).MN:MNIST,KM: KMNIST
KMJapanese 0 Japanese 1 Japanese 2 Japanese 3 Japanese 4 Japanese 5 Japanese 6 Japanese 7 Japanese 8 Japanese 9
MNMATMAMATMAMATMAMATMAMATMAMATMAMATMAMATMAMATMAMATMA
090.395.989.994.293.094.691.195.490.496.091.196.390.394.990.495.590.795.190.696.5
188.796.288.295.489.095.792.095.988.694.889.296.488.594.988.995.989.596.489.096.2
288.994.788.694.989.395.990.195.789.096.189.596.189.096.189.295.390.896.189.495.8
390.894.890.195.591.395.192.195.790.595.991.696.490.495.590.696.690.895.990.896.2
490.395.589.496.191.495.490.494.989.895.290.795.890.195.589.996.090.296.290.196.5
591.795.690.895.391.994.691.994.991.295.892.295.791.195.891.395.791.595.991.596.6
688.795.288.394.989.695.790.895.588.796.189.396.088.895.688.896.289.496.089.196.2
791.194.789.995.091.695.690.695.690.495.291.294.790.495.590.895.390.995.890.796.4
890.395.688.894.789.696.091.395.289.195.989.596.089.095.289.295.489.696.089.696.6
989.995.489.795.290.795.791.195.790.195.190.996.290.195.690.396.091.496.590.695.9
MA: Decomposed ModuleAccuracy, TMA:Trained ModelAccuracy.
theexamplesfromtheoutputlabels.Wereporttheevaluationfor
100 combinations (10x10) of re(use) from MNIST and E-MNIST
modules. Our result suggests that for only six scenarios, reusing
themodulescanperformthesameasthetrainedmodel.Reusing
themodulesbetweendifferentdatasetscancausealossofaccuracy
5.36% on average (median 1.41%) in comparison to DNN models
trained with those examples. Similarly, we evaluate with MNIST
andKMNISTandreporttheresultsinTable 3(bottomhalf).Our
result suggests that loss of accuracy is 8.28% on average (-5.67%
median).
Also,webuilda duodecimalclassifierbasedonour motivating
exampledepictedintheFigure 1,whereallthemodulesdecomposed
from MNIST-4and decomposedmodulesresponsibleforEnglish
letterAandBrun in parallel, and the composed accuracy of the
decomposed module is 83.40%. Furthermore, a DNN model has
been trained based on the 0-9 and A-B, and the testing accuracy
of that model is 91.45%. This shows that decomposing the DNN
into modules and reusing them to build a problem to recognize the
duodecimal(0-9AB)digitsispossible.However,theaccuracyofthe
compositionofthemodulesislower(8.05%)thanthemodeltrained
from scratch. This could be a potential direction for future work.
5.2.3 DoesmodularityinDNNenablereplacement? Inthisresearch
question,weanswerwhetherthedecomposedmodulesdecomposed
canbereplacedbyothermodules.ReplacingamodulefromasetofmodulesbuiltbydecomposingaDNNmodelcanhelpineither
of these two directions. First, referring to the bottom half of the
Figure1, where the faulty part of the DNN has been replaced with
a better-fitted part from a different DNN model trained on the
same problem, and this scenario we refer to as the intra dataset
replacement.Second,apartoftheDNNmodelcanbereplacedby
apartfromaDNNmodelthatrepresentsadifferentconcern.We
represent these scenarios as inter problem replacement, where the
datasetoftheproblemsisdifferent.Thesebroadercategoriesofthe
replaceability study have discussedinthe nextfewparagraphs.
Intra Dataset Replacement. In this scenario, we replace a
modulefromasetofmodulesdecomposedfromaDNNmodelwith
amodulebuiltonthesamedatasetbutwithdifferentconfigurations
(decomposedfromadifferentDNNmodel).Toevaluatethesesce-
narios,wereplaceeachmodulefromtheleastcomplexmodel(based
on the number of hidden layers (1)) from each dataset and replace
thatwithamoduleofsameoutputlabelfromamorecomplexmodel
(modelwithfourhiddenlayers).Finally,wecomputethecomposed
accuracyofthemodulesandcomparethemwiththeaccuracyof
the DNN model from which the modules were decomposed and
the prior accuracy of the modules. Table 4depicts the scenarios
for four datasets and we report the composed accuracy for MNIST-
1, FMNIST-1, KMNIST-1, and EMNIST-1 with modules replaced
from MNIST-4, FMNIST-4, KMNIST-4, and EMNIST-4, respectively.
898On Decomposing aDeep Neural Network intoModules ESEC/FSE ’20, November8–13,2020,VirtualEvent, USA
We found that replacing modules with decomposed modules from
morecomplexmodelscanincreasethecomposedaccuracyofthe
decomposed modules for 10 out of 40 scenarios. On average, there
is an average 0.76% (median is 0.50%) drop in the accuracy when
compared to the composed accuracy of the modules before replace-
ment.Furthermore,weevaluatetheinterdatasetreplacementon
the example depicted in Figure 1. To make a part of the model
faulty, we impose bias in the training dataset. We build a DNN
model with 6000 training examples for all the output labels except
theoutputlabel5,whereweuse500examples.ThetrainedDNN
model achieves testing accuracy of 96.82%. Then, we decompose
the DNN model intro modules and replace the module 5 with a
module responsible for the same output label, decomposed from
MNIST-4.Ourresultshowsthattheaccuracyafterthereplacement
is98.66%(+1.84%).Thus,wecanconcludethatourapproachcanbe
utilizedtoreplaceafaultypieceofDNNmodelwithaparttaken
from abetterDNN model.
InterDatasetReplacement. While a module can be replaced
with a module with the same concern, there can be a situation
that needs amodule to be replaced with a different concern. Here,
wereplaceonemodulefromeachdatasetandreplacethatwitha
module taken from a different dataset, e.g., the module responsible
for classifying English 1 replaced with a module for classifying
Japanese 1. We validate our approach to replacing the modules
from different datasets by conducting the experiments on the mod-
ules decomposed from the DNN models with four hidden layers
(MNIST-4, EMNIST-4, and KMNIST-4). Our scenarios involve re-
placing modules from MNIST with KMNIST and modules from
MNIST with EMNIST. Our results suggest that the accuracy of the
replaced modules perform worse than the DNN models trained
withthesamestructure(fourhiddenlayers)withthetrainingex-
amples from the same output classes. In Table 5, we depict such
scenarios.Ourevaluationshowsthatbyreplacingmodulesfrom
MNIST with Extended MNIST, theaccuracy is decreased 1.62% on
average (median1.16%) in comparison to the DNN models trained
withthesameconfigurations.In thecaseofsubstitutingmodules
from MNIST with Kuzushiji MNIST, the accuracy drop is 5.44%
(median 5.40%)compare to the models trainedfrom scratch.
Based on the overall evaluation, we found that replacing a mod-
ulewithsimilarconcernsanddifferentconcernscanbeachieved.
However,thereisalossofaccuracyincomparisontothemodels
trainedfrom scratch.
5.2.4 Summary. Wefoundthatthecompositionofthedecomposed
modules losses only 0.01% accuracy on average, and it performs
the same or the better for 56.25% (9 out of 16) cases compared to
themonolithicmodels.Also,reusingthemodulesfromthesame
problem can gain 0.03% accuracy, on average, compared to the
modeltrainedwiththesamedatasets.However,reusingmodules
from differentdatasets loss6.82% accuracy, on average. We found
that replacing a module with another module, decomposed from a
differentmodel, losses0.76% and 3.53% for intra-datasetand inter-
dataset scenarios,respectively.
6 THREATS TO VALIDITY
External Threats. While the idea of decomposing deep neural
networksintomodulesisgeneral,ourdecompositionalgorithmshave utilized certain simplifying assumptions about the input deep
neuralnetworks.Inthiscurrentwork,wehavetackledfeedforward
fullyconnectedneuralnetworks,thatuseReLUandSoftmaxasthe
activationfunctionforallhiddenlayersandoutputlayer,respec-
tively. Further research is needed to generalize these algorithms
to other categories of deep neural networks. In particular, concern
identification will need to be generalized to other kinds of deep
neural network layers. We anticipate that tangling identification
and concern modularization techniques will be applicable to other
kindsoflayers but do not have evidenceas of this writing.
InternalThreats. Inthisstudy,aninternalthreatcanbethechoice
of datasets for evaluation. To alleviate this threat, we have used
MNISTandFashionMNISTfromKerasandKuzishijiMNISTand
ExtendedMNISTfrom Tensorflow.
7 CONCLUSION
Inthiswork,weexploredwhetheraDNNcanbedecomposedso
that parts of the DNN, which we call DNN modules, can be reused
to build different DNNs or replaced with better DNN modules. We
described our technique that relies on concern identification to
identify thesub-network,identifyingtanglingofother concerns,
andfinallydecomposingthesub-networkintoDNN modules. We
described four different techniques for reducing tangling. We have
evaluatedourapproachusingfourcanonicaldatasetsandsixteen
differentmodels.Oftendecompositionintomoduleshassomecosts,
butwefindthatthecostsforDNNdecompositionisalreadyvery
minimal.In56.25%ofcases,decomposedmodulesareslightlymore
accurate (0.00%-3.45%), and in remaining cases lose very little accu-
racy (0.01%-2.14%). The benefits of decomposition are observed in
enablingreuseand replacement.We observethatforour datasets
and models both reuse and replacement is possible. Based on these
results,webelievethatthisworktakesthefirststeptowardenabling
more modulardesignsfor deep learning.
There are a number of exciting avenues for future work, e.g.,
betterdecompositiontechniquesbuildingonthisworkcanbede-
veloped to improve the accuracy of DNN modules in reuse and
replacementscenarios,decompositionforotherkindsofDNNscan
bedeveloped,unit-testingofDNNmodulescouldbeexplored,types
andcontractsforDNNmodulescouldbedeveloped,composition
ofDNN moduleswithtraditional functionscould be explored,etc.
Though our current algorithms are implemented based on a cer-
tainassumptions,webelievethatthekeyconceptofdecomposinga
monolithic model into smaller components can be applied in other
kinds of deepneural networks. For instance,for models built with
convolutionlayers,theconceptofactive-inactivenodecancertainly
be altered to introduce the sliding window based approach that
captures the section ofthe network responsible for aconcern.
ACKNOWLEDGEMENTS
This work was supported in partby US NSF undergrants CNS-15-
13263, and CCF-19-34884. All opinions are of the authors and do
not reflect the view of sponsors. We thank ESEC/FSE’20 reviewers
for constructive comments that were very helpful. We are thankful
to Md. Johirul Islam for the discussion and suggestion about the
implementation.
899ESEC/FSE ’20, November8–13,2020,VirtualEvent, USA Rangeet Pan andHrideshRajan
REFERENCES
[1]MuffyCalder,MarioKolberg,EvanH.Magill,andStephanReiff-Marganiec.2003.
FeatureInteraction:ACriticalReviewandConsideredForecast. Comput.Netw.
41,1 (Jan. 2003),115âĂŞ141. https://doi.org/10.1016/S1389-1286(02)00352-3
[2]Tarin Clanuwat, Mikel Bober-Irizar, Asanobu Kitamoto, Alex Lamb, Kazuaki
Yamamoto, and David Ha. 2018. Deep Learning for Classical Japanese Literature.
arXiv:cs.CV/cs.CV/1812.01718
[3]Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre Van Schaik. 2017.
EMNIST: Extending MNIST to handwritten letters. 2017 International Joint
Conference on Neural Networks (IJCNN) (2017).https://doi.org/10.1109/ijcnn.
2017.7966217
[4]EdsgerWDijkstra.1982. Ontheroleofscientificthought. In Selectedwritingson
computing:a personalperspective . Springer, 60ś66.
[5] Keras. 2020. KerasDataset. https://keras.io/datasets/ .
[6]GregorKiczales,JohnLamping,AnuragMendhekar,ChrisMaeda,CristinaLopes,
Jean-Marc Loingtier, and John Irwin. 1997. Aspect-oriented programming. In
ECOOP’97 Ð Object-Oriented Programming , Mehmet Akşit and Satoshi Matsuoka
(Eds.).SpringerBerlin Heidelberg, Berlin, Heidelberg, 220ś242.
[7]LouisKirsch,JuliusKunze,andDavidBarber.2018. Modularnetworks:Learning
to decompose neural computation. In Advances in Neural Information Processing
Systems. 2408ś2418.
[8]YannLeCun, LéonBottou, Yoshua Bengio,andPatrick Haffner.1998. Gradient-
basedlearning applied to documentrecognition. Proc.IEEE 86,11 (1998), 2278ś
2324.
[9]Nan Li, Ivor W Tsang, and Zhi-Hua Zhou. 2012. Efficient optimization of perfor-
mancemeasuresbyclassifieradaptation. IEEEtransactionsonpatternanalysis
and machineintelligence 35,6 (2012), 1370ś1382.
[10]YiLiuandYuanFZheng.2005. One-against-allmulti-classSVMclassification
usingreliabilitymeasures.In Proceedings.2005IEEEInternationalJointConference
onNeural Networks,2005. , Vol. 2.IEEE,849ś854.
[11]Eneldo Loza Mencía and Johannes Furnkranz. 2008. Pairwise learning of multil-
abel classifications with perceptrons. In 2008 IEEE International Joint Conference
onNeuralNetworks(IEEEWorldCongressonComputationalIntelligence) .IEEE,
2899ś2906.
[12]Guobin Ou and Yi Lu Murphey. 2007. Multi-class pattern classification using
neural networks. PatternRecognition 40,1 (2007), 4ś18.
[13]Rangeet Pan and Hridesh Rajan. 2020. On Decomposing a Deep Neural Network
intoModules. https://doi.org/10.5281/zenodo.3874077
[14]SinnoJialinPan,IvorWTsang,JamesTKwok,andQiangYang.2010. Domain
adaptationviatransfercomponentanalysis. IEEETransactionsonNeuralNetworks
22,2 (2010), 199ś210.[15]D. L. Parnas. 1972. On the Criteria to Be Used in Decomposing Systems into
Modules. Commun.ACM 15,12(Dec.1972),1053âĂŞ1058. https://doi.org/10.
1145/361598.361623
[16]Lorien Y. Pratt, Jack Mostow, and Candace A. Kamm. 1991. Direct Transfer
of Learned Information among Neural Networks. In Proceedings of the Ninth
National Conference on Artificial Intelligence - Volume 2 (AAAIâĂŹ91) . AAAI
Press,584âĂŞ589.
[17]Christian Prehofer. 1997. Feature-oriented programming: A fresh look at ob-
jects.InECOOP’97ÐObject-OrientedProgramming ,MehmetAkşitandSatoshi
Matsuoka(Eds.).SpringerBerlin Heidelberg, Berlin, Heidelberg, 419ś443.
[18]PeterPrettenhoferandBennoStein.2010.Cross-languagetextclassificationusing
structural correspondencelearning. In Proceedings ofthe 48th annual meetingof
the associationfor computational linguistics . 1118ś1127.
[19]Hridesh Rajan and Gary T. Leavens. 2008. Ptolemy: A Language with Quanti-
fied, Typed Events. In ECOOP ’08:22nd European Conference on Object-Oriented
Programming .
[20]Hridesh Rajan and Kevin J. Sullivan. 2005. Classpects: Unifying Aspect- and
Object-OrientedLanguageDesign.In ICSE’05:Proceedingsofthe27thInterna-
tional Conference onSoftwareEngineering . ACM,NewYork, NY, USA.
[21]Xiaoxiao Shi, Qi Liu, Wei Fan, S Yu Philip, and Ruixin Zhu. 2010. Transfer
learningonheterogenousfeaturespacesviaspectraltransformation.In 2010IEEE
international conference ondatamining . IEEE,1049ś1054.
[22]Kevin Sullivan, William G. Griswold, Yuanyuan Song, Yuanfang Cai, Macneil
Shonle,NishitTewari,andHrideshRajan.2005. Informationhidinginterfaces
for aspect-oriented design. In ESEC/FSE-13: Proceedings of the 10th European
software engineering conference held jointly with 13th ACM SIGSOFT international
symposium on Foundations of software engineering . ACM, New York, NY, USA,
166ś175.
[23]P. Tarr, H. Ossher, W. Harrison, and S. Sutton. 1999. N degrees of separation:
multi-dimensional separation of concerns. In Proceedings of the 21st Interna-
tional Conference on Software Engineering (ICSE ’99) . IEEE Computer Society, Los
Alamitos, CA, USA,107ś119. https://doi.ieeecomputersociety.org/
[24] Tensorflow. 2020. TensorflowDataset. https://www.tensorflow.org/datasets .
[25]Rui Xia, Chengqing Zong, Xuelei Hu, and Erik Cambria. 2013. Feature ensemble
plus sample selection: domain adaptation for sentiment classification. IEEE
IntelligentSystems 28,3 (2013), 10ś18.
[26]Han Xiao, Kashif Rasul, and Roland Vollgraf. 2017. Fashion-mnist: a novel
image dataset for benchmarking machine learning algorithms. arXiv preprint
arXiv:1708.07747 (2017).
[27]Zhi-Hua Zhou. 2016. Learnware: on the future of machine learning. Frontiers
Comput. Sci. 10,4 (2016), 589ś590.
[28]WeiweiZongandGuang-BinHuang.2011. Facerecognitionbasedonextreme
learning machine. Neurocomputing 74,16(2011), 2541ś2551.
900