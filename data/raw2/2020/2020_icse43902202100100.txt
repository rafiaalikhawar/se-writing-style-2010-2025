White-Box Analysis over Machine Learning:
Modeling Performance of Conﬁgurable Systems
Miguel Velez
Carnegie Mellon UniversityPooyan Jamshidi
University of South CarolinaNorbert Siegmund
Leipzig University
Sven Apel
Saarland Informatics Campus
Saarland UniversityChristian Kästner
Carnegie Mellon University
Abstract —Performance-inﬂuence models can help stakeholders
understand how and where conﬁguration options and their
interactions inﬂuence the performance of a system. With this
understanding, stakeholders can debug performance behavior
and make deliberate conﬁguration decisions. Current black-box
techniques to build such models combine various sampling and
learning strategies, resulting in tradeoffs between measurement
effort, accuracy, and interpretability. We present Comprex, a
white-box approach to build performance-inﬂuence models for
conﬁgurable systems, combining insights of local measurements,
dynamic taint analysis to track options in the implementation,
compositionality, and compression of the conﬁguration space,
without relying on machine learning to extrapolate incomplete
samples. Our evaluation on 4 widely-used, open-source
projects demonstrates that Comprex builds similarly accurate
performance-inﬂuence models to the most accurate and
expensive black-box approach, but at a reduced cost and with
additional beneﬁts from interpretable and local models.
I. I NTRODUCTION
Conﬁguring software systems is often challenging. In prac-
tice, many users execute systems with inefﬁcient conﬁgura-
tions in terms of performance and, often directly correlated,
energy consumption [22, 23, 33, 55]. While users can ad-
just conﬁguration options to tradeoff between performance
and the system’s functionality, this conﬁguration task can be
overwhelming; many systems, such as databases, Web servers,
and video encoders, have numerous conﬁguration options that
may interact, possibly producing unexpected and undesired
behavior. For this reason, understanding how options and
their interactions affect the system’s performance and making
suitable conﬁguration decisions can be difﬁcult [4, 78]; users
often have, at most, vague intuitions [27, 63, 80]. We aim at ef-
ﬁciently building performance-inﬂuence models, without ma-
chine learning, that characterize how options inﬂuence the sys-
tem’s performance. We intent our models to be easy to inspect
and interpret by developers and users to ease performance
debugging and to make informed conﬁguration decisions.
Good performance-inﬂuence models help users understand
how, where, and why options and their interactions inﬂuence
the performance of the system in a speciﬁc way [38, 63]. In
the situations where these models are useful, simple manual
experiments are often not practical due to a system’s expo-
nentially large conﬁguration space. For example, a developer
considering whether to enable encryption would likely expect
that encryption would slow down the system. However, the+ Interpretable
x Not interpretable
R50: 50 random conﬁgurations
R200: 200 random conﬁgurations
LR: Stepwise linear regression
RF: Random forest
Comprex: Our approach
0+
Cost (minutes)Error (MAPE)
0
40 20Comprex
xR200 & RF
510+R200 & LR
200400
xR50 & RF1000 +R50 & LRDensity Converter
100300
Fig. 1: Our white-box approach, Comprex, builds similarly accurate
(low error) performance-inﬂuence models to the most accurate and
expensive sampling and machine learning approaches, but can often
be built more efﬁciently (low cost) and are interpretable. Additionally,
our models are signiﬁcantly more accurate than other sampling and
machine learning approaches that produce interpretable linear models.
developer may need to perform numerous experiments to
quantify the effect of the option itself and of its interactions
with other conﬁguration decisions in a given environment and
workload. To understand why and where encryption causes
the slow down, a developer would have to carefully proﬁle
or debug the system in different conﬁgurations.
Most existing techniques build global performance-
inﬂuence models and treat the system as a black box,
measuring the system’s execution in an environment with a
given workload for a subset of all conﬁgurations, and learning
a model from these observations. The sampling (i.e., selecting
which conﬁgurations to measure) and learning techniques
used [15, 17, 35, 36, 49, 61, 63–65] result in tradeoffs among
the cost to build the models and the accuracy and interpretabil-
ity of the models [15, 35, 38]. For example, larger samples
are more expensive, but usually lead to more accurate models;
random forests, with large enough samples, tend to learn more
accurate models than those built with linear regression, but the
models are harder to interpret when users want to understand
performance or debug their systems [15, 35, 49] (see Fig. 1).
In this paper, we introduce Comprex, a white-box approach
to build accurate, interpretable, and local performance-
10722021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)
1558-1225/21/$31.00 ©2021 IEEE
DOI 10.1109/ICSE43902.2021.00100
inﬂuence models at low cost. Our approach departs
drastically from traditional black-box approaches: It analyzes
and instruments the source code to accurately capture
conﬁguration-speciﬁc performance behavior, without relying
on machine learning to extrapolate incomplete samples. We
reduce measurement cost by simultaneously analyzing and
measuring multiple code regions of the system, building a
local linear performance-inﬂuence model per region with
a few conﬁgurations (an insight we call compression).
Subsequently, we compose the local models into a global
model for the entire system. We use an iterative dynamic taint
analysis to identify where and how load-time conﬁguration
options inﬂuence control-ﬂow statements in the system,
through control-ﬂow and data-ﬂow dependencies.
Our empirical evaluation on 4medium- to large-scale,
widely-used, open-source projects demonstrates that Comprex
builds similarly accurate performance-inﬂuence models to
those learned with the most accurate and expensive black-box
approach, but at a reduced cost. Furthermore, Comprex
generates interpretable and local models, which quantify
the inﬂuence of individual options and interactions on
performance, and can even map the inﬂuence to code regions.
In summary, we make the following contributions:
•An approach that combines compression andcomposition
to accurately infer the inﬂuence of options on the
performance of numerous independent regions of a
system with a few conﬁgurations, composing the
inﬂuence in a full performance model.
•An iterative dynamic taint analysis to identify how
conﬁgurations inﬂuence the performance of independent
regions and a conjecture to reduce the cost of running the
analysis to build accurate performance-inﬂuence models.
•An empirical evaluation on 4medium- to large-scale,
open-source systems demonstrating that our white-box
approach builds interpretable, but similarly accurate
models with less cost compared to the state of the art.
•A replication package with subject systems, experimental
setup, and data of several months of measurements [74].
II. P ERFORMANCE MODELS FOR CONFIG . SYSTEMS
There is substantial literature on modeling the performance
of software systems [e.g., 15, 35, 38, 75]. Performance-
inﬂuence models solve a speciﬁc problem: Explaining
how options and their interactions inﬂuence a system’s
performance for a given workload and environment, designed
to help users understand performance and make deliberate
conﬁguration decisions.
Traditional performance models are typically used
by designers and developers to model and analyze the
performance of a system’s architecture (e.g., using Queuing
networks, Petri Nets, and Stochastic Process Algebras) and
workload in the design stage of a project [24, 39, 62]. At this
stage, design decisions are usually modeled as conﬁguration
options [7, 13]. By contrast, performance-inﬂuence models
describe the performance behavior of a system implementation1defmain(List workload)
2a = getOpt("A"); b = getOpt("B");
3c = getOpt("C"); d = getOpt("D");
4... // execution: 1s
5inti = 0; Tmain= 3 - 1A
6if(a) // variable depends on option A
7 ... // execution: 1s
8 foo(b); // variable depends on option B
9 i = 20;
10else
11 ... // execution: 2s
12 i = 5;
13while(i > 0)
14 bar(c); // variable depends on option C
15 i--;
16deffoo(boolean x) Tfoo= 1A + 3AB
17if(x) ... // execution: 4s
18else...// execution: 1s
19defbar(boolean x) Tbar= 5 + 15A + 10C + 30AC
20if(x) ... // execution: 3s
21else...// execution: 1s
Fig. 2: Example system with three colored regions (methods)
inﬂuenced by conﬁguration options. Local performance-inﬂuence
models for a speciﬁc workload are shown per region.
in a given workload and environment, and are typically learned
by observing the behavior of the system implementation.
In the context of conﬁgurable systems, performance-
inﬂuence models predict a system’s performance in terms of
conﬁguration options and their interactions [17, 19, 20, 30, 31,
31, 32, 38, 63, 72, 73]. Some models are explicitly designed
toexplain how speciﬁc options and their interactions inﬂuence
a system’s performance [17, 38, 63, 64, 66, 73]. For example,
the sparse linear model 8+15A+10C+3AB+30AC captures the
execution time of the system in Fig. 2, which predicts the per-
formance of arbitrary conﬁgurations, and explains how the op-
tionsA,B, andCand their interactions inﬂuence the system’s
performance. The model can be inspected by developers, for
example, to determine whether the large performance impact
when combining AandCis an unexpected performance bug.
Building performance-inﬂuence models: Performance-
inﬂuence models are typically built by measuring a system’s
execution time with the target workload in the target
environment under different conﬁgurations [63]. Almost
all existing approaches are black-box in nature: They do
not analyze the system’s implementation and measure the
end-to-end execution time of the system.
The simplest approach is to observe the execution of
all conﬁgurations in a brute-force approach. The approach
obviously does not scale, as the number of conﬁgurations
grows exponentially with the number of options.
In practice, most approaches measure executions only
for a sampled subset of all conﬁgurations and extrapolate
performance behavior for the rest of the conﬁguration space
using machine learning [15], which we collectively refer to as
sampling and learning approaches. Speciﬁc approaches differ
in how they sample, learn, and represent models: Common
sampling techniques include uniform random, feature-wise,
and pair-wise sampling [47], design of experiments [50], and
combinatorial sampling [3, 21, 25, 26, 53]. Common learning
techniques include linear regression [36, 63–65], regression
1073trees [15, 17, 18, 61], Fourier Learning [19], and neural net-
works [20]. Different sampling and learning techniques yield
different tradeoffs between measurement effort, prediction ac-
curacy, and interpretability of the learned models [15, 35, 38].
Goals of performance-inﬂuence modeling: Performance-
inﬂuence models can be used for different tasks in different
scenarios, which beneﬁt from different model characteristics.
In the simplest case, a user wants to optimize a system’s per-
formance by selecting the fastest conﬁguration for their work-
load and in the target environment. Performance-inﬂuence
models have been used for optimization [17, 51, 57, 82],
though metaheuristic search (e.g., hill climbing) is often more
effective at pure optimization problems [28, 29, 57, 58], as
they do not need to understand the entire conﬁguration space.
In other scenarios, users want to predict the performance of
individual conﬁgurations. Scenarios include automatic recon-
ﬁguration andruntime adaptation, where there is no human in
the loop and online search is impractical. For example, when
dynamically deciding during a robot’s mission which options
to change to react to low battery levels [31, 32, 76, 82]. In these
scenarios, the model’s prediction accuracy is important across
the entire conﬁguration space, but understanding the structure
of the model is not important. In this context, deep regression
trees [17, 18, 61], Fourier Learning [19], and neural net-
works [20] are commonly used, which build accurate models,
but are not easy to interpret by humans [15, 35, 38, 49, 63].
When performance-inﬂuence models are used by users to
make deliberate conﬁguration decisions [15, 35, 38, 63, 76, 80]
(e.g., whether to accept the performance overhead of
encryption), interpretability regarding how options and
interactions inﬂuence performance becomes paramount. In
these settings, researchers usually suggest sparse linear
models, such as 8 + 15A + 10C+ 3AB+ 30AC above,
typically learned with stepwise linear regression or similar
variations [36, 63–65]. In such models, users can directly
recognize the inﬂuence of an option or an interaction.
In addition to end users who conﬁgure a system, developers
who maintain the system can also beneﬁt from performance-
inﬂuence models to understand and debug the performance
behavior of their systems [38, 40, 73]. For example, when
presenting performance-inﬂuence models to developers in
high-performance computing, Kolesnikov et al. [38] reported
that a developer “was surprised to see that [an option] had
only a small inﬂuence on system performance”, indicating a
potential bug. In such setting, understanding how individual
options and interactions inﬂuence performance is again
paramount, favoring interpretable models. Ideally, models
would also indicate where the inﬂuence occurs in the
implementation. For example, in prior work [73], we shared
experience of how mapping performance inﬂuence of options
to speciﬁc code regions was useful in identifying several
inconsistencies between documentation and implementation.
In this paper, we build performance models that can be
used in all of these scenarios. The models are accurate for
optimization and prediction, they are interpretable explaining
how options and interactions inﬂuence performance, and they!"#$%"&'()*#)+%,-(./,*"0(,-(&1#2("
1(%30)(&4536(+73&'()*#)+%,-(
8#+9#3(4536(+
:(-#+9#3(
;(<=#, >#-%"&'()*#)+%,-(./,*"0(,-(&1#2("1(%30)(&;(<=#,37&'()*#)+%,-(
Fig. 3: Building performance-inﬂuence models is compositional:
Instead of building a single model for the entire system (dotted
black arrow), we can simultaneously build a local model per region
and compose those models (dashed blue arrows).
map performance inﬂuence to code regions explaining where
performance inﬂuence manifests. Our white-box approach,
Comprex, builds these models by analyzing and observing
system internals, without using machine learning.
III. C OMPREX
We introduce Comprex, a white-box approach to efﬁciently
and accurately generate performance-inﬂuence models,
without using traditional sampling and learning techniques.
Similar to black-box approaches, we build performance-
inﬂuence models by observing the execution of a system in
different conﬁgurations, but we guide the exploration with a
white-box analysis of the internals of the system. For a given
input, a system with a set of Boolean options Ocan exhibit
up to2|O|distinct execution paths, one per conﬁguration.1If
we measure the execution time of each distinct path, we can
build a performance-inﬂuence model that accurately maps
performance differences to options and their interactions,
without any approximation through machine learning. The
resulting models can be expressed as familiar linear models.
Our approach to efﬁciently build performance-inﬂuence
models relies on two insights: (1) Performance-inﬂuence
models can be built compositionally, by composing models
built independently for smaller regions of the code (cf.
Fig. 3). (2) Multiple performance-inﬂuence models for
smaller regions can be built simultaneously by observing a
system’s executions often with only a few conﬁgurations,
which we call compression.
First, building performance-inﬂuence models is composi-
tional: We can measure the performance of smaller regions
in the system (e.g., considering each method as a region) and
build a performance-inﬂuence model per region separately,
which describes each region’s performance behavior in terms
of options. Subsequently, we can compose the local models
to describe the performance of the entire system, computed
1For simplicity, we describe Comprex in terms of Boolean options, but
other option types can be encoded and discretized. The distinction between
inputs and options is subjective and domain speciﬁc. We consider options as
special inputs with a small ﬁnite domain (e.g., Boolean) that a user might
explore to change functionality or inﬂuence quality attributes. We consider
ﬁxed values for other inputs. Note that a user might ﬁx some conﬁguration
options and consider alternative values for inputs (e.g., use an option for dif-
ferent workloads). However, we explore the performance inﬂuence of options
with ﬁnite domains, assuming all other inputs are ﬁxed at speciﬁc values.
This setting results in a ﬁnite, but typically very large conﬁguration space.
1074!"#$%&'()*#+,-./%01#2%3'4*%5*#/%#6*7./%0#
82-*9&-.:*#;&.%-#$%&'(0.0#<#=*5#222<$>
?"#@*&049*#A*93/9B&%5*#/3#6*7./%0#
8A9/3.'*9#<#=*5#222<C>6*7./%0
A*93/9B&%5*#,*9#6*7./%#
&%D#E/%3.749&-./%A&9-.-./%0#,*9#
6*7./%
F"#C4.'D#@/D*'#
8=*5#222<E>
m=6+16A+10C+...=(0-*B
E/%3.749&-./%0
Fig. 4: Overview of Comprex’s three components.
as the sum of the individual inﬂuences in each model (e.g.,
composing 5+4A and1−1A+2B to6+3A+2B).
Compositionality helps reduce the cost of our approach,
as many smaller regions of a system are often inﬂuenced
only by a subset of all options, conﬁrmed by prior empirical
research [48, 52]. Hence, the number of distinct paths to
observe in a region is usually much smaller than the number of
distinct paths in the entire system. If we have an analysis that
can identify the subset of options that directly and indirectly
inﬂuence the smaller region (see Sec. III-A), we can build a
local performance-inﬂuence model by observing all distinct
paths in a region often with only a few conﬁgurations.
Second, compression makes our approach scale without
relying on machine learning approximations: When executing
a single conﬁguration, we can simultaneously observe the
execution of multiple regions. In addition, if the regions are
inﬂuenced by different options, a common case conﬁrmed
by prior research [48, 52], then we can separately measure,
in one conﬁguration, the inﬂuence of different options in
different regions in the system, instead of exploring all
combinations of all options. For example, three independent
regionsif(a){} if(b){} if(c){} inﬂuenced by
optionsA,B, andC, respectively, each have two distinct
paths. Instead of exploring all 8combinations of the three
options, we can explore all distinct paths in each region with
only2conﬁgurations, as long as each option is enabled in
one conﬁguration and disabled in the other conﬁguration.
Our approach combines compositionality and compression
to build accurate performance-inﬂuence models, without tradi-
tional sampling or machine-learning techniques. The resulting
models can be presented in an interpretable format and even be
mapped to individual code regions. Key to our approach is the
property that not all options interact in the same region, instead
inﬂuencing different parts of the system independently; a
pattern observed empirically in conﬁgurable systems [48, 52].
To operationalize compression and compositionality for
building accurate and interpretable performance-inﬂuence
models with low cost, we need three technical components,
as illustrated in Fig. 4: First, we identify which regions
are inﬂuenced by which options to select conﬁgurations to
explore all paths per region and to map measured execution
times to options and their interactions (Sec. III-A). Second,
we execute the system to measure the performance of all
regions (Sec. III-B). Third, we build local performance-inﬂuence models per region and compose them into one
global model for the entire system (Sec. III-C).
A. Analyzing Options’ Inﬂuence on Regions
As a ﬁrst step, we identify which options (directly or indi-
rectly) inﬂuence control-ﬂow decisions in which code regions.
We use this information to select conﬁgurations to explore all
paths per region and map measured performance differences
to options and their interactions (Sec. III-B).2To this end, we
track information ﬂow from conﬁguration options (sources) to
control-ﬂow decisions (sinks) in each region. If a conﬁguration
option ﬂows, directly or indirectly (including implicit ﬂows),
into a control-ﬂow decision in a region, this implies that select-
ing or deselecting the option may lead to different execution
paths within the region. Thus, we should observe at least one
execution with a conﬁguration in which the option is selected
and another execution in which the option is not selected.
More speciﬁcally, we conservatively partition the
conﬁguration space per region into subspaces, such that
every conﬁguration in each subspace takes the same path
through the control-ﬂow decisions within a region, and that
all distinct paths are explored when taking one conﬁguration
from each subspace. Formally, a set of Boolean options
Oforms a conﬁguration space C=P(O)where each
conﬁguration c∈Cis represented by the set of selected
options. A partition of the conﬁguration space (p ⊆ P(C))
is a grouping of conﬁgurations into nonempty subsets, which
we call subspaces, such that each conﬁguration is part of
exactly one subspace. For notational convenience, we describe
subspaces using propositional formulas over options. For
example, /llbracketA∧¬B/rrbracket describes the subspace of all conﬁgurations
in which option Ais selected and option Bis deselected.
To track information ﬂow between options and control-ﬂow
decisions in regions, we employ a dynamic taint analysis and
iteratively collect information through repeated executions of
the system in different conﬁgurations, until we have explored
all distinct paths in each region. A taint analysis, typically used
in the security domain [5, 8], tracks how values are affected
by selected inputs (sources) and are used in speciﬁc locations
(sinks). During each execution, we track how API calls load
conﬁguration options (Lines 2–3 in Fig. 2) (sources) and
propagate them along data-ﬂow and control-ﬂow dependen-
cies, including implicit ﬂows, to the decisions of control-ﬂow
statements (sinks). We use a dynamic rather than a static
analysis [45, 73], since it scales to larger systems and avoids
some overtainting problems by tracking actual executions [8].
Incrementally partitioning the conﬁguration space:
Alg. 1 describes how we partition the conﬁguration space per
region, based on incremental updates from our dynamic taint
analysis. Intuitively, we execute the system in a conﬁguration
2We focus on different execution paths caused by conﬁguration changes,
ﬁxing all other inputs. We focus on conﬁguration changes in control-ﬂow
statements, as a system’s execution time changes in those statements,
depending on which branch is executed and how many times it is executed,
conﬁrmed by empirical research [23, 33, 54, 66, 73]. Execution differences
caused by nondeterminism are orthogonal and must be handled in conventional
ways (e.g., averaging multiple observations or controlling the environment).
1075Algorithm 1: Iterative Dynamic Taint Analysis
Input: conﬁgurable system p
Output: partition for each region R→ P(P(C))
1Functionpartion_all_regions(p )
2 partitions :=R→ {C};executed_conﬁgs :=∅
3 untilexplored_all_subspaces(executed_conﬁgs, partitions) do
4 cc:=get_next_config(executed_conﬁgs, partitions)
5 executed_conﬁgs :=executed_conﬁgs ∪cc
6 duringexecute_taint_analysis(p,cc) when reaching a
control-ﬂow decision with taints tdandtcin region r:
7 partitions[r ]:=partitions[r ]×get_part(t d,tc,cc)
8 end
9 end
10 return partitions
11end
Input: executed conﬁgurations ec:P(C),partitions :R→ P(P(C))
Output:true orfalse
12Functionexplored_all_subspaces(ec, partitions)
13 all_subspaces :=/uniontextimage(partitions)
14 return∀s∈all_subspaces. ∃c∈ec. c∈s
15end
Input: data-ﬂow taints td, control-ﬂow taints tc, current conﬁguration cc
Output: partitionp:P(P(C))for the current decision
16Functionget_part(t d,tc,cc)
17sreach :={c∈C|∀o∈tc. o∈c⇔o∈cc}
18p:=/braceleftbig
C\sreach/bracerightbig
// subspace of conﬁg. that might not reach decision
// add one subspace for every combination of options in data-ﬂow taints
19 fora∈ P(td)do
20 s:=/braceleftbig
c∈C|∀o∈td. o∈c⇔o∈a/bracerightbig
21 p:=p∪ {s∩sreach}
22 end
23 returnp\ ∅
24end
and observe when data-ﬂow and control-ﬂow taints from op-
tions reach each control-ﬂow decision in each region, and sub-
sequently update each region’s partition: Whenever we reach a
control-ﬂow statement during execution, we identify, based on
taints that reach the condition of the statement, the sets of con-
ﬁgurations that would possibly make different decisions, thus
updating the partition that represents different paths for this
region (Line 7). Since a dynamic taint analysis can only track
information ﬂow in the current execution, but not for alterna-
tive executions (i.e., for paths not taken), we repeat the process
with new conﬁgurations, selected from the partitions identiﬁed
in prior executions, updating partitions until we have explored
one conﬁguration from each subspace of each partition (main
loop, Line 3); that is, until we have observed each distinct path
in each region at least once. Note that some subspaces in the
region might make the same control-ﬂow decision as other
subspaces, but we do not know which subspace will make
which decision until we actually execute those conﬁgurations.
Updating partitions works as follows: When we reach a
control-ﬂow statement in a region with data-ﬂow taints td, this
indicates that the options in tdaffect the control-ﬂow decision,
but other options do not. Thus, we know that all conﬁgurations
that share the same selection for all options in tdwill result
in the same control-ﬂow decision, while conﬁgurations with
different selections of these options may result in different
decisions. Since the taint analysis tells us only that the
options in tdmay somehow (directly or indirectly) affect the
decision’s condition, but not how, we will need to explore at
least one conﬁguration for every possible assignment to these
options, even though multiple or even all may end up takingthe same branch. Therefore, we partition the conﬁguration
space at this decision corresponding to all combinations of
the options in td(Lines 19–20). For example, for a decision
inﬂuenced by options AandB, we partition the conﬁguration
space into four subspaces: all conﬁgurations in which AandB
are selected together, all conﬁgurations in which Ais selected
but notB, all conﬁgurations in which Bis selected but not A,
and all conﬁgurations in which neither AnorBare selected.
Finally, we update the region’s partition with the partition
derived for the decision by computing their cross product (×,
Line 7). This operation reﬂects that, to explore all paths among
multiple control-ﬂow decisions in a region, including multiple
executions of the same control-ﬂow statement, we need to
explore all combinations of the individual paths in the region.
Distinguishing data-ﬂow taints from control-ﬂow taints
allows us to optimize the exploration of nested decisions (e.g.,
if(a){ if(b) ... }). Control-ﬂow taints specify which
options (directly or indirectly) inﬂuenced outer control-ﬂow
decisions, which indicates that different assignments to options
in the control-ﬂow taints may lead to paths where the current
decision is not reached in the ﬁrst place. Hence, we do not
necessarily need to explore all interactions of options affecting
outer and inner decisions. Instead of exploring combinations
for all options of data-ﬂow and control-ﬂow taints, we ﬁrst
split the conﬁguration space into ( 1) those conﬁgurations for
which we know that they will reach the current decision, as
they share the assignments of options in control-ﬂow taints
(sreach, Line 17), and (2) the remaining conﬁgurations which
may not reach the current decision (C \sreach, Line 18).
Then, we only create subspaces for interactions of options in
data-ﬂow taints within sreach (Lines 19–21) and consider the
entire set of conﬁgurations outside sreach as a single subspace
(Line 18). The iterative nature of our analysis ensures that at
least one of the conﬁgurations outside sreach will be explored,
and, if the conﬁguration also reaches the same decision, the
region’s partition will be further divided.
The iterative analysis executes the system in different con-
ﬁgurations until one conﬁguration from each subspace of each
partition in each region has been explored. That is, we start
by executing any conﬁguration (e.g., the default conﬁguration),
which reveals the subspaces per regions that could make differ-
ent decisions. The algorithm then selects the next conﬁguration
to explore unseen subspaces in the regions (Line 4), which may
further update the regions’ partitions. To select the next con-
ﬁguration, we use a greedy algorithm to pick a conﬁguration
that explores the most unseen subspaces across all regions.3
Example: We exemplify running the iterative analysis in
our running example from Fig. 2, considering each method
as a region. If we execute the conﬁguration {A,D}, in which
optionsAandDare selected and the other options are
3To avoid enumerating an exponential number of conﬁgurations, we use a
greedy algorithm that picks a random subspace and incrementally intersects
it with other non-disjoint subspaces, which seems sufﬁciently effective
in practice. The problem can also be encoded as a MAXSAT problem,
representing subspaces as propositional formulas, to ﬁnd the conﬁguration
that satisﬁes the formula with the most subspaces.
1076deselected, the taint analysis will indicate that the value of
the variable ais tainted by option A(from Line 2) when
reaching the ifstatement in Line 6 (region main). Thus, all
conﬁgurations in which Ais selected result in the same control-
ﬂow decision and all conﬁgurations in which Ais not selected
result potentially in the same or a different decision. Hence,
we derive the initial partition {/llbracketA/rrbracket,/llbracket¬A/rrbracket} for this method.
Continuing the execution, we next reach the ifstatement
in Line 16 (region foo), where the value of the variable
xis tainted with control-ﬂow taint A(from Line 6) and
data-ﬂow taint B(from variable b). Thus, all conﬁgurations
in whichAandBare selected result in the same control-ﬂow
decision, all conﬁgurations in which Ais selected and
Bis not selected may result in a different control-ﬂow
decision, and all conﬁgurations in which Ais not selected
may not reach this decision. Hence, we derive the partition
{/llbracketA∧ ¬B/rrbracket, /llbracket¬A/rrbracket, /llbracketA∧B/rrbracket} for this region. Note how we
explore this nested ifstatement with 3instead of 4subspaces
by separately tracking data-ﬂow and control-ﬂow taints.
Further in the execution, the decision in the while
statement (Line 13) depends on the tainted value of the
variablei(implicit ﬂow), in each loop iteration, resulting
in{/llbracketA/rrbracket, /llbracket¬A/rrbracket}, which is consistent with main’s existing
partition. Hence, the cross product does not change the
partition. Similarly, the decision in Line 19 (region bar)
repeatedly depends on data-ﬂow taint Cand control-ﬂow taint
A, resulting in the partition {/llbracketA∧¬C/rrbracket,/llbracket¬A/rrbracket, /llbracketA∧C/rrbracket}.
After this ﬁrst execution, we identiﬁed six distinct
subspaces among the partitions of the three regions
(/llbracketA/rrbracket,/llbracket¬A/rrbracket, /llbracketA∧B/rrbracket,/llbracketA∧ ¬B/rrbracket,/llbracketA∧C/rrbracket, and /llbracketA∧ ¬C/rrbracket), of
which /llbracketA/rrbracket,/llbracketA∧ ¬B/rrbracket, and /llbracketA∧ ¬C/rrbracket were explored with
the initial conﬁguration. In the next iteration, we select
a conﬁguration, for example {A,B,C}, to explore unseen
subspaces in the regions and update partitions. In this case,
however, nothing changes. We continue executing new
conﬁgurations to explore unseen subspaces, possibly updating
the regions’ partitions, until we have explored all subspaces in
the regions. After executing only 4out of16conﬁgurations,
for example {A,D},{A,B,C},{}, and{C}, we have explored
at least one conﬁguration from each subspace of each partition
in each region, and the iterative analysis terminates. The
subspaces derived for the three regions’s partitions are /llbracketA/rrbracket,
/llbracket¬A/rrbracket, /llbracketA∧¬B/rrbracket,/llbracketA∧B/rrbracket,/llbracket¬A∧¬C/rrbracket,/llbracket¬A∧C/rrbracket,/llbracketA∧¬C/rrbracket,/llbracketA∧C/rrbracket.
Discussion: Note how the iterative analysis explores
regions independently and does not explore paths for options
that do not inﬂuence that region (e.g., we do not explore
the interaction of BandCand never explore conﬁgurations
speciﬁcally for D). Also, note how the taint analysis tracks
both direct and indirect dependencies interprocedurally.
The iterative analysis is guaranteed to terminate, as it
explores new conﬁgurations during each iteration. In the worst
case, all conﬁgurations in the system (ﬁnite set) will be exe-
cuted, but in practice often much fewer executions are needed.
Our algorithm will produce the same partitions independent
of the order in which conﬁgurations are executed. All sub-
spaces that are derived during any execution of the taint anal-ysis will be derived at some point, because (1) we eventually
explore all paths in each region and (2) we update the partition
of each region with the commutative cross-product operation.
Theoretically, we can measure performance while running
the taint analysis. In practice though, running a dynamic
taint analysis with control-ﬂow tracking imposes signiﬁcant
overhead, which signiﬁcantly alters measurement results. A
second reason for separating performance measurement from
taint tracking is an optimization to perform taint tracking
on a different (smaller) workload than the one used for
performance measurement, which we discuss in Sec. IV.
B. Measuring Performance of Regions
We measure the execution time of each region when
executing a conﬁguration, resulting in performance
measurements for each pair of conﬁguration and region. To
this end, we use an off-the-shelf proﬁler. We measure self-time
per region to track the time spent in the region itself, which
excludes the time of calls to execute code from other regions.
Since we measure performance in a separate step than the it-
erative analysis, we can pick a new and possibly smaller set of
conﬁgurations to explore all paths per region. Ideally, we want
to ﬁnd a minimal set of conﬁgurations, such that we have at
least one conﬁguration per subspace of each region’s partition.
Since ﬁnding the optimal solution is NP-hard4, and existing
heuristics from combinatorial interaction testing [3, 25, 26, 41]
are expensive, we developed our own simple greedy algo-
rithm: Incrementally intersecting subspaces that overlap in
at least one conﬁguration, until no further such intersections
are possible. Then, we simply pick one conﬁguration from
each subspace. However, we never found a smaller set of
conﬁgurations than the one used in the iterative analysis. Thus,
we use the same set of conﬁgurations to measure performance.
Example: Four conﬁgurations cover all subspaces for
the three regions of our example, e.g., {},{A},{C},{A,B,C}.
C. Building Performance-Inﬂuence Models
In the ﬁnal step, we build performance-inﬂuence models
for each region based on (1) the partitions identiﬁed per
region and ( 2) the performance measured per region and
observed conﬁguration. Then, we compose the local models
into a performance-inﬂuence model for the entire program.
Since we collect at least one measurement per distinct
path through a region, building models is straightforward.
For a region with a partition and a set of conﬁgurations with
corresponding performance measurements, we associate each
measurement with the subspace of the partition to which the
conﬁguration belongs. If multiple measured conﬁgurations
belong to the same subspace of the region’s partition, we
expect the same performance behavior for that region (modulo
measurement noise) and average the measured results. As a
result, we can map each subspace of a region’s partition to
4The problem can be reduced to the set cover problem, in which the union
of a collection of subsets (all subspaces) equals a set of elements called "the
universe" (the union of all subspaces). The goal is to identify the smallest
sub-collection whose union equals the universe.
1077a performance measurement. For instance, for region main
in our example, we report an execution time of 2seconds for
conﬁgurations in /llbracketA/rrbracketand3seconds for conﬁgurations in /llbracket¬A/rrbracket.
For interpretability, to highlight the inﬂuence of options and
avoid sets or propositional formulas, we write linear models in
terms of options and interactions, for example mmain= 3−1A.
The global performance-inﬂuence model is obtained simply
by aggregating all local models; we add the individual
inﬂuences of each model. Note that local models can be
useful for understanding and debugging individual regions,
as they describe the performance behavior of each region.
Example: In our running example, we derive the
local models mmain= 3−1A,mfoo= 1A+ 3AB, and
mbar= 5+15A +10C+30AC, which can be composed into
the global performance-inﬂuence model m=mmain+mfoo+
mbar= 8+15A +10C+3AB+30AC.
IV. D ESIGN , OPTIMIZATIONS ,AND IMPLEMENTATION
Dynamic Taint Analysis Overhead: Comprex executes
a dynamic taint analysis with control-ﬂow tracking, which
imposes signiﬁcant overhead in the system’s execution. For in-
stance, one execution of our subject system Berkeley DB takes
about1hour with the taint analysis, whereas about 300conﬁg-
urations can be executed in the same time! In general, we ob-
serve26× to300× overhead from taint tracking, which varies
widely between systems. To reduce cost, we separate the iter-
ative analysis and performance measurement in two steps and
perform the former with a drastically reduced workload size.
This optimization is feasible when the workload is
repetitive and repetitions of operations are affected similarly
by options, which we conjecture to be common in practice.
Many performance benchmarks execute many operations,
which are similarly affected by conﬁguration options. For
instance, Berkeley DB’s benchmark populates a database,
where options determine, for example, whether duplicates are
allowed and the durability characteristics of a transaction.
The benchmark can be scaled by a parameter that controls
the number of entries to insert, but does not affect which
operations are performed. In our evaluation, we show that
Comprex generates accurate models using a signiﬁcantly
smaller workload in the iterative analysis.
We conduct the actual performance measurements with the
original workload to record realistic performance.
Granularity of Regions: Our approach can be applied to
different levels of granularity, but with different tradeoffs. On
one extreme, we could consider the entire system as a single
region, but would not beneﬁt from compression. At the other
extreme, we could consider each control-ﬂow statement as
the start of its own region, ending with its immediate post-
dominator, which allows maximum compression, but results
in excessive measurement cost. Note the latter is analogous
to using an instrumentation proﬁler, but instead of focusing
on few locations of interest, as usually recommended [42],
one would add instrumentation throughout the entire system
at control-ﬂow statements. In our evaluation, we show that
considering each method as a region is a practical compromise.When considering methods as regions, we may lose some
compression potential compared to more ﬁne-grained regions,
if multiple control-ﬂow statements within a method are
inﬂuenced by distinct options. On the other hand, we can use
off-the-shelf sampling proﬁlers that accurately capture perfor-
mance with low overhead, and simply map the performance
of methods to the closest regions on the calling stack. Our
empirical evaluation shows that method level regions do not
require more conﬁgurations compared to more ﬁne-grained
regions, but signiﬁcantly reduce the measurement overhead.
Implementation: We implemented Comprex for Java
systems [74]. We used Phosphor, a state of the art tool for
dynamic taint analysis [8] to track options. We annotated the
APIs to load options as sources and control-ﬂow statements
as sinks. We encoded subspaces as propositional formulas
and use SAT4J [1] for operations on those formulas. We used
JProﬁler10.1[2] to measure performance at method level.
V. E VALUATION
To evaluate the efﬁciency and effectiveness of our approach,
we compare Comprex to state of the art performance-
inﬂuence modeling approaches for conﬁgurable systems, in
terms of the cost to generate the models and their accuracy,
and discuss their interpretability. We evaluate the efﬁciency
of compression by choosing regions at different granularities.
Speciﬁcally, we address the following research questions:
RQ1: How does Comprex compare to state of the
art performance modeling approaches in terms of cost,
accuracy, and interpretability?
RQ2: How efﬁcient is compression at different
granularities to reduce the cost to build models?
A. Experiment Setup
Subject Systems: We selected 4conﬁgurable, widely-
used, open-source Java systems that satisfy the following
criteria (common in our domain): (a) medium- to large-scale
systems with over 40K SLOC and over 20options, (b) systems
with binary and non-binary options, (c) systems with fairly
stable execution time (despite nondeterminism and concur-
rency, we observed execution times within usual measurement
noise for repeated execution of the same conﬁguration), and
(d) systems with different performance behaviors (see Fig. 5).
Table I provides an overview of all subject systems.
We focus on a large subset of all options that are potentially
relevant for performance. We considered options for which
the systems’ documentation indicated that they would affect
performance, but excluded options that might not inﬂuence
performance, (e.g., --help). This selection is representative
of common use cases where users are interested in the
performance behavior of many, but not all options. Following
the evaluation of state of the art approaches [18, 21, 36,
45, 47, 61, 63–66], we selected, for non-binary options, two
different values and encoded the values as a binary option.
We executed a long-running benchmark shipped with the
system, representative of a user analyzing the system’s per-
1078Configurations24262830323436Performance (s)Apache Lucene
Three discernible steps
Configurations20406080Performance (s)H2
Discernible stepsSubstantial jump
Configurations510152025Performance (s)Berkeley DB
Complex behavior 
with several steps
Configurations50100150200Performance (s)Density Converter
One discernible
stepComplex behavior
Fig. 5: Performance behaviors of our subject systems, from simple to complex. We randomly selected 2000 conﬁgurations and sorted their
execution time from fastest to slowest.
TABLE I: Subject systems.
System Domain #KLOC #Opt. #Conf. V/ID
Apache Lucene Index/Search 396 17 131K 7.4.0
H2 Database 142 16 65K 1.4.201
Berkeley DB Database 164 16 65K 7.5.11
Density Converter Image processor 49122 4.9M 110c4
Opt: Options; Conf: Conﬁgurations; V/ID: Version/Commit ID;1: Interface to several
libraries for processing images with 1.5K SLOC, included in the evaluation, as the system
has a large conﬁguration space.
formance under different conﬁgurations. Detailed information
about the benchmarks is included in our appendix [74].
For each system, we changed the workload parameter to run
the iterative dynamic taint analysis with a smaller workload
by factors ranging from 20to50000, depending on the
system. The smaller workload reduced the iterative analysis
time for a single conﬁguration, for example, on average, from
1hour to5seconds for Berkeley DB. Information about the
changes in the workload can be found in our appendix [74].
Hardware: Analyses and measurements were executed
on an Ubuntu 18.04LTS desktop, with a 3.4GHz8-core
Intel Core i7 processor, 16GB of RAM, and Java HotSpotTM
64-bit Server VM (v1.8. 0_202).
Performance Measurement: We established a large
evaluation set for quantifying accuracy by measuring the
performance of 2000 randomly selected conﬁgurations, as the
conﬁguration spaces of the subject systems are intractably
large. We executed each conﬁguration ﬁve times and used
the median to reduce the effects of measurement noise.
We initiated one VM invocation per conﬁguration, thus all
measures include startup time [14]. For Comprex, we proﬁled
each system with JProﬁler’s default sampling rate of 5ms.
B. RQ1: Comparison to the State of the Art
With RQ1, we evaluate the cost to generate performance-
inﬂuence models with Comprex, at method granularity,
asses their accuracy and interpretability, and compare them
to state of the art approaches. Speciﬁcally, we compare
Comprex to numerous combinations of sampling and
learning approaches. For learners, we evaluate variations
of linear regressions [63–65], decision trees and random
forests [15, 17, 18, 61], and a neural network. For sampling,
we evaluate uniform random sampling with 50and200
conﬁgurations, feature-wise sampling (i.e., enable one option
at a time), and pair-wise sampling (i.e., cover all combinationsTABLE II: Cost comparison.
Sample Apache Lucene H2 Berkeley DB Density Conv.
BF 217[~48.4d] 216[~16.0d] 216[~8.6d] 222[~3.6y]
R50 50 [26.7m] 50 [16.4m] 50 [9.1m] 50 [10.1m]
R200 200 [1.8h] 200 [1.1h] 200 [36.4m] 200 [40.4m]
FW 17 [8.6m] 16 [2.4m] 16 [4.8m] 22 [8.2m]
PW 154 [1.3h] 137 [21.1m] 137 [39.4m] 254 [1.8h]
Comprex 26 [14.9m] 64 [22.6m] 144 [30.2m] 88 [16.6m]
The time to measure conﬁgurations for brute-force (BF) is extrapolated from 2000
randomly selected conﬁgurations.
(a) Cost of sampling conﬁgurations.
Approach Apache Lucene H2 Berkeley DB Density Conv.
R50 & LR 8.9s 6.6s 5.7s 14.9s
R200 & LR 6.8m 4.6m 4.9m 1.6m
FW & LR 9.4s 4.3s 7.7s 19.8s
PW & LR 1.7m 44.8s 3.6m 5.5m
* & RF ≤0.2s≤0.2s ≤0.3s ≤0.2s
Comprex 28.9m 9.3m 11.2m 8.5m
(b) Learning/Analysis time.
BF: Brute Force; R50: 50 random conﬁgurations; R200: 200 random
conﬁgurations; FW: Feature-wise; PW: Pair-wise; LR: Stepwise linear regression;
RF: Random forest; *: Results stable across all samples.
of all pairs of options) [47]. We selected 50and200random
conﬁgurations to use more conﬁgurations than other sampling
strategies and use sampling sets comparable to ones used in
related research. Due to space restrictions, we report results
only for stepwise linear regression and random forest. The
other results can be found in our appendix [74].
Note that we do not compare against approaches for
selecting the fastest conﬁguration [36, 57, 82], as those
approaches solve a pure optimization problem where
modeling the entire conﬁguration space is not necessary.
We do not evaluate existing white-box approaches due
to their limitations [66, 73]. None of the approaches could
analyze any of our subject systems, except for a subset of
Density Converter’s code base, which we discuss in Sec. VI.
Cost Metric: We report the number of conﬁgurations exe-
cuted to generate a model and time to measure conﬁgurations.
For the learning approaches, we report the learning time. For
Comprex, we report the time to execute the iterative analysis.
Accuracy Metric: We report the Mean Absolute
Percentage Error (MAPE), which measures the mean
difference between the values predicted by a model and the
values actually observed (i.e., the baseline), lower is better.
As proﬁling the performance of regions with our approach
1079TABLE III: MAPE comparison (lower is better).
Approach Apache Lucene H2 Berkeley DB Density Conv.
R50 & LR 4.5 124.1 19.7 1037.2
R200 & LR 2.9 93.9 14.9 434.5
FW & LR 7.9 129.3 768.7 1596.0
PW & LR 4.7 113.3 34.2 1596.0
R50 & RF 0.4 1.1 16.4 59.9
R200 & RF 0.3 0.7 1.1 5.5
FW & RF 8.7 119.0 106.1 1185.9
PW & RF 4.0 124.6 46.9 27.3
Comprex (O) 8.0 9.3 6.7 11.2
Comprex (C) 3.2± 0.2 2.9± 0.5 5.0± 0.6 9.4± 2.41
LR: Stepwise linear regression; RF: Random forest; R50: 50random conﬁgurations;
R200:200 random conﬁgurations; FW: Feature-wise; PW: Pair-wise; O: Original
model; C: Model corrected for proﬁler overhead, reporting mean and standard
deviations over 5 corrections. Bolded values in cells indicate similarly low errors.
1Non-linear proﬁling overhead might have increased the error [77].
adds some overhead (about 8%in our subject systems), we ex-
pect our models to be systematically biased. We performed an
optional linear correction to account for the average overhead
in our subject systems, learned from measurements with and
without proﬁling of 5randomly selected conﬁgurations. We
separately report these "corrected" MAPE values in our results.
Interpretability: We intend our models to also be
used in understanding and debugging tasks (see Sec. II).
Unfortunately, measuring interpretability of models is
nontrivial and controversial. In the machine learning
community, interpretability is an open research problem with
an active community, but without a generally agreed measure
or even deﬁnition for interpretability [12, 49].
Generally, interpretability captures the ability of humans
to make predictions, understand predictions, or understand
the decisions of a model [49]. When the model is complex
(e.g., the model includes numerous decisions), humans have
more difﬁculty understanding the model directly. Some sim-
pler forms of models are usually considered inherently inter-
pretable, because humans can inspect and understand the mod-
els directly. For example, scientists have decades of experience
using and interpreting linear models (e.g., much of empirical
software engineering research relies on interpreting linear
model coefﬁcients). Models with more complex structures and
very large numbers of decisions (e.g., deep neural networks
with millions of weights or random forests with hundreds
of trees), exceed human capacity for directly understanding
the model. With these more complex models, the trend is to
develop post-hoc explanations, where tools provide explana-
tions for speciﬁc aspects of the model (e.g., the reason for
a given prediction) without having to understand the model’s
internals [46, 49, 59, 68]. The use of post-hoc explanations is,
however, controversial, as the explanations usually are only ap-
proximations that may be unreliable or even misleading [60].
We do not attempt to quantify interpretability. Instead, we
generally consider sparse linear models as inherently inter-
pretable (assuming a moderate numbers of terms). This is
supported by prior interviews that have shown that developers
understand linear performance-inﬂuence models with a few
dozen terms [38]. By contrast, we consider random forests
and neural networks as not inherently interpretable, and do notevaluate the reliability or usefulness of post-hoc explanations.
To assure readers that the linear models that we produce are
indeed sparse and, hence, likely interpretable by humans, we
report the number of terms they contain. As our algorithm to
build these models (Sec. III-C) does not include any machine
learning and regularization, the algorithm detects and reports
even minuscule amounts of measurement noise. Hence, we
exclude all trivial terms that do not make meaningful contri-
butions to the systems’ performance. We report the number
of terms (options or interactions) that contribute, at least, 0.3
seconds, which is approximately 1%of the execution time of
the default conﬁgurations of our subject systems. Our appendix
contains additional data at other thresholds [74].
Results: We report the main results in Table II (cost)
and III (accuracy), and our appendix [74] includes results for
additional learners (neural networks, decision trees). Overall,
Comprex builds models that are similarly accurate to those
learned by the most accurate and expensive black-box ap-
proach (random forests with 200 samples), but our models are
interpretable and usually built more efﬁciently, despite the cost
of the additional taint analysis step. Comprex outperforms
other approaches that build linear models by a wide margin.
While random forest with 200 samples produced slightly
more accurate models than Comprex, our approach was
usually more efﬁcient, in some cases building models in half
the time, while also generating local and interpretable
models (see Fig. 1). The efﬁciency originates from
Comprex’s white-box analysis to identify a small number of
relevant conﬁgurations to capture the performance-relevant
interactions. By contrast, as our results show, black-box
approaches perform signiﬁcantly worse on such small
samples (e.g., compare R50 and R200 results).
The linear models produced by Comprex are moderate
in size, with 19,16,32, and72performance-relevant terms
(options or interactions) for Lucene, H2, Berkeley DB, and
Density Converter respectively. Our models are similar in size
to models learned with linear regression from samples (e.g.,
26,13,28, and30terms using R200 & LR), but much more
accurate. At this size, we argue that manual inspection of the
models is still plausible. More importantly, the performance
inﬂuences can be mapped to 24,5,15, and9speciﬁc regions
in the code for the four systems respectively.
RQ1: In summary, models produced with Comprex have
comparable accuracy to the most accurate and expensive
black-box approaches, but can often be built more efﬁ-
ciently. Additionally, the models signiﬁcantly outperform
other black-box approaches that produce (interpretable)
linear models. The models are interpretable and can be
mapped to speciﬁc code regions.C. RQ2: Compression Efﬁciency
With RQ2, we explore the impact of choosing regions at
different granularities on the efﬁciency of Comprex, both
1080in terms of the number of conﬁgurations to measure and the
overhead to perform these measurements.
Procedure: For RQ1, we executed the taint analysis
considering each method as a region. We additionally tracked
partitions for control-ﬂow statements and derived partitions for
the entire program by combining the partitions of all methods.
Result – Number of Conﬁgurations: In Table IV, we
report the size of the minimum set of conﬁgurations needed
to cover each subspace of each region’s partition for each
granularity. When considering the entire program as a region,
signiﬁcantly more conﬁgurations need to be explored, as
we do not beneﬁt from compression. Interestingly though,
while there are, as expected, fewer regions at the method
level than at the control-ﬂow statement level, the number of
conﬁgurations needed is the same. These results show that
compression at ﬁner-grained levels than the method level do
not yield additional beneﬁts in our subject systems.
We found that the control-ﬂow statement regions combined
within a method are usually partitioned in the same way.
Only in3out of the 2263 method-level regions, the method’s
partition had more subspaces than the corresponding control-
ﬂow statement regions (e.g., two ifstatements depending on
different options). However, in all three cases, the additional
subspaces were already explored in other parts of the program.
Hence, no additional conﬁgurations needed to be explored.
We conclude that ﬁned-grained compression is highly
effective, but that control-ﬂow granularity does not seem to
offer signiﬁcant compression beneﬁts over method granularity.
Results – Measurement Overhead: Measuring performance
at different granularities requires different strategies, each
with vastly different amounts of measurement overhead.
Measuring at the program level is cheap, as a single
end-to-end measurement is sufﬁcient to measure the entire
program (e.g., Unix time). At ﬁner granularities, multiple
measurements of different parts of the program are required.
Instrumenting the program at control-ﬂow statements
and corresponding post-dominators leads to signiﬁcant
measurement overhead, as the measurement instructions are
executed frequently (similar to an instrumentation proﬁler).
For instance, in the time to measure methods (e.g., Berkeley
DB executed 144conﬁgurations in 30.2minutes), not a single
conﬁguration ﬁnished measuring control-ﬂow statements.
By contrast, measuring the performance of numerous
methods is inexpensive with a sampling proﬁler (cf.
Comprex in Table IIa), for which we observed a mostly
linear overhead of about 8%in our subject systems.
RQ2: In summary, compression at method and control-
ﬂow granularities is highly efﬁcient to reduce measure-
ment effort. Compression at method granularity provides
a good compromise between compression potential and
measurement overhead.TABLE IV: Number of regions and conﬁgurations to measure with
compression at different region granularities.
Control-ﬂow Method Program
System #Reg. #Conf. #Reg. #Conf. #Reg. #Conf.
Lucene 1654 26 551 26 1 16384
H2 2483 64 932 64 1 256
Berkeley DB 2152 144 718 144 1 2048
Density Converter 190 88 62 88 1 4608
#Reg: Number of regions; #Conf: Number of conﬁgurations. Bolded values in cells
indicate the minimum number of conﬁgurations to cover all partitions’ subspaces.
D. Limitations and Threats to Validity
Limitations of the Taint Analysis: Our approach to
compute partitions per region may produce inaccurate results
due to two sources of inaccuracy: (1) A standard dynamic
taint analysis cannot reason about paths not taken [6] and,
thus, may miss some taints, and (2) our use of a small
workload to reduce the cost of the iterative analysis may lead
to some missed interactions. The former threat is somewhat
mitigated by exploring multiple conﬁgurations and using the
cross product when updating partitions; we see both branches
of each control-ﬂow decision. The latter issue depends on
how the workload is shortened (see Sec. IV), but will likely
have a low impact in highly repetitive workloads. Both threats
can lead to generating inaccurate models.
Importantly, our results for RQ1 suggest that inaccuracies
on the regions’ partition caused by these threats resulted in,
at most, minor accuracy degradation in our performance-
inﬂuence models, given the consistently high accuracy
achieved across all subject systems.
Furthermore, we used a debugging strategy to identify
potential effects of inaccurate partitions. As discussed in
Sec. III-C, multiple executions of conﬁgurations within the
same subspace of a region’s partition must have the same
performance behavior. Signiﬁcant differences, beyond normal
measurement noise, indicate that the region’s partition might
be inaccurate and not capturing all relevant interactions.
Speciﬁcally, we analyzed the performance measurements
of all regions, searching for regions with a signiﬁcant perfor-
mance inﬂuence (> 0.1ms in any observed conﬁguration) and
with a high variance among the execution times of the same
subspace in a region (coefﬁcient of variation of the execution
times>1.0). Among the 2263 method-level regions that we
analyzed, we found only 8of such regions, all in Lucene.
We executed the iterative analysis with the regular workload,
which resulted in additional subpaces in 7out of8regions,
due to slightly different taints in the shorter workload. While
the missing subspaces slightly decreased the MAPE from 8.0
to7.4(as a result of slight changes in the coefﬁcients in
the model, not from new options nor interactions becoming
performance relevant), the time to run the taint analysis with
the regular workload is extremely expensive; 11hours instead
of29minutes to run the same 26conﬁgurations. In fact, the
iterative analysis did no ﬁnish executing after 24hours in the
other subject systems!
We argue that the extremely high cost and inability to use
1081the regular workload does not outweigh a potential slight ac-
curacy increase of the already highly accurate models that we
generated. These results support our conjecture that inaccura-
cies on the regions’ partition caused by using an unsound anal-
ysis with a small workload are only a minor issue in practice.
Threats to Validity: Beyond the limitations of the
taint analysis, measurement noise cannot be excluded and
may affect allresults. We reduced this threat by repeating
measurements on a dedicated machine and using the median.
The primary threat to external validity is the selection of
subject systems. While we selected medium- to large-scale
widely-used open-source Java conﬁgurable systems from
different domains, readers should be careful when generalizing
results. For instance, all analyzed systems are multi-
threaded, but had mostly deterministic performance behavior.
Additionally, we analyzed a single conﬁgurable system,
whereas systems composed of numerous conﬁgurable systems,
deployed in distributed environments, and implemented in
different languages are beyond the scope of this paper.
Another threat is the selected subset of options, which might
not affect performance at all, making modeling a trivial task.
We selected options for which the systems’ documentation
or the options’ functionality indicated that they would affect
performance, and we observed a wide range of execution
times for the conﬁgurations that we measured (see Fig. 5).
VI. R ELATED WORK
In Sec. II, we discussed performance modeling in general
and evaluated closely related state of the art black-box
approaches for performance-inﬂuence modeling to Comprex.
In this section, we discuss additional research to position
Comprex in a broader context of prior work.
Only few researchers have explored white-box performance
modeling of conﬁgurable systems in prior work [44, 66, 73].
Siegmund et al. [66] introduced the idea of white-box analysis,
but assumed a speciﬁc programming style that provided a
static mapping from options to regions and ignored data-ﬂow
between regions, severely limiting the systems that could be
analyzed. In prior work, we developed ConﬁgCrusher [73],
which used static data-ﬂow analysis to map options to regions.
ConﬁgCrusher also measures the performance of regions and
builds local performance-inﬂuence models. ConﬁgCrusher is,
however, limited by the overhead of the static analysis and was
unable to scale beyond small systems (e.g., it never terminated
when we tried it on our subject systems). By contrast,
Comprex uses a dynamic data-ﬂow analysis that scales to
large systems, demonstrating the feasibility of white-box
approaches to efﬁciently build accurate performance-inﬂuence
models for large-scale conﬁgurable systems. For comparison,
we evaluated both approaches on Density Converter, though
Comprex analyzed all used libraries, resulting in comparable
accuracies (4. 3with ConﬁgCrusher vs. 9.4with Comprex
in terms of MAPE), but measuring fewer conﬁgurations (256
with ConﬁgCrusher vs. 88with Comprex).
In prior work [73], we also adapted SPLat [37] for perfor-
mance modeling, originally designed for testing conﬁgurablesystems. SPLat uses lightweight instrumentation to observe
which options are accessed during execution, and in which
order, to explore all combinations of options that it encounters
during execution. This type of analysis is cheaper than our
taint tracking, but explores many spurious interactions,
resulting in SPLat essentially exploring all conﬁgurations.
Li et a. [44] used intraprocedural control-ﬂow analysis to
identify usage patterns of individual options in the source code
for predicting performance properties based on the patterns. In
contrast to our work, they do not measure performance, build
performance-inﬂuence models, nor consider interactions.
While our line of work focuses on the inﬂuence of conﬁg-
uration options, which are easy to change by users without
modifying the implementation, there is a active research
ﬁeld to identify bottlenecks and performance bugs, generally
using different forms of static or dynamic analysis [e.g.,
9, 10, 16, 22, 34, 43, 56, 81]. For example, Castro et al. [9]
use both techniques to identify performance bottlenecks that
can be analyzed and optimized in isolation. These kinds of ap-
proaches are focusing on different concerns than performance-
inﬂuence models but are likely complementary for developers
seeking to understand a system’s performance behavior.
At the same time, researchers have used static and dynamic
analyses to characterize and track options in conﬁgurable
systems [11, 45, 48, 52, 67, 69–71, 79]. For example, Toman
and Grossman [71] use dynamic taint analysis to identify
the use of stale conﬁguration data. Our work is inspired by
insights from analyzing conﬁgurable systems and uses similar
analysis strategies as foundations to map options to regions.
VII. C ONCLUSION
We presented Comprex, a white-box approach that efﬁ-
ciently builds accurate and interpretable performance-inﬂuence
models for conﬁgurable systems, without relying on traditional
sampling and learning techniques. Comprex employs an itera-
tive dynamic taint analysis to identify where options inﬂuence
the system, building and composing local linear performance-
inﬂuence models. Our empirical evaluation on 4systems
demonstrates the accuracy and efﬁciency of Comprex.
VIII. A CKNOWLEDGEMENTS
We specially want to thank Katherine Hough and Jonathan
Bell for their indispensable help with Phosphor. We thank
Chu-Pan Wong, Jens Meinicke, and Florian Sattler for their
comments during the development of this work, the FOSD
2019 meeting participants for their feedback on the iterative
dynamic taint analysis, and Claire Le Goues and Rohan
Padhye for their feedback on earlier drafts of this paper.
This work has been supported in part by the Software
Engineering Institute, NSF (Awards 1552944, 1717022, and
2007202), NASA (RASPERRY-SI 80NSSC20K1720), the
German Federal Ministry of Education and Research (BMBF,
01IS19059A and 01IS18026B), and the German Research
Foundation (DFG, SI 2171/2, SI 2171/3-1, AP 206/11).
1082REFERENCES
[1] Sat4j. [Online]. Available: www.sat4j.org
[2] (2019) Jproﬁler 10. [Online]. Available: https://www.ej-technologies.
com/products/jproﬁler/overview.html
[3] M. Al-Hajjaji, S. Krieter, T. Thüm, M. Lochau, and G. Saake, “Incling:
Efﬁcient product-line testing using incremental pairwise sampling,” in
Proc. Int’l Conf. Generative Programming and Component Engineering
(GPCE). ACM, Oct. 2016, pp. 144–155.
[4] S. Apel, D. Batory, C. Kästner, and G. Saake, Feature-Oriented Software
Product Lines: Concepts and Implementation. Springer-Verlag, 2013.
[5] S. Arzt, S. Rasthofer, C. Fritz, E. Bodden, A. Bartel, J. Klein,
Y . Le Traon, D. Octeau, and P. McDaniel, “FlowDroid: Precise context,
ﬂow, ﬁeld, object-sensitive and lifecycle-aware taint analysis for Android
apps,” in Proc. Conf. Programming Language Design and Implementa-
tion (PLDI). ACM, Jun. 2014, pp. 259–269.
[6] T. H. Austin and C. Flanagan, “Efﬁcient purely-dynamic information
ﬂow analysis,” in Proc. Workshop Programming Languages and Analysis
for Security (PLAS). ACM, Jun. 2009, pp. 113–124.
[7] S. Becker, H. Koziolek, and R. Reussner, “The Palladio component
model for model-driven performance prediction,” J. Syst. Softw., vol. 82,
no. 1, pp. 3–22, Jan. 2009.
[8] J. Bell and G. Kaiser, “Phosphor: Illuminating dynamic data ﬂow in
commodity JVMs,” SIGPLAN Notices, vol. 49, no. 10, pp. 83–101, Oct.
2014.
[9] P. D. O. Castro, C. Akel, E. Petit, M. Popov, and W. Jalby, “Cere:
Llvm-based codelet extractor and replayer for piecewise benchmarking
and optimization,” ACM Trans. Archit. Code Optim. (TACO), vol. 12,
no. 1, pp. 6:1–6:24, Apr. 2015.
[10] J. Cito, P. Leitner, C. Bosshard, M. Knecht, G. Mazlami, and H. C.
Gall, “PerformanceHat: Augmenting source code with runtime perfor-
mance traces in the IDE,” in Proc. Int’l Conf. Software Engineering:
Companion Proceeedings. ACM, 2018, pp. 41–44.
[11] Z. Dong, A. Andrzejak, D. Lo, and D. Costa, “ORPLocator: Identifying
read points of conﬁguration options via static analysis,” in Proc. Int’l
Symposium Software Reliability Engineering (ISSRE). IEEE, Oct. 2016,
pp. 185–195.
[12] F. Doshi-Velez and B. Kim, “Towards a rigorous science of interpretable
machine learning,” arXiv preprint arXiv:1702.08608, 2017.
[13] N. Esfahani, A. Elkhodary, and S. Malek, “A learning-based framework
for engineering feature-oriented self-adaptive software systems,” IEEE
Transactions on Software Engineering, vol. 39, no. 11, pp. 1467–1493,
Nov. 2013.
[14] A. Georges, D. Buytaert, and L. Eeckhout, “Statistically rigorous Java
performance evaluation,” SIGPLAN Notices, vol. 42, no. 10, pp. 57–76,
Oct. 2007.
[15] A. Grebhahn, N. Siegmund, and S. Apel, “Predicting performance of
software conﬁgurations: There is no silver bullet,” 2019.
[16] M. Grechanik, C. Fu, and Q. Xie, “Automatically ﬁnding performance
problems with feedback-directed learning software testing,” in Proc. Int’l
Conf. Software Engineering (ICSE). IEEE, Jun. 2012, p. 156–166.
[17] J. Guo, K. Czarnecki, S. Apel, N. Siegmund, and A. W ˛ asowski,
“Variability-aware performance prediction: A statistical learning ap-
proach,” in Proc. Int’l Conf. Automated Software Engineering (ASE).
ACM, Nov. 2013, pp. 301–311.
[18] J. Guo, D. Yang, N. Siegmund, S. Apel, A. Sarkar, P. Valov, K. Czar-
necki, A. Wasowski, and H. Yu, “Data-efﬁcient performance learning
for conﬁgurable systems,” Empirical Software Engineering, vol. 23, pp.
1826–1867, 2017.
[19] H. Ha and H. Zhang, “Performance-inﬂuence model for highly con-
ﬁgurable software with fourier learning and lasso regression,” in Proc.
Int’l Conf. Software Maintance and Evolution (ICSME), Sep. 2019, pp.
470–480.
[20] H. Ha and H. Zhang, “DeepPerf: Performance prediction for conﬁg-
urable software with deep sparse neural network,” in Proc. Int’l Conf.
Software Engineering (ICSE). IEEE, May 2019, p. 1095–1106.
[21] A. Halin, A. Nuttinck, M. Acher, X. Devroey, G. Perrouin, and
B. Baudry, “Test them all, is it worth it? assessing conﬁguration
sampling on the JHipster web development stack,” Empirical Software
Engineering, Jul. 2018.
[22] S. Han, Y . Dang, S. Ge, D. Zhang, and T. Xie, “Performance debuggingin the large via mining millions of stack traces,” in Proc. Int’l Conf.
Software Engineering (ICSE). IEEE, Jun. 2012, pp. 145–155.
[23] X. Han and T. Yu, “An empirical study on performance bugs for highly
conﬁgurable software systems,” in Proc. Int’l Symposium Empirical
Software Engineering and Measurement (ESEM). ACM, Sep. 2016,
pp. 23:1–23:10.
[24] M. Harchol-Balter, Performance Modeling and Design of Computer
Systems: Queueing Theory in Action, 1st ed. Cambridge University
Press, 2013.
[25] A. Hervieu, B. Baudry, and A. Gotlieb, “PACOGEN: Automatic gener-
ation of pairwise test conﬁgurations from feature models,” in Proc. Int’l
Symposium Software Reliability Engineering, Nov. 2011, pp. 120–129.
[26] A. Hervieu, D. Marijan, A. Gotlieb, and B. Baudry, “Optimal Min-
imisation of Pairwise-covering Test Conﬁgurations Using Constraint
Programming,” Information and Software Technology, vol. 71, pp. 129
– 146, Mar. 2016.
[27] A. Hubaux, Y . Xiong, and K. Czarnecki, “A user survey of conﬁguration
challenges in Linux and eCos,” in Proc. Workshop Variability Modeling
of Software-Intensive Systems (VAMOS). ACM, Jan. 2012, pp. 149–155.
[28] F. Hutter, H. H. Hoos, and K. Leyton-Brown, “Sequential model-based
optimization for general algorithm conﬁguration,” in Proc. Int’l Conf.
Learning and Intelligent Optimization. Springer-Verlag, Jan. 2011, pp.
507–523.
[29] P. Jamshidi and G. Casale, “An uncertainty-aware approach to optimal
conﬁguration of stream processing systems,” in Int’l Symp. Modeling,
Analysis and Simulation of Computer and Telecommunication Systems
(MASCOTS), Sep. 2016, pp. 39–48.
[30] P. Jamshidi, N. Siegmund, M. Velez, C. Kästner, A. Patel, and Y . Agar-
wal, “Transfer learning for performance modeling of conﬁgurable sys-
tems: An exploratory analysis,” in Proc. Int’l Conf. Automated Software
Engineering (ASE). ACM, Oct. 2017.
[31] P. Jamshidi, M. Velez, C. Kästner, and N. Siegmund, “Learning to
sample: Exploiting similarities across environments to learn performance
models for conﬁgurable systems,” in Proc. Int’l Symp. Foundations of
Software Engineering (FSE). ACM, Nov. 2018, pp. 71–82.
[32] P. Jamshidi, M. Velez, C. Kästner, N. Siegmund, and P. Kawthekar,
“Transfer learning for improving model predictions in highly conﬁg-
urable software,” in Proc. Int’l Symp. Software Engineering for Adaptive
and Self-Managing Systems (SEAMS). IEEE, May 2017, pp. 31–41.
[33] G. Jin, L. Song, X. Shi, J. Scherpelz, and S. Lu, “Understanding and
detecting real-world performance bugs,” in Proc. Conf. Programming
Language Design and Implementation (PLDI). ACM, Jun. 2012, pp.
77–88.
[34] M. Jovic, A. Adamoli, and M. Hauswirth, “Catch me if you can: Perfor-
mance bug detection in the wild,” in Proc. Int’l Conf. Object-Oriented
Programming, Systems, Languages and Applications (OOPSLA). ACM,
Oct. 2011, p. 155–170.
[35] C. Kaltenecker, A. Grebhahn, N. Siegmund, and S. Apel, “The interplay
of sampling and machine learning for software performance prediction,”
IEEE Software, 2020.
[36] C. Kaltenecker, A. Grebhahn, N. Siegmund, J. Guo, and S. Apel,
“Distance-based sampling of software conﬁguration spaces,” in Proc.
Int’l Conf. Software Engineering (ICSE). IEEE, May 2019.
[37] C. H. P. Kim, D. Marinov, S. Khurshid, D. Batory, S. Souto, P. Barros,
and M. d’Amorim, “SPLat: Lightweight dynamic analysis for reducing
combinatorics in testing conﬁgurable systems,” in Proc. Europ. Software
Engineering Conf. Foundations of Software Engineering (ESEC/FSE).
ACM, Aug. 2013, pp. 257–267.
[38] S. Kolesnikov, N. Siegmund, C. Kästner, A. Grebhahn, and S. Apel,
“Tradeoffs in modeling performance of highly conﬁgurable software
systems,” Software and System Modeling (SoSyM), Feb. 2018.
[39] S. Kounev, “Performance modeling and evaluation of distributed
component-based systems using queueing Petri Nets,” IEEE Transac-
tions on Software Engineering, vol. 32, no. 7, pp. 486–502, 2006.
[40] R. Krishna, M. S. Iqbal, M. A. Javidian, B. Ray, and P. Jamshidi,
“CADET: A systematic method for debugging misconﬁgurations using
counterfactual reasoning,” 2020.
[41] D. R. Kuhn, R. N. Kacker, and Y . Lei, Introduction to Combinatorial
Testing, 1st ed. Chapman & Hall/CRC, 2013.
[42] F. Lange. (2011, Jan.) Measure Java performance – sampling or instru-
1083mentation?
[43] C. Lemieux, R. Padhye, K. Sen, and D. Song, “Perffuzz: Automatically
generating pathological inputs,” in Proc. Int’l Symp. Software Testing
and Analysis (ISSTA). ACM, 2018, pp. 254–265.
[44] C. Li, S. Wang, H. Hoffmann, and S. Lu, “Statically inferring perfor-
mance properties of software conﬁgurations,” in Proc. European Conf.
Computer Systems (EuroSys). ACM, Apr. 2020.
[45] M. Lillack, C. Kästner, and E. Bodden, “Tracking load-time conﬁgu-
ration options,” IEEE Transactions on Software Engineering, vol. 44,
no. 12, pp. 1269–1291, 12 2018.
[46] S. M. Lundberg and S.-I. Lee, “A uniﬁed approach to interpreting model
predictions,” in Advances in Neural Information Processing Systems 30.
Curran Associates, Inc., 2017, pp. 4765–4774.
[47] F. Medeiros, C. Kästner, M. Ribeiro, R. Gheyi, and S. Apel, “A
comparison of 10 sampling algorithms for conﬁgurable systems,” in
Proc. Int’l Conf. Software Engineering (ICSE). ACM, May 2016, pp.
643–654.
[48] J. Meinicke, C.-P. Wong, C. Kästner, T. Thüm, and G. Saake, “On
essential conﬁguration complexity: Measuring interactions in highly-
conﬁgurable systems,” in Proc. Int’l Conf. Automated Software Engi-
neering (ASE). ACM, Sep. 2016, pp. 483–494.
[49] C. Molnar, Interpretable Machine Learning, 2019, https://christophm.
github.io/interpretable-ml-book/.
[50] D. C. Montgomery, Design and Analysis of Experiments. John Wiley
& Sons, 2006.
[51] V . Nair, T. Menzies, N. Siegmund, and S. Apel, “Using bad learners to
ﬁnd good conﬁgurations,” in Proc. Europ. Software Engineering Conf.
Foundations of Software Engineering (ESEC/FSE), ser. ESEC/FSE 2017.
ACM, Aug. 2017, p. 257–267.
[52] T. Nguyen, T. Koc, J. Cheng, J. S. Foster, and A. A. Porter, “iGen
dynamic interaction inference for conﬁgurable software,” in Proc. Int’l
Symp. Foundations of Software Engineering (FSE). IEEE, Nov. 2016.
[53] C. Nie and H. Leung, “A survey of combinatorial testing,” ACM Comput.
Surv. (CSUR), vol. 43, no. 2, pp. 11:1–11:29, Feb. 2011.
[54] A. Nistor, P.-C. Chang, C. Radoi, and S. Lu, “Caramel: Detecting and
ﬁxing performance problems that have non-intrusive ﬁxes,” in Proc. Int’l
Conf. Software Engineering (ICSE). IEEE, May 2015, pp. 902–912.
[55] A. Nistor, T. Jiang, and L. Tan, “Discovering, reporting, and ﬁxing
performance bugs,” in Proc. Int’l Conf. Mining Software Repositories.
IEEE, May 2013, pp. 237–246.
[56] A. Nistor, L. Song, D. Marinov, and S. Lu, “Toddler: Detecting per-
formance problems via similar memory-access patterns,” in Proc. Int’l
Conf. Software Engineering (ICSE). IEEE, 5 2013, pp. 562–571.
[57] J. Oh, D. Batory, M. Myers, and N. Siegmund, “Finding near-optimal
conﬁgurations in product lines by random sampling,” in Proc. Europ.
Software Engineering Conf. Foundations of Software Engineering (ES-
EC/FSE). ACM, Sep. 2017, pp. 61–71.
[58] R. Olaechea, D. Rayside, J. Guo, and K. Czarnecki, “Comparison of
exact and approximate multi-objective optimization for software product
lines,” in Proc. Int’l Software Product Line Conference (SPLC). ACM,
Sep. 2014, pp. 92–101.
[59] M. T. Ribeiro, S. Singh, and C. Guestrin, “"Why should I trust
you?": Explaining the predictions of any classiﬁer,” in Proc. Int’l Conf.
Knowledge Discovery and Data Mining (KDD). ACM, Aug. 2016, pp.
1135–1144.
[60] C. Rudin, “Stop explaining black box machine learning models for high
stakes decisions and use interpretable models instead,” Nature Machine
Intelligence, no. 5, pp. 206–215, 2019.
[61] A. Sarkar, J. Guo, N. Siegmund, S. Apel, and K. Czarnecki, “Cost-
efﬁcient sampling for performance prediction of conﬁgurable systems,”
inProc. Int’l Conf. Automated Software Engineering (ASE). IEEE,
Nov. 2015, pp. 342–352.
[62] G. Serazzri, G. Casale, M. Bertoli, G. Serazzri, G. Casale, and
M. Bertoli, “Java modelling tools: An open source suite for queueing
network modelling andworkload analysis,” in Proc. Int’l Conf. the
[63] N. Siegmund, A. Grebhahn, S. Apel, and C. Kästner, “Performance-
inﬂuence models for highly conﬁgurable systems,” in Proc. Europ.
Software Engineering Conf. Foundations of Software Engineering (ES-Quantitative Evaluation of Systems - (QEST’06), Sep. 2006, pp. 119–
120.
EC/FSE). ACM, Aug. 2015, pp. 284–294.
[64] N. Siegmund, S. S. Kolesnikov, C. Kästner, S. Apel, D. Batory,
M. Rosenmüller, and G. Saake, “Predicting performance via automated
feature-interaction detection,” in Proc. Int’l Conf. Software Engineering
(ICSE). IEEE, Jun. 2012, pp. 167–177.
[65] N. Siegmund, M. Rosenmüller, M. Kuhlemann, C. Kästner, S. Apel,
and G. Saake, “SPLConqueror: Toward optimization of non-functional
properties in software product lines,” Software Quality Journal, vol. 20,
no. 3-4, pp. 487–517, Sep. 2012.
[66] N. Siegmund, A. von Rhein, and S. Apel, “Family-based performance
measurement,” in Proc. Int’l Conf. Generative Programming and Com-
ponent Engineering (GPCE). ACM, Oct. 2013, pp. 95–104.
[67] S. Souto and M. d’Amorim, “Time-space efﬁcient regression testing for
conﬁgurable systems,” Journal of Systems and Software, 2018.
[68] E. Štrumbelj and I. Kononenko, “Explaining prediction models and
individual predictions with feature contributions,” Knowledge and in-
formation systems, vol. 41, no. 3, pp. 647–665, 2014.
[69] T. Thüm, S. Apel, C. Kästner, I. Schaefer, and G. Saake, “A classiﬁcation
and survey of analysis strategies for software product lines,” ACM
Comput. Surv. (CSUR), vol. 47, no. 1, pp. 6:1–6:45, Jun. 2014.
[70] J. Toman and D. Grossman, “Legato: An at-most-once analysis with
applications to dynamic conﬁguration updates,” in European Conf.
Object-Oriented Programming (ECOOP), 7 2016.
[71] ——, “Staccato: A bug ﬁnder for dynamic conﬁguration updates,”
inProc. European Conf. Object-Oriented Programming (ECOOP).
Schloss Dagstuhl–Leibniz-Zentrum fuer Informatik, Jul. 2016.
[72] P. Valov, J.-C. Petkovich, J. Guo, S. Fischmeister, and K. Czarnecki,
“Transferring performance prediction models across different hardware
platforms,” in Proc. Int’l Conf. on Performance Engineering (ICPE).
ACM, Apr. 2017, pp. 39–50.
[73] M. Velez, P. Jamshidi, F. Sattler, N. Siegmund, S. Apel, and C. Kästner,
“Conﬁgcrusher: Towards white-box performance analysis for conﬁg-
urable systems,” Autom Softw Eng, 2020.
[74] M. Velez, P. Jamshidi, N. Siegmund, S. Apel, and C. Kästner, “White-
box analysis over machine learning: Modeling performance of conﬁg-
urable systems - Supplementary Material - https://bit.ly/3bbbgG8,” 2021.
[75] N. Viswanadham and Y . Narahari, Performance modeling of automated
systems. PHI Learning Pvt. Ltd., 2015.
[76] S. Wang, C. Li, H. Hoffmann, S. Lu, W. Sentosa, and A. I. Kistijan-
toro, “Understanding and auto-adjusting performance-sensitive conﬁg-
urations,” in Proc. Int’l Conf. Architectural Support for Programming
Languages and Operating Systems (ASPLOS) . ACM, Mar. 2018, pp.
154–168.
[77] M. Weber, S. Apel, and N. Siegmund, “White-Box Performance-
Inﬂuence models: A proﬁling and learning approach,” in Proc. Int’l
Conf. Software Engineering (ICSE). IEEE, May 2021.
[78] T. Xu, L. Jin, X. Fan, Y . Zhou, S. Pasupathy, and R. Talwadker,
“Hey, you have given me too many knobs!: Understanding and dealing
with over-designed conﬁguration in system software,” in Proc. Europ.
Software Engineering Conf. Foundations of Software Engineering (ES-
EC/FSE). ACM, Aug. 2015, pp. 307–319.
[79] T. Xu, X. Jin, P. Huang, Y . Zhou, S. Lu, L. Jin, and S. Pasupathy, “Early
detection of conﬁguration errors to reduce failure damage,” in Proc.
Conf.Operating Systems Design and Implementation (OSDI). USENIX
Association, Nov. 2016, pp. 619–634.
[80] T. Xu, J. Zhang, P. Huang, J. Zheng, T. Sheng, D. Yuan, Y . Zhou, and
S. Pasupathy, “Do not blame users for misconﬁgurations,” in Proc. Symp.
Operating Systems Principles. ACM, Nov. 2013, pp. 244–259.
[81] T. Yu and M. Pradel, “Pinpointing and repairing performance bottlenecks
in concurrent programs,” Empirical Softw. Eng., vol. 23, no. 5, pp. 3034–
3071, Oct. 2018.
[82] Y . Zhu, J. Liu, M. Guo, Y . Bao, W. Ma, Z. Liu, K. Song, and Y . Yang,
“Bestconﬁg: Tapping the performance potential of systems via automatic
conﬁguration tuning,” in Proc. Symposium Cloud Computing (SoCC).
ACM, 9 2017, pp. 338–350.
1084