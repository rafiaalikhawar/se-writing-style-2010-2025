Good Things Come In Threes: Improving Search-based Crash
Reproduction With Helper Objectives
Pouria Derakhshanfar
p.derakhshanfar@tudelft.nl
Delft University of Technology
Delft, The NetherlandsXavier Devroey
x.d.m.devroey@tudelft.nl
Delft University of Technology
Delft, The NetherlandsAndy Zaidman
a.e.zaidman@tudelft.nl
Delft University of Technology
Delft, The Netherlands
Arie van Deursen
arie.vandeursen@tudelft.nl
Delft University of Technology
Delft, The NetherlandsAnnibale Panichella
a.panichella@tudelft.nl
Delft University of Technology
Delft, The Netherlands
ABSTRACT
Writing a test case reproducing a reported software crash is a com-
monpracticetoidentifytherootcauseofananomalyinthesoft-
wareundertest.However,thistaskisusuallylabor-intensiveand
time-taking.Hence,evolutionaryintelligenceapproacheshavebeen
successfully applied to assist developers during debugging by gen-
erating a test case reproducing reported crashes. These approaches
use a single fitness function called Crash Distance to guide the
searchprocesstowardreproducingatargetcrash.Despitethere-
ported achievements, these approaches do not always successfully
reproducesomecrashesduetoalackoftestdiversity(premature
convergence).Inthisstudy,weintroduceanewapproach,called
MO-HO,thataddressesthisissueviamulti-objectivization.Inpar-
ticular,weintroducetwonewHelper-Objectivesforcrashreproduc-
tion,namely testlength (tominimize)and methodsequencediversity
(to maximize), in addition to Crash Distance . We assessed MO-
HOusingfivemulti-objectiveevolutionaryalgorithms(NSGA-II,
SPEA2, PESA-II, MOEA/D, FEMO) on 124 non-trivial crashes stem-
mingfromopen-sourceprojects.OurresultsindicatethatSPEA2
isthebest-performingmulti-objectivealgorithmfor MO-HO.W e
evaluatedthisbest-performingalgorithmfor MO-HOagainstthe
state-of-the-art:single-objectiveapproach(Single-ObjectiveSearch)
and decomposition-based multi-objectivization approach (De-MO ).
Our results show that MO-HOreproduces five crashes that cannot
be reproduced by the current state-of-the-art. Besides, MO-HOim-
proves the effectiveness (+10% and +8% in reproduction ratio) and
theefficiencyin34.6%and36%ofcrashes(i.e.,significantlylower
running time) compared to Single-Objective Search and De-MO,
respectively. For some crashes, the improvements are very large,
beingupto+93.3%forreproductionratioand-92%fortherequired
running time.
ASE ’20, September 21–25, 2020, Virtual Event, Australia
© 2020 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-6768-4/20/09.
https://doi.org/10.1145/3324884.3416643CCS CONCEPTS
•Softwareanditsengineering →Softwaretestinganddebug-
ging;Search-based software engineering.
KEYWORDS
crash reproduction, search-based software testing, multi-objective
evolutionary algorithms
ACM Reference Format:
Pouria Derakhshanfar, Xavier Devroey, Andy Zaidman, Arie van Deursen,
and AnnibalePanichella. 2020. GoodThings Come InThrees: Improving
Search-basedCrashReproductionWithHelperObjectives.In 35thIEEE/ACM
International Conference on Automated Software Engineering (ASE ’20), Sep-
tember 21–25, 2020, Virtual Event, Australia. ACM, New York, NY, USA,
13 pages. https://doi.org/10.1145/3324884.3416643
1 INTRODUCTION
When a software application crashes, a report (or issue), including
informationgatheredduringthecrash,isassignedtodevelopersfor
debugging [ 43]. One common practice to identify the root cause of
acrashistoprovideatestcasethatreproducesit[ 45].Thistestcase
can later be adapted and integrated into the test suite to prevent
futureregressions.However,thistestcaseisnotalwaysavailable
inthecrashreports.Also,dependingontheamountofinformation
available in the report, writing this crash reproducing test case can
be time-consuming and labor-intensive [39].
Consequently, various approaches have been proposed in the
literaturetoautomate crashreproduction [4,6,23,31,32,36,39,44].
These approaches use the information about a crash (e.g., stack
tracesfromcrashreports)togenerateacrashreproducingtestcasebyutilizingdifferenttechniquessuchassymbolicexecution,model
checking, etc.Among these approaches, two evolutionary-based
techniqueshavebeenintroduced:ReCore[ 36]andEvoCrash[ 39].
These two approaches generate test cases able, when executed,
toreproducethetargetcrashusingsingle-objectiveevolutionary
algorithms.Theempiricalevaluationof EvoCrash[ 39]showsthat
it outperforms other, evolutionary-based and non-evolutionary-
basedapproachesintermsof crashreproductionratio (percentage
ofcrashesthatcouldbereproduced)and efficiency (timetakento
reproduceagivencrashsuccessfully).Thisevaluationalsoconfirms
that EvoCrash significantly helps developers during debugging.
EvoCrashreliesonasingle-objectiveevolutionaryalgorithm
(Single-Objective Search hereafter) that evolves test cases according
2112020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE)
This work is licensed under a Creative Commons Attribution International 4.0 License. 
toanobjective(CrashDistance hereafter)measuringhowfaragen-
erated test is from reproducing the crash. Crash Distance combines
threeheuristics: linecoverage (howfaristhetestfromexecuting
theline causingthe crash?), exceptioncoverage (doesthe testthrow
thesameexceptionasinthecrash?),and stacktracesimilarity (how
similar is the exception stack trace from the one reported in the
crash?).AlthoughSingle-ObjectiveSearchperformswellcompared
totheothercrashreproductionapproaches,amoreextensiveem-
pirical study [ 37] evidenced that it is not successful in reproducing
complex crashes (i.e., large stack traces). Hence, further studies to
enhance the guidance of the search process are required.
Justlikeanyotherevolutionary-basedalgorithm,Single-Objecti-
veSearch requirestomaintain abalancebetween exploration and
exploitation [42]. The former refers to the generation of completely
new solutions (i.e., test cases executing new paths in the code); the
latter refers to the generation of solutions in the neighborhood
of the existing ones (i.e., test cases with similar execution paths).
Single-ObjectiveSearchensuresexploitationthroughGuidedMuta-
tion, which guarantees that each solution contains the method call
causingthecrash(andreportedinthestacktrace)[ 39].However,
thelowexplorationof Single-ObjectiveSearchmayleadtoalack
of diversity, trapping the search in local optima [42].
To tackle this problem, a prior study [ 38] investigated the usage
ofDecomposition-based Multi-Objectivization (De-MO) to decom-
posetheCrashDistance inthree distinct(sub-)objectives.Atarget
crashisreproducedwhenthesearchprocessfullfilsallthreesub-
objectivesatthesametime.Theempiricalevaluationshowsthat
De-MOslightly improves the efficiency for some crashes. However,
sincethesub-objectivesarenotconflicting,theircombinedusage
can be detrimental for crash reproduction [ 38]. A recent study [ 13]
alsoconjecturedthatincreasingdiversityviaadditionalobjectiveis
a feasible yet unexplored research direction to follow. However, no
systematicempiricalstudyhas been conductedtodrawstatistical
conclusions.
In this study, we investigate a new strategy to Multi-Objectivize
crashreproductionbasedonHelper-Objectives(MO-HO )[13]rather
thandecomposition.Morespecifically,weaddtwoadditionalhelper-
objectives to Crash Distance (first objective): method sequence di-
versity(secondobjective)and testcaselengthminimization (third
objective).Thesecondobjectiveaimstoincreasethediversityin
themethodsequences;morediversesequencesaremorelikelyto
coverdiversepathsand,consequently,improveexploration.Thethird objective aims to address the bloating effect (i.e.,the gener-
ated test cases can become longer and longer after each generation
until the all of the system memory is used), as diversity can lead
to an unnecessary and counter-productive increase of the test case
length[1,33].Sincethesethreeobjectivesare conflicting,weexpect
animprovementinthesolutions’diversityand,hence,improving
the effectiveness (crash reproduction ratio) and efficiency.
To assess the performance of MO-HOon crash reproduction, we
use five multi-objective evolutionary algorithms (MOEAs): NSGA-
II [10], SPEA2 [ 47], MOEA/D [ 46], PESA-II [ 8], and FEMO [ 25].
We apply them to 124 non-trivial crashes from JCrashPack [ 37], a
crash benchmark used by previous crash reproduction studies [ 12].
Those crashes can only be reproduced by a test case that brings
the software under test to a specific state and invokes the target
method with one or more specific input parameters. We performed0 java.lang.Ar rayIndexOutOfBoundsException: 4
1 at [...].FastDateParser.toArray(FastDateParser.java:413)
2 at [...].FastDateParser.getDisplayNames([...]:381)
3 at [...].FastDateParser$TextStrategy.addRegex([...]:664)
4 at [...].FastDateParser.init([...]:138)
5 at [...].FastDateParser.<init >([...]:108)
6 [...]
Figure 1: LANG-9b crash stack trace [24, 37]
an internal assessment among MO-HOalgorithms to find the best
multi-objective evolutionaryalgorithm forthis optimizationprob-
lem.Accordingtotheresultsobservedinthisassessment, SPEA2
outperforms other MOEAs in crash reproduction using MO-HO
helper-objectives.
Furthermore,wecomparedthebest-performing MO-HO(MO-HO
+SPEA2)againsttwostate-of-the-artapproaches(Single-Objecti-
ve Search [ 39] andDe-MO[38]) from the perspectives of crash
reproduction ratio andefficiency. Our results show that MO-HO
outperforms the state-of-the-art in terms of crash reproduction
ratioandefficiency.Thisalgorithmimprovesthecrashreproduction
ratio by up to 100% and 93.3% (10% and 8%, on average) compared
toSingle-ObjectiveSearchand De-MO,respectively.Also,afterfive
minutes of search, MO-HOreproduces five and six crashes (4% and
5% more crashes) that cannot be reproduced by Single-ObjectiveSearchand De-MO,respectively.Inaddition, MO-HOreproduces
crashessignificantlyfasterthanSingle-ObjectiveSearchand De-MO
in 34.6% and 37.9% of the crashes, respectively.
A replication package, enabling the full-replication of our evalu-
ation and data analysis of our results is available on Zenodo [ 14].
2 BACKGROUND AND RELATED WORK
Several approaches have been introduced in the literature that aim
to reproduce a given crash. Some of these techniques (e.g., ReCore
[36])useruntimedata(i.e., coredumps).Howev er,collectingthe
runtime data may induce a significant overhead and raises privacy
concerns.Incontrast,otherapproaches[ 4,6,32,44]onlyrequirethe
stacktraces oftheunhandledexceptioncausingthecrash,collected
from executions logs or reported issues. For Java programs, a stack
traceincludesthelistofclasses,methods,andcodelinenumbers
involved in the crash. As an example, Figure 1 shows a stack trace
producedbyacrash(duetoabug)inApacheCommonsLang.This
stack trace contains the type of the exception (ArrayIndexOutOf-
BoundsException ) andframes(lines 1-6) indicating the stack of
active method calls during the crash.
Amongthevariousapproachessolelyusingastacktraceasinput,
STAR [6] and BugRedux [ 23] use backward and forward symbolic
execution, respectively; MuCrash [ 44] mutates the existing test
cases of the classes involved in the stack trace; JCharming [ 31,32]
applies model checking and program slicing for crash reproduc-tion; and ConCrash [
4] is designed to use pruning strategies to
reproduce the crash-reproducing test case.
EvoCrash is an evolutionary-based approach that applies a
Single-Objective Genetic Algorithm (Single-Objective Search) to
generateacrash-reproducingtestcaseforagivenstacktraceand
atarget frame (i.e.,the class under test for which the test case is
generated). The generated test will trigger a crash with a stack
212trace that is identical to the original one, up to the target frame.
Forinstance,forthestacktraceinFigure1withatargetframeat
line 3, EvoCrash generates a test case that reproduces the first
threeframesofthisstacktrace(i.e., identicalfromlines0to3).A
previous empirical evaluation [ 39] shows that EvoCrash performs
bettercomparedtoothercrashreproductionapproachesrelyingon
modelcheckingandprogramslicing[ 31,32],backwardsymbolic
execution [ 6], or exploiting existing test cases [ 44]. The study also
confirmsthatautomaticallygeneratedcrash-reproducingtestcases
help developers to reduce their debugging effort.
2.1 Single-Objective Search Heuristics
To evaluate the candidate tests, and consequently guide the search
process,Single-ObjectiveSearchappliesafitnessfunctioncalledthe
CrashDistance .Thisfitnessfunctioncontainsthreecomponents:
(i) theline coverage distance , indicating the distance between
the execution trace and the target line (the line number pointed to
by the target frame), (ii) the exceptiontype coverage , indicating
whether the target exception is thrown, and (iii) the stack trace
similarity ,indicating whetherall frames(fromthe beginningup
to the target frame) are included in the triggered stack trace.
Definition 2.1 (CrashDistance [39]).Foragiventestcaseexecu-
tiont, theCrash Distance (f) is defined as follows:
f(t)=⎧⎪⎪⎨⎪⎪⎩3×ds(t)+2×max (de)+max (dtr)if line not reached
3×min (ds)+2×de(t)+max (dtr)if line reached
3×min (ds)+2×min (de)+dtr(t)if exception thrown
(1)
Whereds(t)∈[0,1]indicateshowfarthetest tisfromreaching
thetargetline usingtwoheuristics: approachlevel andbranchdis-
tance[27].Theformermeasurestheminimumnumberofcontrol
dependenciesbetweentheexecutionpathof tandthetargetline;
thelatterindicateshowfar tisfromsatisfyingthebranchcondition
on which the target line is control dependent. And de(t)∈{0,1}
indicateswhetheranexceptionwiththesametypeasthetargetex-
ceptionisthrown(0)ornot(1).Finally, dtr(t)∈[0,1]calculatesthe
similarity between the stack trace produced by tand the expected
one,basedonclasses,methods,andlinenumbersappearinginboth
stack traces. Functions max (.)andmin (.)denote the maximum
and minimum possible values for a function, respectively. Con-
cretely,de(t)anddtr(t)areonlycalculateduponthesatisfaction
oftwoconstraints :exceptiontypecoverage andstacktracesimilarity
are relevant only when we reach the target line (first constraint)
and when we have the same type of exception (second constraint),
respectively.
2.2 Single-Objective Search
The search process starts with a guided initialization during
which an initial population of randomly generated test cases is
created. The algorithm ensures that each test case calls the tar-
getmethod (pointedtobythetargetframe)atleastonce.Ineach
generation, the fittest test cases are evolved by applying guided
mutation andguided crossover . Guided mutation applies a clas-
sicalmutationtothetestcaseswhileensuringthatthemutatedtest
containsoneormorecallstothetargetmethod.Similarly,guided
crossoverisavariantofthesingle-pointcrossoverthatpreservescalls to the target methods in the offsprings. Accordingly, each
generatedtestcasecontainsatleastonecalltothetargetmethod
(i.e.,the method triggering the crash) [39].
With thoseoperators, Single-ObjectiveSearch improvesthe ex-
ploitation, but it penalizes exploration of new areas of the search
spacebynotgeneratingdiverseenoughtestcases.Asaconsequence,
the search process may get stuck in local optima.
2.3 Decomposition-based Multi-objectivization
Toincreasediversityduringthesearch,apriorstudy[ 38]investi-
gatedtheusageof Decomposition-basedMulti-Objectivization (called
De-MOhereafter) to decompose the Crash Distance in three dis-
tinct(sub-)objectives. De-MOontheCrashDistance (temporarily)
decomposes the function in three distinct (sub-)objectives: ds(t),
de(t), anddtr(t). Then,De-MOuses a multi-objective evolution-
ary algorithm optimizing three objectives to generate one crash-reproducing solution. In the end, the global optimal solution is a
testcaseintheParetofrontproducedbyMOEAsthatsatisfiesallof
the sub-objectives simultaneously. The empirical evaluation shows
thatDe-MOincreases the efficiency of the crash reproduction pro-
cessforsomespecificcasescomparedtoSingle-ObjectiveSearch.
However, it loses efficiency in some other cases.
Inparticular,in Multi-objectivization,searchobjectivesshould
be conflicting to increase the diversity of generated solutions [ 22].
However,thethreesub-objectivesin De-MO[38]aretightlycoupled
and not conflicting: the stack trace similarity ( dtr(t)) cannot be
computedfortestcase twithoutexecutingthetargetline( ds(t)=0)
and throwing the correct type of exception ( de(t)=0). Also, the
typeofexception( de(t))isnotrelevant,whiletest tdoesnotcover
the statement in the target line ( ds(t)=0.0).
3 MULTI-OBJECTIVIZATION WITH
HELPER-OBJECTIVES (MO-HO)
Decomposingthe CrashDistance leadstoasetofdependentsub-
objectives,whichreducestheeffectofimprovingdiversitythrough
multi-objectivization [ 22]. In this study, we focus on using new
helper-objectives in addition to the Crash Distance , rather than
decomposing it. We define two helper-objectives called method
sequence diversity andtest length minimization that aim to
(i) increase diversity in the population (i.e., generated tests) and (ii)
address the bloatingeffect [30,33]. Then, we use five different evo-
lutionaryalgorithmsbelongingtodifferentcategoriesofMOEAs
(e.g.,decomposition-based and rank-based) to solve this optimiza-
tionproblem.Intheremainderofthissection,wefirstdiscussthe
twohelper-objectives.Next, wepresenttheMOEAsused tosolve
this problem.
3.1 Helper-Objectives
AssuggestedbyJensen etal.[22],addinghelper-objectivestoan
existing single objective can help search algorithms escape from
local optima. However, this requires that the helper objectives are
in conflict with the primary one [ 22]. Therefore, defining proper
helper-objectives is crucial.
MethodSequenceDiversity. Thefirsthelper-objectiveseeks
tomaximizethediversityofthemethod-callsequencesthatcom-
pose the generated tests because more diverse tests might execute
213differentpathsorbehaviorsofthe targetclass.Noticethateachtest
case is a sequence of statements, where each statement belongs to
oneofthefollowingfivedifferentcategories[ 33]:primitive state-
ments,constructors, fieldstatements, methodcalls,orassignments.
Furthermore, the length of a test case is variable, i.e.,it is not fixed
a priori and can vary during the search.
Inrecent years,severalfunctions havebeenintroduced tomea-
sure test case diversity [ 30]. These functions measure the diversity
between two test cases by using a binary encoding function to
calculate the distance between the corresponding encoded vectors
using the Levenshtein distance [ 26], Hamming distance [ 19],etc.
For three or more test cases, the overall diversity corresponds to
the average pairwise diversity of the existing test cases [ 30]. These
metricshavebeenusedinothertestingtasks(e.g., automatedtest
selection), but not in crash reproduction.
To measure the value of this helper-objective for the generated
solutions, we follow a similar procedure. Let us assume that F=
{f1,f2,..fn}isasetofpublicandprotectedmethodsinthetarget
class (i.e., method calls that can be called directly by the generated
tests), and T={t1,t2,..tm}is a set of generated test cases. To
calculate the diversity of T, we first need to encode each tk∈T
into a binary vector. We use the same encoding function proposed
by Mondal et al.[30]: each test case tk∈Tcorresponds to a binary
vectorvkof length n(i.e.,the number of public and protected
methodsinthetargetclass).Eachelement vk[i]ofthebinaryvector
denotes whether the corresponding method fi∈Fis invoked
by the test case tk. More formally, for each method fi∈F, the
corresponding entry vk[i]=1i ftkcallsfi;vk[i]=0 otherwise.
Then, we calculate the diversity for each pair of test cases tk
andtias the Hamming distance between the corresponding binary
vectorsvkandvi[19].TheHammingdistance(Hamming )between
twovectorscorrespondstothenumberofmismatches1overtheto-
tallengthofthebinaryvectors.Forinstance,theHammingdistance
betweenA=/angbracketleft1,1,0,1,0/angbracketrightandB=/angbracketleft0,1,0,1,1/angbracketrightequals to 2 /5=0.4.
Definition 3.1 (MethodSequenceDiversity) .Givenanencoding
functionV(.), the method sequence diversity ( MSD) of a test t∈T
corresponds to the average Hamming distance of that test from the
other test cases in T:
MSD (t)=/summationtext
ti∈T\{t}Hamminд (V(t),V(ti))
|T|−1(2)
In our approach, MSDshould be maximized to increase the
chanceofthegeneratedtesttoexecutenewpathsorbehaviorsin
thetargetclass.Sinceourtool(seeSection4.1)isdesignedformini-
mizationproblems,weminimizethemethodsequencesimilarity
using the formula:
fMSD (t)=1−MSD (t) (3)
TestLengthMinimization Whileincreasingmethodsequence
diversity can help to execute diverse paths of the target class, a
previousstudy[ 1]alsoshowedthat testdiversitymetrics (suchas
call sequence diversity) can reduce coverage. This is due to the
bloating effect, i.e.,diversity will also promote larger test cases over
short ones. Let us assume that we have a set of short test cases
withfewmethodcallsinourpopulation(mostoftheelementsin
theirbinaryvectorsare0).Alengthytestcase tLthatcallsallthe
1The number of positions at which the corresponding bits are different.methods of the target class will have a binary vector containing
only 1 values. As a consequence, tLwill have a large Hamming
distance from the existing test cases.
Larger tests introduce two potential issues: (i) they are likely
more expensive to run (extra overhead), and (ii) they may contain
spurious statements that do not help code coverage (which is apart ofCrash Distance ). In the latter case, mutation can become
less effective as it may mutate spurious statements rather than
therelevantpartofthechromosomes.Therefore,testdiversityis
in conflict with Crash Distance . To avoid the bloatingeffect, our
secondhelper-objectiveis testlengthminimization,whichcounts
the number of statements in a given test:
Definition3.2 (TestLengthMinimization) .Foratestcase twitha
length|t|, the fitness function is:
flen (t)=|t| (4)
3.2 Multi-Objective Evolutionary Algorithms
Inthisstudy,ourgoalistosolveamulti-objectivizedproblemby
minimizing the three objective functions (Crash Distance ,fMSD,
andflen). In theory, we could consider various MOEAs, each com-
ing with different advantages and disadvantages over different
optimization problems (e.g., multimodal, convex, etc.). However,
wecannotestablishupfrontwhattypeofMOEAworksbetterfor
crash reproduction as the shape of the Pareto Front (i.e., type of
problem) for crash reproduction is unknown. Hence, we chose five
MOEAsfromdifferentcategoriestodeterminethebestalgorithm
forMO-HO:NSGA-IIusesthenon-dominatedsortingprocedure;
SPEA2is an archive-based algorithm that selects the best solutions
according to the fitness value; PESA-IIdivides the objective space
tohyper-boxesandselectsthesolutionsfromthehyper-boxeswith
thelowerdensity; MOEA/D decomposestheproblemtomultiple
sub-problems; and FEMO, is a (1+1) evolutionary algorithm that
evolves tests solely with mutation and without crossover.
Weusethesamestoppingconditionsforallsearchalgorithms,
which is a maximum search budget, or when the target crash is
successfully reproduced, i.e.,a solution with a Crash Distance of
0.0 is found. Also, to increase exploitation during the search, all
algorithms use the guided crossover andguided mutation operators.
In the following subsections, we briefly describe the selected
search algorithms and their core characteristics.
3.2.1 Non-dominated Sorting Genetic Algorithm II (NSGA-II) [ 10].
In NSGA-II, offspring tests are generated, from given a population
of sizeN, usinggenetic operators (crossover andmutation). Next,
NSGA-IIunionstheoffspringpopulationwiththeparentpopulation
intoasetofsize2 Nandappliesa non-dominatedsorting toselect
theNindividualsforthenextgeneration.Thissortingisperformed
basedonthe dominance relationand crowdingdistance :thesolutions
aresortedintosubsequentdominancefronts.Thenon-dominated
solutions are in the first front ( Front0). These solutions have a
higher chance of being selected. Furthermore, crowding distance is
used to raise the chance of the most diverse solutions within the
samefronttobeselectedforthenextgeneration.Ineachgeneration,
parent test cases are selected for reproduction using the binary
tournament selection.
2143.2.2 Strength Pareto Evolutionary Algorithm 2 (SPEA2) [ 47].Be-
sidesthecurrentpopulation,SPEA2containsanexternalarchive
thatcollectsthenon-dominatedsolutionsamongallofthesolutions
consideredduringthesearchprocess.SPEA2assignsa fitnessvalue
to each solution (test) in the archive. The fitness value of solution i
is calculated by summing up two values: Raw fitness (R(i)∈N0),
which represents the dominance relation of i; andStrength value
(S(i)∈[0,1]), which estimates the density of solutions in the same
Pareto front (solutions that are not dominating each other). A solu-
tion with lower fitness value is “better” and has a higher chance of
beingselected.Forinstance,thenon-dominatedsolutionshavea
R(i)=0, and their fitness values are lower than 1.
The external archive has a fixed size, which is given at the be-
ginningofthesearchprocess.Afterupdatingthearchiveineach
iteration,thealgorithmchecksifthesizeofthearchiveexceedsthis
givensize.Ifthesizeofthearchiveissmallerthanthegivensize,
SPEA2fillsthe archivewiththeexisting dominated solutions.In
contrast, if the size of the archive is bigger than the given size, this
algorithm uses a truncation operator to remove the solutions with
a highfitness value from the archive. After updating the archive,
SPEA2 applies binary tournament selection based on the calculated
fitnessvalues,selectsparentsolutions,andgeneratesoffspringso-
lutions via crossover andmutation.
3.2.3 Pareto Envelop-based Selection Algorithm (PESA-II) [ 8].Sim-
ilar to SPEA2, PESA-II benefits from an external archive. In each
generation,thearchiveisupdatedbystoringthenon-dominated
solutionsinthearchiveandthecurrentpopulation.However,the
difference is in the selection strategy and archive truncation. Inthis algorithm, instead of assigning a fitness value to each of thesolutions in the archive, the objective space is divided, based onthe existing solutions, into hyper-boxes or grids. Non-dominated
solutionsinahyper-boxwithlowerdensityhaveahigherchance
of being selected and a lower chance of being removed.
3.2.4 Multi-objective Evolutionary Algorithm Based on Decomposi-
tion (MOEA/D) [ 46].This algorithm decomposes the M-objectives
problem into Ksingle-objective sub-problems and optimizes them
simultaneously. Each sub-problem has different weights for the
optimizationobjectives.The Ksub-problems д(x|w1),...,д(x|wK)
are obtained using a scalarization function д(x|w)and a set of
uniformly-distributed weight vectors W={w1,...,wk}. The de-
composition can be done with several techniques such as weighted
sum[29],Tchebycheff [29],orBoundaryIntersection [9,28].Ineach
generation,MOEA/Dmaintainsthebestindividualsforeachsub-problem
д(x|wi),whilethereproduction(basedoncrossoverand
mutation) is allowed only among solutions (tests) within the same
neighbourhood (mating restriction ).
3.2.5 Fair Evolutionary Multi-objective Optimizer (FEMO) [ 25].
This algorithm is a local (1+1) evolutionary algorithm. It means
that in each iteration, only one solution is evolved by the mutation
operatortohaveonlyoneoffspringsolutionforthenextgeneration.
FEMO contains an archive. In the first iteration, it generates a
randomsolutionandplacesitinthearchive.Inthenextgenerations,itselectsoneindividualfromthearchiveandevolvesitbymutation
operator to generate a new solution. Finally, if the new solutiondominates at least one of the solutions in the archive, it adds the
new solution to the archive and removes the dominated solutions.
Each solution inthe archive has a weight ( w) that indicatesthe
number of times that a solution was selected from the archive. So,
the initial weight of a newly generated test case is 0. During the
selection, FEMO selects a solution randomly from the solutions in
the archive that have the lowest w.
4 EMPIRICAL EVALUATION
To assess the impact of MO-HOon crash reproduction, we per-
formed an empirical evaluation and answered the following re-
search questions.
RQ1:Which Multi-Objective algorithm performs better with MO-
HO’s search objectives in terms of crash reproduction?
RQ2:What is the impact of the MO-HO algortihm on crash re-
production compared to Single-Objective Search and De-MO?
RQ3:HowdoesMO-HO’sefficiencycomparetoSingle-Objective
Search and De-MO?
4.1 Implementation
Sinceothercrashreproductionapproachesarenotopenlyavailable,
weimplementedanewopen-sourceevolutionary-basedcrashre-
productionframework,calledBotsing.2Botsingiswell-testedand
designedtobeeasilyextensiblefornewtechniques(newevolution-
ary algorithms, newgenetic operators, etc.). It relieson EvoSuite
[17],anevolutionary-basedunittestgenerationtool,forcodein-
strumentationand forthe internalrepresentation ofan individual
(i.e.,a test case) by using evosuite-client as a dependency.
For this study, we implemented the techniques used in previous
studies for crash reproduction (Single-Objective Search and De-
MO) in Botsing. Moreover, we implemented all of the MO-HO
approaches, which include the two fitness functions for our newhelper-objectives (method sequence diversity andtest length ) and
the five MOEAs mentioned above.
4.2 Setup
Crash Selection. Weselected ourcrashesfrom JCrashPack [ 11,
37], a collection of crashes from open-source projects and created
forcrashreproductionbenchmarking.Basedonthereportedresults
of the prior studies about search-based crash reproduction [ 37,38],
we know that Single-Objective Search and De-MOface various
challengestoreproducemanyofthecrashesinthisbenchmark.For
thisstudy, weapplyourapproachandstate-of-the-artalgorithms
to 124 crashes from JCrashPack, which are used in the recentsearch-based crash reproduction study [
12]. These crashes stem
fromsixopen-sourceprojects:JFreeChart,aframeworkforbuildinginteractivecharts;Commons-lang,alibraryprovidingextrautilities
to the java.lang API; Commons-math, a library for mathematical
andstatisticalusages;Mockito,atestingframeworkformocking
objects;Joda-time,alibraryfordateandtimemanipulation;XWiki,
a large-scale enterprise wiki management system.
AlgorithmSelection. Weattemptedtoreproducetheselected
crashesusingsevenevolutionaryalgorithms:Single-ObjectiveSear-ch,De-MO,andMO-HOwithfiveMOEAs(NSGA-II,SPEA2,PESA-II,
MOEA/D, and FEMO). For each crash, we ran each algorithm on
2Available at https://github.com/STAMP-project/botsing
215each frame of crash stack traces. We repeated each execution 30
timestotakerandomnessintoaccount,foratotalnumberof199,710
independent executions. We ran the evaluation on servers with 40
CPU-cores, 128 GB memory, and 6 TB hard drive.
Evaluationprocedure. InRQ1,weperformaninternalassess-
ment ofMO-HOby comparing all MOEAs to determine the best-
performing one when optimizing the search objectives in MO-HO.
Then,toanswerRQ 2andRQ3,weusethebest-performing MO-HO
configuration(MOEA)toevaluateitseffectivenessandefficiency
against the state-of-the-art crash reproduction approaches.
Parameter Settings. We set the search budget to five minutes,
assuggestedbypreviousstudiesonevolutionary-basedcrashre-
production[ 39].Also,wefixedthepopulationsizeandarchivesize
(ifneeded)to50individuals,asrecommendedinpriorstudieson
test case generation [ 33]. ForMO-HOwith PESA-II, the number of
bisectionsforgriddingissettothedefaultvalueoffivegrids.In MO-
HOwith MOEA/D, the weight vectors are obtained using a variant
simplex-lattice design [40] and using the Tchebycheff approach as
the aggregation function. Finally, we set the neighborhood selection
probability to0.2(settothedefaultvalue[ 15])andthemaximum
numberof solutionsthatcanbereplaced ineachgenerationto50.
ForallMOEAs,weusethe guidedmutation withmutationprobabil-
itypm=1/n(nis the length of the test case), and guided crossover
with crossover probability pc=0.8 (the same parameters used for
the suggested baselines).
4.3 Data Analysis
To evaluate the crash reproduction ratio (i.e., the percentage of
successful crash reproduction attempts in 30 rounds of runs) of
different algorithms, we follow the same procedure as the previous
studies[12,38]:foreachcrash C,wefindthehighestframethatcan
bereproducedbyatleastoneofthealgorithms( rmax).Weanalyze
the crash reproduction ratio of each algorithm for a target crash C
targeting frame rmax.
Tocheckwhethertheperformance(reproductionratio)ofMO-
EAs significantly differs from one another, we use the Friedmantest [
18]. The Friedman test is a non-parametric version of the
ANOVAtest[ 16],i.e.,itdoesnotmakeanyassumptionaboutthe
data distribution. It is a multiple-problem statistical test and hasbeen widely used in the literature to compare randomized algo-rithms [
21,34]. Friedman’s test allows to rank and statistically
comparedifferentMOEAsovermultipleindependentproblems,i.e.,
crashes in our case. For Friedman’s test, we use a level of signifi-cance
α=0.05.Ifthe p-valuesobtainedfromFriedman’stestare
significant ( p-values <=0.05), we apply pairwise multiple com-
parison using Conover’s post-hoc procedure [ 7]. To correct for
multiplecomparisonerrors,weadjustthe p-valuesfromConover’s
procedure using Holm-Bonferroni [20].
To answer RQ2, we need to determine whether an algorithm
reproduces a crash. Since we repeat each execution 30 times, weuse the majority of outcomes for a crash reproduction result. In
other words, if an algorithm could reproduce a crash in ≥15 runs
(i.e.,reproductionratioof ≥50%),wecountthatframeas reproduced.
Tocomparethenumberofreproducedcrashesbyeachalgorithm,
we used the same procedure used by Almasi et al.[2] and Campos
etal.[5]:wecheckcrashreproductionstatusandreproductionratioTable1:MOEAsranking(in MO-HO)intermsofcrashrepro-
duction ratio (Friedman’s test) and results of the pairwisecomparison ( p-value≤0.05)
Rank MOEA Rank value Significantly better than
1 SPEA2 2.63 (2), (3), (4), (5)2 PESA-II 2.86 (4), (5)3 NSGA-II 2.90 (4), (5)4 MOEAD 4.97 (5)5 FEMO 5.05
of the best-performing MO-HOalgorithm (according to the results
ofRQ1), Single-Objective Search, and De-MOat five time intervals:
1, 2, 3, 4 and 5 minute.
To evaluatethe efficiencyof thealgorithms ( RQ3), weanalyze
thetimespentbythebest MO-HOalgorithm,Single-ObjectiveSear-
ch, andDe-MOfor generating a crash reproducing test cases. Since
efficiencyisonlyapplicabletothereproducedcrashes,wecompare
the efficiency of algorithms on the crashes that are reproducedat least once by one of the algorithms. If, for one execution, analgorithm was not able to reproduce the crash, it means that it
consumedthemaximumallowedtimebudget(5minutes).Toassesstheeffectsizeofdifferencesbetweenalgorithms,weusetheVargha-DelaneyÂ
12statistic[41].AvalueofÂ 12<0.5forapairoffactors
(A,B)showsthat Areproducedthetargetcrashinashortertime,
whileavalueofÂ 12>0.5indicatestheopposite.Besides,Â 12=0.5
means thatthere isno differencebetween thefactors. Toevaluate
the significance of effect sizes (Â 12), we use the non-parametric
Wilcoxon Rank Sum test, with α=0.05 for the Type I error.
AreplicationpackageofourevaluationisavailableonZenodo
[14].Itcontainstheselectedcrashes,theresultsanddataanalysis
presentedinthispaper,aswellastheimplementationofMOEAs
in Botsing and a Docker-based infrastructure to enable the full-
replication of our evaluation.
5 RESULTS
This section presents the results of our empirical evaluation and
answers, one by one, our research questions.
5.1 Best MOEA for MO-HO(RQ1)
Figure2presentsthecrashreproductionratiooftheMOEAsappliedtoourMO-HOframework.Forthisanalysis,weconsiderthenumber
of times (in percentage) each MOEAs could reproduce a given
crash across 30 runs and using a search budget of five minutes. On
average (the squares in Figure 2), the best algorithm for MO-HOis
SPEA2, with an average and median of 76% and 100% of successful
reproductions,respectively. SPEA2isFollowedby PESA-II,NSGA-II,
andMOEAD. Also, this figure shows that the first quartile of the
crashreproductionratioof SPEA2is,atleast,about25%higherthan
other MOEAs.
According to Friedman’s test, the differences in reproduction
ratios are statistically significant ( p-value≤0.05). This means that
someMOEAsaresignificantlybetterthanotherswithinour MO-
HOframework. For completeness, Table 1 reports the ranking pro-
ducedbytheFriedmantest.Tobetterunderstandforwhichpairs
216●
● ● ● ● ● ● ● 0.000.250.500.751.00
FEMO MOEA/D NSGA −II PESA −II SPEA2
AlgorithmsReproduction Ratio (percent)
Figure 2: Crash reproduction ratio (out of 30 executions) of
MO-HOalgorithms. The upper and lower edge of each box
present the upper and lower quartile, respectively. (/square) de-notes the arithmetic mean and (—) is the median.
of MOEAS the statistical significance holds, we applied the post-
hoc Conover’s procedure for the pairwise comparison. The results
ofthecomparisonarealsoreportedinTable1.Accordingtothistable, the best-performing algorithm is MO-HO+SPEA2, which
has a significantly higher crash reproduction ratio compared to
otherMO-HOalgorithms.The nextalgorithmsare MO-HO+PESA-
IIandMO-HO+NSGA-II.Thesetwoalgorithmsaresignificantly
better than MO-HO+MOEADandMO-HO+FEMO. Finally, the
worst algorithmin termsof crashreproduction is FEMO, whichis
significantly worse than other MOEAs.
Summary (RQ 1).MO-HO + SPEA2 achieved the highest perfor-
mance in terms of crash reproduction ratio compared to MO-HO +other MOEAs. The next best-performing MOEAs, in terms of crash
reproduction, are PESA-II and NSGA-II.
5.2 Crash Reproduction (RQ2)
Figure3depictsthecrashreproductionratioofthebest-performing
MO-HOconfiguration(i.e.,with SPEA2),Single-ObjectiveSearch,
andDe-MOat five time intervals (search budgets). As indicated
in this figure, the average crash reproduction ratio of MO-HOis
higherthanotheralgorithmsatallofthetimeintervals.Also,the
median crash reproduction ratio for this algorithm is always 100%.
Furthermore,themaximumimprovementachievedby MO-HOwith
thefive-minutessearchbudgetisin XWIKI-14599 (with100%im-
provement) and MATH-3b (with 93.3% improvement) compared
to Single-Objective Search and De-MO, respectively. In contrast,
the largest reduction in reproduction ratio by MO-HO(with the
five-minutesbudget)isin XCOMMONS-1057 (with30%drop)and
XWIKI-13616 (with40%reduction)comparedtoSingle-Objective
Searchand De-MO,respectively.We willexplainthe negativefac-
tors inMO-HO, which lead to negative results for this algorithm in
some corner cases, in Section 5.4.
Moreover, wecan seethat De-MOis thesecond-bestalgorithm
in all of the time intervals. In the first 60 seconds of the crash
reproductionprocess,onaverage,itscrashreproductionratiois4%
better than Single-Objective Search. However, in contrast to the
othertwoalgorithms,thecrashreproductionratioofthisalgorithmchangesonlyslightlyafterthefirst120seconds.Hence,attheendof
the search process, the average crash reproduction ratio of De-MOis only 2% better than Single-Objective Search. In contrast, since
the crash reproduction ratio of MO-HOkeeps growing, on average,
it remains more effective than Single-Objective Search (about 10%)
even after 300 seconds. The other interesting point in Figure 3 isthe first quantile of MO-HO. In the first 60 seconds, this value is
lower than 12%, but it grows up to 62% after 300 seconds. This
improvement is not observable in state-of-the-art algorithms.
Furthermore, MO-HOis more stable in crash reproduction after
300 seconds budget compared to the other algorithms. Figure 3demonstrates that the interquartile range (i.e., the difference be-
tweenfirstandthirdquartile)ofcrashreproductionratioin MO-HO
withthe300secondsbudgetis46%smallerthantheinterquartile
rangeofotheralgorithms(being38.3%for MO-HO,76.6%.forSingle-
Objective Search, and 70.8% for De-MO).
Also, Figure 4 shows the number of crashes, which are repro-
duced by MO-HO, but not by the state-of-the-art algorithms and
vice versa in different time intervals. As indicated in this figure, in
all of the time intervals, the number of crashes that are reproduced
byMO-HOishigherthanthecrashesthatitcannotreproduce.In
thebestcase(after1minuteofsearch), MO-HOreproduceseight
and seven new crashes that cannot be reproduced by Single-Ob-jectiveSearchand De-MO,respectively.Incontrast,thereisonly
one crash that can be reproduced by De-MOand not by MO-HO.
Also,afterfiveminutes, MO-HOstillreproducesmorecrashesthan
thebaselines:itreproducesfiveandsixnewcrashes thatcannotbe
reproduced by Single-Objective Search and De-MO, respectively.
Thecrashesthatarereproducedby MO-HOafterfiveminutesbut
not by Single-Objective Search are: TIME-10b frame 5, XCOMMONS-
928frame 2, XWIKI-14227 frame 2, XWIKI-14475 frame 1, and
XWIKI-14599 frame1.Andthecrashesthatarereproducedby MO-
HOafterfiveminutesbutnotby De-MOare:MOCKITO-16b frame
4,TIME-5bframe 3, XWIKI-13377 frame 3, XWIKI-14227 frame 2,
MATH-3b frame 1, and MOCKITO-10b frame 1.
Figure 5 shows the crash’s stack trace reported in the issue
XWIKI-14227 .MO-HOis the only approach that can reproduce
the first two frames of this stack trace. Here, the target methodis
useMainStore (Figure6),whichdoesnothaveanyinputargu-
ment. Hence, to reproduce this crash, the crash reproducing test
generated by MO-HO(depicted in Figure 8) should invoke specific
methods (e.g., setWiki,setWikiId ) to set different local variables
inthe xwikiContext0 object,andthen,passthisobjecttotheclass
undertest(here, ActivityStreamConfiguration ).Sincethecrash
reproducingtestcasegeneratedby MO-HOdoesnotaddanyplu-
gin to the xWiki0object, the execution of this test indeed leads to
aNullPointerException thrown at line 5619 of the getPlugin
method in Figure 7. Generating such a specific test case requires a
searchprocesswithhighexplorationability,whichcangenerate
diverse test cases.
We do note that Single-Objective Search cannot even generate a
test case covering the target line (line 85 of the useMainStore
method). However, De-MOcan cover the target line thanks to
moretestgenerationdiversitydeliveredbytheapplicationofmulti-
objectivization.
Moreover,Single-ObjectiveSearchand De-MOreproducestwo
crashes that cannot be reproduced by MO-HOafter five minutes.
We will analyze these corner cases later in Section 5.4.
217●
● ● ● ● ● ● ●60 seconds budget 120 seconds budget 180 seconds budget 240 seconds budget 300 seconds budget
De−MO MO−HO Single De−MO MO−HO Single De−MO MO−HO Single De−MO MO−HO Single De−MO MO−HO Single0.000.250.500.751.00
0.000.250.500.751.00
0.000.250.500.751.00
0.000.250.500.751.00
0.000.250.500.751.00
AlgorithmsReproduction Ratio (percent)
Figure3:Crashreproductionratio(outof30executions)of MO-HOagainststate-of-the-artinfivedifferenttimeintervals.(/square)
denotes the arithmetic mean and (—) is the median.
●
●●
●●
●●
●●
●
01235678
60 120 180 240 300
Time budget (seconds)# of crashesMOHO.reproduces ● ● less more than●De−MO Single objective
Figure 4: Number of reproduced crashes only by MO-HOor
only by one of the state-of-the-art algorithms.
0 java.lang.Nu llPointerException: null
1 at [...].XWiki .getPlugin( XWiki.java:5619)
2 at [...].ActivityStreamConf iguration.useMainStore([...]:85)
3 [...]
Figure 5: XWIKI-14227 crash’s stack trace [37].
82public boolean useMainStore(){
83 XWikiContext context = contextProvider.get();
84 if(context.isMainWiki()) { return false ;}
85 ActivityStreamPlugin plugin = (
ActivityStreamPlugin) context.getWiki().
getPlugin [...] context); // <−−target line
86 }
Figure6:Method useMainStore appearsinthesecondframe
of the XWIKI-14227 crash’s stack trace.
In addition, after five minutes of crash reproduction, De-MO
reproduced six crashes, which are not reproduced by Single-Ob-
jective Search. Still, there are more crashes (seven) that can bereproduced by Single-Objective Search but not by De-MO. This5617public XWikiPluginInterface getPlugin ([...]){
5618 XWikiPluginManager pl ugins = getPluginManager() ;
5619 Vector<St ring> p luginlist = plu gins.getPlugins();
5620 [ . . . ]
5621 }
Figure7:Method getPlugin appearsinthefirstframeofthe
XWIKI-14227 crash’s stack trace.
1public void test0() throws Throwable {
2 ActivityStreamConfiguration ac0 = new
ActivityStreamConfiguration();
3 XWikiContext x WikiContext0 = newXWikiContext() ;
4 XWiki xWiki0 = newXWiki() ;
5 xWikiContext0.se tWiki(xWiki0);
6 xWikiContext0.se tWikiId("4~YRlfI >.U{ib");
7 Provider< XWikiContext> prov ider0 = ( Provider<
XWikiContext>) mock ([...]);
8 doReturn(xWikiContext0).when(prov ider0).get();
9 Injector. inject(ac0 ,[...] , " contextProvider", (
Object) provider0);
1011 // Undeclared excep tion!
12 ac0.useMainStore() ;13 }
Figure 8: Crash-reproducing test case generated by MO-HO
for the XWIKI-14227 crash.
resultshowsthatdespitethenewcrashesreproducedby De-MO,
this algorithm was counter-productive with respect to the total
number of reproduced crashes.
Summary (RQ 2).On average, MO-HO has the highest crash
reproduction ratio independently from the search budgets.
5.3 Efficiency (RQ3)
Figure9showsthetime(inseconds)neededbythe MO-HOandthe
state-of-the-art algorithms to successfully reproduce the crashes in
our benchmark. On average, the fastest algorithm is MO-HO, with
218●●● ● ● ● ● ● ● ● ● ● ● ● ● ●
●● ● ● ● ● ● ● ● ● ●
●● ●
●● ● ●
●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●
●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●
●● ● ● ● ●
●●
●● ● ● ● ● ● ● ● ● ● ● ● ● ● ●
●● ● ● ● ●●● ● ● ● ●
●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●
●●
●● ●
●● ●
●● ● ●
●● ● ● ● ● ● ●
●● ● ●
●●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●
●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●
●● ● ●
●●
●● ●
●● ● ● ● ●
●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●
●
●●● ●
●●
●●●
●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●
●● ●
●● ● ●
●● ● ● ● ● ●
●● ● ● ● ● ● ● ● ● ●
●
●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●
●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●
●●●● ● ● ●●● ● ● ● ●
●● ● ● ● ●
●● ● ● ● ● ● ●
●● ● ● ● ● ● ● ● ● ● ●
●● ● ● ● ● ● ● ● ● ● ● ●
●● ● ● ● ● ● ● ● ● ● ● ● ● ● ●
●● ● ● ● ● ● ● ●
●●
●●
●●
●
●●●
●●
●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●
●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●
●● ● ● ● ● ● ● ● ● ● ●
●● ● ● ● ●
●●
●● ●
●● ● ● ● ● ● ● ● ● ●
●● ● ● ● ● ● ● ● ●
●●
●●
●●
●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●
●● ●
●●● ● ● ● ● ● ● ● ● ● ● ●
●●
●●● ● ● ● ● ● ● ● ● ● ● ● ●
●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●
●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●
●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●
●●●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●
●● ● ● ● ●●● ● ● ● ● ● ● ●
●● ●
●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●
●● ● ● ● ● ●
●● ● ● ●
●● ● ● ● ● ●
●●
●● ● ● ● ● ● ● ● ● ● ● ●
●● ● ● ● ● ● ● ●●● ●
●● ●
●● ● ● ● ● ● ● ● ●
●● ● ●
●●
●●
●● ● ●
●
●● ● ● ● ●
●●
●●
●●● ● ● ● ● ●
●● ● ● ● ● ● ● ● ●
●
●● ● ●
●● ● ●
●● ● ● ● ●
●
●● ● ●
●● ● ●
●●
●● ● ● ● ●
●● ● ● ● ●
●●
●● ●
●● ● ● ●
0100200300
De−MO MO−HO Single
AlgorithmsComsumed Time (seconds)
Figure9:Overallbudgetconsumptioninseconds(log.scale).
(/square) denotes the arithmetic mean and (—) is the median.
Table 2: Pairwise comparison of the budget consumption
with a small (S), medium (M), and large (L) effect size Â12<
0.5and a statistical significance <0.05.
#(Â12<0.5)Single De-MO MO-HO
LMS LMS LMS
Single --- 7-4 1-2
De-MO 13 7 2 --- 32-
MO-HO 35 6 2 33 10 4 ---
an average search time of 71 seconds per crash replication. The
medianofitsrunningtimeislowerthan10seconds.Thesecondfastest algorithm is De-MOthat, on average, uses 84 seconds to
reproducethecrashes.TheslowestalgorithmisSingle-Objective
Search, which demands, on average, about 100 seconds.
Moreover, the biggest improvements achieved by MO-HOin
terms of efficiency are for XWIKI-14599, in which MO-HOrequires
only 3% of the time required by Single-Objective Search to achieve
crash reproduction, and MATH-3b, in which MO-HOrequires only
7% of the time required by De-MOto finish the crash reproduction
task.Howeve r,thebiggestefficiencylossesby MO-HOareinMATH-
81bwith 45 seconds drop (15% of time budget) and XRENDERING-
481with145secondsdrop(48%oftimebudget)comparedtoSingle-
Objective Search and De-MO, respectively.
Table2comparesthebudgetconsumptionofthealgorithmsfrom
astatisticalpointofview, i.e.,accordingtotheeffectsizes( Â12<
0.5) and statistical significance ( p-value <0.5). According to this
table,MO-HOisthefastestalgorithm:itsignificantlyreproduced
43(34.6%ofcrashes)and47(37.9%ofcrashes)crashesfasterthan
Single-Objective Search and De-MO, respectively. Most of these
significant improvements have large effect sizes (35 against Single-
Objective Search and 33 against De-MO). In cases that MO-HO
improves efficiency, on average, this algorithmdecreases the time
requiredforcrashreproductionby47%and58%comparedto De-MO
and Single-Objective Search, respectively.
Furthermore, Table 2 shows a few cases, in which MO-HOin-
creases the consumed time compared to the state-of-the-art: 3againstSingle-ObjectiveSearchand5against De-MO.Inmostof
these cases (7 out of 8), the crash reproduction process needs to
reproduce acrash with onlyone frame.Even the exceptionalcase
is a stack trace with three frames. In contrast, in cases that MO-HO
wins, we have many crashes with more frames (six frames, for0 java.lang.Ar rayIndexOutOfBoundsException: 2
1 at org.apache.commons.math.linear.BigMatrixImpl.operate(
BigMatrixImpl. java:997)
Figure 10: MATH-98b crash’s stack trace [24, 37].
991public BigDecimal[] operate(BigDecimal[] v) {
992 final int nRows = this.getRowDimension() ;
993 final int nCols = this.getColumnDimension() ;
994 finalBigDecimal[] out = newBigDecimal[v.length];
995 for(introw = 0; row < nRows; row++) {
996 . . .
997 out[row] = sum; // <−−target line
998 }
999 . . .
1000 }
Figure 11: Method operate appears in the first frame of the
MATH-98b crash’s stack trace [24, 37].
instance). Also, this table shows that De-MOis significantly slower
than Single-Objective Search in 11 crashes. Meanwhile, MO-HOis
only slow in reproducing three crashes. Hence, our proposed algo-
rithm reducesthe cases in whichthe multi-objectivization search
process is slower than the single objective search by 73%.
Summary (RQ 3).The fastest crash reproduction algorithm is
MO-HO with an average improvement in running time in 34.6% of
the crashes compared to the state of the art.
5.4 Corner cases analysis
Despitethenotableimprovementsachievedby MO-HO,thereare
few specific cases, in which Single-Objective Search or De-MO
outperform MO-HO. For instance, in Section 5.2, Single-Objective
Search and De-MOreproduce two crashes that are not reproduced
byMO-HO. Also,we observedin Section5.3 thatthe efficiencyof
these two algorithms is higher than MO-HOin 8 crashes.
Tounderstandwhy MO-HOiscounter-productiveinafewcases,
we performed a manual analysis to analyze the factors in MO-
HOthat negatively impact the crash reproduction process. Re-
sults of our analysis point to two adverse factors: extra overhead
in calculating the objectives (fitness evaluation) andhelper-
objectives misguidance.
Extracalculationinfitnessevaluation. Insomecases,crash
reproduction is trivial, and the search process reproduces it in a
few seconds. For instance, in TIME-8b [24,37], Single-Objective
Searchand De-MOreproducethecrashinaboutasecond.Thetime
required by MO-HOto reproduce this crash is three seconds (3
timesmore).Thisstemsfromthefactthatfitnessfunctionevalua-
tioninMO-HOismoretime-consumingthanthestate-of-the-art:
Single-ObjectiveSearchand De-MOneedtocalculateonlythecrash
distanceforeachtestcaseevaluation,while MO-HOneedstocalcu-
late the call diversity, as well. This extra calculation lengthens the
search process by a couple of seconds. In these cases, the increasedcrashreproductiontimeislowerthan5seconds,anditisnegligible
in practice.
Helper-objectivesmisguidance. Insomeothercases,thesce-
nario, which leads to crash reproduction, needs a simple sequence
219of methods calls to the target class. Still, the complexity of this
scenariostemsfromtheinputargumentsusedforthemethodcalls.
Inthesecases,sincecrashreproductiondoesnotneedthecalldi-
versity,methodsequencediversity objectivemisguidesthesearch
process. Alternatively, we need another objective for method input
argumentdiversity(i.e., improvesthediversityoftheinputargu-
ments for method calls). Adding new helper-objectives to consider
other aspects of diversity is part of our future agenda.
As an example, let us analyze MATH-98b (Figure 10), in which
MO-HO doubled the time consumed by the crash reproduction
search process against state-of-the-art. This crash concerns an
ArrayIndexOutOfBoundsException .Also,thiscrashhasonlyone
frame. For reproducing this crash, the generated test case needsto instantiate a class called
BigMatrixImpl and call a method
named operate (Figure 11) with precise input values. Method
getColumnDimension usedin operatereturnsthenumberofrows
inthedatavariable,whichhasbeensetintheconstructor.Tore-produce this crash, the generated test case should pass an array
withasizesmallerthanthepassedsizetotheconstructor.Inthis
case,methodargumentdiversitycouldhelpthesearchprocess,and
the method call diversity is not helpful.
6 DISCUSSION
6.1 Effectiveness and applicability
Generally, De-MOreproducessomecrashesthatcannotberepro-
ducedbySingle-ObjectiveSearchduetoitsimprovedexploration
ability, resulting from the multi-objectivization of the crash dis-
tance. However, since the decomposed objectives in this approach
dependononeanother(e.g., thestacktracesimilarityisnothelpful
ifthegeneratedtestdoesnotthrowthegiventypeofexception),
theymaymisguidethesearchprocessinvariouscases.Forinstance,
as we saw in Section 5.2, Single-Objective Search reproduces six
crashes that are not reproducible by De-MO.
Incontrast, MO-HOhasthree conflicting searchobjectives.From
the theory [ 22], the objective function must be conflicting to in-
creasetheoverallexplorationability.Ourresultsconfirmthetheory:
thechanceofthesearchprocessgettingtrappedinalocaloptimum
is lower by using MO-HOobjectives compared to the ones used
inDe-MO. As we observed in Section 5.2, after 1 minute of search,
MO-HOreproduces 8 and 7 crashes more than Single-Objective
Search and De-MO, respectively. Also, it continues outperforming
with larger search budgets (2, 3, 4, and 5 minutes) until the end
of the search process. It reproduces 5 and 6 crashes more than
Single-ObjectiveSearchand De-MO,respectively,whileitcannot
reproduce only two crashes, reproduced by the other algorithms.
Note that reproducing each crash needs a particular test case
whichdrivesthesoftwareundertesttoaparticularstate,andthen,it
callsamethodwithproperinputvariables.Toachievethisgoal,eachcrashreproducingtestcaseneedstocreatemultiplecomplexobjects.Hence,reproducingfivenewcrashes(4%ofcrashesavailableinour
benchmark) is a significant improvement for MO-HO.
6.2 Factors in the benchmark crashes that
impact the Success of MO-HO
There are multiple factors/characteristics of the crashes in our
benchmark that might impact the performance of our approachpositively.Weidentifythefollowingrelevantfactors:(1)thetype
of the exception (e.g., null pointer exception), (2) the size the stack
frames, (3) the number of classes involved in the crashes, (4) thenumber of methods of the deepest class in the crash stack. To
verify whether these factors influence the performance of our algo-
rithm,weusedthetwo-waypermutationtest[ 35].Thepermutation
test is a well-established non-parametric to assess the significance
offactorinteractionsinmulti-factorialanalysisofvariance(non-
parametric ANOVA). We usea significancelevel alpha=0.05 anda
verylargenumberofiterations(1,000,000)toensurethestability
of the results over multiple executions of the procedure [35].
For the sake of our analysis, we considered the difference in
crashreproductionratebetween MO-HOandthebaselinesasthe
dependent variable, while the co-factors are our independent vari-
ables. According to the permutation test, the type of exception ( p-
value=0.006) and the number of crash stack frames ( p-value=0.001)
significantlyimpacttheperformanceof MO-HOcomparedtoSingle
ObjectiveSearch.Wecanalsoobservesimilarresultswhenconsider-ingtheimprovementsof MO-HOagainstDe-MO:
p-values= <10−12
for both exception type and the number of frames). In other words,therearecertaintypesofexceptionsandstacktracesizesforwhich
MO-HOis statistically better than the state-of-the-art approaches.
Fromadeeperanalysis,weobservethatfor NullPointerExcep -
tionandorg.joda.time.IllegalFieldValueException ,MO-HO
achieves a higher reproduction ratio than Single Objective Search
when the stack traces contain up to three frames for NPE(+22% in
reproductionrate)anduptofiveframesfor IllegalFieldValueEx -
ception (+50% in reproduction rate). Instead, for stack traces with
more frames, the differences in reproduction ratio are negligible
(±1% on average) or negative (-10% in reproduction ratio). Besides,
MO-HOachieves better reproduction ratios for the following ex-
ceptions independently of the stack size: XWikiExceptions (+23%
on average), UnsupportedOperationException (+6% on average),
MathRuntimeException (+14% on average).
Finally,MO-HOoutperforms De-MOwhenreproducing NullPoin -
terException with1-3frames(+8%onaverage), ClassCastExcep -
tion(+8%onaverage), StringOutOfBoundsException (+18%with
more than 2 frames, on average), IllegalFieldException (+8%
on average), UnsupportedOperationException (+23% on aver-
age), MockitoException (+83%forshorttraces,onaverage),and
MissingMethodInvocation (+80% on average).
6.3 Crash reproduction cost
Inthisstudy,weobservedthatsince MO-HOincreasesthediversity
ofthegeneratedtestcases,itcandramaticallyimprovetheefficiency
ofcrashreproduction.Thisalgorithmsignificantlyimprovedthe
speed of the sear ch process in more than 36% of crashes compared
toSingle-ObjectiveSearchand De-MO.Incasesinwhich MO-HO
had a significant impact, it improves the crash repr oduction speed
by more than 47%.
The prior studies on search-based crash reproduction [ 37,38]
suggested5minutesasthesearchbudgetbecausethesearchpro-
cesscannotreproducemoreafter5minutes.However,weobservedthatdespitethehighefficiencyof MO-HO,thisalgorithmcontinues
to reproduce more crashes in the second half of the time budget.
220Section 5.2 shows that MO-HOkeeps increasing the crash repro-
duction ratio even in the last minutes of the search process, while
the previous multi-objectivization approach (De-MO ) changes only
slightlyafterthefirst2minutesofcrashreproduction.Hence,in-
creasingthesearchbudgetfor MO-HOcanleadtoahighercrash
reproduction ratio.
6.4 Extendability
Theimprovementachievedbytheproposedhelper-objectivesshows
the impact of suitable objectives on increasing the diversity of the
generated test cases and result in improving the effectiveness and
efficiency of the crash reproduction search process. Hence, wehypothesize that this approach can be extended by adding new
relevant helper-objectives.
7 THREATS TO VALIDITY
Internalvalidity. Wecannotensurethatourimplementationof
Botsingiswithoutbugs.However,wemitigatedthisthreatbytest-ingourtoolandmanuallyanalyzingsomesamplesoftheresults.Weusedapreviouslydefinedbenchmarkforcrashreproduction,which
contains 124 non-trivial crashes from six open-source projects and
applications. Moreover, we explained how we parametrized the
evolutionary algorithms in Section 4.2. We used the default values
of these algorithms in the other open-source implementations like
EvoSuite and JMetal. The effect of these values for crash repro-
duction is part of our future work. Finally, to take the randomness
ofthesearchprocessintoaccount,wefollowedtheguidelinesof
the related literature [ 3] and executed each evolutionary crash
reproduction algorithm for 30 times.
External validity. We report our results for only 124 crashes
introduced by JCrashPack [ 37], which is an open-source crash
reproductionbenchmarkcollectedfromsixopen-sourceprojects.
However, we recall here that we cannot guarantee that our results
are generalizable to all crashes. Evaluation MO-HOon a larger
benchmark from more projects is part of our future work.
Reproducibility. WeprovideBotsingasanopen-sourcepub-
liclyavailabletool.Also,thedataandtheprocessingscriptsused
to present the results of this paper, including the subjects of our
evaluation (inputs), the evolution of the best fitness function value
ineach generationof eachexecution,and theproducedtest cases
(outputs), are openly available as a docker image [14].
8 CONCLUSION AND FUTURE WORK
Crash reproduction can ease the process of debugging for devel-opers. Evolutionary approaches have been successfully used to
automate this process. Existing evolutionary-based approaches use
one single objective (i.e., Crash Distance ) to guide the search and
rely on guided genetic operators. Later strategies applied multi-
objectivizationviadecomposition( De-MO)inanattempttoimprove
diversity (and, therefore, e xploration). Ho wever, the latter strategy
may misguide the search process because the sub-objectives are
not strongly conflicting.
Inthisstudy,weapplyanewapproachcalledMulti-Objectivizati-
on using Helper-Objectives (MO-HO ) to tackle the problems of the
formertechniques.In MO-HO,multi-objectivizationisperformedby adding two helper-objectives that are in conflict with Crash Dis-
tance.We evaluated MO-HOwithfive MOEAs,which areselected
from different categories of multi-objective algorithms. Our results
indicatethat MO-HOisthemostefficientalgorithm,significantly
outperforming Single-Objective Search and De-MO. Also, this algo-
rithm is able to reproduce 8 and 5 more crashes in 1 and 5 minutes,
respectively,comparedtothestate-of-the-art.Moreover,incontrasttothepreviousmulti-objectivizedcrashreproductionapproach(De-MO), the crash reproduction ability of MO-HOincreases with large
search budgets (i.e., above two minutes).
Weperformedanadditionalanalysistofindthecorrelationbe-
tweenthedifferentaspectsofthecrashesandtheabilityof MO-HO
in reproducing them. The result of this analysis shows that two
factors in crashes significantly impact the performance of MO-HO:
(i) type of exception and (ii) the number of crash stack frames.
Furthermore, we observed that Single-Objective Search and De-
MOcouldoutperform MO-HObutonlyinafewcases.Weperformed
amanualanalysistocharacterizethenegativefactorsleadingtotheadverseresultsinthesecases.Ouranalysisrevealsthattwonegative
factors are at play in these cases: (i) extra calculations in fitness
evaluation and (ii) helper-objectives misguidance. We also showed
inSection5.4thatwhilethedifferencesin extracalculationsinfitness
evaluation are significant, they are often negligible in practice.
The contributions of the paper are as follows:
(1)Anopen-sourceimplementationofsevencrashreproduction
techniques (Section 4.1).
(2)An empirical comparison of seven search-based crash repro-
duction approaches (Section 4).
(3)An analysis of the benefits of multi-objectivization withhelper objectives in terms of reproduction ratio and effi-
ciency (Section 5).
(4)The identification of the special situations in which MO-HO
can be counter-productive (Section 5.4).
(5)Theidentificationofastrongcorrelationbetweentheability
ofMO-HOinimprovingtheefficiencyandeffectivenessof
crash reproduction for combinations of exception types and
thenumberofframesinthestacktraceofthetargetcrash
(Section 6.2).
In our future work, we will investigate additional helper-objecti-
vesforcrashreproduction.Forinstance,thecurrenthelper-objectiv-
esinMO-HOconcernthetestlengthandmethodsequencediversity.
However,furtherobjectivescanbeadded,suchastestinput/data
diversity. Increasing the number of objectives will require to evalu-
ate their performance using different many-objective evolutionary
algorithms. We will also analyze the evolution of the fitness values
of existing and new objective to further investigate the root causes
of goodand badperformances of MO-HOand otherobjectives for
different crashes and different MOEAs.
Moreover,thesearchobjectivesintroducedby De-MOisonlyop-
timizedby NSGA-IIMOEA.Asfuturework,wewillinvestigatethe
impactofutilizingotherMOEAsforoptimizing De-MOobjectives.
ACKNOWLEDGMENTS
This researchwas partially funded bythe EU ProjectSTAMP ICT-
16-10 No.731529.
221REFERENCES
[1]NasserMAlbunian.2017. Diversityinsearch-basedunittestsuitegeneration.In
InternationalSymposiumonSearchBasedSoftwareEngineering.Springer,183–189.
[2]M. Moein Almasi, Hadi Hemmati, Gordon Fraser, Andrea Arcuri, and Janis Bene-
felds.2017. Anindustrialevaluationofunittestgeneration:Findingrealfaults
in a financial application. In 2017 IEEE/ACM 39th International Conference on
SoftwareEngineering:SoftwareEngineeringinPracticeTrack(ICSE-SEIP).IEEE,
263–272. https://doi.org/10.1109/ICSE-SEIP.2017.27
[3]AndreaArcuriandLionelBriand.2014. Ahitchhiker’sguidetostatisticaltests
forassessingrandomizedalgorithmsinsoftwareengineering. SoftwareTesting,
VerificationandReliability 24,3(2014),219–250. https://doi.org/10.1002/stvr.1486
[4]Francesco A. Bianchi, Mauro Pezzè, and Valerio Terragni. 2017. Reproducingconcurrency failures from crash stacks. In Proceedings of the 2017 11th Joint
Meeting on Foundations of Software Engineering - ESEC/FSE 2017. ACM Press,
705–716. https://doi.org/10.1145/3106237.3106292
[5]José Campos, Rui Abreu, Gordon Fraser, and Marcelo d’Amorim. 2013. Entropy-
based test generation for improved fault localization. In 2013 28th IEEE/ACM
InternationalConferenceonAutomatedSoftwareEngineering(ASE).IEEE,257–267.
[6]Ning Chen and Sunghun Kim. 2015. STAR: Stack trace based automatic crash
reproductionviasymbolicexecution. IEEETrans.onSoftwareEngineering 41,2
(2015), 198–220. https://doi.org/10.1109/TSE.2014.2363469
[7]W. J. Conover and Ronald L. Iman. 1981. Rank Transformations as a Bridge
between Parametric and Nonparametric Statistics. The American Statistician 35,
3 (1981), 124–129. https://doi.org/10.1080/00031305.1981.10479327
[8]DavidW.Corne,NickR.Jerram,JoshuaD.Knowles,andMartinJ.Oates.2001.
PESA-II:Region-BasedSelectioninEvolutionaryMultiobjectiveOptimization.InProceedingsofthe3rdAnnualConferenceonGeneticandEvolutionaryComputation
(SanFrancisco,California) (GECCO01).MorganKaufmannPublishersInc.,San
Francisco, CA, USA, 283–290.
[9]Indraneel Das and J. E. Dennis. 1998. Normal-Boundary Intersection: A New
MethodforGeneratingtheParetoSurfaceinNonlinearMulticriteriaOptimization
Problems. SIAM J. on Optimization 8, 3 (March1998), 631–657. https://doi.org/
10.1137/S1052623496307510
[10]Kalyanmoy Deb, Amrit Pratap, Sameer Agarwal, and TAMT Meyarivan. 2002. A
fastandelitistmultiobjectivegeneticalgorithm:NSGA-II. IEEEtransactionson
evolutionary computation 6, 2 (2002), 182–197.
[11]Pouria Derakhshanfar and Xavier Devroey. 2020. JCrashPack: A Java Crash
Reproduction Benchmark. Zenodo. https://doi.org/10.5281/zenodo.3766689
[12]Pouria Derakhshanfar, Xavier Devroey, Gilles Perrouin, Andy Zaidman, and
ArieDeursen.2020. Search-basedcrashreproductionusingbehaviouralmodel
seeding.STVR30, 3 (may 2020), e1733. https://doi.org/10.1002/stvr.1733
[13]PouriaDerakhshanfar,XavierDevroey,AndyZaidman,ArievanDeursen,and
Annibale Panichella. 2020. Crash Reproduction Using Helper Objectives. In
Genetic and Evolutionary Computation Conference Companion (GECCO ’20 Com-
panion). ACM, Cancún, Mexico. https://doi.org/10.1145/3377929.3390077
[14]PouriaDerakhshanfar,XavierDevroey,AndyZaidman,ArievanDeursen,and
AnnibalePanichella.2020. Replicationpackageof"GoodThingsComeInThrees:
Improving Search-basedCrash Reproduction WithHelper Objectives". https://doi.
org/10.5281/zenodo.3979097
[15]Juan J Durillo, Antonio J Nebro, and Enrique Alba. 2010. The jMetal framework
for multi-objective optimization: Design and architecture. In IEEE congress on
evolutionary computation. IEEE, 1–8.
[16]RonaldAFisher.1936. Theuseofmultiplemeasurementsintaxonomicproblems.
Annals of Eugenics 7, 2 (1936), 179–188. https://doi.org/10.1111/j.1469-1809.1936.
tb02137.x
[17]GordonFraserandAndreaArcuri.2011.EvoSuite:Automatictestsuitegenerationforobject-orientedsoftware.In Proceedingsofthe19thACMSIGSOFTSymposium
andthe13thEuropeanConferenceonFoundationsofSoftwareEngineering (Szeged,
Hungary) (ESEC/FSE’11).ACM,NewYork,NY,USA,416–419. https://doi.org/
10.1145/2025113.2025179
[18]Salvador García, Daniel Molina, Manuel Lozano, and Francisco Herrera. 2008.
A study on the use of non-parametric tests for analyzing the evolutionary al-
gorithms’ behaviour: a case study on the CEC’2005 special session on real pa-rameter optimization. Journal of Heuristics 15, 6 (14 May 2008), 617. https:
//doi.org/10.1007/s10732-008-9080-4
[19]R. W. Hamming. 1950. Error Detecting and Error Correcting Codes. Bell System
Technical Journal 29, 2 (apr 1950), 147–160.
[20]S. Holm. 1979. A simple sequentially rejective multiple test procedure. Scandina-
vian Journal of Statistics 6 (1979), 65–70.
[21]Sadeeq Jan, Annibale Panichella, Andrea Arcuri, and Lionel Briand. 2017. Au-tomatic Generation of Tests to Exploit XML Injection Vulnerabilities in Web
Applications. IEEETransactionsonSoftwareEngineering i(2017),1–27. https:
//doi.org/10.1109/TSE.2017.2778711
[22]Mikkel T Jensen. 2004. Helper-objectives: Using multi-objective evolutionary
algorithms for single-objective optimisation. Journal of Mathematical Modelling
and Algorithms 3, 4 (2004), 323–347.
[23]Wei Jin and Alessandro Orso. 2012. BugRedux: reproducing field failures for
in-housedebugging.In 201234thInternationalConferenceonSoftwareEngineering(ICSE). IEEE, 474–484. https://doi.org/10.1109/ICSE.2012.6227168
[24]RenéJust,DarioushJalali,andMichaelD.Ernst.2014. Defects4J:Adatabaseofex-istingfaultstoenablecontrolledtestingstudiesforJavaprograms.In Proceedings
ofthe2014InternationalSymposiumonSoftwareTestingandAnalysis-ISSTA2014.
ACMPress,SanJose,CA,USA,437–440. https://doi.org/10.1145/2610384.2628055
[25]Marco Laumanns, Lothar Thiele, Eckart Zitzler, Emo Welzl, and Kalyanmoy Deb.
2002. Running time analysis of multi-objective evolutionary algorithms on asimple discrete optimization problem. In International Conference on Parallel
Problem Solving from Nature. Springer, 44–53.
[26]Vladimir Levenshtein. 1966. Binary codes capable of correcting deletions, inser-
tions, and reversals. In Soviet Physics Doklady, Vol. 10. 707–710.
[27]Phil McMinn. 2004. Search-based software test data generation: A survey. Soft-
ware Testing Verification and Reliability 14, 2 (2004), 105–156. https://doi.org/10.
1002/stvr.294
[28]Achille Messac, Amir Ismail-Yahaya, and Christopher A Mattson. 2003. The nor-
malized normal constraint method for generating the Pareto frontier. Structural
and multidisciplinary optimization 25, 2 (2003), 86–98.
[29]Kaisa Miettinen. 1999. Nonlinear Multiobjective Optimization: Kaisa Miettinen
(1st ed.). Springer US.
[30]Debajyoti Mondal, Hadi Hemmati, and Stephane Durocher. 2015. Exploring test
suite diversificationand codecoveragein multi-objectivetest case selection.In
2015 IEEE8th International Conferenceon Software Testing, Verificationand Vali-
dation(ICST) (ICST’15).IEEE,1–10. https://doi.org/10.1109/ICST.2015.7102588
[31]Mathieu Nayrolles, Abdelwahab Hamou-Lhadj, Sofiene Tahar, and Alf Larsson.
2015. JCHARMING:Abugreproductionapproachusingcrashtracesanddirected
model checking. In 2015 IEEE 22nd International Conference on Software Analysis,
Evolution,andReengineering(SANER).IEEE,101–110. https://doi.org/10.1109/
SANER.2015.7081820
[32]Mathieu Nayrolles, Abdelwahab Hamou-Lhadj, Sofiène Tahar, and Alf Larsson.
2017. A bug reproduction approach based on directed model checking and
crashtraces. JournalofSoftware:EvolutionandProcess 29,3(mar2017),e1789.
https://doi.org/10.1002/smr.1789 arXiv:1408.1293
[33]Annibale Panichella, Fitsum Meshesha Kifetew, and Paolo Tonella. 2018. Au-tomated test case generation as a many-objective optimisation problem with
dynamic selection of the targets. IEEE Transactions on Software Engineering 44, 2
(2018), 122–158. https://doi.org/10.1109/TSE.2017.2663435
[34]Annibale Panichella and Urko Rueda Molina. 2017. Java unit testing toolcompetition - Fifth round. Proceedings - 2017 IEEE/ACM 10th International
Workshop on Search-Based Software Testing, SBST 2017 (2017), 32–38. https:
//doi.org/10.1109/SBST.2017.7
[35]FortunatoPesarinandLuigiSalmaso.2010. Permutationtestsforcomplexdata:
theory, applications and software. John Wiley & Sons.
[36]Jeremias Rößler, Andreas Zeller, Gordon Fraser, Cristian Zamfir, and GeorgeCandea. 2013. Reconstructing core dumps. In Proc. International Conference
on Software Testing, Verification and Validation (ICST). IEEE, 114–123. https:
//doi.org/10.1109/ICST.2013.18
[37]Mozhan Soltani,Pouria Derakhshanfar,XavierDevroey, andArie van Deursen.
2020. Abenchmark-basedevaluationofsearch-basedcrashreproduction. Empiri-
calSoftwareEngineering 25,1(jan2020),96–138. https://doi.org/10.1007/s10664-
019-09762-1
[38]Mozhan Soltani, Pouria Derakhshanfar, Annibale Panichella, Xavier Devroey,
Andy Zaidman, and Arie van Deursen. 2018. Single-objective Versus Multi-
objectivizedOptimizationforEvolutionaryCrashReproduction.In Symposium
on Search-Based Software Engineering. SSBSE 2018. (LNCS), Thelma Elita Colanzi
andPhilMcMinn(Eds.),Vol.11036.Springer,Montpellier,France,325–340. https:
//doi.org/10.1007/978-3-319-99241-9_18
[39]MozhanSoltani,AnnibalePanichella,andArieVanDeursen.2018. Search-BasedCrash Reproduction and Its Impact on Debugging. IEEE Transactions on Software
Engineering (2018). https://doi.org/10.1109/TSE.2018.2877664
[40]Yan-Yan Tan, Yong-Chang Jiao, Hong Li, and Xin-Kuan Wang. 2012. A modifica-
tion to MOEA/D-DE for multiobjective optimization problems with complicated
Pareto sets. Information Sciences 213 (2012), 14–38.
[41]András Vargha and Harold D Delaney. 2000. A critique and improvement of
the CL common language effect size statistics of McGraw and Wong. Journal of
Educational and Behavioral Statistics 25, 2 (2000), 101–132.
[42]Matej Črepinšek, Shih-Hsi Liu, and Marjan Mernik. 2013. Exploration and ex-
ploitation in evolutionary algorithms: A survey. ACM Comput. Surv. 45, 3 (2013),
33. https://doi.org/10.1145/2480741.2480752
[43]MartinWhite,MarioLinares-Vásquez,PeterJohnson,CarlosBernal-Cárdenas,
andDenysPoshyvanyk.2015. Generatingreproducibleandreplayablebugre-
portsfromandroidapplicationcrashes.In 2015IEEE23rdInternationalConference
on Program Comprehension. IEEE, 48–59.
[44]JifengXuan,XiaoyuanXie,andMartinMonperrus.2015. Crashreproductionvia
test case mutation: Let existing test cases help. In Proceedings of the 2015 10th
JointMeetingonFoundationsofSoftwareEngineering-ESEC/FSE2015.ACMPress,
New York, New York, USA, 910–913. https://doi.org/10.1145/2786805.2803206
[45]Andreas Zeller.2009. Why ProgramsFail, SecondEdition: AGuideto Systematic
Debugging (2nded.). MorganKaufmannPublishersInc.,SanFrancisco,CA,USA.
222[46]QingfuZhangandHuiLi.2007. MOEA/D:Amultiobjectiveevolutionaryalgo-
rithmbasedondecomposition. IEEETransactionsonevolutionarycomputation
11, 6 (2007), 712–731.[47]Eckart Zitzler, Marco Laumanns, and Lothar Thiele. 2001. SPEA2: Improving the
strength Pareto evolutionary algorithm. TIK-report 103 (2001).
223