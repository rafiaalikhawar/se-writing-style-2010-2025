Test Automation in Open-Source Android Apps:
A Large-Scale Empirical Study
Jun-Wei Lin, Navid Salehnamadi, and Sam Malek
School of Information and Computer Sciences
University of California, Irvine, USA
{junwel1,nsalehna,malek}@uci.edu
ABSTRACT
Automatedtestingofmobileappshasreceivedsignificantattention
in recent years from researchers and practitioners alike. In this
paper, we report on the largest empirical study to date, aimed atunderstanding the test automation culture prevalent among mo-bile app developers. We systematically examined more than 3
.5
million repositories on GitHub and identified more than 12 ,000
non-trivialandreal-worldAndroidapps.Wethenanalyzedthese
non-trivial apps to investigate (1) the prevalence of adoption oftest automation; (2) working habits of mobile app developers in
regardstoautomatedtesting;and(3)thecorrelationbetweenthe
adoption of test automation and the popularity of projects. Among
others,wefoundthat(1)only8%ofthemobileappdevelopment
projectsleverageautomatedtestingpractices;(2)developerstend
tofollowthesametestautomationpracticesacrossprojects;and(3)
popular projects, measured in terms of the number of contributors,
stars,andforksonGitHub,aremorelikelytoadopttestautomation
practices. To understand the rationale behind our observations, we
further conducted a survey with 148 professional and experienced
developerscontributingtothesubjectapps.Ourfindingsshedlight
onthecurrentpracticesandfutureresearchdirectionspertaining
to test automation for mobile app development.
CCS CONCEPTS
‚Ä¢Software and its engineering ‚ÜíSoftware testing and de-
bugging.
KEYWORDS
Empirical Study, Automated Testing, Mobile Apps, Android
ACM Reference Format:
Jun-Wei Lin, Navid Salehnamadi, and Sam Malek. 2020. Test Automation
in Open-Source Android Apps: A Large-Scale Empirical Study. In 35th
IEEE/ACM International Conference on Automated Software Engineering (ASE
‚Äô20), September 21‚Äì25, 2020, Virtual Event, Australia. ACM, New York, NY, 
USA, 12 pages. https://doi.org/10.1145/3324884.3416623
1 INTRODUCTION
Testing is an indispensable phase of software development life
cycle. It is the primary way through which quality of software is
ASE ‚Äô20, September 21‚Äì25, 2020, Virtual Event, Australia
¬© 2020 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-6768-4/20/09.
https://doi.org/10.1145/3324884.3416623improved.Incomparisonwithmanualtesting,automatedtestingis
reportedtobemoreadvantageousforanumberofreasons,such
asreliability,repeatability,andexe cutionspeed ,especiallyinthe
context of continuous integration [ 16]. Since mobile apps are an
integralcomponentofourdailylifeandusedtoperformtasksin
criticalfieldssuchasbanking,health,andtransportation,automated
testingofmobileappshasreceivedsignificantattentioninrecent
years from researchers and practitioners alike.
Foranumberofresearchtopicsintheareaofmobilesoftware
engineering, such as automated program repair [ 5,55], automated
test transfer [ 6,40], mutation testing [ 15,30,41], regression test
management[ 9,31,32],andtestrepair[ 8,39,49],understanding
theextentmobiletestsexist,thetypeandqualityofthesetests,and
whether the tests are adopted in a particular way is of great impor-
tance.Forinstance,automatedprogramrepairofmobileapps[ 5,55]
is a plausible idea, only if apps come with a substantial number
of teststo ensurethe repairsare notbreakingtheir functionality.Similarly, automated test transfer [
6,40] is going to yield good
results, only if there is a large number of apps with tests, such that
tests can be migrated from one app to another. In addition, mobile
developers care about why and how to adopt automated testing
practicesandparticularly,whethersuchadoptionimpactstheover-allqualityoftheirappsandwaysinwhichtheirappsareperceived
bythedevelopercommunity.Therefore,aholisticviewregarding
practicaladoptionoftestautomationinmobileappdevelopment
can contribute to both academia and industry.
To understand the test automation culture prevalent among
mobile app developers, researchers have investigated the extent
towhichtest automationisadoptedin practice[ 12,13,33,37,42].
However,thosestudiesarelimitedintermsofbothscaleandqualityofthecurateddataset.First,mostpriorworkshaveonlyconsidered
hundredsofappsfromasinglesource,i.e.,F-Droid.Thefindings
andconclusionsdrawnfromarelativelysmallsetofsampleapps
may not generalize to the overall app ecosystem.
Second, previous studies have failed to exclude dummy and
invalid tests; an important factor that might severely affect theirconclusion. That is, when developers create a new project with
Android Studio, the official IDE for Android app development, it
generates some example test cases which are irrelevant for the
created app. Including these default tests may influence the results
ofresearchquestionsastotheadoptionoftestautomationpractices.
Finally, appropriate and representative subjects are of critical
importanceforanempiricalstudy.Inthecaseoftestautomation
for Android apps, a practical inclusion criterion is to consider only
non-trivial apps,sinceitisnotcost-effectivetowritetestsfortrivial
apps such as class assignments, tutorials, or simple apps with only
one component. Studying trivial apps cannot reveal useful insights
10782020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE)
This work is licensed under a Creative Commons Attribution International 4.0 License.
into the adoption of test automation practices. Nevertheless, no
previous study has focused exclusively on non-trivial apps.
In this paper, we report on a large-scale empirical study on
open-source Android apps from GitHub from three complemen-
taryperspectives:apps,developers,andimpacts.Wesystematically
examined more than 3 .5 million non-forked repositories in Java
andKotlin,andinvestigatedmorethan12 ,000real-worldappsto
determine(1)theprevalenceoftestautomationinmobileappde-
velopment projects; (2) working habits of mobile app developers
with respect to automated testing; and (3) the correlation between
theadoptionoftestautomationandthepopularityofprojectsin
termsofdifferentmetrics,suchascontributorsandstarsonGitHub,
and ratings on Google Play Store.
Two important contributions of our work are the scale of study
andthewaywehavecuratedthedataset.First,weconsideredmore
than 12 ,000 apps across 16 app markets including Google Play
Store, F-Droid, and PlayDrone. We also developed novel heuristics
to exclude irrelevant and example tests in data collection and anal-
ysis.Lastly,thesubjectappswereselectedaccordingtoacriteria
designatedforidentifyingnon-trivialapps(detailedinSection4).
AspresentedinSection5,theseeffortsledtofindingsthatarequite
different from prior work.
Another contribution of our work is that we considered both
unit tests and UI tests. Given the interactive nature of mobile apps,
UI testing, which requires an emulator or a real device to run, is
the primary way to examine the functionality and usability of mo-
bileapps.Therefore,inadditiontounittests,weareinterestedin
whether and how automated UI tests are adopted by mobile devel-
opers. We discuss related research questions such as developers‚Äô
preference for unit and UI testing and their compliance with the
Testing Pyramid practice [27] in Section 5.
To gather a deeper understanding of the underlying reasons for
our observations from the source code, we further conducted a
surveywiththecontributorsofthesubjectapps,andendedupwith
148 responses mainly from professional and experienced develop-
ers.Interestingly,withrespecttosomeoftheresearchquestions,
the results obtained from the analysis of project data and survey
responses areinconsistent, indicating agap between whatthe de-
velopers believe they do versus what they actually do.
Overall, this paper makes the following contributions:
‚Ä¢Wereportonthefirstlarge-scaleanalysisfocusingonnon-trivial
appsinover12 ,000open-sourceprojectsfrom16appmarkets
andspanningaperiodof5years,toinvestigatehowtestautoma-
tion is practically adopted.
‚Ä¢We present the working habits of mobile app developers regard-
ingtestautomation,suchasthetendencytowritetestsorlack
thereof and the compliance with the Testing Pyramid practice.
‚Ä¢Wediscusshow thepresenceofautomatedtests,anditsextent,
impact the popularity of apps in terms of different metrics on
GitHub and Google Play Store.
‚Ä¢Wepresentthefindingsofasurveyinvolving148practitioners
whodevelopedthesubjectappstounderstandtherationalebe-
hindourobservationsaswellasthechallengesinAndroidapp
testing.
‚Ä¢We create a publicly available dataset for this study [ 14]. The
dataset was built by referring to multiple data sources includingpublic class ExampleInstrumentedTest {
@Test
public void useAppContext() {
Context appContext = InstrumentationRegistry
.getInstrumentation().getTargetContext();
assertEquals("com.example",
appContext.getPackageName());
}
}
public class ExampleUnitTest {
@Testpublic void addition_isCorrect() {
assertEquals(4,2+2 ) ;
}
}
Figure1:ExampleTestClassesGeneratedbyAndroidStudio
GitHub,GooglePlayStore,F-Droid,andAndroZoo.Webelieve
the dataset can be of great utility for researchers working in the
aforementionedresearchareas (e.g.,automated programrepair,
automated test transfer, mutation testing) that need access to
mobile apps with tests.
The remainder of this paper is organized as follows. Section 2
provides a background on mobile app test automation, followed
by a brief review of prior research efforts in Section 3. Section 4presents our approach for data collection, subject selection, and
developer survey. Section 5 details our findings. Section 6 outlines
theimplicationsofthisstudyforresearchersandpractitioners.The
paper concludes with a discussion of threats to validity and future
work.
2 TEST AUTOMATION IN ANDROID
2.1 Unit and UI Tests
Given the interactive nature of mobile apps, there are roughly two
types of tests in Android: unit tests and UI tests.1According to the
definition from Google [ 27], unit tests are small tests that ‚Äúvalidate
the app‚Äôs behavior one class at a time‚Äù. In contrast, UI tests or end-
to-end tests are medium or large tests that ‚Äúvalidate user journeys
spanningmultiplemodulesoftheapp‚Äù.Thekeydifferencebetween
unit and UI tests, besides the scope of testing, is that unit tests
run on a local machine with JVM, while UI tests need an emulated
or real device to run, and almost always use the Android OS or
Android framework.
InAndroidStudio,theofficialIDEforAndroidappdevelopment,
unit and UI tests are clearly separated‚Äîthey are placed in differ-
ent directories. The tests in the testfolder are unit tests that run
locally on JVM. The tests in the androidTest folder are UI tests that
require an emulator or real device to run. These two directories
are automatically generated when developers create a new project
with Android Studio. In this study, we consider the tests under the
testfolderasunittests,andthetestsunderthe androidTest folder
as UI tests.
AfeatureofAndroidStudiohighlyrelatedtoourstudyisthat,
when developers create a new project, it generates not only the
folders,butalsoexamplesfordifferenttypesoftests.Bydefault,the
1Sometimes they are called local tests and instrumented tests [21].
1079Figure 2: Illustration of the Testing Pyramid Practice
from [27]
testfolder contains a class called ExampleUnitTest.java, and the an-
droidTest foldercontainsaclasscalled ExampleInstrumentedTest.java ,
as shown in Figure 1. They are executable examples of unit and UI
tests to help developers get started with test automation. However,
includingtheseexamplefilesmayresultinoverestimatedconclu-
sions for research questions about the prevalence or adoption of
automatedtests,becausedevelopersmayaccidentallycommitthese
files without an intention to write automated tests. In this study,
we exclude these example files when counting number of tests
contained in an app.
2.2 The Testing Pyramid Practice
TheTestingPyramidisamindsetorpracticetoguidedevelopers
in terms of how much effort they should put on creating different
kindsofautomatedtests[ 1,11,18,27,47].Itessentiallysaysthat
developershavetobalancetheirautomatedtestsbyhavingmany
morelow-levelunitteststhanhigh-levelUItests,asillustratedin
Figure 2.
There are many reasons to follow the Test Pyramid practice.
First,unittestsmakedebuggingeasierbecausetheyfocusonsmall
modules that can be tested independently. When unit tests fail,
developerscanquicklypinpointtherootcauseoffailureandsavea
lot of time. On the other hand, if there is a failure reported by a UI
test,itusuallymeansthatthecorrespondingunittestsareincorrectormissing.Furthermore,unittestsaremorerobustandrunfasterin
general, while UI tests may be subject to flakiness [ 45] and almost
always run slower. As a result, while UI tests are still important to
validateend-to-endworkflows,overlyrelyingonthemwillmake
testing expensive, slow, and brittle.
Although the proportion of tests for each layer in the Testing
Pyramid varies based on different apps, a general recommendation
from Google is a 70/20/10 split: 70% unit tests, 20% integrationtests, and 10% UI tests [
27]. Note that, while there is a layer of
integrationtests,andtheycanbeunderstoodasteststhat‚Äúvalidatethecollaborationandinteractionofagroupofunits[
27]‚Äù,thescope
for integration tests is controversial [ 35]. In fact, these three layers
arenottotallyclear-cutandsometimesoverlapwitheachother[ 48].
In this paper, we leverage the characteristics of Android apps and
AndroidStudiotoidentifythetwomajortypesoftests,unitandUI
tests.Furthermore,accordingtotheaboveguideline,anappropriate
ratio of UI tests could be 20% to 30% of the total number of tests.3 RELATED WORK
Empiricalstudiesonmobileapptesting. Previously,researchers
haveinvestigatedhowtestautomationispracticallyadopted[ 12,
13,33,37,42,43,50].Kochharetal.[ 37]analyzedover600Android
apps on F-Droid to check the presence of test cases and computed
the code coverage. They also conducted surveys to understand the
usage of automated testing tools and the challenges faced by devel-
operswhiletesting.Cruzetal.[ 13]analyzed1,000Androidapps
onF-Droidtochecktheirusageofautomatedtestingframeworks
and continuous integration tools. They also found that projects
usingautomatedtestinghavemorecontributorsandcommitson
GitHub. Recently, Fabiano et al. [ 50] analyzed 1,780 Android apps
onF-Droidtoinvestigatetheprominenceoftestsdevelopedfortheapps,aswellasotherqualitymetricsofthetestssuchastestsmells,
code coverage, and assertion density. Our work is different fromtheirs in terms of the scale and data source, as we analyzed over
12,000 apps across 16 app markets.
In addition, Coppola et al. [ 12] analyzed more than 15,000 apps
on GitHub to examine the diffusion, evolution, and modification
causesofUItestsinopen-sourceAndroidapps. Whiletheirwork
ishighlyrelatedtoours,thekeydifferenceisthatwefocusononly
non-trivial apps as they did not factor out toy apps and forks of
realappsfromtheirdataset.Forexample,amongthelistof1,042
repositories with tests released by the authors2, only 42 (4%) of
them are considered in our study. That means our study considers
a very different set of apps from theirs.
Ontheotherhand,toknowthemainchallengesthatdevelopers
face while building mobile apps, Joorabchi et al. [ 33] conducted
a qualitative study with 12 mobile developers from 9 companies,followed by a survey with 188 respondents. Linares-V√°squez et
al.[42]alsoanalyzedresponsesfrom102open-sourceAndroidapp
developers to understand their practices and preferences regarding
Androidapptesting.Unlikeourwork,thesepapersdidnotanalyze
open-sourcedatainthewildandmerelyreliedoninterviewsand
survey responses.
Finally, Linares-V√°squez et al. [ 43] reviewed the frameworks,
tools,andservicesforautomatedmobiletesting,andtheirlimita-
tions. From a survey, they identified several key challenges thatshould be addressed in the near future by the researchers in thearea of mobile test automation. Nevertheless, their work did not
include any source code analysis or developer survey.
Another related topic is the empirical study on automated in-
put generation (AIG) tools [ 10,56,57]. The work by Choudhary et
al.[10]focusedonthecomparisonofdifferentAIGtoolsinterms
their usability, compatibility, code coverage and fault detection ca-
pability. Another empirical study by Wang et al [ 56] performed
asimilarcomparisonofAIGtoolsbutfocusedonindustrialapps.
Zeng et al. [ 57] further investigate the limitations of Android Mon-
key, the most widely used AIG tool, in an industrial setting with a
popularandcommercialmessengerapp.Ourworkdoesnotcon-
sider AIG tools, rather focuses on automated or scripted test cases
created by developers.
Empiricalstudiesonopen-sourcesoftwaretesting. Anum-
ber of studies investigate the test adequacy in general open-source
2We have contacted the authors to ask for the complete list of repositories under their
study but get no response.
1080Figure3:FlowofDataCollectionandAnalysisinThisStudy
software. Kochhar etal. [ 36] studied morethan 20,000 projects on
GitHubregardingtheir adoption oftesting,andthecorrelationof
test cases with various project development characteristics such
as project size and number of bugs. To answer questions related
totheusage,costs,andbenefitsofcontinuousintegration,Hilton
et al. [29] analyzed more than 34,000 projects on GitHub. Fraser
and Arcuri [ 19] empirically evaluated the code coverage ability of
EvoSuite, a search-based testing tool, with public classes retrieved
from100JavaprojectsfromSourceForge.Ontheotherhand,Belleretal.[
7]reportedafieldstudywith416softwareengineersinwhich
theirdevelopmentactivitywasmonitoredwithanEclipsepluginto
understandhowandwhendevelopersconducttesting.Ourwork
complements these studies by providing insights in the context of
Android app development.
4 METHODOLOGY
Figure3depictstheflowofdatacollectionandanalysisinourstudy.
This study consisted of the following steps: (1) we first collected a
large list of GitHub repositories from the GHTorrent database [ 28];
(2) we set filtering criteria to identify the repositories representing
non-trivial Android apps; (3) we further analyzed the identified
repositories to collect their meta-data and information about auto-
matedtestsandpopularity;(4)weevaluatedthecollecteddatasettoanswerresearchquestionsaboutthetestautomationculturepreva-
lent among mobile app developers; and finally (5) we conducteda survey with the developers of the subject apps to get a deeper
understandingoftheunderlyingreasonsforourobservationsfrom
the dataset. We now describe each of these steps in further detail.
4.1 Study Subjects and Selection Criteria
TheinitiallistofGitHubrepositoriesforourstudywasobtained
fromtheGHTorrentdatabase[ 28];aresearchprojectthatmonitors
theGitHubpubliceventtimelineandpopulatesarelationaldata-
basewiththecollectedinformation,i.e.,meta-data.Wedownloaded
the latest dump of their database [ 20], and queried the repositories
writteninJavaorKotlinthatareneitherforkednordeleted.The
query returned a list of more than 3.5 million repositories.
Toidentifytherepositoriesofnon-trivialandreal-worldAndroid
appsfrom thereturned list,weset thefollowing selectioncriteria:
(1)Therepositorymustcontainexactlyone AndroidManifest.xml.
Themanifestfileisamust-haveforeveryAndroidapptoprovide
essential information about the app to the Android build tools [ 23].The reason for exactly one manifest file is that the repository con-
taining multiple such files is likely a tutorial or class assignment
withmultipledemoapps.WeusedGitHubAPItowalkthroughthe
directory tree of the projects to search for the files.
(2)Therepositorymustcontain build.gradle withaspecific
string ‚Äúcom.android.application ‚Äù inside. Android Studio uses
Gradle as its build system, and a Gradle plugin with this specific
stringmeansthatthisprojecthasatasktobuildanAndroidapp.WeusedGitHubAPItosearchtheprojectswiththespecifiedcondition.
Mostoftherepositorieswerefilteredoutwiththesetwocriteria,
with about 537 thousand apps left.
(3)At least two components have to be declared in the manifest
file.Weparsedthemanifestfileandlookedforthedeclarationof
four Android component types (i.e., Activity, Service, Broadcast
Receiver,andContentProvider[ 24])inside.Wesetathresholdof
2 components because we believe it is not cost-effective to write
tests for a simple app with only one component. About half of the
apps were removed by this step, with 287 thousand apps left.
(4)The package name stated in the manifest file must appear in
an app market. We believe that the apps published in app markets,
especially the markets that charge fees to join such as Google
PlayStore,aremorelikelybeyondtoyordemoapps,becausethe
developers want the apps to reach general users (and even willing
to pay for it). From the manifest file of each app, we retrievedthe package name and tried to match it with apps hosted in the
followingappmarkets:GooglePlayStore,F-Droid[ 17],andthelist
of package names and markets provided by AndroZoo [ 3].3This
criterion was critical to identify non-trivial apps and left us with a
list of about 19 thousand apps.
(5)We removed the apps with duplicate package names, and
ended up with a list of 14 ,914 GitHub repositories of non-trivial
Android apps.4
The above filtering process took two months, primarily because
of the rate limit of GitHub API (5,000 requests per hour).
4.2 Data Collection and Analysis
Foreachoftheselectedrepositories,weusedGitHubAPItofurther
collect its meta-data: creation date, number of forks, number of
stars,numberofcommits,numberofcontributors,numberofissues,
and number of pull requests. If the app is on Google Play Store,
we also collected its category and user ratings by crawling the app
page.
Tocollecttheinformationabouthowtestautomationisadopted
intheproject,weusedGitHubAPItowalkthroughthedirectory
tree of the project, and parsed all the files under the testandan-
droidTest folders, if any exist. We considered a method as a test
case if it is annotated with ‚Äú@Test‚Äù. This annotation is used by
JUnit-based testing frameworks, including both unit and UI test-
ingframeworkssuchasJUnit[ 34],Robolectric[ 52],Mockito[ 46],
and Espresso [ 26]. A prior study investigating the usage of testing
frameworks in 1 ,000 apps on F-Droid [13] shows that 100% of the
adoptedunittestingframeworksand97%oftheUItestingframe-
works are JUnit-based. Furthermore, we classify a test case as a
3A list of app markets considered by AndroZoo can be found at [4].
4Sometimes two repositories contain the same package name because one is a direct
copyoftheother(notbyforking).Inthissituation,wekeeptherepositorywiththe
oldest creation date.
1081Table 1: Distribution of Apps by App Market*
Market* #Apps
Google Play 11265
PlayDrone 539
fdroid 434
anzhi 408
appchina 294
mi.com 70
VirusShare 62
angeeks 41
1mobile 26
freewarelovers 12
slideme 10
torrents 4
praguard 3
hiapk 2
proandroid 1
apk_bang 1
*An app may belong to multiple markets
Table 2: Distribution of Apps by Year Created
Year Created #Apps
2015 3614
2016 2330
2017 17312018 28982019 1989
Total 12562
unit test if it is under the testfolder, and otherwise as a UI test (i.e.,
underthe androidTest folder).Finally,asmentionedinSection2.1,
we excluded the example unit and UI test generated by Android
Studio.
An assumption of our study is that the subject apps were devel-
oped with Android Studio. Because Android Studio has been the
officialIDEforAndroidappdevelopmentsinceitsfirststablerelease
in December 2014 [ 22], we further factored out the repositories
before 2015 from the list described in Section 4.1. We finally ended
upwith12 ,562repositories/appsinourdataset.Thedistribution
ofappsbyappmarketisshowninTable1.Whilethemajorityof
theappswerepublishedonGooglePlayStore,thedatasetcovers
apps across 16 app markets. Table 2 shows the distribution of apps
bytheyeartheywerecreated.FortheappsonGooglePlayStore,
Figure 4 shows the distribution by category.
4.3 Survey
To complement our findings, we conducted an online survey with
the developers of the subject apps in our dataset. In this section,
we describe the design, participant selection, and data collection of
the survey.
4.3.1 Survey Design. The online survey was designed to under-
stand the rationale behind our findings from the dataset as well as
the challenges in Android app testing. We first asked demographic
questionstounderstandtherespondents‚Äôbackground,suchastheir
Figure 4: Distribution of the Google Play Apps by Category
experiences in terms of the number of years of Android app de-
velopment. We then asked them about their current practices of
Android app testing. For the respondents reporting the use of auto-
matedtests,wefurtheraskedthemrelatedquestionssuchasthe
preferenceforunitandUItestingandwhethertheyfollowtheTest-
ingPyramidpractice,andthereasonsfortheirchoices.Next,we
presented some of our findings in the correlation analysis between
the adoption of test automation and the popularity of apps, and
askedfortheiropinionsonpossibleexplanations.Finally,weaskedtherespondentsforthedifficultiesinadoptingautomatedtestsand
general challenges of testing Android apps. For all questions about
practicesandopinions,weprovidedasetofchoicesidentifiedfrom
previous studies [ 13,37,42], as well as an ‚Äúother‚Äù choice with free
form text if none of the provided choices apply. A sample of the
survey can be found at the companion website [14].
Toensurethatthequestionswereclearandthesurveycanbe
finished in 10 minutes, we conducted a pilot survey with graduate
students in Computer Science who have experience in Android
app development and survey design. We rephrased some questions
according to the feedback. The responses from the pilot survey
wereusedsolelytoimprovethequestionsandwerenotincluded
in the final results.
4.3.2 Participant Selection. From eachsubject appin our dataset,
wetriedtoretrievetheemailofitsmaincontributorinthefollowing
order: (1) the email found in the GitHub profile of the repository‚Äôs
owner;(2)theemailofthecontributorwhomadethemostcommits;
and (3) the email of the contributor who made the most recent
commit.Afterremovinginvalidandduplicatedata,weidentified
7,490 unique email addresses for our survey.
4.3.3 Data Collection. We used Qualtrics [ 51] to distribute the
survey to the 7 ,490 targeted email addresses, and 653 of them
bounced. From the 6 ,837 emails successfully sent, we received
148 valid and complete responses with a 2.2% response rate. The
responserateisclosetotheresultsofpreviousstudiessuchas2.1%
(83/3905) reported in [ 37] and 1.0% (102/10000) reported in [ 42]o n
very similar surveys with mass developers on GitHub.
The 148 received responses are from 45 countries. The toptwo
countrieswheretherespondentsresideareUnitedStatesofAmerica
(22.3%)andIndia(10 .8%).70 .3%oftherespondentsareprofessional
software developers paid by a company, and 68 .9% of them have
more than 2 years of experience in Android app development.
1082Table 3: Distribution of Apps in Terms of Presence of Test
Cases
Group #Apps Percentage
Appswith any tests 1002 7.98%
Apps without tests 11560 92.02%
Apps with unit tests 766 6.10%
Apps with UI tests 502 4.00%
Apps with both unit and UI tests 266 2.22%
5 RESULTS
In this section, we present the results of our study from three
complementary perspectives: apps, developers, and impacts.
5.1 App Perspective
Westartedbyanalyzingourcurateddatasettounderstandthestate
of affairs with respect to test automation adoption in open-source
projects.Answerstothesequestionsareimportantforemergingar-
eas of research interest (e.g., automated program repair, automated
test transfer) that rely on the availability of large number of tests.
RQ1. How prevalent is test automation in open-source An-
droid apps, in terms of the presence of unit and UI
tests?
Table3showsthenumberofrepositoriesgroupedbythepres-
ence of different types of tests. The results indicate that only 7.98%
ofthesubjectappscontaintests,andmostofthemarepoorlytested
inanautomatedmanner‚Äîeventhoughtheyarenon-trivial.This
percentageismuchlowerthanpreviousfindings:20%reportedin
[12], 14% reported in [37], and 40% reported in [13].
There are many possible reasons for the inconsistency between
ourresultsandpreviousfindings.First,ouranalysisexcludesthe
placeholder tests that are automatically generated by Android Stu-
dio, as mentioned in Section 2.1. This check was critical for ourresults, since such tests are common in our dataset (7,017 of the
12,562 apps examined, 56%). We also manually checked the dataset
released by Coppola et al. [ 12], and found such examples in the
reportedtestcases.Wearenotabletoverifytheresultsreported
by Kochhar et al. [ 37] because they are not willing to release their
dataset. Regarding the results reported by Cruz et al. [ 13], since
theydidnotsearchfortestcases(detailedinthenextparagraph),
we are unable to compare their results with ours.
The way one computes the existence of tests can also influence
theresultssignificantly.Forexample,inthestudybyCruzetal.[ 13],
theyinspectthebuildconfigurationsandlookforimportsrelated
totestingframeworkstodeterminethepresenceoftestsinarepos-itory. Since having related imports in the build configurations does
notnecessarilymeantherearetestcasesintheproject,theirfind-
ingsaboutprevalenceoftestautomationispronetooverestimation.
Ourinclusioncriterionforsubjectsaredifferentfrompriorstud-
iestoo.Weexcludedthetrivialapps(i.e.,simple/demoappswith
only one component), which is not the case with all prior studies.
Finally, the scale of study might also affect the results. In the
papers by Kochhar et al. [ 37] and Cruz et al. [ 13], only 627 and
1,000 apps from F-Droid were analyzed, respectively. In contrast,
our study considers more than 12,000 apps on GitHub across 16
Figure 5: Prevalence of Test Automation of the Google Play
Apps by Category
markets, which is substantially different from their works in terms
of scale and source of data.
Another finding from Table 3 is that UI testing is not adopted as
extensivelyasunittesting(i.e.,4%vs.6.1%).Wewillfurtherdiscuss
this in Section 5.2.


Observation 1: Only 8% of the non-trivial and real-world apps
have automated tests. Automated UI testing is less adopted than
unit testing.
RQ2. Istheprevalenceoftestautomationvariedacrossdif-
ferent categories of apps?
Tounderstandwhetherthereareanypatternsastotheadoption
ofautomatedtestingpracticesacrossdifferentcategoriesofapps,for
the Google Play apps with category information in our dataset, we
reporttheiradoptionofautomatedtestsbycategoryinFigure5.AsdepictedinFigure5,whileoveralltheprevalenceoftestautomation
is 8%, the percentage is substantially higher for some categories of
appssuchasfinance(19%)andvideoplayers(15%).Ontheotherhand, some categories of apps such as shopping (3%) and dating
(0%) are poorly tested in an automatic manner. This variance could
be attributed to the quality requirements for different categories.
Notethattheobservedpatternsmaynotgenerallyapplytoapps
on Google Play Store, since many commercial and closed-source
apps, such as popular shopping apps, are not included in our study.
The observed patterns have practical implications for both re-
searchers and practitioners. For instance, the fact that certain cate-
goriesofappscontainmoreteststhanothersindicatesthattech-
niqueslikeautomatedtesttransfer[ 6,40]mayworkmuchbetter
for apps of a certain category than others. The results also provide
invaluablehintsastowhatarethecustomarydevelopmentprac-
tices for apps of a certain category. This might help developers set
up the right development practice for their open-source projects to
gain traction and amass contributors.


Observation 2: Some categories of apps, such as finance and
videoplayers,aremoreextensivelyleveragingtestautomation
techniques than others.
1083Table 4: The Ways of Testing Android Apps by the Survey
Participants
Way #Respondents
Manually 130
With scripted/automated tests 85
With dedicated QA team or 3rd party testing services 43
With automatic input generation tools 12
Other 6
Not at all 3
Table 5: The Reasons for not Adopting Test Automation by
the Survey Participants
Difficulty #Respondents
Cost to create and maintain automated tests 77
Time constraints 74
Size or maturity of the app 66
Lack of exposure or knowledge of existing frameworks 52Cumbersome to use 50
Lack of support from management or organization 30
Other 11
5.2 Developer Perspective
In this section, we present our findings regarding the associations
between developers and test automation, including the rationale
and preferences reported by the survey participants.
RQ3. Howprevalentistestautomationandwhataretherea-
sons for not adopting it (as reported by developers)?
WhatarethechallengesintestingofAndroidappsin
general?
Inoursurvey,weaskedthedevelopershowtheytesttheirAn-
droidapps,andtheywereallowedtoselectalloptionsthatapply.
Table 4 shows the results. Interestingly, over 57% (85/148) of the
respondentsstatethattheyareusingautomatedtests,yetwedonot
observethisdegreeoftestautomationadoptionfromthesubject
apps they develop. One possible explanation for this inconsistency
isthat theproponentsof testautomationare morewillingto take
our survey, while the developers not interested in test automation
have no incentive to provide feedback. Another reason could be
thattheprofessionaldevelopersadoptautomatedtestsatwork,but
not for their pet projects on GitHub. Finally, it is also possible that
the developers only uploaded their source code on GitHub without
corresponding tests.
To understand why the observed adoption of test automation is
low,weaskedthedeveloperstospecifythereasonsfornotadopting
test automation. From the results in Table 5, we can see the top
threereasonsare:(1)costtocreateandmaintainautomatedtests,
e.g., caused by changing requirements or rapid development; (2)time constraints, e.g., because of time-to-market or customer‚Äôs
schedule; and (3) size or maturity of the app, e.g., the app is not big
or complex enough to require automated tests. Note that the third
reasoncorrespondstoourinsightthatitisnotcost-effectivetowrite
automated tests for trivial apps, and they should be excluded in
theempiricalstudy,aswehavedone.Besides,therespondentsalsoTable 6: The Biggest Challengs in Testing Android Apps by
the Survey Participants
Challenge #Respondents
Fragmentation 104
Concurrency 66Performance 51Security 44
Energy 43
Functionality 43Accessibility 35Other 14
Table 7: The Most Important or Useful Criteria for Evaluat-
ing Android App Tests by the Survey Participants
Criterion #Respondents
Faultdetection capability of tests 96
Feature or use case coverage of tests 83Code coverage of tests 67
Code or test case reviews 59
Other 7
mentionedotherinterestingdifficultiesinadoptingtestautomation
as follows:
‚ÄúLegacycodenotdesignedtobetestedrequireslotsofrefactoring
which makes it harder to justify the additional effort to write tests.‚Äù
‚Äú...hard to test unexpected GUI aspects or unexpected hardware
(manufactorfirmware) issuesor unexpectedpermissionissues orun-
expected Android behavoir or unexpected 3rd party data formats.‚Äù
It is worth mentioning that we also asked two general questions
to understand (1) the biggest challenges in testing of Android apps;and(2)themostusefulcriteriaforevaluatingtestsforAndroidapps.
TheresultsarereportedinTables6and7.AccordingtoTable6,the
topthreechallengesare:(1)fragmentation,e.g.,multipleAndroid
OS or API versions, devices with different sizes or resolutions, etc.;
(2) concurrency, e.g., detecting data races, deadlock, or violation
ofexecutionorderofmethods;and(3)performance,e.g.,app‚Äôsre-
sponsivenesssuchasframespersecondforgamingapps.Moreover,
fromTable7wecanseethatthedevelopersdonotconsidercode
coverageasthemostimportantcriterionforevaluatingtests,whichisinlinewiththepriorstudy[
42].Webelievethereportedconcerns
call for additional research and development in test automation
frameworks and tools. We take a closer look at the implications of
this result in Section 6.

Observation 3: 57% of the survey participants reported the use
oftestautomation,whichvariesdrasticallyfromthatobservedinthedataset.Thetopthreedifficultiesinadoptingtestautomation
are: cost to create and maintain tests, time constraints, and size
or maturity of the app.
RQ4. Do the same developers have the same testing habits
across apps?
In this section, we investigate whether developers are following
thesametestautomationhabitsacrossapps.Tothatend,wefirst
clusteredallsubjectappsbytheirowner,i.e.,theGitHubaccount,
1084Table8:ProbabilityofObservingConsistentBehavioronthe
Apps by the Same Developers. Sa: Clusters of Apps by the
Same Developers. Sb: by Different Developers
SetSize #ClustersProbability p-value(#Clusters) Same Behavior
Sa985 902 91.57%4.06e‚àí12Sb985 763 77.46%
andobtainedasetof985clusters, Sa,inwhicheachclustercontains
two or more apps by the same developer. Next, we defined and
computed the test adoption rate for each cluster CinSaas follows:
rate(C)=#Apps with t est in C
#Apps in C
Aclusterwitharateof1or0meansthedeveloperhasfollowedthe
samebehavioracrossapps.Thatis,thedevelopereitherwrotetests
forallofherappsordidnotwritetestsatall.Wefurthercomputed
the probability of observing the same behavior in Saby dividing
thenumberofclustersshowingthesamebehavior(i.e.,achievetest
adoption rate of 1 or 0) by the size of Sa.
Moreover, to understand if the probability observed in Sais
high, we created another set of clusters, Sb, as a control group.
The number of clusters and the size of each cluster in Sbis exactly
the same as Sa. However, the apps in Sbwere randomly selected
from the apps not in Sa. We computed the test adoption rate for
eachclusterin Sbusingthesameequation,andtheprobabilityof
observing the same behavior in Sbaccordingly.
Finally, to determine if the observed difference between Saand
Sbis statistically significant, we applied hypothesis testing on the
rate distribution of SaandSbusing the non-parametric test Mann-
WhitneyU[ 44]withasignificancelevelof0.05.WechosetheMann-
WhitneyUtestbecause SaandSbarenotnormallydistributedand
did not pass the normality test of Shapiro-Wilk [54].
The results in Table 8 show that in Sa, the set of clusters in
which each cluster consists of the apps by the same developer, it
ismorelikelytoobserveaclustermanifestingthesamebehavior.
In other words, for a group of apps by the same developer, theprobability that either all or none of them have tests (91
.57%) is
higherthanagroupofappsbydifferentdevelopers(77 .46%).The
difference between SaandSbis statistically significant, because
thenullhypothesisthat SaandSbarefromthesamedistribution
is rejected by the Mann-Whitney U test with a p-valueof 4.06√ó
10‚àí12.This findingvouches forthe effectofsoftware engineering
educationregardingtestautomation:oncelearned,developerskeep
their habits.


	Observation 4: App developers tend to follow the same test
automation practices across projects.
RQ5. Do developers prefer unit or UI testing and why?
FromTable 3inSection5.1,wesee thattheappsadoptingunit
tests(6.1%)aremorethanUItests(4%).Tovalidateourobservationandunderstandthereasonsbehindthis,forthedevelopersreporting
theuseoftestautomation,wefurtheraskedwhattypeoftesting
(unit testing or UI testing) they do mostly and why. Among the 83
respondents, the majority of them (55 /83, 66%) prefer unit testing.Table 9: The Reasons for the Preference of Unit Testing by
the Survey Participants
Reason #Respondents
Speed 39
Scope 30
Simpleness 28Robustness 26Other 6
Table 10: Distribution of the Number of Tests in the Apps
with Both Types of Tests. 1Q: 1stquartile. 2Q: 2ndquartile
(median). 3Q: 3rdquartile.
Distribution
MinMax Mean 1Q 2Q 3Q
#UnitTests 1 685 34.75 3 11 27.25
#UI Tests 1 178 14.32 2 7 17
Ratioof UI Tests to All Tests 0.3% 97.1% 41.9% 17.5% 40.0% 64.4%
This is in line with our observation from the dataset. Furthermore,
27%(22/83)oftherespondentshavenopreferenceand7%(6 /83)
of them prefer UI testing.
We also asked the proponents of unit testing for their rationale.
Table 9 shows that the top three reasons by the developers are: (1)
speed,e.g., unittestsrunfasterthanUIorend-to-endtests;(2)scope,
e.g., unit tests focus on small or independent modules, thereby
simplifythedebugging;and(3)simpleness,e.g.,unittestsareeasier
to learn and write. On the other hand, developers preferring UItesting indicate that the interactivity is the top reason, becauseUI or end-to-end tests can test the app in a more interactive andstraightforwardway.InSection6,wediscusshowtheseinsightscould be used for possible improvements of UI testing tools and
libraries.

Observation 5: Majority of the developers prefer unittesting,
corroborated through both project dataset and survey results.
Thetopthreereasonsar espeed,scop e and simpleness.
RQ6. Is the practice of Test Pyramid followed by develop-
ers?
As mentioned in Section 2.2, the Testing Pyramid practice is
a guideline for developers to have a balanced portfolio of differ-ent types of automated tests. To understand if the guideline is
appropriatelyfollowedbythedevelopers,weanalyzedthe266apps
containingbothunittestsandUItestsinourdatasetbycounting
the number different types of tests. Furthermore, we computedtheratioofthenumberofUIteststothetotalnumberoftestsas
follows:
#UI tests
#Un it tests +#UI tests√ó100
Table 10 shows that the distribution of the numbers of unit and
UI tests in the apps are skewed, because the averages are much
largerthanthemedians(i.e.,34 .75vs.11forunittests,and14 .32
vs. 7 for UI tests). That means some apps contain many more tests
thanothers.Ontheotherhand,thethirdquartileshowsthat75%
1085of the apps have fewer than 27 .25 unit tests and 17 UI tests. We
believe these are reasonable numbers for general apps.
Regarding the developers‚Äô compliance with the Test Pyramid
practice,Table10showsthatinmorethanhalfoftheapps,theratio
ofUItestsishigherthan40%,whichdiffersfromtherecommended
ratio of 20%-30% by Google [ 27]. In other words, the developers
putmoreeffortthanrecommendedinwritingUItests.Apossible
explanation is that the interactive nature of mobile apps drives
thedeveloperstowritemoreUItests.However,whileUItestsare
essential to validate certain types of requirements such as business
logicandusability,overlyrelyingonthemmaymaketestingand
debugging cumbersome, as mentioned in Section 2.2.
Inoursurvey,weaskedtheparticipantswhethertheyarefollow-
ing the Testing Pyramid practice, and 52% (51 /98) of them said no,
whichisconsistentwithourobservationfromthedataset.Apromi-
nent reason from the respondents reporting the non-complianceis the lack of exposure or knowledge about the Testing Pyramidpractice (40
/51, 78%). Other interesting reasons include ‚Äúspecial
needs for myteam or projects‚Äù and‚Äúthe Testing Pyramid practice
is misleading/flawed‚Äù.


Observation6: Developers put more effort than recommended
inwritingUItests,astheaverageratioofUIteststoalltestsis
40%.
5.3 Impact Perspective
Mobile app developers often strive to have their apps become pop-
ular. As members of an open-source community, developers are
pleasedtoseetheirappsreceivemoreattentionfromotherdevelop-ersintermsofstars,forks,contributors,etc.onGitHub.Asproductowners, developers want their apps to satisfy the users and receive
good ratings and feedback on the market. While these popular-
ity metrics are not necessarily related to thedevelopment process
of apps, we would like to investigate whether they are impactedby the adoption of test automation. Specifically, we consider thefollowing popularity metrics on GitHub: number of stars, forks,contributors, commits, issues
5, and pull requests. Moreover, we
consider user ratings on Google Play Store as the metric of user
satisfaction. These metrics were collected in the manner described
in Section 4.2. Table 11 presents the distribution of data in terms of
different metrics.
RQ7. How does test automation relate to project popular-
ity?
Wewouldliketoknowwhetherappswithtestsaredifferentfrom
apps without tests in terms of the popularity metrics on GitHub.
First,toeliminatetheeffectcausedbeappsize,weexcludedtheappsthathavefewerthan3components(the1
stquartile)andmorethan
8components(the3rdquartile)inourdataset,endingupwithaset
of7,664appsunderconsideration.Next,weconductedstatistical
analysis for each metric with the following steps:
(1)Wedividedthedataintotwodisjointsets, RwandR/prime.Rwconsists
ofthemetricvaluesfromtheappswithtests. R/primeconsistofthemetric
values form the apps without tests.
5Issuesmaybeconsideredasanindicatorofappquality.Infact,thetopicspostedwith
issuescanbeverybroad,suchasfeaturerequestorusagediscussion.Therefore,we
consider it as an indicator of popularity.Table 11: Distribution of the Popularity and Satisfaction
Metrics of the Apps. 1Q: 1stquartile. 2Q: 2ndquartile (me-
dian). 3Q: 3rdquartile.
Distribution
SampleSize Min Max Mean 1Q 2Q 3Q
Stars 12533 0 7897 15.09 0 0 1
Forks 12533 0 2209 4.49 0 0 1
Contributors 12533 0 451 2.30 1 1 2Commits 12527 1 13844 75.95 4 15 55Issues 12527 0 2442 5.5 0 0 0
Pull Requests 12527 0 1679 3.80 0 0 0Ratings 3937 1 5 4.23 3.91 4.38 4.78
(2)WeappliedtheZ-scoremethod[ 38]withathresholdofthree
times of standard deviation to remove the outliers from both sets.
(3)Since the apps without tests are much more than the apps with
testsinourdataset, RwandR/primeareextremelyunbalancedintermsof
their sizes. Given that unequal sample sizes maygenerally reduce
statisticalpowerof equivalencetests[ 53],we created Rowiththe
same size as Rwby randomly selecting the values in R/prime.
(4)We computed the mean and median of Rwand Roand the
difference between the mean and median.
(5)To determine if the difference observed in RwandRois sta-
tistically significant, as in Section 5.2, we performed hypothesistesting on
RwandRousing the Mann-Whitney U test with a sig-
nificance level of 0 .05. The null hypothesis on RwandRois that
they were selected from populations having the same distribution.
Forexample,inthecaseofstars,thenullhypothesisisthat‚Äúanapp
withtests(from Rw)hasthesamenumberofstarsonGitHubasan
appwithouttests(from Ro)‚Äù.WechosetheMann-WhitneyUtest
because RwandRoarenotnormallydistributedanddidnotpass
the normality test of Shapiro-Wilk.
(6)The above process is repeated for all the popularity metrics.
Table 12 shows the results of our statistical analysis. The sta-
tisticalevidenceshowsthattestautomationisassociatedwithall
popularity metrics. Namely, on average, open-source Android apps
withtestsareexpectedtohavemorestars,forks,contributors,com-
mits,issues,andpullrequestsonGitHub.Ourfindingisnotexactly
inlinewiththepriorworkbyCruzetal.[ 13],inwhichtheyonly
foundsuchcorrelationwithcontributorsandcommitsbutnotother
metrics. We believe this inconsistency is caused by similar reasons
discussed in Section 5.1.
We presented this correlation to the survey participants and
askedfortheiropinionsastothepossibleexplanations.57%(84 /148)
of the respondents believe that there is a cause-and-effect relation-
ship between test automation and popularity. The causation, how-
ever, could be direct, reverse, bidirectional, etc., as explained by
some of the respondents below:
‚ÄúI would say they have a direct connection since the quality and
rigidnessoftheapp‚Äôscodecandefinitelyinfluenceanapp‚Äôspopularity.‚Äù
(direct)
‚ÄúProjectscanonlygrowtolargenumbersiftheyarestable.Auto-
mated testing can ensure this happens to some degree.‚Äù (direct)
1086Table 12: Impact of Having Tests on the Popularity of Apps.
Rw: Apps with Tests. Ro: Apps Without Tests.
Stars* Forks*
SizeMean Median p-value Size Mean Median p-value
Rw62910.95 04.07e‚àí11630 3.74 03.59e‚àí12Ro629 4.57 0 630 1.31 0
Œî 6.38 0 2.43 0
Contributors* Commits*
SizeMean Median p-value Size Mean Median p-value
Rw6302.76 21.75e‚àí14628 147.21 84.53.53e‚àí67Ro630 1.63 1 628 39.93 14
Œî 1.13 1 107.28 70.5
Issues* Pull Requests*
SizeMean Median p-value Size Mean Median p-value
Rw63510.39 01.40e‚àí27633 8.76 01.95e‚àí29Ro635 1.35 0 633 0.85 0
Œî 9.04 0 7.91 0
*Thedifference is statistically significant.
‚ÄúFirst you build the app, then it gets popular, then you get re-
sources/motivationtoincrease it‚Äôsquality.That‚Äôswhenyou gotoUI
tests.‚Äù(reverse)
‚ÄúIthinkbecausetheprojectswerebigtheyweremotivatedtocreate
a comprehensive testing suite.‚Äù (reverse)
‚ÄúProjectsthatbecomepopularendupwritingmoretestsbecause
they need to ensure the stability of the project. As the project becomes
more stable (due to more testing) it provides a positive feedback loop.
Theproject,inpart,ismorelikelytobepopularifitisperceivedas
stable, and testing helps to increase that stability.‚Äù (bidirectional)
Ontheotherhand,34%(50 /148)oftherespondentsconsiderthis
correlationtobemoreofaconnectionthancausation.Forexample,
the following responses claim common causes for them:
‚ÄúCommoncause:Experienceddeveloperwhocaresaboutmaking
code evolvable.‚Äù
‚ÄúPopular projects are usually bigger, with multiple developers and
with more management. Tests is just a part of that process.‚Äù

Observation 7: Popular projects are more likely to adopt test
automation practices. 57% of the developers believe it implies
causality between them.
RQ8. How does test automation relate to user satisfaction?
Following the same steps, we conducted statistical analysis to
investigate whethertest automationrelates touser satisfaction in
terms of Google Play ratings. As shown in Table 13, we do not find
the association between them with statistical significance.
Surprisingly, when we presented this to the survey participants
andaskedfortheiropinions,52%(77 /148)oftherespondentsbe-
lieve that test automation and user ratings should be somehow
related.Namely,thedevelopersdonotbelieveourfindingiscorrect.
Examples of their reasons are as follows:
‚ÄúI think it would depend on the type of application. Games and
such areharder to testand the qualityof test does notcorrelate withTable13:ImpactofHavingTestsontheUserSatisfactionof
Apps. Rw: Apps with Tests. Ro: Apps Without Tests.
SizeMean Median p-value
Rw2114.14 4.250.0689Ro211 4.2 4.33
Œî -0.06 -0.08
howfunthegameis.Forabankingapplicationtestsareessentialand
do effect the quality of the final product.‚Äù
‚ÄúPlayStoreratingsareanoisymetricofappqualityandoverall
userexperience,sothenoapparentcorrelationdoesn‚Äôtconvinceme
that app quality isn‚Äôt impacted at least somewhat by automated
testing‚Äù

Observation 8: Users‚Äôsatisfaction withapps appearstobe un-
related to the adoption of automated testing practices in their
development, while half of the developers think differently.
6 DISCUSSION
Automated testing is not widely adopted. Only 8% of the sub-
ject apps in our study have adopted automated testing. As men-
tionedearlier,thisfindingcontradictsearlierstudiesthathavere-
portedsubstantiallyhigheradoptionrate[ 12,13,37],butitisinline
withthegeneralperceptionthatitischallengingtofindcomplex
andopen-sourceappswithlotsoftestsforresearchpurposes,as
noted by Adamsen et al. [ 2]. Nevertheless, our study addresses this
issuebyprovidingadatasetofreal-worldandnon-trivialappswith
automated tests, which can by of significant utility for emerging
areasofresearchinterest,suchasautomatedprogramrepair[ 5,55],
automated test transfer [ 6,40], and mutation testing [ 15,30,41].
Moreover, researchers may hold out hope on specific categories
of apps when looking for automated tests for their experiments,
since our results indicate that the prevalence of test automation is
varied across different categories. Note that the focus of our study
is on automated or scripted tests. The subject apps may have gone
throughpropermanualtestingbythedevelopers,butthatisoutside
the scope of this study.
Automatedtestingcanbeusefulandimportant. Wefound
a strong correlation between the adoption of automated testing
practices and the popularity of development projects. The majority
ofthesurveyrespondents(91%,134 /148)believethatthecorrela-
tionis eithercausation ora connection.On theotherhand, while
users‚Äôsatisfactionappearsunrelatedtotestautomation,aconsider-
able amount of survey participants think that automated testing
contributes to app quality in terms of stability and maintainability,
andhasimpactsonusers‚Äôsatisfaction.Asnotedbypreviousstud-
ies[16,37],automatedtestingisnotuniversallyapplicable,butcan
beusefulandimportant,especiallyforappsthatupdateregularly
and frequently.
Automatedtestingneedsmoreattention,organizationally
and culturally. Despite the benefits of automated testing, our
study shows that it is not adequately adopted in practice. Many
reporteddifficultiesinadoptingtestautomation,however,canbe
addressed from the perspective of organization and culture. For
instance, management or organization could provide more support
1087in terms of budget or schedule to allow for the introduction and
maintenanceofautomatedtests.Inotherwords,adoptionoftestau-
tomationinvolvesaculturechange;organizationsneedtobewillingto incur the additional cost and effort of setting up test automationpracticesearlyonforthepromiseofproducinghigher-qualityapps
at a faster pace later on.
Toolsandlibrarieshaveroomforimprovement. Oneofthe
difficultiesreportedbydevelopersinadoptingautomatedtesting
practicesiscumbersometools,includingsteeplearningcurve,poor
documentation,usability,andcompatibilityissues.Apossibleim-
provement of such tools is a comprehensive information hub that
aggregatesandsummarizesscatteredpiecesofinformationfrom
tutorials, forums, blogs, case studies, etc., to make the learning and
use of such tools easier. Besides, in our study, UI testing is less
adopted than unit testing, and developers have concerns about the
speed,simpleness,androbustnessofUItesting.Asaresult,current
UItestingtoolscouldbeimprovedbyaddressingtheseconcerns.
For example, supporting ‚Äúheadless mode‚Äù such as done by Robolec-
tric [52] can let developers run UI testswithout an emulator and
saveamassiveamountofexecutiontime.Inaddition,interactive
toolssuchasEspressoTestRecorder[ 25]canhelpdeveloperscre-
ate UI tests without writing test code. Finally, efforts to prevent or
resolveflakinessofUItestsmayincreasetherobustnessandattract
more users.
Awareness matters. Ourstudy indicatesaprimary reasonfor
not following specific practices in automated testing is the lackof exposure or knowledge about them. Moreover, we found that
once developers learn and begin to use test automation techniques,
theymaintainthathabitacrossotherprojects.Therefore,raising
the developers‚Äô awareness of existing test automation frameworks,
tools, and practices may increase their adoption.
7 THREATS TO VALIDITY
External validity. The major external validity is the generaliza-
tion of our findings to all open-source Android apps. We mitigated
thisthreatbyincludingmorethan 12 ,000appsthatvaryinterms
of size, created year, category, published market, and popularitymetrics on GitHub. However, findings in this study may not be
applicabletotrivialappsorcommercialappsdevelopedprivately.
Furthermore,therespondentsofoursurveymaynotberepresenta-
tive of the entire developer community of the subject apps, or the
globalcommunityofAndroidappdevelopers.Wetriedtoreduce
this threat by collecting the responses of 148 developers from 45
countries withvarious years ofprofessional experience. Thenum-
ber ofresponses toour survey isalso comparable toother similar
studies of mobile developers [33, 37, 42].
Internal validity. Weproposedcertainheuristicstoautomat-
icallyidentifynon-trivialapps.Whilewemayhavemissedsome
complexandpublishedapps,e.g.,appswithsingleActivityandmul-
tiplefragments,webelievethatthefindingsinthispaperarestill
useful for practitioners and researchers regarding test automation.
Moreover,weautomaticallydeterminethenumberoftestcasescon-
tainedinarepositorybasedontheassumptionthatthetestcases
arewritteninJUnit-basedtestingframeworks.WhileJUnit-based
testingframeworksoverwhelminglydominateAndroidapptesting
(e.g.,97%to100%accordingtoapriorstudy[ 13]),itispossiblethat
sometestcasesbuiltontopofothertypesofframeworksarenotincluded in our study. To mitigate this threat, we manually verified
a small set of projects in our dataset and did not find any missed
test cases. As a result, we argue that such cases are rare and would
not significantly impact our conclusions.
8 CONCLUSION
This paper provides a holistic view regarding how and why test
automation is practically adopted in open-source Android apps.
With the analysis of more than 12 ,000 non-trivial apps on GitHub
andasurveyof148developersoftheseapps,weinvestigated(1)the
prevalence of test automation in mobile app development projects;
(2)workinghabitsofmobileappdevelopers;and(3)thecorrelation
between the adoption of test automation and the popularity ofprojects. Among others, we found that: (1) only 8% of the non-
trivialappscontainautomated tests;(2)developerstendtofollow
the same test automation practices across apps; and (3) popularprojects are more likely to adopt test automation practices. We
believethefindingsinthispapershedlightonthecurrentpractices
and future research directions pertaining to test automation for
mobileappdevelopment.Inourfuturework,weplantoincorporate
additional open-source projects, such as those hosted on Bitbucket,
andinvestigatenewresearchquestions,e.g.,questionsrelatedto
the interplay between test automation techniques and continuous
integration practices.
ACKNOWLEDGMENT
Thisworkwassupportedinpartbyawardnumber1823262from
the National Science Foundation.
REFERENCES
[1]360logica. 2020. A sneak peek into test framework and testing pyramid. Re-
trieved January 19, 2020 from https://www.360logica.com/blog/sneak-peek-test-
framework-test-pyramid-testing-pyramid/
[2]Christoffer Quist Adamsen, Gianluca Mezzetti, and Anders M√∏ller. 2015. System-
aticExecutionofAndroidTestSuitesinAdverseConditions.In Proceedingsof
the 2015 International Symposium on Software Testing and Analysis (Baltimore,
MD,USA) (ISSTA2015).AssociationforComputingMachinery,NewYork,NY,
USA, 83‚Äì93. https://doi.org/10.1145/2771783.2771786
[3]Kevin Allix, Tegawend√© F. Bissyand√©, Jacques Klein, and Yves Le Traon. 2016.
AndroZoo: Collecting Millions of Android Apps for the Research Community. In
Proceedingsofthe13thInternationalConferenceonMiningSoftwareRepositories
(Austin, Texas) (MSR ‚Äô16).ACM,New York,NY, USA,468‚Äì471. https://doi.org/
10.1145/2901739.2903508
[4]AndroZoo. 2020. AndroZoo Markets. Retrieved January 19, 2020 from https:
//androzoo.uni.lu/markets
[5]LarissaAzevedo,AltinoDantas,andCelsoG.Camilo-Junior.2018. DroidBugs:
An Android Benchmark for Automatic Program Repair. CoRRabs/1809.07353
(2018). arXiv:1809.07353 http://arxiv.org/abs/1809.07353
[6]F.BehrangandA.Orso.2019. TestMigrationBetweenMobileAppswithSimilar
Functionality. In 2019 34th IEEE/ACM International Conference on Automated
Software Engineering (ASE). 54‚Äì65.
[7]Moritz Beller, Georgios Gousios,Annibale Panichella, andAndy Zaidman. 2015.
When, How, and Why Developers (Do Not) Test in Their IDEs. In Proceedings
ofthe201510thJointMeetingonFoundationsofSoftwareEngineering (Bergamo,
Italy)(ESEC/FSE 2015). Association for Computing Machinery, New York, NY,
USA, 179‚Äì190. https://doi.org/10.1145/2786805.2786843
[8]N. Chang, L. Wang, Y. Pei, S. K. Mondal, and X. Li. 2018. Change-Based Test
ScriptMaintenanceforAndroidApps.In 2018IEEEInternationalConferenceon
Software Quality, Reliability and Security (QRS). 215‚Äì225.
[9]W. Choi, K. Sen, G. Necul, and W. Wang. 2018. DetReduce: Minimizing Android
GUI Test Suites for Regression Testing. In 2018 IEEE/ACM 40th International
Conference on Software Engineering (ICSE). 445‚Äì455.
[10]Shauvik Roy Choudhary, Alessandra Gorla, and Alessandro Orso. 2015. Auto-
mated Test Input Generation for Android: Are We There Yet? (E). In Proceedings
1088of the 2015 30th IEEE/ACM International Conference on Automated Software Engi-
neering (ASE) (ASE ‚Äô15). IEEE Computer Society, Washington, DC, USA, 429‚Äì440.
https://doi.org/10.1109/ASE.2015.89
[11]Mike Cohn. 2010. Succeeding with agile: software development using Scrum.
Pearson Education.
[12]RiccardoCoppola,MaurizioMorisio,MarcoTorchiano,andLucaArdito.2019.
Scripted GUI testing of Android open-source apps: evolution of test code and
fragilitycauses. EmpiricalSoftwareEngineering 24,5(01Oct2019),3205‚Äì3248.
https://doi.org/10.1007/s10664-019-09722-9
[13]LuisCruz,RuiAbreu,andDavidLo.2019. Totheattentionofmobilesoftware
developers:guesswhat,testyourapp! EmpiricalSoftwareEngineering 24,4(01
Aug 2019), 2438‚Äì2468. https://doi.org/10.1007/s10664-019-09701-0
[14]The Dataset. 2020. The Dataset. Retrieved January 19, 2020 from https://github.
com/seal-hub/ASE20Empirical
[15]Lin Deng, Jeff Offutt, Paul Ammann, and Nariman Mirzaei. 2017. Mutationoperators for testing Android apps. Information and Software Technology 81
(2017), 154‚Äì168.
[16]Paul M Duvall, Steve Matyas, and Andrew Glover. 2007. Continuous integration:
improving software quality and reducing risk. Pearson Education.
[17]F-Droid. 2020. F-Droid - Free and Open Source Android App Repository. Retrieved
January 19, 2020 from https://f-droid.org
[18]Martin Fowler. 2020. Test Pyramid. Retrieved January 19, 2020 from https:
//martinfowler.com/bliki/TestPyramid.html
[19]G. Fraser and A. Arcuri. 2012. Sound empirical evidence in software testing.In2012 34th International Conference on Software Engineering (ICSE). 178‚Äì188.
https://doi.org/10.1109/ICSE.2012.6227195
[20]GHTorrent. 2020. GHTorrent Downloads. Retrieved January 19, 2020 from
https://ghtorrent.org/downloads.html
[21]Google. 2020. Advanced Android in Kotlin: Testing Basics. Retrieved January 19,
2020fromhttps://codelabs.developers.google.com/codelabs/advanced-android-
kotlin-training-testing-basics/index.html#4
[22]Google. 2020. Android Studio release notes. Retrieved January 19, 2020 from
https://developer.android.com/studio/releases
[23]Google. 2020. App Manifest Overview. Retrieved January 19, 2020 from https:
//developer.android.com/guide/topics/manifest/manifest-intro
[24]Google. 2020. Application Fundamentals. Retrieved January 19, 2020 from
https://developer.android.com/guide/components/fundamentals.html
[25]Google.2020. CreateUItestswithEspressoTestRecorder. RetrievedJanuary19,
2020 from https://developer.android.com/studio/test/espresso-test-recorder
[26]Google.2020. Espresso. RetrievedJanuary19,2020fromhttps://developer.android.
com/training/testing/espresso
[27]Google.2020. FundamentalsofTesting. RetrievedJanuary19,2020fromhttps:
//developer.android.com/training/testing/fundamentals
[28]Georgios Gousios. 2013. The GHTorrent dataset and tool suite. In Proceedings of
the 10th Working Conference on Mining Software Repositories (San Francisco, CA,
USA)(MSR‚Äô13).IEEEPress,Piscataway,NJ,USA,233‚Äì236. http://dl.acm.org/
citation.cfm?id=2487085.2487132
[29]MichaelHilton,TimothyTunnell,KaiHuang,DarkoMarinov,andDannyDig.
2016. Usage, Costs, and Benefits of Continuous Integration in Open-Source
Projects.In Proceedingsofthe31stIEEE/ACMInternationalConferenceonAuto-
matedSoftwareEngineering (Singapore,Singapore) (ASE2016).Associationfor
ComputingMachinery,NewYork,NY,USA,426‚Äì437. https://doi.org/10.1145/
2970276.2970358
[30]ReyhanehJabbarvandandSamMalek.2017. MuDroid:AnEnergy-AwareMu-
tation Testing Framework for Android. In Proceedings of the 2017 11th Joint
MeetingonFoundationsofSoftwareEngineering (Paderborn,Germany) (ESEC/FSE
2017). Association for Computing Machinery, New York, NY, USA, 208‚Äì219.
https://doi.org/10.1145/3106237.3106244
[31]Reyhaneh Jabbarvand, Alireza Sadeghi, Hamid Bagheri, and Sam Malek. 2016.
Energy-AwareTest-SuiteMinimizationforAndroidApps.In Proceedingsofthe
25th International Symposium on Software Testing and Analysis (Saarbr√ºcken,
Germany) (ISSTA2016).AssociationforComputingMachinery,NewYork,NY,
USA, 425‚Äì436. https://doi.org/10.1145/2931037.2931067
[32]B.Jiang,Y.Wu,Y.Zhang,Z.Zhang,andW.K.Chan.2018. ReTestDroid:TowardsSaferRegressionTestSelectionforAndroidApplication.In 2018IEEE42ndAnnual
Computer Software and Applications Conference (COMPSAC), Vol. 01. 235‚Äì244.
[33]MonaErfaniJoorabchi,AliMesbah,andPhilippeKruchten.2013. RealChallenges
in Mobile App Development. In 2013 ACM / IEEE International Symposium on
Empirical Software Engineering and Measurement, Baltimore, Maryland, USA,
October 10-11, 2013. 15‚Äì24. https://doi.org/10.1109/ESEM.2013.9
[34] JUnit. 2020. JUnit. Retrieved January 19, 2020 from https://junit.org
[35]Kostis Kapelonis. 2020. Software Testing Anti-patterns. Retrieved January 19,
2020fromhttp://blog.codepipes.com/testing/software-testing-antipatterns.html
[36]P.S.Kochhar,T.F.Bissyand√©,D.Lo,andL.Jiang.2013. AnEmpiricalStudyof
Adoption of Software Testing in Open Source Projects. In 2013 13th International
Conference on Quality Software. 103‚Äì112. https://doi.org/10.1109/QSIC.2013.57
[37]Pavneet Singh Kochhar, Ferdian Thung, Nachiappan Nagappan, Thomas Zim-
mermann,andDavidLo.2015. UnderstandingtheTestAutomationCultureofApp Developers. In Software Testing, Verification and Validation (ICST), 2015 IEEE
8th International Conference on. IEEE, 1‚Äì10.
[38]Erwin Kreyszig. 2009. Advanced Engineering Mathematics, 10th Eddition. Wiley.
[39]X. Li, N. Chang, Y. Wang, H. Huang, Y. Pei, L. Wang, and X. Li. 2017. ATOM:
Automatic Maintenance of GUI Test Scripts for Evolving Mobile Applications. In
2017IEEEInternationalConferenceonSoftwareTesting,VerificationandValidation
(ICST). 161‚Äì171. https://doi.org/10.1109/ICST.2017.22
[40]J. Lin, R. Jabbarvand, and S. Malek. 2019. Test Transfer Across Mobile Apps
Through Semantic Mapping. In 2019 34th IEEE/ACM International Conference on
Automated Software Engineering (ASE). 42‚Äì53.
[41]MarioLinares-V√°squez,GabrieleBavota,MicheleTufano,KevinMoran,Massi-
miliano Di Penta, Christopher Vendome, Carlos Bernal-C√°rdenas, and Denys
Poshyvanyk. 2017. Enabling Mutation Testing for Android Apps. In Proceedings
of the 2017 11th Joint Meeting on Foundations of Software Engineering (Paderborn,
Germany) (ESEC/FSE 2017). Association for Computing Machinery, New York,
NY, USA, 233‚Äì244. https://doi.org/10.1145/3106237.3106275
[42]M. Linares-V√°squez, C. Bernal-Cardenas, K. Moran, and D. Poshyvanyk. 2017.How do Developers Test Android Applications?. In 2017 IEEE International
Conference on Software Maintenance and Evolution (ICSME). 613‚Äì622. https:
//doi.org/10.1109/ICSME.2017.47
[43]M. Linares-V√°squez, K. Moran, and D. Poshyvanyk. 2017. Continuous, Evolu-
tionary and Large-Scale: A New Perspective for Automated Mobile App Testing.
In2017 IEEE International Conference on Software Maintenance and Evolution
(ICSME). 399‚Äì410. https://doi.org/10.1109/ICSME.2017.27
[44]H. B. Mann and D. R. Whitney. 1947. On a Test of Whether one of Two Random
Variables is Stochastically Larger than the Other. Ann. Math. Statist. 18, 1 (03
1947), 50‚Äì60. https://doi.org/10.1214/aoms/1177730491
[45]AtifM.MemonandMyraB.Cohen.2013. AutomatedTestingofGUIApplications:
Models, Tools, and Controlling Flakiness. In Proceedings of the 2013 International
Conference on Software Engineering (San Francisco, CA, USA) (ICSE ‚Äô13). IEEE
Press, 1479‚Äì1480.
[46]Mockito. 2020. Most popular mocking framework for Java . Retrieved January 19,
2020 from https://github.com/mockito/mockito
[47]DuncanNisbet.2020. TestAutomationBasics‚ÄìLevels,Pyramids&Quadrants. Re-
trieved January 19, 2020 from http://www.duncannisbet.co.uk/test-automation-
basics-levels-pyramids-quadrants
[48]Chairat Onyaem. 2020. Separate Unit, Integration, and Functional
Tests for Continuous Delivery. Retrieved January 19, 2020 fromhttps://medium.com/pacroy/separate-unit-integration-and-functional-
tests-for-continuous-delivery-f4dc240d8f2f
[49]M. Pan, T. Xu, Y. Pei, Z. Li, T. Zhang, and X. Li. 2019. GUI-Guided Repair of
Mobile Test Scripts.In 2019 IEEE/ACM 41stInternational Conference on Software
Engineering: Companion Proceedings (ICSE-Companion). 326‚Äì327.
[50]Fabiano Pecorelli, Gemma Catolino, Filomena Ferrucci, Andrea De Lucia, and
Fabio Palomba. 2020. Testing of Mobile Applications in the Wild: A Large-Scale
Empirical Study on Android Apps. In 28th IEEE/ACM International Conference on
Program Comprehension (ICPC).
[51]Qualtrics. 2020. Qualtrics Survey Software. Retrieved January 19, 2020 from
https://www.qualtrics.com/
[52]Robolectric. 2020. Test-drive your Android code. Retrieved January 19, 2020 from
http://robolectric.org/
[53]Shayna Rusticusand Chris Lovato. 2014. Impact of SampleSize and Variability
onthePowerandTypeIErrorRatesofEquivalenceTests:ASimulationStudy.
Practical Assessment, Research and Evaluation 19 (01 2014).
[54]S. S. SHAPIRO and M. B. WILK. 1965. An analysis of variance test for normality
(completesamples) ‚Ä†.Biometrika 52,3-4(121965),591‚Äì611. https://doi.org/10.
1093/biomet/52.3-4.591 arXiv:https://academic.oup.com/biomet/article-pdf/52/3-
4/591/962907/52-3-4-591.pdf
[55]S. H. Tan, Z. Dong, X. Gao, and A. Roychoudhury. 2018. Repairing Crashesin Android Apps. In 2018 IEEE/ACM 40th International Conference on Software
Engineering (ICSE). 187‚Äì198.
[56]Wenyu Wang, Dengfeng Li, Wei Yang, Yurui Cao, Zhenwen Zhang, Yuetang
Deng, and Tao Xie. 2018. An Empirical Study of Android Test Generation Tools
in Industrial Cases. In Proceedings of the 33rd ACM/IEEE International Conference
on Automated Software Engineering (Montpellier, France) (ASE 2018). Association
for Computing Machinery, New York, NY, USA, 738‚Äì748. https://doi.org/10.
1145/3238147.3240465
[57]XiaZeng,DengfengLi,WujieZheng,FanXia,YuetangDeng,WingLam,Wei
Yang, and Tao Xie. 2016. Automated Test Input Generation for Android: Are
We Really There yet in an Industrial Case?. In Proceedings of the 2016 24th ACM
SIGSOFTInternationalSymposiumonFoundationsofSoftwareEngineering (Seattle,
WA,USA) (FSE2016).AssociationforComputingMachinery,NewYork,NY,USA,
987‚Äì992. https://doi.org/10.1145/2950290.2983958
1089