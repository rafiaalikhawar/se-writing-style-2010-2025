EfÔ¨Åcient state synchronisation in model-based
testing through reinforcement learning
1stUraz Cengiz T ¬®urker
The School of Computing
and Mathematical Sciences
The University of Leicester
Leicester, UK
u.c.turker@leicester.ac.uk2ndRobert M. Hierons
Department of Computer Science
The University of ShefÔ¨Åeld
ShefÔ¨Åeld, UK
r.hierons@shefÔ¨Åeld.ac.uk3rdMohammad Reza Mousavi
Department of Informatics
King‚Äôs College London
London, UK
mohammad.mousavi@kcl.ac.uk4thIvan Y . Tyukin
The School of Computing
and Mathematical Sciences
The University of Leicester
Leicester, UK
i.tyukin@leicester.ac.uk
Abstract ‚ÄîModel-based testing is a structured method to test
complex systems. Scaling up model-based testing to large systems
requires improving the efÔ¨Åciency of various steps involved in test-
case generation and more importantly, in test-execution. One of
the most costly steps of model-based testing is to bring the system
to a known state, best achieved through synchronising sequences.
A synchronising sequence is an input sequence that brings a given
system to a predetermined state regardless of system‚Äôs initial
state. Depending on the structure, the system might be complete,
i.e., all inputs are applicable at every state of the system. However,
some systems are partial and in this case not all inputs are
usable at every state. Derivation of synchronising sequences
from complete or partial systems is a challenging task. In this
paper, we introduce a novel Q-learning algorithm that can derive
synchronising sequences from systems with complete or partial
structures. The proposed algorithm is faster and can process
larger systems than the fastest sequential algorithm that derives
synchronising sequences from complete systems. Moreover, the
proposed method is also faster and can process larger systems
than the most recent massively parallel algorithm that derives
synchronising sequences from partial systems. Furthermore, the
proposed algorithm generates shorter synchronising sequences.
Index Terms‚ÄîModel based testing, synchronising sequence,
reinforcement learning, Q-learning
I. I NTRODUCTION
Model-based testing [1]‚Äì[6] is a rigorous method for vali-
dation and veriÔ¨Åcation. The application of model-based testing
to areas such as autonomous reactive systems has been further
facilitated in recent years by advances in model-learning tech-
niques, and their extensions to adaptive and evolving systems
[7], [8]. The industrial-scale application of model-based testing
in these challenging areas requires the scaling up of various
parts of the underlying techniques.
Many systems are state-based: they have an internal state
that affects behaviour and is updated by events and operations.
When testing a state-based system, one typically needs to
apply a number of test sequences, or adaptive test cases, from
the initial state of the system under test (SUT). As a result,
the state of the SUT is normally reset to this initial state
between test sequences. Sometimes it is straightforward to
reset the state of the SUT by, for example, turning the system
off and then on again or issuing the reset signal provided by
the manufacturer/developer. However, the process of resettingthe SUT between test-cases can be one of the most time-
consuming parts of test execution [9] and there may be no
simple approach to resetting the SUT; there may be no reset
input or the use of a reset input may be infeasible. In such
circumstances, the reset operation is implemented through the
use of synchronising sequences (SSs) [10]. SSs are used to
bring the underlying system to a speciÔ¨Åc state to resume testing
with a new test case [2], [9], [11]‚Äì[13]. In such cases, the
length of SSs affects the cost of test execution. This motivates
the work in this paper, which utilises reinforcement learning
in the generation of short SSs. The development of efÔ¨Åcient
techniques that produce short SSs will contribute to the agenda
of producing systematic MBT techniques that scale to large
models.
In addition to model-based testing, synchronising sequences
are also heavily used in motion planing of robotic controllers
such as orienting parts on assembly lines [14]. Moreover, SSs
have also been studied in automata theory and genomics and
form an active area of research [12], [14]‚Äì[27].
A. Problem statement & Related Work
The underlying transition function of a reactive system test
model can be complete orpartial. In a complete model, all
inputs of the system are applicable in each of its states. The
development of efÔ¨Åcient methods to generate short SSs from
such a model is very appealing as the use of shorter SSs
reduces the cost and time of test execution. However, the
problem of generating a minimal SS, from a complete model,
isNP-hard [26].
Even though there are several methods for generating an
SS [26], [28]‚Äì[35], the initial step of all these methods is
the construction of a product-automaton; a data structure that
requires quadratic space with respect to the number of states
of the model. All existing methods use this data-structure
as the basis of different approaches/heuristics to derive short
SSs. Amongst these, the fastest known algorithm [26] requires
O(n3+kn2)time where ndenotes the number of states and
kdenotes the number of inputs of the model.
To address the computational cost, recent work [36] intro-
duced many-core and multi-core approaches for deriving short
synchronising sequences from complete systems. However,
3682021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)
DOI 10.1109/ASE51524.2021.000412021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE) | 978-1-6654-0337-5/21/$31.00 ¬©2021 IEEE | DOI: 10.1109/ASE51524.2021.9678566
978-1-6654-0337-5/21/$31.00  ¬©2021  IEEE
it is relatively difÔ¨Åcult to implement these methods, as they
require (i) proÔ¨Åciency in parallel programming and program-
ming general purpose graphics processing units (GPGPUs) and
(ii) sophisticated graphics cards and CPUs.
In a partial model, not all inputs are applicable in all of
the states of the model. For such models, the problem of
deciding whether there is an SS is PSPACE-complete [37]. As
a result, the current focus of research is merely related to the
derivation of SSs. In this line of research, the Ô¨Årst published
algorithm was based on constructing a breadth Ô¨Årst search tree,
which requires exponential space and time and hence is not
efÔ¨Åcient [38], [39]. The only known efÔ¨Åcient algorithm [15]
checks all input sequences that are not longer than some
upper-bound. Experimental results show that a combination
of a relatively powerful GPU card and CPU can process
systems with 16;000;000states and 10inputs. However, the
performance of the algorithm degrades as the number of
inputs increases. Moreover, this method also requires (i) solid
expertise in programming general purpose graphics processing
units and (ii) a powerful graphics card.
Finally, in [40], the authors introduce an algorithm that
uses Answer Set Programming to generate SSs from complete
systems. As the method requires a Conjunctive Normal Form
formulation of the system and the SS generation problem, it
is not scaleable to large systems with many states.
Given recent breakthroughs in the Reinforcement Learning
(RL) spectra, the objective of the work presented in this paper
is to use RL as the basis for novel algorithms that efÔ¨Åciently
Ô¨Ånd short synchronisation sequences. RL deÔ¨Ånes a family of
machine learning algorithms and is becoming a major tool
in computational intelligence [41]. In RL, computers (agents)
make their own choices (take actions) in a given environment
without having prior information or labelled data [42].
Recently, the use of RL in test generation has received
signiÔ¨Åcant attention [43]‚Äì[51] and we now brieÔ¨Çy review a
few examples. RL has formed the basis of an algorithm to
automatically generate test cases for Android applications with
the aim of improving code coverage [50] and has also been
used in mutation testing to predict whether a mutant will
be killed by a given test suite without incurring the cost of
executing the mutant [52]. RL has also been used in graphical
user interface (GUI) testing [44], [45].
In online testing, test inputs are derived during test execu-
tion. It has been shown that RL can be used to address the
associated problem of optimizing the choice of test actions to
reduce test costs [51]. RL has been used to learn a behaviour
model of the system under test to aid risk-based testing [47].
Three RL algorithms have been proposed and embedded in
EvoSuiteFIT [43] to support hyperheuristic search based test
generation algorithms [48].
RL has also been used in security testing. For example,
researchers have developed an RL based testing algorithm
that trains dishonest agents to reveal dangerous behaviours
of autonomous cyber-physical systems [49]. An RL based test
generation technique has also been devised with the aim of
increasing hardware Trojan detection accuracy [53], [54].This paper usesQ-learning, which deÔ¨Ånes a family of RL
methods in which the actions taken by an agent are decided
according to quality values (Q-values) [55]. Q-learning has
become the basis of many RL algorithms because unlike other
methods,Q-learning is simple and exhibits excellent learning
facilities [56].Q-learning techniques have recently attracted
signiÔ¨Åcant attention with the introduction of Deep QNetworks
(DQNs) [57]. Although Q-learning algorithms are promising,
it is still challenging to use them in our setting. The initial Q-
learning algorithm has good convergence properties, however
it requires tables or matrices to hold Q-values. When the
agent interacts with the environment, a single Q-value on
the table is updated. So this simple setting introduces at least
two problems, (i) in circumstances where the environment is
complicated, critical states might not be experienced/learned
(generalisation problem), and (ii) because the underlying data
structure is a table, it leads to a large amount of storage space
being required. Therefore, for complex learning problems with
large, complex environments, it is difÔ¨Åcult to achieve effective
learning by using the tabular Qlearning algorithm. Function
approximators have been used, to overcome generalisation
problem, as an alternative to keeping Qvalues [58].
B. Research questions
In light of these Ô¨Åndings, the research questions studied in
this paper are summarised as follows:
RQ 1 Is our proposed Q-learning algorithm more efÔ¨Åcient and
scalable than the state of the art algorithms?
We further reÔ¨Åne this research question into the fol-
lowing sub-questions pertaining to the execution time,
scalability, and memory usage, respectively:
RQ 1.1 Is the growth of the execution time more modest in the
Q-learning algorithm?
RQ 1.2 How scalable can a Q-learning algorithm be? By Ô¨Åxing
the amount of memory and execution time, can a Q-
learning algorithm generate SSs from speciÔ¨Åcations that
cannot be processed by the state of the art SS generation
algorithms?
RQ 1.3 How does the memory usage behaviour of the Q-
learning algorithm compare to state of the art SS gener-
ation algorithms?
RQ 2 Is our proposed Q-learning algorithm more effective in
generating shorter synchronising sequences for a larger
class of system models?
We also reÔ¨Åne this research question into the following
sub-questions pertaining to the length of the generated
synchronising sequence and the types of systems han-
dled by the algorithm, respectively:
RQ 2.1 Does the Q-learning algorithm generate shorter
SSs (relative to execution time and memory con-
sumption) than the state of the art SS generation
algorithms?
RQ 2.2 Can theQ-learning algorithm be extended to miti-
gate the above-mentioned generalisation problem?
C. Contributions
In this paper, we report what is, to the best our knowledge,
the Ô¨Årst application of reinforcement learning to deriving
369state synchronisation sequences, in order to make model-based
testing more efÔ¨Åcient. In particular, we propose a sequential,
Q-learning (reinforcement learning) algorithm that derives
synchronising sequences from partial and complete systems.
Regarding deriving SSs using machine learning, apart from
the work surveyed before, we are aware of only one closely
related publication [59]. In this work, the authors report a
Deep Learning method to predict the length of the SS without
generating it. We, however, aim to generate the SS from a
given speciÔ¨Åcation.
Regarding our novel Q-graph formalism, we are aware of
two related pieces of work that used different types of data
structures in theQ-learning setting to ease the generalisation
problem. In one approach [60], the Markov Decision Process
(MDP) search space has been represented as a kdimensional
tree which makes it possible to keep unrelated part of the
environment in a database. This reduces the memory cost of
the RL algorithm. A second piece of work [61] shows how the
steps taken by an agent can be represented as a graph. This
graph is then used to aid exploration/exploitation dilemma
using a shortest path algorithm. None of these methods,
however, employ a similar technique to ease the generalisation
problem as the presented method.
OurQ-learning framework, unlike the tabular setting, does
not require prior knowledge of the entire search space. Instead,
it explores and constructs the search space as the agent
interacts with the environment. Therefore with this framework,
we are allowed to apply the Q-learning method to problems
that possess a very large search space using a limited memory
space. For example, the classical tabular Q-learning method
would require 2nrows and in total x2ncells for a given
system with nstates andxinputs.
Through empirical evaluation, we show that the proposed
algorithm is superior to the state of the art sequential and par-
allel algorithms: the proposed algorithm can generate shorter
SSs and 1000 (256) times faster, and can process 256(2048)
times larger speciÔ¨Åcations than the state of the art sequential
(parallel) algorithm. To ensure the scalability of our approach,
we retrieved the speciÔ¨Åcation of an industrial-scale system
to manage the engine status of Oc ¬¥e printers and copiers (a
subsidiary of Canon). The speciÔ¨Åcation has 3410 states and
77inputs; it has been developed in an industrial context and
has been formally cross-checked against the design documents
of the system, from which the implementation code is auto-
matically generated [62]. Our method could derive an SS from
this speciÔ¨Åcation in less than a second where other state of the
art SS generation methods would not terminate in an hour.
Our proposed algorithm serves as a basis for future efÔ¨Åcient
algorithms for large state-space exploration that cannot be
explored exhaustively.
D. Organisation of the paper
The paper is organised as follows. In the next section,
we provide the terminology and notation regarding reactive
systems andQ-learning used throughout the paper. This is
then followed by a section in which we explain the proposedalgorithm. In Section IV, we present the experimental subjects,
conducted experiments, and their results. Later, in Section V,
we discuss the results. Finally, Section VI draws conclusions
and discusses some future research directions.
II. P RELIMINARIES
A. Automata
We use the standard notation A=hS;;Hito denote an
automaton. Here Sis a set of states, is a Ô¨Ånite input alphabet
andHSSis the set of transitions. The set of all
input sequences is represented by ?. We letjS0jdenote the
number of elements in a given set of states.
We let dom-A denote the set of pairs (s;x)2Ssuch
that there exists a transition leaving state swith inputxand we
refer it as ‚Äôx is deÔ¨Åned in state s‚Äô.Ais said to be completely-
speciÔ¨Åed ifdom-A =Sand otherwise A is partial. A
transition of Ais represented by tuple = (s;x;s0)2H,
wheresis the starting state, s0is the ending state, and xis
the input label of the transition .
An inputxis said to be deÔ¨Åned at statesif and only if
(s;x)2dom-A. Otherwise xisundeÔ¨Åned at states.
Awalk of an automaton Ais a sequence (s1;x1;s2)
(s2;x2;s3):::(si;xi;sj)(sj;xj;sk)of consecutive transitions
ofA. The input sequence of this walk !=x1x2:::x ixjis
called its trace, we use "to denote the empty sequence. We can
extend the notion of a deÔ¨Åned input to deÔ¨Åned input sequences
as follows. An input sequence !isdeÔ¨Åned at a given set of
statesS0Sif one of the following is true (i) !is an empty
sequence; or (ii) for any preÔ¨Åx !1of!, with!=!1!2,!1
is deÔ¨Åned at S0and!2is deÔ¨Åned at the states reached from
S0using!1.
Let: 2S?!2Sbe a map specifying the set of
states which can be reached using an input sequence from a
given set of states. Let !=x!0be an input sequence. If
for a given s2S0,!is not deÔ¨Åned, then (S0;!)is also
not deÔ¨Åned. Otherwise the delta function can be deÔ¨Åned in
a recursive way (S0;") =S0,(S0;x) =[s2S0(s;x), and
(S0;!) =((S0;x);!0). If for some pair of states s;s02
S0,(s;!) =(s0;!)then!is said to be a Merging Input
Sequence (MIS). If8si;sj2S;(si;!) =(sj;!)then!is
called a Synchronising Sequence (SS).
In Figure 11, we provided the speciÔ¨Åcation for the ceil-
ing speed monitoring with service brake intervention (SBI)
from [63]. Note that this speciÔ¨Åcations is partial. This
can be completed by adding self-loop transitions that label
missing input, and this completion method is well known
in this Ô¨Åeld [64]. Note that an input sequence SS1=
?c3?c5?c6?c7 resets the automaton to state Normal , i.e.,
(S;?c3?c5?c6?c7) =fNormalg.
While testing a given system, we need to apply a number of
test sequences, all of which start from the reset state (Normal
in this case), and there is a need to reset the system under test
between the execution of these test sequences. Thus, the length
1We did not draw the added transitions in Figure 1.
370of the SS has an impact on test execution time and shorter SSs
are preferable.
The automaton ASBI given in Figure 1 has another SS
SS2=?c3?c0?c7 that also resets the system to state Normal .
SinceSS2is25% shorter, using SS2is preferable for testing.
For example, consider the classical test generation method,
theWmethod [2], [13]. Asymptotically, the Wmethod can
generate a test suite with jSj+jSjjj elements. If we use
SS2instead ofSS1then we would save (jSj+jSjjj) = 40
inputs during testing an implementation of automaton ASBI.
Normal
OverspeedSBrake
WarningEBrake?c0?c1
?c2
?c3?c0?c5?c0?c6
?c7?c1
Fig. 1: Automaton ASBI withS =fNormal;
Warning; Overspeed; S Brake; E Brakeg,
 =f?c0; ?c1;?c2;?c3;?c5;?c6;?c7g.
B.Q-learning environment
Q-learning algorithms can operate on a Markov Decision
Process (MDP) [56]. An MDP is deÔ¨Åned with a tuple P=
(S;A;P;R), where Sis a set of states,Ais a set of actions,P
is the probability function in the form of P(S0jS;a), that is for
each actiona2A and stateS2S, it deÔ¨Ånes the probability of
reaching stateS02SandRis the immediate reward function
in the form ofR(S;a;S0), i.e., it returns the reward received
after transitioning from state StoS0with action a.
TheQ-learning is a value-based reinforcement learning
method which is used to Ô¨Ånd the optimal policy when state
transition probabilities Pareunknown for a given MDP
P= (S;A;P;R). Instead of estimating these unknown
probabilities, the method uses a value function,Q[55]. The
value functionQis recursively deÔ¨Åned as
Q(S;a) =8
>>>><
>>>>:Q(S;a) fR(S;a) +arg max
a0Q(S0;a0)
 Q(S;a)g;ifS=current state, and
an actionais executed
no change; otherwise;
(1)
whereS2 S,a2A, andQ(S;a)is the value of applying
actionaat stateS,R(S;a)is the immediate reward received
after applying action aat stateS,is the learning rate, and 
is the future reward discount factor. , andare values within
the range [0;1].
TheQ-learning algorithm asymptotically reconstructs the
true expected discounted reward [55] and as a result workstowards recovering the optimal policy. In this respect, policy
selection based on Q-learning can be viewed as an off-policy
temporal difference control algorithm which asymptotically
approximates the optimal policy [56].
III. T HE PROPOSED ALGORITHM
A. FromQ-tables toQ-graphs and problem formulation
The naive implementation of the Q-learning algorithm relies
on aQ-table which holds the Q-values for each of the states
of the underlying environment i.e., MDP P[56]. This is a
drastic improvement over cases where complete knowledge of
the transition probabilities Pis required. The use of a Q-table
is straightforward: if the agent wants to learn the Q-value of
a stateS2Sit just reads the information from the table after
it computes its index.
However, consider a scenario in which the agent does not
require the whole Q-table while learning the environment. In
such a case, we may not have to store the Q-values using a
preset table but instead we can use a directed graph. Using
such an approach, we let the graph grow as the agent dis-
covers states on-the-Ô¨Çy. This method will reduce the memory
requirements of the algorithm in situations in which the agent
can Ô¨Ånd an optimum policy based on only a portion of the
state-space, which is the case in our application domain.
AQ-graph has a Ô¨Ånite set of nodes N(Q-nodes), such that
each noden2Nis associated with a state of the environment
(say MDP state)S(nS) and aQ-value (Q(S ;a)) for each
action (a) of P. Intuitively, in the set of admissible edges in
aQ-graph there exists an edge labelled with action afrom
Q-nodentoQ-noden0if and only if one can reach nS0from
nSusing an action a.
Finally, we represent the SS construction problem as an
MDPP. An element S0of the power set of S(S02pow(S))
corresponds to a state (S02S) of the MDP Pand each input
x2XofAis an action a2A ofP.
We use one-to-one and onto functions to denote correspond-
ing inputs and set of states: (i) i()maps an input x2of the
automatonAto the corresponding action a2A of the MDP
Pand vice versa, and (ii) st() maps a set of states S02P(S)
of the automaton Ato the corresponding state S02Sof the
MDPPand vice versa.
For eachSanda, we let
P(S0jS;a) =
1;if and only if (st(S );i(a)) =st(S0);
0;otherwise:
That is inPthere exists a transition from a given state
Sto another stateS0labelled with action aif and only if
(st(S );i(a)) =st(S0).
The immediate rewards (R (S;a)) are computed as follows,
letS0be the next MDP state such that (st(S );i(a)) =st(S0).
R(S;a) =8
><
>: 1; ifjst(S )j=jst(S0)j,
NAN; ifi(a)is undeÔ¨Åned for st(S)
100=jst(S )j; else ( =jst(S )j jst(S0)j).
The above formulation introduces a heuristic that helps to
break ties when we have a set of merging inputs. The heuristic
371step considers the number of merged states and promotes the
input that is causing more states to merge. Note that when jS0j
is equal tojSj, the reward is 1, this is a step to prevent the
agent from introducing redundant inputs to SS. Moreover, with
immediate reward NAN , the proposed algorithm can derive
SSs from partial systems without worrying about constructing
SSs with undeÔ¨Åned inputs. However, the algorithm may gen-
erate longer SSs when the underlying system is partial. This is
due to the fact that the length of a shortest SS for a complete
system is bounded by O(n2)[26] and for a partial system the
bound isO(n24n=3)[65].
Finally, note that the for a given SS construction problem
instance, the constructed MDP is Ô¨Ånite. We formally state this
property in the Corollary below
Corollary 1. LetAbe an automaton, the MDP Pconstructed
fromAfor constructing an SS is a Ô¨Ånite MDP .
B. The Algorithm
Before going into the details of the algorithm we will
introduce some basic concepts that the algorithm uses. The
proposed algorithm uses an -greedy approach [56]. Using this
approach we addressed the exploration and the exploitation
trade off. With probability the agent chooses inputs ran-
domly. Otherwise it selects inputs according to the Q-values,
which are computed using the standard Q-value function given
in Formula 1 where the next state (S0) is computed using the 
function, i.e.,S0=(st(S );i(a)). The learning rate and future
reward discount values were both set to 0:9, i.e.,= 0:9 and
= 0:92.
The algorithm receives an automaton Aand an upper-bound
on the number of episodes3(E) as its inputs. Then it constructs
theQ-graph. The construction of the Q graph is done by
introducing a single Q-node: a node that is associated with the
set of states nS=SofAwith randomQ-values. (Lines 1-3 of
Algorithm 1). The graph then gradually grows by introducing
new nodes while the agent explores the environment. Once the
initial node has been created, the algorithm initialises the graph
with an empty input sequence and a pointer to the initial node
(initialNode ) that will be used when the algorithm wants
to reach the Ô¨Årst node of the Q-graph i.e., when it picks an
undeÔ¨Åned input. The algorithm also sets the episode counter
to0(Line 4 of Algorithm 1).
Afterwards it enters a loop that ends when (i) it Ô¨Ånds an
SS, i.e., it reaches a node n0such thatjn0
Sj= 1, or (ii) the
maximum number of episodes has been reached. Note that in a
Ô¨Ånite MDP theQ-learning algorithm converges after a number
of episodes, i.e., runs [56]. However, not all automata possess
an SS. When there is no SS, the agent will repeatedly compute
!and never succeed. As a result of this observation, the
algorithm requires an upper-bound on the number of episodes
as input (E ).
2Theandvalues were manually set based on controlled experiments
during which we measure the total rewards gained. This is then followed by
the experiments. That is hyperparameters andwere constant throughout
the experiments.
3Throughout the experiments Ewas set to 1,000,000 episodes.Input: AutomatonA= (S;;H)such thatjSj>1,E
Output: An SS!forA
begin
1 Initiate aQ nodensuch thatnS st(S):
2 foreachx2do
3 Assign a random value to Q(nS;i(x)) ofn.
end
4InitialNode n,n0 n,N N[n,! ",e 0.
5 whilee<E do
6 foreachx2do
7 ifn00withn00
S=(st(n0
S);x)does not exist in N
then
8 Introducen00toNsuch that
n00
S=(st(n0
S);x)having random
Q-values.
9 Introduce an edge from n0ton00labelled with
i(x).
end
end
10e e+ 1, Pick a random value rin the range (0;1].
11 ifrthen
12 Select a random input x, updaten0, andQ-values.
end
13 else
14 SelectxusingQ(nS0;i(x)), update n0, and
Q-values.
end
15 ifR(n0
Sji(x)) =NAN then
16 n0 InitialNode, !=".
end
17 else
18 ! !x.
19 ifjn0
Sj= 1 then
20 return!.
end
end
end
21 return".
end
Algorithm 1: TheQ-Synch algorithm.
At each iteration, the algorithm picks the current node (n0),
extracts the set of states nS0and it checks if all the adjacent
nodes ofn0are inNi.e., for each input x2it checks if
an adjacent node n00is included in the Q-Graph. If not, then
it introduces the missing Q-nodes with random Q-values, and
introduces the edge information (Lines 6-9 of Algorithm 1).
After this the algorithm increments e, updates the input
symbol (x), the current Q-node (n0), andQvalues according
to the-greedy method (Lines 10-14 of Algorithm 1).
If the immediate reward is NAN , then the algorithm clears
!, sets the current node as the initial node, and repeats the
process. Otherwise, it appends the input to !and checks
whether the current node is associated with a singleton set or
not. If so, the algorithm returns !as the SS (Lines 15-20 of
Algorithm 1). Otherwise it repeats the above mentioned pro-
cess. If the upper-bound on the number of episodes is reached
and no SS has been found then the algorithm terminates and
returns an empty !.
The above algorithm can generate an SS from a given Aif
372and only ifAhas an SS and a suitably large episode value E
is provided.
Proposition 1. TheQ-Synch algorithm can construct an SS
from an automaton Aif and only if Ahas one and Eis
sufÔ¨Åciently large.
Proof.!!!This part follows from the Corollary 1, and the fact
thatQ-learning algorithm Ô¨Ånds the optimal policy in Ô¨Ånite
MDPs [66].
   Now assume that the Q-Synch algorithm returns a
non-empty input sequence !but this is not a synchronising
sequence. We consider two cases (i) there exists s;s02S
such that(s;!)6=(s0;!)and (ii) there exist s2Ssuch
that(s;!)is not deÔ¨Åned.
Since!is non-empty the algorithm must return when a
singleton set is reached. Therefore j(S;! )j= 1and (i) cannot
be true. Next, assume that !is in the form of !0x!00where
!0and!00are sequences. Let us assume that (s;!0) =s0but
(s0;x)is not deÔ¨Åned. We need to consider two sub-cases: (a)
!0is an SS for A, and (b)!0is not an SS for A. Note that the
algorithm returns as soon as it reaches an SS, so (a) cannot
be true. If (b) happens then the algorithm should clear !and
cannot return a non-empty sequence. Therefore (ii) cannot be
true. Hence the result follows.
IV. E XPERIMENTS
In this section, we provide the details of the controlled
experiments conducted to answer our research questions. We
Ô¨Årst recall the research questions and the evaluation criteria,
focusing on which aspects of the algorithms were compared.
This is then followed by a description of the experimental
subjects. Next, we outline the benchmark algorithms used as
a baseline and the experiment environment used. Finally, we
provide the results of the experiments.
A. Research Questions and Evaluation Criteria
The following research questions were posed at the outset
(in Section 1):
RQ 1 Is our proposed Q-learning algorithm more efÔ¨Åcient and
scalable than the state of the art algorithms?
RQ 1.1 Is the growth of the execution time more modest in
theQ-learning algorithm?
RQ 1.2 How scalable can a Q-learning algorithm be? By
Ô¨Åxing the amount of memory and execution time, can
aQ-learning algorithm generate SSs from speciÔ¨Åca-
tions that cannot be processed by the state of the art
SS generation algorithms?
RQ 1.3 How does the memory usage behaviour of the Q-
learning algorithm compare to state of the art SS
generation algorithms?
RQ 2 Is our proposed Q-learning algorithm more effective in
generating shorter synchronising sequences for a larger
class of system models?
RQ 2.1 Does theQ-learning algorithm generate shorter SSs
(relative to execution time and memory consumption)
than the state of the art SS generation algorithms?
RQ 2.2 Can theQ-learning algorithm be extended to mitigate
the above-mentioned generalisation problem?To answer these questions, we consider a number of eval-
uation criteria. The Ô¨Årst one is the time an algorithm requires
to generate an SS, with a faster algorithm being better. The
second criterion is related to the length of the SSs generated
by the algorithms. Since there is a cost associated with the
application of an SS, we say that an algorithm is better
than others if it generates a shorter sequence for a given
automaton. The last criterion relates to the scalability of the
algorithms. By scalability, we refer to two different aspects
of the algorithms. First, we consider the maximum number of
model states that the algorithm can process in a given time.
Second, we considered the memory used by the algorithm
while constructing SSs. So a scalable algorithm is the one
that can process larger automata and requires less memory
than others.
B. Experiment subjects
We used two sets of automata in the experiments. The
sets (S 1andS2) contained randomly constructed synthetic
automata, where S1had completely speciÔ¨Åed automata and
S2contained partially speciÔ¨Åed automata. Synthetic automata
were constructed using the following procedure.
Letnbe the number of states and pbe the number of inputs.
To generate a completely speciÔ¨Åed automaton with nstates and
pinputs, we Ô¨Årst generated a graph with nnodes (states) and
for each node (state) we randomly generate padjacent nodes.
Then we checked whether the resultant automaton has an SS.
If so, we kept it, we discarded it otherwise.
If the underlying automaton is to be partial then we again
generated a graph with nnodes but this time for each node we
pickedkadjacent nodes from the graph where kwas picked
randomly in the range [1;p]. If the underlying automaton had
an SS then we stored the automaton. We used each (n;p) pair
in whichn2f32;64;128;:::; 131072g andp2f10;16;22g.
For each such (n;p) pair, we randomly generated 100 au-
tomata forS1and another 100automata for S2, resulting in
a total of 7800 automata.
In order to complement the experiments, we also used
a speciÔ¨Åcation of a real software Engine Status Manager
(ESM). An ESM is a piece of control software that is used to
manage the status of the engine in Oc ¬¥e printers and copiers
(a subsidiary of Canon). This example is chosen, because its
structure and behaviour is representative of embedded control
software [67]. Moreover, the ESM model was not retrieved
from its designers/developers but it was learned by another
piece of software LearnLib [68] and rigorous veriÔ¨Åcation has
conÔ¨Årmed that the behavioural model is indicative of the actual
system. The ESM model is partial and has 77 inputs and 3410
states.
C. Benchmark algorithms and experiment environment
ForS1, we compared the Q-synch algorithm with the
fastest sequential algorithm, algorithm called the The Greedy
Method [26]. There are other more recent algorithms such as
FastSynchro and SynchroP that can Ô¨Ånd shorter SSs than the
Greedy approach; however the computational complexities of
373these algorithms are much worse than the Greedy algorithm
and areO(n4jj) andO(n5jj) respectively [30]. An algo-
rithm based on a SAT solver is provided in [69] however this
algorithm is slow and cannot process large automata. For S2,
we compared theQ-synch algorithm against the only existing
algorithm reported in the literature, the parallel brute-force
(parallel BF) SS generation algorithm [15].
The sequential algorithms were implemented in C++, and
compiled using Microsoft Visual Studio, edition 2012. The
parallel BF algorithm was implemented in CUDA 6.0 using
compute capability 2:1. The source-code, the constructed SSs,
the automata in sets S1,S2, and the ESM are publicly
available4. The computer used in the experiments had an Intel
I7 3630QM CPU at 2:0GHz with 8GB RAM equipped with
an NVIDIA Geforce 610M with 2GB memory. The operating
system used was 64-bit Windows 75.
D. Results
In order to evaluate the relative performance of the algo-
rithms, for each automaton A, we separately computed SSs
using the proposed algorithm and the benchmark algorithms.
We recorded the generated SS, the execution time, and mem-
ory required by the algorithms. Throughout the experiments,
we set the execution time limit to one hour and set the memory
limit to 1GB of RAM.
We ran the Greedy Algorithm with automata from set S1.
The Greedy Algorithm could generate SS when n512.
Whenn >512 the memory requirement exceeded the given
limit. We ran the parallel BF algorithm with automata from
setS2, and the parallel BF algorithm could not compute SSs
whenn>64within the given time limit.
In the rest of this section we discuss the results in greater de-
tail. We used R for statistical tests and to generate graphs [70]6.
1) Time comparison: We provided the time comparison
results conducted on S1in Figure 2a. The y-axis values give
the mean ratio of the time taken by the Q-Synch algorithm
to the time taken by the Greedy Method. The results are
promising and show that the proposed Q-synch algorithm is
500times faster than the Greedy algorithm on average. We also
observe that as the number of states increases the difference
increases. When n= 512 theQ-Synch algorithm is about
1000 times faster than the Greedy method.
To investigate the results further, we conducted a statisti-
cal effect size analysis through computing Cohen‚Äôs distance
d[71], using the R tool [70]. The results regarding to the effect
sizes for execution time are given in Table I. The statistical
analyses indicate that the effect size between the population‚Äôs
is large.
We provide the time comparison results conducted on S2
in Figure 3a. Again we observe that the results are promising.
4Code and data are anonymously available at https://Ô¨Ågshare.com/articles/
software/CodeAndData/14478132
5Please note that in [15] the authors used a Tesla K40 GPU to conduct the
experiments which is about 10,000 times faster than the GPU card used in
these experiments.
6The R source codes and the data are anonymously published in https:
//Ô¨Ågshare.com/s/cbf85c54a1ff11674374The proposedQ-synch algorithm was 1000 times faster than
the parallel BF algorithm on average and when n= 64 the
proposed algorithm was 10000 times faster on average. The
result of Cohens‚Äô danalysis is given in Table IV. Results again
indicate that the effect size between the populations is large.
2) Length of constructed SSs: The length comparison re-
sults for the SSs generated for S1are given in Figure 2b.
Similar to before, the yaxis denotes the ratio of the lengths
(length of an SS constructed by the Greedy method/ length
of an SS constructed by the Q-Synch algorithm); therefore,
the higher the value, the better the Q-Synch algorithm. The
results indicate that the Q-Synch algorithm constructs shorter
SSs (30% shorter on average) than the Greedy method. The
difference appears to plateau at around 30% and does not
change with the number of states and inputs. The result of
effect size analysis is given in Table II. The results also
suggest that the populations effect size is slightly different.
Regardless of the number of inputs and the number of states,
the median for the length of SSs generated by the Greedy
method is slightly higher than the median of the length of the
SSs generated by the Q-Synch algorithm.
The results conducted on S2are given in Figure 3b. This
time theyaxis denotes the averages of the ratio of the
lengths of SSs constructed by the parallel BF method to the
lengths of SSs constructed by the Q-Synch algorithm. Since
the parallel BF algorithm is a brute-force algorithm it Ô¨Ånds
one of the shortest SS for each automaton from set S2. The
SSs generated by the Q-Synch algorithm are 38% longer than
the SSs generated by the parallel BF algorithm on average.
One promising observation is that the difference seems to
plateau at around 38% and does not grow with the number
of states or inputs. The Cohen‚Äôs danalysis also indicates that
the median of the lengths of the SSs constructed by the parallel
BF algorithm is smaller the median of the lengths of the SSs
constructed by the Q-Synch algorithm (Table V).
3) Memory requirements: The memory requirements
for automata in S1are given in Figure 2c. The y
axis provides the ratio of the memory required by the
Greedy algorithm to the memory required by the Q-
Synch algorithm. We used WorkingSetSize property of
PROCESS_MEMORY_COUNTERS of Windows API to get the
memory information. The Ô¨Ågure indicates that the Greedy
algorithm requested more memory than the Q-Synch algo-
rithm (30 times more memory on average) and the difference
increases with the number of states. When n= 512 the
Greedy algorithm requires 76times more memory than the
proposed algorithm on average and when n= 1024 the
Greedy algorithm failed to generate SSs within the given
memory limit. Moreover, considering the effect size analysis
in Table III, we see the results of effect size analysis and
that the results are conclusive, the effect size between the two
populations is large.
The memory consumption comparison for the parallel BF
method andQ-Synch algorithm is given in Figure 3c. Again,
theyaxis gives the ratio of the amount of memory required
by the parallel BF algorithm to the memory required by the
37432 64 128 256 512
10 16 22 10 16 22 10 16 22 10 16 22 10 16 220500100015002000
States/InputsTime(E/Q)(a)
32 64 128 256 512
10 16 22 10 16 22 10 16 22 10 16 22 10 16 22123
States/InputsLength(E/Q) (b)
32 64 128 256 512
10 16 22 10 16 22 10 16 22 10 16 22 10 16 22020406080
States/InputsMemory (E/Q) (c)
Fig. 2: Comparison of the performances of the Greedy and the Q-Synch algorithms on dataset S1. Figure 2a summarises the ratio of the
time required to construct SSs, Figure 2b summaries the ratio of the lengths of the SSs, and Figure 2c summarises the ratio of memory
required to construct SSs.
32 64
10 16 22 10 16 2201000020000300004000050000
States/InputsTime(P/Q)
(a)
32 64
10 16 22 10 16 220.000.250.500.75
States/InputsLength(P/Q) (b)
32 64
10 16 22 10 16 2212162024
States/InputsMemory (P/E) (c)
Fig. 3: Comparison of the performances of the parallel BF and the Q-Synch algorithms on dataset S2. Figure 3a summarises the ratio of
the time required to construct SSs, Figure 3b summaries the ratio of the lengths of the SSs, and Figure 3c summarises the ratio of memory
required to construct SSs.
Q-Synch algorithm. The results are similar, with the proposed
algorithm requiring less memory (18% on average) than the
parallel BF algorithm. Similar to before, we can observed
that the ratio increases as the number of states increases. We
again complement the analysis using the Cohen‚Äôs dmetric
(Table VI). Results suggest that the effect size is large,
meaning that the populations are different.
np Cohens‚Äôd
3210 13.863
16 1.837
22 2.488
6410 8.123
16 19.045
22 33.176
12810 30.253
16 47.210
22 63.369
25610 60.639
16 74.934
22 83.587
51210 30.491
16 42.241
22 46.386
TABLE I: Cohens‚Äôd anal-
ysis on the amount of
time required by the al-
gorithms on S1.np Cohens‚Äôd
3210 0.200
16 0.802
22 0.998
6410 0.301
16 0.945
22 1.204
12810 0.588
16 1.173
22 1.518
25610 0.475
16 1.466
22 1.648
51210 0.618
16 1.427
22 2.272
TABLE II: Cohens‚Äôd
analysis on the lengths
of SSs constructed by
the algorithms on S1.np Cohens‚Äôd
3210 43.385
16 32.861
22 6.456
6410 1394.707
16 33.283
22 1215.009
12810 1599.230
16 452.060
22 481.147
25610 1005.881
16 852.319
22 825.083
51210 2307.331
16 2426.511
22 1901.071
TABLE III: Cohens‚Äôd
analysis on the memory
requirements of the
algorithms on S1.4) Results on the benchmark automaton: Recall that the
ESM speciÔ¨Åcation is partially speciÔ¨Åed. To conduct the exper-
iments we generated two versions of the ESM speciÔ¨Åcation
(v1,v2).v1was generated by completing the missing transi-np Cohens‚Äôd
3210 0.870
16 0.751
22 1.221
6410 0.887
16 1.125
22 1.511
TABLE IV: Cohens‚Äôd
analysis on the amount
of time required by the
algorithms on S2.np Cohens‚Äôd
3210 -2.304
16 -2.586
22 -3.058
6410 -2.393
16 -2.748
22 -2.866
TABLE V: Cohens‚Äôd
analysis on the lengths
of SSs constructed by
the algorithms on S2.np Cohens‚Äôd
3210 89.739
16 105.909
22 115.326
6410 132.892
16 235.355
22 260.244
TABLE VI: Cohens‚Äôd
analysis on the memory
requirements of the
algorithms on S2.
tions to obtain a completely speciÔ¨Åed model. Completing a
partial model is a well known approach in MBT [64], [72]. To
complete the missing transitions, we Ô¨Årst introduced an error
state and for each state sand for each missing transition
labeled by an input x, we introduced a transition from sto the
error state with inputx. The second version of ESM was
the original partial version. We then investigated the model and
discovered that the ESM model does not possess an SS. This
is because state pairs (s524;s 721), (s70;s 71), (s304;s 3088),
(s344;s 2933), and (s1080;s 3258) are absorbing pairs. An
absorbing pair is a pair of states (si;sj)such that for each
inputx2we have that (si;x) =sjand(sj;x) =si.
That is, no transitions leave these pairs. In order to be able
to use the automaton in the experiments, for each of these 10
states, we modiÔ¨Åed transitions labelled with a common input
375symbolI21:1 such that they all merge at state s524.
After the above modiÔ¨Åcations were made, we provided v1
to the Greedy and the Q-Synch algorithms and provided v2
to the parallel BF and the Q-Synch algorithms as input. In
the experiments, the Greedy and the parallel BF algorithms
could not generate SSs. As in the case of randomly generated
automata, the Greedy algorithm failed to construct an SS due
to memory problem and similarly the parallel BF algorithm
could not Ô¨Ånish computing an SS within one hour. However,
theQ-Synch algorithm did generate an SS for these speciÔ¨Å-
cations. The lengths of the SSs for v1andv2were 356 and
374 respectively and they reset the system to state s524 as
expected. TheQ-Synch algorithm was able to generate the
sequences in 351milliseconds and used 326MBs of RAM on
average.
5) Scalability: As indicated earlier, we investigated the
scalability of the algorithms with respect to (i) the maximum
number of states, and (ii) amount of memory required to
construct SSs. In Figure 4, we provided the average times
required to construct SSs from S1andS2using theQ-Synch
algorithm. The proposed algorithm is able to construct SSs
from speciÔ¨Åcations with 131072 states and 22inputs. Since
the Greedy and the parallel BF algorithms could only generate
SSs whenn512andn64, respectively, with respect to
number of states the proposed algorithm is 256 times more
scalable than the Greedy algorithm. Moreover, the Q-Synch
algorithm is 2048 times more scalable than the parallel BF
algorithm.
In Figure 5, we present the results for S1. Here, each
value is the mean memory required for the Greedy algorithm
(Figure 5a) to process the given size of automata in S1.
Similarly, Figure 5b gives the mean memory usage by the
implementation of the parallel BF algorithm for S2. Moreover,
in Figure 6, we give the mean memory required, in MB, by
the proposed algorithm to construct SSs (S 1in Figure 6a, and
S2in Figure 6b).
The memory requirement of the proposed algorithm does
not grow as fast as that of the other algorithms. To investigate
this, as theQ-learning algorithm uses Q-nodes, we checked
the mean number of Q-nodes created while computing SSs.
In Figure 7, we provided these values (mean number of Q-
nodes constructed by the proposed algorithm): Figure 7a for
S1, and Figure 7b for S2. We observe that the Q-Synch
algorithm is very economic in the sense that it generates Q-
nodes tentatively. This is important and implies that the Q-
learning algorithm selects its inputs wisely. If this was not the
case, in each episode the agent would select different inputs
and introduce new nodes to the Q-graph.
V. D ISCUSSIONS
In this section, in light of the conducted experiments, we
discuss the answers to the research questions. This is then
followed by an analysis of some threats to validity.A. Answers to the research questions
RQ 1 Is our proposed Q-learning algorithm more efÔ¨Åcient and scalable than the
state of the art algorithms?
Answer 1: The results of the experiments suggest that our
algorithm is more efÔ¨Åcient and scalable than the most scalable
algorithms reported in the literature. However, we also noted
that the number of inputs and states have negative impact on
the scalability of the proposed method.
RQ 1.1 Is the growth of the execution time more modest in the Q-learning
algorithm?
Answer 1.1: The results indicate that the time requirement
of the proposed algorithm is modest and its growth rate is
much slower than the state of the art SS generation algorithms.
Results indicate that the proposed algorithm is 500/1000 times
faster than the fastest sequential/parallel algorithm on average.
RQ 1.2 How scalable can a Q-learning algorithm be? By Ô¨Åxing the amount
of memory and execution time, can a Q-learning algorithm generate SSs from
speciÔ¨Åcations that cannot be processed by the state of the art SS generation
algorithms?
Answer 1.2: The proposed algorithm can quickly generate
SSs while using less memory. The experimental results show
that the proposed method can process 256times larger speci-
Ô¨Åcations than the state of the art sequential algorithm. What is
more, the proposed algorithm is 2048 times more scalable than
the state of the art GPU based massively parallel algorithm.
Finally, the experimental results suggest that when there are
limited computation resources, the proposed algorithm can
generate SSs from real speciÔ¨Åcations where other algorithms
cannot.
RQ 1.3 How does the memory usage behaviour of the Q-learning algorithm compare
to state of the art SS generation algorithms?
Answer 1.3: The proposed algorithm requires neither a
product automaton nor a preset Q-table to be built; therefore,
the memory requirement of the proposed method grows much
slower than the state of the art sequential SS generation
method. Experimental study indicates that the Greedy algo-
rithm requires 30times more memory than the proposed
algorithm on average. The parallel BF SS generation algorithm
on the other hand requires 18times more memory than the
proposed algorithm.
RQ 2 Is our proposed Q-learning algorithm more effective in generating shorter
synchronising sequences for a larger class of system models?
Answer 2 The result of the experiments indicate that the
proposed algorithm is more effective in generating short SSs.
We compared the results with the Greedy algorithm and the
results suggested that the proposed algorithm Ô¨Ånds SSs that
are30% shorter on average.
37632 64 128 256 512 1024 2048 4096 8192 16384 32768 65536 131072
10 16 22 10 16 22 10 16 22 10 16 22 10 16 22 10 16 22 10 16 22 10 16 22 10 16 22 10 16 22 10 16 22 10 16 22 10 16 2201000020000300004000050000
States/InputsTime (msecs)(a)
32 64 128 256 512 1024 2048 4096 8192 16384 32768 65536 131072
10 16 22 10 16 22 10 16 22 10 16 22 10 16 22 10 16 22 10 16 22 10 16 22 10 16 22 10 16 22 10 16 22 10 16 22 10 16 22010000200003000040000
States/InputsTime (msecs) (b)
Fig. 4: The average amount of time spent to generate SSs from S1(Figure 4a) and from S2by using theQ-Synch algorithm
(Figure 4b).
32 64 128 256 512
10 16 22 10 16 22 10 16 22 10 16 22 10 16 220100200300
States/InputsMemory (MBs)
(a)
32 64
10 16 22 10 16 224050607080
States/InputsMemory (MBs) (b)
Fig. 5: The averages of memory requirement to generate SSs from
S1using The Greedy Method (Figure 5a) and from S2using the
parallel BF algorithm (Figure 5b).
RQ 2.1 Does the Q-learning algorithm generate shorter SSs (relative to execution
time and memory consumption) than the state of the art SS generation algorithms?
Answer 2.1: The proposed algorithm can generate shorter
SSs and consumes much less memory and time than the
fastest sequential SS generation algorithm does. Moreover the
proposed algorithm can compute SSs that are comparable in
length with the parallel BF SS generation algorithm.
RQ 2.2 Can the Q-learning algorithm be extended to mitigate the above-mentioned
generalisation problem?
Answer 2.2: We introduced a new Q-learning framework in
which we abandoned the idea of keeping the entire search
space in a preset form. Instead, we employ a method where the
search space grows as the agent interacts with the environment.
This allows us to represent the search space using a Q-
graph. We used this formalism in the classical Q-learning
algorithm. To our knowledge, this is new and experimental
studies showed that it allows agents to learn Q-values from
large environments.
Experimental evaluation indicates that using the Q-graph
formalism we can solve problems using a search space whose
size is a fraction of the size of the search space needed in
the classicalQ-table setting. For example, as presented in
Figure 7a, and Figure 7b, our method needs 70Q-nodes to
derive an SS from an automaton that has jSj= 131072 statesandjj= 22 inputs on average. If we were using a tabular
setting, we would generate a table having pow(S)jj =
(2131072 1)22cells which would not be possible.
B. Threats to validity
There are number of threats to the validity of the experi-
ments. The Ô¨Årst threat is to generalisability which originates
from the fact that the experimental subjects may not be
representative of real systems. The use of randomly generated
automata clearly introduces such a threat and we addressed
this by creating two versions of the speciÔ¨Åcation of Engine
Status Manager software, which has 3410 states and 77inputs.
Importantly, the experimental results obtained with these real-
world models are similar to those obtained with randomly
generated automata.
There are also threats to internal validity and the possibility
that one or more of the implementations were incorrect. To
reduce this threat, we applied unit testing in the development
cycle. Moreover, when a sequence (! ) was generated by an
algorithm, we checked that the generated sequence was an SS.
To achieve this, when an SS !was computed for an automaton
A, we randomly selected an initial state (s) of Aand, starting
froms, we applied !to Ô¨Ånd the reset state s0. Clearlys0
should be the state that the Areaches regardless of the initial
state from which !is applied. To conÔ¨Årm this, for every state
s00ofA, we checked that the application of !ins00tookAto
s0. Throughout the experiments we did not encounter a case
where the underlying automaton failed to reach the reset state.
Finally, there is potential to misinterpret the results obtained
from the experiments. To address this threat, we validated our
results by conducting Cohen‚Äôs deffect size analysis.
VI. C ONCLUSION
Model based testing (MBT) is an increasingly important
type of software testing. Most MBT techniques, require some
method that brings the system under test (SUT) to a speciÔ¨Åc
initial state in order for a test sequence to be applied to the
SUT. This requirement can be fulÔ¨Ålled by a synchronising
sequence (SS) [38]. The length of an SS used affects the cost
of test execution and so there has been long standing interest in
the problem of Ô¨Ånding a short SS [21]. However, the problem
of generating short SSs is known to be NP-hard.
The other motivation for the work described in this paper
comes from the fact that previous work has developed a variety
37732 64 128 256 512 1024 2048 4096 8192 16384 32768 65536 131072
10 16 22 10 16 22 10 16 22 10 16 22 10 16 22 10 16 22 10 16 22 10 16 22 10 16 22 10 16 22 10 16 22 10 16 22 10 16 220200400600
States/InputsMemory (MBs)(a)
32 64 128 256 512 1024 2048 4096 8192 16384 32768 65536 131072
10 16 22 10 16 22 10 16 22 10 16 22 10 16 22 10 16 22 10 16 22 10 16 22 10 16 22 10 16 22 10 16 22 10 16 22 10 16 220200400600
States/InputsMemory (MBs) (b)
Fig. 6: The averages of the memory requirement of the Q-Synch algorithm when generating SSs from S1(Figure 6a) and from S2
(Figure 6b).
32 64 128 256 512 1024 2048 4096 8192 16384 32768 65536 131072
10 16 22 10 16 22 10 16 22 10 16 22 10 16 22 10 16 22 10 16 22 10 16 22 10 16 22 10 16 22 10 16 22 10 16 22 10 16 22255075100
States/InputsNumber of Q Nodes
(a)
32 64 128 256 512 1024 2048 4096 8192 16384 32768 65536 131072
10 16 22 10 16 22 10 16 22 10 16 22 10 16 22 10 16 22 10 16 22 10 16 22 10 16 22 10 16 22 10 16 22 10 16 22 10 16 220255075
States/InputsNumber of Q Nodes (b)
Fig. 7: The averages of the number of Q-nodes generated while constructing SSs from S1(a) andS2(b) by using theQ-Synch
algorithm.
of successful automated test generation methods based on
Reinforcement Learning (RL) [7], [47], [52]. In this paper, we
proposed a newQ-learning algorithm to derive synchronising
sequences. The proposed method introduces the notion of Q-
graph which, instead of holding the entire search space in
memory, allows the search space to be expanded on-the-Ô¨Çy.
Experimental results indicate that the proposed method is more
efÔ¨Åcient and effective in generating SSs than the state of the
art SS generation methods.
There are a number of lines of future work. First, we
will investigate the implications of the introduced Q-learning
framework on other state-exploration problems in model-based
testing and beyond. Instead of holding the entire search space
in a table, ourQ-learning framework allows the search space
to be represented as a graph and therefore uses less memory
space, allowing learning from very large search spaces. This
study might lead to new RL algorithms. Besides, there may
be scope to investigate other RL approaches for deriving
SSs. Further potential directions include extensions of the
framework to probabilistic automata with unknown transition
probabilities. Moreover, it would be interesting to study the
effect of shorter SS on testing. Since the impact will depend
on the test technique used, a systematic evaluation of different
test methods should be carried out.
Finally, the experimental results suggest that as the number
of states and inputs of the automata grow, the time and memory
requirements of the method increase. Although this is unsur-prising, we plan to explore approaches, such as parallelisation,
that might allow the technique to scale further.
Acknowledgements We have been partially supported by the
UKRI Trustworthy Autonomous Systems Node in VeriÔ¨Åability,
Grant Award Reference EP/V026801/1.
REFERENCES
[1] S. R. Dalal, A. Jain, N. Karunanithi, J. M. Leaton, C. M. Lott, G. C.
Patton, and B. M. Horowitz, ‚ÄúModel-based testing in practice,‚Äù in Pro-
ceedings of the 1999 International Conference on Software Engineering
(IEEE Cat. No.99CB37002), 1999, pp. 285‚Äì294.
[2] T. S. Chow, ‚ÄúTesting software design modeled by Ô¨Ånite-state machines,‚Äù
IEEE Trans. Software Eng., vol. 4, no. 3, pp. 178‚Äì187, 1978.
[3] E. Brinksma, ‚ÄúA theory for the derivation of tests,‚Äù in Proceedings of
Protocol SpeciÔ¨Åcation, Testing, and VeriÔ¨Åcation VIII. Atlantic City:
North-Holland, 1988, pp. 63‚Äì74.
[4] D. Lee and M. Yannakakis, ‚ÄúTesting Ô¨Ånite-state machines: State iden-
tiÔ¨Åcation and veriÔ¨Åcation,‚Äù IEEE Transactions on Computers, vol. 43,
no. 3, pp. 306‚Äì320, 1994.
[5] K. Sabnani and A. Dahbura, ‚ÄúA protocol test generation procedure,‚Äù
Computer Networks, vol. 15, no. 4, pp. 285‚Äì297, 1988.
[6] D. P. Sidhu and T.-K. Leung, ‚ÄúFormal methods for protocol testing: A
detailed study,‚Äù IEEE Transactions on Software Engineering, vol. 15,
no. 4, pp. 413‚Äì426, 1989.
[7] N. Walkinshaw, R. Taylor, and J. Derrick, ‚ÄúInferring extended Ô¨Ånite
state machine models from software executions,‚Äù in 2013 20th Working
Conference on Reverse Engineering (WCRE), 2013, pp. 301‚Äì310.
[8] D. Damasceno, M. R. Mousavi, and A. Sim Àúao, ‚ÄúLearning to reuse:
Adaptive model learning for evolving systems,‚Äù in Integrated Formal
Methods - 15th International Conference, IFM 2019, ser. Lecture Notes
in Computer Science, vol. 11918. Springer, 2019, pp. 138‚Äì156.
378[9] R. M. Hierons, ‚ÄúMinimizing the number of resets when testing from a
Ô¨Ånite state machine,‚Äù Information Processing Letters, vol. 90, no. 6, pp.
287‚Äì292, 2004.
[10] F. C. Hennie, ‚ÄúFault-detecting experiments for sequential circuits,‚Äù in
Proceedings of Fifth Annual Symposium on Switching Circuit Theory
and Logical Design, Princeton, New Jersey, 1964, pp. 95‚Äì110.
[11] G. V . Jourdan, H. Ural, and H. Yenigun, ‚ÄúReduced checking sequences
using unreliable reset,‚Äù Inf. Process. Lett., vol. 115, no. 5, pp. 532‚Äì535,
2015. [Online]. Available: http://dx.doi.org/10.1016/j.ipl.2015.01.002
[12] R. Boute, ‚ÄúDistinguishing sets for optimal state identiÔ¨Åcation in checking
experiments,‚Äù IEEE Transactions on Computers, vol. 23, no. 8, pp. 874‚Äì
877, 1974.
[13] M. Vasilevskii, ‚ÄúFailure diagnosis of automata,‚Äù Cybernetics, vol. 9,
no. 4, pp. 653‚Äì665, 1973. [Online]. Available: http://dx.doi.org/10.
1007/BF01068590
[14] B. K. Natarajan, ‚ÄúAn algorithmic approach to the automated design of
parts orienters,‚Äù in FOCS, 1986, pp. 132‚Äì142.
[15] U. C. T ¬®urker, ‚ÄúParallel brute-force algorithm for deriving reset sequences
from deterministic incomplete Ô¨Ånite automata,‚Äù Turkish Journal of
Electrical Engineering & Computer Sciences, vol. 27, pp. 3544‚Äì3556,
09 2019.
[16] U. C. T ¬®urker and H. Yenig ¬®un, ‚ÄúComplexities of some problems related
to synchronizing, non-synchronizing and monotonic automata,‚Äù Interna-
tional Journal of Foundations of Computer Science, vol. 26, no. 01, pp.
99‚Äì121, 2015.
[17] D. S. Ananichev and M. V . V olkov, ‚ÄúSynchronizing monotonic au-
tomata,‚Äù Theor. Comput. Sci., vol. 327, no. 3, pp. 225‚Äì239, 2004.
[18] ‚Äî‚Äî, ‚ÄúSynchronizing generalized monotonic automata,‚Äù Theor. Comput.
Sci., vol. 330, no. 1, pp. 3‚Äì13, 2005. [Online]. Available: http:
//dx.doi.org/10.1016/j.tcs.2004.09.006
[19] A. N. Trakhtman, ‚ÄúSome results of implemented algorithms of synchro-
nization,‚Äù in 10th Journees Montoises d‚ÄôInform, 2004.
[20] ‚Äî‚Äî, ‚ÄúSynchronization of some DFA,‚Äù in Theory and Applications
of Models of Computation, 4th International Conference, TAMC 2007,
Shanghai, China, May 22-25, 2007, Proceedings, 2007, pp. 234‚Äì243.
[21] ‚Äî‚Äî, ‚ÄúModifying the upper bound on the length of minimal synchroniz-
ing word,‚Äù in Fundamentals of Computation Theory - 18th International
Symposium, FCT 2011, Oslo, Norway, August 22-25, 2011. Proceedings,
2011, pp. 173‚Äì180.
[22] ‚Äî‚Äî, ‚ÄúThe length of a minimal synchronizing word and the ÀáCerny
conjecture,‚Äù CoRR, vol. abs/1405.2435, 2014.
[23] ‚Äî‚Äî, ‚ÄúSome aspects of synchronization of DFA,‚Äù J. Comput. Sci.
Technol., vol. 23, no. 5, pp. 719‚Äì727, 2008.
[24] M. V . V olkov, ‚ÄúSynchronizing automata and the Àácerny conjecture,‚Äù in
LATA, 2008, pp. 11‚Äì27.
[25] ‚Äî‚Äî, ‚ÄúSynchronizing automata preserving a chain of partial orders,‚Äù
Theor. Comput. Sci., vol. 410, no. 37, pp. 3513‚Äì3519, 2009.
[26] D. Eppstein, ‚ÄúReset sequences for monotonic automata,‚Äù SIAM J.
Comput., vol. 19, no. 3, pp. 500‚Äì510, 1990.
[27] Y . Benenson, T. Paz-Elizur, A. Rivka, E. Keinan, Z. Livneh, and
E. Shapiro, ‚ÄúProgrammable and autonomous computing machine made
of biomolecules,‚Äù Nature, vol. 414, no. 6862, pp. 430‚Äì434, 2001.
[Online]. Available: http://dx.doi.org/10.1038/35106533
[28] A. Roman, ‚ÄúNew algorithms for Ô¨Ånding short reset sequences in syn-
chronizing automata,‚Äù in International Enformatika Conference, IEC‚Äô05,
August 26-28, 2005, Prague, Czech Republic, CDROM, 2005, pp. 13‚Äì17.
[29] O. RaÔ¨Åq and L. Cacciari, ‚ÄúCoordination algorithm for distributed test-
ing,‚Äù The Journal of Supercomputing, vol. 24, no. 2, pp. 203‚Äì211, 2003.
[30] R. Kudlacik, A. Roman, and H. Wagner, ‚ÄúEffective synchronizing
algorithms,‚Äù Expert Systems with Applications, vol. 39, no. 14, pp.
11 746‚Äì11 757, 2012.
[31] A. Roman and M. Szykula, ‚ÄúForward and backward synchronizing
algorithms,‚Äù Expert Syst. Appl., vol. 42, no. 24, pp. 9512‚Äì9527, 2015.
[32] R. Raz and S. Safra, ‚ÄúA sub-constant error-probability low-degree test,
and a sub-constant error-probability PCP characterization of NP,‚Äù in
STOC, 1997, pp. 475‚Äì484.
[33] A. Roman, ‚ÄúGenetic algorithm for synchronization,‚Äù in Language and
Automata Theory and Applications, Third International Conference,
LATA 2009, Tarragona, Spain, April 2-8, 2009. Proceedings, 2009, pp.
684‚Äì695.
[34] ‚Äî‚Äî, ‚ÄúSynchronizing Ô¨Ånite automata with short reset words,‚Äù Applied
Mathematics and Computation, vol. 209, no. 1, pp. 125‚Äì136, 2009.[35] A. Kisielewicz, J. Kowalski, and M. Szyku≈Ça, ‚ÄúA fast algorithm Ô¨Ånding
the shortest reset words,‚Äù Computing and Combinatorics, vol. 7936, pp.
182‚Äì196.
[36] S. Karahoda, O. T. Erenay, K. Kaya, U. C. T ¬®urker, and H. Yenig ¬®un,
‚ÄúMulticore and manycore parallelization of cheap synchronizing
sequence heuristics,‚Äù J. Parallel Distributed Comput., vol. 140, pp. 13‚Äì
24, 2020. [Online]. Available: https://doi.org/10.1016/j.jpdc.2020.02.009
[37] A. N. Trakhtman, ‚ÄúModifying the upper bound on the length of minimal
synchronizing word,‚Äù ArXiv e-prints, 2011.
[38] A. Gill, Introduction to The Theory of Finite State Machines. McGraw-
Hill, New York, 1962.
[39] Z. Kohavi, Switching and Finite State Automata Theory. McGraw-Hill,
New York, 1978.
[40] C. G ¬®unic ¬∏en, E. Erdem, and H. Yenig ¬®un, ‚ÄúGenerating shortest
synchronizing sequences using answer set programming,‚Äù CoRR, vol.
abs/1312.6146, 2013. [Online]. Available: http://arxiv.org/abs/1312.6146
[41] M. I. Jordan and T. M. Mitchell, ‚ÄúMachine learning: Trends,
perspectives, and prospects,‚Äù Science, vol. 349, no. 6245, pp. 255‚Äì260,
2015. [Online]. Available: https://science.sciencemag.org/content/349/
6245/255
[42] D. Chapman and L. P. Kaelbling, ‚ÄúInput generalization in delayed
reinforcement learning: An algorithm and performance comparisons,‚Äù
inProceedings of the 12th International Joint Conference on ArtiÔ¨Åcial
Intelligence - Volume 2, ser. IJCAI‚Äô91. San Francisco, CA, USA:
Morgan Kaufmann Publishers Inc., 1991, p. 726‚Äì731.
[43] H. Almulla and G. Gay, ‚ÄúLearning how to search: Generating effective
test cases through adaptive Ô¨Åtness function selection,‚Äù ArXiv, vol.
abs/2102.04822, 2021.
[44] S. Bauersfeld and T. V os, ‚ÄúA reinforcement learning approach to
automated GUI robustness testing,‚Äù 2012.
[45] J. Eskonen, J. Kahles, and J. Reijonen, ‚ÄúAutomating GUI testing with
image-based deep reinforcement learning,‚Äù 2020 IEEE International
Conference on Autonomic Computing and Self-Organizing Systems
(ACSOS), pp. 160‚Äì167, 2020.
[46] L. R. Harries, R. S. Clarke, T. Chapman, S. V . P. L. N. Nallamalli,
L.¬®Ozg¬®ur, S. Jain, A. Leung, S. Lim, A. Dietrich, J. M. Hern ¬¥andez-
Lobato, T. Ellis, C. Zhang, and K. Ciosek, ‚ÄúDrift: Deep reinforcement
learning for functional software testing,‚Äù ArXiv, vol. abs/2007.08220,
2020.
[47] A. Reichstaller, B. Eberhardinger, A. Knapp, W. Reif, and M. Gehlen,
‚ÄúRisk-based interoperability testing using reinforcement learning,‚Äù in
Testing Software and Systems, F. Wotawa, M. Nica, and N. Kushik,
Eds. Cham: Springer International Publishing, 2016, pp. 52‚Äì69.
[48] T. V os, P. Tonella, I. Prasetya, P. M. Kruse, O. Shehory, A. Bagnato,
and M. Harman, ‚ÄúThe FITTEST tool suite for testing future internet
applications,‚Äù in FITTEST@ICTSS, 2013.
[49] X. Qin, N. Ar ¬¥echiga, A. Best, and J. Deshmukh, ‚ÄúAutomatic testing
and falsiÔ¨Åcation with dynamically constrained reinforcement learning,‚Äù
2020.
[50] H. Yasin, S. Hamid, and R. Yusof, ‚ÄúDroidbotx: Test case generation tool
for android applications using Q-learning,‚Äù Symmetry, vol. 13, p. 310,
02 2021.
[51] M. Veanes, P. Roy, and C. Campbell, ‚ÄúOnline testing with reinforcement
learning,‚Äù in Formal Approaches to Software Testing and Runtime
VeriÔ¨Åcation, K. Havelund, M. N ¬¥uÀúnez, G. Ros ¬∏u, and B. Wolff, Eds.
Berlin, Heidelberg: Springer Berlin Heidelberg, 2006, pp. 240‚Äì253.
[52] J. Zhang, Z. Wang, L. Zhang, D. Hao, L. Zang, S. Cheng, and L. Zhang,
‚ÄúPredictive mutation testing,‚Äù in Proceedings of the 25th International
Symposium on Software Testing and Analysis, ser. ISSTA 2016. New
York, NY , USA: Association for Computing Machinery, 2016, p.
342‚Äì353. [Online]. Available: https://doi.org/10.1145/2931037.2931038
[53] Z. Pan and P. Mishra, ‚ÄúAutomated test generation for hardware trojan
detection using reinforcement learning,‚Äù in Proceedings of the 26th Asia
and South PaciÔ¨Åc Design Automation Conference, ser. ASPDAC ‚Äô21.
New York, NY , USA: Association for Computing Machinery, 2021, p.
408‚Äì413. [Online]. Available: https://doi.org/10.1145/3394885.3431595
[54] Z. Pan, J. Sheldon, and P. Mishra, ‚ÄúTest generation using reinforcement
learning for delay-based side-channel analysis,‚Äù in 2020 IEEE/ACM
International Conference On Computer Aided Design (ICCAD), 2020,
pp. 1‚Äì7.
[55] C. J. C. H. Watkins and P. Dayan, ‚ÄúQ-learning,‚Äù Machine Learning,
vol. 8, no. 3, pp. 279‚Äì292, May 1992. [Online]. Available:
https://doi.org/10.1007/BF00992698
379[56] R. S. Sutton and A. G. Barto, Reinforcement Learning: An
Introduction, 2nd ed. The MIT Press, 2018. [Online]. Available:
http://incompleteideas.net/book/the-book-2nd.html
[57] V . Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.
Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski,
S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran,
D. Wierstra, S. Legg, and D. Hassabis, ‚ÄúHuman-level control through
deep reinforcement learning,‚Äù Nature, vol. 518, no. 7540, pp. 529‚Äì533,
Feb. 2015. [Online]. Available: http://dx.doi.org/10.1038/nature14236
[58] J. A. Boyan and A. W. Moore, ‚ÄúGeneralization in reinforcement learn-
ing: Safely approximating the value function,‚Äù in Proceedings of the 7th
International Conference on Neural Information Processing Systems, ser.
NIPS‚Äô94. Cambridge, MA, USA: MIT Press, 1994, p. 369‚Äì376.
[59] I. Podolak, A. Roman, M. Szyku≈Ça, and B. Zieli ¬¥nski, ‚ÄúA machine
learning approach to synchronization of automata,‚Äù Expert Systems
with Applications, vol. 97, pp. 357‚Äì371, 2018. [Online]. Available:
https://www.sciencedirect.com/science/article/pii/S0957417417308655
[60] A. Moore and C. Atkeson, ‚ÄúThe parti-game algorithm for variable
resolution reinforcement learning in multidimensional state-spaces,‚Äù
Machine Learning, vol. 21, 05 2000.
[61] H. Li, T. Chen, H. Teng, and Y . Jiang, ‚ÄúA graph-based reinforcement
learning method with converged state exploration and exploitation,‚Äù
Computer Modeling in Engineering & Sciences, vol. 118, pp. 253‚Äì274,
02 2019.
[62] W. Smeenk, ‚ÄúApplying automata learning to complex industrial soft-
ware,‚Äù Master‚Äôs thesis, Computer Science, 2012.
[63] S. C. Paiva, A. Simao, M. Varshosaz, and M. R. Mousavi, ‚ÄúComplete
ioco test cases: A case study,‚Äù in Proceedings of the 7th International
Workshop on Automating Test Case Design, Selection, and Evaluation,
ser. A-TEST 2016. New York, NY , USA: ACM, 2016, pp. 38‚Äì44.
[Online]. Available: http://doi.acm.org/10.1145/2994291.2994297
[64] A. Petrenko and N. Yevtushenko, ‚ÄúTesting from partial deterministic
FSM speciÔ¨Åcations,‚Äù IEEE Transactions on Computers, vol. 54, no. 9,
pp. 1154‚Äì1165, 2005.
[65] Z. Gazdag, S. Iv ¬¥an, and J. Nagy-Gy ¬®orgy, ‚ÄúImproved upper bounds
on synchronizing nondeterministic automata,‚Äù Inf. Process. Lett.,
vol. 109, no. 17, pp. 986‚Äì990, 2009. [Online]. Available: http:
//dx.doi.org/10.1016/j.ipl.2009.05.007
[66] F. Melo and I. Ribeiro, ‚ÄúConvergence of Q-learning with linear function
approximation,‚Äù 2007 European Control Conference, ECC 2007, 03
2015.
[67] W. Smeenk, J. Moerman, F. Vaandrager, and D. N. Jansen, ‚ÄúApplying
automata learning to embedded control software,‚Äù in Formal Methods
and Software Engineering, M. Butler, S. Conchon, and F. Za ¬®ƒ±di, Eds.
Cham: Springer International Publishing, 2015, pp. 67‚Äì83.
[68] B. Steffen, F. Howar, and M. Merten, Introduction to Active Automata
Learning from a Practical Perspective. Berlin, Heidelberg: Springer
Berlin Heidelberg, 2011, pp. 256‚Äì296.
[69] H. Shabana and M. V . V olkov, ‚ÄúUsing Sat solvers for synchronization
issues in partial deterministic automata,‚Äù in Mathematical Optimization
Theory and Operations Research, I. Bykadorov, V . Strusevich, and
T. Tchemisova, Eds. Cham: Springer International Publishing, 2019,
pp. 103‚Äì118.
[70] P. Teetor, R Cookbook, 1st ed. O‚ÄôReilly, 2011.
[71] J. Cohen, Statistical power analysis for the behavioral sciences. Rout-
ledge, 1988.
[72] R. M. Hierons and U. C. T ¬®urker, ‚ÄúDistinguishing sequences for par-
tially speciÔ¨Åed FSMs,‚Äù in NASA Formal Methods - 6th International
Symposium, NFM 2014, Houston, TX, USA, April 29 - May 1, 2014.
Proceedings, 2014, pp. 62‚Äì76.
380