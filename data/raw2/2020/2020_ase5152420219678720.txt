Finding A Needle in a Haystack:
Automated Mining of Silent Vulnerability Fixes
Jiayuan Zhou∗, Michael Pacheco∗, Zhiyuan Wan†, Xin Xia‡/bardbl, David Lo§, Y uan Wang∗and Ahmed E. Hassan¶
∗Centre for Software Excellence, Huawei
†College of Computer Science and Technology, Zhejiang University, Hangzhou, China
‡Faculty of Information Technology, Monash University, Melbourne, Australia
§School of Information Systems, Singapore Management University, Singapore
¶Software Analysis and Intelligence Lab (SAIL), Queen’s University
{jiayuan.zhou1,michael.pacheco1,yuan.wang1}@huawei.com,wanzhiyuan@zju.edu.cn, Xin.Xia@monash.edu,
davidlo@smu.edu.sg, ahmed@cs.queensu.ca
Abstract —Following the coordinated vulnerability disclosure
model, a vulnerability in open source software (OSS) is sug-
gested to be ﬁxed “silently”, without disclosing the ﬁx untilthe vulnerability is disclosed. Yet, it is crucial for OSS usersto be aware of vulnerability ﬁxes as early as possible, as oncea vulnerability ﬁx is pushed to the source code repository, amalicious party could probe for the corresponding vulnerabilityto exploit it. In practice, OSS users often rely on the vulnerabilitydisclosure information from security advisories (e.g., NationalVulnerability Database) to sense vulnerability ﬁxes. However,the time between the availability of a vulnerability ﬁx andits disclosure can vary from days to months, and in somecases, even years. Due to manpower constraints and the lackof expert knowledge, it is infeasible for OSS users to manuallyanalyze all code changes for vulnerability ﬁx detection. Therefore,it is essential to identify vulnerability ﬁxes automatically andpromptly. In a ﬁrst-of-its-kind study, we propose VulFixMiner, aTransformer-based approach, capable of automatically extractingsemantic meaning from commit-level code changes to identifysilent vulnerability ﬁxes. We construct our model using sampledcommits from 204 projects, and evaluate using the full set ofcommits from 52 additional projects. The evaluation results showthat VulFixMiner outperforms various state-of-the-art baselinesin terms of AUC (i.e., 0.81 and 0.73 on Java and Python dataset,respectively) and two effort-aware performance metrics (i.e.,EffortCost, P
opt). Especially, with an effort of inspecting 5% of
total LOC, VulFixMiner can identify 49% of total vulnerabilityﬁxes. Additionally, with manual veriﬁcation of sampled commitsthat were identiﬁed as vulnerability ﬁxes, but not marked assuch in our dataset, we observe that 35% (29 out of 82) of thecommits are for ﬁxing vulnerabilities, indicating VulFixMiner isalso capable of identifying unreported vulnerability ﬁxes.
Index T erms—Software Security, Vulnerability Fix, Open
Source Software, Deep Learning
I. I NTRODUCTION
Coordinated vulnerability disclosure (also known as responsi-
ble disclosure) [1], [2] is a widely used vulnerability disclosuremodel,
1,2,3in which the relevant information of a vulnera-
bility is only disclosed after a period of time that allowsfor the vulnerability to be ﬁxed. Following this process, a
/bardblCorresponding author.
1https://www.microsoft.com/en-us/msrc/cvd
2https://github.com/google/oss-vulnerability-guide
3https://www.apache.org/security/committers.htmlvulnerability in an open source software (OSS) is suggested tobe reported privately to the OSS maintainers, who “silently”ﬁx the vulnerability (i.e., push the commit(s) to the sourcecode repository, without explicit log messages indicating thevulnerability). They then integrate the ﬁx “publicly” into theOSS (e.g., including the ﬁx in a new release), and ﬁnallydisclose the vulnerability and its ﬁx.
It is important for OSS users to be aware of vulnerability
ﬁxes and apply ﬁxes in time. In 2017, Equifax suffered froma data breach, compromising the personal information ofover 143 million U.S. consumers, due to a missed securityupdate [3], [4]. To be aware of vulnerability ﬁxes, OSS usersusually monitor the vulnerability disclosure information frompublic vulnerability advisories.
Common Vulnerabilities and Exposures (CVE) and the
National Vulnerability Database (NVD) are two of the mostpopular public vulnerability advisories. NVD is a robust andwidely used vulnerability advisory, fully synchronized withCVEs, and provides additional information (e.g., the severity)regarding vulnerabilities. Thus, it is a common practice forOSS users to monitor NVD primarily, as a method of becom-ing aware of vulnerabilities and their ﬁxes. However, due tothe slow progress in disclosing reported vulnerabilities [5], itis challenging to promptly discover vulnerability ﬁxes. Also,the time interval between the availability of a vulnerabilityﬁx and its disclosure can vary from days to years
4, and the
median of this time interval was reported to be more than oneweek in NVD [5]. As an example, CVE-2018-11776 [6] is aremote code execution vulnerability in Apache Struts, whichis the same type as the critical vulnerability that led to thedata breach of Equifax. This vulnerability was “silently” ﬁxedin the public code repository in June 2018 [7], though thepatch was disclosed two months later. Given the public natureof OSS development, once a vulnerability ﬁx is pushed tothe source code repository, a malicious party could infer thecorresponding vulnerability and exploit it before the securitypatch is publicly integrated or disclosed. As a result, the users
4https://www.ﬁreeye.com/blog/threat-research/2020/04/time-between-
disclosure-patch-release-and-vulnerability-exploitation.html
7052021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)
DOI 10.1109/ASE51524.2021.000682021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE) | 978-1-6654-0337-5/21/$31.00 ©2021 IEEE | DOI: 10.1109/ASE51524.2021.9678720
978-1-6654-0337-5/21/$31.00  ©2021  IEEE
of Apache Struts were exposed to huge security risks during
the two months between patch availability and disclosure.
The disclosure latency will extend the vulnerability’s win-
dow of exposure and it makes OSS users, especially theenterprise users who use OSS in their products, at a greatdisadvantage in defending security attacks. Hence, it is crucialto facilitate the awareness of vulnerability ﬁxes for OSS users,so that they can react to the vulnerabilities as early as possible.For example, when enterprise users receive the notiﬁcations ofvulnerability ﬁxes, they can infer the corresponding vulnerabil-ity and evaluate the security impact on their product and takeactions (e.g., recompile the software to incorporate the criticalﬁxes that might cause huge loss) before the vulnerability ispublicly disclosed. Without the early awareness, OSS usersneed to rush to address the vulnerability at the date of thedisclosure. Hence, early awareness helps OSS users reducethe stress of keeping their system safe. In addition, beingaware of the ﬁx earlier could help shorten the vulnerabilityremediation time for OSS users. For example, with the help ofhot-patching frameworks [8]–[11], hot-patches can be derivedfrom the original vulnerability ﬁxes to ease the deployment ofsecurity updates.
Following coordinated vulnerability disclosure in OSS con-
texts [1], [2], a vulnerability is suggested to be ﬁxed silently,which means that any information indicating the nature ofthe vulnerability should not be exposed. For example, thevulnerability handling process of Apache
5suggests messages
associated with commits should not make any reference toany security-related nature. Hence, for general OSS users, theonly way to be aware of silent vulnerability ﬁxes promptly isto monitor and analyze the commit code changes constantly,which is infeasible due to manpower constraints. Therefore, itis essential to propose approaches that automatically identifysilent vulnerability ﬁxes in real-time.
Previous studies [12]–[15] leverage vulnerability-related
artefacts (e.g., commit messages and issue reports) to identifyvulnerability ﬁxes. They extract features from textual data andleverage machine learning techniques to predict whether acommit is for ﬁxing a vulnerability or not. Different from pre-vious work, we incorporate a deep learning solution designedfor analyzing the code of commits.
In our work, we propose VulFixMiner, a Transformer-
based [16] model, to automatically identify silent vulnerabilityﬁxes in a practical setting (i.e., extremely imbalanced classdistribution of the ﬁxes). Since we are interested in silentvulnerability ﬁxes, of which the commit message should notleak information related to the vulnerability, we only con-sider code change information. Given the outstanding abilityin learning effective contextual representation, we leveragethe Transformer-based [16] language model CodeBERT [17],which is pre-trained on a large programming language corpus,to learn the semantic meaning of code changes. We ﬁrst ﬁne-tune CodeBERT to learn the semantic meaning of ﬁle-levelcode changes, to generate contextual embedding vectors for
5https://www.apache.org/security/committers.htmlchanged ﬁles. The vectors are then aggregated to generatea commit-level contextual embedding vector, which is usedfor classiﬁcation. VulFixMiner is capable of cross-project andcross-language just-in-time vulnerability ﬁx identiﬁcation.
We construct our model using 63,331 commits from 204
projects and evaluate it using 143,989 commits from 52projects that are unseen during the training phase. The evalua-tion results show that VulFixMiner outperforms various state-of-the-art baselines in terms of AUC (i.e., 0.81 and 0.73 onJava and Python dataset, respectively), and two effort-awareevaluation metrics (i.e., EffortCost, P
opt). Particularly, with
an effort of inspecting 5% of the total LOC, VulFixMinercan identify 49% of total vulnerability ﬁxes. Additionally,by manually investigating a sample of commits identiﬁed asvulnerability ﬁxes that are not marked as vulnerability ﬁxes inour dataset, we observe 35% (29 out of 82) of them are forﬁxing vulnerabilities, indicating VulFixMiner is also capableof identifying unreported vulnerability ﬁxes.
In summary, this paper makes the following contributions:
•We propose the use of VulFixMiner to automaticallyidentify silent vulnerability ﬁxes, supporting both cross-project and cross-language scenarios.
•To the best of our knowledge, we are the ﬁrst to use deeplearning techniques to identify silent vulnerability ﬁxes ina practical setting.
•VulFixMiner achieves higher discriminative power andefﬁciency compared to state-of-the-art baselines in identi-fying silent vulnerability ﬁxes, when evaluated on a com-plete set of commits from 52 Java and Python projects.
•We ﬁnd that VulFixMiner is capable of identifying unre-ported vulnerability ﬁxes.
•To promote future work, we release the vulnerability ﬁxesin our study [18].
6
Paper organization. This paper is organized as follows:
Section II introduces background information. We elaborateon our approach on Section III. Section IV presents theexperimental procedures and results. In Section V, we discussthe unreported vulnerability ﬁxes, the ethical consideration,the other application scenarios, the commit messages of silentvulnerability ﬁxes, and time efﬁciency. Sections VI and VIIcover the possible threats to validity, and highlight relatedwork, respectively. We conclude this paper and discuss futurework opportunities in Section VIII.
II. P
RELIMINARIES
In this section, we brieﬂy introduce the vulnerability disclosuremodels, Common Vulnerabilities and Exposures, National Vul-nerability Database, and pre-trained NLP models for naturallanguage tasks.
A. Vulnerability disclosure models
Full Disclosure. In the Full Disclosure model [19], the vul-
nerability is fully disclosed onto public disclosure channels as
early as possible. The idea behind this model is that the early
6Note that we are undergoing the company’s internal process for model
publishing.
706*YLH[LKI`:[\HY[4J4VYYPZ
MYVT[OL5V\U7YVQLJ[
*YLH[LKI`1HZVU+9V^SL`MYVT[OL5V\U7YVQLJ[ *YLH[LKI`)LJYPZ
MYVT[OL5V\U7YVQLJ[*YLH[LKI`(UKYLQZ2PYTH
MYVT[OL5V\U7YVQLJ[*YLH[LKI`(UKYLQZ2PYTHMYVT[OL5V\U7YVQLJ[Commit*YLH[LKI`(UKYLQZ2PYTH
MYVT[OL5V\U7YVQLJ[*YLH[LKI`(UKYLQZ2PYTHMYVT[OL5V\U7YVQLJ[Commit *YLH[LKI`1HZVU+9V^SL`MYVT[OL5V\U7YVQLJ[Pre-trained  
language model
File-level contextual  
embedding vectorFile-level change 
code input tokensFile change  
transformerNeural network 
classiﬁer
*YLH[LKI`(UKYLQZ2PYTH
MYVT[OL5V\U7YVQLJ[*YLH[LKI`(UKYLQZ2PYTH
MYVT[OL5V\U7YVQLJ[*YLH[LKI`(UKYLQZ2PYTH
MYVT[OL5V\U7YVQLJ[*YLH[LKI`(UKYLQZ2PYTH
MYVT[OL5V\U7YVQLJ[New commits
OSS users*YLH[LKI`WYP`HURH
MYVT[OL5V\U7YVQLJ[VulFixMinerPhase 1:  
Fine-tuning
Phase 2:  
Training 
Phase 3:  
Application Commit change 
Aggregator
... ... ...0.04 C5 50... ... ...Rank Commit Score
1
2 0.81 C1C3 0.96*YLH[LKI`)LJYPZ
MYVT[OL5V\U7YVQLJ[
*YLH[LKI`)LJYPZ
MYVT[OL5V\U7YVQLJ[Code change 
preprocessor
Code change 
preprocessor
Commit-level contextual  
embedding vectorLegend
Fig. 1: The overall framework of VulFixMiner.
public awareness of the vulnerability will beneﬁt users who
are vulnerable, more than malicious actors.
Coordinated Vulnerability Disclosure. In the Coordinated
Vulnerability Disclosure model [1], [2], a vulnerability remains
undisclosed, or silent, as long as possible to provide developersof the software enough time to ﬁx it. After a new releaseincluding the ﬁx is published, the vulnerability is disclosed.This model is commonly and widely applied in many OSSorganizations
7,8,9. In this disclosure model, users remain at
risk as long as they are unaware that they are vulnerable.Also, a ﬁx for the vulnerability might be committed to an OSSrepository before the vulnerability is disclosed, which we referto as a “silent ﬁx”. Due to the lack of public exposure of thevulnerability, it is expected that exploitation from malicioususers will be less likely. However, given the public nature ofOSS development, if a vulnerability is silently ﬁxed beforethe ﬁx is integrated, a malicious party can discover these ﬁxesfrom the public code repository, and exploit the vulnerabilityin the vulnerable systems.
Non-disclosure. In the non-disclosure model [19], no details
of a vulnerability, nor its ﬁx, are disclosed. The software
developers, users, and malicious attackers do not have accessto any public information regarding these vulnerabilities. Therisk involved with this model appears as users are never noti-ﬁed of vulnerabilities within the software. Although the non-disclosure model prevents attackers from analyzing securityﬁxes and stops them from further ﬁnding similar vulnerabil-ities,
10OSS users might not be aware of the urgency of a
security ﬁx and do not apply the ﬁx. In this case, OSS usersare always exposed to security risks. In our paper, we referto these ﬁxes as “unreported vulnerability ﬁxes”. We discussthe capability of VulFixMiner in identifying such ﬁxes inSection V-A.
7https://www.microsoft.com/en-us/msrc/cvd
8https://googleprojectzero.blogspot.com/
9https://www.apache.org/security/committers.html
10https://redmondmag.com/articles/2011/02/16/microsoft-silent-ﬁx-due-
diligence.aspxB. Common Vulnerabilities and Exposure (CVE), and theNational Vulnerability Database (NVD)
CVE, developed by The Mitre Corporation (MITRE), provides
a standardized method for software developers to disclose,identify, and manage software vulnerabilities. Once a vulner-ability is identiﬁed, the OSS developers can request a uniqueCVE ID from a CVE Numbering Authority (CNA), such asMITRE, for the vulnerability. Once this ID is assigned, theCVE will be included in a vulnerability database for publicdisclosure. CVE is one of the most popular vulnerabilityadvisories; OSS users monitor them to discover vulnerabilitiesthat might impact them, and the ﬁxes to defend againstexploitation.
NVD
11is a popular CVE database, maintained by the Na-
tional Institute for Science and Technology (NIST). NVD
is automatically synchronized with the MITRE database andincludes all CVE information included in MITRE, as well asadditional information. More speciﬁcally, NVD assigns labelsto each URL in the list of references of a CVE, including thevulnerability ﬁx. This labeling can only be performed if thevulnerability ﬁx is already disclosed, MITRE or NVD becomeaware of it, and MITRE or NVD have included it in theirdatabases.
C. Pre-Trained NLP models for natural language tasks.
Transformer. The Transformer [16] is a model originally
designed for tasks related to natural language processing
(NLP). The canonical architecture of the model consists ofan encoder-decoder and includes several layers with attentionmechanisms. This architecture allows for the use of pre-trainedmodels, which have already been trained on related domain-speciﬁc data by others, and can be reused for multiple relatedtasks through ﬁne-tuning.
CodeBERT. CodeBERT [17], a variant of RoBERTa [20],
is a multi-layer bidirectional Transformer, and one of the
ﬁrst models to incorporate both natural language (NL) andprogramming language (PL) information. As there is a naturaldifference between NL and PL, CodeBERT is further pre-trained on RoBERTa with 2.1 million function-level source
11https://nvd.nist.gov/
707code data, together with the corresponding function documen-
tation. This allows CodeBERT to handle NL to PL-relatedtasks, including NL code search, NL-PL probing, and codedocumentation generation.
III. P
ROPOSED APPROACH
In this section, we ﬁrst introduce the overall framework ofVulFixMiner, then we describe the details of each module inVulFixMiner.
A. Overall Framework
The goal of VulFixMiner is to effectively identify vulnerability
ﬁxes using code change information. Figure 1 shows the over-all framework of VulFixMiner, which consists of three phases(i.e., Fine-tuning, Training, and Application). We ﬁrst ﬁne-tune a pre-trained language model to learn the representationof ﬁle-level code change in the Fine-tuning Phase. In theTraining Phase, we consider the ﬁne-tuned model as the ﬁle
change transformer, and we use the commit change aggregator
to aggregate the ﬁle-level code change representations intocommit-level code change representations, and then train aneural network classiﬁer to identify commits. In the Applica-tion Phase, the trained VulFixMiner consumes new commitsfrom OSS repositories and computes scores, which indicatethe likelihood that a commit is for ﬁxing vulnerabilities. Weelaborate on the details of each phase as follows:
Three
ﬁles
Codechangesof the1st ﬁle Added code
Removed code
...
Fig. 2: A sample commit which is for ﬁxing CVE-2018-11776vulnerability [6].
B. Phase 1: Fine-tuning
We choose CodeBERT [17] as our pre-trained language model.
We ﬁrst consider code changes as a sequence of tokensin a BoW. The code change preprocessor (See Figure 1)extracts code changes from commits and constructs a ﬁle-levelsequence of input tokens, which are then fed into CodeBERTfor ﬁne-tuning. We introduce the code change preprocessingand CodeBERT ﬁne-tuning as below:[CLS] [SEP] [EOS] T1
CodeBERTTn ... T1 Tm ...
CET1 CETnCE[SEP] CE[EOS] CETm CET1 ... ...Rem-code segment Add-code segment
Input
tokens
Contextual
embedding
vectorsFine-tuned
CodeBERT
CEf
Fully connected layer1 2 n+1 n+2 n+3 255 256
256 255 n+3 n+2 n+1 2 1
Output layer Fine-tuning CodeBERT
Fig. 3: The architecture of ﬁne-tuning CodeBERT. CE fis the
embedding vector of the classiﬁcation token [CLS] and the
vector works as the contextual embedding of the input tokens.
1) Code change preprocessing: As shown in Figure 2, a
commit may contain code changes across multiple ﬁles. Wepreprocess code changes in the following three steps:
1. Extract ﬁle-level code changes. We ﬁrst extract the
information of ﬁle-level changes into separate code change
documents. For each document, we further extract removedand added code lines and split them into the rem-code and
add-code segments, respectively.
2. Process the removed code and added code lines. For each
segment, we ﬁrst tokenize code lines into a sequence of tokens,and then further split camel case style (e.g., “addData”)and snake case style (e.g., “add
data”) tokens, by using
codeprep [21].
3. Input tokens construction. To construct the same input
representation in CodeBERT [17], we use the same tokenizer12
that CodeBERT used to concatenate two segments. Threeseparator tokens (i.e., [CLS], [SEP], and [EOS]), are used for
concatenation. [CLS] is a special classiﬁcation token found
always as the ﬁrst token of input. The ﬁnal hidden stateof CodeBERT corresponding to [CLS] is considered as the
contextual embedding (i.e., aggregated sequence embedding)of the input tokens, and can be used for classiﬁcation. [SEP]
and [EOS] are used to separate two segments, and to indicate
the end of input, respectively. All inputs are either paddedor truncated to the same length (i.e., 256) by the CodeBERTtokenizer. Figure 3 shows an example of the constructed inputtokens.
2) CodeBERT ﬁne tuning: The downstream ﬁne-tuning task
is to predict the probability that the code changes in a ﬁle arefor ﬁxing vulnerabilities. Figure 3 shows the architecture ofﬁne-tuning CodeBERT. Note that we use the same CodeBERTtokenizer to generate the input embedding vector. Since it isnot the main focus of our approach, we omit the details of theinput embedding generation (which is explained in the originalwork of CodeBERT [17]). During ﬁne-tuning, the output ofCodeBERT includes the contextual embedding vectors (i.e.,CE
T) of each token and an embedding vector (i.e., CE f)
of[CLS], which works as the contextual embedding of the
input tokens. We use the contextual embedding vector of the
12https://huggingface.co/microsoft/codebert-base/tree/main
708input tokens (i.e., the removed and the added code tokens) to
represent the semantic relevance between removed and addedcode lines. Then we connect CE
fwith a fully connected layer
followed by an output layer, which computes a score indicatingthe probability that the code changes in a ﬁle are for ﬁxingvulnerabilities. To improve the robustness of our model, weapply the dropout technique [22] on the fully connected layer.
During ﬁne-tuning, the parameters of CodeBERT are
learned to minimize the cross-entropy loss. After ﬁne-tuning,all parameters of CodeBERT are frozen, and the ﬁne-tunedCodeBERT is used as a ﬁle change transformer to consumeﬁle-level input tokens, and output contextual embedding vec-tors of the ﬁle-level code changes.
C. Phase 2: Training
Following the same code change preprocessing in Sec-
tion III-B1, the constructed ﬁle-level input tokens are encodedinto ﬁle-level contextual embedding vectors (i.e., CE
f) by the
ﬁle change transformer, which is the ﬁne-tuned CodeBERT.We then use a commit change aggregator to aggregate theﬁle-level vectors belonging to the same commit, to generate auniﬁed ﬁle-level code change representation (i.e., CE
commit ),
representing commit-level code changes.
In the commit change aggregator, we compute the element-
wise mean for the vectors of ﬁles belonging to the same
commit as CE commit =/summationtextn
i=1CE fi
n, where CE fiis the
contextual embedding vector of the ithﬁle in a given commit,
andnis the number of ﬁles belonging to that commit.
Finally, we use a one-layer neural network classiﬁer to
classify commits. The aggregated CE commit is fed into a fully
connected layer, followed by an output layer which computes
a score indicating the probability that the code changes ina commit are for ﬁxing vulnerabilities. We also apply thedropout technique [22] on the fully connected layer here.
D. Phase 3: Application
After the Training Phase, OSS users can use the trained
VulFixMiner to mine vulnerability ﬁxes from new commits.For example, VulFixMiner can be integrated into an automaticmonitoring service of OSS code repositories. Given a set ofnew commits, VulFixMiner computes scores for each of themand outputs a commit ranking to OSS users. The commitsare ranked by scores and the higher score of a commitindicating the higher probability of the commit being for ﬁxingvulnerabilities (see Figure 1).
IV . E
XPERIMENTS
In this section, we aim to answer the following RQs:
•RQ1: How effective is VulFixMiner compared to thestate-of-the-art baselines?
•RQ2: Does VulFixMiner beneﬁt from ﬁne tuning usingcross-domain data?
Next, we describe our dataset and each step of data pro-
cessing. We then present the details of ﬁve baselines, fourevaluation metrics, and experiment setup. Finally, we showour research questions and results.ExtractExtract
Mitre
ExtractGitHubExtractSAP KBLabel
Vulnerability
Fix
InformationExtractVulnerability
Fixes Link
ExtractIssues and
Pull Requests
LinkIssue and Pull
Requests
InformationLabel
Repo
InformationAll commits
Extract
Fig. 4: An overview of our data collection approach. Wecollect vulnerability data from SAP KB project [23] and Mitre.
We collect commit data from GitHub.
A. Data Collection
Our dataset consists of commit information from a set of Java
and Python OSS. We rely on two vulnerability-related datasources to ﬁrst collect a set of commits that are for ﬁxingvulnerabilities and then collect all commits of OSS that containany of the vulnerability ﬁxes we collected. Figure 4 illustratesthe overview of data collection approach.
1. Collecting vulnerability ﬁx links . We ﬁrst collect data
from an existing Java vulnerability ﬁx dataset [23], which is
manually curated by SAP KB project.
13From this data source,
we obtain 1,055 vulnerability ﬁx links, spanning across 183Java OSS projects and corresponding to 615 CVEs.
Next, we collect all CVEs (disclosed by January 26, 2021)
from Mitre CVE database
14, resulting in 201,234 CVEs. We
proceed by extracting CVEs containing a patch ﬁxing thatvulnerability, with a link toward a GitHub
15commit, issue,
or pull request. We then collect the commit information andﬁlter for commits that contain Java or Python ﬁle extensions.For Java, we obtain a total of 199 commits, 227 issues,and 155 pull requests, spanning across 189 projects andcorresponding to 340 CVEs. For Python, we obtain a totalof 288 commits, 244 issues, and 353 pull requests, spanningacross 256 projects and corresponding to 444 CVEs. We thenmerge all commits collected up until this point and remove anyduplicates. We note that in these steps we include vulnerabilityﬁxes regardless of when they were disclosed in relation to thevulnerability itself.
2. Collecting issue and pull request information. We pro-
ceed by collecting the commit links related to the total number
of issues (471) and pull requests (508) from Java and Pythonprojects using GitHub. This results in 383 Java vulnerabilityﬁxes, spanning across 101 projects and corresponding to 186CVEs. As for Python, this results in 597 vulnerability ﬁxingcommits spanning across 141 projects and corresponding to233 CVEs. We add this result to the list from the previousstep and remove any duplicates.
3. Collecting OSS repository and commit information.
Finally, we collect the commit information from each vul-nerability ﬁx link and collect all commits made within their
13https://sap.github.io/project-kb/
14https://cve.mitre.org/
15https://github.com/
709projects, up until February 26, 2021. If their IDs are found
within the list of vulnerability ﬁxes from the previous step,we label these commits by assigning a ”1” to them, deﬁningthe positive label.
As a result, the Java dataset includes 1,436 vulnerability
ﬁxes and 839,682 non-vulnerability ﬁxing commits, spanningacross 310 projects and corresponding to 839 CVEs. ThePython dataset includes 885 vulnerability ﬁxes and 722,291non-vulnerability ﬁxing commits, spanning 256 projects andcorresponding to 444 CVEs.
B. Data Preprocessing
We further preprocess our dataset to ﬁlter out noisy data and
enhance existing data.
1. Remove noisy data. First, we remove duplicate code
change information, keeping the ﬁrst instance. Next, we re-
move OSS that only have one vulnerability ﬁx. One challengefor identifying vulnerability ﬁxes in a practical setting is theextremely imbalanced dataset. We expect that it is likely forthese speciﬁc OSS to offer less opportunity for our modelsto learn and differentiate between vulnerability and non-vulnerability ﬁxing commits. We then remove large commitsthat are less likely to ﬁx vulnerabilities, by calculating twothresholds using the 95
thpercentile of the total modiﬁed lines
of code and the number of changed ﬁles of vulnerability ﬁxes.For Java, these values are 309 and 15 respectively. The removalresults in 474,555 non-vulnerability ﬁxing commits, and 1,353vulnerability ﬁxes, across 150 projects. For Python, these val-ues are 80 and 6 respectively. The removal results in 357,696non-vulnerability ﬁxing commits, and 751 vulnerability ﬁxes,across 106 projects.
2. Enhance the dataset by identifying more secret vulner-
ability ﬁxes. Next, we enhance our dataset by labeling more
commits that are relevant to vulnerability ﬁxes. To do this, wedeﬁne a regular expression adapted from the work of Zhou andSharma [12]. The original regular expression contains a diverseset of security-related keywords, and as a result, commonlymislabels commits. For example, the message of a commit
16is
“Migrate xml-insecure groovy ->java”, containing a security-
related keyword, “insecure”, though the purpose of the com-mit is for code refactoring. To avoid such false alarms, weselect three conservative keywords (i.e., “vuln”, “CVE”, and“NVD”) from [12] to generate a new regular expression. Weuse this regular expression to relabel commits not identiﬁedby analyzing the ﬁx links (described in Section IV-A) asvulnerability ﬁxes, by matching the words found within theircommit messages. In the Java dataset, we relabel 420 non-vulnerability commits across 123 OSS. In the Python dataset,we relabel 501 non-vulnerability commits across 98 OSS.
3. Split dataset into training, testing, and validation sets. In
this step, we ﬁrst split the data project-wise, using an 80%/20%
split, and consider the 20% split as the testing dataset. We thenfurther split the 80% training set project-wise, into a 90%/10%
16https://github.com/spring-projects/spring-security/commit/
2e2b22f87ecbd40e3328a089efb3189a3b9cdd99split, and consider the 10% split as the validation dataset, andthe remaining split as the training dataset. In this case, thecommits in test data are from different projects which are neverseen in the training and validation sets.
We randomly undersample non-vulnerability ﬁxing commits
for each OSS, and combine the result with the unmodiﬁedvulnerability ﬁxes, to form our ﬁnal training and validationdatasets. We perform this approach as another mechanism toreduce the imbalanced nature of the dataset; however, we notethat these resulting datasets continue to remain imbalanced.Also, we do not apply this undersampling to the test dataset,as our goal is to evaluate our model in a practical setting.
Table I describes our ﬁnal dataset. We notice an extremely
imbalanced class distribution of vulnerability ﬁxes. For thepercentage of vulnerability ﬁx within each OSS, the mediannumber is 0.35%. The median number of vulnerability ﬁxeswithin each OSS is only 4.
C. Baselines
We compare VulFixMiner with several baselines:
•RandomGuess: Random Guess is a strawman baseline
that randomly predicts whether a code change is for ﬁxing
vulnerabilities.
•SVM: The closest work to ours is by Sabetta and Bezzi
[13]. Their work treated code and commit messages as acollection of tokens in a bag of words (BoW), and trainedtwo linear support vector machine (SVM) models basingon commit messages and code of commit, respectively.We replicate the code-based model.
•RandomF orest: It is widely applied throughout software
engineering studies (e.g., defect prediction [24] and se-curity bug report prediction [25]) and showed robust andhigh performances [26]–[28].
•LSTM: Long short-term memory (LSTM) network [29] is
a widely applied RNN-based model throughout softwareengineering studies [30]. Instead of treating code as abag of words, we use LSTM to represent the sequentialinformation of code by considering code as a sequenceof tokens.
•Transformer : Transformer [16] is an encoder-decoder,
neural network-based language model with multi-headattention layers, giving way to the use of pre-trainedmodels (e.g., CodeBERT), and is used for a variety ofnatural language processing tasks.
D. Evaluation Metrics
Given the fact that the vulnerability ﬁxes are rare among
the code commits (i.e., the extremely imbalanced datasetscenario), the goal of VulFixMiner is to help users reduce theefforts in identifying vulnerability ﬁxes from a large scopeof code commits. We leverage two effort-aware performancemetrics (i.e., CostEffort@L andP
opt[31]–[34]) to evaluate the
effectiveness of VulFixMiner in practical setting. Similar toprior studies [35]–[38], instead of using threshold-dependentmeasures (e.g., precision and recall) which depend on arbi-trarily thresholds and are sensitive to imbalanced data, we use
710TABLE I: Description of our dataset after preprocessing. We refer to vulnerability ﬁxes and non-vulnerability ﬁxing commits
as V .F. and N.V .F, respectively.
Training Set Validation Set Testing Set
#V .F. #N.V .F #Projects #V .F. #N.V .F #Projects #V .F. #N.V .F #Projects
Java 983 31,323 120 191 6,921 119 300 87,856 30
Python 522 20,362 84 80 2,949 83 195 55,638 22
02 5 50 75 100100
75
5025Models
Worse model
% LOCs Inspected% Vul-ﬁxes DetectedOptimal model
Prediction model
Fig. 5: An example of the relationship between the percentageof vulnerability ﬁxes detected and the amount of inspectioncost (i.e., %LOC) for different prediction models.
AUC to quantify the discriminative capability of VulFixMiner.
We introduce each measure as follows:
CostEffort@L: We wish to identify more vulnerability ﬁxes
under the limited inspection effort. Similar to the effort eval-
uation in the defect prediction task [31]–[33], [39], we alsoconsider the LOC as the proxy of the inspection effort. CostEf-
fort@L is the proportion of inspected vulnerability ﬁxes among
all the actual vulnerability ﬁxes when L LOC of all commitsare inspected. CostEffort@L is computed as
n
N, where n
accounts for the correctly identiﬁed vulnerability ﬁxes, N
accounts for the total vulnerability ﬁxes, and Laccounts
forL%of total LOC of commits. The high CostEffort@L
indicates more vulnerability ﬁxes could be identiﬁed costingthe effort of inspecting L LOC. We calculate CostEffort@5%
and CostEffort@20% in our study.
P
opt:is the normalized version of the effort-aware per-
formance metrics which is ﬁrst introduced by Mende andKoschke [34], basing on the concept of the “code-churn-based” Alberg diagram [40]. P
optis a widely used effort-aware
performance metric in defect prediction [31]–[33], [39]. In ourstudy, we calculate P
optas same as they do.
Figure 5 is an example of the Alberg diagram in the vul-
nerability ﬁx detection context, where the diagram shows therelationship between the percentage of identiﬁed vulnerabilityﬁxes (i.e., y-axis) achieved by a model and the percentageof LOCs that are inspected (i.e., x-axis). The optimal andworst model represents the cases where all commits arerespectively sorted in decreasing and ascending order by vul-
ﬁx-density. The vul-ﬁx-density of a commit cis deﬁned as
D(c)=
Y(c)
Effort (c), where Y(c) is 1 if the commit cis vul-ﬁx
and 0 otherwise, and Effort(c) is the LOC the commit c.B y
doing this, in Figure 5 the points on the optimal model andworst model represent the maximum and minimum percentageof vul-ﬁx detected, respectively, with %LOCs inspected. Thenwe can use optimal model and worst model as the upper boundand lower bond, respectively, to further assess the predictionmodel.For a given prediction model m, its P
opt is computed
asPopt(m)=Area(O,P )
Area(O,W ), where O, P , and R repre-
sent the optimal model curve, the prediction model curve,and the worst model curve, respectively, and the functionArea( curve 1,c u r v e 2)represents the corresponding area be-
tween the two curves. A larger P
optvalue indicates a smaller
difference of performance between the prediction model andthe optimal case. In our study, we calculate P
opt when 5%
and 20% of the LOCs are inspected and we denote them asP
opt@5 andPopt@20, respectively.
AUC: is the area under the receiver operating characteristic
(ROC) curve [41], which measures the prediction performanceof the model for all possible classiﬁcation thresholds (i.e., from0 to 1). It is robust in quantifying the discriminative capabilityof a classiﬁer, especially in imbalanced class distributions,due to its insensitivity toward them [26]. The AUC has been
recommended as the primary metric to determine and comparethe performance of classiﬁers [42], and should be used overother metrics which are threshold-dependent (e.g., F1-score)[43]. A classiﬁer with an AUC≥0.7 (0≤AUC≤1) is
considered to have achieved an acceptable performance [44].In our experiment, the AUC measures the probability that our
classiﬁer will rank a randomly selected vulnerability ﬁxingcommit higher than a randomly selected non-vulnerabilityﬁxing commit.
E. Experiment Setup
For VulFixMiner, we use the CodeBERT tokenizer for input
embedding vector generation. The size of the vocabulary is50,265 and the size of the input embedding vector is 256. Thesizes of the fully connected layers described in Sections III-B2and III-C are both set to 768, which is the same size as thehidden state of CodeBERT. Before computing the score, adropout [22] rate of 0.1 is used for both fully connected layersin the ﬁne-tuning and classiﬁcation steps.
VulFixMiner is ﬁne-tuned using Adam [45] with shufﬂed
mini-batches. During ﬁne-tuning, we set the learning rate ofAdam to 1e-5 and the batch size to 8. We ﬁne-tune CodeBERTfor 15 epochs with an early stopping strategy [46], [47] toavoid overﬁtting problems during the ﬁne-tuning process. Westopped the ﬁne-tuning if the value of the cross-entropy losshas not been updated on the validation dataset in the last 5epochs. We train the neural network classiﬁer with a batchsize of 32 for 60 epochs with an early stopping strategy.
The input of each baseline model is the ﬁle-level code
change tokens. Except for adding separator tokens and ap-plying truncating and padding, we process the code changesin the same way as what we do in Section III-B1. We choose
7114,000 most frequent tokens in the training set to build the
token vocabulary.
For the SVM baseline, we use the same hyperparameter set-
ting in [13]. Similar to [48], for the RandomForest baseline, wetune the mtry (i.e., number of randomly sampled variables).
We ﬁnd that using the square root of the feature number (i.e.,the size of vocabulary) can achieve the best performance onthe validation dataset. Hence, we set mtry to 200. LSTM
baseline has a single-layer LSTM network with an unrollinglength of 32 and a hidden unit size of 256. For transformerbaseline, except for the size of hidden states, we use the samehyperparameter setting in [16]. We set the size of hidden statesto 768, which is the same size used in CodeBERT.
Our experiments are conducted on EulerOS v1.13.10-r1 64
bits, with a V100-32GB GPU.
17.
F . Research Questions and Results
RQ1: VulFixMiner vs. Baselines
To answer this RQ, we evaluate VulFixMiner and baseline
models on Java and Python test dataset respectively, in termsofAUC ,CostEffort@5%, CostEffort@20%, P
opt@5%, and
Popt@20%. Tables II and III show the evaluation results on
Java and Python projects, respectively. We observe that Vul-FixMiner outperforms all baselines on both Java and Pythonprojects in terms of all evaluation performance metrics.
For Java projects, VulFixMiner achieves AUC ,CostEf-
fort@5%, CostEffort@20%, P
opt@5%, and Popt@20% of
0.81, 0.61, 0.71, 0.53, and 0.63, respectively. Using Random-Forest, the best performing model for comparison, these resultsconstitute improvements of 1%, 41%, 14%, 39%, and 24%,respectively. VulFixMiner thus achieves about the same AUCas RandomForest, but outperforms by substantial marginsin terms of CostEffort@5% and P
opt@5%, which indicates
VulFixMiner can reduce the inspection effort in a practicaluse. When comparing to RandomGuess, SVM, LSTM, andTransformer, VulFixMinerava outperforms them in terms ofall evaluation metrics by large margins.
For Python projects, VulFixMiner achieves AUC ,Cost-
Effort@5%, CostEffort@20%, P
opt@5%, and Popt@20% of
0.73, 0.32, 0.56, 0.24, and 0.39, respectively. Using all base-lines for comparison, these results constitute improvements of11-23%, 18-26%, 18-41%, 16-21%, and 18-30% respectively.
These results indicate that VulFixMiner has high discrimi-
native power in identifying vulnerability ﬁxes, and is capableof identifying vulnerability ﬁxes with less inspection effort inpractical use.
RQ2: Impact of Cross-domain DataTo answer this RQ, we construct three variants of VulFixMiner
with different ﬁne-tuning strategies: 1) without ﬁne-tuning,2) only ﬁne-tuning with Java projects, 3) only ﬁne-tuningwith Python projects, and we refer these three variants asto VulFixMiner
NoFT , VulFixMiner J, and VulFixMiner P, re-
spectively. We evaluate these three variants using the sameevaluation metrics on Java and Python projects. Tables IV
17https://www.nvidia.com/en-us/data-center/v100/TABLE II: Performance of VulFixMiner and baseline modelsfor the Java projects.
Model AUC CostEffort Popt
@5% @20% @5% @20%
RandomGuess 0.50 0.07 0.19 0.05 0.20
SVM 0.59 0.20 0.57 0.14 0.39
RandomForest 0.80 0.44 0.6 0.41 0.50
LTSM 0.52 0.05 0.22 0.03 0.11
Transformer 0.64 0.33 0.50 0.06 0.24
VulFixMiner 0.81 0.61 0.71 0.53 0.63
TABLE III: Performance of VulFixMiner and baseline modelsfor the Python projects.
Model AUC CostEffort Popt
@5% @20% @5% @20%
RandomGuess 0.53 0.06 0.23 0.05 0.20
SVM 0.55 0.14 0.15 0.08 0.14
RandomForest 0.62 0.11 0.31 0.08 0.18
LTSM 0.50 0.06 0.18 0.03 0.09
Transformer 0.56 0.08 0.38 0.04 0.21
VulFixMiner 0.73 0.32 0.56 0.24 0.39
and V show the evaluation results on Java and Pythonprojects, respectively. We observe that VulFixMiner
NoFT
performs the worst on both tasks. Comparing VulFixMinerto VulFixMiner
NoFT , we ﬁnd that the ﬁne-tuning process
improves VulFixMiner in terms of AUC ,CostEffort@5%,
CostEffort@20%, Popt@5%, and Popt@20% by 14-17%, 27-
50%, 42-44%, 22-46%, and 31-42%, respectively. We alsoobserve that VulFixMiner
Jand VulFixMiner Poutperform all
baselines on Java and Python projects, respectively. However,the performance of VulFixMiner, which is ﬁne tuned with bothJava and Python data, outperforms both VulFixMiner
Jand
VulFixMiner Pon both Java and Python projects
When ﬁne-tuning is done using data from only one domain
(e.g., Java projects), the ﬁne-tuned model achieves betterperformance on the task within the same domain. For example,VulFixMiner
Jis ﬁne-tuned using Java projects, and performs
better than VulFixMiner Pon Java projects. One possible
explanation is different programming languages vary in theirsyntactical features. Hence, the knowledge learned solely fromone language may not be aligned well on the task involvingcode in another language.
However, the knowledge learned from one domain may
contribute to the task belonging to the other domain. Figure 6shows V enn diagrams indicating the number of vulnerabilityﬁxes identiﬁed by VulFixMiner
Jand VulFixMiner P, with an
inspection effort of 5% of the total LOC. In Java projects,we observe that VulFixMiner
Pidentiﬁes 11% (32 out of
300) Java vulnerability ﬁxes, which cannot be identiﬁed byVulFixMiner
J. On Python projects, VulFixMiner Jidentiﬁes
14% (27 out of 195) Python vulnerability ﬁxes, which cannotbe identiﬁed by VulFixMiner
P. This could be a possible
explanation of why VulFixMiner beneﬁts from ﬁne-tuningwith cross-domain data.
The overall performance of VulFixMiner on both Java and
712TABLE IV: Performance of VulFixMiner and 3 variants on
Java projects.
AUC CostEffort Popt
@5% @20% @5% @20%
VulFixMiner NoFT 0.64 0.11 0.37 0.07 0.21
VulFixMiner J 0.8 0.52 0.66 0.46 0.57
VulFixMiner P 0.61 0.24 0.42 0.17 0.3
VulFixMiner 0 0.81 0.61 0.71 0.53 0.63
TABLE V: Performance of VulFixMiner and 3 variants onPython projects.
AUC CostEffort Popt
@5% @20% @5% @20%
VulFixMiner NoFT 0.59 0.05 0.14 0.02 0.08
VulFixMiner J 0.71 0.26 0.49 0.18 0.33
VulFixMiner P 0.66 0.3 0.53 0.21 0.38
VulFixMiner 0.73 0.32 0.56 0.24 0.39
Python projects are 0.77, 0.49, 0.62, 0.41, and 0.53 for AUC ,
CostEffort@5%, CostEffort@20%, Popt@5%, and Popt@20%,
respectively.
V. Q UALITA TIVE ANALYSIS AND DISCUSSION
A. Identiﬁcation of Unreported Vulnerability Fixes
During our study, we observed that not all of the vulnerabilities
and the corresponding ﬁxes have been reported to the CVEdatabase. Thus, we conduct a user study to evaluate theusefulness of our approach for identifying those secret (i.e.,unreported) vulnerability ﬁxes.
Experimental Tasks. We create tasks based on the 577
commits that are identiﬁed as false positives by VulFixMiner(with 0.5 as the threshold). Speciﬁcally, we randomly selecteda statistically representative sample of 82 commits from thesefalse positives (with a 95% conﬁdence level and 10% conﬁ-dence interval), which belong to 20 OSS systems. Each OSSsystem has 1 to 27 commits in the sampled set. For eachcommit, we ask two questions:
•Q1. Does this commit ﬁx a security vulnerability?
•Q2. If the answer to Q1 is “Yes”, what type of vulnera-bility does the commit ﬁx?
In terms of the second question, we provide 11 options fortypes of vulnerabilities as referring to the top 10 frequent CWEsoftware vulnerabilities [5], including (1) Buffer Overﬂow, (2)Improper Input V alidation, (3) Access Control Error, (4) Cross-Site Scripting, (5) Information Disclosure, (6) Numeric Error,(7) Resource Management Error, (8) Race Condition, (9) SQLInjection, (10) Cryptographic Issues, and (11) Other.
Participants. We invite 6 security experts who have 5 to 12
years of experience with software security to participate in ouruser study. We ask each of them to ﬁnish an experimental taskthat includes 12 to 15 commits.
Results. Among the 82 commits, 29 commits are conﬁrmed
by the security experts as ﬁxes of security vulnerabilities. Theresult indicates that our approach can effectively identify secretvulnerability ﬁxes. In addition, the security experts suggest that41 32 114
35VulFixMiner J 
(Effort@5%)VulFixMiner P 
(Effort@5%)
27 23113
110300 Java Vulnerability ﬁxes
195 Python Vulnerability ﬁxes
Fig. 6: V enn diagrams showing the number of identiﬁedvulnerability ﬁxes by VulFixMiner
Jand VulFixMiner Pwith
inspection effort of 5% of total LOC.
TABLE VI: An example commit that ﬁxes an unreported
vulnerability related to XXE and XML bomb attack.
[Jul 20, 2017] Key Code Change Snippet:
- private static final XMLInputFactory inputFactory
= XMLInputFactory.newInstance();
+ private static final XMLInputFactory inputFactory
= StaxUtils.createDefensiveInputFactory();
[Jul 20, 2017] Commit Message: XmlEventDecoder uses common
defensive XMLInputFactory (now in StaxUtils). Issue: SPR-15797.
[Jul 20, 2017] Related Issue Title: [SPR-15797] Disable DTD and
external entities support in XmlEventDecoder to prevent XXE and
XML bomb attack.
[Sep 22, 2017 ] Commit Comment: @dmak: “... In our applicationwe are also concerned about the security and would like to re-use
the code from StaxUtils.createDefensiveInputFactory(). ...”
the 29 ﬁxed security vulnerabilities can be further classiﬁed
into 6 categories, i.e., 12 of resource management error, 10of improper input validation, 4 of access control error, 1 ofinformation exposure, 1 of cross-site scripting, and 1 nullpointer dereference. Table VI shows an example commit
18that
ﬁxes a resource management error vulnerability in the spring-framework project on Jul 20, 2017. In the code snippet of theexample commit, the developer replaces “XMLInputFactory”with a defensive “XMLInputFactory” variant. Interestingly,the corresponding issue report conﬁrms that the commit canprevent XXE and XML bomb attacks by disabling the supportof DTD and external entities in “XMLEventDecoder”. Also,another contributor of the spring-framework project cameacross this commit on September 21, 2017, commenting thathe would like to port this commit to another branch to improvesecurity.
B. Ethical consideration
VulFixMiner identiﬁes silent vulnerability ﬁxes before these
ﬁxes are disclosed. Although attackers might use the pre-disclosed information to gain temporal advantages, OSS userscan beneﬁt from our tool by being aware of the ﬁxes as soonas they are developed, which is usually one week earlier thanthe disclosure [5]. Hence, we are not “arming” the potentialattackers. Instead, we help OSS users become aware of thevulnerability earlier and point out the corresponding ﬁx, sothey could have more time and ease in defending againstpotential attacks.
18https://github.com/spring-projects/spring-framework/commit/
e4651d6b50c5bc85c84ff537859c212ac4e33434
713TABLE VII: The number of commit messages that are security
related and not security related according to the regularexpression that is used to match security-related issues [12].
Commit type #Commit messages
Security related Security unrelated
Vulnerability ﬁx 754 1,226
Non-vulnerability ﬁx 58,979 1,502,994
Note that we are not ﬁghting against the coordinated vul-
nerability disclosure model since we do not leak informationabout the vulnerabilities before they are ﬁxed. Different fromthe full disclosure model, which discloses vulnerabilities tooearly and puts OSS users at a great disadvantage (since there isno remediation solution available at the time of disclosure), weaim at providing vulnerability ﬁx information to OSS users.In turn, OSS users are able to react to vulnerabilities earlier,to avoid the potential security attacks due to the disclosurelatency. We advocate that OSS maintainers should disclosevulnerabilities as soon as the corresponding vulnerability ﬁxesare publicly available. If OSS maintainers need more time tointegrate vulnerability ﬁxes into a new release or test the ﬁxes,they should consider postponing submitting the ﬁxes to publiccode repositories.
C. Other possible application scenarios
Basing on the ability of identifying silent vulnerability
ﬁxes, VulFixMiner can be further used to enhance published
vulnerability information by locating the corresponding vul-nerability ﬁxes. Many vulnerabilities are disclosed without theinformation of the corresponding ﬁxes and such information isimportant for security researchers in vulnerability research, forexample, automated generation of security patches. In order tolocate vulnerability ﬁx information for disclosed vulnerabili-ties, the existing works mainly rely on the keyword-matching-based approach, which is not able to locate silent vulnerabilityﬁxes, while VulFixMiner can. In addition, VulFixMiner isalso useful in facilitating some other downstream tasks, forexample, the vulnerable dependency alert. Dependabot is aGitHub security feature which is enabled in 3.5m activeGitHub repositories [49]. Dependabot analyzes the dependentgraph of a project with the published vulnerability infor-mation [50], and sends security notiﬁcations to OSS userswhen the vulnerable dependencies are detected. By usingVulFixMiner to identify silent vulnerability ﬁxes before thevulnerabilities are disclosed, we believe Dependabot coulddetect the vulnerable dependencies at an earlier stage.
D. The challenges of leveraging commit messages on mining
silent vulnerability ﬁxes
Although the coordinated vulnerability disclosure [1], [2] is
commonly and widely applied in OSS,
19,20,21we still observe
many cases of which the information of a vulnerability is
19https://www.microsoft.com/en-us/msrc/cvd
20https://github.com/google/oss-vulnerability-guide
21https://www.apache.org/security/committers.htmlTABLE VIII: Training and inference time of VulFixMiner.
VulFixMiner J VulFixMiner P VulFixMiner
Training 3h4 7m i n 1h2 9m i n 4h2 6m i n
Inference (Avg.) 0.04 sec/commit 0.02 sec/commit 0.04 sec/commit
leaked before disclosure. For example, the commit message ofthe ﬁx
22for CVE-2015-7326 [51] is “patch XXE vulnerabil-
ity”, which explicitly mentions the vulnerability type and thepurpose of the commit. Such a practice is not recommended,yet provides an opportunity for identifying vulnerability ﬁxesusing commit messages [12]–[14]. To further check how oftenthis “leakage” happens, we apply a regular expression that isused to match security-related words [12] in commit messages.Table VII shows the result of matching. We observe that38% (754 out of 1,982) of commit messages in vulnerabilityﬁxes contain security-related words, indicating the remaining62% of vulnerabilities were ﬁxed secretly. For such “secret”cases, the message-based approach fails to identify vulnera-bility ﬁxes. On the other hand, the different commit messagedocumentation styles across projects is also a challenge foridentifying vulnerability ﬁxes in a cross-project setting.
E. Time Efﬁciency
Table VIII shows the time costs of training and average infer-
ence for per commit. The training time varies as it dependson the size of the dataset. Once models have been trained, itonly takes a few milliseconds to generate the prediction scorefor a given commit.
VI. T
HREA TS TOVALIDITY
Internal validity. Threats to internal validity relate to theexperimenter bias and errors. The automated approach weproposed for labeling unreported vulnerability ﬁxes in Sec-tion IV-B may introduce bias. To mitigate the threat ofbias during the labeling, we design and apply a simple yetconservative regular expression, which only matches commitswith messages containing “cvd”, “nvd”, and “vuln”. We alsorandomly sampled 10% of 921 new labeled commits andmanually conﬁrmed that all of them are labeled correctly. Tomitigate the threat of bias in the manual examination of false-positive cases, we invite security experts who have at least 5years of experience in software security and express interestin our study. In addition, we give the security experts enoughtime to conduct these tasks.
External validity . Threats to external validity relate to the
generalizability of VulFixMiner. Our dataset is built using
disclosed vulnerability ﬁxes, which may not be representativeof all vulnerability ﬁxes, especially unreported vulnerabilityﬁxes. To mitigate the threat, we designed a strict regular ex-pression to label the unreported vulnerability ﬁxes and includethem in our dataset. Also, we have demonstrated the capabilityof VulFixMiner to mine unreported vulnerability ﬁxes inSection V-A. Future research should include more unreportedvulnerability ﬁxes if possible. We also note that we study only
22https://github.com/miltonio/milton2/commit/
5f81b0c48a817d4337d8b0e99ea0b4744ecd720b
714vulnerability ﬁxes that were reported to CVE. Although the
use of CVE is one of the most popular public vulnerabilityadvisory methods, future research should consider includingmore data sources (e.g., Exploit Database
23or project-speciﬁc
web resources), and code or vulnerability related artefacts.However, we evaluate VulFixMiner on a large dataset (i.e.,143,989 commits from 52 projects which are unseen duringthe training phase) and achieve high performance in a prac-tical setting. Especially, we include two of the most popularprogramming language, Java and Python in our study. In thefuture, we plan to include more programming languages.
VII. R
ELA TED WORK
Zhou and Sharma [12] ﬁrst explored the identiﬁcation
of vulnerability ﬁxes by leveraging commit messages. Theyextract features using word2vec embedding technique [52]and use ensemble classiﬁers to classify commits.They ﬁrstﬁlter out security-unrelated commits using a regular expressionincluding a variety of security-related words. This ﬁlteringreduces the imbalanced nature of the data, but also removessecret vulnerability ﬁxes of which messages do not containsecurity-related words. Chen et al. [14] further considers codechanges of commits. By introducing a self-training process,they can also utilize the ﬁltered-out commits. Different fromtheir work, we use deep learning technique in the analysisof code changes in commits. Their tool is not made opensource due to its commercial nature and thus we cannot makea comparison with them.
Sabetta and Bezzi [13] used an SVM model, constructed
using a BoW representation of code change and commit mes-sage tokens, to classify vulnerability ﬁxing commits. Althoughtheir approach achieves high performance, the imbalance oftheir sampled dataset does not reﬂect the natural imbalancewithin a realistic scenario. Under the same evaluation setting,VulFixMiner outperforms their approach (see Section IV-F).
Xu et al. [53] proposed SPAIN, a binary-level patch anal-
ysis framework, to automatically identify vulnerability ﬁxes.SPAIN locates and identiﬁes the code changes that ﬁx vul-nerabilities between two versions (original and patched) of abinary ﬁle. Different from SPAIN, VulFixMiner uses commit-level code changes to identify vulnerability ﬁxes, enablingOSS users to discover them in real-time.
Li and Paxson [5] conducted a large-scale empirical study
on the development cycle of vulnerability ﬁxes. They ob-serve that vulnerability ﬁxes are poorly timed with publicdisclosures, and the disclosure time for the majority of ﬁxesis delayed by more than one week. They emphasize thepotential risk caused by disclosure delays. Combined with ourﬁndings, this further advocates for the importance of real-time,automated identiﬁcation of vulnerability ﬁxes.
In comparison, we use a deep learning approach to identify
vulnerability ﬁxes. We leverage a Transformer-based languagemodel to learn the semantic meaning of code changes.
23https://www.exploit-db.com/VIII. C ONCLUSION AND FUTURE WORK
In this paper, we propose VulFixMiner, a Transformerarchitecture-based model for automated secret vulnerabilityﬁx mining. To the best of our knowledge, we are the ﬁrst touse deep learning to identify vulnerability ﬁxes in a practicalsetting. Speciﬁcally, VulFixMiner is the ﬁrst ﬁne-tuned modelon cross-project and cross-language data, using a pre-trainedlanguage model, CodeBERT. After ﬁne-tuning, VulFixMinergenerates a contextual embedding vector for each commitbased on code changes in each affected ﬁle. This vector is thenused to compute a prediction score, indicating the likelihoodthat a commit ﬁxes a vulnerability. We evaluate VulFixMineron the full set of commits from 52 projects, which are neverseen during the training phase. The evaluation results showthat VulFixMiner outperforms ﬁve baselines in discriminativepower; VulFixMiner is capable of identifying vulnerabilityﬁxes with less inspection effort in practical use. For example,with an inspection effort of reviewing 5% of total LOC,VulFixMiner can identify 49% of the total vulnerability ﬁxes.
Future work could investigate ways to improve our ap-
proach, including using additional data, improving contextualembedding vector learning by using a ﬁner granularity ofcode changes (e.g., at hunk level or line level), as well asimproving the generalizability of our approach by involvingmore programming languages.
R
EFERENCES
[1] “ISO/IEC 29147:2018: Security techniques - Vulnerability disclosure,”
https://www.iso.org/standard/72311.html, 2018.
[2] A. D. Householder, G. Wassermann, A. Manion, and C. King, “The cert
guide to coordinated vulnerability disclosure,” Carnegie-Mellon Univ
Pittsburgh Pa Pittsburgh United States, Tech. Rep., 2017.
[3] “Examining apache struts remote code execution vulnerabilities,”
https://www.synopsys.com/blogs/software-security/apache-struts-remote-code-execution-vulnerabilities/, 2017, accessed: 2020-04-06.
[4] “Equifax releases details on cybersecurity incident, announces per-
sonnel changes,” https://investor.equifax.com/news-and-events/press-releases/2017/09-15-2017-224018832, 2017, accessed: 2020-04-06.
[5] F. Li and V . Paxson, “A large-scale empirical study of security patches,”
inProceedings of the 24th ACM SIGSAC Conference on Computer and
Communications Security (CCS), 2017, pp. 2201–2215.
[6] Apache Software Foundation, “CVE-2018-11776,” https://nvd.nist.gov/
vuln/detail/CVE-2018-11776, 2018.
[7] Semmle Team, “Apache struts vulnerability - CVE-2018-11776,”
https://blog.semmle.com/remote-code-execution-vulnerability-in-apache-struts-cve-2018-11776/, 2018.
[8] G. Altekar, I. Bagrak, P . Burstein, and A. Schultz, “OPUS: Online
patches and updates for security.” in Proceedings of the 14th USENIX
Security Symposium (USENIX Security), 2005, pp. 287–302.
[9] J. Arnold and M. F. Kaashoek, “Ksplice: Automatic rebootless kernel
updates,” in Proceedings of the 4th ACM European conference on
Computer systems (EuroSys), 2009, pp. 187–198.
[10] Y . Chen, Y . Zhang, Z. Wang, L. Xia, C. Bao, and T. Wei, “Adaptive
android kernel live patching,” in Proceedings of the 26th USENIX
Security Symposium (USENIX Security), 2017, pp. 1253–1270.
[11] C. Mulliner, J. Oberheide, W. Robertson, and E. Kirda, “Patchdroid:
Scalable third-party security patches for android devices,” in Proceed-
ings of the 29th Annual Computer Security Applications Conference(ACSAC), 2013, pp. 259–268.
[12] Y . Zhou and A. Sharma, “Automated identiﬁcation of security issues
from commit messages and bug reports,” in Proceedings of the 11th
2017 joint meeting on foundations of software engineering (FSE), 2017,pp. 914–919.
715[13] A. Sabetta and M. Bezzi, “A practical approach to the automatic
classiﬁcation of security-relevant commits,” in Proceedings of the 34th
IEEE International Conference on Software Maintenance and Evolution
(ICSME). IEEE, 2018, pp. 579–582.
[14] Y . Chen, A. E. Santosa, A. M. Yi, A. Sharma, A. Sharma, and D. Lo, “A
machine learning approach for vulnerability curation,” in Proceedings
of the 17th International Conference on Mining Software Repositories(MSR), 2020, pp. 32–42.
[15] R. Ramsauer, L. Bulwahn, D. Lohmann, and W. Mauerer, “The sound of
silence: Mining security vulnerabilities from secret integration channelsin open-source projects,” in Proceedings of the 11st ACM SIGSAC
Conference on Cloud Computing Security Workshop (CCSW), 2020, pp.147–157.
[16] A. V aswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
L. Kaiser, and I. Polosukhin, “Attention is all you need,” in Proceedings
of the 31st International Conference on Neural Information ProcessingSystems (NIPS), 2017.
[17] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou, B. Qin,
T. Liu, D. Jiang, and M. Zhou, “Codebert: A pre-trained model forprogramming and natural languages,” in Findings of EMNLP, September
2020.
[18] “Our replication package,” https://github.com/2021-CONFDA TA/2021-
CONF-DA TA, 2020.
[19] Wikipedia, “Full disclosure,” https://en.wikipedia.org/wiki/Full
disclosure (computer security), 2020, accessed: 2020-04-06.
[20] “Roberta: A robustly optimized bert pretraining approach.”[21] R.-M. Karampatsis, H. Babii, R. Robbes, C. Sutton, and A. Janes,
“Big code!= big vocabulary: Open-vocabulary models for source code,”inProceedings of the 42nd IEEE/ACM International Conference on
Software Engineering (ICSE). IEEE, 2020, pp. 1073–1085.
[22] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhut-
dinov, “Dropout: a simple way to prevent neural networks from over-ﬁtting,” The journal of machine learning research, vol. 15, no. 1, pp.
1929–1958, 2014.
[23] S. E. Ponta, H. Plate, A. Sabetta, M. Bezzi, and C. Dangremont, “A
manually-curated dataset of ﬁxes to vulnerabilities of open-source soft-ware,” in Proceedings of the 16th IEEE/ACM International Conference
on Mining Software Repositories (MSR). IEEE, 2019, pp. 383–387.
[24] Y . Kamei, T. Fukushima, S. McIntosh, K. Yamashita, N. Ubayashi,
and A. E. Hassan, “Studying just-in-time defect prediction using cross-project models,” Empirical Software Engineering (EMSE), vol. 21, no. 5,
pp. 2072–2106, 2016.
[25] X. Wu, W. Zheng, X. Xia, and D. Lo, “Data quality matters: A case
study on data label correctness for security bug report prediction,” IEEE
Transactions on Software Engineering (TSE), 2021.
[26] G. K. Rajbahadur, S. Wang, Y . Kamei, and A. E. Hassan, “The impact
of using regression models to build defect classiﬁers,” in Proceedings
of the 14th IEEE/ACM International Conference on Mining SoftwareRepositories (MSR). IEEE, 2017, pp. 135–145.
[27] B. Ghotra, S. McIntosh, and A. E. Hassan, “Revisiting the impact
of classiﬁcation techniques on the performance of defect predictionmodels,” in Proceedings of the 37th IEEE/ACM IEEE International
Conference on Software Engineering (ICSE) , vol. 1. IEEE, 2015, pp.
789–800.
[28] S. Lessmann, B. Baesens, C. Mues, and S. Pietsch, “Benchmarking clas-
siﬁcation models for software defect prediction: A proposed frameworkand novel ﬁndings,” IEEE Transactions on Software Engineering (TSE),
vol. 34, no. 4, pp. 485–496, 2008.
[29] K. Greff, R. K. Srivastava, J. Koutn ´ık, B. R. Steunebrink, and J. Schmid-
huber, “Lstm: A search space odyssey,” IEEE transactions on neural
networks and learning systems (TNNLS), vol. 28, no. 10, pp. 2222–2232, 2016.
[30] Y . Yang, X. Xia, D. Lo, and J. Grundy, “A survey on deep learning for
software engineering,” arXiv preprint arXiv:2011.14597, 2020.
[31] Q. Huang, X. Xia, and D. Lo, “Revisiting supervised and unsupervised
models for effort-aware just-in-time defect prediction,” Empirical Soft-
ware Engineering (EMSE), vol. 24, no. 5, pp. 2823–2862, 2019.
[32] Y . Yang, Y . Zhou, J. Liu, Y . Zhao, H. Lu, L. Xu, B. Xu, and H. Leung,
“Effort-aware just-in-time defect prediction: simple unsupervised modelscould be better than supervised models,” in Proceedings of the 24th
ACM SIGSOFT international symposium on foundations of softwareengineering, 2016, pp. 157–168.
[33] Y . Kamei, E. Shihab, B. Adams, A. E. Hassan, A. Mockus, A. Sinha,
and N. Ubayashi, “A large-scale empirical study of just-in-time qualityassurance,” IEEE Transactions on Software Engineering (TSE), vol. 39,
no. 6, pp. 757–773, 2012.
[34] T. Mende and R. Koschke, “Effort-aware defect prediction models,” in
Proceedings of the 14th European Conference on Software Maintenanceand Reengineering (CSMR). IEEE, 2010, pp. 107–116.
[35] S. McIntosh and Y . Kamei, “Are ﬁx-inducing changes a moving target?
a longitudinal case study of just-in-time defect prediction,” IEEE Trans-
actions on Software Engineering (TSE), vol. 44, no. 5, pp. 412–428,2017.
[36] G. H. Nguyen, A. Bouzerdoum, and S. L. Phung, “Learning pattern
classiﬁcation tasks with imbalanced data sets,” Pattern recognition, pp.
193–208, 2009.
[37] A. Severyn and A. Moschitti, “Learning to rank short text pairs with
convolutional deep neural networks,” in Proceedings of the 38th in-
ternational ACM SIGIR conference on research and development ininformation retrieval (SIGIR), 2015, pp. 373–382.
[38] T. Hoang, H. K. Dam, Y . Kamei, D. Lo, and N. Ubayashi, “Deepjit: an
end-to-end deep learning framework for just-in-time defect prediction,”inProceedings of the 16th IEEE/ACM International Conference on
Mining Software Repositories (MSR). IEEE, 2019, pp. 34–45.
[39] X. Y u, K. E. Bennin, J. Liu, J. W. Keung, X. Yin, and Z. Xu, “An
empirical study of learning to rank techniques for effort-aware defectprediction,” in Proceedings of the 26th IEEE International Conference
on Software Analysis, Evolution and Reengineering (SANER). IEEE,2019, pp. 298–309.
[40] E. Arisholm, L. C. Briand, and E. B. Johannessen, “A systematic and
comprehensive investigation of methods to build and evaluate faultprediction models,” Journal of Systems and Software, vol. 83, no. 1,
pp. 2–17, 2010.
[41] J. A. Hanley and B. J. McNeil, “The meaning and use of the area under
a receiver operating characteristic (ROC) curve.” Radiology, vol. 143,
no. 1, pp. 29–36, 1982.
[42] S. Lessmann, B. Baesens, C. Mues, and S. Pietsch, “Benchmarking clas-
siﬁcation models for software defect prediction: A proposed frameworkand novel ﬁndings,” IEEE Transactions on Software Engineering (TSE) ,
vol. 34, no. 4, pp. 485–496, 2008.
[43] C. Tantithamthavorn and A. E. Hassan, “An experience report on defect
modelling in practice: Pitfalls and challenges,” in Proceedings of the
40th International Conference on Software Engineering: Software En-gineering in Practice (ICSE-SEIP). New Y ork, NY , USA: Associationfor Computing Machinery, 2018, p. 286–295.
[44] D. Romano and M. Pinzger, “Using source code metrics to predict
change-prone java interfaces,” in Proceedings of the 27th IEEE Inter-
national Conference on Software Maintenance (ICSM). USA: IEEEComputer Society, 2011, p. 303–312.
[45] D. P . Kingma and J. Ba, “Adam: A method for stochastic optimization,”
Proceedings of the 3rd International Conference on Learning Represen-tations (ICLR), 2014.
[46] R. Caruana, S. Lawrence, and L. Giles, “Overﬁtting in neural nets:
Backpropagation, conjugate gradient, and early stopping,” Advances in
neural information processing systems (NIPS), pp. 402–408, 2001.
[47] L. Prechelt, “Automatic early stopping using cross validation: quantify-
ing the criteria,” Neural Networks, vol. 11, no. 4, pp. 761–767, 1998.
[48] G. K. Rajbahadur, S. Wang, G. Ansaldi, Y . Kamei, and A. E. Hassan,
“The impact of feature importance methods on the interpretation ofdefect classiﬁers,” IEEE Transactions on Software Engineering (TSE),
2021.
[49] GitHub, “Automated security updates,” https://github.blog/changelog/
2019-11-14-automated-updates/, 2019.
[50] ——, “Keep all your packages up to date with dependabot,”
https://github.blog/2020-06-01-keep-all-your-packages-up-to-date-with-dependabot, 2020.
[51] MITRE, “CVE-2015-7326,” https://nvd.nist.gov/vuln/detail/CVE-2018-
11776, 2015.
[52] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean, “Distributed
representations of words and phrases and their compositionality,” inProceedings of the 26th International Conference on Neural InformationProcessing Systems (NIPS), 2013.
[53] Z. Xu, B. Chen, M. Chandramohan, Y . Liu, and F. Song, “Spain: security
patch analysis for binaries towards understanding the pain and pills,”inProceedings of the 39th IEEE/ACM International Conference on
Software Engineering (ICSE). IEEE, 2017, pp. 462–472.
716