Layout and Image Recognition Driving
Cross-Platform Automated Mobile Testing
Shengcheng Yu, Chunrong Fang, Yexiao Yun, Yang Feng
State Key Laboratory for Novel Software Technology, Nanjing University, China
Corresponding author: fangchunrong@nju.edu.cn
Abstract ‚ÄîThe fragmentation problem has extended from An-
droid to different platforms, such as iOS, mobile web, and even
mini-programs within some applications (app), like WeChat1.
In such a situation, recording and replaying test scripts is one
of the most popular automated mobile app testing approaches.
However, such approach encounters severe problems when cross-
ing platforms. Different versions of the same app need to be
developed to support different platforms relying on different
platform supports. Therefore, mobile app developers need to
develop and maintain test scripts for multiple platforms aimed at
completely the same test requirements, greatly increasing testing
costs. However, we discover that developers adopt highly similar
user interface layouts for versions of the same app on different
platforms. Such a phenomenon inspires us to replay test scripts
from the perspective of similar UI layouts.
In this paper, we propose an image-driven mobile app test-
ing framework , utilizing Widget Feature Matching and Layout
Characterization Matching to analyze app UIs. We use computer
vision (CV) technologies to perform UI feature comparison and
layout hierarchy extraction on mobile app screenshots to obtain
UI structures containing rich contextual information of app
widgets, including coordinates, relative relationship, etc. Based
on acquired UI structures, we can form a platform-independent
test script, and then locate the target widgets under test. Thus,
the proposed framework non-intrusively replays test scripts
according to a novel platform-independent test script model. We
also design and implement a tool named LIRAT to devote the
proposed framework into practice, based on which, we conduct
an empirical study to evaluate the effectiveness and usability of
the proposed testing framework. The results show that the overall
replay accuracy reaches around 65.85% on Android (8.74%
improvement over state-of-the-art approaches) and 35.26% on
iOS (35% improvement over state-of-the-art approaches).
Index Terms ‚ÄîCross-Platform Testing, Mobile Testing, Image
Analysis, Record and Replay
I. I NTRODUCTION
Fragmentation problem is proposed in [1]. In the situation of
Android fragmentation problems, recording and replaying test
scripts on a large scale device cluster is one of the key quality
assurance technologies for mobile apps. It can automatically
execute preset test cases and detect various bugs [2]. Test
scenarios recorded on one device can be replayed on other de-
vices of different hardware or software (e.g. operating system
versions). Moreover, the fragmentation problem has extended
to multiple platforms, including Android, iOS, mobile web,
and even mini-programs within some apps, like WeChat. Here,
we deÔ¨Åne the ‚Äúplatform‚Äù much more than operating system,
1A popular chatting app in China, providing a platform for other manufac-
turers to deploy mobile apps.but refer to a set of complete frameworks that independently
provide an environment to support the apps to run. The feature
of rapid iteration and frequent requirement change of mobile
apps on different platforms triggers even increasing demands
on app quality assurance. For a speciÔ¨Åc app, the expanded
fragmentation problem on all mobile platforms means mul-
tiple clients for different platforms sharing the same services
provided by a uniÔ¨Åed server. More importantly, they also share
similar UI layouts for better human-computer interaction.
The expanded fragmentation problem raises a higher de-
mand for developers when they test their mobile apps. In
other words, they have to write different test scripts based
on different framework supports for completely the same
test requirements, leading to tedious and repetitive work.
Moreover, different customized operating system versions can
even have different supports for test script execution. This
phenomenon causes a great burden on developers because they
need to get familiar with platform-speciÔ¨Åc features. Besides,
the test scripts are impossible to execute generally for different
platforms. In lack of platform-free testing technologies, it is
impossible for cross-platform test script record and replay.
Fig. 1. An Example: Multiple Clients for JingDong
Most mobile developers design the app user interface (UI)
with a high similarity when supporting various platforms
to improve the user experience. Here, we take one of the
most popular online shopping apps in China (like Amazon
in the US), JingDong , as an example to illustrate such a
phenomenon (See Fig. 1). It is evident from Fig. 1 that the
general layouts are with high similarity on different platforms,
as are the UI elements and their relative positions, despite
some slight differences on contents. However, faced with these
highly similar UI layouts, developers are still required to
15612021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)
1558-1225/21/$31.00 ¬©2021 IEEE
DOI 10.1109/ICSE43902.2021.00139
develop test scripts respectively for different platforms based
on platform-speciÔ¨Åc features to test the same functions, and to
meet completely the same test requirements. They also need
to adopt complete different testing frameworks on various
platforms, which signiÔ¨Åcantly increases both economic and
human resource costs. The similar UI layouts enlighten us
to research from the UI perspective instead of the respective
underlying implementation.
To reduce the costs of adapting scripts among a wide
range of platforms, some existing researches start with dif-
ferent points of views, like exploring the runtime statements,
matching source code of the apps on various platforms, taking
advantage of UI images, etc. However, such techniques tend
to be intrusive and have much overhead [3] [4]. Also, they
can hardly identify dynamic or similar widgets, which are
common in current mobile apps. Different platform features
and different testing framework supports always make it a
failure to relocate the target widgets and replay test scripts
even on the same platform, let alone cross-platform replays.
Some researchers have done primary studies based on image
understanding techniques. Sikuli [5] [6] is an image-based
testing tool focused on desktop apps, and it can identify and
manipulate GUI widgets without source code. However, Sikuli
has a poor support for the mobile environment. Sikuli relies
on simple image feature matching, which can lead to failures
when images are too simple to extract enough features to
match. Airtest [7] is another testing tool based on image-driven
technologies, and it is developed on the basis of Sikuli. How-
ever, Airtest adopts different matching solutions for various
mobile platforms, thus making it still hardly possible to record
and replay test scripts among different platforms.
Tools mentioned above merely make simple image feature
extraction and matching, making it tough to deal with dynamic
elements, which is common in such a data explosion era. For
example, in a news app, each piece of news has a different
content, constructing a distinct image feature set. When the
content is refreshed, traditional technologies will have trouble
locating the recorded widgets via image features. Such tools
take ‚Äú images ‚Äù as ‚Äú images ‚Äù only instead of ‚Äú widget
sets ‚Äù. In other words, they ignore the contents and mutual
relationships, and some important information is left out.
In this paper, we propose an image-driven mobile app test-
ing framework to solve the cross-platform record and replay
problem of test scripts for the Ô¨Årst time. We combine image
context understanding and layout extraction to solve the prob-
lem. During the recording phase, the proposed testing frame-
work automatically characterize the layout, and extracts widget
screenshots, layout coordinates, and other attributes from the
testing devices. With the obtained information, we form a
test script according to the platform-independent test script
model introduced in Section III-A, including all the extracted
screenshots and well-organized hierarchy XML Ô¨Åles. In the
replaying phase, the proposed testing framework adopts both
traditional computer vision and deep learning technologies.
The image-driven mobile app testing framework is composed
of Widget Feature Matching and Layout CharacterizationMatching. Layout Characterization Matching can compensate
for the drawbacks of Widget Feature Matching when a mobile
app activity has dynamic or several similar widgets. After a
comprehensive analysis of the intermediate results of Widget
Feature Matching and Layout Characterization Matching, LI-
RAT can reach a high accuracy when positioning widgets on
different devices. Therefore, the corresponding operations can
be successfully replayed.
The proposed image-driven mobile app testing framework
realizes ‚Äúone script record, multiple script replays‚Äù on devices
of different platforms. The framework utilizes the combination
of image understanding and layout extraction for the Ô¨Årst time,
and the framework signiÔ¨Åcantly reduces the complexity of test
script developing.
Based on the image-driven mobile app testing framework,
we design and implement a tool named LIRAT. LIRAT simu-
lates real app manipulation and simpliÔ¨Åes the test script devel-
oping process into operation sequence record. Users operate
on webpages, where a projection of real mobile devices are
shown. Also, We conduct an empirical experiment to evaluate
our image-driven mobile app testing framework on LIRAT.
Main contributions of this paper are as follows:
We propose an image-driven mobile app testing frame-
work for cross-platform test script record and replay, solv-
ing the problem of reusing test scripts cross platforms.
We introduce a platform-independent test script model
containing rich information recorded from mobile apps,
including screenshots, widget information, etc.
We declare a comprehensive cross-platform widget
matching approach, including Widget Feature Match-
ing and Layout Characterization Matching, and based on
which we design and implement a novel tool, which can
record and replay test scripts on multiple platforms.
We conduct an empirical experiment on how our ap-
proach with real-world apps and devices, and the tool
can effectively improve testing efÔ¨Åciency.
The rest of this paper is organized as follows. Section II
introduces the problems and existing solutions, together with
their drawbacks. Section III illustrates the methodology in
detail, including the pivotal technologies in the record phase
and replay phase. The speciÔ¨Åc tool design and implementation
are presented in Section IV. In Section V, an empirical
experiment is conducted to evaluate the effectiveness of the
proposed approach. Section VI introduces the related work.
Finally, this research is concluded in Section VII.
More details of the framework design and the experiment
data can be found on https://sites.google.com/view/lit2020.
II. B ACKGROUND & M OTIVATION
Many researches have been done to deal with the record and
replay of test scripts. They start from different perspectives,
including runtime statements, source codes, UI images, etc.
However, such solutions still have drawbacks and limitations.
1562Fig. 2. Image-Driven Mobile App Testing Framework
A. Script Replay Dilemma
The fragmentation problem originates from the Android
platform, which is open for all manufacturers to make modiÔ¨Å-
cations to satisfy their own demands. The openness of Android
has led to hundreds of thousands of different types of mo-
bile models with different brands, operating system versions,
screen shapes, resolutions, etc. Currently, the fragmentation
problem has extended to many other platforms, such as 1)
iOS, where devices have different screen shapes, different
resolutions; 2) mini-programs within some apps, where mobile
apps need to show the same appearances but base on different
operating system kernels; and 3) mobile web, where mobile
apps need to make special modiÔ¨Åcations to user interfaces for
mobile web browsers. Developers have to develop different
versions of an app to Adapt to different platforms.
We also conduct an analysis on the fragmentation problem.
We Ô¨Ånd that 8 apps in the top 10 free apps and 7 apps in
the top 10 paid apps in the Google Play [8] have their iOS
versions, and most of them have mobile web versions2. This
result proves the fragmentation problem actually exists and
has a deep impact on the mobile app developing.
Such extended fragmentation problem leads to a great
burden for developers on app testing. Developing test scripts
for each platform and each version respectively can be indeed
painful and time-consumption, and makes it easy for develop-
ers to make mistakes. Each platform will require a group of de-
velopers to work on app testing. Moreover, as for developers,
testing work is much more substantial. In traditional testing,
developers must acquire the underlying information, such as
the widget ID or XPath. This phenomenon makes it hard
for cross-platform replay, because the implementations for
multiple platforms of the same mobile app are quite different
and based on the features of each platform.
B. Limitations of Current Solutions
Sikuli is an automated testing tool presented by Yeh et
al. [6]. Sikuli uses activity screenshots to generate test cases
and execute automated testing for desktop apps. It can be
2Some applications have no mobile web version due to their categories,
such as tools, video games, etc.used for various web-based apps and desktop applications
[3]. However, Sikuli has a poor support for mobile devices.
Though it can be used to operate mobile device projections on
desktop emulators, being unable able to operate on real devices
is a signiÔ¨Åcant drawback. Moreover, Sikuli adopt simple
image feature extraction and matching, which is unsuitable
for the constantly refreshing apps. Therefore, the problem of
recording and replaying test scripts for mobile apps is still
unresolved. However, the ideas of Sikuli inspired us to make
use of mobile app UI elements.
Airtest is an automated testing tool for GUI testing. Airtest
has a better support for mobile platforms. Airtest technology
can acquire the whole UI tree structures from the .apk
Ô¨Åles, and identify the target widgets. Then, related simu-
lative operations to replay scripts will be executed. Airtest
mainly focuses on video game testing, where widgets have
different image features. Therefore, Airtest still has problems
when facing mobile apps of a wider range of categories,
such as tools, news apps, etc., especially when the widgets
have similar image features. Moreover, Airtest cannot execute
cross-platform replays of the same script, such as iOS and
mobile web client because it adopts different script developing
implementations on different platforms.
III. M ETHODOLOGY
Our proposed image-driven mobile app testing framework
consists of two processes: Script Record and Script Replay
(See Fig. 2). Besides, the proposed framework also adopts a
novel platform-independent test script model.
In the Script Record process, the proposed framework
records the screenshots and layout information of the widgets
operated by the users required in the test requirements, and
translates obtained information into test scripts according to
the proposed platform-independent test script model LITS .
In the Script Replay process, we extracted the LITS script,
and combine Widget Feature Matching and Layout Charac-
terization Matching to match the corresponding widgets on
the replaying devices according to the screenshots and layout
information recorded in the LITS instances.
1563A. Platform-Independent Test Script Model
We propose a novel test script model, named LITS , which
means Layout & Image Test Script .LITS is platform-
independent because we extract and record all the informa-
tion from UI screenshots without relying on platform-speciÔ¨Åc
features or functions. Moreover, we make further processing
to make the obtained information free of device attributes.
Therefore,LITS gets rid of both software and hardware
dependence and can be used uniformly without adaptation.
During the script record, we extract the rich but necessary
information after each operation. During the script replay, we
also extract the same information from the replaying device
and match with the information stored in LITS scripts. The
information for each operation includes the screenshot of the
app activity under test, denoted as SSa; the screenshot of the
operated widget, denoted as SSw; the coordinate of the widget,
denoted as Cw, which is composed of the top-left and the
bottom-right coordinates of the operated widget; the operated
point coordinate, denoted as Co; the recording device serial
number, denoted as DSN ; the recording device resolution,
denoted asDR; texts on the widget, denoted as T; and the
operation type, denoted as O.
All the above information are critical to the cross-platform
record and replay. SSais used to analyze the whole context
of the activity, we can extract all the widgets, including
CwfromSSa.SSwis extracted from SSa, representing the
target widget. SSwplays an important role in Widget Feature
Matching replay. Cwis representing with the distance from the
top-left vertex of the screenshot, and it is used for the layout
characterization. Cois the operated point, and it can help judge
the operated point falls in which extracted widget screenshot
range.DSN is combined with the recording timestamp as the
script id for store. DR is also an important element. It is used
to calculate the relative proportional position of the widget
and the operation point. Tis the texts on the widget, which
can assist identify the widgets. Orefers to the operation type,
like click, slide, etc., which is also an indispensable part.
When the above information is obtained, further processing
is required for making the script platform-independent. First,
we combine the DSN and recording timestamp as the script
id. Second, we calculate the relative proportional position
of the widget and the operation point, speciÔ¨Åcally, which
are calculated as the proportion of the absolute coordinate
and the resolution of the device, and the results are denoted
asCwr,Cor. Therefore, LITS is a list of 7-tuples, which
is< ID;SS a;SSw;O;C wr;Cor;T > , and each 7-tuple
represent for a user operation.
B. Script Record
Script record is implemented by a series of single-step
operations record. The screenshots and layout information
of the operated widgets are extracted and recorded for each
operation as a LITS instance, attached with some attribute
information of the widgets, such as texts, widget types, etc.
When received by the recording device, the user‚Äôs operations
are converted into executable instructions on different plat-forms through the ADB [9] (for Android) or WDA [10] (for
iOS). The extracted widget information, including coordinates,
texts, etc. is recorded, and based on the information, XML
Ô¨Åles representing the activity layout in tree structures will be
generated automatically through the proposed framework. To-
gether with the activity screenshots and the widget screenshots
cropped from activity screenshots, the XML Ô¨Åles are stored in
the form of a nested directory, the root directory represents
for the test script, and each subdirectory represents for the
widget information Ô¨Åle for each operation, and the operation
sequence is stored in the root directory. Algorithm 1 shows
the formal expression. The input is a sequence of operations,
denoted asOS, and the output is a platform-independent test
script deÔ¨Åned in Section III-A, denoted as LITS .
Script Record : Based onLITS model, each operation on
a widget is recorded. Then, necessary information mentioned
in Section III-A is automatically extracted and primary pro-
cessing like relative coordinate calculation are done.
Algorithm 1 Script Record
Input: Operation Sequence OS
Output: Test ScriptLITS
1:initiateLITS
2:foreach Operation O2OSdo
3: initiate 7-dimension tuple TSo
4:TSo(DSN
5:TSo(O
6:TSo(SSa
7: Crop the screenshot of operated widget SSw
8:TSo(SSw
9: Extract the top-left and the bottom-right coordinate of
the operated widget Cw
10:TSo((Cwr=Cw/DR)
11: Extract the coordinate of the operated point Co
12:TSo((Cor=Co/DR)
13: Extract the text on the widget T
14:TSo(T
15:LITS(TSo
16:end for
17:returnLITS ;
C. Script Replay
For script replay, the proposed framework retrieves the
script Ô¨Åle from the database, and then orderly executes widget
matching and single-step replay.
According to the formal expression in Algorithm 2, Ô¨Årst,
in the order of the operation sequences that obey the
testing requirements, each step is replayed on the replay-
ing devices. For each step, we extract the screenshot of
the activity under test and the operated widget, and per-
form matching using both Widget Feature Matching and
Layout Characterization Matching separately. Widget Fea-
ture Matching will output a set of possible widgets with
runImageMatching() (Line 2 in Algorithm 2), and Lay-
out Characterization Matching will output only one candidate
1564widget with runLayoutMatching() (Line 3 in Algorithm
2). The nearest one in the possible widget set from Widget
Feature Matching to the candidate widget from Layout Char-
acterization Matching is considered as the candidate widget of
Widget Feature Matching(Line 4 to Line 17 in Algorithm 2).
The 2 candidate widgets will be merged with parameter to
obtain a target widget.
Then, the coordinates of the target widget will be matched,
and the operation information will be converted into executable
commands on the replaying devices. A successful replay refers
to the success of the operation on the right widgets and makes
the app redirect to the preconceived activity.
The proposed image-driven mobile app testing framework
adopts two algorithms to match the operated widgets in
the scripts and on the replaying devices: Widget Feature
Matching andLayout Characterization Matching .
Script Replay : For each step, the activity screenshot and
widget information on the replaying device are extracted
and compared with the information in the recorded LITS
script. Then the coordinate is acquired, and the corresponding
operation is therefore done.
Algorithm 2 Script Replay
Input: Test ScriptLITS , Replaying Device Dreplay
Output: Test Result TR
1:initiate target widget Wtarget
2:
image(runImageMatching()
3:Wtarget layout(runLayoutMatching()
4:if
image .size() == 1then
5:Wtarget image(
image .get(0)
6:else
7:dmin= +1
8: foreach widget Wimage2
image do
9:dwidget(distance(Wimage ,Wtarget layout )
10: ifdwidget< dmin then
11: dmin(dwidget
12: Wtarget image(Wimage
13: else
14: continue
15: end if
16: end for
17:end if
18:set
19:TR(operate(Wtarget image +(1 )Wtarget layout )
20:returnTR;
1)Widget Feature Matching. :Widget Feature Match-
ing algorithm is used to match the screenshots of app widgets
recorded in the scripts and the corresponding widgets on the
replaying devices. It takes the target image and the image to
match as input and outputs the coordinate value of the widgets.
The algorithm includes Ô¨Åve main processes: preprocessing,
feature extraction, feature matching, mismatch elimination,
and distortion calculation. Each process is described below:
Preprocessing. The input image of the widget is per-
formed with grayscale processing since the color informa-tion of the image is not treated as a processing attribute.
This process make a projection from a 3-channel image to
an 1-channel, greatly improving the efÔ¨Åciency of subse-
quent processing and keeping the relative features of color
changing and object contours [11]. The preprocessing
enables a better effect in subsequent processing.
Feature Extraction. This process includes detection of
image features and construction of image feature descrip-
tor set and feature point set. The target widget image
and the activity image to match are processed separately,
and two feature point sets (represented as Ktarget and
Ksource ), and two descriptor sets (represented as Dtarget
andDsource ) are obtained.
Feature Matching. We perform the feature matching and
then estimate nearest neighbors. In the processing of two
feature point sets, the nearest points found in Dtarget
andDsource are considered as matching points. Then, the
preliminary matching of the feature points is completed.
Mismatch Elimination. Matching points may have mis-
matches, so it is necessary to eliminate such mismatches.
We employ a ratio testing to address this problem. If
the calculated value is smaller than a preset threshold ,
the match is considered good. Otherwise, the match is
considered as a mismatch and will be removed.
Distortion Calculation. Since the target images may
have distortions such as rotation and zoom, in order to ob-
tain the position of the matching region more accurately,
the homography matrix between the target widget image
and the activity image to match is calculated. Finally,
the perspective transformation of the target image is
performed to obtain the coordinate position information.
Widget Feature Matching can complete the area matching
of the widget screenshot to the image of the replaying device
activity page. It can almost complete the processing from
image input to coordinate output in a few milliseconds, which
largely ensures the soundness in the replay process.
However, the limitation is that when the widget screenshots
in the replaying devices change frequently or when there are
many dynamic or similar widgets in the activity, the target
widget can hardly be correctly positioned. Therefore, we will
record all the suspected widget information and compare
them with the result acquired from Layout Characterization
Matching, and Ô¨Ånally calculate the probability for the suspect
widgets to be operated.
2)Layout Characterization Matching. :Because of the
rapid refreshing of app contents, Widget Feature Match-
ing would be out of effect because it relies heavily on the
image features of app contents. In the UI testing tasks, the
smoothness of app functionality, instead of the app contents, is
the main concern. Therefore, a supplement of Widget Feature
Matching positioning is necessary. Due to the similarity of app
UI layouts among different platforms, we are considering fur-
ther employing Layout Characterization Matching to improve
the replay accuracy. Here we deÔ¨Åne the ‚Äúlayout‚Äù as the widget
localization and the hierarchy relation among the widgets.
In the Layout Characterization Matching, we Ô¨Årst extract
1565Fig. 3. Widget Feature Matching
Fig. 4. Layout Characterization Matching
the widget contours based on the recorded activity screenshot
stored in the scripts, and then divide the activity screenshot
according to the widget contours, and acquire the relative
position of the widgets on the activity. After obtaining all the
widget contours, we will characterize the layout of the activity.
First, we execute Group operation, which means a rough
horizontal characterization to the activity. In this process,
widgets wrapped in other widgets are omitted in this step, and
we will get several groups of widgets. Then, we will divide
each group into several lines by Line operation. In the Line
operation, some widgets that wrapping other widgets will be
segmented according to the contours of the wrapped widgets.
Finally, in each line, we execute the Column operation to
segment each line into several columns vertically. Therefore,
each widget can be represented as a 3-tuple (g;l;c ), which
means the group number, the line number and the column
number. Also, the relative relationship among the widgets and
the activity structure are also acquired according to the 3-
tuple. The 3-tuple is platform-independent, and for replaying,
the 3-tuples will be translated into 2-dimension coordinates
according to the corresponding position in the Layout Char-
acterization Matching results.
Our approach might generate some noise data. We also
make efforts to eliminate such noise data. According to our
survey on an open-sourced dataset [12], we observe that the
size of most widgets is more than 1% of the screenshot size.
Therefore, we discard the data whose size is smaller than the
1% of the screenshot size.
In order to further improve the accuracy, we also extract
text information on the widgets. On many occasions, some
highly similar widgets with different texts are close to each
other, making the matching hard to handle, so that the texts
can assist the matching.
After the necessary information is collected, then starts
the script replay process. During the replay, Ô¨Årst, the same
process is performed on the replaying devices, then we load
the recorded information from the record device to match theinformation from the replaying devices.
With the Layout Characterization Matching, we can easily
solve the problem of dynamic widgets that the contents are
rapidly refreshing. Take the news app as an example. While
replaying the test script, the piece of news in the recorded
script may have changed, and in the place of the news, there
is a new piece of news. It is equivalent to click on the new
piece of news. For Widget Feature Matching, the 2 different
pieces of new are deÔ¨Ånitely different, so the matching would
fail. However, with the Layout Characterization Matching, the
framework will ignore the detailed content and pay attention
to the widget position and the activity layout.
IV. T OOL IMPLEMENTATION
In order to devote the proposed image-driven mobile app
testing framework into practice, we design and implement
a tool, namely LIRAT, which is short for Layout and
Image Recognition Driving Cross-Platform Automated Mobile
Testing. In this section, we illustrate the speciÔ¨Åc design and
detailed algorithms and parameters. Based on LIRAT, we also
conduct an empirical experiment to evaluate the effectiveness
of our proposed image-driven mobile app testing framework,
which will be discussed in the next section.
LIRAT is user-friendly. For script record, developers can
select one speciÔ¨Åc device and operate on the webpage of the
LIRAT, and the operation will be automatically recorded and
analyzed. For script replay, developers only need to invoke
a recorded script, and then select the devices they want to
execute the script replay. The following process is automatic.
A. Widget Feature Matching Replay
To replay with Widget Feature Matching, we extract image
features of activity screenshots and widget screenshots with
SIFT algorithm [11]. The process can be seen in Fig. 3.
The SIFT algorithm can effectively detect and describe local
features in images, it is also a key technique adopted in
state-of-the-art image-based record and replay tools. After
1566extracting the image features and forming the feature point
sets and descriptor sets (represented as Ktarget ,Ksource ,
Dtarget andDsource ), we employ FLANN library proposed by
Muja et al. [13]. FLANN performs fast approximate nearest
neighbor searches in high dimensional spaces. We perform
FLANN algorithm on the extracted feature point sets from
both the recorded activity screenshot under test and real-time
activity on the replaying device. In the processing of two
feature point sets, the KD-Tree index is used, and the KNN
algorithm helps Ô¨Ånd the nearest points in Dtarget andDsource
as matching points. Therefore, the preliminary matching of
the feature points is executed. To eliminate mismatches of
the matching points, we utilize a ratio testing method given
by Lowe [14], which is calculated according to the formula
ratio =Dmin
Dsecond min. According to the practical experience,
the threshold is set as 0.5 in our tool ( =0.5).
B. Layout Characterization Matching Replay
To solve the problems triggered by the drawbacks of the
Widget Feature Matching, we introduce the Layout Char-
acterization Matching. Layout Characterization Matching di-
vides the activity screenshot into small areas according to
the widget contours, and then uses the Canny algorithm to
perform layout characterization. Then, we obtain the coor-
dinate position information of the widgets on the recorded
activity page of the recording device. Meanwhile, the same
Layout Characterization Matchingwith the Canny algorithm is
performed on the activity screenshots of the replaying devices,
and the target widget is positioned according to the extracted
3-tuple coordinate information. The main process can be seen
in Fig. 4. Since most apps have a similar layout for different
versions on multiple platforms, and the test script model we
propose is platform-independent, the cross-platform replay can
be successfully realized.
We also refer to some other research work, like REMAUI
[15], which is designed to generate code based on UI images.
REMAUI uses Canny and OCR to characterize the image
layout and combines the two algorithms to generate the page
layout data structure. LIRAT encapsulates and improves the
layout characterization approach REMAUI uses.
The Canny algorithm is elaborate on extracting edges, but
if the detection for the contours is performed without extra
processing, many tiny and redundant edge contours will be
produced, which often have no signiÔ¨Åcance in Layout Char-
acterization Matching. Instead, they can negatively affect the
processing of contour data and layout characterization. There-
fore, we expand the widget edges and connect the redundant
contours to retain large, meaningful widgets, texts, etc. Con-
tour detection is implemented by the ‚Äú findContours() ‚Äù
function in the Canny algorithm, and the complete layout
hierarchy is stored in the form of four vertex coordinates
of the rectangular contour. Finally, we calculate the size of
the extracted widgets, the ones which are smaller than 1% of
the activity size are discarded. We also introduce the OCR
technology into LIRAT to assist the matching.TABLE I
EXPERIMENT MOBILE APPLICATION
Open-Sourced Apps Commercial Apps
App Name Category App Name Category
AdGuard System Keep Sports
Jamendo Music Booking Shopping
Kiwix Internet NBA App Sports
Linphone Phone & SMS Bing Search Tool
Matomo Mobile 2 Development Evernote Tool
Monkey Development McDonald Shopping
openHAB Internet investing Finance
OsmAnd Map Taobao Shopping
VLC Music QQ Music Music
WikiPedia Internet KFC Shopping
Kindle Tool
However, the obtained widget set still has a lot of redun-
dancy. Through the multiple empirical trials on different app
activities, we conclude the following 2 rules to basically Ô¨Ålter
out redundancy, and to improve the effect.
Clear the contour with the length and width less than
the pre-deÔ¨Åned threshold in the contour. Practice from
the analysis on large amount real-world apps shows that
when the threshold is 2% of the current device screen
width, the contour element is not a functional widget.
Clear the inner contour of the contour. (This rule is
ignored when the contour is longer or wider than 60%
of the width of the corresponding device). When widgets
occupy a small part of the device screens, the function
of the inner widget is generally equivalent to the outer
widget. Therefore, under such circumstances, such inner
contour is meaningless.
After the layout characterization of the activity, each widget
is assigned with a 3-tuple (g;l;c )to represent its position.
Results both from Widget Feature Matching and Layout
Characterization Matching are considered for Ô¨Ånal widget
positioning. and we use a parameter to calculate the Ô¨Ånal
result, which can be seen in Line 19 of Algorithm 2.
V. E MPIRICAL EVALUATION
Based on LIRAT, we conduct an empirical experiment to
evaluate the effectiveness of the proposed image-driven mobile
app testing framework. In this experiment, we investigate to
answer the following research questions:
RQ1 : How effective can LIRAT replay test scripts?
RQ2 : How much can LIRAT outperform the state-of-the-
art image-based record and replay tools?
RQ3 : Why does LIRAT fail to replay in some cases?
A. Experiment Setup
To evaluate the effectiveness of the proposed image-driven
mobile app testing framework and the tool LIRAT, we deÔ¨Åne
a matrix Replay Accuracy to evaluate the effectiveness.
Replay Accuracy : The percentage of the successful replays
account for total replays.
During the selection of the experimental subjects, we
consider the compatibility on multiple platforms. With this
1567TABLE II
ANDROID DEVICE IN THE EXPERIMENT
Device ID Serial No. Usage Brand Market Name Model SDKAndroid
VersionResolution
D0 WBUBB18923510113 Record Huawei Honor Honor 8X Max ARE-AL10 27 8.1.0 10802244
D1 2003161a Replay Oppo Oppo PBET00 27 8.1.0 10802340
D2 63fa9ed5 Replay Xiaomi Xiaomi MI 8 28 9 10802248
D3 7SWK89SO4HY5NJT8 Replay Vivo Vivo V1901A 28 9 7201544
D4 CLB0218724002507 Replay Huawei Huawei P20 EML-AL00 28 9 10802244
D5 ce0717179034e124027e Replay Samsung Samsung SM-N9508 28 9 10802220
TABLE III
IOS D EVICE IN THE EXPERIMENT
Device ID UDID Usage Market Name Model OS Version Resolution UIKit Size
D6a81e386cf822ce0edeba741d
64b04a8ca7d272e4Replay iPhone 6 MG482LL/A iOS 12.4.4 7501334 375667
D7ba149a8863cee87c7dec7ec2
c6e4620e3f0568beReplay iPhone 7 MNGX3CH/A iOS 12.0(16A366) 7501334 375667
concern, we totally select 21 mobile apps, which can be
referred to in TABLE I. The selected apps include commercial
ones and open-sourced ones, and cover 10 different categories,
including system ,music ,internet ,phone & SMS ,development ,
Ô¨Ånance ,tool,sports ,shopping , and map, which can show the
generalization capability. The selection is according to the
ranking of Google Play and apps are Ô¨Åltered by the criteria of
multiple platform supporting.
We also select multiple experimental devices for this ex-
periment, including Android platform and iOS platform. The
device list can be referred to in TABLE II and TABLE III.
Our experiment covers most mainstream mobile brands and
models, including Samsung, Huawei, Apple, Oppo, Vivo and
Xiaomi. And the devices are of different operating system
versions and different screen resolutions.
B. RQ1: Script Replay
Success
65.85%
Failure34.15%Android
Success
35.26%
Failure64.74%iOS
Fig. 5. Replay Accuracy
To answer RQ1, we recruit 4 senior software engineering
majored students to organize 105 test scenarios on the ex-
perimental mobile apps, and record the test scenarios on an
Android device labeled as D0. We require each test scenario
includes 15 test operations, and each operation contains an app
widget. Then, we simultaneously replay the recorded scripts
on 5 Android devices and 2 iOS devices. As is shown in Fig. 5,
the average replay accuracy of Android and iOS devices arearound 65.85% and 35.26%3respectively. The results show
that the mobile app testing framework is promising.
The replay accuracy of Android device is around
65.85%, and the replay accuracy of iOS device is
around 35.26%. Results show that the framework and
LIRAT to be promising.
C. RQ2: Baseline Comparison
We further research on the comparison between LIRAT and
the state-of-the-art approaches. The Ô¨Ånal results are denoted
as ‚ÄúÔ¨Ånal results‚Äù, results from Widget Feature Matching are
denoted as ‚Äúimage results‚Äù and results from Layout Charac-
terization Matching are denoted as ‚Äúlayout results‚Äù. Images
results can be considered as the results of the state-of-the-art
approaches because we obtain them with the same algorithms,
and such tools are not available. Fig. 6(a) and Fig. 6(c) show
the accuracy comparison among the Ô¨Ånal results, image results
and layout results. Fig. 6(b) and Fig. 6(d) show the analysis
of the inÔ¨Çuence among the Ô¨Ånal results, image results and
layout results. There are 7 bars in subÔ¨Ågure (b) and (d).
Labels containing ‚Äú I‚Äù mean the image results are right; Labels
containing ‚Äú L‚Äùmean the layout results are right and labels
containing ‚Äú F‚Äù mean the Ô¨Ånal results are right4.
In the subÔ¨Ågure (a) and (c), Widget Feature Matching bar
represents the results from the same algorithms adopted by
state-of-the-art image-based record and replay approaches,
such as Sikuli and Airtest. We can Ô¨Ånd that our framework has
an improvement with Layout Characterization Matching over
the state-of-the-art approaches by 8.74% and 35% on An-
droid and iOS platform respectively. The improvements are
especially obvious on cross-platform test script replay (from
Android to iOS).
From the subÔ¨Ågure (b) and (d), we can Ô¨Ånd that with
the combination of Widget Feature Matching and Layout
3Kiwix and Jamendo are unusable on iOS devices due to the apps
themselves.
4We omit the situation when Ô¨Ånal results and results from 2 algorithms are
all wrong
1568Fig. 6. Approach Comparison & Algorithm InÔ¨Çuence (Labels containing ‚Äú I‚Äù mean the Widget Feature Matching results are right; Labels containing
‚ÄúL‚Äùmean the Layout Characterization Matching results are right and labels containing ‚Äú F‚Äù mean the Ô¨Ånal results are right)
Characterization Matching, the replay accuracy is much higher
than the two respective results. Especially for iOS, the increase
is especially apparent. Moreover, compared with Widget Fea-
ture Matching, the Layout Characterization Matching also
achieves a higher replay accuracy.
Among the results where the Ô¨Ånal results are right, 51.75%
and 26.47% of successful replays (on Android and iOS) are
due to the success from both algorithms; 19.94% and 24.78%
of successful replays are due to the success from Widget
Feature Matching; 25.84% and 47.46% of successful replays
are due to the success from Layout Characterization Matching.
However, even if 2 algorithms fail, there are 2.47% and 1.29%
of Ô¨Ånal results to be successful. Among the data, we can
Ô¨Ånd that Layout Characterization Matching can lead to much
more Ô¨Ånal success than Widget Feature Matching, which is
29.59% on Android and 91.57% on iOS. This phenomenon
also proves that the introduction of Layout Characterization
Matching to compromise the drawbacks of Widget Feature
Matching alone achieves success, which is especially apparent
in cross-platform replay.
The introduction of Layout Characterization Match-
ing greatly improves the replay accuracy compared
with the same algorithms of the state-of-the-art image-
based record and replay approaches. The improve-
ments on Android and iOS platforms reach 8.74% and
35%. The improvement is especially apparent in cross-
platform replay. Moreover, Layout Characterization
Matching shows a more positive inÔ¨Çuence on the Ô¨Ånal
replay accuracy.
D. RQ3: Failure Analysis
We review and analyze the failure cases one by one, and
summarize some failure reasons.
In the Widget Feature Matching, failures due to repeated
highly similar widgets account for almost one-third of all
failure cases, which is the most important reason for failure;
secondly, the parsing failures in the recording phase result
in the wrong screenshot of the target widgets. Such failures
account for approximately 20% of all failure cases. Some
minor reasons have also led to individual failure cases, such
as the missing of corresponding widgets on the replay device,too few feature points extracted by the algorithm, making the
algorithm output result set empty.
In the Layout Characterization Matching, 63% failure cases
are caused by layout characterization errors. Subtle layout
changes caused by changes in activity contents caused ap-
proximately 16.3% of failures. In addition, about 12.1% of
the failures are due to changes in the device status bar.
Therefore, there is much space for improvement in layout
characterization, and our approach will perform much better
if the layout characterization algorithms has improvement.
Several factors lead to replay failures, including repeat
of highly-similar widgets, wrong widget screenshots,
missing widgets on the replaying device, layout charac-
terization error, layout changes led by activity changes
or differences on status bar.
E. Threats to Validity
The devices we use are limited , we totally use 6 An-
droid devices and 2 iOS device to complete the experiments.
However, the mobile brand, model, and Android version have
thousands of different types, thus the limitation of the device
cluster can lead to a threat. However, the mobile devices we
select are all the most popular ones on the current market,
which accounts for a large percentage of the mainstream
mobile device market.
The representativeness of apps in our experiment is
also a potential threat . We select the popular apps that
support both Android and iOS platforms. Even if we consider
the diversity of the app category, we cannot cover all the
categories. Also, some widely used apps that support only one
single platform are also not considered. However, we think our
proposed image-driven mobile app testing framework focuses
on the UI of the mobile app, therefore, the different kinds
of apps will not affect much. One important thing we have
to claim that game apps that have no obvious layout are not
applicable to the proposed framework.
We recruit senior software engineering majored students
to design the test scenarios and record test scripts in the
experiment . This may be a threat. However, Salman et al.
propose that senior students are sufÔ¨Åcient developer proxies
in well controlled experiments [16].
1569VI. R ELATED WORK
A. Code-Based Mobile Test Record and Replay
Traditional Android test script record tools such as Instru-
mentation [17], Robotium [18], UIAutomator [19], Espresso
[20] are some of the mainstream automated testing framework
for Android platform. They encapsulate most operations and
are easy to use. In iOS platform, KIF [21] and UIAutomation
[22] are the most widely used automated testing tools. How-
ever, the above tools have poor capabilities for cross-platform
replay [23], and the quality of test scripts depends largely on
developers‚Äô capabilities.
Monkeyrunner [24] is another automated testing tool on the
Android platform. Monkeyrunner can replay test scripts on
different devices simultaneously, greatly improving the test
efÔ¨Åciency. However, users have to get familiar with the shell
programming or python programming to write the test scripts,
which is a demanding requirement and is unfriendly to users.
Appium [25] encapsulates different frameworks to support
different platforms. Appium does not compile or adjust the app
under test and can complete the automated test non-intrusively
[26]. However, the test scripts for different platforms cannot be
generally used, so cross-platform replay is still hard to realize.
RERAN [27] is a very early tool supporting Android test
script record and replay. It captures events of low level with
ADB by reading logs in ‚Äú /dev/input/event *‚Äù Ô¨Åles.
Some similar tools like appetizer [28] and Mosaic [29] utilize
similar techniques. SARA, presented by Guo et al. [30] in
2019, further improves the Android application test script
replay accuracy. However, such tools still cannot support
cross-platform replay. Barista [31] proposed by Fazzini et al.
is an approach that can help generate platform independent
test cases while it start from the runtime state analysis.
The above tools are of high record and replay efÔ¨Åciency.
However, they heavily rely on the platform frameworks and
features. Even if some of them can support different plat-
forms, like Android and iOS, developers still need to develop
complete different test scripts for different platforms, and the
developers need to know the different knowledge well, which
is quite a high bar.
B. GUI-Based Mobile Test Record and Replay
Behrang and Orso [32] [33] propose AppTestMigrator,
which allows for migrating test cases between apps with
similar features using the similarity among GUI widgets.
Sikuli, a tool presented by Yeh et al. [6], allows developers
to use GUI element screenshots as a parameter. In the replay
phase, Sikuli applies image retrieval algorithms to match
widgets according to the screenshots in the scripts. Such ideas
make Sikuli able to cross devices [34]. However, the traditional
computer vision algorithms it applies signiÔ¨Åcantly limit the
usability when used for cross-platform replay. UI elements
become more dynamic and tend to have different scaling ratio,
which is the drawback of pixel comparison method in the
computer vision algorithm Sikuli uses.
Based on Sikuli, Airtest presented by NetEase is a cross-
platform UI automated testing framework based on imagerecognition technology and Poco widget recognition technol-
ogy. Airtest improves the image recognition technology and
adds a Poco widget recognition technology in order to position
the widgets by the XPath orIDvalues of the widgets. Airtest
has higher accuracy in the replay phase. However, Airtest
mainly focuses on the video games and has a relatively weaker
support for a wider range of mobile apps.
Moreover, some state-of-the-art approaches analyze the
video information to record and replay test scripts within
Android platform. Qian et al. [35] propose a tool leveraging
visual test scripts to express GUI actions and using a physical
robot to drive automated test execution. Bernal-Cardenas et al.
[36] introduce V2S to translate video recordings of Android
app usages into replayable scenarios.
The GUI based tools can alleviate the severity of the cross-
platform problem. However, the problem is still not well
solved. The depended image feature extraction and matching
algorithms will meet quite much obstacles under the circum-
stances that app contents are constantly refreshed, leading to
the constantly changing of image features.
C. Widget Recognition Technology
Chang et al. [37] present a new approach using computer
vision technology for developers to automate their GUI testing
tasks and execute all kinds of GUI behaviors. Nguyen et al.
[15] Ô¨Årstly introduce an approach, namely REMAUI, to use
input images to identify UI elements such as texts, images,
and containers, using computer vision and optical character
recognition (OCR) techniques.
Moreover, Moran et al. [38] implement a tool on the
basis of REMAUI, namely REDRAW, which can generate
codes from UI images using the deep learning technology.
Additionally, Qin et al. [39] present a tool, namely TestMig,
for migrating test scripts from iOS to Android platform using
widget hierarchy information and screenshots.
Chen et al. [40] also present a neural network machine
translator which combines recent advances in computer vision
and machine translation, and translates UI images into GUI
skeletons. Chen et al. [41] present an approach to automati-
cally add labels to UI components using deep learning models.
Lowe [11] present an object recognition system, SIFT
(scale-invariant feature transform), which uses a kind of news
local image features. These features are invariant to image
translation, scaling, and rotation, and partially invariant to
illumination changes, afÔ¨Åne or 3D projection.
Their work greatly enlightens us, and we absorb their ideas
into widget recognition and test script record and replay in the
image-driven mobile app testing framework.
VII. C ONCLUSION
Mobile apps often run on multiple platforms, so the limited
capability of test scripts to work on multiple platforms can
lead to repetitive developing work. The proposed image-
driven mobile app testing framework solves such problems
by the proposed platform-independent test script model and
the Widget Feature Matching and Layout Characterization
1570Matching algorithms, realizing the accurate positioning of the
target widgets on different platforms. Our approach greatly
simpliÔ¨Åes the scripting work and makes it possible of ‚Äúone
script record, multiple script replays‚Äù on multiple platforms.
The experiment we conduct shows that the proposed image-
driven mobile app testing framework achieves promising suc-
cess in replaying mobile app test scripts on different platforms,
and outperforms the state-of-the-art approaches much for
cross-platform replay.
ACKNOWLEDGEMENT
This work is supported partially by National Natural Sci-
ence Foundation of China (61932012, 61802171, 61772014),
Fundamental Research Funds for the Central Universities
(14380021), and National Undergraduate Training Program for
Innovation and Entrepreneurship (202010284073Z).
REFERENCES
[1] L. Wei, Y . Liu, and S.-C. Cheung, ‚ÄúTaming android fragmentation:
Characterizing and detecting compatibility issues for android apps,‚Äù
inProceedings of the 31st IEEE/ACM International Conference on
Automated Software Engineering , 2016, pp. 226‚Äì237.
[2] Z. Qin, Y . Tang, E. Novak, and Q. Li, ‚ÄúMobiplay: A remote execution
based record-and-replay tool for mobile applications,‚Äù in Proceedings
of the 38th International Conference on Software Engineering . ACM,
2016, pp. 571‚Äì582.
[3] I. Singh and B. Tarika, ‚ÄúComparative analysis of open source automated
software testing tools: Selenium, sikuli and watir,‚Äù International Journal
of Information & Computation Technology , vol. 4, no. 15, pp. 1507‚Äì
1518, 2014.
[4] C. Guo, T. He, W. Yuan, Y . Guo, and R. Hao, ‚ÄúCrowdsourced re-
quirements generation for automatic testing via knowledge graph,‚Äù in
Proceedings of the 29th ACM SIGSOFT International Symposium on
Software Testing and Analysis , 2020, pp. 545‚Äì548.
[5] ‚ÄúSikuli,‚Äù 2009. [Online]. Available: http://www.sikulix.com
[6] T. Yeh, T.-H. Chang, and R. C. Miller, ‚ÄúSikuli: using gui screenshots
for search and automation,‚Äù in Proceedings of the 22nd annual ACM
symposium on User interface software and technology . ACM, 2009,
pp. 183‚Äì192.
[7] ‚ÄúAirtest,‚Äù 2019. [Online]. Available: http://airtest.netease.com
[8] ‚ÄúGoogle play,‚Äù 2019. [Online]. Available: https://play.google.com/store/
apps/top
[9] ‚ÄúAdb,‚Äù 2020. [Online]. Available: https://developer.android.com/studio/
command-line/adb
[10] ‚ÄúWda,‚Äù 2020. [Online]. Available: https://github.com/openatx/
facebook-wda
[11] D. G. Lowe et al. , ‚ÄúObject recognition from local scale-invariant
features.‚Äù in iccv, vol. 99, no. 2, 1999, pp. 1150‚Äì1157.
[12] S. Yu, ‚ÄúCrowdsourced report generation via bug screenshot understand-
ing,‚Äù in 2019 34th IEEE/ACM International Conference on Automated
Software Engineering (ASE) . IEEE, 2019, pp. 1277‚Äì1279.
[13] M. Muja and D. G. Lowe, ‚ÄúFast approximate nearest neighbors with
automatic algorithm conÔ¨Åguration.‚Äù VISAPP (1) , vol. 2, no. 331-340,
p. 2, 2009.
[14] D. G. Lowe, ‚ÄúDistinctive image features from scale-invariant keypoints,‚Äù
International Journal of Computer Vision , 2004.
[15] T. A. Nguyen and C. Csallner, ‚ÄúReverse engineering mobile application
user interfaces with remaui (t),‚Äù in IEEE/ACM International Conference
on Automated Software Engineering , 2016.
[16] I. Salman, A. T. Misirli, and N. Juristo, ‚ÄúAre students representatives
of professionals in software engineering experiments?‚Äù in Proceedings
of the 37th IEEE International Conference on Software Engineering ,
vol. 1. IEEE, 2015, pp. 666‚Äì676.
[17] ‚ÄúInstrumentation,‚Äù 2019. [Online]. Available: https://developer.android.
com/reference/android/app/Instrumentation
[18] ‚ÄúRobotium,‚Äù 2009. [Online]. Available: https://github.com/
RobotiumTech/robotium
[19] ‚ÄúUi automator,‚Äù 2019. [Online]. Available: https://developer.android.
com/training/testing/ui-automator[20] ‚ÄúEspresso,‚Äù 2019. [Online]. Available: https://developer.android.com/
training/testing/espresso
[21] ‚ÄúKif,‚Äù 2011. [Online]. Available: https://github.com/kif-framework/KIF
[22] ‚ÄúUiautomation,‚Äù 2019. [Online]. Available: http://appium.io/docs/en/
drivers/ios-uiautomation/
[23] S. Roy Choudhary, ‚ÄúCross-platform testing and maintenance of web
and mobile applications,‚Äù in Companion Proceedings of the 36th Inter-
national Conference on Software Engineering .
[24] ‚ÄúMonkeyrunner,‚Äù 2015. [Online]. Available: https://developer.android.
com/studio/test/monkeyrunner/index.html
[25] (2015) Appium. [Online]. Available: http://appium.io
[26] G. Shah, P. Shah, and R. Muchhala, ‚ÄúSoftware testing automation using
appium,‚Äù International Journal of Current Engineering and Technology ,
vol. 4, no. 5, pp. 3528‚Äì3531, 2014.
[27] L. Gomez, I. Neamtiu, T. Azim, and T. Millstein, ‚ÄúReran: Timing-and
touch-sensitive record and replay for android,‚Äù in Proceedings of the
2013 International Conference on Software Engineering . IEEE Press,
2013, pp. 72‚Äì81.
[28] ‚ÄúAppetizer,‚Äù 2016. [Online]. Available: https://github.com/appetizerio/
replaykit
[29] M. Halpern, Y . Zhu, R. Peri, and V . J. Reddi, ‚ÄúMosaic: cross-platform
user-interaction record and replay for the fragmented android ecosys-
tem,‚Äù in 2015 IEEE International Symposium on Performance Analysis
of Systems and Software (ISPASS) . IEEE, 2015, pp. 215‚Äì224.
[30] J. Guo, S. Li, J.-G. Lou, Z. Yang, and T. Liu, ‚ÄúSara: self-replay aug-
mented record and replay for android in industrial cases,‚Äù in Proceedings
of the 28th ACM SIGSOFT International Symposium on Software Testing
and Analysis . ACM, 2019, pp. 90‚Äì100.
[31] M. Fazzini, E. N. D. A. Freitas, S. R. Choudhary, and A. Orso, ‚ÄúBarista:
A technique for recording, encoding, and running platform independent
android tests,‚Äù in 2017 IEEE International Conference on Software
Testing, VeriÔ¨Åcation and Validation (ICST) . IEEE, 2017, pp. 149‚Äì160.
[32] F. Behrang and A. Orso, ‚ÄúAutomated test migration for mobile apps,‚Äù
inProceedings of the 40th International Conference on Software Engi-
neering: Companion Proceeedings . ACM, 2018, pp. 384‚Äì385.
[33] ‚Äî‚Äî, ‚ÄúTest migration for efÔ¨Åcient large-scale assessment of mobile
app coding assignments,‚Äù in Proceedings of the 27th ACM SIGSOFT
International Symposium on Software Testing and Analysis . ACM,
2018, pp. 164‚Äì175.
[34] J.-l. Sun, S.-w. Zhang, S. Huang, and Z.-w. Hui, ‚ÄúDesign and application
of a sikuli based capture-replay tool,‚Äù in 2018 IEEE International
Conference on Software Quality, Reliability and Security Companion .
IEEE, 2018, pp. 42‚Äì44.
[35] J. Qian, Z. Shang, S. Yan, Y . Wang, and L. Chen, ‚ÄúRoscript: a visual
script driven truly non-intrusive robotic testing system for touch screen
applications,‚Äù in 2020 IEEE/ACM 42st International Conference on
Software Engineering (ICSE) . IEEE, 2020.
[36] C. Bernal-C ¬¥ardenas, N. Cooper, K. Moran, O. Chaparro, A. Marcus, and
D. Poshyvanyk, ‚ÄúTranslating video recordings of mobile app usages into
replayable scenarios,‚Äù in 2020 IEEE/ACM 42st International Conference
on Software Engineering (ICSE) . IEEE, 2020.
[37] T.-H. Chang, T. Yeh, and R. C. Miller, ‚ÄúGui testing using
computer vision,‚Äù ser. CHI ‚Äô10, 2010. [Online]. Available: http:
//doi.acm.org/10.1145/1753326.1753555
[38] K. Moran, C. Bernal-C ¬¥ardenas, M. Curcio, R. Bonett, and D. Poshy-
vanyk, ‚ÄúMachine learning-based prototyping of graphical user interfaces
for mobile apps,‚Äù arXiv preprint arXiv:1802.02312 , 2018.
[39] X. Qin, H. Zhong, and X. Wang, ‚ÄúTestmig: migrating gui test cases from
ios to android,‚Äù in Proceedings of the 28th ACM SIGSOFT International
Symposium on Software Testing and Analysis . ACM, 2019, pp. 284‚Äì
295.
[40] C. Chen, T. Su, G. Meng, Z. Xing, and Y . Liu, ‚ÄúFrom ui design image
to gui skeleton: a neural machine translator to bootstrap mobile gui
implementation,‚Äù in Proceedings of the 40th International Conference
on Software Engineering . ACM, 2018, pp. 665‚Äì676.
[41] J. Chen, C. Chen, Z. Xing, X. Xu, L. Zhu, G. Li, and J. Wang, ‚ÄúUnblind
your apps: Predicting natural-language labels for mobile gui components
by deep learning,‚Äù in 2020 IEEE/ACM 42st International Conference on
Software Engineering (ICSE) . IEEE, 2020.
1571