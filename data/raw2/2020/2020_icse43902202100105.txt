Bounded Exhaustive Search
of Alloy SpeciÔ¨Åcation Repairs
Sim¬¥on Guti ¬¥errez Briday, Germ ¬¥an Regis, Guolong Zhengz,
Hamid Bagheriz, ThanhVu Nguyenz, Nazareno Aguirrey, Marcelo Friasyx
Department of Computer Science, FCEFQyN, University of R ¬¥ƒ±o Cuarto, Argentina
yNational Council for ScientiÔ¨Åc and Technical Research (CONICET), Argentina
zDepartment of Computer Science & Engineering, University of Nebraska-Lincoln, USA
xDepartment of Software Engineering, Buenos Aires Institute of Technology, Argentina
Abstract ‚ÄîThe rising popularity of declarative languages and
the hard to debug nature thereof have motivated the need
for applicable, automated repair techniques for such languages.
However, despite signiÔ¨Åcant advances in the program repair of
imperative languages, there is a dearth of repair techniques for
declarative languages. This paper presents BeAFix, an automated
repair technique for faulty models written in Alloy, a declarative
language based on Ô¨Årst-order relational logic. BeAFix is backed
with a novel strategy for bounded exhaustive, yet scalable, ex-
ploration of the spaces of Ô¨Åx candidates and a formally rigorous,
sound pruning of such spaces. Moreover, different from the state-
of-the-art in Alloy automated repair, that relies on the availability
of unit tests, BeAFix does not require tests and can work with
assertions that are naturally used in formal declarative languages.
Our experience with using BeAFix to repair thousands of real-
world faulty models, collected by other researchers, corroborates
its ability to effectively generate correct repairs and outperform
the state-of-the-art.
I. I NTRODUCTION
Software has become ubiquitous, and many of our activities
depend directly or indirectly on it. Having adequate software
development techniques and methodologies that contribute
to producing quality software systems has therefore become
essential for many human activities. A well-established ap-
proach to achieving quality is to emphasize good problem
understanding and planning ahead of development, i.e., to put
an emphasis on the analysis and design phases of software
development [1]. These phases need to deal with descriptions
of software and problem domains, which are typically cap-
tured using speciÔ¨Åcation, or modeling, languages. Techniques
and tools that allow users to analyze speciÔ¨Åcations are very
important, as they help developers in discovering Ô¨Çaws, such
as missing cases in the speciÔ¨Åcations, wrong interpretations
of requirements, etc. Two main problems arise in this phase:
correctly understanding the problem situation (thus capturing
the right problem), and correctly stating the problem in the
language at hand (thus capturing the problem right). In the
context of formal speciÔ¨Åcation, where formalisms with for-
mal syntax and semantics are employed, the latter problem
is particularly relevant, as the developer has to master the
notation to correctly capture, in a formal way, a given software
description [2]. Even for experienced developers, many times
subtle errors arise, like mistakenly using the wrong expression
to capture a property, omitting an operator or using an operatorin place of another, leading to incorrect speciÔ¨Åcations that
do not capture the developer‚Äôs intentions [3]. These kinds of
mistakes share characteristics with program defects. Therefore,
techniques for dealing with these defects and, in general, to
assess or improve software quality (such as techniques for
bug Ô¨Ånding and program debugging), are also relevant in the
context of software speciÔ¨Åcations. In particular, techniques
for improving debugging, e.g., via the automation of fault
localization or program repair, are pertinent in the context of
software speciÔ¨Åcation.
This paper targets the problem of automatically repairing
formal speciÔ¨Åcations, more precisely, speciÔ¨Åcations in Alloy
[4], a formal language that has many applications in software
development and has been successfully applied in a number of
domains such as the discovery of design Ô¨Çaws in telecommu-
nication applications [5], the analysis of security mechanisms
in mobile and IoT platforms [6], [7], [8], the automation
of software testing [9], [10], [11], and the veriÔ¨Åcation of
programs [12], [13], [14], among other applications [15].
While speciÔ¨Åcations share a number of characteristics with
programs, certain characteristics make it non-trivial to apply
the broad range of techniques for program repair, in the
context of speciÔ¨Åcations. For instance, as a way to tame the
space of candidates, various program repair techniques such as
GenProg [16] only use coarse-grained syntactic modiÔ¨Åcations,
such as block replacement, swapping, deletion and insertion,
but no intra-statement modiÔ¨Åcations are allowed. The rationale
is that good levels of repairability in programs are achieved via
coarse-grained modiÔ¨Åcations thanks to redundancies that are
present in code, especially in larger programs. Such redundan-
cies are not often seen in speciÔ¨Åcations, in particular due to the
relative conciseness of speciÔ¨Åcations compared to programs.
Other approaches to program repair, e.g., PAR [17], restrict the
modiÔ¨Åcations to patterns learned from human-written patches,
mined from large repositories categorizing Ô¨Åxes; such inputs
for the repair process are not available in the context of
formal speciÔ¨Åcation, simply because, as opposed to source
code, there are no large repositories of speciÔ¨Åcations. Finally,
most program repair techniques rely directly or indirectly on
the availability of test cases; while there exist initiatives that
incorporate test cases to speciÔ¨Åcations [18], other forms of
checking, such as property satisÔ¨Åability and veriÔ¨Åcation, are
11352021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)
1558-1225/21/$31.00 ¬©2021 IEEE
DOI 10.1109/ICSE43902.2021.00105
more naturally found in speciÔ¨Åcations.
In this paper, we present BeAFix, a novel technique that
automatically repairs faulty Alloy speciÔ¨Åcations. BeAFix has
several features distinguishing it from the state of the art
[19]. Firstly, the technique does not depend on test cases,
neither for fault localization nor for speciÔ¨Åcation repair; it
supports any kind of speciÔ¨Åcation oracle, notably the typical
assertion checks and property satisÔ¨Åability checks found in
Alloy speciÔ¨Åcations, as well as test cases. It is then more
widely applicable in the context of formal speciÔ¨Åcation, where
test cases are rarely found accompanying speciÔ¨Åcations. Sec-
ondly, the technique tackles automated repair in a bounded
exhaustive way, i.e., by exhaustively exploring allpossible
repair candidates, for a given set of mutation operators and
maximum number of applications (on a set of identiÔ¨Åed
suspicious speciÔ¨Åcation locations). Thus, it either Ô¨Ånds a Ô¨Åx,
or guarantees that no Ô¨Åx is possible, within the provided bound
and with the considered mutation operators over the identiÔ¨Åed
faulty locations. This approach is natural to the context of
Alloy, where users are accustomed to bounded exhaustive
analyses.
BeAFix supports Ô¨Åne-grained mutations and is designed
to enable the repair of multi-location speciÔ¨Åcation defects.
Since bounded exhaustive exploration suffers from inherent
scalability issues, our technique features a number of prun-
ingstrategies, that leverage the use of the Alloy Analyzer
tosoundly prune large parts of the candidate space. More
precisely, given a candidate repair for a speciÔ¨Åc suspicious
location, our technique exploits both a syntactic analysis of the
speciÔ¨Åcation and a semantic analysis using the Alloy Analyzer
for checking the feasibility of this candidate, in the sense that
applying this speciÔ¨Åc repair candidate to the corresponding
location preserves the feasibility of the overall (multi-location)
repair. When feasibility fails, it allows us to prune, in a sound
way, i.e., without losing valid Ô¨Åxes, signiÔ¨Åcant parts of the
search space for repair candidates, thus reducing speciÔ¨Åcation
repair running times.
We evaluate our technique on a benchmark of Alloy speci-
Ô¨Åcations, including speciÔ¨Åcations previously used in assessing
ARepair [19], [20], and a large benchmark of faulty Alloy
speciÔ¨Åcations produced by students [21]. Our evaluation shows
that our pruning technique signiÔ¨Åcantly reduces speciÔ¨Åcation
repair running times, duplicating the number of repairs that
can be produced within a 1-hour timeout, and reducing the
repair time by 62X, on average. Moreover, when speciÔ¨Åcations
feature typical assertions, and these are used as oracles,
our technique shows a signiÔ¨Åcant improvement in overÔ¨Åtting
reduction, compared to the test-based technique ARepair.
II. A NILLUSTRATING EXAMPLE
In this section, we introduce both Alloy and our tech-
nique by means of a motivating example. Alloy is a formal
speciÔ¨Åcation language, with a simple syntax and a relational
semantics. The syntax of the language is rather small, and
is compatible with an intuitive reading of speciÔ¨Åcations, or
models , as they are typically called in the context of Alloy [4](we will use speciÔ¨Åcation andmodel interchangeably in this
paper). SpeciÔ¨Åcations can resemble object-oriented notions
that are familiar to developers. The basic syntactic elements
of Alloy speciÔ¨Åcations are: signatures , which declare data
domains; signature Ô¨Åelds (akin to class attributes), that give
structure to speciÔ¨Åcations and declare relations between sig-
natures; predicates , parameterized formulas that can be used
to state properties, represent operations, etc.; facts , formulas
that constrain the speciÔ¨Åcations and represent assumptions;
and assertions , formulas that capture intended properties of
the speciÔ¨Åcation, i.e., properties that the user would like to
verify. Formulas in Alloy are expressed in relational logic ,
a Ô¨Årst-order logic extended with relational operators such as
relational transpose, union, difference and intersection. Alloy
supports various quantiÔ¨Åers ( all andsome are the usual
universal and existential quantiÔ¨Åers, respectively, one and
lone are for ‚Äúexists exactly one‚Äù and ‚Äúexists at most one‚Äù,
respectively). It also features additional important relational
operators: relational join , a generalization of composition to
n-ary relations, which can be used to express navigations as in
object orientation; and transitive closure , which can be applied
only to binary relations, and extends the expressiveness of
Alloy beyond that of Ô¨Årst-order logic.
Consider the Alloy model in Figure 1, a modiÔ¨Åed version of
an Alloy speciÔ¨Åcation of linked lists, that is part of the bench-
mark used in [19]. This model declares domains for booleans
(with its two constants captured via singleton relations), and
signatures for nodes and lists. Nodes have a link (a set of
nodes), and associated elements (a set of integers); lists have
a header (a set of nodes). A factconstrains the cardinalities of
these signature Ô¨Åelds: lists have at most one header, and nodes
have at most one successor node, and exactly one element
(when applied to expressions, lone ,one andnoconstrain a
given expression to have a cardinality of at most one, exactly
one, and exactly zero, respectively). Notice the additional fact,
which is there for analysis purposes: it states that exactly one
List is going to be considered in each instance of the model,
and that all nodes present in an instance will be those in the
list (no unreachable ‚Äúheap‚Äù objects). Predicate Loop captures
lists with a loop in its last node, saying that a list satisÔ¨Åes
the predicate if it either has no header, or for exactly one of
its nodes, the elements reachable in one or more steps from
link are exactly the same reachable in zero or more steps
through link . Predicate Sorted attempts to capture that lists
are non-decreasingly sorted (this predicate is buggy though,
as the order constraint is strict). Predicate RepOk is simply
deÔ¨Åned as the conjunction of Loop andSorted . Predicate
Contains is used to model an operation on lists, namely, the
operation for querying membership of an integer as an element
of a node of a list. The result of the operation is captured by
an additional Boolean parameter. This predicate is buggy , it
does not correctly model the intended operation (e.g., it admits
the predicate to return True despite the contents of the list).
Alloy speciÔ¨Åcations can be automatically analyzed, by an
analysis mechanism that resorts to SAT solving, and is imple-
mented in a tool called Alloy Analyzer [4]. Two kinds of analy-
1136abstract sig Boolean { }
one sig True, False extends Boolean { }
sig Node {
link: set Node,
elem: set Int
}
sig List {
header: set Node
}
fact CardinalityConstraints {
all l : List | lone l.header
all n : Node | lone n.link
all n : Node | one n.elem
}
fact IGNORE {
one List && List.header. *link = Node
}
pred Loop[This: List] {
noThis.header ||
one n : This.header. *link | n.ÀÜlink = n. *link
}
pred Sorted[This: List] { // buggy
all n: This.header. *link | n.elem < n.link.elem
}
pred RepOk[This: List] {
Loop[This] && Sorted[This]
}
run RepOk for 1 but exactly 3 Node expect 1
// buggy
pred Contains[This: List, x: Int, res: Boolean]{
RepOk[This] &&
((x ! inThis.header. *link.elem => res=False ) ||
res = True)
}
pred Count[This: List, x: Int, res: Int] {
RepOk[This] &&
res = #{ n:This.header. *link | n.elem = x }
}
assert ContainsCorrect {
all l : List, i, j : Int |
(Count[l, i, j] && j > 0) iff Contains[l, i, True]
}
check ContainsCorrect for 10
Fig. 1. A (faulty) sample Alloy speciÔ¨Åcation.
sis are possible: running a predicate and checking an assertion.
Both are analyzed in bounded scenarios. Running a predicate
searches for instances (scenarios) that satisfy all the constraints
(cardinalities, facts, etc.), including the predicate being run.
Assertion checking looks for counterexamples of the asserted
properties. Analysis is performed up to a bound k(typically
referred to as the scope of the analysis), meaning, e.g., that
assertion checking will either Ô¨Ånd a counterexample within the
given scope, or guarantee the validity of the formula within
the bound (similarly, a predicate will be found to be satisÔ¨Åablewithin the provided scope, or not to have a satisfying instance
within the scope). This bounded exhaustive analysis , of course,
does not necessarily mean that the formula is valid (resp.,
satisÔ¨Åable), as counterexamples (resp., instances) of greater
size may exist if larger scopes are considered.
The Alloy language is the vehicle for deÔ¨Åning abstract
software models in a lightweight and incremental way, with
immediate feedback via automated analysis [4]. Typically, the
process of constructing an Alloy model, as the one in our
example, starts very much in the same way one would proceed
while eliciting requirements, or sketching an abstract software
design: basic domains of the model are identiÔ¨Åed (signatures
of the model), over which more structured components are or-
ganized (signatures equipped with Ô¨Åelds). How these domains
and components are constituted, the inherent constraints of the
problem domain and the operations that represent the software
model capacities, are all incrementally created, via a constant
interaction with the Alloy Analyzer. This process eventually
involves the use of assertions and predicates , that capture
intended properties of the model, and that serve essentially
as the oracle of the speciÔ¨Åcation, i.e., the properties that
would convey the acceptance of the model. Sometimes these
properties can help Ô¨Ånd surprising counterexamples, that lead
to reÔ¨Ånements of the properties themselves, but more often
they help one in ‚Äúdebugging‚Äù the core of the model, i.e.,
in getting the model ‚Äúright‚Äù, adapting it until the intended
properties result as expected. For instance, for the linked lists
model, the developer would expect the representation invariant
RepOk to be satisÔ¨Åable, and the deÔ¨Ånition of Contains
to have the relationship with Count captured in property
ContainsCorrect .
While the intended properties are subject to defects too, they
are typically signiÔ¨Åcantly shorter and clearer than the ‚Äúcore‚Äù
of the speciÔ¨Åcation. They capture high level properties of the
model, so they are expected to be simpler to write and get
right. So, once the intended properties are set, the user may
perform the corresponding analyses and use the results as an
acceptance criterion for the speciÔ¨Åcation, and the correspond-
ing design it conveys. That is, a model will be considered
incorrect if any of the analyses of the intended properties
fails, i.e., has a result that contradicts the user expectations. In
Figure 1, for instance, the user may consider the consistency of
RepOk , the assertion ContainsCorrect and the auxiliary
predicate Count as the oracle of the speciÔ¨Åcation, meaning
that when this intended property is found to be invalid, the
user would start modifying the remainder of the speciÔ¨Åcation,
as an attempt to Ô¨Åx the error. BeAFix as well as other model
repair techniques aim at reducing human intervention along
this overall modeling process, by automatically Ô¨Åxing errors
in incorrect models.
Let us describe how the technique works, assuming for
the moment that the faulty locations in the model have been
correctly identiÔ¨Åed. In order to attempt to repair the speciÔ¨Å-
cation, and assuming that for the Ô¨Årst location the syntactic
mutation operators lead to ndifferent Ô¨Åx candidates (for that
speciÔ¨Åc location), and for the second location we have m
1137different Ô¨Åx candidates, in the worst case we have to check
nmpotential Ô¨Åxes, as we would want to consider all
combinations of candidate Ô¨Åxes for each repair location. The
model expectations, in our example the satisÔ¨Åability of RepOk
and the bounded validity of ContainsCorrect , will be the
acceptance criterion Ô¨Åx repair, i.e., if a Ô¨Åx candidate ‚Äúpasses‚Äù
these analyses, it will be considered a Ô¨Åx.
The automated repair process for the above faulty speci-
Ô¨Åcation is then straightforward to describe: we have nm
repair candidates (the combinations of Ô¨Åx candidates for
the suspicious locations), and since we aim at exhaustively
exploring this candidate space, we would run the oracles on
each candidate, stopping as soon as we Ô¨Ånd one that ‚Äúpasses‚Äù
all predicates and assertions.
Let us describe some situations that allow for sound pruning,
i.e., pruning that only avoids invalid Ô¨Åx candidates.
Notice that, in our case, we have two defective lines,
but these are not symmetric : the bugs in Sorted affect
Contains , asContains depends on RepOk which in
turn depends on Sorted , but the latter does not depend
(i.e., calls directly or indirectly) on Contains . Thus, when
checking a speciÔ¨Åc candidate for Sorted that does not pass
an oracle involving Sorted but not Contains , as for
instance the satisÔ¨Åability of RepOK , we can stop analyzing
the Ô¨Åx candidate for Sorted altogether, and not consider
it in combination with any further candidates for the other
location. Consider, for instance, the following combination of
Ô¨Åx candidates for Sorted andContains :
pred Sorted[This: List] {
all n: This.header. *link | n.elem != n.link.elem
}
pred Contains[This: List, x: Int, res: Boolean] {
RepOk[This] &&
(x ! inThis.header. *link.elem => res = False) &&
res = True
}
Assuming that we consider the above described oracles for the
speciÔ¨Åcation, this combination does not pass the oracles, it is
an invalid Ô¨Åx candidate. Moreover, if we leave the current
Ô¨Åx candidate for Sorted and iterate over other candidates
forContains , the property check requiring RepOk to be
satisÔ¨Åable will continue to fail, as the unsatisÔ¨Åability of RepOk
cannot be solved by changing the deÔ¨Ånition of Contains .
Thus, if we are able to identify this situation (as we explain
later on, our technique does so), we can safely consider a
different mutation for Sorted , or equivalently, soundly skip
all combinations of the current mutation to Sorted with all
other mutations for Contains .
Now let us look at another situation, that will also allow us
to soundly prune parts of the Ô¨Åx candidate space, even in the
presence of bidirectional (or multi-directional) dependencies
between faulty locations. Consider the above Ô¨Åx candidate for
predicate Contains , that replaced ||by&&. This ‚Äúlocal‚Äù
candidate that fails to pass an oracle such as the assertion
onContains (in combination with a particular candidate
forSorted ) does not allow us to discard it altogether,as the failing cannot in principle be blamed on &&on its
own: it may be the case that this candidate ‚Äúworks‚Äù with a
different candidate for Sorted . So in order to check the
local feasibility of the candidate for Contains , we need
to consider it in combination with any other candidate for
Sorted , of course, trying to avoid checking allcandidates
for this predicate. Assuming that we identiÔ¨Åed the body of
the quantiÔ¨Åcation of Sorted as the problematic part in that
predicate (fault localization techniques for Alloy, in particular
the one we use in this paper, can identify Ô¨Åne grained faulty
locations, such as particular subexpressions), what we would
need to intuitively check is whether there exists a (boolean)
value for that location, that in combination with &&would
make the oracles pass:
pred Sorted[This: List] {
all n: This.header. *link | (??)
}
pred Contains[This: List, x: Int, res: Boolean] {
RepOk[This] &&
(x ! inThis.header. *link.elem => res = False) &&
res = True
}
That is, can we replace the double question mark above by a
value that would make oracles pass? If the answer is no, then
we can blame &&, and try another candidate for Contains ,
avoiding considering of &&with candidates for Sorted . If we
are able to correctly identify these situations, as our technique
does and we describe later on in this paper, we can again safely
prune a large number of candidates, namely all combinations
of&&with all the mutations for Sorted .
It is worth remarking that we do not assume any particular
format or characteristic, neither from the speciÔ¨Åcation itself,
nor from the oracle. This is in contrast with previous work
on repairing Alloy speciÔ¨Åcations [19], which requires repair
oracles to be provided as Alloy test cases . Alloy test cases
deÔ¨Åne scenario-based expectations , similar to what one would
capture with unit tests for source code. As an example,
consider the evaluation of Contains on a particular con-
crete structure, and its corresponding expected outcome (the
expected outcome represents a boolean, 1 for ‚ÄúsatisÔ¨Åable‚Äù and
0 for ‚ÄúunsatisÔ¨Åable‚Äù):
pred ContainsFalseOnListTest[This: List] {
some n0, n1: Node | {
This.header = n0 &&
n0.link = n1 && n0.elem = 0 &&
n1.link = n1 && n1.elem = 0 &&
Contains[This, 1, False]
}
}
run ContainsFalseOnListTest expect 1
While scenarios do participate in the Alloy modeling process,
they typically do so as a result of analyzing properties . That
is, tests are not a common explicitly described part of Alloy
speciÔ¨Åcations. Recent proposals, notably [18], are starting
to motivate the use of test cases in formal speciÔ¨Åcation.
As mentioned, our approach allows for any kind of oracle,
including test-based oracles.
1138III. T HETECHNIQUE
Our approach to Alloy speciÔ¨Åcation repair involves a series
of tasks, for fault detection, fault localization, Ô¨Åx candidate
generation, and Ô¨Åx candidate assessment. We describe these
in more detail below.
A. Fault Detection and Fix Acceptance Criterion
In general, given an Alloy speciÔ¨Åcation, we may say that
such speciÔ¨Åcation is faulty if at least one of the analysis
commands in the speciÔ¨Åcation has an outcome contrary to
its corresponding expectation. This can be either a failing
assertion (assertion with counterexamples), or a predicate that
is unsatisÔ¨Åable while the user expected it to be satisÔ¨Åable,
or vice versa. We may also allow for other Ô¨Çavors in com-
mands, in particular Alloy test cases, in the spirit of AUnit
[18]. The fault detection stage then resorts to SAT solving,
the underlying analysis mechanism behind Alloy Analyzer,
the tool for Alloy speciÔ¨Åcation analysis [4]. Similarly, a Ô¨Åx
candidate can be considered an acceptable patch when all the
analysis commands in the speciÔ¨Åcation have an outcome that
coincides with the corresponding command‚Äôs expectations.
Our technique requires the user to identify the speciÔ¨Åcation
oracle, i.e., the assertions, predicates or tests that the technique
will have to consider as Ô¨Åx acceptance criterion. The technique
will then identify faults in the remainder of the speciÔ¨Åcation
(the oracle is left out of the analysis space for fault local-
ization), and generate Ô¨Åx candidates for the faulty locations.
Therefore, our repair approach cannot Ô¨Åx any faulty situation,
but only those where the developer is certain about some part
of it (the oracle), and wishes to alter the remainder of the
speciÔ¨Åcation to pass it. Looking for solutions that may modify
the speciÔ¨Åcation and the criterion for acceptance would lead
to Ô¨Åxes that may simply relax the acceptance criterion. Notice
that, in this respect, we follow the same approach that ARepair
and most test-based program repair techniques: the tests (the
repair oracle) cannot be changed in the repair process. As
described later on in this section, other trivial solutions such
as changing a command‚Äôs expectations or simply removing
a command are prevented, due to how the fault localization
is performed (which cannot be blamed on commands) and
how Ô¨Åx candidates are generated (only by mutating the faulty
locations).
B. Fault Localization
Once a speciÔ¨Åcation is deemed faulty, we need to identify
the speciÔ¨Åc parts of the speciÔ¨Åcation that are more likely
to be blamed for the fault or faults. We do not deal with
fault localization in this paper, and we assume an external
technique/tool provides fault localization information. There
exist techniques for fault localization that speciÔ¨Åcally tar-
get Alloy speciÔ¨Åcations, such as the spectrum-based fault
localization mechanism behind ARepair [19], and our fault
localization technique presented in [22]. While in principle
any fault localization technique would Ô¨Åt our technique, as
long as the employed fault localization can handle the oracles
present in the faulty speciÔ¨Åcation, it is worth to remark that thefault localization within ARepair inherently depends on having
tests as oracles (acceptance criteria) for speciÔ¨Åcations [19].
Moreover, the fault localization in ARepair can dynamically
change the identiÔ¨Åed faulty locations, as the speciÔ¨Åcation is
transformed during the repair process. Our technique, on the
other hand, uses an ofÔ¨Çine process for fault localization: the
faulty program is fed to the fault localization tool, and a
number of suspicious speciÔ¨Åcation locations are returned. This
is the input to our speciÔ¨Åcation repair approach, and the space
ofallpossible patches for these locations, for a maximum
depth in mutation application and a given set of mutation
operators, will be considered.
For our experiments in Section IV , we use the FLACK
fault localization technique [22]. While we do not describe
in detail the fault localization technique in this paper (we
refer the reader to [22]), let us remark a number of facts
about FLACK: it supports arbitrary satisÔ¨Åability checks and
assertions, as well as tests, as speciÔ¨Åcation oracles; it is based
on the use of (partial) maximum satisÔ¨Åability procedures, to
process counterexamples of an Alloy model (witnessing the
faulty status of the speciÔ¨Åcation); and it can only identify
faults within formulas and relational expressions, it cannot
locate faults in data deÔ¨Ånitions, such as signature and Ô¨Åeld
declarations, nor in commands (Alloy‚Äôs runs and checks).
C. Generation of Fix Candidates
Once the suspicious expressions are identiÔ¨Åed, syntactical
variants of these expressions are produced. We consider an
ample set of mutation operations, including the obvious logical
and relational operator insertion, removal and replacement,
quantiÔ¨Åcation mutation (e.g., changing a quantiÔ¨Åer), multiplic-
ity constraint replacement, Ô¨Åeld/variable swap/replacement,
etc., based on Alloy‚Äôs grammar. Our tool processes the speci-
Ô¨Åcation to obtain some typing information, so that some legal
expressions that necessarily lead to empty relation/contradic-
tory formulas are disregarded, as well as innocuous operation
application (e.g., double transitive closure). Two elements
are important to highlight here, namely the use of join to
produce navigation chains, using Ô¨Åelds, signatures, etc., and
the possibility of combining mutations, i.e., applying further
mutations to an already mutated expression, akin the so called
higher-order mutants [23] in mutation testing.
Both the mutation operators and the maximum depth, i.e.,
the number of cumulative mutations (hence, the higher order
nature of the generated mutants) that can be applied to a
given faulty location, are conÔ¨Ågurable. These are bounded-
exhaustively generated as the space of Ô¨Åx candidates is tra-
versed (see below). In our experiments, we used 21 mutation
operators in total, typically leading to roughly between 60 and
260 1-level mutants per location.
D. Fix Candidate Space Traversal
Here we present our general repair approach. The two
pruning techniques just introduced, are also described in more
detail, and we argue about their soundness. The search space
is organized as a search tree in a traditional search problem:
1139the root is the original speciÔ¨Åcation, with its faulty locations
identiÔ¨Åed; and if a speciÔ¨Åcation sis in the tree and s0can be
obtained by applying a mutation to a faulty location, then s0
is also in the tree, with the same locations marked as faulty
(so that the mutation process can be iterated). This in principle
leads to an inÔ¨Ånite Ô¨Åx candidate space, which we explore up to
a maximum depth. While any search strategy may be applied,
we explore the state space in a breadth-Ô¨Årst fashion.
1) Partial repair checking: Our Ô¨Årst pruning strategy con-
sists of identifying one of the suspicious locations for which
a current repair candidate fails, as established by an analysis
check that does not depend on the remainder of the faulty
locations. We will describe it in more detail, assuming two
faulty locations, without loss of generality. Let Spec be an
Alloy speciÔ¨Åcation, Check 1; : : : ; Check kits analysis checks
used as oracles, and L0; L1the suspicious locations identiÔ¨Åed
by the fault localization phase. Each analysis check Check i
refers to a speciÔ¨Åc part of Spec , which can be determined
by a straightforward syntactic analysis: Check irefers to the
formula it directly mentions (the body of the corresponding
predicate or assertion), all the facts (axioms of the speciÔ¨Åcation
that are implicitly involved in every analysis check), and the
symbols directly and indirectly referred syntactically to by
these (predicates called, relations used, etc.). This syntactic
analysis can determine then, for every Check i, which of the
suspicious locations L0andL1it involves.
Most logics, and certainly Alloy‚Äôs relational logic, have a
sort of syntactic locality property, that guarantees that the
validity/satisÔ¨Åability of a formula depends only on the symbols
it refers to. (In the case of Alloy, since validity/satisÔ¨Åability is
actually bounded validity/satisÔ¨Åability, it can also depend on
the scope, the bound, of analysis; but since the bound of anal-
ysis cannot be modiÔ¨Åed in the patch generation phase, we can
disregard it). Moreover, the logic is monotonic, meaning that
adding more assumptions to a formula can never reduce the
conclusions drawn originally from it. These properties allow us
to make the following observation. Let m0andn0constitute
the modiÔ¨Åcations to locations L0andL1, respectively, in the
current Ô¨Åx candidate (i.e., be the expressions substituting the
original expressions in locations L0andL1ofSpec ). If a
failing satisÔ¨Åability check Check irefers to only one of the
suspicious locations, say L0and its current expression m0,
this means that the formula in Check iis determined to be false
independently of n0. Then, for every alternative expression
nifor location L1, the corresponding Ô¨Åx candidate (m0; ni)
(the replacement expressions for locations L0andL1) will
still make Check ito be false, due to the monotonicity of
the logic. In other words, the speciÔ¨Åcation cannot be repaired
by modifying location L1if the current Ô¨Åx for location L0
is maintained. We can therefore exclude (prune from the
checking) all (m0; ni)Ô¨Åx candidates as soon as we determine
this situation, which in turn can be determined by a syntactic
analysis of the speciÔ¨Åcation, and the analysis outcome for Ô¨Åx
candidate (m0; n0).
We refer to this analysis and the corresponding pruning it
enables as partial repair checking , due to the partiality of Ô¨Åxcandidates when these do not involve all suspicious locations.
2) Variabilization: Our second pruning strategy is called
variabilization , due to the mechanism employed for prune
checking, that requires introducing fresh variables to refer to
Ô¨Åx candidates to speciÔ¨Åc locations, in a general way.
LetCheck ibe a failing assertion (validity) check that refers
to suspicious locations L0andL1, and let (m0; n0)be the
current failing Ô¨Åx candidate. Notice that since Check iis a
failing validity check, we have a counterexample CEX ias a
result of the violation. That is, we have that:
CEX i6j=Spec[m0; n0])Check i;
where Spec[m0; n0]denotes the Ô¨Åx candidate obtained by
replacing L0andL1bym0andn0, respectively, in Spec . The
purpose of variabilization is to check whether the current Ô¨Åx
forL0, i.e., m0,may work with some candidate for L1(other
thann0, of course, which we already know it does not work).
For technical reasons, we actually check whether some Ô¨Åx for
L1may work in combination with m0, for counterexample
CEXi. Let us describe the process for performing this check.
Notice that fault locations can be subexpressions of a
formula; let us refer by F1to the formula (predicate, fact,
etc) containing L1. Also, let Tbe the most general type for
L1in context F1(in Alloy, this most general type will depend
on the arity required by L1inF1, the context in which L1
may depend upon, and will use the most general unary type,
the universe univ ). Let SpecL1be the speciÔ¨Åcation obtained
by replacing F1inSpec by
9l1:TjF1[l1=L1]
i.e., we substitute L1by an existentially quantiÔ¨Åed variable of
typeT(hence the name variabilization ). We now can check:
CEX ij=SpecL0[m0])Check i;
i.e., whether there exists a value of type Tthat can be
put in place of location L1, so that CEX iceases to be a
counterexample. If this is the case, then local Ô¨Åx m0works
as a Ô¨Åx for L0, at least as far as CEX iis concerned, and we
may traverse the space of local candidates for L1to attempt
to Ô¨Ånd a complete Ô¨Åx. But, on the other hand, if the above
check fails, then there is no value that can be put in place
ofL1such that the local Ô¨Åx m0would work ( CEX iwould
still be a counterexample). Therefore, we can again exclude
(prune from the checking) all (m0; ni)Ô¨Åx candidates if the
check fails.
One may argue why not check the ‚Äúvariabilized‚Äù spec-
iÔ¨Åcation in the general case, instead of doing so only for
counterexample CEX i. The reason has to do with the type T
of location L1. When this type is a relation of an arity greater
than one, variabilization leads to a higher-order quantiÔ¨Åcation,
that Alloy cannot handle as a general analysis check, but it can
do so for the speciÔ¨Åc counterexample CEX i.
To clarify this variabilization process, and especially the
reason why we typically have higher-order quantiÔ¨Åcation,
let us consider the example introduced in Section II, where
1140one local Ô¨Åx candidate is applied and the other was
generalized with question marks. Assuming that assertion
ContainsCorrect failed, a counterexample CEX was gen-
erated from this Ô¨Åx. To check whether variabilization pruning
can be applied, we turn the question marks into existen-
tial quantiÔ¨Åcations. Intuitively, the corresponding variabilized
speciÔ¨Åcation would then be as follows (we are abusing the
notation below, using Boolean for the type of the variabilized
formula within Sorted ):
pred Sorted[This: List] {
some b: Boolean | all n: This.header. *link | b
}
pred Contains[This: List, x: Int, res: Boolean]{
RepOk[This]
(x ! inThis.header. *link.elem => res=False )
&& res = True
}
However, we need to take into account that the variabilization
context, the place where the location being variabilized occurs,
depends in this case both on This andn. Thus, the actual
variabilization for the check is as follows (we are again
abusing the notation for the sake of clarity):
pred Sorted[This: List] {
some b: List -> Node -> Boolean |
all n: This.header. *link | b[This, n]
}
pred Contains[This: List, x: Int, res: Boolean]{
RepOk[This]
(x ! inThis.header. *link.elem => res=False )
&& res = True
We cannot check ContainsCorrect over this speciÔ¨Åcation
due to the higher-order quantiÔ¨Åcation in Sorted ; but we can
check it for CEX .
It is worth remarking that the above check, if successful, will
produce a relational value forbthat makes the variabilized
speciÔ¨Åcation work. It will notproduce an expression to put in
place of the body of the quantiÔ¨Åcation, as a local Ô¨Åx candidate.
It would not even produce a relational value that can be ‚Äúhard-
wired‚Äù as a local Ô¨Åx of the corresponding location, since it is in
principle just a relational value that works for counterexample
CEX . But its existence is what enables us to decide that a local
Ô¨Åx for L1(Sorted ) may be possible, considering the current
local Ô¨Åx for L0(the&&inContains ). Our check essentially
corresponds to only checking for feasibility of a local Ô¨Åx with
respect to other locations.
An alternative to the above would be to attempt to turn
the relational value bound to binto a relational expression ,
that can be considered a local Ô¨Åx candidate. Such a process
would correspond to a synthesis procedure , which would
require a grammar for expressions, so that the solver can
attempt to work out an instance (an actual expression) during
the satisÔ¨Åability process. While it is technically feasible, it
is also signiÔ¨Åcantly more costly than our simpler query for
satisÔ¨Åability, which we solely use for pruning.IV. E VALUATION
We now assess our technique for automated repair of Alloy
speciÔ¨Åcations. Our evaluation is based on two benchmarks of
real faulty Alloy speciÔ¨Åcations, one taken from [3] and used in
the evaluation of ARepair [19], and the other originated in the
Alloy4Fun project [21], which includes 6 new models, with a
total of 1936 faulty variants (considering different speciÔ¨Åcation
assignments resolved by different students). All the presented
experiments were run on a 3.6GHz Intel Core i7 processor
with 16 GB RAM, running GNU/Linux. We used a 1 hour
timeout for each repair analysis instance.
Our evaluation considers the following research questions:
RQ1 What is the impact of the pruning strategies in the
performance of our technique?
RQ2 How does our technique compare to previous work
on automated repair for Alloy speciÔ¨Åcations?
For RQ1 , notice that the pruning strategies only apply to
speciÔ¨Åcations with multiple faulty locations. We then evaluate
our technique, with pruning enabled vs. pruning disabled, over
the following cases:
From ARepair‚Äôs benchmark (we will refer in this way to
the benchmark used in the original evaluation of [19]),
we consider 18 speciÔ¨Åcations out of the 38 that are part
of the benchmark. We disregard cases that have exactly
one bug (20 in total in the benchmark), as these will not
make pruning checks, nor trigger the pruning.
From Alloy4Fun, we consider a total of 273 faulty
speciÔ¨Åcations. To build these speciÔ¨Åcations, we tracked
the models with multiple assignments, and identiÔ¨Åed the
cases in which a given model was submitted with more
than one bug by the same student. (While the student id is
not reported as part of the Alloy4Fun dataset, submissions
are organized as chains of interaction ids, that correspond
to a same student session. We use this information to
organize submissions based on student sessions.)
The results are summarized in Table I. This table shows, for
each of the benchmarks, the number of cases, how many
were repaired with pruning enabled and disabled (recall the
1 hour timeout), and the average time for those cases that
were repaired within the timeout (time is in milliseconds). We
also report the increased repairability, and improved efÔ¨Åciency,
obtained by pruning. We considered the cases that were not
repaired with pruning disabled, but were repaired with pruning
enabled, as if they were repaired in 1 hour. So, the increased
efÔ¨Åciency is actually a lower bound of the actual improve-
ment. For reference, we also report the range of efÔ¨Åciency
improvement along all cases in each benchmark.
ForRQ2 , we compare our technique with the only other
approach for repairing Alloy models, namely ARepair [19].
We analyze both tools in their corresponding abilities to repair
speciÔ¨Åcations in our considered benchmarks. For ARepair‚Äôs
benchmark, we used the models‚Äô corresponding assertions as
oracles for BeAFix, and automatically generated test suites,
using AUnit [18], for ARepair. Recall that ARepair requires
tests as oracles for the repair process; we actually follow the
1141TABLE I
IMPACT OF PRUNING IN REPAIRABILITY .
Benchmark Total Pruning Disabled Pruning Enabled Improved repairability/efÔ¨Åciency
cases Repaired Cases Avg. Time Repaired Cases Avg. Time Repaired Cases Avg. Time [Range min - max]
ARepair‚Äôs benchmarks
balancedBST 2 0 0 - -
cd 1 1 1765 1 540 1.00 X 3X[3X- 3X]
dll 3 2 290366 2 2756 1.00 X 80X[26X- 133 X]
farmer 1 0 0 - -
fsm 1 0 0 - -
student 10 0 5 184030 5.00 X 26X[1X- 81X]
Total: 18 3 146066 8 62442 2.66 X 37X[1X- 133 X]
Alloy4Fun‚Äôs benchmarks
Graphs 22 6 409667 16 6821 2.66 X 123 X[9X- 387 X]
LTS 33 0 1 1983 1.00 X 181 X[181X- 181 X]
Trash 23 7 94960 15 8084 2.14 X 46X[2X- 107 X]
Production 2 0 0 - -
Classroom 169 14 755978 32 138447 2.28 X 82X[1X- 433 X]
CV 24 0 0 - -
Total: 273 27 420201 64 38833 2.36 X 85X[1X- 433 X]
procedure suggested in [19], as test cases are not commonly
found accompanying Alloy speciÔ¨Åcations. Notice then that the
results reported in [19] do not coincide with those reported
here for ARepair‚Äôs benchmark, as we use the same models
with different test suites. The test suites used in [19] include
manually designed cases, to help ARepair in overcoming
overÔ¨Åtting. In our evaluation, we favored a comparison in
which only the original assertions are available, and thus we
generated test cases automatically, with AUnit (using the best
performing criterion, predicate coverage [18]).
From the Alloy4Fun dataset, we generated a benchmark
consisting of: (i)every faulty submission of the dataset as
a single speciÔ¨Åcation (these correspond to every intermediate
speciÔ¨Åcation submitted for analysis check in Alloy4Fun); and
(ii)the speciÔ¨Åcations combining all modiÔ¨Åcations within a
single user session, that we used for RQ1 . The total number
of faulty speciÔ¨Åcations in this benchmark is 2209 (1936 faulty
submissions, plus 273 sessions combining submissions of the
same user). For BeAFix, we used the models‚Äô corresponding
assertions as oracles. Since we do not have tests for these
speciÔ¨Åcations, and ARepair inherently requires tests as repair
oracles, we generated tests automatically using AUnit [18]
(with predicate coverage as a target criterion), using the
speciÔ¨Åcation assertions, and employed these generated test
suites for running ARepair.
In all of the above cases, we contrasted the obtained repairs
against correct versions of the corresponding speciÔ¨Åcations,
using Alloy Analyzer, to account for overÔ¨Åtting. The results
for ARepair and Alloy4Fun benchmarks are summarized in
Tables II and III, respectively. For each model, we report the
number of cases, and for each tool, the number of Ô¨Åxes found
(percentage also reported), and how many of these are correct
and incorrect (the latter, due to overÔ¨Åtting) patches. We also
report the percentage of correct and incorrect patches, with
respect to the total number of cases, and the average repairtime in milliseconds, for each tool (these are the averages only
for the repaired cases).
A. Discussion
Let us discuss the evaluation results. For RQ1 , the results
are conclusive: the impact of pruning is signiÔ¨Åcant. Let us
remark that the efÔ¨Åciency speed up is better than the increase
in repairability (38X to 85X speed up, as opposed to roughly
2.5X increase in repairability). This may be explained by the
timeout that we have set: 1 hour may be a small timeout for
speciÔ¨Åcation repair using BeAFix: increasing it may show a
repairability increase closer to the speed up. Another important
issue about these results is that the semantic check that we
need to perform for pruning using variabilization, does in fact
pay off. In other words, the variabilization checks, that require
additional calls to the SAT solver, implied a time saving thanks
to pruning that improved the overall analysis time. This, of
course, is relative to the considered case studies. We did not
observe any case where the overhead caused by pruning made
the tool to actually take longer to repair a faulty speciÔ¨Åcation,
which may in fact happen for a speciÔ¨Åcation, if most feasibility
checks succeed, consuming time and leading to no pruning.
The benchmarks were taken from other authors‚Äô work; we
did not purposely look for speciÔ¨Åcations that may favor or
harm the pruning strategies. We plan to design synthetic
speciÔ¨Åcations, and extend the set of case studies, to further
assess the effect of pruning.
Regarding RQ2 , the comparison between BeAFix and
ARepair can be analyzed along various dimensions. Let us
Ô¨Årst consider the evaluation over ARepair‚Äôs benchmark. For
this benchmark, the test suites used for running ARepair
are solely composed of automatically generated tests, using
AUnit with predicate coverage. As a result, the number of
correct speciÔ¨Åcation Ô¨Åxes differ from the experiments in [19],
where manually designed test cases helped the tool from
overÔ¨Åtting. In our current experiments, ARepair is affected
1142TABLE II
EXPERIMENTS TAKEN FROM AREPAIR ‚ÄôS BENCHMARKS .
Total ARepair BeAFix
Model CasesRepaired (%)Avg.Correct (%) Incorrect (%) Repaired (%)Avg.Correct (%) Incorrect (%)time time
addr 1 1(100%) 9010 1(100%) 0(0%) 1(100%) 351 1(100%) 0(0%)
arr 2 2(100%) 7651 2(100%) 0(0%) 2(100%) 2394 2(100%) 0(0%)
balancedBST 3 2(67%) 120276 1(33%) 1(33%) 1(33%) 358 1(33%) 0(0%)
bempl 1 0(0%) 0(0%) 0(0%) 0(0%) 0(0%) 0(0%)
cd 2 2(100%) 3302 0(0%) 2(100%) 2(100%) 742 2(100%) 0(0%)
ctree 1 1(100%) 6774 1(100%) 0(0%) 0(0%) 0(0%) 0(0%)
dll 4 3(75%) 22585 0(0%) 3(75%) 3(75%) 2624 3(75%) 0(0%)
farmer 1 0(0%) 0(0%) 0(0%) 0(0%) 0(0%) 0(0%)
fsm 2 2(100%) 6068 2(100%) 0(0%) 1(50%) 313 1(50%) 0(0%)
grade 1 1(100%) 124797 0(0%) 1(100%) 0(0%) 0(0%) 0(0%)
other 1 0(0%) 0(0%) 0(0%) 1(100%) 3120 1(100%) 0(0%)
student 19 12(63%) 76120 9(47%) 3(16%) 13(68%) 71197 13(68%) 0(0%)
Total: 38 26(68%) 41843 16(42%) 10(26%) 24(63%) 10137 24(63%) 0(0%)
TABLE III
EXPERIMENTS TAKEN FROM ALLOY 4FUN‚ÄôS BENCHMARKS .
Total ARepair BeAFix
Model CasesRepaired (%)Avg.Correct (%) Incorrect (%) Repaired (%)Avg.Correct (%) Incorrect (%)time time
Graphs 305 276 (90%) 2625 18(6%) 258 (85%) 248 (81%) 6734 248 (81%) 0(0%)
LTS 282 165 (59%) 8729 7(2%) 158 (56%) 42(15%) 5999 42(15%) 0(0%)
Trash 229 220 (96%) 4077 68(30%) 152 (66%) 199 (87%) 4915 199 (87%) 0(0%)
Production 63 47(75%) 6232 8(13%) 39(62%) 56(89%) 4124 56(89%) 0(0%)
Classroom 1168 911 (78%) 95717 92(8%) 819 (70%) 418 (36%) 82856 418 (36%) 0(0%)
CV 162 132 (81%) 4966 4(2%) 128 (79%) 82(51%) 2805 82(51%) 0(0%)
Total: 2209 1751 (79%) 20391 197 (9%) 1554 (70%) 1045 (47%) 17905 1045 (47%) 0(0%)
by overÔ¨Åtting: 16 out of the 26 produced Ô¨Åxes are correct
Ô¨Åxes. BeAFix outperforms ARepair in terms of the number
of repaired models: 16 models repaired by ARepair, against
24 repaired by BeAFix (a 21% difference in the number
of repaired models, over the size of the benchmark). It is
worth remarking that the two techniques complement each
other in terms of the repaired models: ARepair is able to
repair models that BeAFix does not repair (see for instance
ctree andfsm), and BeAFix repairs models that ARepair
is not able to repair (see for instance student andother ).
In terms of efÔ¨Åciency, both tools show comparable running
times. The average time to produce a repair is however just
a reference, since the tools perform different kinds of tasks.
BeAFix does not include fault localization, so the times here
account for absolute repair times, given that the faults have
been localized ofÔ¨Çine. ARepair, on the other hand, includes
both the time to localize faults and perform the repair. Let us
remark however that, in ARepair, on average 62% of the time
corresponds to repair and 38% to fault localization. Unlike
ARepair, that alternates between patching and calling fault
localization, BeAFix calls fault localization only once, before
triggering repair. As such, the proportion of time devoted to
fault localization is much less. In our experiments, when weconsider the combination of fault localization and BeAFix, on
average 4% is devoted to fault localization (in the worst case,
Student6, the fault localization time was 13% of the total time).
Further details can be found in the tool‚Äôs site (see below).
Now let us consider the Alloy4Fun benchmark. For this
benchmark, we did not have any choice but to automatically
generate test cases, as these were not available for these
models. We generated test cases automatically, using AUnit
[18] (again, using the best performing generation criterion, as
reported in [18]). ARepair is able to repair a signiÔ¨Åcant number
of models: 1751 out of 2209. However, only 197 were correct
Ô¨Åxes; the remaining 1554 were overÔ¨Åtting cases, that passed
the automatically generated tests, but were not correct Ô¨Åxes for
the corresponding speciÔ¨Åcations. BeAFix, on the other hand,
produced a smaller number of Ô¨Åxes: 1042 out of the 2209. But
since it uses Alloy assertions as repair oracles, instead of test
cases, it showed no overÔ¨Åtting issues for these speciÔ¨Åcations.
As a result, BeAFix shows a better effectiveness in repair:
47% of correctly repaired models by BeAFix, against 9% of
correctly repaired models by ARepair. Regarding the cases
themselves, again, the tools complement each other: there are
cases correctly repaired by one tool that were not repaired by
the other, and vice versa.
1143The observed overÔ¨Åtting is an important difference between
the two tools and their approaches, and conÔ¨Årms our intuition
and motivation regarding the use of stronger repair oracles,
that naturally come in speciÔ¨Åcations. Clearly, one may argue
that ARepair‚Äôs performance, in terms of overÔ¨Åtting, can be
improved by feeding the tool with different/stronger test suites.
We fully agree, and in fact, this is conÔ¨Årmed with ARepair‚Äôs
benchmark: if the test suites used in [19] are fed to ARepair
(which, as we mentioned, include manually crafted tests), then
26 out of 38 models are repaired, compared to the 14 out of
38 repaired models obtained with just automatically generated
tests (effectiveness is increased from 42% to 68%). Writing
theright set of test cases for speciÔ¨Åcation repair is a time
consuming task, that would require a manual design of a test
suite for each of the models, to improve the tool‚Äôs results. The
overÔ¨Åtting problem is an inherent problem of using tests as
speciÔ¨Åcations, and thus it is expected of tools such as ARepair.
It is important to remark that we do not claim that our
technique leads to no overÔ¨Åtting, since this will depend on
the oracle being used, and how faithfully it captures the de-
veloper‚Äôs intentions. In the case of our controlled experiments,
where we had the ground truths as oracles (which would not be
the general case in formal speciÔ¨Åcation), we had no overÔ¨Åtting,
although overÔ¨Åtting may still have been observed due to the
bounded nature of the analysis. In any case, being forced to
use test cases as opposed to more general properties makes it
more prone to overÔ¨Åtting.
Other attributes of the generated patches may be considered.
One of these is readability. We can remark that candidate
patches are built out of mutations of the faulty expressions,
and the space of faulty expressions is visited in breadth-Ô¨Årst.
Therefore, simpler/shorter Ô¨Åx candidates are considered Ô¨Årst.
While we did not evaluate readability in a systematic fashion,
BeAFix‚Äôs patches can be simpler and clearer than manual,
human-written ones. For instance, for Production.Inv4 in the
Alloy4Fun benchmark, the faulty expression:
all c: Component |
(c.parts).position in(c.position).ÀÜÀúnext
is manually Ô¨Åxed by a student with the following expression:
all c: Component |
((c.ÀÜparts) & Component).position not in
(c.position).ÀÜnext or no (c.ÀÜparts & Component)
BeAFix on the other hand, produces the following:
all c : one Component |
c.parts.position inc.position.Àú *next
Another dimension to consider is efÔ¨Åciency of our tech-
nique, compared with manual repairs. In Alloy4Fun we can
measure the effort of human patches, by considering the time
of the sessions of a same student, from defect introduction to
its Ô¨Åxing. On average, it takes a student about 10 minutes to Ô¨Åx
a defect, once it is introduced. On the other hand, the average
time to repair in the case of BeAFix is about 10 seconds. For
instance, for the above faulty speciÔ¨Åcation, it took the student
a total of 49 minutes to get it right. BeAFix repaired it in 3
seconds. Due to space reasons, we do not present here a moredetailed comparison. The benchmarks, the tool‚Äôs output with
further statistical information, and the tool itself, can be found
in the tool‚Äôs site (see below).
V. R ELATED WORK
The problem of automatically repairing software defects has
received great attention in the last decade, and a variety of
techniques have been proposed to tackle it, including generate-
and-validate techniques (e.g., based on evolutionary computa-
tion [16] or other forms of search in the space of candidates),
techniques based on patch synthesis (e.g., techniques that
gather constraints for correct program behavior and produce
patches from these [24]) and techniques driven by data (e.g.,
techniques based on learning [25]). The emphasis is largely
targeted at programs , rather than speciÔ¨Åcations . As explained
earlier in this paper, the context of formal speciÔ¨Åcation has
some signiÔ¨Åcant differences with programs (source code), that
render many of these techniques not applicable, or at least
difÔ¨Åcult to adapt, to repairing speciÔ¨Åcations. The problem of
dealing with the explosion of repair candidates has been dealt
with in different ways, in the context of automated program
repair. Some approaches attempt to bring down the branching
factor in the search space by using a single mutation (e.g.,
[26]); others consider a very small set of mutators (e.g.,
based on patterns of human-written Ô¨Åxes [17]), or consider
coarse grained mutations (e.g., no intra-statement program
modiÔ¨Åcations [16]). Most of these approaches perform non-
exhaustive heuristic searches, as opposed to our technique,
that proposes safely pruning the search space.
Our technique produces Ô¨Åne-grained repair candidates that
are akin to mutations [27], such as operator and operand
replacements, etc., or more generally, combinations of muta-
tions (as in higher order mutations in the context of mutation
testing [28]). The motivation for this decision is based on a
number of issues, that seem to impact the effectiveness of
larger-grained modiÔ¨Åcations (such as the copying, deletion and
swapping of whole expressions) as operations to build repairs
in the context of speciÔ¨Åcation (for instance, for the case studies
presented in [20], our manual inspection showed no case where
one may repair the speciÔ¨Åcation by deleting, swapping or
copying whole expressions within the speciÔ¨Åcation). Firstly,
speciÔ¨Åcations do not seem to feature the same level of reuse
that programs have. For instance, in text books on formal
speciÔ¨Åcation with more traditional languages such as Z [29]
or B [30], one does not see modularization mechanisms (e.g.,
schema/machine composition) being used for reuse across
different speciÔ¨Åcations, with the exception of the reuse of
some general purpose speciÔ¨Åcations of sets, sequences, etc.
Rather, modularization mechanisms seem to be exploited
mainly for speciÔ¨Åcation organization, with little impact in
reuse. Secondly, most declarative speciÔ¨Åcation languages are
order-insensitive (the order of declarations and statements is
irrelevant, as opposed to operational languages, making order-
changing modiÔ¨Åcations ineffective). Thirdly, speciÔ¨Åcations are
signiÔ¨Åcantly shorter than source code, and therefore less
redundancy that could be exploited for repairs is observed.
1144While most work on automated repair applies to programs,
there are some notable exceptions [31], [19], [20]. The tool
AutoFix [31] targets contract-equipped programs, and can
produce repairs that make the programs satisfy their contracts
(at least as far as a test suite can determine). The technique can
modify contracts as well as the code itself, and therefore can
be considered as a speciÔ¨Åcation repair technique. The approach
differs from ours in many respects: it applies to speciÔ¨Åcations
at the source-code level, as opposed to the more abstract
speciÔ¨Åcations we target in this paper; it is not constrained
to speciÔ¨Åcations, it can indistinguishably alter programs and
speciÔ¨Åcations; and the speciÔ¨Åcation is notthe oracle for repair,
the tests are. An approach closely related to ours, as it applies
to Alloy speciÔ¨Åcations too, is ARepair [19], [20]. ARepair
repairs faulty Alloy speciÔ¨Åcations by combining a number
of techniques, including a technique for synthesis known as
sketching [32], and mutation-based repairs, as in program
repair. ARepair can Ô¨Åx speciÔ¨Åcations with multiple buggy
locations, and is able to do so considering a manageable set of
candidates, thanks to an effective fault localization approach
(and resorting to sketching rather than arbitrary mutations).
In effect, ARepair is guided by its own fault localization
approach, and the whole process is supported by Alloy tests.
Our approach, on the other hand, is not coupled with fault
localization, and can use different techniques (e.g., [33], [22],
as long as they can be used with the fault localization oracle
at hand) for fault localization. Alloy tests are similar to unit
tests for source code: they provide speciÔ¨Åc scenarios with an
expected outcome when evaluating speciÔ¨Åc parts of an Alloy
speciÔ¨Åcation, e.g., a predicate. The tool has been successfully
applied to repair speciÔ¨Åcations taken from a benchmark of
Alloy models [3] very efÔ¨Åciently, by being combined with
techniques for automated Alloy test generation (as tests are
necessary for repair). As for program repair techniques which
use tests as acceptance criteria, they are subject to overÔ¨Åtting ,
the problem that arises when a candidate passes all tests, but is
not a true repair, i.e., there are situations in which the program
(in this case, speciÔ¨Åcation) fails to comply with the intended
behavior. This, as usual, is strongly related to the quality of
the provided test suite, and many of the cases from [3] were
repaired thanks to additionally, manually provided, test cases
[19], [20]. ARepair inherently depends on test cases, while our
technique works on arbitrary Alloy speciÔ¨Åcation oracles. See
the previous section for a more detailed comparison of BeAFix
with ARepair, from a more experimental point of view.
Our technique uses Alloy counterexamples to weakly check
variabilization feasibility, since fully checking feasibility re-
quires dealing with higher-order quantiÔ¨Åcation. To perform
this higher-order checking, one may use Alloy* [34]. We
experimented with this approach, but due to performance
issues, we favored our current counterexample-based mecha-
nism. Also in this line, one may proÔ¨Åt from Alloy* to capture
Alloy‚Äôs grammar and semantics into Alloy*, and use the solver
to encode the whole repair approach. In this way, Alloy* would
function as a synthesis engine, with the solver doing the search
for repairs, as in some semantic program repair approaches(e.g., [24]). In our initial attempts we did not manage to obtain
results, due to the available heap space being exceeded, for
fragments of Alloy‚Äôs grammar signiÔ¨Åcantly smaller than what
we are considering with our ad-hoc search approach. We plan
however to further investigate this possibility.
VI. C ONCLUSION
Software speciÔ¨Åcation and modeling are crucial activities
of most software development methods. Getting a software
speciÔ¨Åcation right, i.e., capturing correctly a software design,
the constraints and expected properties, etc., especially when
the language to capture these is formal , is very challenging.
Thus, techniques and tools that help developers in correctly
specifying software is highly relevant. In this paper, we have
presented a technique that helps precisely in this task, in the
context of formal speciÔ¨Åcation using the Alloy language [4].
Our technique has a number of characteristics that distinguish
it from related work [20]. Firstly, it does not require any
particular form of the oracles , i.e., the properties to be used for
assessing Ô¨Åx candidates (as opposed to existing work which
require such oracles to be expressed in terms of test cases).
Secondly, it bounded exhaustively explores the state space of
Ô¨Åx candidates, thus Ô¨Ånding a speciÔ¨Åcation Ô¨Åx, or guaranteeing
that such a Ô¨Åx is impossible within the established bounds,
for the identiÔ¨Åed faulty locations, and with the provided
mutation (syntactic modiÔ¨Åcation) operators. This is suitable
in an Alloy context, where users are accustomed to bounded-
exhaustive analyses. This bounded-exhaustive exploration of
Ô¨Åx candidates demands then appropriate mechanisms to make
the search more efÔ¨Åcient. Our technique comes with two sound
pruning strategies, that allow us to avoid visiting large parts
of the state space for Ô¨Åx candidates, which are guaranteed
not to contain valid Ô¨Åxes. We have assessed our technique
on a large benchmark of Alloy speciÔ¨Åcations, and shown that
the pruning strategies have an important impact in analysis.
The technique has an efÔ¨Åciency comparable to that of the
previous work [20], it complements the latter in terms of the
Ô¨Åxes it is able to generate, and is less prone to overÔ¨Åtting,
as it naturally supports stronger oracles based on assertion
checking and property satisÔ¨Åability, that usually accompany
Alloy speciÔ¨Åcations.
VII. D ATA AVAILABILITY
BeAFix, all benchmark data, further statistical information
and the instructions to replicate the experiments in this paper,
are available at [35]. A snapshot of the tool and benchmark,
as used in the paper, is available at [36].
ACKNOWLEDGMENTS
We thank the anonymous reviewers for their helpful com-
ments. This work was supported in part by awards W911NF-
19-1-0054 from the Army Research OfÔ¨Åce; CCF-1948536,
CCF- 1755890, and CCF-1618132 from the National Science
Foundation; and PICT 2016-1384, 2017-1979 and 2017-2622
from the Argentine National Agency of ScientiÔ¨Åc and Tech-
nological Promotion (ANPCyT).
1145REFERENCES
[1] C. Ghezzi, M. Jazayeri, and D. Mandrioli, Fundamentals of Software
Engineering , 2nd ed. Upper Saddle River, NJ, USA: Prentice Hall
PTR, 2002.
[2] E. M. Clarke and J. M. Wing, ‚ÄúFormal methods: State of the art and
future directions,‚Äù ACM Comput. Surv. , vol. 28, no. 4, pp. 626‚Äì643,
1996. [Online]. Available: https://doi.org/10.1145/242223.242257
[3] T. Nelson, N. Danas, D. J. Dougherty, and S. Krishnamurthi, ‚ÄúThe
power of ‚Äùwhy‚Äù and ‚Äùwhy not‚Äù: enriching scenario exploration with
provenance,‚Äù in Proceedings of the 2017 11th Joint Meeting on
Foundations of Software Engineering, ESEC/FSE 2017, Paderborn,
Germany, September 4-8, 2017 , E. Bodden, W. Sch ¬®afer, A. van
Deursen, and A. Zisman, Eds. ACM, 2017, pp. 106‚Äì116. [Online].
Available: https://doi.org/10.1145/3106237.3106272
[4] D. Jackson, Software Abstractions - Logic, Language, and Analysis .
MIT Press, 2006.
[5] P. Zave, ‚ÄúReasoning about identiÔ¨Åer spaces: How to make chord correct,‚Äù
IEEE Transactions on Software Engineering , vol. 43, no. 12, pp. 1144‚Äì
1156, 2017.
[6] H. Bagheri, E. Kang, S. Malek, and D. Jackson, ‚ÄúA formal approach
for detection of security Ô¨Çaws in the android permission system,‚Äù
Formal Asp. Comput. , vol. 30, no. 5, pp. 525‚Äì544, 2018. [Online].
Available: https://doi.org/10.1007/s00165-017-0445-z
[7] M. Alhanahnah, C. Stevens, and H. Bagheri, ‚ÄúScalable analysis
of interaction threats in iot systems,‚Äù in ISSTA ‚Äô20: 29th ACM
SIGSOFT International Symposium on Software Testing and Analysis,
Virtual Event, USA, July 18-22, 2020 , S. Khurshid and C. S.
Pasareanu, Eds. ACM, 2020, pp. 272‚Äì285. [Online]. Available:
https://doi.org/10.1145/3395363.3397347
[8] H. Bagheri, A. Sadeghi, R. J. Behrouz, and S. Malek, ‚ÄúPractical, formal
synthesis and automatic enforcement of security policies for android,‚Äù
in46th Annual IEEE/IFIP International Conference on Dependable
Systems and Networks, DSN 2016, Toulouse, France, June 28 - July
1, 2016 . IEEE Computer Society, 2016, pp. 514‚Äì525. [Online].
Available: https://doi.org/10.1109/DSN.2016.53
[9] S. A. Khalek, G. Yang, L. Zhang, D. Marinov, and S. Khurshid,
‚ÄúTestera: A tool for testing java programs using alloy speciÔ¨Åcations,‚Äù
in26th IEEE/ACM International Conference on Automated Software
Engineering (ASE 2011), Lawrence, KS, USA, November 6-10,
2011 , P. Alexander, C. S. Pasareanu, and J. G. Hosking, Eds.
IEEE Computer Society, 2011, pp. 608‚Äì611. [Online]. Available:
https://doi.org/10.1109/ASE.2011.6100137
[10] N. Mirzaei, J. Garcia, H. Bagheri, A. Sadeghi, and S. Malek, ‚ÄúReducing
combinatorics in GUI testing of android applications,‚Äù in Proceedings
of the 38th International Conference on Software Engineering, ICSE
2016, Austin, TX, USA, May 14-22, 2016 , L. K. Dillon, W. Visser, and
L. A. Williams, Eds. ACM, 2016, pp. 559‚Äì570. [Online]. Available:
https://doi.org/10.1145/2884781.2884853
[11] P. Abad, N. Aguirre, V . S. Bengolea, D. Ciolek, M. F. Frias, J. P.
Galeotti, T. Maibaum, M. M. Moscato, N. Rosner, and I. Vissani,
‚ÄúImproving test generation under rich contracts by tight bounds and
incremental SAT solving,‚Äù in Sixth IEEE International Conference on
Software Testing, VeriÔ¨Åcation and Validation, ICST 2013, Luxembourg,
Luxembourg, March 18-22, 2013 . IEEE Computer Society, 2013, pp.
21‚Äì30. [Online]. Available: https://doi.org/10.1109/ICST.2013.46
[12] G. Dennis, F. S. Chang, and D. Jackson, ‚ÄúModular veriÔ¨Åcation
of code with SAT,‚Äù in Proceedings of the ACM/SIGSOFT
International Symposium on Software Testing and Analysis, ISSTA
2006, Portland, Maine, USA, July 17-20, 2006 , L. L. Pollock and
M. Pezz `e, Eds. ACM, 2006, pp. 109‚Äì120. [Online]. Available:
http://doi.acm.org/10.1145/1146238.1146251
[13] J. P. Galeotti, N. Rosner, C. L. Pombo, and M. F. Frias, ‚ÄúAnalysis
of invariants for efÔ¨Åcient bounded veriÔ¨Åcation,‚Äù in Proceedings of
the Nineteenth International Symposium on Software Testing and
Analysis, ISSTA 2010, Trento, Italy, July 12-16, 2010 , P. Tonella
and A. Orso, Eds. ACM, 2010, pp. 25‚Äì36. [Online]. Available:
http://doi.acm.org/10.1145/1831708.1831712
[14] J. P. Galeotti, N. Rosner, C. G. L. Pombo, and M. F. Frias, ‚ÄúTACO:
efÔ¨Åcient sat-based bounded veriÔ¨Åcation using symmetry breaking and
tight bounds,‚Äù IEEE Trans. Software Eng. , vol. 39, no. 9, pp. 1283‚Äì1307,
2013. [Online]. Available: https://doi.org/10.1109/TSE.2013.15[15] D. Jackson, ‚ÄúAlloy: a language and tool for exploring software designs,‚Äù
Commun. ACM , vol. 62, no. 9, pp. 66‚Äì76, 2019. [Online]. Available:
https://doi.org/10.1145/3338843
[16] C. Le Goues, T. Nguyen, S. Forrest, and W. Weimer, ‚ÄúGenprog:
A generic method for automatic software repair,‚Äù IEEE Trans.
Software Eng. , vol. 38, no. 1, pp. 54‚Äì72, 2012. [Online]. Available:
https://doi.org/10.1109/TSE.2011.104
[17] D. Kim, J. Nam, J. Song, and S. Kim, ‚ÄúAutomatic patch generation
learned from human-written patches,‚Äù in 35th International Conference
on Software Engineering, ICSE ‚Äô13, San Francisco, CA, USA, May
18-26, 2013 , D. Notkin, B. H. C. Cheng, and K. Pohl, Eds.
IEEE Computer Society, 2013, pp. 802‚Äì811. [Online]. Available:
https://doi.org/10.1109/ICSE.2013.6606626
[18] A. Sullivan, K. Wang, and S. Khurshid, ‚ÄúAunit: A test automation
tool for alloy,‚Äù in 11th IEEE International Conference on Software
Testing, VeriÔ¨Åcation and Validation, ICST 2018, V ¬®aster Àöas, Sweden, April
9-13, 2018 . IEEE Computer Society, 2018, pp. 398‚Äì403. [Online].
Available: https://doi.org/10.1109/ICST.2018.00047
[19] K. Wang, A. Sullivan, and S. Khurshid, ‚ÄúAutomated model repair
for alloy,‚Äù in Proceedings of the 33rd ACM/IEEE International
Conference on Automated Software Engineering, ASE 2018, Montpellier,
France, September 3-7, 2018 , M. Huchard, C. K ¬®astner, and
G. Fraser, Eds. ACM, 2018, pp. 577‚Äì588. [Online]. Available:
https://doi.org/10.1145/3238147.3238162
[20] ‚Äî‚Äî, ‚ÄúArepair: a repair framework for alloy,‚Äù in Proceedings of the
41st International Conference on Software Engineering: Companion
Proceedings, ICSE 2019, Montreal, QC, Canada, May 25-31, 2019 ,
J. M. Atlee, T. Bultan, and J. Whittle, Eds. IEEE / ACM,
2019, pp. 103‚Äì106. [Online]. Available: https://doi.org/10.1109/ICSE-
Companion.2019.00049
[21] N. Macedo, A. Cunha, J. Pereira, R. Carvalho, R. Silva, A. C. R. Paiva,
M. S. Ramalho, and D. C. Silva, ‚ÄúExperiences on teaching alloy with an
automated assessment platform,‚Äù in Rigorous State-Based Methods - 7th
International Conference, ABZ 2020, Ulm, Germany, May 27-29, 2020,
Proceedings , ser. Lecture Notes in Computer Science, A. Raschke,
D. M ¬¥ery, and F. Houdek, Eds., vol. 12071. Springer, 2020, pp. 61‚Äì77.
[Online]. Available: https://doi.org/10.1007/978-3-030-48077-6 5
[22] G. Zheng, T. Nguyen, S. Guti ¬¥errez-Brida, G. Regis, M. Frias, N. Aguirre,
and H. Bagheri, ‚ÄúFLACK: Counterexample-guided fault localization for
alloy models,‚Äù in Proceedings of the 43rd ACM/IEEE International
Conference on Software Engineering ICSE 2021, Virtual (originally
Madrid, Spain), 23-29 May 2021 , 2021.
[23] Y . Jia and M. Harman, ‚ÄúHigher order mutation testing,‚Äù Inf. Softw.
Technol. , vol. 51, no. 10, pp. 1379‚Äì1393, 2009. [Online]. Available:
https://doi.org/10.1016/j.infsof.2009.04.016
[24] S. Mechtaev, J. Yi, and A. Roychoudhury, ‚ÄúAngelix: scalable multiline
program patch synthesis via symbolic analysis,‚Äù in Proceedings of the
38th International Conference on Software Engineering, ICSE 2016,
Austin, TX, USA, May 14-22, 2016 , L. K. Dillon, W. Visser, and
L. Williams, Eds. ACM, 2016, pp. 691‚Äì701. [Online]. Available:
https://doi.org/10.1145/2884781.2884807
[25] F. Long and M. Rinard, ‚ÄúAutomatic patch generation by learning correct
code,‚Äù in Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT
Symposium on Principles of Programming Languages, POPL 2016,
St. Petersburg, FL, USA, January 20 - 22, 2016 , R. Bod ¬¥ƒ±k and
R. Majumdar, Eds. ACM, 2016, pp. 298‚Äì312. [Online]. Available:
https://doi.org/10.1145/2837614.2837617
[26] D. Gopinath, M. Z. Malik, and S. Khurshid, ‚ÄúSpeciÔ¨Åcation-based
program repair using SAT,‚Äù in Tools and Algorithms for the
Construction and Analysis of Systems - 17th International Conference,
TACAS 2011, Held as Part of the Joint European Conferences on
Theory and Practice of Software, ETAPS 2011, Saarbr ¬®ucken, Germany,
March 26-April 3, 2011. Proceedings , ser. Lecture Notes in Computer
Science, P. A. Abdulla and K. R. M. Leino, Eds., vol. 6605. Springer,
2011, pp. 173‚Äì188. [Online]. Available: https://doi.org/10.1007/978-3-
642-19835-9 15
[27] P. Ammann and J. Offutt, Introduction to software testing . Cambridge
University Press, 2008.
[28] Y . Jia and M. Harman, ‚ÄúHigher order mutation testing,‚Äù Inf. Softw.
Technol. , vol. 51, no. 10, pp. 1379‚Äì1393, 2009. [Online]. Available:
https://doi.org/10.1016/j.infsof.2009.04.016
[29] J. C. P. Woodcock and J. Davies, Using Z - speciÔ¨Åcation, reÔ¨Ånement,
and proof , ser. Prentice Hall international series in computer science.
Prentice Hall, 1996.
1146[30] J. Abrial, The B-book - assigning programs to meanings . Cambridge
University Press, 2005.
[31] Y . Pei, C. A. Furia, M. Nordio, Y . Wei, B. Meyer, and A. Zeller,
‚ÄúAutomated Ô¨Åxing of programs with contracts,‚Äù IEEE Trans. Software
Eng., vol. 40, no. 5, pp. 427‚Äì449, 2014. [Online]. Available:
https://doi.org/10.1109/TSE.2014.2312918
[32] A. Solar-Lezama, ‚ÄúProgram sketching,‚Äù Int. J. Softw. Tools Technol.
Transf. , vol. 15, no. 5-6, pp. 475‚Äì495, 2013. [Online]. Available:
https://doi.org/10.1007/s10009-012-0249-7
[33] K. Wang, A. Sullivan, D. Marinov, and S. Khurshid, ‚ÄúFault Localiza-tion for Declarative Models in Alloy,‚Äù in International Symposium on
Software Reliability Engineering (ISSRE) , 2020, p. to appear.
[34] A. Milicevic, J. P. Near, E. Kang, and D. Jackson, ‚ÄúAlloy*:
a general-purpose higher-order relational constraint solver,‚Äù Formal
Methods Syst. Des. , vol. 55, no. 1, pp. 1‚Äì32, 2019. [Online]. Available:
https://doi.org/10.1007/s10703-016-0267-2
[35] ‚ÄúBeAFix site,‚Äù https://sites.google.com/view/beaÔ¨Åxevaluation, 2021.
[36] ‚ÄúBeAFix version and experimental data snapshot,‚Äù
https://doi.org/10.6084/m9.Ô¨Ågshare.13626776.v1, 2021.
1147