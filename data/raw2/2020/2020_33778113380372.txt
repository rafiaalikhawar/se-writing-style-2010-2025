Lazy Product Discovery in Huge Configuration Spaces
Michael Lienhardt
ONERA âˆ’The French Aerospace Lab
France
michael.lienhardt@onera.frFerruccio Damiani
University of Turin
Italy
ferruccio.damiani@unito.it
Einar Broch Johnsen
University of Oslo
Norway
einarj@ifi.uio.noJacopo Mauro
University of Southern Denmark
Denmark
mauro@sdu.dk
ABSTRACT
Highly-configurable software systems can have thousands of inter-
dependent configuration options across different subsystems. In the
resulting configuration space, discovering a valid product configu-
ration for some selected options can be complex and error prone.
The configuration space can be organized using a feature model,
fragmented into smaller interdependent feature models reflecting
the configuration options of each subsystem.
We propose a method for lazy product discovery in large frag-
mented feature models with interdependent features. We formalize
the method and prove its soundness and completeness. The evalu-
ation explores an industrial-size configuration space. The results
show that lazy product discovery has significant performance ben-
efits compared to standard product discovery, which in contrast
to our method requires all fragments to be composed to analyze
the feature model. Furthermore, the method succeeds when more
efficient, heuristics-based engines fail to find a valid configuration.
CCS CONCEPTS
â€¢Software and its engineering â†’Software product lines ;Fea-
ture interaction ;Abstraction, modeling and modularity ;Software
libraries and repositories ;Software creation and management ;
KEYWORDS
Software Product Lines, Configurable Software, Variability Model-
ing, Feature Models, Composition, Linux Distribution
ACM Reference Format:
Michael Lienhardt, Ferruccio Damiani, Einar Broch Johnsen, and Jacopo
Mauro. 2020. Lazy Product Discovery in Huge Configuration Spaces. In
42nd International Conference on Software Engineering (ICSE â€™20), May 23â€“
29, 2020, Seoul, Republic of Korea. ACM, New York, NY, USA, 13 pages.
https://doi.org/10.1145/3377811.3380372
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
ICSE â€™20, May 23â€“29, 2020, Seoul, Republic of Korea
Â©2020 Copyright held by the owner/author(s). Publication rights licensed to Associa-
tion for Computing Machinery.
ACM ISBN 978-1-4503-7121-6/20/05.. .$15.00
https://doi.org/10.1145/3377811.33803721 INTRODUCTION
Highly-configurable software systems can have thousands of in-
terdependent configuration options across different subsystems. In
the resulting configuration space, different software variants can be
obtained by selecting among these configuration options. The inter-
dependencies between options are typically caused by interaction
in the resulting software system. Constructing a well-functioning
software variant can be a complex and error-prone process [7].
Feature models [8] allow us to organize the configuration space
and facilitate the construction of software variants by describing
configuration options using interdependent features [32]: a feature
is a name representing some functionality, a set of features is called
aconfiguration, and each software variant is identified by a valid
configuration (called a product, for short).
Highly-configurable software systems can consist of thousands
of features and combine several subsystems [ 12,13,37,56], each
with different features. The construction and maintenance of feature
models with thousands of features for such highly-configurable
systems, can be simplified by representing large feature models as
sets of smaller interdependent feature models [ 12,49] which we
callfragments. However, the analysis of such fragmented feature
models usually requires the fragments to be composed, to enable
the application of existing analysis techniques [ 9,10,43,53,58,59].
To this aim, many approaches for composing feature models from
fragments have been investigated [3, 6, 14, 16, 48, 52].
The analysis of fragmented feature models can be simplified if
suitable abstractions can safely replace some of the feature model
fragments in the analysis. This simplification can be realized by
means of feature-model interfaces [51]. A feature-model interface is
a feature model that hides some of the features and dependencies of
another feature model (thus, interfaces are closely related to feature-
model slicing [4]). An interface can be used instead of a feature
model fragment to simplify the overall feature model. For certain
analyses, working on the simplified feature model produces results
that also hold for the original feature model and for any feature
model where the interface is replaced by a fragment compatible
with the interface.
This paper addresses automated product discovery in large config-
uration spaces represented as sets of interdependent feature models.
Product discovery (sometimes called product configuration) is a
particular analysis for finding a product which includes a desired set
of features [ 26]. We aim at automatically discovering a product that
contains a given set of features from the feature model fragments,
15092020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE)
without having to compose all the fragments to apply the analy-
sis. This work is motivated by our recent experiences in applying
techniques for variability modeling to automated product discovery
in industrial use cases such as Gentoo [ 23], a source-based Linux
distribution that consists of many highly-configurable packages.
The March 1st 2019 version of the Gentoo distribution comprises
671617 features spread across 36197 feature models. Gentooâ€™s huge
configuration space can be seen as the composition of the feature
models for all its packages, where package interdependencies are
modeled as shared features. Gentooâ€™s official package manager and
distribution system Portage [24] achieves (via its emerge tool) ef-
ficiency at the expense of completeness; i.e., in some cases this
tool fails to discover a product that contains a given set of features,
although such a product exists. We show that feature model in-
terfaces [ 51], which were developed to support analysis reuse for
feature model evolution in fragmented feature models, do not allow
us to reach our aim of complete and efficient automated product
discovery.
We propose a novel method for product discovery in sets of
interdependent feature models. The proposed method is lazy in the
sense that features are added incrementally to the analysis until a
product is found. We provide a formal account of the method and
evaluate it by implementing an efficient and complete dependency
solver for Gentoo. In short, our contributions are:
(1)we strengthen feature model interfaces to enable lazy prod-
uct discovery in sets of interdependent feature models;
(2)we propose an efficient and complete algorithm for lazy
product discovery in huge configuration spaces;
(3)we provide an open-source implementation of the proposed
algorithm;1and
(4)we evaluate the potential of lazy product discovery in terms
of experiments on an industrial-size configuration space.2
2 MOTIVATION AND OVERALL CONCEPT
A software system like the Gentoo distribution comprises 36197
configurable packages, as of its March 1st 2019 version. The con-
figuration space of each package can be represented by a feature
model; the overall configuration space of Gentoo can then be rep-
resented by a feature model that is the composition of the feature
models of the 36197 packages. The resulting feature model has
671617 features, and thus a configuration space with up to 2671617
solutions.
Gentooâ€™s official package manager Portage implements an opti-
mized, heuristics-based product-discovery algorithm to find prod-
ucts in this configuration space. This algorithm is not complete;
i.e., it fails to solve some product-discovery problems that have
solutions. To the best of our knowledge, existing complete product-
discovery approaches need to load the entire feature model to find
products. Consequently, they do not scale to product-discovery
problems of the size of Gentooâ€™s configuration space.
In this paper we target product discovery in huge configuration
spaces, such as for Gentoo, that can be described by a feature model
1The lazy product-discovery tool is available at https://github.com/gzoumix/pdepa and
at archive.softwareheritage.org/browse/origin/https://github.com/gzoumix/pdepa.git
2The evaluation artifact is available at https://doi.org/10.6084/m9.figshare.11728914.v4
and https://doi.org/10.5281/zenodo.3633643Listing 1: Lazy product-discovery algorithm
1inputğ‘†: set of feature models
2inputğ‘: configuration
3varğ‘Œ=ğ‘
4varMâ€²=compose({pick_cut(M,ğ‘Œ)|Mâˆˆğ‘†})
5varsolution = select(Mâ€²,ğ‘)
6while (solution â‰ Noneâˆ§solution âŠˆğ‘Œ):
7ğ‘Œ=ğ‘Œâˆªsolution
8Mâ€²=compose({pick_cut(M,ğ‘Œ)|Mâˆˆğ‘†})
9 solution = select(Mâ€²,ğ‘)
10 return solution
represented as a set ğ‘†of feature models with shared features, where
loading the overall feature model (i.e., the whole set ğ‘†) is too ex-
pensive. We propose lazy product discovery, a product-discovery
method that loads the elements of ğ‘†incrementally, until it finds
a product of the overall feature model. The method relies on the
notion of a cut of a feature model Mfor a set of features ğ‘Œ. This is a
feature modelMâ€²whose products are products of Mand include
all the products ofMthat contain a feature in ğ‘Œ.
The proposed algorithm, shown in Listing 1, takes as input a
setğ‘†of feature models with shared features and a set ğ‘of features
to be included in the discovered product. After initialization, the
algorithm incrementally loads cuts until a solution has been found.
LetM0denote the composition of the feature models in ğ‘†. The
algorithm returns a (not necessarily minimal) product of M0which
includes all features in ğ‘, whenever such a product exists; other-
wise, it returns the special value None. The algorithm relies on the
following three auxiliary functions:
(1)pick_cut(M,ğ‘Œ): a function that, given a feature model M
and a set of features ğ‘Œ, returns a cut ofMforğ‘Œ;
(2)compose({M 1,...,Mğ‘›}): a function that, given a set of fea-
ture modelsM1,...,Mğ‘›, returns the composition of the
feature models in the set; and
(3)select(M,ğ‘): a function that, given a feature model Mand
a set of features ğ‘, returns a product of Mcontaining all the
features inğ‘if it exists, and None otherwise.
Assuming that the auxiliary functions (1), (2) and (3) work, we
have that on Line 6 the following loop invariants hold:
Inv1:ğ‘âŠ†ğ‘Œ.
Inv2: solution is a product ofMâ€²which includes all features in
ğ‘, whenever such a product exists; otherwise solution is the
special value None.
Inv3: ifsolution is a product ofMâ€²andsolutionâŠ†ğ‘Œ, then solution
is also a product of M0.
Inv4: IfMâ€²has no product which includes all features in ğ‘, then
neither doesM0.
Checking that Inv1 holds is straightforward: just observe that on
Line 3 the variable ğ‘Œis initialized to ğ‘and that at each iteration of
thewhile loop new features are added to ğ‘Œon Line 7. Checking that
Inv2 holds is equally straightforward: according to the description
of the auxiliary functions (1), (2) and (3), the invariant is established
on Lines 4 and 5 as well as on Lines 8 and 9. The fact that Inv3 and
Inv4 hold is shown in the proof of Theorem 4 in Section 5. The
1510algorithm terminates because at each iteration of the while loop, the
size of the set ğ‘Œ(which, by construction, only contains features from
the features models in ğ‘†) increases. When the algorithm terminates
we have that either solution =None orNoneâ‰ solutionâŠ†ğ‘Œ. In the
first case (by Inv4 ) we have thatM0has no product that contains
all the features in ğ‘, while in the second case (by Inv3 ) we have
that solution is a product ofM0that contains all the features in ğ‘.
The laziness of this algorithm stems from the fact that it does
not need to consider M0at once. Instead, the algorithm starts by
considering the composition of the cuts of the feature models for
ğ‘Œ=ğ‘and then iterates by considering bigger and bigger cuts until
the candidate solution is contained in the set ğ‘Œ. When this happens
we know, for the properties of the cut, that the found solution is
also a solution forM0.
The algorithmâ€™s efficiency in finding a product with the features
inğ‘(see Lines 4, 5 and 8, 9 of Listing 1) compared to executing
select(M0,ğ‘), depends on the degree to which the feature models
inğ‘†are such that:
- computing pick_cut(M,ğ‘Œ)is efficient,
- the feature models Mâ€²are small compared to M0,
-select(Mâ€²,ğ‘)performs better than select(M0,ğ‘), and
- a small number of iterations of the while-loop is required.
For the Gentoo distribution, each feature model Mğ‘–inğ‘†has
a distinguished feature ğ‘“ğ‘–such that the constraints expressed by
Mğ‘–are enabled only if ğ‘“ğ‘–is selected (see Section 6.1). This reflects
that eachMğ‘–corresponds to a Gentoo package that is installed if
and only ifğ‘“ğ‘–is selected. Therefore, the function pick_cut(M,ğ‘Œ)
can be efficiently implemented by returning Mğ‘–ifğ‘“ğ‘–âˆˆğ‘Œ, and by
returning a feature model that expresses no constraints (and can,
therefore, be ignored by the composition that builds Mâ€²) otherwise.
The rest of this paper is organized as follows: Sections 3â€“5 pro-
vide a formal account of the lazy product-discovery method that
culminates in the proof that Inv3 andInv4 hold, Section 6 evalu-
ates the performance of the lazy product-discovery algorithm by
means of experiments, and Sections 7 and 8 discuss related work
and conclude the paper, respectively.
3 A FORMALIZATION OF FEATURE MODELS
This section presents a formalization of feature models (FM) and
related notions, including feature model interfaces and composition.
3.1 Feature Model Representations
Different representations of feature models are discussed, e.g., by
Batory [ 8]. In this paper, we will rely on the propositional formula
representation of feature models. In this representation, a feature
model is given by a pair (F,ğœ™)where:
-Fis a set of features, and
-ğœ™is a propositional formula where the variables ğ‘¥are feature
names:ğœ™::=ğ‘¥|ğœ™âˆ§ğœ™|ğœ™âˆ¨ğœ™|ğœ™â†’ğœ™|Â¬ğœ™.
A propositional formula ğœ™over a set of features Frepresents the
feature models whose products are configurations {ğ‘¥1,...,ğ‘¥ğ‘›}âŠ†F
(ğ‘›â‰¥0) such that ğœ™is satisfied by assigning value true to the
variablesğ‘¥ğ‘–(1â‰¤ğ‘–â‰¤ğ‘›) and false to all other variables.
Example 1 (A propositional representation of glibc FM).
Gentoo packages can be configured by selecting features (called useflags in Gentoo), which may trigger dependencies or conflicts between
packages. Version 2.29 of the glibc library, that contains the core
functionalities of most Linux systems, is provided by the package sys-
libs/glibc-2.29-r2 (abbreviated to glibc in the sequel). This package
has many dependencies, including (as expressed in Gentooâ€™s notation):
doc? ( sysâˆ’apps/texinfo )
vanilla?( !sysâˆ’libs/timezoneâˆ’data )
This dependency expresses that glibc requires the texinfo docu-
mentation generator (provided by any version of the sys-apps/texinfo
package) whenever the feature docis selected and if the feature vanilla
is selected, then glibc conflicts with any version of the time zone data-
base (as stated with the !sys-libs/timezone-data constraint). These
dependencies andconflicts can be expressed by a feature model
(Fglibc,ğœ™glibc)where
Fglibc={glibc, txinfo, tzdata, glibc:doc, glibc:v},and
ğœ™glibc=glibcâ†’(( glibc:docâ†’txinfo)âˆ§( glibc:vâ†’(Â¬tzdata)) .
Here, the feature glibc represents the glibc package; txinfo repre-
sents any sys-apps/texinfo package; tzdata represents any version
of the sys-libs/timezone-data package; and glibc:doc (resp. glibc:v)
represents the glibcâ€™s doc(resp. vanilla) use flag.
The propositional representation of feature models works well
in practice [ 9,44,58] and we shall use it for the evaluation of the
proposed method (in Section 6). In contrast, to simplify the proofs,
we follow SchrÃ¶ter et al. [51] in using an extensional representation
of feature models to present our theory.
Definition 1 (Feature model, extensional representation).
AFeature ModelMis a pair(F,P)whereFis a set of features and
PâŠ† 2Fa set of products.
Example 2 (An extensional representation of glibc FM).
Let2ğ‘‹denote the powerset of ğ‘‹. The feature model of Example 1
can be given an extensional representation Mglibc=(Fglibc,Pglibc)
whereFglibc is the same as in Example 1 and
Pglibc={{glibc},{glibc, txinfo},{glibc, tzdata},{glibc, txinfo, tzdata}}âˆª
{{glibc, glibc:doc, txinfo},{glibc, glibc:doc, txinfo, tzdata}}âˆª
{{glibc, glibc:v},{glibc, glibc:v, txinfo}}âˆª
{{glibc, glibc:doc, glibc:v, txinfo}}âˆª
2{txinfo, tzdata, glibc:doc, glibc:v}.
In the description of Pglibc, the first line contains products with glibc
but none of its use flags are selected, so texinfo andtzdata can be
freely installed; the second line contains products with the use flag
doc selected in glibc, so a package of sys-apps/texinfo is always
required; the third line contains products with the use flag vanilla
selected in glibc, so no package of sys-libs/timezone-data is allowed;
the forth line contains products with both glibcâ€™s use flags selected,
sosys-apps/texinfo is mandatory and sys-libs/timezone-data for-
bidden; finally, the fifth line represents products without glibc, so all
combinations of other features are possible, including the empty set.
Definition 2 (Empty FM, void FMs, and pre-products). The
empty feature model, denoted Mâˆ…=(âˆ…,{âˆ…}) , has no features and has
just the empty product âˆ…. Avoid feature model is a feature model that
has no products, i.e., it has the form (F,âˆ…)for someF. Apre-product
of a feature modelMis a configuration ğ‘that can be extended to a
product ofM(more formally, ğ‘âŠ†ğ‘for some product ğ‘ofM).
1511Based on the above definition of a pre-product, we identify two
related search problems.
Definition 3 (Feature compatibility, product discovery).
Consider a feature model Mand a set of features ğ‘inM. The feature-
compatibility problem forğ‘inMis the problem of determining
whetherğ‘is a pre-product ofM(i.e., whether the features in ğ‘are
compatible with the products in M). The product-discovery problem
forğ‘inMis the problem of finding a product of Mthat extends ğ‘.
Clearly, the feature-compatibility problem for ğ‘inMhas a posi-
tive answer if and only if the product-discovery problem for ğ‘in
Mhas a solution.
3.2 Feature Model Interfaces
Feature model interfaces were defined by SchrÃ¶ter et al. [51] as a
binary relationâª¯, expressing that a feature model Mâ€²is an interface
of a feature modelMifMâ€²ignores some features of M.
Definition 4 (FM interface relation). A feature modelMâ€²=
(Fâ€²,Pâ€²)is an interface of feature modelM=(F,P), denoted as
Mâ€²âª¯M , iffFâ€²âŠ†F andPâ€²={ğ‘âˆ©Fâ€²|ğ‘âˆˆP} .
Note that, for all feature models Mâ€²=(Fâ€²,Pâ€²)andM, if
Mâ€²âª¯M then (i) all products of Mâ€²are pre-products of Mand
(ii)Mâ€²is the only interface of Mwhich has exactly the features
Fâ€²(i.e.,Mâ€²is completely determined by Fâ€²).
Example 3 (An interface for glibc FM). The feature model
F={glibc, glibc:v}
P={âˆ…,{glibc},{glibc, glibc:v}}
is the interface of the feature model Mglibc from Example 2 that is
determined by the features glibc andglibc:v.
The interface relation for feature models is a partial order (i.e., it
is reflexive, transitive and anti-symmetric) and the empty feature
modelMâˆ…is an interface of every non-void feature model M.
Moreover,Mis void if and only if (âˆ…,âˆ…)âª¯M .
The notion of a feature model interface is closely related to that
of afeature model slice, which was defined by Acher et al. [4] as a
unary operator Î ğ‘Œrestricting a feature model to a set ğ‘Œof features.
Given a feature model M,Î ğ‘Œ(M) is the feature model obtained
fromMby removing the features not in ğ‘Œ.
Definition 5 (FM slice operator). Theslice operator Î ğ‘Œon
feature models, where ğ‘Œis a set of features, is defined by:
Î ğ‘Œ((F,P))=(Fâˆ©ğ‘Œ,{ğ‘âˆ©ğ‘Œ|ğ‘âˆˆP}).
Note that, for every feature model M=(F,P)and set of fea-
turesğ‘Œ, the feature model Î ğ‘Œ(M)=(Fâ€²,Pâ€²)is the unique inter-
face ofMsuch thatFâ€²=Fâˆ©ğ‘Œ. Moreover, for every interface
M1=(F1,P1)ofMit holds thatM1=Î F1(M) .
Example 4 (A slice of glibc FM). The feature model interface in
Example 3 can be obtained by applying Î {glibc, glibc:v}to the feature
modelMglibc of Example 2.
3.3 Feature Model Composition
Highly-configurable software systems often consist of many in-
terdependent, configurable packages [ 23,37,38]. The variabilityconstraints of each of these packages can be represented by a fea-
ture model. Therefore, configuring two (or more packages) in such a
way that they can be installed together corresponds to identifying a
product in a suitable composition of their associated feature models.
In the propositional representation of feature models, such compo-
sition corresponds to logical conjunction; i.e., the composition of
two feature models (F1,ğœ™1)and(F2,ğœ™2)is the feature model
(F1âˆªF2,ğœ™1âˆ§ğœ™2).
In the extensional representation of feature models, this form of
composition corresponds to the binary operator â€¢of SchrÃ¶ter et
al.[51], which is similar to the join operator from relational alge-
bra [17].
Definition 6 (FM composition). The composition of two feature
modelsM1=(F1,P1)andM2=(F2,P2), denotedM1â€¢M 2, is
the feature model defined by:
M1â€¢M 2=(F1âˆªF2,{ğ‘âˆªğ‘|ğ‘âˆˆP1,ğ‘âˆˆP2,ğ‘âˆ©F2=ğ‘âˆ©F1}).
The composition operator â€¢is associative and commutative,
withMâˆ…as identity element (i.e., Mâ€¢Mâˆ…=M). Composing a
feature model with a void feature model yields a void feature model:
(F1,P1)â€¢(F 2,âˆ…)=(F1âˆªF2,âˆ…).
Example 5 (Composing glibc and gnome-shell FMs). Let us
consider another important package of the Gentoo distribution: gnome-
shell, a core component of the Gnome Desktop environment. Version
3.30.2 of gnome-shell is provided by the package gnome-base/gnome-
shell-3.30.2-r2 (abbreviated to g-shell in the sequel), and its depen-
dencies include the following statement:
networkmanager?( sysâˆ’libs/timezoneâˆ’data ).
This dependency expresses that g-shell requires any version of the
time zone database when the feature networkmanager is selected.
Thepropositional representation of this dependency can be cap-
tured by the feature model (Fg-shell,ğœ™g-shell), where
Fg-shell={g-shell, tzdata, g-shell:nm},and
ğœ™g-shell=g-shellâ†’(g-shell:nmâ†’tzdata).
The corresponding extensional representation of this feature model
isMg-shell =(Fg-shell,Pg-shell), where:
Pg-shell={{g-shell},{g-shell, tzdata}}âˆª
{{g-shell, tzdata, g-shell:nm}}âˆª
2{tzdata, g-shell:nm}.
Here, the first line contains products with g-shell but none of its use
flags are selected: tzdata can be freely selected; the second line is
the product where g-shell:nm is also selected and tzdata becomes
mandatory; finally, the third line represents products without g-shell.
Thepropositional representation of the composition is the feature
model(Ffull,ğœ™full), where
Ffull=FglibcâˆªF g-shell
={glibc, txinfo, tzdata, g-shell, glibc:doc, glibc:v, g-shell:nm},and
ğœ™full=ğœ™glibcâˆ§ğœ™g-shell
=(glibcâ†’(( glibc:docâ†’txinfo)âˆ§( glibc:vâ†’(Â¬tzdata)))âˆ§
(g-shellâ†’(g-shell:nmâ†’tzdata)).
1512Theextensional representation of the composition is the feature
modelMfull=Mglibcâ€¢M g-shell =(Ffull,Pfull)where
Pfull=PglibcâˆªP g-shellâˆª2{txinfo, tzdata, glibc:doc, glibc:v, g-shell:nm}âˆª
{{glibc, g-shell}âˆªğ‘|ğ‘âˆˆ2{txinfo, tzdata}}âˆª
{{glibc, glibc:doc, txinfo, g-shell}âˆªğ‘|ğ‘âˆˆ2{tzdata}}âˆª
{{glibc, glibc:v, g-shell}âˆªğ‘|ğ‘âˆˆ2{txinfo}}âˆª
{{glibc, g-shell, g-shell:nm, tzdata}âˆªğ‘|ğ‘âˆˆ2{txinfo}}âˆª
{{glibc, glibc:doc, glibc:v, txinfo, g-shell}}âˆª
{{glibc, glibc:doc, txinfo, g-shell, g-shell:nm, tzdata}}.
Here, the first line contains the products where glibc andg-shell do not
interact, i.e., either when they are not installed, or only one of them is
installed; the second line contains the products where both glibc and
g-shell are installed, but without use flags selected, so all optional
package can be freely selected; the third line contains the products
with the glibcâ€™s use flag docselected, so sys-apps/texinfo becomes
mandatory; the fourth line contains the products with the glibcâ€™s
use flag vanilla selected, so sys-libs/timezone-data is forbidden; the
fifth line contains the products with the g-shellâ€™s use flag vanilla
network manager, so sys-libs/timezone-data is mandatory; the sixth
line contains the product with glibcâ€™s both use flags selected and
the seventh line contains the product with glibcâ€™s use flag docand
g-shellâ€™s use flag networkmanager are selected.
4 PROBLEM STATEMENT
Many case studies show that the size of feature models used to
model real configuration spaces can be challenging for both hu-
mans and machines [ 12,51,55,59], including the feature model for
the source-based Linux distribution Gentoo [ 23] mentioned above.
The state-of-the-art strategy used to address this challenge is to
represent large feature models by sets of smaller interdependent
feature models [ 12,49]. The resulting interdependencies between
different feature models can be expressed using shared features [ 51].
The feature compatibility problem for a given set of features (see
Definition 3) can be decided without first composing the considered
feature models when the feature models are disjoint, as it suffices to
inspect each feature model independently. Namely, feature-model
slices can be used to formulate a feature-compatibility criterion for
the case with no shared features between the feature models, as
shown by the following theorem:
Theorem 1 (Feature-compatibility criterion for disjoint
FMs). Consider the feature models Mğ‘–=(Fğ‘–,Pğ‘–)(1â‰¤ğ‘–â‰¤ğ‘›)with
pairwise no shared features (i.e., 1â‰¤ğ‘–â‰ ğ‘—â‰¤ğ‘›impliesFğ‘–âˆ©Fğ‘—=âˆ…).
Then a configuration ğ‘is a pre-product of the feature model M=â€¢1â‰¤ğ‘–â‰¤ğ‘›Mğ‘–if and only if ğ‘is a subset ofÃ
1â‰¤ğ‘–â‰¤ğ‘›Fğ‘–and for allMğ‘–
the configuration ğ‘âˆ©Fğ‘–is a product of Î ğ‘(Mğ‘–).
Proof. LetM=(F,P).
Caseâ‡’.Sinceğ‘is a pre-product ofM, by definition there exist
ğ‘âˆˆ P such thatğ‘âŠ†ğ‘. Henceğ‘âŠ† F =Ã
1â‰¤ğ‘–â‰¤ğ‘›Fğ‘–. Let now
consider Î ğ‘(Mğ‘–)for any 1â‰¤ğ‘–â‰¤ğ‘›: by definition ğ‘âˆ©ğ‘âˆ©Fğ‘–is a
product of this feature model, and by construction, ğ‘âˆ©ğ‘âˆ©Fğ‘–=ğ‘âˆ©Fğ‘–.
Hence,ğ‘âˆ©Fğ‘–is a product of Î ğ‘(Mğ‘–)for any 1â‰¤ğ‘–â‰¤ğ‘›.
Caseâ‡.Since for any 1â‰¤ğ‘–â‰¤ğ‘›,ğ‘âˆ©Fğ‘–is a product of Î ğ‘(Mğ‘–),
there existğ‘ğ‘–âˆˆ Pğ‘–such thatğ‘âˆ©Fğ‘–=ğ‘ğ‘–âˆ©ğ‘. Let consider theconfiguration ğ‘=Ã
1â‰¤ğ‘–â‰¤ğ‘›ğ‘ğ‘–. Since the feature models Mğ‘–do not
share features, we have ğ‘ğ‘–âˆ©Fğ‘—=âˆ…=ğ‘ğ‘—âˆ©Fğ‘–for all 1â‰¤ğ‘–â‰ ğ‘—â‰¤ğ‘›.
Henceğ‘is a product ofM. Moreover, we have that:
ğ‘âˆ©ğ‘=Ã˜
1â‰¤ğ‘–â‰¤ğ‘›(ğ‘ğ‘–âˆ©ğ‘)=Ã˜
1â‰¤ğ‘–â‰¤ğ‘›(ğ‘âˆ©Fğ‘–)=ğ‘âˆ©Ã˜
1â‰¤ğ‘–â‰¤ğ‘›Fğ‘–=ğ‘.
Henceğ‘âŠ†ğ‘holds, which means that ğ‘is a pre-product of M.â–¡
Unfortunately, the feature compatibility criterion of Theorem 1
does not work for feature models with shared features. The problem
can be illustrated by the following example.
Example 6 (Feature compatibility with shared features).
Consider the two feature models Mglibc andMg-shell from Examples 2
and 5, and the configuration ğ‘={glibc, glibc:v, g-shell, g-shell:nm}.
We have
Î ğ‘(M glibc)=({glibc, glibc:v},2{glibc, glibc:v}),and
Î ğ‘(M g-shell)=({g-shell, g-shell:nm},2{g-shell, g-shell:nm}).
Here, we have that ğ‘âŠ† F glibcâˆªF g-shell and it is clear from the
previous equation that ğ‘âˆ©F glibc={glibc, glibc:v}is a product of
Î ğ‘(Mglibc)and thatğ‘âˆ©Fg-shell ={g-shell, g-shell:nm}is a product
ofÎ ğ‘(Mg-shell). However,ğ‘is not a pre-product of Mglibcâ€¢M g-shell ,
since the use flag g-shell:nm requires a timezone database to be
installed while the use flag glibc:v forbids it.
In this paper we address complete and efficient product discovery
in sets of interdependent feature models. To this aim, we define a
novel criterion which, given some selected features, enables solving
the product-discovery problem for a set of feature model fragments
with shared features, without composing all the fragments.
5 LAZY PRODUCT DISCOVERY
We are looking for a product-discovery criterion which works
for interdependent feature models, similar to how the feature-
compatibility criterion given in Theorem 1 works for disjoint feature
models. The solution lies in a novel criterion based on strengthen-
ing the feature model interfaces. Given feature models with shared
featuresMğ‘–=(Fğ‘–,Pğ‘–)and a set of selected features ğ‘, we need
feature model interfaces Mâ€²
ğ‘–that reflect how ğ‘is related to other
features inMğ‘–in order to guarantee that the interface behaves
similarly toMğ‘–with respect to the feature-compatibility problem
forğ‘. More formally, the interface Mâ€²
ğ‘–must satisfy the following
conditions:
(1)Î ğ‘(Mğ‘–)âª¯Mâ€²
ğ‘–; and
(2) the products of Mâ€²
ğ‘–are among the products of Mğ‘–.
Example 7 (Feature compatibility with shared features
continued). Consider feature models Mglibc andMg-shell and con-
figurationğ‘, as discussed in Example 6. Let ğ‘1={glibc, glibc:v}and
ğ‘2={glibc, tzdata, glibc:v}. We can see that the interface Mâ€²
glibc=
Î ğ‘2(Mglibc)ofMglibc satisfies (with ğ‘–=glibc ) conditions (1) and
(2) above. Since Î ğ‘(Mglibc)=Î ğ‘1(Mglibc)andğ‘2\ğ‘1={tzdata},
this shows that it is important to consider the feature tzdata when
checking whether ğ‘is a pre-product of a composed feature model
includingMglibc.
Let us now introduce terminology for different restrictions to
the interface relation that satisfy one or both of the conditions (1)
and (2) given above, and investigate some of their properties.
1513Definition 7 (FM extended slice, conservative interface,
and cut relations). Given a set of features ğ‘Œand two feature
modelsMâ€²=(Fâ€²,Pâ€²)andM=(F,P), we say that
(1)Mâ€²is an extended slice for ğ‘ŒofM, denotedMâ€²âª¯ğ‘ŒM, iff
Î ğ‘Œ(M)âª¯Mâ€²âª¯M holds;
(2)Mâ€²is aconservative interface ofM, denotedMâ€²âŠ´M, iff
bothMâ€²âª¯M andPâ€²âŠ†P hold; and
(3)Mâ€²is acut forğ‘ŒofM, denotedMâ€²âŠ´ğ‘ŒM, iffMâ€²is both
an extended slice for ğ‘Œand a conservative interface.
Note thatâŠ´âˆ…=âŠ´. The relationâŠ´is a partial order; the feature
model(âˆ…,âˆ…)is the minimum (i.e., the smallest w.r.t. both âª¯andâŠ´)
conservative interface of every void feature model; and the empty
feature modelMâˆ…is the minimum conservative interface of every
feature model that has the empty product.
The following theorem proves, in a constructive way, the ex-
istence of the minimum cut of Mforğ‘Œ, for any feature model
M=(F,P). Let the minimal products ofMbe the products that
are not included in other products, and let ğ‘Œâ€²=(Fâˆ©ğ‘Œ)be the set
of features ofMthat occur in ğ‘Œ. Intuitively, the minimum cut of
Mforğ‘Œis the feature model obtained from (ğ‘Œâ€²,âˆ…)by incremen-
tally adding all the minimal products of M(and their features) that
contain a feature occurring in the feature model, until a fixed point
is reached.
Theorem 2 (Characterization of the minimum cut). For all
setsğ‘Œof features and all feature models M=(F,P), letâŠ¥âŠ´ğ‘Œ(M)
be the minimum cut of Mforğ‘Œ, i.e.,
âŠ¥âŠ´ğ‘Œ(M)=minâŠ´{Mâ€²|Mâ€²âŠ´ğ‘ŒM}.
ThenâŠ¥âŠ´ğ‘Œ(M)=ğ‘“âˆ(((Fâˆ©ğ‘Œ),âˆ…)), whereğ‘“is the function between
feature models defined by
ğ‘“((F1,P1))=(F1âˆª(Ã˜
ğ‘âˆˆP2ğ‘),P1âˆªP 2)
withP2={ğ‘âˆˆP|âˆ€ğ‘â€²âˆˆP,(ğ‘â€²âŠŠğ‘)â‡’((ğ‘\ğ‘â€²)âˆ©F 1â‰ âˆ…)}.
Proof. LetMâ€²=((Fâˆ©ğ‘Œ),âˆ…)and consider the partially ordered
set of feature models (ğ‘†,â‰¤), defined by
-ğ‘†={(Fâ€²â€²,Pâ€²â€²)|(Fâˆ©ğ‘Œ)âŠ†Fâ€²â€²âŠ†Fâˆ§Pâ€²â€²âŠ†P} , and
-(F1,P1)â‰¤(F 2,P2)iff(F1âŠ†F 2)and(P1âŠ†P 2).
It is straightforward to see that (ğ‘†,â‰¤)is a complete lattice (with
minimumMâ€²and maximumM) and thatğ‘“is monotonic increasing
forâ‰¤. Hence, by [ 33],ğ‘“âˆ(Mâ€²)exists and is the minimum fixpoint
ofğ‘“.
We prove that the fixpoints of ğ‘“are exactly the cuts of Mforğ‘Œ.
Let us first consider a feature model Mğ‘Œ=(Fğ‘Œ,Pğ‘Œ)that is a cut
ofMforğ‘Œ. SinceMğ‘Œ=Î Fğ‘Œ(M) andPğ‘ŒâŠ†P for allğ‘âˆˆP, we
haveğ‘âˆ©Fğ‘ŒâˆˆP. This implies that for any ğ‘âˆˆP\Pğ‘Œ, there exists
ğ‘â€²âˆˆPwithğ‘â€²âŠŠğ‘such that(ğ‘\ğ‘â€²)âˆ©Fğ‘Œ=âˆ…. By definition, we
haveğ‘“(Mğ‘Œ)=(Ã
ğ‘âˆˆP2ğ‘âˆªFğ‘Œ,Pğ‘ŒâˆªP 2)with
P2={ğ‘âˆˆP|âˆ€ğ‘â€²âˆˆP,(ğ‘â€²âŠŠğ‘)â‡’((ğ‘\ğ‘â€²)âˆ©Fğ‘Œâ‰ âˆ…)}âŠ†Pğ‘Œ.
Henceğ‘“(Mğ‘Œ)=Mğ‘Œ.
Let us now consider a feature model Mâ€²
ğ‘Œ=(Fâ€²
ğ‘Œ,Pâ€²
ğ‘Œ)inğ‘†such
thatğ‘“(Mâ€²
ğ‘Œ)=Mâ€²
ğ‘Œ. First, it is clear by construction that Pâ€²
ğ‘ŒâŠ†P.
Moreover, if we write Pâ€²={ğ‘âˆˆP|âˆ€ğ‘â€²âˆˆP\{ğ‘},ğ‘â€²âŠˆğ‘}, it
is clear from the definition of ğ‘“thatPâ€²âŠ†Pâ€²
ğ‘Œ. Suppose that thesetğ‘€={ğ‘âˆˆP |ğ‘âˆ©Fâ€²
ğ‘Œâˆ‰Pâ€²
ğ‘Œ}is not empty and consider ğ‘1
a minimal element of ğ‘€w.r.t.âŠ†. Sinceğ‘1âŠˆFâ€²
ğ‘Œ, by definition of
Pâ€², the setğ‘={ğ‘â€²âˆˆPâ€²
ğ‘Œ|ğ‘â€²âŠ†ğ‘1}is not empty. Consider any
maximal element ğ‘2ofğ‘w.r.t.âŠ†. Sinceğ‘1âˆ©Fâ€²
ğ‘Œâˆ‰Pâ€²
ğ‘Œ, we have
(ğ‘1\ğ‘2)âˆ©Fâ€²
ğ‘Œâ‰ âˆ…, and so the condition âˆ€ğ‘â€²âˆˆP,(ğ‘â€²âŠŠğ‘1)â‡’
((ğ‘1\ğ‘â€²)âˆ©Fâ€²
ğ‘Œâ‰ âˆ…)holds. It follows that Mâ€²
ğ‘Œis not a fixpoint
ofğ‘“(since applying ğ‘“toMâ€²
ğ‘Œwould add the product ğ‘1), which
contradicts the hypothesis. Hence for all ğ‘âˆˆP,ğ‘âˆ©Fâ€²
ğ‘ŒâˆˆPâ€²
ğ‘Œ, this
means thatMâ€²
ğ‘Œ=Î Fâ€²
ğ‘Œ(M) . Since by construction ğ‘Œâˆ©F âŠ†Fâ€²
ğ‘Œ,
we have Î ğ‘Œ(M)âª¯Mâ€²
ğ‘Œâª¯M :Mâ€²
ğ‘Œis a cut ofMforğ‘Œ.
To conclude, observe that the orders âª¯andâ‰¤are equal on the
set of cuts ofMforğ‘Œ. Sinceğ‘“(Mâ€²)is the minimum fixpoint of ğ‘“
w.r.t.â‰¤, it is also the minimum cut of Mforğ‘Œ.â–¡
Example 8 (A minimum cut ofglibc FM). Consider the fea-
ture modelMglibc of Example 2 and ğ‘Œ={glibc, glibc:doc}. The
minimal cutâŠ¥âŠ´ğ‘Œ(Mglibc)can be computed by starting with the
feature model(ğ‘Œ,âˆ…)and then applying ğ‘“. In the first application
ofğ‘“, the setP2collects the products âˆ…,{glibc},{glibc:doc}, and
{glibc,glibc:doc,txinfo }. The setF1after the first application becomes
{glibc,glibc:doc,txinfo }and therefore, in the second application of
ğ‘“, the products{txinfo},{glibc, txinfo}, and{glibc:doc, txinfo}are
added toP2. At this point, further applications of ğ‘“do not add further
products.
In this case, the minimum cut âŠ¥âŠ´ğ‘Œ(Mglibc)is different from the
sliceÎ ğ‘Œ(Mglibc), since the cut keeps the information that when glibc
andglibc:doc are selected, then txinfo also has to be selected.
The following theorem proves sufficient criteria to guarantee
that a product of the composition of cuts is also a product of the
composition of the original feature models and, conversely, that
the original feature model does not have a product that contains
a given set of features. Intuitively, given a set of features ğ‘Œand
a productğ‘of the composition of cuts for ğ‘Œ, ifğ‘is a subset of ğ‘Œ
we have that ğ‘is also a product of the composition of the original
feature models. Moreover, if the composition of cuts for ğ‘Œhas no
products with the features in a set ğ‘âŠ†ğ‘Œ, then neither does the the
original feature model.
Theorem 3 (Product-discovery criterion for interdepen-
dent FMs). Consider a set ğ‘Œof features, a finite set ğ¼of indices, and
two sets of feature models {Mğ‘–=(Fğ‘–,Pğ‘–) |ğ‘–âˆˆğ¼}and{Mâ€²
ğ‘–=
(Fâ€²
ğ‘–,Pâ€²
ğ‘–) |ğ‘–âˆˆğ¼}such that for all ğ‘–âˆˆğ¼,Mâ€²
ğ‘–âŠ´ğ‘ŒMğ‘–. LetM=
(F,P)=â€¢ğ‘–âˆˆğ¼Mğ‘–andMâ€²=(Fâ€²,Pâ€²)=â€¢ğ‘–âˆˆğ¼Mâ€²
ğ‘–. Then
(1)each product ğ‘ofMâ€²such thatğ‘âŠ†ğ‘Œis a product ofM, and
(2)for each set of features ğ‘âŠ†ğ‘Œand for each product ğ‘ofMsuch
thatğ‘âŠ†ğ‘, there exists a product ğ‘ofMâ€²such thatğ‘âŠ†ğ‘âŠ†ğ‘.
Proof. (1) Consider a product ğ‘âˆˆPâ€². By construction, for every
ğ‘–âˆˆğ¼, there exists ğ‘ğ‘–âˆˆPâ€²
ğ‘–such thatğ‘=Ã
ğ‘–âˆˆğ¼ğ‘ğ‘–and, for allğ‘–,ğ‘—âˆˆğ¼,
ğ‘ğ‘—âˆ©Fâ€²
ğ‘–=ğ‘ğ‘–âˆ©Fâ€²
ğ‘–. By Definition 7, for all ğ‘–âˆˆğ¼, sinceğ‘ğ‘–âˆˆPâ€²
ğ‘–,
we have that ğ‘ğ‘–âˆˆPğ‘–. Let us now consider ğ‘–,ğ‘—âˆˆğ¼. We have that
ğ‘ğ‘–âˆ©Fğ‘—=ğ‘ğ‘–âˆ©ğ‘Œâˆ©Fğ‘—=ğ‘ğ‘–âˆ©Fâ€²
ğ‘—=ğ‘ğ‘—âˆ©Fâ€²
ğ‘–=ğ‘ğ‘—âˆ©ğ‘Œâˆ©Fğ‘–=ğ‘ğ‘—âˆ©Fğ‘–.
Hence,ğ‘=Ã
ğ‘–âˆˆğ¼ğ‘ğ‘–âˆˆP.
(2) By Definition 7, since Mâ€²
ğ‘–âª¯ğ‘ŒMğ‘–, we have Î ğ‘Œ(Mğ‘–)âª¯Mâ€²
ğ‘–.
Then, for all ğ‘–âˆˆğ¼, there exists ğ‘Œğ‘–such thatğ‘âŠ†ğ‘ŒâŠ†ğ‘Œğ‘–and
Î ğ‘Œğ‘–(Mğ‘–)=Mâ€²
ğ‘–. Consider a product ğ‘âˆˆP such thatğ‘âŠ†ğ‘. By
definition, for all ğ‘–âˆˆğ¼, there exists ğ‘ğ‘–âˆˆPğ‘–such thatğ‘=Ã
ğ‘–âˆˆğ¼ğ‘ğ‘–
1514and for allğ‘–,ğ‘—âˆˆğ¼, we haveğ‘ğ‘–âˆ©Fğ‘—=ğ‘ğ‘—âˆ©Fğ‘–. Letğ‘=Ã
ğ‘–âˆˆğ¼(ğ‘ğ‘–âˆ©ğ‘Œğ‘–).
Clearlyğ‘âŠ†ğ‘âŠ†ğ‘. Moreover, consider ğ‘–,ğ‘—âˆˆğ¼; sinceğ‘ğ‘–âˆ©Fğ‘—=ğ‘ğ‘—âˆ©Fğ‘–
holds, we have:(ğ‘ğ‘–âˆ©ğ‘Œğ‘–)âˆ©(Fğ‘—âˆ©ğ‘Œğ‘—)=(ğ‘ğ‘–âˆ©Fğ‘—)âˆ©(ğ‘Œğ‘–âˆ©ğ‘Œğ‘—)=
(ğ‘ğ‘—âˆ©Fğ‘–)âˆ©(ğ‘Œğ‘–âˆ©ğ‘Œğ‘—)=(ğ‘ğ‘—âˆ©ğ‘Œğ‘—)âˆ©(Fğ‘–âˆ©ğ‘Œğ‘–). Henceğ‘âˆˆPâ€².â–¡
Example 9 (Using the product-discovery criterion with
glibc and g-shell FMs). Consider the packages glibc andg-shell of
Example 5 and the set ğ‘Œ={glibc, glibc:v, tzdata}. It is easy to see
that the minimum cut of Mglibc forğ‘ŒisâŠ¥âŠ´ğ‘Œ(Mglibc)=(ğ‘Œ,2ğ‘Œ\ğ‘Œ)
because tzdata can not be selected when glibc andglibc:v are se-
lected. Now consider the package g-shell instead. The minimum cut of
Mg-shell forğ‘ŒisâŠ¥âŠ´ğ‘Œ(Mg-shell)=(ğ‘Œ,2ğ‘Œ). By the definition of fea-
ture model composition, we have that âŠ¥âŠ´ğ‘Œ(Mglibc)â€¢âŠ¥âŠ´ğ‘Œ(Mg-shell)
is the same asâŠ¥âŠ´ğ‘Œ(Mglibc).
Now, due to Theorem 3, we can for example derive that the product
{glibc, tzdata}that contains the shared feature tzdata is also a prod-
uct of the composition of Mglibc andMg-shell . Note that to discover
this fact, we avoided computing the composition of the entire feature
models and could ignore, e.g., features such as glibc:doc andg-shell.
The criteria provided by Theorem 3 allow us to prove that the
lazy product-discovery algorithm (Listing 1 in Section 2) is correct
and complete.
Theorem 4 (Soundness and completeness of lazy product
discovery). Given a finite set ğ¼of indices, a set of feature models
ğ‘†={Mğ‘–=(Fğ‘–,Pğ‘–)|ğ‘–âˆˆğ¼}such that all products of Mğ‘–are finite,
and a finite configuration ğ‘, the lazy product-discovery algorithm
(Listing 1) applied to ğ‘†andğ‘always finishes and returns a product ofâ€¢ğ‘–âˆˆğ¼Mğ‘–that contains ğ‘if and only if such a product exists.
Proof. Recall the definitions of auxiliary functions (Section 2):
(1)pick_cut(M,ğ‘Œ)=Mâ€²for someMâ€²s.t.Mâ€²âŠ´ğ‘ŒM,
(2)compose({M 1,...,Mğ‘›})=M1â€¢Â·Â·Â·â€¢Mğ‘›,
(3)select(M,ğ‘)is a product ofMcontaining all the features
inğ‘if such a product exists, None otherwise;
and the loop invariants Inv1 â€“Inv4 on Line 6. In Section 2 we have
already shown that the invariants Inv1 andInv2 hold, and that
the algorithm always finishes (because the set of examined features
ğ‘Œ, which strictly increases during each traversal of the while loop,
is bounded by(Ã
ğ‘–âˆˆğ¼Ã
ğ‘âˆˆPğ‘–ğ‘)âˆªğ‘, which is finite by hypothesis).
We can now conclude the proof by observing that the invariants
Inv3 andInv4 follow straightforwardly from Theorem 3(1) and
Theorem 3(2), respectively. â–¡
It is worth observing that a suitable structure of the feature mod-
els can enable a particular efficient implementation of the function
pick_cut(M,ğ‘Œ). For instance, if the feature-model Mis proposi-
tionally represented with a pair of the form (F,ğ‘“â†’ğœ“)(for some
set of featuresF, featureğ‘“âˆˆF and formula ğœ“) then, whenever
ğ‘“âˆ‰ğ‘Œ,pick_cut(M,ğ‘Œ)can return the feature model (ğ‘Œâ€²,2ğ‘Œâ€²)
withğ‘Œâ€²=ğ‘Œâˆ©F, which corresponds to the pair (ğ‘Œâ€²,true) in
propositional representation. Therefore, feature models of the form
(F,ğ‘“â†’ğœ“)such thatğ‘“âˆ‰ğ‘Œcan be filtered away before computing
the composition compose({pick_cut(M,ğ‘Œ)|Mâˆˆğ‘†})in Lines 4
and 8 of the algorithm.6 EVALUATION
With lazy product discovery, we aim to efficiently address the
product-discovery problem in huge configuration spaces, consist-
ing of hundreds of thousands of features in tens of thousands of
feature models. Therefore, we evaluate the performance of the lazy
product-discovery algorithm introduced in Section 2. The proposed
algorithm loads feature model fragments by need to examine spe-
cific features. A feature is loaded during a configuration process if
it occurs in one of the loaded feature model fragments. In contrast,
standard product-discovery algorithms (e.g., [ 41,43,59]) load all
the feature models before the product-discovery process starts.
We compare the number of loaded features, the time, and the
memory needed to solve a product-discovery problem using a lazy
and a standard product-discovery algorithm. In detail, we investi-
gate the following research questions:
RQ1. How is the number of loaded features affected by the choice of
a lazy or a standard product-discovery algorithm?
RQ2. How are the speed and memory consumption of product discov-
ery affected by the choice of a lazy or a standard product-discovery
algorithm?
In industrial practice, product-discovery tools are often opti-
mized for efficiency at the expense of completeness. As a conse-
quence, there may be product-discovery problems for which so-
lutions exist but no solution is found by the tool. We compare
the lazy product-discovery algorithm to one such state-of-the-art
tool by looking at the percentage of cases in which no product is
found by the state-of-the-art tool (although products exists), and at
the difference in performance for cases when the state-of-the-art
product-discovery tool return a correct answer (that is, it either
discovers a product or fails when there are no products). For this
purpose, we investigate the following research questions:
RQ3. How often does a state-of-the-art product-discovery tool fail
because of its incompleteness (i.e., the tool does not discover any
product, although there is at least one product)?
RQ4. Is lazy product discovery a feasible alternative to state-of-the-
art product-discovery tools in terms of execution time and memory
consumption?
6.1 Experimental Design and Subject
To answer these research questions, we performed experiments on
an industrial system with a huge configuration space. We chose
Gentoo, a source-based Linux distribution with highly-configurable
packages [ 23], which is among the largest fragmented feature mod-
els studied in the literature [ 37]. The experiments were performed
on the March 1st 2019 version of the distribution, that contained
36197 feature models with 671617 features overall.
There are no standard benchmarks for product reconfiguration
requests. Therefore, we constructed a set of 1000 product-discovery
problems for the evaluation. The problems were generated by ran-
domly selected a set of features (between one and ten) such that
each of these features requires the installation of a different pack-
age. Solving a product-discovery problem ğ‘in this context amounts
to computing a Gentoo product that includes any version of the
1515packages associated to the features in ğ‘and of other packages such
that that all dependencies are fulfilled.
We implemented the algorithm of Listing 1 as a tool. This tool,
called pdepa, targets Gentooâ€™s package dependencies, which are de-
fined using an ad-hoc syntax [ 22]. As shown in Example 1, Gentooâ€™s
dependencies can be encoded into feature models where features
represent both packages and configuration options (called use flags
in Gentoo). pdepa parses a package dependency and generates the
equivalent propositional formula representing the package feature
model. A particularity of Gentoo is that the feature model of a pack-
ageğ‘“can be translated into a propositional representation of the
form(F,ğ‘“â†’ğœ“), where a package selection feature ğ‘“represents
the package ğ‘“. The pdepa tool exploits this structure of the feature
model in the implementation of the key functions pick_cut and
compose by using the optimization discussed at the end of Section 5.
Specifically, pdepa can avoid loading the feature models of pack-
ages whose package selection feature is not in the set ğ‘Œof required
features, when composing cuts (Listing 1, Lines 4 and 8).
As its solving engine, pdepa uses the state-of-the-art SMT solver
Z3 [19], known for its performance and expressivity. Solvers such
as Z3 allow constraints to be added incrementally, reusing part of
the search done previously without always restarting the search
from scratch. This is extremely useful for composing cuts (Listing 1,
Lines 4 and 8) since the existing constraints can be reused, only
adding incrementally the new constraints not implied by the exist-
ing ones. Although this does not formally reduce the complexity
of the algorithm, which is NP-hard in the worst case,3in practice
these optimizations enable a significant speed-up.
To investigate the research question RQ 2, we need to compare
pdepa to a standard product-discovery algorithm. Unfortunately,
there is no off-the-shelf complete product-discovery tool for Gentoo
and therefore we implemented one to establish a baseline for our
experiments. We constructed a software that loads all the feature
models of all the Gentoo packages and then, as done by pdepa, calls
the SMT solver Z3 [ 19] to solve the configuration problem. We then
compared the results of pdepa to the corresponding results of this
baseline tool (baseline for short) in terms of computation time and
memory consumption. To ensure a fair comparison, we employ
a white-box evaluation, and both pdepa and the baseline use the
same implementation for translating the Gentoo dependencies and
for loading the feature models.
For research questions RQ 3 andRQ 4, we compare the results of
pdepa to the corresponding results of optimized, heuristics-based
product-discovery with emerge, the command-line interface to
Gentooâ€™s official package manager and distribution system Portage,
which is not complete (i.e., it fails to solve some product-discovery
problems that have solutions).
All experiments were performed on virtual machines provided
by the IaaS OpenStack cloud of the University of Oslo.4Every
virtual machine had 8 GB of RAM, 2 vCSPUs (2.5 GHz Intel Haswell
processors), and was running an Ubuntu 19.04 operating system.
The Gentoo operating system was virtualized by running Docker
and the image used for the experiments is publicly available.5
3The NP-hardness derives immediately from the NP-hardness of the problem of finding
a valid model for a propositional formula.
4https://www.uio.no/english/services/it/hosting/iaas/
5https://hub.docker.com/r/gzoumix/pdepa6.2 Results and Discussion
This section is organized according to research questions RQ1â€“
RQ4. To facilitate the discussion of the experiments, the figures
presenting the different results use a fixed ordering of the 1000
product-discovery problems we considered along the ğ‘¥-axis; this
ordering is determined by the number of features loaded by pdepa
during its computation for a given problem. Each of the 1000 ex-
periments was repeated 5 times for pdepa, for emerge and for the
baseline; Figures 1â€“5 report the mean values for each experiment.
RQ 1. Figure 1 shows the results of the experiments for research
question RQ 1 and reports on the number of features loaded by
pdepa to solve each product-discovery problem. To highlight how
lazy product discovery performs compared to standard product
discovery, which needs to load all features before the analysis can
start, these numbers are shown as the percentage of features from
the full feature model, for each of the product discovery problems.
The product-discovery problems have been sorted along the ğ‘¥-
axis according to this percentage. The figure shows the loaded
features as a full line, the mean number for all the product discovery
problems as a dashed line, and the standard deviation (abbreviated
to SD in the figures) as a the bar. We see that for the considered
product-discovery problems, the mean number of loaded features is
only 1.53% of the overall number of features. In summary, the gain in
loaded features when solving each of the considered 1000 product-
discovery problems using lazy product discovery over standard
product discovery is significant.
RQ 2. For research question RQ 2, we compared the speed and
memory consumption of product discovery when using pdepa and
the baseline on the defined product-discovery problems. For each
problem, pdepa loads parts of the FM and calls Z3 incrementally
(until a valid product for the whole FM is found), while the baseline
first loads the whole FM and then calls Z3.
Figure 2 shows the computation time for product discovery using
pdepa (green line) and Figure 3 shows the computation time for
product discovery using the baseline. The mean execution time for
the baseline is 949 seconds, compared to 78 seconds for pdepa. The
minimum and maximum execution times of the baseline are 861.9
and 1222.6 seconds, respectively. The standard deviation for the
baseline is negligible (around 35 seconds). It is worth mentioning
that about one third of the execution time is devoted to loading the
overall feature model, while the remaining time is taken by Z3. The
minimum and maximum execution time of pdepa are 1.7 and 155.22
seconds, respectively. The standard deviation is lower than the one
for the baseline, about 18 seconds. The maximum computation time
ofpdepa is less than one third of the computation time used by the
baseline to simply load the overall feature model, and it is about
the 16% of the minimum execution time of the baseline.
Figure 4 shows the memory consumption for product discovery
using pdepa (green line) and Figure 5 shows the memory con-
sumption for the baseline. The mean memory consumption for the
baseline is 3,919.4 MB, compared to 400.715 MB for pdepa. The
minimum and maximum memory consumption of the baseline are
3016 and 3980 MB, respectively. About 1 GB of the used memory
here is for the feature model itself. The standard deviation for the
baseline is negligible (about 70.84 MB). The 7 memory consumption
15160 200 400 600 800 1 ,0000.0211.2311.5341.837
loaded features SD
meanPercentage
Figure 1: Features loaded by pdepa.
0 200 400 600 800 1 ,0006.89712.56953.580
1.70059.33077.88796.444
1.226pdepa emerge
pdepa mean emerge mean
pdepa SD emerge SDSeconds
Figure 2: Execution times for pdepa andemerge.
0 200 400 600 800 1 ,000914.2949.2984.1
861.9baseline mean SDSeconds
Figure 3: Baseline execution time.
values that fall outside the standard deviation correspond to the
product discovery problems that have no solution. The minimum
and maximum memory consumption of pdepa are 73 and 620 MB,
respectively. The standard deviation, 67.38 MB, is about the same as
for the baseline. The maximum memory consumption of pdepa is
about 19.62% of the minimum memory consumption of the baseline.
The experiments show a clear correlation between the time and
the memory taken by pdepa to solve a product-discovery problem
and the number of features loaded by pdepa (cf. Figure 1).
In summary, the experiments clearly demonstrate that lazy prod-
uct discovery allows significant speed-up and significant reduction
of memory consumption, compared to standard product discovery.
RQ 3. We investigated the failures of a heuristics-based incom-
plete product-discovery tool (emerge) compared to the cases when
the complete lazy product discovery algorithm showed that no so-
lution exists, for the 1000 considered product-discovery problems.
Figure 6 shows the product-discovery problems for which emerge
does not find a product (red and blue bars). For the considered0 200 400 600 800 1 ,00077.793101.215186.000333.330400.715468.100
54.371pdepa emerge
pdepa mean emerge mean
pdepa SD emerge SDMB
Figure 4: Memory consumption for pdepa andemerge.
0 200 400 600 800 1 ,0003,848.63,919.4
3,016.0baseline
mean
SDMB
Figure 5: Baseline memory consumption.
0 200 400 600 800 1 ,0000.0001.000
no solution exists emerge failureNo solution
found
Figure 6: Product-discovery problems with no solution and
emerge failures.
product-discovery problems, emerge fails to find a valid config-
uration in 26.7% of the cases. In 0,7% of the cases (red bars), no
solution exists. Therefore, in 26% of the cases, emerge fails to solve
a product-discovery problem that has a solution. The experiments
show an interesting correlation between the failures of emerge
observed in Figure 6 and the number of features loaded by pdepa
during the product-discovery process: the failures of emerge occur
more frequently as the number of loaded features needed for lazy
product discovery increases. This can be seen since the sorting
of theğ‘¥-axis is the same in Figures 1 and 6. In summary, on 1000
randomly selected product-discovery problems, emerge fails to find
a solution that exists in around 26% of the cases.
RQ 4. For research question RQ 4, we investigated how well
pdepa performs as an alternative to the state-of-the-art configura-
tion tool emerge. Figure 2 shows the time for product discovery
using pdepa (green line) and emerge (blue line). The light green
and the light blue bars show the standard deviations and the corre-
spondingly colored dashed lines show the mean times in seconds
1517forpdepa andemerge, respectively. The difference in mean times
suggests that pdepa is 11.29 times slower than emerge in average,
which corresponds to 70 additional seconds. However, as the results
for RQ 3 above shows that emerge fails for a significant number of
the considered product-discovery problems, lazy product discovery
appears to be a feasible alternative to emerge.
Figure 4 shows the memory consumption for product discovery
using pdepa (green line) and emerge (blue line). The light green and
the light blue bars show the standard deviations and the correspond-
ing colored dashed lines show the mean memory consumption in
MB for pdepa andemerge, respectively. The difference in mean
times suggests that pdepa consumes four times more memory than
emerge in average (which amounts to around 300 MB).
In summary, lazy product discovery appears as a feasible al-
ternative to emerge if around one order of magnitude additional
computation time and four times additional memory consumption
are acceptable to always find products when these exist.
6.3 Threats to Validity
6.3.1 External Validity. The results of the evaluation strongly
depend on the product-discovery problems considered in the exper-
iments, i.e., on the feature models of the Gentoo packages identified
by the features in each product-discovery problem. Due to the lack
of standard benchmarks, we considered 1000 product-discovery
problems that were randomly selected from the 671617 features of
the March 1st 2019 version of the Gentoo distribution. The random
selection used the standard random python library [ 25], that allows
to get a set of elements uniformly chosen from a given set.
Different product-discovery problems could potentially lead to
different results. We plan to investigate other product-discovery
problems for Gentoo and for other domains to get more insights. In
particular, it would be interesting to investigate how lazy product
discovery performs when varying both the size and the amount of
interdependencies of the feature models (see Section 2).
6.3.2 Internal Validity. We used prototype implementations of
the lazy product-discovery algorithm and of the standard product-
discovery algorithm. Both implementations rely on the Z3 solver [ 19].
Z3 was chosen because it is a mature solver and freely available. The
standard product-discovery algorithm just performs a call to the
Z3 solver. The lazy product-discovery algorithm calls the Z3 solver
whenever a new feature fragment is loaded. Using a different solver
than Z3 may affect the execution time and memory consumption of
both the standard and the lazy product-discovery algorithms. We
plan to repeat the experiments using another solver.
Introducing optimizations in the lazy product-discovery algo-
rithm could potentially reduce the number of loaded features, the
execution time, and the memory consumption for the algorithm.
One possible optimization could be to pre-compute at compile time
the modal implication graphs [ 18,34] of features, which could po-
tentially avoid loading feature models that, e.g., are found to be
conflicting in the pre-analysis. Another possible optimization could
be the definition and usage of an ad-hoc search strategy for the
back-end solver, instead of using solverâ€™s default search strategy.
Another threat to validity is that Gentooâ€™s package dependencies
are not formally specified, but only given in a textual representation.
To reduce the probability of errors in the implementation of the lazyproduct-discovery algorithm, we have used unit tests to compare
the results of pdepa with known correct products. These unit tests
were performed by extending the package repository of portage
with custom testing and interdependent packages.
Possible bugs in Gentooâ€™s package manager may also be con-
sidered a threat to validity. When performing the experiments, we
identified the following surprising behavior in emerge:
(1)For some sets of packages6,emerge implements a heuristic
that only considers the feature model of the most recent
package in the set, thus forgetting possible solutions.
(2)Foremerge to consider a package, some part of its feature
model must be configured. Specifically, some of its features
must be selected or deselected such that the constraint iden-
tified by the variable REQUIRED_USE [ 22] evaluates to true.
(3)For a given product-discovery problem, the dependency anal-
ysis of emerge considers each package individually. This can
trigger the installation of a package in conflict with the rest of
the product-discovery problem, thus preventing the product-
discovery problem to be solved even if it has a solution.
We reported these issues to the Gentoo developer community, which
replied that they could be considered as bugs of emerge.
We were not able to install the Gentoo variants corresponding
to the products discovered by pdepa because of Bug (3) above.
Indeed, in many cases, emergeâ€™s dependency solver triggers the
installation of packages that conflict with pdepaâ€™s solution. We plan
to overcome this limitation by extending pdepa into a complete
package installation tool for Gentoo.
7 RELATED WORK
We discuss related work on interfaces, composition, and configura-
tion of feature models.
Interfaces of Feature Models. The feature-model cut in this paper
strengthens the feature-model interfaces introduced by SchrÃ¶ter
et al. [51], which, as pointed out in Section 3.2, are closely related
tofeature model slices introduced by Acher et al. [4]. In the work
of Acher et al. [4], the focus is on feature model decomposition. In
subsequent work [ 2], Acher et al.address evolutionary changes for
extracted variability models by using the slice operator in combi-
nation with a merge operator, and focus on detecting differences
between feature-model versions during evolution. Instead, SchrÃ¶ter
et al. [51] study how feature model interfaces can be used to support
evolution for a feature model composed from feature models frag-
ments. Changes to fragments which do not affect their interfaces
do not require the overall feature model to be rebuilt (by composing
the fragments) in order to reanalyze it. Challenges encountered
to support evolution in software product line engineering have
previously been studied by Dhungana et al. [20]. They use inter-
faces to hide information in feature model fragments and save a
merge history of fragments to give feedback and facilitate fragment
maintenance. No automated analysis is considered. In contrast to
this work on feature model interfaces for evolution, the cut in our
work is for efficient automated product discovery in huge feature
models represented as interdependent feature model fragments.
6These sets consisted of packages with an identical SLOT [ 22]. SLOTs are used in
portage to identify which versions of the same package can coexist in one system.
1518Feature-model views [30,39,50] focus on a subset of the relevant
features of a given feature model, similarly to feature-model inter-
faces. Different views regarding one master feature model are used
to capture the needs of different stakeholders, so that a product
of the master feature model can be identified based on the viewsâ€™
partial configurations. This work on multiple views to a product in
a feature model is orthogonal to our work on feature-model cuts,
which targets the efficient configuration of systems comprising
many interdependent configurable packages.
Composition of Feature Models. Feature-model composition is
often used for multi software product lines (i.e., sets of interdepen-
dent product lines) [ 29,35,37,47]. Eichelberger and Schmid [ 21]
provide an overview of textual-modeling languages which support
variability-model composition (like FAMILIAR [ 5], VELVET [ 49],
TVL [ 16], VSL [ 1]) and compare how they support composition,
modularity, and evolution. Acher et al. [6] compare different feature-
model composition operators by considering possible implementa-
tions and discuss advantages and drawbacks. For the investigation
of efficient automated configuration of huge feature models in this
paper, we use the propositional representation of feature models
and a composition operator that corresponds to logical conjunction.
Configuration of Feature Models. Product discovery (also called
product configuration or product derivation) is the process of select-
ing and deselecting features in a feature model in order to obtain
a product [ 26]. This is a central and widely studied problem in
the field of automated reasoning [ 9]; e.g., more than 50 different
methods for product discovery are discussed in a recent survey [ 26].
We are not aware of any method that addresses how complete
and efficient product-discovery can be achieved in configuration
spaces comprising different interdependent feature model frag-
ments without composing all the fragments. The tool for lazy prod-
uct discovery is in the class of product discovery tools which auto-
matically produce valid configurations.
Automated configuration is supported by a number of tools,
including FeatureIDE [ 59], GEARS [ 36], GUIDSL [ 8], IBED [ 60],
HyVarRec [ 40], SATIBEA [ 27] S2T2 Configurator [ 15], SIP [ 28],
SPL Conqueror [ 54], S.P.L.O.T. [ 43], and VariaMos [ 42]. However,
in contrast to our work, all these tools are eager and require the
building of the global feature model by composing all its fragments.
As such, these tools are in line with the standard product discovery
algorithm, as discussed in Section 6.
Some of these standard product discovery tools are interactive,
i.e., they support and interact with the user by guiding her in pro-
ducing a valid configuration or finding one that maximizes her
preferences [ 8,15,42,43]. Our method for lazy product discovery
can be exploited to support interactive product discovery either (i)
by requiring the user to enter preferences over different configura-
tions or (ii) by interacting with the user when deciding what partial
configuration should be extended (i.e., when the select function
of the algorithm in Listing 1 is performed). An extension of the lazy
product discovery algorithm in this direction is left as future work.
Different computational techniques can be used to solve the
product discovery problem: satisfiability solvers, constraint pro-
gramming, evolutionary algorithms, stochastic algorithms, or bi-
nary decision diagrams [ 9,10,46]. Due to the NP-hardness of the
configuration problem itself, most complete approaches rely onSAT solvers [ 31,44], but more recently, the use of more power-
ful backend solvers, such as constraint solvers and SMT solvers,
are starting to be explored for automatic configuration of feature
models [ 11,41,45,57]. In our work, we have used Z3 [ 19] which is
one of the most powerful and mature SMT solvers available today.
We would like to remark, however, that the lazy product discov-
ery method itself is orthogonal to the tool chosen, as long as the
backend solver allows to implement the pick_cut ,compose , and
select operations of Listing 1.
8 CONCLUSION AND FUTURE WORK
Product discovery in huge configuration spaces represented as sets
of interdependent feature models is challenging. Standard analysis
techniques for fragmented feature models require all the feature
models to be composed in order to apply the analysis. Recent work
has shown that several analyses of fragmented feature models can
be simplified using techniques such as feature model interfaces
and slicing, however these techniques do not work for product
discovery in sets of interdependent feature models.
In this paper, we introduce a method for automated product
discovery in configuration spaces represented as sets of interde-
pendent feature models. The method is lazy as features are added
incrementally to the analysis until a product is found. We introduce
and formalize the feature model cut, and leverage this concept to
define a product-discovery criterion. We exploit this criterion to
define a complete and efficient algorithm for lazy product discovery
in sets of interdependent feature models. We have evaluated the
potential of lazy product discovery on randomly constructed con-
figuration problems for the configuration space of the source-based
Linux distribution Gentoo, with 36197 interdependent feature mod-
els and a total of 671617 features. The evaluation has demonstrated
significant gains compared to standard product discovery and that
the trade-off of performance for completeness is reasonable com-
pared to the heuristics-based product-discovery with emerge, the
command-line interface to Gentooâ€™s official package manager and
distribution system Portage.
We are now investigating different optimizations of the current
prototype, such as the exploitation of modal implication graphs
pre-computed at compile time and the usage of ad-hoc SMT search
strategies. In future work we plan to investigate other product-
discovery problems for Gentoo as well as for other domains, to
gain more insights into lazy product discovery. While our results
make us confident that lazy product discovery is a viable method
for product discovery in huge configuration spaces, we believe
that it may also be used to complement optimized but incomplete
algorithms when these fail, such as emerge for Gentoo. We also
plan to investigate how lazy product discovery can be combined
with interactive product discovery.
ACKNOWLEDGMENTS
This work is partially funded by the Sirius Center for Scalable Data
Access and the Compagnia di San Paolo. We thank the reviewers
for constructive feedback, Thomas ThÃ¼m, Andrzej Wasowski and
Sven Apel for useful discussions on the topic of this paper, and
Simone Donetti for testing the publicly available artifact.
1519REFERENCES
[1]Andreas Abele, Yiannis Papadopoulos, David Servat, Martin TÃ¶rngren, and
Matthias Weber. 2010. The CVM Framework - A Prototype Tool for Composi-
tional Variability Management. In Proc. 4th International Workshop on Variabil-
ity Modelling of Software-Intensive Systems (VaMoS 2010) (ICB-Research Report),
Vol. 37. UniversitÃ¤t Duisburg-Essen, 101â€“105. http://www.vamos-workshop.net/
proceedings/VaMoS_2010_Proceedings.pdf
[2]Mathieu Acher, Anthony Cleve, Philippe Collet, Philippe Merle, Laurence
Duchien, and Philippe Lahire. 2014. Extraction and Evolution of Architectural
Variability Models in Plugin-based Systems. Software and Systems Modeling 13, 4
(Oct. 2014), 1367â€“1394. https://doi.org/10.1007/s10270-013-0364-2
[3]Mathieu Acher, Philippe Collet, Philippe Lahire, and Robert B. France. 2010.
Comparing Approaches to Implement Feature Model Composition. In Proc. 6th
European Conference on Modelling Foundations and Applications (ECMFA 2010),
Thomas KÃ¼hne, Bran Selic, Marie-Pierre Gervais, and FranÃ§ois Terrier (Eds.).
Springer, 3â€“19.
[4]Mathieu Acher, Philippe Collet, Philippe Lahire, and Robert B. France. 2011.
Slicing feature models. In Proc. 26th International Conference on Automated Soft-
ware Engineering (ASE 2011) . IEEE Computer Society Press, 424â€“427. https:
//doi.org/10.1109/ASE.2011.6100089
[5]Mathieu Acher, Philippe Collet, Philippe Lahire, and Robert B. France. 2013.
FAMILIAR: A domain-specific language for large scale management of feature
models. Science of Computer Programming 78, 6 (2013), 657â€“681. https://doi.org/
10.1016/j.scico.2012.12.004
[6]Mathieu Acher, BenoÃ®t Combemale, Philippe Collet, Olivier Barais, Philippe
Lahire, and Robert B. France. 2013. Composing Your Compositions of Variabil-
ity Models. In Proc. 16th International Conference on Model-Driven Engineer-
ing Languages and Systems (MODELS 2013), Ana Moreira, Bernhard SchÃ¤tz,
Jeff Gray, Antonio Vallecillo, and Peter J. Clarke (Eds.). Springer, 352â€“369.
https://doi.org/10.1007/978-3-642-41533-3_22
[7]Sven Apel, Don S. Batory, Christian KÃ¤stner, and Gunter Saake. 2013. Feature-
Oriented Software Product Lines: Concepts and Implementation. Springer.
[8]Don Batory. 2005. Feature Models, Grammars, and Propositional Formulas. In
Proc. 9th International Software Product Line Conference (SPLC 2005). Springer,
7â€“20.
[9]David Benavides, Sergio Segura, and Antonio Ruiz-CortÃ©s. 2010. Automated
analysis of feature models 20 years later: A literature review. Information Systems
35, 6 (2010), 615â€“636. https://doi.org/10.1016/j.is.2010.01.001
[10] David Benavides, Sergio Segura, Pablo Trinidad, and Antonio Ruiz-CortÃ©s. 2007.
FAMA: Tooling a framework for the automated analysis of feature models. In
Proc. 1st International Workshop on Variability Modelling of Software-Intensive
Systems (VaMoS 2007) (Lero Technical Report), Vol. 2007-01. 129â€“134.
[11] David Benavides, Pablo Trinidad, and Antonio Ruiz CortÃ©s. 2005. Using Con-
straint Programming to Reason on Feature Models. In Proc. 17th International
Conference on Software Engineering and Knowledge Engineering (SEKE 2005).
677â€“682. http://ksiresearchorg.ipage.com/seke/Proceedings/seke/SEKE2005_
Proceedings.pdf
[12] Thorsten Berger, Ralf Rublack, Divya Nair, Joanne M. Atlee, Martin Becker,
Krzysztof Czarnecki, and Andrzej WÄ…sowski. 2013. A survey of variability mod-
eling in industrial practice. In Proc. 7th International Workshop on Variability
Modelling of Software-Intensive Systems (VaMoS 2013), Stefania Gnesi, Philippe
Collet, and Klaus Schmid (Eds.). ACM Press, 7:1â€“7:8.
[13] Thorsten Berger, Steven She, Rafael Lotufo, Andrzej WÄ…sowski, and Krzysztof
Czarnecki. 2010. Variability modeling in the real: a perspective from the operating
systems domain. In Proc. 25th International Conference on Automated Software
Engineering (ASE 2010), Charles Pecheur, Jamie Andrews, and Elisabetta Di Nitto
(Eds.). ACM Press, 73â€“82.
[14] Marko BoÅ¡koviÄ‡, Gunter Mussbacher, Ebrahim Bagheri, Daniel Amyot, Dragan
GaÅ¡eviÄ‡, and Marek Hatala. 2010. Aspect-Oriented Feature Models. In Proc. Models
in Software Engineering - Workshops and Symposia at MODELS 2010, JÃ¼rgen Dingel
and Arnor Solberg (Eds.). Springer, 110â€“124.
[15] Goetz Botterweck, MikolÃ¡s Janota, and Denny Schneeweiss. 2009. A De-
sign of a Configurable Feature Model Configurator. In Proc. 3rd International
Workshop on Variability Modelling of Software-Intensive Systems (VaMoS 2009)
(ICB Research Report), Vol. 29. UniversitÃ¤t Duisburg-Essen, 165â€“168. http:
//www.vamos-workshop.net/proceedings/VaMoS_2009_Proceedings.pdf
[16] Andreas Classen, Quentin Boucher, and Patrick Heymans. 2011. A text-based
approach to feature modelling: Syntax and semantics of TVL. Science of Computer
Programming 76, 12 (2011), 1130 â€“ 1143. https://doi.org/10.1016/j.scico.2010.10.
005
[17] E. F. Codd. 1970. A Relational Model of Data for Large Shared Data Banks.
Commun. ACM 13, 6 (1970), 377â€“387. https://doi.org/10.1145/362384.362685
[18] Roberto Di Cosmo and JÃ©rÃ´me Vouillon. 2011. On software component co-
installability. In Proc. 19th Symposium on the Foundations of Software Engineering
(FSE-19) and 13th European Software Engineering Conference (ESEC-13). ACM
Press, 256â€“266. https://doi.org/10.1145/2025113.2025149[19] Leonardo MendonÃ§a de Moura and Nikolaj BjÃ¸rner. 2008. Z3: An Efficient SMT
Solver. In Proc. 14th International Conference on Tools and Algorithms for the
Construction and Analysis of Systems (TACAS 2008), C. R. Ramakrishnan and
Jakob Rehof (Eds.). Springer, 337â€“340.
[20] Deepak Dhungana, Paul GrÃ¼nbacher, Rick Rabiser, and Thomas Neumayer. 2010.
Structuring the modeling space and supporting evolution in software product
line engineering. Journal of Systems and Software 83, 7 (2010), 1108 â€“ 1122.
https://doi.org/10.1016/j.jss.2010.02.018
[21] Holger Eichelberger and Klaus Schmid. 2013. A Systematic Analysis of Textual
Variability Modeling Languages. In Proc. 17th International Software Product Line
Conference (SPLC 2013). ACM Press, 12â€“21. https://doi.org/10.1145/2491627.
2491652
[22] Gentoo Foundation. 2017. Package Manager Specification. Gentoo Foundation.
https://dev.gentoo.org/~ulm/pms/head/pms.html Last visited, 2019-08-20.
[23] Gentoo Foundation. 2019. Gentoo Linux. Gentoo Foundation. https://gentoo.org
Last visited, 2019-08-20.
[24] Gentoo Foundation. 2019. Portage - Gentoo Wiki. Gentoo Foundation. https:
//wiki.gentoo.org/wiki/Portage Last visited, 2019-08-20.
[25] Python Software Foundation. 2019. random â€” Generate pseudo-random numbers.
Python Software Foundation. https://docs.python.org/3/library/random.html
Last visited, 2019-08-20.
[26] JosÃ© A. Galindo, David Benavides, Pablo Trinidad, Antonio Manuel GutiÃ©rrez-
FernÃ¡ndez, and Antonio Ruiz-CortÃ©s. 2019. Automated analysis of feature mod-
els: Quo vadis? Computing 101, 5 (2019), 387â€“433. https://doi.org/10.1007/
s00607-018-0646-1
[27] Christopher Henard, Mike Papadakis, Mark Harman, and Yves Le Traon. 2015.
Combining Multi-Objective Search and Constraint Solving for Configuring
Large Software Product Lines. In Proc. 37th International Conference on Soft-
ware Engineering (ICSE 2015). IEEE Computer Society Press, 517â€“528. https:
//doi.org/10.1109/ICSE.2015.69
[28] Robert M. Hierons, Miqing Li, Xiaohui Liu, Sergio Segura, and Wei Zheng. 2016.
SIP: Optimal Product Selection from Feature Models Using Many-Objective
Evolutionary Optimization. ACM Transactions on Software Engineering and
Methodology 25, 2 (2016), 17:1â€“17:39. https://doi.org/10.1145/2897760
[29] Gerald Holl, Paul GrÃ¼nbacher, and Rick Rabiser. 2012. A systematic review and
an expert survey on capabilities supporting multi product lines. Information &
Software Technology 54, 8 (2012), 828â€“852. https://doi.org/10.1016/j.infsof.2012.
02.002
[30] Arnaud Hubaux, Patrick Heymans, Pierre-Yves Schobbens, and Dirk Deridder.
2010. Towards Multi-view Feature-Based Configuration. In Proc. 16th International
Working Conference on Requirements Engineering: Foundation for Software Quality
(REFSQ 2010), Roel J. Wieringa and Anne Persson (Eds.). Springer, 106â€“112.
[31] MikolÃ¡s Janota. 2008. Do SAT Solvers Make Good Configurators?. In Proc. 12th
International Software Product Line Conference (SPLC 2008) Workshops. Lero Int.
Science Centre, University of Limerick, Ireland, 191â€“195.
[32] Kyo Kang, Sholom Cohen, James Hess, William Novak, and A. Peterson. 1990.
Feature-Oriented Domain Analysis (FODA) Feasibility Study. Technical Report
CMU/SEI-90-TR-021. Software Engineering Institute, Carnegie Mellon University.
http://resources.sei.cmu.edu/library/asset-view.cfm?AssetID=11231
[33] S. C. Kleene. 1938. On notation for ordinal numbers. Journal of Symbolic Logic 3,
4 (1938), 150â€“155. https://doi.org/10.2307/2267778
[34] Sebastian Krieter, Thomas ThÃ¼m, Sandro Schulze, Reimar SchrÃ¶ter, and Gunter
Saake. 2018. Propagating Configuration Decisions with Modal Implication Graphs.
InProc. 40th International Conference on Software Engineering (ICSE 2018). ACM
Press, 898â€“909. https://doi.org/10.1145/3180155.3180159
[35] Charles W. Krueger. 2006. New Methods in Software Product Line Development.
InProc. 10th International Software Product Line Conference (SPLC 2006). IEEE
Computer Society Press, 95â€“102. https://doi.org/10.1109/SPLINE.2006.1691581
[36] Charles W. Krueger and Paul Clements. 2018. Feature-based systems and software
product line engineering with gears from BigLever. In Proc. 22nd International
Software Product Line Conference (SPLC 2018). ACM Press, 1â€“4. https://doi.org/
10.1145/3236405.3236409
[37] Michael Lienhardt, Ferruccio Damiani, Simone Donetti, and Luca Paolini. 2018.
Multi Software Product Lines in the Wild. In Proc. 12th International Workshop
on Variability Modelling of Software-Intensive Systems (VaMoS 2018). ACM Press,
89â€“96. https://doi.org/10.1145/3168365.3170425
[38] Rafael Lotufo, Steven She, Thorsten Berger, Krzysztof Czarnecki, and Andrzej
WÄ…sowski. 2010. Evolution of the Linux Kernel Variability Model. In Proc. 14th
International Software Product Line Conference (SPLC 2010), Jan Bosch and Jaejoon
Lee (Eds.). Springer, 136â€“150. https://doi.org/10.1007/978-3-642-15579-6_10
[39] Mike Mannion, Juha Savolainen, and Timo Asikainen. 2009. Viewpoint-Oriented
Variability Modeling. In Proc. 33rd International Computer Software and Ap-
plications Conference (COMPSAC 2009). IEEE Computer Society Press, 67â€“72.
https://doi.org/10.1109/COMPSAC.2009.19
[40] Jacopo Mauro, Michael Nieke, Christoph Seidl, and Ingrid Chieh Yu. 2016. Context
Aware Reconfiguration in Software Product Lines. In Proc. 10th International
Workshop on Variability Modelling of Software-intensive Systems (VaMoS 2016).
ACM Press, 41â€“48. https://doi.org/10.1145/2866614.2866620
1520[41] Jacopo Mauro, Michael Nieke, Christoph Seidl, and Ingrid Chieh Yu. 2018.
Context-aware reconfiguration in evolving software product lines. Science of
Computer Programming 163 (2018), 139â€“159. https://doi.org/10.1016/j.scico.2018.
05.002
[42] RaÃºl Mazo, Camille Salinesi, and Daniel Diaz. 2012. VariaMos: a Tool for Product
Line Driven Systems Engineering with a Constraint Based Approach. In Proc.
CAiSEâ€™12 Forum at the 24thInternational Conference on Advanced Information
Systems Engineering (CAiSE 2012) (CEUR Workshop Proceedings), Vol. 855. CEUR-
WS.org, 147â€“154.
[43] MarcÃ­lio MendonÃ§a, Moises Branco, and Donald D. Cowan. 2009. S.P.L.O.T.:
software product lines online tools. In Companion to the 24th Conference on
Object-Oriented Programming, Systems, Languages, and Applications (OOPSLA
2009). ACM Press, 761â€“762. https://doi.org/10.1145/1639950.1640002
[44] MarcÃ­lio MendonÃ§a, Andrzej WÄ…sowski, and Krzysztof Czarnecki. 2009. SAT-
based Analysis of Feature Models is Easy. In Proceedings of the 13th International
Software Product Line Conference (ACM International Conference Proceeding Series),
Dirk Muthig and John D. McGregor (Eds.), Vol. 446. ACM Press, 231â€“240.
[45] RaphaÃ«l Michel, Arnaud Hubaux, Vijay Ganesh, and Patrick Heymans. 2012.
An SMT-based approach to automated configuration. In Proc. 10th International
Workshop on Satisfiability Modulo Theories (SMT 2012) (EPiC Series in Computing),
Vol. 20. EasyChair, 109â€“119.
[46] Lina Ochoa, Juliana Alves Pereira, Oscar GonzÃ¡lez Rojas, Harold E. Castro, and
Gunter Saake. 2017. A survey on scalability and performance concerns in ex-
tended product lines configuration. In Proc. 11th International Workshop on Vari-
ability Modelling of Software-intensive Systems (VaMoS 2017). ACM Press, 5â€“12.
https://doi.org/10.1145/3023956.3023959
[47] Marko RosenmÃ¼ller and Norbert Siegmund. 2010. Automating the Configuration
of Multi Software Product Lines. In Proc. 4th International Workshop on Variabil-
ity Modelling of Software-Intensive Systems (VaMoS 2010) (ICB-Research Report),
Vol. 37. UniversitÃ¤t Duisburg-Essen, 123â€“130.
[48] Marko RosenmÃ¼ller, Norbert Siegmund, Christian KÃ¤stner, and Syed Saif Ur
Rahman. 2008. Modeling Dependent Software Product Lines. In Proc. Workshop
on Modularization, Composition and Generative Techniques for Product Line Engi-
neering (McGPLE). Department of Informatics and Mathematics, University of
Passau, 13â€“18.
[49] Marko RosenmÃ¼ller, Norbert Siegmund, Thomas ThÃ¼m, and Gunter Saake. 2011.
Multi-dimensional Variability Modeling. In Proc. 5th International Workshop on
Variability Modelling of Software-Intensive Systems (VaMoS 2011). ACM Press,
11â€“20. https://doi.org/10.1145/1944892.1944894
[50] Julia Schroeter, Malte Lochau, and Tim Winkelmann. 2012. Multi-perspectives on
Feature Models. In Proc. 15th International Conference on Model Driven Engineering
Languages and Systems (MODELS 2012), Robert B. France, JÃ¼rgen Kazmeier, RuthBreu, and Colin Atkinson (Eds.). Springer, 252â€“268.
[51] Reimar SchrÃ¶ter, Sebastian Krieter, Thomas ThÃ¼m, Fabian Benduhn, and Gunter
Saake. 2016. Feature-Model Interfaces: The Highway to Compositional Analyses
of Highly-Configurable Systems. In Proc. 38th International Conference on Software
Engineering (ICSE 2016). ACM Press, 667â€“678. https://doi.org/10.1145/2884781.
2884823
[52] Reimar SchrÃ¶ter, Thomas ThÃ¼m, Norbert Siegmund, and Gunter Saake. 2013.
Automated Analysis of Dependent Feature Models. In Proc. 7th International
Workshop on Variability Modelling of Software-Intensive Systems (VaMoS 2013),
Stefania Gnesi, Philippe Collet, and Klaus Schmid (Eds.). ACM Press, 9:1â€“9:5.
https://doi.org/10.1145/2430502.2430515
[53] Sergio Segura, JosÃ© A. Galindo, David Benavides, JosÃ© A. Parejo, and Antonio
Ruiz-CortÃ©s. 2012. BeTTy: Benchmarking and Testing on the Automated Analysis
of Feature Models. In Proc. 6th International Workshop on Variability Modelling of
Software-Intensive Systems (VaMoS 2012). ACM Press, 63â€“71. https://doi.org/10.
1145/2110147.2110155
[54] Norbert Siegmund, Marko RosenmÃ¼ller, Martin Kuhlemann, Christian KÃ¤stner,
Sven Apel, and Gunter Saake. 2012. SPL Conqueror: Toward optimization of
non-functional properties in software product lines. Software Quality Journal 20,
3-4 (2012), 487â€“517. https://doi.org/10.1007/s11219-011-9152-9
[55] Reinhard Tartler, Daniel Lohmann, Christian Dietrich, Christoph Egger, and
Julio Sincero. 2011. Configuration coverage in the analysis of large-scale system
software. Operating Systems Review 45, 3 (2011), 10â€“14.
[56] Reinhard Tartler, Daniel Lohmann, Julio Sincero, and Wolfgang SchrÃ¶der-
Preikschat. 2011. Feature consistency in compile-time-configurable system soft-
ware: facing the linux 10, 000 feature problem. In Proc. 6th European Conference
on Computer systems (EuroSys 2011), Christoph M. Kirsch and Gernot Heiser
(Eds.). ACM Press, 47â€“60.
[57] Thomas ThÃ¼m. 2018. . TU Braunschweig. https://github.com/FeatureIDE/
FeatureIDE/issues/836
[58] Thomas ThÃ¼m, Sven Apel, Christian KÃ¤stner, Ina Schaefer, and Gunter Saake.
2014. A Classification and Survey of Analysis Strategies for Software Product
Lines. ACM Comput. Surv. 47, 1 (2014), 1â€“45.
[59] Thomas ThÃ¼m, Christian KÃ¤stner, Fabian Benduhn, Jens Meinicke, Gunter Saake,
and Thomas Leich. 2014. FeatureIDE: An extensible framework for feature-
oriented software development. Science of Computer Programming 79 (2014),
70â€“85. https://doi.org/10.1016/j.scico.2012.06.002
[60] Yinxing Xue, Jinghui Zhong, Tian Huat Tan, Yang Liu, Wentong Cai, Manman
Chen, and Jun Sun. 2016. IBED: Combining IBEA and DE for optimal feature
selection in software product line engineering. Applied Soft Computing 49 (2016),
1215â€“1231. https://doi.org/10.1016/j.asoc.2016.07.040
1521