Log-based Anomaly Detection Without Log Parsing
Van-Hoang Le and Hongyu Zhang†
The University of Newcastle, NSW, Australia
vanhoang.le@uon.edu.au, hongyu.zhang@newcastle.edu.au
Abstract —Software systems often record important runtime
information in system logs for troubleshooting purposes. There
have been many studies that use log data to construct machinelearning models for detecting system anomalies. Through ourempirical study, we ﬁnd that existing log-based anomaly detec-tion approaches are signiﬁcantly affected by log parsing errorsthat are introduced by 1) OOV (out-of-vocabulary) words, and2) semantic misunderstandings. The log parsing errors couldcause the loss of important information for anomaly detection.To address the limitations of existing methods, we proposeNeuralLog, a novel log-based anomaly detection approach thatdoes not require log parsing. NeuralLog extracts the semanticmeaning of raw log messages and represents them as semanticvectors. These representation vectors are then used to detectanomalies through a Transformer-based classiﬁcation model,which can capture the contextual information from log sequences.Our experimental results show that the proposed approach caneffectively understand the semantic meaning of log messages andachieve accurate anomaly detection results. Overall, NeuralLogachieves F1-scores greater than 0.95 on four public datasets,outperforming the existing approaches.
Index T erms—Anomaly Detection, Log Analysis, Log Parsing,
Deep Learning
I. I NTRODUCTION
High availability and reliability are essential for large-
scale software-intensive systems [1], [2]. With the increasing
complexity and scale of systems, anomalies have becomeinevitable. A small problem in the system could lead to perfor-mance degradation, data corruption, and even a signiﬁcant lossof customers and revenue. Anomaly detection is, therefore,necessary for the quality assurance of complex software-intensive systems.
Software-intensive systems often generate console logs to
record system states and critical events at runtime. Engineerscan utilize log data to understand the system status, detectthe anomalies, and identify the root causes. As the amountof logs could be huge, anomaly detection based on manualanalysis of logs is time-consuming and error-prone. Overthe years, many data-driven methods have been proposed toautomatically detect anomalies by analyzing log data [3], [4],[5], [6], [7], [8], [9]. Machine learning-based methods (suchas Logistic Regression [10], Support Vector Machine [6],Invariant Mining [11]) extract log events and adopt supervisedor unsupervised learning to detect the occurrences of systemanomalies.
Recently, some deep learning-based approaches have been
proposed. For example, LogRobust [8] and LogAnomaly [12]adopt Word2vec model [13] to obtain embedding vectors oflog events, then applied an LSTM model to detect anomalies.
†Hongyu Zhang is the corresponding author.However, the existing approaches rely on log parsing to
preprocess semi-structured log data. Log parsers remove thevariable part from log messages and retain the constant partto obtain log events. To investigate the inaccuracy of logparsing, we have performed an empirical study on real-worldlog data. We ﬁnd that existing log parsers produce a noticeablenumber of parsing errors, which directly downgrade anomalydetection performance. The log parsing errors are mainly dueto the following two reasons: 1) The logging statements couldfrequently change during software evolution, resulting in newlog events that were not seen in training data; 2) Valuableinformation could be lost while parsing log messages into logevents, which may lead to misunderstanding of the semanticmeaning of log messages. Our empirical study also ﬁndsthat the log parsing errors can affect the follow-up anomalydetection task and decrease the detection accuracy.
To overcome the above-mentioned limitations of existing
approaches, we propose NeuralLog, a novel anomaly de-tection approach, which can achieve effective and efﬁcientanomaly detection on real-world datasets. Unlike the existingapproaches, NeuralLog does not rely on any log parsing, thuspreventing the loss of information due to log parsing errors.Each log message is directly transformed into a semantic vec-tor, which is capable of capturing both semantic informationembedded in log messages and the relationship between logmessages. Then, taking a sequence of semantic vectors asinput, a Transformer-based classiﬁcation model is applied todetect anomalies. The Transformer-based model with multi-head self-attention mechanism [14] can learn the contextualinformation from the log sequences in the form of vectorrepresentations. As a result, NeuralLog is effective for log-based anomaly detection.
We have evaluated the proposed approach using four public
datasets. The experimental results show that NeuralLog canunderstand the semantic meaning of log data and adequatelyhandle OOV words. It achieves high F1-scores (all greaterthan 0.95) for anomaly detection and outperforms the existinglog-based anomaly detection approaches.
The main contributions of this paper are as follows:
1) We perform an empirical study of log parsing errors.
We ﬁnd that existing log-based anomaly detection ap-
proaches are adversely affected by the log parsing errorsintroduced by the OOV words and semantic misunder-standing.
2) We propose NeuralLog, a novel deep learning-based
approach that can detect system anomalies without logparsing. NeuralLog utilizes BERT, a widely-used pre-trained language representation, to encode the semantic
4922021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)
DOI 10.1109/ASE51524.2021.000512021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE) | 978-1-6654-0337-5/21/$31.00 ©2021 IEEE | DOI: 10.1109/ASE51524.2021.9678773
978-1-6654-0337-5/21/$31.00  ©2021  IEEE
meaning of log messages.
3) We have evaluated NeuralLog using public datasets.
The results conﬁrm the effectiveness of NeuralLog for
representing log messages and detecting anomalies.
II. B ACKGROUND
A. Log Data
Large and complex software-intensive systems often pro-
duce a large amount of log data for troubleshooting purposesduring system operation. Log data records the system’s eventsand internal states during runtime. By analyzing logs, oper-ators can better understand systems’ status and diagnose thesystem when a failure occurs.
  
Fig. 1. An Example of HDFS Logs and Parsed Results
Figure 1 shows a snippet of raw logs generated by HDFS
(Hadoop Distributed File System). The raw log messagesare semi-structured texts, which contain header and content.
The header is determined by the logging framework andincludes information such as timestamp, verbosity level (e.g.,WARN/INFO), and component [15]. The log content consistsof a constant part (keywords that reveal the event template)and a variable part (parameters that carry dynamic runtimeinformation). Log parsing automatically converts each logmessage into a speciﬁc event template by removing parametersand keeping the keywords. The log events can be grouped intolog sequences (i.e., series of log events that record speciﬁc
execution ﬂows) according to sessions or ﬁxed/sliding timewindows [16].
B. Log Parsing Methods
Log parsing automatically converts each log message into a
speciﬁc event template by removing parameters and keeping
the keywords. For example, the log template “∗Served block
∗to∗”can be extracted from the ﬁrst log message in Figure
1. Here, “∗” denotes the position of a parameter.
There are many log parsing techniques, including frequent
pattern mining [17], [18], [19], clustering [20], [21], [22],language modeling [23], heuristics [24], [25], [26], etc. Theheuristics-based approaches make use of the characteristicsof logs and have been found to perform better than othertechniques in terms of accuracy and time efﬁciency [15].In this study, we evaluate four top-ranked parsers includeDrain [24], AEL [25], IPLoM [26] and Spell [27]. Theyutilize the characteristics of tokens (e.g., occurrences, po-sition, relation, etc.) and special structures (e.g., a tree) torepresent log messages and extract common templates. Drainapplies a ﬁxed-depth tree structure to represent log messagesand extracts common templates effectively. Spell utilizes thelongest common subsequence algorithm to parse logs in astream manner. AEL separates log messages into multiplegroups by comparing the occurrences between constants andvariables. IPLoM employs an iterative partitioning strategy,which partitions log messages into groups by message length,token position, and mapping relation. These log parsers arewidely used in existing studies and have proven their efﬁciencyon real-world datasets [15], [16], [28], [29].
C. Log-based Anomaly Detection
Over the years, many log-based anomaly detection ap-
proaches have been proposed. Some of them are based on
unsupervised learning methods, which require only unlabeleddata to train. For example, Xu et al. [7] employed Princi-pal Component Analysis (PCA) to generate two subspaces(normal space and anomaly space) of log count vectors. Ifa log sequence has its log count vector far from the normalspace, it is considered an anomaly. IM [11] and ADR [4]discover the linear relationships among log events from logcount vectors. Those log sequences that violate the relationshipare considered anomalies. There are also many supervisedanomaly detection approaches. For example, [30], [6], [10]represent log sequences as log count vectors, then appliedSupport Vector Machine (SVM), Logistic Regression (LR),and Decision Tree algorithm to detect anomalies, respectively.These approaches have many common characteristics. They allrequire a log parser to preprocess and extract log templatesfrom log messages. Then, the occurrences of log templatesare counted, resulting in log count vectors. Finally, a machinelearning model is constructed to detect anomalies. Figure 2shows an example of log sequence and log count vectors fromlog templatesproduced within Drain [24].
Log Sequence #3 
1 * Served block * to * 
2 * Served block * to * 
3 * Got exception while serving * to * 
…  Log Sequence #2 
1 * Served block * to * 
2 * Served block * to * 
3 * Got exception while serving * to * 
…  Log Sequence #1 
 Count Vector #3 
[4, 2, 0, …, 3] Count Vector #2 
[4, 2, 0, …, 3] Count Vector #1 
Fig. 2. An Example of Log Sequence, and Log Count Vector
In recent years, many deep learning-based models have
been proposed to analyze log data and detect anomalies [5],[12], [8]. For example, DeepLog [5] ﬁrst applies the Spell[27] parser to extract log templates. Then, it leverages theindexes of log templates and inputs them to an LSTM modelto predict the next log templates. Finally, DeepLog detectsanomalies by determining whether or not the incoming logtemplates are unexpected. LogAnomaly [12] uses log countvector to detect the anomalies reﬂected by anomalous logevent numbers. It proposes a synonyms and antonyms based
493method to represent the words in log templates. LogRobust [8]
incorporates a pre-trained Word2vec model, namely FastText[31], and combines with TF-IDF weight [32] for learning therepresentation vectors of log templates, which are generatedby Drain [24]. Then, these vectors input an Attention-basedBi-LSTM model to detect anomalies. Due to the imperfectionof log parsing, the above methods tend to the lose semanticmeaning of log messages, thus leading to inaccurate detectionresults.
III. A
NEMPIRICAL STUDY OF LOGPARSING ERRORS
In this section, we describe an empirical study on the
problem of existing log parsers and their impact on log-based anomaly detection. We use two public datasets in ourstudy, namely Blue Gene/L (BGL) [33], [34] and Thunderbird[33], [34]. The datasets are collected between 2004 and 2006from real-world supercomputing systems [33] and consist of14,672,653 log messages in total, among which 353,394 logmessages are manually labeled as anomalies.
A. Log Parsing Errors Introduced by OOV Words
During development and maintenance, developers can add
new log statements to source code and modify the content of
existing log statements. Besides, runtime information can beadded to log messages as parameters to record system status.As a consequence, new words, i.e., out-of-vocabulary (OOV)words, frequently appear in log data. For example, for a logmessage “memory manager address parity error” in BGL,
if the word “parity” did not appear in historical logs, it isan OOV word. To determine OOV words, we ﬁrst sort logmessages by the timestamps of logs and leverage the frontP% (according to the timestamps of logs) as the training data
and the rest as the testing data. Then, we split each training logmessage into a set of tokens by the whitespace character andbuild a vocabulary from these tokens. OOV words are those
words in testing data that do not exist in the vocabulary. Inthis section, we increase the percentages of training data from20% to 80%. Then we calculate the proportion of OOV wordsin all the splits.
To facilitate understanding, we use the BGL data at the
60/40 splitting (the ﬁrst 60% of the BGL dataset is used fortraining, and the rest is for testing) to explain in detail theanalysis of OOV words. The training set contains 153,786unique words, and the testing set contains 384,730 uniquewords. Among the unique words in the testing set, 362,123words (94.12%) are unseen in the training set. These OOVwords only concentrate in a small subset of logs (i.e., 8.51%of the testing set, which is 160,403 out of 1,885,398 logmessages). These OOV words result in 1,304 unseen logevents (i.e., log templates with OOV words) in the testingset, accounting for 86.59% of the total number of log eventsin the testing set.
Figure 3 shows the percentages of OOV words in testing
data when the percentages of training data increase from20% to 80% on BGL and Thunderbird datasets. On the BGLdataset, there are always more than 80% of words in the testingset that are unseen in the training set. On the Thunderbirddataset, with the growth of training data, the proportion ofOOV words in the testing set is gradually decreasing. However,when we use 80% Thunderbird logs to train the model, we stillﬁnd 30.4% of OOV words in the testing set.
20% 40% 60% 80%00.20.40.60.81
Percentage of training setPercentage of OOV words in testing set
BGL
Thunderbird
Fig. 3. Analysis of OOV words in log messages in public datasets
Next, we evaluate the number of log messages (log lines)
and log templates that contain OOV words. The percentage of
log messages containing OOV words is shown in Figure 4(a).It can be seen that the percentage of log messages containingOOV words in the testing set of both BGL and Thunderbirddatasets is small. When we use 80% logs to train the model,we ﬁnd only 6.7% and 1.7% log messages containing OOVwords in the testing set of the BGL and Thunderbird dataset,respectively.
20% 40% 60% 80%00.10.20.3
Percentage of training setPercentage of log messages
with OOV words in testing setBGL
Thunderbird
(a) Log messages with OOV words20% 40% 60% 80%00.20.40.60.81
Percentage of training setPercentage of log templates
with OOV words in testing setBGL
Thunderbird
(b) Log templates with OOV words
Fig. 4. Analysis of log with OOV words
Figure 4(b) shows the percentages of log templates (pro-
duced by Drain [24]) that contains OOV words on BGL
and Thunderbird datasets, as the percentage of training dataincreases from 20% to 80%. We observe that all testing setshave log templates with OOV words, even when trained with80% of the data. The proportion of log templates containingOOV words on the BGL dataset is always more than 80%, nomatter how much data is used for training. The percentage oftemplates containing OOV words on the Thunderbird datasetdecreases with the growth of training data, but still, more than60% when 80% of data is trained.
The results show that a small number of log messages
containing OOV words can produce many unseen log eventsin the testing set. There are three main reasons for this ﬁnding:
494•Many log events only appear during a speciﬁc period [5].
For example, there are 842 events that only appear in thelast 20% of logs, in the 80/20 splitting.
•The distribution of log events is imbalanced. For exam-ple, the event “generating ∗”appears in 1,706,751 log
messages (35.95% of the dataset), while others such as“memory manager ∗buffer∗”only appear less than 100
times.
•OOV words can cause log parsing errors and lead to manyextra log events. These extra log events usually appear afew times but still make up a majority of log templates.For example, 1,165 log events only appear once in theBGL dataset.
Our ﬁnding indicates that anomaly detection methods based
only on log events could lead to many inaccurate detectionresults. For example, SVM [30] and LR [6], which transformlog sequences into log count vectors, cannot take new logevents as input because the dimension of log count vectorsis ﬁxed (i.e., the number of original log events). Moreover,DeepLog [5], using the indexes of log templates to predictthe next log event, considers all new log events as anomaliesbecause they cannot be predicted by the model.
B. Log Parsing Errors Introduced by Semantic Misunder-
standing
We identify two main cases of parsing errors that are
introduced by semantic misunderstanding:
•Case 1: Misidentifying parameters as keywords.
•Case 2: Misidentifying keywords as parameters.
   Case 1: 
    Case 2: - Parsing results: 
L3 ecc status register: 00200000              Æ L3 ecc status  register: * 
L3 global control register: 0x001249f0   Æ L3 * control register: *  
- Ground truth Log Template: 
L3 * * register: * 
- Parsing results: 
machine check enable        Æ     machine check ∗ 
machine check interrupt    Æ     machine check ∗ 
- Ground truth Log Templates: 
machine check enable  
machine check interrupt 
Fig. 5. Examples of Log Parsing Error (Drain)
For Case 1, the parameters in log messages are misidentiﬁed
as keywords and included in the log templates produced by the
log parsers, thus leading to many extra log events. We comparethe parsed template of each log message with the ground truthof the BGL dataset. If a template contains more keywords thanthe ground truth, it is considered wrongly parsed and an extralog event. The two log messages in Case 1 in Figure 5 onlyrefer to one log template but are parsed into two different logtemplates. Figure 6 shows the percentages of extra log eventsproduced by the four log parsers on two datasets. For example,there are about 80% extra log events on the BGL dataset and72% extra log events on the Thunderbird dataset using Drain.BGL Thunderbird00.20.40.60.81
0.80
0.720.98
0.140.79 0.81
0.53 0.53PercentageDrain Spell AEL IPLoM
Fig. 6. Percentages of extra log events produced by four log parsers
For Case 2, some essential keywords in log messages
could be removed after log parsing, resulting in different log
messages being parsed into one log event. Figure 5 shows anexample of this case. Two different log messages are parsedinto the same log event “machine check ∗”. However, one
indicates a normal behavior (i.e. “machine check enable”),
while the other indicates a system anomaly (i.e., “machine
check interrupt”). The errors of this type make the detectionmodel difﬁcult to distinguish between normal or abnormallogs based only on log events. Figure 7 shows examples ofCase 2 parsing errors introduced by the four log parsers. Eachexample shows one normal log and one abnormal log whichare parsed into the same log event. Valuable information suchas the reason for login failure (i.e., Figure 7(a)) is missingfrom log events and leads to many wrong detection results.


-Anomaly: ciod: LOGIN chdir(/p/gb1/stel-
la/RAPTOR/2183) failed: Input/output error
- Normal: ciod: LOGIN chdir(/home/berts-
ch2/src/bgl hello) failed: Permission denied
- Parsed event: ciod: LOGIN ∗failed:∗∗
(a) Errors introduced by Drain

-Anomaly: mptscsih: ioc0: attempting task
abort! (sc=00000101bfc7a480)
- Normal: mptscsih: ioc0: task abort: SUC-
CESS (sc=00000101bfc7a480)
- Parsed event: mptscsih ioc0 ∗∗∗ sc∗
(b) Errors introduced by Spell


-Anomaly: ﬂoating point unavailable inter-
rupt
- Normal: ﬂoating point instr. enabled.....1
- Parsed event: ﬂoating point ∗∗
(c) Errors introduced by AEL

-Anomaly: ciod: Error creating node map
from ﬁle map.dat: No child processes
- Normal: ciod: Error creating node map
from ﬁle map.dat: Bad ﬁle descriptor
- Parsed event: ciod Error creating node map
from ﬁle∗∗∗∗
(d) Errors introduced by IPLoM
Fig. 7. Examples of Valuable Information Removed by Log Parser
Table I shows examples of wrongly parsed log templates by
the Drain parser [24]. For instance, on the BGL dataset, there
are 6,541 log messages that have the template “ﬂoating point
∗∗”. However, only log messages in the form of “ﬂoating
point unavailable interrupt” are labeled as anomalies, while
others (such as “ﬂoating point instr . enabled” or“ﬂoating
point alignment exceptions”) indicate normal system behavior.Similarly, Drain also produces 899 log messages that have thelog templates indicating both normal and abnormal states onthe Thunderbird dataset.
We also observe similar results for other log parsers. On
BGL, the numbers of misidentiﬁed log messages producedby Spell, AEL, and IPLoM are 58,228, 20,154, and 31,298,
495respectively. On Thunderbird, the numbers of misidentiﬁed
log messages produced by Spell, AEL, and IPLoM are 3,851,1,463, and 5,687, respectively.
TABLE I
EXAMPLES OF LOG PARSING ERRORS INTRODUCED BY DRAIN
#Anom. Log Template Occu.
BGL 348,460ﬂoating point ∗∗ 6,541
machine check ∗ 6,594
ciod: Error creating node map from ﬁle ∗∗∗∗ 2,952
ciod: LOGIN ∗failed:∗∗ 1,289
Thunderbird 4,934mptscsih: ioc0: attempting ∗∗∗ 771
EXT3-fs error (device ∗∗∗∗∗aborted) 121
Out of Memory: Killed process ∗∗ 7
Note: #Anom. denotes the number of anomalies. Occu. denotes the number
of occurrences of the log template.
C. The Impact of Log Parsing Errors on Anomaly Detection
The existing approaches share a common process: they all
utilize a log parser to parse the log messages into log events(i.e., the templates of log messages), construct log sequences,and then build unsupervised or supervised machine learningmodels to detect anomalies. The existing approaches can beadversely affected by the log parsing errors introduced by theOOV words and semantic misunderstanding.
In this section, we evaluate the impact of log parsing errors
on two representative anomaly detection methods SVM-basedmethod [30], and LogRobust [8]. SVM represents those ML-based approaches that use the log count vectors as input.LogRobust represents recent DL-based approaches that utilizethe semantic vectors of log templates as input. They bothuse a log parser to generate a set of log events. SVM-based method [30] represents log sequences as log countvectors and then constructs a hyperplane to separate normaland abnormal samples in a high-dimension space. LogRobust[8] incorporates a pre-trained Word2vec model [31] to learnthe semantic vectors of log templates instead of countinglog events’ occurrences. Using the Word2vec model allowsLogRobust to discover the semantic relationship between logevents and handle the instability of log data.
Figure 8 shows the results of the SVM-based method and
LogRobust with four different log parsers (i.e., Drain [24],Spell [27], AEL [25], and IPLoM [26]). We observe thatthe performance of current anomaly detection methods isaffected by the accuracy of log parsing, and different logparsers could lead to different results. SVM achieves betterresults when using Drain [24] and AEL [25] since theseparsers produce a smaller amount of inaccurate log events,as discussed in Section III-B. Figure 8(a) and Figure 8(c)show that SVM with Drain achieves an F1-score of 0.46 and0.50 on BGL and Thunderbird datasets, respectively. WhileSVM with Spell only produces F1-scores of 0.29 on theBGL dataset and 0.17 on the Thunderbird dataset. LogRobustcan achieve better results than SVM since LogRobust canidentify unstable log events with similar semantic meaningthrough semantic vectorization. Still, LogRobust suffers fromthe log parsing errors caused by semantic misunderstanding(see Section III-B) and achieves F1-scores of less than 0.8 onboth datasets.Precision Recall F-measure00.20.40.60.810.97
0.300.460.99
0.170.290.98
0.310.470.98
0.300.46AccuracyDrain Spell AEL IPLoM
(a) SVM on BGLPrecision Recall F-measure00.20.40.60.810.630.96
0.76
0.620.97
0.75
0.610.95
0.74
0.520.96
0.67AccuracyDrain Spell AEL IPLoM
(b) LogRobust on BGL
Precision Recall F-measure00.20.40.60.81
0.340.91
0.50
0.100.90
0.170.330.83
0.48
0.110.84
0.19AccuracyDrain Spell AEL IPLoM
(c) SVM on ThunderbirdPrecision Recall F-measure00.20.40.60.810.610.78
0.680.85
0.520.650.81
0.390.530.720.750.73AccuracyDrain Spell AEL IPLoM
(d) LogRobust on Thunderbird
Fig. 8. Results of anomaly detection with different log parsers on BGL and
Thunderbird datasets
Next, we manually ﬁx the errors produced by log parsing
methods on the BGL dataset, then apply SVM and LogRobust
to conﬁrm whether or not the accuracy of anomaly detectionmethods is improved if log parsing performs more accurately.We leverage the ground truth log templates for the BGL datasetfrom [35]. For each wrongly parsed log message, we match itwith the most similar log template in the ground truth. Afterthe ﬁxing process, the outputs produced by the log parser areactually the ground truth. Also, the outputs of different logparsers are the same after ﬁxing (as they are all the sameas the ground truth). Figure 9 shows the accuracy (measuredin terms of F-measure) of each individual parser before andafter ﬁxing the log parsing errors. We can see that both SVMand LogRobust perform better when the log parsing errors areﬁxed (on average, 25% improvement for LogRobust and 29%improvement for SVM).
Drain Spell AEL IPLoM00.20.40.60.81
0.46
0.290.47 0.460.54 0.54 0. 54 0. 54F-measureBefore After
(a) Accuracy of SVMDrain Spell AEL IPLoM00.20.40.60.810.76 0.75 0.74
0.670.91 0.91 0.91 0.91F-measureBefore After
(b) Accuracy of LogRobust
Fig. 9. The accuracy of anomaly detection before and after ﬁxing the log
parsing errors on the BGL dataset
Overall, the results show that log parsing accuracy affects
the performance of anomaly detection. Existing log parsing
methods cannot handle well the OOV words in new logs,
496thus losing semantic information while detecting anomalies.
Furthermore, current log parsing methods could produce er-rors due to semantic misunderstanding. Therefore, existinganomaly detection methods that leverage log events are unableto achieve satisfying results due to the imperfections of logparsing methods.
IV . N
EURAL LOG:LOG-BASED ANOMALY DETECTION
WITHOUT LOGPARSING
To overcome the limitation of existing approaches, we
propose NeuralLog, a new log-based anomaly detection ap-proach that directly uses raw log messages to detect anomalies.The overview of the proposed approach is shown in Figure10. Overall, NeuralLog consists of three steps: preprocessing(Section IV-A), neural representation (Section IV-B), andtransformer-based classiﬁcation (Section IV-C). The ﬁrst stepis log preprocessing. After that, each log message is encodedinto a semantic vector by using BERT. In this way, ourapproach can prevent the loss of valuable information from logmessages. Finally, we leverage the Transformer [14] model todetect the anomalies.
 
 
 History 
Logs New Logs 
 
 
1. Preprocessing  
Raw log messages 
… 
݅ .081109 205931 13 INFO dfs.DataBlockScanner: 
Verification succeeded for blk_4980916519894289 
… 
Contents 
… 
݅ .info dfs datablockscanner  
verification succeeded for  
… 
Remove number, 
punctuation, …  
 
 
 
2. Neural Representation  
Contents 
… 
݅ .info dfs datablockscanner  
verification succeeded for  
… 
Semantic Vectors 
… ݔ
௜. [-0.160, -0.590, 0.206, 
0.166, ...]  
… 
WordPiece Tokenization 
BERT Encoder 
 
 
3. Transformer-based Classification  
(ݔଵ,ݔଶ,…,ݔ௡) 
Transformer 
Encoder 
Anomaly? 
Pooling & Dropout 
Linear 
Softmax 
(݌ଵ,݌ଶ,…,݌௡) 
Positional 
Embeddings Train Predict 
Fig. 10. An Overview of NeuralLog
A. Preprocessing
Preprocessing log data is the ﬁrst step for building our
model. In this step, we ﬁrst tokenize a log message into
a set of word tokens. We use common delimiters in thelogging system (i.e., white space, colon, comma, etc.) to splita log message. Then, every capital letter is converted to alower letter, and we remove all non-character tokens from theword set. These non-characters contain operators, punctuationmarks, and number digits. This type of tokens is removedsince it usually represents variables in the log message andis not informative. As an example, the raw log message“081109 205931 13 INFO dfs.DataBlockScanner: V eriﬁcationsucceeded for blk
-4980916519894289629” is ﬁrst split into a
set of words based on common delimiters. Then non-charactertokens are excluded from the set. Finally, a set of words {info,
dfs, datablockscanner , veriﬁcation, succeeded }is obtained.
B. Neural Representation
Each log message records a system event with its header
and message content. The message header contains ﬁeldsdetermined by the logging framework, such as component andverbosity level. The message content written by developersreﬂects a speciﬁc state of the system. Existing methods usuallyanalyze only message content and remove other information.In this paper, NeuralLog uses all textual information suchas verbosity, component, and content to extract the semanticmeaning of log messages. In order to reserve semantic in-formation and capture relationships among existing and newlog messages, the representation phase tries to represent logmessages in the vector format.
1) Subword Tokenization: Tokenization can be considered
as the ﬁrst step to handle OOV words. In our work, we adoptthe WordPiece tokenization [36], [37], which is widely usedin many recent language modeling studies [38], [39], [40].
WordPiece includes all the characters and symbols into
its base vocabulary ﬁrst. Instead of relying on the frequencyof the pairs, WordPiece chooses the one that maximizes thetraining data’s likelihood. It trains a language model startingfrom the base vocabulary and picks the pair with the highestlikelihood. This pair is added to the vocabulary, and thelanguage model is again trained on the new vocabulary. Thesesteps are repeated until the desired vocabulary is reached. Forexample, the rare word “datablockscanner” is split into more
frequent subwords: {“data”, “block”, “scan”, “ner”}. In this
way, the number of OOV words is reduced and their meaningsare captured.
The reason we choose WordPiece is that it can effectively
handle the OOV words and reduce the vocabulary size. Com-pared with other tokenization (chunking) approaches, Word-Piece is more effective. For example, space/stemming/camelcase based tokenization strategies can lead to many OOVwords and a big vocabulary [41].
2) Log Message Representation: After preprocessing and
tokenization, NeuralLog transforms each log message into aset of words and subwords. Conventionally, words of log con-tent are further transformed into vectors by using Word2Vec[42], then the representation vector of each sentence wouldbe calculated based on the word vectors. However, Word2Vecproduces the same embedding for the same word. In manycases, a word can have different meanings based on itsposition and context. BERT [38] is a recent deep learningrepresentation model that has been pre-trained on a hugenatural language corpus. In our work, we employ the featureextraction function of pre-trained BERT to obtain the semanticmeaning of log messages.
More speciﬁcally, after tokenizing, the set of words and
subwords is passed to the BERT model and encoded intoa vector representation with a ﬁxed dimension. NeuralLogutilizes the BERT base model [43] that contains 12-layers oftransformer encoder and 768-hidden units of each transformer.
497Each layer generates embeddings for each subword in a log
message. We use the word embeddings generated by the lastencoder layer of BERT in our work. Then, the embedding of alog message is calculated as the average of its correspondingword embeddings. As any word that does not occur in thevocabulary (i.e., OOV words) is broken down into subwords,BERT can learn the representation vector of those OOVwords based on the meaning of subword collections. Besides,the positional embedding layer allows BERT to capture therepresentation of a word based on its context in a log mes-sage. BERT also contains self-attention mechanisms that caneffectively measure the importance of each word in a sentence.
C. Transformer-based Classiﬁcation
To better understand the semantics of logs, we adopt
the transformer model [14], which has been introduced
to overcome the limitations of RNN-based models. Takingthe semantic vectors of log messages as input (i.e. X=
{x
1,x2,...,x n}), we use a transformer encoder-based model
for anomaly detection. In this section, we brieﬂy describethe proposed transformer-based classiﬁcation model, whichcontains Positional Encoding and Transformer Encoder.
a) Positional Encoding: The order of a log sequence
conveys important information for the anomaly detectiontask. BERT encoder represents a log message into a ﬁxed-dimensional vector where log messages with similar meaningsare closer to each other. However, those vectors do notcontain the relative position information of log messages ina log sequence. Therefore, a sinusoidal encoder is applied togenerate an embedding p
iusing sinandcosfunctions for each
position iin the log sequence X[14]. Then, piis added to the
semantic vector xiat position i, and xi+piwill be used to
feed the transformer-based model (see Figure 10 (Step 3)). Inthis way, the model can learn the relative position informationof each log message in the sequence and can distinguish logmessages at different positions.
b) Transformer Encoder: This model is based on the
transformer architecture [14], which contains self-attentionlayers followed by position-wise feed-forward layers. Givenan input X={x
1,x2,...,x n}, the positional embeddings are
added before it enters into the transformer. In the transformermodule, multi-head attention layers calculate the attentionscore matrices for each log message with different attentionpatterns. The attention score is calculated by training the queryand key matrices of the attention layers. Different attentionpatterns are obtained with multi-head self-attention layers,which enable the model to consider which attention scoreis signiﬁcant. The inter-layer features are connected into afeed-forward network, which contains two fully connectedlayers in order to reach the combination of different attentionscores. Then, the output of the transformer model is fed intothe pooling, dropout, and a fully connected layer. The classprobabilities, which identify normal/abnormal log sequences,are calculated using the softmax classiﬁer. The architecture ofthe classiﬁcation model is shown in Figure 10 (Step 3).D. Anomaly Detection
Following the above steps, we can train a transformer-based
model for log-based anomaly detection. When a set of new logmessages arrives, NeuralLog ﬁrstly conducts preprocessing.Then it transforms the new log messages into semantic vectors.The log sequence, represented as a list of semantic vectors,is fed into the trained model. Finally, the transformer-basedmodel can predict whether this log sequence is anomalous ornot.
V. E
V ALUATION
A. Experimental Design
1) Research Questions: In this section, we evaluate our ap-
proach by answering the following research questions (RQs):
RQ1: How effective is NeuralLog in log-based anomaly
detection?
RQ2: How effective is NeuralLog in understanding the
semantic meaning of log data?
RQ3: How effective is NeuralLog under different settings?
2) Datasets: In this paper, we evaluate NeuralLog on four
public datasets [44], namely HDFS, Blue Gene/L, Thunder-
bird, and Spirit. HDFS dataset [34], [7] contains 11,175,629log messages collected from a Hadoop Distributed File Systemon the Amazon EC2 platform. Each session identiﬁed by blockID in the HDFS dataset is labeled as normal or abnormal. BGLdataset [33], [34] contains 4,747,963 log messages collectedfrom the Blue Gene/L supercomputer at Lawrence LivermoreNational Labs. Thunderbird and Spirit datasets [33] were col-lected from two real-world supercomputers at Sandia NationalLabs. Each log message in these datasets was manually labeledas anomalous or not. In this experiment, we leverage 10million continuous log messages from the Thunderbird dataset,and 1GB log messages from the Spirit dataset, which were alsoused in prior work [45]. The details of the datasets are shownin Table II.
TABLE II
THE DETAILS OF LOG DATASETS
Category Size #Messages #Anomalies
HDFS Distributed system 1.5 G 11,175,629 16,838
Blue Gene /L Supercomputer 743 M 4,747,963 348,460
Thunderbird Supercomputer 1.4 G 10,000,000 4,934
Spirit Supercomputer 1.0 G 7,983,345 768,142
3) Implementation and Environment: In our experiments,
NeuralLog has one layer of the transformer encoder. Thenumber of attention heads is 12, and the size of the feed-forward network that takes the output of the multi-head self-attention mechanism is 2048. The Transformer-based modelof NeuralLog is trained using AdamW optimizer [46] withthe initial learning rate of 3e−4. We set the mini-batch size
and the dropout rate to 64 and 0.1, respectively. We use thecross-entropy as the loss function. We train the Transformer-based model for a maximum of 20 epochs and perform earlystopping for ﬁve consecutive iterations.
498We implement NeuralLog with Python 3.6 and Keras tool-
box and conduct experiments on a server with Windows Server
2012 R2, Intel Xeon E5-2609 CPU, 128GB RAM, and anNVIDIA Tesla K40c.
4) Evaluation Metrics: To measure the effectiveness of
NeuralLog in anomaly detection, we use the Precision, Recall,and F1-Score metrics. We calculate these metrics as follows:
•Precision: the percentage of correctly detected abnor-
mal log sequences amongst all detected abnormal logsequences by the model. P recision =
TP
TP+FP.
•Recall: the percentage of log sequences that are correctly
identiﬁed as anomalies over all real anomalies.Recall =
TP
TP+FN.
•F1-Score: the harmonic mean of Precision and Recall.
F1−score =2∗P recision∗Recall
P recision+Recall
TP (True Positive) is the number of abnormal log sequencesthe are correctly detected by the model. FP (False Positive) isthe number of normal log sequences that are wrongly identiﬁedas anomalies. FN (False Negative) is the number of abnormallog sequences that are not detected by the model.
B. RQ1: How effective is NeuralLog?
This RQ evaluates whether or not NeuralLog can work
effectively on public log datasets. For the HDFS dataset, we
construct log sequences by correlating log messages with thesame block ID, as the data is labeled by blocks. Then, werandomly select 80% of log sequences for training, and therest of the dataset is used for testing. For BGL, Thunderbird,and Spirit datasets, we ﬁrst sort the log messages by time.Then, we leverage the ﬁrst 80% (according to the timestampsof logs) log messages as the training set and the rest 20%as the testing set. This design ensures that the testing datacontains new log messages previously unseen in the trainingset. Following the previous work [47], [12], we apply a slidingwindow with a length of 20 messages and a step size of 1message to construct log sequences.
TABLE III
RESULTS OF DIFFERENT METHODS ON PUBLIC DATASETS
Dataset LR SVM IM LogRobust Log2Vec NeuralLog
P 0.99 0.99 1.00 0.98 0.94 0.96
HDFS R 0.92 0.94 0.88 1.00 0.94 1.00
F1 0.96 0.96 0.94 0.99 0.94 0.98
P 0.13 0.97 0.13 0.62 0.80 0.98
BGL R 0.93 0.30 0.30 0.96 0.98 0.98
F1 0.23 0.46 0.18 0.75 0.88 0.98
Thunder -
birdP 0.46 0.34 - 0.61 0.74 0.93
R 0.91 0.91 - 0.78 0.94 1.00F1 0.61 0.50 - 0.68 0.84 0.96
P 0.89 0.88 - 0.97 0.91 0.98
Spirit R 0.96 1.00 - 0.94 0.96 0.96
F1 0.92 0.93 - 0.95 0.95 0.97
’-’ denotes timeout (30 hours), P denotes Precision, R denotes Recall, and
F1 is the F1-score.
We compare the results of NeuralLog and ﬁve existing ap-
proaches, including Support Vector Machine-based approach
(SVM) [6], Logistic Regression-based approach (LR) [10],
Invariant Mining (IM) [11], LogRobust [8], and Log2Vec[48]. Traditional approaches, such as SVM, LR, and IM,transform the log sequences into log count vectors, then build
unsupervised or supervised machine learning models to detectanomalies. In our work, we utilize Drain [24] to generate thelog events for SVM, LR, and IM. LogRobust incorporates apre-trained Word2vec model [31] to learn the representationsvector of log templates instead of counting the occurrencesof log events. LogRobust then leverages an Attention-basedBi-LSTM to learn and detect anomalies. Log2Vec [48] accu-rately extracts the semantic and syntax information from logmessages and leverages the Deeplog [5] model to improvethe accuracy of anomaly detection. We do not compare withDeepLog [5] because previous studies already showed thatLog2Vec outperforms DeepLog [48]. Note that there are someother recent state-of-the-art methods such as LogAnomaly[12]. However, LogAnomaly [12] has no publicly availableimplementation and requires operators’ domain knowledge(to manually add domain-speciﬁc synonyms and antonyms).Therefore, it is not experimentally compared in this paper.
The comparison results are shown in Table III. Overall,
NeuralLog achieves the best results on BGL, Thunderbird, andSpirit datasets and comparable results on the HDFS dataset.It is worth noting that the recall value achieved by NeuralLogon the HDFS dataset is 1.00, which means that NeuralLogcan identify all anomalies captured by the dataset with highprecision. NeuralLog achieves the best F1-score of 0.98 onthe BGL dataset, 0.96 on the Thunderbird dataset, and 0.97on the Spirit data.
As discussed in Section II-C, existing approaches (including
SVM, Decision Tree, and LR) are heavily affected by theaccuracy of log parsing. Besides, these approaches cannotcapture the semantic information of log messages. Therefore,these approaches perform poorly on BGL and Thunderbirddatasets when the log parsing is inaccurate. They can achievea high F1-Score on the Spirit dataset since the parsing errorrate is only 0.1% for this dataset.
LogRobust [8], which encodes log templates into semantic
vectors using the FastText pre-trained model [31], cannotwork well on 2 out of 4 datasets. LogRobust shows alower F1-Score of 0.75 and 0.68 on BGL and Thunderbirddatasets, respectively. The main reason is that LogRobustutilizes the Drain log parser [24] to obtain log templates.As aforementioned in Section III-C, the Drain parser couldinaccurately parse a noticeable number of log messages onBGL and Thunderbird datasets. Log2Vec [48] transforms rawlog messages into semantic vectors, thus can avoid errors fromlog parsers. Besides, Log2Vec also adopts MIMICK [49], anapproach of inducing word embedding from character-levelfeatures to handle OOV words to improve anomaly detectionperformance. However, it is hard to extract contextual infor-mation from characters to form meaningful words [50], [51],[52]. Therefore, Log2Vec could not effectively handle somedomain-speciﬁc words, such as technical terms or entity names[50], [51], [52]. Consequently, compared to NeuralLog thatuses subword-level feature to handle OOV words, Log2Vecachieves lower F1-scores on BGL and Thunderbird datasets
499(0.88 and 0.84, respectively).
We also evaluate the time efﬁciency of NeuralLog on the
four datasets. On average, it takes NeuralLog 14.3 minutes per
dataset to encode all log messages and 5.2 minutes to train adetection model (20 epochs). The average time of the anomalydetection phase is 3.1 milliseconds per log sequence. Baselinemethods that require log parsing take 102 minutes on averagefor preprocessing. Log2Vec spends an average of 314 minutesin the preprocessing phase. SVM and LR models can ﬁnishtraining within 0.5∼ 0.7 minutes. LogRobust and Log2Vec can
train a detection model in an average of 2.2 and 13.1 minutes,respectively. In the detection phase, it takes LogRobust 0.2milliseconds per log sequence, and it is 26.3 milliseconds forLog2Vec. NeuralLog can scale to large datasets. For example,NeuralLog is able to handle the HDFS dataset, which contains11,175,629 log messages. It took NeuraLog 19.7 minutes forpreprocessing, 7.2 minutes for training, and 0.6 minutes fortesting to perform the experiment for this RQ on the HDFSdataset.
In summary, the experimental results conﬁrm that Neural-
Log can work effectively and efﬁciently for log-based anomalydetection.
C. RQ2: How effective is NeuralLog in understanding the
semantic meaning of log data?
In this section, we evaluate the ability of NeuralLog to
capture the semantic meaning of log messages. To this end,we examine the effectiveness of the encoding component thatrepresents log messages as semantic vectors and the subwordtokenization component that handles OOV words.
In NeuralLog, we preprocess the raw log messages and
directly encode the preprocessed log messages into semanticvectors. We compare NeuralLog with two variants:
•NeuralLog-Index: the indexes of log templates, obtainedby Drain [24], are simply encoded into numeric vectorsand passed to the Transformer model for anomaly detec-tion. The rest of NeuralLog is kept the same.
•NeuralLog-Template: we utilize BERT to encode thelog templates produced by the Drain [24] into semanticvectors. We then feed these semantic vectors to theTransformer model for anomaly detection. The rest ofNeuralLog is kept the same.
Table IV shows the results of two variants of NeuralLog. We
can see that, on HDFS and Spirit dataset, these two variantscan achieve high F1-scores. The reason is that log parsersperform well on these datasets. We ﬁnd that the parsing errorrate on the Spirit dataset is only 0.1%. Besides, the HDFSsystem records relatively simple operations with only 29 eventtypes, making log parsers easy to analyze. In contrast, theresults of the variants on BGL and Thunderbird datasets aregreatly affected by log parsing methods because they cannotprecisely represent the meaning of log messages, especiallywhen using the indexes of log templates. For example, themodel using log templates’ indexes and template embeddingsonly achieve F1-scores of 0.46 and 0.90 on the BGL dataset,which are much lower than the 0.98 F1-score achieved byNeuralLog (which uses raw log messages).
TABLE IV
RESULTS OF DIFFERENT REPRESENTATION METHODS
Dataset MetricNeuralLog-
IndexNeuralLog-
TemplateNeuralLog
Precision 0.93 0.93 0.96
HDFS Recall 1.00 1.00 1.00
F1-Score 0.96 0.96 0.98
Precision 0.98 0.92 0.98
BGL Recall 0.30 0.88 0.98
F1-Score 0.46 0.90 0.98
Precision 0.58 0.89 0.93
Thunderbird Recall 0.98 0.91 1.00
F1-Score 0.73 0.90 0.96
Precision 0.96 0.93 0.98
Spirit Recall 0.95 0.95 0.96
F1-Score 0.95 0.94 0.97
We next evaluate whether or not NeuralLog can effectively
handle OOV words. NeuralLog utilizes WordPiece [36], [37]to split an OOV word into a set of subwords and then extractsthe embedding of the OOV words based on its subwords. Wecompare NeuralLog with two variants:
•NeuralLog-Word2Vec: We use a pre-trained Word2vecmodel [31] to generate the embeddings of log messages.Those words that do not exist in the vocabulary areremoved from log messages. Then, embedding vectorsare passed to the Transformer model to detect anomalies.
•NeuralLog-NoWordPiece: We exclude WordPiece tok-enizer from the model (see Figure 10). Log messages,after preprocessing, are directly input to the BERT modelto obtain the semantic vectors. These vectors are theninput to the Transformer model for anomaly detection. Inthis way, OOV words that do not exist in the vocabularywill be removed instead of broken down into subwords.
The experimental results are shown in Table V. NeuralLog
achieves the best performance since it utilizes WordPiece[36], [37] to tokenize an OOV word into a set of subwords.Therefore, the meaning of an unseen word is kept by its sub-words. Both variants achieve lower F1-scores than NeuralLogsince they rely on a ﬁxed-size vocabulary and cannot handlethe OOV words. For example, on the Thunderbird dataset,F1-scores achieved by NeuralLog-Word2Vec and NeuralLog-NoWordPiece are 0.80 and 0.90, respectively, which are muchlower than the F1-score of 0.96 achieved by NeuralLog.
In summary, our results show that NeuralLog can effec-
tively represent the semantic meaning of log messages. SinceNeuralLog uses raw log messages (after preprocessing) foranomaly detection, the problem of inaccurate log parsingcan be avoided. The results also show that NeuralLog caneffectively learn the meaning of OOV words.
D. RQ3: Effectiveness of NeuralLog under different settings
NeuralLog utilizes BERT [38] as a pre-trained language
representation to understand the semantic meaning of log mes-
sages. In this RQ, we would like to evaluate the performance
500TABLE V
RESULTS OF HANDLING OOV WORDS
Dataset MetricNeuralLog-
Word2VecNeuralLog-
NoWordPieceNeuralLog
Precision 0.94 0.94 0.96
HDFS Recall 0.93 1.00 1.00
F1-Score 0.94 0.97 0.98
Precision 0.94 0.93 0.98
BGL Recall 0.88 0.96 0.98
F1-Score 0.91 0.96 0.98
Precision 0.80 0.90 0.93
Thunderbird Recall 0.80 0.89 1.00
F1-Score 0.80 0.90 0.96
Precision 0.94 0.93 0.98
Spirit Recall 0.92 0.80 0.96
F1-Score 0.93 0.86 0.97
of NeuralLog with different pre-trained language models. We
replace the BERT model in NeuralLog with GPT2 [53] andRoberta [54], and then perform experiments to evaluate theperformance of log-based anomaly detection. For GPT2 andRoberta encoders, we use their base model with 12 layers,12 attention heads, and 768 hidden units. Table VI shows theresults. We observe that these three pre-trained models can allunderstand the semantic meaning of log messages and achievepromising results. Overall, the performance of BERT is higherthan that of GPT2 and Roberta.
TABLE VI
RESULTS OF DIFFERENT PRE -TRAINED MODELS
Dataset Metric BERT [38] GPT2 [53] RoBERTa [54]
Precision 0.96 0.95 0.85
HDFS Recall 1.00 1.00 1.00
F1-Score 0.98 0.97 0.92
Precision 0.98 0.95 0.95
BGL Recall 0.98 0.99 0.90
F1-Score 0.98 0.97 0.93
Precision 0.93 0.85 0.78
Thunderbird Recall 1.00 0.91 1.00
F1-Score 0.96 0.88 0.88
Precision 0.98 0.88 0.84
Spirit Recall 0.96 0.95 0.90
F1-Score 0.97 0.91 0.87
The number of attention heads and the feed-forward net-
work size are two major hyperparameters of the Transformermodel used in NeuralLog. To evaluate the impact of theseparameters on detection accuracy, we vary their values andperform experiments on the four datasets. The resulting F1-scores are shown in Figure 11. We observe that reducingthe number of attention heads and feed-forward network sizecan slightly hurt the performance of NeuralLog. For example,NeuralLog achieves F1-scores ranging from 0.96 to 0.98 whenusing 12 attention heads. These results are higher than thoseobtained by using only one attention head (0.88 - 0.93).Similarly, F1-scores achieved by a larger feed-forward networkare usually better. Overall, the Transformer model achievespromising results with different hyperparameter values (mostF1-scores are above 0.90). We observe that the performanceis best when the number of attention heads is between 4 and12, and the feed-forward network size is from 512 to 2048.1 2 84210.60.70.80.91
Number of attention headsF1-score
HDFS
BGL
Thunderbird
Spirit
(a) Results of NeuralLog with differ-
ent number of attention heads2048 1024 512 256 1280.60.70.80.91
Size of feed forward networkF1-Score
HDFS
BGL
Thunderbird
Spirit
(b) Results of NeuralLog with differ-
ent size of feed forward network
Fig. 11. Results of different hyperparameter settings
VI. D ISCUSSION
A. Why does NeuralLog Work?
There are three main reasons that make NeuralLog perform
better than the related approaches. First, NeuralLog directly
uses raw log messages instead of using a log parser inpreprocessing. Since there is no loss of information fromlog messages, NeuralLog can precisely learn the semanticrepresentation of log messages, compared to other approachesthat depend on log parsing. Second, NeuralLog leveragesBERT [38] and WordPiece [36], [37] to capture the meaning ofOOV words at the subword level. Moreover, the transformer-based classiﬁcation model can also improve the performanceof anomaly detection. The transformer utilized by NeuralLogcan learn different sequence patterns in log messages anddetermine which patterns are more relevant to anomalies.
Our study has demonstrated the effectiveness of Neural-
Log for anomaly detection. However, NeuralLog still haslimitations. Our approach is based on the learning of thesemantic meanings of log messages. Given a log message,we ﬁrst remove those words that contain numbers and specialcharacters. However, in some cases, the removed words maycarry important information, such as node ID, task ID, IPaddress, or exit code. These information could be useful foranomaly detection in certain scenarios. In our future work, wewill encode more log-related information and investigate theirimpact on log-based anomaly detection.
B. Threats to V alidity
We have identiﬁed the following major threats to validity.
Subject datasets. In this work, we use datasets collected
from the distributed system (i.e., HDFS) and supercomputer
(including BGL, Thunderbird, and Spirit). Although thesedatasets all come from real-world systems and contain millionsof logs, the number of subject systems is still limited and donot cover all the domains. In the future, we will evaluate theproposed approach on more datasets collected from a widevariety of systems.
Tool comparison. In our evaluation, we compared our
results with those of related approaches (i.e., SVM, LR, IM,LogRobust, and Log2Vec). We adopt the implementation ofSVM, LR, and IM-based methods provided by Loglizer [55].We adopt the implementation of LogRobust and Log2Vec
501provided by their authors. We apply the default parameters
and settings (e.g., sliding window size, step size, etc.) used inthe previous work [28], [48], [8]. Still, the correctness of theseimplementations could be a threat. To reduce this threat, wemake sure that the implementation of related work can producesimilar results as those reported in the original papers.
Noises in labeling. Our experiments are based on four
public datasets that are widely used by related work [47],[4], [12], [56], [28]. These datasets are manually inspectedand labeled by engineers. Therefore, data noise (false posi-tive/negatives) may be introduced during the manual labelingprocess. Although we believe the amount of noise is small(if it exists), we will investigate the data quality issue in ourfuture work.
VII. R
ELATED WORK
A. Log Parsing Errors
The log parsing accuracy highly inﬂuences the performance
of log mining [16]. Log parsers could produce inconsistentresults depend on the preprocessing step and the set ofparameters [16], [15]. The preprocessing step can furtherimprove log parsing accuracy [16] and despite the simplicity,it still requires some additional manual work [15]. Zhu et al.[15] benchmarked 13 automated log parsers on a total of 16datasets. They found that Drain [24] is the most accurate logparser, which attains high accuracy on 9 out of 16 datasets. Theother top-ranked log parsers include IPLoM [26], AEL [25]and Spell [27]. They also found that some model parametersneed to be tuned manually, and some models did not scalewell with the volume of logs. He et al [16] evaluated fourwidely used log parsers, including SLCT [57], IPLoM [26],LKE [58] and LogSig [20].
In practice, new types of logs always appear [12], as OOV
words can be added to log templates and lead to many extralog events, which will confuse the downstream tasks. Zhanget al. [8] indicated that log data is unstable, meaning thatnew log events often appear due to software evolution at itslifetime. Their empirical study on a Microsoft online servicesystem shows that up to 30.3% logs are changed in thelatest version. In our work, we perform an empirical studyof the log parsing errors caused by the OOV problem andsemantic misunderstanding, and investigate their impact on theperformance of anomaly detection.
B. Log Representations
As described in Section II, most of the existing log-based
anomaly detection approaches use log parsers to obtain log
events and represent log messages as log events. Therefore,the existing approaches suffer from the OOV problem and theinaccurate log parsing. Recently, deep learning-based modelshave been adopted into log-based anomaly detection. DeepLog[5] applies Spell [27] to extract log events, then each logevent is assigned with an index. Since DeepLog represents logmessages as the indexes of log templates, it cannot preventsemantic information loss and could produce many wrongdetection results [47], [48]. LogRobust [8] leverages Drain[24] to obtain log templates, then encodes these templatesusing the FastText [31] framework combined with TF-IDF[32] weight. LogAnomaly [12] applies FT-Tree [59] to parselog messages to templates, then proposes template2Vec toencode these templates based on Word2Vec [42]. SwissLog[47] obtains the semantic information of log messages af-ter parsing log messages using a dictionary-based approach.Due to imperfect log parsing, these methods could fail tocapture the semantic meaning of log messages and produceincorrect results. Log2Vec [48] transforms raw log messagesinto semantic vectors. As it utilizes character-level features,it could not effectively handle some domain-speciﬁc words[50], [51], [52]. Besides, Log2Vec adopts word2vec-basedmodel that ignores the contextual information in sentences[47], thus it cannot fully understand the semantic meaning oflog messages. Nedelkoski et al. [56] proposed Logsy, whichis a classiﬁcation-based method to learn log representationsin a way to distinguish between normal data from the targetsystem and anomaly samples from auxiliary log datasets. Itdoes not provide mechanism for handling OOV words in logmessages either.
To overcome the limitations of existing methods, we pro-
pose NeuralLog, a deep learning-based anomaly detectionapproach using raw log data. NeuralLog utilizes WordPiecetokenization to effectively handle OOV words that constantlyappear in log messages. It also leverages BERT, a widelyused pre-trained language representation, to understand thesemantic meaning and capture the contextual informationof raw log messages. Combined with a Transformer-basedclassiﬁcation model, NeuralLog achieves high accuracy onanomaly detection. Furthermore, we only use log data fromthe target systems and do not require any auxiliary data.
VIII. C
ONCLUSION
Log-based anomaly detection is important for improving
the availability and reliability of large-scale software systems.Our empirical study shows that existing approaches sufferfrom inaccurate log parsing and cannot handle OOV wordswell. To overcome the limitations introduced by log parsing,in this paper, we propose NeuralLog, a log-based anomalydetection approach that does not require log parsing. Ourapproach employs BERT encoder to capture the semanticmeaning of raw log messages. To better capture contextualinformation from log sequences, we construct a Transformer-based classiﬁcation model. We have evaluated the proposedapproach using four public datasets. The experimental resultsshow that NeuralLog is effective and efﬁcient for log-basedanomaly detection.
Our source code and experimental data are publicly avail-
able at https://github.com/vanhoanglepsa/NeuralLog.
A
CKNOWLEDGMENT
This research was supported by the Australian Government
through the Australian Research Council’s Discovery Projectsfunding scheme (project DP200102940).
502REFERENCES
[1] E. Bauer and R. Adams, Reliability and availability of cloud computing.
John Wiley & Sons, 2012.
[2] R. S. Kazemzadeh and H.-A. Jacobsen, “Reliable and highly available
distributed publish/subscribe service,” in 2009 28th IEEE International
Symposium on Reliable Distributed Systems. IEEE, 2009, pp. 41–50.
[3] J. Breier and J. Brani ˇsov´a, “Anomaly detection from log ﬁles using data
mining techniques,” in Information Science and Applications. Springer,
2015, pp. 449–457.
[4] B. Zhang, H. Zhang, P. Moscato, and A. Zhang, “Anomaly detection via
mining numerical workﬂow relations from logs,” in 2020 International
Symposium on Reliable Distributed Systems (SRDS). IEEE, 2020, pp.
195–204.
[5] M. Du, F. Li, G. Zheng, and V . Srikumar, “Deeplog: Anomaly detection
and diagnosis from system logs through deep learning,” in Proceedings
of the 2017 ACM SIGSAC Conference on Computer and Communica-tions Security, 2017, pp. 1285–1298.
[6] M. Chen, A. X. Zheng, J. Lloyd, M. I. Jordan, and E. Brewer,
“Failure diagnosis using decision trees,” in International Conference on
Autonomic Computing, 2004. Proceedings. IEEE, 2004, pp. 36–43.
[7] W. Xu, L. Huang, A. Fox, D. Patterson, and M. I. Jordan, “Detecting
large-scale system problems by mining console logs,” in Proceedings
of the ACM SIGOPS 22nd symposium on Operating systems principles,2009, pp. 117–132.
[8] X. Zhang, Y . Xu, Q. Lin, B. Qiao, H. Zhang, Y . Dang, C. Xie, X. Yang,
Q. Cheng, Z. Li et al., “Robust log-based anomaly detection on unstable
log data,” in Proceedings of the 2019 27th ACM Joint Meeting on
European Software Engineering Conference and Symposium on theFoundations of Software Engineering, 2019, pp. 807–817.
[9] H. Guo, S. Yuan, and X. Wu, “Logbert: Log anomaly detection via bert,”
arXiv preprint arXiv:2103.04475, 2021.
[10] P. Bodik, M. Goldszmidt, A. Fox, D. B. Woodard, and H. Andersen,
“Fingerprinting the datacenter: automated classiﬁcation of performancecrises,” in Proceedings of the 5th European conference on Computer
systems, 2010, pp. 111–124.
[11] J.-G. Lou, Q. Fu, S. Yang, Y . Xu, and J. Li, “Mining invariants
from console logs for system problem detection.” in USENIX Annual
Technical Conference, 2010, pp. 1–14.
[12] W. Meng, Y . Liu, Y . Zhu, S. Zhang, D. Pei, Y . Liu, Y . Chen, R. Zhang,
S. Tao, P. Sun et al., “Loganomaly: Unsupervised detection of sequential
and quantitative anomalies in unstructured logs.” in IJCAI, vol. 7, 2019,
pp. 4739–4745.
[13] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efﬁcient estimation of
word representations in vector space,” arXiv preprint arXiv:1301.3781,
2013.
[14] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
L. Kaiser, and I. Polosukhin, “Attention is all you need,” arXiv preprint
arXiv:1706.03762, 2017.
[15] J. Zhu, S. He, J. Liu, P. He, Q. Xie, Z. Zheng, and M. R. Lyu, “Tools
and benchmarks for automated log parsing,” in 2019 IEEE/ACM 41st In-
ternational Conference on Software Engineering: Software Engineeringin Practice (ICSE-SEIP). IEEE, 2019, pp. 121–130.
[16] P. He, J. Zhu, S. He, J. Li, and M. R. Lyu, “An evaluation study on
log parsing and its use in log mining,” in 2016 46th annual IEEE/IFIP
international conference on dependable systems and networks (DSN).IEEE, 2016, pp. 654–661.
[17] M. Nagappan and M. A. V ouk, “Abstracting log lines to log event types
for mining software system logs,” in 2010 7th IEEE Working Conference
on Mining Software Repositories (MSR 2010). IEEE, 2010, pp. 114–117.
[18] R. Vaarandi and M. Pihelgas, “Logcluster-a data clustering and pattern
mining algorithm for event logs,” in 2015 11th International conference
on network and service management (CNSM). IEEE, 2015, pp. 1–7.
[19] H. Dai, H. Li, C. S. Chen, W. Shang, and T.-H. Chen, “Logram: Efﬁcient
log parsing using n-gram dictionaries,” IEEE Transactions on Software
Engineering, 2020.
[20] L. Tang, T. Li, and C.-S. Perng, “Logsig: Generating system events
from raw textual logs,” in Proceedings of the 20th ACM international
conference on Information and knowledge management , 2011, pp. 785–
794.
[21] H. Hamooni, B. Debnath, J. Xu, H. Zhang, G. Jiang, and A. Mueen,
“Logmine: Fast pattern recognition for log analytics,” in Proceedingsof the 25th ACM International on Conference on Information andKnowledge Management, 2016, pp. 1573–1582.
[22] K. Shima, “Length matters: Clustering system log messages using length
of words,” arXiv preprint arXiv:1611.03213, 2016.
[23] S. Thaler, V . Menkonvski, and M. Petkovic, “Towards a neural language
model for signature extraction from forensic logs,” in 2017 5th Inter-
national Symposium on Digital Forensic and Security (ISDFS). IEEE,2017, pp. 1–6.
[24] P. He, J. Zhu, Z. Zheng, and M. R. Lyu, “Drain: An online log parsing
approach with ﬁxed depth tree,” in 2017 IEEE International Conference
on Web Services (ICWS). IEEE, 2017, pp. 33–40.
[25] Z. M. Jiang, A. E. Hassan, P. Flora, and G. Hamann, “Abstracting execu-
tion logs to execution events for enterprise applications (short paper),” in2008 The Eighth International Conference on Quality Software. IEEE,2008, pp. 181–186.
[26] A. A. Makanju, A. N. Zincir-Heywood, and E. E. Milios, “Clustering
event logs using iterative partitioning,” in Proceedings of the 15th ACM
SIGKDD international conference on Knowledge discovery and datamining, 2009, pp. 1255–1264.
[27] M. Du and F. Li, “Spell: Streaming parsing of system event logs,” in
2016 IEEE 16th International Conference on Data Mining (ICDM) .
IEEE, 2016, pp. 859–864.
[28] S. He, J. Zhu, P. He, and M. R. Lyu, “Experience report: System
log analysis for anomaly detection,” in 2016 IEEE 27th International
Symposium on Software Reliability Engineering (ISSRE) . IEEE, 2016,
pp. 207–218.
[29] D. El-Masri, F. Petrillo, Y .-G. Gu ´eh´eneuc, A. Hamou-Lhadj, and
A. Bouziane, “A systematic literature review on automated log ab-straction techniques,” Information and Software Technology, vol. 122,
p. 106276, 2020.
[30] Y . Liang, Y . Zhang, H. Xiong, and R. Sahoo, “Failure prediction in ibm
bluegene/l event logs,” in Seventh IEEE International Conference on
Data Mining (ICDM 2007). IEEE, 2007, pp. 583–588.
[31] A. Joulin, E. Grave, P. Bojanowski, M. Douze, H. J ´egou, and T. Mikolov,
“Fasttext. zip: Compressing text classiﬁcation models,” arXiv preprint
arXiv:1612.03651, 2016.
[32] G. Salton and C. Buckley, “Term-weighting approaches in automatic
text retrieval,” Information processing & management, vol. 24, no. 5,
pp. 513–523, 1988.
[33] A. Oliner and J. Stearley, “What supercomputers say: A study of ﬁve
system logs,” in 37th Annual IEEE/IFIP International Conference on
Dependable Systems and Networks (DSN’07) . IEEE, 2007, pp. 575–
584.
[34] S. He, J. Zhu, P. He, and M. R. Lyu, “Loghub: a large collection of
system log datasets towards automated log analytics,” arXiv preprint
arXiv:2008.06448, 2020.
[35] (2021) Logpai. [Online]. Available: https://github.com/logpai/logparser
[36] M. Schuster and K. Nakajima, “Japanese and korean voice search,” in
2012 IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP). IEEE, 2012, pp. 5149–5152.
[37] Y . Wu, M. Schuster, Z. Chen, Q. V . Le, M. Norouzi, W. Macherey,
M. Krikun, Y . Cao, Q. Gao, K. Macherey etal., “Google’s neural
machine translation system: Bridging the gap between human andmachine translation,” arXiv preprint arXiv:1609.08144, 2016.
[38] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training
of deep bidirectional transformers for language understanding,” arXiv
preprint arXiv:1810.04805, 2018.
[39] V . Sanh, L. Debut, J. Chaumond, and T. Wolf, “Distilbert, a distilled
version of bert: smaller, faster, cheaper and lighter,” arXiv preprint
arXiv:1910.01108, 2019.
[40] K. Clark, M.-T. Luong, Q. V . Le, and C. D. Manning, “Electra: Pre-
training text encoders as discriminators rather than generators,” arXiv
preprint arXiv:2003.10555, 2020.
[41] R.-M. Karampatsis, H. Babii, R. Robbes, C. Sutton, and A. Janes, “Big
code!= big vocabulary: Open-vocabulary models for source code,” in2020 IEEE/ACM 42nd International Conference on Software Engineer-ing (ICSE). IEEE, 2020, pp. 1073–1085.
[42] Q. Le and T. Mikolov, “Distributed representations of sentences and
documents,” in International conference on machine learning. PMLR,
2014, pp. 1188–1196.
[43] (2021) Bert pretrained models. [Online]. Available: https://github.com/
google-research/bert
[44] (2021) Loghub. [Online]. Available: https://github.com/logpai/loghub
503[45] K. Yao, H. Li, W. Shang, and A. E. Hassan, “A study of the performance
of general compressors on log ﬁles,” Empirical Software Engineering,
vol. 25, no. 5, pp. 3043–3085, 2020.
[46] I. Loshchilov and F. Hutter, “Decoupled weight decay regularization,”
arXiv preprint arXiv:1711.05101, 2017.
[47] X. Li, P. Chen, L. Jing, Z. He, and G. Yu, “Swisslog: Robust and uniﬁed
deep learning based log anomaly detection for diverse faults,” in 2020
IEEE 31st International Symposium on Software Reliability Engineering
(ISSRE). IEEE, 2020, pp. 92–103.
[48] W. Meng, Y . Liu, Y . Huang, S. Zhang, F. Zaiter, B. Chen, and D. Pei,
“A semantic-aware representation framework for online log analysis,” in2020 29th International Conference on Computer Communications andNetworks (ICCCN). IEEE, 2020, pp. 1–7.
[49] Y . Pinter, R. Guthrie, and J. Eisenstein, “Mimicking word embeddings
using subword rnns,” arXiv preprint arXiv:1707.06961, 2017.
[50] S. Sasaki, J. Suzuki, and K. Inui, “Subword-based compact reconstruc-
tion of word embeddings,” in Proceedings of the 2019 Conference
of the North American Chapter of the Association for ComputationalLinguistics: Human Language Technologies, V olume 1 (Long and ShortPapers), 2019, pp. 3498–3508.
[51] J. Zhao, S. Mudgal, and Y . Liang, “Generalizing word embeddings using
bag of subwords,” arXiv preprint arXiv:1809.04259, 2018.
[52] Z. Hu, T. Chen, K.-W. Chang, and Y . Sun, “Few-shot representation
learning for out-of-vocabulary words,” arXiv preprint arXiv:1907.00505,
2019.
[53] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever,
“Language models are unsupervised multitask learners,” OpenAI blog,
vol. 1, no. 8, p. 9, 2019.
[54] A. Conneau, K. Khandelwal, N. Goyal, V . Chaudhary, G. Wenzek,
F. Guzm ´an, E. Grave, M. Ott, L. Zettlemoyer, and V . Stoyanov, “Unsu-
pervised cross-lingual representation learning at scale,” arXiv preprint
arXiv:1911.02116, 2019.
[55] (2021) Loglizer. [Online]. Available: https://github.com/logpai/loglizer[56] S. Nedelkoski, J. Bogatinovski, A. Acker, J. Cardoso, and O. Kao, “Self-
attentive classiﬁcation-based anomaly detection in unstructured logs,”arXiv preprint arXiv:2008.09340, 2020.
[57] R. Vaarandi, “A data clustering algorithm for mining patterns from event
logs,” in Proceedings of the 3rd IEEE Workshop on IP Operations &
Management (IPOM 2003)(IEEE Cat. No. 03EX764). Ieee, 2003, pp.119–126.
[58] Q. Fu, J.-G. Lou, Y . Wang, and J. Li, “Execution anomaly detection
in distributed systems through unstructured log analysis,” in 2009 ninth
IEEE international conference on data mining. IEEE, 2009, pp. 149–158.
[59] S. Zhang, W. Meng, J. Bu, S. Yang, Y . Liu, D. Pei, J. Xu, Y . Chen,
H. Dong, X. Qu et al., “Syslog processing for switch failure diagnosis
and prediction in datacenter networks,” in 2017 IEEE/ACM 25th Inter-
national Symposium on Quality of Service (IWQoS). IEEE, 2017, pp.1–10.
504