End-to-End Automation of Feedback
on Student Assembly Programs
Zikai Liu
ETH Zurich
Zurich, Switzerland
zikail2@illinois.eduTingkai Liu
University of Illinois at Urbana-Champaign
Champaign, IL, USA
tingkai2@illinois.eduQi Li
Purdue University
West Lafayette, IN, USA
qili8@illinois.edu
Wenqing Luo
University of Illinois at Urbana-Champaign
Champaign, IL, USA
wenqing4@illinois.eduSteven S. Lumetta
University of Illinois at Urbana-Champaign
Champaign, IL, USA
lumetta@illinois.edu
Abstract —We developed a set of tools designed to provide
rapid feedback to students as they learn to write programs in
assembly language (LC-3, a RISC-like educational instructionset architecture). At the heart of the system is an extendedversion of KLEE, KLC3, that enables us to both identify issuesand perform equivalence checking between student code and agold (correct) version of each assignment. Feedback begins whenstudents edit their code using a VSCode extension that leveragesstatic analysis to perform a variety of correctness and stylechecks, encouraging students to improve their code quality. Eachtime a student commits code to their Git repository, our systemtriggers. Using KLC3 (KLEE), the student code is executed alongwith the gold version, and issues and behavioral differences aredelivered back to the student through their Git repository asa human-readable report, test cases, and scripts. A queueingsystem allows students to monitor progress, but responses aregenerally available within minutes. We also extended the LC-3simulation tools to support reverse debugging, making the processof ﬁnding complex bugs much more tractable for students,and used Emscripten to develop a browser-based interface foruse in testing and debugging. Finally, our system maintains anindividual regression test suite for each student and requiresa submission to pass all previous tests before re-evaluation inKLC3, thus avoiding encouraging programming-by-guesswork.We deployed the system to provide feedback for the assemblyprogramming assignments in a class of over 100 students inFall 2020. Students wrote a median of around 700 lines ofassembly for these assignments, making heavy use of our toolsto understand and eliminate their bugs. Anonymous studentfeedback on the tools was uniformly positive. Since that semester,we have continued to reﬁne and expand our tools’ analysiscapabilities and performance, and plan to deploy the system againin the near future (the class is offered every Fall).
I. I NTRODUCTION
Learning to program is difﬁcult. As with all topics, students
learn more quickly when given immediate feedback tailored to
their efforts. However, while university staff—instructors andteaching assistants—are capable of providing such feedback,staff are not available 24/7 and lack the time needed to provideindividual attention to each student’s programs.
Automating feedback has been an important topic for
years, and systems such as Web-CAT[1] are widely-used inclasses. We address the need for rapid feedback by leveragingKLEE [2] to perform both symbolic analysis of student codeas well as equivalence checking with a correct implementationof the given assignment. In our class, students ﬁrst programin assembly language for the LC-3 instruction set architec-ture (ISA), which was invented for educational purposes inthe textbook by Patt and Patel [3]. The symbolic equivalencechecking that forms the core of our feedback system was oneof the approaches explored by the early KLEE work [2]. Use ofKLEE in providing rapid feedback on student programs writ-ten in C was pioneered by Gao [4], but the focus in that workis on C programs supported by the standard KLEE/LLVM in-frastructure. In fact, most feedback systems focus on high-levellanguages [5], and few make use of symbolic analysis. Thispaper represents the ﬁrst use of symbolic analysis to providerapid feedback on LC-3 assembly programs through in-depthcustomization of KLEE, and describes the implementation andresults of deployment as an end-to-end system.
We decided to make use of the low-level infrastructure
provided by KLEE but to implement our own modules forpreliminary LC-3 code analysis, LC-3 state execution, a 16-bitmemory model, and search heuristics. We also generalize theidea of loop reduction [6] to accommodate the multi-entry-point loops often found in assembly language programs. Also,as we felt that the KLEE output might be difﬁcult for noviceprogrammers to understand and utilize, we developed issueﬁltering and human-readable report and script generation tohelp students through their ﬁrst signiﬁcant debugging efforts.We refer to our extended KLEE as KLC3.
We then extended the tools surrounding KLC3 to allow stu-
dents to make use of more familiar interfaces, such as VSCodefor editing and real-time static checks as well as Git to submittheir code and receive the KLC3 analysis and feedback results.Finally, we added support for reverse (back-in-time) debuggingto the existing LC-3 simulation tools and made them availablethrough a browser interface by leveraging Emscripten [7] andminor modiﬁcations to the C implementations of the tools. Aswe extended the tools around KLC3, our system is able to to
182021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)
DOI 10.1109/ASE51524.2021.000132021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE) | 978-1-6654-0337-5/21/$31.00 ©2021 IEEE | DOI: 10.1109/ASE51524.2021.9678837
978-1-6654-0337-5/21/$31.00  ©2021  IEEE
cover the whole workﬂow of editing, testing and debugging
when students work on their assignments.
After developing much of this framework, we deployed it to
provide feedback to over 100 students taking our introductoryassembly language and C programming course in Fall 2020,providing us with several thousand code samples as well asan opportunity to survey student opinions about the tools.
The remainder of this paper is organized as follows. In the
next section, we provide additional details about our Fall 2020deployment and student use as background. In Sec. III, wedescribe our LC-3 extension to the VSCode editor [8] thatprovides feedback as students edit their programs. Sec. IV thenprovides an overview of the system from when the studentcommits a new copy of an assignment through when theyreceive feedback. We follow with details of our extensionsto KLEE in Sec. V. After deploying the system, we madea number of further optimizations, two of which we explainin Sec. VI. In Sec. VII, we discuss changes made to theLC-3 simulation tools (provided with the textbook) to supportreverse debugging, and the browser-based interface that wedeveloped. Sec. VIII provides timing information about thespeed at which we are now able to offer feedback on studentprograms. Sec. IX describes a few other aspects of our system,including a summary of student feedback, and comparesour approach with the commonly used Web-CAT[1]. Finally,Sec. X offers our conclusions.
II. D
EPLOYMENT CONTEXT
Prototype versions of most of the tools developed for
this project were ready in the Fall 2020 semester, so wedeployed them for use in providing feedback to students tak-ing our introductory assembly language and C programmingcourse. Conditions were fairly normal despite the COVID-19pandemic: students and teaching assistants were resident oncampus and made use of the usual classrooms and computerlabs. The instructor, however, gave lectures live over Zoom,using multiple cameras to observe the students and to hear anyquestions. A video inset of the instructor enabled students tosee gestures and facial expressions. While the interaction wasnot identical to a normal semester, we believe that it was asclose as possible in the physical absence of the instructor.
A total of 109 students completed the course. Each student
implemented three assignments over four weeks using LC-3assembly language. Each assignment is built upon the previousone by including a student’s previous code directly. In theﬁrst assignment, which we call S
UBROUTINES here, students
were required to write two subroutines to perform formattedoutput to the display. In the second assignment, S
CHEDULE ,
students populated a weekly schedule with events, then printedthe schedule to the display. Each event consisted of a name,an hour, and a bit vector of the days in a week on whichthe event occurred. Finally, in the third assignment, D
FS,
students implemented a depth-ﬁrst search with backtrackingto ﬁt additional events with ﬂexible hours (again speciﬁed bya bit vector) into an existing weekly schedule. Students wrotea median of 693 lines of LC-3 code for the three assignments.Since students received feedback each time they committedtheir code to the Github server, they were inclined to commitas they made progress, providing us with 1079 code samplesfor S
UBROUTINES , 1474 samples for S CHEDULE , and 960 for
DFS. For automating feedback, we also made use of correct
implementations authored by the course staff.
The KLC3 feedback system was linked to the assignment
submission system, so feedback was automatic for all students.We did not maintain a control group, as we felt it unfair todeprive students of the opportunity for feedback from KLC3.In this paper, we instead compare with a nearly identicalversion of the class held two years earlier, in which the sameinstructor presented the same material to students in personon the campus. Students and other course staff were different,of course. Students were not forced to make use of the LC-3VSCode extension, and the survey results suggest that onlyabout half did so. Students were also not forced to make useof our browser-based LC-3 tools, but about two-thirds did.
III. E
DITING IN VSC ODE
Initially, we added basic static checking into KLC3 to
identify problems such as dead code, but we wanted to givestudents feedback as early as possible. VSCode [8] is a populareditor amongst students in later programming classes, so wedecided to encourage students to try it by developing an LC-3extension that gives real-time feedback as students edit theircode. By delivering appropriate warnings to students beforethey submit their code, we also enable them to ﬁx potentialbugs before they commit to their repositories and add load toour analysis server.
The VSCode extension reports three kinds of feedback:
errors, warnings, and information. Errors imply that the codecannot assemble. Warnings indicate potential bugs or poorstyle. Information shows the results of analyses on the code,such as which registers in a subroutine are callee-saved (havetheir values preserved by the subroutine).
Our VSCode extension implements per-instruction analysis,
control ﬂow analysis, and procedure-based analysis. Feedbackmessages about potential issues in the code are conveyed viaVSCode’s squiggles and pop-up windows.
Although none of the extension’s feedback is speciﬁc to any
particular assignment, nor does the extension have any infor-mation about what constitutes correct behavior, we observethat the ability to convey meaningful feedback messages tostudents while they write their code may still increase theirexpected functionality grade in assignments. Functionalitygrade is the part of a student’s grade allocated to correctbehavior, as opposed to points obtained for demonstratinggood coding style and including adequate comments. Using the
S
UBROUTINES program as an example, for which 770 code
samples assemble, we used our grading script to calculate thefunctionality grade (out of 65) for each sample, then computedan average functionality grade among samples for which ourextension reports the same number of warnings. The resultsappear in Tab. I. Code samples that generate no warningsoften still fail to implement the assignment correctly, hence the
19TABLE I
FUNCTIONALITY GRADES VS NUMBER OF WARNINGS
Number of Warnings Count A vg. Functionality Grade
0 492 46.9
1 169 36.0
2 42 40.9
3+ 67 35.9
Fig. 1. A canonical example of a fully-unrolled loop typical of those produced
by students not yet able to formulate iterative constructs.
average functionality grade for warning-free samples is not 65.
Nevertheless, grades for warning-free samples averaged over10 points higher than samples that contain warnings, indicatingthat feedback during editing can be helpful in guiding studentsto develop correct solutions.
In addition to helping with correctness, early feedback can
also help students to avoid developing bad coding habits.For example, while extending the idea of loop reductionto assembly code (see Sec. VI-B), we found code samplesin which students, unable to formulate loops properly, hadinstead written fully-unrolled loops. Fig. 1 shows a canonicalexample, in which ﬁve adjacent addresses containing pointersare checked for NULL (0), and a common value is writtenfrom R3 to the address referenced by each non-NULL pointerfound. In the example, the branch instructions are independent,creating 32 possible paths. In some samples, however, studentslinked several such constructs, producing thousands of paths,many of which were impossible to execute due to correlationsamongst the branches. Such style is not encouraged and a largenumber of paths undermines the performance of our symbolicanalysis on the code. The extension identiﬁes unrolled loops byscanning through the code with different strides and detectingrepeated code segments that can potentially form a loop. Whenthe extension ﬁnds an unrolled loop, it raises a warning toindicate that the code can be turned into a loop.
Interestingly, using the extension, we found that hand-
unrolled loops are common in student code, and that theirfrequency depends strongly on the complexity of the particularloop that students are asked to write. To illustrate this idea, wecompared the ﬁnal versions of the D
FSassignment of students
in the Fall 2018 semester with those of the Fall 2020 students.The assignment was changed in minor ways to reduce thelikelihood of sharing code between semesters. In particular,the days of the week were printed as three-letter abbreviationsin 2018, but as full names in 2020. Also, the encoding ofTABLE II
PERCENTAGE OF STUDENTS VS .NUMBER OF UNROLLED LOOPS
Number of Unrolled Loops Fall 2018 Fall 2020
0 36% 14%
1 29% 31%
2+ 35% 55%
days in the bit vector for each event was reversed: in 2018,Monday was represented as 1, Tuesday as 2, and so forth. In2020, Monday was 16, Tuesday was 8, and so forth.
The results are in Tab. II: failure to write loops is generally
common in both classes, but is more common amongst the2020 students. In terms of the assignments, the slight changesproduced visible differences in the results by changing thecomplexity to conceptualize loops. Speciﬁcally, the variable-length weekday names complicate the process of ﬁnding thestarting address of each string, and the reversal of bit vectorordering makes using these data more challenging because theLC-3 ISA makes left shift easy, but right shift difﬁcult.
Fortunately, our VSCode extension is able to identify hand-
unrolled loops and to raise warnings, as shown in Fig. 1 bythe squiggles under the LD instructions—the ﬁrst instructionin the loop body—to encourage students to think harder or toseek help for implementing a loop.
Fig. 2. Overview of dynamic code analysis system.
IV . D YNAMIC ANALYSIS OVERVIEW
An overview of our dynamic code analysis system appears
in Fig. 2. The system consists of four main components: aGitHub server maintained by the university, a Webhook server,a job dispatcher, and the KLC3 execution engine. When astudent submits a new version of a program by pushing code,the GitHub server immediately notiﬁes the Webhook server,which is implemented as an HTTP server in Golang. TheWebhook server ﬁlters out events other than modiﬁcations tothe program residing in the master branch of the student’scurrent assignment, then applies any policy decisions to thesubmission. Generally, we limited each student to one newevaluation every ﬁve to ten minutes in order to discouragestudents from attempting guesswork while debugging. Theexact policy varied by assignment and is easily modiﬁable.Approved new submission events are wrapped up as KLC3execution tasks and sent to the job dispatcher. The job
20Fig. 3. KLC3 architecture and workﬂow.
dispatcher is also implemented in Golang, allowing us to
leverage Go’s channel abstraction to implement a queue andthen to parallelize execution of multiple KLC3 executionson our server. The job dispatcher uses four worker threadsthat continuously fetch tasks from a single queue. For eachsubmission drawn from the queue, a worker updates a localcopy of the student’s Git repository using the go-git library [9],
extracts the submission, and forks off a instance of KLC3to analyze the student’s code. Once KLC3 has produced areport and associated ﬁles, the worker incorporates everythinginto a new directory in a “feedback” branch of the student’srepository. The directory name indicates both the assignmentand the date and time at which the student pushed the newversion, allowing students to easily locate their feedback. Theworker then returns to the queue to obtain a new assignment.Students can view the report on the GitHub websites or byupdating their own local Git repositories. We also providea web interface to our queue system to enable students tomonitor queue status and to display KLC3 reports. For thispurpose, we make use of the OAuth2 API of the Github server.
To encourage students to make use of test cases generated
by KLC3 (see Sec. V) and to discourage programming-by-guesswork, we implemented a regression test system. A privateclass repository is used to maintain sets of regression tests foreach student, each of which is initialized to the test casesprovided with the assignment. Before analyzing code, KLC3re-evaluates all regression tests. Only code that passes all suchtests is then analyzed symbolically. Newly generated test casesare added to the regression test suite, so a student’s nextsubmission must pass the new tests as well.
V . KLC3 A
RCHITECTURE
KLEE offers a well-deﬁned infrastructure for symbolic
representations, optimizers, caches and interfaces with SMTsolver backends, upon which higher-level modules such as theLLVM IR executor are built. KLC3 is similarly built uponthe KLEE infrastructure (with a few modiﬁcations), but wereplaced the higher-level modules with our own code, includ-ing an LC-3 symbolic executor, a 16-bit-addressable memorymodel, code ﬂow analyzers, state searchers, and generatorsfor test cases and human-readable reports. Fig. 3 illustratesthe structure of KLC3, our modiﬁed KLEE architecture.
A. LC-3 Assembly Parser and Symbolic Executor
Rather than requiring instructors to learn the KLEE API, we
developed a set of annotations on LC-3 assembly ﬁles through
which instructors can specify symbolic variables as well asconstraints, identify different types of memory regions (read-only, uninitialized but accessible, and so forth), and selecttypes of output to be compared between student and goldcodes to generate behavioral issues. Users can also overridethe default behavior, messages, and hints provided by thedifferent types of issues tested by KLC3, and can to a limitedextent deﬁne new issues. The input ﬁles are parsed along withcommand-line options to automatically generate the equivalentof the additional C code normally required for use with KLEE.
Our symbolic executor enables KLC3 to support direct
symbolic execution of LC-3 code even when that code isquestionable or obviously buggy. For example, a jump vio-lating subroutine call/return semantics can be executed ratherthan immediately terminating the state (some students did sointentionally), although such operations are not encouragedand KLC3 does raise a warning by default. With precisecontrol of each instruction, KLC3 is able to reproduce theexact behavior of the ofﬁcial LC-3 simulator (lc3sim), whichis critical when students debug their code using the generatedtest cases in lc3sim. Detection of more subtle problems, suchas using uninitialized registers, can also be performed in amore controlled manner.
B. Issue Detection and Equivalence Checking
KLC3 executes an LC-3 program and detects any im-
proper operations by the code, such as out-of-bound memory
accesses. When applied to a programming assignment andprovided with a correct version of the assignment solution(a gold version), KLC3 not only detects the problems inthe test program itself, which we call execution issues,b u t
also performs equivalence checking between the test programand the gold version, thus identifying any behavioral issues
between the two.
Execution issues typically indicate undeﬁned or irrepro-
ducible behavior or the possibility of a crash when theprogram executes. For example, KLC3 can detect the use ofuninitialized registers. When a program starts, all registers areconsidered to be uninitialized. If a test program state uses aregister without ﬁrst writing a value into the register, KLC3raises an issue for that state. Most execution issues arise inthe executor, but some rely on control ﬂow analysis, such asidentiﬁcation of improper subroutine structure, in which a stateexecutes a RET (return) instruction that does not return to the
instruction after the most recent JSR (jump to subroutine)
instruction.
The set of execution issues reported by KLC3 was devel-
oped based on experience with student code. Initially, wereported only a few common mistakes based on our ownexperience, such as reading uninitialized memory or registers.
21Fig. 4. Equivalence checking between test program and gold program.
Early versions of KLC3 were then tested on student codes
from a previous semester (and later on students during Fall2020 as they wrote their assignments), which helped us toidentify additional issues through manual analysis, such as us-ing a symbolic value for the program counter (PC), overwritinginstructions, and broken subroutine calls. After testing severalhundred student codes, we arrived at a stable set of issues,as detailed in the KLC3 manual (included in the replicationpackage [10]), which we may extend in future semesters.
Behavioral issues signify differences between the output
produced by a student’s code and that produced by the goldcode, for example, incorrect answers printed to the screen.These issues are identiﬁed by comparing symbolic equalityfor the display (an I/O device), memory, and registers aftera program terminates, as shown in Fig. 4. Speciﬁcally, whena state of the test program terminates normally (instead ofencountering a terminating issue), its ﬁnal path constraints areused to launch a state of the gold program. After the goldstate terminates, the equivalence checker module symbolicallycompares displayed output, memory, registers, and/or the lastexecuted instruction of the two states. If the gold state forksto multiple states, all of them are compared with the teststate. Assignments typically specify comparison for a subsetof these possible outputs, so only those outputs relevant to theassignment are compared, as speciﬁed in the KLC3 input ﬁles.Divergence in the outputs raises behavioral issues.
C. Generation of Test Cases, Scripts, and Reports
Just like KLEE, KLC3 generates concrete test cases that
reproduce detected issues. Issues are frequently triggered many
times due to forked states and repeated execution of instruc-tions. Ideally, each bug in a student’s code should be reported
exactly once. However, there is no easy way to localize bugsin the test code, particularly behavioral issues. Issues triggeredon the same instruction may not necessarily result from thesame bug, while a single bug may trigger a series of issues atdifferent instructions. To avoid overwhelming students withfailed test cases, we ﬁlter the set of issues produced bya student’s code before reporting them to the student. Inparticular, we report only one instance of any given executionissue at any location in a student’s program. In that way, forexample, if 50 states access illegal addresses at a particularload instruction, the student sees only one report and one testcase. For behavioral issues (such as incorrect output), whichcan’t be localized in the test code, only one instance of eachtype is reported from a single run.
For each reported issue, a description, the instruction that
triggers the issue (for execution issues), runtime information(such as the address accessed for memory issues and theoutput for incorrect output issues), and sometimes a hint aboutpossible ﬁxes for the issue, is provided to the student. Eachreported issue is associated with a subdirectory containing atest case (one or more assembly ﬁles) and an LC-3 script. Thetest case contains concrete values derived from the symbolicsubspace of the state that triggered the issue. The test casesare designed to be used with the LC-3 simulator (lc3sim),so students need understand nothing about KLC3 nor aboutsymbolic execution in general. Ideally, a test case follows thesame control path in lc3sim as it does in KLC3 and triggers thesame issues (except for a small number of pitfalls, as describedin the KLC3 manual [10]). The LC-3 script can be executed bylc3sim to help the student load the test case and reproduce thebug. Sample reports are available in the KLC3 manual [10].
VI. O
PTIMIZATIONS
Timeliness is critical for effective feedback. Our goal is to
provide feedback within 5 minutes after any submission. Staticanalysis in the VSCode extension is real-time, while achievingsuch an aggressive goal in symbolic analysis requires tuningof both the execution engine and input spaces. The raw speedof the execution engine dictates the number and length ofpaths that can be explored, and improving that speed enablesexploration of larger input spaces. Tightly-constrained inputspaces ﬁnish quickly, but may not expose bugs, while an overlygeneral input space may make KLC3 run out of time exploringcorrect paths, thus again failing to expose bugs.
Most of our KLC3 optimizations had not been developed
in time for the Fall 2020 deployment, forcing us to use fairlysmall input spaces, particularly for D
FS, and thus to miss
some bugs in our analysis. Using the code samples that wecollected, we have been able to signiﬁcantly improve KLC3’sperformance, and are now able to fully explore much largerspaces while meeting our 5-minute goal for most submissions.
In this section, we describe two of the more interesting
optimizations: implementation of an additional cache withinone of the lower layers of KLEE, and extension of the loopreduction algorithm [6] to assembly language in order tosidestep path explosions within loops.
A. IndependentElementSet Cache
The IndependentSolver module of KLEE removes irrelevant
constraints from SMT queries before passing them to the
next-level solver [2]. To identify the relevant constraints, thesolver iterates through the query expression to construct anIndependentElementSet, a set of symbolic variables that areinvolved in each constraint and the query expression.
We noticed that student code samples produced large num-
bers of IndependentElementSets: for the 909 D
FSsamples that
22TABLE III
TIMEOUT RATES FOR 960 D FSSAMPLES WITH AND WITHOUT THE
INDEPENDENT ELEMENT SETCACHE
Timeout in Minutes Cache No Cache
5 13.54% 65.10%
10 5.31% 22.08%
assemble and require 10 minutes or less to analyze, the average
number of IndependentElementSets constructed is 2.20×108.
Constructing such a large number requires substantial time.We also noticed that student code samples led to signiﬁcantoverlap in the constraints on queries issued by KLC3. Forexample, the constraints deﬁning the input space are includedin every query issued to the IndependentSolver. Given the costof construction and the overlap in constraints, we decidedto investigate adding a cache that maps symbolic constraintexpressions to IndependentElementSets.
An effective cache must have a reasonably high hit rate
and speed when results are cached. As KLEE infrastructureuses dynamic allocation for symbolic expression instances,cache comparisons can be either pointer-based or value-based.Pointer hashing and comparison are fast, but fail to matchidentical expressions if they are constructed separately (indifferent states, for example). Value-based hashing and com-parison require walking through the nested expressions, whichtakes more time. We evaluated both approaches (and also ahybrid) and found that, for the purpose of KLC3, pointer-basedcomparison achieves a high hit rate and provides a substantialperformance boost for most student codes.
For the 960 D
FSsamples that assemble, we measured the
KLC3 analysis time up to 10 minutes with the cache enabled,and up to 15 minutes with the cache disabled (to captureperformance information more accurately). In light of our5-minute feedback goal, we summarize the fraction of samplesthat require more than 5 and 10 minutes of analysis in Tab. III.We then eliminated samples that timed out as well as those thatﬁnished within 5 seconds (to reduce measurement errors). Forthe remaining 803 samples, the fraction of baseline (no cache)analysis time required with the cache appears in Fig. 5. Thegeometric mean of the fraction is 0.288—an average speedup
of3.47× . The cache hit rate is over 99.9% for more than
98.5% of the D
FSsamples. Use of the cache for S CHEDULE
samples shows a similar result: for 1239 samples that neithertime out nor ﬁnish within 5 seconds, the geometric mean ofthe analysis time ratio is 0.273—a speedup of 3.67× .
B. Loop Path Reduction on LC-3 Programs
Loop reduction is effective in reducing KLEE execution
time while maintaining high code coverage and bug detectionfor student C programs [6]. The key observation behind loopreduction is that even simple control ﬂow within a loop bodycan produce an exponential number of paths over multipleloop iterations, but rarely are most such paths relevant toidentifying bugs. Loop reduction identiﬁes all paths througha loop body and prioritizes execution of states that coverpreviously unexplored paths through the loop body, while at
Fig. 5. Impact of the IndependentElementSet cache: distribution of analysis
time reduction ratios for 803 D FSsamples. The orange line shows the
cumulative fraction of samples with reduction ratios below a given value.
the same time de-prioritizing or even avoiding execution of
states that follow paths through the loop body that have alreadybeen covered by other states. The original loop reductionalgorithm assumed that every loop had only a single entrypoint, which holds for any C program that doesn’t use goto
statements, and is also true in most C programs that do makeuse of goto (generally for exception handling).
In contrast, the single entry point assumption for loops fails
to hold for many programs written in assembly language.Without C statements such as if for, and while, loops in
assembly are constructed purely from branches and jumps,generally resulting in more complicated control ﬂow and lessclearly deﬁned structure. As a result, we had to generalize theloop reduction algorithm to accommodate the more ﬂexibleforms of control ﬂow used by our students (and in ourown solutions to the assignments). We then implemented thegeneralized version of loop reduction and tested it on thesamples collected from our class.
Loop structures in an LC-3 program must be identiﬁed and
extracted automatically based on control ﬂow analysis. Theﬁrst step is to construct a control-ﬂow graph (CFG), in whichthe nodes are basic blocks (sequences of instructions witha single entry point and no control ﬂow except for the lastinstruction) and arcs connect each node to any other node thatmay follow it in dynamic execution. In this paper, we consideronly the context of control ﬂow within a single, well-deﬁnedsubroutine (we do not apply the algorithm to codes in whichsubroutine structure is deﬁned improperly, whether we detectsuch behavior statically or dynamically).
Starting with the CFG for a subroutine, we identify all
strongly connected components (SCCs). Each SCC forms oneor more of the outermost loops within the subroutine. Foreach SCC, we then identify each possible entry point—theCFG nodes at which arcs from outside the SCC arrive. Foran SCC S, let’s call one such entry point e. The node e
forms one outermost loop, L(e, S). Other entry points may
form additional loops with the same code (the SCC S); often,
these are used to implement similar but separate operations inthe assembly code. A C compiler might produce such code ifit found common subexpressions or common code sequences
23within a single subroutine, for example.
For each loop L(e, S), we identify all paths through a single
iteration of L. In one iteration, loop Lcan either terminate or
continue to another iteration. Continuing to another iteration
we interpret as returning to e, and the Head to Head (H2H)
paths are deﬁned as all paths that form simple cycles within S,
starting and ending at e. Similarly, the Head to Exit (H2E)
paths are deﬁned as all simple paths that start at eand exit S
(without returning to e). We can then deﬁne the exit nodes for
loop L(e, S)as the set of CFG nodes outside of Sat which
one or more of the H2E paths of loop Lterminate.
The nested loops (subloops) of a loop L(e, S)consist of
all loops in S\{e}. In other words, a subloop has at least
one H2H path in its parent loop that doesn’t pass throughthe entry point of the parent loop. For each loop, we identifynested loops by executing recursively on the CFG induced bythe nodes in S\{e}, noting that the H2E paths of a subloop
may also exit any number of containing loops as well.
During static analysis, we identify loop paths as follows.
We deﬁne an H2H(H2E) segment of a loop as an H2H(H2E)path of the loop, but with each arc within any subloop replacedby a single virtual edge from the entry node to the exit node.In other words, the subpath in the subloop is invisible to theparent loop except for the entry and the exit nodes. The numberof H2H(H2E) segments in a parent loop can be less than thenumber of H2H(H2E) paths, as multiple subpaths through asubloop are counted as one segment in the parent loop if thesubpaths share the same exit node.
As loop analysis is static, dynamic jumps are not allowed.
Subroutine calls are assumed to return and are summarized asa single edge from the call to the next instruction in memory(at the return address). Beginning with the most nested loops,loops and their segments are identiﬁed through depth-ﬁrst-search (DFS) starting from each subroutine’s entry point. Thealgorithm is shown in Fig. 6.
We implemented the sample coverage update algorithm
from [6] to track loop coverage in KLC3. The algorithmuses a stack to record the current loop nest in each stateand to update H2H and H2E coverage. We also implementedthe StatePruningSearcher from [6], except that postponed
states are selected randomly for reactivation, without check-ing constraint compatibility with the uncovered path. UnlikeLLVM IR, LC-3 uses only the sign of the last operation’sresult to control conditional branches. Branch outcomes andconstraints thus depend on both the preceding instructions aswell as the control ﬂow path into the branch, which is hard todetermine without actually executing postponed states. Ratherthan making complicated speculations and incurring additionalsolver overhead, we chose to select a state randomly.
Among 1363 S
CHEDULE samples, generalized loop re-
duction reduced analysis time for 307 (22.52%) of them.Excluding 26 samples that reported 0-second analysis, theaverage speedup for the samples that beneﬁt is 9.67× faster
than the original DFS search heuristic. After acceleration, theanalysis time of the 307 samples ranges from 0 to 16 seconds,with 91.21% (280) ﬁnishing within 5 seconds and an aver-1:function ANALYZE LOOP DFS(u,G,parent ,path )
⊿Gis the parent loop SCC excluding its entry node
2:l←none
3: ifuis the entry of a known loop l0then
4: ifl0⊆Gthen ⊿required for l0to be a subloop
5: parent.subLoops ←parent.subLoops ∪{l0}
6: l←l0
7: else
8: ifu∈SCCSofGof size>1then⊿found a new subloop
9: l←new loop with entry node uand SCC S
10: parent.subLoops ←parent.subLoops ∪{l}
11: ANALYZE LOOP DFS(u,S\{u},l, empty path)
12: iflis not none then ⊿reached existing/new subloop
13: E←l.h2eEdg ⊿skip paths in the subloop
14: else ifuis a subroutine call then
15: E←{(u,u.next )}⊿skip subroutine and assume it returns
16: else
17: E←u.outEdges
18: for all (u,v )∈Edo
19: ifv=parent.entry then ⊿reach the the parent’s entry
20: parent.h2hSeg ←parent.h 2hSeg∪{path +(u,v)}
21: else ifv/∈Gthen ⊿exit the parent loop
22: parent.h2eSeg ←parent.h2eSeg ∪{path +(u,v)}
23: parent.h2eEdg ←parent.h2eEdg ∪{(parent.entry,v )}
24: else ⊿still in the parent loop
25: ANALYZE LOOP DFS(v,G,parent ,path +(u,v))
Fig. 6. Analysis algorithm for generalized loop reduction.
age of 2.30 seconds. Among 960 D FSsamples, 69 (7.19%)
samples were accelerated. Again excluding 4 samples whichreported 0-second analysis, the average speedup is 37.89× ,
the resulting analysis times range from 0 to 63 seconds, andthe average time is 4.55 seconds.
We did observe one D
FSsample for which loop reduction
ﬁnished early but reported no issues, whereas issues werefound without loop reduction. Considering how quickly theanalysis ﬁnishes for the samples that beneﬁt from loop re-duction, we believe that we can simply execute the DFSsearch heuristic if loop reduction terminates without ﬁndingany issues in a student’s code.
The fraction of LC-3 samples that beneﬁt from loop reduc-
tion is lower than we had expected based on the C programsreported in [6]. While investigating this issue, we found thatmany of our samples contain unrolled nested loops, whichproduce large numbers of segments in the outer loops. Someof these segments may be difﬁcult or impossible to cover witha limited input space. In fact, some samples contain provablyuncoverable loop paths, sometimes due to poor style, such asconsecutive branches based on the same condition. While wehave found many such samples by hand, however, we have yetto identify fast and efﬁcient ways to eliminate the impossiblesegments automatically, as most of the issues are more subtlethan consecutive branches.
As another effect of samples with many paths, loop analysis
can sometimes add signiﬁcant time, on the order of tens ofseconds. However, since the programs that beneﬁt from thetechnique are those for which the analysis ﬁnishes quickly, wecan set a limit of a few seconds on analysis time and therebyavoid any practical impact on the KLC3 response time.
24VII. T ESTING AND DEBUGGING ENVIRONMENT
Programmers often rely on “cyclic debugging” [11], in
which a program is relaunched to reproduce a single bug.
However, restarting a program doesn’t necessarily produce thesame bug at the same place. Such difﬁculties are especiallyconfusing for novice programmers. On the other hand, evenwhen a bug can be reproduced, identifying its source oftenrequires some kind of unexpected program behavior to benoticed, whereas the “reason” for the bug occurs earlier inthe program’s execution, forcing the programmer to use acombination of re-execution and reasoning to determine thecause. Enabling a debugger to support reversing executionback to the cause of the bug thus becomes attractive.
Students debug LC-3 programs by executing them under
the control of the LC-3 simulator, lc3sim. In order to makedebugging easier for our students, we extended this simula-tor to support reverse execution, sometimes called back-in-time or omniscient debugging. Two main approaches havebeen developed to support reverse execution: recording andreconstructing [11]. The recording method saves necessarytrace information for each step of execution and uses thisinformation to “undo” the effect of each step when executingin reverse. This method usually results in a large log and oftenrequires hardware support for acceptable performance. Thereconstructing method instead saves checkpoints (full stateinformation) during forward execution. To perform reverseexecution, the method reloads the closest checkpoint before thedesired reverse execution stopping point, then executes in theforward direction to reach that stopping point. Reconstructingrequires less log information and less hardware support, butcheckpoint positions must be chosen carefully, making theapproach less ﬂexible than the recording method.
We chose to implement the recording method in the LC-3
simulator. LC-3 is a 16-bit ISA with 8 general purpose regis-ters, so the architectural state is small, allowing state changesto be recorded in a compact log. And, as the LC-3 simulatoruses software to simulate execution of LC-3 instructions,recording trace information at the ISA level does not addsubstantial overhead to instruction execution. We also felt thatstudents beneﬁt from the speed of reverse execution based onthe recording approach, in which single instructions can beexecuted at approximately the same speed in both directionsin time, allowing students to go easily back and forth in theircode’s execution trace.
The reverse execution functionality relies on two modules
of the original LC-3 simulator: the ﬁrst, the user interfacemodule, handles three kinds of commands: information com-mands, management commands, and execution commands.Reverse execution is closely related to the execution com-mands that instruct the execution of the LC-3 instructions.By design, these commands are similar to the “step”, “next”,and “continue” commands available in most debuggers. Thesecond module, the execution module, simulates the executionof LC-3 instructions.
We upgraded both modules with reverse execution func-tionality and documented the changes for students in a newLC-3 tool manual (provided as supplemental material). For theexecution module, we added recording of state changes causedby each instruction’s execution. An LC-3 instruction causesat most four registers and one memory location to change.We record the original values and the address of the changedmemory location (if any) when executing an instruction. Eachof these sets of changes is small and sufﬁces to revert the effectof the execution step. The recording cache is implemented asa cyclic-array with enough space for all reasonable studentcodes.
In the user interface module, we added a new category
of reverse execution commands. For every forward executioncommand, such as “step” and “continue,” we implemented areverse version, such as “rstep” (reverse step) and “rcontinue”(reverse continue). The semantics of each new command wereselected carefully to be symmetric with the forward executioncommands. For example, just as “ﬁnish” executes through theRET instruction at the end of the current subroutine and returnsto the caller, “rﬁnish” executes in reverse back to the callsite that entered the current subroutine. The LC-3 simulatoralso has a graphical interface version, in which simulatorcommands appear as buttons. We also upgraded this interfaceto include command buttons for reverse execution.
After ﬁnishing the implementation, we were pleased to real-
ize that our extension can also simplify grading procedures. Inparticular, some information is lost when a program executesto completion in the simulator, making it difﬁcult to test thatinformation. For example, our S
UBROUTINES assignment tests
students’ understanding of caller- and callee-saved registers,and register values are checked as part of grading. Doing soin certain cases requires asking students to add speciﬁc labelsto their code so that grading scripts can check register valuesafter setting a breakpoint at the labels. If a student fails toinclude the label, or puts the label in the wrong place, staffmust fall back on time-consuming manual grading (with aminor penalty for the student). Using reverse execution, wecan simply back out of the changes made when the studentprogram terminates, revealing the ﬁnal register values left inplace by the student’s code.
The LC-3 tools provided with the textbook assume the
availability of a environments that novice programmers maynot yet have learned to use, such as Unix or Cygwin. To makethe tools more accessible to our students, we explored theautomatic translation of these programs into JavaScript andWebAssembly for use through a web browser. We made useof Emscripten [7] to do so. We started by translating the LC-3assembler and simulator into JavaScript modules, enabling oursystem to manage their use and execution lifetime. We thenreplaced the standard Unix ﬁlesystem used in the tools withan in-memory ﬁlesystem, adding import and export commandsto the web interface so that instructors (and, eventually, ourKLC3 feedback system) can populate the ﬁlesystem with apredeﬁned set of ﬁles. Finally, we implemented a modern webinterface to the tools to enable students to make use of themwithout the need to install a Unix-like platform and then to
25Fig. 7. Queueing delay for all Fall 2020 code samples.
Fig. 8. KLC3 analysis time for the 3188 samples from Fall 2020 that
assemble. We set a time limit of 5 minutes (300 seconds).
download and build the tools themselves. Supporting this inter-
face from the original C code required a few modiﬁcations, asthe textbook’s GUI tool is based on Tcl/Tk and communicatesthrough pipes with the simulator.
VIII. F
EEDBACK TIMING
Our goal is to provide feedback on each student submission
within 5 minutes. The time required from a student’s point ofview includes not only analysis by KLC3, but also queueingdelays in the job dispatcher and other system components.Failures and downtime also contribute to perceived delay, andwhile the prototype deployed in Fall 2020 did suffer fromseveral outages, most of the bugs have been tracked down andeliminated, leaving the system reasonably stable.
We examined queueing time as recorded in our logs for
all submissions in the Fall 2020 semester to produce thehistogram in Fig. 7. The vertical scale is logarithmic, andthe distribution is dominated by delays of 10 seconds or less.Based on this data, we believe that our virtual server withfour processors and 8 GB of DRAM more than sufﬁces fora class of 100 students. While we expected more signiﬁcantdelays near deadlines, in fact the value of the feedback seemsto have convinced students to work earlier, thus spacing outtheir submissions in a way that enabled them to digest and acton KLC3 feedback reports.Looking forward, in Fig. 8 we show the distribution of
KLC3 analysis times using all KLC3 optimizations currentlyimplemented for the full set of 3188 samples that assemble(the remaining 360 require a negligible amount of time todetermine that assembly fails). The input spaces are deﬁned tobe general enough to fully explore student code samples, andare substantially larger than those used during the Fall 2020semester. For these data, we set a time limit of 5 minutes onKLC3, leading to timeouts for 130 (4.08%) samples. For eachsample that timed out, however, KLC3 reported issues.
IX. D
ISCUSSION
A. Survey Results
As the assembly assignments occupy only about the ﬁrst
third of the semester, ample time remained to survey studentsanonymously on their opinions about the tools. Roughly aquarter of the students answered the survey.
Few analytic features of our VSCode extension were ready
in time for students, and our distribution method was pri-vate, requiring manual updates. Nevertheless, roughly half ofrespondents had used our VSCode and our extension ratherthan other editors to write their programs, and 80% of thoseusers found the extension helpful. As described earlier, wehave since extended both the syntactic and static analysesand have added style-speciﬁc features such as identiﬁcationof unrolled loops, which we only realized were a major issueafter analyzing student codes more carefully.
Survey respondents generally found KLC3 feedback to be
useful in identifying and understanding their bugs, particularlywhen bugs were subtle. One student mentioned, for example,that KLC3, “...reminded me of an extreme case that I wouldotherwise neglect.” As many bugs occur for corner cases,helping students to think more carefully about their programsis also a positive outcome. We were also surprised by studentcomments on the ﬂow charts (visualizations of control ﬂow)produced automatically for their code by KLC3. These werecreating as a debugging aid for us, to help us to understandstudent code, but the students themselves also found themuseful in identifying differences between the intended andactual control ﬂow. We hadn’t expected novice programmersto be able to make use of them, but the class does deﬁne andencourage the use of ﬂow charts in understanding programs,and several respondents commented positively about theirinclusion in the KLC3 reports.
Two-thirds of respondents made use of our new browser-
based interface to the assembler and simulator. However, whenmaking use of the KLC3 reports to debug their code, studentspreferred to use the traditional simulation and execution tools,perhaps because of the lack of scripting support in the browserinterface at the time. We have rectiﬁed that lack, and plan toenable KLC3 reports to be pulled directly into the browseralong with the scripts needed to reproduce any speciﬁc failure.
Although support for reverse debugging was available only
in time for the third assignment, D
FS, half of the students
made use of it in completing that assignment. We werepleasantly surprised by this number and expect more students
26and instructors to ﬁnd the functionality useful. Although the
idea has been around for decades [11], and reverse debuggingis supported in open source debuggers such as GDB, it isturned off by default for performance reasons.
The survey also produced a number of interesting com-
ments. First, although we had already kept response timesfrom the server to minutes, students still wanted faster results.Some of this attitude may arise from instances in whichour prototype system was unavailable, sometimes for severalhours. However, as illustrated in Sec. VI, we have alsosigniﬁcantly improved the time required for analysis sincethe class deployment. Second, although we implemented per-student regression testing and rate-limited new submissionsto deter students from making guesses about their programs,students did seem to rely on KLC3 for ﬁnding bugs ratherthan developing their own tests, going so far as to expresssurprise when the tool (with a limited input space) missedbugs that showed up during grading. About half of the respon-dents admitted to relying completely on KLC3 for identifyingbugs. Philosophically, providing more deﬁnitive guidance inan introductory class may be acceptable. Alternatively, byrestricting the input space used by KLC3, one can leave certainaspects of testing to the students themselves. Either approach iseasy to deﬁne and to use based on the input scripting languagedeveloped for KLC3.
B. Comparison with Samples from Fall 2018
To understand the effect of our system on student’s ability
to produce correct code, we compare code samples from 2020
with those produced by students in the Fall 2018 offeringof the course, before the development of our tools. Manyaspects of the two courses were the same: the same instructorpresented the same material (albeit in person in 2018, ratherthan over Zoom as in 2020), and the assignments were nearlyidentical, with slight modiﬁcations to the ordering of inputs,the meanings of speciﬁc bits, and the exact output formatrequired. As noted in Sec. III, the differences in assignmentsmay introduce some difference in behavior, as might thedifferences in the student populations and the TAs.
For the Fall 2020 samples, we extracted the ﬁrst and the last
commits that assemble from each student’s commit sequenceof S
UBROUTINES and D FSsamples, then computed the frac-
tions on which KLC3 reports any memory-related warnings(such as accessing uninitialized memory) or any error (suchas incorrect output or timeout). Results are presented in Fig. 9using blue triangles to indicate the percentage of students witheach type of issue. Generally, student code improved from ﬁrstto last commit, as expected.
In Fall 2018, students submitted their code once, after they
had ﬁnished testing and debugging it. For comparison, weadjusted the symbolic input spaces to match the speciﬁcationchanges and executed KLC3 on each of these submissions.The orange lines in Fig. 9 show the percentage of studentcodes exhibiting each type of issue.
In the S
UBROUTINES assignment, student subroutines were
required to preserve the values of most registers. A code
Fig. 9. Percentage of student code samples on which KLC3 reports warnings
or errors. Samples that fail to assemble are excluded.
fragment provided with the assignment (in both semesters)
tested one register’s value across a subroutine call to showstudents how such testing could be accomplished. To test theircode, students needed to adapt that code to the other registers.In Fall 2018, this code fragment was the only method providedto help students test their programs, yet 19.28% of the studentsdid not make use of the test provided, as shown in the top-leftplot in Fig. 9. In Fall 2020, students also received feedbackabout incorrectly modiﬁed register values from KLC3. Thefraction of the ﬁrst commits that assemble from Fall 2020 withincorrectly modiﬁed registers is 30.21%, higher than that ofFall 2018, while the fraction of the last commits that assembleis much lower, only 1.04%.
The higher initial rate of errors in 2020 may indicate
that students preferred the feedback system over performingtheir own testing, even in the early stages of development.Alternatively, since KLC3 executed automatically wheneverstudents committed their code, students may have simplywanted to preserve a copy of their work before beginning totest. Regardless, the differences in the ﬁnal submission showthat the targeted feedback and test cases were more effectivein identifying problems in their code as they made progresson testing and debugging. A similar pattern is observed forthe other errors of S
UBROUTINES and the errors of D FS.
Memory warnings show somewhat different behavior. For
these, the rates among students in the Fall 2020 class for bothassignments were lower in their ﬁrst commits that assemblethan in the ﬁnal submissions (again, our only data point) forthe students in 2018. We believe that this behavior resultsfrom changes in the assignment speciﬁcations: while studentsare never encouraged to use memory outside of the speciﬁc re-gions deﬁned in the speciﬁcations, the 2018 speciﬁcations didnot explicitly forbid such use, whereas the 2020 speciﬁcationsdid. This ﬁnding shows the potential for improving pedagogywith insights from students’ submissions.
Although the fractions of erroneous submissions decrease,
the overall impact of automatic feedback on student learningremains an open area for further investigation. Ideally, suchimpact should be evaluated through longitudinal assessment,in which students who have learned with and without suchfeedback are compared in terms of their proﬁciency in later
27classes or even in their careers. We plan to deploy the system
again in near future to further investigate this topic.
C. Comparison with Web-CAT
Many universities now use Web-CAT [1] rather than simple
execution of test vectors in programming classes. The key idea
behind Web-CAT is to have students generate their own tests,in a manner similar to Test-Driven Development. Like KLC3,Web-CAT relies on the availability of a gold version of anassignment—a solution.
Students are then evaluated based on a combination of the
validity and correctness of their tests along with coverage ofthe gold version. In particular, validity evaluates the student-provided tests against the gold version of the code, whilecorrectness evaluates them against the student version of thecode. Coverage measures the degree to which a student’s testsfully exercise the gold version, and can in practice be basedon code coverage, branch coverage, or even on something likea full test set from symbolic execution.
Web-CAT’s focus on encouraging and rewarding test de-
velopment by students is an interesting and valid goal, butwe believe that some of the more subtle errors and variationsintroduced by novice programmers are likely to be missed bysuch a system. For example, we note that roughly a third ofstudents’ ﬁnal submissions of D
FSstill suffered from some
type of memory error in Fall 2020 (see Fig. 9). While sucherrors may effect output, often they have no direct impacton program behavior. Some such errors are akin to out-of-bounds array accesses in languages like C, which may or maynot cause a program to produce incorrect results. Often, thebehavior ends up depending on the compiler, operating system,or even on the ISA. In the case of LC-3 code, for example, thesimulator initializes most memory locations to 0, so studentcode with erroneous behavior often works ﬁne, or works theﬁrst time, but not the second time, and so forth.
In our experience, and also as reported in [4] for C pro-
grams, using gold code coverage as the basis for student codeevaluation is also limiting. Test generation using KLC3 with asymbolic input space on the gold program typically generatesa superset of the tests required for code or branch coverage,but even those tests do not uncover all bugs in student code,even when combined with a similar set generated by KLC3with the student code itself. In particular, while all possiblecode paths are covered by the tests generated by KLC3, theactual tests consist of speciﬁc input vectors, and the vectorsthat differentiate the student and gold versions may be missed.Equivalence checking, as described in this paper (and by theearlier work on C) speciﬁcally targets such vectors, and is thusbetter able to identify subtle behavioral differences.
Nevertheless, we recognize that Web-CAT may be better
in avoiding the tendency of students to rely on the feedbacksystem for testing. In the future, it may be interesting tocombine the two approaches, using something like Web-CATto provide initial feedback, but switching to KLC3 afterstudents have surpassed some threshold with their own tests.D. Alternative Designs
Early in our project, we considered an alternative approach
to handling LC-3 programs with KLEE: translating LC-3 codeto C (speciﬁcally, to an LC-3 virtual machine implementedin C), then compiling to LLVM IR and using KLEE. Aftersome initial research, we decided on the more direct approachdescribed in this paper. The LC-3 ISA differs from C in severalnoteworthy ways: LC-3 programs operate directly on registersand make explicit accesses to the 16-bit addressable memory,while registers and addresses are managed by compilers andhidden from C programmers. Also, LC-3 assembly can containdirect jumps to arbitrary memory locations (using JMP or
JSRR), while C offers only the limited goto statement.
Finally, subroutines calls in C rely on the concept of stackframes, which are not explicit in LC-3. We feel that theadditional indirection implied by mapping LC-3 code througha virtual machine and then through a C compiler to LLVMwould add too much overhead to analyzing the relativelysimple programs produced by our students. The complexity ofensuring consistent behavior as well as inverting the mappingto explain issues found in the ﬁnal version clearly to studentsin terms of their original code is also somewhat daunting.
E. Pushing Optimizations into KLEE
The authors of KLEE have also noted the possible need
for an IndependentElementSet cache as a comment in the
source code. To investigate the value for C programs, wetranslated one author’s D
FSsolution into C, compiled it using
Clang (-O2), and executed it with KLEE on the same symbolicinput space. In that form, the code required construction ofonly 3.01×10
6IndependentElementSets, 73.1× fewer than
did the LC-3 version. The gap may be due to differencesbetween LC-3 and the LLVM IR: LC-3 lacks multiplicationinstructions, comparison instructions (use condition codesinstead), and other features. Consequently, more LC-3 in-structions are needed to implement the same functionality.Compiler optimizations also reduce the number of instructions.The average number of LC-3 instructions executed by KLC3for 909 D
FSsamples is 2.41×108, while KLEE executed
only 5.05×106LLVM IR instructions for the C version. The
impact of the cache is therefore less pronounced, although webelieve that the cache may be useful in analyzing certain typesof C programs.
X. C
ONCLUSION
Considering both student feedback and our success in using
the system to deliver feedback based on symbolic evaluation ofstudent submissions within our goal of 5 minutes, we plan tomake continued use of these tools in future classes and to makethem available to others using LC-3 to teach programming.Towards that end, we provide a replication package [10] withsource code and manuals for all components, as well as theassignments and several sample solutions for each. Detailedimplementations and discussions of the tools can also be foundin the authors’ theses [12][13][14][15].
28REFERENCES
[1] S. Edwards, “Using test-driven development in the classroom: Providing
students with automatic, concrete feedback on performance,” in Proc.
Int’l Conf. Education and Information Systems: Technologies and
Applications (EISTA 03), Aug. 2003. [Online]. Available: https://web-cat.org/
[2] C. Cadar, D. Dunbar, and D. Engler, “Klee: Unassisted and automatic
generation of high-coverage tests for complex systems programs,” inProceedings of the 8th USENIX Conference on Operating SystemsDesign and Implementation, ser. OSDI’08. USA: USENIX Association,2008, p. 209–224.
[3] Y . Patt and S. Patel, Introduction to Computing Systems: From Bits and
Gates to C and Beyond, 2nd ed. McGraw-Hill Higher Education, 2004.
[4] J. Gao, “Use of symbolic execution as automated grading
tool for introductory programming courses,” Ph.D. dissertation,University of Illinois at Urbana-Champaign, 2019. [Online]. Available:http://hdl.handle.net/2142/105768
[5] A. Luxton-Reilly, Simon, I. Albluwi, B. A. Becker, M. Giannakos,
A. N. Kumar, L. Ott, J. Paterson, M. J. Scott, J. Sheard, andC. Szabo, “Introductory programming: A systematic literature review,”inProceedings Companion of the 23rd Annual ACM Conference
on Innovation and Technology in Computer Science Education,ser. ITiCSE 2018 Companion. New York, NY , USA: Associationfor Computing Machinery, 2018, p. 55–106. [Online]. Available:https://doi.org/10.1145/3293881.3295779[6] J. Gao and S. S. Lumetta, “Loop path reduction by state pruning,”
inProceedings of the 33rd ACM/IEEE International Conference on
Automated Software Engineering, ser. ASE 2018. New York, NY ,USA: Association for Computing Machinery, 2018, p. 838–843.[Online]. Available: https://doi.org/10.1145/3238147.3240731
[7] A. Zakai, “Emscripten: An llvm-to-javascript compiler,”
https://emscripten.org/.
[8] “Visual studio code: Code editing. redeﬁned.”
https://code.visualstudio.com/.
[9] “go-git: A highly-extensible git implementation library written in pure
go,” https://github.com/go-git/go-git.
[10] Z. Liu, T. Liu, Q. Li, W. Luo, and S. S. Lumetta, “Replication package
of the lc-3 automatic feedback system,” Aug. 2021. [Online]. Available:https://doi.org/10.5281/zenodo.5337117
[11] J. Engblom, “A review of reverse debugging,” in Proceedings of the
2012 System, Software, SoC and Silicon Debug Conference, 2012, pp.1–6.
[12] Z. Liu, “Using concolic execution to provide automatic feedback on lc-3
programs,” 2021. [Online]. Available: http://hdl.handle.net/2142/110284
[13] T. Liu, “Improved feedback and debugging support for
student assembly programming,” 2021. [Online]. Available:http://hdl.handle.net/2142/110283
[14] Q. Li, “Vscode extension for lc-3 programming,” 2021. [Online].
Available: http://hdl.handle.net/2142/110287
[15] W. Luo, “In-browser lc-3 toolchain and queue management for symbolic
testing,” 2021. [Online]. Available: http://hdl.handle.net/2142/110286
29