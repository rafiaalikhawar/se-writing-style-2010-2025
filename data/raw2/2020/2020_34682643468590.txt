An Automatic Refactoring Framework forReplacing
Test-Production InheritancebyMockingMechanism
Xiao Wang
xwang97@stevens.edu
StevensInstitute of Technology
Hoboken,NJ, USALuXiao
lxiao6@stevens.edu
StevensInstitute of Technology
Hoboken, NJ, USATingting Yu
tingting.yu@uc.edu
Universityof Cincinnati
Cincinnati,OH,USA
Anne Woepse
anne.woepse@ansys.com
AnalyticalGraphics, Inc.
Exton, PA,USASunnyWong
sunny@computer.org
AnalyticalGraphics, Inc.
Exton, PA,USA
ABSTRACT
Unittestingfocusesonverifyingthefunctionsofindividualunitsof
asoftwaresystem.Itischallengingduetothehighinterdependen-
ciesamongsoftwareunits.DevelopersaddressthisbymockingÃ
replacing the dependency by a Å‚fakeÅ¾ object. Despite the existence
of powerful, dedicated mocking frameworks, developers often turn
to a Å‚hand-rolled" approachÃinheritance. That is, they create a
subclass of the dependent class and mock its behavior through
method overriding. However, this requires tedious implementation
andcompromisesthedesignqualityofunittests.Thisworkcon-
tributesafullyautomatedrefactoringframeworktoidentifyand
replace the usage of inheritance by using MockitoÃa well received
mockingframework.Ourapproach is built upontheempiricalex-
perience from fiveopen source projects that use inheritance for
mocking. We evaluate our approach on fourother projects. Results
showthatourframeworkisefficient,generallyapplicabletonew
datasets,mostlypreservestestcasebehaviorsindetectingdefects
(intheformofmutants), and decouples testcodefromproduction
code.Thequalitativeevaluationbyexperienceddeveloperssuggests
that the auto-refactoring solutions generated by our framework
improvethequalityoftheunittestcasesinvariousaspects,suchas
makingtestconditionsmoreexplicit,aswellasimprovedcohesion,
readability,understandability,andmaintainability withtest cases.
CCS CONCEPTS
Â·Software and its engineering â†’Software testing and de-
bugging;Softwaredesigntechniques ;Softwaremaintenance
tools;Maintaining software ;Softwareevolution .
KEYWORDS
software refactoring, software testing
Permissionto make digitalor hard copies of allorpart ofthis work for personalor
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACM
mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,
topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ESEC/FSE â€™21, August 23Å›28,2021, Athens,Greece
Â©2021 Associationfor Computing Machinery.
ACM ISBN 978-1-4503-8562-6/21/08...$15.00
https://doi.org/10.1145/3468264.3468590ACMReference Format:
XiaoWang,LuXiao,TingtingYu,AnneWoepse,andSunnyWong.2021.An
Automatic Refactoring Framework for Replacing Test-Production Inheri-
tancebyMockingMechanism.In Proceedingsofthe29thACMJointEuropean
Software Engineering Conference and Symposium on the Foundations of Soft-
wareEngineering(ESEC/FSE â€™21), August 23Å›28, 2021,Athens, Greece. ACM,
NewYork, NY, USA, 13pages.https://doi.org/10.1145/3468264.3468590
1 INTRODUCTION
Software testing is a critical element of software quality assur-
ance [24,64]. Unit testing is an important phase of testing that
focusesonindividualunitsofasoftwaresystem[ 70].Auniquechal-
lenge to unit testing is that software elements are inter-dependent
on each other [ 36,70]. That is, when testing one function, we have
toconsideritsdependenciestootherfunctions.Thishindersour
ability to test easily and promptly. For example, the function under
test(FUT)maydependonanexternaldatabasethathasnotbeen
deployed.This challengealso appliesto debuggingÃ ifaunittest
fails, it is unclear whether the failure is caused by the fault in FUT
orits dependent functions.
A general methodology to address this challenge is isolating
the coreFUTfrom its dependencies throughmocking[ 73,74], i.e.,
replacing the dependencyby a Å‚fakeÅ¾ object. For example,instead
ofwaitinguntiltheexternaldatabaseisdeployed,developerscreate
a Å‚fakeÅ¾ database with dummy data populated in a local file system
andcontrolitsbehaviortoserveforthetestingpurposes.Thereare
variousdedicatedmockingframeworks,suchas easyMock ,Mockito,
andPowerMock [1Å›3], which provide well constructed solutions to
isolateFUTfrom its dependencies. Specifically, they provide pow-
erfulfunctionsallowingdeveloperstoeasilycreatemockobjects,
control their behavior, and verify the execution/status of the mock
objects.Theseframeworksworktogetherwithclassicautomated
unittestingframeworks, such as JUnit[ 4]andPyUnit [ 5].
Despitetheexistenceofpowerfulmockingframeworks,develop-
ersoftenturntoaÅ‚hand-rolledÅ¾approachÃinheritance[ 68].That
is,tocreateaÅ‚fakeÅ¾object,developerscreateasubclassofthede-
pendent production class and control its behavior through method
overriding. For example, in the nineopen source projects exam-
ined inthis study(Section 3.1and Section 6.1),developers already
adopt an existing mocking framework for testing their systems.
However, in about half of the cases when mocking is potentially
needed, developers still use inheritance instead of using a mock-
ing framework. The problem is that inheritance is not intended
540
ESEC/FSE â€™21, August 23â€“28, 2021,Athens,Greece XiaoWang,Lu Xiao, Tingting Yu, Anne Woepse, Sunny Wong
for mocking. As such, it requires tedious implementation when
being used for this purpose. In addition, it may compromise the
designqualityofunittestsandleadtomaintenancedifficultiesin
the test cases [ 43,56,63,65]. More specifically, as illustrated in
Section2, inheritance has the following drawbacks compared to
usingamockingframeworksuchasMockito:1)Implicittestcondi-
tionandblurredtestlogic;2)Difficult-to-maintaintestcodethat
coupleswiththeproductioncode;and3)Incohesivetestdesignthat
separates the mocking behavior from the test case that leverages it.
Thegoalofthisworkistodevelopafullyautomatedrefac-
toringframeworktoidentifyandreplacetheusageofinheri-
tancebyusingMockitoformockinginunittesting. Wechoose
Mockitobecauseitisoneofthemostwellreceivedmockingframe-
workforJavaprojects[ 63].Itisadoptedinbothcommercialand
open source projects [ 73]. The key challenge is to preserve the
test behaviors before and after the refactoring. To overcome this
challenge,wefirstconductanempiricalstudy(Section 3)involving
fivereal-life, open-source projects as the learning dataset. The goal
is to gain empirical experience of whether it is feasible and how
toperformrefactoringfollowinganautomatedprocedure.Based
on the empirical observations, we formalize the problem definition
ofauto-refactoringtoreplaceinheritancebyusingMockito(Sec-
tion4). Next, we propose a fully automated refactoring framework
and implement it as an Eclipse-Plugin (Section 5). This framework
firstidentifies allfeasible refactoringcandidates and then performs
the refactoring oneachcandidate for agiven project.
We perform both quantitative and qualitative evaluation (Sec-
tion6) of the proposed framework using another fouropen-source
projects. The quantitative evaluation shows the general applica-
bility, overall reduced code complexity, high test case behavior
preservation, and efficient run-time performance of the refactor-
ingframework.ThequalitativeevaluationÃparticipatedbyexpe-
rienced,full-timedevelopersÃprovesthattheauto-refactoringso-
lutions generated by our approach are of good designqualityand
providevariousbenefitsfor improvingtest code design.
In summary,this work makesthe following contributions:
â€¢Anempiricalstudyinvolving fiveopen-sourceprojectsin-
vestigatingwhetheritisfeasibleandhowtoautomatically
replace inheritancebyMockitofor mocking.
â€¢A fully automated refactoring framework and its Eclipse-
Pluginimplementationtoidentifyfeasiblerefactoringcandi-
datesandperform the refactoring oneachcandidate.
â€¢Quantitativeandqualitativeevaluationoftheproposedframe-
work onfouropen-sourceprojects.
2 BACKGROUNDAND MOTIVATION
Thissectionintroducesthebasicconceptsofunittesting,andan
motivating example comparing the difference between mocking
throughinheritanceandthroughMockito.
2.1 UnitTesting
Unit testing aims at validating that each unit of function performs
asexpected[ 39,70].Theunittestcodeiscomposedof testclasses ,
test cases, andtest suites . Atest class is similar to a production
class. Atest class contains one or more test cases. Eachtest case
focuses on verifying the behavior of a certain unit of function(e.g. method) in the project. A test caseshould follow the Å‚AAA
(Arrange, Act, Assert)Å¾ patternÃarrange for setting up required
testenvironment;actforinvocationofthefunctionbeingtested;
and assert for checking whether the expectations were met [ 48].
A group of test cases for testing related functions are grouped and
executedtogetheras a test suite.
The interdependence among software units hinder our ability
to perform unit testing. A key for creating high-quality, easy-to-
maintain and debugunittest cases isto isolate thecore FUTfrom
its dependencies. In practice, this is achieved through mockingÃ
replacing the dependency byaÅ‚fakedÅ¾ object.
2.2 A Motivating Example
Inane-Commercesystem, CustomerService definesaservice, sub-
scribeCustomer ,tosubscribecustomersbyemail.Thisservicede-
pends on another class, EmailManager , which is responsible of
managing and sending emails. Its method, subscribe, first sends an
email to the customer to confirm the address; once confirmed, it
stores the email address in a database. Another method, sendEmail ,
sends email through an external server. We aim to test the logic
ofsubscribeCustomer inCustomerService .Theproblemisthat,its
dependencyfunctions, EmailManager ,isnotfullyimplementedyetÃ
neither the database nor the external service is available. Thus, we
isolatethe FUT,subscribeCustomer ,fromitsdependency, EmailMan-
ager, by mocking the latter. Next, we illustrate mocking through
inheritanceandMockito:
2.2.1 MockingbyInheritance. Inheritanceisamechanismtoderive
asubclassfromabaseclass.Thesubclassinheritstheattributesand
methodsofthebaseclass.Meanwhile,methodoverridingallows
the subclass to replace certain method implementation of the base
class. Inheritance is used as a Å‚hand-rolledÅ¾ approach for mock-
ing. Developersdefine a testsubclass toÅ‚mockÅ¾ certain behaviors
of the production class through method overriding or interface
implementationfor testing.
In Figure 1a,ğ‘€ğ‘œğ‘ğ‘˜ğ¸ğ‘šğ‘ğ‘–ğ‘™ğ‘€ğ‘ğ‘›ğ‘ğ‘”ğ‘’ğ‘Ÿ extendstheğ¸ğ‘šğ‘ğ‘–ğ‘™ğ‘€ğ‘ğ‘›ğ‘ğ‘”ğ‘’ğ‘Ÿ
(line1).TheformermocksthebehaviorsÃ ğ‘ ğ‘¢ğ‘ğ‘ ğ‘ğ‘Ÿğ‘–ğ‘ğ‘’ andğ‘ ğ‘’ğ‘›ğ‘‘ğ¸ğ‘šğ‘ğ‘–ğ‘™ Ã
ofthelatterthroughmethodoverriding.Twonewprivateattributes,
ğ‘ ğ‘¢ğ‘ğ‘ ğ‘ğ‘Ÿğ‘–ğ‘ğ‘’ğ‘‘ (line 2) and ğ‘›ğ‘¢ğ‘š(line 3), are defined for tracking the
executionofthetwooverriddenmethods.Thatis, ğ‘ ğ‘¢ğ‘ğ‘ ğ‘ğ‘Ÿğ‘–ğ‘ğ‘’ğ‘‘ isset
to betrue(line 6) when ğ‘ ğ‘¢ğ‘ğ‘ ğ‘ğ‘Ÿğ‘–ğ‘ğ‘’ executes; while ğ‘›ğ‘¢ğ‘šincrements
(line 10) each time ğ‘ ğ‘’ğ‘›ğ‘‘ğ¸ğ‘šğ‘ğ‘–ğ‘™ executes. Of particular note, since
thelogicdefinedinthissubclasspreparesmockingbehaviorsfor
the unittest case,itispart of the Å‚Arrange" inthe Å‚AAA" pattern.
Thetestcase, ğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘†ğ‘¢ğ‘ğ‘ ğ‘ğ‘Ÿğ‘–ğ‘ğ‘’ğ¶ğ‘¢ğ‘ ğ‘¡ğ‘œğ‘šğ‘’ğ‘Ÿ ,followstheÅ‚AAA"pattern.
First,itarrangestheenvironmentfortesting.Thisincludescreating
aninstanceof ğ‘€ğ‘œğ‘ğ‘˜ğ¸ğ‘šğ‘ğ‘–ğ‘™ğ‘€ğ‘ğ‘›ğ‘ğ‘”ğ‘’ğ‘Ÿ Ãğ‘’ğ‘šğ‘ğ‘–ğ‘™ğ‘€ğ‘ğ‘›ğ‘ğ‘”ğ‘’ğ‘Ÿ Ã(line15),and
creatinganinstanceof ğ¶ğ‘¢ğ‘ ğ‘¡ğ‘œğ‘šğ‘’ğ‘Ÿğ‘†ğ‘’ğ‘Ÿğ‘£ğ‘–ğ‘ğ‘’ ,ğ‘šğ‘¦ğ‘ ğ‘’ğ‘Ÿğ‘£ğ‘–ğ‘ğ‘’ ,whichisthe
FUT. Next, it acts the FUT(line 17 and line 18). Lastly, the test case
asserts the value of ğ‘ ğ‘¢ğ‘ğ‘ ğ‘ğ‘Ÿğ‘–ğ‘ğ‘’ğ‘‘ andğ‘›ğ‘¢ğ‘šwithğ‘€ğ‘œğ‘ğ‘˜ğ¸ğ‘šğ‘ğ‘–ğ‘™ğ‘€ğ‘ğ‘›ğ‘ğ‘”ğ‘’ğ‘Ÿ
(line 19 and 20). They confirm that ğ‘ ğ‘¢ğ‘ğ‘ ğ‘ğ‘Ÿğ‘–ğ‘ğ‘’ğ‘‘ istrue, indicating
methodğ‘ ğ‘¢ğ‘ğ‘ ğ‘ğ‘Ÿğ‘–ğ‘ğ‘’ğ‘‘ isexecuted;andthat ğ‘›ğ‘¢ğ‘šequals2,indicating
that two emails are sent (one asks the customer to confirm; the
othersends aconfirmationof subscription).
2.2.2 MockingbyMockito. Mockitooffersthreeaspectsofcapa-
bilities for mocking. First, Mockito allows easy creation of a mock
541AnAutomatic Refactoring Framework forReplacing Test-ProductionInheritanceby MockingMechanism ESEC/FSE â€™21, August 23â€“28, 2021,Athens,Greece
(a)Mocking by Inheritance
(b) Mocking by Mockito
Figure 1:AMotivatingExample
object as a Å‚mockÅ¾ or a Å‚spyÅ¾. The Å‚mockÅ¾ is a completely faked
objectandisentirelyinstrumentedtotracktheinteractionswithit.
Incomparison,theÅ‚spyÅ¾wrapsarealinstanceofthemockedobject.
The Å‚spyÅ¾ should be used when the execution of real methods is
necessaryintesting.Second,Mockitoofferslight-weightedmethod
stubbingforcontrollingthebehaviorsofthemockobjectfortesting
purposes. Mockito provides dedicated syntax for different types
ofbehaviorÃi.e.avoidmethod,areturnmethod,oramethodfor
throwing exceptions. Third, Mockito provides explicit mechanism
forverifyingthebehaviors/statusofthemockobjects.Forinstance,
Mockito can ensure whether a mock method is being called or not,
check on the number of calls made on a particular method, and
take care ofthe order ofcalls,etc..
InFigure 1b,MockitodirectlycreatesaÅ‚mockÅ¾ofthe EmailMan-
ager(line 26), since the goal is to avoid its real execution and focus
on its interactionswith ğ‘ ğ‘¢ğ‘ğ‘ ğ‘ğ‘Ÿğ‘–ğ‘ğ‘’ğ¶ğ‘¢ğ‘ ğ‘¡ğ‘œğ‘šğ‘’ğ‘Ÿ . In line 27-29, we stub
themockingbehaviorwhen ğ‘ ğ‘¢ğ‘ğ‘ ğ‘ğ‘Ÿğ‘–ğ‘ğ‘’ isinvoked.The ğ‘ ğ‘’ğ‘›ğ‘‘ğ¸ğ‘šğ‘ğ‘–ğ‘™
shoulddonothing ,sincewewanttoavoidsendingrealemails.Thus,
thereisnoneedtostubit.Actingthe FUT(line31andline32)re-
mains thesameasusing inheritance. Finally, in line 33and 34,we
directlyverifythe executionof ğ‘ ğ‘¢ğ‘ğ‘ ğ‘ğ‘Ÿğ‘–ğ‘ğ‘’ andğ‘ ğ‘’ğ‘›ğ‘‘ğ¸ğ‘šğ‘ğ‘–ğ‘™ .
2.2.3 Benefits of Mockito Over Inheritance. Mockito enables ex-
plicitandeasytounderstandtestinglogic.Itallowseasycreation
ofmockobjectsfordifferentlevelsoffunctionisolation(i.e.Å‚mockÅ¾
and Å‚spyÅ¾). The verifyfunctions in Mockito provide an explicit
mechanism for checking the execution and status of the mock
objects.Incomparison,inheritancerequiresthedevelopertomanu-
ally craft additional attributes/features in the subclass for tracking
the execution of the mock objects. For example, new attributes,
ğ‘ ğ‘¢ğ‘ğ‘ ğ‘ğ‘Ÿğ‘–ğ‘ğ‘’ğ‘‘ andğ‘›ğ‘¢ğ‘š, are used to keep track of method execution
in the mock object. The logic behind the attributes is implicit, and
mayblurthe testinglogic.
Mockito decouples test and production code to ease the mainte-
nance of the test code. Renaming methods/interfaces or reorderingparameters in the production code will not break the test code,
sinceMockitowiresthemockobjectsatrun-time.Incomparison,
inheritance relationship increases the coupling between the test
andproductioncode.Thisunnecessarilycripplestheinheritancehi-
erarchyandincreasesmaintenancedifficulty.Whentheproduction
code changes,its subclasseshave to changeaccordingly.
Mockitoimprovesthecohesionoftestdesignbyenforcingthe
Å‚AAAÅ¾ pattern of unit test case. Method stubbing through Mockito
cohesively associates with the mock object when it is arranged
inthetestcase.Incomparison, ininheritance,themockbehavior
(which is part of the Å‚ArrangeÅ¾) is defined in a separate subclass
throughmethodoverriding.Itisdetachedfromwherethebehavior
isusedfortesting.Thisincreasesthecognitiveloadforunderstand-
ingthe test behavior.
3 EMPIRICAL STUDY
We first conduct an empirical study to investigate whether it is
feasible andhowto automaticallyreplace inheritancebyMockito.
3.1 Dataset
Weselect fiveopensourceprojectsasourempiricalstudysubjectsÃ
they are Dubbo [ 6], Druid [ 7], Accumulo [ 8], Cayenne [ 9], and
CloudStack [ 10]. We select these projects because, first, they are
popularopensourceprojectsfromdiverseproblemdomains.Sec-
ond, test-production inheritance is commonÃeachproject contains
81 (CloudStack) to 291 (Druid) test subclasses for mocking. Thirdly,
weareabletorunthetestcasesintheseprojects,whichisimpor-
tantforverifyingthecorrectnessofthemanualrefactoring.Most
importantly,theseprojectsalready use Mockito.
3.2 StudyProcess
Foreachcasewhereatestsubclassinheritsorimplementsaproduc-
tion class or interface, we investigate the following questions: Can
wemanuallyrefactor the inheritance byusing Mockito basedon our
understanding?Ifso,istherefactoringprocessautomatable?Ifnot,
what is the reason that makes the refactoringÃand the automationÃ
infeasible? OneauthorÃthedriverÃmanuallyreviewsandrefactors
each test subclass, and the research team meets weekly to inspect
anddiscuss the manual refactoring solutions:
(1)If the manual refactoring is not feasible or not successful,
the driver recordsdetailedreasons.
(2)For each refactored case, the driver summarizes the key
refactoring steps, and determines whether the refactoring
procedurecanbeautomated.Ifautomationisnotpossible,
the driver recordsthe reasons.
(3)Intheweeklymeetings,theteam:i)discussandimprovethe
manual refactoring solutions, and ii) discuss and define the
auto-refactoring problem formalization.
3.3 Findings
Overall,25%(208)ofthetest-productioninheritancecasesinthe
empiricalstudycanbepotentiallyrefactoredautomatically.This
non-trivialportionofcasesmotivatethedesignof ourautomated
refactoringframework.Later,itisconfirmedthatthese25%cases
canindeedberefactoredfullyautomaticallybyapplyingourimple-
mentedrefactoring framework (Section 5).
542ESEC/FSE â€™21, August 23â€“28, 2021,Athens,Greece XiaoWang,Lu Xiao, Tingting Yu, Anne Woepse, Sunny Wong
Table 1:Manual RefactoringDatasets
Proj. #SubCl. Succ.Others
Infeasible Not-Auto Exec.Iss.
Dubbo 14843 (29%) 79 (53%) 12 (8%) 14 (9%)
Druid 29166 (23%) 173(59%) 12 (4%) 40 (14%)
Accumulo 16146 (29%) 85 (53%) 21 (13%) 9 (6%)
Cayenne 15140 (26%) 99 (66%) 10 (7%) 2 (1%)
CloudStack 8113 (16%) 58 (72%) 3 (4%) 7 (9%)
Sum 832208(25%) 494(59%) 58 (7%) 72 (9%)
Figure 2:IllustrationofRefactoring
Table1shows the details. The first column (Å‚Proj.Å¾) lists the
projectnames.Thesecondcolumn(Å‚#SubCl.Å¾)indicatesthetotal
numberoftestsubclassesinaproject.Thethirdcolumn(Å‚Succ.Å¾)
shows the total number (percentage) of test subclasses that can
be successfully replaced by using Mockito. That is, the refactoring
of 208 (25%) cases from the five projects can be automated. The
othercasesareinthreecategories:1)Therefactoringisnotfeasible
(columnÅ‚InfeasibleÅ¾).Eithercertaindesignfeaturesintheinheri-
tancearenotsuitableforrefactoring;ortherearedetailedtechnical
issues that prevent the refactoring. 2) Although manual refactor-
ing is feasible, full automation of the refactoring is not possible
(columnÅ‚Not-AutoÅ¾)becauseofthecomplicateddesignofthein-
heritance that requires case-by-case understanding for refactoring.
For example, some test subclass contains an inner class definition.
Refactoringrequiresin-depthunderstandingoftheinnerclass.And
3)therefactoringisnotsuccessfulduetoissueswithtestexecution
(columnÅ‚Exec.Iss.Å¾).Inthesecases,weeitherhaveissuesexecuting
related test cases; or the test behavior changes after refactoring for
reasons that require case-by-case investigation.
4 PROBLEM FORMALIZATION
Based on the 208 successful refactoring cases from the empirical
study,weformalizetheauto-refactoringproblem.Itisaconversion
from the left sideto the right side:
ğ‘…ğ‘’ğ‘“ğ‘ğ‘ğ‘¡ğ‘œğ‘Ÿ (ğ‘ğ‘œğ‘‘ğ‘’ğ‘–ğ‘›â„ğ‘’ğ‘Ÿğ‘–ğ‘¡ğ‘ğ‘›ğ‘ğ‘’ ) â†’ğ‘ğ‘œğ‘‘ğ‘’â€²
ğ‘šğ‘œğ‘ğ‘˜ğ‘–ğ‘›ğ‘”.
4.1 BeforeRefactoring
Arefactoringcandidate ğ‘ğ‘œğ‘‘ğ‘’ğ‘–ğ‘›â„ğ‘’ğ‘Ÿğ‘–ğ‘¡ğ‘ğ‘›ğ‘ğ‘’ canbeabstractedasatriad:
ğ‘ğ‘œğ‘‘ğ‘’ğ‘–ğ‘›â„ğ‘’ğ‘Ÿğ‘–ğ‘¡ğ‘ğ‘›ğ‘ğ‘’ =<ğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘†ğ‘¢ğ‘ğ¶ğ‘™ğ‘ğ‘ ğ‘ ,ğ‘ğ‘Ÿğ‘œğ‘‘ğ‘¢ğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ¶ğ‘™ğ‘ğ‘ ğ‘ ,ğ‘¡ğ‘’ğ‘ ğ‘¡ğ¶ğ‘™ğ‘ğ‘ ğ‘  >
Here,ğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘†ğ‘¢ğ‘ğ¶ğ‘™ğ‘ğ‘ ğ‘  extends the ğ‘ğ‘Ÿğ‘œğ‘‘ğ‘¢ğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ¶ğ‘™ğ‘ğ‘ ğ‘  . Theğ‘¡ğ‘’ğ‘ ğ‘¡ğ¶ğ‘™ğ‘ğ‘ ğ‘ 
leverages ğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘†ğ‘¢ğ‘ğ¶ğ‘™ğ‘ğ‘ ğ‘  to assist testing. The left-side code snip-
petsinFigure 2illustrates the formalizationof ğ‘ğ‘œğ‘‘ğ‘’ğ‘–ğ‘›â„ğ‘’ğ‘Ÿğ‘–ğ‘¡ğ‘ğ‘›ğ‘ğ‘’ .Theğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘†ğ‘¢ğ‘ğ¶ğ‘™ğ‘ğ‘ ğ‘  is further consisted of four key elements (The
convention Å‚[]*Å¾ indicates that there is zero or more of a design
element).Theupper-leftcodesnippetillustratesanexample test-
SubClass.
ğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘†ğ‘¢ğ‘ğ¶ğ‘™ğ‘ğ‘ ğ‘  =<[ğ‘ğ‘œğ‘›ğ‘ ğ‘¡ğ‘Ÿğ‘¢ğ‘ğ‘¡ğ‘œğ‘Ÿ ]âˆ—,[ğ‘ğ‘¡ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘¢ğ‘¡ğ‘’ ]âˆ—,
[ğ‘œğ‘£ğ‘’ğ‘Ÿğ‘Ÿğ‘–ğ‘‘ğ‘‘ğ‘’ğ‘›ğ‘€ğ‘’ğ‘¡â„ğ‘œğ‘‘ ]âˆ—,[ğ‘ğ‘Ÿğ‘–ğ‘£ğ‘ğ‘¡ğ‘’ğ‘€ğ‘’ğ‘¡â„ğ‘œğ‘‘ ]âˆ—>
â€¢constructor creates a ğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘†ğ‘¢ğ‘ğ¶ğ‘™ğ‘ğ‘ ğ‘  instance.
â€¢attributeisfor trackingthe executionof ğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘†ğ‘¢ğ‘ğ¶ğ‘™ğ‘ğ‘ ğ‘  .
â€¢overriddenMethod defines dummy implementation of a func-
tioninğ‘ğ‘Ÿğ‘œğ‘‘ğ‘¢ğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ¶ğ‘™ğ‘ğ‘ ğ‘  .
â€¢privateMethod definesadditionalfunction in ğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘†ğ‘¢ğ‘ğ¶ğ‘™ğ‘ğ‘ ğ‘  .
Ağ‘¡ğ‘’ğ‘ ğ‘¡ğ¶ğ‘™ğ‘ğ‘ ğ‘  leveragesthe ğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘†ğ‘¢ğ‘ğ¶ğ‘™ğ‘ğ‘ ğ‘  fortesting,whichcanbe
formalizedas following:
ğ‘¡ğ‘’ğ‘ ğ‘¡ğ¶ğ‘™ğ‘ğ‘ ğ‘  =<[ğ‘¡ğ‘’ğ‘ ğ‘¡ğ¶ğ‘ğ‘ ğ‘’]+>
ğ‘¡ğ‘’ğ‘ ğ‘¡ğ¶ğ‘ğ‘ ğ‘’ =<[ğ‘ğ‘œğ‘›ğ‘ ğ‘¡ğ‘Ÿğ‘¢ğ‘ğ‘¡ğ‘–ğ‘œğ‘› ],[ğ‘Ÿğ‘’ğ‘“ğ‘’ğ‘Ÿğ‘’ğ‘›ğ‘ğ‘’ ]+>
Ağ‘¡ğ‘’ğ‘ ğ‘¡ğ¶ğ‘™ğ‘ğ‘ ğ‘  containsatleastone ğ‘¡ğ‘’ğ‘ ğ‘¡ğ¶ğ‘ğ‘ ğ‘’.Ağ‘¡ğ‘’ğ‘ ğ‘¡ğ¶ğ‘ğ‘ ğ‘’ involves
ağ‘¡ğ‘’ğ‘ ğ‘¡ğ‘†ğ‘¢ğ‘ğ¶ğ‘™ğ‘ğ‘ ğ‘  in two parts for fulfilling the testing goal: 1) con-
struction,whichinvokesa ğ‘ğ‘œğ‘›ğ‘ ğ‘¡ğ‘Ÿğ‘¢ğ‘ğ‘¡ğ‘œğ‘Ÿ ofğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘†ğ‘¢ğ‘ğ¶ğ‘™ğ‘ğ‘ ğ‘  tocreatean
instance; and 2) reference, which accesses the attributes or call the
methods of the instance. The lower-left code snippet of Figure 2
illustrates asimpleexample.
4.2 After Refactoring
Theoriginal ğ‘ğ‘œğ‘‘ğ‘’ğ‘–ğ‘›â„ğ‘’ğ‘Ÿğ‘–ğ‘¡ğ‘ğ‘›ğ‘ğ‘’ isrefactoredinto ğ‘ğ‘œğ‘‘ğ‘’â€²
ğ‘šğ‘œğ‘ğ‘˜ğ‘–ğ‘›ğ‘”,which
eliminates ğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘†ğ‘¢ğ‘ğ¶ğ‘™ğ‘ğ‘ ğ‘  andreplaces itbyamockobject:
ğ‘ğ‘œğ‘‘ğ‘’â€²
ğ‘šğ‘œğ‘ğ‘˜ğ‘–ğ‘›ğ‘”=<ğ‘ğ‘Ÿğ‘œğ‘‘ğ‘¢ğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ¶ğ‘™ğ‘ğ‘ ğ‘ ,ğ‘¡ğ‘’ğ‘ ğ‘¡ğ¶ğ‘™ğ‘ğ‘ ğ‘ â€²>
Thusğ‘¡ğ‘’ğ‘ ğ‘¡ğ¶ğ‘™ğ‘ğ‘ ğ‘  becomes ğ‘¡ğ‘’ğ‘ ğ‘¡ğ¶ğ‘™ğ‘ğ‘ ğ‘ â€², and each ğ‘¡ğ‘’ğ‘ ğ‘¡ğ¶ğ‘ğ‘ ğ‘’ in it be-
comesğ‘¡ğ‘’ğ‘ ğ‘¡ğ¶ğ‘ğ‘ ğ‘’â€²:
ğ‘¡ğ‘’ğ‘ ğ‘¡ğ¶ğ‘™ğ‘ğ‘ ğ‘ â€²=<[ğ‘¡ğ‘’ğ‘ ğ‘¡ğ¶ğ‘ğ‘ ğ‘’â€²]+,[ğ‘ğ‘Ÿğ‘–ğ‘£ğ‘ğ‘¡ğ‘’ğ‘€ğ‘’ğ‘¡â„ğ‘œğ‘‘â€²]âˆ—>
ğ‘¡ğ‘’ğ‘ ğ‘¡ğ¶ğ‘ğ‘ ğ‘’â€²=<[ğ‘ğ‘œğ‘›ğ‘ ğ‘¡ğ‘Ÿğ‘¢ğ‘ğ‘¡ğ‘–ğ‘œğ‘›â€²],[ğ‘ ğ‘¡ğ‘¢ğ‘ğ‘€ğ‘’ğ‘¡â„ğ‘œğ‘‘ ]âˆ—,[ğ‘Ÿğ‘’ğ‘“ğ‘’ğ‘Ÿğ‘’ğ‘›ğ‘ğ‘’â€²]+>
Asillustratedintheright-sidecodesnippetinFigure 2,testClassâ€²
iscomposedof testCaseâ€²and[privateMethodâ€²].The[privateMethodâ€²]
isthe[privateMethod] movedfrom testSubClass totestClassâ€².And,
each refactored testCaseâ€™ is consisted of 1) [constructionâ€²]to create
amockobjectofthe productionClass ,whichreplacestheinstance
created by [construction] intestCase; 2)[stubMethod] , which re-
places the [overridenMethod] intestSubClass ; and 3)[referenceâ€²]
to the mock object, which replaces the respective [reference] to
thetestSubClass instance in testCase. We will explain the formal
refactoring procedure inSection 5.2.
5 REFACTORING FRAMEWORK
Theauto-refactoringframework,implementedasanEclipse-plugin1,
addresses the above formalizationwithtwocomponents:
1) Identifying refactoring candidates. After loading a project
in Eclipse, a user first selects the scope, e.g. the entire project, a
package, or a group of files, from which refactoring candidates
shouldbe identified.The identificationrelieson the AST Parser of
Eclipse JDT [14] to filter out cases that cannot be refactored based
onthe detailedcode syntax.
1https://github.com/wx930910/JMocker
543AnAutomatic Refactoring Framework forReplacing Test-ProductionInheritanceby MockingMechanism ESEC/FSE â€™21, August 23â€“28, 2021,Athens,Greece
Table 2:RefactoringCandidate Identification-Filters
ID Filter Criterion(What condition to lookfor?) Rationale(Why isit notfeasible/automatable?)
F-1.1 A ğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘†ğ‘¢ğ‘ğ‘ğ‘™ğ‘ğ‘ ğ‘  implements multiple production
interfaces.This indicates that the ğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘†ğ‘¢ğ‘ğ‘ğ‘™ğ‘ğ‘ ğ‘  mocks multipleproduction interfaces.
Mocking multiple ğ‘ğ‘Ÿğ‘œğ‘‘ğ‘¢ğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ¶ğ‘™ğ‘ğ‘ ğ‘ ğ‘’ğ‘  isnotrecommended[ 11].
F-1.2 A ğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘†ğ‘¢ğ‘ğ‘ğ‘™ğ‘ğ‘ ğ‘  overrides JDK APIs Ãparticularly
hashCode() orequals().Å‚Only mock types you ownÅ¾ [43]Ãavoid mocking JDK APIs. Plus, mocking
thetwoAPIs will break Mockitosince it isbuilt upon them [ 12].
F-1.3 A ğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘†ğ‘¢ğ‘ğ‘ğ‘™ğ‘ğ‘ ğ‘  defines a newpublicmethod; this
method is notin theğ‘ğ‘Ÿğ‘œğ‘‘ğ‘¢ğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ¶ğ‘™ğ‘ğ‘ ğ‘ ğ‘’ğ‘  .Theğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘†ğ‘¢ğ‘ğ‘ğ‘™ğ‘ğ‘ ğ‘  no longer Å‚mocksÅ¾ the ğ‘ğ‘Ÿğ‘œğ‘‘ğ‘¢ğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ¶ğ‘™ğ‘ğ‘ ğ‘ ğ‘’ğ‘  ,when it
containsextra, new behaviors.
F-1.4 A ğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘†ğ‘¢ğ‘ğ‘ğ‘™ğ‘ğ‘ ğ‘  hasself-reference (e.g.declaring
itselfasanattribute).Self-referenceimpliesself-mockingÃamockobject cannot mockitself.
F-1.5 A ğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘†ğ‘¢ğ‘ğ‘ğ‘™ğ‘ğ‘ ğ‘  containsa method that returns a
generic type .Thegeneric type indicates uncertainmocking behavior;a mockobject
should have certainbehavior.
F-2.1 A ğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘†ğ‘¢ğ‘ğ‘ğ‘™ğ‘ğ‘ ğ‘  isnotinstantiated through its
constructor.Theğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘†ğ‘¢ğ‘ğ‘ğ‘™ğ‘ğ‘ ğ‘  isnotused anywhere (noneedto mock) or is
instantiated through dynamic binding(thus cannot use Mockito).
F-2.2 A ğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘†ğ‘¢ğ‘ğ‘ğ‘™ğ‘ğ‘ ğ‘  containsspecial code annotations Theannotations are mostly project specific or from a speciallibrary [ 13].
Mockitodoesnotsupportthem.
F-2.3 A ğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘†ğ‘¢ğ‘ğ‘ğ‘™ğ‘ğ‘ ğ‘  hasexternal access to a protected
attribute/method in theğ‘ğ‘Ÿğ‘œğ‘‘ğ‘¢ğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ¶ğ‘™ğ‘ğ‘ ğ‘  .Mockitodoes not support the access to protected elements. This requires a
more powerful framework,suchasPowerMock[ 3]
F-3.1 A ğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘†ğ‘¢ğ‘ğ‘ğ‘™ğ‘ğ‘ ğ‘  instance is passed as aparameter
across multiple test cases/methods in a ğ‘¡ğ‘’ğ‘ ğ‘¡ğ¶ğ‘™ğ‘ğ‘ ğ‘  .It requires manual effort to create a good test design, where a mock object
and therelated variablespass alongtest casesand methods.
F-3.2 A ğ‘¡ğ‘’ğ‘ ğ‘¡ğ¶ğ‘ğ‘ ğ‘’ createsand uses acollection (e.g.Setor
Map)ofğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘†ğ‘¢ğ‘ğ‘ğ‘™ğ‘ğ‘ ğ‘  instances.Itrequiresmanualeffort to create a goodtest designwitha collection of
mockobjects and thevariablesthat associate witheach mockobject.
F-3.3 A ğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘†ğ‘¢ğ‘ğ‘ğ‘™ğ‘ğ‘ ğ‘  containsan inner class definition. Itrequiresmanualunderstanding to properlyrefactor theinnerclass.
2) Refactoring each candidate. The tool will notify users the
listofidentifiedrefactoringcandidates(i.e.sub-classes).Theuser
needs to select a candidate to proceed with the refactoring. The
implementationofrefactoringreliesonthe ASTRewrite mechanism
oftheEclipseJDT [14].
Next,we introduce the details ofeachstep.
5.1 Refactoring Candidate Identification
Based on the empirical study, we construct a taxonomy of 11 ex-
clusion criteria to automatically exclude test subclasses that do not
match the problem formalizationÃi.e. those that are not feasible
to refactorortherefactoringcannot beautomated.InTable 2,we
list the detailed filtering conditions (column 2), as well as the ratio-
naleofeachfilteringcondition(column3).Forthesakeofclarify,
weorganizethe11filteringconditionsinthreegenerallayers:1)
Layer1ÃF-1.1toF-1.5Ãwhichexcludescasesthatarenotsuitable
formocking;2)Layer2Ã F-2.1toF-2.3Ãwhichexcludescasesthat
cannot be refactored due to detailed implementation limitations
withMockito;and3)Layer3Ã F-3.1toF-3.3Ãwhichexcludescases
withcomplicateddesignthat cannotbe refactoredautomatically.
5.2 Auto-Refactoring Procedure
Figure3showstherefactoringproceduretoconvert ğ‘ğ‘œğ‘‘ğ‘’ğ‘–ğ‘›â„ğ‘’ğ‘Ÿğ‘–ğ‘¡ğ‘ğ‘›ğ‘ğ‘’
toğ‘ğ‘œğ‘‘ğ‘’â€²
ğ‘šğ‘œğ‘ğ‘˜ğ‘–ğ‘›ğ‘”foreachrefactoringcandidateidentifiedfromthe
previous step.Our approach involves five logical parts:
(1)Createmockobject:Thisstepconstructsamockobjectusing
Mockito to replace the ğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘†ğ‘¢ğ‘ğ‘ğ‘™ğ‘ğ‘ ğ‘  instance, and ensures
that they have equivalentinitialstatus.
(2)Preservemockingbehavior:Thisstepextractstheoverridden
methods and moves the private methods in ğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘†ğ‘¢ğ‘ğ¶ğ‘™ğ‘ğ‘ ğ‘  to
ensurethatthemockobjecthasequivalentbehaviorasthe
ğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘†ğ‘¢ğ‘ğ¶ğ‘™ğ‘ğ‘ ğ‘  instance.
(3)Preservereferencestothemockobject:Thisstepensuresthat
theexecution/verificationofthemockobjectisequivalent
to that ofthe ğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘†ğ‘¢ğ‘ğ¶ğ‘™ğ‘ğ‘ ğ‘  instance.(4)Infrastructure Procedure- translateToMocking : This proce-
durecross-cutsthethreepreviouspartstoensurethatthe
refactoring followsthe mocking syntax.
(5)CreateMockMethod for code reusability: This applies when
multiple test casescould reuse the mockobjectcreation.
In the following subsections, we will explain each part in detail.
5.2.1 Step1-CreateMockObject. Thisstepcreatesamockobject
using Mockito to replace the ğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘†ğ‘¢ğ‘ğ¶ğ‘™ğ‘ğ‘ ğ‘  instance. To ensure that
the initial status of the ğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘†ğ‘¢ğ‘ğ¶ğ‘™ğ‘ğ‘ ğ‘  instance and themock object
are equivalent,the following three sub-steps are performed:
Step-1.1: Replaceğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘†ğ‘¢ğ‘ğ¶ğ‘™ğ‘ğ‘ ğ‘  instance creation by mock ob-
jectcreation.TherearetwowaystodosoÃthrough spyormock,
asillustratedinFigure 4aandFigure 4b,respectively. Spycreates
a real object; while mockcreates a complete mock or fake object.
Based on the empirical study, if the productionClass is an interface
without any method definition, we should use mock, since an in-
terface cannot be instantiated as a real object. In comparison, if
theproductionClass hasmethodimplementation,weshoulduse spy
to ensure that the mock object has the same behavior as the real
object, except for the purposely stubbed methods. There are other
minorsyntax variations for spyandmock,summarizedhere2.
Step-1.2: Extract the ğ‘ğ‘¡ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘¢ğ‘¡ğ‘’ğ‘  ofğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘†ğ‘¢ğ‘ğ¶ğ‘™ğ‘ğ‘ ğ‘  toğ‘¡ğ‘’ğ‘ ğ‘¡ğ¶ğ‘™ğ‘ğ‘ ğ‘ â€².
This ensures that the status of the ğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘†ğ‘¢ğ‘ğ¶ğ‘™ğ‘ğ‘ ğ‘  instance is pre-
servedforthemockobject.Weobservedtwotypesof ğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘†ğ‘¢ğ‘ğ¶ğ‘™ğ‘ğ‘ ğ‘ 
attributes from the empirical study,whichare treateddifferently.
ThefirsttypeofattributeistheÅ‚ counter/checker Å¾asshowninthe
motivatingexampleinFigure 1.Theseattributesarefortracking
the execution of the mock object. We recognize the type using
threeheuristics:1)itisa booleanoranint;2)itisonlyread/written
in a certain methods of the mock object; and 3) it is asserted for
checkingtheexecutionoftheassociatedmethods.Mockitohasa
designated mechanismÃ Mockito.verify Ãfor verifying its execution.
Thus,thereisnoneedtopreservethistypeofattributes.Instead,we
2https://sites.google.com/view/mockrefactoring
544ESEC/FSE â€™21, August 23â€“28, 2021,Athens,Greece XiaoWang,Lu Xiao, Tingting Yu, Anne Woepse, Sunny Wong
Figure 3:Automated RefactoringProcedure
(a)Å‚Spy" and Å‚doAnswer"
(b) Å‚Mock" and Å‚thenAnswer"
Figure 4:MockObjectCreation andStubMethod
justkeeparecordofthetrackedmethodsandverifytheirexecution
laterusing Mockito.verify to replace the assertions.
The other type of attributes are in diverse types, and could be
referenced anywhere in ğ‘¡ğ‘’ğ‘ ğ‘¡ğ¶ğ‘™ğ‘ğ‘ ğ‘ . The way that we extract such
an attribute from the ğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘†ğ‘¢ğ‘ğ¶ğ‘™ğ‘ğ‘ ğ‘  toğ‘¡ğ‘’ğ‘ ğ‘¡ğ¶ğ‘™ğ‘ğ‘ ğ‘ â€²depends on how
ğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘†ğ‘¢ğ‘ğ¶ğ‘™ğ‘ğ‘ ğ‘  is originally used in ğ‘¡ğ‘’ğ‘ ğ‘¡ğ¶ğ‘™ğ‘ğ‘ ğ‘ . More specifically, if
theğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘†ğ‘¢ğ‘ğ¶ğ‘™ğ‘ğ‘ ğ‘  instance is an attribute of the ğ‘¡ğ‘’ğ‘ ğ‘¡ğ¶ğ‘™ğ‘ğ‘ ğ‘ , the at-
tributeof ğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘†ğ‘¢ğ‘ğ¶ğ‘™ğ‘ğ‘ ğ‘  willbecomeanattributefor ğ‘¡ğ‘’ğ‘ ğ‘¡ğ¶ğ‘™ğ‘ğ‘ ğ‘ â€²,to
ensure the same access scope. Otherwise, if the ğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘†ğ‘¢ğ‘ğ¶ğ‘™ğ‘ğ‘ ğ‘  in-
stance is createdas a local variable insidea ğ‘¡ğ‘’ğ‘ ğ‘¡ğ¶ğ‘ğ‘ ğ‘’,the attribute
ofğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘†ğ‘¢ğ‘ğ¶ğ‘™ğ‘ğ‘ ğ‘  willbecome alocal variable in ğ‘¡ğ‘’ğ‘ ğ‘¡ğ¶ğ‘ğ‘ ğ‘’â€².
Step-1.3: Extract the constructor logic from ğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘†ğ‘¢ğ‘ğ¶ğ‘™ğ‘ğ‘ ğ‘  to
[ğ‘ğ‘œğ‘›ğ‘ ğ‘¡ğ‘Ÿğ‘¢ğ‘ğ‘¡ğ‘–ğ‘œğ‘›â€²].This ensures that themock objecthas equivalent
initial status as the ğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘†ğ‘¢ğ‘ğ¶ğ‘™ğ‘ğ‘ ğ‘  instance. If the ğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘†ğ‘¢ğ‘ğ¶ğ‘™ğ‘ğ‘ ğ‘  in-
stanceiscreatedusingadefaultconstructor,thisstepcanbeskipped.
Ifğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘†ğ‘¢ğ‘ğ¶ğ‘™ğ‘ğ‘ ğ‘  instance is created using a non-default constructor
(which comes with additional settings for the created instance),theconstructorlogicneedstobeextractedto [ğ‘ğ‘œğ‘›ğ‘ ğ‘¡ğ‘Ÿğ‘¢ğ‘ğ‘¡ğ‘–ğ‘œğ‘›â€²].Each
statementintheconstructorneedstobetranslatedtofollowthesyn-
tax after the refactoring. Here, an infrastructure procedure named
translateToMocking takesthecodebodyoftheconstructorasinput,
and translates each statement following the mocking syntax. Since
translateToMocking cross-cutsallthreelogicstepsoftherefactoring
procedure,we willintroduce its details inSection 5.2.4.
5.2.2 Step2-PreserveMockingBehavior. Thispreservesthemock-
ingbehaviorsbytreatingthe overriddenMethods andprivate-Methods
intheğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘†ğ‘¢ğ‘ğ¶ğ‘™ğ‘ğ‘ ğ‘  :
Step-2.1: Extract the ğ‘œğ‘£ğ‘’ğ‘Ÿğ‘Ÿğ‘–ğ‘‘ğ‘‘ğ‘’ğ‘›ğ‘€ğ‘’ğ‘¡â„ğ‘œğ‘‘ inğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘†ğ‘¢ğ‘ğ¶ğ‘™ğ‘ğ‘ ğ‘  tothe
ğ‘ ğ‘¡ğ‘¢ğ‘ğ‘€ğ‘’ğ‘¡â„ğ‘œğ‘‘ which directly associates with the mock object cre-
ated/used in ğ‘¡ğ‘’ğ‘ ğ‘¡ğ¶ğ‘ğ‘ ğ‘’â€². There are two common ways to stub a
method:doAnswer andthenAnswer . ThethenAnswer addsaddi-
tionalactions to the stubbed method [ 15]. It ensures type safety
thusshouldbepreferredwheneverpossible.While, doAnswer en-
tirely replacestheoriginal method behavior[ 46], working similar
to method overridden in inheritance. Based on empirical experi-
ence,thenAnswer workswithobjectscreatedusing mock;whilethe
spyobjectshouldworkwith doAnswer topreservetheÅ‚overridden"
behavior. Figure 4illustrates doAnswer in Figure 4a(line 20 to line
24) andthenAsnwer in Figure 4b(line 20 to line 23) respectively.
They are used to replace the overridden methods between line 2 to
line7inFigure 4aandinFigure 4b.Inaddition,therearespecific
methodstubbingsyntaxfordifferentkindsofbehaviors,summa-
rizedhere2.Forexample, doReturn andthenReturn areforstubbing
methodsthat justreturn certainobjects.
Note that the internal logic of the overridden methods in Fig-
ure4is straightforward Ãi.e. without referencing attributes or
methods in the ğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘†ğ‘¢ğ‘ğ‘ğ‘™ğ‘ğ‘ ğ‘  . Thus we can directly move them to
thestub methodblocks.If theinternallogic hasareference tothe
ğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘†ğ‘¢ğ‘ğ¶ğ‘™ğ‘ğ‘ ğ‘  attributes/methods,wealsoneedtouse translateTo-
Mockingprocedure to convertthe syntax before moving.
Step-2.2: Moveeachprivatemethodfrom ğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘†ğ‘¢ğ‘ğ¶ğ‘™ğ‘ğ‘ ğ‘  totest-
Classâ€².Thesemethodmovementscannotbedirectlycopy-and-paste
due to the overall syntax change. Similarly, we use the translateTo-
Mockingprocedure toconvert the methodsyntax whenmovingit.
In addition,the methodsignature mayneedtobe updatedaccord-
ingly,totakeadditionalinputparameters,foraccessingthelocal
variables in ğ‘¡ğ‘’ğ‘ ğ‘¡ğ¶ğ‘ğ‘ ğ‘’â€²whichwere the attributes in ğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘†ğ‘¢ğ‘ğ¶ğ‘™ğ‘ğ‘ ğ‘  .
5.2.3 Step3-PreserveReferencetotheMockObject. Inağ‘¡ğ‘’ğ‘ ğ‘¡ğ¶ğ‘ğ‘ ğ‘’,
therecouldbereferencestotheattributesand/ormethodsofthe
ğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘†ğ‘¢ğ‘ğ¶ğ‘™ğ‘ğ‘ ğ‘  instanceÃas suchthe instanceis created forfacilitat-
ingtesting.Toensurethatthebehaviorof ğ‘¡ğ‘’ğ‘ ğ‘¡ğ¶ğ‘ğ‘ ğ‘’ andğ‘¡ğ‘’ğ‘ ğ‘¡ğ¶ğ‘ğ‘ ğ‘’â€²
remains consistent, we need to preserve these references on the
545AnAutomatic Refactoring Framework forReplacing Test-ProductionInheritanceby MockingMechanism ESEC/FSE â€™21, August 23â€“28, 2021,Athens,Greece
mockobject.Again,weusethe translateToMocking proceduretopre-
serve[ğ‘Ÿğ‘’ğ‘“ğ‘’ğ‘Ÿğ‘’ğ‘›ğ‘ğ‘’ ]âˆ—inğ‘¡ğ‘’ğ‘ ğ‘¡ğ¶ğ‘ğ‘ ğ‘’ tobe therespective [ğ‘Ÿğ‘’ğ‘“ğ‘’ğ‘Ÿğ‘’ğ‘›ğ‘ğ‘’â€²]âˆ—
inğ‘¡ğ‘’ğ‘ ğ‘¡ğ¶ğ‘ğ‘ ğ‘’â€².
5.2.4 InfrastructureProcedureâ€”translateToMocking. Asmentioned
earlier,eachpreviousstepreliesonthe translateToMocking proce-
dure3, which takes a certain code body in the ğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘†ğ‘¢ğ‘ğ¶ğ‘™ğ‘ğ‘ ğ‘  Ãe.g.
methods,constructors,referrencestatementsÃasinput,andconvert
themfollowthe syntax after refactoring.
For each statement, ğ‘ ğ‘¡ğ‘š, in the input code body, this procedure
makes the following conversions: If ğ‘ ğ‘¡ğ‘šhas a reference to an at-
tributein ğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘†ğ‘¢ğ‘ğ‘ğ‘™ğ‘ğ‘ ğ‘  ,itistreatedintwodifferentwaysdepending
ontheattributetype.First,iftheattributeisa Å‚checker/counterÅ¾ ,we
justremove ğ‘ ğ‘¡ğ‘š,sincethereisnoneedtokeeptrackofthisattribute
anymore.Therespectiveassertionstatements,wheretheattributeis
checked,arereplacedbythe MockVerify statementsoftheassociated
methods. Second, if the attribute is a general type (i.e. other than a
Å‚checker/counterÅ¾ ),wejustreplacetheattributein ğ‘ ğ‘¡ğ‘šbytherespec-
tivelocalvariablein ğ‘¡ğ‘’ğ‘ ğ‘¡ğ¶ğ‘ğ‘ ğ‘’â€²Ãortheattributeinthe ğ‘¡ğ‘’ğ‘ ğ‘¡ğ¶ğ‘™ğ‘ğ‘ ğ‘ ğ‘ â€²Ã
depending on where the attribute is extracted in step 1.2. If ğ‘ ğ‘¡ğ‘š
containsreferencetoa ğ‘ğ‘Ÿğ‘–ğ‘£ğ‘ğ‘¡ğ‘’ğ‘€ğ‘’ğ‘¡â„ğ‘œğ‘‘ intheğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘†ğ‘¢ğ‘ğ¶ğ‘™ğ‘ğ‘ ğ‘  ,were-
placethereferencetobethe ğ‘ğ‘Ÿğ‘–ğ‘£ğ‘ğ‘¡ğ‘’ğ‘€ğ‘’ğ‘¡â„ğ‘œğ‘‘â€²inğ‘¡ğ‘’ğ‘ ğ‘¡ğ¶ğ‘™ğ‘ğ‘ ğ‘ â€².Similarly,
ifğ‘ ğ‘¡ğ‘šcontains reference to an ğ‘œğ‘£ğ‘’ğ‘Ÿğ‘Ÿğ‘–ğ‘‘ğ‘‘ğ‘’ğ‘›ğ‘€ğ‘’ğ‘¡â„ğ‘œğ‘‘ inğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘†ğ‘¢ğ‘ğ¶ğ‘™ğ‘ğ‘ ğ‘  ,
we replace this reference by the stub method (created in step 2.1)
associatedwiththe mockobjectin ğ‘¡ğ‘’ğ‘ ğ‘¡ğ¶ğ‘ğ‘ ğ‘’â€².
5.2.5 Create MockMethod for Code Reusability. AtestSubClass
could be created and used in multiple ğ‘¡ğ‘’ğ‘ ğ‘¡ğ¶ğ‘ğ‘ ğ‘’ğ‘  . For each con-
structorin ğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘†ğ‘¢ğ‘ğ¶ğ‘™ğ‘ğ‘ ğ‘  ,therespective [ğ‘ğ‘œğ‘›ğ‘ ğ‘¡ğ‘Ÿğ‘¢ğ‘ğ‘¡ğ‘–ğ‘œğ‘›â€²]blockafter
refactoringÃgenerated by Step 1.1, 1.2, and 1.3, as well as all the
[ğ‘ ğ‘¡ğ‘¢ğ‘ğ‘€ğ‘’ğ‘¡â„ğ‘œğ‘‘ ]blocksÃgenerated by Step 2.1Ãcan be reused when-
ever this constructor is called. To prevent code-clone in such cases,
we encapsulate these blocks within a separate MockMethod in the
ğ‘¡ğ‘’ğ‘ ğ‘¡ğ¶ğ‘™ğ‘ğ‘ ğ‘  forreuse.However,the MockMethod isnotappropriate
whenthereexistsexternalreferencetothe ğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘†ğ‘¢ğ‘ğ¶ğ‘™ğ‘ğ‘ ğ‘  â€™sattributes
in theğ‘¡ğ‘’ğ‘ ğ‘¡ğ¶ğ‘ğ‘ ğ‘’ğ‘  . The external reference to the ğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘†ğ‘¢ğ‘ğ¶ğ‘™ğ‘ğ‘ ğ‘  at-
tributes cannot be preserved, since the attributes become the local
variables in the MockMethod . Thus, the condition to apply Mock-
Methodincludes:1)themockobjectisreusedinmultiple ğ‘¡ğ‘’ğ‘ ğ‘¡ğ¶ğ‘ğ‘ ğ‘’ğ‘ â€²;
and2)therewasnoreferencetothe ğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘†ğ‘¢ğ‘ğ¶ğ‘™ğ‘ğ‘ ğ‘  â€™sattributesbefore
refactoring.
6 EVALUATION
6.1 EvaluationDataset
We select four new projects, with a total of 610 test subclasses.
They are: JackRabbitÃan open source content repository for the
Java platform [ 16], Log4J2Ãa Java-based logging utility [ 17], Qpid-
Proton-JÃa high-performance, lightweight messaging library [ 18],
ApacheCommonsÃwhichfocusesonallaspectsofreusableJava
Components,with40subprojects,includingCommons-Collections,
Commons-Lang,Commons-Logging,etc.Toavoidbias,weinten-
tionally select these projects since their domains differ from the
training dataset. The rationale ofother selection criteria is similar
to that ofthe training dataset inSection 3.
3Dueto space limit, the pseudo-codeof thisprocedurecanbefound here6.2 Research Questions
We aim to evaluate our approach indifferentaspectsbyfive RQs.
â€¢RQ1: How generally applicable is the refactoring framework?
Wereportthenumberandpercentageoftestsubclassesthat
arefilteredoutintheidentification,andthataresuccessfully
refactored,when appliedto the evaluation dataset.
â€¢RQ2: Do the test behaviors remain consistent before and after
the refactoring with injected mutations? Mutation testing is a
proxyforevaluatingthebehaviourpreservationoftherefac-
toredtestcasesintermsofdetectingpotentialdefects.We
usemutationtestingtoinjectpotentialdefects,asmutants,
intotheproductioncode.Behaviourpreservationmeansthat
the same mutants should be covered and killed (or survived)
consistentlybythetestcasesbeforeandaftertherefactoring.
â€¢RQ3: How does the refactoring affect code complexity? We
reportthecodecomplexitymeasuredbytheLOC,#methods,
and #fields, as well the amount of dependencies from test to
production code andamong the test code.
â€¢RQ4:What isthe performanceof ourrefactoring framework?
We report the execution times in identifying refactoring
candidates andinperformingthe refactoring, respectively.
â€¢RQ5:Howisthequalityoftheauto-refactoringsolutionsinreal-
developersâ€™opinion?Andhowdoesitcomparetothemanual
refactoring solution implemented by developers? We conduct
auserstudyinvolvingfull-timedeveloperstobothmanually
implement refactoring and review the refactoring solutions
generated byour framework.The goalisto understandthe
value,benefits,andqualityofourrefactoringsolutions,espe-
ciallywhen comparedto manualrefactoringbydevelopers.
RQ1 to RQ4 are answered by quantitative evaluation; RQ5 is
answeredbyqualitative evaluation.
6.3 QuantitativeEvaluationResults (RQ1-RQ4)
RQ1:In Table3, in all four projects (row Å‚TotalÅ¾), there are totally
610 test sub-classes (column Å‚#SubCl.Å¾). Among these, 217 (column
Å‚F-1Å¾), 50 (column Å‚F-2Å¾), and 86 (column Å‚F-3Å¾) cases are filtered
outbythethreefilteringlayers(Section 5.1).Therefore,thereare
257 (42%) (column Å‚#CandidatesÅ¾) identified as feasible refactoring
candidates. Furthermore, the refactoring of 27 (4%) test sub-classes
leads to compile errors (column Å‚Comp.Å¾) due to syntax issues that
were not captured in the dataset of the empirical study. In addition,
16(3%)testsub-classes,aftertherefactoring,leadtotestbehavior
discrepancies(columnÅ‚Discre.Å¾)duetospecialcases.Forexample,
sometestcasesusethemetadataofthetestsubclassatrun-time,
and they fail after the refactoring. One can keep refining our ap-
proach by incorporating these special cases. However, like any
learningprocess,itisimpossibletoguarantee100%generalizability
underunknown,newdata. Totally,214 cases(columnÅ‚Succ.Å¾)are
successfully refactored.
Summary: Our approach successfully refactors 214 casesÃ
indicating a 83% (214/257) successful rate over the feasible
casesand35%(214/610)successfulrateoverall610testsub-
classes.This suggestsgoodapplicabilityof our approach.
546ESEC/FSE â€™21, August 23â€“28, 2021,Athens,Greece XiaoWang,Lu Xiao, Tingting Yu, Anne Woepse, Sunny Wong
Table 3:Applicable Auto-RefactoringinTesting Datasets
Proj. #SubCl.Identification Refactoring
F-1F-2F-3#Candidates Comp. Discre. Succ.
JackRabbit 71154943 (60%) 3 (4%) 0 (0%) 40 (56%)
Log4J2 10031191733 (33%) 5 (5%) 3 (3%) 25 (25%)
Qpid-Proton-J 34130129 (26%) 0 (0%) 0 (0%) 9 (26%)
Commons 4051582748172(42%) 19 (5%) 13 (3%) 140(35%)
Total 6102175086257(42%) 27 (4%) 16 (3%) 214(35%)
RQ2:WeusePIT[ 19],astate-of-the-artmutationtestingsystem,
to generate mutators. Table 4summarizes all the generated default
mutators[ 20]andtheirpercentageinour dataset.
Table 4:Applicable Default Mutators by Pitest
Mutator Description %
Negate Conditions mutateallconditionsby its logicalnegation 33%
Void MethodCalls remove methodcallstovoid methods 26%
Empty/NullReturns replace return values with an â€˜emptyâ€™ orâ€˜nullâ€™ 18%
True/False Returns mutatesa truereturn value tobe falseand viceversa 9%
Math replaces binary arithmeticoperations with another operation 6%
Primitive Returns replace int,short,long,char, float and double return values with 0 4%
ConditionalsBoundary replaces the relational operatorswith theirboundary counterpart 4%
Increments replaces increments with decrements and viceversa 1%
The execution of the mutations are reported in Table 5. Column
2 shows the number of refactored test cases executing the mutants,
ranging from 126 (Qpid) to 2510 (Commons). Column 3 shows
the number of successfully mutated production classes by PIT,
accounting for 35% to 56% of all production classes called by the
testcasesinColumn2.Thereasonwhysomeproductionclasses
arenotmutatedisbecausetheyareabstractclassorinterfaceswith
little real method implementation and PIT cannot apply mutation
operatorsto them. For example, if there is no method thatreturns
abooleantype,theoperator True/FalseReturns (seeTable 4)cannot
be applied. Column 4 shows that a total of 51811 mutantsÃfrom
1156 (Qpid) to 35902 (Commons)ineachprojectÃare generated.
Table 5:Mutation Status Change
Proj. #T.Cs #P.(%M) #Mut.Covered Killed Survived
Before AfterBefore After Before After
JackRabbit 431107 (47%) 97303803 (39%) âœ“2981 +1, -2 822 +2, -1
Log4J2 424152 (35%) 50231548 (31%) âœ“992 âœ“ 556 âœ“
Qpid-Proton-J 12643(51%) 1156332 (29%) âœ“298 âœ“ 34 âœ“
Commons 2510700(56%) 3590217120 (48%) +312709+121, -120 4411+123, -121
Total 34911002 (50%) 5181122803 (44%) +316980+122, -122 5823+125, -122
Column5and6reportthecoveragestatisticsofexecutingthe
mutantsbeforeandaftertherefactoring.Amongallmutants,332
(29%) to 17120 (48%) are covered before applying our refactoring
approach. The coverage on the mutants remains highly consistent
after the refactoring, except for 3 (out of totally 51811) mutants.
The discrepancy on these 3 mutants needs further investigation.
Note that not all mutants are covered since we use the test cases
suppliedbytheprojects,whichdonotachieve100%coverage.In
addition,we only executetest casesaffectedbythe refactoring.
Amongthecoveredmutants,weobserve(Column7Ãcolumn
10) that, except Commons and JackRabbit, the mutation status
(KilledorSurvived)remainsconsistentbeforeandafterrefactoring.
Column 7 and 9 show the number of mutants that are killed and
survivedbeforetherefactoring.Columns8and10showthenumber
of mutants that change the execution status (killed/survived) after
the refactoring. For example, the Commons project, in column 8,
+121indicates that, after the refactoring, an additional 121 mutants
are killed; and -120indicates that 120 mutants no longer got killed
aftertherefactoring.Thenumbersincolumns8and10shouldsumto the number in column 6Ãmutants that change their coverage.
We sample 30 mutants to investigate the reasons for the change.
We find that, in all 30 cases, the behaviors of the tests become non-
deterministic after injecting the mutantsÃthe status changes even
withoutrefactoring.Thus,thenon-determinismiscausedbythe
mutationsinstead ofthe refactoring.
Summary: The coverage of the mutants before and after
applyingourapproachishighlyconsistent( <0.001%ofthe
generated mutants changed coverage). The test cases, after
refactoring, consistently cover, kill, or survive with 99% of
the 51,811 mutants injected into the production code. This
indicatesthatourapproachgenerallypreservestestbehaviors
interm ofdetecting potentialdefects.
RQ3:We investigate the complexity in two main aspects: 1) the
basiccomplexitymetrics,includingtheLOC,#Methods,and#Fields;
and 2) the coupling, in terms of the number of dependencies, from
testtoproductioncode,andthatamongthetestcode.Table 6shows
the study results.
Columns2Å›13reporttheaverageLOC,#Methods,and#Fields
before the refactoring, and the average, minimal and maximal per-
centage of increase after the refactoring over all refactoring cases.
A negative percentage indicates that the measure decreases. We
observethataftertherefactoring,1)theLOCaveragelyincreases
4%to8%foreachrefactoringcase.Thisisduetocaseswherethe
reusablemock method (Section5.2.5) is not applicable. And 2) both
the #Methods and the #Fields decreases, in average by 5% to 14%
and9%to22%respectively.ThedecreaseisduetoStep1.2andStep
2.1 (Section 5.2).
Columns 14Å›15 report the test-to-production coupling in terms
of the number of dependencies before the refactoringin the entire
system,andthepercentageofincreaseaftertherefactoring(column
Å‚#(%In.)ofT-PDps.Å¾).Anegativepercentageindicatesdecreasing
thedependenciesaftertherefactoring.Here,weseparatetheinher-
itance (column Å‚Inherit.Å¾) and other general dependencies (column
Å‚RegularÅ¾). We observe that: 1) the number of test-to-production
inheritancenon-triviallydecreasedby24%(Log4J2)to38%(JackRab-
bit); and 2) the number of test-to-production regular dependencies
decreasedbyupto12%(JackRabbit).Similarly,Incolumns16Å›17,
weexaminethenumberofdependenciesamongthetestclasses,and
the percentage of increase after the refactoring (column Å‚#(%In.)
of T-T Dps.Å¾). We separate the inheritance relationship and any
other dependencies. We observe that: 1) the number of inheritance
relationshipamongtestclassesdecreasedinJackRabbitandCom-
mons, by 8% and 1%, respectively, and remained stable in the other
twoprojects;and2)theregulardependenciesamongtestclasses
decreasedby7% or9% inprojectsexceptJackRabbit.
Summary: Therefactoringoveralldecreasescodecomplex-
ity.Inparticular,itnon-triviallydecouplesthetestfromthe
productioncode,byremoving24%to38%oftheinheritance
and5%to12%oftheregulardependencies.Italsodecouples
thetestcodeitselfÃremovingtheinternalinheritancebyup
to 8% and regular dependencies by up to 9%. Meanwhile, it
slightlyincreasestheLOCoftherefactoredtestclasses,but
more obviously decreasesthe number of methodsandfields.
547AnAutomatic Refactoring Framework forReplacing Test-ProductionInheritanceby MockingMechanism ESEC/FSE â€™21, August 23â€“28, 2021,Athens,Greece
Table 6:CodeComplexity Change
Proj.LOC Method Field # (%In.)of T-PDps. # (%In.)of T-T. Dps.
#Avg.Avg. %In. Min.%In. Max %In. #Avg.Avg. %In. Min.%In. Max %In. #Avg.Avg. %In. Min.%In. Max %In. Inherit. Regular Inherit. Regular
JackRabbit 353 4% -45% 61% 64 -5% -88% -1% 6-9% -100% 0% 37 (-38%) 225 (-12%) 329 (-8%) 956 (-0%)
Log4J2 108 8% -24% -50% 17-11% -78% 0% 4-20% -100% 0%100 (-24%) 445 (-10%) 16 (0%) 75 (-7%)
Qpid-Proton-J 178 6% 1% 33% 28-14% -80% 0% 6-22% -100% 0% 34 (-26%) 264 (-9%) 22 (0%) 75 (-7%)
Commons 261 4% -9% 126% 29 -5% -67% 12% 7-11% -100% 0%399 (-35%) 4010 (-5%) 168 (-1%) 737 (-9%)
RQ4:Table7shows the running time (in seconds) of the frame-
work. Column Å‚#TestCl.Å¾ shows the total number of test classes
ineachprojectÃ131(Qpid)to3599(Commons).Thisistheinitial
inputsizetothecandidateidentification.Amongthese,weidentify
34(Qpid)to405(Commons)testsubclasses,whichareprocessed
bythethreefilteringlayers(Section 5.1).Thedetectiontimeranges
from 30 to 250 seconds per project. The average execution time of
refactoringeachcaserangesfrom0.7(JackRabbit)to1.5(Log4J2)
seconds,with thestandard deviation of 0.4 (Qpid)to2.2(Log4J2).
Table 7:Auto-refactoringPerformance
Proj.Detection Time (s) Refactoring Time (s)
#TestCl. #SubCl. Total-T #CaseAvg-TMax-T Min-TStd
JackRabbit 1060 71 42 260.72.2 0.30.6
Log4J2 1069 100 203 261.510.6 0.32.2
Qpid-Proton-J 131 34 30 90.91.7 0.40.4
Commons 3599 405 250 1391.011.4 0.21.3
Total 5859 610 535 2001.011.4 0.21.3
Summary: The run-time performance of the framework is
a few minutes for detecting all refactoring candidates in a
project, and a few seconds for refactoring each case. This
suggeststhat our approach isefficient.
6.4 QualitativeEvaluation(RQ5)
6.4.1 Study Design. Participants: We invite six full-time devel-
opers from a software company, who remain anonymous in the
study.Accordingtotheentrancesurvey,theparticipantsarewell
qualified in this study. Four participants have 1 to 4 years working
experience as a software engineer/developer. And the other two
have5-9yearsand10+yearsofexperience.Allparticipantshave
experiencewithunittesting.Plus,theyallhavepriorexperience
with JUnit and MockitoÃ fourparticipants with 1 to 4 years, one
with5-9years, and onewith10+years.
Study Cases: We select a total of six test cases that use test
subclassesfromourstudydataset.Thesecasesareintwodistinc-
tive sets, ğ‘†1andğ‘†2, each with three cases. Each set is selected to
ensure that the three cases are comprehensive to cover all features
illustratedintheproblemformalizationinSection 4andallpossible
refactoring steps introduced in Section 5.2. In addition, the logic of
the selected cases are easy to understand such that developers can
finish the study inafewhours.
Study Process: Both sets, ğ‘†1andğ‘†2, are given toall six partici-
pants.EachparticipantimplementsmanualrefactoringononesetÃ
namely,the implementation set,andreviewstheauto-refactoring
solutions to the other setÃnamely the reviewset. Of a particular
note,theparticipantsarenottoldthattheprovidedsolutionsare
auto-generatedbyourtool. ğ‘†1andğ‘†2servedifferentrolesfordif-
ferentparticipants.Forexample,forparticipant#1, ğ‘†1servesasthe
implementation set andğ‘†2serves as the reviewset; then for partici-
pant#2,ğ‘†1andğ‘†2switchtherolesÃ ğ‘†1forreviewandğ‘†2implement .
Weaskeachparticipanttofirstimplementmanualrefactoringontheimplementation set and then review the provided solutions for
thereviewset.Afterrefactoring/reviewingacase,theparticipant
takes a survey to evaluate the value, the quality (only for review
case),andthebenefitsofrefactoring.Assuch,eachcaseismanually
refactoredbythreeparticipants,andtherespectiveauto-refactoring
solution is reviewed by another three participants. Thus, each case
receivessixsurveyresultsÃthreeforthemanualrefactoringand
threefortheprovidedauto-refactoringsolutionÃfromallsixpar-
ticipants. This minimizesindividualbiases of participants.
The study is held remotely on the AWS servers [ 21]. Theimple-
mentation casesareloadedandconfiguredinEclipse.The review
cases are provided with the GitHublinks to the original case, as
wellasthelinkstothecommitidandadiffviewoftheprovided
auto-refactoring solution. This is the same environment as the
participants normally perform code reviewintheir daily work.
Manual Refactoring Status: As shown in Table 8, for each
case,atleastoneinthreeparticipantssuccessfullyperformsmanual
refactoring.Weobtainatotalof13successfulmanualrefactoring
versions out of the 18 manual refactoring attempts on the six cases.
Among these, fivemanual refactoring versions each takes 5-10
minutes, onetakes 10-15 minutes, and seventakes more than 15
minutestofinish. Insummary,72%ofthemanualrefactoring
cases are successful, and most cases require more than 15
minutes to refactormanually.
Table 8:Successful Manual Refactoring
Case1Case2Case3Case4Case5Case6
# Succ.Participants 2/31/32/32/33/33/3
Time1 (#Min) 5-10>15>15>155-105-10
Time2 (#Min) >15 ->155-1010-15 5-10
Time3 (#Min) ---->15>15
SurveyQuestions: Thesurveyquestionsarelistedbelow.Note
that Å‚(I&R)Å¾ indicates the question applies to both the implementa-
tionandreviewcases;Å‚(R)Å¾indicatesthatthequestiononlyapplies
to the reviewcases.
â€¢SQ1(R):Ratethequalityoftheprovidedrefactoringsolution.(1-
6Scale).Andpleaseprovideanysuggestions(OpenEnded).
â€¢SQ2 (I&R): Rate your agreement with this statement: Using
mocksinsteadofsub-classingimprovedthecodequalityofthis
example.(1-4Scale).Explain your rating(OpenEnded).
â€¢SQ3 (I&R): Rate your agreement with these statements regard-
ing thebenefitsofmocks: Therefactored codeÃ
Å›makesthetest design more cohesive/concise (Scale 1-6).
Å›makesthetest conditions more explicit(Scale 1-6).
Å›is lesscoupled from the production code (Scale 1-6).
â€¢SQ4(I&R):Doyouseeanyotherbenefitsordrawbacksfrom
this refactoring?
SQ1focusesonassessingthequalityoftheauto-refactoringsolu-
tions.SQ2assessesthegeneralbenefitsofrefactoring.SQ3andSQ4
evaluatethedetailedbenefits(ordrawbacks)oftherefactoring.For
548ESEC/FSE â€™21, August 23â€“28, 2021,Athens,Greece XiaoWang,Lu Xiao, Tingting Yu, Anne Woepse, Sunny Wong
SQ2toSQ4,wealsoinvestigatethediscrepanciesbetween imple-
mentandreviewcasestoshowhowtheauto-refactoringsolution
isdifferentfrom the manual refactoring.
6.4.2 Study Results. SQ1 (R): Figure5shows the participantsâ€™
rating on the quality of auto-refactoring solutions. The x-axis is
the case ID. The y-axis is the score in the scale of 1 (poor) to 6
(excellent)Ãoverall,ascoreof4oraboveindicatesapositiveopinion.
The size of (and the number in) each circle shows the three scores
given to each case. We observe that, for each case, at least one
participant rates positivelyÃscore at least 4. In particular, case 4
and case 5 receive unanimous positive scores. And case 3 and case
6receive 2(outof3)positive scores.
Figure 5:SQ1:QualityofAuto-RefactoringSolution
We investigate the comments from the participants. We find
that the the negative scores fall into three types: 1) The criticism is
abouttheoriginaltestdesign,whichisirrelevanttotheMockito-
based refactoring(Case 1,case3,andcase 6). 2)Participants have
misunderstandingsonthereviewedcases(case2).Therearetwo
sub-classesinvolvedincase2Ãoneofthemcannotberefactored
duetoF-1.1(Section5.1).However, theparticipantthinksthatwe
should also refactor it. The other misunderstanding is that the
participantsuggestsusing mockmethod (Section5.2.5)whenitis
not appropriate. 3) There exists subjective preferencesÃone par-
ticipantfavorstheseparatedtestsubclassandtestcasebeforethe
refactoring in case 1, rather than merging the logic of the subclass
withthetestcase. Overall, participantsrate positivelyon the
refactoringsolutionsgenerated by our tool.
SQ2 (I&R): As shown in Figure 6, in 17 out of the total 18
implementation cases, participants agree or strongly agree that
using mocks instead of inheritance improves the code quality. The
responseonthe reviewcasesishighlyconsistentÃin13outof18
cases, participants agree or strongly agree that the code quality
improves. The disagreement on the reviewcases is due to two
reasons 1) participants expect to see improvements on the test
design/logicitself;and2)participantsprefertoseparatethemock
behaviorsinasubclass. Overall, participantsagree that using
mocks to replace inheritanceimprovesthecodequality.
SQ3(I&R): Table9summarizesratingsontherefactoringbene-
fitsincohesion/conciseness,explicit,anddecoupling.Wereportthe
meanrating(columnÅ‚MeanÅ¾)ofeachbenefitonthe implementation
(on row 1) and review(on row 2) cases separately. In addition, a
ratingof4( Å‚SomewhatAgreeÅ¾ )oraboveindicatespositiveopinionÃ
thuswealsoreportthepercentageofratingof4andabove(Column
Å‚Agree%Å¾)oneachbenefit.Thediscrepanciesoftheratingsbetween
theimplementation andreviewcasesare summarizedinrow3.
We observe that for the majority casesÃat least 77% and 61%
of theimplementation andreviewsets respectivelyÃparticipants
Figure 6:SQ2:Improved CodeQuality(Impl. vs.Review)
Table 9:RefactoringBenefits(Implementation vs.Review)
BenefitsCohesion/Concise Explicit Decoupled
Mean Agree% MeanAgree% MeanAgree%
Implementation 5.3 100% 5.285% 4.577%
Review 3.8 61% 3.867% 4.172%
Discrepancy 1.5 39% 1.518% 0.4 5%
agree with these benefits. However, the agreement is weaker on
thereviewcases compared to the implementation cases. The review
casesreceive0.4to1.5lowermeanrating,and5%to39%lesspositive
rating. This indicates that the manual refactoring by developers
boosts thesebenefitsonmore casesandto ahigher degree.
To further investigate the causes of the discrepancies, we re-
viewed the code difference between the manual refactoring and
theauto-refactoringsolutions.Wefindthatthediscrepanciesare
largelyduetotheimprovementsparticipantsmadetothetestlogic
and design, such as simplifying the test logic and removing redun-
dancy.Inconclusion, our auto-refactoring frameworkalone
helpsdevelopersreapthethreebenefitstosomeextent.For
furtherbenefits,developers should improve the testitself.
SQ4 (I&R): Theparticipantsreportadditionalbenefitsforthe
implementation cases,includingimprovedreadabilityandunder-
standability (four cases), maintainability (one case), and test power
(twocases).Similarly,in two,one,andone review cases,participants
reportthesethreebenefitsaswell.Thereadability/understandability
and maintainability are associated with the three benefit aspects
surveyedinSQ4.TheimprovedtestpowerisbecauseMockitoal-
lows additional verification of the mock object execution/status.
Anotabledrawbackonthereviewcasesisthattheoriginalcode
comments, which explain the test logic, are not preserved after the
refactoring. Insummary,ourapproachcanimprovetheread-
ability/understandability and maintainability the test code,
and can make the test more powerful. However, the draw-
backisthat theoriginalcodecomments cannot be retained.
Potential Improvements: Finally, we compare the auto and
manualrefactoringsolutions.Thisleadstoasummaryof(potential)
improvements.First,weshouldimportMockitostaticmethodsin
refactoringtosimplifythecode.Atthetimeofwriting,thisissueis
fixed.Second,wecanfurtherenhancetheusageofMockito,suchas
verifyingexecutionorderofmockobjects,usingargumentcaptor,
checking input argument value and type. However, this potentially
relies on dynamic analysis. Finally, more future work should focus
on improving the general test design, such as removing redundant
code and simplifying the test logic. We plan to address the last two
directions inour future work.
549AnAutomatic Refactoring Framework forReplacing Test-ProductionInheritanceby MockingMechanism ESEC/FSE â€™21, August 23â€“28, 2021,Athens,Greece
Summary: 1) Participants generally rate positively on the
refactoringsolutionsgeneratedbyourframework.2)Partic-
ipantsagreethattherefactoringsolutionsgeneratedbythe
framework improve the cohesion/conciseness of test code,
make test condition more explicit, and decouple test code
fromproductioncode.Theyalsopointoutadditionalbenefits,
including readability/understandability and more powerful
test.However,tofurtherenhancethesebenefits,developers
need to improve the test logic itself with manual effort. Thus,
our tool can serve as an efficient first step in refactoring. 3) A
obvious draw-back of our framework is that the original code
comments cannotbe retainedafter refactoring.
7 LIMITATIONSAND THREATS TO VALIDITY
Our framework has five limitations. First, it only focuses on re-
placing inheritance by using Mockito for mocking. It does not
improvethetestcaselogic/designitself.Inaddition,comparedto
manualrefactoringcreatedbydevelopers,ourframeworkislimited
inleveragingtheadvancedfeaturesofMockito,suchasverifying
execution order of mock objects. In manual refactoring, the us-
age of advanced features is based on manual understanding of the
test intention. This could potentially be automated by dynamically
analyzing the test case execution. However, our framework cur-
rentlyispurelybasedonstaticcodeanalysis.Third,ourframework
wonâ€™t preserve the code comments after the refactoring. Fourth,
although we generated a total of 51,811 mutants to evaluate the
test behaviour preservation of test cases, we cannot guarantee that
thetestbehaviourpreservesunderallpossibledefects.Sinceitis
impossibletoexhaustivelyevaluatethetestbehaviorsunderallpos-
sible defects through generating mutants. Lastly, our framework is
limited to Java and Mockito. However, the overall design principle
of mocking and refactoring rationale in this work should still hold
for other languages and mocking frameworks.We plan to address
theselimitationsinthe future.
Onlyverylimitedempiricalevidenceisavailabletoshowthat
inheritance-based mocking leads to code that is more difficult to
maintainthanusingamockingframework[ 68].Inthiswork,we
investigate this problem in a qualitative study involving real devel-
opers (RQ5) and the results indicate that using a mocking frame-
workcanimprovetestcodequalityandachievethethreeaspectsof
benefitsthatrelatetomaintainability(SQ2andSQ3inSection6.4.1)
compared to using inheritance. However, we acknowledge that
theconclusionmayvarydependingonthegroupofparticipants.
Another external threat to validity is that the benefits of using a
mocking framework over inheritance requires that the user has
preliminaryunderstandingofthemockingframework.Ifauserhas
zeropriorknowledge,he/shemayfindinheritanceeasiertouseand
understand. Particularly, the participants in the qualitative evalua-
tion all have prior experience with Mockito. This poses an internal
threatto validity towardsthe findingsreportedinSection 6.4.
8 RELATED WORK
Software Refactoring: Significantsoftwaredevelopmentcostis
devoted to software maintenance [ 51,53,60,80], as software be-
comesmorecomplexanddriftsawayfromits originaldesign[ 38,49,59,76].Refactoringisanimportantmaintenanceactivitythat
restructures a system and improves code quality [ 23,26,26,40,42,
59,66,84,85].Kimshowedthatrefactoringischallengingandthere
generally lacks tool support [ 54]. In past years, researchers pro-
posedmethodsandtoolstoautomatetherefactoringprocess[ 25,
28,41,44,52,57,62,77Å›79,87,88].Mostrefactoringtoolsfocuson
detecting and refactoring God Classes [ 27,29,37,41,42], and elim-
inating Code Clone [ 25,50,58,72,77,78]. Tsantalis et. al proposed
arefactoringapproachtoreplacestatechecking(i.e. if/else)with
polymorphism to reduce code complexity [ 79].Despite numer-
ous prior works, we are the first to focusing on refactoring
the usage of inheritance by using mocking framework, to
improve unittestingdesign.
Test CodeSmells: Codesmellisasurfaceindication thatusu-
ally corresponds to a deeper problem in the system [ 42]. Test
smells are the sub-optimal design choices in test code [ 61,82].
They can make test cases less effective and more difficult to under-
stand[30,33Å›35,75,82,86].Therearevarioustechniquesandtools
tosupportautomatedtestsmellanalysis[ 32,45,47,55,69,71,82,83].
Van Deursen et. al defined a catalog of 11 test smells [ 82]. Based
onthiscatalog,VanRompaeyet.alintroducedametric-basedtech-
nique to identify two smells, General Fixture andEager Test [83].
Greileret.aldevelopedaMavenplugintodetecttestfixturerelated
smellsand provide guidance for refactoring them [ 47]. Santanaet.
al implemented an Eclipse plugin to refactor Assertion Roulette and
DuplicateAssert [71].Otherworksfoucsonanalyzingtheimpact
of test smells [ 31,67,75,81].To our best knowledge, no prior
workinvestigatedsub-optimalpracticeinunittestmocking.
9 CONCLUSION
We proposed a refactoring framework and implemented it as an
Eclipse-Plugin [ 22] toautomaticallysearchfor theusage of inher-
itance and replace it by Mockito for mocking. The framework is
builtupontheempiricalexperiencedrawnfrom fiveopen-source
projects.Weevaluatedourframeworkon fouropen-sourceprojects,
bothquantitativelyandqualitatively.Thequantitativeevaluation
provedthatourframeworkwasgenerallyapplicabletonewdataset
that was independent from the empirical study. The refactoring
solution generally preserves test behaviors in term of detecting
defects (in terms of mutants). The refactoring reduced the code
complexityÃparticularlydecoupledtestcodefromproductioncode.
Lastly, the framework provided efficient run-time performance on
real-life projects. The qualitative evaluation, involving experienced
developers, suggestedthat auto-refactoringsolutions byourframe-
workwere ofgood quality. Furthermore, therefactoring solutions
improvedthequalityoftheunittestcasesinvariousaspects,suchas
improving the cohesion/conciseness, readability/understandability
and maintainability of the test code, made test condition more
explicit andthe test casesmore powerful.
ACKNOWLEDGMENTS
ThisworkwassupportedinpartbytheU.S.NationalScienceFoun-
dation(NSF)undergrants CCF-1909085andCCF-1909763.
REFERENCES
[1] [n.d.]. https://easymock.org/ .
[2] [n.d.]. https://site.mockito.org/ .
550ESEC/FSE â€™21, August 23â€“28, 2021,Athens,Greece XiaoWang,Lu Xiao, Tingting Yu, Anne Woepse, Sunny Wong
[3] [n.d.]. https://powermock.github.io/ .
[4] [n.d.]. https://junit.org/junit5/ .
[5] [n.d.]. https://wiki.python.org/moin/PyUnit .
[6] [n.d.]. https://dubbo.apache.org/ .
[7] [n.d.]. https://druid.apache.org/ .
[8] [n.d.]. https://accumulo.apache.org/ .
[9] [n.d.]. https://cayenne.apache.org/ .
[10] [n.d.]. https://cloudstack.apache.org/ .
[11][n.d.].https://javadoc.io/static/org.mockito/mockito-core/3.1.0/org/mockito/
MockSettings.html#extraInterfaces-java.lang.Class...- .
[12][n.d.].https://github.com/mockito/mockito/wiki/FAQ#what-are-the-limitations-
of-mockito .
[13][n.d.]. https://github.com/FasterXML/jackson-annotations/wiki/Jackson-
Annotations .
[14] [n.d.]. https://projects.eclipse.org/projects/eclipse.jdt .
[15][n.d.].https://javadoc.io/doc/org.mockito/mockito-core/latest/org/mockito/
stubbing/OngoingStubbing.html#thenAnswer-org.mockito.stubbing.Answer- .
[16] [n.d.]. https://jackrabbit.apache.org/jcr/index.html .
[17] [n.d.]. https://logging.apache.org/log4j/ .
[18] [n.d.]. https://qpid.apache.org/ .
[19] [n.d.]. https://pitest.org/ .
[20] [n.d.]. https://pitest.org/quickstart/mutators/ .
[21] [n.d.]. https://aws.amazon.com/lightsail/ .
[22] [n.d.]. https://doi.org/10.5281/zenodo.5111183
[23]MesfinAbebeandCheol-JungYoo.2014. Trends,opportunitiesandchallenges
ofsoftwarerefactoring:Asystematicliteraturereview. internationalJournalof
softwareengineering and its Applications 8,6 (2014), 299Å›318.
[24] Paul Ammann and Jeff Offutt. 2016. Introduction tosoftware testing . Cambridge
UniversityPress. https://doi.org/10.1017/9781316771273
[25]Magdalena Balazinska, Ettore Merlo, Michel Dagenais, Bruno Lague, and Kostas
Kontogiannis. 2000. Advanced clone-analysis to support object-oriented system
refactoring.In ProceedingsSeventhWorkingConferenceonReverseEngineering .
IEEE,98Å›107. https://doi.org/10.1109/WCRE.2000.891457
[26]AbdulrahmanAhmedBobakrBaqaisandMohammadAlshayeb.2020. Automatic
software refactoring: a systematic literature review. Software Quality Journal 28,
2 (2020), 459Å›502. https://doi.org/10.1007/s11219-019-09477-y
[27]Gabriele Bavota, Andrea De Lucia, Andrian Marcus, and Rocco Oliveto. 2010. A
two-step technique for extract class refactoring. In Proceedings of the IEEE/ACM
international conference on Automated software engineering . 151Å›154. https:
//doi.org/10.1145/1858996.1859024
[28]GabrieleBavota,MalcomGethers,RoccoOliveto,DenysPoshyvanyk,andAn-
drea de Lucia. 2014. Improving software modularization via automated analysis
of latent topics and dependencies. ACM Transactions on Software Engineering
and Methodology (TOSEM) 23,1 (2014), 1Å›33. https://doi.org/10.1145/2559935
[29]GabrieleBavota,Rocco Oliveto, AndreaDeLucia,Giuliano Antoniol, and Yann-
GaÃ«l GuÃ©hÃ©neuc. 2010. Playing with refactoring: Identifying extract class oppor-
tunities through game theory. In 2010 IEEE International Conference on Software
Maintenance . IEEE,1Å›5. https://doi.org/10.1109/ICSM.2010.5609739
[30]Gabriele Bavota, Abdallah Qusef, Rocco Oliveto, Andrea De Lucia, and David
Binkley.2012. Anempiricalanalysisofthedistributionofunittestsmellsand
theirimpactonsoftwaremaintenance.In 201228thIEEEInternationalConference
onSoftwareMaintenance(ICSM) .IEEE,56Å›65. https://doi.org/10.1109/ICSM.2012.
6405253
[31]Gabriele Bavota, Abdallah Qusef, Rocco Oliveto, Andrea De Lucia, and Dave
Binkley. 2015. Are test smells really harmful? an empirical study. Empirical
Software Engineering 20, 4 (2015), 1052Å›1094. https://doi.org/10.1007/s10664-
014-9313-0
[32]KentBeck.2003. Test-drivendevelopment:byexample . Addison-WesleyProfes-
sional.
[33]MoritzBeller,GeorgiosGousios,AnnibalePanichella,SebastianProksch,Sven
Amann, and Andy Zaidman. 2017. Developer testing in the ide: Patterns, beliefs,
andbehavior. IEEETransactionsonSoftwareEngineering 45,3(2017),261Å›284.
https://doi.org/10.1109/TSE.2017.2776152
[34]Moritz Beller,GeorgiosGousios,Annibale Panichella, andAndyZaidman. 2015.
When, how, and why developers (do not) test in their IDEs. In Proceedings of
the 2015 10th Joint Meeting on Foundations of Software Engineering . 179Å›190.
https://doi.org/10.1145/2786805.2786843
[35]StefanBerner,RolandWeber,andRudolfKKeller.2005. Observationsandlessons
learnedfromautomatedtesting.In Proceedingsofthe27thinternationalconference
onSoftwareengineering . 571Å›579. https://doi.org/10.1109/ICSE.2005.1553603
[36]AntoniaBertolino.2007. Softwaretestingresearch:Achievements,challenges,
dreams.In FutureofSoftwareEngineering(FOSEâ€™07) .IEEE,85Å›103. https://doi.
org/10.1109/FOSE.2007.25
[37]AlexanderChatzigeorgiou,SpirosXanthos,andGeorgeStephanides.2004. Evalu-
atingobject-orienteddesignswithlinkanalysis.In Proceedings.26thInternational
Conference on Software Engineering . IEEE, 656Å›665. https://doi.org/10.1109/ICSE.
2004.1317487[38]Don Coleman, Dan Ash, Bruce Lowther, and Paul Oman. 1994. Using metrics to
evaluatesoftwaresystemmaintainability. Computer 27,8(1994),44Å›49. https:
//doi.org/10.1109/2.303623
[39]Ermira Daka and Gordon Fraser. 2014. A survey on unit testing practices and
problems. In 2014 IEEE 25th International Symposium on Software Reliability
Engineering . IEEE,201Å›211. https://doi.org/10.1109/ISSRE.2014.11
[40]Karim O Elish and Mohammad Alshayeb. 2009. Investigating the effect of refac-
toringonsoftwaretestingeffort.In 200916thAsia-PacificSoftwareEngineering
Conference . IEEE,29Å›34. https://doi.org/10.1109/APSEC.2009.14
[41]MariosFokaefs,NikolaosTsantalis,EleniStroulia,andAlexanderChatzigeorgiou.
2012. Identificationandapplicationofextractclassrefactoringsinobject-oriented
systems. Journal of Systems and Software 85, 10 (2012), 2241Å›2260. https:
//doi.org/10.1016/j.jss.2012.04.013
[42]Martin Fowler. 2018. Refactoring: improving the design of existing code . Addison-
WesleyProfessional. https://doi.org/10.1007/3-540-45672-4_31
[43]SteveFreeman,TimMackinnon,NatPryce,andJoeWalnes.2004. Mockroles,
notobjects.In Companiontothe19thannualACMSIGPLANconferenceonObject-
oriented programming systems, languages, and applications . 236Å›246. https:
//doi.org/10.1145/1028664.1028765
[44]George Ganea, Ioana Verebi, and Radu Marinescu. 2017. Continuous quality
assessment with inCode. Science of Computer Programming 134 (2017), 19Å›36.
https://doi.org/10.1016/j.scico.2015.02.007
[45]VahidGarousi,BarisKucuk,andMichaelFelderer.2018. Whatweknowabout
smellsinsoftwaretestcode. IEEESoftware 36,3(2018),61Å›73. https://doi.org/
10.1109/MS.2018.2875843
[46]Java Code Geeks. [n.d.]. Mockito Programming Cookbook. https:
//www.javacodegeeks.com/wp-content/uploads/2016/09/Mockito-
Programming-Cookbook.pdf .
[47]MichaelaGreiler,ArieVanDeursen,andMargaret-AnneStorey.2013. Automated
detection of test fixture strategies and smells. In 2013 IEEE Sixth International
Conference on Software Testing, Verification and Validation . IEEE, 322Å›331. https:
//doi.org/10.1109/ICST.2013.45
[48] Jeff Grigg.2012. http://wiki.c2.com/?ArrangeActAssert/ .
[49]Tor Guimaraes. 1983. Managing application program maintenance expenditures.
Commun. ACM 26,10(1983), 739Å›746. https://doi.org/10.1145/358413.358421
[50]KeisukeHotta,YoshikiHigo,and ShinjiKusumoto.2012. Identifying, tailoring,
andsuggestingformtemplatemethodrefactoringopportunitieswithprogram
dependencegraph.In 201216thEuropeanConferenceonSoftwareMaintenance
and Reengineering . IEEE,53Å›62. https://doi.org/10.1109/CSMR.2012.16
[51]ClementeIzurietaandJamesMBieman.2007. Howsoftwaredesignsdecay:A
pilotstudyofpatternevolution.In FirstInternationalSymposiumonEmpirical
Software Engineering and Measurement (ESEM 2007) . IEEE, 449Å›451. https:
//doi.org/10.1109/ESEM.2007.55
[52]YoshioKataoka,MichaelDErnst,WilliamGGriswold,andDavidNotkin. 2001.
Automatedsupportforprogramrefactoringusinginvariants.In ProceedingsIEEE
International Conference on Software Maintenance. ICSM 2001 . IEEE, 736Å›743.
https://doi.org/10.1109/ICSM.2001.972794
[53]ChrisF.KemererandSandraSlaughter.1999. Anempiricalapproachtostudying
softwareevolution. IEEEtransactionsonsoftwareengineering 25,4(1999),493Å›509.
https://doi.org/10.1109/32.799945
[54]MiryungKim,ThomasZimmermann,andNachiappanNagappan.2012. Afield
study of refactoring challenges and benefits. In Proceedings of the ACM SIGSOFT
20thInternationalSymposiumontheFoundationsofSoftwareEngineering .1Å›11.
https://doi.org/10.1145/2393596.2393655
[55]NegarKoochakzadehandVahidGarousi.2010. Atester-assistedmethodology
for test redundancy detection. Advances in Software Engineering 2010 (2010).
https://doi.org/10.1155/2010/932686
[56]Madhuri R Marri, Tao Xie, Nikolai Tillmann, Jonathan De Halleux, and Wolfram
Schulte. 2009. An empirical study of testing file-system-dependent software
with mock objects. In 2009 ICSE Workshop on Automation of Software Test . IEEE,
149Å›153. https://doi.org/10.1007/s10664-018-9663-0
[57]PhilipMayerandAndreasSchroeder.2014. Automatedmulti-languageartifact
bindingandrenamerefactoringbetweenJavaandDSLsusedbyJavaframeworks.
InEuropeanConferenceonObject-OrientedProgramming .Springer,437Å›462. https:
//doi.org/10.1007/978-3-662-44202-9_18
[58]Davood Mazinanian, Nikolaos Tsantalis, Raphael Stein, and Zackary Valenta.
2016. JDeodorant: clone refactoring. In Proceedings of the 38th international
conference on software engineering companion . 613Å›616. https://doi.org/10.1145/
2889160.2889168
[59]Tom Mens and Tom TourwÃ©. 2004. A survey of software refactoring. IEEE
Transactions on software engineering 30, 2 (2004), 126Å›139. https://doi.org/10.
1109/TSE.2004.1265817
[60]Tom Mens, Michel Wermelinger, StÃ©phane Ducasse, Serge Demeyer, Robert
Hirschfeld,andMehdiJazayeri.2005. Challengesinsoftwareevolution.In Eighth
InternationalWorkshoponPrinciplesofSoftwareEvolution(IWPSEâ€™05) .IEEE,13Å›22.
https://doi.org/10.1109/IWPSE.2005.7
[61]Gerard Meszaros. 2007. xUnit test patterns: Refactoring test code . Pearson Educa-
tion.https://doi.org/10.1145/1869542.1869622
551AnAutomatic Refactoring Framework forReplacing Test-ProductionInheritanceby MockingMechanism ESEC/FSE â€™21, August 23â€“28, 2021,Athens,Greece
[62]Mohamed Wiem Mkaouer, Marouane Kessentini, Slim Bechikh, Kalyanmoy Deb,
and Mel Ã“ CinnÃ©ide. 2014. Recommendation system for software refactoring
usinginnovizationandinteractivedynamicoptimization.In Proceedingsofthe
29th ACM/IEEE international conference on Automated software engineering . 331Å›
336.https://doi.org/10.1145/2642937.2642965
[63]Shaikh Mostafa and Xiaoyin Wang. 2014. An empirical study on the usage of
mocking frameworks in software testing. In 2014 14th international conference on
qualitysoftware . IEEE,127Å›132. https://doi.org/10.1109/QSIC.2014.19
[64]GlenfordJMyers,Tom Badgett,ToddMThomas,andCoreySandler.2004. The
art of software testing . Vol. 2. Wiley Online Library. https://doi.org/10.1002/
9781119202486
[65]Jagadeesh Nandigam, Venkat N Gudivada, Abdelwahab Hamou-Lhadj, and Yon-
glei Tao. 2009. Interface-based object-oriented design with mock objects. In 2009
Sixth International Conference on Information Technology: New Generations . IEEE,
713Å›718. https://doi.org/10.1109/ITNG.2009.268
[66] William F Opdyke. 1992. Refactoring object-oriented frameworks. (1992).
[67]Fabio Palomba, Dario Di Nucci, Annibale Panichella, Rocco Oliveto, and Andrea
DeLucia.2016.Onthediffusionoftestsmellsinautomaticallygeneratedtestcode:
Anempiricalstudy.In 2016IEEE/ACM9thInternationalWorkshoponSearch-Based
SoftwareTesting(SBST) . IEEE,5Å›14. https://doi.org/10.1145/2897010.2897016
[68]GustavoPereiraandAndreHora.2020. AssessingMockClasses:AnEmpirical
Study. In 2020 IEEE International Conference on Software Maintenance and Evolu-
tion (ICSME) . IEEE,453Å›463. https://doi.org/10.1109/ICSME46990.2020.00050
[69]StefanReichhart,TudorGÃ®rba,andStÃ©phaneDucasse.2007. Rule-basedAssess-
mentof Test Quality. J.Object Technol. 6,9 (2007), 231Å›251.
[70]Per Runeson. 2006. A survey of unit testing practices. IEEE software 23, 4 (2006),
22Å›29.https://doi.org/10.1109/MS.2006.91
[71]RailanaSantana,LuanaMartins,LarissaRocha,TÃ¡ssioVirgÃ­nio,AdrianaCruz,
Heitor Costa, and Ivan Machado. 2020. RAIDE: a tool for Assertion Roulette
and Duplicate Assert identification and refactoring. In Proceedings of the 34th
Brazilian Symposium on Software Engineering . 374Å›379. https://doi.org/10.1145/
3422392.3422510
[72]Sandro Schulze and Martin Kuhlemann. 2009. Advanced analysis for code clone
removal. In Proceedings des Workshops der GI-Fachgruppe Software Reengineering
(SRE),erschienenindenGISoftwaretechnik-Trends29 (2) . Citeseer, 10Å›12.
[73]Davide Spadini, MaurÃ­cio Aniche, Magiel Bruntink, and Alberto Bacchelli. 2017.
To mock or not to mock? An empirical study on mocking practices. In 2017
IEEE/ACM14thInternationalConferenceonMiningSoftwareRepositories(MSR) .
IEEE,402Å›412. https://doi.org/10.1109/MSR.2017.61
[74]Davide Spadini, MaurÃ­cio Aniche, Magiel Bruntink, and Alberto Bacchelli. 2019.
Mockobjectsfortestingjavasystems. EmpiricalSoftwareEngineering 24,3(2019),
1461Å›1498. https://doi.org/10.1007/s10664-018-9663-0
[75]DavideSpadini,FabioPalomba,AndyZaidman,MagielBruntink,andAlberto
Bacchelli.2018. Ontherelationoftestsmellstosoftwarecodequality.In 2018
IEEE International Conference on Software Maintenance and Evolution (ICSME) .
IEEE,1Å›12. https://doi.org/10.1109/ICSME.2018.00010[76]GÃ¡bor Szoke, Csaba Nagy, Rudolf Ferenc, and Tibor GyimÃ³thy. 2016. Designing
and developing automated refactoring transformations: An experience report.
In2016IEEE23rdInternationalConferenceonSoftwareAnalysis,Evolution,and
Reengineering(SANER) ,Vol.1.IEEE,693Å›697. https://doi.org/10.1109/SANER.
2016.17
[77]Robert Tairas and Jeff Gray. 2009. Get to know your clones with CeDAR. In
Proceedingsofthe24thACMSIGPLANconferencecompaniononObjectoriented
programming systems languages and applications . 817Å›818. https://doi.org/10.
1145/1639950.1640030
[78]Robert Tairas and Jeff Gray. 2012. Increasing clone maintenance support by
unifying clone detection and refactoring activities. Information and Software
Technology 54, 12 (2012), 1297Å›1307. https://doi.org/10.1016/j.infsof.2012.06.011
[79]NikolaosTsantalisandAlexanderChatzigeorgiou.2010. Identificationofrefac-
toringopportunitiesintroducingpolymorphism. JournalofSystemsandSoftware
83,3 (2010), 391Å›404. https://doi.org/10.1016/j.jss.2009.09.017
[80]Qiang Tu et al .2000. Evolution in open source software: A case study. In Pro-
ceedings2000InternationalConferenceonSoftwareMaintenance .IEEE,131Å›142.
https://doi.org/10.1109/ICSM.2000.883030
[81]Michele Tufano, Fabio Palomba,Gabriele Bavota,Massimiliano DiPenta, Rocco
Oliveto, Andrea De Lucia, and Denys Poshyvanyk. 2016. An empirical inves-
tigation into the nature of test smells. In Proceedings of the 31st IEEE/ACM
International Conference on Automated Software Engineering . 4Å›15. https:
//doi.org/10.1145/2970276.2970340
[82]ArieVanDeursen, LeonMoonen, AlexVanDenBergh,and GerardKok.2001.
Refactoringtestcode.In Proceedingsofthe2ndinternationalconferenceonextreme
programming and flexible processes in software engineering (XP2001) . Citeseer,
92Å›95.
[83]B.VanRompaey,B.DuBois,S.Demeyer,andM.Rieger.2007. OnTheDetection
of Test Smells: A Metrics-Based Approach for General Fixture and Eager Test.
IEEE Transactions on Software Engineering 33, 12 (2007), 800Å›817. https://doi.
org/10.1109/TSE.2007.70745
[84]Frens Vonken and Andy Zaidman. 2012. Refactoring with unit testing: A match
made in heaven?. In 2012 19th Working Conference on Reverse Engineering . IEEE,
29Å›38.https://doi.org/10.1109/WCRE.2012.13
[85] William CWake. 2004. Refactoringworkbook . Addison-WesleyProfessional.
[86]Andy Zaidman, Bart Van Rompaey, Arie van Deursen, and Serge Demeyer. 2011.
Studying the co-evolution of production and test code in open source and in-
dustrial developer test processes through repository mining. Empirical Software
Engineering 16,3 (2011), 325Å›364. https://doi.org/10.1007/s10664-010-9143-7
[87]Marcelo Serrano Zanetti, Claudio Juan Tessone, Ingo Scholtes, and Frank
Schweitzer.2014. Automatedsoftwareremodularizationbasedonmoverefac-
toring: a complex systems approach. In Proceedings of the 13th international
conference onModularity . 73Å›84.https://doi.org/10.1145/2577080.2577097
[88]LimingZhaoandJaneHuffmanHayes.2011. Rank-basedrefactoringdecision
support: two studies. Innovations in Systems andSoftware Engineering 7, 3 (2011),
171Å›189. https://doi.org/10.1007/s11334-011-0154-3
552