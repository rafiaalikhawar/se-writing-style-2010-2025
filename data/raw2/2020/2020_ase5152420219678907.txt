Can Neural Clone Detection Generalize to Unseen
Functionalities?
Chenyao Liu‚àó
School of Software
Tsinghua University
liucy19@mails.tsinghua.edu.cnZeqi Lin¬ß
Microsoft Research Asia
Zeqi.Lin@microsoft.comJian-Guang Lou
Microsoft Research Asia
jlou@microsoft.com
Lijie Wen¬ß
School of Software
Tsinghua University
wenlj@tsinghua.edu.cnDongmei Zhang
Microsoft Research Asia
dongmeiz@microsoft.com
Abstract ‚ÄîMany recently proposed code clone detectors exploit
neural networks to capture latent semantics of source code, thus
achieving impressive results for detecting semantic clones. Theseneural clone detectors rely on the availability of large amountsof labeled training data. We identify a key oversight in thecurrent evaluation methodology for neural clone detection: cross-functionality generalization (i.e., detecting semantic clones ofwhich the functionalities are unseen in training). SpeciÔ¨Åcally, wefocus on this question: do neural clone detectors truly learn theability to detect semantic clones, or they just learn how to modelspeciÔ¨Åc functionalities in training data while cannot generalizeto realistic unseen functionalities? This paper investigates howthe generalizability can be evaluated and improved.
Our contributions are 3-folds: (1) We propose an evalua-
tion methodology that can systematically measure the cross-functionality generalizability of neural clone detection. Based onthis evaluation methodology, an empirical study is conducted andthe results indicate that current neural clone detectors cannotgeneralize well as expected. (2) We conduct empirical analysisto understand key factors that can impact the generalizability.We investigate 3 factors: training data diversity, vocabulary,and locality. Results show that the performance loss on unseenfunctionalities can be reduced through addressing the out-of-vocabulary problem and increasing training data diversity. (3) Wepropose a human-in-the-loop mechanism that help adapt neuralclone detectors to new code repositories containing lots of unseenfunctionalities. It improves annotation efÔ¨Åciency with the com-bination of transfer learning and active learning. Experimentalresults show that it reduces the amount of annotations by about88%. Our code and data are publicly available
1.
Index T erms‚ÄîCode Clone Detection, Generalization, Neural
Network, Evaluation Methodology, Human-in-the-Loop
I. I NTRODUCTION
Code clone detection is the task of Ô¨Ånding similar code frag-
ment pairs (i.e., clones) within or between software systems.
It has become an important part in many software engineeringtasks, such as software refactoring ([1]‚Äì[4]), quality manage-ment ([5]‚Äì[8]), defect prediction [9], plagiarism detection ([9],[10]), and program comprehension ([9], [11]).
‚àóWork done during an internship at Microsoft Research
¬ßCorresponding author
1https://github.com/thousfeet/Functionality-generalizationIn recent years, many neural network-based methods are
proposed for detecting semantic clones , and they have
achieved impressive results ([12]‚Äì[19]). Semantic clones areclones in which code fragments implement the same func-tionality, but may have low syntactic similarity. For example,a quick sort code and a heap sort code should be con-sidered semantically equivalent. Traditional matching-basedcode clone detectors (e.g., token matching-based methods, treematching-based methods, and graph matching-based methodswork well in detecting syntactic clones, while previous studies([20], [21]) found that they had limited success with semanticclones. To address this problem, a recent research trend isto leverage deep neural networks to effectively capture com-plex semantic information in code fragments. For example,some studies (e.g., CDLH [12], ASTNN [15], and TBCCD[16]) focus on learning from Abstract Syntax Trees (ASTs),and some other studies ([13], [19]) focus on learning fromControl Flow Graphs (CFGs) or Program Dependency Graphs(PDGs). These studies have achieved impressive results: inwidely-used code benchmarks for code clone detection (e.g.,BigCloneBench [22], GCJ [13], and OJClone [23]), state-of-the-art neural clone detectors achieve more than 90% precisionand recall.
Existing neural clone detectors are supervised, relying on a
large number of annotated true/false code fragment pairs fortraining. This paper identiÔ¨Åes a key oversight in the currentevaluation methodology for neural clone detection:
Cross-Functionality Generalizability: the ability to detect
semantic clones of which the functionalities have never beenpreviously observed in the training dataset.
For example, a good neural clone detector should be able
to Ô¨Ånd clones of sort algorithms, even if the training datacontain no code fragments of sorting. This generalizability isa critical aspect to measure whether neural clone detectioncan be applied in practice at scale, because: (1) there area potentially inÔ¨Ånite number of functionalities in real-worldsoftware systems (especially for domain-speciÔ¨Åc softwaresystems), making it almost impossible to construct a large-
6172021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)
DOI 10.1109/ASE51524.2021.000612021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE) | 978-1-6654-0337-5/21/$31.00 ¬©2021 IEEE | DOI: 10.1109/ASE51524.2021.9678907
978-1-6654-0337-5/21/$31.00  ¬©2021  IEEE
scale training dataset that covers most functionalities; (2) it is
expensive and not scalable to annotate speciÔ¨Åc training datasetfor each domain.
The current evaluation methodology for neural clone detec-
tion does not systematically test the cross-functionality gener-alizability. Due to annotation difÔ¨Åculties, the benchmarks withmany semantic clones usually have a limited number of func-tionalities. For example, BigCloneBench, GCJ and OJClonehave 43/12/104 functionalities respectively. Training/test setsare randomly sampled from all annotated code fragment pairs,with the restriction that a code fragment should not appear inboth training set and test set. In this setting, whether a clonedetector can generalize to unseen code fragments or not iswell tested, but the cross-functionality generalizability is not.A reasonable concern is: do neural clone detectors really learnto model semantic equivalence of code fragment pairs, or theyjust simply remember Ô¨Åxed patterns of Ô¨Åxed functionalities?
In this paper, we aim to answer three questions:
1) How does neural clone detection generalize to unseen
functionalities?
2) If the cross-functionality generalizability of neural clone
detection is limited, what are the key factors that impact
it?
3) In a new domain, how to learn an accurate neural clone
detector with minimal cost?
Our Ô¨Årst contribution is a simple yet realistic evaluation
methodology for the generalizability of neural clone detec-tion and an empirical study based on it. Currently, annotated
code fragment pairs are divided into training/test sets basedon code fragments. This setting tests whether the detector cangeneralize to unseen code fragments, but not a complete testof the generalizability for unseen functionalities. To addressthis problem, we improve the evaluation methodology throughfunctionality-based re-partition of training/test sets. We thenuse this methodology to test the generalizability of neuralclone detection on OJClone dataset (which contains 104 func-tionalities). In particular, we divide these 104 functionalitiesinto 7 groups and run experiments on different training-testsettings. Our empirical observation is: neural clone detectionsuffers from signiÔ¨Åcant performance degradation (on average,F1 score decreases from 0.96 to 0.44) in cross-functionalitysettings.
This result motivates our second contribution: a series of
experiments are conducted to understand the factors that
impact the cross-functionality generalizability. SpeciÔ¨Åcally,
we examine 3 potential improvement directions:
1)Training Data Diversity. We deÔ¨Åne training data di-versity as the total number of functionalities in trainingset. We hyphothesize that the cross-functionality gen-eralizability can beneÔ¨Åt from increasing training datadiversity. Our empirical results conÔ¨Årm this hypothesiswhile signiÔ¨Åcant marginal effects are observed.
2)Vocabulary. Neural networks in software engineeringtasks usually suffer from the out-of-vocabulary (OOV)problem ([16], [24], [25]): tokens in test set may rarelyor even never occur in training set, thus these tokensare not effectively modeled in neural networks. Thevocabulary of different functionalities is likely to bevery different. Therefore, we hypothesize that adress-ing the OOV problem can alleviate the lack of cross-functionality generalizability. Our empirical results con-Ô¨Årm this hypothesis.
3)Locality. Common wisdom in machine learning com-munity suggests that we should use attention mechanismto better model local structures of data, especially fortheir latent alignments ([26]‚Äì[29]). We apply this ideato neural clone detection and study whether it canhelp improve cross-functionality generalizability. Ourempirical results show that this mechanism brings littleimprovement for generalizability.
Finally, our third contribution is a human-in-the-loop
mechanism that efÔ¨Åciently bootstraps neural clone detec-tors for unseen functionalities with only a small amountof human efforts. The scenario is: we have a training dataset
A, but we want to learn a good neural clone detector thatcan Ô¨Ånd semantic clones in a new, lower-resourced domainB(containing many functionalities that have never been
previously observed in A). The key point of this human-in-
the-loop mechanism is the combination of transfer learningand active learning: we learn a preliminary neural clonedetector on domain A, then use it to actively select informative
code fragment pairs in domain Bfor human annotation,
thus transferring the neural clone detector from domain Ato
domainB. Experimental results show that this human-in-the-
loop mechanism reduces the amount of annotations by 88%,thus alleviating the difÔ¨Åculty that neural clone detectors cannotwell extend to various real-world code repositories.
More broadly, these contributions may impact research
on neural source code representation (i.e., encoding codefragments to continuous vectors, based on neural networks),which has attracted much attention in recent years [30].Neural source code representation has been widely used andachieved impressive results not only in code clone detection,but also in various software engineering tasks (e.g., codecompletion ([24], [25], [31]‚Äì[35]), code search ([36]‚Äì[39]) ,code summarization ([40]‚Äì[48]), code translation ([49], [50]),and defect prediction ([51]‚Äì[53])). In this paper, we use codeclone detection as a case study to show the importance of(1) studying the dependence of supervised neural methods ontraining data; (2) probing whether these methods can gener-alize beyond training data. We suggest that a generalization-aware evaluation methodology should be used to better eval-uate neural methods in software engineering community, andmore future efforts should be made to improve generalizability.
II. B
ACKGROUND
A. Semantic Clones and Neural Clone Detection
Existing research work divides code clones into four major
types ([54], [55]):
‚Ä¢Type-1: Identical code fragments, except for differencesin white-space, layout and comments.
618‚Ä¢Type-2: Identical code fragments, except for differences
in identiÔ¨Åer names and literal values, as well as Type-1differences.
‚Ä¢Type-3: Syntactically similar code fragments that differ atthe statement level. The fragments have statements added,modiÔ¨Åed and/or removed with respect to each other, inaddition to Type-1 and Type-2 clone differences.
‚Ä¢Type-4: Syntactically dissimilar code fragments that im-plement the same functionality.
As there is no clear boundary between Type-3 clones and
Type-4 clones, we vaguely deÔ¨Åne semantic clones as the union
of Type-4 clones and Type-3 clones that cannot easily bedetected by pre-deÔ¨Åned rules.
Figure 1 shows an examlple of semantic clone (Type-4).
Even through the functionality of these two code fragmentsis very simple (calculates xraised to the power n), different
programmers may implement it in totally different ways.
In recent years, many neural network-based methods are
proposed for detecting such semantic clones, and they haveachieved impressive results (both precision and recall arehigher than 90%). Most of these neural clone detectors sharethe same model paradigm:
match(c,c
/prime)=F(Œ¶(c),Œ¶(c/prime)) (1)
wherecandc/primeare two code fragments, Œ¶is a learnable
neural network that encodes each code fragment as a vector,andFis a function that measures the semantic similarity
between two vectors. The code fragment pair (c,c
/prime)will be
regarded as a clone if and only if match(c,c/prime)is larger than
a threshold Œ¥.
Researchers usually deÔ¨Åne Fbased on cosine similarity,
Euclidean distance, or linear classiÔ¨Åcation. The key in theseneural clone detectors is the source code representation methodŒ¶. In CDLH [12], Œ¶is an AST-based LSTM network. In
ASTNN [15], Œ¶is an AST-based RNN network, in which
each large AST is split into a sequence of small statementtrees, thus alleviating the long-term dependency problem. InTBCCD [16], Œ¶is an AST-based convolution network. In
DeepSim [13], Œ¶is based on a matrix-based representation
which encodes code control Ô¨Çow and data Ô¨Çow.
B. Rethinking Semantic Clone Benchmarks
To evaluate the effectiveness of neural clone detectors, some
benchmarks that contain a large number of semantic clones
have been proposed and widely used.
As it is challenging for annotators to Ô¨Ånd in-the-wild seman-
tic clones from large-scale code repositories, these benchmarkswere usually created based on speciÔ¨Åc functionalities. Big-CloneBench [22] is a clone detection benchmark containing7,868,560 ground truth clones (98.23% of them are semanticclones), while all of them are clones of 43 speciÔ¨Åc func-tionalities. To create this benchmark, the researchers beganby selecting 43 commonly needed functionalities in open-source Java projects as target functionalities (e.g., Bubble Sort,
Web Download, and Decompress Zip). Code fragments (i.e.,1doublepower(double x,intn) {
2doubletemp;
3if(n==0) return1.00;
4temp=power(x, n/2);
5if(n%2==0)
6 returntemp*temp;
7else{
8 if(n>0) returnx*temp*temp;
9 else return (temp*temp)/x;
10}
11}
12
13doubleexponent(double x,intn) {
14if( n<0& &n! =INT_MIN) n--;
15doubleans = 1.0;
16for(int i=0 ;i<sizeof(int) *CHAR_BIT-1; i++) {
17 if( ( n&1 )ÀÜ( n<0& &n! =INT_MIN))
18 ans*=x ;
19 x=x*x;
20 n=n> >1 ;
21}
22if(n == INT_MIN) ans *=x ;
23returnn<0?1.0/ans : ans;
24}
Fig. 1: An example of semantic clone (Type-4).
functions) that might implement a target functionality were
identiÔ¨Åed using keywords and source code pattern heuristics,then these identiÔ¨Åed code fragments were manually tagged astrue or false positive of the target functionality by judges. Alltrue positive code fragments of a functionality form a largeclone group. OJClone [23] is a dataset that contains 104 func-tionalities. SpeciÔ¨Åcally, each functionality is a programmingquestion on OpenJudge
2, and there are 500 corresponding
solutions (submitted by students, written in C, passing alltest cases) for each functionality. Originally this dataset wascreated for program classiÔ¨Åcation, but researchers also widelyused it as a clone detection benchmark: two code fragments(i.e., solutions) are regarded as a ground truth clone if andonly if they are solutions of the same functionality. GCJ[13] is a benchmark similar to OJClone. It contains 1,669solutions (written in Java) for 12 different functionalities (i.e.,programming questions from Google Code Jam contests
3).
We carefully rethink the impact of speciÔ¨Åc functionalities on
the evaluation of neural clone detection. Our main concern isthat: neural networks may just learn how to classify these spe-ciÔ¨Åc functionalities, rather than how to detect semantic clones.This concern origins from the fact that neural clone detectorsrely on a large number of true/false clones for training. Thestandard evaluation methodology is to divide data into disjointtraining and test sets. However, the diversity of functionalitiesis limited in these benchmarks, leading to the result that: foreach code fragment in test set, there are always many codefragments in training set that have the same functionality asit. Therefore, though neural clone detectors achieved goodperformance in this experimental setup, a possible reason forthe good performance is that neural networks learn to representthese speciÔ¨Åc functionalities well.
2http://openjudge.cn/
3https://codingcompetitions.withgoogle.com/codejam
619This is not a true evaluation of neural clone detection, as
it is an essential need that clone detectors should Ô¨Ånd clones
of various functionalities, rather than just speciÔ¨Åc functional-ities that have been previously observed in training set. Thecurrent evaluation methodology cannot tell us about a neuralclone detector‚Äôs generalizability to handle code fragments ofunseen functionalities. Therefore, it is necessary to improvethe evaluation methodology and revisit existing neural clonedetectors based on it.
III. E
V ALUA TING GENERALIZABILITY
A. An Improved Evaluation Methodology
To evaluate the cross-functionality generalizability of neural
clone detection, we propose an improved evaluation method-ology. The intuition is simple yet effective: training/test setsin current benchmarks should be re-partitioned, with therestriction that no functionality is allowed to appear in bothof them.
Here we give the formalism description. Dis a dataset that
contains many code fragments: C={c
1,c2,...,c|C|}.Fis the
set of functionalities: F={f1,f2,...,f|F|}. Take OJClone
as an example: we have |F|= 104 and|C|= 104√ó500.
L:C‚ÜíFis a function that indicates the functionality of
each code fragment. For each dataset, Lis a known function
that is determined from the collection procedure of the dataset.
To obtain training/test sets for evaluating neural clone
detectors, we propose the following 3 steps:
1)Creating Functionality Groups. We divide functionali-ties (F ) intoKdisjoint groups as evenly as possible (K
is a hyper-parameter). We denote these groups as G
1,
G2, ...,GK.
2)Creating Training-Test Grid. For each 1‚â§i,j‚â§K,w e
set an experiment Ei,jin which neural clone detectors
are trained on functionalities in Giand tested on func-
tionalities in Gj. Therefore, we have K√óKexperiments
with different training-test functionality groups, and weform them as a grid. We deÔ¨Åne that an experiment isanunseen-functionality experiment,i f G
i/negationslash=Gj; other-
wise we deÔ¨Åne this experiment as a seen-functionality
experiment.
3)Sampling Code Fragment Pairs. For each function-ality group G
i(1‚â§i‚â§K), we randomly sam-
ple 3 disjoint sets of code fragment pairs from{(c
x,cy,I(c,c/prime))|c,c/prime‚ààC;L(c),L(c/prime)‚ààGi;c/negationslash=c/prime},
whereIis an indicator function:
I(c,c/prime)=/braceleftBigg
0L(c)/negationslash=L(c/prime)
1L(c)=L(c/prime)(2)
We denote these 3 sets as Ptrain
i ,Pdev
i, andPtest
i ,
respectively. For each experiment Ei,j(1‚â§i,j‚â§K),
neural clone detectors will be trained on Ptrain
i , vali-
dated on Pdev
j, and then tested on Ptest
j .
Neural clone detectors formulate semantic clone detection
as a binary-classiÔ¨Åcation task and use precision/recall/F1-scoreas evaluation metrics. Previous studies proved that existingTABLE I: Performance of two neural clone detectors whengeneralizing to unseen functionalities
Average F1 Score
G1G2G3G4G5G6G7 AV G
ASTNN
Seen 0.96 0.91 0.98 0.96 0.98 0.97 0.98 0.96
Unseen 0.39 0.33 0.49 0.43 0.50 0.37 0.45 0.42
TBCCD
Seen 0.98 0.92 0.97 0.97 0.98 0.95 0.98 0.96Unseen 0.42 0.40 0.56 0.53 0.53 0.51 0.43 0.48
‚ãÜAFR-rate(ASTNN) = 0.44, AFR-rate(TBCCD) = 0.50
:XGOTB:KYZ -------
-      
-      
-      
-      
-      
-      
-      
Fig. 2: F1 results of ASTNN trained on different functionality
groups (y-axis) and tested on different functionality groups (x-axis). Each cell is colored according to F1 score: the deepera cell is colored, the better the neural clone detector performsin the corresponding experiment setting (E
i,j).
neural clone detectors can achieve very high performanceonE
i,jifi=j. However, we cannot conclude that these
neural code detectors well learn how to Ô¨Ånd semantic clones,or they just learn how to classify speciÔ¨Åc functionalities inG
i. Therefore, we need to report P/R/F1 results for each
Ei,j(i/negationslash=j). As this may involve many result numbers, we
further introduce Average F1 Remaining Rate (AFR rate),
a new metric to summarily evaluate the cross-functionalitygeneralizability of neural clone detection:
AFR rate =/summationtext
i/negationslash=j
i,jF1(Ei,j)
(K‚àí1)¬∑/summationtext
iF1(Ei,i)(3)
The more AFR rate is higher than 0, the more it indicates
that the neural clone detector has cross-functionality general-
izability.
B. Experimental Setup
We conduct empirical experiments to evaluate the cross-
functionality generalizability of neural clone detection. We
choose two state-of-the-art neural clone detectors, ASTNN andTBCCD, as our evaluation objects.
We build our benchmark based on OJClone dataset. This
dataset has 104 functionalities, which is much more thanBigCloneBench (43 functionalities) and GCJ (12 functionali-ties). All these 104 OJClone functionalities are divided into7 groups (G
1,G2,G3, ...G7):G1contains functionality
620TABLE II: How does training data diversity impact generalizability
Average F1 Score AFR rate
G1G2G3G4G5G6G7 Average (w.r.t. baseline)
ASTNN
training data diversity = 15 0.32 0.30 0.52 0.48 0.55 0.46 0.43 0.44 0
training data diversity = 30 0.50 0.35 0.60 0.61 0.57 0.51 0.56 0.53 +0.09
training data diversity = 45 0.62 0.45 0.68 0.63 0.61 0.51 0.57 0.58 +0.15
training data diversity = 60 0.61 0.45 0.72 0.64 0.58 0.61 0.63 0.60 +0.17
training data diversity = 75 0.64 0.44 0.71 0.67 0.71 0.62 0.59 0.62 +0.19
training data diversity = 89 0.66 0.46 0.71 0.70 0.66 0.58 0.56 0.61 +0.18
TBCCD
training data diversity = 15 0.42 0.40 0.56 0.53 0.53 0.51 0.43 0.48 0
training data diversity = 30 0.51 0.47 0.61 0.63 0.64 0.53 0.58 0.57 +0.09
training data diversity = 45 0.61 0.49 0.63 0.63 0.64 0.60 0.63 0.60 +0.13
training data diversity = 60 0.64 0.53 0.67 0.58 0.59 0.54 0.65 0.60 +0.13
training data diversity = 75 0.62 0.49 0.65 0.65 0.63 0.66 0.65 0.62 +0.15
training data diversity = 89 0.63 0.52 0.68 0.69 0.66 0.65 0.61 0.63 +0.16
IDs 1-15, G2contains functionality IDs 16-30, and so on.
G7only contains 14 functionalities (IDs 91-104). For each
groupGi(1‚â§i‚â§7), we sample 30,000/10,000/10,000 code
fragment pairs as Ptrain
i /Pdev
i/Ptest
i .
C. Results and Observations
Table I shows the performance of two neural clone de-
tectors when generalizing to unseen functionalities. Columns
G1,G2,...,G 7represents different functionality groups for
test. We use Seen to denote that training data are collected
from the same functionality group as the test set, and weuse Unseen to denote that the training set share no common
functionality with the test set. For example, for ASTNN, wehave/summationtext
7
i=2F1(Ei,1)/6=0.39. Figure 2 shows detailed F1
results of ASTNN trained on different functionality groups
(y-axis) and tested on different functionality groups (x-axis).
Our observations are as follows:
‚Ä¢Good ability for modeling seen functionalities.F o reach experiment in which all test functionalities havebeen previously observed in training set (i.e., 7 exper-iments on the diagonal from top left to bottom right inFigure 2), ASTNN achieves very high performance. Allthe 7 experiments have F1 score higher than 0.9, and theaverage F1 score is 0.96. These results are consistent withthe results reported in the original ASTNN paper. How-ever, as discussed in Section II-B, we argue that resultson such experiment settings can only indicate that theneural clone detector‚Äôs ability to represent code fragmentsof seen functionalities, but not the true ability to detectsemantic clones that may involve unseen functionalities.
‚Ä¢Cannot well generalize to unseen functionalities. Fromexperiments outside the aforementioned diagonal (i.e.,E
i,jfor each 1‚â§i,j‚â§7andi/negationslash=j), we can observe
that ASTNN cannot generalize to unseen functionalitiesas expected. F1 scores of these experiments range from0.251 (E
6,2) to 0.609 ( E4,5). The average F1 score is
0.423. These results indicate that: the essence of thelearned models is likely to be program classiÔ¨Åcation,rather than clone detection. Therefore, it is difÔ¨Åcult to useASTNN in real-world code clone detection scenarios.In TBCCD, our observations are the same as those in
ASTNN.
Finding: To evaluate neural clone detectors, we need tominimize the functionality overlap between trainingset and test set, thus truly indicating the generalizabilityfor detecting real-world semantic clones.
The ideal way to minimize functionality overlap is to collect
semantic clones in the wild, rather than specifying severaltarget functionalities in advance (just as BigCloneBench,OJClone and GCJ do). However, this would be too costlyfor human annotation. Therefore, to evaluate neural clonedetectors more efÔ¨Åciently, we make a trade-off: we still needto specify several target functionalities in advance, but thetotal number of target functionalities should be as manyas possible (e.g., ‚â•100), and the training/test set should
be split based on functionalities. Previous researches usingrandom training/test splits suffer from serious ‚Äúfunctionalityleak‚Äù problem, resulting in models achieving almost perfectevaluation results exhibit poor real-world performance. Ourproposed evaluation methodology addresses this problem, thuscan better indicating the real performance of neural clonedetectors (though it is still not as solid as evaluating in thewild).
IV . K
EYFACTORS OF GENERALIZABILITY
In this section, we explore to understand key factors that
impact the cross-functionality generalizability of neural clonedetection. SpeciÔ¨Åcally, we mainly investigate 3 potential di-rections: (1) training data diversity, (2) vocabulary, and (3)locality.
A. Training Data Diversity
To investigate key factors of cross-functionality generaliz-
ability, one hypothesis is that:
H1. The cross-functionality generalizability of neural clone
detection can be improved through increasing training data
diversity.
Here we deÔ¨Åne training data diversity as the total number
of functionalities in training set.
621This hypothesis is proposed based on the fact that each of
our experiments in Section III-B uses only one functionality
group for training, i.e., training data diversity is 14 (G 7)o r1 5
(G1‚àíG6). A possible reason for the lack of cross-functionality
generalizability is that: neural clone detectors are likely todegenerate to program classiÔ¨Åers for speciÔ¨Åc functionalitieswhen they are trained on a dataset with small functionalitydiversity; this problem may be alleviated or addressed throughincreasing training data diversity.
It is essential to study this hypothesis: if cross-functionality
generalizability can be signiÔ¨Åcantly improved through in-creasing training data diversity, an important direction forfuture work is to improve the data collection methodology forbetter functionality diversity; otherwise, it indicates that wecannot equip existing neural clone detectors with true cross-functionality generalizability through collecting much moretraining data, thus future work should focus on improvingthese neural model architectures.
We conduct a series of experiments to study this hypothesis.
These experiments are set up based on the following steps:
1) Select a functionality group for test. Here we use G
1as
an example.
2) UseG2for training, that is, train a neural clone detector
(ASTNN/TBCCD) on P2
train and test it on P1
test. In this
experiment, the training data diversity is 15.
3) UseG2‚à™G3for training. Training data are sampled from
pairs of which code fragments are of functionalities inG
2‚à™G3, i.e., the training data diversity is 30. We use the
same sampling amount as P2
train (i.e., 30,000). We also
keep the same positive rate in training data (i.e., 1/15)t o
prevent suffer from the class imbalance problem causedby the growth of training data diversity.
4) UseG
2‚à™G3‚à™G4for training. The training data diversity
is 45.
5) UseG2‚à™G3‚à™G4‚à™G5for training. The training data
diversity is 60....
6) Draw results of the above 6 experiments as a line chart of
the inÔ¨Çuence of training data diversity on clone detectionperformance (F1 score).
We useG
1,G2,...,G 7for test respectively, thus we draw 7
lines in the line chart. Figure 3 shows the results of ASTNN.Our observations are as follows:
‚Ä¢Increasing training data diversity can signiÔ¨Åcantly im-prove the cross-functionality generalizability of neuralclone detection. For example, consider ASTNN G
1: the
F1 score is 0.320 when the training data diversity is 15;the F1 score will increase to 0.661 when the training datadiversity is 89. An increase of 0.341 is observed. ForG
1,G2,...,G 7, the increase ranges from 0.115 to 0.341,
and the average increase is 0.183.
‚Ä¢There are signiÔ¨Åcant marginal effects to improve neuralclone detection through increasing training data diversity.For example, consider ASTNN G
1: the F1 score increases
from 0.320 to 0.617 when the training data diversityFig. 3: InÔ¨Çuence of training data diversity on performanceof neural clone detection (ASTNN). We can observe that:increasing training data diversity can signiÔ¨Åcantly improve theperformance of neural clone detection, but there are signiÔ¨Åcantmarginal effects.
increases from 15 to 45, while the increase is only 0.044(from 0.617 to 0.661) when the training data diversityincreases from 45 to 89. For G
1,G2,...,G 7, the average
F1 increase for training data diversity 15‚Üí45is 0.146,
accounting for 79.8% of the increase for 15‚Üí89.
We regard these results as signiÔ¨Åcant marginal effects,which indicates that it is not likely to be sustainable thatimproving neural clone detection through continuouslyincreasing training data diversity.
Table II lists detailed results of both ASTNN and TBCCD.
In TBCCD, our observations are the same as those in ASTNN.
Finding: To train neural clone detectors, training set
with diverse functionalities can alleviate the problem
of lacking cross-functionality generalizability. However,due to the marginal effect, this is not a silver bullet tocompletely address this problem.
B. V ocabulary
Common wisdom suggests that vocabulary is a key factor
that may impact the effectiveness of neural networks, espe-cially for source code modeling ([16], [24], [25]). Neural clonedetectors need to represent tokens as numerical representationsso that the lexical information can be fed into the neural net-works. These tokens include reserved words in programminglanguages (e.g., ‚Äúif ‚Äù, ‚Äúint ‚Äù, and ‚Äúbreak ‚Äù), built-in functions and
data structures (e.g., ‚Äúabs‚Äù, ‚Äú+‚Äù, and ‚Äúvector ‚Äù), programmer-
deÔ¨Åned identiÔ¨Åers (e.g., ‚Äúx ‚Äù, ‚Äúy‚Äù, and ‚Äúmax
distance‚Äù), etc. In
general methods, a static vocabulary is extracted from trainingset (mainly based on token frequency), then all tokens whichare not in this vocabulary will be converted to a speciÔ¨Åctoken: ‚Äú< unknown> ‚Äù. This works well in natural language
processing, but may be problematic for source code. This ismainly because that programmers are free to create various
622TABLE III: How does vocabulary impact generalizability
Average F1 Score AFR rate
G1G2G3G4G5G6G7 Average (w.r.t. baseline)
ASTNN
test data average UNK rate = 13.7% 0.39 0.33 0.49 0.43 0.50 0.37 0.45 0.42 0
test data average UNK rate = 11.5% 0.41 0.34 0.51 0.47 0.51 0.37 0.44 0.44 +0.02
test data average UNK rate = 10.5% 0.42 0.36 0.58 0.52 0.52 0.38 0.45 0.46 +0.04
test data average UNK rate = 9.4% 0.46 0.39 0.64 0.58 0.55 0.42 0.50 0.51 +0.09
TBCCD
with PACE 0.42 0.40 0.56 0.53 0.53 0.51 0.43 0.48 0
w/o PACE 0.26 0.25 0.36 0.31 0.36 0.34 0.31 0.31 -0.18
tokens (especially variable names and function names), thus
aggravating the out-of-vocabulary (OOV) problem. That is,
in test set, a large amount of tokens will be converted to<unknown> , thus the lexical information they carry will be
lost. Therefore, an intuitive hypothesis is that:
H2. the cross-functionality generalizability of neural clone
detection can be improved through addressing the OOV prob-lem caused by vocabulary.
In TBCCD, Position-Aware Character Embedding (PACE),
a simple yet effective method for alleviating the OOV problemin source code, is proposed. Therefore, TBCCD does notsuffers from the OOV problem. The key point of PACEis to not treat each token as an individual building block,but a position-weighted combination of characters one-hotembeddings. That means for a token that has kcharacters
denoted as c
1,c2,...,ck, its embeddings can be obtained with
equation/summationtextk
i=1k‚àíi+1
k√óemb[ci], whereemb[ci]is the one-
hot embedding of ci. To summarize, PACE learns character
embeddings, then generates the embedding of each word by
assembling embeddings of every characters in this word.Therefore, PACE addresses the OOV problem, at the cost oflower capability of word-level semantics. The TBCCD paperreported that: though the effectiveness of PACE is marginalin random training/test splits, it can bring signiÔ¨Åcant gain incross-functionality splits.
We conduct an ablation experiment in which PACE is
replaced by a regular token embedding layer (i.e., TBCCD
w/o PACE in Table III) and the result shows that the cross-
functionality generalizability is signiÔ¨Åcantly reduced withoutPACE. This indicates that the OOV problem brought byvocabulary is likely to be a key factor of generalizability.ASTNN uses a regular token embedding layer (the vocabularyis deÔ¨Åned as top 3,000 frequent tokens in training set, andtoken embeddings are pre-trained using word2vec), thus it
may suffer from the OOV problem. When we apply PACEto ASTNN, we observe no improvement. This indicates thatPACE is not a universal solution to the OOV problem in allmodel architectures.
We speculate that the reason is: ASTNN requires a larger
capability of word-level semantics than TBCCD. TBCCD isa tree-based CNN model, which mainly captures programsemantics from AST structures (words are also important,yet secondary). Therefore, for TBCCD, addressing the OOVproblem at the cost of lower capability of word-level semanticswill do more good than harm. Unlike TBCCD, ASTNN isan RNN-based model, in which each code fragment are pre-processed as a sequence, rather than a tree. Therefore, word-level semantics plays a more important role in ASTNN thanin TBCCD. Though PACE can address the OOV problem, thisbeneÔ¨Åt is offset by its lower capability of word-level semantics.
We investigate the impact of vocabulary in ASTNN through
breaking down test sets according to the percentage of<unknown> tokens. SpeciÔ¨Åcally, we create test sets that
contain less <unknown> tokens than P
test
i(1‚â§i‚â§7). Our
assumption is: if ASTNN performs better in test sets with less<unknown> tokens, it means that ASTNN can beneÔ¨Åt from
reducing <unknown> tokens, thereby indicating that the OOV
problem is a key factor that can impact cross-functionalitygeneralizability.
For each experiment E
i,j, we re-sample the test set Ptest
j
as follows. For each functionality f‚ààGj, we sort all code
fragments of fby the percentage of <unknown> tokens in
ascending order. We keep the top 80%/60%/40% of these codefragments. Then, test sets are created from these code frag-ments. In P
test
i(1‚â§i‚â§7), the average UNK rate is 13.7%;
In test sets created from 80%/60%/40% code fragments, theaverage UNK rate is 11.5%/10.5%/9.4%. Therefore, we denotethese test settings as ‚ÄúASTNN, tested data average UNK rate= 13.7%/11.5%/10.5%/9.4%‚Äù in Table III. We observe thatASTNN has better performance in test cases which less sufferfrom the OOV problem: on average, the F1 score of ‚ÄúASTNN,tested data average UNK rate = 9.4%‚Äù is 0.51, which is muchbetter than the baseline (0.42). This indicates that the problemof lacking cross-functionality generalizability is likely to bepartly alleviated by addressing the OOV problem.
Finding: The out-of-vocabulary problem is an im-
portant factor that limits the cross-functionality gener-alizability of neural clone detection. Character-level orsubword-level token embeddings (e.g., PACE) can helpalleviate this problem, but are not universal enough forvarious neural clone detectors.
C. Locality
We consider a consensus in machine learning community:
local structure inference between two objects is essential fordetermining the overall inference between these two objects([26]‚Äì[29]). For example, in natural language processing, if we
623TABLE IV: How does locality impact generalizability
Average F1 Score
G1G2G3G4G5G6G7 AV G
ASTNN 0.39 0.33 0.49 0.43 0.50 0.37 0.45 0.42
with locality 0.39 0.36 0.47 0.41 0.43 0.33 0.50 0.41
want to learn a neural network model to determine whether
two natural language sentences are semantically equivalent,this model needs to employ some forms of alignment toassociate the relevant local structures (e.g., words, phrases,and clauses) between two sentences.
In code clone detection task, the objects are code fragments,
and the local structures can be statements, code blocks, sub-ASTs, etc. Figure 4 shows an example of local structurealignment between code fragments. Intuitively, suppose thatcode fragment Ais semantically equivalent to code fragment
B, it is likely that some local structures in Acan be aligned
with some local structures in B. Local structure alignment is
more in line with human perception of code clone detection,thus preventing neural clone detectors degenerate to programclassiÔ¨Åers.
Based on this intuition, we hypothesize that:
H3. incorporating locality into model architecture can help
improve the cross-functionality generalizability of neural clone
detection.
Some recent research works of neural clone detection have
incorporate locality into their model architectures ([18], [19]),but it is not easy to adapt their codes to OJClone. Therefore, toinvestigate this hypothesis, we use ASTNN as the base modelarchitecture and add a locality component on top of it basedon common practices in machine learning community.
We brieÔ¨Çy summarize the workÔ¨Çow of ASTNN as follows:
1) A code fragment will be parsed into an AST, then the
AST will be split into a sequence of statement trees (ST-
trees, which are trees consisting of statement nodes asroots and corresponding AST nodes of the statements).
2) For each code fragment, all ST-trees in it are encoded
into individual vectors, denoted as e
1,...,et. Then,
ASTNN uses a Bidirectional Gated Recurrent Unit (Bi-GRU) network to obtain their contextual vectors, de-noted as h
1,...,ht.
3) Code fragment representation is computed by max pool-
ing of h1,...,ht, then whether a pair of code fragments
is a clone is determined by Euclidean distance betweentheir representations.
We incorporate locality into ASTNN through introducing
attention-based alignment between contextual vectors of ST-trees (h). Notice that: we do not propose a novel localitycomponent for improving neural clone detection; instead, thegoal of this part is to leverage a state-of-the-art localitycomponent [27] (of which the effectiveness has been wellproved in machine learning community) to study whetherlocality is a key factor for cross-functionality generalizability.
Suppose that we have two code fragments c={h
1,...,ht}
andc/prime={h/prime1,...,h/primet/prime}, we add a soft alignment layer to
Fig. 4: An example of local structure alignment betweencode fragments. Intuitively, suppose that code fragment Ais
semantically equivalent to code fragment B, it is likely that
some local structures (e.g., statements or blocks) in Acan be
aligned with some local structures in B.
ASTNN:
/tildewideh
i=t/prime/summationdisplay
j=1exp(hi¬∑h/primej)/summationtextt/prime
k=1exp(hi¬∑h/primek)h/prime
j,‚àÄi‚àà[1,...,t] (4)
Intuitively, in Equation 4, the content in {h/prime
j}t/prime
j=1that is
relevant to hiwill be selected and represented as /tildewidehi. The
same is performed for each ST-tree in c/prime.
Then, we use a Bi-GRU network to convert local alignment
information ( /tildewideh) to local alignment-aware contextual represen-
tations (ÀÜh):
mi=[hi;/tildewidehi;hi‚àí/tildewidehi;hi‚äó/tildewidehi] (5)
ÀÜh1,...,ÀÜht=Bi-GRU( m1,...,mt) (6)
Representation of code fragment cwill be computed by max
pooling of ÀÜh1,...,ÀÜht, and the remaining steps remain exactly
the same as ASTNN.
Table IV shows the experimental results. We disappointedly
Ô¨Ånd that the cross-functionality generalizability of neural clone
detection cannot beneÔ¨Åt from incorporating with the localitycomponent. We speculate that the reasons may be three-folds: (1) Code fragments that are semantically equivalent mayhave totally different syntactic structures (as Figure 1 shows),making neural networks difÔ¨Åcult to leverage local structurealignment information. (2) The OOV problem discussed inSection III also lead to lots of noisy alignments. (3) Thelocality component we use is more suitable for sequence data(i.e., natural language sentences) rather than tree data (i.e.,ASTs of code fragments), thus tree-based locality componentmay be a potential direction for improvement.
Finding: In neural clone detection, local structure align-
624Annotator Neural Clone Detectoractively provide
informative code fragment pairs
human annotation
for model transferringSource Domain Dataset
(Labeled)preliminary trainingTarget Domain Dataset
(Unlabeled)
candidate pool
Fig. 5: Human-in-the-loop mechanism for domain adaptation
of neural clone detection.
ment has NOT yet been well leveraged to improve cross-functionality generalization.
V. H
UMAN -IN-THE -LOOP FOR DOMAIN ADAPTA TION
A. Task: Domain Adaptation
From Section III and IV we can conclude that: neural clone
detectors cannot well generalize to unseen functionalities asexpected. Though we Ô¨Ånd that this problem can be alleviatedthrough addressing the OOV problem caused by vocabularyand increasing training data diversity, there is still a largeperformance gap (AFR rate is about 46%/50% for ASTNN/T-BCCD). Therefore, it is not a good idea to train existing neuralclone detectors on a common dataset and directly use them toÔ¨Ånd semantic clones in various code repositories that containlots of functionalities that have never been previously observedin the training set. Instead, for each individual code repository,if we want to Ô¨Ånd semantic clones effectively (with highprecision and recall), we have to annotate a speciÔ¨Åc trainingset for this code repository. This is expensive, thus limiting thescalability of neural clone detection in real-world scenarios.
We formulate this as a domain adaptation problem, that is,
how to adapt a model (i.e., neural clone detector) learned fromone domain (i.e., the training set) to a new domain (i.e., anew code repository containing lots of unseen functionalities).Here we deÔ¨Åne that two domains are different if they containdifferent functionalities.
B. Solution: Human-in-the-Loop
We propose a human-in-the-loop mechanism (see Figure 5)
to address this domain adaptation problem. The key point
is‚Äúa little annotation does a lot of good‚Äù, based on the
combination of transfer learning and active learning. Givena neural clone detector learned from a high-resource domain,our goal is to adapt it to a unlabeled target domain. To achievethis, our human-in-the-loop mechanism automatically exploresthe target domain and actively select most informative (ratherthan random) code fragment pairs that can help transfer theneural clone detector learned from source domain to the targetdomain. Human annotate these actively selected code fragmentpairs (whether this pair is a clone or not), then these annotateddata are leveraged as new training samples to update theAlgorithm 1: Human-in-the-Loop Mechanism
Input:Dsrc (annotated dataset of source domain),
Ctrg(code fragments of target domain),
Oracle (human annotators), M(active query
batch size), V(informative degree
measurement function)
Output: model (a neural clone detector for target
domain)
1model‚ÜêTrainModel (Dsrc)
2pool‚Üêcandidate code fragment pairs from Ctrg
3Dtrg‚Üê‚àÖ
4while budget not exhausted do
5queries‚ÜêtopMpairs inpool byV(model,pair )
6Dtrg‚ÜêDtrg‚à™Oracle(queries)
7model‚ÜêTrainModel (Dsrc‚à™Dtrg)
8end
neural clone detector. This mechanism helps domain adap-tation of neural clone detection through improving annotationefÔ¨Åciency: human just need to annotate a small collection ofsamples (which is much smaller than regular annotation) toachieve an accurate neural clone detector for the low-resourcetarget domain.
Algorithm 1 is an overall procedure of this human-in-the-
loop mechanism. It exploits both transfer learning and activelearning to improve annotation efÔ¨Åciency. In the following weexplain this algorithm from these two aspects.
1) Transfer Learning: The goal of transfer learning is to
leverage knowledge learned from source domain and apply itto target domain. In Algorithm 1, we train an neural clonedetector on source domain as our preliminary model (Line1). Once the oracle (i.e., human annotators) provides someannotated code fragment pairs in target domain, these data willbe used to update the model (Line 7). Therefore, the neuralclone detector will be transferred from source domain to targetdomain iteratively. There are two optional strategies for modelupdate: one is to iteratively Ô¨Åne-tune the model with newlyannotated data, and the other is to learn from scratch (i.e.,re-train the model on D
src‚à™Dtrg) in each iteration. In the
Ô¨Åne-tuning strategy, there are some hyper-parameters such aslearning rate and the number of Ô¨Åne-tuning epochs, and it isnot easy to Ô¨Ånd the best setting of these hyper-parameters foreach scenario (in different scenarios, the best hypher-parametersettings are usually different). Our preliminary experimentsshow that results of these two model update strategies arecomparable, thus we Ô¨Ånally choose the learning-from-scratchstrategy (Line 7) as it is much simpler than Ô¨Åne-tuning.
2) Active Learning: After training a preliminary model
from source domain, we start the active learning process basedon this model‚Äôs outputs.
The Ô¨Årst step of active learning is to create a pool consisting
of many candidate (unlabeled) code fragment pairs from targetdomain (Line 2). There are 3 optional strategies for creatingthis pool: it can be all possible code fragment pairs in the target
625(a) Performance when the source domain is G1and the target domain
isG3
(b) Performance when the target domain is G3, and the source domain
isG1‚à™G2‚à™G4‚à™G5‚à™G6‚à™G7
Fig. 6: Human-in-the-loop mechanism performance. The hor-
izontal dashed line is the performance of the model trainedon 30,000 annotated samples from target domain. Reachingthe horizontal dashed line means that our human-in-the-loopmechanism successfully transfer the neural clone detector(ASTNN) to the target domain.
domain, if the total number of code fragments in this domain
is not large; or we can just use a part of it through simplerandom-sampling; in real-world scenarios, considering thatclones are sparse in randomly sampled code fragment pairs, wecan also use some heuristics to Ô¨Ålter out code fragment pairsthat are unlikely to be clones before sampling (e.g., whethertwo code fragments have the same input/output data types).
After that, we actively select Mcode fragment pairs from
the pool (Line 5). Human annotators label whether each oftheseMpairs is a clone or not (Line 6), then these newly
annotated data are used for transfer learning (Line 7). Thisprocedure iterates until the budget for data annotation (usuallyvery limited) is exhausted.
The key in active learning is the informative degree mea-
surement function V. The actively selected pairs should behighly informative, thus they can help the model be trans-ferred to target domain efÔ¨Åciently. In this paper, we computeV(model,pair )based on model uncertainty [56]:
V(model,pair )=|1‚àímax
l‚àà{+,‚àí}P(l|model,pair )| (7)
P(+|model,pair )is the model‚Äôs estimated probability that
this pair is a clone, and we have P(‚àí|model,pair )=
1‚àíP(+|model,pair ). Intuitively, V(model,pair )can be
regarded as the uncertainty of the model given the pair. Thehigher the uncertainty is, the more it means that this pair islikely to help improve the model. Therefore, in each activelearning iteration, we select candidate code fragment pairs withtopMuncertainty for human annotation.
Besides these uncertainty-based methods, many methods
have been proposed in the literature to better Ô¨Ånd informativesamples for active learning. For example, some methods arebased on representative [57], and some others are based ondiversity [58]. In this paper, we think our uncertainty-basedmethod is simple and effective enough, thus we leave theexploration of other sample selecting methods to future work.
C. Simulation Experiments
We conduct simulation experiments to evaluate the effec-
tiveness of our human-in-the-loop mechanism.
1) Setup: We use ASTNN as the model architecture. Eval-
uation benchmark is created based on OJClone. As discussed
in Section III-A, we have divided all the 104 functionalitiesin OJClone into 7 groups and denote them as G
1,G2,...,G 7.
For each group Gi(1‚â§i‚â§7), we have created trainning/de-
velopment/test sets (Ptrain
i/Pdev
i/Ptest
i ) for it. We select
one functionality group Gsrc as source domain and another
functionality group Gtrg as target domain for simulation
experiment. Following Algorithm 1, the ASTNN model will beinitially trained on P
train
src , then iteratively updated by samples
actively selected from Ptrain
trg (each iteration has Msamples),
and Ô¨Ånally validated/tested on Pdev
trg/Ptest
trg . To investigate the
impact of the hyper-parameter M (i.e., active query batch
size), we evaluate the human-in-the-loop mechanism withdifferent Mvalues: 100/500/1,000.
2) Results and Analysis: Figure 6 shows the performance
of our human-in-the-loop mechanism in one simulation ex-periment. In Figure 6a, we randomly select G
1as the source
domain and G3as the target domain. The ‚Äúrandom‚Äù curve
means that the samples for annotation (i.e., queries in Al-
gorithm 1) are randomly sampled from pool , rather than
actively selected by model . It is used as an ablation study
for proving the effectiveness of active learning. The horizontaldashed line is the performance of the model trained on 30,000annotated samples from target domain. If a curve reaches thishorizontal line when the x-axis value is X, it indicates that our
human-in-the-loop mechanism successfully transfer the neuralclone detector from source domain to target domain with X
annotated samples for target domain.
From Figure 6a, we can observe that:
6261) Our human-in-the-loop mechanism can signiÔ¨Åcantly re-
duce data annotation efforts. When M= 100/500,i t
requires about 3600 annotations to transfer the ASTNN
model to the target domain. Comparing to Ptrain
trg (which
has 30,000 annotated samples in total), we reduce theamount of annotations by about 88%.
2) Selecting informative candidate samples actively is im-
portant in this human-in-the-loop mechanism. Whensamples for annotation are randomly sampled (the ran-
dom curve) rather than actively selected (curves M=
100/500/1000), the performance signiÔ¨Åcantly drops.
We also conduct another simulation experiment to examine
how this human-in-the-loop mechanism performs on trainingdata with higher diversity. In this experiment, the targetdomain is G
3, and the source domain is G1‚à™G2‚à™G4‚à™
G5‚à™G6‚à™G7(training data diversity = 89). Figure 6b shows
the results. In Figure 6b, we have the same observations as inFigure 6a.
Finding: To adapt neural clone detection to real-worldlow-resourced domains, our human-in-the loop mecha-
nism can effectively reduce annotation efforts.
Note that this experiment is simulated, which means that we
already have these annotations, but only use some of them tosimulate the human annotation process. This approach allevi-ates the problem of lacking cross-functionality generalizability,but it is not a cure for the problem: it can just reduce theannotation cost, but still require users to label data in the targetdomains.
VI. T
HREA TS TO VALIDITY
We have identiÔ¨Åed the following main threats to validity:
‚Ä¢Programming languages. Our experiments are conducted
based on OJClone dataset, in which code fragmentsare written in C. There are some other clone detec-tion datasets (e.g., BigCloneBench and GCJ), which aremainly in Java language. The reasons why we onlyconduct experiments on OJClone is to prevent issues thatmay caused by the lack of functionality diversity (e.g.,GCJ dataset only has 12 functionalities) and data im-balance (e.g., in BigCloneBench, the functionality ‚ÄúCopyFile‚Äù accounts for 54.3% of clone pairs, and the top 10functionalities accounts for 91.7%). In principle, neuralclone detectors are designed based on generalized pro-gram structures (e.g., AST and PDG), rather than speciÔ¨Åcprogramming languages. Therefore, we can speculate thatour Ô¨Åndings in this paper can generalize to other program-ming languages. In the future, we will work to improveclone detection benchmarks in terms of functionalitydiversity and programming language diversity.
‚Ä¢Model evaluated. In this paper, we choose two state-of-
the-art neural clone detectors, ASTNN and TBCCD, asour evaluation objects. ASTNN is the representative ofRNN-based models, and TBCCD is the representativeof CNN-based models. Therefore, we think Ô¨Åndingsin this paper can generalize to other RNN/CNN-basedmodels, which accounts for a large part of previous work.However, we still need to further explore whether recentmethods based on GNNs (Graph Neural Networks) orPLMs (pretrained language models) have better cross-functionality generalizability or not. Though currently wedo not know whether our Ô¨Åndings got from RNN/CNN-based models can apply to GNN/PLM-based models, wesuggest that newly proposed neural clone detectors shouldbe evaluated based on a cross-functionality methodologyto alleviate threats to validity.
VII. C
ONCLUSION
In this work, we identify a key oversight in the current
evaluation methodology for neural clone detection: cross-functionality generalizability (i.e., the ability to detect se-mantic clones of which the functionalities have never beenpreviously observed in the training dataset). Our contributionsare 3-folds: (1) By proposing an evaluation methodology forcross-functionality generalizability, we conduct experimentson two state-of-the-art neural clone detectors, and Ô¨Ånd thatthey cannot well generalize to unseen functionalities as ex-pected. (2) To understand key factors that impact the cross-functionality generalizability, we conduct empirical analysison 3 factors (training data diversity, vocabulary, and locality),and Ô¨Ånd that the performance loss on unseen functionalitiescan be reduced through increasing training data diversityand addressing the out-of-vocabulary problem. (3) To adaptneural clone detectors to new code repositories containinglots of unseen functionalities, we propose a human-in-the-loopmechanism that helps reduce the amount of annotations byabout 88%.
Our analysis has clear implications for future work: (1)
New neural code clone detectors should be evaluated onfunctionality-based data splits to ensure that they can general-ize to real-world scenarios. (2) Based on Ô¨Åndings in this paper,future research directions for improving cross-functionalitygeneralizability include: addressing the OOV problem; ex-ploring better model architectures that have the capability tobeneÔ¨Åt from more diverse training data; exploring unsuper-vised or semi-supervised methods that can exploit large-scaleunlabeled code fragments to improve neural clone detection.(3) Human-in-the-loop is an efÔ¨Åcient way to adapt neural clonedetection to low-resource real-world code repositories, and apotential research direction is to explore better algorithms forselecting informative code fragment pairs. (4) More broadly,these implications are not limited to neural clone detection,but also various neural source code representation methods.
A
CKNOWLEDGMENTS
The work was supported by the National Key Research and
Development Program of China (No. 2019YFB1704003), theNational Nature Science Foundation of China (No. 71690231and No. 62021002), Tsinghua BNRist and Beijing Key Labo-ratory of Industrial Bigdata System and Application.
627REFERENCES
[1] P . Weissgerber and S. Diehl, ‚ÄúIdentifying refactorings from source-code
changes,‚Äù in 21st IEEE/ACM international conference on automated
software engineering (ASE‚Äô06). IEEE, 2006, pp. 231‚Äì240.
[2] S. Kawaguchi, T. Yamashina, H. Uwano, K. Fushida, Y . Kamei,
M. Nagura, and H. Iida, ‚ÄúShinobi: A tool for automatic code clone
detection in the ide,‚Äù in 2009 16th Working Conference on Reverse
Engineering. IEEE, 2009, pp. 313‚Äì314.
[3] Y . Dang, D. Zhang, S. Ge, C. Chu, Y . Qiu, and T. Xie, ‚ÄúXiao: tuning
code clones at hands of engineers in practice,‚Äù in Proceedings of the 28th
Annual Computer Security Applications Conference, 2012, pp. 369‚Äì378.
[4] N. A. Milea, L. Jiang, and S.-C. Khoo, ‚ÄúV ector abstraction and con-
cretization for scalable detection of refactorings,‚Äù in Proceedings of
the 22nd ACM SIGSOFT International Symposium on Foundations ofSoftware Engineering, 2014, pp. 86‚Äì97.
[5] E. Juergens, F. Deissenboeck, B. Hummel, and S. Wagner, ‚ÄúDo code
clones matter?‚Äù in 2009 IEEE 31st International Conference on Software
Engineering. IEEE, 2009, pp. 485‚Äì495.
[6] E. Juergens, F. Deissenboeck, and B. Hummel, ‚ÄúClonedetective-a work-
bench for clone detection research,‚Äù in 2009 IEEE 31st International
Conference on Software Engineering. IEEE, 2009, pp. 603‚Äì606.
[7] E. Juergens, F. Deissenboeck, M. Feilkas, B. Hummel, B. Schaetz,
S. Wagner, C. Domann, and J. Streit, ‚ÄúCan clone detection supportquality assessments of requirements speciÔ¨Åcations?‚Äù in Proceedings of
the 32nd ACM/IEEE International Conference on Software Engineering-V olume 2, 2010, pp. 79‚Äì88.
[8] J. Doe, ‚ÄúRecommended practice for software requirements speciÔ¨Åcations
(ieee),‚Äù IEEE, New York, 2011.
[9] C. K. Roy, J. R. Cordy, and R. Koschke, ‚ÄúComparison and evaluation
of code clone detection techniques and tools: A qualitative approach,‚ÄùScience of computer programming, vol. 74, no. 7, pp. 470‚Äì495, 2009.
[10] B. Hummel, E. Juergens, L. Heinemann, and M. Conradt, ‚ÄúIndex-based
code clone detection: incremental, distributed, scalable,‚Äù in 2010 IEEE
International Conference on Software Maintenance. IEEE, 2010, pp.1‚Äì9.
[11] M. Rieger, ‚ÄúEffective clone detection without language barriers,‚Äù Ph.D.
dissertation, V erlag nicht ermittelbar, 2005.
[12] H. Wei and M. Li, ‚ÄúSupervised deep features for software functional
clone detection by exploiting lexical and syntactical information insource code.‚Äù in IJCAI, 2017, pp. 3034‚Äì3040.
[13] G. Zhao and J. Huang, ‚ÄúDeepsim: deep learning code functional similar-
ity,‚Äù in Proceedings of the 2018 26th ACM Joint Meeting on European
Software Engineering Conference and Symposium on the Foundationsof Software Engineering, 2018, pp. 141‚Äì151.
[14] H. Wei and M. Li, ‚ÄúPositive and unlabeled learning for detecting
software functional clones with adversarial training.‚Äù in IJCAI, 2018,
pp. 2840‚Äì2846.
[15] J. Zhang, X. Wang, H. Zhang, H. Sun, K. Wang, and X. Liu, ‚ÄúA novel
neural source code representation based on abstract syntax tree,‚Äù in2019 IEEE/ACM 41st International Conference on Software Engineering(ICSE). IEEE, 2019, pp. 783‚Äì794.
[16] H. Y u, W. Lam, L. Chen, G. Li, T. Xie, and Q. Wang, ‚ÄúNeural detection
of semantic code clones via tree-based convolution,‚Äù in 2019 IEEE/ACM
27th International Conference on Program Comprehension (ICPC).IEEE, 2019, pp. 70‚Äì80.
[17] Y .-Y . Zhang and M. Li, ‚ÄúFind me if you can: Deep software clone detec-
tion by exploiting the contest between the plagiarist and the detector,‚Äù inProceedings of the AAAI Conference on ArtiÔ¨Åcial Intelligence, vol. 33,2019, pp. 5813‚Äì5820.
[18] Y . Li, C. Gu, T. Dullien, O. Vinyals, and P . Kohli, ‚ÄúGraph matching
networks for learning the similarity of graph structured objects,‚ÄùinProceedings of the 36th International Conference on Machine
Learning, ICML 2019, 9-15 June 2019, Long Beach, California,USA, ser. Proceedings of Machine Learning Research, K. Chaudhuriand R. Salakhutdinov, Eds., vol. 97. PMLR, 2019, pp. 3835‚Äì3845.[Online]. Available: http://proceedings.mlr.press/v97/li19d.html
[19] W. Wang, G. Li, B. Ma, X. Xia, and Z. Jin, ‚ÄúDetecting code clones with
graph neural network and Ô¨Çow-augmented abstract syntax tree,‚Äù in 2020
IEEE 27th International Conference on Software Analysis, Evolutionand Reengineering (SANER), 2020, pp. 261‚Äì271.
[20] J. Svajlenko and C. K. Roy, ‚ÄúEvaluating clone detection tools with
bigclonebench,‚Äù in 2015 IEEE International Conference on Software
Maintenance and Evolution (ICSME). IEEE, 2015, pp. 131‚Äì140.[21] H. Sajnani, V . Saini, J. Svajlenko, C. K. Roy, and C. V . Lopes,
‚ÄúSourcerercc: Scaling code clone detection to big-code,‚Äù in Proceedings
of the 38th International Conference on Software Engineering , 2016,
pp. 1157‚Äì1168.
[22] J. Svajlenko, J. F. Islam, I. Keivanloo, C. K. Roy, and M. M. Mia,
‚ÄúTowards a big data curated benchmark of inter-project code clones,‚Äùin2014 IEEE International Conference on Software Maintenance and
Evolution. IEEE, 2014, pp. 476‚Äì480.
[23] L. Mou, G. Li, L. Zhang, T. Wang, and Z. Jin, ‚ÄúConvolutional neural
networks over tree structures for programming language processing,‚Äù inProceedings of the Thirtieth AAAI Conference on ArtiÔ¨Åcial Intelligence,ser. AAAI‚Äô16. AAAI Press, 2016, p. 1287‚Äì1293.
[24] V . J. Hellendoorn and P . Devanbu, ‚ÄúAre deep neural networks the best
choice for modeling source code?‚Äù in Proceedings of the 2017 11th Joint
Meeting on Foundations of Software Engineering, 2017, pp. 763‚Äì773.
[25] R. M. Karampatsis, H. Babii, R. Robbes, C. Sutton, and A. Janes,
‚ÄúBig Code != Big V ocabulary: Open-V ocabulary Models for Sourcecode,‚Äù in Proceedings of the 42nd International Conference on
Software Engineering, ser. ICSE ‚Äô20. ACM, 2020. [Online]. Available:https://doi.org/10.1145/3377811.3380342
[26] A. Parikh, O. T ¬®ackstr ¬®om, D. Das, and J. Uszkoreit, ‚ÄúA
decomposable attention model for natural language inference,‚Äù inProceedings of the 2016 Conference on Empirical Methods inNatural Language Processing . Austin, Texas: Association for
Computational Linguistics, Nov. 2016, pp. 2249‚Äì2255. [Online].Available: https://www.aclweb.org/anthology/D16-1244
[27] Q. Chen, X. Zhu, Z. Ling, S. Wei, H. Jiang, and D. Inkpen, ‚ÄúEnhanced
lstm for natural language inference,‚Äù in Proceedings of the 55th Annual
Meeting of the Association for Computational Linguistics (ACL 2017).V ancouver: ACL, July 2017.
[28] C. Ciliberto, F. Bach, and A. Rudi, ‚ÄúLocalized structured prediction,‚Äù in
Advances in Neural Information Processing Systems , 2019, pp. 7301‚Äì
7311.
[29] T. Sylvain, L. Petrini, and D. Hjelm, ‚ÄúLocality and compositionality in
zero-shot learning,‚Äù in International Conference on Learning Represen-
tations, 2019.
[30] M. Allamanis, E. T. Barr, P . Devanbu, and C. Sutton, ‚ÄúA survey
of machine learning for big code and naturalness,‚Äù ACM Computing
Surveys (CSUR), vol. 51, no. 4, pp. 1‚Äì37, 2018.
[31] V . Raychev, M. V echev, and E. Yahav, ‚ÄúCode completion with statistical
language models,‚Äù in Proceedings of the 35th ACM SIGPLAN Confer-
ence on Programming Language Design and Implementation, 2014, pp.419‚Äì428.
[32] M. White, C. V endome, M. Linares-V ¬¥asquez, and D. Poshyvanyk,
‚ÄúToward deep learning software repositories,‚Äù in 2015 IEEE/ACM 12th
Working Conference on Mining Software Repositories. IEEE, 2015, pp.334‚Äì345.
[33] V . Raychev, P . Bielik, M. V echev, and A. Krause, ‚ÄúLearning programs
from noisy data,‚Äù ACM SIGPLAN Notices, vol. 51, no. 1, pp. 761‚Äì774,
2016.
[34] A. Bhoopchand, T. Rockt ¬®aschel, E. Barr, and S. Riedel, ‚ÄúLearning
python code suggestion with a sparse pointer network,‚Äù 2016. [Online].Available: http://arxiv.org/abs/1611.08307
[35] J. Li, Y . Wang, M. R. Lyu, and I. King, ‚ÄúCode completion with neural
attention and pointer networks,‚Äù in Proceedings of the 27th International
Joint Conference on ArtiÔ¨Åcial Intelligence, 2018, pp. 4159‚Äì25.
[36] X. Gu, H. Zhang, and S. Kim, ‚ÄúDeep code search,‚Äù in Proceedings of
the 2018 40th International Conference on Software Engineering (ICSE2018). ACM, 2018.
[37] Q. Chen and M. Zhou, ‚ÄúA neural framework for retrieval and summariza-
tion of source code,‚Äù in 2018 33rd IEEE/ACM International Conference
on Automated Software Engineering (ASE) . IEEE, 2018, pp. 826‚Äì831.
[38] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou, B. Qin,
T. Liu, D. Jiang et al., ‚ÄúCodebert: A pre-trained model for programming
and natural languages,‚Äù arXiv preprint arXiv:2002.08155, 2020.
[39] J. Cambronero, H. Li, S. Kim, K. Sen, and S. Chandra, ‚ÄúWhen deep
learning met code search,‚Äù in Proceedings of the 2019 27th ACM Joint
Meeting on European Software Engineering Conference and Symposiumon the Foundations of Software Engineering, 2019, pp. 964‚Äì974.
[40] M. Allamanis, H. Peng, and C. Sutton, ‚ÄúA convolutional attention
network for extreme summarization of source code,‚Äù in International
conference on machine learning, 2016, pp. 2091‚Äì2100.
[41] S. Iyer, I. Konstas, A. Cheung, and L. Zettlemoyer, ‚ÄúSummarizing source
code using a neural attention model,‚Äù in Proceedings of the 54th Annual
628Meeting of the Association for Computational Linguistics (V olume 1:
Long Papers), 2016, pp. 2073‚Äì2083.
[42] Y . Wan, Z. Zhao, M. Yang, G. Xu, H. Ying, J. Wu, and P . S. Y u,
‚ÄúImproving automatic source code summarization via deep reinforce-ment learning,‚Äù in Proceedings of the 33rd ACM/IEEE International
Conference on Automated Software Engineering, 2018, pp. 397‚Äì407.
[43] U. Alon, M. Zilberstein, O. Levy, and E. Yahav, ‚ÄúA general path-
based representation for predicting program properties,‚Äù ACM SIGPLAN
Notices, vol. 53, no. 4, pp. 404‚Äì419, 2018.
[44] U. Alon, S. Brody, O. Levy, and E. Yahav, ‚Äúcode2seq: Generating
sequences from structured representations of code,‚Äù in International
Conference on Learning Representations, 2018.
[45] X. Hu, G. Li, X. Xia, D. Lo, and Z. Jin, ‚ÄúDeep code comment generation
with hybrid lexical and syntactical information,‚Äù Empirical Software
Engineering, vol. 25, no. 3, pp. 2179‚Äì2217, 2020.
[46] U. Alon, M. Zilberstein, O. Levy, and E. Yahav, ‚Äúcode2vec: Learning
distributed representations of code,‚Äù Proceedings of the ACM on Pro-
gramming Languages, vol. 3, no. POPL, pp. 1‚Äì29, 2019.
[47] A. LeClair, S. Jiang, and C. McMillan, ‚ÄúA neural model for gener-
ating natural language summaries of program subroutines,‚Äù in 2019
IEEE/ACM 41st International Conference on Software Engineering(ICSE). IEEE, 2019, pp. 795‚Äì806.
[48] Y . Wang, L. Du, E. Shi, Y . Hu, S. Han, and D. Zhang, ‚ÄúCocogum:
Contextual code summarization with multi-relational gnn on umls,‚ÄùMicrosoft, Tech. Rep. MSR-TR-2020-16, May 2020. [Online]. Avail-able: https://www.microsoft.com/en-us/research/publication/cocogum-contextual-code-summarization-with-multi-relational-gnn-on-umls/
[49] X. Chen, C. Liu, and D. Song, ‚ÄúTree-to-tree neural networks for program
translation,‚Äù in Advances in neural information processing systems,
2018, pp. 2547‚Äì2557.
[50] M.-A. Lachaux, B. Roziere, L. Chanussot, and G. Lample, ‚ÄúUn-
supervised translation of programming languages,‚Äù arXiv preprint
arXiv:2006.03511, 2020.
[51] S. Wang, T. Liu, and L. Tan, ‚ÄúAutomatically learning semantic features
for defect prediction,‚Äù in 2016 IEEE/ACM 38th International Conference
on Software Engineering (ICSE). IEEE, 2016, pp. 297‚Äì308.
[52] M. Pradel and K. Sen, ‚ÄúDeepbugs: A learning approach to name-based
bug detection,‚Äù Proceedings of the ACM on Programming Languages,
vol. 2, no. OOPSLA, pp. 1‚Äì25, 2018.
[53] M. Allamanis, M. Brockschmidt, and M. Khademi, ‚ÄúLearning to rep-
resent programs with graphs,‚Äù in International Conference on Learning
Representations, 2018.
[54] S. Bellon, R. Koschke, G. Antoniol, J. Krinke, and E. Merlo, ‚ÄúCom-
parison and evaluation of clone detection tools,‚Äù IEEE Transactions on
software engineering, vol. 33, no. 9, pp. 577‚Äì591, 2007.
[55] C. K. Roy and J. R. Cordy, ‚ÄúA survey on software clone detection
research,‚Äù Queen‚Äôs School of Computing TR, vol. 541, no. 115, pp. 64‚Äì
68, 2007.
[56] A. Culotta and A. McCallum, ‚ÄúReducing labeling effort for structured
prediction tasks,‚Äù in AAAI, vol. 5, 2005, pp. 746‚Äì751.
[57] S. Dasgupta and D. Hsu, ‚ÄúHierarchical sampling for active learning,‚Äù in
Proceedings of the 25th international conference on Machine learning,2008, pp. 208‚Äì215.
[58] A. Kirsch, J. van Amersfoort, and Y . Gal, ‚ÄúBatchbald: EfÔ¨Åcient and
diverse batch acquisition for deep bayesian active learning,‚Äù 2019.
629