Early Life Cycle Software Defect Prediction.
Why? How?
N.C. Shrikanth, Suvodeep Majumder and Tim Menzies
Department of Computer Science, North Carolina State University, Raleigh, USA
snaraya7@ncsu.edu, smajumd3@ncsu.edu, timm@ieee.org
Abstract ‚ÄîMany researchers assume that, for software analyt-
ics, ‚Äúmore data is better.‚Äù We write to show that, at least for
learning defect predictors, this may not be true.
To demonstrate this, we analyzed hundreds of popular GitHub
projects. These projects ran for 84 months and contained
3,728 commits (median values). Across these projects, most of
the defects occur very early in their life cycle. Hence, defect
predictors learned from the Ô¨Årst 150 commits and four months
perform just as well as anything else. This means that, at least
for the projects studied here, after the Ô¨Årst few months, we need
not continually update our defect prediction models.
We hope these results inspire other researchers to adopt a
‚Äúsimplicity-Ô¨Årst‚Äù approach to their work. Some domains require
a complex and data-hungry analysis. But before assuming com-
plexity, it is prudent to check the raw data looking for ‚Äúshort
cuts‚Äù that can simplify the analysis.
Index Terms ‚Äîsampling, early, defect prediction, analytics
I. I NTRODUCTION
This paper proposes a data-lite method that Ô¨Ånds effective
software defect predictors using data just from the Ô¨Årst 4% of
a project‚Äôs lifetime. Our new method is recommended since
it means that we need not always revise defect prediction
models, even if new data arrives. This is important since
managers, educators, vendors, and researchers lose faith in
methods that are always changing their conclusions.
Our method is somewhat unusual since it takes an opposite
approach to data-hungry methods that (e.g.) use data collected
across many years of a software project [1], [2]. Such data-
hungry methods are often cited as the key to success for data
mining applications. For example, in his famous talk, ‚ÄúThe
Unreasonable Effectiveness of Data,‚Äù Google‚Äôs Chief Scientist
Peter Norvig argues that ‚Äúbillions of trivial data points can
lead to understanding‚Äù [3] (a claim he supports with numerous
examples from vision research).
But what if some Software Engineering (SE) data was not
like Norvig‚Äôs data? What if SE needs its own AI methods,
based on what we learned about the speciÔ¨Åcs of software
projects? If that were true, then data-hungry methods might be
needless over-elaborations of a fundamentally simpler process.
This paper shows that for one speciÔ¨Åc software analytics
task (learning defect predictors), we do not need a data-hungry
approach. We observe in Figure 1 that while the median
lifetime of many projects is 84 months, most of the defects
from those projects occur much earlier than that. That is, very
little of the defect experience occurs later in the life cycle.
Hence, predictors learned after 4 months (the vertical greendotted line in Figure 1) do just as well as anything else; i.e.
learning can stop after just 4% of the life cycle (i.e., 4/84
months). That is to say, when learning defect predictors:
96% of the time, we do not want and
we do not need data-hungry methods .
We stress that we have only shown an ‚Äúearly data is enough‚Äù
effect in the special case of (a) defect prediction for (b) long-
running non-trivial engineering GitHub projects studied here
(what Munaiah et al. [4] would call ‚Äúorganizational projects‚Äù).
Such projects can be readily identiÔ¨Åed by how many ‚Äústars‚Äù
(approval marks) they have accumulated from GitHub users.
Like other researchers (e.g., see the TSE‚Äô20 article by Yan et
al. [5]), we explore projects with at least 1000 stars.
That said, even within these restrictions, we believe we
are exploring an interesting range of projects. Our sample
includes numerous widely used applications developed by
Elastic (search-engine1), Google (core libraries2), Numpy (Sci-
entiÔ¨Åc computing3), etc. Also, our sample of projects is written
in widely used programming languages, including C, C++,
Java, C#, Ruby, Python, JavaScript, and PHP.
Nevertheless, in future work, we need to explore the ex-
ternal validity of our results to other SE tasks (other than
defect prediction) and for other kinds of data. For example,
Abdalkareem et al. [6] show that up to 16% of Python and
JavaScript packages are ‚Äútrivially small‚Äù (their terminology);
i.e., have less than 250 lines of code. It is an open issue if
our methods work for other kinds of software such as those
trivial Javascript and Python packages. To support such further
explorations, we have placed all our data, scripts on-line4.
The rest of this paper is structured as follows. In ¬ß2, we
discuss the negative consequences of excessive data collection,
then in ¬ß3 we show that for hundreds of GitHub [7] projects,
the defect data from the latter life cycle defect data is relatively
uninformative. This leads to the deÔ¨Ånition of experiments in
the early life cycle defect prediction (see ¬ß4,¬ß5). From those
experiments (in ¬ß6), we show that at least for defect prediction,
a small sample of data is useful, but (in contrast to Norvig‚Äôs
claim) more data is notmore useful. Lastly, ¬ß7 discusses some
threats, and conclusions are presented in ¬ß8.
1https://github.com/elastic/elasticsearch
2https://github.com/google/guava
3https://github.com/numpy/numpy
4For a replication package see: https://doi.org/10.5281/zenodo.4459561
4482021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)
1558-1225/21/$31.00 ¬©2021 IEEE
DOI 10.1109/ICSE43902.2021.00050
Fig. 1: 1.2 million commits for 155 GitHub projects. Black:Red (shaded) = Clean:Defective commits. In this paper, we compare
(a) models learned up to the vertical green (dotted) line to (b) models learned using more data.
II. B ACKGROUND
A. About Defect Prediction
Defect prediction uses data miners to input static code
attributes and output models that predict where the code
probably contains most bugs [8], [9]. Wan et al. [10] re-
ports that there is much industrial interest in these predictors
since they can guide the deployment of more expensive
and time-consuming quality assurance methods (e.g., human
inspection). Misirili et al [11] and Kim et al. [12] report
considerable cost savings when such predictors are used in
guiding industrial quality assurance processes. Also, Rahman
et al. [13] show that such predictors are competitive with more
elaborate approaches.
In defect prediction, data-hungry researchers assume that if
data is useful, then even more data is much more useful. For
example:
‚Äú..as long as it is large; the resulting prediction perfor-
mance is likely to be boosted more by the size of the
sample than it is hindered by any bias polarity that may
exist‚Äù [14].
‚ÄúIt is natural to think that a closer previous release
has more similar characteristics and thus can help to
train a more accurate defect prediction model. It is also
natural to think that accumulating multiple releases can
be beneÔ¨Åcial because it represents the variability of a
project‚Äù [15].
‚ÄúLong-term JIT models should be trained using a cache
of plenty of changes‚Äù [16].
Not only are researchers hungry for data, but they are
also most hungry for the most recent data. For example:
Hoang et al. say ‚ÄúWe assume that older commits changes
may have characteristics that no longer effects to the latest
commits‚Äù [17]. Also, it is common practice in defect prediction
to perform ‚Äúrecent validation‚Äù where predictors are tested on
the latest release after training from the prior one or two
releases [16], [18]‚Äì[20]. For a project with multiple releases,
recent validation ignores any insights that are available from
older releases.B. Problems with Defect Prediction: ‚ÄúConclusion Instability‚Äù
If we revise old models whenever new data becomes avail-
able, then this can lead to ‚Äúconclusion instability‚Äù (where
new data leads to different models). Conclusion instability
is well documented. Zimmermann et al. [21] learned defect
predictors from 622 pairs of projects (project1, project2).
In only 4% of pairs, predictors from project1 worked on
project2. Also, Menzies et al. [22] studied defect prediction
results from 28 recent studies, most of which offered widely
differing conclusions about what most inÔ¨Çuences software
defects. Menzies et al. [23] reported experiments where data
for software projects are clustered, and data mining is applied
to each cluster. They report that very different models are
learned from different parts of the data, even from the same
projects.
In our own past work, we have found conclusion instability,
meaning there we had to throw years of data. In one sample
of GitHub data, we sought to learn everything we could
from 700,000+ commits. The web slurping required for that
process took nearly 500 days of CPU (using Ô¨Åve machines
with 16 cores, over 7 days). Within that data space, we found
signiÔ¨Åcant differences in the models learned from different
parts of the data. So even after all that work, we were unable
to offer our business users a stable predictor for their domain.
Is that the best we can do? Are there general defect predic-
tion principles we can use to guide project management, soft-
ware standards, education, tool development, and legislation
about software? Or is SE some ‚Äúpatchwork quilt‚Äù of ideas and
methods where it only makes sense to reason about speciÔ¨Åc,
specialized, and small sets of related projects? Note that if
the software was a ‚Äúpatchwork‚Äù of ideas, then there would
be no stable conclusions about what constitutes best practice
for software engineering (since those best practices would
keep changing as we move from project to project). Such
conclusion instability would have detrimental implications for
trust, insight, training , and tool development .
Trust: Conclusion instability is unsettling for project man-
agers. Hassan [24] warns that managers lose trust in software
analytics if its results keep changing. Such instability prevents
project managers from offering clear guidelines on many is-
449sues, including (a) when a certain module should be inspected;
(b) when modules should be refactored; and (c) deciding where
to focus on expensive testing procedures.
Insight: Sawyer et al. assert that insights are essential
to catalyzing business initiative [25]. From Kim et al. [26]
perspective, software analytics is a way to obtain fruitful
insights that guide practitioners to accomplish software de-
velopment goals, whereas for Tan et al. [27] such insights
are a central goal. From a practitioner‚Äôs perspective Bird et
al. [28] report, insights occur when users respond to software
analytics models. Frequent model generation could exhaust
users‚Äô ability for conÔ¨Ådent conclusions from new data.
Tool development and Training: Shrikanth and Menzies [29]
warns that unstable models make it hard to onboard novice
software engineers. Without knowing what factors most inÔ¨Çu-
ence the local project, it is hard to design and build appropriate
tools for quality assurance activities
All these problems with trust, insight, training, and tool
development could be solved, if early on in the project, a
defect prediction model can be learned that is effective for the
rest of the life cycle. As mentioned in the introduction, we
study here GitHub projects spanning 84 months and containing
3,728 commits (median values). Within that data, we have
found that models learned after just 150 commits (and four
months of data collection), perform just as well as anything
else. In terms of resolving conclusion instability, this is a very
signiÔ¨Åcant result since it means that for 4=84 = 96% of the
life cycle, we can offer stable defect predictors.
One way to consider the impact of such early life cycle
predictors is to use the data of Figure 2. That plot shows that
software employees usually change projects every 52 months
(either moving between companies or changing projects within
an organization). This means that in seven years (84 months),
the majority of workers and managers would Ô¨Årst appear
on a job after the initial four months required to learn a
defect predictor. Hence, for most workers and managers, the
detectors learned via the methods of this paper would be
Fig. 2: Work duration histograms on particular projects;
from [30]. Data from: Facebook, eBay, Apple, 3M, Intel and
Motorola.the ‚Äúestablished wisdom‚Äù and ‚Äúthe way we do things here‚Äù
for their projects. This means that a detector learned in the
Ô¨Årst four months would be a suitable oracle to guide training
and hiring; the development of code review practices; the
automation of local ‚Äúbad smell detectors‚Äù; as well as tool
selection and development.
III. W HYEARLY DEFECT PREDICTION MIGHT WORK
A. GitHub Results
Recently (2020), Shrikanth and Menzies found defect-
prediction beliefs not supported by available evidence [29].
We looked for why such confusions exist ‚Äì which lead to
the discovery of that pattern in Figure 1 of project data
changes dramatically over the life-cycle. Figure 1 shows
data from 1.2m GitHub commits from 155 popular GitHub
projects (the criteria for selecting those particular projects is
detailed below). Note how the frequency defect data (shown
in red/shaded) starts collapsing early in the life cycle. This
observation suggests that it might be relatively uninformative
to learn from later life cycle data. This was an interesting
Ô¨Ånding since, as mentioned in the introduction, it is common
practice in defect prediction to perform ‚Äúrecent validation‚Äù
where predictors are tested on the latest release after training
from the prior one or two releases [16], [18], [19]. In terms
of Figure 1, that strategy would train on red dots (shaded)
taken near the right-hand-side, then test on the most right-
hand-side dot. Given the shallowness of the defect data in that
region, such recent validation could lead to results that are not
representative of the whole life cycle.
Accordingly, we sat out to determine how different training
and testing sampling policies across the life cycle of Figure 1
affected the results. After much experimentation (described
below), we assert that if data is collected up until the vertical
green line of Figure 1, then that generates a model as good as
anything else.
B. Related Work
Before moving on, we Ô¨Årst discuss related work on early life
cycle defect prediction. In 2008, Fenton et al. [62] explored
the use of human judgment (rather than data collected from
the domain) to handcraft a causal model to predict residual de-
fects (defects caught during independent testing or operational
usage) [62]. Fenton needed two years of expert interaction to
build models that compete with defect predictors learned by
data miners from domain data. Hence we do not explore those
methods here since they were very labor-intensive.
In 2010, Zhang and Wu showed that it is possible to estimate
the project quality with fewer programs sampled from an
entire space of programs (covering the entire project life-
cycle) [63]. Although we too draw fewer samples (commits),
we sample them ‚Äòearly‚Äô in the project life-cycle to build defect
prediction models. In another 2013 study about sample size,
Rahman et al. stress the importance of using a large sample
size to overcome bias in defect prediction models [14]. We
Ô¨Ånd our proposed ‚Äòdata-lite‚Äô approach performs similar to
‚Äòdata-hungry‚Äô approaches while we do not deny bias in defect
450TABLE I: Papers discussing different sampling policies. All (papers that utilize all historical data to build defect prediction
models, shaded in gray).
Paper Year Citations Sampling Projects Paper Year Citations Sampling Projects Paper Year Citations Sampling Projects
[31] 2008 172 All 12 [20] 2016 111 Release 17 [32] 2018 61 All 9
[33] 2010 21 All 5 [34] 2016 28 Release 10 [35] 2018 43 Percentage 101
[36] 2010 347 Percentage 10 [37] 2016 130 Release 10 [38] 2018 15 Release 13
[39] 2011 94 All 8 [40] 2017 44 All 6 [41] 2019 16 Percentage 7
[2] 2012 264 All 11 [16] 2017 36 Month 6 [42] 2019 14 Month 10
[1] 2012 387 All 5 [43] 2017 220 All 34 [44] 2019 11 Percentage 26
[45] 2013 322 All 10 [46] 2017 44 Percentage 255 [47] 2019 14 Slice 6
[48] 2014 105 All 1,403 [49] 2017 8 All 10 [50] 2019 1 All 10
[51] 2014 44 Release 1 [52] 2018 36 All 11 [53] 2019 0 Percentage 6
[13] 2014 93 Release 5 [54] 2018 66 Percentage [55] 2019 4 Release 9
[18] 2015 129 All 7 [56] 2018 33 All 18 [57] 2019 10 Release 20
[58] 2016 87 All 10 [59] 2018 25 All 16 [17] 2019 8 All, Slice 2
[60] 2016 42 Release 23 [61] 2018 40 All 6 [19] 2020 0 All 6
prediction data sets. Our proposed approach and recent defect
prediction work handle bias by balancing defective and non-
defective samples [32], [57] (class-imbalance).
Recently (2020), Arokiam and Jeremy [64] explored bug
severity prediction. They show it is possible to predict bug-
severity early in the project development by using data trans-
ferred from other projects [64]. Their analysis was on the
cross-projects, but unlike this paper, they did not explore
just how early in the life cycle did within project data
became effective. In similar work to Arokiam and Jeremy, in
2020, Sousuke [15] explored another early life cycle, Cross-
Version defect prediction (CVDP) using Cross-Project Defect
Prediction (CPDP) data. Their study was not as extensive
as ours (only 41 releases). CVDP uses the project‚Äôs prior
releases to build defect prediction models. Sousuke compared
defect prediction models trained using three within project
scenarios (recent project release, all past releases, and earliest
project release) to endorse recent project release. Sousuke
also combined CVDP scenarios using CPDP (24 approaches)
to recommend that the recent project release was still better
than most CPDP approaches. However, unlike Sousuke, we
offer contrary evidence in this work, as our endorsed policy
based on earlier commits works similar to all other prevalent
policies (including the most recent release) reported in the
literature. Notably, we assess our approach on 1000+ releases
and evaluate on seven performance measures.
In summary, as far as we can tell, ours is the Ô¨Årst study
to perform an extensive comparison of prevalent sampling
policies practiced in the defect prediction space.
IV. S AMPLING POLICES
One way to summarize this paper is to evaluate a novel
‚Äústop early‚Äù sampling policy for collecting the data needed for
defect prediction. This section describes a survey of sampling
policies in defect prediction. Each sampling policy has its way
of extracting training and test data from a project. As shown
below, there is a remarkably diverse number of policies in the
literature that have not been systematically and comparatively
evaluated prior to this paper.
In April 2020, we found 737 articles in Google Scholar
using the query (‚Äúsoftware‚Äù AND ‚Äúdefect prediction‚Äù AND
‚Äújust in time‚Äù, ‚Äúsoftware‚Äù AND ‚Äú defect prediction‚Äù AND
Fig. 3: Summary of sampling types from Table I.
‚Äúsampling policy‚Äù). ‚ÄúJust in time (JIT)‚Äù defect prediction is
a widely-used approach where the code seen in each commit
is assessed for its defect proneness [2], [18], [19], [65].
From the results of that query, we applied some temporal
Ô¨Åltering: (1) we examined all articles more recent than 2017;
(ii) for older articles, we examined all papers from the last 15
years with more than 10 citations per year. After reading the
title, abstracts, and the methodology sections, we found the 39
articles of Table I that argued for particular sampling policies.
Figure 3 shows a high-level view of the sampling policies
seen in the Table I papers:
All: When the historical data (commits/Ô¨Åles/modules etc)
is used for evaluation within some cross-validation study
(where the data is divided randomly into Nbins and the
data from bin ‚Äò i2N‚Äô is used to test a model trained
from all other data) [1].
Percentage : The historical data is stratiÔ¨Åed by some
percentage, like 80-20%. The minimum % we found was
67% [35].
Release : The models are trained on the immediate or
more past releases in order to predict defects on the
current release [57].
Month : When 3 or 6 months of historical data is used to
predict defects in future Ô¨Åles, commits, or release [16].
Slice : An arbitrary stratiÔ¨Åcation is used to divide the data
based on a speciÔ¨Åc number of days (like 180 days or six
months in [48]).
It turns out Figure 3 is only an approximation of the diverse
number sampling policies we see in the literature. A more
comprehensive picture is shown in Figure 4 where we divide
451Fig. 4: A visual map of sampling. Project time-line divided
into ‚ÄòTrain commits‚Äô and ‚ÄòTest commits‚Äô. Learners learn from
‚ÄòTrain‚Äô to classify defective commits in the ‚ÄòTest‚Äô.
software releases Rithat occur over many months Mjinto
some train andtestset.
Using a little engineering judgment and guided by the
frequency of different policies (from Figure 3), we elected
to focus on four sampling policies from the literature and one
‚Äòearly stopping‚Äô policy (see Table II). The % share in Figure 3
show ‚ÄòALL and RR‚Äô are prevalent practices whereas ‚ÄòM3 and
M6‚Äô though not prevalent are used in related literature [16],
[17]. We did not consider separate policies for ‚ÄòPercentage‚Äô
and ‚ÄòSlice‚Äô as the former is similar to ‚ÄòALL‚Äô (100% of
historical data), and the latter is least prevalent and similar
to M6 (180 days or six months).
Note the ‚Äúmagic numbers‚Äù in Table II:
3 months, 6 months : these are thresholds often seen in
the literature.
25 clean + 25 defective commits : We arrived at these
numbers based on the work of Nam et al. built defect
prediction models for using just 50 samples [43].
Sampling at random from the Ô¨Årst 150 commits. Here,
we did some experiments recursively dividing the data in
half until defect prediction stopped working.
We will show below that early sampling (shown in gray in
Table II) works just as well as the other policies.
V. M ETHODS
A. Data
This section describes the data used in this study as well as
what we mean by ‚Äúclean‚Äù and ‚Äúdefective‚Äù commits.
All our data comes from open source (OS) GitHub
projects [7] that we mined randomly using Commit Guru [66].
Fig. 5: Distributions seen in all 1.2 millions commits of all
155 projects: median values of commits (3,728), percent of
defective commits (20%), life span in years (7), releases (59)
and stars (9,149).
Commit Guru is a publicly available tool based on a 2015
ESEC/FSE paper used in numerous prior works [19], [67].
Commit Guru provides a portal where it takes a request (URL)
to process a GitHub repository. It extracts all commits and
their features to be exported to a Ô¨Åle. Commits are categorized
(based on the occurrence of certain keywords) similar to
the approach in SZZ algorithm [68]. The ‚Äúdefective‚Äù (bug-
inducing) commits are traced using the git diff (show changes
between commits) feature from bug Ô¨Åxing commits; the rest
are labeled ‚Äúclean‚Äù.
But data from Commit Guru does not contain release
information, which we extract separately from the project tags.
Then we use scripts to associate the commits to the release
dates. Then those codes associated with those changes were
then summarized by Commit Guru using the attributes of
Table III. Those attributes became the independent attributes
used in our analysis. Note that the use of these particular
attributes has been endorsed by prior studies [2], [69].
SE researchers have warned against using all GitHub data
since this website contains many projects that might be
categories as ‚Äúnon-serious‚Äù (such as homework assignments).
Accordingly, following the advice of prior researchers [4], [5],
we ignored projects with
Less than 1000 stars;
Less than 1% defects;
Less than two releases;
Less than one year of activity;
No license information.
Less than 5 defective and 5 clean commits.
This resulted in 155 projects developed written in many
languages across various domains, as discussed in ¬ßI.
Figure 5 shows information on our selected projects. As
shown in that Ô¨Ågure, our projects have:
Median life spans of 84 months with 59 releases;
The projects have (265, 3,728, 83,409) commits (min,
median, max) with data up to December 2019;
452TABLE II: Four representative sampling policies from literature and an early life cycle policy (the row shown in gray).
Policy Method
ALL Train using all past software commits ( [0; Ri)) in the project before the Ô¨Årst commit in the release under test Ri.
M6 Train using the recent six months of software commits ( [Ri 6months )) made before the Ô¨Årst commit in the release under test Ri.
M3 Train using the recent three months of software commits ( [Ri 3months )) made before the Ô¨Årst commit in the release under test Ri.
RR Train using the software commits in the previous release Ri 1before the Ô¨Årst commit in the release under test Ri.
E Train using early 50 commits (25 clean and 25 defective) randomly sampled within the Ô¨Årst 150 commits before the Ô¨Årst commit in the
release under test Ri.
TABLE III: 14 Commit level features that Commit Guru tool
[2], [66] mines from GitHub repositories
Dimension Feature DeÔ¨Ånition
NS Number of modiÔ¨Åed subsystems
ND Number of modiÔ¨Åed directories
NF Number of modiÔ¨Åed FilesDiffusion
ENTROPY Distribution of modiÔ¨Åed code across each Ô¨Åle
LA Lines of code added
LD Lines of code deleted Size
LT Lines of code in a Ô¨Åle before the change
Purpose FIX Whether the change is defect Changes that Ô¨Åxing
the defect are more likely to introduce more de-
fects Ô¨Åxing ?
NDEV Number of developers that changed the modiÔ¨Åed
Ô¨Åles
AGE The average time interval from the last to the
current change History
NUC Number of unique changes to the modiÔ¨Åed Ô¨Åles
before
EXP Developer experience
REXP Recent developer experience Experience
SEXP Developer experience on a subsystem
Fig. 6: Distributions seen in the Ô¨Årst 150 commits of all
155 projects; median values of project releases (5), project
development months (4) and defective commits (41)
20% (median) of project commits introduce bugs.
Figure 6 focuses on just the data used in the early life cycle
‚ÄúE‚Äù sampler described in Figure 4. In the median case, by
the time we can collect 150 commits, projects have had Ô¨Åve
releases in 4 months (median value).
B. Algorithms
Our study uses three sets of algorithms:
The Ô¨Åve sampling policies described above;
The six classiÔ¨Åers described here;
Pre-processing for some of the sampling policies.
1) ClassiÔ¨Åers: After an extensive analysis, Ghotra et al.
[70] rank over 30 defect prediction algorithms into four ranks.
For our work, we take six of their learners that are widely used
in the literature and which can be found at all four ranks of
the Ghtora et al., study. Those learners were:Logistic Regression (LR);
Nearest neighbour (KNN) (minimum 5 neighbors);
Decision Tree (DT);
Random Forrest (RF);
Na¬®ƒ±ve Bayes (NB);
Support Vector Machines (SVM)
2) Pre-processers: Following some advice from the liter-
ature, we applied some feature engineering to the Table III
data. We normalized LA and LD by dividing by LT and
normalized LT and NUC by dividing by NF; then we dropped
ND and REXP since they reported that NF and ND are highly
correlated with REXP and EXP [2], [19], [71]. Lastly, we
applied the logarithmic transform to the remaining process
measures (except the boolean variable ‚ÄòFIX‚Äô) to alleviate
skewness [72].
In other pre-processing steps, we applied Correlation-based
Feature Selection (CFS). Our initial experiments with this
data set lead to unpromising results (recalls less than 40%,
high false alarms). However, those results improved after we
applied feature subset selection to remove spurious attributes.
CFS is a widely applied feature subset selection method pro-
posed by Hall [73] and is recommended in building supervised
defect prediction models [44]. CFS is a heuristic-based method
to Ô¨Ånd (evaluate) a subset of features incrementally. CFS
performs a best-Ô¨Årst search to Ô¨Ånd inÔ¨Çuential sets of features
that are not correlated with each other, however, correlated
with the classiÔ¨Åcation. Each subset is computed as follows:
merits=krcf=p
k+k(k 1)rwhere:
meritsis the value of subset swithkfeatures;
rcfis a score that explains the connection of that feature
set to the class;
ris the feature to feature mean and connection between
the items in s, wherercfshould be large and r.
Another pre-processor that was applied to some sampling
policies was Synthetic Minority Over-Sampling , or SMOTE.
When the proportion of defective and clean commits (or
modules, Ô¨Åles, etc.) is not equal, learners can struggle to Ô¨Ånd
the target class. SMOTE, proposed by Chawla et al. [74] is
often applied in defect prediction literature to overcome this
problem [32], [54]. To achieve balance, SMOTE artiÔ¨Åcially
synthesizes examples (commits) extrapolating using K-nearest
neighbors (minimum Ô¨Åve commits required) in the data set
(training commits in our case) [74]. Note that:
We do not apply SMOTE to policies that already guaran-
tee class balancing. For example, our preferred early life-
cycle method selects at random 25 defective, and 25 non-
defective (clean) commits from the Ô¨Årst 150 commits.
453Also, just to document that we avoided a potential
methodological error [32], we record here that we applied
SMOTE to the training data, but never the test data.
C. Evaluation Criteria
Defect prediction studies evaluated their model performance
using a variety of criteria. From the literature, we used what
we judged to be the most widely-used measures [1], [2], [16],
[19], [31], [35], [45], [46], [48], [50], [54], [55], [57]. For the
following seven criteria:
Nearly all have the range 0 to 1 (except Initial number
of False Alarms, which can be any positive number);
Four of these criteria need to be minimized: D2H, IFA,
Brier, PF ; i.e., for these criteria lessisbetter .
Three of these criteria need to be maximized: AUC,
Recall, G-Measure ; i.e. for these criteria more isbetter .
One reason we avoid precision is that prior work shows this
measure has signiÔ¨Åcant issues for unbalanced data [31].
1) Brier: Recent defect prediction papers [16], [19], [35],
[54] measure the model performance using the Brier absolute
predictive accuracy measure. Let Cbe the total number
of the test commits. Let yibe 1 (for defective commits)
or 0 otherwise. Let ^yibe the probability of commit being
defective (calculated from the loss functions in scikit-learn
classiÔ¨Åers [75]). Then:
Brier =1
CCX
t=1(yi ^yi)2(1)
2) Initial number of False Alarms (IFA): Parnin and Orso
[76] say that developers lose faith in analytics if they see too
many initial false alarms. IFA is simply the number of false
alarms encountered after sorting the commits in the order of
probability of being detective, then counting the number of
false alarms before Ô¨Ånding the Ô¨Årst true alarm.
3) Recall: Recall is the proportion of inspected defective
commits among all the actual defective commits.
Recall =True Positives
True Positives +False Negatives(2)
4) False Positive Rate (PF): The proportion of predicted
defective commits those are not defective among all the
predicted defective commits.
PF=False Positives
False Positives +True Negatives(3)
5) Area Under the Receiver Operating Characteristic curve
(AUC): AUC is the area under the curve between the true
positive rate and false-positive rate.
6) Distance to Heaven (D2H): D2H or ‚Äúdistance to heaven‚Äù
aggregates on two metrics Recall and False Positive Rate (PF)
to show how close to ‚Äúheaven‚Äù (Recall=1 and PF=0) [77].
D2H =p
(1 Recall )2+ (0 PF)2
p
2(4)7) G-measure (GM): A harmonic mean between Recall and
the compliment of PF measured, as shown below.
G Measure =2Recall(1 PF)
Recall + (1 PF)(5)
Even though GM and D2H combined the same underlying
measures, we include both here since they both have been
used separately in the literature. Also, as shown below, it is
not necessarily true that achieving good results on GM means
that good results will also be achieved with D2H.
Due to the nature of the classiÔ¨Åcation process, some criteria
will always offer contradictory results:
A learner can achieve 100% recall just by declaring that
all examples belong to the target class. This method will
incur a high false alarm rate.
A learner can achieve 0% false alarms just by declaring
that no examples belong to the target class. This method
will incur a very low recall rate.
Similarly, Brier and Recall are also antithetical since
reducing the loss function also means missing some
conclusions and lowering recall.
D. Statistical Test
Later in ¬ßVI, we compare distributions of evaluation mea-
sures of various sampling policies that may have the same
median while their distribution could be very different. Hence
to identify signiÔ¨Åcant differences (rank) among two or more
populations, we use the Scott-Knott test recommended by
Mittas et al. in TSE‚Äô13 paper [78]. This test is a top-down bi-
clustering approach for ranking different treatments, sampling
policies in our case. This method sorts a list of lsampling
policy evaluations with lsmeasurements by their median
score. It then splits linto sub-lists m, n in order to maximize
the expected value of differences in the observed performances
before and after divisions.
For listsl;m;n of size ls;ms;nswherel=m[n, the
‚Äúbest‚Äù division maximizes E(); i.e. the difference in the
expected mean value before and after the spit:
E() =ms
lsabs(m: l:)2+ns
lsabs(n: l:)2
We also employ the conjunction of bootstrapping and A12
effect size test by Vargha and Delaney [79] to avoid ‚Äúsmall
effects‚Äù with statistically signiÔ¨Åcant results.
Important note: we apply our statistical methods separately
to all the evaluation criteria; i.e., when we compute ranks, we
do so for (say) false alarms separately to recall.
E. Experimental Rig
By deÔ¨Ånition, our different sampling policies have different
train and different test sets. But, methodologically, when we
compare these different policies, we have to compare results
on the same sets of releases. To handle this we:
First, run all our six policies, combined with all our
six learners. This offers multiple predictions to different
commits.
454Next, for each release, we divide the predictions into
those that come from the same learner:policy pair. These
divisions are then assessed with statistical methods de-
scribed above.
VI. R ESULTS
Tables IV and V show results when our six learners applied
our Ô¨Åve sampling policies. We plot these results into two tables
since our policies lead to results with different samples sizes:
the recent release, or ‚ÄúRR‚Äù, the policy uses data from just two
releases while ‚ÄúALL‚Äù uses everything.
In the Ô¨Årst row of those tables, ‚Äú+‚Äù and ‚Äú-‚Äù denote criteria
that need to be maximized or minimized, respectively. Within
the tables, gray cells show statistical test results (conducted
separately on each criterion). Anything ranked ‚Äúbest‚Äù is col-
ored gray, while all else have white backgrounds.
Columns one and two show the policy/learners that lead to
these results. Rows are sorted by how often policy/learners
‚Äúwin‚Äù; i.e., achieve best ranks across all criteria. In Tables IV
and V, no policy+learner wins 7 out of 7 times on all
criteria, so some judgment will be required to make an
overall conclusion. SpeciÔ¨Åcally, based on results from the
multi-objective optimization literature, we will Ô¨Årst remove
the policies+learners that score worse on most criteria, then
debate trade-offs between the rest.
To simplify that trade-off debate, we offer two notes. Firstly,
across all our learners, the median value for IFA is very small‚Äì
zero or one; i.e., developers using these tools only need to
suffer one false alarm or less before Ô¨Ånding something they
need to Ô¨Åx. Since these observed IFA scores are so small,
we say that ‚Äúlosing‚Äù on IFA is hardly a reason to dismiss a
learner/sampler combination. Secondly, D2H and GM combine
multiple criteria. For example, ‚Äúwinning‚Äù on D2H and GM
means performing well on both Recall and PF; i.e. these two
criteria are somewhat more informative than the others.
Turning now to those results, we explore two issues. For
defect prediction:
RQ1: Is more data, better?
RQ2: When is more recent data better than older data?
Note that we do explore a third research issue: are different
learners better at learning from a little, a lot, or all the
available data. Based on our results, we have nothing deÔ¨Ånitive
to offer on that issue. That said, if we were pressed to
recommend a particular learning algorithm, then we say there
are no counterexamples to the claim that ‚Äúit is useful to apply
CFS+LR‚Äù.
RQ1: Is more data, better?
Belief1: Our introduction included examples where propo-
nents of data-hungry methods advocated that if data is useful,
then even more data is much more useful.
Prediction:1 If that belief was the case, then in Table IV,
data-hungry sampling policies that used more data should
defeat ‚Äúdata-lite‚Äù sampling policies.
Observation1a: In Table IV, Our ‚Äúdata hungriest‚Äù sampling
policy (ALL) loses on most criteria. While it achieves thehighest Recall (83%), it also has the highest false alarm range
(40%). As to which other policy is preferred in the best wins=4
zone of Table IV, there is no clear winner. What we would
say here is that our preferred ‚Äúdata-lite‚Äù method called ‚ÄúE‚Äù
(that uses 25 defective and 25 non-defective commits selected
at random from the Ô¨Årst 150 commits) is competitive with the
rest. Hence:
Answer1a: For defect prediction, it is not clear that
more data is inherently better.
Observations1b: Figure 5 of this paper showed that within
our sample of projects, we have data lasting a median of
84 months. Figure 6 noted that by the time we get to 150
commits, most projects are 4 months old (median value). The
‚ÄúE‚Äù results of Table IV showed that defect models learned
from that 4 months of data are competitive with all the other
policies studied here. Hence we say,
Answer1b: 96% of the time, we do not want and we
do not need data-hungry methods
RQ2: When is more recent data better than older data?
Belief2: As discussed earlier in our introduction, many
researchers prefer using recent data over data from earlier
periods. For example, it is common practice in defect predic-
tion to perform ‚Äúrecent validation‚Äù where predictors are tested
on the latest release after training from the prior one or two
releases [16], [18]‚Äì[20]. For a project with multiple releases,
recent validation ignores all the insights that are available from
older releases.
Prediction2: If recent data is comparatively more informa-
tive than older data, then defect predictors built on recent data
should out-perform predictors built on much older data.
Observations2: We observe that:
Figure 5 of this paper showed that within our sample of
projects, we have data lasting a median of 84 months.
Figure 6 noted that by the time we get to 150 commits,
most projects are 4 months old (median value).
Table V says that ‚ÄúE‚Äù wins over ‚ÄúRR‚Äù since it falls in
the best wins=4 section.
Hence we could conclude that older data is more effective
than recent data.
That said, we feel somewhat more the circumspect conclusion
is in order. When we compare E+LR to the next learner in
that table (RR+NB) we only Ô¨Ånd a minimal difference in
their performance scores. Hence we make a somewhat humbler
conclusion:
Answer2: Recency based methods perform no better
than results from early life cycle defect predictors.
This is a startling result for two reasons. Firstly, compared
to the ‚ÄúRR‚Äù training data, the ‚ÄúE‚Äù training data is very old
indeed. For projects lasting 84 months long, ‚ÄúRR‚Äù is trained
455on information from recent few months, with ‚ÄúE‚Äù data comes
from years before that. Secondly, this result calls into question
any conclusion made in a paper that used recent validation to
assess their approach; e.g. [16], [18]‚Äì[20].
VII. T HREATS TO VALIDITY
A. Sampling Bias
The conclusion‚Äôs generalizability will depend upon the
samples considered; i.e., what matters here may not be true
everywhere. To improve our conclusion‚Äôs generalizability, wemined 155 long-running OS projects that are developed for
disparate domains and written in numerous programming lan-
guages. Sampling trivial projects (like homework assignments)
is a potential threat to our analysis. To mitigate that, we
adhered to the advice from prior researchers as discussed
earlier in ¬ßI and ¬ßV-A. We Ô¨Ånd our sample of projects have
20% (median) defects as shown in Figure 5 nearly the same
as data used by Tantithamthavorn et al. [35] who report 30%
(median) defects.
TABLE IV: 24 defect prediction models tested in all 4,876 applicable project releases. In the Ô¨Årst row ‚Äú+‚Äù and ‚Äú-‚Äù denote
the criteria that need to be maximized or minimized, respectively. ‚ÄòWins‚Äô is the frequency of the policy found in the top #1
Scott-Knott rank in each of the seven evaluation measures (the cells shaded in gray).
Policy ClassiÔ¨Åer Wins D2H- AUC+ IFA- Brier- Recall+ PF- GM+
M6 NB
40.37 0.67 1.0 0.32 0.78 0.33 0.67
M3 NB 0.37 0.67 1.0 0.31 0.76 0.32 0.68
E LR 0.36 0.68 1.0 0.32 0.71 0.31 0.68
M6 SVM 0.43 0.65 0.0 0.21 0.44 0.1 0.48
M3 SVM 0.43 0.65 0.0 0.21 0.43 0.1 0.48
ALL NB 3 0.4 0.65 1.0 0.36 0.83 0.40 0.67
E KNN
20.39 0.65 1.0 0.33 0.65 0.32 0.62
ALL LR 0.38 0.66 1.0 0.3 0.65 0.25 0.62
M6 LR 0.36 0.68 1.0 0.25 0.59 0.19 0.60
M3 LR 0.36 0.68 1.0 0.24 0.58 0.17 0.60
M6 KNN 0.4 0.65 0.0 0.23 0.50 0.14 0.53
ALL SVM 0.4 0.66 0.0 0.25 0.50 0.14 0.54
M3 KNN 0.41 0.65 0.0 0.23 0.47 0.13 0.51
M6 RF 0.44 0.63 0.0 0.24 0.43 0.12 0.47
E SVM
10.4 0.64 1.0 0.31 0.6 0.26 0.59
ALL KNN 0.38 0.66 1.0 0.25 0.55 0.17 0.57
ALL DT 0.42 0.62 1.0 0.32 0.52 0.25 0.54
M6 DT 0.43 0.62 1.0 0.29 0.5 0.2 0.51
ALL RF 0.42 0.64 1.0 0.26 0.49 0.15 0.51
M3 DT 0.43 0.62 1.0 0.28 0.48 0.19 0.5
M3 RF 0.44 0.63 1.0 0.24 0.42 0.11 0.46
E DT
00.46 0.58 1.0 0.38 0.57 0.35 0.54
E NB 0.54 0.54 1.0 0.37 0.55 0.29 0.41
E RF 0.44 0.61 1.0 0.33 0.52 0.26 0.52
KEY: More data (ALL,M6 and M3) Early (E)
TABLE V: 12 defect prediction models tested on 3,704 project releases. In the Ô¨Årst row ‚Äú+‚Äù and ‚Äú-‚Äù denote criteria that need
to be maximized or minimized, respectively. ‚ÄòWins‚Äô is the frequency of the policy found in the top #1 Scott-Knott rank in
each of the seven evaluation measures (the cells shaded in gray).
Policy ClassiÔ¨Åer Wins D2H- AUC+ IFA- Brier- Recall+ PF- GM+
E LR 4 0.36 0.68 1.0 0.32 0.71 0.31 0.68
RR NB
30.38 0.66 1.0 0.32 0.71 0.30 0.65
RR LR 0.35 0.68 1.0 0.24 0.59 0.18 0.61
RR SVM 0.42 0.64 0.0 0.23 0.47 0.12 0.5
E KNN
20.39 0.64 1.0 0.34 0.64 0.32 0.62
RR KNN 0.41 0.64 1.0 0.25 0.5 0.15 0.53
RR RF 0.43 0.63 1.0 0.24 0.43 0.13 0.48
E SVM 1 0.4 0.64 1.0 0.31 0.6 0.26 0.59
E DT
00.46 0.58 1.0 0.39 0.56 0.35 0.54
E NB 0.54 0.54 1.0 0.37 0.54 0.29 0.42
E RF 0.44 0.61 1.0 0.33 0.52 0.26 0.53
RR DT 0.42 0.62 1.0 0.28 0.50 0.20 0.51
KEY: Recency (RR) Early (E)
456B. Learner bias
Any single study can only explore a handful of classiÔ¨Åcation
algorithms. For building the defect predictors in this work, we
elected six learners (Logistic Regression, Nearest neighbor,
Decision Tree, Support Vector Machines, Random Forrest,
and Na ¬®ƒ±ve Bayes). These six learners represent a plethora of
classiÔ¨Åcation algorithms [70].
C. Evaluation bias
We use seven evaluation measures (Recall, PF, IFA, Brier,
GM, D2H, and AUC). Other prevalent measures in this defect
prediction space include precision. However, as mentioned
earlier, precision has issues with unbalanced data [31].
D. Input Bias
Our proposed sampling policy ‚ÄòE‚Äô randomly samples 50
commits from early 150 commits. Thus it may be true that
different executions could yield different results. However,
this is not a threat because each time, the early policy ‚ÄòE ‚Äô
randomly samples 50 commits from early 150 commits to test
sizeable 8,490 releases (from Table IV and Table V) across
all the six learners. In other words, our conclusions about ‚ÄòE‚Äô
hold on a large sample size of numerous releases.
VIII. C ONCLUSION
When data keep changing, the models we can learn from
that data may also change. If conclusions become too Ô¨Çuid
(i.e., change too often), then no one has a stable basis for
making decisions or communicating insights.
Issues with conclusion instability disappear if, early in the
life cycle, we can learn a predictive model that is effective for
the rest of the project. This paper has proposed a methodology
for assessing such early life cycle predictors.
1)DeÔ¨Åne a project selection criteria . For this paper, our
selection criteria are taken from related work (from
recent EMSE, TSE papers [4], [5]);
2)Select some software analytics task. For this paper, we
have explored learning defect predictors.
3)See how early projects selected by the criteria can
be modeled for that task. Here we found that defect
predictors learned from the Ô¨Årst four months of data
perform as well as anything else.
4)Conclude that projects matching criteria need more data
fortask before time found in step 3. In this paper, we
found that for 96% of the time, we neither want nor
need data-hungry defect prediction.
We stress that this result has only been shown here for defect
prediction and only for the data selected by our criteria.
As for future work, we have many suggestions:
The clear next step in this work is to check the validity
of this conclusion beyond the speciÔ¨Åc criteria andtask
explored here.
We need to revisit all prior results that used recent
validation to assess their approach; e.g. [16], [18]‚Äì[20]
since our RQ2 suggests they may have been working in
a relatively uninformative region of the data.While the performance scores of Tables IV and V are
reasonable, there is still much room for improvement.
Perhaps if we augmented early life cycle defect predictors
with a little transfer learning (from other projects [43]),
then we could generate better performing predictors.
Further to the last point, another interesting avenue of
future research might be hyper-parameter optimization
(HPO) [20], [80], [81]. HPO is often not applied in
software analytics due to its computational complexity.
Perhaps that complexity can be avoided by focusing only
on small samples of data from very early in the life cycle.
ACKNOWLEDGEMENTS
This work was partially supported by NSF grant #1908762.
REFERENCES
[1]M. D‚ÄôAmbros, M. Lanza, and R. Robbes, ‚ÄúEvaluating defect prediction
approaches: a benchmark and an extensive comparison,‚Äù Empirical
Software Engineering , vol. 17, no. 4-5, pp. 531‚Äì577, 2012.
[2]Y . Kamei, E. Shihab, B. Adams, A. E. Hassan, A. Mockus, A. Sinha,
and N. Ubayashi, ‚ÄúA large-scale empirical study of just-in-time quality
assurance,‚Äù IEEE Transactions on Software Engineering , vol. 39, no. 6,
pp. 757‚Äì773, 2012.
[3]P. Norvig. (2011) The Unreasonable Effectiveness of Data. Youtube.
[Online]. Available: https://www.youtube.com/watch?v=yvDCzhbjYWs
[4]N. Munaiah, S. Kroh, C. Cabrey, and M. Nagappan, ‚ÄúCurating github for
engineered software projects,‚Äù Empirical Software Engineering , vol. 22,
no. 6, pp. 3219‚Äì3253, 2017.
[5]M. Yan, X. Xia, Y . Fan, A. E. Hassan, D. Lo, and S. Li, ‚ÄúJust-in-time
defect identiÔ¨Åcation and localization: A two-phase framework,‚Äù IEEE
Transactions on Software Engineering , 2020.
[6]R. Abdalkareem, V . Oda, S. Mujahid, and E. Shihab, ‚ÄúOn the impact
of using trivial packages: an empirical case study on npm and pypi,‚Äù
Empirical Software Engineering , vol. 25, no. 2, pp. 1168‚Äì1204, Mar.
2020. [Online]. Available: https://doi.org/10.1007/s10664-019-09792-9
[7]‚ÄúGitHub Inc - provides hosting for software development version control
using git.‚Äù https://github.com/, accessed: 2019-03-18.
[8]T. J. Ostrand, E. J. Weyuker, and R. M. Bell, ‚ÄúPredicting the location
and number of faults in large software systems,‚Äù IEEE Transactions on
Software Engineering , vol. 31, no. 4, pp. 340‚Äì355, 2005.
[9]T. Menzies, J. Greenwald, and A. Frank, ‚ÄúData mining static code
attributes to learn defect predictors,‚Äù IEEE transactions on software
engineering , vol. 33, no. 1, pp. 2‚Äì13, 2006.
[10]Z. Wan, X. Xia, A. E. Hassan, D. Lo, J. Yin, and X. Yang, ‚ÄúPerceptions,
expectations, and challenges in defect prediction,‚Äù IEEE Transactions on
Software Engineering , 2018.
[11]A. T. Misirli, A. Bener, and R. Kale, ‚ÄúAi-based software defect predic-
tors: Applications and beneÔ¨Åts in a case study,‚Äù AI Magazine , vol. 32,
no. 2, pp. 57‚Äì68, 2011.
[12]M. Kim, D. Cai, and S. Kim, ‚ÄúAn empirical investigation into the role of
api-level refactorings during software evolution,‚Äù in Proceedings of the
33rd International Conference on Software Engineering . ACM, 2011,
pp. 151‚Äì160.
[13]F. Rahman, S. Khatri, E. T. Barr, and P. Devanbu, ‚ÄúComparing static
bug Ô¨Ånders and statistical prediction,‚Äù in Proceedings of the 36th
International Conference on Software Engineering , ser. ICSE 2014.
New York, NY , USA: Association for Computing Machinery, 2014, p.
424‚Äì434. [Online]. Available: https://doi.org/10.1145/2568225.2568269
[14]F. Rahman, D. Posnett, I. Herraiz, and P. Devanbu, ‚ÄúSample size vs.
bias in defect prediction,‚Äù in Proceedings of the 2013 9th joint meeting
on foundations of software engineering . ACM, 2013, pp. 147‚Äì157.
[15]S. Amasaki, ‚ÄúCross-version defect prediction: use historical data, cross-
project data, or both?‚Äù Empirical Software Engineering , pp. 1‚Äì23, 2020.
[16]S. McIntosh and Y . Kamei, ‚ÄúAre Ô¨Åx-inducing changes a moving target?
a longitudinal case study of just-in-time defect prediction,‚Äù IEEE Trans-
actions on Software Engineering , vol. 44, no. 5, pp. 412‚Äì428, 2017.
[17]T. Hoang, H. Khanh Dam, Y . Kamei, D. Lo, and N. Ubayashi, ‚ÄúDeepjit:
An end-to-end deep learning framework for just-in-time defect predic-
tion,‚Äù in 2019 IEEE/ACM 16th International Conference on Mining
Software Repositories (MSR) , 2019, pp. 34‚Äì45.
457[18]M. Tan, L. Tan, S. Dara, and C. Mayeux, ‚ÄúOnline defect prediction
for imbalanced data,‚Äù in 2015 IEEE/ACM 37th IEEE International
Conference on Software Engineering , vol. 2. IEEE, 2015, pp. 99‚Äì108.
[19]M. Kondo, D. M. German, O. Mizuno, and E.-H. Choi, ‚ÄúThe impact of
context metrics on just-in-time defect prediction,‚Äù Empirical Software
Engineering , vol. 25, no. 1, pp. 890‚Äì939, 2020.
[20]W. Fu, T. Menzies, and X. Shen, ‚ÄúTuning for software analytics: Is it
really necessary?‚Äù Information and Software Technology , vol. 76, pp.
135‚Äì146, 2016.
[21]T. Zimmermann, N. Nagappan, H. Gall, E. Giger, and B. Murphy,
‚ÄúCross-project defect prediction: a large scale experiment on data vs.
domain vs. process,‚Äù in Proceedings of the 7th joint meeting of the
European software engineering conference and the ACM SIGSOFT
symposium on The foundations of software engineering , 2009, pp. 91‚Äì
100.
[22]T. Menzies, A. Butcher, D. Cok, A. Marcus, L. Layman, F. Shull,
B. Turhan, and T. Zimmermann, ‚ÄúLocal versus global lessons for
defect prediction and effort estimation,‚Äù IEEE Transactions on software
engineering , vol. 39, no. 6, pp. 822‚Äì834, 2013.
[23]T. Menzies, A. Butcher, A. Marcus, T. Zimmermann, and D. Cok,
‚ÄúLocal vs. global models for effort estimation and defect prediction,‚Äù in
2011 26th IEEE/ACM International Conference on Automated Software
Engineering (ASE 2011) . IEEE, 2011, pp. 343‚Äì351.
[24]A. Hassan, ‚ÄúRemarks made during a presentation to the ucl crest open
workshop,‚Äù Mar. 2017.
[25]R. Sawyer, ‚ÄúBi‚Äôs impact on analyses and decision making depends
on the development of less complex applications,‚Äù in Principles and
Applications of Business Intelligence Research . IGI Global, 2013, pp.
83‚Äì95.
[26]M. Kim, T. Zimmermann, R. DeLine, and A. Begel, ‚ÄúThe emerging
role of data scientists on software development teams,‚Äù in Proceedings
of the 38th International Conference on Software Engineering , ser.
ICSE ‚Äô16. New York, NY , USA: ACM, 2016, pp. 96‚Äì107. [Online].
Available: http://doi.acm.org/10.1145/2884781.2884783
[27]S.-Y . Tan and T. Chan, ‚ÄúDeÔ¨Åning and conceptualizing actionable insight:
a conceptual framework for decision-centric analytics,‚Äù arXiv preprint
arXiv:1606.03510 , 2016.
[28]C. Bird, T. Menzies, and T. Zimmermann, The Art and Science of
Analyzing Software Data , 1st ed. San Francisco, CA, USA: Morgan
Kaufmann Publishers Inc., 2015.
[29]N. Shrikanth and T. Menzies, ‚ÄúAssessing practitioner beliefs about
software defect prediction,‚Äù in 2020 IEEE/ACM 42nd International
Conference on Software Engineering: Software Engineering in Practice
(ICSE-SEIP) . IEEE, 2020, pp. 182‚Äì190.
[30]A. Sela and H. Ben-Gal, ‚ÄúBig data analysis of employee turnover in
global media companies, google, facebook and others,‚Äù 12 2018, pp.
1‚Äì5.
[31]T. Menzies, B. Turhan, A. Bener, G. Gay, B. Cukic, and Y . Jiang,
‚ÄúImplications of ceiling effects in defect predictors,‚Äù in Proceedings
of the 4th international workshop on Predictor models in software
engineering , 2008, pp. 47‚Äì54.
[32]A. Agrawal and T. Menzies, ‚ÄúIs‚Äù‚Äù better data‚Äù‚Äù better than‚Äù‚Äù better
data miners‚Äù‚Äù?‚Äù in 2018 IEEE/ACM 40th International Conference on
Software Engineering (ICSE) . IEEE, 2018, pp. 1050‚Äì1061.
[33]H. Zhang, A. Nelson, and T. Menzies, ‚ÄúOn the value of learning from
defect dense components for software defect prediction,‚Äù in Proceedings
of the 6th International Conference on Predictive Models in Software
Engineering , 2010, pp. 1‚Äì9.
[34]W. Fu, V . Nair, and T. Menzies, ‚ÄúWhy is differential evolution bet-
ter than grid search for tuning defect predictors?‚Äù arXiv preprint
arXiv:1609.02613 , 2016.
[35]C. Tantithamthavorn, A. E. Hassan, and K. Matsumoto, ‚ÄúThe impact of
class rebalancing techniques on the performance and interpretation of
defect prediction models,‚Äù IEEE Transactions on Software Engineering ,
pp. 1‚Äì1, 2018.
[36]T. Menzies, Z. Milton, B. Turhan, B. Cukic, Y . Jiang, and A. Bener,
‚ÄúDefect prediction from static code features: current results, limitations,
new approaches,‚Äù Automated Software Engineering , vol. 17, no. 4, pp.
375‚Äì407, 2010.
[37]B. Ray, V . Hellendoorn, S. Godhane, Z. Tu, A. Bacchelli, and P. De-
vanbu, ‚ÄúOn the ‚Äùnaturalness‚Äù of buggy code,‚Äù in 2016 IEEE/ACM 38th
International Conference on Software Engineering (ICSE) , 2016, pp.
428‚Äì439.[38]L. Pascarella, F. Palomba, and A. Bacchelli, ‚ÄúRe-evaluating method-
level bug prediction,‚Äù in 2018 IEEE 25th International Conference on
Software Analysis, Evolution and Reengineering (SANER) , 2018, pp.
592‚Äì601.
[39]D. Romano and M. Pinzger, ‚ÄúUsing source code metrics to predict
change-prone java interfaces,‚Äù in 2011 27th IEEE International Con-
ference on Software Maintenance (ICSM) , 2011, pp. 303‚Äì312.
[40]Q. Huang, X. Xia, and D. Lo, ‚ÄúSupervised vs unsupervised models:
A holistic look at effort-aware just-in-time defect prediction,‚Äù in 2017
IEEE International Conference on Software Maintenance and Evolution
(ICSME) . IEEE, 2017, pp. 159‚Äì170.
[41]X. Chen, D. Zhang, Y . Zhao, Z. Cui, and C. Ni, ‚ÄúSoftware defect
number prediction: Unsupervised vs supervised methods,‚Äù Information
and Software Technology , vol. 106, pp. 161‚Äì181, 2019.
[42]L. Pascarella, F. Palomba, and A. Bacchelli, ‚ÄúFine-grained just-in-time
defect prediction,‚Äù Journal of Systems and Software , vol. 150, pp. 22‚Äì36,
2019.
[43]J. Nam, W. Fu, S. Kim, T. Menzies, and L. Tan, ‚ÄúHeterogeneous defect
prediction,‚Äù IEEE Transactions on Software Engineering , vol. 44, no. 9,
pp. 874‚Äì896, 2017.
[44]M. Kondo, C.-P. Bezemer, Y . Kamei, A. E. Hassan, and O. Mizuno, ‚ÄúThe
impact of feature reduction techniques on defect prediction models,‚Äù
Empirical Software Engineering , vol. 24, no. 4, pp. 1925‚Äì1963, 2019.
[45]S. Wang and X. Yao, ‚ÄúUsing class imbalance learning for software defect
prediction,‚Äù IEEE Transactions on Reliability , vol. 62, no. 2, pp. 434‚Äì
443, 2013.
[46]F. Zhang, A. E. Hassan, S. McIntosh, and Y . Zou, ‚ÄúThe use of
summation to aggregate software metrics hinders the performance of
defect prediction models,‚Äù IEEE Transactions on Software Engineering ,
vol. 43, no. 5, pp. 476‚Äì491, 2017.
[47]Q. Huang, X. Xia, and D. Lo, ‚ÄúRevisiting supervised and unsupervised
models for effort-aware just-in-time defect prediction,‚Äù Empirical Soft-
ware Engineering , vol. 24, no. 5, pp. 2823‚Äì2862, 2019.
[48]F. Zhang, A. Mockus, I. Keivanloo, and Y . Zou, ‚ÄúTowards building a
universal defect prediction model,‚Äù in Proceedings of the 11th Working
Conference on Mining Software Repositories , 2014, pp. 182‚Äì191.
[49]M. M. ¬®Ozt¬®urk, ‚ÄúWhich type of metrics are useful to deal with class
imbalance in software defect prediction?‚Äù Information and Software
Technology , vol. 92, pp. 17‚Äì29, 2017.
[50]M. Yan, X. Xia, D. Lo, A. E. Hassan, and S. Li, ‚ÄúCharacterizing and
identifying reverted commits,‚Äù Empirical Software Engineering , vol. 24,
no. 4, pp. 2171‚Äì2208, 2019.
[51]H. Lu, E. Kocaguneli, and B. Cukic, ‚ÄúDefect prediction between
software versions with active learning and dimensionality reduction,‚Äù
in2014 IEEE 25th International Symposium on Software Reliability
Engineering , 2014, pp. 312‚Äì322.
[52]H. K. Dam, T. Pham, S. W. Ng, T. Tran, J. Grundy, A. Ghose,
T. Kim, and C.-J. Kim, ‚ÄúLessons learned from using a deep tree-based
model for software defect prediction in practice,‚Äù in 2019 IEEE/ACM
16th International Conference on Mining Software Repositories (MSR) .
IEEE, 2019, pp. 46‚Äì57.
[53]X. Yang, H. Yu, G. Fan, K. Yang, and K. Shi, ‚ÄúAn empirical study on
progressive sampling for just-in-time software defect prediction,‚Äù 2019.
[54]C. Tantithamthavorn, S. McIntosh, A. E. Hassan, and K. Matsumoto,
‚ÄúThe impact of automated parameter optimization on defect prediction
models,‚Äù IEEE Transactions on Software Engineering , vol. 45, no. 7,
pp. 683‚Äì711, 2018.
[55]S. Yatish, J. Jiarpakdee, P. Thongtanunam, and C. Tantithamthavorn,
‚ÄúMining software defects: should we consider affected releases?‚Äù in
2019 IEEE/ACM 41st International Conference on Software Engineering
(ICSE) . IEEE, 2019, pp. 654‚Äì665.
[56]F. Wu, X.-Y . Jing, Y . Sun, J. Sun, L. Huang, F. Cui, and Y . Sun, ‚ÄúCross-
project and within-project semisupervised software defect prediction: A
uniÔ¨Åed approach,‚Äù IEEE Transactions on Reliability , vol. 67, no. 2, pp.
581‚Äì597, 2018.
[57]K. E. Bennin, J. W. Keung, and A. Monden, ‚ÄúOn the relative value of
data resampling approaches for software defect prediction,‚Äù Empirical
Software Engineering , vol. 24, no. 2, pp. 602‚Äì636, 2019.
[58]D. Ryu, O. Choi, and J. Baik, ‚ÄúValue-cognitive boosting with a support
vector machine for cross-project defect prediction,‚Äù Empirical Software
Engineering , vol. 21, no. 1, pp. 43‚Äì71, 2016.
[59]S. Wang, T. Liu, J. Nam, and L. Tan, ‚ÄúDeep semantic feature learning for
software defect prediction,‚Äù IEEE Transactions on Software Engineering ,
2018.
458[60]R. Krishna, T. Menzies, and W. Fu, ‚ÄúToo much automation? the bell-
wether effect and its implications for transfer learning,‚Äù in Proceedings
of the 31st IEEE/ACM International Conference on Automated Software
Engineering , 2016, pp. 122‚Äì131.
[61]X. Chen, Y . Zhao, Q. Wang, and Z. Yuan, ‚ÄúMulti: Multi-objective effort-
aware just-in-time software defect prediction,‚Äù Information and Software
Technology , vol. 93, pp. 1‚Äì13, 2018.
[62]N. Fenton, M. Neil, W. Marsh, P. Hearty, ≈Å. Radli ¬¥nski, and P. Krause,
‚ÄúOn the effectiveness of early life cycle defect prediction with bayesian
nets,‚Äù Empirical Software Engineering , vol. 13, no. 5, p. 499, 2008.
[63]H. Zhang and R. Wu, ‚ÄúSampling program quality,‚Äù in 2010 IEEE
International Conference on Software Maintenance . IEEE, 2010, pp.
1‚Äì10.
[64]J. Arokiam and J. S. Bradbury, ‚ÄúAutomatically predicting bug severity
early in the development process,‚Äù in Proceedings of the ACM/IEEE
42nd International Conference on Software Engineering: New Ideas and
Emerging Results , 2020, pp. 17‚Äì20.
[65]T. Fukushima, Y . Kamei, S. McIntosh, K. Yamashita, and N. Ubayashi,
‚ÄúAn empirical study of just-in-time defect prediction using cross-project
models,‚Äù in Proceedings of the 11th Working Conference on Mining
Software Repositories . ACM, 2014, pp. 172‚Äì181.
[66]C. Rosen, B. Grawi, and E. Shihab, ‚ÄúCommit guru: analytics and risk
prediction of software commits,‚Äù in Proceedings of the 2015 10th Joint
Meeting on Foundations of Software Engineering . ACM, 2015, pp.
966‚Äì969.
[67]X. Xia, E. Shihab, Y . Kamei, D. Lo, and X. Wang, ‚ÄúPredicting
crashing releases of mobile applications,‚Äù in Proceedings of the 10th
ACM/IEEE International Symposium on Empirical Software Engineering
and Measurement , 2016, pp. 1‚Äì10.
[68]J.¬¥Sliwerski, T. Zimmermann, and A. Zeller, ‚ÄúWhen do changes induce
Ô¨Åxes?‚Äù in Proceedings of the 2005 International Workshop on Mining
Software Repositories , ser. MSR ‚Äô05. New York, NY , USA: ACM,
2005, pp. 1‚Äì5. [Online]. Available: http://doi.acm.org/10.1145/1082983.
1083147
[69]F. Rahman and P. Devanbu, ‚ÄúHow, and why, process metrics are better,‚Äù
in2013 35th International Conference on Software Engineering (ICSE) .
IEEE, 2013, pp. 432‚Äì441.
[70]B. Ghotra, S. McIntosh, and A. E. Hassan, ‚ÄúRevisiting the impact
of classiÔ¨Åcation techniques on the performance of defect prediction
models,‚Äù in 37th ICSE-Volume 1 . IEEE Press, 2015, pp. 789‚Äì800.[71]N. Nagappan and T. Ball, ‚ÄúUse of relative code churn measures to
predict system defect density,‚Äù in Proceedings of the 27th international
conference on Software engineering . ACM, 2005, pp. 284‚Äì292.
[72]E. Shihab, Z. M. Jiang, W. M. Ibrahim, B. Adams, and A. E. Hassan,
‚ÄúUnderstanding the impact of code and process metrics on post-release
defects: a case study on the eclipse project,‚Äù in Proceedings of the 2010
ACM-IEEE International Symposium on Empirical Software Engineer-
ing and Measurement , 2010, pp. 1‚Äì10.
[73]M. A. Hall and G. Holmes, ‚ÄúBenchmarking attribute selection techniques
for discrete class data mining,‚Äù IEEE Transactions on Knowledge and
Data engineering , vol. 15, no. 6, pp. 1437‚Äì1447, 2003.
[74]N. V . Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer, ‚ÄúSmote:
synthetic minority over-sampling technique,‚Äù Journal of artiÔ¨Åcial intel-
ligence research , vol. 16, pp. 321‚Äì357, 2002.
[75]F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel, B. Thirion,
O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V . Dubourg, J. Vander-
plas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duch-
esnay, ‚ÄúScikit-learn: Machine learning in Python,‚Äù Journal of Machine
Learning Research , vol. 12, pp. 2825‚Äì2830, 2011.
[76]C. Parnin and A. Orso, ‚ÄúAre automated debugging techniques actually
helping programmers?‚Äù in Proceedings of the 2011 international sym-
posium on software testing and analysis . ACM, 2011, pp. 199‚Äì209.
[77]D. Chen, W. Fu, R. Krishna, and T. Menzies, ‚ÄúApplications of psycho-
logical science for actionable analytics,‚Äù FSE‚Äô19 , 2018.
[78]N. Mittas and L. Angelis, ‚ÄúRanking and clustering software cost
estimation models through a multiple comparisons algorithm,‚Äù IEEE
Trans SE , vol. 39, no. 4, pp. 537‚Äì551, Apr. 2013.
[79]A. Vargha and H. D. Delaney, ‚ÄúA critique and improvement of the cl
common language effect size statistics of mcgraw and wong,‚Äù Journal
of Educational and Behavioral Statistics , vol. 25, no. 2, pp. 101‚Äì132,
2000.
[80]C. Tantithamthavorn, S. McIntosh, A. E. Hassan, and K. Matsumoto,
‚ÄúAutomated parameter optimization of classiÔ¨Åcation techniques for
defect prediction models,‚Äù in ICSE 2016 . ACM, 2016, pp. 321‚Äì332.
[81]A. Agrawal, W. Fu, D. Chen, X. Shen, and T. Menzies, ‚ÄúHow to‚Äù
dodge‚Äù complex software analytics,‚Äù IEEE Transactions on Software
Engineering , 2019.
459