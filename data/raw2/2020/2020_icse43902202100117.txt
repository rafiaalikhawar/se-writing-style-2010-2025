Automatic Solution Summarization for Crash Bugs
Haoye Wang, Xin XiaykDavid Loz, John GrundyyXinyu Wangk
College of Computer Science and Technology, Zhejiang University
yFaculty of Information Technology, Monash University
zSchool of Information Systems, Singapore Management University
fwhy ,wangxinyug@zju.edu.cn,fxin.xia,john.grundy g@monash.edu, davidlo@smu.edu.sg
Abstract ‚ÄîThe causes of software crashes can be hidden
anywhere in the source code and development environment.
When encountering software crashes, recurring bugs that are
discussed on Q&A sites could provide developers with solutions
to their crashing problems. However, it is difÔ¨Åcult for developers
to accurately search for relevant content on search engines,
and developers have to spend a lot of manual effort to Ô¨Ånd
the right solution from the returned results. In this paper, we
present CRASOLVER , an approach that takes into account both
thestructural information of crash traces and the knowledge of
crash-causing bugs to automatically summarize solutions from
crash traces. Given a crash trace, CRASOLVER retrieves relevant
questions from Q&A sites by combining a proposed position
dependent similarity ‚Äì based on the structural information of
the crash trace ‚Äì with an extra knowledge similarity, based on
the knowledge from ofÔ¨Åcial documentation sites. After obtaining
the answers to these questions from the Q&A site, CRASOLVER
summarizes the Ô¨Ånal solution based on a multi-factor scoring
mechanism. To evaluate our approach, we built two repositories
of Java and Android exception-related questions from Stack
OverÔ¨Çow with size of 69,478 and 33,566 questions respectively.
Our user study results using 50 selected Java crash traces
and 50 selected Android crash traces show that our approach
signiÔ¨Åcantly outperforms four baselines in terms of relevance,
usefulness, and diversity. The evaluation also conÔ¨Årms the ef-
fectiveness of the relevant question retrieval component in our
approach for crash traces.
I. I NTRODUCTION
Software crashes are a serious software defect problem
and often requires developers to solve them with a high
priority. However, as the complexity of a software system
increases, the reasons for software crashes become ever more
complicated. Fortunately, many bugs are recurring, and occur
in different projects but are similar [1]. Previous research [2]
[3] reported that there are about 17-45% of total bugs that
can be considered as recurring. Therefore, the recurring bugs
which have already been discussed on Q&A sites, such as
Stack OverÔ¨Çow (SO), could potentially help developers solve
their own software crashes.
Most mainstream programming languages have their own
exception handling mechanism that produces crash traces ,
which can then be used for further investigation. Figure 1
shows an example of such a crash trace. We refer to red parts
ascrash reasons and blue parts as stack frames .
Typically, developers will organize their questions about a
crash into a query to the search engines. However, for a large
crash trace thrown by a program, it is often difÔ¨Åcult for a
kCorresponding authors.
java.lang.RuntimeException: Unexpected exceptionat com.google.appengine.tools.enhancer.Enhancer.execute(Enhancer.java:76)at com.google.appengine.tools.enhancer.Enhance.<init>(Enhance.java:71)at com.google.appengine.tools.enhancer.Enhance.main(Enhance.java:51)Caused by: java.lang.reflect.InvocationTargetExceptionat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)at java.lang.reflect.Method.invoke(Unknown Source)at com.google.appengine.tools.enhancer.Enhancer.execute(Enhancer.java:74)... 2 moreCaused by: java.lang.IllegalArgumentExceptionat org.objectweb.asm.ClassReader.<init>(Unknown Source)at org.objectweb.asm.ClassReader.<init>(Unknown Source)at org.objectweb.asm.ClassReader.<init>(Unknown Source)at org.datanucleus.enhancer.asm.ASMClassEnhancer.getClassNameForFileName(ASMClassEnhancer.java:272)at org.datanucleus.enhancer.DataNucleusEnhancer.getFileMetadataForInput(DataNucleusEnhancer.java:727)at org.datanucleus.enhancer.DataNucleusEnhancer.enhance(DataNucleusEnhancer.java:525)at org.datanucleus.enhancer.DataNucleusEnhancer.main(DataNucleusEnhancer.java:1258)... 7 morecrash reasons
stack framesFig. 1. An example of a crash trace.
developer to summarize this crash trace into a query. Due to
limitations of input length, search engines like Google and
Stack OverÔ¨Çow can not deal with the entire crash trace. In
general, developers will try to directly use the crash reasons ,
as shown in Figure 1, as search engine input, then read
the returned posts to Ô¨Ånd solutions to their software crash.
However, this method will miss a lot of important information
instack frames and the result may not be accurate enough.
To make matters worse, there is much noisy and redundant
information in the returned posts, and developers need to spend
a lot of time to digest and Ô¨Ånd the right solution.
An automated approach considering both crash reasons
andstack frames to provide solutions to a crash trace will
help developers solve crash bugs more effectively. Recently,
two approaches have been proposed to automatically generate
solutions for programming questions: AnswerBot [4] and
CROKAGE [5]. AnswerBot [4] generates a query-focused
multi-answer-posts summary for a given technical question.
It aims to help developers quickly capture the key points of
several answer posts relevant to a technical question before
they read the details of the posts. CROKAGE [5] takes the nat-
ural language description of a programming task and provides
a comprehensive solution for the task. CROKAGE suggests
programming solutions containing both code and explanations.
However, both the approaches are not designed for crash trace
analysis and require a natural language description of the
programming problem as input, which is hard for developers.
Several techniques use crash trace for bug localization [6]‚Äì
12862021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)
1558-1225/21/$31.00 ¬©2021 IEEE
DOI 10.1109/ICSE43902.2021.00117
[11], automated program repair [1] [12], and duplicate bug
report detection [13]‚Äì[16] (see Section VI for more detail).
However, to the best of our knowledge, automatic generation
ofexplanations that can guide a developer to Ô¨Åx the problem
causing a given crash trace has not yet been investigated.
Unlike the question answering task [17]‚Äì[19], a crash trace
may share no lexical units with an answer and the information
they convey may be totally different. To address this, we
utilize the fact that some Q&A posts discussing a crash bug
usually have a crash trace attached, which contains certain
information about the crash. We can retrieve related questions
by measuring the similarities between crash traces. However,
many modern programs depend on common packages and
thus different crash traces may contain lots of similar tokens.
Two different crash bugs may share many identical tokens in
different positions of their crash traces if the programs rely
on some common packages. Thus, traditional information re-
trieval methods, i.e., BM25 [20], TF-IDF based information re-
trieval [21], word-embedding based information retrieval [22],
and document-to-vector based information retrieval [23], will
bring bias and not work very well. The crash reasons and
the structural information of stack frames may be the key
points to improve our retrieval ability. Additionally, some crash
traces on Q&A sites may be incomplete, as the questioner only
pastes the crash reason and describes the crash in short. Only
using crash traces will not be able to retrieve all related Q&A
posts. If we can introduce some additional knowledge about
the crash bugs to retrieve, it will greatly improve our approach.
Unfortunately, it is easy for developers to have a concept what
a crash trace is about but it is difÔ¨Åcult for a program.
To address these we propose C RASOLVER (CrashSolver )
that takes a crash trace as input and then returns a solution
summary for the bug. Our approach takes into account both the
structural information of the crash traces and the knowledge
of crash bugs. First, we extract all crash traces from question
bodies on Stack OverÔ¨Çow and use BM25 [20] to retrieve a set
of relevant questions based on the crash reasons part of every
crash trace. Then, we propose two metrics to evaluate the sim-
ilarities between given crash trace and all posts in the question
set: (i) a proposed position dependent similarity based on stack
frames between a given crash trace and question‚Äôs crash trace;
(ii) an extra knowledge similarity measure, being the similarity
between the knowledge from ofÔ¨Åcial documentation of a crash
reason and the question‚Äôs title. The sorted top-k question list
and the answers belonging to these questions are returned. We
then score these answer paragraphs according to several kinds
of features, and we borrow ideas from the Maximal Marginal
Relevance algorithm [24] to generate solution summaries.
Finally, we Ô¨Ålter some low quality sentences out and output
the solution.
We built two repositories for Java questions and Android
questions from Stack OverÔ¨Çow where there is crash trace in
the question body, of size of 69,478 and 33,566 respectively.
We randomly selected another 50 Java questions and 50
Android questions and extracted all crash traces in these
question posts. We chose four approaches as our baselines‚Äì Google Search Engine, SO Search Engine, AnswerBot [4],
and CROKAGE [5]. Our experiments demonstrate that C RA-
SOLVER signiÔ¨Åcantly outperforms all baselines in terms of
relevance, usefulness and diversity. The evaluation of the
retrieval component in our approach shows strong ability to
search related questions on Q&A sites for a given crash trace.
The main contributions of this paper include:
We propose a novel approach that automatically summa-
rizes solutions for crash traces;
We introduce two similarity metrics for retrieving rele-
vant questions related to crash traces, position dependent
similarity and extra knowledge similarity;
We conduct extensive experiments to evaluate C RA-
SOLVER , demonstrating its effectiveness compared to
state of the art approaches.
The rest of this paper is organized as follows. Section II de-
scribes the details of proposed approach. Section III describes
data preprocessing and the experiments. Section IV discusses
the advantages and disadvantages of C RASOLVER . Section V
elaborates the threats to validity and Section VI reviews the
related work. Finally, Section VII concludes the paper and
mentions future work.
II. P ROPOSED APPROACH
We propose a new approach, C RASOLVER , to generate
potential solution summaries for a given crash trace thrown
by a program. Its framework is illustrated in Figure 2. C RA-
SOLVER takes a crash trace as input and produces a solution
summary for the crash trace. The details of its preprocessing
and preparation phase, relevant question retrieval phase and
solution generation phase are described below.
A. Preprocessing and Preparation
To Ô¨Ånd questions with a crash trace on Stack OverÔ¨Çow,
we Ô¨Årst manually checked a large number of exception-
related questions on Stack OverÔ¨Çow. We observe that most
crash traces are highlighted with the HTML tag hcodeior
hblockquoteiand contain the string ‚ÄùCaused by‚Äù or ‚ÄùEx-
ception in‚Äù. We thus use Beautiful Soup [25] and regular
expressions to extract all questions with crash traces based
on these heuristics ofÔ¨Çine. In order to support the relevant
question retrieval phase and the solution generation phase, we
prepare the following materials:
Crash Trace Index : We use the NLTK package [26] to
tokenize all the crash traces extracted from SO questions. All
the pairs of question id and crash trace are collected to build
the document corpus. The preprocessed document corpus is
then utilized to build an index using Whoosh [27], similar to
Lucene [28] but implemented in Python. C RASOLVER utilizes
this index to initially Ô¨Ålter out the relevant question set.
Exception Dictionary : To determine the concept of what a
crash trace is about, we introduce knowledge from ofÔ¨Åcial
documentation into our approach. We build a dictionary that
stores the names of all exceptions and corresponding descrip-
tions, crawled from ofÔ¨Åcial documentation sites.
1287Fig. 2. The overall framework of our approach.
Language Models : To measure the similarity between a SO
post and an exception description from ofÔ¨Åcial documentation,
we build domain-speciÔ¨Åc language models. We collect all
titles and body text of Stack OverÔ¨Çow posts and tokenize
the extracted contents with white spaces and punctuation. We
remove stop words and stem each word to its root form in
the corpus using the NLTK package [26]. We then train a
word embedding model FastText [29] to represent each word
in the corpus as a Ô¨Åxed-length vector. We also compute the
IDF metric (inverse document frequency) of each word in the
vocabulary. The IDF metric represents the possibility that a
word may carry important semantic information. We use the
IDF-weighted word embedding vector as the representation of
each word in the corpus. In this way, the semantic similarity
is able to be computed in the question retrieval phase and the
solution generation phase.
B. Relevant Question Retrieval
Given a crash trace CT, C RASOLVER Ô¨Årst uses regular
expressions to extract the crash reasons andstack frames re-
spectively. C RASOLVER also checks every token contained in
the trace crash reasons . If the token fully matches any names
of exceptions in the Exception Dictionary (see Section II-A),
the corresponding description and the exception name are
added as extra knowledge for the crash trace.
In order to greatly narrow down the search space, C RA-
SOLVER uses the Crash Trace Index to retrieve a relevant ques-
tion set. C RASOLVER tokenizes the extracted crash reasons
by NLTK [26] and loads the Crash Trace Index. C RASOLVER
uses BM25 [20] to measure the lexical similarity of each crash
trace in the index. The crash reason and the crash trace from
the pre-loaded index are denoted as CR and P respectively.
The similarity score is computed as follows:
Sim(P;CR ) =nX
i=1score i (1)score i=IDF (wi)f(wi;P)(k+ 1)
f(wi;P) +k(1 b+bjPj
avgdl)
(2)
wheref(wi;P)is the word wi‚Äôs term frequency in crash
trace P,jPjis the length of the crash trace, IDF(wi)is the
inverse document frequency of word wi,kandbare two
free parameters. kis used to normalize the range of term
frequencies, and bcontrols the inÔ¨Çuence of document length.
We setkandbto be 1.2 and 0.75 by default. Prior works
have demonstrated that this setting performs well for various
corpora [30] [31]. In this paper, C RASOLVER only retrieves
the top-50 similar questions to avoid introducing too much
noise in later stages. C RASOLVER further Ô¨Ålters out questions
where the crash trace in question body shares no exception
name with the query crash trace.
After getting the relevant question set based on the crash
reasons , CRASOLVER further utilizes structural information
contained in stack frames and extra knowledge from ofÔ¨Åcial
documentation for a more precise result:
1) Position Dependent Similarity: We introduce our pro-
posed position dependent similarity in this section, based on
insights from Dang et al. [13]:
the frame that causes the bug most likely occur near the
top of the a call stack ; and
the alignment offset between two matched functions in
two similar call stacks is likely to be small.
For the given a crash trace, C RASOLVER extracts all of its
call stacks and corresponding stack frames . We denote the call
stacks‚Äô position in the entire crash trace as s, and the distance
of every function (token) to the top of its belonging call stack
asd. CRASOLVER thus records the position information of
each tokenwiin the crash trace as a set:
position wi=f(si;1;di;1);;(si;k;di;k)g (3)
where k is the total number that wiappears in the crash trace.
For example, the position information of the token marked
in blue in Figure 3 can be represented as position wblue=
f(0;2);(1;4)g.
1288Crash TraceCall Stack 1:!"=0Call Stack 2:!%=1Stack framesStack framesthe distance to the top of call stack:'"=2the distance to the top of call stack:'%=4Fig. 3. Illustration of the Position information.
LetWbe the set of all the matched functions between the
given crash trace CT 1and a crash trace from the relevant
question set CT 2. The position dependent weight of each word
inWis computed as:
PDweight wi= 1=(emin (dis1;dis 2)(jdis1 dis2j+ 1:0))
(4)
wheredisis the average of din theposition wifrom their
respective crash trace. Based on BM25 [20], C RASOLVER
calculates the position dependent similarity between CT 1and
CT 2as follows:
PDSim (CT 1;CT 2) =nX
i=1score iPDweight wi (5)
where thescore iis calculated by Equation 2. Even if two crash
traces have many identical tokens, if the position distributions
of these tokens are very different (meaning the two bugs are
not similar), our algorithm can distinguish them and give a low
score. In this way, the position dependent similarities between
the given crash trace and each crash trace in the relevant
question set can be calculated.
2) Extra Knowledge Similarity: To understand what the
crash trace about, we introduce extra knowledge sourced from
ofÔ¨Åcial documentation. Given a crash trace and the relevant
question set, C RASOLVER transforms the corresponding de-
scription from ofÔ¨Åcial documentation of the crash trace and the
title of a relevant question into two bag of words, denoted as D
and T, respectively. Unlike Yang et al. [22], who average word
embedding vectors of words in a document, C RASOLVER
uses a IDF-weighted word embedding vector to represent a
document.
CRASOLVER loads the prepared language model mentioned
in Section II-A and computes the extra knowledge similarity
as follows:
EKSim (D;T) =  !vD  !vT
k  !vDkk  !vTk(6)
  !v=Pn
i=1IDF (wi)FastText (wi)Pn
i=1IDF (wi)(7)
wherewiis the word in the document, FastText (wi)is the
word embedding vector of the wirepresented by pre-trained
FastText model (see Section II-A).Now we obtain extra knowledge similarities and position
dependent similarities between the given crash trace and all
questions in the relevant question set. We normalize both
similarity scores to make them comparable. The Ô¨Ånal relevant
score between the given crash trace (CT) and each question
(Q) in the relevant question set is calculated as:
Rel(Q;CT ) = (1 )PDSim +EKSim (8)
whereis a hyper-parameter set to 0.25 by default. We give
more weight to position dependent similarities because we
need knowledge of ofÔ¨Åcial documentation to help retrieve,
rather than lead, the search for relevant questions.
Based on the calculated Ô¨Ånal relevant scores, C RASOLVER
returns the top-10 relevant questions along with their answers
for the given crash trace.
C. Solution Generation
In order to avoid some redundant information and generate
a comprehensive solution, we borrow an idea from Maximal
Marginal Relevance algorithm [24] to generate the solution.
Given the answers of the top-10 relevant questions, C RA-
SOLVER uses the granularity of answer paragraphs to generate
the Ô¨Ånal solution. We Ô¨Årst score each answer paragraph,
denoted as A, based on four features:
the relevant score of the question to which the paragraph
belongs, which is described in Section II-B;
semantic similarity score with crash trace‚Äôs description,
calculated by Equation 6;
vote on answer ‚Äì we set the vote on the answer post
where the paragraph comes from as the vote score; and
whether the answer is accepted. If the answer to which
the paragraph belongs is accepted, we set the score to 2,
otherwise 1.
The Ô¨Ånal score for a answer paragraph Score (A;CT )is
computed by multiplying the above four scores and normaliz-
ing the result. We then apply the MMR algorithm to select a
set of paragraphs as:
MMRdef=Arg max
Ai2RnS[Score (Ai;CT) 
(1 ) max
Aj2SSim(Ai;Aj)
(9)
whereCTis the given crash trace, Sis the set of paragraphs
which have been selected, RnSis the paragraphs which have
not been selected, Sim is the cosine similarity between the
vector representations  !vof two answer paragraphs, and is
a parameter to adjust the relevance and diversity of the results,
which is set at 0.75 by default.
In this work, C RASOLVER selects 3 relevant paragraphs to
prevent too long output. The short code fragments which are
enclosed in HTML tag hcodeiin natural language paragraphs
are preserved in the paragraphs. The long code snippets with
no more than 20 lines are extracted along with the answer
paragraph. C RASOLVER will append the corresponding long
code snippets, if any, to the end of the solution.
1289Finally we follow the method from Wang et al. [32] that uses
grammatical dependency analysis by Stanford CoreNLP [33]
to identify the higher quality sentences from these answer
paragraphs.
VP << (NP <=NN: ?=)<=VB: ?=
NP!<PRP [<<VPj$VP](10)
We keep only sentences that have a verb phrase followed by
a noun, and have a noun phrase followed by a verb phrase [32].
The other sentences are Ô¨Åltered out and then C RASOLVER
outputs the Ô¨Ånal solution to the users.
III. E VALUATION
In this section, we describe the experimental setup that we
follow to evaluate the performance of our C RASOLVER . Our
experiments aim to answer the following research questions:
RQ1: How does C RASOLVER perform compared to other
approaches?
RQ2: What impact does each of the proposed metrics have
in the C RASOLVER approach?
RQ3: How effective is the C RASOLVER relevant question
retrieval component for crash traces?
A. Data Collection and Tool Implementation
We downloaded the ofÔ¨Åcial data dump of Stack Over-
Ô¨Çow [34] published on March 2020. Considering that our
approach is designed for crash traces based on the Java
language, we extracted the questions on SO that are tagged
with ‚Äùjava‚Äù or ‚Äùandroid‚Äù. To create our knowledge base of
exception-related questions, we selected questions satisfying
the following criteria: (i) there is at least one answer to the
question; (ii) there is a crash trace in the question body.
Note that questions where there is a crash trace in the body
were automatically identiÔ¨Åed by the heuristics described in
Section II-A. We collected 69,478 Java questions and 33,566
Android questions respectively.
We downloaded the Java SE 8 API documentation [35] to
introduce additional crash solving knowledge. We parsed the
html Ô¨Åles using Beautiful Soup [25] and extracted all the class
names along with their descriptions. We collected all classes
and did not distinguish whether they are exceptions for the
sake of simplicity. In total, we built a dictionary containing
4,216 class name-description pairs.
Based on these repositories, we built a text corpus using the
title and body of each post. We used Gensim [36] to train the
FastText model.
In order to evaluate C RASOLVER , we randomly selected 50
Java questions and 50 Android questions where there are crash
traces attached and made sure there are no duplicate questions.
We extracted all crash traces in these questions to build the
experimental queries. Note that these 100 questions and their
duplicate questions are removed from our exception-related
question base. We refer the 50 crash traces extracted from
Java questions as java test crashes and the 50 crash traces
from Android questions as android test crashes .TABLE I
TASK ALLOCATION
RQs Java 1-25 Java 26-50 Android 1-25 Android 26-50
RQ1 P1,P2,P3 P4,P5,P6 P7,P8,P9 P10,P11,P12
RQ2 P1,P2,P3 P4,P5,P6 P7,P8,P9 P10,P11,P12
RQ3 P13,P14,P15 P13,P14,P15 P16,P17,P18 P16,P17,P18
B. Baselines
Since there is no previous work to generate solutions for
crash traces that we could Ô¨Ånd, we choose two approaches for
automatic solution generation for programming questions and
two solution search methods commonly used by developers.
AnswerBot [4]: AnswerBot is a three-stage framework to
achieve the goal of generating an answer summary for a non-
factoid technical question. AnswerBot evaluation has shown
that its generated answer summaries are relevant, useful and
diverse. In our experiment, we use the crash trace as the input.
CROKAGE [5]: CROKAGE is a tool that takes the descrip-
tion of a programming task as a query and provides a compre-
hensive solution for the task. We conduct our experiment by
using their online website [37] and take the crash reasons part
of crash trace as input due to the limitation of input length.
Google Search Engine: Google search engine is the most
common way for developers to search for solutions. However,
the length of input to google search engine is limited to
32 words. We take the crash reasons part of test crash
trace as input in our experiment, which is also the general
practice of most developers. For the sake of fairness, we add
‚Äúsite:stackoverÔ¨Çow.com‚Äù to the input of Google search engine
so that it searches only posts on Stack OverÔ¨Çow.
Stack OverÔ¨Çow Search Engine: Stack OverÔ¨Çow is an im-
portant body of knowledge for solving developers‚Äô technical
questions. Developers formulate their question as a query to
Stack OverÔ¨Çow, and the search engine will return a list of
relevant questions. We also use the crash reasons as query
due to its limitation of query input.
For the baselines based on a search engine, we use the
answer that is accepted or has the highest votes if there is
no accepted answer from the Ô¨Årst ranked Stack OverÔ¨Çow
question returned by a search engine. Due to the different
implementation of their search engines, even if we input the
same thing into Google and Stack OverÔ¨Çow, the order of SO
pages returned by both search engines is often different.
C. Participant Selection and Task Allocation
We invited 18 students who major in Computer Science to
participate in our study, including 14 Ph.D. students and 4
graduate students. As shown in Table I, they will be assigned
to the user studies for RQ1, RQ2 and RQ3, respectively.
All of them are not co-authors and are indexed from P1 to
P18. All study participants have industrial experience in Java
programming ranging from 4 to 8 years, and participants eval-
uating the Android crash traces have at least 3 years‚Äô Android
development experience. We have a tutorial for participants
and they are allowed to conduct the study in their own lab.
1290TABLE II
COMPARISON RESULTS WITH BASELINES ON DIFFERENT LANGUAGES
Languages Approaches Relevance Usefulness Diversity
JavaCraSolver 3.433 2.900 2.247
Google 2.740*** 2.307*** 1.540***
Stack OverÔ¨Çow 2.500*** 2.093*** 1.333***
AnswerBot 2.160*** 1.70*** 1.320***
CROKAGE 2.410*** 1.893*** 1.353***
AndroidCraSolver 3.540 3.047 2.640
Google 2.833*** 2.313*** 1.560***
Stack OverÔ¨Çow 2.313*** 2.020*** 1.480***
AnswerBot 2.253*** 1.753*** 1.520***
CROKAGE 2.317*** 1.847*** 1.373***
***p-value <0.001
Table I presents the task allocation. The 100 crash traces
(50 from Java questions and 50 from Android questions)
are divided into four groups, e.g., Java 1-25, Java 26-50,
Android 1-25 and Android 26-50, to reduce overall task size
for participants. RQ1 and RQ2 both evaluate the overall perfor-
mance of C RASOLVER , and RQ3 evaluates the effectiveness
of the retrieval component. To avoid biasing results, we have
participants analyzing the same crash traces in RQ1 and RQ2,
and arranged another group of participants to conduct RQ3
(e.g. P1, P2 and P3 evaluate Java 1-25 in RQ1 and RQ2 but
Java 1-25 are evaluated by P13, P14 and P15 in RQ3).
D. Experimental Results
RQ1: How does C RASOLVER perform compared to other
approaches?
Motivation. CRASOLVER aims to automatically provide a
solution summarization for a given crash trace. We choose two
approaches for automatic solution generation for programming
questions, and two solution search methods commonly used
by developers as baselines. Compared to these baselines, we
wanted to evaluate the overall performance of our C RA-
SOLVER approach in terms of the relevance, usefulness and
diversity.
Approach. We compare C RASOLVER against four baselines
described in Section III-B. In this user study, participants will
Ô¨Årst read the given crash trace, followed by Ô¨Åve solutions
from C RASOLVER and the four baseline approaches. Note that
participants do not know which approach each solution comes
from and the order is randomized.
Similar with Xu et al. [4], participants are asked to score the
Ô¨Åve solutions from three aspects, i.e., relevance, usefulness,
and diversity. Relevance refers to how relevant the solution
is to the crash trace. Usefulness refers to how useful the
solution is for solving the crash bug. If a solution contains
a variety of methods to solve the crash bug, as long as
one of them can solve the problem well, we give it a high
usefulness score. Diversity refers to whether the generated
solution contains multiple relevant methods for the crash trace.
Note that if some contents are not related to the crash, the
Fig. 4. Box plots of the relevance, usefulness and diversity of different ap-
proaches, e.g., (A) CraSolver, (B) Google, (C) Stack OverÔ¨Çow, (D) AnswerBot
and (E) CROKAGE.
diversity score cannot be improved because of this irrelevant
content in the generated solution. The score ranges from 1 to
5. Score 1 means ‚Äùirrelevant/useless/identical‚Äù and 5 means
‚Äùhighly relevant/useful/diverse‚Äù.
Results. Table II presents the mean of relevance, usefulness,
and diversity scores and Figure 4 presents in box plots these
scores of each solution from different approaches. The baseline
built on Google is the best in all three aspects, besides our
CRASOLVER method. Although Xu et al. [4] have reported
that AnswerBot diversity is much higher than that of Google,
it has not performed well because their approach is not aimed
at crash traces.
However, ourCRASOLVER approach performs far better
than the best baseline built on Google . This phenomenon
demonstrates that directly using the crash reason of a crash
trace as input to Google has a limited effectiveness on iden-
tifying solutions to the crash bug. For the java test crashes ,
the relative improvements of C RASOLVER are 25.3%, 25.7%
and 45.9%, w.r.t., relevance, usefulness, and diversity, respec-
tively. For the android test crashes , CRASOLVER outperforms
Google solution search by 25.0%, 31.7% and 69.2% in terms
of relevance, usefulness and diversity, respectively. On aver-
age, C RASOLVER outperforms the baseline built on Google
by 25.4%, 28.7% and 57.6% in terms of relevance, usefulness
and diversity, respectively.
We conducted a Wilcoxon signed-rank test [38] with a
Bonferroni correction [39] to evaluate whether the differences
between C RASOLVER and these baselines are statistically
signiÔ¨Åcant. The improvements of our C RASOLVER approach
over these baselines are statistically signiÔ¨Åcant on three aspects
at the conÔ¨Ådence level of 99.9%. This suggests that the solu-
tion for crash trace solution generation by our C RASOLVER
approach is better than other baselines in terms of relevance,
usefulness, and especially diversity.
In summary, C RASOLVER signiÔ¨Åcantly outperforms other
approaches for both Java and Android crash traces.
RQ2: What impact does each of the proposed metrics have in
our C RASOLVER approach?
1291TABLE III
RESULTS OF THE ABLATION STUDY
Languages Approaches Relevance Usefulness Diversity
JavaCraSolver EKSim3.007*** 2.507*** 2.007**
CraSolver PDSim2.753*** 2.147*** 1.833***
CraSolver 3.433 2.900 2.247
AndroidCraSolver EKSim3.027*** 2.547*** 2.200**
CraSolver PDSim2.780*** 2.320*** 1.867***
CraSolver 3.540 3.047 2.640
***p-value <0.001, **p-value <0.01
Motivation. In order to improve the quality of the Ô¨Ånal gener-
ated solutions, we introduce position dependent similarity and
extra knowledge similarity to the relevant question retrieval
component, described in Section II-B. However, C RASOLVER
can still work with only relying on one similarity metric.
Thus, we need to conduct an ablation study to investigate the
inÔ¨Çuence of these two similarity metrics.
Approach. To answer this research question, we analyze
the performance gain achieved due to various components
of our approach by performing an ablation study. We Ô¨Årst
remove the extra knowledge from ofÔ¨Åcial documentation
and set all the extra knowledge similarity scores to 0 (i.e.,
EKSim ) in C RASOLVER . We refer to this reduced approach
as CraSolver EKSim. The second variant, CraSolver PDSim,
disables the position dependent similarity, setting all the
PDweight to be 1. Finally we compare the performance of so-
lutions generated by the above two methods with C RASOLVER
using both similarity metrics. Participants are asked to score
each solution from these approaches as in RQ1. The tasks are
assigned according to Table I.
Results. Table III presents the results of our ablation study.
We observe that C RASOLVER performs signiÔ¨Åcantly better
than using each similarity metric individually , on both
java test crashes andandroid test crashes . On average, C RA-
SOLVER outperforms CraSolver PDSimby 26.0%, 33.2% and
32.0% in terms of relevance, usefulness and diversity; C RA-
SOLVER outperforms CraSolver EKSimby 15.6%, 17.7% and
16.0% respectively. Comparing the improvement ratio over
CraSolver PDSimand CraSolver EKSim, we can see that
the proposed position dependent similarity greatly improves
the performance of our approach. The introduction of extra
knowledge further improves the effectiveness of C RASOLVER .
As for RQ1, we conduct a Wilcoxon signed-rank test [38]
with a Bonferroni correction [39], which has been marked
in the table. The improvement of our approach over the
variant CraSolver PDSimon all three aspects is statistically
signiÔ¨Åcant at the conÔ¨Ådence level of 99.9%. Compared with
CraSolver EKSim, the improvement of C RASOLVER on di-
versity is statistically signiÔ¨Åcant at the conÔ¨Ådence level of
99%, the improvement on relevance and usefulness is statisti-
cally signiÔ¨Åcant at the conÔ¨Ådence level of 99.9%. In summary,
the combination of the two similarity metrics makes our
CRASOLVER approach achieve its best performance.RQ3: How effective is C RASOLVER ‚Äôs relevant question re-
trieval component for crash traces?
Motivation. In order to generate high-quality solutions, C RA-
SOLVER must Ô¨Årst narrow down the search scope to Ô¨Ånd the
Stack OverÔ¨Çow questions related to the given crash trace.
Whether the retrieval questions are relevant to the given crash
trace will greatly affect the quality of the Ô¨Ånal solution.
Here we investigate the effectiveness of our relevant question
retrieval component (see Section II-B) in C RASOLVER , com-
pared with several traditional information retrieval methods.
Approach. We choose four most commonly used information
retrieval methods as our baselines: BM25 [20], TF-IDF based
information retrieval [21], word embedding based information
retrieval [22], and document-to-vector based information re-
trieval [23]. BM25 [20] is a ranking function used by search
engines to estimate the relevance of documents to a given
search query. TF-IDF [21] is also a traditional IR metric
which has been widely used in software engineering [40]‚Äì[42].
Yang et al. [22] use the mean of word embedding vectors to
represent the document and calculate the similarity. an unsu-
pervised framework that learns continuous distributed vector
representations for pieces of texts Document-to-vector [23] is
an unsupervised framework that learns continuous distributed
vector representations for pieces of texts.
For each test crash trace, we collect the top-10 relevant
questions returned by our relevant question retrieval compo-
nent or one of the baselines. We put all the top-10 results
together from 5 approaches and there are many overlapping
questions. Then, according to the task allocation (Table I), we
ask 6 participants (P13, P14 and P15 for java test crashes , P16,
P17 and P18 for android test crashes ) to identify the relevant
questions within the results. It is easy for participants because
they only need to give yes/no response. Each participant had
5 days to complete the task, and they spent an average of 3.5
hours on it. We further use Fleiss Kappa [43] to measure the
agreement between the three participants. The kappa values
are 0.86 and 0.83 for java test crashes and android test
crashes respectively, which indicate that participants have a
high degree of consistency. If there are inconsistent labels
among the participants, we take the label by majority rule
as the Ô¨Ånal result.
The evaluation metrics we used to evaluate our retrieval
component and other baselines are Top-K Accuracy (Top@K),
Mean Reciprocal Rank (MRR) and Mean Average Precision
(MAP), which are widely used in previous software engi-
neering studies [4], [40], [44]‚Äì[46]. Top-K Accuracy is the
percentage of test crash traces where there is at least one
relevant question within the top-K returned questions. In this
paper, we set the K to be 1, 5 and 10. Mean Reciprocal Rank
is the multiplicative inverse of the rank of the Ô¨Årst relevant
questions. Mean Average Precision is the average of precision
for each test crash trace, where the precision is the percentage
of relevant questions within the top-10 returned results.
Results. Table IV presents the performance of different ap-
proaches in terms of Top@K Accuracy, MRR and MAP. The
1292TABLE IV
PERFOMANCE OF CRASOLVER AND OTHER BASELINES IN TERMS OF TOP@K A CCURACY , MEAN RECIPROCAL RANK (MRR)
AND MEAN AVERAGE PRECISION (MAP)
Languages Approaches Top@1 Top@5 Top@10 MRR MAP
JavaTF-IDF 0.68 0.84 0.90 0.76 0.58
BM25 0.78 0.86 0.94 0.82 0.68
Word Embedding 0.70 0.92 0.94 0.80 0.60
Doc2Vec 0.34 0.60 0.70 0.46 0.24
CraSolver 0.94 1.00 1.00 0.96 0.86
AndroidTF-IDF 0.62 0.88 0.92 0.73 0.56
BM25 0.74 0.94 0.94 0.81 0.73
Word Embedding 0.72 0.80 0.90 0.76 0.52
Doc2Vec 0.50 0.66 0.80 0.58 0.28
CraSolver 0.92 0.96 0.98 0.94 0.83
TABLE V
SOME EXAMPLES OF GENERATED SOLUTIONS
Crash Trace Generated solutions
Process: social.com.networking.social.media.app, PID: 28258
java.lang.NoClassDefFoundError: Failed resolution of: Lcom/squareup/okhttp/OkHttpClient;
at com.squareup.picasso.OkHttpDownloader.defaultOkHttpClient(OkHttpDownloader.java:31)
at com.squareup.picasso.OkHttpDownloader. <init>(OkHttpDownloader.java:76)
at com.squareup.picasso.OkHttpDownloader. <init>(OkHttpDownloader.java:65)
at social.com.networkingsocialmediaapp.Global.onCreate(Global.java:17)...
Caused by: java.lang.ClassNotFoundException: Didn‚Äôt Ô¨Ånd class
‚Äùcom.squareup.okhttp.OkHttpClient‚Äù on path: DexPathList
[[zip Ô¨Åle ‚Äù/data/app/social.com.networking.social.media.app-2/base.apk‚Äù],
nativeLibraryDirectories=[/data/app/social.com.networking.social.
media.app-2/lib/arm64, /vendor/lib64, /system/lib64]]
at dalvik.system.BaseDexClassLoader.Ô¨ÅndClass(BaseDexClassLoader.java:56)
at java.lang.ClassLoader.loadClass(ClassLoader.java:511)
at java.lang.ClassLoader.loadClass(ClassLoader.java:469)
at com.squareup.picasso.OkHttpDownloader.defaultOkHttpClient(OkHttpDownloader.java:31)
at com.squareup.picasso.OkHttpDownloader. <init>(OkHttpDownloader.java:76)...Crasolver:
Well This is not an appropriate solution but it seems that there is some bug in okhttp
library.. To solve this error i just replaced my gradle with the older version of okhttp.
compile ‚Äôcom.squareup.okhttp:okhttp:2.5.0‚Äô. compile ‚Äôcom.squareup.okhttp3:okhttp:3.6.0‚Äô.
And it works like a charm.. Thankyou.
Those are the steps I followed to do the signup / login / linking of users with
ParseUsers according to Parse documentation and some tips I found on the web:.
First, I went to this website and created a new application..
I downloaded the Facebook SDK, I did the steps 4 and 5 and I added my package
name and the default class name. I added the key hashes:...
Baseline Google:
Got the solution ,it is happening because of instant run, Just disable it
Goto :
Android Studio ‚Äì >File ‚Äì>Setting ‚Äì >Build, execution, deploy
‚Äì>Instant run.
Exception in thread ‚Äùmain‚Äù java.io.IOException: Not in GZIP format
at java.util.zip.GZIPInputStream.readHeader(GZIPInputStream.java:141)
at java.util.zip.GZIPInputStream. <init>(GZIPInputStream.java:56)
at java.util.zip.GZIPInputStream. <init>(GZIPInputStream.java:65)
at postgresjdbc.PostgresJDBC.gunzip(PostgresJDBC.java:237)
at postgresjdbc.PostgresJDBC.readBytes(PostgresJDBC.java:230)
at postgresjdbc.PostgresJDBC.main(PostgresJDBC.java:208)
Java Result: 1Crasolver:
Since fcontents contains your gzipped data it should be a byte[] and not a String?.
I recommend using IOUtils for reading the Ô¨Åle into a byte array as reading it
into a string will most likely corrupt your data...
Baseline Google:
String outStr = out.toString(‚ÄùUTF-8‚Äù); This ‚Äùout‚Äù is ziped byte stream,encode
it to String then decode it from String will be lose some bytes.This maybe a
bug of java. To Resolve it,you can encode bytes to String in compress()
to return, such as :
String infoBase64Encode = new String(Base64.encodeBase64(out.toByteArray()))...
Exception in thread ‚Äùmain‚Äù java.lang.OutOfMemoryError: Java heap space
at java.util.HashMap.createEntry(HashMap.java:869)
at java.util.HashMap.addEntry(HashMap.java:856)
at java.util.HashMap.put(HashMap.java:484)
at com.Kaggle.baseline.BaselineNew.createSimMap(BaselineNew.java:70)
at com.Kaggle.baseline.BaselineNew. <init>(BaselineNew.java:25)
at com.Kaggle.baseline.BaselineNew.main(BaselineNew.java:315)Crasolver:
The problem is that Eclipse Memory Analyser does not have enough heap space to open
the Heap dump Ô¨Åle.You can solve the problem as follows:open the MemoryAnalyzer.ini
Ô¨Åle. change the default -Xmx1024m to a larger size
Note that on OS X, to increase the memory allocated to MAT, you need to right-click
Memory Analyzer.app and show the package contents. The MemoryAnalyzer.ini Ô¨Åle is
under /Contents/MacOS/. I was facing the same issue but with the eclipse plugin and I
did not have any Memory Analyzer App in Applications Folder. The solution which
worked for me was:. Right Click on Eclipse icon and select Show Package Content.
Go to Contents >Eclipse. Open Eclipse.ini. Restart Eclipse.
Baseline Google:
Well, it‚Äôs fairly self-explanatory: you‚Äôve run out of memory. You may want to try starting
it with more memory, using the -Xmx Ô¨Çag, e.g., java -Xmx2048m [whatever you‚Äôd have
written before] This will use up to 2 gigs of memory. See the non-standard options list
for more details.
results show that the relevant question retrieval component
ofCRASOLVER signiÔ¨Åcantly outperforms other baselines
in terms of Top@1, MRR and MAP for both test datasets.
Compared to the best baseline approach BM25, our approach
outperforms BM25 by 21%, 17% and 26% in terms of Top@1,
MRR and MAP on java test crashes . As for the android
test crashes , the relative improvements of C RASOLVER are
24%, 16% and 14% in terms of Top@1, MRR and MAP
respectively.
Another phenomenon is that almost all the metrics of
our method are close to 1.0, indicating its powerful per-
formance in Ô¨Ånding relevant SO questions for crash traces.
Our approach decomposes a crash trace into crash reasons andstack frames , and introduces the position dependent similarity
and extra knowledge similarity for retrieval component. These
enable C RASOLVER to achieve the best performance. This
also ensures that the resulting solution is mostly composed
of answer paragraphs to the relevant questions. The results
indicate the effectiveness of the relevant question retrieval
component, and the superior performance of this component
further improves the quality of our Ô¨Ånal generated solutions.
IV. D ISCUSSION
In order to help developers resolve software crash bugs more
efÔ¨Åciently, we propose C RASOLVER to automatically generate
solutions for thrown crash traces. C RASOLVER decomposes
1293every crash trace into crash reasons andstack frames , and
utilizes the information of these two parts for better retrieval
of similar crash traces. In particular, we designed two similar-
ity scoring metrics for crash trace, e.g., position dependent
similarity and extra knowledge similarity. The experiments
described in Section III have shown the effectiveness of
our C RASOLVER approach. In this section, we qualitatively
analyze the advantages and disadvantages of C RASOLVER .
Until now, there have been no approaches designed for auto-
matic crash solution generation. According to the experimental
results from Section III, the baseline built on Google search
engine performs the best among these other four baselines. We
manually compare solutions from the baseline built on Google
search engine and the solutions generated by C RASOLVER .
Table V presents three examples. In the Ô¨Årst example, we
can see from the crash trace that the cause of this crash bug
may be a problem related to the ‚ÄùOkHttp‚Äù package or project
conÔ¨Åguration. The solution generated by C RASOLVER Ô¨Årst
points out that there may be some bugs in the okhttp library
and then gives a feasible solution ‚Äúreplace my gradle with the
older version of okhttp‚Äù . However, the second paragraph of
the solution describes the ‚ÄùFacebook SDK‚Äù settings which is
not very relevant to this crash trace. We check the answer post
corresponding to this paragraph and Ô¨Ånd that it comes from
a question that throws the same crash reason as this crash
trace report, but involves different packages. We introduce the
MMR algorithm (see Section II-C) to generate diversiÔ¨Åed and
comprehensive solution suggestions, which may also introduce
some noise into the Ô¨Ånal output. This leads to C RASOLVER
suggested solutions occasionally containing some irrelevant
content. The solution from the baseline built on Google
provides a common method to solve the problem of project
package conÔ¨Åguration. From this case, we can Ô¨Ånd that the
solutions given by C RASOLVER are more targeted, while those
from baseline built on Google are more general.
In some cases like the second crash trace in Table V, the
proportion of keywords (GZIP) in the crash reason part is
relatively low. If we use crash reason to search for relevant
answers in search engines, as most developers do, the an-
swers returned by search engines may contain a lot about
‚ÄùIOException‚Äù. This will cost developers a lot of time to
Ô¨Ånd the right solution. In contrast, C RASOLVER Ô¨Årst utilizes
thecrash reason to narrow down the search space. Among
the relevant found questions, C RASOLVER then introduces a
position dependent similarity and an extra knowledge simi-
larity to retrieve more relevant questions. The tokens (e.g.,
zip and GZIPInputStream) in the stack frames of the crash
trace help C RASOLVER to better locate relevant questions.
The experimental results of RQ3 in Section III-D also show
the effectiveness of the relevant question retrieval component
in our approach. We can see from the solution generated
by C RASOLVER it Ô¨Årst explains the cause of the crash bug,
and then gives the targeted solution. This also shows the
disadvantage of using only the crash reason part of crash
trace to search. Our approach utilizes both the crash reasonandstack frames contained in the crash trace, which ensures
better quality of the Ô¨Ånal generated solutions.
The third case is an ‚ÄùOutOfMemoryError‚Äù, which is a crash
bug that we often encounter in our daily development. The
solution from baseline built on Google suggests using the -
Xmx Ô¨Çag to start the program with more memory. In contrast,
CRASOLVER provides not only the reason for the crash bug,
but also several alternative methods to solve it. In this exam-
ple, C RASOLVER provides solutions for different situations
with or without MAT (Memory Analyzer Tool). Even better,
CRASOLVER specially suggests a way to solve this crash bug
on the OS X system. The solutions returned by the baseline
built on Google usually focus on a speciÔ¨Åc aspect of the
solution to the crash bug. Our generated solutions are derived
from answers posted in several relevant questions. Therefore,
the solutions generated by C RASOLVER can provide solutions
with more alternative methods to solve crash bugs and more
useful information for developers.
In addition to the advantages and disadvantages of our
approach discussed above, C RASOLVER is also limited to
situations that developers can Ô¨Ånd relevant solutions for the
thrown crash traces by referring to the relevant content on
Q&A sites. If there is no relevant content on Q&A sites,
CRASOLVER is not able to generate useful and comprehensive
solutions to the given crash traces. In contrast, the Google
search engine can search for relevant solutions from a much
wider range of website resources, and its results returned may
be more useful. However, many programs depend on a certain
framework, thus many crash bugs are recurring. The scale of
questions on Q&A sites is also growing with the development
of community. Our C RASOLVER approach has the potential
to generate solutions for more crash traces in the future.
Another limitation is that our implementation of C RA-
SOLVER can only deal with crash traces based on Java
language at present. At present, we collect Stack OverÔ¨Çow
questions tagged with Java or Android to build our reposi-
tories, and the ofÔ¨Åcial documentation we use is Java SE 8
API documentation [35]. However, the crash logs in other
languages, e.g., python and c#, also have a structure similar
to the Java call stack, which makes it not difÔ¨Åcult for our
approach to support other language crash traces. Based on the
idea proposed in this paper, we plan to design algorithms that
can analyze crash traces based on other programing languages
to further enhance the generality of C RASOLVER . In addition,
we will collect more ofÔ¨Åcial documentations from various
languages and frameworks to extend the extra knowledge of
our approach in the future.
V. T HREATS TO VALIDITY
Threats to internal validity are related to the implementation
of different approaches and the design of our user study.
We have double checked our code to make sure that the
questions used in test datasets are excluded from the ques-
tion base when conducting experiments. We directly use the
published tool of AnswerBot [47] and the online tool website
of CROKAGE [37]. As for the input to the search engines,
1294we choose the crash reasons as long as possible in crash
traces as the input, which is also one of the most commonly
ways for developers to search for solutions. For the user study,
each participant‚Äôs development experience and understanding
of the given crash trace may affect the results. We mitigate
this threat by asking three participants with at least 3 years
of programming experience to score for each solution. In
addition, we recruited participants who show interest in our
research and give them enough time to do the user study to
reduce any threats caused by a participant‚Äôs impatience.
Threats to external validity relate to the generalizability
of the our experimental results. There are crash traces from
various programming languages on Stack OverÔ¨Çow and each
program language has its own crash trace structure. At present,
CRASOLVER only supports crash traces based on Java lan-
guage. But it is not difÔ¨Åcult to support other languages based
on the approach we propose. For this reason, we only use the
Java and Android questions in this work. As the experimental
part needs a lot of human participation, we only collected
50 example Java and 50 example Android crash traces for
evaluation. In future, we will collect crash traces containing
more types of crash bugs to mitigate these threats.
Threats to construct validity relate to suitability of our
evaluation metrics. We use relevance, usefulness and diversity
to evaluate the quality of generated solutions. These metrics
are widely used to evaluate summarization tasks in software
engineering [4], [48]‚Äì[51]. To investigate the effectiveness of
retrieval component in our approach, we use Top@k accuracy,
MRR and MAP, which are classical evaluation metrics for
information retrieval [4], [40], [44]‚Äì[46].
VI. R ELATED WORK
Text Summarization. Although there is to our knowledge no
previous work that automatically generates solutions for given
crash traces, this task is similar to other summarization tasks
in software engineering, e.g., bug reports summarization [49],
[52]‚Äì[54] and comment summarization [32] [50] [55]). Rastkar
et al. [52] [53] used a bug report corpus to train a classiÔ¨Åer
and is based on conversational data for automatic bug report
summarization. Mani et al. [49] proposed an unsupervised
approach based on noise reducer to improve the precision
of bug report summarization. Lotufo et al. [54] pose three
hypotheses on what makes a sentence relevant and used
heuristic rules to perform bug report summarization.
For comment summarization, Wong et al. [32] use natural
language processing to Ô¨Ålter relevant sentences to compose the
descriptions for a code. Our approach borrows two patterns
they used to identify important sentences. SURF [50] classi-
Ô¨Åes each user review sentence to one of the user-intention
categories and generates user review summary based on a
scoring mechanism. Gias et al. [55] proposed two algorithms
(statistical and aspect-based) to summarize opinions about
APIs. Different from the above studies, our work aims at
providing useful solution generation for a given crash trace.
Mining Developer Forums. There are abundant resources for
researchers to explore in developer Q&A forums. Treude etal. [56] analyzed how programmers ask and answer questions
on the web. They Ô¨Ånd that Q&A websites are effective for
code reviews and conceptual questions. ACE [57] is a novel
traceability recovery approach to extract the code elements
contained in informal documentation like Stack OverÔ¨Çow.
SeaHawk [58] and Prompter [59] retrieve API names and
keywords from Stack OverÔ¨Çow by the formulated queries
based on the code context. However, their approach is not
applicable to the crash trace solution recommendation prob-
lem. Huang et al. [40] proposed an approach named BIKER to
recommend APIs for the programming task via analyzing the
posts on Stack OverÔ¨Çow. AnswerBot [4] bridges the lexical
gap and provides relevant, useful and diverse answer to the
natural language queries. CROKAGE [5] provides comprehen-
sive solution containing not only relevant code examples but
also their succinct explanations. Different from our approach,
the inputs to both AnswerBot and CROKAGE are natural
language queries, which are difÔ¨Åcult for developers to use to
describe a complex crash bug in natural language. In addition,
a lot of information contained in the crash trace will be lost.
Analysing Crash Traces. The crash trace has been used
in many software engineering studies, such as bug localiza-
tion [6]‚Äì[11], automated program repair [1], [12], [60]‚Äì[63]
and duplicate bug report detection [13]‚Äì[16].
CrashLocator [6] locates faulty functions by expanding
function call sequences in a static call graph and then ranking
suspicious functions, according to the suspiciousness of each
function. Wang et al. [7] proposed three rules to identify
correlated crash types and an algorithm to locate and rank
buggy Ô¨Åles using crash correlation groups. Moreno et al. [8]
and Wong et al. [9] proposed to use code segments and
stack traces to retrieve code elements relevant to bug reports.
Mohammad et al. [10] transform stack traces into a trace graph
and reformulate queries to improve IR-based bug localization.
STMLocator+ [11] automatically locates the relevant buggy
source Ô¨Åles for a given bug report.
For automated program repair, Gao et al. [1] proposed
an approach to Ô¨Åx recurring crash bugs by generating edit
scripts for source code via analyzing Q&A sites. However,
their approach only uses the crash reason of crash traces to
retrieve relevant questions by a search engine. Droix [12] uses
user event sequences (e.g., clicks and touches) as input to
repair buggy apps and utilizes the crash traces to locate the
bug. Some other existing techniques in automated program
repair typically rely on unit tests [60] or test scripts [61]‚Äì[63]
to guide repair process. Different from these techniques that
aim at generating patches, our approach provides explanations
for crash traces. Thus, they are complementary and can work
hand-in-hand to help developers resolve crashes.
Kim et al. [14] proposed crash graphs which provide an ag-
gregated view of multiple crashes in the same bucket to detect
duplicate bug report. Dang et al. [13] proposed a method for
clustering crash reports based on call stack matching. Similar
to our position dependent similarity, they also considered the
distance to the top frame and alignment offset. However, they
1295used frame as the matching granularity and their similarity
metric is formulated in a different way. Johannes et al. [15]
only used stack traces and their structure as input for detecting
bug-report duplicates. DURFEX [16] is a feature extraction
technique that extracts features from bug reports with a focus
on crash traces for detection of duplicate bug reports. However,
our approach not only analyses the structure of crash traces
but also introduces extra knowledge for retrieving potential
solution posts. Our experimental results in Section III-D have
shown its effectiveness.
VII. C ONCLUSION
In this paper, we propose C RASOLVER to automatically
generate possible solution summaries from a given crash trace.
CRASOLVER parses a crash trace into crash reasons and
stack frames , and introduces position dependent similarity
and extra knowledge similarity measures to retrieve relevant
questions on Stack OverÔ¨Çow. After obtaining the answers to
these questions, C RASOLVER summarizes a solution based on
several features. We leverage both information contained in
crash traces and also the knowledge in ofÔ¨Åcial documentation
to improve the effectiveness of C RASOLVER . Our evaluation
using Java and Android crash traces demonstrates the rele-
vance, usefulness and diversity of our generated solutions. We
believe the C RASOLVER approach will help developers solve
crash bugs more efÔ¨Åciently and accurately in practice. The
code for our approach and the data of experiments are available
at http://tiny.cc/qzhqsz.
In the future, we plan to integrate C RASOLVER into an IDE
and provide timely solutions when developers encounter crash
bugs during development. Furthermore, we will extend C RA-
SOLVER to support crash traces based on more programming
languages and introduce more ofÔ¨Åcial documentation into its
knowledge base.
ACKNOWLEDGMENT
This research was partially supported by the National Key
R&D Program of China (No. 2019YFB1600700), Australian
Research Council‚Äôs Discovery Early Career Researcher Award
(DECRA) funding scheme (DE200100021), ARC Laureate
Fellowship funding scheme (FL190100035), ARC Discov-
ery grant (DP200100020), Key Research and Development
Program of Zhejiang Province (No.2021C01014), and the
National Research Foundation, Singapore under its Industry
Alignment Fund ‚Äì Prepositioning (IAF-PP) Funding Initiative.
Any opinions, Ô¨Åndings and conclusions or recommendations
expressed in this material are those of the author(s) and do not
reÔ¨Çect the views of National Research Foundation, Singapore.
REFERENCES
[1] Q. Gao, H. Zhang, J. Wang, Y . Xiong, L. Zhang, and H. Mei, ‚ÄúFixing re-
curring crash bugs via analyzing q&a sites (t),‚Äù in 2015 30th IEEE/ACM
International Conference on Automated Software Engineering (ASE) .
IEEE, 2015, pp. 307‚Äì318.
[2] S. Kim, K. Pan, and E. J. Whitehead Jr, ‚ÄúMemories of bug Ô¨Åxes,‚Äù in
Proceedings of the 14th ACM SIGSOFT international symposium on
Foundations of software engineering , 2006, pp. 35‚Äì45.[3] T. T. Nguyen, H. A. Nguyen, N. H. Pham, J. Al-Kofahi, and T. N.
Nguyen, ‚ÄúRecurring bug Ô¨Åxes in object-oriented programs,‚Äù in Pro-
ceedings of the 32nd ACM/IEEE International Conference on Software
Engineering-Volume 1 , 2010, pp. 315‚Äì324.
[4] B. Xu, Z. Xing, X. Xia, and D. Lo, ‚ÄúAnswerbot: Automated generation
of answer summary to developers‚Äô technical questions,‚Äù in 2017 32nd
IEEE/ACM International Conference on Automated Software Engineer-
ing (ASE) . IEEE, 2017, pp. 706‚Äì716.
[5] R. Silva, C. Roy, M. Rahman, K. Schneider, K. Paixao, and M. Maia,
‚ÄúRecommending comprehensive solutions for programming tasks by
mining crowd knowledge,‚Äù in 2019 IEEE/ACM 27th International Con-
ference on Program Comprehension (ICPC) . IEEE, 2019, pp. 358‚Äì368.
[6] R. Wu, H. Zhang, S.-C. Cheung, and S. Kim, ‚ÄúCrashlocator: locating
crashing faults based on crash stacks,‚Äù in Proceedings of the 2014
International Symposium on Software Testing and Analysis , 2014, pp.
204‚Äì214.
[7] S. Wang, F. Khomh, and Y . Zou, ‚ÄúImproving bug localization using
correlations in crash reports,‚Äù in 2013 10th Working Conference on
Mining Software Repositories (MSR) . IEEE, 2013, pp. 247‚Äì256.
[8] L. Moreno, J. J. Treadway, A. Marcus, and W. Shen, ‚ÄúOn the use of
stack traces to improve text retrieval-based bug localization,‚Äù in 2014
IEEE International Conference on Software Maintenance and Evolution .
IEEE, 2014, pp. 151‚Äì160.
[9] C.-P. Wong, Y . Xiong, H. Zhang, D. Hao, L. Zhang, and H. Mei, ‚ÄúBoost-
ing bug-report-oriented fault localization with segmentation and stack-
trace analysis,‚Äù in 2014 IEEE International Conference on Software
Maintenance and Evolution . IEEE, 2014, pp. 181‚Äì190.
[10] M. M. Rahman and C. K. Roy, ‚ÄúImproving ir-based bug localization
with context-aware query reformulation,‚Äù in Proceedings of the 2018
26th ACM Joint Meeting on European Software Engineering Conference
and Symposium on the Foundations of Software Engineering , 2018, pp.
621‚Äì632.
[11] Y . Wang, Y . Yao, H. Tong, X. Huo, M. Li, F. Xu, and J. Lu, ‚ÄúEnhancing
supervised bug localization with metadata and stack-trace,‚Äù Knowledge
and Information Systems , pp. 1‚Äì24, 2020.
[12] S. H. Tan, Z. Dong, X. Gao, and A. Roychoudhury, ‚ÄúRepairing crashes
in android apps,‚Äù in 2018 IEEE/ACM 40th International Conference on
Software Engineering (ICSE) . IEEE, 2018, pp. 187‚Äì198.
[13] Y . Dang, R. Wu, H. Zhang, D. Zhang, and P. Nobel, ‚ÄúRebucket: A
method for clustering duplicate crash reports based on call stack simi-
larity,‚Äù in 2012 34th International Conference on Software Engineering
(ICSE) . IEEE, 2012, pp. 1084‚Äì1093.
[14] S. Kim, T. Zimmermann, and N. Nagappan, ‚ÄúCrash graphs: An ag-
gregated view of multiple crashes to improve crash triage,‚Äù in 2011
IEEE/IFIP 41st International Conference on Dependable Systems &
Networks (DSN) . IEEE, 2011, pp. 486‚Äì493.
[15] J. Lerch and M. Mezini, ‚ÄúFinding duplicates of your yet unwritten bug
report,‚Äù in 2013 17th European Conference on Software Maintenance
and Reengineering . IEEE, 2013, pp. 69‚Äì78.
[16] K. K. Sabor, A. Hamou-Lhadj, and A. Larsson, ‚ÄúDurfex: a feature
extraction technique for efÔ¨Åcient detection of duplicate bug reports,‚Äù
in2017 IEEE international conference on software quality, reliability
and security (QRS) . IEEE, 2017, pp. 240‚Äì250.
[17] M. Asaduzzaman, A. S. Mashiyat, C. K. Roy, and K. A. Schneider,
‚ÄúAnswering questions about unanswered questions of stack overÔ¨Çow,‚Äù in
2013 10th Working Conference on Mining Software Repositories (MSR) .
IEEE, 2013, pp. 97‚Äì100.
[18] M. Iyyer, J. Boyd-Graber, L. Claudino, R. Socher, and H. Daum ¬¥e III,
‚ÄúA neural network for factoid question answering over paragraphs,‚Äù in
Proceedings of the 2014 conference on empirical methods in natural
language processing (EMNLP) , 2014, pp. 633‚Äì644.
[19] D. Chen, A. Fisch, J. Weston, and A. Bordes, ‚ÄúReading wikipedia to
answer open-domain questions,‚Äù arXiv preprint arXiv:1704.00051 , 2017.
[20] S. E. Robertson and S. Walker, ‚ÄúSome simple effective approximations
to the 2-poisson model for probabilistic weighted retrieval,‚Äù in SIGIR‚Äô94 .
Springer, 1994, pp. 232‚Äì241.
[21] S. Haiduc, G. Bavota, A. Marcus, R. Oliveto, A. De Lucia, and
T. Menzies, ‚ÄúAutomatic query reformulations for text retrieval in soft-
ware engineering,‚Äù in 2013 35th International Conference on Software
Engineering (ICSE) . IEEE, 2013, pp. 842‚Äì851.
[22] X. Yang, D. Lo, X. Xia, L. Bao, and J. Sun, ‚ÄúCombining word em-
bedding with information retrieval to recommend similar bug reports,‚Äù
in2016 IEEE 27Th international symposium on software reliability
engineering (ISSRE) . IEEE, 2016, pp. 127‚Äì137.
1296[23] Q. Le and T. Mikolov, ‚ÄúDistributed representations of sentences and
documents,‚Äù in International conference on machine learning , 2014, pp.
1188‚Äì1196.
[24] J. Carbonell and J. Goldstein, ‚ÄúThe use of mmr, diversity-based rerank-
ing for reordering documents and producing summaries,‚Äù in Proceedings
of the 21st annual international ACM SIGIR conference on Research and
development in information retrieval , 1998, pp. 335‚Äì336.
[25] ‚ÄúBeautiful soup,‚Äù https://www.crummy.com/software/BeautifulSoup/,
2020.
[26] E. Loper and S. Bird, ‚ÄúNltk: the natural language toolkit,‚Äù arXiv preprint
cs/0205028 , 2002.
[27] ‚ÄúWhoosh,‚Äù https://whoosh.readthedocs.io/en/latest/, 2012.
[28] ‚ÄúApache, lucene,‚Äù http://lucene.apache.org/, 2020.
[29] P. Bojanowski, E. Grave, A. Joulin, and T. Mikolov, ‚ÄúEnriching word
vectors with subword information,‚Äù Transactions of the Association for
Computational Linguistics , vol. 5, pp. 135‚Äì146, 2017.
[30] Y . Lv and C. Zhai, ‚ÄúAdaptive term frequency normalization for bm25,‚Äù
inProceedings of the 20th ACM international conference on Information
and knowledge management , 2011, pp. 1985‚Äì1988.
[31] H. Fang, T. Tao, and C. Zhai, ‚ÄúA formal study of information retrieval
heuristics,‚Äù in Proceedings of the 27th annual international ACM SIGIR
conference on Research and development in information retrieval , 2004,
pp. 49‚Äì56.
[32] E. Wong, J. Yang, and L. Tan, ‚ÄúAutocomment: Mining question and an-
swer sites for automatic comment generation,‚Äù in 2013 28th IEEE/ACM
International Conference on Automated Software Engineering (ASE) .
IEEE, 2013, pp. 562‚Äì567.
[33] C. D. Manning, M. Surdeanu, J. Bauer, J. R. Finkel, S. Bethard,
and D. McClosky, ‚ÄúThe stanford corenlp natural language processing
toolkit,‚Äù in Proceedings of 52nd annual meeting of the association for
computational linguistics: system demonstrations , 2014, pp. 55‚Äì60.
[34] ‚ÄúStack overÔ¨Çow data dump,‚Äù https://archive.org/download/
stackexchange, 2020.
[35] ‚ÄúJava se 8 api documentation,‚Äù https://www.oracle.com/java/
technologies/javase-jdk8-doc-downloads.html, 2020.
[36] R. Rehurek and P. Sojka, ‚ÄúSoftware framework for topic modelling with
large corpora,‚Äù in In Proceedings of the LREC 2010 Workshop on New
Challenges for NLP Frameworks . Citeseer, 2010.
[37] ‚ÄúCrokage tool,‚Äù http://isel.ufu.br:9000/, 2019.
[38] F. Wilcoxon, ‚ÄúIndividual comparisons by ranking methods,‚Äù in Break-
throughs in statistics . Springer, 1992, pp. 196‚Äì202.
[39] H. Abdi, ‚ÄúBonferroni and Àásid¬¥ak corrections for multiple comparisons,‚Äù
Encyclopedia of measurement and statistics , vol. 3, pp. 103‚Äì107, 2007.
[40] Q. Huang, X. Xia, Z. Xing, D. Lo, and X. Wang, ‚ÄúApi method
recommendation without worrying about the task-api knowledge gap,‚Äù in
2018 33rd IEEE/ACM International Conference on Automated Software
Engineering (ASE) . IEEE, 2018, pp. 293‚Äì304.
[41] D. Shepherd, K. Damevski, B. Ropski, and T. Fritz, ‚ÄúSando: an
extensible local code search framework,‚Äù in Proceedings of the ACM
SIGSOFT 20th International Symposium on the Foundations of Software
Engineering , 2012, pp. 1‚Äì2.
[42] J. Cambronero, H. Li, S. Kim, K. Sen, and S. Chandra, ‚ÄúWhen deep
learning met code search,‚Äù in Proceedings of the 2019 27th ACM Joint
Meeting on European Software Engineering Conference and Symposium
on the Foundations of Software Engineering , 2019, pp. 964‚Äì974.
[43] J. L. Fleiss, ‚ÄúMeasuring nominal scale agreement among many raters.‚Äù
Psychological bulletin , vol. 76, no. 5, p. 378, 1971.
[44] A. N. Lam, A. T. Nguyen, H. A. Nguyen, and T. N. Nguyen, ‚ÄúCombining
deep learning with information retrieval to localize buggy Ô¨Åles for
bug reports (n),‚Äù in 2015 30th IEEE/ACM International Conference on
Automated Software Engineering (ASE) . IEEE, 2015, pp. 476‚Äì481.
[45] M. Wen, R. Wu, and S.-C. Cheung, ‚ÄúLocus: Locating bugs from
software changes,‚Äù in 2016 31st IEEE/ACM International Conference
on Automated Software Engineering (ASE) . IEEE, 2016, pp. 262‚Äì273.
[46] C. Chen, Z. Xing, Y . Liu, and K. L. X. Ong, ‚ÄúMining likely analogical
apis across third-party libraries via large-scale unsupervised api seman-
tics embedding,‚Äù IEEE Transactions on Software Engineering , 2019.[47] L. Cai, H. Wang, B. Xu, Q. Huang, X. Xia, D. Lo, and Z. Xing, ‚ÄúAnswer-
bot: an answer summary generation tool based on stack overÔ¨Çow,‚Äù in
Proceedings of the 2019 27th ACM Joint Meeting on European Software
Engineering Conference and Symposium on the Foundations of Software
Engineering , 2019, pp. 1134‚Äì1138.
[48] D. Radev and W. Fan, ‚ÄúAutomatic summarization of search engine hit
lists,‚Äù in ACL-2000 Workshop on Recent Advances in Natural Language
Processing and Information Retrieval , 2000, pp. 99‚Äì109.
[49] S. Mani, R. Catherine, V . S. Sinha, and A. Dubey, ‚ÄúAusum: approach
for unsupervised bug report summarization,‚Äù in Proceedings of the ACM
SIGSOFT 20th International Symposium on the Foundations of Software
Engineering , 2012, pp. 1‚Äì11.
[50] A. Di Sorbo, S. Panichella, C. V . Alexandru, J. Shimagaki, C. A.
Visaggio, G. Canfora, and H. C. Gall, ‚ÄúWhat would users change in my
app? summarizing app reviews for recommending software changes,‚Äù in
Proceedings of the 2016 24th ACM SIGSOFT International Symposium
on Foundations of Software Engineering , 2016, pp. 499‚Äì510.
[51] R. Hao, Y . Feng, J. A. Jones, Y . Li, and Z. Chen, ‚ÄúCtras: Crowdsourced
test report aggregation and summarization,‚Äù in 2019 IEEE/ACM 41st
International Conference on Software Engineering (ICSE) . IEEE, 2019,
pp. 900‚Äì911.
[52] S. Rastkar, G. C. Murphy, and G. Murray, ‚ÄúSummarizing software
artifacts: a case study of bug reports,‚Äù in 2010 ACM/IEEE 32nd
International Conference on Software Engineering , vol. 1. IEEE, 2010,
pp. 505‚Äì514.
[53] S. Rastkar, G. C. Murphy, and G. Murray, ‚ÄúAutomatic summarization
of bug reports,‚Äù IEEE Transactions on Software Engineering , vol. 40,
no. 4, pp. 366‚Äì380, 2014.
[54] R. Lotufo, Z. Malik, and K. Czarnecki, ‚ÄúModelling the ‚Äòhurried‚Äôbug
report reading process to summarize bug reports,‚Äù Empirical Software
Engineering , vol. 20, no. 2, pp. 516‚Äì548, 2015.
[55] G. Uddin and F. Khomh, ‚ÄúAutomatic summarization of api reviews,‚Äù in
2017 32nd IEEE/ACM International Conference on Automated Software
Engineering (ASE) . IEEE, 2017, pp. 159‚Äì170.
[56] C. Treude, O. Barzilay, and M.-A. Storey, ‚ÄúHow do programmers ask
and answer questions on the web?(nier track),‚Äù in Proceedings of the
33rd international conference on software engineering , 2011, pp. 804‚Äì
807.
[57] P. C. Rigby and M. P. Robillard, ‚ÄúDiscovering essential code elements
in informal documentation,‚Äù in 2013 35th International Conference on
Software Engineering (ICSE) . IEEE, 2013, pp. 832‚Äì841.
[58] L. Ponzanelli, A. Bacchelli, and M. Lanza, ‚ÄúLeveraging crowd knowl-
edge for software comprehension and development,‚Äù in 2013 17th
European Conference on Software Maintenance and Reengineering .
IEEE, 2013, pp. 57‚Äì66.
[59] L. Ponzanelli, G. Bavota, M. Di Penta, R. Oliveto, and M. Lanza,
‚ÄúMining stackoverÔ¨Çow to turn the ide into a self-conÔ¨Ådent programming
prompter,‚Äù in Proceedings of the 11th Working Conference on Mining
Software Repositories , 2014, pp. 102‚Äì111.
[60] M. Martinez, T. Durieux, R. Sommerard, J. Xuan, and M. Monperrus,
‚ÄúAutomatic repair of real bugs in java: A large-scale experiment on the
defects4j dataset,‚Äù Empirical Software Engineering , vol. 22, no. 4, pp.
1936‚Äì1964, 2017.
[61] F. Long and M. Rinard, ‚ÄúAutomatic patch generation by learning correct
code,‚Äù in Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT
Symposium on Principles of Programming Languages , 2016, pp. 298‚Äì
312.
[62] S. Mechtaev, J. Yi, and A. Roychoudhury, ‚ÄúAngelix: Scalable multiline
program patch synthesis via symbolic analysis,‚Äù in Proceedings of the
38th international conference on software engineering , 2016, pp. 691‚Äì
701.
[63] W. Weimer, T. Nguyen, C. Le Goues, and S. Forrest, ‚ÄúAutomatically
Ô¨Ånding patches using genetic programming,‚Äù in 2009 IEEE 31st Interna-
tional Conference on Software Engineering . IEEE, 2009, pp. 364‚Äì374.
1297