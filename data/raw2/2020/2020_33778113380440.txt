POSIT: Simultaneously Tagging
Natural and Programming Languages
Profir-Petru PÃ¢rt ,achi
profir-petru.partachi.16@ucl.ac.uk
University College London
London, United KingdomSantanu Kumar Dash
s.dash@surrey.ac.uk
University of Surrey
Guildford, Surrey, United Kingdom
Christoph Treude
christoph.treude@adelaide.edu.au
University of Adelaide
Adelaide, South Australia, AustraliaEarl T. Barr
e.barr@ucl.ac.uk
University College London
London, United Kingdom
ABSTRACT
Softwaredevelopersuseamixofsourcecodeandnaturallanguage
texttocommunicatewitheachother:StackOverflowandDevelopermailinglistsaboundwiththismixedtext.Taggingthismixedtextisessentialformakingprogressontwoseminalsoftwareengineering
problems â€” traceability, and reuse via precise extraction of code
snippets from mixed text. In this paper, we borrow code-switching
techniquesfromNaturalLanguageProcessingandadaptthemto
apply to mixed text to solve two problems: language identification
and token tagging. Our technique, POSIT, simultaneously providesabstractsyntaxtreetagsforsource codetokens,part-of-speechtags
fornaturallanguagewords,andpredictsthesourcelanguageofa
tokeninmixedtext.TorealizePOSIT,wetrainedabiLSTMnetwork
withaConditionalRandomFieldoutputlayerusingabstractsyntax
treetagsfromtheCLANGcompiler andpart-of-speechtags from
theStandardStanfordpart-of-speechtagger. POSITimprovesthe
state-of-the-artonlanguageidentificationby10 .6%andPoS/AST
tagging by 23 .7% in accuracy.
CCS CONCEPTS
â€¢Generalandreference â†’Generalconferenceproceedings ;
â€¢Software and its engineering â†’Documentation ;Formal
language definitions.
KEYWORDS
part-of-speec hTagging,Mixed-Code,Code-Switching,Language
Identification
ACM Reference Format:
Profir-Petru PÃ¢r t,achi, Santanu Kumar Dash, Christoph Treude, and Earl
T.Barr.2020.POSIT:SimultaneouslyTaggingNaturalandProgramming
Languages . In 42nd International Conference on Software Engineering (ICSE
â€™20),May23â€“29,2020,Seoul,RepublicofKorea. ACM,NewYork,NY,USA,
11 pages. https://doi.org/10.1145/3377811.3380440
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
onthe firstpage.Copyrights forcomponentsof thisworkowned byothersthan the
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/or a fee. Request permissions from permissions@acm.org.
ICSE â€™20, May 23â€“29, 2020, Seoul, Republic of Korea
Â© 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-7121-6/20/05...$15.00
https://doi.org/10.1145/3377811.33804401 INTRODUCTION
Programmersoftenmixnaturallanguageandcodewhentalking
about the source code. Such mixed text is commonly found in mail-
inglists,documentation,bugdiscussions,andonlineforasuchas
StackOverflow.Searchingandminingmixedtextisinevitablewhen
tacklingseminalsoftwareengineeringproblems,liketraceability
andcodereuse.Mostdevelopmenttoolsaremonolingualorworkat
a levelof abstractionthat does notexploit language-specificinfor-
mation.Fewtoolsdirectlyhandlemixedtextbecausethedifferences
between natural languages and formal languages call for different
techniques and tools. Disentangling the languages in mixed text,
whilesimultaneouslyaccountingforcrosslanguageinteractions,
iskeytoexploitingmixedtext:itwilllaythefoundationfornew
tools that directly handle mixed text and enable the use of exist-
ing monolingual tools on pure text snippets extracted from mixed
text.Mixed-text-awaretoolingwillhelpbindspecificationstotheir
implementation or help link bug reports to code.
Themixedtexttagging problemisthetaskoftaggingeachtoken
inatextthatmixesatleastonenaturallanguagewithseveralformal
languages. It has two subproblems: identifying a tokenâ€™s origin
language (language tagging) and identifying the tokenâ€™s part of
speech(PoS) oritsabstractsyntaxtree(AST)tag(PoS/ASTtagging).
A token may have multiple PoS/AST tags. In the sentence â€œI foo-ed
the String â€˜Barâ€™.â€, â€˜fooâ€™ is a verb in English and a method name
(of an object of type String).Therefore, PoS/AST tagging involves
building amap that pairs alanguage to thetokenâ€™s PoS/AST node
in that language, for each language operative over that token.
We present POSIT to solve the 1+1 mixed text tagging problem:
POSITdistinguishesaNaturallanguage(English)fromprogram-
minglanguagesnippetsandtagseachtextorcodesnippetunderitslanguageâ€™sgrammar.Tothisend,POSITjointlysolvesboththelan-guagesegmentationandtaggingsubproblems.POSITemploystech-niquesfromNaturalLanguageProcessing(NLP)forcode-switched
1
text.Code-switchingoccurswhenmultilingualindividualssimul-
taneouslyusetwo(ormore)languages.Thishappenswhenthey
want to use the semantics of the embedded language in the host
language. Within the NLP space, such mixed text data tends tobe bi- and rarely tri-lingual. Unique to our setting is, as our datataught us, the mixing of more than three languages, one natural
1ThefactthattheNLPliteratureusesthewordâ€œcodeâ€intheirnamefortheproblem
ofhandlingtextthatmixesmultiplenaturallanguagesisunfortunateinourcontext.
They mean code in the sense of coding theory.
13482020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE)
and often many formal ones â€” in our corpus, many posts combine
a programming language, file paths, diffs, JSON, and URLs.
TovalidatePOSIT,wecompareittoPonzanelli et al.â€™spioneer-
ing work StORMeD [ 22], the first context-free approach for mixed
text. They use an island grammar to parse Java, JSON and XML
snippetsembeddedwithinEnglish.AsEnglishisrelegatedtowater,
StORMeD neglects natural language, builds ASTs for its islands,
then augments them with natural language snippets to create het-
erogenous ASTs. POSIT tags both natural languages and formal
languages,butdoesnotbuildtrees.Bothtechniquesidentifylan-
guageshiftsandbothtoolslabelcodesnippetswiththeirASTlabels.
POSITisdesignedfromthegrounduptohandle untagged mixed
textaftertraining.StORMeDlooksfortagsandresortstoheuristics
in their absence. On the language identification task, StORMeD
achieves anaccuracy of 71%; onthe same dataset, POSIT achieves
81.6%.TocompareStORMeDandPOSITonthePoS/ASTtagging
task, we extracted AST tags from the StORMeD output. Despite
not being designed for this task, StORMeD establishes the existing
stateoftheartandachieves61 .9%againstPOSITâ€™s85 .6%.POSIT
outperformsStORMeDhere,inpart,becauseitfindsmanymore
small code snippets in mixed text. In short, POSIT advances the
state-of-the-art on mixed text tagging.
POSIT is not restricted to Java. On the entire Stack Overflow
corpus(Javaandnon-Javaposts),POSITachievesanaccuracyof
98.7% for language identification and 96 .4% for PoS or AST tag-
ging. A manual examination of POSITâ€™s output on Stack Over-
flow posts containing 3,233 tokens showed performance consistent
withPOSITâ€™sresults ontheevaluationset: 95 .1%accuracy onlan-
guagetaggingand93 .7%onPoS/ASTtagging.Toassesswhether
POSITgeneralisesbeyonditstwotrainingcorpora,wemanually
validated it on e-mails from the Linux Kernel mailing list. Here,
POSIT achieved 76 .6% accuracy on language tagging and 76 .5% on
PoS/AST tagging.
POSITisdirectlyapplicabletodownstreamapplications.First,
itslanguageidentificationachieves95%balancedaccuracywhen
predictingmissedcodelabelsandcouldbethebasisofatoolthatau-tomaticallyvalidatespostsbeforesubmission.Second,TaskNav[
29]
is a tool that extracts mixed text for development tasks. POSITâ€™s
languageidentificationandPoS/ASTtaggingenablesTaskNavto
extractmorethantwonew,reasonabletasksperdocument:onacorpus of 30 LKML e-mails, it extracts 97 new tasks, 65 of which
are reasonable.
Our main contributions follow:
â€¢Wehavebuiltthefirstcorpusformixedtextthatistagged
at token granularity for English and C/C++.
â€¢Wepresent POSIT, anNLP-basedcode-switching approach
for the mixed text tagging problem;
â€¢POSITcandirectlyimprovedownstreamapplications:itcan
improve the code tagging of Stack Overflow posts and it
improves TaskNav, a task extractor.
We make our implementation and the code-comment corpus
used for evaluation available at https://github.com/PPPI/POSIT.
2 MOTIVATING EXAMPLE
The mix of source code and natural language in the various doc-
umentsproducedandconsumedbysoftwaredeveloperspresentsOn Fri, 24 Aug 2018 02:16:12 +0900 XXX<xxx@xxx.xxx>
wrote:
[...]Looking at the change that broke this we have:
<-diff removed for brevity->
Where " real" was added as a parameter to
__copy_instruction .Note that we pass in " dest
+ len" but not " real + len " as you patch fixes.
__copy_instruction was changed by the bad commit
with:
<-diff removed for brevity->
[...]
Figure 1: Example e-mail snippet from the Linux Kernelmailing list. It discusses a patch that fixes a kernel freeze.Here the fix is performed by updating the RIP address byadding lento the realvalue during the copying loop. Code
tokens are labelled by the authors using the patches as con-text and rendered using monospace.
WhereADV"real"string_literal
âˆ— wasVERB
addedVERBasADPaDETparameterNOUNtoADP
__copy_instructionmethod_name
âˆ— ..NoteNOUNthatADP
wePRONpassVERBinADPâ€œdest + lenâ€string_literal
âˆ—
butCONJnotADVâ€œreal + lenâ€string_literal
âˆ— asADPyouPRON
patchVERBfixesNOUN..__copy_instructionmethod_name
âˆ—
wasVERBchangedVERBbyADPtheDETbadADJ
commitNOUNwithADP:.
Figure2:POSITâ€™soutputfromwhichTaskNav++extractsthe
tasks(passinâ€œdest+lenâ€)and(passinâ€œreal+lenâ€).Weshowthe PoS/AST tags as superscript and mark tokens with
âˆ—if
they are identified as code. POSIT spots the two mention-roles of code tokens as â€˜string_literalâ€™s.
manychallengestotoolsthataimtohelpdevelopersmakesenseofthesedocumentsautomatically.AnexampleisTaskNav[
29],atool
that supports task-based navigation of software documentation by
automaticallyextractingtaskphrasesfromadocumentationcorpusandbysurfacingthesetaskphrasesinaninteractiveauto-complete
interface. For the extraction of task phrases, TaskNav relies on
grammatical dependencies between tokens in software documenta-tionthat,inturn,reliesoncorrectparsingoftheunderlyingtext.Tohandletheuniquecharacteristicsofsoftwaredocumentationcaused
by themix of codeand natural language,the TaskNav developers
hand-craftedanumberofregularexpressionstodetectcodetokens
as wellas anumberof heuristicsfor sentence completion,such as
addingâ€œThis methodâ€atthe beginningofsentences withmissing
subject.Theseheuristicsarespecifictoaprogramminglanguage
(PythoninTaskNavâ€™scase)andaparticularkindofdocument,such
as API documentation dominated by method descriptions.
1349POSIT has the potential to augment tools such as TaskNav to
reliablyextracttaskphrasesfromanydocumentthatmixescode
and natural language. As an example, in Figure 1, we can see an
e-mail excerpt from the LKML2. TaskNav only manages to extract
trivialtaskphrasesfromthisexcerpt(e.g.,â€œpatchfixesâ€)andmisses
task phrases related to the code tokens of dest,real, andlen
due to incorrect parsing of the sentence beginning with â€œNote
that...â€.AfteraugmentingTaskNavwithPOSIT,thenewversion,
which we call TaskNav++, manages to extract two additional task
phrases: (pass in â€œdest + lenâ€) and (pass in â€œreal + lenâ€); we present
POSITâ€™s output on this sentence in Figure 2. These additional task
phrases extracted with the help of POSIT will help developers find
resourcesrelevanttothetaskstheyareworkingon,e.g.,whenthey
are searching for resources explaining which parameters to use in
whichscenario.WediscusstheperformanceofTaskNav++inmore
detail in Section 6.2.
3 MIXED TEXT TAGGING
Tags are the non-terminals that produce terminals in a languageâ€™s
grammar. Given mixed text with ğ‘˜natural languages and ğ‘™formal
languages, let a tokenâ€™s tag map bind the token to a tag for each of
theğ‘˜+ğ‘™languages.Weconsideraformallanguagetobeonewhich,
to a first approximation, has a context-free grammar. The mixed
texttaggingproblem isthentheproblemofbuildingatokenâ€™stag
map.Forexample,inthesentence,â€œâ€™liebenâ€™meansloveinGermanâ€,
â€™liebenâ€™ is a subject in the frame language English and a verb in
German. Moving to a coding example, in a sentence such as â€œI foo-
ed the String â€™Barâ€™.â€, we observe â€™fooâ€™ to be a verb in English and a
method name (of an object of type String).
Ageneralsolutionproducesalistofpairs:part-of-speechtagsfor
each of the ğ‘˜natural languages together with the natural language
for which we have the tag, and AST tags for each of the ğ‘™formal
languagestogether withthe languagewithinwhich wehave the
ASTtag.Wealsoconsidertwospecialtags Î©andğœ–thatarefresh
relativetothesetofalltagswithinallnaturalandformallanguages.
Weuseğœ–toindicatethataparticularlanguagehasnocandidatetag,
while Î©ispairedwiththeoriginlanguage,answeringthefirsttask
ofourproblem.In thefirstexampleabove,â€™liebenâ€stagmap is[( Î©,
De), (Verb, De), (Noun, En), ( ğœ–, C)], if we consider English, German
andC.Inthecodeexample,â€™fooâ€™sâ€™tagmapis[( Î©,C),(ğœ–,De),(Verb,
En),(method_name,C)].Inmultilingualscenarios,atokenmight
have a tag candidate for every language.
The mixed text tagging problem is context-sensitive. We argue
below that determining the tokenâ€™s origin language is context-
sensitivefor asingletoken code-switch.The proofrestson aseries
of definitions from linguistics which we state next. To bootstrap, a
morpheme is an atomic unit of meaning in a language. Morphemes
differ from words in that they may or may not be free, or stand
alone. We source these definitions from Poplack [24].
â€œCode-switching isthealternationoftwolanguagesinasingledis-
course,sentenceorconstituent....[deletia]...[It]wascharacterised
accordingthedegreeofintegrationofitemsfromonelanguage( ğ¿1)
tothephonological,morphological,andsyntacticpatternsofthe
other(ğ¿2)â€[24,Â§2/pilcrow2].Weuse ğ¿1torefertotheframelanguageand
2https://lkml.org/lkml/2018/8/24/19ğ¿2totheembeddedone.Further,context-switchinghastworestric-
tionsonwhenitmayoccur.Itcanonlyoccurafterfreemorphemes.
Thesecondrestrictionisthatcode-switchingoccursatpointswhere
juxtapositions between ğ¿1andğ¿2do not violate the syntactic rules
ofeitherlanguage.Code-switchingallowsintegratingitemsfrom
ğ¿2intoğ¿1alonganyoneofphonological,morphological,orsyntac-
ticaxis,butnotallthreesimultaneously.Thislastcaseisconsidered
to be mono-lingual ğ¿1.
Adaptation occurs when an item from ğ¿2changes when used in
ğ¿1toobeyğ¿1â€™srules.Adaptationhasthreeforms:morphological,
phonological, and syntactical. Morphological adaptation represents
modifyingthespellingof ğ¿2itemstofit ğ¿1patterns. Phonological
adaptation represents changing the pronunciation of an ğ¿2item in
anğ¿1context.Syntactic adaptation represents modifying ğ¿2items
embeddedinadiscourse,sentence,orconstituentin ğ¿1toobeyğ¿1â€™s
syntax. Finally, ğ¿2items can be used in ğ¿1without adaptation .I n
this case, these items often reference the code-entity by name and
are used as a â€˜nounâ€™ in ğ¿1.
Wenowconsiderthreecases:(I) ğ¿2itemsaremorphologically
adapted to ğ¿1, (II)ğ¿2items are syntactically adapted to ğ¿1, and (III)
no adaptationof ğ¿2items occursbefore their use in ğ¿1.W ed on o t
consider phonological adaptation of ğ¿2items into ğ¿1as that is not
observable in text.
CaseI: MorphologicalAdaptation. Considerusing affixationto
convertfoo/class tofooâˆ’ify/verbto denote the action of con-
verting to the class foo. In this case, fooâˆ’ifybehaves as a bona
fide word in ğ¿1. Such examples obey the free-morpheme restric-
tion mentioned above. This enables it to be a separate, stand-alone
morpheme/item within ğ¿1. The juxtaposition restriction, further
ensures that this parses within ğ¿1. Lacking a context to indicate
fooâ€™s origin, a parsers would need to assume that it is from ğ¿1.
Case II: Syntactic Adaptation. This case manifests similarly to
morphological adaptation, such as tense agreement, or, potentially,
aswordorderrestrictions. Ifspellingchangesdooccur, thiscase
reprises themorphological adaptationcase. Ifthe onlyadaptation
iswordorder,thenthetaskbecomesspottinga ğ¿2tokenthathas
stayedunchangedina ğ¿1sentenceorconstituent.Thisisimpossible
in general if the two languageâ€™s vocabularies overlap.
Case III: No Adaptation. If no adaptation occurs, then the for-
maltokenoccursin ğ¿1.Thisreducestothesecondsubcaseofthe
syntactic adaptation case.
4 POSIT
POSIT starts from the biLSTM-CRF model presented in Huang
et al.[14],augmentsittohaveacharacter-levelencodingasseen
inWinata et al.[32]andaddstwolearningtargetsasinSotoand
Hirschberg[ 26].Figure4 presentstheresultingnetwork. Thenet-
work architecture employed by POSIT is capable of learning to
provide a language tag for any ğ‘˜+ğ‘™languages considered. This
model iscapable ofconsidering thecontext inthe inputusing the
LSTMs, it can bias its subsequence choices as it predicts tags based
on the predictions made thus far, and the character-level encoding
allows it to learn token morphology features beyond those that we
may expose to it directly as a feature vector.
1350(a) Character-level embeddings
(b) Feature vector embeddings
Figure 3: Computation of embeddings at the character level and from coding naming and spelling convention features. In
the bottom most layer, the circles represent an embedding operation on characters or features to a high-dimensional space.
The middle layer represents the forward LSTM and the top most layer â€” the backward LSTM. At the word level, character
and feature vector embeddings are represented by the concatenation of the final states of the forward and backward LSTMsrepresented by [Â·;Â·]in the diagrams above.ADV
ADP
DET
NOUN
raw_identifier
equal
numeric_constant
.
raw_identifier
equal
numeric_constant
.
raw_identifier
equal
numeric_constantSo
for
the
values
x
=
2
;
y
=
1
;
z
=
0forwardbackward
Figure 4: A representation of the neural network used forpredicting English PoS tags together with compiler derived
ASTtags.TheshadedcellsrepresentLSTMcells,arrowsrep-
resenttheflowofinformationinthenetwork.ThetoplayerrepresentsalinearConditionalRandomField(CRF)andthetransition probabilities are used together with a Viterbi de-codetoobtainthefinaloutput.Thefirstlayerisrepresentedby Equation (1) and converts the tokenised sentences into
vector representations.
Feature Space.
We rely on source code attributes to separate
codefromnaturallanguagewhiletaggingbothsimultaneously.Wederive vector embeddings for individual characters to model subtle
variations in how natural language is used within source code
snippets. Examples of such variations are numbered variables such
asi1ori2that often index axes during multi-dimensional array
operations. Another such variation arises in the naming of loop
control variables where the iterator could be referred to in diverse,
but related ways, as i,itoriter. These variations create out-of-
vocabulary(OOV)wordswhichinhibitmodellingofthemixedtext.
Theconfoundingeffectsofspellingmistakesandinconsistencies
in the NLP literature have been independently observed by Winata
et al.[32].TheyproposedabilingualcharacterbidirectionalRNNtomodel OOV words. POSIT uses this approach to capture character
level information and address diversity in identifier names.
Additionally,weconsiderthestructuralmorphologyoftheto-
kens.Codetokensarerepresenteddifferentlytonaturallanguage
tokens. This is due to coding conventions in naming variables. We
utilise these norms in developing a representation for the token.
Specifically, we encode common conventions and spelling features
intoafeaturevector.Werecordifthetokenis:(1)UPPERCASE,(2)
Title Case, (3)lower case, (4) CamelCase,(5) snake_case; orif any
character: (6) other than the first one is upper case, (7) is a digit,
or(8)isasymbol.Itmaysurpriseyouthatfont,whileoftenused
by humans to segment mixed text, is not in our token morphology
featurevector.Wedidnotuseitasitisnotavailableinourdatasets.Forthepurposesofcodereuse,weuseasequentialmodeloverthis
vector as well, similar to the character level vector, although there
is no inherent sequentiality to this data. By ablating the high-level
modelfeatures,wefoundthatthistokenmorphologyfeaturevector
did not significantly improve model performance (Section 5.3).
Encoding and Architecture. At a glance, our network, which
we present diagrammatically in Figure 4, works as follows:
x(ğ‘¡)=[fw(ğ‘¤ğ‘¡);fc(ğ‘¤ğ‘¡);ff(ğ‘¤ğ‘¡)], (1)
h(ğ‘¡)=ğ‘“(Wx(ğ‘¡)+Uh(ğ‘¡âˆ’1)), (2)
y(ğ‘¡)=ğ‘”(Vh(ğ‘¡)). (3)
In Equation (1), we have three sources of information: character-
level encodings ( fc(ğ‘¤ğ‘¡)), token-level encodings ( fw(ğ‘¤ğ‘¡)) and a fea-
ture vector over token morphology ( ff(ğ‘¤ğ‘¡)). Each captures proper-
ties at a different level of granularity. To preserve information, we
embed each source independently into a vector space, represented
bythethree ğ‘“functions.Forboththefeaturevectorandthecharac-
ters withina word,we computea representationby passingthem
as sequences through the biLSTM network in Figure 3. This figure
representstheinternalsof fc(ğ‘¤ğ‘¡)andff(ğ‘¤ğ‘¡)fromEquation(1)and
allows the model to learn patterns within sequences of characters
as well as coding naming or spelling conventions cooccurrence
1351patterns.TheresultsofthesetwobiLSTMstogetherwithaword
embedding function fware concatenated to become the input to
the mainbiLSTM, x(ğ‘¡)in Equation (1). Thisenables thenetwork
tolearn,basedonacorpus,semanticsforeachtoken.Thisvector
represents the input cells in our full network overview in Figure 4,
which is enclosed in the box.
Wepasstheinputvector x(ğ‘¡)throughabiLSTM.ThebiLSTM
considers both left and right tokens when predicting tags. Each
cell performs the actions of Equation (2) and Equation (3), with
the remark that the backwards-LSTM has the index reversed. This
allows the network to consider context up to sentence boundaries.
We then make use of the standard softmax function:
softmax(z)ğ‘—=ğ‘’ğ‘§ğ‘—
Î£ğ¾
ğ‘˜=1ğ‘’ğ‘§ğ‘˜forğ‘—=1,...,ğ¾; (4)
which allows us to generate output probabilities over our learning
targets as such:
p(tagğ‘¡|tagğ‘¡âˆ’1)=softmax(y(ğ‘¡)), (5)
p(lidğ‘¡|lidğ‘¡âˆ’1)=softmax(2LP(h(ğ‘¡))), (6)
Equation (6) represents language ID transition probabilities, andEquation (5) â€” tag transition probabilities. In Equation (6),
2LP
represents a 2-layer Multi Layer Perceptron. We make use of these
transition probabilities in the CRF layer to output Language IDs
andtagsforeachtokenwhileconsideringpredictionsmadethusfar.
ThetrainedeyemayrecogniseinEquation(5)andEquation(6)the
transition probabilities of two Markov chains. Indeed, we obtain
the optimal output sequence by Viterbi-decoding [ 30]. While Equa-
tion(5)mayseemtoindicatethatonlysingletagscanbeoutputby
thisarchitecture,thisisnottrue.Givenenoughdata,wecanmap
tuples of tags to new fresh tags and decode at output time. This
may not be as efficient as performing multi-tag output directly.
To train the network, we use the negative log-likelihood of
the actual sequence being decoded from the CRF network and
we backpropagate this through the network. Since we have two
traininggoals,wecombinetheminthelossfunctionbyperforming
a weighted sum of the negative log-likelihood losses for each indi-
vidual task, then train the network to perform both tasks jointly.
Whendeployed,POSITmakesuseoftheCLANGlexerpythonport
to generate the token input required by ğ‘“ğ‘¤,ğ‘“ğ‘, andğ‘“ğ‘“.
5 EVALUATION
For each token, POSIT makes two predictions: language IDs andPoS/AST tags. The former task represents correctly identifying
where to add </code>-tags. This measures how well POSIT seg-
ments English and code. Section 5.2 reports POSITâ€™s performance
onthistask ontheevaluationset.ForPoS/AST tagprediction,we
focus on POSITâ€™s ability to provide tags describing the function
of tokens for both modalities reliably. To measure POSITâ€™s perfor-
mancehere,weconsiderhowwellthemodelpredictsthetagsfora
withheldevaluationdataset,whichSection5.2.presentsalongwith
the English-code segmentation result.
POSIT implements the network discussed in Section 4 in Ten-
sorFlow[ 4].ItusestheAdaptiveMomentEstimation(Adam)[ 16]
optimiser to assign the weights in the network. We trained it upto 30 epochs or until we did not observe improvement in three
consecutiveepochs.Weusedmicro-batchesof64,alearningrateof 10âˆ’2, and learning decay rate of 0 .95. We use a 100 dimensional
word embedding space and a 50 dimensional embedding space for
characters. The LSTM hidden state is 96 dimensional for the word
representation,48dimensionalforcharactersand4forthetoken
morphologyfeaturevector.TheoutputofthetagCRFisthecon-
catenationofallfinalbiLSTMstates.Weusea2layerperceptron
with64and8dimensionalhiddenlayersforlanguageIDprediction.
We apply a dropout of 0 .5. Section 5.2 uses this implementation for
validation andSection 5.3uses itfor ablation.The modelâ€™ssource
code is available at https://github.com/PPPI/POSIT.
AllPOSITruns,trainingandevaluation,wereperformedona
high-end laptop using an Intel i7-8750H CPU clocked at 3.9GHz,
24.0 GB of RAM and a Nvidia 1070 GPU with 8 GB of VRAM.
Thestate-of-the-arttoolStORMeD,whichweuseforcomparison,
isavailableasawebservice,whichweusebyaugmentingthedemo
files made available at https://stormed.inf.usi.ch/#service.
5.1 Corpus Construction
For our evaluation, we make use of two corpora. We use both totrain POSIT, and we evaluate on each to see the performance in
twoimportantuse-cases,anaturallanguageframelanguagewith
embedded code and the reverse. Table 1 presents their statistics.
The first corpus is the Stack Overflow Data-dump [ 2] that Stack
Overflow makes available online as an XML-file. It contains theHTML of Stack Overflow posts with code tokens marked using
</code>-aswellas </pre class="code"> -tags.Thesetagsenable
us to construct a ground-truth for the English-code segmentation
task.ToobtainthePoStagsforEnglishtokens,weusethetokeniser
andStandardStanford part-of-speec htaggerpresentinNLTK[ 8].
ForAST tags,we usea pythonport of theCLANG lexerand label
tokensusingafrequencytablebuiltfromthesecond,CodeComment
corpus. This additionally ensures that both corpora have the same
setofASTtags.WeallowmatchesuptoaLevensteindistanceof
three for them; we choose three from spot-checking the results
of variousdistances: afterthree, thelists were longand noisy.We
address the internal threat introduced by our corpus labelling in
Section 7.2.
Webuiltthesecond,CodeCommentcorpus[ 3],fromtheCLANG
compilationof11nativelibrariesfromtheAndroidOpenSource
Project (AOSP): boringssl, libcxx, libjpeg-turbo, libmpeg2, libpcap,
libpng,netcat,netperf,opencv,tcpdumpandzlib.Wechosethese
libraries in a manner that diversifies across application areas, such
as codecs, network utilities, productivity, and graphics. We wrote a
CLANG compiler plugin to harvest all comments and the snippets
in the source code around those comments. Our compiler pass
furtherharveststokenASTtagsforindividualtokensinthesource
codesnippets.In-linecommentsareoftenpureEnglish;however,
documentation strings, before the snippets with which they areassociated, contain references to code tokens in the snippet. Wefurther process the output of the plugin offline where we parse
doc-strings to decompose intra-sentential mixed text and add part-
of-speech tags to the pure English text. Thus, by construction, we
havebothtagandlanguageIDground-truthdata.Weallowmatches
up to 3 edits away to account for misspellings that may exist indoc-strings. The former ground-truth is obtained from CLANG
duringthecompilationoftheprojects,whileEnglishcommentsare
1352Table 1: Corpus statistics for the two corpora considered together with the Training and Development and Evaluation splits.
We performed majority class (English) undersampling only for the Stack Overflow training corpus.
Corpus Name Tokens Sentences English Only Sentences Code Only Sentences Mixed Sentences
Train&Dev Eval Train&Dev Eval Train&Dev Eval Train&Dev Eval Train&Dev Eval
Stack Overflow 7645103 2612261 214945 195021 55.8% 57.0% 32.6% 38.0% 11.6% 4.9%
CodeComments 132189 176418 21681 8677 11.3% 11.0% 79.4% 79.6% 9.4% 9.3%
Total 7777292 2788679 236626 203698 51.7% 55.1% 36.9% 39.7% 11.4% 5.1%
tokenised and labelled using NLTK as above. For code tokens in
comments,weoverridetheirlanguageIDandtagusinginformation
about them from the snippet associated with the comment.
A consequence of using CLANG to source our AST tags is that
we are limited to the mixed text tagging problem with a single
natural language ( ğ‘˜=1) and a single formal language ( ğ‘™=1). This
limitation is a property of the data and not the model.
5.2 Predicting Tags
Here, we explore POSITâ€™s accuracy on the language identification
andPoS/ASTtaggingsubtasksofthemixedtexttaggingproblem.
We adapt StORMeD for use as our baseline and compare its perfor-
mance against that of POSIT. We note, even after adaption for the
ASTtaggingtaskforwhichitwasnotdesigned,StORMeDistheex-
isting state-of-the-art. We close by reporting POSITâ€™s performance
on non-Java posts.
TocomparewiththeexistingtoolStORMeD,werestrictourStack
OverflowcorpustoJavaposts,becausePonzanelli et al.designed
StORMeD to handle Java, JSON, and XML. Further, StORMeD and
POSIT do not solve the same problems. StORMeD parses mixedposts into HAST trees; POSIT tags sequences. Thus, we flatten
StORMeDâ€™s HASTs and use the AST label of the parent of terminal
nodesasthetag.BecauseStORMeDbuildsHASTsforJava,JSON
or XMLwhile POSIT uses CLANGto tag code tokens, webuilt a
mapfromStORMeDâ€™sASTtagsettoours3.Asthismappingmay
beimperfect,StORMeDâ€™sobservedperformanceonthePoS/AST
tagging task is a lowerbound (Section 7.2).
Forthelanguageidentificationtask,StORMeDexposesaâ€™taggerâ€™
webservice.GivenmixedtextinHTML,itreplieswithastringthat
has</code>HTML-tagsadded. Weparse thisreply toobtainour
token-levellanguagetagsasinSection5.1.ForPoS/ASTtagging,
StORMeDexposesaâ€™parseâ€™webservice.GivenaStackOverflowpost
withwithcorrectlylabelledcodeinHTML(Section5.1),thisservice
generatesHASTs.WeflattenandtranslatetheseHASTsasdescribed
above. To use these services, we break our evaluation corpus upinto2000callstoStORMeDâ€™swebservices,1000forthelanguage
identificationtask,andtheother1000forHASTgeneration.This
allows us to comply with its terms of service.
LanguageTagging. Here,wecomparehowwellStORMeDand
POSITsegmentEnglishandcodeintheJavaStackOverflowcorpus.
Unlike StORMeDâ€™s original setting, we elide user-provided code
tokenlabels,bothfromStORMeDand POSITtoavoiddataleakage.
Predicting them is the very task we are measuring. The authors of
3The mapping can be found online at https://github.com/PPPI/POSIT/blob/
92ef801e5183e3f304da423ad50f58fdd7369090/src/baseline/StORMeD/stormed_
evaluate.py#L33.StORMeDaccountforthisscenario[ 22,Â§II.A].AlthoughStORMeD
must initially treat the input as a text fragment node, StORMeD
stillrunsanislandgrammarparsertofindcodesnippetsembedded
within it. Despite being asked to perform on a task for which it
wasnotdesigned,duetotheelisionofuser-providedcodelabels,
StORMeD performs very well on our evaluation set and, indeed, as
pioneering,post-regexwork,definedthepreviousstateoftheart
on this task. In this setting, StORMeD obtains 71% accuracy, POSIT
achieves 81 .6%.
PoS/ASTTagging. Here,weuseStORMeDasabaselinetobench-
mark POSITâ€™s performance on predicting PoS/AST tags for each
token.Granted,onthetextfragmentnodes,weareactuallymea-
suring the performance of the NLTK PoS tagger. Unlike the first
task,we allowStORMeD touseuser-provided code-labelsforthis
subtask. POSIT, however, solves the two subtasks jointly, so giving
it these labels as input remains a data leak. Therefore, we do not
provide them to POSIT. After flattening and mapping HAST labels
toourlabeluniverse,asdescribedabove,StORMeDachievesamore
than respectable accuracy of 61 .9%, while POSIT achieves 85 .6%.
Onauniformsamplesetof30postsfromqueriestoStORMeD,
weobservedStORMeDtostrugglewithsinglewordtokensorother
shortcodesnippetsembeddedwithinasentence,especiallywhen
these, like foo,bar, do not match peculiar-to-code naming conven-
tions.WhilethisisalsoamoredifficulttaskforPOSITaswell,it
fares better. Consider the sentence â€˜ Class Ahas a one-to-many
relationshipto B.Hence,Ahasanattribute collectionOfB .â€™.Here,
StORMeDspots Class AandcollectionOfB ,theusesof AandBas
stand-alonetokensslipspassedtheheuristics.POSITmanagesto
spot all four code tokens. POSITâ€™s use of word embeddings, allows
it to learn typical one word variable names and find unmarkedcodetokensthatescapeStORMeDâ€™sheuristics,suchasalllower-
case function names that are defined in a larger snippet within the
post.Foritspart,StORMeDhandleddocumentationstringswell,
identifyingwhencodetokensarereferencedwithinthem.POSIT
preferredtotreatthedoc-stringasbeingfullyinanaturallanguage,
missingcodereferencesthatexistedwithinthemevenwhenthey
contained special mark-up, such as @.
Beyond Java, JSON, and XML. POSIT is not restricted to Java,
so we report its performance on the entire Stack Overflow corpus
andontheCodeCommentcorpus.Theformermeasurestheperfor-
manceonmixedtextwhichhasEnglishasaframelanguage;the
lattermeasurestheperformanceonmixedtextwithsourcecodeastheframelanguage.OnthecompleteStackOverflowcorpus,POSIT
achieves an accuracy of 97 .7% when asked to identify the language
of the token and an accuracy of 93 .8% when predicting PoS/AST
tags.Wecalculatedthefirstaccuracyagainstuser-providedcode
1353tags and the second against our constructed tags (Section 5.1). On
theCodeCommentcorpus,wetweakPOSITâ€™straining.Asexam-
ples within this corpus tend to be longer, we reduce the numberof micro-batches to 16. After training on CodeComment, POSITachieves an accuracy of 99
.7% for language identification and an
accuracy of 98 .9% for PoS/AST tag predictions.
5.3 Model Ablation
POSITdependsonthreekindsofembeddingsâ€”character,token,
andtokenmorphologyâ€”andCRFlayerpriortodecoding(Equa-
tion(1)).Wecanablateallexceptthetokenembeddings,ourbedrockembedding.Weusedthesameexperimentalset-updescribedatthe
beginningofthissection,withoneexception:Whenablatingthe
CRF layer, we replaced it with a 2-Layer Perceptron whose output
we then apply softmax to.
Table2showstheresults.KeepingonlytheCRF-layerreduces
thetimeperepochfrom3:30hoursto1:03hours(thefirstbolded
row).Onaverage,POSITâ€™smodelrequires6to7epochsuntilitstops
improvingon thedevelopmentset, sowestop. Thisconfiguration
reduces training time by âˆ¼14 hours. Further, it slightly increases
performance. Only using the CRF, however, manual spot-checking
reveals that POSIT incorrectly assigns token that obey common
coding conventions and method call tokens as English. This is due
toEnglishtocodeclassimbalance,andinspectingTable1makes
this clear. The best performing model under human assessment of
uniformly sampled token (the second bolded row) removes only
the token morphology feature vector. Essentially, this model drops
precisely those heuristics that we anecdotally know humans use
when performing this task. Since dropping either the character or
the token morphology embeddings yields almost identical perfor-
mance, we hypothesise that POSIT learns these human heuristics,
and perhaps others, in the character embeddings. We choose to
keep character embeddings, despite training cost, for this reason.
6 POSIT APPLIED
POSIT can improvedownstream tasks. First, weshow how POSIT
accurately suggests code tags to separate code from natural lan-guage, such as Stack Overflowâ€™s backticks. POSIT achieves 95%balanced accuracy on this task. Developers could use these accu-rate suggestions to improve their posts before submitting them;researchers could use them to preprocess and clean data beforeusing it in downstream applications. For instance, Yin
et al.[33]
start from a parallel corpus of Natural Language and Snippet pairs
and seek to align it. POSIT could help them extend their initial
corpus beyond StackOverflow by segmenting mixed text into pairs.
Second,weshowhowPOSITâ€™slanguageidentificationandPoStag-
ging predictions enable TaskNav â€” a tool that supports task-based
navigation of software documentation by automatically extracting
task phrases from a documentation corpus and by surfacing these
task phrases in an interactive auto-complete interface â€” to extract
new and more detailed tasks. We conduct these demonstrations
using POSITâ€™s best performing configuration, which ignores token
morphology (Section 5.3).6.1 Predicting Code Tags
Moderndeveloperfora,notablyStackOverflow,providetagsforseparating code and NL text. These tags are an unusual form of
punctuation, so it is, perhaps, not surprising that developers often
neglect to add them. Whatever the reason, these tags are oftenmissing [
21]. POSIT can help improve post quality by serving as
the basis of a post linter that suggests code tags. A developer could
usesuchalinterbeforesubmittingtheirpostortheservercould
use this linter to reject posts.
Our Stack Overflow corpus contains posts that have been edited
solelytoaddmissingcodetags.ToshowPOSITaccuracyatsuggest-ingmissingcodetags,weextractedthesepostsusingtheSOTorrent
dataset [6]. First, we selected all posts that contain a revision with
the message â€œcode formattingâ€. We uniformly, and without replace-
ment, sampled this set for 30 candidates. We kept only those posts
thatmadewhitespaceeditsandintroducedsingleortriplebackticks.
By construction, this corpus has a user-defined ground truth for
codetags.Weusethepostbeforetherevisionasinputandcompare
against the post after the revision to validate. POSIT manages toachieve a balanced accuracy of 95% on the code label prediction
task on this corpus.
6.2 TaskNav++
To demonstrate the usefulness of POSITâ€™s code-aware part-of-
speechtagging, we augment Treude et al.â€™s TaskNav [ 29]t ou s e
POSITâ€™s language identification and its PoS/AST tags.
ToconstructTaskNav++,wereplacedTaskNavâ€™sStanfordNLP
PoS tagger with POSIT. Like TaskNav, TaskNav++ maps AST tags
to â€œNNâ€. TaskNav uses the Penn Treebank [ 17] tag set; POSIT uses
trainingdatalabelledwithUniversaltagsettags[ 19].Thesetags
sets differ; notably, the Penn Treebank tags are more granular.
To expose POSITâ€™s tags to TaskNavâ€™s rules to use those rules inTaskNav++, we converted our tags to the Penn Treebank tag set.
ThisconversionharmsTaskNav++â€™sperformance,becauseituses
the Java Stanford Standard NLP library which expects more granu-
lar tags, although it can handle the coarser tags POSIT gives it.
To compare TaskNav and TaskNav++, we asked both systems to
extract tasks from the same Linux Kernel Mailing List corpus that
we manually analysed (Section 7.1). TaskNav++ finds 97 new tasks
inthe30threadsor3 .2newtasksperthread.Ofthese,65(67 .0%)
are reasonable tasks for the e-mail they were extracted from. Twoof the authorsperformed the labelling of thesetasks, we achieved
a Cohen Kappa of 0 .21, indicating fair agreement. Treude et al.
alsoreportlowagreementregardingwhatisarelevanttask[ 29].
Toresolvedisagreements,weconsiderataskreasonableifeither
author labelled it as such. The ratio of reasonable tasks is in the
samerangeasthatreportedintheTaskNavwork, viz.71%ofthe
tasks TaskNav extracted from two documentation corpora wereconsidered meaningful by at least one of two developers of therespective systems. TaskNav prioritises recall over precision toenable developers to use the extracted tasks as navigation cues.POSITâ€™s ability to identify more than two additional reasonable
tasks per email thread contributes towards this goal.
Inspectingthetasksextracted,wefindthatsometasksbenefit
fromPOSITâ€™stokenisation.Forexampleinâ€˜removeexcessiveuntag-
ging in gupâ€™ vs â€˜remove excessive untagging in gup.câ€™ the standard
1354Table2:Theresultontheevaluationsetforthedifferentablationconfigurations.Allconfigurationsusethetokenembedding
asitisourcoreembedding.ObservethatusingonlytheCRFlayerperformsbestonthelanguageidentificationandthePOSITâ€™s
tagging tasks.
High-level features Language ID Accuracy Tagging Accuracy Mean Accuracy Time per Epoch (hh:mm)
Only token embeddings 0.209 0.923 0.568 00:37
Only CRF 0.970 0.926 0.948 01:03
Only feature vector 0.450 0.928 0.689 00:45
Only character embeddings 0.312 0.923 0.617 02:32
No CRF 0.409 0.913 0.661 02:59
No feature vector 0.966 0.924 0.945 03:19
No character embeddings 0.966 0.919 0.943 01:39
All features 0.970 0.917 0.944 03:30
tokeniser assumed that the use of â€˜.â€™ in â€˜gup.câ€™ indicates the end of
a sentence. Our tokenisation also helps correctly preserve mention
uses of code tokens: â€˜pass in â€œreal + lenâ€â€™ and â€˜pass in â€œdest + lenâ€â€™,
and even English-only mention uses: â€˜call writeback bits â€œtagsâ€â€™,
â€˜split trampoline buffer into â€œtemporaryâ€ destination bufferâ€™. In all
thesecases,eitherTaskNavfindsanincorrectversionofthetask
(â€˜add len to real ripâ€™) or simply loses the double-quotes indicating a
mention use (for the English-only mention cases).
POSITâ€™s restriction to a single formal language proved to be a
double-edged sword. It helped separate patches that are in-lined
withe-mailsinourmanualanalysisof POSITontheLKML(Sec-
tion 7.1), while here we can see that it is problematic. By train-ing only on a single programming language, POSIT misidenti-fies change-log and file-path lines as code. This propagates to
TaskNav++, which in turn incorrectly adds these as tasks since
POSITstashesthepathorchange-logintoasinglecodeelement.
Attimes,thisbehaviourwasalsobeneficial,suchasannotatingthe
codeinthetask:â€˜read<tt>extent[i]</tt>â€™,thiscomesatthecost
ofgeneratingincorrecttaskssuchas:â€˜changeandroid<tt>/ion/
ion.c | 60 +++ +++ +++ +++ +++ +++ +</tt>â€™. We hypothesise that
a solution to the general mixed text tagging problem would avoid
this problem by explicitly training to identify file paths.
7 DISCUSSION
In this section, we first perform a deep dive into POSITâ€™s outputand performance. Then we address threats to POSITâ€™s model, its
training, and methodology.
7.1 POSIT Deep Dive
POSITisunlikelytobethelasttooltotacklethemixedtexttagging
problem. To better understand what POSIT does well and where it
can be improved, we manually assessed its output on two corpora:
a random uniform sample of 10 Stack Overflow posts from our
evaluationsetandarandomuniformsampleof10e-mailsfromtheLinuxKernelMailingListsentduringAugust2018.TheStackOver-
flowsamplecontains3,233tokenswhiletheLKMLâ€”17,451.We
finishby showing POSITâ€™soutput ona smallStack Overflowpost.
Broadly, POSITâ€™s failures are largely due to tokenisation problems,
classimbalance,andlackoflabels.Concerningthelabelproblem,
our data actually consists of a single natural language and several
formallanguages,oneofwhichisaprogramminglanguage,theoth-ersincludediffs,URLs,mailheaders,andfilepaths.ThisnegativelyimpactedTaskNav++byexposingdiffheadersandfilepathsascode
elements,inducingincorrecttaskstobeextracted.Ourdeepdive
alsorevealedthatPOSITaccuratelyPoS-tagsEnglish,accurately
AST-tags lone code tokens, and learned to identify diffs as formal,
despite lack of labels.
Topre-processtrainingdata,POSITusestwotokenisers:thestan-
dardNLTKtokeniserandaPythonportoftheCLANGlexer.POSIT
useslabels(StackOverflowâ€™scodetags)inthetrainingtoswitchbetween them. In the data, we observed that POSIT had tagged
somedouble-quotationmarksasNouns.Sincetheuser-provided
code labels are noisy [ 21], we hypothesise the application of the
codetokenisertoEnglishcausedthismisprediction.Designedto
dispense with code-labels, POSIT exclusively relies on the CLANG
lexer port during evaluation. Unsurprisingly, then, we observed
POSITincorrectlytaggingpunctuationascode-unknownasmul-
tiple punctuation tokens are grouped into single tokens that do
notnormallyexistinEnglish.Wesuspectthistoduetoapplying
Englishtokenisationtocodesnippets.Clearly,POSITwouldbenefit
from tokenisation tailored for mixed text.
Withincodesegments,wealsoobservedthatPOSIThadapro-
clivity to tag tokens as â€˜raw_identifierâ€™. This indicates that context
did not always propagate the â€˜method_nameâ€™, or â€˜variableâ€™ tagsacross sentence boundaries. As the â€˜raw_identifierâ€™ tag was the
go-to AST label for code, it suggests a class imbalance in our train-
ingdatawithregardstothislabel.Indeed,weobservedPOSITto
only tag a token as â€˜method_nameâ€™ if it was followed by tokens
thatsignifycallingsyntaxâ€”argumentlists,includingtheempty
argument list ().
This deep dive rev ealed a double-edged sword. Our sample con-
tained snippets that represent diffs, URLs or file paths. POSITâ€™s
trainingdatadoesnotlabelstheseformallanguagesnordidtokeni-
sation always preserve URLs and file paths. Nonetheless, POSIT
managed to correctly segment diffs by marking them as code, per-
formingthistaskexceptionallywellontheLKMLsample.URLsandfilepathswereseenasEnglishunlesstheresourcenamesmatchedanamingconventionforcode.ForURLs,POSITtaggedkey-argument
pairs (post_id=42) as (â€˜variableâ€™, â€˜operationâ€™, â€˜raw_identifierâ€™). Later
inSection6.2,POSITâ€™stendencytosegmentdiffsascodewasdetri-mental,sinceitstasheddiffheadersintoasinglecodetoken,causing
TaskNav++ to produce incorrect tasks.
1355Anadditionalobservationduringourmanualinvestigationisthe
incorrecttypeoftagrelativetothelanguageofthetoken.Consider
the following:
English Code
PoS output 33.4% 1.5%
AST output 5.6% 59.5%
Weobtainthesenumbersbyconsideringtheagreementbetween
thelanguageidentificationtaskandthetypeoftagoutputforthe
StackOverflowpostsandLKMLmailsusedinthisdeepdive.We
canseethatfor7.1%ofthetokens(908)inourmanualinvestigation
POSIT outputs the wrong type of label given the language predic-
tion. This was also observed by the authors for cases where one
of the predictions was wrong while the other was correct, such as
taggingaNounassuchwhilemarkingitascode.Thisisbecause
weseparatedthetwotasksafterthebiLSTMandtrainedtheminde-
pendently. We hypothesise that adding an additional loss term that
penalises desynchronising these tasks would solve this problem.
Alternatively, one could consider a more hierarchical approach, for
examplefirstpredictingthelanguageid,thenpredicatingthetag
output conditioned on this language id prediction.
Formonolingualsentences,eitherEnglishorcode,POSITcor-
rectly PoS- or AST-tagged the sequences. Spare the occasional
hiccup at switching from English to code a single token too late,
POSITcorrectlydetectedthelargercontiguoussnippets.Ascode
snippetsended,POSITwasalmostalwaysimmediatetoswitchback
toidentifyingtokensasEnglish.For smallerembeddedcodesnip-
pets,POSITcorrectlyidentifiedalmostallmethodcallsthatwere
followedbyargumentlists,includingâ€˜()â€™.POSITalmostalwayscor-
rectly identified operators and keywords even when used on their
owninamentionroleinthehostlanguage.Further,singletoken
mentions of typical example function names, like fooorbar,c od e
elements that followed naming conventions, or code tokens that
wereusedinlargersnippetswithinthesamepostwerecorrectly
identified as code.
In Figure 5, we observe 91% tag accuracy for English and 66 .7%
tagaccuracyforcode.Thelanguagesegmentationis76 .7%accurate.
POSIT correctly identifies the two larger code snippets as code
exceptforthefirsttokenineach:â€˜#defineâ€™andâ€˜ifâ€™.Itfailstospot
do ... while as code, perhaps due to doandwhilebeing used
within English sufficiently often to obscure the mention-role of the
construct. On the other hand, it correctly spots f(X)as code since
f and X are rarely used on their own in English.
7.2 Threats to Validity
TheexternalthreatstoPOSITâ€™svalidityrelatemainlytothecorpora,
including the noisy nature of StackOverflow data [ 20], and the
potentialofthemodeltooverfit.POSITgeneralisestotheextent
to which its training data is representative. To avoid overfitting,
we use a development set and an early stopping criterion (three
epochs without improvement), as is conventional.
In Section 6.1, we show that despite the noisy training labels,
POSIT is capable of predicating code-tags/spans that users origi-
nally forgot to provide. We also explore POSITâ€™s performance on a
corpus that is likely to differ from both training corpora, the Linux
KernelMailingList(LKML)whichwasusedduringthedeepdive
(Section7.1).ThisvalidationwasperformedmanuallyduetolackThere are two ways of fixing the problem. The first is to
use a comma to sequence statements within the macro
without robbing it of its ability to act like an expression.
#define BAR(X) f(X), g(X)
The above version of bar BARexpands the above code into
what follows, which is syntactically correct.
if (corge)
f(corge), g(corge);
else
gralt();
This does not work if instead of f(X)you have a more
complicatedbodyofcodethatneedstogoinitsownblock,
say for example to declare local variables. In the most
generalcasethesolutionistousesomethinglike do ...
whileto cause the macro to be a single statement that
takes a semicolon without confusion.
Figure 5: Example sentence taken from Stack Overflow
which freely mixes English and very short code snippets,
hererenderedusingmonospacedfont.Wecanseebothinter-sententialcode-switching,suchasthemacrodefinitionandthe short example if statement snippet, as well as intra-sentential code-switching, the mention of the code tokenf(X)and the code construct â€˜do ... whileâ€™.
of a ground truth; automatically generating a ground truth for this
datawouldnotescapetheinternalthreatspresentedbelow.Onthiscorpus,POSITachievesalanguageidentificationaccuracyof76
.6%
and a PoS/AST tagging accuracy of 76 .5%. Two of the authors have
performed the labelling of this task and the Cohen kappa agree-ment [
10] for the manual classification is 0 .783, which indicates
substantialagreement.Weresolveddisagreementsbyconsidering
an output correct if both authors labelled it as such.
Neuralnetworksareaformofsupervisedlearningandrequirela-
bels.Welabelledourtrainingintwoways,usingoneprocedureforlanguagelabelsandanotherforPoS/ASTtags.Bothproceduresare
subjecttoathreattotheirconstructvalidity.Thelanguagelabels
areuser-providedandthussubjecttonoise,PoStagsarederived
fromanimperfectPoStagger,andASTtagsareaddedheuristically.
For language labels, we both manually labelled data and exploited
a human oracle. We manually labelled a uniformly sampled subset
of 10 postswith 3,233 tokens from our Stack Overflowevaluation
data, then manually assessed POSITâ€™s performance on this sub-set. Two authors performed the manual labelling and achieved a
Cohenkappaof0 .711indicatingsubstantialagreement.Asimilar
procedurewasappliedtotheLKMLlabellingtask.Onthisvalida-
tion, POSIT achieved 93.8 accuracy. Our Stack Overflow corpus
containsrevisionhistories.Wesearchedthishistoryforversions
whose edit comment is â€œcode formattingâ€. We then manually fil-
teredtheresultingversionstothosethatonlyaddcodetokenlabels
(defined in Section 6.1). POSIT achieved 95% balanced accuracy on
thisvalidation.ForthePoS/ASTtaggingtask,wemanuallyadded
the PoS/AST tags on the same 10 Stack Overflow posts, we used
above. Here, POSIT achieved 93.7%.
1356In Section 5.2, we used StORMeD as a baseline for POSIT. As
previously discussed, StORMeD was not designed for our task and
handlesJava,JSON,andXML.Adaptingtooursettingintroducesan
internalthreat.Toaddressthisthreat,weevaluatedbothStORMeDandPOSITonlyonthoseStackOverflowpoststaggedasJava.These
tags are noisy [ 11,20]. When evaluating StORMeD, its authors,
Ponzanelli et al.usedthesamefilter.WealsomaptheStORMeDAST
tagsettoours4.Ifthetruemappingisarelation,notafunction,then
this would understate StORMeDâ€™s performance. This is unlikely
becauseJavaASTsandCLANGASTsarenotthatdissimilar.Further,POSITmustalsocontendwiththisnoise.WhenbuildingTaskNav++
(Section 6.2), we use a more coarse grained PoS tag set than the
original TaskNav potentially reducing its performance.
8 RELATED WORK
In software engineering research, part-of-speec h tagging has been
directly applied for identifier naming [ 7], code summarisation [ 12,
13], concept localisation [ 5], traceability-link recovery [ 9], and
bug fixing [ 27]. We first review natural language processing (NLP)
research on code-switching,the natural language analogue ofthe
mixed text problem. This is work on which we based POSIT. Then
we discuss initial efforts to establish analogues for parts of speech
categories for code and use them to tag code tokens. We close with
thepioneeringworkonStORMeD,thefirstcontext-freeworkto
automatically tackle the mixed text tagging problem.
NLP researchers are growing more interested in code-switching
text and speech5. The main roadblock had been the lack of high-
quality labelled corpora. Previously, such data was scarce because
code-switchingwasstigmatised[ 24].Theadventofsocialmedia,
has reduced the stigma and provided code-switching data, espe-cially text that mixes English with another language [
31]. High
qualitydatasetsofcode-switchedutterancesarenowunderproduc-tion[
1].Forthetaskofpart-of-speech(PoS)taggingcode-switching
text, Solorio and Liu [ 25] presented the first statistical approach to
the task of part-of-speech (PoS) tagging code-switching text. On a
Spanglishcorpus,theyheuristicallycombinePoStaggerstrained
onlargermonolingualcorporaandobtain85%accuracy.Jamatia
et al.[15],workingonanEnglish-HindicorpusgatheredfromFace-
book and Twitter, recreated Solorioâ€™s and Liuâ€™s tagger and theyproposed a tagger using Conditional Random Fields. The former
performedbetterat72%vs71.6%.In2018,SotoandHirschberg[ 26]
proposedaneuralnetworkapproach,optingtosolvetworelated
problems simultaneously: part-of-speech tagging and Language
ID tagging. They combined a biLSTM with a CRF network at both
outputs and fused the two learning targets by simply summing the
respectivelosses.Thisnetworkachievesatestaccuracyof90.25%on
the inter-sentential code-switched dataset from Miami Bangor [ 1].
POSIT builds upon theirmodel extended withWinata et al.â€™s [32]
handling of OOV tokens, as discussed in Section 4.
Operating directly on source code (not mixed text), Newman
et al.[18] sought to discover categories for source code identifiers
analogous to PoS tags. Specifically, they looked for source codeequivalents to Proper Nouns, Nouns, Pronouns, Adjectives, and
4The mapping can be found online at https://github.com/PPPI/POSIT/blob/
92ef801e5183e3f304da423ad50f58fdd7369090/src/baseline/StORMeD/stormed_
evaluate.py#L33.
5The NLP term for text and speechthat mixed multiple natural languages.Verbs.Theyderivetheircategoriesfrom1)Abstractsyntaxtrees,2)
how the tokens impact memory, 3) where they are declared, and 4)
whattypetheyhave.Theyreporttheprevalenceofthesecategories
in source-code. Their goal was to map these code categories to
PoS tags, thereby building a bridge for applying NLP techniques to
codefortaskssuchasprogramcomprehension.Treude et al.[28]
describedthechallengesofanalysingsoftwaredocumentationwrit-
teninPortuguesewhichcommonlymixestwonaturallanguages
(Portuguese and English) as well as code. They suggested the intro-
duction ofa new part-of-speec h tag calledLexical Item tocapture
caseswheretheâ€œcorrectâ€tagcannotbedeterminedeasilydueto
language switching.
Ponzanelli et al.are the first to go beyond using regular expres-
sions to parse mixed text. When customising LexRank [ 23], a sum-
marisationtoolformixedtext,theyemployedanislandgrammar
thatparsesJavaandstack-traceislandsembeddedinnaturallan-guage, which is relegated to water. They followed up LexRank
withStORMeD,atoolthatusesanislandgrammartoparseJava,
JSON,andXMLislandsinmixedtextStackOverflowposts,again
relegating natural language to water [ 22]. StORMeD produces het-
erogeneous abstract syntax trees (AST), which are ASTs decorated
with natural language snippets.
StORMeD relies on Stack Overflowâ€™s code tags; when these tags
arepresent,islandgrammarsareanaturalchoiceforparsingmixed
text. Mixed text is noisy and Stack Overflow posts are no excep-
tion[21].Tohandlethisnoise,StORMeDresortstoheuristics(anti-
patternsinthenomenclatureofislandgrammars),whichtheybuild
into their island grammarâ€™s recognition of islands. For instance,if whitespace separates a method identifier from its â€™(â€™, they toss
thatmethodidentifierintowater.Toidentifyclassnamesthatap-
pear in isolation, they use three heuristics: the name is a class ifit is a fully qualified name with no internal spaces, contains twoinstances of CamelCase, or syntactically matches a Java generic
typeannotationoverbuiltins.TheyusesimilarrulestohandleJava
annotation because Stack Overflow also uses â€˜@â€™ to mention users
in posts. Heuristics, by definition, do solve a problem in general.
For example, the generic method names often used in examples â€”
foo,bar,o rbuzzâ€” slip past their heuristics when appearing alone
inthehostlanguage.Thisistrueevenwhenthepostdefinesthe
method.Indeed,weshowthatnoislandgrammar,which,bydefini-
tion, extend a context-free grammar, can solve this Sisyphean task
formixedtext,becauseweshowthistasktobecontext-sensitive
in Section 3. Island grammarâ€™s anti-patterns do not make island
grammars context-sensitive.
StORMeDandPOSITsolverelatedbutdifferentmixedtextprob-
lems. StORMeDrecovers naturallanguage, unprocessed, fromthe
water, buildsASTsfor itsislands,thendecoratesthose ASTswith
natural language snippets to build its HAST. In contrast, POSITtags both natural languages and formal languages, but does notbuild trees. StORMeD and POSIT do overlap on two subtasks of
mixed text tagging: language identification and AST-tagging code.
To compare them on these tasks, we had to adapt StORMeD. Es-
sentially, we traverse the HASTsand consider the first parent of a
terminalnodetobetheASTtag.WemapthesefromtheStORMeD
tag set to ours (Section 5.2). POSIT advances the state of the art on
these two tasks (Section 5.2).
1357As a notable service to the community, Ponzanelli et al.both
provided a corpus of HASTs and StORMeD as an excellent, well-
maintained web service. Their HAST corpus is a structured dataset
thatallowsresearchersaquickstartonminingmixedtext:itsparesthemfromtediouspre-processingandpermitsthequickextraction
and processing of code-snippets, for tasks like summarisation. We
have published our corpus at https://github.com/PPPI/POSIT to
complementtheirs.Ourproject,whichculminatedinPOSIT,would
not have been possible without these contributions.
9 CONCLUSION
We have defined the problem of tagging mixed text. We present
POSIT, implemented using a biLSTM-CRF Neural Network and
comparedittoPonzanelli et al.â€™spioneeringwork,StORMeD[ 22]
onJavapostsinStackOverflow.WeshowthatPOSITaccurately
identifies English and code tokens (81.6%), then accurately tags
those tokens with their part-of-speech tag for English or their AST
tagforcode(85.6%).WeshowthatPOSITcanhelpdevelopersby
improvingtwodownstreamtasks:suggestingmissingcodelabels
in mixed text (with 95% accuracy) and extracting tasks from mixed
text through TaskNav++, which exploits POSITâ€™s output to find
more than two new reasonable tasks per document.
POSIT and our CodeComment corpus are available at https:
//github.com/PPPI/POSIT.
10 ACKNOWLEDGEMENTS
WethankPonzanelli et al.fordevelopingandmaintainingStORMeD,
a powerful and easy-to-use tool, and for their prompt technical as-
sistance with the StORMeD webservice. This research is supported
by the EPSRC Ref. EP/J017515/1.
REFERENCES
[1]2011. Bangor Talk Miami Corpus. http://www.bangortalk.org.uk/speakers.php?
c=miami.
[2]2018. Stack Exchange Data Dump. https://archive.org/details/stackexchange.
[Online; accessed 05-Sep-2018].
[3]2019. Code Comment Corpus. https://github.com/PPPI/POSIT/blob/master/data/
corpora/lucid.zip. [Online; accessed 24-Jan-2020].
[4]MartÃ­n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,
Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, San-jay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard,
Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Leven-
berg,DanManÃ©,RajatMonga,SherryMoore,DerekMurray,ChrisOlah,Mike
Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul
Tucker,VincentVanhoucke,VijayVasudevan,FernandaViÃ©gas,OriolVinyals,
Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng.
2015. TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems.
http://tensorflow.org/ Software available from tensorflow.org.
[5]Surafel Lemma Abebe and Paolo Tonella. 2010. Natural language parsing of
programelementnamesforconceptextraction.In IEEE18thInt.Conf.onProg.
Comp. (ICPC) 2010. IEEE, 156â€“159.
[6]Sebastian Baltes, Lorik Dumani, Christoph Treude, and Stephan Diehl. 2018.
Sotorrent: Reconstructing and analyzing the evolution of Stack Overflow posts.
InProc. 15th Int. Conf. Min. Soft. Rep. ACM, 319â€“330.
[7]Dave Binkley, Matthew Hearn, and Dawn Lawrie. 2011. Improving identifier
informativeness using part ofspeechinformation. In Proceeding 8th Work. Conf.
Min. Softw. Repos. - MSR â€™11. ACM Press, New York, New York, USA, 203. https:
//doi.org/10.1145/1985441.1985471
[8]Steven Bird, Ewan Klein, and Edward Loper. 2009. Natural Language Processing
with Python. Oâ€™Reilly Media.
[9]Giovanni Capobianco, Andrea De Lucia, Rocco Oliveto, Annibale Panichella,
andSebastianoPanichella.2013. ImprovingIR-basedtraceabilityrecoveryvianoun-based indexing of software artifacts. Journal of Software: Evolution and
Process25, 7 (2013), 743â€“762.
[10]Jacob Cohen. 1960. A Coefficient of Agreement for Nominal Scales. Educational
and Psychological Measurement 20, 1 (1960), 37â€“46. https://doi.org/10.1177/
001316446002000104 arXiv:https://doi.org/10.1177/001316446002000104
[11]JensDietrich,MarkusLuczak-Roesch,andElroyDalefield.2019. Manvsmachine:astudyintolanguageidentificationofstackoverflowcodesnippets.In Proc.16th
Int. Conf. Min. Soft. Repo. IEEE Press, 205â€“209.
[12]Sonia Haiduc, Jairo Aponte, and Andrian Marcus. 2010. Supporting ProgramComprehension with Source Code Summarization. In Proc. 32Nd ACM/IEEE
International Conference on Software Engineering - Volume 2 (Cape Town, South
Africa)(ICSE â€™10). ACM, NewYork, NY, USA, 223â€“226. https://doi.org/10.1145/
1810295.1810335
[13]S. Haiduc, J. Aponte, L. Moreno, and A. Marcus. 2010. On the Use of Automated
TextSummarizationTechniquesforSummarizingSourceCode.In 201017thWork.
Conf. on Rev. Eng. 35â€“44. https://doi.org/10.1109/WCRE.2010.13
[14]Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirectional LSTM-CRF Models for
SequenceTagging. (2015). https://doi.org/10.1061/(ASCE)CO.1943-7862.0000274.
arXiv:1508.01991
[15]AnupamJamatia,BjÃ¶rnGambÃ¤ck,andAmitavaDas.2015. part-of-speechtagging
for code-mixed english-hindi twitter and facebook chat messages. In Proc. Int.
Conf. Rec. Adv. in Nat. Lang. Proc. 239â€“248.
[16]DiederikP.KingmaandJimmyBa.2014. Adam:AMethodforStochasticOpti-
mization. CoRRabs/1412.6980(2014). http://dblp.uni-trier.de/db/journals/corr/
corr1412.html#KingmaB14
[17]MitchellMarcus,BeatriceSantorini,andMaryAnnMarcinkiewicz.1993.Building
a large annotated corpus of English: The Penn Treebank. (1993).
[18]Christian D Newman, Reem S Alsuhaibani, Michael L Collard, and Jonathan I
Maletic. 2017. Lexical Categories for Source Code Identifiers. SANERâ€™17 (2017).
[19]SlavPetrov,DipanjanDas,andRyanMcDonald.2011. Auniversalpart-of-speech
tagset.arXiv preprint arXiv:1104.2086 (2011).
[20]LucaPonzanelli.2014. Holisticrecommendersystemsfor softwareengineering.
InCompanion Proc. 36th Int. Conf. Soft. Eng. 686â€“689.
[21]Luca Ponzanelli, Andrea Mocci, Alberto Bacchelli, Michele Lanza, and DavidFullerton. 2014. Improving low quality stack overflow post detection. In 2014
IEEE Int. Conf. Soft. Maint. Evol. IEEE, 541â€“544.
[22]Luca Ponzanelli, Andrea Mocci, and Michele Lanza. 2015. StORMeD: Stack
overflowreadymadedata. IEEEInt.Work.Conf.Min.Softw.Repos. 2015-Augus
(2015), 474â€“477. https://doi.org/10.1109/MSR.2015.67
[23]Luca Ponzanelli, Andrea Mocci, and Michele Lanza. 2015. Summarizing complex
development artifacts by mining heterogeneous data. IEEE Int. Work. Conf. Min.
Softw. Repos. 2015-Augus (2015), 401â€“405. https://doi.org/10.1109/MSR.2015.49
[24]Shana Poplack. 1980. Sometimes Iâ€™ll start a sentence in Spanish Y TERMINO
ENESPAÃ‘OL:towardatypologyofcode-switching1. Linguistics 18(011980),
581â€“618. https://doi.org/10.1515/ling.1980.18.7-8.581
[25]Thamar Solorio and Yang Liu. 2008. part-of-speec h tagging for English-Spanish
code-switched text. In Proc. Conf. onEmp. Meth. in Nat.Lang. Proc. Association
for Computational Linguistics, 1051â€“1060.
[26]Victor Soto and Julia Hirschberg. 2018. Joint part-of-speech and Language ID
Tagging for Code-Switched Data. (2018), 1â€“10.
[27]Yuan Tian and David Lo. 2015. A comparative study on the effectiveness of
part-of-speec h tagging techniques on bugreports. In SANERâ€™15. IEEE, 570â€“574.
[28]ChristophTreude,CarlosAProlo,andFernandoFigueiraFilho.2015. Challenges
in analyzing software documentation in Portuguese. In Proc. 29th Bra. Sym. Soft.
Eng.IEEE, 179â€“184.
[29]Christoph Treude, Mathieu Sicard, Marc Klocke, and Martin Robillard. 2015.
TaskNav:Task-basedNavigationofSoftwareDocumentation.In Proc.37thInt.
Conf. Soft. Eng. - Volume 2 (Florence, Italy) (ICSE â€™15). IEEE Press, Piscataway, NJ,
USA, 649â€“652. http://dl.acm.org/citation.cfm?id=2819009.2819128
[30]A. Viterbi. 1967. Error bounds for convolutional codes and an asymptoticallyoptimum decoding algorithm. IEEE Transactions on Information Theory 13, 2
(April 1967), 260â€“269. https://doi.org/10.1109/TIT.1967.1054010
[31]YogarshiVyas,SpandanaGella,JatinSharma,KalikaBali,andMonojitChoudhury.2014. Postaggingofenglish-hindicode-mixedsocialmediacontent.In Proc.2014
Conf. on Emp. Meth. in Nat. Lang. Proc. (EMNLP). 974â€“979.
[32]Genta Indra Winata, Chien-Sheng Wu, Andrea Madotto, and Pascale Fung. 2018.
BilingualCharacterRepresentationforEfficientlyAddressingOut-of-Vocabulary
WordsinCode-SwitchingNamedEntityRecognition.In Proc.ThirdWorkshop
Comp. Appr. Ling. Code-Switching. Association for Computational Linguistics,
Melbourne, Australia, 110â€“114. https://doi.org/10.18653/v1/W18-3214
[33]PengchengYin,BowenDeng,EdgarChen,BogdanVasilescu,andGrahamNeubig.
2018. Learning to mine aligned code and natural language pairs from stack
overflow. Proc.-Int.Conf.Softw.Eng. (2018),476â€“486. https://doi.org/10.1145/
3196398.3196408 arXiv:1805.08949
1358