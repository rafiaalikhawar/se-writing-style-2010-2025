Identifying Software Performance Changes Across
Variants and Versions
Stefan MÃ¼hlbauer
Leipzig University
GermanySven Apel
Saarland University
Saarland Informatics Campus
GermanyNorbert Siegmund
Leipzig University
Germany
ABSTRACT
We address the problem of identifying performance changes in
the evolution of configurable software systems. Finding optimal
configurations and configuration options that influence perfor-mance is already difficult, but in the light of software evolution,configuration-dependentperformancechangesmaylurkinapo-
tentially large number of different versions of the system.
In this work, we combine two perspectivesâ€”variability and timeâ€”
intoanovelperspective.Weproposeanapproachtoidentifycon-
figuration-dependent performance changesretrospectively across
thesoftwarevariantsandversionsofasoftwaresystem.Inanut-
shell, we iteratively sample pairs of configurations and versions
andmeasuretherespectiveperformance,whichweusetoupdatea
modeloflikelihoodsforperformancechanges.Pursuingasearch
strategy with the goal of measuring selectively and incrementally
further pairs, we increase the accuracy of identified change points
related to configuration options and interactions.
We have conducted a number of experiments both on controlled
synthetic data sets as well as in real-world scenarios with differ-ent software systems. Our evaluation demonstrates that we canpinpoint performance shifts to individual configuration optionsand interactions as well as commits introducing change points
withhighaccuracyandatscale.Experimentsonthreereal-world
systems explore the effectiveness and practicality of our approach.
CCS CONCEPTS
â€¢Softwareanditsengineering â†’Softwareperformance ;Soft-
ware evolution.
KEYWORDS
Softwareperformance,softwareevolution,configurablesoftware
systems, machine learning, active learning
ACM Reference Format:
StefanMÃ¼hlbauer,SvenApel,andNorbertSiegmund.2020.IdentifyingSoft-
warePerformanceChangesAcrossVariantsandVersions.In 35thIEEE/ACM
International Conference on Automated Software Engineering (ASE â€™20), Sep-
tember 21â€“25, 2020, Virtual Event, Australia. ACM, New York, NY, USA,
12 pages. https://doi.org/10.1145/3324884.3416573
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ASE â€™20, September 21â€“25, 2020, Virtual Event, Australia
Â© 2020 Association for Computing Machinery.
ACM ISBN 978-1-4503-6768-4/20/09...$15.00
https://doi.org/10.1145/3324884.34165731 INTRODUCTION
Softwareperformanceplaysacrucialroleinusersâ€™perceptionof
software quality. Excessive execution times, low throughput, or
otherwiseunexpectedperformancewithoutaddedvaluecanrender
software systems unusable. Poorperformance is often a symptom
of deficiencies in particular software components or the overall
softwarearchitecture.Changesintheobservedperformanceofa
software system can be attributed to changes to the software at
different levels of granularity (architecture, code, etc.).
Typically, modern software systems provide configuration options
to enable users and admins customizing behavior to meet different
user requirements. Configuration options usually correspond to
pieces of selectable functionality (features), which contribute to
overallperformancewithdifferentproportions.Thatis,different
configurations of a software system exhibit different performance
characteristicsdependingonconfigurationdecisionsmadebythe
user.
Performance changes during software evolutionâ€”intended or notâ€”
can affect all or only a subset of configurations since changes to a
softwaresystemoftenrelatetoaparticularconfigurationoptionorsetofoptions.Thisiswhyperformancebugsarerarelyvisibleinde-faultconfigurations,butrevealedonlyincertainconfigurations[
8].
Ifundetected,suchperennialbugscanpersistforthe lifetimeofa
softwaresystemassoftwareevolves.Addingthis technicaldebt con-
stantlycanaccumulateandentailtrendsofdegradingperformance
quality [5].
Performance assessment and estimation is a resource-intense task
withmanypossiblepitfallsprevailing.Bestpracticesforconducting
performancemeasurementsemphasizeadedicatedandseparated
hardware setup to prevent measurement bias by side-processes
and,subsequently, obtain reproducibleresults[ 18].Whatisoften
overlooked is the fact that most software systems are configurable,
whichintroducesanotherlayerofcomplexity.Duetocombinatorics,
even for a small number of configuration options, the range ofpossible valid configurations renders exhaustive measurements
infeasible.
Ourgoalistoenablethe retrospective detectionofchangesinthe
performance ofconfigurable softwaresystems andpinpoint them
to a specific option or interaction. For example, a patch of a cer-
tainfeatureislikelytoaffectonlyconfigurationswheresuchfea-
ture is selected. We would like to know in which revision this
patch was introduced and which features were affected. Many soft-
waresystemsareconfigurable,buthavenoperformanceregression
testingroutinesetinplace.Touncoverperformancedeficiencies
that emerged from revisions in the past development history, itis infeasible to test each and every commit and configuration. In
essence,wefaceacombinatorialexplosionalongtwodimensions:
6112020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE)
time (versions) and configuration space (variants). First, we aim
at finding changes in software performance from one version to
another. The detection of such changes is referred to as change
point detection [2,3,10,19,24,25]. The main limiting factor is that
exhaustivemeasurementsacrosstheconfigurationspacearenei-
ther available nor feasible [ 31]. Change point detection techniques
with both exhaustive measurement [ 2,3] as well as limited data
availability[ 10,19,24,25]havebeensuccessfullyappliedtoidentify
performancechanges. Second,weaimatassociatingperformance
changes(fromoneversiontoanother)withparticularconfigura-
tionoptions orinteractions amongthem. There issubstantial work
regardingarelatedproblem,whichisestimatingtheinfluenceof
individualoptionsonperformance[ 7,26,28,29]forasingleversion
of the software system, ignoring the temporaldimension. Instead
ofestimatingtheinfluenceofoptionsandinteractionsonperfor-
mance,wewanttoknow:Whichoptionorinteractionis responsible
foraparticularchangepointintheversionhistoryofasoftware
system?
Our main idea is as follows: We address the configuration com-
plexityofthisproblembyselectingrepresentativesamplesetsof
configurations. Then, we uniformly sample a constant number
ofcommitsforeachconfigurationandconductrespectiveperfor-
mance measurements. Based on these measurements, we learn aprediction model. For each configuration, we estimate the likeli-
hoodofeachcommitbeingachangepoint(i.e.,thatperformance
of some configurations changes abruptly compared to the previous
commit).Next,weleveragesimilaritiesintheperformancehisto-
ries of configurations that share common options. Since we sample
thecommitsforeachconfigurationindependently,weobtainfor
each change point many estimations using measurements of dif-ferent commits. Overall, this allows us to obtain more accurateestimations of performance-changing commits with tractable ef-
fort. Based on a mapping from configurations to predicted change
pointprobabilitiesforeachconfigurationandcommit,wederive
the options responsible for each particular change point, which we
callconfiguration-dependent change points. In summary, we offer
the following contributions:
â€¢A novel technique to effectively identify shifts in performance
ofconfigurablesoftwaresystems.Itisabletopinpointcausativecommitsandaffectedconfigurationoptionswithhighaccuracy
and tractable effort.
â€¢Afeasibilitydemonstrationofourapproachbyimplementing
an adaptive learning strategy to obtain accurate estimations
with acceptable measurement cost.
â€¢Anevaluationusingbothsyntheticandreal-worldperformance
data from three configurable software systems. Synthetic data
letus assess ourmodeland approach conceptuallyandatscale,
whereas we are able to assess practicality with real-world data.
â€¢AcompanionWebsite1providingsupplementarymaterialin-
cluding a reference implementation of our approach, perfor-
mance measurement data, and additional visualizations.
1https://github.com/AI-4-SE/Changepoints-Across-Variants-And-Versions/ or an
archivedversionathttps://archive.softwareheritage.org/browse/origin/https://github.
com/AI-4-SE/Changepoints-Across-Variants-And-Versions/2 CONFIGURATION-DEPENDENT CHANGE
POINTS
Performanceasapropertyemergesfromavarietyoffactors.Besides
external factors, including hardware setup or execution environ-ment [
22], the configuration of a software system can influence
performance to a large extent [ 29]. Most modern software systems
exhibitconfigurationoptionsthatcorrespondtoselectablepieces
offunctionality.Withconfigurationoptions,weturnfeatureson
and off creating a variant of the software system. Depending on
the configuration, different system variants with different behavior
and performance can be derived.
2.1 Performance-Influence Models
Consider the following running example of a database manage-ment system (DBMS) with two selectable features: Encryption
andCompression.Eitherfeatureaddsexecutiontimetotheoverall
performance, but, if both features are selected, the execution time
issmallerthanthesumoftheindividualfeaturesâ€™contributionto
performance: Less data is encrypted if it is compressed beforehand.
Aninteraction in this setting is the combined effect of features
Encryption and compression. We can assign each feature andinteraction an influence that it contributes to the overall perfor-
mance, if selected. In our example, the individual influences of the
two features are positive (increasing execution time), whilst the
interactionâ€™s influence is negative (decreasing execution time).
The influence of features on performance can be described using a
performance-influence model [28]. A performance-influence model
is a linear prediction model of the form Î :ğ¶â†’R, whereby
ğ¶denotes a configuration vector (assignment of concrete values
to configuration options) and the estimate is a system variantâ€™s
real-valued performance:
Î (ğ‘)=ğ›½ğ‘‡ğ‘=â¡â¢â¢â¢â¢â¢â£ğ›½
1
...
ğ›½
2|ğ¹|â¤â¥â¥â¥â¥â¥â¦ğ‘‡
Â·â¡â¢â¢â¢â¢â¢â£ğ‘
1
...
ğ‘
2|ğ¹|â¤â¥â¥â¥â¥â¥â¦ğ›½
ğ‘–âˆˆR2|ğ¹|
ğ‘ğ‘–âˆˆ{0,1}2|ğ¹|(1)
ğ¹denotesthesetofallfeatures.Ourlinearmodelhas |2ğ¹|terms,
whereby each term ğ‘¡âŠ†ğ¹corresponds to a subset of ğ¹and is
described by a coefficient ğ›½ğ‘“and a configuration parameter ğ‘ğ‘“.
Coefficientsencodetheperformance-influenceoffeaturesandin-
teractions. ğ‘is a vector and denotes the assignment of concrete
values to configuration options or interactions. A configuration
parameter ğ‘ğ‘¡evaluatesto1,ifallconfigurationoptionsofthecorre-
spondingsubset ğ‘¡âŠ†ğ¹areselected,and0otherwise.Theemptyset
correspondstotheinvariablecorefunctionalityofthesoftwaresys-
tem.Singletonsubsetscorrespondtoindividualfeatures,compound
subsets to interactions of features.
ForourDBMSexample,wepresentafullydeterminedperformance-
influencemodelinEquation2.Thefirsttwotermscorrespondto
thefeaturesEncryptionandCompression,respectively.Thethird
term represents the interaction of both features. The configura-
tion parameter ğ‘Comprâˆ§Encrypt evaluates to 1, if both ğ‘Encryptand
ğ‘Comprevaluate to 1. That is, ğ‘Comprâˆ§Encrypt â‰¡ğ‘EncryptÂ·ğ‘Compr.
In the last term, we omit the configuration parameter ğ‘âˆ…, as this
term represents invariable functionality.
612Î DBMS(ğ‘)=ğ›½EncryptÂ·ğ‘Encrypt+ğ›½ComprÂ·ğ‘Compr
+ğ›½Comprâˆ§EncryptÂ·ğ‘Comprâˆ§Encrypt+ğ›½âˆ…(2)
Prediction models of configuration-dependent performance of this
formarenottheonlypossiblerepresentation[ 4,7],butlinearmod-
elsareeasytointerpret[ 28].Wereviewextensionsandalternatives
to linear performance-influence models in Section 6.
2.2 Evolution of Performance Influences
Performance-influence models describe how features contribute to
performance,buttheydonotallowforunderstandingconfiguration-
dependent performance changes over time. We expand our DBMS
example with a temporal dimension, considering a development
historyofhundredcommits.Inparticular,weassumethefollowing
three events. (1) At commit 25, feature Compression has been
modified, increasing the execution time; feature Encryption is
introduced at commit 25 and can be combined with Compression.
(2)Atcommit50,theinteractionof CompressionandEncryption
changes,resultinginanincreasedexecutiontime.(3)Atcommit80,
the core functionality of the DBMS is refactored, which decreased
execution time for all variants.
Figure 1: Performance of 4 variants with 3 change points
The performance histories of the four valid variants in Figure 1
exhibit three change points: commits 25, 50, and 80. These change
points,however, d onotaffectallconfigurations.Forinstance,ex-
ecution time decreases for all configurations at commit 80, but
increasesatcommit50onlyforoneconfiguration.Giventhisex-
ample, the target outcomes of our approach are (1) the locationsof the three change points (commits 25, 50, and 80), (2) the asso-ciation of such commits to the features and interactions (Compr,
Comprâˆ§Encrypt), and the invariable functionality, respectively.
2.3 Taming Complexity
A naive approach for identifying change points in the evolution of
configurable software systems is to simply combine existing work
on performance modeling of commit histories and performance
prediction of software configurations.
For our example, we could measure all variants for each commit
buildingaperformance-influencemodelpercommit.Having4con-figurationsand100commits,thiswouldresultin400measurements.
Sincethe numberof configurationsgrows exponentiallywiththe
number of features, we end up with 2ğ‘›timesğ‘‡measurements
whereğ‘›is the number of features and ğ‘‡is the number of com-
mits. Clearly, this does not scale along both dimensions: Already a
few features would render even small commit histories intractable,CommitsVanillaCompressionEncryptionCompr. & Encr..
conguration-speci c
change points?
Exploration: new con gurations
Exploitation: new con gurationsExploration: new commits
Exploitation: new commits
Existing measurements
Figure 2: Active sampling strategies for a DBMS with 4 con-figurations
and even small configuration spaces make this approach infeasible
given realistic commit histories of a few thousands of commits.
Toobtainaccurateestimationswithalimitedbudgetofmeasure-
ments,werequireadifferentapproach.Weproposeaniterativeand
adaptive sampling approach. That is, we use an initial, but small
samplesettoexploretheproblemspaceandthenincreasethelevel
ofgranularityatpromisingregionsanddimensions(development
history segments as well as individual features and interactions, in
our case).
BasedontheintroductoryDBMSexample(cf.Figure1),weillus-
trate our sampling strategy in Figure 2. Here, each box depicts a
possible measurement (i.e., a pair of a configuration and a commit).
Blackbarsrepresentchangepointshiddeninthesoftwaresystemâ€˜s
performance histories (i.e.,measurements immediatelybefore and
afterachangepointaredissimilar).Thecurrentstateofthesample
set comprises all measurements that are filled in grey.
To include new measurements into the sample set, we consider the
followingsituations.Ontheonehand,oursamplesetmightalready
hinttosomepossiblechangepointsandassociatedconfiguration
options. On the other hand, our sample set might be too sparse,
such that we cannot infer somechange points yet. To address this
trade-off, we devise two strategies, exploration and exploitation,
both of which address both configurations and commits.
Forexploitation,wesamplenewmeasurementstoverifyguesses
basedonthecurrentstate.Thatis,inFigure2,wemightincludethe
measurements depicted by black-filled symbols ( /). Black-filled
stars()representmeasurementsthatareofinterestbecausewe
have already identified one change point for configuration Com-
pression.Weexploitthisknowledgetotestfurtherconfiguration
optionsinvolvedinthischangepoint.Theblack-filledpentagons( )
representmeasurementsthatareinteresting,becausewehaveiden-
tified another early change point for Compression, but within a
rather broad range of commits. Thus,we include further measure-ments from that range to narrow down the possible range for this
particular change point.
Forexploration, we might include the measurements denoted by
white symbols ( /). White stars ( ) represent measurements that
explore new configurations. In our example, the interaction be-tween Compression and Encryption is the only configurationleft to be measured. We include several commits of a particularnew configuration to unveil possible performance variation thatindicates possible change points. White pentagons (
) represent
613measurementsthatareofinterestbecausetheyincreasetheoverall
measurementcoverage.Weincludemeasurementsfromlargein-
tervals of not yet measured commits, since possible change points
can be hidden there.
Thesefourstrategies(explorationandexploitationofconfigurations
and commits) prioritize measurements to be included next into the
sampleset.Adaptivesamplingtechniqueshavebeensuccessfully
applied to obtain both performance-influence models [ 26,29] and
performancehistories[ 19]before. However,itis unclearwhether
fewermeasurementsaresufficienttoassesstheperformance-influence
ofconfigurationoptionsandinteractionswithrespecttoversion
changes. This is what we address in this work.
3 AN ALGORITHM FOR CHANGE POINT
DETECTION
Weproposeanalgorithmtodetectsubstantialshiftsintheperfor-
mance of software configurations and associate them to individual
commitsandoptionsorinteractions.Welayoutourapproachasan
iterative search across the commit history and configuration space.
We provide an overview of our approach in Figure 3. It starts with
a small initial sample set of measurements (i.e., performance obser-
vations of varying configurations and varying commits). Based on
thissampleset,itcalculatesforeachconfigurationthelikelihood
of each commit being a change point.
Subsequently,ouralgorithm estimatesa candidatesolution,which
is a set of pairs of a commit and a configuration option. Each of
such pairsdescribes the estimatedinvolvement of aconfiguration
option in the shift of performance. Interactions are conceived as
multiple tuples with identical commits. That is, if a commit occurs
inmultipletuples,eachwithdifferentconfigurationoptions,this
indicates that the shift arises from an interaction among two or
moreoptions.Henceforth,wewillrefertosuchpairsas associations.
Afterobtainingonecandidatesolutionpersearchiteration,weaug-
mentthesamplesetofmeasurementswithregardtotwoobjectives:
explorationandexploitation. Exploration aimsatincludingprevi-
ouslyunseencommitsandconfigurationstoimprovecoverageof
thesearchspace. Exploitation aimsatincludingmeasurementsin
the sample set that, based on the previous candidate solution, may
increaseconfidenceinassociationsorruleoutfalsepositives(i.e.,
commits falsely identified as change points or options falsely asso-
ciatedwithacommit).Asforexploitation,wemakeaninformed
decisionofwhichmeasurementsaretobeincluded,whereasexplo-
ration is agnostic of previous candidate solutions.
Aseachiterationyieldsacandidatesolution,wekeeptrackofasso-
ciationsina solutioncache andrepeatedlyupdatetheconfidence
of associations. The rationale is not to lose previously identified
change points, but at the same time, allow for removing identified
changespointsthatarelikelyfalsepositivesduetonewmeasure-
ments. Some associations, especially in the beginning, might be
influenced by sampling bias and can be removed if successive iter-
ationsdonotrepeatedlyrevisittheseassociations.Thealgorithm
terminatesifthesolutioncachedoesnotchangeforanumberof
iterationsoramaximumnumberofiterations/measurementshas
beenreached.Inwhatfollows,wepresentthestepsofourapproach
in detail.(1) 
Initial Sampling(2) 
Compute Change Point
Likelihoods 
(for each conï¬guration)(3) 
Associate Change
Points and Con ï¬guration
Options
Â¬
(4) Acquire  New MeasurementsAcquire Commits...Acquire
Conï¬gurations...(5)
Update Solution Cache &
Check Stoppage Criteria
Figure 3: Overview of our approach: 1/circlecopyrtSelecting training
data across configuration space and version history, 2/circlecopyrtesti-
matingthechangepointprobabilitydistributionperconfig-uration, 3/circlecopyrtestimatingchangepointsascandidatesolutions,
4/circlecopyrtadaptively augmenting the training set, and 5/circlecopyrtupdating
the solution cache and evaluating termination criteria.
3.1 Initialization and Sampling
The first step of our approach is to select a sample set of perfor-
mancemeasurements.Weselectarelativelysmall,butfixednumber
of configurations, ğ‘›configurations . For each configuration, we select
a fixed percentage of commits, ğ‘Ÿcommits, and assess their respective
performance.Theinitialsamplesetinthissetupiskeptsmalland
may not represent all relationships between configuration options
andperformanceevolution.Therationaleisthat,tomakeourap-
proachmorescalable,weexplorethecombinedsearchspaceand
refinethetemporalandspatial(i.e.,configuration-related)resolu-
tion where necessary.
For the initial configuration sampling, we use distance-based sam-
pling [14], which is a form of random sampling that strives for
uniformcoverageatlowcost.Ingeneral,uniformrandomsamplingofconfigurationsisconsideredtoyieldthemostrepresentativecov-
erage of a configuration space, but it is prohibitively expensive for
real-world configurable software systems with constraints among
options(i.e.,notallcombinationsofconfigurationoptionsarevalid
configurations).Distance-basedsamplingaddressesthisproblem
by demandingthe number ofselected optionsto be uniformlydis-
tributed to avoid local concentration.
Thekeyideaofouralgorithmtoiterativelyaugmentthetrainingset
addressestwoissues:(1)Theconfigurationspaceexhibitsexponen-
tial complexity. (2) Interactions of higher degrees (i.e., interactions
involving two or more configuration options) are possible, but rela-tivelyrareamongconfigurablesoftwaresystems[
15,16].Therefore,
insteadofexhaustivesamplingwithrespectinteractiondegrees,weiteratively add new configurations to our training sample to search
previously undetected influences of options and interactions.
For each configuration in our sample set, the algorithm selects a
smallnumberofcommits(e.g.,oneortwopercentofallcommits)
forwhichitmeasuresperformance.Therationaleofhavingonly
few commits is that, given a relatively large number of configu-rations, many similar configurations will exhibit change pointsof the same cause. We mitigate the poor temporal resolution by
selectingthecommitsindependently.Compared toafixedsample
ofcommitsacrossallconfigurations,thisway,eachcommitismorelikelytobemeasured,atleast,once.Thatis,weobtainchange-point
estimationsforrelatedconfigurationsfromindependenttraining
614samples. For instance, consider the third change point in the intro-
ductory DBMS example: At commit 80, all configurations exhibit a
performance change. If we sampled two commits, 70 and 90, for all
configurations,allthatwewouldlearnisthatthereisachangepoint
somewherebetweenthesetwoversions.Instead,ourapproachsam-
ples the commits 70 and 90 for the first two configurations, and
commits 75 and 95, as well as 65 and 85 for the remaining two con-figurations respectively. Our best guess then is to assume a change
point between commits 75 and 85 since all measurements agree
withthisconclusion.Thisway,weincreasethetemporalresolution
whilekeepingtheoverallnumberofperformancemeasurements
manageable.
3.2 Iteration: Change Point Likelihoods
For each configuration in our sample set, we estimate the proba-
bilityofeachcommitbeingachangepointforthecorresponding
configuration. To this end, we need to define what counts as a
performancechange.Weuseauser-definedthreshold,whichdis-
criminates between measurement noise and performance changes
such that different application scenarios as well as system-specific
peculiaritiescanbeaccountedfor.Iftheperformancedifferencefor
aconfigurationbetweentwocommitsexceedsthisthreshold,we
count this difference as a performance change. Although manually
defined, there are several possibilities to estimate this threshold au-
tomatically. Prior to learning, the measurement variation obtained
bytherepeatingmeasurementsforthesameconfigurationmultiple
times can be estimatedand employed as a minimum threshold. In
addition,arelativeorabsolutethresholdcanbederivedfromthe
applicationcontext,suchasatenpercentortensecondsincrease
in execution time.
We encode the threshold in a step function ğœƒğœ:
ğœƒğœ(ğ‘,ğ‘)=/braceleftBigg
0,|ğœ‹(ğ‘)âˆ’ğœ‹(ğ‘)|<ğœ
1,|ğœ‹(ğ‘)âˆ’ğœ‹(ğ‘)|â‰¥ğœğ‘,ğ‘âˆˆğ‘‰,ğœâˆˆR(3)
The function evaluates to 1 if the difference between performance
ğœ‹ğ‘andğœ‹ğ‘of commits ğ‘andğ‘exceeds the threshold ğœand 0 if not.
Given a pair of commits, we can now decide whether performance
haschangedsomewherebetweenthetwocommits.However,not
eachpairofcommitsisequallyinformative.Thefartherthedistance
between two commits, the lesser the information we can obtain, as
theremightbeseveralchangepointsinbetween.Inaddition,the
effect of one change point between two commits can be shadowed
by another change point in the opposite direction, such as that one
changepoint increasestheexecution timeanda seconddecreases
theexecutiontimeagain.Wedefinetheinfluenceofeachpaironourestimationbyweighingeachpairinverselyproportionallyto
the distance between two commits.
ğ‘/prime(ğ‘£)=/summationdisplay.1
{ğ‘âˆˆğ‘‰|ğ‘<ğ‘£}/summationdisplay.1
{ğ‘âˆˆğ‘‰|ğ‘>ğ‘£}ğœƒğœ(ğ‘,ğ‘)/bracehtipupleft/bracehext/bracehext/bracehtipdownright/bracehtipdownleft /bracehext/bracehext/bracehtipupright
step functionÂ·(ğ‘âˆ’ğ‘)âˆ’2
/bracehtipupleft/bracehext/bracehext/bracehext/bracehext/bracehext/bracehtipdownright/bracehtipdownleft /bracehext/bracehext/bracehext/bracehext/bracehext/bracehtipupright
weighting term(4)
ğ‘(ğ‘£)=ğ‘/prime(ğ‘£)/summationtext.1ğ‘›
ğ‘–=1ğ‘/prime(ğ‘–)(5)
For a given commit ğ‘£âˆˆğ‘‰, we can now estimate a change point
probability by comparing each pair of commits before and after
ğ‘£. This is illustrated in Equation 4, where ğ‘/prime(ğ‘£)is the sum of the0 200 400 600 800 1000
time0.0000.0020.0040.006p ( v )all versions
appr o ximation
sample measur ements
0 200 400 600 800 1000
Ti m e246P erformance
Figure 4: Performance history with 1,000 commits and two
change points (bottom); Ground truth and approximatedchange point likelihood ğ‘(ğ‘£)(top)
influence times the performance change indicator ğœƒğœfor each pair
of commits. In practice, however, measuring all commits ğ‘‰is unde-
sirable. Therefore, we sample a small number of commits ğ‘‡âŠ‚ğ‘‰
instead and compare each pair of commits before and after ğ‘£to ob-
tain an approximation of ğ‘/prime(ğ‘£). Last, to obtain a proper probability
distribution, weneed tonormalize eachvalue ğ‘/prime(ğ‘£), asillustrated
inEquation5.Consequently,weobtainanapproximation ğ‘(ğ‘£)that
represents a probability distribution withâˆ«|ğ‘‰|
0ğ‘(ğ‘£)ğ‘‘ğ‘£=1. The
resultingprobabilitydistributionaswellasitsapproximationare
illustratedinFigure4,whereeachchangepointcorrespondstoa
peak in the probability distribution.
3.3 Iteration: Assembling a Candidate Solution
We now have change point likelihood estimations for all config-
urations in our sample set. Different configuration options and
interactions contribute to this change point likelihood as we have
seen in the introductory DBMS example. In the following step, we
estimate the coordinates (pair of commit and configuration option)
of likely change points. So, we first estimate candidate commits
based on the change point likelihood from the previous step. Then,
we associate these candidate commits with configuration options.
3.3.1 Candidate Commits. For each configuration, we compute
an approximation of the change point likelihood over commits (cf.
Figure 4) to identify local maxima (peaks). We select such peaks
undertheconditionthatthepeakchangepointlikelihoodisgreater
than a threshold:
ğ‘¡CPL=1
|ğ‘‰|+ğ‘CPLÂ·/radicalBigg
1
|ğ‘‰|Â·/summationdisplay.1
ğ‘£âˆˆğ‘‰/parenleftBig1
|ğ‘‰|âˆ’ğ‘/prime(ğ‘£)/parenrightBig2
/bracehtipupleft/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext /bracehtipdownright/bracehtipdownleft /bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext /bracehtipupright
standard deviation of ğ‘/prime(ğ‘£)
over all commits(6)
Thethreshold ğ‘¡CPListheaveragechangepointlikelihoodoverall
commits plus a factor ğ‘CPLtimes its corresponding standard de-
viation over all commits. The factor ğ‘CPL(default value: 3) allows
615usto filterpeaksthat donotstand outenoughand mightbefalse
positives. The greater ğ‘CPLis, the stricter the filtering of peaks.
That is, for each configuration in our learning set, we obtain a
setofcommits(configuration-dependentcandidatecommits)that
representpossiblechangepoints.Toreducevariationamongthe
obtainedcommits,weclusterthecommitsusingkerneldensityesti-
mation(KDE).ThepurposeofKDEinoursettingistoestimatethe
probabilitymassfunctionofwhetheracommitisachangepoint.
ThelocalmaximaofthisKDE,subsequently,representoursetof
candidate commits. Note that, if two distinct change points are
almost coincident commit-wise, they can be mistakenly identified
by our approach as one change point that, subsequently, can be as-
sociatedwithconfigurationoptionsbelongingtothetwoindividual
change points.
3.3.2 AssociatingCommitsandOptions. Foreachcandidatecom-
mit that we obtain, we want to know which configuration op-tions are most likely responsible for the respective peak. Hence,
we estimate the influence of each configuration option on the
change point likelihood. The core idea is to train a linear model
ğ‘€:[0,1]ğ‘›optionsâ†’[0,1], in which each configuration optionâ€™s
coefficient corresponds to its influence.
Instead of an ordinary linear regression model, we use a linear
model that implements the ğ¿1norm for regularization (LASSO). In
addition to the least squares penalty, this technique favors solu-
tions with more parameters coefficients set to zero. This technique
is commonly used to help prevent over-fitting, decrease model
complexity,andeliminatenon-influentialmodelparameters(con-
figuration options, in our case) [ 30]. The effect of ğ¿1regularization
inamodelisspecifiedbyanadditionalhyper-parameter ğœ†.Wetune
this hyper-parameter using threefold cross-validation.
For each model (for a candidate commit), we consider aconfigura-
tion option as associated with a commit, if its influence ğ‘ğ‘–(i.e., the
absolute value of its coefficient) is greater than a threshold:
ğ‘¡influence =1
ğ‘›options+ğ‘influenceÂ·/radicalBigg
1
ğ‘›optionsÂ·/summationdisplay.1/parenleftBig1
ğ‘›optionsâˆ’ğ‘ğ‘–/parenrightBig2
/bracehtipupleft/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext /bracehtipdownright/bracehtipdownleft /bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext /bracehtipupright
standard deviation of ğ‘ğ‘–
(7)
We consider options as influential, if their estimated influence is
greaterthanifeachconfigurationoptionwereequallyinfluential.
In addition, weadd to thisthreshold the standard deviationof the
influences of all configuration options. In theory, our regressionmodel eliminates non-influential configuration options. If more
thanconfigurationoptionisinfluential(i.e.,twoormoreoptions
interact) their respective estimated influence should be roughly
equal.Toavoidfalsepositivesinheestimationofinfluentialconfig-
urationoptions,weusethestandarddeviationandtheparameter
ğ‘influence(default value: 1) to filter only estimations with low vari-
ationamongtheestimatedinfluences.Iftheinterceptofamodel
(i.e., the coefficient not related to any regression model variable)
exceedsthisthreshold,weconsiderthiscandidatecommitachange
point thatis notconfiguration-dependent (i.e.,it affects allconfig-
urations).Theoutcomeisalistof associations :pairsofcandidate
commits and (likely) influential configuration options.3.4 Iteration: Acquiring New Measurements
The last step in each iteration is the acquisition of new measure-
ments. We extend the existing sample with both new commitsfor configurations already sampled as well for an additional set
ofnewconfigurations.Theroleofincludingnewdataistwofold.
First,previouslyassessedconfigurationsorcommitsmightnothave
capturedunseenperformanceshifts.Therefore,aportionofnew
dataisacquiredwithoutfurtherknowledge(exploration).Second,a
candidate solution might over- or under-approximate associations
andcancontributetothealgorithmâ€™soverallestimation.Therefore,
samplingthesecondportionofdataisguidedbyexploitingeach
iterationâ€™s candidate solution (exploitation).
3.4.1 AcquiringCommits. Theexplorationofnewcommitsfollows
asimplerule:Foreachconfiguration,wesampleanumbercommits
that exhibit the maximum distance to already sampled commits.
Theexploitationofaniterationâ€™scandidatesolutionemploysthe
estimation of a configurationâ€™s change point likelihood (cf. Sec-
tion3.2).Werandomlysampleamongthosecommitsforwhichthe
change point likelihood indicates a possible change point, but is
notconfident.Indetail,weselectanumberofcommitsforwhichthechangepointlikelihoodisgreaterthantheaverage(1divided
by thenumber of commits), butsmaller than the averageplus the
standard deviation of change point likelihood over all commits. By
sampling in this range of commits, we incorporate existing knowl-
edge (above average likelihood), but control over-fitting by only
sampling commits with maximum likelihood.
3.4.2 AcquiringConfigurations. Theexplorationofnewconfigu-
rationsissimilar totheinitialsampleselectionstrategy (distance-
based sampling) described in Section 3.1. We select a constant
number of configurations (default value: 75) for exploration.
The exploitation part of acquiring new configurations is guided by
the current set of candidate solutions. Each candidate solution de-
scribes a change point and associated configuration options. These
associated options can be a correct assignment or be an over- or
under-approximation.Inthelattercases,toomanyortoofewop-
tions are associated with a change point. We exploit the existingcandidate solutions in a way that addresses both under- as well
asover-approximation.Weselectnewconfigurationsusingacon-
straintsolverandthereforecanspecifyadditionalconstraints.In
thecaseofunder-approximation,toofewrelevantconfigurationop-tions are associated with a change point. Given a candidate change
point, we require that all associated options keep enabled and that
50%ofallnot-associatedoptionscanbeselected.Thisway,wekeep
already associated options, but allow new configuration optionsto be included. Likewise, in the case of over-approximation, too
manyirrelevantconfigurationoptionsareassociatedwithachange
point. Given a candidate change point, we require that variation
only occurs among 50% of the options associated with the change
point.Thatis,wecanremoveupto50%ofconfigurationoptions
and narrow down the selection of relevant configuration options.
We limit the total number of new measurements per iteration with
a budgetğ‘›measurements . This budget is split between acquisition for
commitsandconfigurationswithafactor ğ‘›commits_to_configs =0.5.
The measurement budget for commit acquisition is further split be-
tween exploration and exploitation with a factor ğ‘›commits_explore =
6160.5.Thatis,initially,50%ofthebudgetareused for configuration
acquisitionand25%forcommitexplorationandexploitation,re-
spectively. As the algorithm proceeds, we want to shift the budget
from an equal split between exploration and exploitation towards
exploitation.Therationaleisthatinearlyiterations,wefocuson
identifyingchangepointsintheconfigurationsâ€™performancehisto-
ries, whereas, in later iterations, our approach focuses on pinpoint-
ing configuration options to change points. Therefore, we multiply
the factor ğ‘›commits_to_configs by 0.95 and the factor ğ‘›commits_explore
by 0.9. That is, the algorithm shifts (1) towards sampling configura-tionsindepthand(2)focusesonexploitationinlateriterations.The
rationale is that we consider pinpointing commit to configuration
options a more difficult task than finding performance-changing
commits.
3.5 Solution Cache and Stoppage Criteria
After each iteration, we insert the candidate solution in a solution
cache.This solutioncache isamappingofassociationstoweights
indicatingadegreeofconfidence.Therationaleisthat,ifanassoci-
ation isincluded repeatedly inan iterationâ€™s candidatesolution, it
islikelyatruepositive(i.e.,atruechangepoint).Bycontrast,an
associationthatisincludedonlyafewtimesislikelyafalseposi-
tive and can be discarded. We update the solution cache after each
iteration in three steps. First, all associations that are either newly
included orhave been seen beforehave their weight increasedby
a constant factor ğ‘¤increase =1. Our default value for ğ‘˜is set to 3.
Second, the weights of all associations in the solution cache aremultiplied by a constant decrease factor
ğ‘’decreaseâˆˆ]0,1[.W es e t
thedefaultvaluefor ğ‘’decreaseto0.3.Last,weremoveallassociations
from the solutioncache if theirweight is smaller thana threshold
ğ‘¡drop. We define ğ‘¡dropas the weight an association exhibits if it
is included once in a candidate solution but not in ğ‘˜âˆˆNsucces-
sive iterations. Effectively, value for ğ‘¡dropis(ğ‘’decrease)ğ‘˜since the
increment ğ‘¤increaseis 1.
Similarly to the conditions for dropping an association from the
solutioncache,thealgorithmterminatesifnoassociationisdroppedfromthesolutioncachefor
ğ‘˜iterationsinarowandallassociationâ€™s
weights are greater than 1. As a fallback termination criterion, the
algorithm alsoterminates ifa user-specifiedmaximum numberofmeasurements ğ‘š
maxor number of iterations ğ‘–maxis reached.
4 EVALUATION
When evaluating our approach with a single experiment, we face a
conflict between internal and external validity, a problem that is
prevalent in software engineering research [ 27]. To assure internal
validity,theassessmentofourapproachwithrespecttoaccuracy
requires prior knowledge of change points as ground truth, which
is hardly obtainable as this would require exhaustive performance
measurements across commits and configurations. Moreover, to as-
sureexternalvalidity,werequireagreatdegreeofvariationamong
subject systems (e.g., number of commits and options, domains of
subjectsystems,etc.)tolearnaboutscalabilityandsensitivity.So,
for a fair assessment, we require not only a large set of systems,
butalsotherespectivegroundtruthperformancemeasurements.The caveat is that it is practically impossible to conduct such ex-
haustiveperformancemeasurementsinthelarge,whichwasthe
main reason for proposing our approach in the first place.
Toaddressthisdilemmainourevaluation,weconducttwosepa-
rateexperiments,basedonsyntheticandreal-worldperformance
measurements.Thefirstsetofexperimentsusessynthesizedper-
formance data providing a controlled experimental setup to assess
scalability,accuracy,andefficiencyatlowcostwhilesimultaneouslybeingabletosimulatedifferentscenariosbyvaryingthenumberof
change points and affected configurations in the synthesized data.
Thesecondsettofexperimentsusesabatchofreal-worldperfor-
mance measurements of three software systems as a necessarily
incomplete ground truth to explore whether our algorithm can be
practically applied to real-world systems.
Withthissplitexperimentsetup,weaimatansweringthefollowing
two research questions:
RQ1:Canweaccurately andefficiently identifyconfiguration-de-
pendent performance change points?
RQ2:Can wepractically identify configuration-dependent change
pointsin a real-world setting ?
4.1 Controlled Experiment Setup
We break down the research question RQ1into three objectives to
study the influence of the size of configurable software systems,
change point properties, and measurement effort.
4.1.1 InfluenceofSystemSize. Weareinterestedinhowthesize
ofaconfigurablesoftwaresysteminfluencestheaccuracyandef-
ficiencyofourapproach.Toanswerthisquestion,wesynthesize
performancedataforsystemsofvaryingsizeintermsofnumberof
configuration options and the number of commits. For the number
of configurations ğ‘›options, we selected a range that resembles con-
figurable software systems studied in previous work [ 19]; likewise,
we selected arange forthe number ofcommits ğ‘›commitstocover
young as well asmature software systems. We present the ranges
of the two size parameters in Table 1.
4.1.2 Influence of Change Point Properties. A change point may
correspond to a single option or an interaction among multiple op-
tions.Furthermore,twochangepointsmightbeonlyafewormanycommitsapart.Therefore,forthesynthesizedsoftwaresystems,we
varyboththetotalnumberofchangepointsaswellasthedegreeof
interactionsthatasoftwaresystemcontains.Thenumberofchange
points ranges from one to ten, reflecting findings of a recent study
about performance change points [ 19]. We sample the degree of
interactions from a geometric distribution (the discrete form of an
exponential distribution), which is specified by a single parameter
ğ‘between 0 and 1, henceforth called ğ‘interaction. The greater the
valueofğ‘interaction,thelesslikelywegeneratehigher-orderinter-
actions. The rationale of this setting stems from previous empirical
work [16,17] that has shown that, by far, most performance issues
arerelatedtoonlysingleoptions,orinteractionsoflowdegree.We
present the specific ranges of the two parameters in Table 1.
4.1.3 InfluenceofMeasurementEffort. Wewanttounderstandhow
the invested measurement effort affects accuracy and efficiency.
Aninitialsamplesetchosentoosmallorlargemightfailtocover
change pointsto exploitor wastemeasurement effort. In addition,
617Table 1: Parameter ranges for the synthetic experiment.
Parameter Range
Synthesized
Systemsğ‘›options 8, 16, 32, 64
ğ‘›commits 1000, 2500
ğ‘›changepoints 1, 2, 5, 10
ğ‘interaction 0.5, 0.7, 0.9
Initializationğ‘initial 2, 5, 10
ğ‘›measurements 100, 200, 500
thenumberofmeasurementsperiterationcanbeselectedirrespec-
tiveofthesoftwaresystemsize,resultingintoofewmeasurements
per configuration, and, thus failing to identify change points. That
is,fortheinitializationofouralgorithm,wevaryboththeinitial
number of configurations and the number of measurements per
iteration.Weselectthenumberofconfigurationsintheinitialsam-
plesetas ğ‘›changepoints timesthenumberofconfigurationoptions,
and the number of measurements per iteration from a range of
threevalues.Wefixthepercentageofcommitsperconfiguration
at3percent,whichhasbeenapromisingsamplingrateinprevious
work [19]. We present the ranges of the two parameters in Table 1.
4.1.4 Operationalization. We synthesize performance data by ini-
tializing each option with a randomly selected influence (cf. coeffi-
cients from Equation 1) from the range [âˆ’1,1]. In addition, we ran-
domly select a number of interactions among options to introduce
interactionsofvaryingdegrees.Theinteractiondegree(i.e.,number
of selected configuration options) follows a geometric distribution.
We assign to each interaction a real-valued influence uniformly
fromthe range [âˆ’1,1]. We definesix parameterranges (cf.Table 1)
for the number of configuration options ğ‘›options, the number of
commits ğ‘›commits, the number of change points ğ‘›changepoints , the
parameter for the geometric distribution of interaction degrees
ğ‘interaction,afactor ğ‘initial,andthenumberofmeasurementsper
iterationğ‘›measurements . The initial number of configurations is the
product of ğ‘›optionsandğ‘initial. We construct the Cartesian product
ofallparameterrangesandspecifyamaximumnumberof30itera-
tions.Inaddition,wesynthesizeforeachparametercombination
in the parameter grid five different software systems by employingdifferentseedssuchthatnoseedisusedtwice.Thetotalnumberof
experiments with seeds is 10,800.
For each parameter combination, we record the number of mea-
surementsateachiteration,therequirednumberofiterationsfor
termination, as well as each iterationâ€™s candidate solution. To as-
sesstheaccuracyofaparametercombinationâ€™soutcome(aswell
asofintermediateiterations),weusethe F1score,acombination
ofprecision andrecall. Precision refersto thefractionof correctly
identifiedassociations(truepositives)amongtheassociationsoftheretrieved(candidate)solution. Recallisthefractionoftotalnumber
of the relevant associations (i.e., those we intend to find). The F1
score is the harmonic mean of precision ( ğ‘ƒ) and recall ( ğ‘…), defined
asF1=2Â·ğ‘ƒÂ·ğ‘…
ğ‘ƒ+ğ‘…. In our context, we defined a correctly identified
changepoint whena commitsfalls ina narrow5 commitinterval
fromthegroundtruth.Toassessefficiency,weemploytherequired
numberofiterationsforterminationandrequiredmeasurementsTable 2: Project characteristics of subject systems.
Name #Options #Commits
xz 16 1193
lrzip 9 743
oggenc 12 935
in relation to the F1score as proxy metrics. The idea is that we
consider an experiment run as efficient if it provides an accurate
change point estimation with few iterations or few measurements.
4.2 Real-World Experiment Setup
Inthesecondexperiment,weevaluateourapproachfromaprac-
titionerâ€™s perspective, where the properties and whereabouts ofchange points are unknown. By means of three real-world con-figurable software systems, we investigate in particular practi-cal challenges and check whether our algorithm is able to detectperformance-relevant commits with respect to configuration op-
tions.
4.2.1 ExploratoryPre-Study. Ofcourse,wecannotobtaincomplete
ground truth data of our selected subject systems since the search
space is exponential in the number of configuration options. How-
ever,toprovidesomecontextforinterpretationofourresults,we
measuredperformanceforarepresentativesubsetofconfigurations
across all commits. To be precise, we sampled configurations using
feature-wise, negative feature-wise, pair-wise, and also uniform
random sampling strategies, which have been successfully applied
to learnperformance influencesbefore [ 1]. We sampledas many
randomconfigurationsastherearevalidconfigurationssampled
with pairwise sampling, which amounts to 79 configurations for
lrzip, 152 for oggenc, and 161 for xz. An overview of the three
software systems is given in Table 2.
As a workload for the file compression tools lrzip and xz, we used
the Silesia corpus, which contains over 200 MB of files of different
types.Itwasdesignedtocomparedifferentfilecompressionalgo-
rithmsandtoolsandhasbeenusedinpreviousstudies[ 19].Forthe
audio transcoder oggenc, we encoded a raw WAVE audio file of
over 60 MB from the Wikimedia Commons collection. For all three
subject systems, we assess performance by reporting the execution
time.
All measurements were conducted on clusters of Ubuntu machines
with Intel Core 2 Quad CPUs (2.83 GHz) and 8 GB of RAM (xz
and lrzip) and 16 GB of RAM (oggenc). To mitigate measurement
bias,we repeatedeachmeasurement fivetimes.The coefficientof
variation (the ratio of standard deviation and the arithmetic mean)
across allmachines waswell belowten percent.For commits that
did not build, we reported the performance measurement of the
most recent commit that did not fail to build.
4.2.2 Operationalization. For the actual experiment, we apply our
approachontheperformancemeasurementsobtainedinthepre-
study, with a few adjustments. First, although our study provides a
broadandrepresentativesamplesetofconfigurations,wecannot
arbitrarilysamplevalidconfigurationsforacoupleofreasons.In
particular,theseincludethecommitsthatdonotbuildaswellas
618hidden configuration options and constraints. While we collected
configuration options and constraints thoroughly from documenta-
tionartifacts,wecannotassurethatourselectioniscomplete.We
discuss this limitation further in Section 5.
Instead, when acquiring new configurations (cf. Section 3.4), we
selectconfigurationsfromourbatchofmeasurementsthat(1)have
not been used already by our algorithm, and (2) are "closest" to the
requested configuration (i.e., with the minimal Hamming distance).
The rationale is that this allows us to quickly run rapid repetitions
with different initializations of our algorithm. Since we employ
multiple sampling strategies in our pre-study, we are confident
that our broad batch of measurements is representative. Second,
we set the number of measurements per iteration ğ‘›measurements to
300.Third,asaninitialsampleset,werandomlysample5config-
urations from our pre-study set. Last, we initialize our approach
with a relative performance threshold ğœƒğœ(cf. Equation 3) of ten
percent.Inthecontextofthereportedrelativevariationamongper-
formance measurements on the machines used, we consider this a
rather conservative threshold. We repeat the experiments 10 times
with different seeds to quantify the robustness of our approach
andtoaccountfortherandomnessinexploringtheconfiguration
space.Weassesspracticalitybyreportingmeasurementeffort,num-
ber of iterations in relation to the pre-studyâ€™s results, which arebased on a vastly greater measurement budget. We qualitativelyinvestigate whether the detected commits and options are actu-ally causes of performance changes. That is, we analyze commit
messages of the respective repositories for the identified commits
tofindoccurrencesofoptionnamesorindicatorsofperformance
changes.Moreover,welookedintothecodechanges(e.g.,whenthe
commitmessagejuststatesâ€™mergeâ€™)torationalizeaboutpossible
performance affecting code changes.
4.3 Results
4.3.1RQ1(ControlledExperiment). Weillustratetheresultsofour
first set of experiments in Figures 5a, 5b, and 5c. The vast major-
ityofexperimentsterminatedwithinthelimitof30iterations,as
showninFigure5b.Asmallportionofexperiments,however,did
not meet our termination criteria (more on that below). From allexperiments that terminated in Figure 5a, we depict the F1 scoreafter they have terminated, that is, after their last iteration
2. For
most iterations, the mean F1 score falls around or over 0.7, with
thefirstquartileonlyslightlybeingaslowas0.5(atiteration29).
In the grand scheme of things, the vast majority of experiments
terminatedwithreasonablyhighF1score.Regardingmeasurement
effort, for small systems with 8 configuration options, we required
upto50%ofallpossiblemeasurements.Forgreatersystems,the
requiredmeasurementeffortwaswellbelow1%.Thehighreported
measurement effort for smaller systems is due to our choice of
the measurementsper iterations, which vastlyover-approximates.
Since we are able handle systems with more configuration options
with similar effort, we are confident that a smaller number of mea-
surements per iteration would reduce the relative measurement
effort required substantially.
2Duetospacelimitations,wereportprecisionandrecallonthepaperâ€™scompanion
Web site.(a) F1 score of experiments terminating at different iterations.
5 10 15 20 25 30
Iterations Required for Termination02004006008001000
(b) Frequency of experiments terminating at different iterations.
(c)Influenceofthenumberofchangepointsandinteractiondegree
on accuracy.
Figure 5: Result for the synthetic experiment.
In Figure 5c, we decompose the reported F1 scores to learn howthe number of change points or the interaction degree influence
ourapproach.Thex-axisshowsthehighestinteractiondegreeof
changepointsforanexperimentrun.Forinteractionsofadegree
up to 5,the F1 score ismostly above 0.5. We observe adownward
trend in the box plots: the more change points a system contained,
the less accurate our algorithmâ€™s prediction is.
Fortheexperimentsnotterminatingwithinlimit(therightmostbar
inFigure5b),weconductedanadditionalanalysis.Wecompared
whetherandhowtheparametersettingofTable1explainsthisnon-
termination.Onesettingstandsoutasthecausefornotfinishingthe
experiment within 30 iterations: the number of measurements per
iterations. The lower the number of measurements ( ğ‘›measurements ),
the more iterations our approach needs.
Summary (RQ1): We are able to identify and pinpoint config-
uration-dependent performance change points accurately and
atscale.Thenumberofmeasurementsperiterationisthemain
factor influencing how fast our algorithm terminates.
4.3.2RQ2(Pre-Study). For our three subject systems, we have
manually identified a number of commits, for which performance
changed substantially. We have found 7 change points for lrzip,
6192 for xz, and 2 for oggenc. For lrzip, 5 of 7, and for xz, 2 of 2
performance changes affect multiple configurations. By contrast, 2
changepointsforlrzipaffectallmeasuredconfigurations.Allofthe
measuredconfigurationsforoggencshowashiftinperformance.
That is, the identified change points for oggenc as well as the
two for lrzip are likely not configuration-dependent. To further
understand possible relations of change points with configuration
options,wesearchedincommitmessagesforclues.Wediscusstwo
notable findings.
For xz, the commit messages for both change points referenced
three particular configuration options (hc3, hc4,MatchFinder). For
lrzip, one commit message references configuration option (zpac).
Four of seven commit messages contained keywords relating to
performance. Two examples are the following:
"liblzma: Adjust default depth calculation for HC3
and HC4. [...] " (revision 626 of xz)
"Use ffsl for a faster lesser_bitness function. "
(revision 521 of lrzip)
Oftheremaining6commitsforlrzip,weidentifiedoneasasuc-
cessor to a merge commit. Within the related pull request3,w e
discovered that the configuration lzma was set to be ignored. This
plausibly corresponds to an observed performance shift for con-
figurations with this option enabled. However, this orphan config-
uration option results in an inconsistent configuration interface,
which we discovered with our approach.
4.3.3RQ2(Real-WorldPerformanceData). Acrosstherepetitions
ofourexperiments,wefoundanumberofcandidatesolutionsclose
to the two change points commits for xz and oggenc, respectively,
onaverageafter,3to5iterations.Aftermorethanfiveiterations,
the majority of repetitions pinpointed commits correctly within
ournarrow5commitinterval.Bycontrast,forlrzip,werequired
up to fifteeniterations to reach this temporal precision.Note that,
for lrzip, the number of change points is more than three times
greater than for the other two software systems. In addition, three
pairs of change points are 10 commits or less apart, which led to
falsely identified commits.
Theassociationofcommitstoconfigurability,withtheexception
of oggenc, is inconclusive. For oggenc, our approach consistently
reportedbothchangepointsasnotconfiguration-related.Forlrzip,
thereportedassociationsincludedavarietyofpossibleoptions,butdidnotconvergeandreportsolutionsinlinewithourpre-study.For
xz,themajorityof associationconfigurationoptionswereâ€match
finderâ€œ options referenced in the commit messages.
We attribute the inconclusive results for lrzip in part to the incon-
sistencyoftheconfigurationinterface.Forbothxzandlrzip,many
configurationoptionsarealternatives.Thatis,forafunctionality
suchasthecompressionalgorithm,onlyoneoftheavailableoptions
can be selected. In our approach, we employ LASSO regression for
automated feature selection. This regression model might, given
onlyasmallconfigurationsample,havedifficultieswithattributing
a change point to multiple alternative configuration options and
rather opt for one instead.
3https://github.com/ole-tange/lrzip/commit/1203a1853e892300f79da19f14aca11152b5b890Summary(RQ2):Ourapproachisabletopreciselylinkchange
points to commits with real-world data. Manually associating
commits with configuration options worked in many cases,
whereas the automated analysis was inconclusive due to in-
complete data in the repositories. We observed, at least, one
inconsistency in the configuration interface of one subject sys-
tem.
5 DISCUSSION
Efficiency. Among all experiments conducted with our synthetic
setup, our approach yielded reasonable to high accuracy across
mostparametersettings.Asubsequentanalysisofparameterset-
tings showed that our algorithm is not limited by the size of a
problem space, but rather by the number of measurements per iter-
ation. Our algorithm found multiple change points across different
parametersettings,includingthosewithlargenumbersofcommitsandconfigurationoptions.Thevastmajorityofcasesterminatedinearlyiterationswithhighaccuracy.Thatis,wewereabletotacklea
problem with exponential complexity (with respect to the number
of configuration options from which we can draw new measure-
ments) with a measurement budget that is linear in the number of
iterations (we used only 30) and the measurements per iteration
(between 100 and 500 measurements per iteration), respectively.
Hence,weareconfidentthatourapproachscalesandcanefficientlyapproximatechangepointsforlargesoftwaresystems.Notably,we
are able to reproduce this finding in real-world settings when iden-
tifyingthetemporallocationofchangepoints.Forourcontrolled
experimentsetup,wehavenottunedthealgorithmsâ€™parametersto
reachoptimalefficiencyoraccuracy.However,evenwithouttun-
ing,wedemonstratedthefeasibilityandefficiencyofourapproach.
Project-specific fine tuning of parameters may further improve the
performance of our approach.
Pinpointing Configuration Options. The synthetic setup has shown
that our algorithm can conceptually handle both time and space.
We were able to precisely identify commits for which performance
changed substantially. By contrast, we were not able to fully re-
producethisinasettingwithreal-worlddatawhenattemptingto
pinpointchangepointstoconfigurationoptions,whichweattribute
to missing information on and inconsistencies in the configuration
interface. Missinginformation in thereal-world setup emphasizes
the importance of a synthetic setup to evaluate approaches such as
ours and to explore possible limitations and necessary extensions.
So, we have documented that, given a precisely identified commit,
we can link the affected files to configuration options with little
effort. In turn, given a set of code segments of a commit, match-ing this with the overall code base has been extensively studied
beforeundertheumbrellaoffeaturelocation[ 9].Inconsistencies
in the documentation of configuration options is a well known
andprevalentproblem[ 23].Ourapproachcomplementsexisting
feature location techniques with information on configuration-
dependent performance changes. In our latter experiment, instead
of directly sampling new arbitrary configurations, we used a finite
set of configurations as a proxy (cf. Section 4.2.2). This does notreflectthe originallayout ofour algorithm,butlets usefficientlyexplore some limitations of the approach. We have learned that,for interpreting the results of our approach, domain knowledge
620isadvantageous,especially,aswehaveidentifiedincompleteand
inconsistent configuration interfaces as possible limiting factors.
We acknowledge that, while our approach performs well as an
automated pipeline in a controlled environment, for real-world
applications, a semi-automated setup with additional manual mea-
surement prioritization and deeper knowledge of configuration
constraints would improve the applicability of our approach.
ThreatstoValidity. Threatsto internalvalidity includemeasurement
noise, which may distort change point approximation. We mitigate
thisthreatbyrepeatingeachmeasurementfivetimesandreporting
the mean. With respect to the mean performance, the reported
variationwasbelow10%.ForRQ 2,weconsideredonlyperformance
changesof,atleast,10%.Hence,weareconfidentthatourrawdata
are robust against outliers. The setup in RQ 2relies on a limited set
of previously sampled configurations instead of actively acquiring
new ones. We mitigate this limitation by selecting a broad set of
configurations in our pre-study with different sampling strategies.
We sampled as many randomly selected configurations as there
arepairsofconfigurationoptions,resultinginareasonablyhigh
coverage of the configuration space.
Regarding externalvalidity,wecannotclaimthatwehaveidenti-
fied alllimitations possiblyarising in apractical setting.However,
weselectedpopularsoftwaresystemsthatarewelloptimizedfor
performance and often make use of configuration options to tune
performance.Althoughwehaveencounteredpossiblelimitationsin
a practical setting, our results, in conjunction with our exhaustive
syntheticstudy,arereproducible,andwewereabletotestcorner
cases and assess scalability so that we believe that our results hold
for many practical use cases.
6 RELATED WORK
Performance-relevant Commits. Identifying performance-changing
commits is a prevalent challenge in regression testing, often apply-
ingchangepoint detectionalgorithmsonbatchesofperformance
observations [ 2,3]. A way to reduce testing overhead is to em-
ploy only a fraction of performance observations or to prioritize
commits. Alcocer et al. assess performance for uniformly selected
commitstoestimatetheriskofaperformancechangeintroduced
byunobservedcommits[ 24,25].Huangetal.estimatetheriskof
performancechangessolelybasedonstaticanalysisofindividual
commits to prioritize commits for regression testing without as-
sessing performance [ 10]. MÃ¼hlbauer et al. use Gaussian Processes
anditerativesamplingtolearnperformancehistorieswithfewper-
formanceobservations[ 19].They extractperformancechanging
commits by applying change point detection algorithms to learned
performance histories. All of the above approaches reduce test-
ing overhead, but do not address the performance across different
configurations of software systems. Our approach incorporates
performance changes across different configurations by leveraging
similarities in related configurationâ€™s performance histories.
Performance of Configurable Software Systems. Associating configu-
rationoptionsandinteractionswiththeirinfluenceonperformance
is a conceptual basis for our work. There exists extensive workonmodelingconfigurationoptionsasfeaturesformachinelearn-
ing,suchasclassificationandregressiontrees[ 4,6,20,26],multi-
variableregression[ 28],anddeepneuralnetworks[ 7].Predictive
models for software performance can be used to find optimal soft-
wareconfigurations[ 21].Incontrasttolearningperformancefor
arbitraryconfigurations,ourapproachaimsatfindingperformance
changes,andsubsequently,pinpointingthemtoconfigurationop-
tions and interactions. A similar scenario to ours has been studied
byJamshidietal.[ 11â€“13].Betweensoftwareversions,theperfor-
mance influence of only few configuration options changes. Theauthors propose two approaches for learning a transfer functionbetween performance models of different environments, such as
versions, based on Gaussian Processes [ 13] and progressive shrink-
ing the configuration space [ 12]. We see great potential for future
work in applying Gaussian Processes to our problem.
7 CONCLUSION
Finding optimal configurations and relevant configuration options
that influence the performance of a software system is alreadychallenging,butinthelightofsoftwareevolution, configuration-
dependent performance changes may lurk in a potentially large
numberofdifferentversionsofthesystem.Consequently,detecting
configuration-dependent performance changes by measuring allconfigurations is often intractable in practice. We devise a novel
approachtoidentifyconfiguration-dependentperformancechanges
withloweffort.Startingfromasmallsampleofmeasurements(pairs
of configurations and commits), our approach iteratively selects
new measurements along two dimensions: time and configuration
space.Theselectionisguidedbyanestimationofthelikelihoodthatacommitintroducesaperformancechangeaswellasbyidentifyingconfigurationoptionsthatarerelevantforperformance,bothbased
on previous measurements.
To demonstrate soundness and efficiency, we applied our approach
to a synthetic data set modelling different software systems that
vary in the length of their version history and in the complexity
of their configuration space. With only few measurements, our
approach was able to accurately identify configuration-dependent
performance changes for software systems with thousands of com-
mits and millions of configurations. To demonstrate feasibility in a
practicalsetting,weappliedourapproachtoperformancedataof
three substantial configurable software systems. Again, we were
able to detect performance changes accurately, and we even identi-
fied aninconsistency inthe configuration interfaceof lrzip. While
pinpointingperformancechangestoindividualconfigurationop-
tions was possible by hand, an automate analysis was inconclusive.
In further work, we will supplement our approach with results
fromstaticanalysisandcommitmessages,increasingaccuracyand
providingamoreholisticpictureofperformancebehavioracross
configuration space and project evolution.
8 ACKNOWLEDGMENTS
Apelâ€™s work has been supported by the German Research Foun-
dation (DFG) under the contract AP 206/11-1. Siegmundâ€™s work
has been supported by the DFG under the contracts SI 2171/2 and
SI 2171/3-1. We would also like to thank our reviewers for their
thoughtful comments.
621REFERENCES
[1]JulianaAlvesPereira,MathieuAcher,HugoMartin,andJean-MarcJÃ©zÃ©quel.2020.
Sampling Effect on Performance Prediction of Configurable Systems: A Case
Study.In ProceedingsoftheACM/SPECInternationalConferenceonPerformance
Engineering (ICPE). ACM, 277â€“288. https://doi.org/10.1145/3358960.3379137
[2]JÃ¼rgenCito,DritanSuljoti,PhilippLeitner,andSchahramDustdar.2014. Identify-
ing Root Causes of Web Performance Degradation Using Changepoint Analysis.
InProceedingsoftheInternationalConferenceonWebEngineering(ICWE).Springer,
181â€“199. https://doi.org/10.5167/uzh-96073
[3]David Daly, William Brown, Henrik Ingo, Jim Oâ€™Leary, and David Bradford.
2020. IndustryPaper:TheUseofChangePointDetectiontoIdentifySoftware
Performance Regressions in a Continuous Integration System. In Proceedings of
the ACM/SPEC International Conference on Performance Engineering (ICPE). ACM,
67â€“75. https://doi.org/10.1145/3358960.3375791
[4]Jianmei Guo,Krzysztof Czarnecki, SvenApely, NorbertSiegmund, andAndrzej
Wasowski.2013. Variability-awarePerformancePrediction:AStatisticalLearning
Approach.In ProceedingsoftheInternationalConferenceonAutomatedSoftware
Engineering (ASE). IEEE, 301â€“311. https://doi.org/10.1109/ASE.2013.6693089
[5]JianmeiGuo,JulesWhite,GuangxinWang,JianLi,andYinglinWang.2011. A
GeneticAlgorithmforOptimizedFeatureSelectionwithResourceConstraints
inSoftwareProductLines. JournalofSystemsandSoftware(JSS) 84,12(2011),
2208â€“2221. https://doi.org/10.1016/j.jss.2011.06.026
[6]Jianmei Guo, Dingyu Yang, Norbert Siegmund, Sven Apel, Atrisha Sarkar, Pavel
Valov, Krzysztof Czarnecki, Andrzej Wasowski, and Huiqun Yu. 2018. Data-
efficient performance learning for configurable systems. Empirical Software
Engineering 23,3 (2018),1826â€“1867. https://doi.org/10.1007/s10664-017-9573-6
[7]Huong Ha and Hongyu Zhang. 2019. DeepPerf: Performance Prediction for
Configurable Software with Deep Sparse Neural Network. In Proceedings of
the International Conference on Software Engineering (ICSE). IEEE, 1095â€“1106.
https://doi.org/10.1109/ICSE.2019.00113
[8]Xue Han and Tingting Yu. 2016. An Empirical Study on Performance Bugsfor Highly Configurable Software Systems. In Proceedings of the International
SymposiumonEmpiricalSoftwareEngineeringandMeasurement(ESEM).ACM,
1â€“10. https://doi.org/10.1145/2961111.2962602
[9]EmilyHill,AlbertoBacchelli,DaveBinkley,BogdanDit,DawnLawrie,andRocco
Oliveto. 2013. Which Feature Location Technique is Better?. In Proceedings of
theACM/SPECInternationalConferenceonSoftwareMaintenanceandEvolution
(ICMSE). IEEE, 408â€“411. https://doi.org/10.1109/ICSM.2013.59
[10]PengHuang,XiaoMa,DongcaiShen,andYuanyuanZhou.2014. Performance
RegressionTestingTargetPrioritizationviaPerformanceRiskAnalysis.In Pro-
ceedings of the International Conference on Software Engineering (ICSE). ACM,
60â€“71. https://doi.org/10.1145/2568225.2568232
[11]PooyanJamshidi,NorbertSiegmund,MiguelVelez,ChristianKÃ¤stner,Akshay
Patel,andYuvrajAgarwal.2017. TransferLearningforPerformanceModeling
of ConfigurableSystems: AnExploratoryAnalysis. In Proceedingsof theInter-
national Conference on Automated Software Engineering (ASE). IEEE, 497â€“508.
https://doi.org/10.1109/ASE.2017.8115661
[12]Pooyan Jamshidi, Miguel Velez, Christian KÃ¤stner, and Norbert Siegmund. 2018.
LearningtoSample:ExploitingSimilaritiesacrossEnvironmentstoLearnPer-
formanceModelsforConfigurableSystems.In ProceedingsoftheJointMeeting
of the European Software Engineering Conference and the ACM SIGSOFT Sym-posium on the Foundations of Software Engineering (ESEC/FSE). ACM, 71â€“82.
https://doi.org/10.1145/3236024.3236074
[13]PooyanJamshidi,MiguelVelez,ChristianKÃ¤stner,NorbertSiegmund,andPrasad
Kawthekar. 2017. Transfer Learning for Improving Model Predictions in Highly
ConfigurableSoftware.In ProceedingsoftheInternationalSymposiumonSoftware
EngineeringforAdaptiveandSelf-ManagingSystems(SEAMS).IEEE,31â€“41. https:
//doi.org/10.1109/SEAMS.2017.11
[14]Christian Kaltenecker, Alexander Grebhahn, Norbert Siegmund, Jianmei Guo,
andSvenApel.2019. Distance-basedSamplingofSoftwareConfigurationSpaces.
InProceedings of the International Conference on Software Engineering (ICSE).
IEEE, 1084â€“1094. https://doi.org/10.1109/ICSE.2019.00112
[15]SergiyKolesnikov,NorbertSiegmund,ChristianKÃ¤stner,andSvenApel.2019.
OntheRelationof Control-FlowandPerformanceFeatureInteractions:ACase
Study.Empirical Software Engineering (ESE) 24, 4 (2019), 2410â€“2437. https:
//doi.org/10.1007/s10664-019-09705-w
[16]SergiyKolesnikov,NorbertSiegmund,ChristianKÃ¤stner,AlexanderGrebhahn,
andSvenApel.2019. TradeoffsinModelingPerformanceofHighlyConfigurableSoftwareSystems. SoftwareandSystemsModeling(SoSyM) 18,3(2019),2265â€“2283.
https://doi.org/10.1007/s10270-018-0662-9
[17]FlÃ¡vio Medeiros, Christian KÃ¤stner, MÃ¡rcio Ribeiro, Rohit Gheyi, and Sven Apel.
2016. A Comparison of 10 Sampling Algorithms for Configurable Systems. In
Proceedings of the International Conference on Software Engineering (ICSE). ACM,
643â€“654. https://doi.org/10.1145/2884781.2884793
[18]Ian Molyneaux. 2015. The Art of Application Performance Testing (2nd ed.).
Oâ€™Reilly, Beijing.[19]Stefan MÃ¼hlbauer, Sven Apel, and Norbert Siegmund. 2019. Accurate Modeling
ofPerformanceHistoriesforEvolvingSoftwareSystems.In Proceedingsofthe
InternationalConferenceonAutomatedSoftwareEngineering(ASE) .IEEE,640â€“652.
https://doi.org/10.1109/ASE.2019.00065
[20]Vivek Nair, Tim Menzies, Norbert Siegmund, and Sven Apel. 2017. Using Bad
Learners to Find Good Configurations.In Proceedings of the JointMeeting of the
European Software Engineering Conference and the ACM SIGSOFT Symposiumon the Foundations of Software Engineering (ESEC/FSE) . ACM, 257â€“267. https:
//doi.org/10.1145/3106237.3106238
[21]Vivek Nair, Zhe Yu, Tim. Menzies, Norbert Siegmund, and Sven Apel. 2020.Finding Faster Configurations Using FLASH. IEEE Transactions on Software
Engineering(TSE) 46,7(2020),794â€“811. https://doi.org/10.1109/TSE.2018.2870895
[22]John Ousterhout. 2018. Always Measure One Level Deeper. Communications of
the ACM 61 (2018), 74â€“83. https://doi.org/10.1145/3213770
[23]Ariel Rabkin and Randy Katz. 2011. Static Extraction of Program Configuration
Options.In ProceedingsoftheInternationalConferenceonSoftwareEngineering
(ICSE). ACM, 131â€“140. https://doi.org/10.1145/1985793.1985812
[24]JuanPabloSandovalAlcocer,AlexandreBergel,andMarcoTulioValente.2016.
Learning from Source Code History to Identify Performance Failures. In Proceed-
ingsoftheACM/SPECInternationalConferenceonPerformanceEngineering(ICPE).
ACM, 37â€“48. https://doi.org/10.1145/2851553.2851571
[25]JuanPabloSandovalAlcocer,AlexandreBergel,andMarcoTulioValente.2020.
PrioritizingVersionsforPerformanceRegressionTesting:ThePharoCase. Science
of Computer Programming 191 (2020), 102415. https://doi.org/10.1016/j.scico.
2020.102415
[26]Atri Sarkar, Jianmei Guo, Norbert Siegmund, Sven Apel, and Krzysztof Czar-
necki. 2015. Cost-Efficient Sampling for Performance Prediction of Configurable
Systems. In Proceedings of the International Conference on Automated Software
Engineering (ASE). IEEE, 342â€“352. https://doi.org/10.1109/ASE.2015.45
[27]Janet Siegmund, Norbert Siegmund, and Sven Apel. 2015. Views on Internal
andExternalValidityinEmpiricalSoftwareEngineering.In Proceedingsofthe
International Conference on Software Engineering (ICSE) . IEEE, 9â€“19. https:
//doi.org/10.1109/ICSE.2015.24
[28]NorbertSiegmund,AlexanderGrebhahn,SvenApel,andChristianKÃ¤stner.2015.
Performance-Influence Models for Highly Configurable Systems. In Proceedings
of the Joint Meeting of the European Software Engineering Conference and the
ACMSIGSOFTSymposiumontheFoundationsofSoftwareEngineering(ESEC/FSE).
ACM, 284â€“294. https://doi.org/10.1145/2786805.2786845
[29]Norbert Siegmund, Sergiy Kolesnikov, Christian KÃ¤stner, Sven Apel, Don Ba-
tory,MarkoRosenmÃ¼ller,andGunterSaake.2012. PredictingPerformancevia
AutomatedFeature-InteractionDetection.In ProceedingsoftheInternationalCon-
ferenceonSoftwareEngineering(ICSE).IEEE,167â€“177. https://doi.org/10.1109/
ICSE.2012.6227196
[30]RobertTibshirani.1996. Regressionshrinkageandselectionviathelasso. Journal
oftheRoyalStatisticalSociety:SeriesB(Methodological) 58(1996),267â€“288. https:
//doi.org/10.1111/j.2517-6161.1996.tb02080.x
[31]JulesWhite,BrianDougherty,andDouglasC.Schmidt.2009. SelectingHighly
Optimal Architectural Feature Sets with Filtered Cartesian Flattening. Journal of
Systems and Software (JSS) 82 (2009), 1268â€“1284. https://doi.org/10.1016/j.jss.
2009.02.011
622