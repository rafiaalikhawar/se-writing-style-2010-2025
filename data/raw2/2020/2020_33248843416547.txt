Owl Eyes: Spotting UI Display Issues via Visual Understanding
Zhe Liu1,3,Chunyang Chen4, Junjie Wang1,2,3,âˆ—, Yuekai Huang1,3, Jun Hu1,3, Qing Wang1,2,3,âˆ—
1Laboratory for Internet Software Technologies,2State Key Laboratory of Computer Sciences,
Institute of Software Chinese Academy of Sciences, Beijing, China;
3University of Chinese Academy of Sciences, Beijing, China;âˆ—Corresponding author
4Monash University, Melbourne, Australia;
liuzhe181@mails.ucas.edu.cn,Chunyang.chen@monash.edu,junjie@iscas.ac.cn,wq@iscas.ac.cn
ABSTRACT
Graphical User Interface (GUI) provides a visual bridge between
asoftwareapplicationandendusers,throughwhichtheycanin-
teract with each other. With the development of technology and
aesthetics,thevisualeffectsoftheGUIaremoreandmoreattracting.
However, such GUI complexity posts a great challenge to the GUI
implementation. According to our pilot study of crowdtesting bug
reports, display issues such as text overlap, blurred screen, missing
image always occur during GUI rendering on different devices due
tothesoftwareorhardwarecompatibility.Theynegativelyinflu-
ence the app usability, resulting in poor user experience. To detect
these issues, we propose a novel approach, OwlEye, based on deep
learning for modelling visual information of the GUI screenshot.
Therefore, OwlEyecan detect GUIs with display issues and also
locate the detailed region of the issue in the given GUI for guiding
developerstofixthebug.Wemanuallyconstructalarge-scalela-
belleddatasetwith4,470GUIscreenshotswithUIdisplayissuesand
develop a heuristics-based data augmentation method for boosting
the performance of our OwlEye. The evaluation demonstrates that
ourOwlEyecanachieve85%precisionand84%recallindetecting
UIdisplayissues,and90%accuracyinlocalizingtheseissues.We
also evaluate OwlEyewith popular Android apps on Google Play
and F-droid, and successfully uncover 57 previously-undetected UI
display issues with 26 of them being confirmed or fixed so far.
KEYWORDS
UI display, Mobile App, UI testing, Deep Learning
ACM Reference Format:
Zhe Liu1,3,Chunyang Chen4, Junjie Wang1,2,3,âˆ—, Yuekai Huang1,3, Jun Hu1,3,
Qing Wang1,2,3,âˆ—. 2020. Owl Eyes: Spotting UI Display Issues via Visual Un-
derstanding. In 35th IEEE/ACM International Conference on Automated Soft-
ware Engineering (ASE â€™20), September 21â€“25, 2020, Virtual Event, Australia.
ACM,NewYork,NY,USA,12pages.https://doi.org/10.1145/3324884.3416547
1 INTRODUCTION
Graphic User Interface (GUI, also short for UI) is ubiquitous in
almost all modern desktop software and mobile applications. It
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACM
mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ASE â€™20, September 21â€“25, 2020, Virtual Event, Australia
Â© 2020 Association for Computing Machinery.
ACM ISBN 978-1-4503-6768-4/20/09...$15.00
https://doi.org/10.1145/3324884.3416547provides a visual bridge between a software application and end
users through which they caninteract with each other. Designing
a UI requires proper user interaction, information architecture and
visual effects of the UI. A good GUI design makes an application
easy,practicalandefficienttouse,whichsignificantlyaffectsthe
success of the application and the loyalty of its users [27].
However,moreandmorefancyvisualeffectsinGUIdesignsuch
asintensivemediaembedding,animation,lightandshadowsposta
greatchallengefordevelopersintheimplementation.Consequently,
many display issues1such astext overlap, missing image, blurred
screenasseeninFigure1alwaysoccurduringtheUIdisplayprocess
especially on different mobile devices.
MostofthoseUIdisplayissuesarecausedbydifferentsystem
settings in different devices, especially for Android, as there are
morethan10majorversionsofAndroidOSrunningon24,000+dis-
tinctdevicemodelswithdifferentscreenresolutions[ 64].Although
the software can still run along with these bugs, they negatively
influencethefluentusagewiththeapp,resultinginthesignificantly
bad user experience and corresponding loss of users. Therefore,
this study is targeting at detecting those UI display issues.
Figure 1: Examples of UI display issues
ToensurethecorrectnessofUIdisplaying,companieshavetore-
cruitmanytestersforappGUItestingorleveragethecrowdtesting.
Although human testers can spot these UI display issues, there are
stilltwoproblemswithsuchmechanism.First,itrequiressignificant
humaneffortastestershavetomanuallyexploretensofpagesby
different interactive ways and also need to check the UI display on
differentOSversionsanddeviceswithdifferentresolutionorscreen
size.Second,someerrorsintheGUIdisplay,especiallyrelatively
minoronessuchas textoverlap,componentocclusion ,aredifficult
to spot manually. To overcome those issues, some app develop-
ment teams adopt the Rapid Application Development (RAD) [ 43],
1wecallthesebugsasUIdisplayissues,andwillinterchangablelyuse bugandissuein
this paper.
3982020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE)
ASE â€™20, September 21â€“25, 2020, Virtual Event, Australia Zhe Liu1,3,Chunyang Chen4, Junjie Wang1,2,3,âˆ—, Yuekai Huang1,3, Jun Hu1,3, Qing Wang1,2,3,âˆ—
whichfocusesondevelopingapplicationsrapidlythroughfrequent
iterations and continuous feedback. They utilize usersâ€™ feedback
to reveal UI display issues, but it is a reactive way for bug fixing
which may have already hurt users of the app, resulting in the loss
of market shares.
Incomparisonwithobtainingfeedbackfromusersforreactive
app UI assurance, we need a more proactive mechanism whichcould check the UI display before the app release, automatically
spotthepotentialissuesintheGUI,andremindthedevelopersto
fixissuesifany.TherearemanyresearchworksonautomatedGUI
testing [3,17,20,24,39,42,44,45,48,49,60] by dynamically ex-
ploringdifferentpageswithrandomactions(e.g.,clicking,scrolling,
filling in the text) until triggering the crash bugs or explicit excep-
tions. Some practical automated testing tools like Monkey [ 21,65],
Dynodroid [ 40] are also widely used in industry. However, these
automatedtoolscanonlyspotcriticalcrashbugs,ratherthanUIdis-
playissueswhichcannotbecapturedbythesystem.Inthiswork,
we aim at detecting the UI display issues with the screenshots
generated during automatic testing by visual understanding.
TounderstandthecommonUIrenderingissues,wefirstcarryout
a pilot study on 10,330 non-duplicate screenshots from 562 mobile
application crowdtesting tasks to observe display issues in these
screenshots. Results show that a non-negligible portion (43.2%) of
screenshots are of display issues which can seriously impact theuser experience, and degrade the reputation of the applications.
Besides, we also examine 1,432 screenshots randomly-chosen from
the commonly-used Rico dataset [ 19], and find 1.2% screenshots
having UI display issues. The common categories of UI display
issuesinclude componentocclusion,textoverlap,missingimage,null
valueandblurred screen. Considering its popularity and lack of
support in current practice of automatic UI testing, it would be
valuableforclassifyingthescreenshotswithUIdisplayissuesfrom
the plenty of screenshots generated during UI testing.
Inspired by the fact that display bugs can be easily spotted by
humaneyes,weproposeanapproach, OwlEye2tomodelthevisual
informationbydeeplearningtoautomaticallydetectandlocalizeUIdisplayissues.Our
OwlEyebuildsontheConvolutionalNeuralNet-
work (CNN) to identify the screenshots with UI display issues, and
utilizes Gradient weighted Class Activation Mapping (Grad-CAM)
to localize the regions with UI display issues in the screenshotsforguidingdeveloperstofixthebug.Toovercomethelackofla-beled data for training our model, we develop a heuristics-based
dataaugmentationmethodtogeneratescreenshotswithUIdisplay
issues from bug-free UI images. We then integrate OwlEyewith
DroidBot[ 36]whichcandynamicallyexploredifferentpagesofthe
mobile apps as a fully automatic tool from collecting the screen-
shotstodetectandlocalizeUIdisplayissues.Notethatonestrength
of our approach over conventional program analysis is that it can
be applied to any platform including Android, iOS, and it takes the
screenshot as theinput which is easy to beobtained in real-world
practice.
Toevaluatetheeffectivenessofour OwlEye,wecarryoutalarge-
scaleexperimenton8,940screenshotsfromcrowdtestingand15,640
augmented screenshots from 7,820 Android apps. Compared with
2Our approach is named as OwlEyeas it is like owlâ€™s eyes to effectively spot UI
displayissues.Andourmodel(nocturnallikeowl)cancomplementwithconventional
automated GUI testing (diurnal like eagle) for ensuring the robustness of the UI.13 state-of-the-art baselines, our OwlEyecan achieve more than
17%and50%boostinrecallandprecisioncomparedwiththebest
baseline, resulting in 85%precision and 84% recall. As our OwlEye
can also locate the detailed position of the bug in the UI, we carry
out a user study to check its accuracy and the results demonstrate
that 90% of bug locations are correct. Apart from the accuracyof our
OwlEye, we also evaluate the usefulness of our OwlEyeby
applyingitindetectingtheUIdisplayissuesinthereal-worldapps
from Google Play and F-Droid. Among 329 apps, we find that 57of them are with UI display issues. We issued bug reports to the
development team and 26 are confirmed and fixed by developers.
The contributions of this paper are as follows:
â€¢Thisisthefirstworktoconductasystematicalinvestigation
ofUIdisplayissuesinreal-worldmobileapps.Wedevelop
afirstlarge-scaledatasetofappUIswiththatkindofbugs
and release it for follow-up studies.
â€¢Based on our pilot findings, we propose a novel approach
OwlEye3withCNN-basedmodelfordetectingscreenshots
with UI display issues, and Grad CAM-based model for lo-
calizing the buggy region in the UI.
â€¢We also propose a heuristics-based training data augmenta-
tionmethod whichcan automaticallygenerate screenshots
of UI display issues with bug-free UI images.
2 MOTIVATIONAL STUDY
TobetterunderstandtheUIdisplayingissuesinreal-worldpractice,
wecarryoutapilotstudytoexaminetheprevalenceoftheseissues.
The pilot study also explores what kindsof UI display issues exist,
so as to facilitate the design of our approach for detecting UIs with
display issues.
2.1 Data Collection
Ourexperimentaldatasetiscollectedfromoneofthelargestcrowd-
testing platforms4in which crowd workers are required to submit
testreportsafterperformingtestingtasks[ 62,63].Thedatasetcon-
tains562Androidmobileapplicationcrowdtestingtasksbetween
January2015andSeptember2016.Theseappsbelongtodifferent
categoriessuchasnews, entertainment,medical,etc.Ineachtask,
crowd workers submit hundreds of testing reports which describe
howthetestingisconductedandwhathappenedduringthetest,as
wellasaccompaniedscreenshotsofthetesting.Thereasonwhywe
utilizethisdatasetisthatitincludesboththeUIscreenshotsandthe
corresponding bug description which facilitates the searching and
analysisofUIdisplayissues.Thisdatasetcontains10,330unique
GUI screenshots.
2.2 Categorizing UI Display Issues
GiventhoseGUIscreenshots,thefirstthreeauthorsindividually
checkeachof themmanuallywithalso itscorrespondingdescrip-
tioninthebugreport.OnlyGUIscreenshotswiththeconsensusfrom all three human markers are regarded as ones with displayissues. A total of 4,470 GUI screenshots are determined with UI
3https://github.com/20200501/OwlEye for the dataset and source code of OwlEye, and
the detailed experimental results of this paper.
4Baidu(baidu.com)isthelargestChinesesearchserviceprovider.Itscrowdsourcing
test platform (test.baidu.com) is also the largest ones in China.
399Owl Eyes: Spotting UI Display Issues via Visual Understanding ASE â€™20, September 21â€“25, 2020, Virtual Event, Australia
Figure 2: Examples of five categories of UI display issues
display issues, which accounts for 43.2% (4470/10330) in all screen-
shots. This result indicates that the UI display issues account for a
non-negligible portion of mobile application bugs revealed during
crowdtesting and should be paid careful attention for improving
the software quality.
During the manually examination process, we notice that there
are different types of UI issues, a categorization of these issues
would facilitate the design and evaluation of related approach. Fol-
lowingtheCardSorting[ 59]method,weclassifythoseUIissues
into five categories including component occlusion, text overlap,
missing image, null value andblurred screen with details as follows:
Componentocclusion(47%) :AsshowninFigure2(a),thetex-
tualinformationorcomponentisoccludedbyothercomponents.
ItusuallyappearstogetherwithTextVieworEditText.Themain
reasons are as follows: the improper setting of elementâ€™s height, or
the adaptive issues triggered when setting a larger-sized font.
Text overlap (21%) : As shown in Figure 2(b), two pieces of
textareoverlappedwitheachother.Thismightbecausedbythe
adaptiveissuesamongdifferentdevicemodels,e.g.,whenusinga
larger-sized font in a device model with small screen might trigger
this bug.
Notethat,fortextoverlapcategory,twopiecesoftextaremixed
together;whileforcomponentocclusion,onecomponentcovers
part of the other component.
Missing image (25%) :AsshowninFigure2(c),intheiconpo-
sition, the image is not showing as its design. The possible reasons
are as follows: wrong image path or layout position, unsuccess-
fulloadingoftheconfigurationfileduetopermissions,oversized
image, network connection, code logic, or picture errors, etc.
NULLvalue(6%) :AsshowninFigure2(d),therightinformation
isnot displaying,instead NULLisshowing incorresponding area.
This category of bugs usually occurs with TextView. The main
reasons are as follows: issues in parameter setting or database
reading,andthelengthoftextinTextViewexceedingthethreshold,
etc.
Blurred screen (1%) : As shown in Figure 2(e), the screen is
blurred. The reason for this bug might because the defects in hard-
ware, or the exclusion of hardware acceleration for some CPU- or
GPU- demanding functionalities.
Tofurthervalidatethegeneralityofourobservations,wealso
manuallycheck1,432screenshotsfrom 200random-chosenappli-
cations in Rico dataset5[19] , which is a commonly-used mobile
5http://interactionmining.org/rico#quick-downloadsapplication dataset with 66K UI screenshots of Android Applica-
tionsandwewillfurtherintroducethatdatasetonSection4.We
find that 18 UIs from 16 apps (16/200 = 8.8% apps) are with UI
displayissues.Notethatnumberishighlyunderestimated,asthe
collected UIs do not cover all pages of the applications, and the
applications are not fully tested on different devices with different
screen resolutions.
2.3 Why Visual Understanding in Detecting UI
Display Issues
These findingsconfirm theseverity of UIdisplay issues, andmoti-
vateustodesignapproachforautomaticallydetectingtheseGUI
issues. One commonly-used practice for bug detection in mobile
apps is program analysis, but it may not be suitable in this senario.
Toapplytheprogramanalysis,oneneedtoinstrumentthetarget
app, develop different rules for different types of UI display issues,
rewritethecodefordifferentplatforms(e.g.,iOS,Android),andcus-
tomizetheircodetobecompatibleondifferentmobiledevices(e.g.,Samsung,Huawei,etc)withdifferentscreenresolution,whichisex-tremelyeffort-consuming.Specifically,itisnottrivialtoenumerate
all display issues and develop corresponding rules for detection.
Takeninthissense,it isworthwhiledevelopinganewefficient
and general method for detecting UI display issues. Inspired bythe fact that these display issues can be spotted by human eyes,
we propose to identify these buggy screenshots with visual under-
standingtechniquewhichimitatesthehumanvisualsystem.Asthe
UI screenshots are easy to fetch (either manually or automatically)
andexertnosignificantdifferenceacrosstheappsfromdifferent
platforms ordevices, ourimage-based approach aremore flexible
and easy to deploy.
3 ISSUES DETECTION AND LOCALIZATION
APPROACH
Thispaperproposes OwlEyetoautomaticallydetectandlocalizeUI
displayissuesinthescreenshotsoftheapplicationundertest,as
showninFigure3.GivenoneUIscreenshot,ourCNN-basedmodel
canfirstclassifyifitrelateswithanydisplayissuesviathevisual
understanding. Once the issue is confirmed, our model can further
localize the detailed issue position on the UI screenshot by Grad
CAM-based model for guiding developers to fix the bug.
3.1 CNN-based UI Display Issues Detection
As the UI display issues can only be spotted via the visual informa-
tion, we adopt the convolutional neural network (CNN) [ 31,34],
400ASE â€™20, September 21â€“25, 2020, Virtual Event, Australia Zhe Liu1,3,Chunyang Chen4, Junjie Wang1,2,3,âˆ—, Yuekai Huang1,3, Jun Hu1,3, Qing Wang1,2,3,âˆ—
Figure 3: Overview of OwlEye
whichhasproventobeeffectiveinimageclassificationandrecogni-
tionincomputervision[ 25,58,61].Figure4showsthestructureof
ourclassificationmodelwhichlinkstheconvolutionallayers,batch
normalization layers, pooling layers, and fully-connected layers.
GiventheinputUIscreenshot,weconvertitintoacertainimage
size with fixed width and height as ğ‘¤Ã—â„. Convolutional layerâ€™s
parametersconsist ofa setoflearnable filters.The purposeofthe
convolutional operation is to extract the different characteristics
of the input (i.e., feature extraction). After convolutional layer, the
screenshots will be abstracted as feature graph.
InordertoimprovetheperformanceandstabilityofCNN,we
addBatchNormalization(BN)[ 26]afterconvolutionallayer,and
standardizetheinputlayerbyadjustingandscalingactivation.In
aneuralnetwork,batchnormalizationisimplementedthrougha
normalization step that fixes the mean and variance of each layerâ€™s
inputs.Indetail,thestepsforbatchnormalizationareshownbelow:
ğ‘¦=ğ‘“âˆ’ğ‘šğ‘’ğ‘ğ‘›(ğ‘“)/radicalbig
ğ‘£ğ‘ğ‘Ÿ(ğ‘“)+ğœ–(1)
Considering a batch training, we input feature as ğ‘“, then calculate
themean( ğ‘šğ‘’ğ‘ğ‘›())andvariance( ğ‘£ğ‘ğ‘Ÿ())ofğ‘“,ğœ–isaddedinthedenom-
inatorfornumericalstabilityandisanarbitrarilysmallconstant.
Figure 4: The architecture of CNN
After the BN layer, the Rectified Linear Unit (ReLU) is added
astheactivationfunctionofthenetwork.Itincreasesthenonlin-
earpropertiesofthedecisionfunctionandoftheoverallnetwork
withoutaffectingthereceptivefields.ReLUperformsathreshold
operation to eachelement of the input, whereany value less than
zero is set to zero.The BN layer is then followed by the pooling layer, which is
to further pick up larger-scale detail than just edges and curvesby further distilling the features. The pooling function uses thetotal statistical characteristics of the adjacent output of a certain
locationoftheinputtedimagetoreplacetheoutputofthenetwork
at that location, and combines the output of one layer of neuron
clusterintoasingleneuroninthenextlayertoreducethesizeof
data. Max pooling uses the maximumvalue from each of a cluster
of neurons at the prior layer. In a CNNâ€™s pooling layers, feature
maps are divided into rectangular sub-regions, and the features in
eachrectangleareindependentlydown-sampledtoasinglevalue,
commonly by taking their average or maximum value. In addition
toreducingthesizesoffeaturemaps,thepoolingoperationgrantsadegreeoftranslationalinvariancetothefeaturescontainedtherein.
The last several layers are fully connected neural networks (FC)
which compile the data extracted by previous layers to form the
finaloutput.Allinputsfromoneoftheselayersareconnectedto
everyactivationunitofthenextlayer.Themultiplefullyconnectedrelationshipsincreasethepossibilityoflearningacomplexfunction.
The fully connected layers further encode all features of the UI
screenshotintoa ğ¾-dimensionalvector.Finally,thedetectionresults
are obtained through softmax [7].
ğ‘ƒ(ğ‘¦=ğ‘|ğ‘“)=ğ‘’ğ‘“ğ‘‡ğ‘¤ğ‘
/summationtext.1ğ¾
ğ‘˜=1ğ‘’ğ‘“ğ‘‡ğ‘¤ğ‘˜(2)
wherethe ğ¾-dimensionalvectorarenormalizedintoaprobability
distributionwith ğ¾probabilities,whichisproportionaltotheindex
oftheinputnumber.Theinput ğ‘“isthefeature,and ğ‘ƒ(ğ‘¦=ğ‘|ğ‘“)is
the predicted probability of ğ‘“belonging to category ğ‘(bug), which
is similar to the result of the previous layer.
3.2 Grad CAM-based UI Display Issues
Localization
Although our classification model can check if the given UI screen-
shot is of display issues, some UI display issues may still be too
small to spot in a large UI screenshot. Therefore, besides the classi-
fication model, we adopt the feature visualization method to locate
the detailed position of the issues for guiding developers to fix the
bug.Thiscanalsohelpusevaluatewhetherthefeatureextracted
by our CNN model is accurate or not. We apply Grad-CAM model
forthelocalizationofUIdisplayissues.GradientweightedClass
Activation Mapping (Grad-CAM) is a technique for visualizing the
401Owl Eyes: Spotting UI Display Issues via Visual Understanding ASE â€™20, September 21â€“25, 2020, Virtual Event, Australia
regionsofinputthatareâ€œimportantâ€forpredictionsonCNN-based
models [55] . The final convolutional layer of CNN model con-
tains the spatial and semantic information, and this technique uses
theclass-specificgradientinformationflowingintothefinalcon-
volutional layer to produce a localization map of the important
regions in the inputted image. The flow of Grad-CAM is shown
in Figure 5. First, a screenshot with UI display issue is input into
the trained CNN model, and the category supervisor to which the
image belongs is set to 1, while the rest is 0.
Figure 5: The architecture of Grad-CAM
Thentheinformationispropagatedbacktotheconvolutional
featuremapofinteresttoobtaintheGrad-CAMpositioning.Sup-
pose that the judgment category is ğ‘(Bug), the calculation method
of the score gradient of ğ‘isğœ•ğ‘œğ‘¢ğ‘¡ğ‘ğ‘¢ğ‘¡ğ‘
ğœ•ğ´ğ¾
ğ‘–ğ‘—, whereğ‘œğ‘¢ğ‘¡ğ‘ğ‘¢ğ‘¡ğ‘is the output
ofcategory ğ‘beforesoftmax.Throughthefeedbackofglobalav-
eragepoolingofthegradient,theweight ğ›¼ğ‘
ğ‘˜oftheimportanceof
neurons is obtained. This weight captures the importance of thefeature map
ğ¾of the target class ğ‘. By performing the weighted
combination of the forward activation graph, we can obtain the
class-discriminative localization map ğ¿ğ‘
ğºğ‘Ÿğ‘ğ‘‘âˆ’ğ¶ğ´ğ‘€.
ğ›¼ğ‘
ğ‘˜=1
ğ‘/summationdisplay.1
ğ‘–/summationdisplay.1
ğ‘—ğœ•ğ‘œğ‘¢ğ‘¡ğ‘ğ‘¢ğ‘¡ğ‘
ğœ•ğ´ğ¾
ğ‘–ğ‘—(3)
ğ¿ğ‘
ğºğ‘Ÿğ‘ğ‘‘âˆ’ğ¶ğ´ğ‘€=ğ‘…ğ‘’ğ¿ğ‘ˆ(/summationdisplay.1
ğ‘˜ğ›¼ğ‘
ğ‘˜ğ´ğ‘˜) (4)
Finally, the point multiplication with the back propagation can
obtain the Grad-CAM as the result of UI display issues localization.
3.3 Implementation
OurCNNmodeliscomposedof12Convolutionallayerswithbatch
normalization, 6 pooling layers and 4 full connection layers for
classifying UI screenshot with display issues. The size of convolu-
tional kernel in convolutional layer is 3 * 3. We set up the number
of convolutional kernels as 16 for convolutional layer 1-4, 32 for
convolutional layer 5-6, 64 for convolutional layer 7-8, and 128 for
convolutionallayer9-12.ThemomentuminBNlayerissetas0.1.
Forthepoolinglayers,weusethemostcommon-usedmax-poolingsetting[57],i.e.,poolingunitsofsize2 Ã—2appliedwithastride[ 58].
Wesetthenumberofneuronsineachofthefullyconnectedlay-
ers as 4096, 1024, 128 and 2 respectively. For data preprocessing,
werotatesomeUIofthehorizontalscreenstovertical,andresize
the screens to 768 * 448. We implement our model based on the
PyTorch6framework.
4 HEURISTIC-BASEDDATAAUGMENTATION
Training an effective CNN model for visual understanding requires
a large amount of input data. For example, RESNET [ 25] model
uses 128 million images from ImageNet as training dataset for
image classification task. Similarly, training our proposed CNN for
UIdisplayissuesdetectionrequiresabundantofscreenshotswith
UI display issues. However, there is so far no such type of open
dataset, and collecting the related buggy screenshots is quite time-
andeffort-consuming.Therefore,wedevelopaheuristic-baseddata
augmentationmethodforgeneratingUIscreenshotswithdisplay
issues from bug-free UI images.
Thedataaugmentationis basedontheRico[ 19]datasetwhich
contains more than 66K unique screenshots from 9.3K Android
applications,aswellastheiraccompaniedJSONfile(i.e.,detailed
run-timeviewhierarchyofthescreenshot).Accordingtoourob-
servationonSection2,mostUIscreenshotsinthisdatasetareof
no dispaly issues.
Figure 6: Examples of data augmentation
Algorithm 1 presents the heuristic-based data augmentation al-
gorithm. With the input screenshot and its associated JSON file,the algorithm first locates all the TextView and ImageView, thenrandomly chooses a TextView or ImageView depending on theaugmented category. Based on the coordinates and size of the
TextView/ImageView, the algorithm then makes its copy and ad-
justs its location or size following specific rules to generate the
screenshotwithcorrespondingUIdisplayissues.Figure6demon-
stratestheillustrativeexamplesoftheaugmentedscreenshotswith
UI display issues.
Note that, among the five categories of UI display issues, the
category of blurred screen is difficult to be generated following
the above idea. Besides, preliminary results reveal the proposedapproach can detect this category of issues with relatively high
accuracy.Hence,weleavethiscategoryforfuturework,andinthis
6https://pytorch.org
402ASE â€™20, September 21â€“25, 2020, Virtual Event, Australia Zhe Liu1,3,Chunyang Chen4, Junjie Wang1,2,3,âˆ—, Yuekai Huang1,3, Jun Hu1,3, Qing Wang1,2,3,âˆ—
studyweobtainthiscategoryofscreenshotsonlinebysearching
â€˜blurredscreen â€™.Wethenpresentthedetailedaugmentationrulesof
the four categories.
Algorithm 1: Heuristic-based data augmentation
Input:ğ‘ ğ‘ğ‘Ÿ: screenshot without bugs;
ğ‘—ğ‘ ğ‘œğ‘›: associated JSON file;
ğ‘ğ‘ğ‘¡ğ‘’ğ‘”ğ‘œğ‘Ÿğ‘¦: category of generated UI display issue;
ğ‘–ğ‘ğ‘œğ‘›: pre-prepared image icon;
Output: ğ‘ğ‘¢ğ‘”ğ‘ ğ‘ğ‘Ÿ: augmented screenshot with categorybug;
1Traverse ğ‘—ğ‘ ğ‘œğ‘›file to obtain all TextView and ImageView;
2ifğ‘ğ‘ğ‘¡ğ‘’ğ‘”ğ‘œğ‘Ÿğ‘¦ ==â€˜ğ‘šğ‘–ğ‘ ğ‘ ğ‘–ğ‘›ğ‘” ğ‘–ğ‘šğ‘ğ‘”ğ‘’ â€™then
3Randomly choose an ImageView;
4else
5Randomly choose a TextView;
6Obtain the coordinates of TextView/ImageView
(ğ‘¥1,ğ‘¦1),(ğ‘¥2,ğ‘¦2);//coordinate of upper left and lower right
7Calculate the width and height of TextView/ImageView ( ğ‘¤,
â„) based on the coordinates;
8Obtain the text content of TextView ( ğ‘¡ğ‘’ğ‘¥ğ‘¡);
9Obtain the background color of TextView/ImageView ( ğ‘ğ‘”);
10ifğ‘ğ‘ğ‘¡ğ‘’ğ‘”ğ‘œğ‘Ÿğ‘¦ ==â€˜ğ‘ğ‘œğ‘šğ‘ğ‘œğ‘›ğ‘’ğ‘›ğ‘¡ ğ‘œğ‘ğ‘ğ‘™ğ‘¢ğ‘ ğ‘–ğ‘œğ‘› â€™then
11ğ‘Ÿğ‘ğ‘›ğ‘‘â†random.uniform(-1,1);
12ğ‘–ğ‘šğ‘ğ‘”ğ‘’.new((ğ‘¤,â„Ã—|ğ‘Ÿğ‘ğ‘›ğ‘‘|)),ğ‘ğ‘”);
13ifğ‘Ÿğ‘ğ‘›ğ‘‘â‰¥0then
14 //Occlude the upper part of component
ğ‘ğ‘¢ğ‘”ğ‘ ğ‘ğ‘Ÿâ†ğ‘ ğ‘ğ‘Ÿ.paste(ğ‘–ğ‘šğ‘ğ‘”ğ‘’,(ğ‘¥1,ğ‘¦1));
15else
16 //Occlude the lower part of component
ğ‘ğ‘¢ğ‘”ğ‘ ğ‘ğ‘Ÿâ†ğ‘ ğ‘ğ‘Ÿ.paste(ğ‘–ğ‘šğ‘ğ‘”ğ‘’,(ğ‘¥1,ğ‘¦2+(â„Ã—ğ‘Ÿğ‘ğ‘›ğ‘‘)));
17ifğ‘ğ‘ğ‘¡ğ‘’ğ‘”ğ‘œğ‘Ÿğ‘¦ ==â€˜ğ‘¡ğ‘’ğ‘¥ğ‘¡ ğ‘œğ‘£ğ‘’ğ‘Ÿğ‘™ğ‘ğ‘ â€™then
18ğ‘¥ğ‘Ÿğ‘ğ‘›ğ‘‘â†random.uniform(âˆ’0 .5Ã—ğ‘¤,0.5Ã—ğ‘¤);
19ğ‘ğ‘¢ğ‘”ğ‘ ğ‘ğ‘Ÿâ†ğ‘ ğ‘ğ‘Ÿ.write([ğ‘¥2âˆ’ğ‘¥ğ‘Ÿğ‘ğ‘›ğ‘‘,ğ‘¦ 1],ğ‘¡ğ‘’ğ‘¥ğ‘¡);
20ifğ‘ğ‘ğ‘¡ğ‘’ğ‘”ğ‘œğ‘Ÿğ‘¦ ==â€˜ğ‘šğ‘–ğ‘ ğ‘ ğ‘–ğ‘›ğ‘” ğ‘–ğ‘šğ‘ğ‘”ğ‘’ â€™then
21ğ‘–ğ‘šğ‘ğ‘”ğ‘’.new((ğ‘¤,â„),ğ‘ğ‘”);
22ğ‘ ğ‘ğ‘Ÿ.paste(ğ‘–ğ‘šğ‘ğ‘”ğ‘’,(ğ‘¥1,ğ‘¦1));
23ğ‘ğ‘¢ğ‘”ğ‘ ğ‘ğ‘Ÿâ†ğ‘ ğ‘ğ‘Ÿ.paste(ğ‘–ğ‘ğ‘œğ‘›,(ğ‘¥1+0.5Ã—ğ‘¤,ğ‘¦1+0.5Ã—â„));
24ifğ‘ğ‘ğ‘¡ğ‘’ğ‘”ğ‘œğ‘Ÿğ‘¦ ==â€˜ğ‘›ğ‘¢ğ‘™ğ‘™ ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’ â€™then
25ğ‘–ğ‘šğ‘ğ‘”ğ‘’.new((ğ‘¤,â„),ğ‘ğ‘”);
26ğ‘ ğ‘ğ‘Ÿ.paste(ğ‘–ğ‘šğ‘ğ‘”ğ‘’,(ğ‘¥1,ğ‘¦1));
27ğ‘ğ‘¢ğ‘”ğ‘ ğ‘ğ‘Ÿâ†ğ‘ ğ‘ğ‘Ÿ.write([ğ‘¥1,ğ‘¦1],â€œnullâ€);
28returnğ‘ğ‘¢ğ‘”ğ‘ ğ‘ğ‘Ÿ;
Augmentation for Component Occlusion Bug : When this
category of bug occurs, the textual information or component is
occludedbyothercomponents.Therefore,wefirstgenerateacolor
block which shares the same background color as the original
TextView but with a smaller height, then put it to cover part of the
TextView randomly.
Augmentation for Text Overlap Bug : The textual contents
are overlapped with each other, when this category of bug occurs.
To augment this category of screenshots, we generate a piece oftextwiththesamecontentastheoriginalTextView,andoffsetit
slightly.
AugmentationforMissingImageBug :Wenoticethatwhen
this category of bug occurs, an image icon would show up to in-
dicate that the area supposes to be an image. To augment this
categoryofscreenshots,wefirstdownloadsomefrequently-used
imageicons online,then coverthe originalimagedisplaying area
withonerandom-chosenimageiconandsetitsbackgroundcolor
as the color of its original image.
Augmentation for NULL Value Bug :Whenthiscategoryof
bugoccurs, NULLisdisplayedintheareawheresupposestobea
piece of text. We generate this category of screenshots by covering
theoriginalTextViewusingacolorblockwhichsharesthesame
background color and with NULLon it.
Note that, both component occlusion andtext overlap involves
coveringaTextView,thedifferenceisthattheformeroneutilizesa
colorblocktocovertheTextViewsothatitlookslikeacomponent
blocks the text, while the latter one employs a piece of text to
covertheTextViewtomakeitlooklikethetwopiecesoftextare
overlapped with each other. Another note is that, based on our
observation on the screenshots with UI display issues in Section 2,
whenconductingtheaugmentation,theTextViewiscoveredinthe
vertical direction in component occlusion, while it is covered in the
horizontal direction in text overlap.
5 EXPERIMENT DESIGN
5.1 Research Questions
â€¢RQ1: (Issues Detection Performance) How effective of our
proposed OwlEyein detecting UI display issues?
For RQ1, we first present some general views of our proposed
approach for UI display issues detection and the comparison with
commonly-usedbaselineapproaches(detailsareinSection5.3).We
also present the performance comparison among the variations of
modelconfiguration(e.g.,thenumberofconvolutionallayers)to
further demonstrate its effectiveness. Besides, we also evaluate the
contribution of data argumentation by comparing the performance
with and without the argumented training data.
â€¢RQ2:(IssuesLocalizationPerformance) Howeffectiveofour
proposed OwlEyein localizing UI display issues?
Forevaluatingtheperformanceofissueslocalization,weconduct
a user study to check its accuracy.
â€¢RQ3:(UsefulnessEvaluation) Howdoesourproposed OwlEye
work in real-world situations?
ForRQ3,weintegrate OwlEyewithDroidBotasafullyautomatic
tooltocollectthescreenshotsanddetectUIdisplayissues,andthen
issue the detected bugs to the development team.
5.2 Experimental Setup
The experimental dataset comes from two sources. The first is the
screenshotsfromcrowdtesting,whichcontains4,470non-duplicate
screenshotswithUIdisplayissuesandequalnumberofbug-free
non-duplicate screenshots (see details in Section 2).
The second is the screenshots generated with the data augmen-
tationmethodinSection4.Indetail,werandomlydownloadone
screenshot from each of the random-chosen 10,000 applications in
403Owl Eyes: Spotting UI Display Issues via Visual Understanding ASE â€™20, September 21â€“25, 2020, Virtual Event, Australia
Ricodataset,andeachscreenshotwouldbeutilizedonceforthedata
augmentation.Inordertomakethetrainingdatabalancedacross
categories, we use 10% screenshots for augmenting the component
occlusion category,whileuse30%screenshotsfordataaugmentation
of each of the other three categories.
For the augmented 10,000 screenshots with UI display issues,
we first extract their features with ORB feature extraction algo-
rithm [54], rank them randomly, compute the cosine similarity
between a specific screenshot and each of its previous ones, and
removeitwhenasimilarityvalueabove0.8isobserved.Inthisway,
7,800screenshots withUIdisplay issuesare remainedandadded
into the experimental dataset. To make the data balanced, we then
randomlydownloadthescreenshotsfromRicoandremovethesim-
ilar ones, and a total of 7,800 bug-free screenshots are collected for
experiment.Forthecategory blurredscreen,werandomlydownload
20screenshotswiththisissueonline,andrandomlychooseequal
number of bug-free screenshots from Rico. Note that, the cosine
similaritybetweeneachpairof these40screenshotsisalsobelow
0.8.
Table 1: The number of 5 categories of buggy screenshots
CategoryTrainTest ValCrowd-data Aug-data
Component occlusion 1745 986 226 131
Text overlap 706 2300 145 87
Missing image 569 2326 326 231
NULL value 130 2188 73 49
Blurred screen 20 20 30 2
Overall 3170 7820 800 500
In orderto simulate the real-worldapplication of ourproposed
approach, we setup the experiment as follows.
For the 8,940 screenshots screenshots from 562 crowdtesting
apps,weutilizethe1,600screenshots(800withUIdisplayissues
and800without)from162appsastestingsettoevaluatetheper-
formanceof OwlEye,andemployanother1,000screenshots(halfof
themwithUIdisplayissues)fromanother50appsasvalidationset
to estimate how well the model has beentrained and further tune
theparameters.The6,340screenshotsfromtheremaining350apps
isutilizedastrainingset.Besides,allthe15,640screenshots(half
of them with UI display issues) generated with data augmentation
is added to the training set to boost the detection performance.
Table1presentsthedistributionofscreenshotsintermsofdifferent
categories. The model is trained in a NVIDIA GeForce RTX 2060
GPU (16G memory) with 100 epochs for about 8 hours.
5.3 Baselines
In order to further demonstrate the advantage of OwlEye, we com-
pare it with 13 baselines utilizing both machine learning and deep
learningtechniques.The12machinelearningapproachesfirstex-
tract visual features from the screenshots, and employ machine
learnerfortheclassification.Thedeeplearningapproachutilizes
artificialneuralnetworkdirectlyonthescreenshotsforclassifica-
tion. We first present thethree types offeature extraction method
used in machine learning approaches.
SIFT[37]: Scale invariant feature transform (SIFT) is a common
featureextractionmethodtodetectanddescribelocalfeaturesinan
image.Itcanextracttheinterestingpointsontheobjecttogeneratethe feature description of the object, which is invariant to uniform
scaling, orientation, and illumination changes.
SURF[4]: Speed up robot features (SURF) is an improvement
ofSIFT. SURF uses an integer approximation of the determinant
of Hessian blob detector, which can be computed with 3 integer
operations using a precomputed integral image.
ORB[54]: Oriented fast and rotated brief (ORB) is a fast feature
point extraction and description algorithm. Based on the rapid
binary descriptor ORB of brief,it has rotation invariance and anti
noise ability.
With these features, we apply four commonly-used machine
learning approaches, i.e., Support Vector Machine (SVM) [ 30], K-
NearestNeighbor(KNN)[ 6],NaiveBayes(NB)[ 30]andRandom
Forests (RF) [ 8], for classifying the screenshots with UI display
issues. The combination of three types of image features and four
learning algorithms generates a total of 12 baselines.
We also experimentwith MultilayerPerceptron (MLP) directly
on the screenshots to better demonstrate the superiority of our
proposedapproach.Indetail,MLPisafeedforwardartificialneural
network [ 23,33]. The network structure is divided into input layer,
hidden layer and output layer. Each node is a neuron that uses
anonlinearactivationfunction,e.g.,correctedlinearunit(ReLU).
It is trained by changing the connection weight according to the
outputerrorcomparedwiththegroundtruth.Weusedeightlayers
of neural network, and we set the number of neurons in each layer
to 190, 190, 128, 128, 64, 64, 32 and 2, respectively.
5.4 Evaluation Metrics
In order to evaluate the issues detection performance of our pro-
posedapproach,weemploythreeevaluationmetrics,i.e.,precision,
recall,F1-Score,whicharecommonly-usedinimageclassification
andpatternrecognition[ 38,41].Forallthemetrics,highervalue
leads to better performance.
Precision is the proportion of screenshots that are correctly pre-
dicted as having UI display issues among all screenshots predicted
as buggy:
precision =Screenshots correctly predicted as buggy
All screenshots predicted as buggy(5)
Recall is the proportion of screenshots that are correctly pre-
dictedasbuggyamongallscreenshotsthatreallyhaveUIdisplay
issues.
recall =Screenshots correctly predicted as buggy
All screenshots really buggy(6)
F1-score (F-measure or F1) is the harmonic mean of precision
and recall, which combines both of the two metrics above.
F1âˆ’score =2Ã—precisionÃ—recall
precision+recall(7)
InSection6.2,weemployKendallâ€™sW(Kendallâ€™scoefficientof
concordance) [ 56] to assess the agreement of the user evaluated
localization results among different practitioners. It is a commonly-
used measurement for the level of agreement between multiple
items of multiple raters. The closer the test outcome is to 1, the
higher agreement among the evaluation results of the raters.
404ASE â€™20, September 21â€“25, 2020, Virtual Event, Australia Zhe Liu1,3,Chunyang Chen4, Junjie Wang1,2,3,âˆ—, Yuekai Huang1,3, Jun Hu1,3, Qing Wang1,2,3,âˆ—
6 RESULTS AND ANALYSIS
6.1 Issues Detection Performance (RQ1)
We first present the issues detection performance of our proposed
OwlEye,aswellastheperformanceintermsoffivecategoriesofUI
displayissuesinTable2.With OwlEye,theprecisionis0.85,indicat-
ing 85% (679/798) of the screenshots which are predicted as having
UI display issues are truly buggy. The recall is 0.84, indicating 84%
(679/800) buggy screenshots can be found with OwlEye.
Table 2: Issues detection performance (RQ1)
Category Precision Recall F1-score
Overall 0.850 0.848 0.849
Component occlusion 0.859 0.814 0.836
Text overlap 0.818 0.806 0.812
Missing image 0.855 0.904 0.879
NULL value 0.855 0.808 0.830
Blurred screen 0.888 0.800 0.842
We then shift our focus to the bottom half of Table 2, i.e., the
performance in terms of each category of UI display issues. All the
fivecategoriesofUIdisplayissuescanbedetectedwitharelative
high precision and recall, i.e., mimimum precision and recall are
0.82and0.80respectively.Thecategory missingimage canbede-
tected with the highest F1-score, indicating both precision (0.86)
andrecall(0.90)achievearelativelyhighvalue.Thismightbecause
screenshotswith missingimage bugshaverelativelyfixedpattern
and the buggy area is relatively large, i.e., the whole image icon
asshowninSection2.Incomparison,thecategory textoverlap is
recognized with the lowest F1-score, e.g., 0.82 precision and 0.81
recall. This is due to the fact that the pattern of this category is
more diversified, and the buggy region is much smaller, i.e., the
overlappingarea between twopieces of textaccounts for amere of
10% of the text component.
Figure 7: Examples of bad case in issues detection (RQ1)
Wefurtheranalyzethescreenshotswhicharewronglypredicted
as bug-free, with examples in Figure 7. One common shared by
thesescreenshotsisthatthebuggyareaistootinytoberecognized
evenwithhumaneye.Futureworkwillfocusmoreonimproving
the detection performance for these screenshots with attention
mechanism and image magnification.
6.1.1 Performance Comparison with Baselines. Table3shows
theperformancecomparisonwiththebaselines.Wecanseethat
our proposed OwlEyeis much better than the baselines, i.e., 17%
higher in recall compared with the best baseline (MLP), and 50%
higherinprecisionwiththebestbaseline(ORB-NB).Thisfurtherindicatestheeffectivenessof OwlEye.Besides,italsoimpliesthat
OwlEyeis especially good at hunting for the buggy screenshots
from candidate ones, i.e., larger improvement in recall.
Table 3: Performance comparison with baselines (RQ1)
Method Precision Recall F1-score
SIFT-SVM 0.486 0.349 0.406
SIFT-KNN 0.510 0.492 0.501
SIFT-NB 0.584 0.411 0.482
SIFT-RF 0.458 0.458 0.432
SURF-SVM 0.561 0.512 0.535
SURF-KNN 0.522 0.526 0.524
SURF-NB 0.428 0.597 0.499
SURF-RF 0.513 0.524 0.519
ORB-SVM 0.551 0.514 0.532
ORB-KNN 0.525 0.522 0.523
ORB-NB 0.567 0.709 0.630
ORB-RF 0.520 0.528 0.524
MLP 0.537 0.727 0.618
OwlEye 0.850 0.848 0.849
MLP achieves the highest recall among the baselines, indicat-
ingthisdeeplearningapproachisbetteratidentifyingthebuggy
screenshots, yet with a lower precision. The machine learning ap-
proacheswithORBfeatureachievethehighestF1-score(i.e.,0.63
by ORB-NB), indicating this kind of feature is more suitable fordetecting UI display issues. This might because ORB algorithmis the state-of-the-art feature extraction algorithm, and has been
proven to be an efficient alternative to SIFT or SURF [54].
6.1.2 Performance Comparison among Model Configura-
tions.We also conduct experiments to compare the detection per-
formance with different configurations of CNN model. Table 4
showstheperformanceofUIdisplayissuesdetectionintermsof
different convolutional layers and with / without batch normaliza-
tion (BN).
We can see that both the convolutional layers and the batch
normalizationcaninfluencetheissuesdetectionperformance.Gen-
erally speaking, when deepening the neural network, i.e., moreconvolutional layers, both precision (P) and recall (R) would in-
crease.Forexample,thepr ecisionundergo18% improvementwhen
convolutionallayersincreasefrom4to12(withbatchnormaliza-
tion),andtheimprovementofrecallis58%withsameconfiguration
changes. Besides, the employment of batch normalization can also
improvethe performance.Therearerespectively18% and47%im-
provementin precisionandrecallwhen addingthebatchnormal-
ization(12Convolutionallayers).Thisindicatestheeffectiveness
of our applied configurations in OwlEye.
Table 4: Performance comparison among model configura-
tions (RQ1)
Layer Without BN With BN
number PRF 1 PRF 1
4 0.704 0.478 0.569 0.722 0.537 0.616
6 0.753 0.492 0.595 0.696 0.631 0.662
8 0.751 0.530 0.621 0.702 0.732 0.717
10 0.742 0.537 0.623 0.779 0.738 0.758
12 0.725 0.576 0.642 0.850 0.848 0.849
6.1.3 Contribution of Data Augmentation .Weinvestigatethe
contribution ofdata augmentationby comparingthe issuesdetec-
tionperformanceontheoveralltrainingdataandonthetraining
405Owl Eyes: Spotting UI Display Issues via Visual Understanding ASE â€™20, September 21â€“25, 2020, Virtual Event, Australia
data by removing the 15,640 screenshots generated with data aug-
mentation (details are in Section 5.2). From Table 5, we can see
that both precision (P) and recall (R) improve when the augmented
screenshotsareaddedtothe trainingdata,indicatingthevalueof
data augmentation for effective UI display issues detection. Specif-
ically, 13% and 35% improvement are observed respectively for
precision and recall. The larger improvement in recall indicates
that,withtheaugmenteddataset,morescreenshotswithUIdisplay
issuescanbefound.Thismightbecausethetrainingsetwiththe
argumented data is more diversified in the screenshots, thus has
greater issues detection capability.
Table 5: Contribution of data augmentation (RQ1)
CategoryWithout DataAug With DataAug
PRF 1 PRF 1
Overall 0.756 0.625 0.684 0.850 0.848 0.849
Component occlusion 0.786 0.699 0.740 0.859 0.814 0.836
Text overlap 0.706 0.531 0.606 0.818 0.806 0.812
Missing image 0.749 0.677 0.711 0.855 0.904 0.879
NULL value 0.742 0.356 0.481 0.855 0.808 0.830
Blurred screen 0.857 0.600 0.705 0.888 0.800 0.842
Wealsopresent theperformanceimprovementintermsoffive
categories of UI display issues in Table 5. Results show that, issues
detectionperformancein nullvalue categoryundergoesthelargest
improvement inF1-score. Thismight becausethe trainingdataset
beforeaugmentationhasfewscreenshots(130/3170=4%)withthisbug(detailsareinSection5.2),andthetinyregionfordecidingthis
category of bug makes it even difficult for the automatic detection.
After adding the augmented screenshots (2188/7820 = 28%), the
diversity of the training screenshots significantly improves the
performance.
6.2 Issues Localization Performance (RQ2)
Figure 8 presents the examples of our issues localization whichhighlights the buggy areas. We conduct a user study to evaluatethelocalizationperformance.Werecruitsixsoftwaredevelopers
online,allofwhommajorincomputerscienceandhavemorethan
two years of software development experience. Each of them is
presentedwith679correctlydetectedbuggyscreenshotsinRQ1,
and the accompanied localization results as shown in Figure 8.
They are required to independently evaluate the issues localiza-
tionresults,andtoanswerthequestionwhethertheyagreewith
each of the localization results using 5-Likert scale, i.e., strongly
agree, agree, neutral, disagree, strongly disagree [ 9,52]. The evalu-
ationresultsshouldbereturnedwithineighthourstoensurethe
credibility of this study.
Table 6: Results of issues localization (RQ2)
Participant S-Agree Agree Neutral Disagree S-Disagree
P1 76.1% 14.3% 6.2% 2.7% 0.7%
P2 76.6% 14.1% 6.0% 2.4% 0.9%P3 75.1% 14.9% 5.7% 3.0% 1.3%P4 74.3% 15.5% 5.9% 3.1% 1.2%P5 73.8% 15.2% 5.7% 3.2% 2.1%P6 74.4% 13.6% 7.2% 3.2% 1.6%
Average 75.0% 14.6% 6.1% 2.9% 1.3%
AsshownintheTable6,thepractitionersstronglyagreeoragree
with the UI display issues localization results in an average of 90%
Figure 8: Examples of issues localization (RQ2)
(i.e.,75%+15%)screenshots,andonlydisagree(orstronglydisagree)
in an average of 4% screenshots. This indicates the accuracy of
our issues localization in the screenshots. We further calculatethe Kendallâ€™s W [
56] (details are in Section 5.4) to judge to what
extent the evaluation results submitted by the six practitioners are
consistentwitheachother.TheresultofKendallâ€™sWis0.946,which
indicatesahighdegreeofinter-agreementontheperformanceof
our issues localization.
Figure 9: Examples of bad case in issues localization (RQ2)
We further analyze the bad case of issues localization as shown
inFigure9,andfindmostoftheminvolvethescreenshotswith text
overlapandcomponent occlusion issues. As mentioned in previous
subsection, this might because of the tiny region for localizing the
issues which can easily mislead the model.
6.3 Usefulness Evaluation (RQ3)
Tofurtherassesstheusefulnessofour OwlEye,werandomlysample
1,500 Android applications from F-droid7and 700 Android applica-
tions from Google Play8. Note that none of these apps appear in
our training dataset.
We use DroidBot, which is a commonly-used lightweight An-
droidtestinputgenerator[ 36],forexploringthemobileappsand
take the screenshot of each UI pages. Among the 2,200 collected
7http://f-droid.org/
8http://play.google.com/store/apps
406ASE â€™20, September 21â€“25, 2020, Virtual Event, Australia Zhe Liu1,3,Chunyang Chen4, Junjie Wang1,2,3,âˆ—, Yuekai Huang1,3, Jun Hu1,3, Qing Wang1,2,3,âˆ—
apps,40%(869/2200)appscanbesuccessfullyrunwithDroidbot,
add only 15% (329/2200) of the apps can be fetched with more than
one screenshot, asthey require register orauthenticate to explore
more screenshots which cannot be done by DroidBot. For the re-
maining 329 apps, an average of eight screenshots are obtained for
eachapp.Wethenfeedthosescreenshotsto OwlEyefordetectingif
there are any UI display issues. Once a display issue is spotted, we
create a bug report by describing the issue attached with buggy UI
screenshot.Finally,wereportthemtotheappdevelopmentteam
through issue reports or emails.
Table 7: Confirmed or fixed issues (RQ3)
APP Name Category Source Download IdStatus
Perfect Piano Music Google 50M+ email confirm
Music Player Music Google 50M+ email confirm
Nox security Tool Google 10M+ email fixed
DegooCloud Storage Tool Google 10M+ email fixed
Proxynel Tool Google 10M+ email confirm
Secure VPN Tool Google 10M+ email confirm
Thunder VPN Tool Google 10M+ email confirm
ApowerMirror Tool Google 5M+ email confirm
MediaFire Product Google 5M+ email confirm
Postegro Commun Google 500K+ email fixed
Deezer Music Player Music Google 500K+ email fixed
MTG Familiar Utilities F-droid 500K+ #512 fixed
Open Food Facts Health F-droid 500K+ #3051 confirm
Linphone Commun F-droid 500K+ #965confirm
Paytm Finance Google 100K+ email confirm
Transdroid Tool F-droid 100K+ #542confirm
Transistor Music F-droid 10K+ #254 fixed
Onkyo Music F-droid 10K+ #138 fixed
DemocracyDroid News F-droid 10K+ #51confirm
NewPipe Legacy Media F-droid 8K+ #24 fixed
LessPass Product F-droid 5K+ #519 fixed
CEToolbox Medical F-droid 500+ #4confirm
OpenTracks-OSM Health F-droid 10+ #26 fixed
Yucata Envoy Tool F-droid N/A #3confirm
ClassyShark3xodus Tool F-droid N/A #3confirm
VlcFreemote Media F-droid N/A #24confirm
Table7showsallbugsspottedbyour OwlEye,andmoredetailed
information of detected bugs can be seen in our website3. For F-
droid applications, 24 UI display issues are detected, among which
6 have been fixed and another 8 have been confirmed by the devel-
opers. For Google Play, 33 UI display issues are detected, among
which 4 have been fixed and another 8 have been confirmed by the
developers.Thesefixedorconfirmedbugreportsfurtherdemon-
stratetheeffectivenessandusefulnessofourproposedapproachin
detecting UI display issues.
7 DISCUSSION
Generality across platforms. Almost all the existing studies of
GUI bug detection[ 32,36,67] are designedfor a specific platform,
e.g.,Android,whichlimitsitsapplicabilityinreal-worldpractice.In
comparison, theprimary idea ofour proposed OwlEyeis todetect
UIdisplayissuesfromthescreenshotsgeneratedwhenrunningthe
applications with visual understanding. Since the screenshots from
differentplatforms(e.g.,Android,iOS)exertalmostnodifference,
our approach can be generalized for UI display issues detection in
otherplatforms.Wehaveconductedasmallscaleexperimentfor
another popular platform iOS, and experiment on seven screen-
shotswithUIdisplayissuescollectedinourdaily-usedapplications.
Resultsshow thatourproposed OwlEyecanaccurately detectfive(71%) of the buggy screenshots. This further demonstrates the gen-
erality of OwlEye, and we will conduct more thorough experiment
in future.
Generalityacrosslanguages. Anotheradvantageof OwlEyeis
that it can be applied for UI display issues detection in terms of dif-
ferent display languages of the application. The testing data of the
experiment for RQ1 contains the screenshots in Chinese, while the
experiment for RQ3 relates with the screenshots in English, which
demonstrates the generality of our approach across languages. We
also collect 22 screenshots with UI display issues in two other lan-
guages (i.e. German and Korean) from the applications in RQ3, and
runourapproachforbugdetection.Resultsshowthatourproposed
OwlEyecan accurately detect 16 (73%) of the buggy screenshots,
which further demonstrates the feasibility of OwlEye.
Potentialwithmoreeffectiveautomatictestingtool. Results
in RQ3 have demonstrated the usefulness of OwlEyein real-world
practicebeingintegratedwithautomatictestingtoolasDroidBot.
However, we have mentioned in Section 6.3 that some applications
can not be run with DroidBot, and some can only be fetched with
one screenshot due to the shortcoming of DroidBot, both of which
limit the full exploration of screenshots. If armed with a more
effectiveautomatictestingtool, OwlEyeshouldplayabiggerrole
in detecting UI display issues in real-world practice.
8 RELATED WORK
GUIprovidesavisualbridgebetweenapplicationsandusers.There-
fore,manyresearchersareworkingonassistingdevelopersorde-
signersintheGUIsearch[ 5,10,12,16,28,53,69]basedonimage
features,GUIcodegeneration[ 11,14,15,46,51]basedoncomputer
vision techniques. Moran et al. [ 47] check if the implemented GUI
violates theoriginal UI designby comparing theimages similarity
with computervision techniques.A follow-upwork bythem [ 50]
further detects and summarizes GUI changes in evolving mobile
applications.Differentfromtheseworks,ourworksarefocusing
on GUI testing.
ToensurethatGUIisworkingwell,therearemanystaticlinting
tools to flag programming errors, bugs, stylistic errors, and sus-
piciousconstructs[ 13,70].Forexample,AndroidLint[ 1]reports
over260differenttypesofAndroidbugs,includingcorrectness,per-
formance,security,usabilityandaccessibility.StyleLint[ 2]helps
developersavoiderrorsandenforceconventionsinstyles.Different
from static linting, automatic GUI testing [ 3,45,60] dynamically
exploresGUIsofanapp.Severalsurveys[ 32,67]comparedifferent
toolsforGUItestingforAndroidapps.Sometestingworksfocus
on more specific UI issues such as UI rendering delays [ 22] and im-
age loading [ 35]. Recently, deep learning based techniques [ 18,66]
havebeenproposedforautomaticGUItesting.Unliketraditional
GUItestingwhichexplorestheGUIsbydynamicprogramanalysis,thesetwotechniquesusecomputervisiontechniquestodetectGUI
componentsonthescreentodeterminenextactions.Inspiredby
their works, we also adopt the CNN in our study.
ButnotethattheseGUItestingtechniquesfocusonfunctional
testing. In contrast, our work is more about non-functional testing
i.e.,GUIvisualissueswhichwillnotcauseappcrash,butnegatively
influence the app usability. The UI display bugs detected by our
approacharemainly caused bytheappcompatibility[ 29,68]due
tothedifferentdevicesandAndroidversions.Itishighlyexpensive
407Owl Eyes: Spotting UI Display Issues via Visual Understanding ASE â€™20, September 21â€“25, 2020, Virtual Event, Australia
and extremely difficult for the developers covering all the popular
contexts when conducting testing. Besides, different from these
works based on static or dynamic code analysis, our work only
requires the screenshot as the input. Such characteristic enables
our light-weight computer vision based method, and also makes
our approach generalised to any platform including Android, IOS,
or IoT devices.
9 CONCLUSION
Improving the quality of mobile applications, especially in a proac-
tiveway,isofgreatvalueandalwaysencouraged.Thispaperfo-
cuses on automatic detecting the UI display issues from the screen-
shots generated during automatic testing. The proposed OwlEyeis
proventobeeffectiveinreal-worldpractice,i.e.,26confirmedor
fixedpreviously-undetectedUIdisplayissuesfrompopularAndroid
apps. OwlEyealso achieves more than 17% and 50% boostin recall
and precision compared with the best baseline. As the first work
of its kind, we also contribute to a systematical investigation of UI
display issues in real-world mobile apps, as well as a large-scale
dataset of app UIs with display issues for follow-up studies.
Inthefuture,wewillkeepimprovingourmodelforbetterperfor-
mance in the classification. Apart from the display issue detection,
wewillfurtherlocatetherootcauseoftheseissuesinourfuture
work.Thenwewilldevelopasetoftoolsforrecommendingpatches
to developers for fixing display bugs.
ACKNOWLEDGMENTS
This work is supported by the National Key Research and Develop-
ment Program of China under grant No.2018YFB1403400.
REFERENCES
[1] 2020. http://tools.android.com/tips/lint.
[2] 2020. https://github.com/stylelint/stylelint.[3]
Young-Min Baek and Doo-Hwan Bae. 2016. Automated model-based android
guitestingusingmulti-levelguicomparisoncriteria.In Proceedingsofthe31st
IEEE/ACM International Conference on Automated Software Engineering. ACM,
238â€“249.
[4]Herbert Bay, Tinne Tuytelaars, and Luc Van Gool. 2006. SURF: Speeded up
RobustFeatures.In Proceedingsofthe9thEuropeanConferenceonComputerVision
- Volume Part I (Graz, Austria) (ECCVâ€™06). Springer-Verlag, Berlin, Heidelberg,
404â€“417. https://doi.org/10.1007/11744023_32
[5]Farnaz Behrang, Steven P Reiss, and Alessandro Orso. 2018. GUIfetch: sup-
portingappdesignanddevelopmentthroughGUIsearch.In Proceedingsofthe
5thInternationalConferenceonMobileSoftwareEngineeringandSystems.ACM,
236â€“246.
[6]Alex Berson, Stephen Smith, and Kurt Thearling. 2004. An overview of data
mining techniques. Building Data Mining Application for CRM (2004).
[7]Christopher M Bishop. 2006. Pattern recognition and machine learning. springer.
[8] Leo Breiman. 2001. Random forests. Machine learning 45, 1 (2001), 5â€“32.
[9]JohnBrookeetal .1996. SUS-Aquickanddirtyusabilityscale. Usabilityevaluation
in industry 189, 194 (1996), 4â€“7.
[10]Chunyang Chen, Sidong Feng, Zhenchang Xing, Linda Liu, Shengdong Zhao,
andJinshuiWang.2019. GalleryDC:DesignSearchandKnowledgeDiscovery
through Auto-created GUI Component Gallery. Proceedings of the ACM on
Human-Computer Interaction 3, CSCW (2019), 1â€“22.
[11]Chunyang Chen, Ting Su, Guozhu Meng, Zhenchang Xing, and Yang Liu. 2018.
Fromuidesignimageto guiskeleton:aneuralmachinetranslatortobootstrap
mobileguiimplementation.In Proceedingsofthe40thInternationalConferenceon
Software Engineering. ACM, 665â€“676.
[12]Jieshan Chen, Chunyang Chen, Zhenchang Xing, Xin Xia, Liming Zhu, John
Grundy, and Jinshui Wang. 2020. Wireframe-based UI design search through
image autoencoder. ACM Transactions on Software Engineering and Methodology
(TOSEM) 29, 3 (2020), 1â€“31.
[13]JieshanChen,ChunyangChen,ZhenchangXing,XiweiXu,LimingZhu,Guo-
qiang Li, and Jinshui Wang. 2020. Unblind Your Apps: Predicting Natural-
Language Labels for Mobile GUI Components by Deep Learning. arXiv preprintarXiv:2003.00380 (2020).
[14]SenChen,LinglingFan,ChunyangChen,TingSu,WenheLi,YangLiu,andLihua
Xu. 2019. Storydroid: Automated generation of storyboard for Android apps.
In2019IEEE/ACM41stInternationalConferenceonSoftwareEngineering(ICSE).
IEEE, 596â€“607.
[15]Sen Chen,Lingling Fan,Chunyang Chen, MinhuiXue, Yang Liu,and Lihua Xu.
2019. GUI-Squatting Attack: Automated Generation of Android Phishing Apps.
IEEE Transactions on Dependable and Secure Computing (2019).
[16]ChenChunyang,FengSidong,LiuZhengyang,XingZhenchang,ZhaoSheng-
dong, and Liu Linda. 2020. From Lost to Found: Discover Missing UI Design
SemanticsthroughRecoveringMissingTags.In ProceedingsoftheACMonHuman-
Computer Interaction, Volume. 4, No. CSCW, November 2020.
[17]RiccardoCoppola,MaurizioMorisio,andMarcoTorchiano.2017. ScriptedGUI
Testing of Android Apps: A Study on Diffusion, Evolution and Fragility. In
Proceedings of the 13th International Conference on Predictive Models and Data
Analytics in Software Engineering (Toronto, Canada) (PROMISE). Association for
Computing Machinery, New York, NY, USA, 22â€“32. https://doi.org/10.1145/
3127005.3127008
[18]Christian Degott, Nataniel P Borges Jr, and Andreas Zeller. 2019. Learning
user interface element interactions. In Proceedings of the 28th ACM SIGSOFT
International Symposium on Software Testing and Analysis. ACM, 296â€“306.
[19]BiplabDeka,ZifengHuang,ChadFranzen,JoshuaHibschman,DanielAfergan,
Yang Li, Jeffrey Nichols, and Ranjitha Kumar. 2017. Rico: A Mobile App Dataset
for Building Data-Driven Design Applications. In Proceedings of the 30th Annual
Symposium on User Interface Software and Technology (UIST â€™17).
[20]Giovanni Denaro, Luca Guglielmo, Leonardo Mariani, and Oliviero Riganelli.
2019. GUITestinginProduction:ChallengesandOpportunities.In Proceedings
oftheConferenceCompanionofthe3rdInternationalConferenceonArt,Science,
and Engineering of Programming (Genova, Italy) (Programming â€™19). Association
for Computing Machinery, New York, NY, USA, Article 18, 3 pages. https:
//doi.org/10.1145/3328433.3328452
[21] Android Developers. 2012. Ui/application exerciser monkey.[22]
YiGao,YangLuo,DaqingChen,HaochengHuang,WeiDong,MingyuanXia,Xue
Liu, and Jiajun Bu. 2017. Every pixel counts: Fine-grained UI rendering analysis
for mobile applications. In IEEE INFOCOM 2017-IEEE Conference on Computer
Communications. IEEE, 1â€“9.
[23]Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. Deep learning.M I T
press.
[24]Tianxiao Gu, Chengnian Sun, Xiaoxing Ma, Chun Cao, Chang Xu, Yuan Yao,
Qirun Zhang, Jian Lu, and Zhendong Su. 2019. Practical GUI Testing of Android
ApplicationsviaModelAbstractionandRefinement.In Proceedingsofthe41st
International Conference on Software Engineering (Montreal, Quebec, Canada)
(ICSE â€™19). IEEE Press, 269â€“280. https://doi.org/10.1109/ICSE.2019.00042
[25]K. He, X. Zhang, S. Ren, and J. Sun. 2016. Deep Residual Learning for Image
Recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition
(CVPR).IEEEComputerSociety,LosAlamitos,CA,USA,770â€“778. https://doi.
org/10.1109/CVPR.2016.90
[26]Sergey Ioffe and Christian Szegedy. 2015. Batch normalization: Accelerating
deep network training by reducing internal covariate shift. arXiv preprint
arXiv:1502.03167 (2015).
[27]Bernard J. Jansen. 1998. The Graphical User Interface. SIGCHI Bull. 30, 2 (April
1998), 22â€“26. https://doi.org/10.1145/279044.279051
[28]Chen Jieshan, Xie Mulong, Xing Zhenchang, Chen Chunyang, Xu Xiwei, Zhu
Liming,andLiGuoqiang.2020. ObjectDetectionforGraphicalUserInterface:
Old Fashioned or Deep Learning or a Combination?. In ACM Joint European
SoftwareEngineeringConferenceandSymposiumontheFoundationsofSoftware
Engineering.
[29]Taeyeon Ki, Chang Min Park, Karthik Dantu, Steven Y Ko, and Lukasz Ziarek.
2019. Mimic:UIcompatibilitytestingsystemforAndroidapps.In 2019IEEE/ACM
41st International Conference on Software Engineering (ICSE). IEEE, 246â€“256.
[30]Sotiris B Kotsiantis, I Zaharakis, and P Pintelas. 2007. Supervised machine
learning:Areviewofclassificationtechniques. Emergingartificialintelligence
applications in computer engineering 160 (2007), 3â€“24.
[31]AlexKrizhevsky,IlyaSutskever,andGeoffreyEHinton.2012. Imagenetclassifica-tion with deep convolutional neural networks. In Advances in neural information
processing systems. 1097â€“1105.
[32]Tomi LÃ¤msÃ¤. 2017. Comparison of GUI testing tools for Android applications.
(2017).
[33] YannLeCun,YoshuaBengio,andGeoffreyHinton.2015. Deeplearning. nature
521, 7553 (2015), 436â€“444.
[34]Yann LeCun, LÃ©on Bottou, Yoshua Bengio, and Patrick Haffner. 1998. Gradient-
based learning applied to document recognition. Proc. IEEE 86, 11 (1998), 2278â€“
2324.
[35]Wenjie Li, Yanyan Jiang, Chang Xu, Yepang Liu, Xiaoxing Ma, and Jian Lu. 2019.
Characterizing and Detecting Inefficient Image Displaying Issues in Android
Apps.In26thIEEEInternationalConferenceonSoftwareAnalysis,Evolutionand
Reengineering, SANER 2019, Hangzhou, China, February 24-27, 2019, Xinyu Wang,
DavidLo,andEmadShihab(Eds.).IEEE,355â€“365. https://doi.org/10.1109/SANER.
408ASE â€™20, September 21â€“25, 2020, Virtual Event, Australia Zhe Liu1,3,Chunyang Chen4, Junjie Wang1,2,3,âˆ—, Yuekai Huang1,3, Jun Hu1,3, Qing Wang1,2,3,âˆ—
2019.8668030
[36]Yuanchun Li, Ziyue Yang, Yao Guo, and Xiangqun Chen. 2017. DroidBot: A
LightweightUI-GuidedTestInputGenerator forAndroid.In Proceedingsofthe
39th International Conference on Software Engineering Companion (Buenos Aires,
Argentina) (ICSE-Câ€™17).IEEEPress,23â€“26. https://doi.org/10.1109/ICSE-C.2017.8
[37]David G. Lowe. 2004. Distinctive Image Features from Scale-Invariant Keypoints.
Int.J.Comput.Vision 60,2(Nov.2004),91â€“110. https://doi.org/10.1023/B:VISI.
0000029664.99615.94
[38]S. Ma, Z. Xing, C. Chen, C. Chen, L. Qu, and G. Li. 2019. Easy-to-Deploy API
Extraction by Multi-Level Feature Embedding and Transfer Learning. IEEE
Transactions on Software Engineering (2019), 1â€“1.
[39]Yun Ma, Yangyang Huang, Ziniu Hu, Xusheng Xiao, and Xuanzhe Liu. 2019.
Paladin: Automated Generation of Reproducible Test Cases for Android Apps. In
Proceedings of the 20th International Workshop on Mobile Computing Systems and
Applications (Santa Cruz, CA, USA) (HotMobile â€™19). Association for Computing
Machinery,NewYork,NY,USA,99â€“104. https://doi.org/10.1145/3301293.3302363
[40]Aravind Machiry, Rohan Tahiliani, and Mayur Naik. 2013. Dynodroid: An Input
GenerationSystemforAndroidApps.In Proceedingsofthe20139thJointMeeting
on Foundations of Software Engineering (Saint Petersburg, Russia) (ESEC/FSE
2013). Association for Computing Machinery, New York, NY, USA, 224â€“234.
https://doi.org/10.1145/2491411.2491450
[41]Christopher D Manning, Prabhakar Raghavan, and Hinrich SchÃ¼tze. 2008. Intro-
duction to information retrieval. Cambridge university press.
[42]Leonardo Mariani, Mauro PezzÃ¨, and Daniele Zuddas. 2018. Augusto: Exploiting
PopularFunctionalitiesfortheGenerationofSemanticGUITestswithOracles.In
Proceedings of the 40th International Conference on Software Engineering (Gothen-
burg, Sweden) (ICSE â€™18). Association for Computing Machinery, New York, NY,
USA, 280â€“290. https://doi.org/10.1145/3180155.3180162
[43]JamesMartin.1991. RapidApplicationDevelopment. MacmillanPublishingCo.,
Inc., USA.
[44]AtifM.MemonandMyraB.Cohen.2013. AutomatedTestingofGUIApplications:
Models, Tools, and Controlling Flakiness. In Proceedings of the 2013 International
Conference on Software Engineering (San Francisco, CA, USA) (ICSE â€™13). IEEE
Press, 1479â€“1480.
[45]NarimanMirzaei,JoshuaGarcia,HamidBagheri,AlirezaSadeghi,andSamMalek.2016. ReducingcombinatoricsinGUItestingofandroidapplications.In Software
Engineering (ICSE), 2016 IEEE/ACM 38th International Conference on. IEEE, 559â€“
570.
[46]Kevin Moran, Carlos Bernal-CÃ¡rdenas, Michael Curcio, Richard Bonett, and
Denys Poshyvanyk. 2018. Machine Learning-Based Prototyping of Graphical
User Interfaces for Mobile Apps. arXiv preprint arXiv:1802.02312 (2018).
[47]KevinMoran,BoyangLi,CarlosBernal-CÃ¡rdenas, DanJelf,andDenysPoshy-
vanyk.2018. AutomatedreportingofGUIdesignviolationsformobileapps.In
Proceedingsofthe40thInternationalConferenceonSoftwareEngineering.ACM,
165â€“175.
[48]Kevin Moran, Mario Linares-VÃ¡squez, Carlos Bernal-CÃ¡rdenas, Christopher Ven-
dome,andDenysPoshyvanyk.2017. CrashScope:APracticalToolforAutomated
Testing of Android Applications. In Proceedings of the 39th International Confer-
ence on Software Engineering Companion (Buenos Aires, Argentina) (ICSE-C â€™17).
IEEE Press, 15â€“18. https://doi.org/10.1109/ICSE-C.2017.16
[49]Kevin Moran, Mario Linares-VÃ¡squez, and Denys Poshyvanyk. 2017. Automated
GUITestingofAndroidApps:FromResearchtoPractice.In Proceedingsofthe
39th International Conference on Software Engineering Companion (Buenos Aires,
Argentina) (ICSE-C â€™17). IEEE Press, 505â€“506. https://doi.org/10.1109/ICSE-
C.2017.166
[50]KevinMoran,CodyWatson,JohnHoskins,GeorgePurnell,andDenysPoshy-
vanyk.2018. DetectingandSummarizingGUIChangesinEvolvingMobileApps.
arXiv preprint arXiv:1807.09440 (2018).
[51]Tuan Anh Nguyen and Christoph Csallner. 2015. Reverse engineering mobile
applicationuserinterfaceswithremaui(t).In AutomatedSoftwareEngineering
(ASE), 2015 30th IEEE/ACM International Conference on. IEEE, 248â€“259.
[52]Yusuke Oda, Hiroyuki Fudaba, Graham Neubig, Hideaki Hata, Sakriani Sakti,
TomokiToda,andSatoshiNakamura.2015. Learningtogeneratepseudo-code
fromsourcecodeusingstatisticalmachinetranslation(t).In 201530thIEEE/ACM
InternationalConferenceonAutomatedSoftwareEngineering(ASE).IEEE,574â€“584.
[53]StevenPReiss,YunMiao,andQiXin.2018. Seekingtheuserinterface. Automated
Software Engineering 25, 1 (2018), 157â€“193.
[54]Ethan Rublee, Vincent Rabaud, Kurt Konolige, and Gary Bradski. 2011. ORB: An
EfficientAlternativetoSIFTorSURF.In Proceedingsofthe2011InternationalCon-
ference on Computer Vision (ICCV â€™11). IEEE Computer Society, USA, 2564â€“2571.
https://doi.org/10.1109/ICCV.2011.6126544
[55]RamprasaathR.Selvaraju,MichaelCogswell,AbhishekDas,RamakrishnaVedan-
tam, Devi Parikh, and Dhruv Batra. 2017. Grad-CAM: Visual Explanations From
Deep Networksvia Gradient-Based Localization. In The IEEEInternational Con-
ference on Computer Vision (ICCV).
[56]Sidney Siegel and N John Castellan. 1988. Nonparametric statistics for the
behavioral sciences (2nd ed.). 33, 1 (1988), 99â€“100.[57]Patrice Y. Simard, Dave Steinkraus, and John C. Platt. 2003. Best Practicesfor Convolutional Neural Networks Applied to Visual Document Analysis. InProceedings of the Seventh International Conference on Document Analysis and
Recognition - Volume 2 (ICDAR â€™03). IEEE Computer Society, USA, 958.
[58]KarenSimonyanandAndrewZisserman.2014.Verydeepconvolutionalnetworks
for large-scale image recognition. arXiv preprint arXiv:1409.1556 (2014).
[59]DonnaSpencer.2009. Cardsorting:Designingusablecategories. RosenfeldMedia.
[60]Ting Su, Guozhu Meng, Yuting Chen, Ke Wu, Weiming Yang, Yao Yao, Geguang
Pu,YangLiu,andZhendongSu.2017. Guided,stochasticmodel-basedGUItesting
ofAndroidapps.In Proceedingsofthe201711thJointMeetingonFoundationsof
Software Engineering. ACM, 245â€“256.
[61]ChristianSzegedy,VincentVanhoucke,SergeyIoffe,JonShlens,andZbigniew
Wojna. 2016. Rethinking the inception architecture for computer vision. In
Proceedings of the IEEE conference on computer vision and pattern recognition.
2818â€“2826.
[62]JunjieWang,YeYang,RahulKrishna,TimMenzies,andQingWang.2019.iSENSE:
Completion-Aware Crowdtesting Management. In ICSEâ€™2019. 932â€“943.
[63]Junjie Wang, Ye Yang, Song Wang, Yuanzhe Hu, Dandan Wang, and Qing Wang.
2020. Context-aware In-process Crowdworker Recommendation (ICSE 2020).
[64]Lili Wei, Yepang Liu, and Shing-Chi Cheung. 2016. Taming Android fragmen-
tation: Characterizing and detecting compatibility issues for Android apps. In
Proceedingsofthe31stIEEE/ACMInternationalConferenceonAutomatedSoftware
Engineering. 226â€“237.
[65]ThomasWetzlmaierandRudolfRamler.2017.HybridMonkeyTesting:Enhancing
Automated GUI Tests with Random Test Generation. In Proceedings of the 8th
ACMSIGSOFTInternationalWorkshoponAutomatedSoftwareTesting (Paderborn,
Germany) (A-TEST 2017). Association for Computing Machinery, New York, NY,
USA, 5â€“10. https://doi.org/10.1145/3121245.3121247
[66]Thomas D White, Gordon Fraser, and Guy J Brown. 2019. Improving random
GUItestingwithimage-basedwidgetdetection.In Proceedingsofthe28thACM
SIGSOFTInternationalSymposiumonSoftwareTestingandAnalysis.ACM,307â€“
317.
[67]Samer Zein, Norsaremah Salleh, and John Grundy. 2016. A systematic mapping
study of mobile application testing techniques. Journal of Systems and Software
117 (2016), 334â€“356.
[68]TaoZhang,JerryGao,JingCheng,andTadahiroUehara.2015. Compatibilitytest-
ing servicefor mobile applications.In 2015 IEEESymposium on Service-Oriented
System Engineering. IEEE, 179â€“186.
[69]Dehai Zhao, Zhenchang Xing, Chunyang Chen, Xin Xia, and Guoqiang Li. 2019.
ActionNet: vision-based workflow action recognition from programming screen-
casts. In2019 IEEE/ACM 41st International Conference on Software Engineering
(ICSE). IEEE, 350â€“361.
[70]DehaiZhao,ZhenchangXing,ChunyangChen,XiweiXu,LimingZhu,Guoqiang
Li, and Jinshui Wang. 2020. Seenomaly: Vision-Based Linting of GUI Animation
Effects Against Design-Donâ€™t Guidelines. In 42nd International Conference on
Software Engineering (ICSEâ€™20). ACM, New York, NY.
409