Zurich Open Repository and
Archive
University of Zurich
University Library
Strickhofstrasse 39
CH-8057 Zurich
www.zora.uzh.ch
Year: 2020
Configuration Smells in Continuous Delivery Pipelines: A Linter and a Six-Month
Study on GitLab
Vassallo, Carmine ; Proksch, Sebastian ; Jancso, Anna ; Gall, Harald C ; Di Penta, Massimiliano
DOI: https://doi.org/10.1145/3368089.3409709
Posted at the Zurich Open Repository and Archive, University of Zurich
ZORA URL: https://doi.org/10.5167/uzh-198329
Conference or Workshop Item
Published Version
Originally published at:
Vassallo, Carmine; Proksch, Sebastian; Jancso, Anna; Gall, Harald C; Di Penta, Massimiliano (2020). Configu-
ration Smells in Continuous Delivery Pipelines: A Linter and a Six-Month Study on GitLab. In: ESEC/FSE ’20:
28th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software
Engineering, Virtual Event, USA, 8 November 2020 - 13 November 2020. ACM, 327-337.
DOI: https://doi.org/10.1145/3368089.3409709Configuration Smells in Continuous DeliveryPipelines:
A Linter and a Six-MonthStudy onGitLab
CarmineVassallo
Universityof Zurich
Zurich, Switzerland
vassallo@ifi.uzh.chSebastian Proksch
Delft Universityof Technology
Delft, The Netherlands
s.proksch@tudelft.nlAnna Jancso
Universityof Zurich
Zurich, Switzerland
anna.jancso@uzh.ch
Harald C. Gall
Universityof Zurich
Zurich, Switzerland
gall@ifi.uzh.chMassimiliano Di Penta
Universityof Sannio
Benevento,Italy
dipenta@unisannio.it
ABSTRACT
An effective and efficient application of Continuous Integration
(CI) and Delivery (CD) requires software projects to follow certain
principlesandgoodpractices.ConfiguringsuchaCI/CDpipeline
ischallenginganderror-prone.Therefore,automated lintershave
beenproposedtodetecterrorsinthepipeline.Whileexistinglinters
identifysyntacticerrors,detectsecurityvulnerabilitiesormisuse
ofthefeaturesprovidedbybuildservers,theydonotsupportde-
velopers that want to prevent common misconfigurations of a CD
pipeline that potentially violate CD principles (łCD smellsž). To
this end, we propose CD-Linter , a semantic linter that can auto-
matically identify four different smells in pipeline configuration
files. We have evaluated our approach through a large-scale and
long-termstudythatconsistsof(i)monitoring145issues(opened
in as many open-source projects) over a period of 6 months, (ii)
manually validating the detection precision and recall on a rep-
resentative sample of issues, and (iii) assessing the magnitude of
the observed smells on 5,312 open-source projects on GitLab. Our
results show that CD smells are accepted and fixed by most of the
developersandourlinterachievesaprecisionof87%andarecallof
94%.Thosesmellscanbefrequentlyobservedinthewild,as31%of
projectswithlongconfigurationsareaffectedbyatleastonesmell.
CCS CONCEPTS
·Software and its engineering →Agile software develop-
ment;Automatedstaticanalysis ;Softwarelibrariesandrepositories;
Software testinganddebugging.
KEYWORDS
Continuous Delivery, Continuous Integration, DevOps, Anti-
pattern, Configuration,Linter.
ESEC/FSE ’20, November 8ś13, 2020, Virtual Event, USA
©2020 Associationfor Computing Machinery.
Thisistheauthor’sversionofthework.Itispostedhereforyourpersonaluse.Not
for redistribution. The definitive Version of Record was published in Proceedings of
the28thACMJointEuropeanSoftwareEngineeringConferenceandSymposiumonthe
FoundationsofSoftwareEngineering(ESEC/FSE’20),November8ś13,2020,VirtualEvent,
USA,https://doi.org/10.1145/3368089.3409709 .1 INTRODUCTION
Continuous Integration (CI) and Delivery (CD) are widely adopted
practices in software development. A CI/CD pipeline automates
theprocessofbuilding,testing,anddeployingnewsoftwarever-
sions. There is plenty of empirical evidence for the positive effects
of using CI/CD, including early defect discovery [ 19], increased
developer productivity [ 43], and fast release cycles [ 5]. To achieve
thesebenefits,itisrecommendedtofollowvariousprinciplesand
best practices. For example, developers should build and test the
software on every change that is committed to a project’s version
control system [ 29]. Several catalogs of CI/CD best practices ex-
ist[5,20,28,37].However,whiletheiradoptionhasbeenadvocated
inresearchpapers,whitepapers,andbooks,developershavedif-
ficultiestoapplytheminpractice[ 18],deviatingfromprinciples
andgeneratinganti-patterns[ 44].
Some of these anti-patterns are related to the way developers
use a CI/CD pipeline. For example, developers do not integrate
their changes frequently or they remove failed tests to repair a
build failure, and previous researchers [ 44] implemented tools
that help developers avoid those bad practices by analyzing logs
and past changes. Other anti-patterns emerge when the CI/CD
pipeline is configured. To support developers when configuring
CI/CD pipelines, DevOps build servers such as GitLab[11] can
validate their configuration files using online linters [ 12]. How-
ever, those tools only spot basic syntactic errors such as the use of
reservedkeywordswhen naming buildsteps.
Previous works have proposed approaches for detecting misuses
of specific configuration options. Gallaba et al., [ 9] achieved a high
user acceptance when pointing out the misuse of four different
configurationoptions,likeexecutingcommandsinthewrongbuild
step. Rahman et al. [ 36] focused on security-related issues and
Sharma et al. [ 38] on Infrastructure-as-Code (IaC) smells. While
theseworksshowthatsemanticlintingofCDpipelinesisuseful,
they do not solve the problem of avoiding CD anti-patterns in
configurationsfiles.Forexample,asystematicmanualjobexecution
isnot amisuse,but itviolates aCD principle.
In this work, we want to help developers avoid violations of
acceptedCDprincipleswhenconfiguringCDpipelines.Wepropose
a novel semantic linter named CD-Linter to detect process-related
violations of CD principles, in the following referred to as łCD
smellsž.CD-Linter is currently capable of detecting four typesESEC/FSE ’20, Nov ember8ś13,2020,VirtualEvent, USA Carmine Vassallo,Sebastian Proksch,AnnaJancso, Harald C.Gall, andMassimiliano Di Penta
of CD smells that are related to violations of principles and best
practices described in the literature [ 20,28]. We evaluated CD-
Linterthrough a large-scale and long-term study consisting of 145
issues opened in as many projects. We monitored the reactions
to those issues over a period of 6 months and found that 53% of
the projectmaintainers agreedwith the reported CDsmells either
accepting the issues (9%) or directly fixing them (44%). We also
analyzedthereasonsforrejectingissuesandusethemtofurther
improveCD-Linter .Finally,wemeasuredtheaccuracyofthelatest
versionof CD-Linter andinvestigatedtheoccurrenceofthefour
CD smellsinthe wild.
The contributionsofthis paper can be summarizedas follows:
(1)The operationalization of four violations of CD principles
in pipeline configurations (CD smells), and the empirical
validation oftheirrelevance.
(2)CD-Linter , an open-source semantic linter that can detect
these CD smells in configuration files of GitLabpipelines.
We show that, overall, CD-Linter has a precision of 87% and
arecallof94%.
(3)Alarge-scaleempiricalinvestigationoftheextenttowhich
theconsideredCDsmellsoccurinalargesetof5,312open-
sourceprojects.
All datasets and scripts used in our studies (together with CD-
Linterimplementation) are available in a replication package [ 45].
2 METHODOLOGYOVERVIEW
This paper investigates the problem of violating CD principles (i.e.,
CD smells) in configurationfilesof CD pipelines. We propose CD-
Linter, a semantic linter that detects CD smells and evaluate its
usefulnessbyansweringthe following researchquestions:
RQ1Are the CD Smells Detected by CD-Linter Relevant to Devel-
opers?
RQ2HowAccurateIs CD-Linter ?
RQ3HowFrequent Are the InvestigatedCD Smells inPractice?
Figure1providesahigh-leveloverviewofthedifferentpartsofthis
paper,thedetailsoftheempiricalstudydesignwillbecoveredin
Section4. Inspired by existing literature in this area, we started by
selectingfourCDsmellsthataffectthedefinitionofCDpipelines
(1) (Section 3.2provides more details about the selection). To study
theseCDsmells,weselectedadatasetof5,312open-sourceprojects
that are publicly available on the GitLabplatform (2). We built
detectors,ranthemagainstthedataset,andincrementallyimproved
thecorrespondingdetectionstrategies(3).ThefourCDsmellstypes
andtheirdetection strategies willbe introducedinSection 3.
Initially,wedetected5,237smellsinourdataset(4).Tovalidate
the relevance of the selected CD smells and the correctness of
our detectors, we started opening issues in the issue trackers of
the affectedopen-source projects.Weusedfeedback fromthe early
iterationstofurtherimprovethedetectionstrategies.Oncewewere
confident thatthedetectorsworkproperly,wecreated abalanced
sample of 168 issues (5). After validating the reports manually,
we rejected 23 issues and posted the approved 145 issues to the
issue trackers of the corresponding open-source projects. We then
monitoredhowprofessionaldevelopersreactedtotheopenedissues
for6months(6).InSection 5.1,wewillanswerRQ 1byanalyzingtheinternal rating ofthe authors and thereactions oftheoriginal
developers to our reports (7).
The feedbackthatwe receivedthrough rejectedissuesalsoen-
abledustofurtherimproveourdetectionstrategies,whichreduced
thetotalnumberofidentifiedissuesto5,011(4).Wecreatedastrat-
ified sample of 868 issues to validate the precision of our detectors
onalargescale,whilewevalidatetherecallbymanuallyinspecting
100 projects (8). The sample size made it infeasible to open further
issues on GitLab, because we could not have followed-up on all of
them,soweonlyratedthevalidityoftheseissuesinternally.Our
rating provided the required data to answer RQ 2(9), which will be
discussedinSection 5.2.
Finally, we analyzed the results for the complete dataset of 5,312
projects toinvestigate how frequently CD smells occur in practice
(10).Theseresults willbe discussedinSection 5.3.
3 CDLINTERDESCRIPTION
Organizations implement CD pipelines using piecesof technology
suchasJenkins,TravisCI,orGitLab.Whilethispapercopeswith
build-server agnostic smells, we implement CD-Linter onGitLab.
GitLabisanintegratedplatformthathostsboththerepositoryand
theissuetracker,whichisparticularlyinterestingforourevaluation.
In February 2020, a search via the GitLabAPI revealed that the
sitehostedmorethan1.57Mprojects.As GitLabcanalsobeused
in private installations, this makes it a very popular solution for
enterprises[ 7].Bysupporting GitLab,CD-Linter targetsindustrial
andopen-sourceprojectsalike.
3.1 Background
Inthefollowing,weprovidesomebackgroundinformationabout
the relevantconfigurationparts for this work.
Build Server. A build server is a reusable infrastructure, which
enables developers to define custom CD pipelines and is config-
ured through configuration files. In GitLabthe configuration file is
.gitlab-ci.yml ; otherbuildservers have similar configurationfiles.
An exampleofsuchaconfigurationfileisshowninFigure 2.In
the top part of the file, the stages of the build, that every change
committedtoaversioncontrolsystemas Githastopassduringthe
build,aredefined.Ifnostagesaredefined,thedefaultstagesin Git-
Labarebuild,test,anddeploy.Theautomationtasksaredefined
asjobs, the basic unit of the CD pipeline. The example defines two
jobs,code_quality andunit_test , which invoke specific scripts
that are defined in the scriptline. For example, a Javaproject
could include the script line script: mvn test to start all unit
tests through the Mavenbuild tool. Developers can also configure
when to run a job (e.g., when: manual ), how many times a job can
beauto-retriedincaseoffailures(e.g., retry: 3 ),andwhethera
jobisallowedto fail (i.e., allow_failure: true ).
Specialized Build Tools. In addition to the configuration of the
high-level orchestration of the build pipeline, most CD pipelines
usespecializedbuildtools toperform the actual automation tasks,
whichrequireseparateconfigurationparameters.Incontrasttothe
buildserver,thesebuildtoolsdependontheprogramminglanguage
that is used in the project. For this paper, we chose to support the
typical build tools of JavaandPythonto have two representativesConfigurationSmellsin Continuous Delivery Pipelines ESEC/FSE ’20, November8ś13,2020,VirtualEvent, USA
Existing
Literature• Fake Success
• Retry Failure 
• Manual Execution
• Fuzzy Version
Selected
Anti Patterns5,312 CI/CD 
projectsGitLab Projects
Pr (868 issues)
Rc (100 projects)Representative
SampleDetection
168 issuesValidation
Sample
Detector
Validation (RQ1)Linting
Accuracy (RQ2)5,237/5,011
issuesAll Detected Issues
Study of CD Smells
in Practice (RQ3)
Reﬁnement
145 issues
Authors
Developers2
34
5
68
7910
1
Figure 1:Methodologyoverview
stages:
- build
- test
...
code_quality :
stage: build
script:"mvn sonar:sonar"
when: manual # Manual Execution
unit_test :
stage: test
script:"mvn test"
retry: 3# Retry Failure
allow_failure : true# Fake Success
Figure 2:Example excerptof GitLabconfiguration
for strictly-typed and dynamically-typed languages. The typical
configurationdiffersbetweentheselanguagesandistoocomplex
to be covered here. We will introduce the relevant bits, once we
have describedthe CD smellsthat we are going to support.
3.2 SelectionofRelevant CDSmells
CD-Linter featurestheimplementationofaninitialsetofCDsmells
to be evaluated. Clearly, there may be many smells in CD pipelines
(e.g.,Duvall[ 28]defined50anti-patterns).Practicallyspeaking,a
CD-Linter can detect a limited subset of smells, and, being a linter,
only those that can be statically identified. Therefore, we aimed to
find a set of suitable CD smells, not all the most relevant ones. We
collectedallthegoodandbadpracticesthatareillustratedinthe
Foundations part of Humble and Farley [ 20], a well-known book
about CD practices. Some CD smells require historical information
for the detection (using artifacts like logs or repositories), which is
onlyavailableaftertheCDpipelineisbeingusedandnotwhenitis
configured[ 44].Thisisout-of-scopeforastaticlinter,sowejudged
the feasibility of detecting the anti-patterns from configurationfilesalone,withoutrelyingonother artifacts.Thecompletelistis
availableinourreplicationpackage[ 45]andweselectedfourCD
smells.
Fake Success. Each stage of the CD pipeline checks for several
categoriesofdefects.Forexample,jobsexecutedinthecodequality
stagecanrevealthepresenceofpoorly-writtencodesnippets,while
jobs in the test stage typically spot bugs at unit and integration
levels. Every executed job should be able to fail the build. If not,
developers can miss or ignore the underlying issue, which adds
technical debt and might result in problems later. A Fake Success
ariseswhenafailureinajobdoesnotaffecttheoverallbuildresult.
Retry Failure. The build process has to be deterministic. Flaky
behavior,e.g.,teststhatsometimesfail[ 21,26],shouldbeavoidedat
any cost, because they hinder development experience, slow down
progress,andhiderealbugs.Somepipelinesaddressthisissueby
rerunning ajobmultipletimesafter failures.However,thismight
not only hide an underlying problem but makes issues also harder
to debug when they only occur sometimes.
ManualExecution. CDmeanstokeepthecodebaseinadeploy-
able stage at any given time. Thus, a fully automated build process
up until the deploy stage is required. Manual jobs might introduce
errors and delay the delivery of code changes to the customers.
ThisCDsmelloccurswhenajob(thatisexecutedbeforethedeploy
stage) needsto be manually startedbyauser.
FuzzyVersion. Developersshouldalwaysspecifytheexactver-
sion of the external libraries that are used. If not, a build could not
be reproduced. Failing to be specific on versions also leads to an
occasional long debugging session tracking down errors due to the
use of different library versions. Using the terminology of semantic
versioning , we differentiate between the following sub-types of the
smell:(i) MissingVersion :Noversionnumberisdefined;(ii) Only
Major Version : Only a major release number is defined; (iii) Any
Minor Version : Any equal or higher minor release with the sameESEC/FSE ’20, November8ś13,2020,VirtualEvent, USA Carmine Vassallo,Sebastian Proksch,AnnaJancso, Harald C.Gall, andMassimiliano Di Penta
<project>
<modelVersion> 4.0.0</modelVersion>
<groupId> a</groupId>
<artifactId> b</artifactId>
<version> 0.0.1-SNAPSHOT </version>
<properties><x.version> 1.0.0</x.version></properties>
<dependencies>
<dependency> <!-- correct -->
<groupId> foo</groupId>
<artifactId> x</artifactId>
<version> ${x.version} </version>
</dependency>
<dependency> <!-- Missing version -->
<groupId> bla</groupId>
<artifactId> blubb</artifactId>
</dependency>
</dependencies>
<modules><module> module-example </module></modules>
</project>
Figure 3:Example ofa pom.xml
major version is allowed; or (iv) Any Upper Version : every equal or
higher versioncan be used.
3.3 ParsingCDConfigurationFiles
To detecttheCDsmells,we parse theconfiguration files andmap
their content onto meta-models that we have created for each type
ofconfigurationfiles CD-Linter supports.Thesemeta-modelscover
theparts of theconfiguration files that matter for the detection of
theCDsmells. CD-Linter considersthreetypesofconfiguration
files used for GitLab(.gitlab-ci.yml ),Maven(pom.xml), andpip
(requirements.txt ).Inthefollowing,wedescribehowweparsethese
configurationsfor our purposes.
GitLab Configuration. From the .gitlab-ci.yml file, we capture the
list of jobs, the stages, and the variables. For each job, we record
the name,the stage, and thescript lines ( script,before_script ,
after_script ) as well as the retry,allow_failure ,when, and
environment parameters. For the retryparameter, we keep track
of the maximum number of retries ( max) and for which kinds of
failures the job is allowed to be retried ( when). We filter out dot-
prefixedjobsas GitLabdoes not processthem.
MavenandpipConfigurations. TheMavenbuildtoolisverypop-
ular among Javaprojects.Mavencan automate various tasks, such
asthedependencyresolutionandtheautomateddownloadfrom
a centralized repository. Figure 3shows a configuration excerpt
("pom.xml"),inwhichtwodependencies, foo.xandbla.blubb are
being defined.
From the pom.xml, we capture the unique coordinates of the
artifact(artifactID,groupID,version),alldefinedproperties,and
thecoordinates ofalldependencies.Allpropertiesareautomatically
replaced with their actual value. We also include all referenced
modulesrecursivelyandlinkthemtogether.Valuessuchasversions
are then inheritedfrom ancestorPOMs where available.
As regards pip, the most used package manager for Pythoncode,
two things are relevant. First, the file requirements.txt is often used
to define all dependencies that are required in the Pythonenvi-
ronmenttorunaparticularpieceofsoftware.Theserequirementfiles can be hierarchical and include other requirement files that
are inherited. Second, the scriptline in the GitLabconfiguration
file often contains manual calls to pipto download external depen-
dencies. To find these, we search for the keyword pip install ,
stripother pipoptions,andremovequotesfromthearguments.It
isalsopossibletospecifydependenciesbypointingtofiles,folders,
andURLstoversioncontrolsystems.Weusesimpleheuristicsto
detectthesecasesandexclude themfrom the linting.
3.4 DetectionoftheCDSmells
Having access to the parsed information in the meta-models, we
canproceedtoimplementvariousstrategiesthatdetectinstances
ofthe fourCD smells.
FakeSuccess. Theallow_failure parametersetto trueallowsa
jobtofailwithoutimpactingtherestofthebuild.Figure 2showsan
exampleinwhichthebuildexecutioncansucceeddespitepotential
errorsinthe unit_test job.WedetectaFakeSuccesseverytime
a job’s definition contains allow_failure: true . Note that we
donotreportFakeSuccessforthestages sast(staticapplication
securitytesting)and dast(dynamicapplicationsecuritytesting).
GitLabdefines templates [ 14,16] that contain allow_failure set
totruefor the defaultjobusedinthesestages.
Retry Failure. Theretryparameter allows developers to con-
figure how many times a job is going to be retried in case of a
failure(seetheexampleinFigure 2).Wedetectallcasesinwhich
retryissettoapositivevalue.Theproposedsolutionforsucha
caseistocontrol retrybymatchingaspecificfailurecause(e.g.,
when:runner_system_failure ). We only found very few cases in
whichprojectsused when,sowedecidedtosimplifythedetectionin
CD-Linter and reportall such usagesof retryfor now. Handling
thesecasesproperly isasimplematterof implementation.
Manual Execution. Thewhenparameter can also be used to spec-
ifywhenajobshallbeexecuted.Todetectmanualtriggersofsteps,
weselectedalljobsthatcontain when:manual intheirdefinitions.
For example, job code_quality in stagebuild(Figure2) needs to
be manually startedbyauser.
Notallmanualtriggersare aproblemthough. CI/CDadvocates
the automated execution of all stages to ensure a releasable project
stateateverypointintime,however,itisacceptabletomanually
decide when this release should happen. Therefore, we do not
report cases in which the manual execution only affects deploy
stages. Apart from using the default deploystage,GitLabusers
canalsodefinecustomdeploystages[ 13].Tobuildacomprehensive
list of deploy stage names, we extracted the stage names from a
random project sample of our dataset (see Section 4). We identified
allkeywordsthathintatadeploystagesuchas‘deploy’,‘release’,or
‘publish’, and exclude jobs and stages that contain these keywords
intheirname.Also,wedidnotreportManualExecutionforjobs
in thetriageandreviewstages, because GitLabsuggests that
thesestages should bestartedmanually[ 15,17].Furthermore,we
excludedcaseswherethe actionparameterof environment isset
tostop,whichisamanualwaytoshutdownanenvironmentused
inthe build.ConfigurationSmellsin Continuous Delivery Pipelines ESEC/FSE ’20, November8ś13,2020,VirtualEvent, USA
Table 1:Fuzzy Versionsyntaxin PythonandMaven
Fuzzy Version Type Python Maven
Correct ==1.1.8 1.1.8
MissingVersion empty empty
OnlyMajor Version 1 1
Any Minor Version 1.* n/a
Any UpperVersion >=1.1.8 [1.1.8,)
Fuzzy Version. The way dependencies are declared is specific
to the programming language and the corresponding dependency
managementtool.Forwhatconcernsversioning, CD-Linter sup-
portsPythonandJavaprojects (the latter using Maven). Table1
showsacomparison ofthe versionsyntax.
Pythonprojects typically use pipto manage their dependencies
andourmeta-modelcontainsinformationaboutalldependencies
thatareeitherdefinedinthe requirements.txt fileorthroughdi-
rectinvocationsof pip install .CD-Linter distinguishesbetween
several Fuzzy Version subcategories. (i) If no version is defined, we
report aMissing Version , (ii) if a version specifier only consists of a
singlenumber,wereportan OnlyMajorVersion violation,(iii)an
AnyMinorVersion whentheminorreleasenumberisanasterisk,
and (iv)Any Upper Version if the version number only defines a
lower bound, but omits the upper bound (e.g., numpy>=10.4 ).
InJavaprojects,dependencyresolutionistypicallyhandledby
thebuildtool.In thecase of Maven,dependencies aredefinedin
pom.xml. To detect Missing Version , we identify dependencies that
do not specify a <version> tag. In dependencies that define the
tag, we detect Only Major Version as we do in Pythonprojects and
AnyUpperVersion checkingwhethertheupperversioninarangeis
missing(e.g., [1.2.3,) ).AnyMinorVersion isimpossiblebydesign
because at least a range will be always declared for minor releases.
When analyzing dependencies, CD-Linter handles transitive de-
pendencies bytraversingthe POMhierarchyrecursively.
When reviewing the detection strategies (step 3 of Figure 1),
werealized howsomelibraries,suchas SpringBoot ,self-manage
dependency versioning [ 41]. We have compiled a list of affected
dependenciesforwhichwedonotreportaFuzzyVersionCDsmell
because omitting the versionisacceptable inthesecases.
As a reaction to developers’ feedback (RQ 1), we differentiate
between libraries used in production code and tools used in the
pipeline. Not specifying a version for a tool is less critical, because
nosourcecodereliesonanAPIthatmightbreakinnewerversions.
Onthecontrary,havinganewversionwithfixedbugsandupdated
features might even be advisable. To this end, we compiled a list of
toolsusedfor PythonandJavaprojects.Theseinclude,forexample,
pipenv[31],pytest[34],pylint[33],andpip[30]forPython,and
JUnit[22],FindBugs [6],CheckStyle [1],andPMD[32]forJava
(the complete listisinour replication package [ 45]).
4 EMPIRICALSTUDYDESIGN
Thegoalof this study is to evaluate CD-Linter and determine
whether it can be useful for developers to avoid CD smells in their
CDpipeline.The qualityfocus istwo-fold:theperceivedusefulness
from original developers of projects where CD smells are detected
andtheaccuracyof CD-Linter .Theperspective isofresearchersthathave developed CD-Linter and want to transfer it to practice. The
contextconsistsof5,312open-sourceprojectshostedon GitLaband
usingCD.Morespecifically,thestudyanswersthethreeresearch
questionsformulatedinSection 2.
4.1 Context Selection
Toanswerourresearchquestions,weselectedopen-sourceprojects
hostedon GitLab. Usingthe GitLabAPI, wefilteredprojects that
do not have at least one star or that are forked from other projects
toavoidduplicates.Fromtheresulting26,984projects,weremoved
all the projects that do not contain a .gitlab-ci.yml file in their
repositories (i.e., do not use GitLabas CD server). The last filter
leftuswith5,312projectsthatwecouldanalyzeforthepresence
of CD smells. These projects have a diverse team size (from 1 to
633 members with a median of 2) and age (from 1 to 133 thousand
commitswithamedianof75).Regardingthelanguages,ourdataset
mainly includes JavaScript (16%),Python(14%),C(10%),Java(7%),
Go(4%), and Ruby(4.4%) repositories. Also, there are projects with
diverseCD adoption history.
4.2 Monitoring oftheOpenedIssues
We first run CD-Linter on the dataset of 5,312 projects that has
beendescribedinSection 4.1.Then,weidentifiedarandomsetof
CD smells in a way toachieve a balancedset of CD smells of each
typeandatmostoneCDsmellperprojectowner,toavoidflooding
the same owner with many issues. For Manual Execution we could
detect a maximum of 42 smells across owners, and we ended up
detecting atotalof168CD smells.
Once the detected CD smells were uploaded to the CD-Linter
web-basedplatform,whichcanautomaticallyreportissues,each
issuewasshowntotwoindependentevaluators(twooftheauthors,
oneofwhichwasnotinvolvedinthe CD-Linter implementation)
to remove false positives (object of a different study in Section 5.2).
Examples of false positives, which received a negative assessment,
are manually-triggered deployment jobs that were erroneously
reported as Manual Execution incidents. Each evaluator could read
thereportgeneratedby CD-Linter ,browsethefileinwhichtheCD
smellwasfound,and,ifneeded,browsetheentirerepositoryand
its history through a GitLablink. Once two evaluators reported a
positiveassessment,theissuewasautomaticallypostedandopened
onGitLab. The disagreement cases were discussed and, in case of
positive agreement, anissuewasalso opened.In total,we opened
145issues.
Wehavemonitoredtheissuesoveraperiodof6months(from
August 2019 to February 2020). During this period, we collected 64
reactions,countingissuesthathavebeenupvoted/downvoted,com-
mented,assigned,orclosed.59projectsdidnotshowanyactivity
duringtheobservationperiod,sowedecidedtoignoretheminour
analysis.Theresponserateoftheremaining,activeprojectswas
74%.Weperformedacardsorting[ 39]ofthereceived120comments
toidentifyagreementsanddisagreementswithourissuesandtheir
motivations.The card sorting was performed bytwo authorsthat,
after a first round of independent tagging, met and merged their
annotations.ESEC/FSE ’20, November8ś13,2020,VirtualEvent, USA Carmine Vassallo,Sebastian Proksch,AnnaJancso, Harald C.Gall, andMassimiliano Di Penta
Inadditiontothereactions,wecheckedthesourcecodetosee
whetherareportedsmellwasremovedorreintroducedintheobser-
vationperiod.Insomecases, thesmellwas fixeddespitea negative
reactionorwithoutany reactionto the issuewhatsoever.
Based on the developers’ reactions, issues have been classified
intothe following 5categories.
IgnoredThe issuehas been closedwithoutany further reaction.
RejectedTheissuehasbeenclosedwithamajorityofdownvotes
orwithanegative feedbackfrom the comments.
PendingThe issue is still open and under discussion among the
maintainerswithoutaclear agreement/disagreement.
Accepted The issue has been assigned for fix, has a majority of
upvotes,orapositive feedback.
FixedThe smellreportedinthe issuehas been removed.
To address RQ 1, we report and discuss the responses to the opened
issuesforeachsmelltype.Wereportthenumberandpercentagesof
positiveandnegativereactions,therationaleforrejectingtheissues,
and provide examples of positive feedback and false positives. The
results ofRQ 1directlyimproved CD-Linter (see Section 3.4).
4.3 Manual ValidationofCDSmells
We executed the enhanced version of CD-Linter on the 5,312
projects, which resulted inthe detectionof5,011 CD smells.Then,
we formed a sample to be manually validated. We selected, for
each owner, one CD smell of each type, if detected. Since for Fuzzy
Versionwehavefoursub-categories,weconsideredoneofeachsub-
category(MissingVersion,OnlyMajorVersion,AnyMinorVersion,
andAnyUpperVersion),ifpresent.Asresult,weobtainedasample
that consists of 868 issues and achieves an error margin of ±3%
(settingaconfidencelevelof95%andapercentageof50%).Then,
similarlytowhatwasdoneinRQ 1,eachissuewasindependently
validatedbytwo authors.Aftereachannotator concludedthetag-
ging,wemeasuredtheCohen’skappainter-rateragreement( k)[3].
Weobtained k=0.76,i.e.,ahighagreement,thereforenore-coding
wasnecessary.Finally,thetwoannotatorsdiscussedandsolvedthe
disagreementcases.ToaddressRQ 2,wereporttheoverallprecision
ofCD-Linter on the validated sample, defined as TP/(TP+FP),
whereTP: true positives and FP: false positives. We also computed
the recall, defined as TP/TTP(TTP: total true positives), using a
randomly selected sample of 100 projects (methodology similar to
Gallaba et al. [ 9]), making sure that those projects were not the
same usedto calculatethe precision.
4.4 MeasurementofCDSmell Occurrences
To address RQ 3, we runCD-Linter on the latest snapshot of the
5,312projectsdescribedinSection 4.1.Theanalysishasbeenper-
formed on an Intel Xeon(R) CPU E5-2640 with 2.50GHz (4 cores)
with 4GB of available main memory and took a total of 74 seconds.
We report the number of CD smells of different types we detected,
aswellasthepercentageofprojectsandownersaffectedbyatleast
oneCDsmellofeachtype.Thelatterprovidesuswithanideaof
the diffuseness ofthe consideredCD smells.
5 EMPIRICALSTUDYRESULTS
In this section, we will answer the three research questions and
report onthe results ofthe study definedinSection 4.
Figure 4:Reactionsto the opened issues (over6 months)
5.1 AretheCDSmells Detectedby CD-Linter
Relevant to Developers?
Duringtheobservationperiodof6months,64projectsreactedto
our issues (response rate of 74%), Figure 4illustrates the reactions.
Overall, 53% of the project maintainers reacted positively to our
issues: 9%acknowledged thepresence ofa problem andare about
to solve it, and 44% fixed the reported CD smells. We have also
verified that the fixes were not reverted later and could not find
casesinwhichthe reportedCD smellswere re-introduced.
Developers took on average 50 days to fix a CD smell, with a
maximum of 5.5 months and a minimum period of 1 hour. This
highvariationisunsurprisingforopen-sourceprojectsbecausethe
activity level or the commitment of contributors strongly depends
on each project. The mean resolution time for different kinds of
smellswas31daysforFakeSuccess,55daysforRetryFailure,43
days for ManualExecution,and65 days for Fuzzy Version.
Looking at the negative cases, 9% of the issues were closed with-
outreactions(i.e.,ignoredissues)and32%wererejected.Several
projectmaintainersthatrejectedissuesprovideduswithreasons
whytheywanttokeeptheCDsmell.Wefoundothercasesinwhich
developersrejectedourissuessimplybecauseofalackoftrustin
automatedissue-reportingtools.Inthefollowing,wedescribethe
reactions to each CD smell type (see Figure 4), the feedback we
received when the issues were rejected, and how we refined our
detectionstrategiesbasedontheanalyzedcomments.Wealsore-
port the percentage of false positives that we found during the
assessment stage.
Fake Success. We found onlyone false-positive case and opened
27 issues that report Fake Success cases, achieving a response rate
of70%.Noissueswereignored,10%ofthemwereaccepted,andthe
CD smell was removed in 37% of the projects. 9 opened issues (47%
of the total) were instead rejected by the project maintainers. In
severalcases,developersgenerallyagreedonourreportedviolation
but decided to accepttheCDsmell nevertheless. Somedevelopers
prefer that non-essential jobs may fail, for example, checks for out-
dateddependencies,executionofstaticanalysistools,orexternal
toolswhichmightfailforunknownreasons.Otherdevelopersal-
lowjobs,thatarenotfullyimplementedyetand,thus,shouldnot
impactthefinalbuildstatus,tofail.Theseprojectstypicallystate
that they plan to remove the CD smell when the pipeline design
iscompleted.Anotherdeveloper,whileagreeingontheviolation,ConfigurationSmellsin Continuous Delivery Pipelines ESEC/FSE ’20, November8ś13,2020,VirtualEvent, USA
did not fix the CD smell because allow_failure:true is recom-
mended for certain jobs that run static and dynamic application
securitytests providedby GitLab.Itisnolonger necessarytouse
thisconfigurationflagexplicitly(ithasbeenmovedtoatemplate
andwillbeusedimplicitlythroughinheritance),butoldtutorials
still describe it as a best practice. While failing the overall build
because of warnings raised by static analysis tools or errors in
externaltoolsisakeyCDprinciple[ 5],wecompletelyagreethat
projects should follow configuration recommendations of their CD
provider. CD-Linter recognizes these cases and does not report
them.
Retry Failure. We found 7 false positives for Retry Failure issues,
whichcorrespondsto16.7%ofthevalidatedsample.Wereported
theremaining19smellswithareturnrateof58%(thelowestrate
among all CD smell types). 9% of the maintainers confirmed the
existenceoftheproblemand55%removedtheCDsmellfromthe
configurationoftheirprojects.Only4issues(36%)wererejected.
This CD smell seems to be introduced to łhidež flakiness instead of
solving it, thus, we decided to not modify our detection strategy.
One developer mentioned that she deploys her application to a
remote service that is randomly failing. Because the tool is out
ofhercontrol,shedecidedtoautomaticallyretrythejobmultiple
timeshopingthatitwillsucceedwithoutbreakingtheoverallbuild.
ManualExecution. ManualExecutionisthe category wherewe
found the largest percentage of false positives (26.2%), due to some
periodic deployment jobs that CD-Linter did not recognize. We
opened 16 issues and achieved a response rate of 81%. While 8% of
thereportedCDsmellswereacceptedand38%werefixed,31%were
ignored.Only2issues(15%)wererejected.Inbothcases,developers
agreed on the importance of detecting this CD smell, but they also
providedreasons forrejecting it. Oneof themset when:manual in
a job executed in a stage that is not fully integrated yet with the
restofthepipeline.Thiscanbeaddressedbyallowingdevelopers
todirectlyconfigure CD-Linter andignorejobsthatarenotpart
of the CD pipeline. The other developer rejected the issue because
of a lack of trust in an automated reporting tool. While this can be
a threat to the study, it does not constitute a problem in a usage
scenario where developers use the toolthemselves.
FuzzyVersion. Weonlyfound3FuzzyVersionfalse-positivein-
stances,andwecouldopen24issuesachievingthehighestresponse
rate (87%) among all CD smell types. 9% of the reported CD smells
wereacceptedand48%fixed.Whilewecannotlearnfromthe9%
of the issues that were ignored, we used the comments from the
remaining28%rejectedissuestorefineourdetectionstrategy.Most
complaintsconcernthereportsabouttoolsforwhichtheversion
is left unspecified. In contrast to libraries, tools that are invoked in
the pipeline (e.g., tools that compute code coverage) should always
beupdatedtothelatestversion,especiallybecausetheymightcon-
tainsecurityimprovements.Furthermore,toolsaredependencies
oftheprojectratherthanofthesourcecode.Thus,inthecaseof
uncontrolledupdates,suchtoolswouldnotaffecttheoutcomeof
the build nor introduce errors, so we decided to incorporate this
feedbackandexclude toolsfrom the detection ofFuzzy Version.Table 2:Detection precision forthe four CDsmells
CD Smell Type TP FP Precision
Fuzzy Version 454 107 0.81
Fake Success 213 0 1.00
Manual Execution 27 10 0.73
Retry Failure 57 0 1.00
Overall 751 117 0.87
Table 3:Detection precision forthe Fuzzy Versionsubtypes
Type TP FP Precision
MissingVersion 335 102 0.77
Any UpperVersion 97 4 0.96
OnlyMajor Version 20 1 0.95
Any Minor Version 2 0 1.00
RQ1Summary: Wereceivedreactionsfrom74%oftheprojects.
53% of the project maintainers reacted positively to our issues,
either accepting (9%) or fixing (44%) the reported CD smells. In
therejectedissues,wereceivedprecioussuggestionsonhowto
improveCD-Linter ,whichweincorporatedwheneverpossible.
5.2 HowAccurate Is CD-Linter ?
Table2reports thedetectionprecisionof CD-Linter .Asthetable
shows,the detection precision varies between 73%of ManualExe-
cution and 100%of Retry Failure and FakeSuccess,withan81%for
FuzzyVersion.LookingattheresultsofdifferentFuzzyVersionsub-
types,Table 3indicatesthatthedetectionprecisionisthelowest
fortheMissingVersioncategory(77%),which,however,isthemost
common one. Instead, when a version is not fully specified (i.e.,
AnyUpperVersion,OnlyMajorVersion,orAnyMinorVersionCD
smell),the CD-Linter accuracyraisesto 95%orabove.
In the following, we discuss false positives. As shown in Table 2,
wefoundnofalsepositivesforRetryFailureandFakeSuccess.Note
thatthisdoesnotmeanthat CD-Linter wouldalwaysbecorrect
in such cases because developers might use these options for a
specific, valid purpose.
For Manual Execution, false positives were mostly related to
cases where the job name, content, or even comments added to the
.gitlab-ci.yml filesuggestedthatthejobisrelatedtoadeploy-
ment activity that developers intentionally perform periodically,
andthereforemanuallytrigger(e.g.,issuingarelease).Despitefilter-
ing out jobs relatedto deployment, as explainedin Section 3.4, we
stillencounteredunforeseen cases.Examplesincludeajobnamed
test-prerelase (the typo in the job name made our filtering fail),
butalsoajobnamed push,whichwaspushing Dockerimagesto
arepository(thiscasemayormaynotbefullyautomated).Also,
the names of several jobs with the whenparameter set to manual
suggest that they should not be manually triggered. However, both
theimplementationandacommentleftthereindicatethatdevel-
opers intentionally configured a manual job. Future work could
improveCD-Linter byusingNaturalLanguageProcessing(NLP)ESEC/FSE ’20, November8ś13,2020,VirtualEvent, USA Carmine Vassallo,Sebastian Proksch,AnnaJancso, Harald C.Gall, andMassimiliano Di Penta
techniques to analyze comments in CD configuration files to infer
the rationaleofchoicesmadebydevelopers.
False positivesofFuzzy Version mostlyrelateto casesinwhich
dependenciesforpipeline-relatedtoolswerelackingaversionnum-
ber.As aconsequenceofthe preliminary analysisconductedwith
developers in RQ 1, we argue that libraries used in production code
should specify an exact version of a dependency, to avoid build
failures or introducing bugs. However, this may not be strictly nec-
essary for tools, because developers may want to always use the
latest version that have fixed bugs and enhanced features. We have
derived an initial list of such tools after the feedback from develop-
ers and exclude tools like coala[2] (rule-based linter), sphinx[40]
(documentation generation),and wheel[35](packagingutility).
We estimated the recall of CD-Linter on a sample of 100
randomly-selected projects. Two authors individually inspected
the configuration files and agreed on the presence of 90 CD smells.
Weapplied CD-Linter tothesamesampleandcoulddetect85of
the manually-identified incidents, achieving a recall of 94%. 3 false
negatives were Fuzzy Version smells. Two of them were not occur-
ring inscriptlines while the other affected a requirements.pip
file, a configuration file that is not considered by our tool. The
remaining 2false negatives wereManualExecutionsmells.Those
smells were not detected by our tool because their names con-
tain deploy-related keywords. However, they were executed in
the stages metrics andbuild_unit_test . Section3.4established
name-based inclusion/exclusion criteria through inspecting a sam-
pleofprojects,butitisnotfeasibletoderiveasimpleheuristicthat
can cover all cases.We believe thatfuture iterations of CD-Linter
canremovethesefalsenegativesbyconsideringotherfeaturesof
the.gitlab-ci.yml (e.g., non-script lines in jobs) or other files
that are currently not supported, and by enabling developers to
configuretheirinclusion/exclusioncriteriaforjobandstagenames.
RQ2Summary: CD-Linter hasaprecisionof87%andarecall
of 94%, with a perfect (100%) precision for Retry Failure and
Fake Success. The false positives for Manual Execution and
Fuzzy Version were caused by the current limitations of the
tooling andcan be addressedinthe future.
5.3 HowFrequentAretheInvestigated CD
Smells in Practice?
To understand the frequency of CD smells in practice, we analyzed
the latest snapshot of 5,312 projects (as described in Section 4.1).
Amongthem,863projectsareeitherwrittenin Java(andbuiltwith
Maven)orinPythonand,therefore,qualifyfor ananalysisofthe
existence of CD smells in the wild, including Fuzzy Version, which
is the only language-specific smell. Note that 136 of the initially
consideredprojectswerethendeletedandwerenotavailablefor
our analysis.
Table4illustratestheoccurrenceofCDsmellsintheanalyzed
projects.Wedetected2,874instancesofCDsmellsthataffect13%
of the projects (14% of the investigated owners). Fuzzy Version
is the most common CD smell (54.6%) and is present in 37.1% of
theanalyzedprojects.FakeSuccessandRetryFailureaccountfor
22% and 18.5% of the identified CD smells respectively. While Fake
Success occurs in 5.4% ofthe projects, Retry Failure is present inTable 4:CDsmellsinprojectsand owners
(# is łnumberofžand% is łpercentage ofžthe analyzable instances)
Projects Owners
Smell Occurrences # % # %
Fuzzy Version 1,569 (54.6%) 320 37.1 242 37.0
Fake Success 633 (22.0%) 282 5.4 217 6.1
Retry Failure 532 (18.5%) 82 1.6 52 1.5
Manual Execution 140 (4.9%) 69 1.3 56 1.6
Overall 2,874 680 13.0 501 14.0
Table 5:CDsmellsacross different .gitlab-ci.yml sizes
(# staysforłnumberofžand% is the percentage respect to the total)
.gitlab-ci.yml Size
Small Medium Long
Smell # % # % # %
Fuzzy Version 206 13.1 564 35.9 799 50.9
Fake Success 5 0.8 70 11.1 558 88.2
Retry Failure 2 0.4 5 0.9 525 98.7
Manual Execution 5 3.6 14 10.0 121 86.4
Overall 218 7.6 653 22.7 2,003 69.7
#Projects 67 9.9 208 30.6 405 59.6
1.6% of them. 4.9% of the CD smells were Manual Execution and
affected69 projects(1.3% of the total).
HumbleandFarleyadvocatethataCDpipelineshouldbecom-
posed of at least three separate stages, i.e., compile, test, and de-
ploy [20].However,organizations cancall those stages differently,
introduce additional stages, and they can define multiple jobs
within one stage. This begs the question of whether more com-
plex pipelines are also more prone to contain CD smells, which
wouldmakeatoollike CD-Linter evenmorerelevant.Aqualita-
tive insight from our manualanalysis of Section 5.2indicated that
longer.gitlab-ci.yml files seem to contain more complex CD
pipeline definitions. We decided to split the analysis and discuss
the differentsubgroupsseparately.
Wedistinguishthreegroups, small,medium,andlong,anddefine
thesecategoriesthroughthefirstandthirdquartileoverthelength
distribution of all .gitlab-ci.yml files. Small .gitlab-ci.yml
files have up to 15 lines (9.9% of the smelly projects have them),
while a long .gitlab-ci.yml file is of at least 55 lines (59.6% of
the files with CD smells are long). The other projects (30.6%) are
medium.InTable 5,weillustratehowCDsmellinstancesarespread
acrossthedifferentclustersanditisimmediatelyclearthattheclus-
teroflong .gitlab-ci.yml filescontainsmostoftheCDsmells.
Theclusterwithsmall .gitlab-ci.yml filesincludes7.6%ofthe
detectedCDsmells,with 13.1%ofthe totalFuzzy Version smells,
3.6% of the Manual Execution incidents, and a few of the other
CD smell types. Projects with medium .gitlab-ci.yml sizes con-
tain 35.9% of the Fuzzy Version smells, around 10% of the FakeConfigurationSmellsin Continuous Delivery Pipelines ESEC/FSE ’20, November8ś13,2020,VirtualEvent, USA
Table 6:Break-downofFuzzy Versionsmell
(YAML is .gitlab-ci.yml , POM ispom.xml, REQ isrequirements.txt )
FileType
Category YAML POM REQ Total
MissingVersion 684 (48.4%) 120 (8.5%) 609 (43.1%) 1,413
OnlyMajor Version 0(0.0%) 6(46.1%) 7(53.9%) 13
Any Minor Version 0(0.0%) 0(0.0%) 3(100%) 3
Any UpperVersion 2(1.4%) 0(0.0%) 138 (98.6%) 140
Overall 686 (43.7%) 126 (8.1%) 757 (48.2%) 1,569
#Files 169 (44.6%) 43(11.3%) 167 (44.1%) 379
Success and Manual Execution problems, and 1% of Retry Fail-
ure, achieving 653 smells (22.7% of the total). The last cluster with
long.gitlab-ci.yml files contains the majority of the identify
CD smells (69.7%). 88.2% of the Fake Success smells, 86.4% of Man-
ualExecution incidents,andeven 98.7% of the RetryFailure affect
this cluster. More than half of the Fuzzy Version instances affect
long.gitlab-ci.yml files. Within this cluster, we find that 40%
oftheprojectshaveFuzzyVersionsmells,17%haveFakeSuccess
incidents, 6% are affected by Retry Failure and 4% contain Manual
Execution.Overall,31% oftheprojectsareaffectedbyatleastone
CDsmell.WhileallotherCDsmellshavemoreoccurrencesthan
FuzzyVersion, thedensityofFuzzyVersion inlongfiles is similar
to the densityinthe wholedataset (see Table 4).
Beingthemostcommonsmell,wefurtheranalyzedFuzzyVer-
sion and investigated its sub-categories concerning the different
filesthatitcanaffect(Table 6).Overall,theFuzzyVersionincidents
are mainly detected in requirements.txt files. Those files were
affected by allAny Minor Version and (almostall) Any Upper Ver-
sion that we found, while Only Major Version is also present in
severalpom.xmlfiles.MissingVersionisthemostfrequentFuzzy
Versionsmell(1,413)anditisspreadacrossthedifferentfiles.While
.gitlab-ci.yml hasthehighestnumberofMissingVersionoccur-
rences (48.4% of the total), this Fuzzy Version type has 609 and 120
instancesin requirements.txt andpom.xmlrespectively.Based
on these results, Fuzzy Version very frequently affect files differ-
ent from .gitlab-ci.yml . Thus, also CD pipelines that are not
so complex (i.e., small .gitlab-ci.yml ) can contain several Fuzzy
Version incidents, which explains why this CD smell is not only
concentratedinlongconfigurationfiles(Table 5).
RQ3Summary: The most frequent CD smell is the Fuzzy Ver-
sion(54.6%oftheinstances).Overall,CDsmellsaffect13%of
the analyzed projects and 14% of the owners, mainly occurring
inlongconfigurationfiles.
6 THREATS TO VALIDITY
Threats to construct validity are related to possible imprecisions in
ourmeasurements.Theycanbemainlyrelatedtopossiblemistakes
intheCD-Linter ’simplementation,beyondwhatwecoulddiscover
bytestingit.TheextensivemanualevaluationperformedinRQ 2
mitigates this threat. In addition, the results of RQ 2, as well as thefeedback provided by developers (RQ 1) gave us indications on how
to makeCD-Linter more accurate.
Threatsto internalvalidity concernfactors,internaltoourevalu-
ation, that could influence the results. One threat is the subjective-
nessof the manual validationof detectedsmells in RQ 2(precision
andrecall).Tolimitthisthreat,weemployedtwoevaluators,which
discussed and resolved the cases of disagreement. Also for the cod-
ing of comments that developers posted on opened issues (RQ 1),
having two coders limited the subjectiveness of the results. The
reactions we got in RQ 1and the results of RQ 3may depend on
thecharacteristicsoftheanalyzedprojects.Inparticular,projects
withdifferentdegreesofmaturitymayadoptCDpipelinesofdiffer-
entcomplexity,andmayormaynotadheretoCDprinciplesand
goodpractices.Wehavemitigatedthisthreatthroughtheproject
selection criteriaillustratedinSection 4.1.
Threatsto externalvalidity concernthegeneralizationofourfind-
ings. While we are aware that GitLabis not as popular as GitHub,
itsadoptionandnumberofrepositoriesareincreasing.Asexplained
in Section 4.1,GitLabgives the advantage of analyzing projects
using the same CD infrastructure. Besides considering a sample
(though relatively large) of projects, our evaluation is limited to
GitLabconfigurationfiles, Mavenbuilds,and Pythondependen-
cies. However,thedetection principles explainedinSection 3can
be appliedto other pieces oftechnology and the underlying con-
ceptswouldnotchange.Inthispaper,ourpurposewastostudythe
reaction of developers to the detection of CD smells, rather than
copingwithany possible technology.
7 DISCUSSION
The empirical evaluation of CD-Linter , especially the develop-
ers’feedbackcollectedinRQ 1,allowedustodistillusefullessons
learned and formulate implications for future research in this area.
LintersAreFastandCanSupportthePipelineDefinition. Undoubt-
edly, a paramount advantage of linters is that they are fast and
that they can already be applied in early development phases. Our
experimentshaveshownthat CD-Linter cananalyzeconfiguration
filesfromthousandsofprojectsintheorderofseconds.Manyof
the contacted developers have acknowledged (and often fixed) CD
smells that we have pointed out in their projects. We can conclude
that using linters to support the pipeline definition and to catch
smellsearly onis, indeed,apromisingresearch direction.
IssueReportingIsUseful,butItMustBeCarefullyDosed. Oneprob-
lem we encountered in our empirical evaluation is that some de-
velopers are irritated by (and tend to discard) automatically-posted
issues. While we tried to elaborate on the opened issues that these
weretheresultofamanualreviewprocess,somedevelopersstill
consideredourissuesasortofspam,evenwhenthesuggestionwas
meaningful.Relatedresearchshowspromisingresultswhenbots
areusedtoaidsoftwareengineers[ 24,25],butwefoundthatdevel-
opers seem to be sensitive in the context of issue trackers. Despite
somenegativereactions,oureffortsweregenerallywell-received
by developers though. To mitigate the negative effect described
above, one author followed-up on all comments on the opened
issues, to explain the purpose of CD-Linter , justify the opened
issue, and -most importantly- show that there was a human in theESEC/FSE ’20, November8ś13,2020,VirtualEvent, USA Carmine Vassallo,Sebastian Proksch,AnnaJancso, Harald C.Gall, andMassimiliano Di Penta
loop. Overall, involving open-source developers in our research
was valuable for both sides, but it was crucial to take the time and
talk to developersto show respectandemphasize the importance
ofthe research.
LintersAreIntrinsicallyImprecise. Acommonissueoflintersis
their intrinsic imprecision. Not every deviation from an advocated
principle is a smell and, often, a violation can only be assessed
when the specific context is being considered. Such decisions have
to be taken on a case by case basis for a project and go beyond the
scopeofstaticanalysistools.Thisphenomenonisnotspecificto
CD-Linter though,lowprecisionofstaticanalysistoolshasalready
beenreportedasanadoptionbarriermultipletimes[ 4,23,48].In
our case, CD-Linter seems to balance precision and recall well.
Despitemanyrejectedsmellreports,thenumberoffixedreports
and the generally positive feedback that we have received from
developersindicatethatdevelopersappreciatetheeffortandthat
toolslike CD-Linter can have apositive effectonCD practices.
Long and Complex CD Configurations Are Often Smelly. While
we find relatively few instances of the CD smells in simple con-
figuration files, the density increases with the length (and com-
plexity). One explanation could be that developers have to cope
with phenomena such as flakiness, the need for manual job trig-
gers,acceptingfailuresfromsomejobs,orspecialrequirementsfor
dependency management. For such reasons, we expect CD-Linter
to be particularlybeneficialfor projectswithacomplex pipeline.
Findings Should Be Reported Quickly. One of our lessons learned
from RQ 1was that identified issues need to be reported timely,
otherwisetheissuemaydisappearornotbevalid.Insomecases,
the CD smell was resolved already by the time we were done with
validating it, so the reported issues were unnecessary. Generally,
timely reportingis essential in case of issues that involve line num-
bers because these are fragile due to frequent source code changes
and can be soon outdated. In these cases, it might be helpful not to
link to the latest version in the repository, but to the exact commit
that has been analyzedfor the issue.
Overall,thispapershowsapromisingfutureforlintersofCI/CD
pipelines.Futurelinterscanextendtheideasinseveralways,for
example, not only considering dependency versions but also other
versionedentitiesinthebuildconfiguration,likebuildpluginsor
containerimages,inwhichthebuildisrun.Theresultsinthispaper
emphasizethe needfor more researchonlinters inthis domain.
8 RELATED WORK
Thissectiondescribesrelatedworkaboutbadpracticesandtheir
identification inCI/CDandinfrastructure-as-code scripts.
8.1 Bad Practices in CI/CD
In their landmark books about CI [ 5] and CD [ 20], previous re-
searchers outlined wrong decisions while applying CI/CD. The
lack of build automation and project visibility together with the
inability to create deployable software are a few examples of those
practicesthatpreventorganizationsfromachievingtheexpected
benefits. Duvall collected these and other bad practices in a cat-
alog of 50 anti-patterns (and their corresponding patterns) thatoccur during several steps of a CI/CD pipeline [ 29]. Zampetti et al.
[49] empirically characterized CI bad practices, finding commonal-
ities but also differences with the ones advocated by Duvall [ 29].
Anti-patternsalsooccurbecausedevelopersfaceseveralbarriers
when adopting CI/CD [ 18]. For instance, developers need to debug
failures occurring ona remoteserver andmaintain complex build
infrastructures.
The catalogs of anti-patterns and the studies discussed above
constitutethefoundationsofourwork,asweusethemtoderive
principlesfor which CD-Linter detectssmells.
8.2 DetectionofSmells in Development
Workflows
Several researchers have proposed approaches to automate the
identification, and, in some cases, the removal of problems arising
inbuildand,more ingeneral, Infrastructure as Code (IaC) scripts.
Gallabaetal.[ 9]developedanapproachfordetectingandelim-
inating misuses such as the presence of unused properties and
bypassed security checks in Travis-CI build scripts. While we also
statically analyze configuration files, our approach detects those
anti-patternsthat are violations of CI/CDprinciples.
Deviationsfromsuchprincipleshavebeenalsoinvestigatedby
Vassallo et al. [ 44]. They proposed CI-Odor, a tool that analyzes
artifacts produced during CI such as logs and revisions to detect
anti-patterns (e.g., builds become slow, developers work on feature
branchesforalongerperiod)thatoccurovertimeandcauseaCI
decay. Differently from this work, we focus on the anti-patterns
that can be staticallydetectedinconfigurationfiles.
Troubleshooting build failures is challenging and often causes
delaysinthedeliveryprocess.Apreviouswork[ 47]hasproposeda
taxonomy of build failures based on their root causes. Researchers
haveimplementedsolutionsthatautomaticallyrepairsomeofthese
build failure types [ 27,42]. Another tool [ 46] improves the un-
derstandability of build failures through log summarization. De-
spitethoseapproaches,developersstillallowfailures[ 8,10].This
strengthensourmotivationforincludingFakeSuccessinourlinter.
Finally,otherrelatedworksaredevotedtothedetectionofsmells
inIaCscripts.Sharmaetal.[ 38]leveragedbestpracticesassociated
withcodequalitymanagementtoassessconfigurationcodequality
and derived a catalog of configuration smells for IaC scripts devel-
oped inPuppet. While those smells are more similar to traditional
codesmells(i.e.,theyconcernwithmaintainabilityandunderstand-
ability of Puppetcode),CD-Linter detects smells specific to the
CI/CD configuration where developers violate principles. Rahman
et al. [36] implemented a linter that detects seven types of security
problems in IaC scripts. Their work is complementary to ours as it
dealswithaveryspecificcategoryofproblemsrelatedtoIaCscripts.
Manyoftheirsecurity smellscan alsooccur inCI/CDpipelines.
9 SUMMARY
Previous work has introduced generic [ 44] or specialized [ 9,36]
lintersthatcanhelpdeveloperstoimprovetheirCDconfigurations.
IncontrasttopreviousworkonCIsmellsthatreliesonhistorical
information [ 44], in this paper, we proposed CD-Linter , a static
analysistoolabletoidentifyfourtypesofCDsmellsinCDpipelines,
right when they are introduced inthe pipeline configuration. OurConfigurationSmellsin Continuous Delivery Pipelines ESEC/FSE ’20, November8ś13,2020,VirtualEvent, USA
empiricalevaluationhasshownthatthesupportedCDsmellsare
relevantinpractice,that CD-Linter isaccurate,andthatthesup-
ported smells frequently occur in the wild. Linters generally suffer
from many false positives, sometimes up to 90% and more [ 48],
butCD-Linter reaches a precision of 87% and recall of 94%, which
represent acceptable results and a good compromise. In a large set
of 5,312projects, we foundthat 31% ofpipelines with longconfig-
uration files are affected by at least one instance of the detected
smells. The empirical evaluation of CD-Linter and the developers’
feedbackthatwehavereceivedforRQ 1illustratedtheusefulnessof
CD-Linter andallowed ustodistill usefulinsightsthat canfoster
the adoption of CD-Linter in practice and stimulate research on
similar toolsto further advancethis area.
10 ACKNOWLEDGMENTS
Wewouldliketothankallthestudyparticipants.C.Vassalloand
H.C.GallacknowledgethesupportoftheSwissNationalScience
FoundationfortheirprojectSURF-MobileAppsData(SNFProject
No.200021-166275).
REFERENCES
[1]CheckstyleTeam.2020. Checkstyle. RetrievedSeptember10,2020from http:
//checkstyle.sourceforge.net
[2]Coala Team. 2020. Coala - Linting and fixing for all languages. Retrieved
September 10,2020 from https://coala.io/
[3]Jacob Cohen. 1960. A Coefficient of Agreement for Nominal Scales. Educational
and Psychological Measurement 20,1 (1960), 37ś46.
[4]CesarCouto,JoãoEduardoMontandon,ChristoferSilva,andMarcoTulioValente.
2011. Static correspondence and correlation between field defects and warnings
reported by a bugfinding tool. SoftwareQualityJournal 21(2011), 241ś257.
[5]P.M.Duvall,S.Matyas,andA.Glover.2007. ContinuousIntegration:Improving
SoftwareQualityand Reducing Risk . Pearson Education.
[6]FindBugs Team. 2020. FindBugs. Retrieved September 10, 2020 from http:
//findbugs.sourceforge.net/
[7]ForresterTeam.2019. The2019ForresterWaveReport. RetrievedSeptember10,
2020 from https://about.gitlab.com/analysts/forrester-cloudci19/
[8]KeheliyaGallaba,ChristianMacho,MartinPinzger,andShaneMcIntosh.2018.
Noise and heterogeneity in historical build data: an empirical study of Travis CI.
InASE. ACM,87ś97.
[9]Keheliya Gallaba and Shane McIntosh. 2020. Use and Misuse of Continuous
Integration Features: An Empirical Study of Projects That (Mis)Use Travis CI.
IEEE Trans. SoftwareEng. 46,1 (2020), 33ś50.
[10]T. A. Ghaleb, D. Alencar da Costa, Y. Zou, and A. E. Hassan. 2019. Studying
the Impact of Noises in Build Breakage Data. IEEE Transactions on Software
Engineering (2019), 1ś1.
[11]GitLabTeam.2020. GitLab. RetrievedSeptember10,2020from https://about.
gitlab.com
[12]GitLab Team. 2020. GitLab-CI Linter. Retrieved September 10, 2020 from
https://docs.gitlab.com/ce/ci/yaml/README.html#validate-the-gitlab-ciyml
[13]GitLab Team. 2020. GitLab CI/CDPipeline ConfigurationReference. Retrieved
September 10,2020 from https://docs.gitlab.com/ee/ci/yaml/
[14]GitLab Team. 2020. GitLab DAST Template. Retrieved September 10, 2020
fromhttps://gitlab.com/gitlab-org/gitlab-ee/blob/master/lib/gitlab/ci/templates/
Security/DAST.gitlab-ci.yml
[15]GitLabTeam.2020. GitLabReviewApps. RetrievedSeptember10,2020from
https://docs.gitlab.com/ee/ci/review_apps/
[16]GitLab Team. 2020. GitLab SAST Template. Retrieved September 10, 2020
fromhttps://gitlab.com/gitlab-org/gitlab-ee/blob/master/lib/gitlab/ci/templates/
Security/SAST.gitlab-ci.yml
[17]GitLabTeam.2020. GitLab TriageTemplate. RetrievedSeptember10,2020from
https://gitlab.com/gitlab-org/gitlab-triage/blob/master/.gitlab-ci.yml
[18]MichaelHilton,NicholasNelson,TimothyTunnell,DarkoMarinov,and Danny
Dig.2017.Trade-offsincontinuousintegration:assurance,security,andflexibility.
InESEC/SIGSOFT FSE . ACM,197ś207.
[19]MichaelHilton,TimothyTunnell,KaiHuang,DarkoMarinov,andDannyDig.
2016. Usage,costs,andbenefitsofcontinuousintegrationinopen-sourceprojects.
InASE. ACM,426ś437.
[20]JezHumbleandDavidFarley.2010. ContinuousDelivery:ReliableSoftwareReleases
Through Build, Test, and Deployment Automation . Addison-Wesley Professional.[21]JohnMicco.2016. FlakytestsatGoogleandhowwemitigatethem. Retrieved
September 10, 2020from https://testing.googleblog.com/2016/05/flaky-tests-at-
google-and-how-we.html
[22]JUnitTeam.2020. JUnit. RetrievedSeptember10,2020from https://junit.org/
junit5/
[23]Sunghun Kim and Michael D. Ernst. 2007. Which warnings should I fix first?. In
ESEC/SIGSOFT FSE . ACM,45ś54.
[24]Carlene Lebeuf, Margaret-Anne D. Storey, and Alexey Zagalsky. 2018. Software
Bots.IEEE Software 35,1 (2018), 18ś23.
[25]Carlene Lebeuf, Alexey Zagalsky, Matthieu Foucault, and Margaret-Anne D.
Storey. 2019. Defining and classifying software bots: a faceted taxonomy. In
BotSE@ICSE . IEEE / ACM,1ś6.
[26]Qingzhou Luo, Farah Hariri, Lamyaa Eloussi, and Darko Marinov. 2014. An
empirical analysis of flaky tests.In SIGSOFTFSE . ACM,643ś653.
[27]Christian Macho, Shane McIntosh, and Martin Pinzger. 2018. Automatically
repairing dependency-related build breakage. In SANER. IEEE Computer Society,
106ś117.
[28]Paul M. Duvall. 2010. Continuous Integration. Patterns and Antipatterns.
Retrieved September 10, 2020 from https://dzone.com/refcardz/continuous-
integration?chapter=1
[29]Paul M. Duvall. 2011. Continuous Delivery: Patterns and Antipatterns in the
Software Life Cycle. Retrieved September 10, 2020 from https://dzone.com/
refcardz/continuous-delivery-patterns
[30]Pip. 2020. Pip. Retrieved September 10,2020 from https://pypi.org/project/pip/
[31]PipTeam.2020. Pipenv:PythonDevelopmentWorkflowforHumans. Retrieved
September 10,2020 from https://docs.pipenv.org/
[32]PMDTeam.2020.PMD. RetrievedSeptember10,2020from https://pmd.github.io/
[33]Pylint Team. 2020. Pylint. Retrieved September 10, 2020 from https://www.
pylint.org/
[34]Pytest Team. 2020. Pytest. Retrieved September 10, 2020 from http://pytest.org/
[35] Python Wheel Team. 2020. PythonWheel. Retrieved September 10,2020 from
https://pypi.org/project/wheel/
[36]AkondRahman,ChrisParnin,andLaurieWilliams.2019.TheSevenSins:Security
Smells in Infrastructure As Code Scripts. In Proceedings of the 41st International
Conference on Software Engineering (Montreal, Quebec, Canada) (ICSE ’19) . IEEE
Press,Piscataway, NJ, USA,164ś175.
[37]Tony Savor, Mitchell Douglas, Michael Gentili, Laurie A. Williams, Kent L. Beck,
and Michael Stumm. 2016. Continuous deployment at Facebook and OANDA. In
ICSE(Companion Volume) . ACM,21ś30.
[38]Tushar Sharma, Marios Fragkoulis, and Diomidis Spinellis. 2016. Does your
configurationcodesmell?. In MSR. ACM,189ś200.
[39]D. Spencer and J.J. Garrett. 2009. Card Sorting: Designing Usable Categories.
(2009).
[40]SphinxTeam.2020. SpinxPythonDocumentationGenerator. RetrievedSeptem-
ber 10,2020 from http://www.sphinx-doc.org/
[41]SpringBootTeam.2020.DependencyManagementinSpringBoot. RetrievedSep-
tember 10, 2020 from https://docs.spring.io/spring-boot/docs/current/reference/
html/using-spring-boot.html#using-boot-dependency-management
[42]Simon Urli, Zhongxing Yu, Lionel Seinturier, and Martin Monperrus. 2018. How
to Design a Program Repair Bot?: Insights from the Repairnator Project. In
Proceedingsofthe40thInternationalConferenceonSoftwareEngineering:Software
Engineering inPractice (Gothenburg,Sweden) (ICSE-SEIP ’18) . ACM,10.
[43]BogdanVasilescu,YueYu,HuaiminWang,PremkumarT.Devanbu,andVladimir
Filkov. 2015. Quality and productivity outcomes relating to continuous integra-
tionin GitHub. In ESEC/SIGSOFT FSE . ACM,805ś816.
[44]Carmine Vassallo, Sebastian Proksch, Harald C. Gall, and Massimiliano Di Penta.
2019. Automated reporting of anti-patterns and decay in continuous integration.
InICSE. IEEE / ACM,105ś115.
[45]Carmine Vassallo, Sebastian Proksch, Anna Jancso, Harald C. Gall, and Mas-
similiano Di Penta. 2020. Replication Package for Configuration Smells in
Continuous Delivery Pipelines: A Linter and A Six-Month Study on GitLab.
https://doi.org/10.5281/zenodo.3861003 .
[46]CarmineVassallo,SebastianProksch,TimothyZemp,andHaraldC.Gall.2020.
Every build you break: developer-oriented assistance for build failure resolution.
EmpiricalSoftwareEngineering 25,3 (2020), 2218ś2257.
[47]CarmineVassallo,GeraldSchermann,FiorellaZampetti,DanieleRomano,Philipp
Leitner, Andy Zaidman, Massimiliano Di Penta, and Sebastiano Panichella. 2017.
ATaleofCIBuildFailures:AnOpenSourceandaFinancialOrganizationPer-
spective. In ICSME. IEEE Computer Society, 183ś193.
[48] Fadi Wedyan, Dalal Alrmuny, and James M.Bieman. 2009. The Effectiveness of
Automated Static Analysis Tools for Fault Detection and Refactoring Prediction.
InICST. IEEE Computer Society, 141ś150.
[49]Fiorella Zampetti, Carmine Vassallo, Sebastiano Panichella, Gerardo Canfora,
Harald C.Gall, and MassimilianoDi Penta. 2020. Anempirical characterization
of badpractices incontinuousintegration. EmpiricalSoftwareEngineering 25,2
(2020), 1095ś1135.