Symbolic Verification of Message Passing Interface Programs
Hengbiao Yu1∗, Zhenbang Chen1∗, Xianjin Fu1,2,J iW a n g1,2∗, Zhendong Su3,
Jun Sun4, Chun Huang1, Wei Dong1
1College of Computer, National University of Defense Technology, Changsha, China
2StateKeyLaboratoryofHighPerformanceComputing,NationalUniversityofDefenseTechnology,Changsha,China
3Department of Computer Science, ETH Zurich, Switzerland
4School of Information Systems, Singapore Management University, Singapore
{hengbiaoyu,zbchen,wj}@nudt.edu.cn,zhendong.su@inf.ethz.ch,junsun@smu.edu.sg,wdong@nudt.edu.cn
ABSTRACT
Messagepassingisthestandardparadigmofprogramminginhigh-
performancecomputing.However,verifyingMessagePassingIn-
terface(MPI)programsischallenging,duetothecomplexprogram
features(suchasnon-determinismandnon-blockingoperations).
Inthiswork,wepresentMPIsymbolicverifier(MPI-SV),thefirst
symbolic execution based tool for automatically verifying MPI pro-
gramswith non-blockingoperations.MPI-SV combinessymbolic
execution and model checking in a synergistic way to tackle the
challenges in MPI program verification. The synergy improves the
scalability and enlarges the scope of verifiable properties. We have
implemented MPI-SV1and evaluated it with 111 real-world MPI
verification tasks. The pure symbolic execution-based technique
successfully verifies 61 out of the 111 tasks (55%) within one hour,
while in comparison, MPI-SV verifies 100 tasks (90%). On aver-
age, compared with pure symbolic execution, MPI-SV achieves 19x
speedupsonverifyingthesatisfactionofthecriticalpropertyand
5x speedups on finding violations.
CCS CONCEPTS
•Software and its engineering →Software verification and
validation;
KEYWORDS
Symbolic Verification; Symbolic Execution; Model Checking; Mes-
sage Passing Interface; Synergy
ACM Reference Format:
Hengbiao Yu, Zhenbang Chen, Xianjin Fu, Ji Wang, Zhendong Su, Jun Sun,
ChunHuang,andWeiDong.2020.SymbolicVerificationofMessagePassing
InterfacePrograms.In 42ndInternationalConferenceonSoftwareEngineering
(ICSE’20),May23–29,2020,Seoul,RepublicofKorea. ACM,NewYork,NY,
USA, 13 pages. https://doi.org/10.1145/3377811.3380419
∗The first two authors contributed equally to this work and are co-first authors. Zhen-
bang Chen and Ji Wang are the corresponding authors.
1MPI-SV is available at https://mpi-sv.github.io.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ICSE ’20, May 23–29, 2020, Seoul, Republic of Korea
© 2020 Association for Computing Machinery.
ACM ISBN 978-1-4503-7121-6/20/05...$15.00
https://doi.org/10.1145/3377811.33804191 INTRODUCTION
Nowadays, an increasing number of high-performance computing
(HPC) applications have been developed to solve large-scale prob-
lems [11]. The Message Passing Interface (MPI) [ 78] is the current
defactostandardprogrammingparadigmfordevelopingHPCappli-
cations.ManyMPIprogramsaredevelopedwithsignificanthuman
effort. One of the reasons is that MPI programs are error-prone
because of complex program features (such as non-determinism
andasynchrony )andtheirscale.ImprovingthereliabilityofMPI
programs is challenging [29, 30].
Program analysis [ 64] is an effective technique for improving
program reliability. Existing methods for analyzing MPI programs
can be categorized into dynamic andstaticapproaches. Most ex-
istingmethodsaredynamic,suchasdebugging[ 51],correctness
checking [ 71] and dynamic verification[ 83]. These methodsneed
concreteinputstorunMPIprogramsandperformanalysisbased
on runtime information. Hence, dynamic approaches may miss
input-relatedprogramerrors.Staticapproaches[ 5,9,55,74]ana-
lyze abstract models of MPI programs and suffer from false alarms,
manualeffort,andpoorscalability.Tothebestofourknowledge,
existingautomatedverification approachesforMPIprogramseither
do not support input-related analysis or fail to support the analysis
of the MPI programs with non-blocking operations, the invocations
of which do not block the program execution. Non-blocking opera-
tions are ubiquitous in real-world MPI programs for improving the
performance but introduce more complexity to programming.
Symbolicexecution[ 27,48]supportsinput-relatedanalysisby
systematically exploring a program’s path space. In principle, sym-
bolicexecutionprovidesabalancebetweenconcreteexecutionand
static abstraction with improved input coverage or more precise
program abstraction. However, symbolic execution based analyses
suffer frompath explosiondue to theexponential increaseof pro-
grampaths w.r.t.thenumberofconditionalstatements.Theproblem
isparticularlyseverewhenanalyzingMPIprogramsbecauseofpar-
allelexecutionandnon-deterministicoperations.Existingsymbolic
execution based verification approaches [ 77][25] do not support
non-blocking MPI operations.
In this work, we present MPI-SV, a novel verifier for MPI pro-
grams bysmartlyintegrating symbolic executionand modelcheck-
ing. As far as we know, MPI-SV is the first automated verifier
thatsupportsnon-blockingMPIprogramsandtheverificationof
LTL [58] properties. MPI-SV uses symbolic execution to extract
path-level models from MPI programs and verifies the models w.r.t.
the expected properties by model checking [ 17]. The two tech-
niques complement each other: (1) symbolic execution abstracts
12482020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE)
thecontrolanddatadependenciestogenerateverifiablemodelsfor
modelchecking,and(2)modelcheckingimprovesthescalabilityof
symbolic execution by leveragingthe verification results to prune
redundant paths and enlarges the scope of verifiable properties ofsymbolic execution.
In particular, MPI-SV combines two algorithms: (1) symbolic
execution of non-blocking MPI programs with non-deterministic
operations, and (2) modeling and checking the behaviors of anMPI program path precisely. To safely handle non-deterministic
operations,thefirstalgorithmdelaysthemessagematchingsofnon-deterministicoperationsasmuchaspossible.Thesecondalgorithm
extracts a model from an MPI program path. The model represents
all the path’s equivalent behaviors, i.e., the paths generated by
changingtheinterleavingsandmatchingsofthecommunication
operationsinthepath.Wehaveprovedthatourmodelingalgorithm
ispreciseandconsistentwiththeMPIstandard[ 24].Wefeedthe
generated models from the second algorithm into a model checker
to perform verification w.r.t.the expected properties, i.e.,safety
andlivenessproperties in linear temporal logic (LTL) [ 58]. If the
extractedmodelfromapath psatisfiestheproperty φ,p’sequivalent
pathscanbesafelypruned;otherwise,ifthemodelcheckerreportsacounterexample,aviolationof
φisfound.Thisway,wesignificantly
boost the performance of symbolic execution by pruning a large
setofpathswhichareequivalenttocertainpathsthathavebeen
already model-checked.
We have implemented MPI-SV for MPI C programs based on
Cloud9[10]andPAT[ 80].WehaveusedMPI-SVtoanalyze12real-
world MPI programs, totaling 47K lines of code (LOC) (three are
beyondthescalethatthestate-of-the-artMPIverificationtoolscan
handle),w.r.t.the deadlockfreedom propertyand non-reachability
properties. For the 111deadlock freedom verification tasks, when
we set the time threshold to be an hour, MPI-SV can complete 100
tasks,i.e., deadlock reported or deadlock freedom verified, while
puresymbolicexecutioncancomplete61tasks.Forthe100com-
pleted tasks, MPI-SV achieves, on average, 19x speedups on verify-
ingdeadlockfreedomand5x speedupsonfindingadeadlock.
The main contributions of this work are:
•A synergistic framework combining symbolic execution and
model checking for verifying MPI programs.
•A method for symbolic execution of non-blocking MPI pro-
gramswithnon-deterministicoperations.Themethodisformallyproventopreservethecorrectnessofverifyingreachabilityprop-
erties.
•Aprecisemethodformodelingtheequivalentbehaviorsofan
MPIpath,whichenlargesthescopeoftheverifiableproperties
and improves the scalability.
•A tool for symbolic verification of MPI C programs and an ex-
tensive evaluation on real-world MPI programs.
2 ILLUSTRATION
Inthissection,wefirstintroduceMPIprogramsanduseanexample
to illustrate the problem that this work targets. Then, we overview
MPI-SV informally by the example.Proc::= varr:T|r:=e|Comm|Proc;Proc|
ifeProc elseProc|while edoProc
Comm ::= Ssend(e) |Send(e) |Recv(e) |Recv(*) |Barrier |
ISend(e,r) |IRecv(e,r) |IRecv(*,r) |Wait(r)
Figure 1: Syntax of a core MPI language.
2.1 MPI Syntax and Motivating Example
MPI implementations, such as MPICH [ 31] and OpenMPI [ 26], pro-
vide the programming interfaces of message passing to support
thedevelopmentofparallelapplications.AnMPIprogramcanbe
implementedindifferentlanguages,suchasCandC++.Without
loss of generality, we focus on MPI programs written in C. Let T
beasetoftypes, Nasetofnames,and Easetofexpressions.For
simplifying the discussion, we define a core language for MPI pro-
cessesinFigure1,where T∈T,r∈N,and e∈E.AnMPIprogram
MPisdefinedby a finitesetof processes {Proc i|0≤i≤n}.For
brevity, we omit complex language features (such as the messagesin the communication operations and pointer operations) although
MPI-SV does support real-world MPI C programs.
The statement varr:Tdeclares avariable rwith type T. The
statement r:=eassigns the value of expression eto variable r.
Aprocesscanbeconstructedfrombasicstatementsbyusingthe
composition operations including sequence, condition and loop.
For brevity, we incorporate the key message passing operations in
the syntax, where eindicates the destination process’s identifier.
These message passing operations can be blockingornon-blocking.
First, we introduce blocking operations:
•Ssend(e) :sendsamessagetothe ethprocess,andthesending
processblocksuntilthemessageisreceivedbythedestination
process.
•Send(e): sends a message to the eth process, and the sending
processblocksuntilthemessageiscopiedintothesystembuffer.
•Recv(e): receives a message from the eth process, and the re-
ceiving process blocks until the message from the eth process is
received.
•Recv(*):receivesamessagefrom anyprocess,andthereceiv-
ing process blocks until a message is received regardless which
process sends the message.
•Barrier: blocks the process until all the processes have called
Barrier.
•Wait(r):theprocessblocksuntiltheoperationindicatedby ris
completed.
ARecv(*) operation,called wildcardreceive,mayreceiveames-
sage from different processes under different runs, resulting innon-determinism. The blocking of a
Send(e)operation depends
on the size of the system buffer, which may differ under differ-
entMPIimplementations.Forsimplicity,weassumethatthesize
of the system buffer is infinite. Hence, each Send(e) operation
returnsimmediately after being issued. Note that our implemen-
tation allows users to configure the buffer size. To improve the
performance, the MPI standard provides non-blockingoperations
to overlap computations and communications.
1249P0 P1 P2 P3
Send(1) if(x!= ‘a’) Send(1) Send(1)
Recv(0)
else
IRecv(*,req);
Recv(3)
Figure 2: An illustrative example of MPI programs.
•ISend(e,r) : sendsa message to the eth process,and the opera-
tion returnsimmediately afterbeing issued.The parameter ris
the handle of the operation.
•IRecv(e,r) : receives a message from the eth process, and the
operation returns immediately after being issued. IRecv(*,r)
is the non-blocking wildcard receive.
The operations above are key MPI operations. Complex operations,
such as MPI_Bcast andMPI_Gather , can beimplemented by com-
posing these key operations. The formal semantics of the core
languageisdefinedbasedoncommunicatingstatemachines(CSM)
[8].WedefineeachprocessasaCSMwithanunboundedreceiving
FIFOqueue.Forthesakeofspacelimit,theformalsemanticscan
be referred to [91].
An MPI program runs in many processes spanned across multi-
plemachines.Theseprocessescommunicatebymessagepassingtoaccomplishaparalleltask.Besidesparallelexecution,thenon-
determinism in MPI programs mainly comes from two sources:
(1)inputs,whichmayinfluencethecommunicationthroughcon-
trol flow, and (2) wildcard receives, which lead to highly non-
deterministic executions.
Consider the MPI program in Figure 2. Processes P0,P2and
P3only send a message to P1and then terminate. For process P1,
if inputxisnotequal to ‘a’, P1receives a message from P0in
a blocking manner; otherwise, P1uses a non-blocking wildcard
receive to receive a message. Then, P1receives a message from
P3. Whenxis ‘a’ and IRecv(*,req) receives the message from
P3,adeadlock occurs,i.e.,P1blocks at Recv(3), and all the other
processes terminate. Hence, to detect the deadlock, we need to
handlethenon-determinismcausedbytheinput xandthewildcard
receive IRecv(*,req).
To handle non-determinism due to the input, a standard remedy
is symbolic execution [ 48]. However, there are two challenges. The
first one is to systematically explore the paths of an MPI program
withnon-blockingandwildcardoperations,whichsignificantlyin-
crease the complexity of MPI programs. A non-blocking operation
does not block but returns immediately, causing out-of-order com-
pletion. The difficulty in handling wildcard operations is to get all
thepossiblymatchedmessages.Thesecondoneisto improvethe
scalabilityofthesymbolicexecution. Symbolicexecutionstruggles
withpathexplosion.MPIprocessesrunconcurrently,resultingin
an exponential number of program paths w.r.t.the number of pro-
cesses.Furthermore,thepathspaceincreasesexponentiallywith
the number of wildcard operations.
2.2 Our Approach
MPI-SVleveragesdynamicverification[ 83]andmodelchecking[ 17]
to tackle the challenges. Figure 3 shows MPI-SV’s basic framework.An MPI 
Program
CSP Model CheckerViolation
PathSymbolic Executor
State Pruner
Violation
MPI-SVYesNoYes
PropertyTest Case
CSP ModelNo
Figure 3: The framework of MPI-SV.
TheinputsofMPI-SVareanMPIprogramandanexpectedproperty,
e.g.,deadlockfreedom expressedinLTL.MPI-SVusesthebuilt-in
symbolic executor to explore the path space automatically andchecks the property along with path exploration. For a path that
violates the property, called a violation path, MPI-SV generatesa
test case for replaying, which includes the program inputs, the
interleavingsequenceofMPIoperationsandthematchingsofwild-
card receives. In contrast, for a violation-free pathp, MPI-SV builds
acommunicatingsequentialprocess(CSP)model Γ,whichrepre-
sents the paths which can be obtained based on pby changing the
interleavingsandmatchingsofthecommunicationoperationsin
p. Then, MPI-SV utilizes a CSP model checker to verify Γw.r.t.the
property.Ifthemodelcheckerreportsacounterexample,aviola-
tion is found; otherwise, if Γsatisfies the property, MPI-SV prunes
all behaviors captured by the model so that they are avoided by
symbolic execution.
SinceMPIprocessesarememoryindependent,MPI-SVwillse-
lectaprocesstoexecuteina round-robin mannertoavoidexploring
all interleavings of the processes. A process keeps running until it
blocksorterminates.WhenencounteringanMPIoperation,MPI-SV
records the operation instead of executing it and doing the mes-sage matching. When every process blocks or terminates and at
leastoneblockedprocessexists,MPI-SVmatchestherecordedMPI
operationsoftheprocesses w.r.t.theMPIstandard[ 24].Theintu-
ition behind this strategy is to collect the message exchanges as
thoroughlyaspossible,whichhelpsfindpossiblematchingsforthe
wildcard receive operations. Consider the MPI program in Figure 2
and thedeadlock freedom property. Figure 4 shows the symbolic
execution tree, where the node labels indicate process communica-
tions,e.g.,(3,1)means that P1receives a message from P3. MPI-SV
first symbolically executes P0, which only sends a message to P1.
TheSend(1)operationreturnsimmediatelywiththeassumption
of infinite system buffers. Hence, P0terminates, and the operation
Send(1) is recorded. Then, MPI-SV executes P1and explores both
branches of the conditional statement as follows.
(1) True branch (x /nequal‘a’).In this case, P1blocks at Recv(0).
MPI-SVrecordsthereceiveoperationfor P1,andstartsexecuting P2.
LikeP0,P2executesoperation Send(1)andterminates,afterwhich
P3is selected and behaves the same as P2. AfterP3terminates, the
globalexecutionblocks, i.e.,P1blocksandalltheotherprocesses
terminate. When this happens, MPI-SV matches the recorded oper-
ations, performs the message exchanges and continues to execute
thematchedprocesses.The Recv(0) inP1shouldbematchedwith
theSend(1) inP0. After executing the send and receive opera-
tions,MPI-SVselects P1toexecute,because P0terminates.Then,
P1blocksat Recv(3).Same asearlier, theglobalexecutionblocks
1250x≠'a' x='a'
(0,1) (0,1) (2,1) (3,1)
(3,1) (3,1)
p1 p2 p3p4
Deadlock (3,1)
Figure 4: The example program’s symbolic execution tree.
andoperationmatchingneedstobedone. Recv(3)ismatchedwith
theSend(1)inP3. After executing the Recv(3)andSend(1)op-
erations, all the processes terminate successfully. Path p1in Figure
4 is explored.
(2) False branch (x =‘a’).The execution of P1proceeds until
reachingtheblockingreceive Recv(3).Additionally,thetwoissued
receiveoperations, i.e.,IRecv(*,req) andRecv(3),arerecorded.
Similartothetruebranch,wheneveryprocessblocksorterminates,
wehandleoperationmatching.Here P0,P2andP3terminate,and P1
blocks at Recv(3).IRecv(*,req) should be matched first because
ofthenon-overtaken policyintheMPIstandard[ 24].Therearethree
Sendoperationcandidatesfrom P0,P2andP3,respectively.MPI-SV
forks a state for each candidate. Suppose MPI-SV first explores the
state where IRecv(*,req) is matched with P0’sSend(1). After
matchingandexecuting P1’sRecv(3)andP3’sSend(1),thepath
terminates successfully, which generates path p2in Figure 4.
Violation detection. MPI-SVcontinuestoexploretheremain-
ing two cases. Without CSP-based boosting, the deadlock would
be found in the last case ( i.e.,p4in Figure 4), where IRecv(*,req)
is matched with P3’sSend(1)andP1blocks because Recv(3)has
no matchedoperation. MPI-SVgenerates a CSPmodel Γbased on
the deadlock-free path p2whereP1’sIRecv(*,req) is matched
withP0’sSend(1). Each MPI process is modeled as a CSP pro-
cess, and all the CSP processes are composed in parallel to form
Γ. Notably, in Γ, we collect the possible matchings of a wildcard
receive through statically matching the arguments of operations in
thepath.Additionally,therequirementsintheMPIstandard, i.e.,
completes-before relations [ 83], are also modeled. A CSP model
checkerthenverifiesdeadlockfreedomfor Γ.Themodelchecker
reports a counterexample where IRecv(*,req) is matched with
theSend(1)inP3. MPI-SV only explores twopaths for detecting
the deadlock and avoids the exploration of p3andp4(indicated by
dashed lines).
Pruning. Because the CSP modeling is precise (cf. Section 4),
inadditiontofindingviolationsearlier,MPI-SVcanalsoperform
pathpruningwhenthemodelsatisfiestheproperty.Supposewe
change theprogram in Figure 2to bethe onewhere the laststate-
mentofP1isaRecv(*)operation.Then,theprogramis deadlock
free.Thetruebranch( x/nequal‘a’)has2paths,becausethelastwildcard
receivein P1hastwomatchings( i.e.,P2’ssendand P3’ssend,and
P0’s send has been matched by P1’sRecv(0)). The false branch
(x=‘a’) has 6 paths because the first wildcard receive has 3 match-
ings (send operations from P0,P2andP3) and the last wildcard
receive has 2 matchings (because the first wildcard receive has
matched one send operation). Hence, in total, there are 8 paths(i.e.,2+3∗2=8)if we usepure symbolicexecution. Incontrast,
with model checking, MPI-SV only needs 2 paths to verify that the
program is deadlock-free. For each branch, the generated model is
verified to be deadlock-free, so MPI-SV prunes the candidate states
forked for the matchings of the wildcard receives.
Properties. Because our CSP modeling encodes the interleav-
ingsoftheMPIoperationsintheMPIprocesses,thescopeofthe
verifiableproperties isenlarged, i.e.,MPI-SV canverifysafetyand
liveness properties in LTL. Suppose we change the property tobe the one that requires the
Send(1) operation in P0should be
completedbeforethe Send(1)operationin P2.Actually,thesend
operationin P2canbecompletedbeforethesendoperationin P0,
due to the nature of parallel execution. However, puresymbolic
execution fails to detect the property violation. In contrast, withthe help of CSP modeling, when we verify the model generatedfrom the first path
w.r.t.the property, the model checker gives a
counterexample, indicating that a violation of the property exists.
3 SYMBOLIC VERIFICATION METHOD
In this section, we present our symbolic verification framework
and then describe MPI-SV’s symbolic execution method.
3.1 Framework
Given an MPI program MP={Proc i|0≤i≤n}, a stateSc
inMP’s symbolic execution is composed by the states of pro-
cesses,i.e.,(s0, ...,sn), and each MPI process’s state is a 6-tuple
(M,Stat,PC,F,B,R), whereMmaps each variable to a concrete
value or a symbolic value, Statis the next program statement to
execute,PCis the process’s path constraint [ 48],Fis the flag of
process status belonging to {active ,blocked ,terminated },Band
Rare infinite buffers for storing the issued MPI operations not
yet matched and the matched MPI operations, respectively. Weuse
si∈Scto denote that siis a process state in the global state
Sc. An element elemofsican be accessed by si.elem,e.g.,si.Fis
theithprocess’sstatusflag.Inprinciple,astatementexecutionin
any process advances the global state, making MP’s state space
exponentialtothenumberofprocesses.Weusevariable Seqide-
fined inMto record the sequence of the issued MPI operations in
Proc i, and Seq(Sc)to denote the set {Seqi|0≤i≤n}of global
stateSc. Global state Sc’s path condition (denoted by Sc.PC)i st h e
conjunctionofthepathconditionsof Sc’sprocesses, i.e.,/logicalandtext.1
si∈Scsi.PC.
Algorithm 1 shows the details of MPI-SV. We use worklistto
store the global states to be explored. Initially, worklistonly con-
tainsSinit,composedoftheinitialstatesofalltheprocesses,and
eachprocess’sstatusis active.AtLine4, Selectpicksastatefrom
worklistas the one to advance. Hence, Selectcan be customized
with different search heuristics, e.g., depth-first search (DFS). Then,
Scheduler selects an active process Proc ito execute. Next, Execute
(cf.Algorithm2)symbolicallyexecutesthestatement Stat iinProc i,
and may add new states into worklist. This procedure continues
untilworklistis empty ( i.e., all the paths have been explored), de-
tectingaviolationortimeout(omittedforbrevity).Afterexecuting
Stat i,ifalltheprocessesinthecurrentglobalstate Scterminate, i.e.,
a violation-free path terminates, we use Algorithm 4 to generate a
CSP model Γfrom the current state (Line 8). Then, we use a CSP
modelcheckertoverify Γw.r.t.φ.IfΓsatisfiesφ(denotedby Γ|=φ),
1251Algorithm 1: Symbolic Verification Framework
MPI-SV(MP ,φ,Sym)
Data:MPis{Proc i|0≤i≤n},φis a property, and Sym
is a set of symbolic variables
1begin
2worklist←{Sinit}
3whileworklist /nequal∅do
4 Sc←Select(worklist )
5 (Mi,Stat i,PCi,Fi,Bi,Ri)←Scheduler( Sc)
6 Execute(Sc,Proc i,Stat i,Sym,worklist)
7 if∀si∈Sc,si.F=terminated then
8 Γ←GenerateCSP( Sc)
9 ModelCheck( Γ,φ)
10 ifΓ|=φthen
11 worklist←worklist \{Sp∈worklist|Sp.PC⇒Sc.PC}
12 end
13 else if Γ/negationslash|=φthen
14 reportViolation andExit
15 end
16 end
17end
18end
we prune the global states forked by the wildcard operations along
thecurrentpath(Line11), i.e.,thestatesin worklistwhosepathcon-
ditions imply Sc’s path condition; otherwise, if the model checker
gives a counterexample, we report the violation and exit (Line 14).
SinceMPIprocessesarememoryindependent,weemploypartial
orderreduction(POR)[ 17]toreducethesearchspace. Scheduler
selectsaprocessina round-robin fashionfromthecurrentglobal
state. In principle, Scheduler starts from the active MPI process
withthesmallestidentifier, e.g.,Proc0atthebeginning,andanMPI
processkeepsrunninguntilit isblockedorterminated.Then,the
next active process will be selected to execute. Such a strategy sig-
nificantlyreducesthepathspaceofsymbolicexecution.Then,with
the help of CSP modeling and model checking, MPI-SV can verify
more properties, i.e., safety and liveness properties in LTL. The
details of such technical improvements will be given in Section 4.
3.2 Blocking-driven Symbolic Execution
Algorithm 2 shows the symbolic execution of a statement. Com-
mon statements such as conditional statements are handled in the
standard way [ 48] (omitted for brevity), and here we focus on MPI
operations. The main idea is to delaythe executions of MPI opera-
tionsasmuchaspossible, i.e.,tryingtogetallthemessagematchings.
Insteadofexecution, Algorithm 2 recordseachMPIoperationfor
each MPI process (Lines 4&8). We also need to update buffer B
after issuing an MPI operation (Lines 5&9). Then, if Stat iis a non-
blockingoperation,theexecutionreturnsimmediately;otherwise,
weblock Proc i(Line10,exceptingthe Waitofan ISendoperation).
When reaching GlobalBlocking (Lines 11&12), i.e., every process is
terminatedorblocked,weuse Matching (cf.Algorithm3)tomatch
the recorded but not yet matched MPI operations and execute the
matchedoperations.Sincetheopportunityofmatchingmessages
isGlobalBlocking, we call it blocking-driven symbolic execution.
Matching matches the recorded MPI operations in different pro-
cesses.Toobtainallthepossiblematchings,wedelaythematchingAlgorithm 2: Blocking-driven Symbolic Execution
Execute(Sc,Proc i,Stat i,Sym,worklist)
Data:Global state Sc, MPI process Proc i, Statement Stat i,
Symbolic variable set Sym,worklistof global states
1begin
2switch(Stat i)do
3 case SendorISendorIRecvdo
4 Seqi←Seqi·/angbracketleftStat i/angbracketright
5 si.B←si.B·/angbracketleftStat i/angbracketright
6 end
7 case BarrierorWaitorSsendorRecvdo
8 Seqi←Seqi·/angbracketleftStat i/angbracketright
9 si.B←si.B·/angbracketleftStat i/angbracketright
10 si.F← blocked
11 ifGlobalBlocking then
//∀si∈Sc,(si.F=blocked∨si.F=terminated)
12 Matching( Sc,worklist)
13 end
14 end
15 default:
Execute(Sc,Proc i,Stat i,Sym,worklist)as normal
16end
17end
of a wildcard operation as much as possible.W eu s e match Nto
matchthenon-wildcardoperationsfirst(Line3) w.r.t.therulesin
the MPI standard [ 24], especially the non-overtaken ones: (1) if two
sendsofaprocesssendmessagestothesamedestination,andboth
can match the same receive, the receive should match the first one;
and(2)ifaprocesshastworeceives,andbothcanmatchasend,the
first receive should match the send. The matched send and receive
operationswillbeexecuted,andthestatusesoftheinvolvedpro-
cesses will be updated to active, denoted by Fire(Sc,pairn)(Line
5). If there is no matching for non-wildcard operations, we use
Algorithm 3: Blocking-driven Matching
Matching( Sc,worklist)
Data:Global state Sc,worklistof global states
1begin
2MSW←∅ // Matching set of wildcard operations
3pairn←match N(Sc) // Match non-wildcard operations
4ifpairn/nequalempty pair then
5 Fire(Sc,pair n)
6end
7else
8 MSW←match W(Sc) // Match wildcard operations
9 forpairw∈MSWdo
10 S/primec←fork(Sc,pairw)
11 worklist←worklist∪{S/primec}
12 end
13 ifMSW/nequal∅then
14 worklist←worklist\{Sc}
15 end
16end
17ifpairn=empty pair ∧MSW=∅then
18 reportDeadlock andExit
19end
20end
1252P0 P1 P2
ISend(1,req 1); IRecv(*,req 2); Barrier;
Barrier; Barrier; ISend(1,req 3);
Wait(req 1) Wait(req 2) Wait(req 3)
Figure 5: An example of operation matching.
match Wto match the wildcard operations (Line 8). For each possi-
ble matchingof awildcardreceive,we fork anew state (denoted
byfork(Sc,pairw)at Line 10) to analyze each matching case. If no
operations can be matched, but there exist blocked processes, a
deadlockhappens(Line17).Besides,fortheLTLpropertiesother
thandeadlockfreedom(suchastemporalproperties),wealsocheck
them during symbolic execution (omitted for brevity).
Take the program in Figure 5for example. When all the pro-
cessesblockat Barrier,MPI-SVmatchestherecordedoperation
in the buffers of the processes, i.e.,s0.B=/angbracketleftISend(1,req 1),Barrier/angbracketright,
s1.B=/angbracketleftIRecv(*,req 2),Barrier/angbracketright, and s2.B=/angbracketleftBarrier /angbracketright. Accordingto
the MPI standard, each operation in the buffers is ready to be
matched.Hence, Matching firstmatchesthenon-wildcardopera-
tions,i.e.,the Barrieroperations,thenthestatusofeachprocessbe-
comes active.Afterthat,MPI-SVcontinuestoexecutetheactivepro-
cesses and record issued MPI operations. The next GlobalBlocking
point is:P0andP2terminate, and P1blocks at Wait(req 2). The
buffersare /angbracketleftISend(1,req 1),Wait(req 1)/angbracketright,/angbracketleftIRecv(*,req 2),Wait(req 2)/angbracketright,
and/angbracketleftISend(1,req 3),Wait(req 3)/angbracketright,respectively.Alltheissued Wait
operations are not ready to match, because the corresponding
non-blocking operations are not matched. So Matching needs to
match the wildcard operation, i.e.,IRecv(*,req 2), which can be
matched with ISend(1,req 1)orISend(1,req 3). Then, a new
state is forked for each case and added to the worklist.
Correctness .Blocking-drivensymbolicexecutionisaninstance
of model checking with POR. We have proved the symbolic execu-
tion method is correct for reachability properties [58]. Due to the
space limit, the proof can be referred to [91].
4 CSP BASED PATH MODELING
In this section, we first introduce the CSP [ 70] language. Then, we
present the modeling algorithm of an MPI program terminatedpath using a subset of CSP. Finally, we prove the soundness and
completeness of our modeling.
4.1 CSP Subset
LetΣbe afiniteset ofevents,Ca set of channels, and Xa set
of variables. Figure 6 shows the syntax of the CSP subset, where
Pdenotes a CSP process, a∈Σ,c∈C,X⊆Σandx∈X. The single
event process aperforms the event aand terminates. There are
three operators: sequential composition ( /fatsemi), external choice ( /square) and
parallel composition with synchronization ( /bardbl
X).P/squareQperforms as P
P:=a|P/fatsemiP|P/squareP|P/bardbl
XP|c?x→P|c!x→P|skip
Figure 6: The syntax of a CSP subset.orQ, and the choice is made by the environment. Let PSbe a finite
setofprocesses, /squarePSdenotestheexternalchoiceofalltheprocesses
inPS.P/bardbl
XQperforms PandQinaninterleavingmanner,but Pand
Qsynchronize on the events in X. The process c?x→Pperforms
asPafterreadingavaluefromchannel candwritingthevalueto
variablex. The process c!x→Pwrites the value of xto channel c
and then behaves as P. Process skipterminates immediately.
4.2 CSP Modeling
For each violation-free program path, Algorithm 4 builds a precise
CSPmodelofthepossiblecommunicationbehaviorsbychanging
the matchings and interleavings of the communication operations
along the path. The basic idea is to model the communication
operationsin eachprocess asaCSP process,then composeallthe
CSP processes in parallel to form the model. To model Proc i,w e
scanits operationsequence Seqiinreverse.Foreach operation,we
generate its CSP model and compose the model with that of the
remainingoperationsin Seqiw.r.t.thesemanticsoftheoperation
and the MPI standard [ 24]. The modeling algorithm is efficient,
and has a polynomial time complexity w.r.t.the total length of the
recorded MPI operation sequences.
We use channel operations in CSP to model send and receive
operations.Eachsendoperation ophasitsownchannel,denoted
byChan(op). We use a zero-sized channel to model Ssendopera-
tion(Line10),because Ssendblocksuntilthemessageisreceived.
In contrast, considering a SendorISendoperation is completed
immediately,weuse one-sized channelsforthem(Line14),sothe
channel writing returns immediately. The modeling of Barrier
(Line 17) is to generate a synchronization event that requires all
the parallel CSP processes to synchronize it (Lines 17&38). Themodeling of receive operations consists of three steps. The first
step calculates the possibly matched channels written by the send
operations(Lines20&25).Thesecondusestheexternalchoiceof
reading actions of the matched channels (Lines 21&26), so as to
model different cases ofthe receive operation. Finally, the refined
externalchoiceprocessiscomposedwiththeremainingmodel.If
theoperationisblocking,thecompositionissequential(Line22);
otherwise, it is a parallel composition (Line 28).
StaticMatchedChannel( opj,S)(Lines20&25)returnsthesetof
thechannelswrittenbythepossiblymatchedsendoperationsof
the receive operation opj. We scan Seq(S)to obtain the possibly
matched send operations of opj. Given a receive operation recvin
process Proc i,SMO(recv,S)calculatedasfollowsdenotestheset
of the matched send operations of recv.
•IfrecvisRecv(j)orIRecv(j,r),SMO(recv,S)contains Proc j’s
send operations with Proc ias the destination process.
•IfrecvisRecv(∗)orIRecv(∗ ,r),SMO(recv,S)containsanypro-
cess’s send operations with Proc ias the destination process.
SMO(op,S)over-approximates op’s precisely matched opera-
tions, and can be optimized by removing the send operations that
aredefinitelyexecutedafter op’scompletion,andtheoneswhose
messages are definitely received before op’s issue. For example,
LetProc0beSend(1);Barrier;Send(1),and Proc1beRecv(*);Barrier.
SMOwill add the two send operations in Proc0to the matching
setofthe Recv(*)inProc1.Since Recv(*)mustcompletebefore
Barrier, we can remove the second send operation in Proc0. Such
1253Algorithm 4: CSP Modeling for a Terminated State
GenerateCSP( S)
Data:A terminated global state S, and
Seq(S)={Seqi|0≤i≤n}
1begin
2PS←∅
3fori←0...ndo
4 Pi←skip
5 Req←{r|IRecv(*,r) ∈Seqi∨IRecv(i,r) ∈Seqi}
6 forj←lenдth(Seqi)−1...0do
7 switchopjdo
8 case Ssend(i) do
9 c1←Chan(opj) //c1’s size is 0
10 Pi←c1!x→Pi
11 end
12 case Send(i)orISend(i,r) do
13 c2←Chan(opj) //c2’s size is 1
14 Pi←c2!x→Pi
15 end
16 case Barrierdo
17 Pi←B/fatsemiPi
18 end
19 case Recv(i)orRecv(*)do
20 C←StaticMatchedChannel( opj,S)
21 Q←Refine(/square{c?x→skip|c∈C},S)
22 Pi←Q/fatsemiPi
23 end
24 case IRecv(*,r) orIRecv(i,r) do
25 C←StaticMatchedChannel( opj,S)
26 Q←Refine(/square{c?x→skip|c∈C},S)
27 ew←WaitEvent( opj)//opj’s wait event
28 Pi←(Q/fatsemiew)/bardbl
{ew}Pi
29 end
30 case Wait(r)andr∈Reqdo
31 ew←GenerateEvent( opj)
32 Pi←ew/fatsemiPi
33 end
34 end
35 end
36 PS←PS∪{Pi}
37end
38P←/bardbl
{B}PS
39returnP
40end
optimization reduces the complexity of the CSP model. For brevity,
we use SMO(op,S)to denote the optimized matching set. Then,
StaticMatchedChannel( opj,S)is{Chan(op)|op∈SMO(opj,S)}.
TosatisfytheMPIrequirements, Refine(P,S)(Lines21&26)re-
fines the models of receive operations by imposing the completes-
before requirements [83] as follows:
•Ifareceiveoperationhasmultiplematchedsendoperationsfrom
the same process, it should match the earlier issued one. This is
ensured by checking the emptiness of the dependent channels.
•Thereceiveoperationsinthesameprocessshouldbematched
w.r.t.their issue order if they receive messages from the sameprocess, except the conditional completes-before pattern [83]. We
use one-sized channel actions to model these requirements.
Wemodela Waitoperationifitcorrespondstoan IRecvoper-
ation (Line 30), because ISendoperations complete immediately
undertheassumptionofinfinitesystembuffer. Waitoperationsare
modeledbythesynchronizationinparallelprocesses. GenerateEvent
generates a new synchronization event ewfor each Waitopera-
tion(Line31).Then, ewisproducedafterthecorrespondingnon-
blocking operation is completed (Line 28). The synchronization on
ewensures that a Waitoperation blocks until the corresponding
non-blocking operation is completed.
We use the example in Figure 5 for a demonstration. After ex-
ploringa violation-free path,therecordedoperationsequencesare
Seq0=/angbracketleftISend(1,req 1),Barrier ,Wait(req 1)/angbracketright,Seq1=/angbracketleftIRecv(*,req 2),
Barrier ,Wait(req 2)/angbracketright,Seq2=/angbracketleftBarrier ,ISend(1,req 3),Wait(req 3)/angbracketright.W e
first scan Seq0in reverse. Note that we don’t model Wait(req 1),
becauseitcorrespondsto ISend.Wecreateasynchronizationevent
Bformodeling Barrier (Lines16&17).Forthe ISend(1,req 1),we
modelitbywritinganelement atoaone-sizedchannel chan1,and
use prefix operationto compose its modelwith B(Lines 12-14). In
this way, we generate CSP process chan1!a→B /fatsemiskip(denoted by
CP0) for Proc0. Similarly, we model Proc2byB/fatsemichan2!b→skip
(denoted by CP2), wherechan2is also a one-sized channel and bis
achannelelement.For Proc1,wegenerateasingleeventprocess ew
to model Wait(req 2), because it corresponds to IRecv(Lines 30-
32). For IRecv(*,req 2), we first compute the matched channels
using SMO(Line25),and StaticMatchedChannel( opj,S)contains
bothchan1andchan2.Then,wegeneratethefollowingCSPprocess
((chan1?a→skip/squarechan2?b→skip) /fatsemiew)/bardbl
{ew}(B/fatsemiew/fatsemiskip)
(denoted by CP1) for Proc1. Finally, we compose the CSP processes
using the parallel operator to form the CSP model (Line 38), i.e.,
CP0/bardbl
{B}CP1/bardbl
{B}CP2.
CSP modeling supports the case where communications depend
onmessagecontents.MPI-SVtrackstheinfluenceofamessagedur-
ing symbolic execution. Whendetecting that the message content
influences the communications, MPI-SV symbolizes the content
on-the-fly.Wespeciallyhandlethewidelyused master-slave pattern
fordynamicloadbalancing[ 32].Thebasicideaistousearecursive
CSP process to model each slave process and a conditional state-
ment for master process to model the communication behaviorsof different matchings. We verified five dynamic load balancing
MPIprogramsinourexperiments( cf.Section5.4).Thedetailsfor
supportingmaster-slavepatternisinthesupplementarydocument.
4.3 Soundness and Completeness
In the following, we show that the CSP modeling is soundand
complete.Suppose GenerateCSP( S)generatestheCSPprocess CSP s.
Here,soundness meansthat CSP smodelsallthepossiblebehaviors
by changing the matchings or interleavings of the communication
operationsalongthepathto S,andcompleteness meansthateach
tracein CSP srepresentsarealbehaviorthatcanbederivedfrom S
bychangingthematchingsorinterleavingsofthecommunications.
Sincewecompute SMO(op,S)bystaticallymatchingtheargu-
ments of the recorded operations, SMO(op,S)may contain some
1254falsematchings.Calculatingthepreciselymatchedoperationsof op
isNP-complete[ 23],andwesupposesuchanidealmethodexists.
We use CSP staticandCSP idealto denote the generated models
using SMO(op,S)and theideal method, respectively.The follow-
ing theorems ensure the equivalence of the two models under the
stable-failuresemantics[ 70]ofCSPand CSP static’sconsistencyto
theMPIsemantics,whichimplythesoundnessandcompleteness
of our CSP modeling method. Let T(P)denote the trace set [ 70]o f
CSP process P, andF(P)denote the failure set of CSP process P.
Each element in F(P)is(s,X), wheres∈T(P)is a trace, and Xis
the set of events Prefuses to perform after s.
Theorem 4.1. F(CSP static)=F(CSP ideal).
Proof. We only give the skeleton of the proof. We first prove
T(CSP static)=T(CSP ideal)
based on which we can prove F(CSP static)=F(CSP ideal). The
main idea of proving these two equivalence relations is to use
contradiction for proving the subset relations. We only give the
proofofT(CSP static)⊆T( CSP ideal);theothersubsetrelations
can be proved in a similar way.
Supposethereisatrace t=/angbracketlefte1, ...,en/angbracketrightsuchthatt∈T( CSP static)
butt/nelementT(CSP ideal). The only difference between CSP staticand
CSP idealis that CSP staticintroduces more channel read oper-
ations during the modeling of receive operations. Hence, theremustexistareadoperationofanextrachannelin
t.Supposethe
first extra read is ek=ce?x, where 1 ≤k≤n. Therefore, cecan-
notbereadin CSP idealwhenthematchingofthecorresponding
receive operation starts, but ceis not empty at ekinCSP static.
Despite of the size of ce, there must exist a write operation ce!yin
/angbracketlefte1, ...,ek−1/angbracketright.Because /angbracketlefte1, ...,ek−1/angbracketrightisalsoavalidtracein CSP ideal,
it means ceis not empty in CSP idealatek, which contradicts
with the assumption that cecannot be read in CSP ideal. Hence,
T(CSP static)⊆T( CSP ideal)holds. /square
Theorem 4.2. CSP staticis consistent with the MPI semantics.
Theproof’smainideaistoprovethat CSP idealisequaltothe
model defined by the formal MPI semantics [ 91]w.r.t.the failure
divergencesemantics.Then,basedonTheorem4.1,wecanprove
that CSP staticis consistent with the MPI semantics. Please refer
to [91] for the detailed proofs for these two theorems.
5 EXPERIMENTAL EVALUATION
Inthissection,wefirstintroducetheimplementationofMPI-SV,
thendescribestheresearchquestionsandtheexperimentalsetup.
Finally, we give experimental results.
5.1 Implementation
WehaveimplementedMPI-SVbasedonCloud9[ 10],whichisbuilt
uponKLEE[ 12],andenhancesKLEEwithbettersupportforPOSIX
environmentandparallelsymbolicexecution.WeleverageCloud9’s
supportformulti-threadedprograms.Weuseamulti-threadedli-
brary for MPI, called AzequiaMPI [ 69], as the MPI environment
modelforsymbolicexecution.MPI-SVcontainsthreemainmodules:
programpreprocessing,symbolicexecution,andmodelchecking.
The program preprocessing module generates the input for sym-
bolicexecution.WeuseClangtocompileanMPIprogramtoLLVMTable 1: The programs in the experiments.
Program LOCBrief Description
DTG 90Dependence transition group
Matmat 105Matrix multiplication
Integrate 181Integral computing
Diffusion2d 197Simulation of diffusion equation
Gauss_elim 341Gaussian elimination
Heat 613Heat equation solver
Mandelbrot 268Mandelbrot set drawing
Sorting 218Array sorting
Image_manip 360Image manipulation
DepSolver 8988Multimaterial electrostatic solver
Kfray 12728KF-Ray parallel raytracer
ClustalW 23265Multiple sequence alignment
Total 4735412 open source programs
bytecode,which isthen linkedwith thepre-compiled MPIlibrary
AzequiaMPI.Thesymbolicexecutionmoduleisinchargeofpath
explorationandpropertychecking.Thethirdmoduleutilizesthe
state-of-the-art CSP model checker PAT [ 80] to verify CSP models,
and uses the output of PAT to boost the symbolic executor.
5.2 Research Questions
We conducted experiments to answer the following questions:
•Effectiveness:CanMPI-SVverifyreal-worldMPIprogramseffec-
tively? How effective is MPI-SV when compared to the existing
state-of-the-art tools?
•Efficiency:HowefficientisMPI-SVwhenverifyingreal-world
MPI programs? How efficient is MPI-SV when compared to the
pure symbolic execution?
•Verifiableproperties:CanMPI-SVverifypropertiesotherthan
deadlock freedom?
5.3 Setup
Table1liststheprogramsanalyzedinourexperiments.Allthepro-
gramsarereal-worldopensourceMPIprograms. DTGisatesting
program from [ 82].Matmat,Integrate andDiffusion2d come
from the FEVS benchmark suite [ 76].Matmatis used for matrix
multiplication, Integrate calculatesthe integralsoftrigonometric
functions,and Diffusion2d isaparallelsolverfortwo-dimensional
diffusionequation. Gauss_elim isanMPIimplementationforgauss-
ian elimination used in [ 88].Heatis a parallel solver for heat equa-
tion used in [ 61].Mandelbrot ,Sorting andImage_manip come
from github. Mandelbrot parallel draws the mandelbrot set for a
bitmap, Sorting usesbubblesorttosortamulti-dimensionalarray,
andImage_manip isanMPIprogramfor imagemanipulations, e.g.,
shifting, rotating and scaling. The remaining three programs are
largeparallelapplications. Depsolver isaparallelmulti-material
3D electrostatic solver, Kfrayis a ray tracing program creating re-
alistic images, and ClustalW is a tool for aligning gene sequences.
To evaluate MPI-SV further, we mutate [ 46] the programs by
rewritingarandomlyselectedreceiveusingtworules:(1)replace
Recv(i)withif(x>a){Recv(i)}else{Recv(*)};(2)replace Recv(*)
withif(x>a){Recv(*)}else{Recv(j)}.Herexisaninputvariable, a
1255isarandomvalue,and jisgeneratedrandomlyfromthescopeofthe
process identifier. The mutations for IRecv(i,r) andIRecv(*,r)
aresimilar.Rule1istoimproveprogramperformanceandsimplify
programming, while rule 2 is to make the communication more
deterministic.Sincecommunicationstendtodependoninputsin
complexapplications,suchasthelastthreeprogramsinTable1,we
also introduce input related conditions. For each program, we gen-
eratefivemutantsifpossible,orgenerateasmanyasthenumberof
receives. We don’t mutate the programs using master-slave pattern
[32],i.e.,MatmatandSorting, and only mutate the static schedul-
ing versions of programs Integrate, Mandelbrot, and Kfray.
Baselines. Weusepuresymbolicexecutionasthefirstbase-
line because: (1) none of the state-of-the-art symbolic execution
basedverificationtoolscananalyzenon-blockingMPIprograms,
e.g.,C IV L[57,75]; (2) MPI-SPIN [ 74] can support input coverage
and non-blocking operations, but it requires building models of
theprogramsmanually; and(3)other automatedtools thatsupport
non-blockingoperations,suchasMOPPER[ 23]andISP[ 83],can
only verify programs under given inputs. MPI-SV aims at cover-
ing both the input space and non-determinism automatically. To
compare with pure symbolic execution, we run MPI-SV under two
configurations: (1) Symbolic execution, i.e., applying only symbolic
execution for path exploration, and (2) Our approach, i.e., using
model checking based boosting. Most of the programs run with6, 8, and 10 processes, respectively.
DTGandMatmatcan only be
run with 5 and 4 processes, respectively. For Diffusion and the
programsusingthe master-slave pattern,weonlyrunthemwith
4 and 6 processes due to the huge path space. We use MPI-SV
to verify deadlock freedom of MPI programs and also evaluate 2non-reachability properties for
Integrate andMandelbrot . The
timeout is one hour. There are three possible verification results:finding a violation, no violation, or timeout. We carry out all the
tasksonanIntelXeon-basedServerwith64Gmemoryand82.5GHz
cores running a Ubuntu 14.04 OS. We ran each verification task
threetimesandusetheaverageresultstoalleviatetheexperimental
errors. To evaluate MPI-SV’s effectiveness further, we also directly
compare MPI-SV with CIVL [ 57,75] and MPI-SPIN [ 74]. Note that,
since MPI-SPIN needs manual modeling, we only use MPI-SV to
verify MPI-SPIN’s C benchmarks w.r.t.deadlock freedom.
5.4 Experimental Results
Table2liststheresultsforevaluatingMPI-SVagainstpuresymbolic
execution. The first column shows program names, and #Procsis
thenumberofrunningprocesses. Tspecifieswhethertheanalyzed
program is mutated, where odenotes the original program, and mi
represents amutant. A taskcomprises aprogram and thenumber
of running processes. We label the programs using master-slave
patternwithsuperscript“*”.Column Deadlock indicateswhethera
taskisdeadlockfree,where0,1,and-1denote nodeadlock ,deadlock
andunknown, respectively. We use unknown for the case that both
configurations fail to complete the task. Columns Time(s)and
#Iterations showtheverificationtimeandthenumberofexplored
paths,respectively,wheretostandsfortimeout.Theresultswhere
Our approach performs better is in gray background.
For the 111 verification tasks, MPI-SV completes 100 tasks (90%)
within one hour, whereas 61 tasks (55%) for Symbolic execution.0 5 10 15 20 25 30 35 40 45 50 55 60050100
Verification time thresholds# Completed verification tasksSymbolic execution
Our approach
Figure 7: Completed tasks under a time threshold.
Our approach detectsdeadlocksin48tasks,whilethenumberof
Symbolic execution is44.Wemanuallyconfirmedthatthedetected
deadlocksarereal.Forthe48taskshavingdeadlocks,MPI-SVon
average offers a 5x speedups for detecting deadlocks. On the other
hand, Our approach canverifydeadlockfreedomfor52tasks,while
only 17 tasks for Symbolic execution. MPI-SV achieves an average
19x speedups. Besides, compared with Symbolic execution, Our
approach requires fewer paths to detect the deadlocks (1/55 on
average) and complete the path exploration (1/205 on average).
These results demonstrate MPI-SV’s effectiveness and efficiency.
Figure 7 shows the efficiency of verification for the two configu-
rations.TheX-axisvariesthetimethresholdfrom5minutestoonehour,whiletheY-axisisthenumberofcompletedverificationtasks.
Our approach cancompletemoretasksthan Symbolic execution
under the same time threshold, demonstrating MPI-SV’s efficiency.
Inaddition, Our approach cancomplete96 (96%)tasksin5minutes,
which also demonstrates MPI-SV’s effectiveness.
Forsometasks, e.g.,Kfray,MPI-SVdoesnotoutperform Sym-
bolic execution.Thereasonsinclude:(a)thepathscontainhundredsofnon-wildcardoperations,andthecorrespondingCSPmodelsare
huge, and thus time-consuming to model check; (b) the number of
wildcard receives or their possible matchings is very small, and as
a result, only few paths are pruned.
Comparison with CIVL. CIVLusessymbolicexecutiontobuild
amodelforthewholeprogramandperformsmodelcheckingonthe
model. In contrast, MPI-SV adopts symbolic execution to generate
path-levelverifiable models.CIVLdoesnotsupportnon-blocking
operations. We applied CIVL on our evaluation subjects. It onlysuccessfully analyzed
DTG.Diffusion2d could be analyzed after
removingunsupportedexternalcalls.MPI-SVandCIVLhadsimilarperformance on these two programs. CIVL failed on all the remain-
ing programs due to compilation failures or lack of support for
non-blockingoperations.Incontrast,MPI-SVsuccessfullyanalyzed
99 of the 140 programs in CIVL’s latest benchmarks. The failedones are small API test programs for the APIs that MPI-SV does
notsupport.Forthereal-worldprogram floydthatbothMPI-SV
andCIVLcananalyze,MPI-SVverifieditsdeadlock-freedomunder
4 processes in 3 minutes, while CIVL timed out after 30 minutes.
The results indicate the benefits of MPI-SV’s path-level modeling.
Comparison with MPI-SPIN. MPI-SPIN relies on manual mod-
eling of MPI programs. Inconsistencies may happen between an
1256Table 2: Experimental results.
Program (#Procs) TDeadlockTime(s) #Iterations
Symbolic execution Our approach Symbolic execution Our approach
DTG(5)o 0 10.12 9.02 3 1
m10 13.69 9.50 10 2
m21 10.02 8.93 4 2
m31 10.21 9.49 4 2
m41 10.08 9.19 4 2
m51 9.04 9.29 2 2
Matmat∗(4) o 0 36.94 10.43 54 1
Integrate (6/8/10)o0/0/0 78.17/to/to 8.87/10 .45/44 .00 120/3912/3162 1/1/1
m10/0/-1 to/to/to 49.94/to/to 4773/3712/3206 32/128/79
m21/1/1 9.35/9 .83/9 .94 9.39/10 .76/44 .09 2/2/2 2/2/2
Integrate∗(4/6) o0/0 24.18/123 .55 9.39/32 .03 27/125 1/1
Diffusion2d (4/6)o0/0 106.86/to 9.84/13.19 90/2041 1/1
m10/1 110.25/11 .95 10.18/13.81 90/2 1/2
m20/1 3236 .02/12 .66 17.05/14.38 5850/3 16/2
m30/0 to/to 19.26/199 .95 5590/4923 16/64
m41/1 11.35/11.52 11.14/14.22 3/2 2/2
m51/0 10.98/to 10.85/13 .44 2/1991 2/1
Gauss_elim (6/8/10)o0/0/0 to/to/to 13.47/15.12/87.45 2756/2055/1662 1/1/1
m11/1/1 155.40/to/to 14.31/16 .99/88 .79 121/2131/559 2/2/2
Heat(6/8/10)o1/1/1 17.31/17 .99/20 .5116.75/19.27/22 .75 2/2/2 1/1/1
m11/1/1 17.33/18 .21/20 .7817.03/19.75/23 .16 2/2/2 1/1/1
m21/1/1 18.35/18 .19/20 .7416.36/19.53/23 .07 2/2/2 1/1/1
m31/1/1 19.64/20 .21/23 .0816.36/19 .72/22 .95 3/3/3 1/1/1
m41/1/1 22.9/24 .73/27 .7816.4/19 .69/22 .90 9/9/9 1/1/1
m51/1/1 24.28/28 .57/32 .6716.61/19 .59/22 .42 7/7/7 1/1/1
Mandelbrot (6/8/10)o0/0/-1 to/to/to 117.68/831.87/to 500/491/447 9/9/9
m1-1/-1/-1 to/to/to to/to/to 1037/1621/1459 173/227/246
m2-1/-1/-1 to/to/to to/to/to 1093/1032/916 178/136/90
m31/1/1 10.71/11 .17/11 .9210.84/11 .68/13 .5 2/2/2 2/2/2
Mandelbort∗(4/6) o0/0 68.09/270 .65 12.65/13 .21 72/240 2/2
Sorting∗(4/6) o0/0 to/to 19.18/46 .19 584/519 1/1
Image_mani (6/8/10)o0/0/0 97.69/118 .72/141 .8718.68/23 .84/27 .89 96/96/96 4/4/4
m11/1/1 12.92/15 .80/15 .5914.15/14.53/16.86 2/2/2 2/2/2
DepSolver (6/8/10) o0/0/0 94.17/116 .5/148 .3897.19/123 .36/151 .83 4/4/4 4/4/4
Kfray(6/8/10)o0/0/0 to/to/to 51.59/68 .25/226 .961054/981/1146 1/1/1
m11/1/1 52.15/53 .50/46 .8353.14/69 .58/229 .97 2/2/2 2/2/2
m2-1/-1/-1 to/to/to to/to/to 1603/1583/1374 239/137/21
m31/1/1 51.31/43 .34/48 .3350.40/71.15/230 .18 2/2/2 2/2/2
Kfray∗(4/6) o0/0 to/to 53.44/282 .46 1301/1575 1/1
Clustalw (6/8/10)o0/0/0 to/to/to 47.28/79 .38/238 .371234/1105/1162 1/1/1
m10/0/0 to/to/to 47.94/80 .10/266 .161365/1127/982 1/1/1
m20/0/0 to/to/to 47.71/90 .32/266 .081241/1223/915 1/1/1
m31/1/1 895.63/to/to 149.71/1083 .95/301 .99175/1342/866 5/17/2
m40/0/0 to/to/to 47.49/79 .94/234 .991347/1452/993 1/1/1
m50/0/0 to/to/to 47.75/80 .33/223 .771353/1289/1153 1/1/1
MPIprogramanditsmodel.Althoughprototypesexistfortrans-
latingCtoPromela[ 45],theyareimpracticalforreal-worldMPI
programs. MPI-SPIN’s state space reduction treats communication
channelsasrendezvousones;thus,thereductioncannothandletheprogramswithwildcardreceives.MPI-SVleveragesmodelchecking
to prune redundant paths caused by wildcard receives. We applied
MPI-SVonMPI-SPIN’s17Cbenchmarkstoverifydeadlockfree-
dom,andMPI-SVsuccessfullyanalyzed15automatically,indicatingtheeffectiveness.Fortheremainingtwoprograms, i.e.,BlobFlow
andMonte,MPI-SVcannotanalyzethemduetothelackofsupport
for APIs. For the real-world program gausselim , MPI-SPIN needs
171s to verify that the model is deadlock-free under 5 processes,
whileMPI-SVonlyneeds27stoverifytheprogramautomatically.If
thenumberoftheprocessesis8,MPI-SPINtimedoutin30minutes,
but MPI-SV used 66s to complete verification.
1257Temporal properties. Wespecifytwotemporalsafetyproperties
φ1andφ2forIntegrate andMandelbrot ,respectively,where φ1
requires process one cannot receive a message before process two,
andφ2requires process one cannot send a message before process
two. Both φ1andφ2can be represented by an LTL formula ! aUb,
which requires event acannot happen before event b. We verify
Integrate andMandelbrot under 6 processes. The verification
resultsshowthatMPI-SVdetectstheviolationsof φ1andφ2,while
pure symbolic execution fails to detect violations.
Runtime bugs. MPI-SVcanalsodetectlocalruntimebugs.Dur-
ing the experiments, MPI-SV finds 5 unknown memory access out-
of-bound bugs: 4 in DepSolver a n d1i nClustalW.
6 RELATED WORK
Dynamic analyses are widely used for analyzing MPI programs.
Debugging or testing tools [ 1,36,50,51,60,71,87] have better
feasibility and scalability but depend on specific inputs and run-
ning schedules. Dynamic verification techniques, e.g., ISP [83] and
DAMPI [84], run MPI programs multiple times to cover the sched-
ulesunderthesameinputs.Böhmetal . [3]proposeastate-space
reductionframeworkfortheMPIprogramwith non-deterministic
synchronization.Theseapproachescandetectthebugsdepending
on specific matchings of wildcard operations, but may still miss
inputs related bugs. MPI-SV supports both input and schedule cov-
erages, and a larger scope of verifiable properties. MOPPER [ 23]
encodesthedeadlockdetectionproblemunderconcreteinputsin
a SAT equation. Similarly, Huang and Mercer [41]use an SMT
formula to reason about a trace of an MPI program for deadlock
detection.However,theSMTencodingisspecificforthezero-buffer
mode. Khanna et al . [47]combines dynamic and symbolic analy-
ses to verify multi-path MPI programs. Compared with these path
reasoning work in dynamic verification, MPI-SV ensures input
space coverage and can verify more properties, i.e., safety and live-
nesspropertiesinLTL.Besides,MPI-SVemploysCSPtoenablea
moreexpressivemodeling, e.g.,supportingconditionalcompletes-
before [83] and master-slave pattern [32].
ForstaticmethodsofanalyzingMPIprogram, MPI-SPIN [73,74]
manuallymodelsMPIprogramsinPromela[ 38],andverifiesthe
modelw.r.t.LTL properties [ 58] by SPIN [ 37](cf.Section 5.4 for
empiricalcomparison).MPI-SPINcanalsoverifytheconsistency
betweenanMPIprogramandasequentialprogram,whichisnot
supported by MPI-SV. Bronevetsky [9]proposes parallel control
flowgraph(pCFG)forMPIprogramstocapturetheinteractionsbe-tweenarbitraryprocesses.ButthestaticanalysisusingpCFGishard
tobeautomated.ParTypes[ 55]usestypecheckinganddeductive
verification to verify MPI programs against a protocol. ParTypes’s
verification results are sound but incomplete, and independent
with the number of processes. ParTypes does not support non-
deterministicornon-blockingMPIoperations.MPI-Checker[ 22]is
astaticanalysis toolbuiltonClang StaticAnalyzer[ 15],andonly
supportsintraproceduralanalysisoflocalpropertiessuchasdouble
non-blocking and missing wait. Botbol et al . [5]abstract an MPI
programtosymbolictransducers,andobtainthereachabilityset
basedonabstractinterpretation[ 18],whichonlysupportsblocking
MPI programs and may generate false positives. COMPI [ 53,54]
usesconcolictesting[ 27,72]todetectassertionorruntimeerrorsinMPI applications. Ye et al .[89]employs partial symbolic execution
[68] to detect MPI usage anomalies. However, these two symbolic
execution-based bug detection methods do not support the non-determinism caused by wildcard operations. Luo and Siegel
[56]
propose a preliminary deductive method for verifying the numericproperties of MPI programs in an unbounded number of processes.
However, this method still needs manually provided verification
conditions to prove MPI programs.
MPI-SVisrelatedtotheexistingworkonsymbolicexecution[ 48],
whichhasbeenadvancedsignificantlyduringthelastdecade[ 10,
12,27,28,66,72,81,86,93]. Many methods have been proposed
to prune paths during symbolic execution [ 4,19,34,43,92]. The
basic idea is to use the techniques such as slicing [ 44] and interpo-
lation[59]tosafelyprunethepaths.Comparedwiththem,MPI-SV
only prunes the paths of the same path constraint but different
messagematchingsoroperationinterleavings.MPI-SVisalsore-
lated to the work of automatically extracting session types [ 63]o r
behavioral types [ 52] for Go programs and verifying the extracted
type models. These methods extract over-approximation models
fromGoprograms,andhencearesoundbutincomplete.Compared
with them, MPI-SV extracts path-level models for verification. Fur-
thermore,there existswork ofcombining symbolicexecution and
modelchecking[ 20,65,79].YOGI[65]andAbstraction-drivencon-
colic testing [ 20] combine dynamic symbolic execution [ 27,72]
with counterexample-guided abstraction refinement (CEGAR) [ 16].
MPI-SVfocusesonparallelprograms,andtheverifiedmodelsare
path-level. MPI-SV is also related to the work of unbounded ver-ification for parallel programs [
2,6,7,85]. Compared with them,
MPI-SV is a bounded verification tool and supports the verifica-tion of LTL properties. Besides, MPI-SV is related to the exist-
ingworkoftestingandverificationofshared-memoryprograms
[13,14,21,34,35,39,40,42,49,62,90]. Compared with them, MPI-
SV concentrates on message-passing programs. Utilizing the ideas
in these work for analyzing MPI programs is interesting and left to
the future work.
7 CONCLUSION
We havepresented MPI-SV forverifying MPI programswith both
non-blocking and non-deterministic operations. By synergistically
combiningsymbolicexecutionandmodelchecking,MPI-SVpro-
vides a general framework for verifying MPI programs. We have
implemented MPI-SV and extensively evaluated it on real-world
MPIprograms.Theexperimentalresultsarepromisingdemonstrate
MPI-SV’seffectivenessandefficiency.Thefutureworkliesinsev-
eraldirections:(1)enhanceMPI-SVtosupportmoreMPIoperations,
(2)investigatetheautomatedperformancetuningofMPIprograms
based on MPI-SV, (3) apply our synergistic framework to other
message-passing programs.
ACKNOWLEDGEMENT
This research was supported by National Key R&D Program of
China (No. 2017YFB1001802) and NSFC Program (No. 61902409,
61632015, 61690203, and 61532007).
REFERENCES
[1] Allinea. 2002. Allinea DDT. http://www.allinea.com/products/ddt/. (2002).
1258[2]AlexanderBakst,KlausvonGleissenthall,RamiGökhanKici,andRanjitJhala.
2017. Verifying distributed programs via canonical sequentialization. PACMPL1,
OOPSLA (2017), 110:1–110:27.
[3]StanislavBöhm,OndrejMeca,andPetrJancar.2016. State-SpaceReductionof
Non-deterministically Synchronizing Systems Applicable to Deadlock Detection
in MPI. In FM. 102–118.
[4]Peter Boonstoppel, CristianCadar,and Dawson Engler. 2008. RWset: attacking
path explosion in constraint-based test generation. In TACAS. 351–366.
[5]Vincent Botbol, Emmanuel Chailloux, and Tristan Le Gall. 2017. Static Analysis
of Communicating Processes Using Symbolic Transducers. In VMCAI. 73–90.
[6]Ahmed Bouajjani and Michael Emmi. 2012. Analysis of recursively parallel pro-
grams.In Proceedingsofthe39thACMSIGPLAN-SIGACTSymposiumonPrinciples
of Programming Languages, POPL 2012, Philadelphia, Pennsylvania, USA, January
22-28, 2012. 203–214.
[7]AhmedBouajjani,ConstantinEnea,KailiangJi,andShazQadeer.2018. Onthe
Completeness of Verifying Message Passing Programs Under Bounded Asyn-
chrony. In Computer Aided Verification - 30th International Conference, CAV 2018,
Held as Part of the Federated Logic Conference, FloC2018, Oxford, UK, July 14-17,
2018, Proceedings, Part II. 372–391.
[8]DanielBrandandPitroZafiropulo.1983.Oncommunicatingfinite-statemachines.
J. ACM(1983), 323–342.
[9]GregBronevetsky.2009. Communication-sensitivestaticdataflowforparallel
message passing applications. In CGO. 1–12.
[10]StefanBucur,VladUreche,CristianZamfir,andGeorgeCandea.2011. Parallel
symbolic execution for automated real-world software testing. In EuroSYS. 183–
198.
[11]Rajkumar Buyya andothers. 1999. Highperformance clustercomputing: archi-
tectures and systems. Prentice Hall (1999), 999.
[12]C. Cadar, D. Dunbar, and D. Engler. 2008. KLEE: Unassisted and automatic
generation of high-coverage tests for complex systems programs. In OSDI. 209–
224.
[13]Sagar Chaki, Edmund M. Clarke, Alex Groce, Joël Ouaknine, Ofer Strichman,
and Karen Yorav. 2004. Efficient Verification of Sequential and Concurrent C
Programs. Formal Methods in System Design 25, 2-3 (2004), 129–166.
[14]Alessandro Cimatti, Iman Narasamdya, and Marco Roveri. 2011. Boosting Lazy
AbstractionforSystemCwithPartialOrderReduction.In ToolsandAlgorithmsfor
the Construction and Analysis of Systems -17th International Conference, TACAS
2011, Held as Part of the Joint European Conferences on Theory and Practice of
Software, ETAPS 2011, Saarbrücken, Germany, March 26-April 3, 2011. Proceedings.
341–356.
[15] Clang. 2016. Clang Static Analyzer. http://clang-analyzer.llvm.org. (2016).[16]
Edmund Clarke, Orna Grumberg, Somesh Jha, Yuan Lu, and Helmut Veith. 2000.
Counterexample-guided abstraction refinement. In CAV. 154–169.
[17]EdmundMClarke,OrnaGrumberg,andDoronPeled.1999. Modelchecking.M I T
press.
[18]Patrick Cousot and Radhia Cousot. 1977. Abstract Interpretation: A Unified
LatticeModelforStaticAnalysisofProgramsbyConstructionorApproximation
of Fixpoints. In POPL. 238–252.
[19]Heming Cui, Gang Hu, Jingyue Wu, and Junfeng Yang. 2013. Verifying systems
rules using rule-directed symbolic execution. In ASPLOS. 329–342.
[20]PrzemysławDaca,AshutoshGupta,andThomasAHenzinger.2016. Abstraction-
driven Concolic Testing. In VMCAI. 328–347.
[21]Brian Demsky and Patrick Lam.2015. SATCheck: SAT-directed stateless model
checking for SC and TSO. In Proceedings of the 2015 ACM SIGPLAN International
ConferenceonObject-OrientedProgramming,Systems,Languages,andApplications,
OOPSLA 2015, part of SPLASH 2015, Pittsburgh, PA, USA, October 25-30, 2015. 20–
36.
[22]AlexanderDroste,MichaelKuhn,andThomasLudwig.2015. MPI-checker:static
analysis for MPI. In LLVM-HPC. 3:1–3:10.
[23]VojtěchForejt,DanielKroening,GaneshNarayanaswamy,andSubodhSharma.
2014. Precise predictive analysis for discovering communication deadlocks in
MPI programs. In FM. 263–278.
[24]MPI Forum. 2012. MPI: A Message-Passing Interface Standard Version 3.0. http:
//mpi-forum.org. (2012).
[25]XianjinFu,ZhenbangChen,YufengZhang,ChunHuang,WeiDong,andJiWang.
2015. MPISE: Symbolic Execution of MPI Programs. In HASE. 181–188.
[26]Edgar Gabriel, Graham E Fagg, George Bosilca, Thara Angskun, Jack J Dongarra,
JeffreyMSquyres,VishalSahay,PrabhanjanKambadur,BrianBarrett,Andrew
Lumsdaine, and others. 2004. Open MPI: Goals, concept, and design of a next
generation MPI implementation. In EuroMPI. 97–104.
[27]PatriceGodefroid,NilsKlarlund,andKoushikSen.2005. DART:directedauto-
mated random testing. In PLDI. 213–223.
[28]Patrice Godefroid, Michael Y. Levin, and David A. Molnar. 2008. Automated
Whitebox Fuzz Testing. In NDSS.
[29]Ganesh Gopalakrishnan, Paul D. Hovland, Costin Iancu, Sriram Krishnamoorthy,IgnacioLaguna,RichardA.Lethin,KoushikSen,StephenF.Siegel,andArmandoSolar-Lezama.2017.ReportoftheHPCCorrectnessSummitJan25-26,2017,Wash-
ington, DC. https://science.energy.gov/~/media/ascr/pdf/programdocuments/docs/2017/HPC_Correctness_Report.pdf. (2017).
[30]Ganesh Gopalakrishnan, Robert M. Kirby, Stephen F. Siegel, Rajeev Thakur,
WilliamGropp,EwingL.Lusk,BronisR.deSupinski,MartinSchulz,andGreg
Bronevetsky.2011. FormalanalysisofMPI-basedparallelprograms. Commun.
ACM(2011), 82–91.
[31]WilliamGropp.2002. MPICH2:AnewstartforMPIimplementations.In EuroMPI.
7–7.
[32]WilliamGropp,EwingLusk,andAnthonySkjellum.2014. UsingMPI:Portable
Parallel Programming with the Message-Passing Interface. The MIT Press.
[33]WilliamGropp,EwingLusk,andRajeevThakur.1999. UsingMPI-2:Advanced
features of the message-passing interface. MIT press.
[34]Shengjian Guo, Markus Kusano, Chao Wang, Zijiang Yang, and Aarti Gupta.
2015. Assertionguidedsymbolicexecutionofmultithreadedprograms.In FSE.
854–865.
[35]ShengjianGuo,MengWu,andChaoWang.2018. Adversarialsymbolicexecution
fordetectingconcurrency-relatedcachetimingleaks.In Proceedingsofthe2018
ACM Joint Meeting on European Software Engineering Conference and Symposium
on the Foundations of Software Engineering, ESEC/SIGSOFT FSE 2018, Lake Buena
Vista, FL, USA, November 04-09, 2018. 377–388.
[36]Tobias Hilbrich, Joachim Protze, Martin Schulz, Bronis R de Supinski, andMatthias S Müller. 2012. MPI runtime error detection with MUST: advances
in deadlock detection. In SC. 30.
[37]GerardJHolzmann.1997. ThemodelcheckerSPIN. IEEETransactionsonSoftware
Engineering (1997), 279–295.
[38]Gerard J. Holzmann. 2012. Promela manual pages. http://spinroot.com/spin/
Man/promela.html. (2012).
[39]Jeff Huang, Charles Zhang, and Julian Dolby. 2013. CLAP: recording local ex-ecutions to reproduce concurrency failures. In ACM SIGPLAN Conference on
ProgrammingLanguageDesignandImplementation,PLDI’13,Seattle,WA,USA,
June 16-19, 2013. 141–152.
[40]ShiyouHuangandJeffHuang.2016. MaximalcausalityreductionforTSOand
PSO.InProceedingsofthe2016ACMSIGPLANInternationalConferenceonObject-
Oriented Programming, Systems, Languages, and Applications, OOPSLA 2016, part
of SPLASH 2016, Amsterdam, The Netherlands, October 30 - November 4, 2016.
447–461.
[41]Yu Huang and Eric Mercer. 2015. Detecting MPI Zero Buffer Incompatibility by
SMT Encoding. In NFM. 219–233.
[42] Omar Inverso, Truc L. Nguyen, Bernd Fischer, Salvatore La Torre, and Gennaro
Parlato.2015. Lazy-CSeq:AContext-BoundedModelCheckingToolforMulti-
threaded C-Programs. In 30th IEEE/ACM International Conference on Automated
Software Engineering, ASE 2015, Lincoln, NE, USA, November 9-13, 2015 . 807–812.
[43]Joxan Jaffar, Vijayaraghavan Murali, and Jorge A Navas. 2013. Boosting concolic
testing via interpolation. In FSE. 48–58.
[44] Ranjit Jhala and Rupak Majumdar. 2005. Path slicing. In PLDI. 38–47.
[45]Ke Jiangand Bengt Jonsson.2009. Using SPIN tomodel check concurrentalgo-
rithms, using a translation from C to Promela. In MCC 2009. 67–69.
[46]RenéJust,DarioushJalali,LauraInozemtseva,MichaelDErnst,ReidHolmes,and
GordonFraser.2014. Aremutantsavalidsubstituteforrealfaultsinsoftware
testing?. In FSE. 654–665.
[47]Dhriti Khanna, Subodh Sharma, César Rodríguez, and Rahul Purandare. 2018.
Dynamic Symbolic Verification of MPI Programs. In FM.
[48]J.C. King. 1976. Symbolic execution and program testing. Commun. ACM (1976),
385–394.
[49]BernhardKragl,ShazQadeer,andThomasA.Henzinger.2018. Synchronizingthe
Asynchronous. In 29th International Conference on Concurrency Theory, CONCUR
2018, September 4-7, 2018, Beijing, China. 21:1–21:17.
[50]Bettina Krammer, Katrin Bidmon, Matthias S Müller, and Michael M Resch. 2004.
MARMOT: An MPI analysis and checking tool. Advances in Parallel Computing
(2004), 493–500.
[51]Ignacio Laguna, Dong H. Ahn, Bronis R. de Supinski, Todd Gamblin, Gregory L.
Lee,MartinSchulz,SaurabhBagchi,MilindKulkarni,BowenZhou,ZhezheChen,
and Feng Qin. 2015. Debugging high-performance computing applications at
massive scales. Commun. ACM 58, 9 (2015), 72–81.
[52]Julien Lange, Nicholas Ng, Bernardo Toninho, and Nobuko Yoshida. 2018. A
static verification framework for message passing in Go using behavioural types.
InProceedings of the 40th International Conference on Software Engineering, ICSE
2018, Gothenburg, Sweden, May 27 - June 03, 2018. 1137–1148.
[53]HongboLi,ZizhongChen,andRajivGupta.2019. EfficientConcolicTestingof
MPI Applications. In Proceedings of the 28th International Conference on Compiler
Construction (CC 2019). 193–204.
[54]Hongbo Li, Sihuan Li, Zachary Benavides, Zizhong Chen, and Rajiv Gupta. 2018.
COMPI:ConcolicTestingforMPIApplications.In 2018IEEEInternationalParallel
andDistributedProcessingSymposium,IPDPS2018,Vancouver,BC,Canada,May21-25, 2018. 865–874.
[55]
HugoA.López,EduardoR.B.Marques,FranciscoMartins,NicholasNg,César
Santos, Vasco Thudichum Vasconcelos, and Nobuko Yoshida. 2015. Protocol-
based verification of message-passing parallel programs. In OOPSLA. 280–298.
1259[56]Ziqing Luo and Stephen F. Siegel. 2018. Towards Deductive Verification of
Message-Passing Parallel Programs. In 2nd IEEE/ACM International Workshop on
SoftwareCorrectness forHPCApplications, CORRECTNESS@SC2018,Dallas, TX,
USA, November 12, 2018. 59–68.
[57]Ziqing Luo, Manchun Zheng, and Stephen F. Siegel. 2017. Verification of MPI
programs using CIVL. In EuroMPI. 6:1–6:11.
[58]ZoharMannaandAmirPnueli.1992. Thetemporallogicofreactiveandconcurrent
systems - specification. Springer.
[59]KennethL.McMillan.2005.ApplicationsofCraigInterpolantsinModelChecking.
InTACAS. 1–12.
[60]Subrata Mitra, Ignacio Laguna, Dong H. Ahn, Saurabh Bagchi, Martin Schulz,
andToddGamblin.2014. Accurateapplicationprogressanalysisforlarge-scale
parallel debugging. In ACM SIGPLAN Conference on Programming Language
Designand Implementation,PLDI ’14,Edinburgh,United Kingdom-June 09- 11,
2014. 193–203.
[61]MatthiasMüller,BronisdeSupinski,GaneshGopalakrishnan,TobiasHilbrich,and
David Lecomber. 2011. Dealing with MPI bugs at scale: Best practices, automatic
detection,debugging,andformalverification. http://sc11.supercomputing.org/
schedule/event_detail.php?evid=tut131, (2011).
[62]Madanlal Musuvathi, Shaz Qadeer, Thomas Ball, Gérard Basler, Pira-
manayagamArumugaNainar,andIulianNeamtiu.2008.FindingandReproducing
Heisenbugs in Concurrent Programs. In 8th USENIX Symposium on Operating
SystemsDesignandImplementation,OSDI2008,December8-10,2008,SanDiego,
California, USA, Proceedings. 267–280.
[63]NicholasNgandNobukoYoshida.2016. Staticdeadlockdetectionforconcurrent
go by global session graph synthesis. In Proceedings of the 25th International
ConferenceonCompilerConstruction,CC2016,Barcelona,Spain,March12-18,2016 .
174–184.
[64]Flemming Nielson, Hanne R Nielson, and Chris Hankin. 2015. Principles of
program analysis. Springer.
[65]AdityaVNori,SriramKRajamani,SaiDeepTetali,andAdityaVThakur.2009.
The YOGI Project: Software property checking via static analysis and testing. In
TACAS. 178–181.
[66]CorinaS.Pasareanu,PeterC.Mehlitz,DavidH.Bushnell,KarenGundy-Burlet,
MichaelR.Lowry,SuzettePerson,andMarkPape.2008. Combiningunit-level
symbolic execution and system-level concrete execution for testing NASA soft-
ware.InProceedingsoftheACM/SIGSOFTInternationalSymposiumonSoftware
Testing and Analysis, ISSTA 2008, Seattle, WA, USA, July 20-24, 2008. 15–26.
[67]WojciechPenczek,MaciejSzreter,RobGerth,andRuurdKuiper.2000. Improving
Partial Order Reductions for Universal Branching Time Properties. Fundam.
Inform.(2000), 245–267.
[68]David A. Ramos and Dawson R. Engler. 2015. Under-Constrained SymbolicExecution: Correctness Checking for Real Code. In SEC. USENIX Association,
49–64.
[69]JuanA.Rico-GallegoandJuanCarlosDíazMartín.2011. PerformanceEvaluation
of Thread-Based MPI in Shared Memory. In EuroMPI. 337–338.
[70] Bill Roscoe. 2005. The theory and practice of concurrency. Prentice-Hall.
[71]Victor Samofalov, V. Krukov, B. Kuhn, S. Zheltov, Alexander V. Konovalov, and J.
DeSouza.2005. AutomatedCorrectnessAnalysisofMPIProgramswithIntel(r)
Message Checker. In PARCO. 901–908.
[72]Koushik Sen, Darko Marinov, and Gul Agha. 2005. CUTE: a concolic unit testing
engineforC.In Proceedingsofthe10thEuropeanSoftwareEngineeringConference
heldjointlywith13thACMSIGSOFTInternationalSymposiumonFoundationsof
Software Engineering, 2005, Lisbon, Portugal, September 5-9, 2005. 263–272.
[73] Stephen F. Siegel. Model Checking Nonblocking MPI Programs. In VMCAI.
[74]Stephen F. Siegel. 2007. Verifying Parallel Programs with MPI-Spin. In PVM/MPI.
13–14.
[75]Stephen F. Siegel, Manchun Zheng, Ziqing Luo, Timothy K. Zirkel, Andre V.
Marianiello,JohnG.Edenhofner,MatthewB.Dwyer,andMichaelS.Rogers.2015.
CIVL: the concurrency intermediate verification language. In Proceedings of the
InternationalConferenceforHighPerformanceComputing,Networking,Storage
and Analysis, SC 2015, Austin, TX, USA, November 15-20, 2015. 61:1–61:12.
[76]StephenFSiegelandTimothyKZirkel.2011. FEVS:Afunctionalequivalence
verification suite for high-performance scientific computing. Mathematics in
Computer Science (2011), 427–435.
[77]StephenF.SiegelandTimothyK.Zirkel.2011. TASS:TheToolkitforAccurate
Scientific Software. Mathematics in Computer Science (2011), 395–426.
[78] Marc Snir. 1998. MPI–the Complete Reference: The MPI core. Vol. 1. MIT press.
[79]TingSu,ZhoulaiFu,GeguangPu,JifengHe,andZhendongSu.2015. Combining
symbolic execution and model checking for data flow testing. In ICSE. 654–665.
[80]Jun Sun, Yang Liu, Jin Song Dong, and Jun Pang. 2009. PAT: Towards flexible
verification under fairness. In CAV. 709–714.
[81]NikolaiTillmannandJonathandeHalleux.2008. Pex-WhiteBoxTestGeneration
for .NET. In TAP. 134–153.
[82]Sarvani Vakkalanka. 2010. Efficient dynamic verification algorithms for MPI
applications. Ph.D. Dissertation. The University of Utah.[83]Sarvani S. Vakkalanka, Ganesh Gopalakrishnan, and Robert M. Kirby. 2008. Dy-
namic Verification of MPI Programs with Reductions in Presence of Split Opera-
tions and Relaxed Orderings. In CAV. 66–79.
[84]Anh Vo, Sriram Aananthakrishnan, Ganesh Gopalakrishnan, Bronis R De Supin-
ski, Martin Schulz, and Greg Bronevetsky. 2010. A scalable and distributed
dynamic formal verifier for MPI programs. In SC. 1–10.
[85]Klaus von Gleissenthall, Rami Gökhan Kici, Alexander Bakst, Deian Stefan, and
RanjitJhala.2019. Pretendsynchrony:synchronousverificationofasynchronous
distributed programs. PACMPL 3, POPL (2019), 59:1–59:30.
[86]Xinyu Wang, Jun Sun, Zhenbang Chen, Peixin Zhang, Jingyi Wang, and Yun Lin.
2018. Towardsoptimalconcolictesting.In Proceedingsofthe40thInternational
Conference on Software Engineering, ICSE 2018, Gothenburg, Sweden, May 27 -
June 03, 2018. 291–302.
[87]Rogue Wave. 2009. TotalView Software. http://www.roguewave.com/products/
totalview. (2009).
[88]Ruini Xue, Xuezheng Liu, Ming Wu, Zhenyu Guo, Wenguang Chen, Weimin
Zheng,ZhengZhang,andGeoffreyVoelker.2009.MPIWiz:subgroupreproducible
replay of MPI applications. ACM Sigplan Notices (2009), 251–260.
[89]Fangke Ye, Jisheng Zhao, and Vivek Sarkar. 2018. Detecting MPI usage anom-
aliesviapartialprogramsymbolicexecution.In ProceedingsoftheInternational
ConferenceforHighPerformanceComputing,Networking,Storage,andAnalysis,
SC 2018, Dallas, TX, USA, November 11-16, 2018. 63:1–63:5.
[90]LiangzeYin,WeiDong,WanweiLiu,YunchouLi,andJiWang.2018. YOGAR-
CBMC:CBMCwithSchedulingConstraintBasedAbstractionRefinement-(Com-petitionContribution).In ToolsandAlgorithmsfortheConstructionandAnalysisof
Systems - 24th International Conference, TACAS 2018, Held as Part of the European
JointConferencesonTheoryandPracticeofSoftware,ETAPS2018,Thessaloniki,
Greece, April 14-20, 2018, Proceedings, Part II. 422–426.
[91]Hengbiao Yu, Zhenbang Chen, Xianjin Fu, Ji Wang, Zhendong Su, Jun Sun,
ChunHuang,andWeiDong.2020. CombiningSymbolicExecutionandModel
CheckingtoVerifyMPIPrograms. CoRRabs/1803.06300(2020). arXiv:1803.06300
http://arxiv.org/abs/1803.06300
[92]Hengbiao Yu, Zhenbang Chen, Ji Wang, Zhendong Su, and Wei Dong. 2018.
Symbolicverificationofregularproperties.In Proceedingsofthe40thInternational
Conference on Software Engineering, ICSE 2018, Gothenburg, Sweden, May 27 -
June 03, 2018. 871–881.
[93]Yufeng Zhang, Zhenbang Chen, Ji Wang, Wei Dong, and Zhiming Liu. 2015.
Regular property guided dynamic symbolic execution. In ICSE. IEEE Press, 643–
653.
1260