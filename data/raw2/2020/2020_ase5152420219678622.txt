DeepCV A: Automated Commit-level Vulnerability
Assessment with Deep Multi-task Learning
Triet Huynh Minh Le‚àó, David Hin‚àó‚Ä†, Roland Croft‚àó‚Ä†and M. Ali Babar‚àó‚Ä†
‚àóCREST - The Centre for Research on Engineering Software Technologies, The University of Adelaide, Australia
‚Ä†Cyber Security Cooperative Research Centre, Australia
{triet.h.le, david.hin, roland.croft, ali.babar}@adelaide.edu.au
Abstract ‚ÄîIt is increasingly suggested to identify Software
Vulnerabilities (SVs) in code commits to give early warnings
about potential security risks. However, there is a lack of effortto assess vulnerability-contributing commits right after they aredetected to provide timely information about the exploitability,impact and severity of SVs. Such information is important toplan and prioritize the mitigation for the identiÔ¨Åed SVs. Wepropose a novel Deep multi-task learning model, DeepCV A,to automate seven Commit-level Vulnerability Assessment taskssimultaneously based on Common Vulnerability Scoring System(CVSS) metrics. We conduct large-scale experiments on 1,229vulnerability-contributing commits containing 542 different SVsin 246 real-world software projects to evaluate the effectivenessand efÔ¨Åciency of our model. We show that DeepCV A is thebest-performing model with 38% to 59.8% higher MatthewsCorrelation CoefÔ¨Åcient than many supervised and unsupervisedbaseline models. DeepCV A also requires 6.3 times less trainingand validation time than seven cumulative assessment models,leading to signiÔ¨Åcantly less model maintenance cost as well. Over-all, DeepCV A presents the Ô¨Årst effective and efÔ¨Åcient solution toautomatically assess SVs early in software systems.
Index Terms‚ÄîSoftware vulnerability, Vulnerability assessment,
Deep learning, Multi-task learning, Mining software repositories,Software security
I. I NTRODUCTION
Software Vulnerabilities (SVs) are security weaknesses that
can make systems susceptible to cyber-attacks; thus, it is
critical to assess SVs [1]. SV assessment is a process ofdetermining characteristics of SVs such as attack vectors andimpacts to help practitioners prioritize remediation for ever-increasing SVs [2]. For example, SVs with simple exploitationand severe impacts likely require high Ô¨Åxing priority.
The expert-based Common Vulnerability Scoring System
(CVSS) [3] is a commonly used SV assessment framework.CVSS provides metrics to quantify the exploitability, impactand severity level of SVs. However, there is usually delay inthe manual process of assigning CVSS metrics to new SVsconducted by security experts [4]. Hence, there is an apparentneed for automation in assessing reported/detected SVs.
Existing techniques (e.g., [5], [6], [7], [8], [9]) to automate
bug/SV assessment have mainly operated on bug/SV reports,but these reports may be only available long after SVs ap-peared in practice. Our motivating analysis revealed that therewere 1,165 days, on average, from when an SV was injectedin a codebase until its report was published on NationalVulnerability Database (NVD) [10]. Our analysis agreed withthe Ô¨Åndings of Meneely et al. [11]. To tackle late-detectedbugs/SVs, recently, Just-in-Time (commit-level) approaches(e.g., [12], [13], [14], [15]) have been proposed to rely onthe changes in code commits to detect bugs/SVs right afterbugs/SVs are added to a codebase. Such early commit-levelSV detection can also help reduce the delay in SV assessment.
Even when SVs are detected early in commits, we argue
that existing automated techniques relying on bug/SV reportsstill struggle to perform just-in-time SV assessment. Firstly,
there are signiÔ¨Åcant delays in the availability of SV reports,which render the existing SV assessment techniques unusable.SpeciÔ¨Åcally, SV reports on NVD generally only appear sevendays after the SVs were found/disclosed [16]. Some of thedetected SVs may not even be reported on NVD [17], e.g., be-cause of no disclosure policy. User-submitted bug/SV reportsare also only available post-release and more than 82% of thereports are Ô¨Åled more than 30 days after developers detectedthe bugs/SVs [18]. Secondly, code review can provide fasterSV assessment, but there are still unavoidable delays (fromseveral hours to even days) [19]. Delays usually come fromcode reviewers‚Äô late responses and manual analyses dependingon the reviewers‚Äô workload and code change complexity [20].Thirdly, it is non-trivial to automatically generate bug/SVreports from vulnerable commits as it would require non-codeartifacts (e.g., stack traces or program crashes) that are mostlyunavailable when commits are submitted [5], [21].
Performing commit-level SV assessment provides a possi-
bility to inform committers about the exploitability, impactand severity of SVs in code changes and prioritize Ô¨Åxingearlier without waiting for SV reports. However, to the best
of our knowledge, there is no existing work on automatingSV assessment in commits. Prior SV assessment techniquesthat analyze text in SV databases (e.g., [6], [7], [8]) alsocannot be directly adapted to the commit level. Contrary totext, commits contain deletions and additions of code withspeciÔ¨Åc structure and semantics [12], [22]. Additionally, wespeculate that CVSS metrics can be related. For example, anSQL injection is likely to be highly severe since attackerscan exploit it easily via crafted input and compromise dataconÔ¨Ådentiality and integrity. We posit that these metrics wouldhave common patterns in commits that can be potentiallyshared between SV assessment models. Predicting relatedtasks in a shared model has been successfully utilized forvarious applications [23]. For instance, an autonomous car is
7172021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)
DOI 10.1109/ASE51524.2021.000692021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE) | 978-1-6654-0337-5/21/$31.00 ¬©2021 IEEE | DOI: 10.1109/ASE51524.2021.9678622
978-1-6654-0337-5/21/$31.00  ¬©2021  IEEE
driven with simultaneous detection of vehicles, lanes, signs
and pavement [24]. These observations motivated us to tackle anew and important research challenge, ‚ÄúHow can we leverage
the common attributes of assessment tasks to performeffective and efÔ¨Åcient commit-level SV assessment?‚Äù
We present DeepCV A, a novel Deep
multi-task learning
model, to automate Commit-level Vulnerability Assessment.
DeepCV A Ô¨Årst uses attention-based convolutional gated re-current units to extract features of code and surroundingcontext from vulnerability-contributing commits (i.e., commitswith vulnerable changes). The model uses these features topredict seven CVSS assessment metrics (i.e., ConÔ¨Ådentiality,Integrity, Availability, Access V ector, Access Complexity, Au-thentication, and Severity) simultaneously using the multi-tasklearning paradigm. The predicted CVSS metrics can guide SVmanagement and remediation processes.
Our key contributions are summarized as follows:
‚Ä¢We are the Ô¨Årst to tackle the commit-level SV assessmenttasks that enable early security risks estimation andplanning for SV remediation.
‚Ä¢We propose a uniÔ¨Åed model, DeepCV A, to automateseven commit-level SV assessment tasks simultaneously.
‚Ä¢We extensively evaluate DeepCV A on our curated large-scale dataset of 1,229 vulnerability-contributing commitswith 542 SVs from 246 real-world projects.
‚Ä¢We demonstrate that DeepCV A has 38% to 59.8%higher Matthews Correlation CoefÔ¨Åcient (MCC) than var-ious supervised and unsupervised baseline models usingtext-based features and software metrics. The proposedcontext-aware features improve the MCC of DeepCV Aby 14.8%. The feature extractor with attention-basedconvolutional gated recurrent units, on average, adds52.9% MCC for DeepCV A. Multi-task learning alsomakes DeepCV A 24.4% more effective and 6.3 timesmore efÔ¨Åcient in training/validation/testing than separatemodels for seven assessment tasks.
‚Ä¢We release our source code, models and datasets at [25].
Paper structure. Section II introduces preliminaries and moti-vation. Section III proposes the DeepCV A model for commit-level SV assessment. Section IV describes our experimentaldesign and setup. Section V presents the experimental results.Section VI discusses our Ô¨Åndings and threats to validity.Section VII covers the related work. Section VIII concludesthe work and proposes future directions.
II. B
ACKGROUND AND MOTIV A TION
A. Vulnerability in Code Commits
Commits are an essential unit of any version control system
(e.g., Git) and record all the chronological changes made to thecodebase of a software project. As illustrated in Fig. 1, changesin a commit consist of deletion(s) (‚Äì) and/or addition(s) (+) ineach affected Ô¨Åle.
Vulnerability-Contributing Commits (VCCs) are commits
whose changes contain SVs [11], e.g., using vulnerable li-braries or insecure implementation. We focus on VCCs ratherthan any commits with vulnerable code (in unchanged parts)		

	
		

	
	
 		
 !
"	!			
			
	#$%&%$	
 "			"		
# 	
		!''('!'))*+'
	!''!"'	,(		!''('!'))*+'
	!''!"'	,(
	 	

	






			
Fig. 1. Exemplary SV Ô¨Åxing commit (right) for the XML external entity
injection (XXE) (CVE-2016-3674) and its respective SV contributing commit(left) in the xstream project.
since addressing VCCs helps mitigate SVs as early as they
are added to a project. VCCs are usually obtained based onVulnerability-Fixing Commits (VFCs) [14], [15]. An exem-plary VFC and its respective VCC are shown in Fig. 1. VFCsdelete, modify or add code to eliminate an SV (e.g., disablingexternal entities processing in the XML library in Fig. 1) andcan be found in bug/SV tracking systems. Then, VCCs arecommits that last touched the code changes in VFCs. Our workalso leverages VFCs to obtain VCCs for building automatedcommit-level SV assessment models.
B. Commit-level SV Assessment with CVSS
Common Vulnerability Scoring System (CVSS) [3] has
been an expert-maintained standard for SV assessment. CVSS
base metrics are prevalently used to determine through whichattack vectors SVs can be exploited and assess their potentialimpacts. This allows developers to better plan and prioritizethe mitigation of such SVs. The base metrics are ConÔ¨Ådential-
ity, Integrity, Availability, Access V ector, Access Complexity,
Authentication and Severity. We use CVSS version 2 of base
metrics to assess SVs as version 2 is still more predominantlyused than version 3 (introduced in 2015). SVs before 2015are also still relevant in the modern context; e.g., CVE-2004-0113 discovered in 2004 was exploited in a cryptoattack in 2018 [26]. Based on CVSS version 2, the VCC(CVE-2016-3674) in Fig. 1 has a considerable impact on theConÔ¨Ådentiality. This SV can be exploited with low (Access)complexity with no authentication via public network (AccessV ector), making it an attractive target for attackers.
Despite the criticality of these SVs, there have been delays
in reporting, assessing and Ô¨Åxing them. Concretely, the VCC inFig. 1 required 1,439 and 1,469 days to be reported
1and Ô¨Åxed
(in VFC), respectively. Existing SV assessment methods basedon bug/SV reports (e.g., [6], [7], [8]) would need to wait morethan 1,000 days for the report of this SV . However, performingSV assessment right after this commit was submitted canbypass the waiting time for SV reports, enabling developersto realize the exploitability/impacts of this SV and plan to Ô¨Åxit much sooner. To the best of our knowledge, there has notbeen any study on automated commit-level SV assessment,i.e., assigning seven CVSS base metrics to a VCC. Our workidentiÔ¨Åes and aims to bridge this important research gap.
1https://github.com/x-stream/xstream/issues/25
718	





	


 
	

		


	
	
	
	

	
	
 

 


 	
	
 

 
	 
 			


		


		

	
		


	

			



	



	
	
	

	
	
 	
!
"

#	



	

	$
%
&	


' (
	
)	#	

*)		+),
  - 	
	.


"

	/
*)	0
	
Fig. 2. WorkÔ¨Çow of DeepCV A for automated commit-level SV assessment. The VCC is the one described in Fig. 1.
C. Feature Extraction from Commit Code Changes
The extraction of commit features is important for building
commit-level SV assessment models. Many existing commit-
level defect/SV prediction models have only considered com-mit code changes (e.g., [12], [22], [27]). However, we arguethat the nearby context of code changes also contributesvaluable information to the prediction. For instance, the sur-rounding code of the changes in Fig. 1 provides extra details;e.g., the method return statement is modiÔ¨Åed and the returntype is XMLInputFactory. Such a type can help learn
patterns of XXE SV that usually occurs with XML processing.
Besides the context, we speculate that SV assessment mod-
els can also beneÔ¨Åt from the relatedness among the assessmenttasks. For example, the XXE SV in Fig. 1 allows attack-ers to read arbitrary system Ô¨Åles, which mainly affects theConÔ¨Ådentiality rather than the Integrity and Availability of asystem. This work investigates the possibility of incorporatingthe common features of seven CVSS metrics into a singlemodel using the multi-task learning paradigm [23] insteadof learning seven cumulative individual models. SpeciÔ¨Åcally,multi-task learning leverages the similarities and the interac-tions of the involved tasks through a shared feature extractor topredict all the tasks simultaneously. Such a uniÔ¨Åed model cansigniÔ¨Åcantly reduce the time and resources to train, optimizeand maintain/update the model in the long run.
III. T
HEDEEP CV A M ODEL
We propose DeepCV A (see Fig. 2), a novel Deep learning
model to automate Commit-level Vulnerability Assessment.
DeepCV A is a uniÔ¨Åed and end-to-end trainable model thatconcurrently predicts seven CVSS metrics (i.e., ConÔ¨Ådential-ity, Integrity, Availability, Access V ector, Access Complexity,Authentication, and Severity) for a Vulnerability-ContributingCommit (VCC). DeepCV A contains: ( i) preprocessing, context
extraction and tokenization of code commits (section III-A),(ii) feature extraction from commits shared by seven assess-ment tasks using attention-based convolutional gated recurrentunits (section III-B), and (iii) simultaneous prediction of sevenCVSS metrics using multi-task learning [23] (section III-C).To assign the CVSS metrics to a new VCC with DeepCV A,we Ô¨Årst preprocess the commit, obtain its code changes andrespective context and tokenize such code changes/context.Embedding vectors of preprocessed code tokens are thenobtained, and the commit feature vector is extracted using thetrained feature extractor. This commit feature vector passesthrough the task-speciÔ¨Åc blocks and softmax layers to getthe seven CVSS outputs with the highest probability values.Details of each component are given hereafter.
A. Commit Preprocessing, Context Extraction & Tokenization
To train DeepCV A, we Ô¨Årst obtain and preprocess code
changes (hunks) and extract the context of such changes. We
then tokenize them to prepare inputs for feature extraction.Commit preprocessing. Preprocessing helps remove noise incode changes and reduce computational costs. We removenewlines/spaces and inline/multi-line comments since they donot change code functionality. We do not remove punctuations(e.g., ‚Äú;‚Äù, ‚Äú(‚Äù, ‚Äú)‚Äù) and stop words (e.g., and/or operators)
to preserve code syntax. We also do not lowercase codetokens since developers can use case-sensitivity for namingconventions of different token types (e.g., variable name:system vs. class name: System). Stemming (i.e., reducing
a word to its root form such as equals toequal)i s
not applied to code since different names can change codefunctionality (e.g., the built-in equals function in Java).
Context extraction algorithm. We customize Sahal etal.‚Äôs [28] Closest Enclosing Scope (CES) to identify the context
of vulnerable code changes for commit-level SV assessment(see section II-C). Sahal et al. [28] deÔ¨Åned an enclosingscope to be the code within a balanced amount of openingand closing curly brackets such as if/switch/while/for
blocks. Among all enclosing scopes of a hunk, the one withthe smallest size (lines of code) is selected as CES to reduceirrelevant code. Sahal et al. [28] found CES usually containshunk-related information (e.g., variable values/types preceding
719public class PlainNegotiator implements SaslNegotiator {
...
- private static final String UTF8 = Standard
Charsets.UTF_8.name();
+  private static final Charset UTF8 = Standard
Charsets.UTF_8;
...
} // End of the PlainNegotiator class
Fig. 3. Code changes outside of a method from the commit 4b9fb37 in the
Apache qpid-broker-j project.
changes). CES also alleviates the need for manually pre-
deÔ¨Åning the context size as in [14], [29]. Some existing studies(e.g., [30], [31]) only used the method/function scope, butcode changes may occur outside of a method. For instance,changes in Fig. 3 do not have any enclosing method, but wecan still obtain its CES, i.e., the PlainNegotiator class.
There are still two main limitations with the deÔ¨Ånition of
CES in [28]. Firstly, a scope (e.g., for/while in Java) with
single-line content does not always require curly brackets.Secondly, some programming languages do not use curlybrackets to deÔ¨Åne scopes like Python. To address these twoissues, we utilize Abstract Syntax Tree (AST) depth-Ô¨Årsttraversal (see Algorithm 1) to obtain CESs of code changes,as AST covers the syntax of all scope types and generalizesto any programming languages.
Algorithm 1 contains: (i) the extract_scope function
for extracting potential scopes of a code hunk (lines 1-8), and(ii) the main code to obtain the CES of every hunk in a commit(lines 9-18). The extract_scope function leverages depth-
Ô¨Årst traversal with recursion to go through every node in anAST of a Ô¨Åle. Line 3 adds the selected part of an AST to thelist of potential scopes (potential_scopes) of the currenthunk. The Ô¨Årst (root) AST is always valid since it encompassesthe whole Ô¨Åle. Line 6 then checks whether each node (sub-tree) of the current AST has one of the following types:class, interface, enum, method, if/else, switch,
for/while/do, try/catch, and is surrounding the current
hunk. If the conditions are satisÔ¨Åed, the extract_scope
function would be called recursively in line 7 until a leafof the AST is reached. The main code starts to extract themodiÔ¨Åed Ô¨Åles of the current commit in line 9. For each Ô¨Åle,we extract code hunks (code deletions/additions) in line 12 andthen obtain the AST of the current Ô¨Åle using an AST parser inline 13. Line 16 calls the deÔ¨Åned extract_scope function
to generate the potential scopes for each hunk. Among theidentiÔ¨Åed scopes, line 17 adds the one with the smallest size(i.e., the number of code lines excluding empty lines andcomments) to the list of CESs (all_ces). Finally, line 18of Algorithm 1 returns all the CESs for the current commit.
We treat deleted (pre-change), added (post-change) code
changes and their CESs as four separate inputs to be vectorizedby the shared input embedding, as illustrated in Fig. 2. Foreach input, we concatenate all the hunks/CESs in all the af-fected Ô¨Åles of a commit to explicitly capture their interactions.
Code-aware tokenization. The four inputs extracted from a
commit are then tokenized with a code-aware tokenizer toAlgorithm 1: AST-based extraction of the Closest
Enclosing Scopes (CESs) of commit code changes.
Input: Current Vulnerability-Contributing Commit (VCC): commit
Scope type: scope types
Output: CESs of code changes in the current commit: allces
1Function extract_scope(AST, hunk, visited =‚àÖ):
2 global potential scopes
3 potential scopes‚Üê‚àí potential scopes +AST
4 visited‚Üê‚àí visited +AST
5 foreach node‚ààAST do
6 ifnode /‚ààvisited and type(node) ‚ààscope types and
start node‚â§start hunk andend node‚â•end hunk then
7 extract scope(AST, hunk, visited)
8 return
9files‚Üê‚àí extract Ô¨Åles(commit)
10allces‚Üê‚àí ‚àÖ
11foreach fi‚ààfiles do
12 hunks‚Üê‚àí extract hunk(commit, f i)
13 AST i‚Üê‚àí extract AST(f i)
14 foreach hi‚ààhunks do
15 potential scopes‚Üê‚àí ‚àÖ
16 extract scopes(AST i,hi)
17 allces‚Üê‚àí allces+a r g m i n
size(potential scopes )
18return allces
preserve code semantics and help prediction models be moregeneralizable. For example, a++ andb++ are tokenized as a,
band++, explicitly giving a model the information about one-
increment operator (++). Tokenized code is fed into a sharedDeep Learning model, namely Attention-based ConvolutionalGated Recurrent Unit (AC-GRU), to extract commit features.
B. Feature Extraction with Deep AC-GRU
Deep AC-GRU has a three-way Convolutional Neural Net-
work to extract n-gram features and Attention-based Gated
Recurrent Units to capture dependencies among code changes
and their context. This feature extractor is shared by four
inputs, i.e., deleted/added code hunks/context. Each input hasthe size of N√óL, where Nis the no. of code tokens and
Lis the vector length of each token. All inputs are truncated
or padded to the same length Nto support parallelization.
The feature vector of each input is obtained from a sharedInput Embedding layer that maps code tokens into Ô¨Åxed-length
arithmetic vectors. The dimensions of this embedding layerare|V|√óL, where |V|is the code vocabulary size, and its
parameters are learned together with the rest of the model.
Three-way Convolutional Neural Network. We use a shared
three-way Convolutional Neural Network (CNN) [32] to ex-tract n-grams (n = 1,3,5) of each input vector. The three-way CNN has Ô¨Ålters with three sizes of one, three and Ô¨Åve,respectively, to capture common code patterns, e.g., public
class Integer. The Ô¨Ålters are randomly initialized andjointly learned with the other components of DeepCV A. Wedid not include 2-grams and 4-grams to reduce the requiredcomputational resources without compromising the modelperformance, which has been empirically demonstrated insection V-B. To generate code features of different windowsizes with the three-way CNN, we multiply each Ô¨Ålter withthe corresponding input rows and apply non-linear ReLUactivation function [33], i.e., ReLU(x)=m a x ( 0 ,x).W e
720repeat the same convolutional process from the start to the
end of an input vector by moving the Ô¨Ålters down sequentiallywith a stride of one. This stride value is the smallest and helpscapture the most Ô¨Åne-grained information from input code ascompared to larger values. Each Ô¨Ålter size returns feature mapsof the size (N‚àíK+1 )√óF, whereKis the Ô¨Ålter size (one,
three or Ô¨Åve) and Fis the number of Ô¨Ålters. Multiple Ô¨Ålters
are used to capture different semantics of commit data.
Attention-based Gated Recurrent Unit. The feature maps
generated by the three-way CNN sequentially enter a GatedRecurrent Unit (GRU) [31]. GRU, deÔ¨Åned in Eq. (1), is anefÔ¨Åcient version of Recurrent Neural Networks and used toexplicitly capture the order and dependencies between codeblocks. For example, the return statement comes after the
function declarations of the VCC in Fig. 2.
z
t=œÉ(Wzxt+Uzht‚àí1+bz)
rt=œÉ(Wrxt+Urht‚àí1+br)
ÀÜht= tanh(W hxt+Uh(rt‚äôht‚àí1)+b h)
ht=( 1‚àízt)‚äôht‚àí1+zt‚äôÀÜht(1)
whereWz,Wr,Wh,Uz,Ur,Uhare learnable weights, bz,
br,bhare learnable biases, ‚äôis element-wise multiplication, œÉ
is sigmoid function and tanh() is hyperbolic tangent function.
To determine the information (h t) at each token (time step) t,
GRU combines the current input ( xt) and the previous time
step (h t‚àí1) using the update (zt) and reset (rt) gates. htis
then carried on to the next token until the end of the input tomaintain the dependencies of the whole code sequence.
The last token output of GRU is often used as the whole
sequence representation, yet it suffers from the information
bottleneck problem [34], especially for long sequences. To
address this issue, we incorporate the attention mechanism [34]
into GRU to explicitly capture the contribution of each inputtoken, as formulated in Eq. (2).
out
attention =m/summationdisplay
i=1wihi
wi= softmax(W stanh(W ahi+ba))
=exp(W stanh(W ahi+ba))
m/summationtext
j=1exp(W stanh(W ahj+ba))(2)
wherewiis the weight of hi;Ws,Waare learnable weights,
bais learnable bias, and mis the number of code tokens.
The attention-based outputs (out attention ) of the three
GRUs (see Fig. 2) are concatenated into a single featurevector to represent each of the four inputs (pre-/post-changehunks/contexts). The commit feature vector is a concatenationof the vectors of all four inputs generated by the shared AC-GRU feature extractor. This feature vector is used for multi-task prediction of seven CVSS metrics.
C. Commit-level SV Assessment with Multi-task Learning
This section describes the multi-task learning layers of
DeepCV A for efÔ¨Åcient commit-level SV assessment using a
single model as well as how to train the model end-to-end.Multi-task learning layers. The last component of DeepCV Aconsists of the multi-task learning layers that simultaneouslygive the predicted CVSS values for seven SV assessmenttasks. As illustrated in Fig. 2, this component contains twomain parts: task-speciÔ¨Åc blocks and softmax layers.O nt o p
of the shared features extracted by AC-GRU, task-speciÔ¨Åcblocks are necessary to capture the differences among theseven tasks. Each task-speciÔ¨Åc block is implemented using afully connected layer with non-linear ReLU activations [33].SpeciÔ¨Åcally, the output vector (task
i) of the task-speciÔ¨Åc
block for assessment task iis deÔ¨Åned in Eq. (3).
task i= ReLU(W txcommit +bt) (3)
wherexcommit is the commit feature vector from AC-GRU;
Wtis learnable weights and btis learnable bias.
Each task-speciÔ¨Åc vector goes through the respective soft-
max layer to determine the output of each task with the highestpredicted probability. The prediction output (pred
i) of task i
is given in Eq. (4).
pred i=a r g m a x ( prob i)
prob i= softmax(W ptask i+bp)
softmax(z j)=exp(z j)
nlabels i/summationtext
c=1exp(z c)(4)
whereprobicontains the predicted probabilities of nlabels i
possible outputs of task i;Wpis learnable weights and bpis
learnable bias.
Training DeepCV A. To compare DeepCV A ‚Äôs outputs with
ground-truth CVSS labels, we deÔ¨Åne a multi-task loss thataverages the cross-entropy losses of seven tasks in Eq. (5).
loss
DeepCV A =7/summationdisplay
i=1loss i
loss i=‚àínlabels i/summationdisplay
c=1yc
ilog(probc
i),yc
i=1 ifcis true class else 0
(5)
whereyc
i,probc
i, andnlabels iare the ground-truth value, pre-
dicted probability and all labels of CVSS task i, respectively.
We minimize this multi-task loss using a stochastic gradient
descent method [35] to optimize the weights of learnable
components in DeepCV A. We also use backpropagation [36]to automate partial differentiation with chain-rule and increasethe efÔ¨Åciency of gradient computation throughout the model.
IV . E
XPERIMENTAL DESIGN AND SETUP
All the experiments ran on a computing cluster that has 16
CPU cores with 16GB of RAM and Tesla V100 GPU.
A. Datasets
To develop commit-level SV assessment models, we built
a dataset of Vulnerability-Contributing Commits (VCCs) and
their CVSS metrics. We used Vulnerability-Fixing Commits(VFCs) to retrieve VCCs, as discussed in section II-A.
72128.5
66.8
4.7
32.7
62.9
4.4
51.8
43.2
5.0
3.1
96.9
65.0
33.9
1.1
79.2
20.8
5.4
70.9
23.7Confiden-
tialityIntegrity AvailabilityAccess
VectorAccess
ComplexityAuthen-
ticationSeverity0 1 02 03 04 05 06 07 08 09 0 1 0 0
CompletePartialNone
CompletePartialNone
CompletePartialNone
NetworkLocal
HighMediumLow
SingleNone
HighMediumLowPercentage (%)
Fig. 4. Class distributions of seven SV assessment tasks.
VFC identiÔ¨Åcation. We Ô¨Årst obtained VFCs from three public
sources: NVD [10], GitHub and its Advisory Database2as well
as a manually curated/veriÔ¨Åed VFC dataset (VulasDB) [37]. Intotal, we gathered 13,310 VFCs that had dates ranging fromJuly 2000 to October 2020. We selected VFCs in Java projectsas Java has been commonly investigated in the literature(e.g., [12], [31], [38]) and also in the top Ô¨Åve most popularlanguages in practice.
3Following the practice of [38], we
discarded VFCs that had more than 100 Ô¨Åles and 10,000 linesof code to reduce noise in the data.
VCC identiÔ¨Åcation with the SZZ algorithm. After the Ô¨Ål-
tering steps, we had 1,602 remaining unique VFCs to identifyVCCs using the SZZ algorithm [39]. This algorithm selectscommits that last modiÔ¨Åed the source code lines deleted ormodiÔ¨Åed to address an SV in a VFC as the respective VCCsof the same SV (see Fig. 1). As in [39], we Ô¨Årst discardedcommits with timestamps after the published dates of therespective SVs on NVD since SVs can only be reportedafter they were injected in a codebase. We then removedcosmetic changes (e.g., newlines and white spaces) and single-line/multi-line comments in VFCs since these elements do notchange code functionality [38]. Like [38], we also consideredcopied or renamed Ô¨Åles while tracing VCCs. We obtained1,229 unique VCCs
4of 542 SVs in 246 real-world Java
projects and their corresponding expert-veriÔ¨Åed CVSS metricson NVD. Distributions of curated CVSS metrics are illustratedin Fig. 4. The details of the number of commits and projectsretained in each Ô¨Åltering step are also given in Table I. Notethat some commits and projects were removed during thetracing of VCCs from VFCs due to the issues coined as ghostcommits studied by Rezk et al. [40]. We did not removelarge VCCs (with more than 100 Ô¨Åles and 10k lines) aswe found several VCCs were large initial/Ô¨Årst commits. Our
2https://github.com/advisories
3https://insights.stackoverÔ¨Çow.com/survey/2020#technology-most-loved-
dreaded-and-wanted-languages-loved
4The SV reports of all curated VCCs were not available at commit time.TABLE I: T HE NUMBER OF COMMITS AND PROJECTS AFTER EACH
FILTERING STEP .
No. Filtering step No. of commits No. of projects
1 All unÔ¨Åltered VFCs 13,310 2,864
2 Removing duplicate VFCs 9,989 2,864
3 Removing non-Java VFCs 1,607 361
4Removing VFCs with more than
100 Ô¨Åles & 10k lines1,602 358
5Tracing VCCs from VFCs usingthe SZZ algorithm3,742 342
6Removing VCCs with nullcharacteristics (CVSS values)2,271 246
7 Removing duplicate VCCs 1,229 246


	





Fig. 5. Time-based splits for training, validating & testing.
observations agreed with the Ô¨Åndings of Meneely et al. [11].
Manual VCC validation. To validate our curated VCCs, we
randomly selected 293 samples, i.e., 95% conÔ¨Ådence leveland 5% error [41], for two authors to independently examine.The manual VCC validation was considerably labor-intensive,which took approximately 120 man-hours. The Cohen‚Äôs kappa(Œ∫) inter-rater reliability score [42] was 0.83, i.e., ‚Äúalmostperfect‚Äù agreement [43]. We also involved the third authorin the discussion to resolve disagreements. Our validationfound that 85% of the VCCs were valid. In fact, the SZZalgorithm is imperfect [44], but we assert that it is nearlyimpossible to obtain near 100% accuracy without exhaustivemanual validation. SpeciÔ¨Åcally, the main source of incorrectlyidentiÔ¨Åed VCCs in our dataset was that some Ô¨Åles in VFCswere used to update version/documentation or address anotherissue instead of Ô¨Åxing an SV . One such false positive VCC wasthe commit 87c89f0 in the jspwiki project that last modiÔ¨Åed
the build version in the corresponding VFC.
Data splitting. We adopted time-based splits [45] for training,
validating and testing the models to closely represent real-
world scenarios where incoming/future unseen data is not
present during training [38], [46]. We trained, validated andtested the models in 10 rounds using 12 equal folds split basedon commit dates (see Fig. 5). SpeciÔ¨Åcally, in round i, folds
1‚Üíi,i+1 andi+2 were used for training, validation
and testing, respectively. We chose an optimal model with thehighest average validation performance and then reported its
respective average testing performance over 10 rounds, whichhelped avoid unstable results of a single testing set [47].
B. Evaluation Metrics
To evaluate the performance of automated commit-level SV
assessment, we utilized the F1-Score and Matthews Corre-
lation CoefÔ¨Åcient (MCC) metrics that have been commonlyused in the literature (e.g., [6], [7], [46]). These two metricsare suitable for the imbalanced classes [48] in our data (seeFig. 4). F1-Score has a range from 0 to 1, while MCC takes
722values from ‚Äì1 to 1, where 1 is the best value for both metrics.
MCC was used to select optimal models since MCC explicitlyconsiders all classes [48]. To evaluate the tasks with more thantwo classes, we used macro F1-Score [7] and the multi-classversion of MCC [49]. MCC of the multi-task DeepCV A modelwas the average MCC of seven constituent tasks. Note thatMCC is not directly proportional to F1-score.
C. Hyperparameter and Training Settings of DeepCVA
Hyperparameter settings. We used the average validation
MCC to select optimal hyperparameters for DeepCV A ‚Äôs com-
ponents. We also ran DeepCV A 10 times each round to reducethe impact of random initialization on model performance.We Ô¨Årst chose 1024 for the input length of the pre-/post-change hunks/context (see Fig. 2), which has been commonlyused in the literature (e.g., [50], [51]). Using a shorter inputlength would likely miss many code tokens, while a longerlength would signiÔ¨Åcantly increase the model complexity andtraining time. Shorter commits were padded with zeros, andlonger ones were truncated to ensure the same input size forparallelization with GPU [12], [22]. We built a vocabulary of10k most frequent code tokens in the Input Embedding layer assuggested by [52]. Note that using 20k-sized vocabulary onlyraised the performance by 2%, yet increased the model com-plexity by nearly two times. We selected an input embeddingsize of 300, i.e., a standard and usually high limit value formany embedding models (e.g., [53], [54]), and we randomlyinitialized embedding vectors [12], [32]. For the number ofÔ¨Ålters of the three-way CNN as well as the hidden units ofthe GRU, Attention and Task-speciÔ¨Åc blocks, we tried {32,
64, 128}, similar to [6]. We picked 128 as it had at least 5%better validation performance than 32 and 64.
Training settings. We used the Adam algorithm [55], the
state-of-the-art stochastic gradient descent method, for trainingDeepCV A end-to-end with a learning rate of 0.001 and abatch size of 32 as recommended by Hoang et al. [12]. Toincrease the training stability, we employed Dropout [56] witha dropout rate of 0.2 and Batch Normalization [57] betweenlayers. We trained DeepCV A for 50 epochs, and we wouldstop training if the validation MCC did not change in the last
Ô¨Åve epochs to avoid overÔ¨Åtting [12], [22].
D. Baseline Models
We considered three types of learning-based baselines for
automated commit-level SV assessment, as learning-based
models can automatically extract relevant SV patterns/featuresfrom input data for prediction without relying on pre-deÔ¨Ånedrules. The baselines were (i) S-CV A: Supervised single-
task model using either software metrics or text-based fea-tures including Bag-of-Words (BoW or token count) andWord2vec [53]; (ii) X-CV A: supervised eXtreme multi-class
model that performed a single prediction for all seven tasksusing the above feature types; and (iii) U-CV A: Unsupervised
model using k-means clustering [58] with the same features
as S-CV A/X-CV A. Note that there was no existing techniquefor automating commit-level SV assessment, so we could onlycompare DeepCV A with the compatible techniques proposedfor related tasks, as described hereafter.
Software metrics (e.g., [13], [14], [15]) and text-based fea-
tures (BoW/Word2vec) (e.g., [27], [59]) have been widelyused for commit-level prediction. We used 84 software metricsproposed by [13], [14], [15] for defect/SV prediction. Amongthese metrics, we converted C/C++ keywords into Java onesto match our dataset. The list of software metrics used inthis work can be found at [25]. As in [13], in each round inFig. 5, we also removed correlated software metrics that hada Spearman correlation larger than 0.7 based on the trainingdata of that round to avoid performance degradation, e.g., no.ofstars vs. forks of a project. For BoW and Word2vec, we
adopted the same vocabulary size of 10k to extract featuresfrom four inputs described in Fig. 2, as in DeepCV A. Featurevectors of all inputs were concatenated into a single vector.For Word2vec, we averaged the vectors of all tokens in aninput to generate its feature vector, which has been shown tobe a strong baseline [60]. Like DeepCV A, we also used anembedding size of 300 for each Word2vec token.
Using these feature types, S-CV A trained a separate super-
vised model for each CVSS task, while X-CV A used a singlemulti-class model to predict all seven tasks simultaneously.X-CV A worked by concatenating all seven CVSS metricsinto a single label. To extract the results of the individualtasks for X-CV A, we checked whether the ground-truth labelof each task was in the concatenated model output. For S-CV A and X-CV A, we applied six popular classiÔ¨Åers: LogisticRegression (LR), Support V ector Machine (SVM), K-NearestNeighbors (KNN), Random Forest (RF), XGBoost (XGB) [61]and Light Gradient Boosting Machine (LGBM) [62]. TheseclassiÔ¨Åers have been used for SV assessment based on SV re-ports [7], [8]. The hyperparameters for tuning these classiÔ¨Åerswere regularization: {l1, l2}; regularization coefÔ¨Åcient :{0.01,
0.1, 1, 10, 100} for LR and {0.01, 0.1, 1, 10, 100, 1,000,
10,000} for SVM; no. of neighbors: {11, 31, 51}, distance
norm:{1, 2} and distance weight :{uniform, distance} for
KNN; no. of estimators :{100, 300, 500}, max. depth: {3,
5, 7, 9, unlimited}, max. no. of leaf nodes: {100, 200, 300,
unlimited} for RF, XGB and LGBM. These hyperparameters
have been adapted from relevant studies [7], [8], [63].
Unlike S-CV A and X-CV A, U-CV A did not require CVSS
labels to operate; therefore, U-CV A required less human effortthan S-CV A and X-CV A. We tuned U-CV A for each task withthe following no. of clusters (k ):{2, 3, 4, 5, 6, 7, 8, 9, 10,
15, 20, 25, 30, 35, 40, 45, 50}. To assess a new commitwith U-CV A, we found the cluster with the smallest Euclideandistance to that commit and assigned it the most frequent classof each task in the selected cluster.
V. R
ESEARCH QUESTIONS AND EXPERIMENTAL RESULTS
A.RQ1: How does DeepCVA Perform Compared to Baseline
Models for Commit-level SV Assessment?
Motivation. We posit the need for commit-level Software
Vulnerability (SV) assessment tasks based on seven CVSS
723TABLE II: T ESTING PERFORMANCE OF DEEP CV A AND BASELINE MODELS .NOTES :O PTIMAL CLASSIFIERS OF S-CV A/X-CV A AND OPTIMAL CLUSTER
NO.(k)OFU-CV A ARE IN PARENTHESES .B OW, W2V AND SM ARE BAG-OF-W ORDS ,W ORD 2VEC AND SOFTW ARE METRICS ,RESPECTIVELY .THE
BEST PERFORMANCE OF DEEP CV A IS FROM THE RUN WITH THE HIGHEST MCC IN EACH ROUND .B EST ROW -WISE V ALUES ARE IN GREY .
CVSS metricEvaluation
metricModel
S-CV A X-CV A U-CV A DeepCV A (Best
in parentheses) BoW W2V SM BoW W2V SM BoW W2V SM
ConÔ¨ÅdentialityF1-Score 0.416 0.406 0.423 0.420 0.434 0.429 0.292 0.332 0.313 0.436 (0.475)
MCC0.174
(LR)0.239
(LGBM)0.232
(XGB)0.188
(LR)0.241
(LR)0.203
(XGB)0.003
(50)0.092
(45)0.017
(50)0.268 (0.299)
IntegrityF1-Score 0.373 0.369 0.352 0.391 0.415 0.407 0.284 0.305 0.330 0.430 (0.458)
MCC0.127
(LGBM)0.176
(LGBM)0.146
(RF)0.114
(LGBM)0.160
(LR)0.128
(LGBM)-0.005
(25)0.091
(30)0.084
(25)0.250 (0.295)
AvailabilityF1-Score 0.381 0.389 0.384 0.424 0.422 0.406 0.254 0.332 0.238 0.432 (0.475)
MCC0.182
(RF)0.173
(LGBM)0.126
(XGB)0.187
(LR)0.192
(LR)0.123
(XGB)0.064
(10)0.092
(45)0.016
(3)0.273 (0.303)
Access VectorF1-Score 0.511 0.487 0.440 0.499 0.532 0.487 0.477 0.477 0.477 0.554 (0.578)
MCC0.07
(XGB)0.051
(LR)0.018
(LR)0.044
(LGBM)0.107
(LR)0.012
(LGBM)0.000
(9)0.000
(40)0.000
(6)0.129 (0.178)
Access ComplexityF1-Score 0.437 0.448 0.417 0.412 0.445 0.361 0.315 0.365 0.385 0.464 (0.475)
MCC0.119
(LR)0.143
(XGB)0.111
(LGBM)0.131
(LR)0.121
(XGB)0.088
(SVM)0.000
(4)0.022
(30)0.119
(15)0.242 (0.261)
AuthenticationF1-Score 0.601 0.584 0.593 0.541 0.618 0.586 0.458 0.526 0.492 0.657 (0.677)
MCC0.258
(SVM)0.264
(XGB)0.268
(LGBM)0.212
(RF)0.282
(SVM)0.208
(XGB)0.062
(50)0.162
(30)0.089
(50)0.352 (0.388)
SeverityF1-Score 0.407 0.357 0.345 0.382 0.381 0.358 0.283 0.288 0.287 0.424 (0.460)
MCC0.144
(LR)0.153
(XGB)0.057
(XGB)0.130
(LR)0.149
(LGBM)0.058
(XGB)-0.018
(4)0.010
(15)0.026
(4)0.213 (0.277)
AverageF1-Score 0.447 0.434 0.422 0.438 0.464 0.433 0.338 0.375 0.360 0.485 (0.514)
MCC 0.153 0.171 0.137 0.144 0.179 0.117 0.015 0.067 0.050 0.247 (0.286)
metrics. Such tasks help developers to understand the ex-
ploitability and impacts of SVs as early as they are introducedin a system and devise remediation plans accordingly. RQ1evaluates our DeepCV A for this new and important task.
Method. We compared the effectiveness of our DeepCV A
model with the S-CV A, X-CV A and U-CV A baselines (seesection IV-D) on the testing sets. We trained, validated and
tested the models using the time-based splits, as described insection IV-A. Because of the inherent randomness of GPU-based implementation of DeepCV A,
5we ran DeepCV A 10
times in each round and then averaged its performance. Thebaselines were not affected by this issue as they did notuse GPU. For DeepCV A, we used the hyperparameter/trainingsettings in section IV-C. For each type of baseline, we usedgrid search on the hyperparameters given in section IV-D toÔ¨Ånd the optimal model with the highest validation MCC (see
section IV-B).
Results. DeepCVA outperformed all baselines
6(X-CVA, S-
CVA and U-CVA) in terms of both MCC and F1-Score7
for all seven tasks (see Table II). DeepCV A got average and
best MCC values of 0.247 and 0.286, i.e., 38% and 59.8%
better than the second-best baseline (X-CV A with Word2vecfeatures), respectively. Task-wise, DeepCV A had 11.2%, 42%,42.2%, 20.6%, 69.2%, 24.8% and 39.2% higher MCC than thebest respective baseline models for ConÔ¨Ådentiality, Integrity,Availability, Access V ector, Access Complexity, Authentica-tion and Severity tasks, respectively. Notably, the best Deep-CV A model achieved stronger performance than all baselineswith MCC percentage gaps from 24.1% (ConÔ¨Ådentiality) to82.5% (Access Complexity). The average and task-wise F1-
5https://keras.io/getting started/faq/#how-can-i-obtain-reproducible-results-
using-keras-during-development
6MCC values of random and most-frequent-class baselines were all <0.01.
7Precision (0.533)/Recall (0.445) of DeepCV A were >than all baselines.Score values of DeepCV A also beat those of the best baseline(X-CV A with Word2vec features) by substantial margins.We found that DeepCV A signiÔ¨Åcantly outperformed the bestbaseline models in terms of both MCC and F1-score averagingacross all seven tasks, conÔ¨Årmed with p-values <0.01 using
the non-parametric Wilcoxon signed-rank tests [64]. Theseresults show the effectiveness of the novel design of DeepCV A.
An example to qualitatively demonstrate the effectiveness of
DeepCV A is the VCC ff655ba in the Apache xerces2-j project,
in which a hashing algorithm was added. This algorithm waslater found vulnerable to hashing collision that could be ex-ploited with timing attacks in the Ô¨Åxing commit 992b5d9. This
SV was caused by the order of items being added to the hashtable in the put(String key, int value) function.
Such an order could not be easily captured by baseline modelswhose features did not consider the sequential nature of code(i.e., BoW, Word2vec and software metrics) [65]. More detailsabout the contributions of different components to the overallperformance of DeepCV A are covered in section V-B.
Regarding the baselines, the average MCC value (0.147) of
X-CV A was on par with that (0.154) of S-CV A. This resultreinforces the beneÔ¨Åts of leveraging the common attributesamong seven CVSS metrics to develop effective commit-levelSV assessment models. However, X-CV A was still not asstrong as DeepCV A mainly because of its much lower trainingdata utilization per output. For X-CV A, there was an averageof 39 output combinations of CVSS metrics in the trainingfolds, i.e., 31 commits per output. In contrast, DeepCV A had13.2 times more data per output as there were at most threeclasses for each task (see Fig. 4). Finally, we found supervisedlearning (S-CV A, X-CV A and DeepCV A) to be at least 74.6%more effective than the unsupervised approach (U-CV A). Thisresult demonstrates the usefulness of using CVSS metrics toguide the extraction of commit features.
724-4.6-1.4-2.8-0.4-3.2-5.6-4.3
-6.71.0-4.2-2.6-1.80.8-0.4
-3.2-0.8-1.9-3.4-7.4-3.90.4
-7.4-4.3-6.4-5.1-5.3-6.2-0.5
-5.4-5.6-4.8-7.7-6.3-6.1-4.1
-19.9-15.0-18.04.0-18.0-22.5-11.9
-0.1-1.1-1.2-7.3-2.5-2.00.2
-7.2-6.0-7.50.6-7.3-2.1-4.4
-30-25-20-15-10-50510
Confidentiality Integrity Availability Access Vector Access Complexity Authentication SeverityMCC difference with respect to
DeepCVA
No context (0.215)
AST inputs (0.227)1-ngrams only (0.218)No attention-based GRU (0.196)No three-way CNN (0.189)No attention mechanism (0.102)No task-specific blocks (0.227)No multi-task learning (0.198)
Fig. 6. Differences of testing MCC (multiplied by 100 for readability) of the model variants compared to the proposed DeepCV A in section III. Note: The
average MCC values (without multiplying by 100) of the model variants are in parentheses.
B.RQ2: What are the Contributions of the Main Components
in DeepCVA to Model Performance?
Motivation. We have shown in RQ1 that DeepCV A sig-
niÔ¨Åcantly outperformed all the baselines for seven commit-level SV assessment tasks. RQ2 aims to give insights intothe contributions of the key components to such a strongperformance of DeepCV A. Such insights can help researchersand practitioners to build effective SV assessment models.
Method. We evaluated the performance contributions of the
main components of DeepCV A: (i) Closest Enclosing Scope(CES) of code changes, (ii) CNN Ô¨Ålter size, (iii) Three-wayCNN, (iv) Attention-based GRU, (v) Attention mechanism,(vii) Task-speciÔ¨Åc blocks and ( vi) Multi-task learning. For each
component, we Ô¨Årst removed it from DeepCV A, retrained themodel variant and reported its testing result. When we re-
moved Attention-based GRU, we used max-pooling [12], [32]after the three-way CNN to generate the commit vector. Whenwe removed Multi-task learning, we trained a separate modelfor each of the seven CVSS metrics. We also investigated anAbstract Syntax Tree (AST) variant of DeepCV A, in whichwe complemented input code tokens with their syntax (e.g.,i n ta=1 is aVariableDeclarationStatement,
where ais an Identifier and1is aNumberLiteral).
This AST-based variant explored the usefulness of syntacticalinformation for commit-level SV assessment. We extracted thenodes in an AST that contained code changes and their CES.If more than two nodes contained the code of interest, wechose the one at a lower depth in the AST. We then Ô¨Çattenedthe nodes with depth-Ô¨Årst traversal for feature extraction [66].
Results. As depicted in Fig. 6, the main components
8uplifted
the average MCC of DeepCVA by 25.9% for seven tasks.
Note that 7/8 model variants (except the model with noattention mechanism) outperformed the best baseline modelfrom RQ1. These results were conÔ¨Årmed with p-values <
0.01 using Wilcoxon signed-rank tests [64]. SpeciÔ¨Åcally, thecomponents
8of DeepCV A increased the MCC values by
25.3%, 20.8%, 21.5%, 35.8%, 35.5%, 18.9% and 23.6% for
8We excluded the DeepCV A variant with no attention mechanism as its
performance was abnormally low, affecting the overall trend of other variants.ConÔ¨Ådentiality, Integrity, Availability, Access V ector, AccessComplexity, Authentication and Severity, respectively.
For the inputs, using the Smallest Enclosing Scope (CES) of
code changes resulted in a 14.8% increase in MCC comparedto using hunks only, while using AST inputs had 8.8% lowerperformance. This Ô¨Ånding suggests that code context is im-portant for assessing SVs in commits. In contrast, syntacticalinformation is not as necessary since code structure can beimplicitly captured by code tokens and their sequential orderusing our AC-GRU.
The key components of the AC-GRU feature extractor
boosted the performance by 13.2% (3-grams vs. 1-grams),25.6% (Attention-based GRU), 30.2% (Three-way CNN) and142% (Attention). Note that DeepCV A surpassed the state-of-the-art 3-gram [6] and 1-gram [12] CNN-only architecturesfor (commit-level) SV/defect prediction. These results showthe importance of combining the (1,3,5)-gram three-way CNNwith attention-based GRUs rather than using them individually.We also found that 1-5 grams did not signiÔ¨Åcantly increasethe performance (p-value = 0.186), conÔ¨Årming our decision insection III-B to only use 1,3,5-sized Ô¨Ålters.
For the prediction layers, we raised 8.8% and 24.4% MCC
of DeepCV A with Task-speciÔ¨Åc blocks and Multi-task learn-ing, respectively. Multi-task DeepCV A took 8,988 s (2.5 hours)and 25.7 s to train/validate and test in 10 rounds √ó10 runs,
which were 6.3 and 6.2 times faster compared to those of sevensingle-task DeepCV A models, respectively. DeepCV A wasonly 11.3% and 12.7% slower in training/validating and testingthan one single-task model on average, respectively. Thesevalues highlight the efÔ¨Åciency of training and maintaining themulti-task DeepCV A model. Finally, obtaining Severity usingthe CVSS formula [7] from the predicted values of the othersix metrics dropped MCC by 17.4% for this task. This resultsupports predicting Severity directly from commit data.
C.RQ3: What are the Effects of Class Rebalancing Tech-
niques on Model Performance?
Motivation. Recent studies (e.g., [67], [68]) have shown that
class rebalancing techniques (i.e., equalizing the class distribu-tions in the training set) can improve model effectiveness fordefect/SV prediction. However, these rebalancing techniques
725can only be applied to single-task models, not multi-task ones.
The reason is that each task has a unique class distribution(see Fig. 4), and thus balancing class distribution of one taskwill not balance classes of the others. RQ3 is important totest whether multi-task DeepCV A still outperforms single-taskbaselines in RQ1/RQ2 using rebalancing techniques.
Method. We compared the testing performance of multi-task
DeepCV A with baselines in RQ1/RQ2 using two popular
oversampling techniques [67]: Random OverSampling (ROS )
and SMOTE [69]. ROS randomly duplicates the existing sam-
ples of minority classes, while SMOTE randomly generatessynthetic samples between the existing minority-class samplesand their nearest neighbor(s) based on Euclidean distance. Wedid not consider undersampling, as such models performedpoorly because of some very small minority classes (e.g.,Low Access Complexity had only 14 samples). We applied
ROS and SMOTE to only the training set and then optimized
all baseline models again. Like [67], we also tuned SMOTEusing grid search with different values of nearest neighbors:{1, 5, 10, 15, 20}. We could not apply SMOTE to single-taskDeepCV A as features were trained end-to-end and unavailableprior training for Ô¨Ånding nearest neighbors. We also did notapply SMOTE to X-CV A as there was always a single-sampleclass in each round, producing no nearest neighbor.
Results. ROS and SMOTE increased the average perfor-
mance (MCC) of 3/4 baselines except X-CVA (see Table III).
However, the average MCC of our multi-task DeepCVAwas still 14.4% higher than that of the best oversampling-augmented baseline (single-task DeepCVA with ROS).O v e r -all, MCC increased by 8%, 6.9% and 9.1% for S-CV A(ROS), S-CV A (SMOTE) and single-task DeepCV A (ROS),respectively. These improvements were conÔ¨Årmed signiÔ¨Åcantwith p-values <0.01 using Wilcoxon signed-rank tests [64].
We did not report oversampling results of U-CV A as theywere still much worse compared to others. We found single-task DeepCV A beneÔ¨Åted the most from oversampling, prob-ably since Deep Learning usually performs better with moredata [70]. In contrast, oversampling did not improve X-CV Aas oversampling did not generate as many samples for X-CV Aper class as for S-CV A (i.e., X-CV A had 13 times, on average,more classes than S-CV A). These results further strengthen theeffectiveness and efÔ¨Åciency of multi-task learning of DeepCV Afor commit-level SV assessment even without the overheadsof rebalancing/oversampling data.
VI. D
ISCUSSION
A. DeepCVA and Beyond
DeepCV A has been shown to be effective for commit-level
SV assessment in the three RQs, but our model still has falsepositives. We analyze several representative patterns of suchfalse positives to help further advance this task and solutionsfor researchers and practitioners.
Some commits were too complex and large to be assessed
correctly. For example, the VCC 015f7ef in the Apache Spark
project contained 1,820 additions and 146 deletions across 29Ô¨Åles; whereas, the untrusted deserialization SV occurred in justTABLE III: T ESTING PERFORMANCE (MCC) OF OPTIMAL BASELINES
USING OVERSAMPLING TECHNIQUES AND MULTI -TASK DEEP CV A. NOTE :
‚Ä†DENOTES THA T THE OVERSAMPLED MODELS OUTPERFORMED THE
NON -OVERSAMPLED ONE REPORTED IN RQ1/RQ2.
CVSS TaskS-CV A
(ROS)S-CV A
(SMOTE)X-CV A
(ROS)Single-task
DeepCV A
(ROS)Multi-task
DeepCV A
ConÔ¨Ådentiality 0.220 0.203 0.185 0.250‚Ä†0.268
Integrity 0.174 0.168 0.179‚Ä†0.206‚Ä†0.250
Availability 0.195‚Ä†0.187‚Ä†0.182 0.209‚Ä†0.273
Access Vector 0.115‚Ä†0.110‚Ä†0.092 0.156‚Ä†0.129
Access Comp. 0.172‚Ä†0.186‚Ä†0.144‚Ä†0.190‚Ä†0.242
Authentication 0.325‚Ä†0.340‚Ä†0.299‚Ä†0.318 0.352
Severity 0.132 0.124 0.141 0.186‚Ä†0.213
Average 0.190‚Ä†0.188‚Ä†0.175 0.216‚Ä†0.247
one line 56 in LauncherConnection.java. Recent tech-
niques (e.g., [71], [72]) can pinpoint more precise locations(e.g., individual Ô¨Åles or lines in commits) of defects. Suchtechniques can be adapted to remove irrelevant code in VCCs(i.e., changes that do not introduce or contain SVs). Morerelevant code potentially gives more Ô¨Åne-grained informationfor the SV assessment tasks. Note that DeepCV A provides astrong baseline for comparing against Ô¨Åne-grained approaches.
DeepCV A also struggled to predict assessment met-
rics for SVs related to external libraries. For instance,the SV in the commit 015f7ef above occurs with the
ObjectInputStream class from the java.io package,
which sometimes prevented DeepCV A from correctly assess-ing an SV . If an SV happens frequently with a package inthe training set, (e.g., the XML library of the VCC bba4bc2
in Fig. 1), DeepCV A still can infer correct CVSS metrics.Pre-trained code models on large corpora [31], [50], [73]along with methods to search/generate code [74] and doc-umentation [75] as well as (SV-related) information fromdeveloper Q&A forums [76] can be investigated to provideenriched context of external libraries, which would supportmore reliable commit-level SV assessment with DeepCV A.
We also observed that DeepCV A, alongside the considered
baseline models, performed signiÔ¨Åcantly worse, in terms ofMCC, for Access V ector compared to the remaining tasks (seeTable II). We speculate that such low performance is mainlybecause Access V ector contains the most signiÔ¨Åcant classimbalance among the tasks, as shown in Fig. 4. For single-task models, we found that using class rebalancing techniquessuch as ROS or SMOTE can help improve the performance,as demonstrated in RQ3 (see section V-C). However, it is stillunclear how to apply the current class rebalancing techniquesfor multi-task learning models such as DeepCV A. Thus, wesuggest that more future work should investigate speciÔ¨Åcclass rebalancing and/or data augmentation to address suchimbalanced data in the context of multi-task learning.
B. Threats to V alidity
The Ô¨Årst threat is the collection of VCCs. We followed the
practices in the literature to reduce the false positives of the
SZZ algorithm. We further mitigated this threat by performingindependent manual validation with three of the authors.
Another concern is the potential suboptimal tuning of base-
lines and DeepCV A. However, it is impossible to try the entire
726hyperparameter space within a reasonable amount of time.
For the baseline models, we lessened this threat by usinga wide range of hyperparameters from the previous studiesto reoptimize these models from scratch on our data. ForDeepCV A, we adapted the best practices recommended in therelevant literature to our tasks.
The reliability and generalizability of our Ô¨Åndings are also
potential threats. We ran DeepCV A 10 times to mitigate theexperimental randomness. We conÔ¨Årmed our results using non-parametric statistical tests with a conÔ¨Ådence level >99%. Our
results may not generalize to all software projects. However,we reduced this threat by conducting extensive experimentson 200+ real-world projects of different scales and domains.
VII. R
ELA TED WORK
A. Data-driven SV Prediction and Assessment
Public security databases like NVD and expert-based SV
scoring frameworks like CVSS have provided large-scaledata to determine different characteristics of SVs. Bozorgi etal. [77] pioneered this area by developing a Support V ectorMachine model to predict when SVs would be exploited. Afterthat, SV information on NVD has been utilized to infer thetypes [78], severity level [79] and exploitability [80] of SVs.Recently, many studies [8], [7], [81], [82] have used data-driven techniques to obtain various CVSS metrics for SVassessment from SV reports on NVD. Other studies [83], [84]have leveraged code patterns in Ô¨Åxing commits of third-partylibraries to assess SVs in such libraries. Our work is funda-mentally different from these previous studies since we arethe Ô¨Årst to investigate the potential of performing assessmentof all SV types (not only vulnerable libraries) using commitchanges rather than bug/SV reports/Ô¨Åxes. Our approach allowspractitioners to realize the exploitability/impacts of SVs intheir systems much earlier, e.g., up to 1,000 days before (seesection II-B), as compared to using bug/SV reports/Ô¨Åxes. Lessdelay in SV assessment helps practitioners to plan/prioritizeSV Ô¨Åxing with fresh design and implementation in theirminds. Moreover, we have shown that multi-task learning, i.e.,predicting all CVSS metrics simultaneously, can signiÔ¨Åcantlyincrease the effectiveness and reduce the model developmentand maintenance efforts in commit-level SV assessment.
B. SV Analytics in Code Changes
Commit-level prediction (e.g., [13], [22], [85]) has been
explored to provide just-in-time information for developers
about code issues, but such studies mainly focused on generic
software defects. However, SV is a special type of defects [86]that can threaten the security properties of a software project.Thus, SV requires special treatment [87] and domain knowl-edge [88]. Meneely et al. [11] and Bosu et al. [89] conductedin-depth studies on how code and developer metrics affectedthe introduction and review of VCCs. Besides analyzing thecharacteristics of VCCs, other studies [14], [15], [90] alsodeveloped commit-level SV detection models that leveragedsoftware and text-based metrics. Different from the previousstudies that have detected VCCs, we focus on the assessmentof such VCCs. SV assessment is as important as the detectionstep since assessment metrics help early plan and prioritizeremediation for the identiÔ¨Åed SVs. It is worth noting that theexisting SV detection techniques can be used to Ô¨Çag VCCsthat would then be assessed by our DeepCV A model.
VIII. C
ONCLUSIONS AND FUTURE WORK
We introduce DeepCV A, a novel deep multi-task learning
model, to tackle a new task of commit-level SV assessment.DeepCV A promptly informs practitioners about the CVSSseverity level, exploitability, and impact of SVs in codechanges after they are committed, enabling more timely andinformed remediation. DeepCV A substantially outperformedmany baselines (even the ones enhanced with rebalanced data)for the seven commit-level SV assessment tasks. Notably,multi-task learning utilizing the relationship of assessmenttasks helped our model be 24.4% more effective and 6.3 timesmore efÔ¨Åcient than single-task models. With the reported per-formance, DeepCV A realizes the Ô¨Årst promising step towardsa holistic solution to assessing SVs as early as they appear.
We plan to extend DeepCV A to other programming lan-
guages and different SV assessment metrics to make the modeleven more practical for developers. We also aim to investigateDeepCV A for SV detection and Ô¨Åxing tasks to provide an all-in-one solution for practitioners to detect, assess and Ô¨Åx SVs.
A
CKNOWLEDGMENTS
The work was supported by the Cyber Security Research
Centre Limited whose activities are partially funded by theAustralian Government‚Äôs Cooperative Research Centres Pro-gramme. This work was supported with supercomputing re-sources provided by the Phoenix HPC service at the Uni-versity of Adelaide. We would also like to sincerely thankthe members from the Centre for Research on EngineeringSoftware Technologies (CREST), Faheem, Chadni, Bushra,Mubin and Huaming, as well as the anonymous reviewers forthe insightful and constructive comments to improve the paper.
R
EFERENCES
[1] S. Khan and S. Parkinson, ‚ÄúReview into state of the art of vulnerability
assessment using artiÔ¨Åcial intelligence,‚Äù in Guide to Vulnerability Anal-
ysis for Computer Networks and Systems. Springer, 2018, pp. 3‚Äì32.
[2] V . Smyth, ‚ÄúSoftware vulnerability management: how intelligence helps
reduce the risk,‚Äù Network Security, vol. 2017, no. 3, pp. 10‚Äì12, 2017.
[3] FIRST, ‚ÄúCommon vulnerability scoring system.‚Äù [Online]. Available:
https://www.Ô¨Årst.org/cvss/
[4] A. Feutrill, D. Ranathunga, Y . Yarom, and M. Roughan, ‚ÄúThe effect of
common vulnerability scoring system metrics on vulnerability exploit
delay,‚Äù in 2018 Sixth International Symposium on Computing and
Networking (CANDAR). IEEE, 2018, pp. 1‚Äì10.
[5] A. LamkanÔ¨Å, S. Demeyer, E. Giger, and B. Goethals, ‚ÄúPredicting the
severity of a reported bug,‚Äù in 2010 7th IEEE Working Conference on
Mining Software Repositories (MSR 2010). IEEE, 2010, pp. 1‚Äì10.
[6] Z. Han, X. Li, Z. Xing, H. Liu, and Z. Feng, ‚ÄúLearning to predict severity
of software vulnerability using only vulnerability description,‚Äù in 2017
IEEE International conference on software maintenance and evolution(ICSME). IEEE, 2017, pp. 125‚Äì136.
[7] G. Spanos and L. Angelis, ‚ÄúA multi-target approach to estimate software
vulnerability characteristics and severity scores,‚Äù Journal of Systems and
Software, vol. 146, pp. 152‚Äì166, 2018.
727[8] T. H. M. Le, B. Sabir, and M. A. Babar, ‚ÄúAutomated software vulnerabil-
ity assessment with concept drift,‚Äù in 2019 IEEE/ACM 16th International
Conference on Mining Software Repositories (MSR), 2019, pp. 371‚Äì382.
[9] T. H. Le, H. Chen, and M. A. Babar, ‚ÄúA survey on data-driven
software vulnerability assessment and prioritization,‚Äù arXiv preprint
arXiv:2107.08364, 2021.
[10] NIST, ‚ÄúNational vulnerability database.‚Äù [Online]. Available: https:
//nvd.nist.gov/
[11] A. Meneely, H. Srinivasan, A. Musa, A. R. Tejeda, M. Mokary,
and B. Spates, ‚ÄúWhen a patch goes bad: Exploring the properties
of vulnerability-contributing commits,‚Äù in 2013 ACM/IEEE Interna-
tional Symposium on Empirical Software Engineering and Measurement.IEEE, 2013, pp. 65‚Äì74.
[12] T. Hoang, H. K. Dam, Y . Kamei, D. Lo, and N. Ubayashi, ‚ÄúDeepjit: an
end-to-end deep learning framework for just-in-time defect prediction,‚Äùin2019 IEEE/ACM 16th International Conference on Mining Software
Repositories (MSR). IEEE, 2019, pp. 34‚Äì45.
[13] Y . Kamei, E. Shihab, B. Adams, A. E. Hassan, A. Mockus, A. Sinha,
and N. Ubayashi, ‚ÄúA large-scale empirical study of just-in-time qualityassurance,‚Äù IEEE Transactions on Software Engineering, vol. 39, no. 6,
pp. 757‚Äì773, 2012.
[14] H. Perl, S. Dechand, M. Smith, D. Arp, F. Yamaguchi, K. Rieck, S. Fahl,
and Y . Acar, ‚ÄúVccÔ¨Ånder: Finding potential vulnerabilities in open-sourceprojects to assist code audits,‚Äù in Proceedings of the 22nd SIGSAC ACM
Conference on Computer and Communications Security, 2015, pp. 426‚Äì437.
[15] L. Yang, X. Li, and Y . Y u, ‚ÄúVuldigger: A just-in-time and cost-aware tool
for digging vulnerability-contributing changes,‚Äù in GLOBECOM 2017-
2017 IEEE Global Communications Conference. IEEE, 2017, pp. 1‚Äì7.
[16] L. G. A. Rodriguez, J. S. Trazzi, V . Fossaluza, R. Campiolo, and
D. M. Batista, ‚ÄúAnalysis of vulnerability disclosure delays from thenational vulnerability database,‚Äù in Anais do I Workshop de Seguranc ¬∏a
Cibern ¬¥etica em Dispositivos Conectados. SBC, 2018.
[17] A. D. Sawadogo, T. F. Bissyand ¬¥e, N. Moha, K. Allix, J. Klein, L. Li,
and Y . L. Traon, ‚ÄúLearning to catch security patches,‚Äù arXiv preprint
arXiv:2001.09148, 2020.
[18] F. Thung, D. Lo, L. Jiang, F. Rahman, P . T. Devanbu et al., ‚ÄúWhen would
this bug get reported?‚Äù in 2012 28th IEEE International Conference on
Software Maintenance (ICSM). IEEE, 2012, pp. 420‚Äì429.
[19] A. Bosu and J. C. Carver, ‚ÄúPeer code review in open source communities
using reviewboard,‚Äù in Proceedings of the ACM 4th Annual Workshop on
Evaluation and Usability of Programming Languages and Tools , 2012,
pp. 17‚Äì24.
[20] P . Thongtanunam, S. McIntosh, A. E. Hassan, and H. Iida, ‚ÄúInvestigating
code review practices in defective Ô¨Åles: An empirical study of theqt system,‚Äù in 2015 IEEE/ACM 12th Working Conference on Mining
Software Repositories. IEEE, 2015, pp. 168‚Äì179.
[21] K. Moran, M. Linares-V ¬¥asquez, C. Bernal-C ¬¥ardenas, and D. Poshy-
vanyk, ‚ÄúAuto-completing bug reports for android applications,‚Äù in Pro-
ceedings of the 2015 10th Joint Meeting on F oundations of SoftwareEngineering, 2015, pp. 673‚Äì686.
[22] T. Hoang, J. Lawall, Y . Tian, R. J. Oentaryo, and D. Lo, ‚ÄúPatchnet:
Hierarchical deep learning-based stable patch identiÔ¨Åcation for the linuxkernel,‚Äù IEEE Transactions on Software Engineering, 2019.
[23] Y . Zhang and Q. Yang, ‚ÄúA survey on multi-task learning,‚Äù arXiv preprint
arXiv:1707.08114, 2017.
[24] S. Chowdhuri, T. Pankaj, and K. Zipser, ‚ÄúMultinet: Multi-modal multi-
task learning for autonomous driving,‚Äù in 2019 IEEE Winter Conference
on Applications of Computer Vision . IEEE, 2019, pp. 1496‚Äì1504.
[25] Authors, ‚ÄúReproduction package.‚Äù [Online]. Available: https://github.
com/lhmtriet/DeepCV A
[26] R. Future, ‚ÄúExploiting old vulnerabilities.‚Äù [Online]. Available:
https://www.recordedfuture.com/exploiting-old-vulnerabilities/
[27] A. Sabetta and M. Bezzi, ‚ÄúA practical approach to the automatic
classiÔ¨Åcation of security-relevant commits,‚Äù in 2018 IEEE International
Conference on Software Maintenance and Evolution (ICSME). IEEE,2018, pp. 579‚Äì582.
[28] E. Sahal and A. Tosun, ‚ÄúIdentifying bug-inducing changes for code addi-
tions,‚Äù in Proceedings of the 12th ACM/IEEE International Symposium
on Empirical Software Engineering and Measurement, 2018, pp. 1‚Äì2.
[29] H. Tian, K. Liu, A. K. Kabor ¬¥e, A. Koyuncu, L. Li, J. Klein, and
T. F. Bissyand ¬¥e, ‚ÄúEvaluating representation learning of code changes for
predicting patch correctness in program repair,‚Äù in 2020 35th IEEE/ACMInternational Conference on Automated Software Engineering (ASE).IEEE, 2020, pp. 981‚Äì992.
[30] Y . Li, S. Wang, and T. N. Nguyen, ‚ÄúDlÔ¨Åx: Context-based code trans-
formation learning for automated program repair,‚Äù in Proceedings of
the ACM/IEEE 42nd International Conference on Software Engineering,2020, pp. 602‚Äì614.
[31] U. Alon, M. Zilberstein, O. Levy, and E. Yahav, ‚Äúcode2vec: Learning
distributed representations of code,‚Äù Proceedings of the ACM on Pro-
gramming Languages, vol. 3, no. POPL, pp. 1‚Äì29, 2019.
[32] Y . Kim, ‚ÄúConvolutional neural networks for sentence classiÔ¨Åcation,‚Äù in
Proceedings of the 2014 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP), 2014, pp. 1746‚Äì1751.
[33] V . Nair and G. E. Hinton, ‚ÄúRectiÔ¨Åed linear units improve restricted
boltzmann machines,‚Äù in Icml, 2010.
[34] D. Bahdanau, K. Cho, and Y . Bengio, ‚ÄúNeural machine translation by
jointly learning to align and translate,‚Äù arXiv preprint arXiv:1409.0473,
2014.
[35] S. Ruder, ‚ÄúAn overview of gradient descent optimization algorithms,‚Äù
arXiv preprint arXiv:1609.04747, 2016.
[36] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, ‚ÄúLearning repre-
sentations by back-propagating errors,‚Äù Nature, vol. 323, no. 6088, pp.
533‚Äì536, 1986.
[37] S. E. Ponta, H. Plate, A. Sabetta, M. Bezzi, and C. Dangremont,
‚ÄúA manually-curated dataset of Ô¨Åxes to vulnerabilities of open-sourcesoftware,‚Äù in 2019 IEEE/ACM 16th International Conference on Mining
Software Repositories (MSR). IEEE, 2019, pp. 383‚Äì387.
[38] S. McIntosh and Y . Kamei, ‚ÄúAre Ô¨Åx-inducing changes a moving target?
a longitudinal case study of just-in-time defect prediction,‚Äù IEEE Trans-
actions on Software Engineering, vol. 44, no. 5, pp. 412‚Äì428, 2017.
[39] J. ¬¥Sliwerski, T. Zimmermann, and A. Zeller, ‚ÄúWhen do changes induce
Ô¨Åxes?‚Äù ACM SIGSOFT Software Engineering Notes, vol. 30, no. 4, pp.
1‚Äì5, 2005.
[40] C. Rezk, Y . Kamei, and S. Mcintosh, ‚ÄúThe ghost commit problem when
identifying Ô¨Åx-inducing changes: An empirical study of apache projects,‚ÄùIEEE Transactions on Software Engineering, 2021.
[41] W. G. Cochran, Sampling techniques. John Wiley & Sons, 2007.
[42] M. L. McHugh, ‚ÄúInterrater reliability: the kappa statistic,‚Äù Biochemia
medica, vol. 22, no. 3, pp. 276‚Äì282, 2012.
[43] H. Hata, C. Treude, R. G. Kula, and T. Ishio, ‚Äú9.6 million links in source
code comments: Purpose, evolution, and decay,‚Äù in 2019 IEEE/ACM 41st
International Conference on Software Engineering (ICSE). IEEE, 2019,pp. 1211‚Äì1221.
[44] Y . Fan, X. Xia, D. A. Da Costa, D. Lo, A. E. Hassan, and S. Li, ‚ÄúThe
impact of changes mislabeled by szz on just-in-time defect prediction,‚ÄùIEEE Transactions on Software Engineering, 2019.
[45] D. Falessi, J. Huang, L. Narayana, J. F. Thai, and B. Turhan, ‚ÄúOn the
need of preserving order of data when validating within-project defectclassiÔ¨Åers,‚Äù Empirical Software Engineering, vol. 25, no. 6, pp. 4805‚Äì
4830, 2020.
[46] M. Jimenez, R. Rwemalika, M. Papadakis, F. Sarro, Y . Le Traon, and
M. Harman, ‚ÄúThe importance of accounting for real-world labellingwhen predicting software vulnerabilities,‚Äù in Proceedings of the 2019
27th ACM Joint Meeting on European Software Engineering Conferenceand Symposium on the F oundations of Software Engineering , 2019, pp.
695‚Äì705.
[47] S. Raschka, ‚ÄúModel evaluation, model selection, and algorithm selection
in machine learning,‚Äù arXiv preprint arXiv:1811.12808, 2018.
[48] A. Luque, A. Carrasco, A. Mart ¬¥ƒ±n, and A. de las Heras, ‚ÄúThe impact
of class imbalance in classiÔ¨Åcation performance metrics based on thebinary confusion matrix,‚Äù Pattern Recognition , vol. 91, pp. 216‚Äì231,
2019.
[49] J. Gorodkin, ‚ÄúComparing two k-category assignments by a k-category
correlation coefÔ¨Åcient,‚Äù Computational Biology and Chemistry, vol. 28,
no. 5-6, pp. 367‚Äì374, 2004.
[50] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‚ÄúBert: Pre-training
of deep bidirectional transformers for language understanding,‚Äù arXiv
preprint arXiv:1810.04805, 2018.
[51] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever,
‚ÄúLanguage models are unsupervised multitask learners,‚Äù OpenAI blog,
vol. 1, no. 8, p. 9, 2019.
[52] M. Pradel and K. Sen, ‚ÄúDeepbugs: A learning approach to name-based
bug detection,‚Äù Proceedings of the ACM on Programming Languages,
vol. 2, no. OOPSLA, pp. 1‚Äì25, 2018.
728[53] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean, ‚ÄúDistributed
representations of words and phrases and their compositionality,‚Äù arXiv
preprint arXiv:1310.4546, 2013.
[54] P . Bojanowski, E. Grave, A. Joulin, and T. Mikolov, ‚ÄúEnriching word
vectors with subword information,‚Äù Transactions of the Association for
Computational Linguistics, vol. 5, pp. 135‚Äì146, 2017.
[55] D. P . Kingma and J. Ba, ‚ÄúAdam: A method for stochastic optimization,‚Äù
arXiv preprint arXiv:1412.6980, 2014.
[56] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhut-
dinov, ‚ÄúDropout: a simple way to prevent neural networks from over-
Ô¨Åtting,‚Äù The Journal of Machine Learning Research, vol. 15, no. 1, pp.
1929‚Äì1958, 2014.
[57] S. Ioffe and C. Szegedy, ‚ÄúBatch normalization: Accelerating deep
network training by reducing internal covariate shift,‚Äù in International
Conference on Machine Learning. PMLR, 2015, pp. 448‚Äì456.
[58] S. Lloyd, ‚ÄúLeast squares quantization in pcm,‚Äù IEEE Transactions on
Information Theory, vol. 28, no. 2, pp. 129‚Äì137, 1982.
[59] Y . Zhou and A. Sharma, ‚ÄúAutomated identiÔ¨Åcation of security issues
from commit messages and bug reports,‚Äù in Proceedings of the 2017
11th Joint Meeting on F oundations of Software Engineering, 2017, pp.914‚Äì919.
[60] D. Shen, G. Wang, W. Wang, M. R. Min, Q. Su, Y . Zhang, C. Li,
R. Henao, and L. Carin, ‚ÄúBaseline needs more love: On simple word-embedding-based models and associated pooling mechanisms,‚Äù arXiv
preprint arXiv:1805.09843, 2018.
[61] T. Chen and C. Guestrin, ‚ÄúXgboost: A scalable tree boosting system,‚Äù
inProceedings of the 22nd ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, 2016, pp. 785‚Äì794.
[62] G. Ke, Q. Meng, T. Finley, T. Wang, W. Chen, W. Ma, Q. Ye, and T.-
Y . Liu, ‚ÄúLightgbm: A highly efÔ¨Åcient gradient boosting decision tree,‚ÄùAdvances in Neural Information Processing Systems, vol. 30, pp. 3146‚Äì3154, 2017.
[63] T. H. M. Le, D. Hin, R. Croft, and M. A. Babar, ‚ÄúPuminer: Mining
security posts from developer question and answer websites with pulearning,‚Äù in Proceedings of the 17th International Conference on
Mining Software Repositories, 2020, pp. 350‚Äì361.
[64] F. Wilcoxon, ‚ÄúIndividual comparisons by ranking methods,‚Äù in Break-
throughs in Statistics. Springer, 1992, pp. 196‚Äì202.
[65] T. H. Le, H. Chen, and M. A. Babar, ‚ÄúDeep learning for source code
modeling and generation: Models, applications, and challenges,‚Äù ACM
Computing Surveys (CSUR), vol. 53, no. 3, pp. 1‚Äì38, 2020.
[66] G. Lin, J. Zhang, W. Luo, L. Pan, Y . Xiang, O. De V el, and P . Montague,
‚ÄúCross-project transfer representation learning for vulnerable functiondiscovery,‚Äù IEEE Transactions on Industrial Informatics, vol. 14, no. 7,
pp. 3289‚Äì3297, 2018.
[67] C. Tantithamthavorn, A. E. Hassan, and K. Matsumoto, ‚ÄúThe impact of
class rebalancing techniques on the performance and interpretation ofdefect prediction models,‚Äù IEEE Transactions on Software Engineering,
vol. 46, no. 11, pp. 1200‚Äì1219, 2018.
[68] Z. Li, D. Zou, J. Tang, Z. Zhang, M. Sun, and H. Jin, ‚ÄúA comparative
study of deep learning-based vulnerability detection system,‚Äù IEEE
Access, vol. 7, pp. 103 184‚Äì103 197, 2019.
[69] N. V . Chawla, K. W. Bowyer, L. O. Hall, and W. P . Kegelmeyer, ‚ÄúSmote:
synthetic minority over-sampling technique,‚Äù Journal of ArtiÔ¨Åcial Intel-
ligence Research, vol. 16, pp. 321‚Äì357, 2002.
[70] W. Zheng, J. Gao, X. Wu, F. Liu, Y . Xun, G. Liu, and X. Chen,
‚ÄúThe impact factors on the performance of machine learning-basedvulnerability detection: A comparative study,‚Äù Journal of Systems and
Software, vol. 168, p. 110659, 2020.
[71] S. Wattanakriengkrai, P . Thongtanunam, C. Tantithamthavorn, H. Hata,
and K. Matsumoto, ‚ÄúPredicting defective lines using a model-agnostictechnique,‚Äù arXiv preprint arXiv:2009.03612, 2020.
[72] L. Pascarella, F. Palomba, and A. Bacchelli, ‚ÄúFine-grained just-in-time
defect prediction,‚Äù Journal of Systems and Software, vol. 150, pp. 22‚Äì36,
2019.[73] T. Hoang, H. J. Kang, D. Lo, and J. Lawall, ‚ÄúCc2vec: Distributed
representations of code changes,‚Äù in Proceedings of the ACM/IEEE 42nd
International Conference on Software Engineering , 2020, pp. 518‚Äì529.
[74] X. Gu, H. Zhang, and S. Kim, ‚ÄúDeep code search,‚Äù in 2018 IEEE/ACM
40th International Conference on Software Engineering (ICSE). IEEE,2018, pp. 933‚Äì944.
[75] X. Hu, G. Li, X. Xia, D. Lo, and Z. Jin, ‚ÄúDeep code comment gener-
ation,‚Äù in 2018 IEEE/ACM 26th International Conference on Program
Comprehension (ICPC). IEEE, 2018, pp. 200‚Äì210.
[76] T. H. M. Le, R. Croft, D. Hin, and M. A. Babar, ‚ÄúA large-scale study of
security vulnerability support on developer q&a websites,‚Äù in Evaluation
and Assessment in Software Engineering, 2021, pp. 109‚Äì118.
[77] M. Bozorgi, L. K. Saul, S. Savage, and G. M. V oelker, ‚ÄúBeyond
heuristics: learning to classify vulnerabilities and predict exploits,‚Äù inProceedings of the 16th ACM SIGKDD International Conference onKnowledge Discovery and Data Mining, 2010, pp. 105‚Äì114.
[78] S. Neuhaus and T. Zimmermann, ‚ÄúSecurity trend analysis with cve
topic models,‚Äù in 2010 IEEE 21st International Symposium on Software
Reliability Engineering. IEEE, 2010, pp. 111‚Äì120.
[79] G. Spanos, L. Angelis, and D. Toloudis, ‚ÄúAssessment of vulnerability
severity using text mining,‚Äù in Proceedings of the 21st Pan-Hellenic
Conference on Informatics, 2017, pp. 1‚Äì6.
[80] B. L. Bullough, A. K. Yanchenko, C. L. Smith, and J. R. Zipkin,
‚ÄúPredicting exploitation of disclosed software vulnerabilities using open-source data,‚Äù in Proceedings of the 3rd ACM on International Workshop
on Security and Privacy Analytics, 2017, pp. 45‚Äì53.
[81] C. Elbaz, L. Rilling, and C. Morin, ‚ÄúFighting n-day vulnerabilities with
automated cvss vector prediction at disclosure,‚Äù in Proceedings of the
15th International Conference on Availability, Reliability and Security,2020, pp. 1‚Äì10.
[82] X. Gong, Z. Xing, X. Li, Z. Feng, and Z. Han, ‚ÄúJoint prediction
of multiple vulnerability characteristics through multi-task learning,‚Äùin2019 24th International Conference on Engineering of Complex
Computer Systems (ICECCS). IEEE, 2019, pp. 31‚Äì40.
[83] S. E. Ponta, H. Plate, and A. Sabetta, ‚ÄúBeyond metadata: Code-centric
and usage-based analysis of known vulnerabilities in open-source soft-ware,‚Äù in 2018 IEEE International Conference on Software Maintenance
and Evolution (ICSME). IEEE, 2018, pp. 449‚Äì460.
[84] ‚Äî‚Äî, ‚ÄúDetection, assessment and mitigation of vulnerabilities in open
source dependencies,‚Äù Empirical Software Engineering, vol. 25, no. 5,
pp. 3175‚Äì3215, 2020.
[85] X. Yang, D. Lo, X. Xia, Y . Zhang, and J. Sun, ‚ÄúDeep learning for just-
in-time defect prediction,‚Äù in 2015 IEEE International Conference on
Software Quality, Reliability and Security. IEEE, 2015, pp. 17‚Äì26.
[86] F. Camilo, A. Meneely, and M. Nagappan, ‚ÄúDo bugs foreshadow
vulnerabilities? a study of the chromium project,‚Äù in 2015 IEEE/ACM
12th Working Conference on Mining Software Repositories. IEEE,2015, pp. 269‚Äì279.
[87] F. Peters, T. T. Tun, Y . Y u, and B. Nuseibeh, ‚ÄúText Ô¨Åltering and ranking
for security bug report prediction,‚Äù IEEE Transactions on Software
Engineering, vol. 45, no. 6, pp. 615‚Äì631, 2017.
[88] M. Gegick, P . Rotella, and T. Xie, ‚ÄúIdentifying security bug reports
via text mining: An industrial case study,‚Äù in 2010 7th IEEE Working
Conference on Mining Software Repositories (MSR 2010) . IEEE, 2010,
pp. 11‚Äì20.
[89] A. Bosu, J. C. Carver, M. HaÔ¨Åz, P . Hilley, and D. Janni, ‚ÄúIdentifying
the characteristics of vulnerable code changes: An empirical study,‚Äù inProceedings of the 22nd ACM SIGSOFT International Symposium onF oundations of Software Engineering, 2014, pp. 257‚Äì268.
[90] X. Chen, Y . Zhao, Z. Cui, G. Meng, Y . Liu, and Z. Wang, ‚ÄúLarge-
scale empirical studies on effort-aware security vulnerability predictionmethods,‚Äù IEEE Transactions on Reliability, vol. 69, no. 1, pp. 70‚Äì87,
2019.
729