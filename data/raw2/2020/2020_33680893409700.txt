AMS:GeneratingAutoMLSearchSpacesfromWeakSpecifications
José
P.Cambronero
MassachusettsInstituteofTechnology
Cambridge,U.S.A.
jcamsan@mit.eduJürgenCito
TUWien
Vienna,Austria
MassachusettsInstituteofTechnology
Cambridge,U.S.A
juergen.cito@tuwien.ac.atMartinC.Rinard
MassachusettsInstituteofTechnology
Cambridge,U.S.A.
rinard@csail.mit.edu
ABSTRACT
Weconsiderausagemodelforautomatedmachinelearning(Au-
toML)inwhichuserscaninfluencethegeneratedpipelinebyprovid-
ingaweakpipelinespecification :anunorderedsetofAPIcomponents
fromwhichtheAutoMLsystemdrawsthecomponentsitplacesinto
thegeneratedpipeline.Suchspecificationsallowuserstoexpress
preferencesoverthecomponentsthatappearinthepipeline,forex-
ampleadesireforinterpretablecomponentstoappearinthepipeline.
We present AMS, an approach to automatically strengthen weak
specificationstoincludeunspecifiedcomplementaryandfunction-
allyrelatedAPIcomponents,populatethespaceofhyperparameters
andtheirvalues,andpairthisconfigurationwithasearchprocedure
toproducea strongpipelinespecification :afulldescriptionofthe
searchspaceforcandidatepipelines.AMSusesnormalizedpointwise
mutualinformationonacodecorpustoidentifycomplementary
components,BM25asalexicalsimilarityscoreoverthetargetAPI’s
documentation to identify functionally related components, and
frequencydistributionsinthecodecorpustoextractkeyhyperpa-
rametersandvalues.Weshowthatstrengthenedspecificationscan
producepipelinesthatoutperformthepipelinesgeneratedfromthe
initialweakspecificationandanexpert-annotatedvariant,while
producingpipelinesthatstillreflecttheuserpreferencescaptured
intheoriginalweakspecification.
CCSCONCEPTS
·Softwareanditsengineering →Search-basedsoftwareengi-
neering;Automaticprogramming ;·Theoryofcomputation
→Programspecifications.
KEYWORDS
automated machine learning, program mining, search-based soft-
ware engineering
ACM Reference Format:
José P. Cambronero, Jürgen Cito, and Martin C. Rinard. 2020. AMS: Gen-
erating AutoML Search Spaces from Weak Specifications. In Proceedings
of the 28th ACM Joint European Software Engineering Conference and Sym-
posium on the Foundations of Software Engineering (ESEC/FSE ’20), Novem-
ber 8ś13, 2020, Virtual Event, USA. ACM, New York, NY, USA, 12 pages.
https://doi.org/10.1145/3368089.3409700
ESEC/FSE ’20, November 8ś13, 2020, Virtual Event, USA
© 2020 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-7043-1/20/11.
https://doi.org/10.1145/3368089.34097001 INTRODUCTION
Automatedmachinelearning(AutoML)[ 15,23,27,37]promisesto
democratizetheuseofmachinelearningtechniquesbyendusers,
allowing non-experts access to a tool that has become standard
fortacklingresearchandapplicationsacrossdomainsasdiverseas
medicine,finance,andsoftwareengineering[ 17,29,34,36].AutoML
toolstypicallytakeasinputatabulardatasetalongwithaclassifica-
tionorregressiontask(i.e.predictaparticularcolumn)andgenerate
an optimizedcomposition of machine learning operators, drawn
fromatargetAPI,toproduceanexecutablepipeline.
AtthecoreofAutoMLtoolsliesearchproceduresthatgenerate
andevaluatepossiblepipelinecandidates.Undertheprevailingus-
agemodel[ 48],theendusertreatstheAutoMLtoolasablackbox
whichproducesapipelinethatoutperformsothergeneratedcandi-
datesbasedonsomepredictiveperformancemetric(e.g.F1score).
In this setting the end user has no direct way of influencing the
pipelinechosenbythesystem.However,ausermayneedtoexpress
preferences,beyondmaximizingapredictiveperformancemetric,to
satisfyconstraintssuchaspipelineinterpretability,domain-specific
bestpractices,anddatascalingconstraints,forexample.
Weproposetheuseofa weakpipelinespecification asawayto
providepartialuserpreferencestotheAutoMLtool.Aweakpipeline
specificationconsistsofanunorderedsetofAPIcomponentsthat
theendusermaywanttoappearintheresultingpipeline.Thisspec-
ificationcanbeautomaticallyextendedtoproducea strongpipeline
specification thatcapturesadditionalAPIcomponentsofinterest,de-
finesasetofhyperparametersandvaluestosearchover,andasearch
proceduretosamplecandidatepipelines.Thestrengthenedpipeline
specificationcantheninfluencetheoutputpipelineproducedbythe
AutoMLtoolbyconstrainingthesearchspace.
Forexample,theusermightprovidetheScikit-Learncomponent
{ LogisticRegression } as a weak specification. Strengthening this
specificationcouldaddotherlinearmodels(e.g.linearSVM),would
specifydifferenttypesofregularization(e.g.L1/L2)andtheirweights,
andwouldincludethesearchprocedure(e.g.geneticprogramming)
usedtosamplepipelines.Thisproposedmodelofinteractionallows
theendusergreatercontrolovertheeventualoutputpipeline,with-
outnegatingthekeyadvantageofAutoML:theuserneednotbean
MLexpert.
WeintroduceAMS(Figure 1),asystemthatautomaticallystrength-
ensAutoMLsearchspacespecifications.Tocarryoutthisstrength-
ening, AMS exploits information in a code corpus and the target
API’sdocumentation.First,AMSautomaticallyminespairsofcom-
plementaryAPIcomponentsfromtheselectedcodecorpus,where
twocomponentsarecomplementaryiftheyco-occurfrequently.To
formalizethisminingprocedure,AMSusesnormalizedpointwise
763This work is licensed under a Creative Commons Attribution International 4.0 License. 
ESEC/FSE’20,November8–13,2020,VirtualEvent,USA JoséP.Cambronero,JürgenCito,andMartinC.Rinard
Search
Procedure
Weak
speciﬁcation
Optimized
PipelineDatasetFunctionally related
API
componentsHyperparameters
and 
values
Code corpus
Complementary
API
components
API documentationStrong
speciﬁcation
Search
Figure 1: AMS system diagram. System boundaries are
depicted as a dashed line. The user provides a weak specifi-
cation, which is automatically extended by AMS to include
complementary, functionally related API components, and
keyhyperparametersandasetofpossiblevalues.
mutualinformation[ 5]torigorouslycharacterizeco-occurrencein
probabilisticterms.Theseminedassociationscanthenbeusedto
extendtheinitialweakspecification.Next,AMSidentifiesunspeci-
fiedAPIcomponentsthatmaybefunctionallyrelatedtothoseinthe
originalspecification.Toreasonaboutcomponentsimilarity,AMS
appliesBM25[ 42],apopularandeffectivemeasureoflexicalsimilar-
ity,overtheAPI’sdocumentation.Withthismetric,AMScanidentify
componentswiththehighestdegreeofrelationtothoseintheorig-
inalspecification.Next,giventhatmachinelearningpipelinesare
knowntoexhibitdifferentperformancebasedonhyperparameter
values[40],AMSusesfrequencydistributions,estimatedfromthe
selectedcodecorpus,todefineahyperparametersearchspacefor
eachcomponentintheextendedspecification.Finally,AMSpairs
this component configuration with a search procedure, which is
usedtosamplecandidatepipelinesfromthegivenspace.
WeempiricallyevaluateAMS’spredictiveperformance,interms
ofmacro-averagedF1score,over9datasetsand15weakpipeline
specifications.Ourresultsshowthat,withtwodifferentsearchpro-
cedures,AMSproducespipelinesthatoutperformthepipelinesob-
tainedusingtheinitialweakspecificationandanexpert-annotated
versionoftheweakspecificationincludinghyperparametersand
values.Toquantifytheextentofoutperformance,weusetheconcept
ofawin.Apipelinewinswhenitobtainsthehighestscoreonaspec-
ification/datasetcombination,andsatisfiesaminimumpredictive
scoredifferencetoruleoutcomparablescores.
Whenusinggeneticprogrammingasasearchprocedure,AMS’s
specificationsresultin38winscomparedto12undertheweakspeci-
ficationextendedwithanexperthyperparameterspace.Whenusing
randomsearchasasearchprocedure,AMS’sspecificationsresult
in41wins comparedto 14wins under the weakspecificationex-
tendedwithanexperthyperparameterspace.Wealsofindthatthe
pipelinesproducedusingAMS’sspecificationqualitativelyreflect
theinfluenceoftheweakspecification.
Tosummarize,thispapermakesthefollowingcontributions:
•We present a novel approach to automatically strengthen
pipelinespecificationsforAutoMLtools,allowingusersto
influencethefinalgeneratedpipeline.Ourapproachreliesonaprobabilisticcharacterizationofco-occurringAPIcom-
ponents,lexicalsimilarityoveralternativecomponents’API
documentation,andfrequencydistributionsforhyperparam-
eterspaces.
•Weimplementthisapproach(andshareourevaluationdataset)
inanopen-sourcesystemcalledAMS1.
•WeevaluateAMSusing9datasetsfromanexistingAutoML
paper[37],15weakpipelinespecifications,andtwodifferent
searchprocedures.OurresultsshowthatAMS’sstrengthened
specificationsproducehigherperformingpipelines.When
usinggeneticprogramming,AMSspecificationsproduce38
winscomparedto12fromanexpert-annotatedvariantofthe
weakspecification,and9fromtheoriginalweakspecification.
Weseeasimilarnumberofwinswhencomparingapproaches
usingrandomsearch.
•Wequalitativelyshowthatthedistributionofcomponents
intheoutputpipelinesproducedusingAMSspecifications
reflecttheinfluenceoftheinitialweakspecification.
InthefollowingsectionswereviewthebackgroundonAutoML
(Section2),introducethenotionofpipelinespecifications(Section 3),
provideanillustrativescenariofortheuseofAMS(Section 4),detail
theapproachanddesignofAMS(Section 5),presentexperimentalre-
sults(Section 6),providecontextonrelatedwork(Section 7),outline
possiblethreats(Section 8)andconclude(Section 9).
2 AUTOMLBACKGROUND
WefirstformallyintroduceAutoMLforclassification[ 27].Letd∈D:
Rn×m×Nnbeadatasetcomprisedofamatrixof nobservations,each
withmrealcovariates,andavectorof nnaturalnumberlabels.Let
H:Rn×m→Nnbethetypeofapipelineprogramdefinedasacom-
positionofpreprocessingandlearningalgorithmsśimplementedas
APIcomponentsinatargetlibraryśalongwiththeircorresponding
hyperparameter settings. A pipeline takes a dataset and predicts
labelsbasedoncovariates.Let Sbethesearchspaceofallpossible
pipelineprograms.Let e∈E:H×D→Rbeanevaluationfunction
thatscorestheabilityofapipelinetosuccessfullypredictlabelsand
generalizetounseenobservations(e.g.cross-validatedF1score).Let
cost∈C:H×D→Rbe a cost function that evaluates pipeline
executiontimeonadataset d,andb∈Rbeasearchtimebudget.
ThenAutoMLcorrespondstotheoptimizationproblem
argmax
h∈Se(h,d)s.t./summationdisplay
h∈Scost(h,d)≤b
Giventhatthepossiblespaceofpipelines Sisexponentiallylarge,
it is impractical to evaluate every pipeline in Swithin the given
budgetb.EffectiveAutoMLsystemswillthereforetypicallyhaveto
performasearchoverthespace,evaluatingonlyasubsetofpipelines.
TheAutoMLsystemiterativelysearchesandevaluatespipelinesin
S,keepingtrackofitsestimateofthebestpipeline.
ExistingAutoMLsystemsemployavarietyofsearchstrategies
to identify candidate pipelines. These strategies include genetic
programming[ 37],Bayesianoptimization[ 15],reinforcementlearn-
ing[11],program-analysis-basedsearch[ 7],andrandomsearch[ 18].
1https://github.com/josepablocam/ams
764AMS:GeneratingAutoMLSearchSpacesfromWeakSpecifications ESEC/FSE’20,November8–13,2020,VirtualEvent,USA
Toimprovetheeffectivenessofthesestrategies,systemsmayalso
incorporatepriorknowledgeaboutpipelineperformanceorimpose
additionalstructureonthecandidatepipelinesthatcanbegener-
ated.Forexample,anAutoMLsystemmaywarmstartthesearchby
incorporatingpreviouslysuccessfulpipelines[ 15,47],conditionon
textualdatasetandalgorithmdescriptions[ 12],manuallyconstrain
thesubsetofalgorithmsavailableforpipelinedefinitions[ 37],or
constraintheshapeofpossiblepipelines[ 10,11].Thelattertwopro-
videwaystorestrictthetypeofpipelinesproduced,however,they
requireuserinvolvementandexpertise.Inthefollowingsection,we
presentanapproachthatbridgesthisgap.
3 PIPELINESPECIFICATIONS
ThecurrentusagemodelforAutoMLtypicallyemphasizesthelack
ofuserinvolvement[ 48].Underthismodel,theuserpresentsthetool
withtheirtargetdataset,forwhichtheywanttolearnaclassification
pipeline,setssomecomputationalbudget,runsthetool,andaccepts
thepipelineproducedbytheAutoMLtool.Inthiscontext,theAu-
toMLtoolreceivesnouserfeedback(beyondtheinputdataset),and
theuserisunabletoinfluencethepipelinesconsideredbythesearch
procedure.Withoutanyformaluserfeedback,theAutoMLtoolisun-
ableto1)exploitanyuserdomainknowledgeor2)provideapipeline
thatsatisfiesanydesireduserconstraints(e.g.interpretability).
MostexistingAutoMLtoolsconstructapipelinebycomposing
and configuring API components drawn from a higher-level ML
library,suchasScikit-Learn[ 39].
Definition3.1. APIComponent.AnAPIcomponent,inthecontext
ofourpaper,referstoapubliclibraryfunctionorclassthatcanbe
composedwithothercomponentstocreateanMLpipeline.Most
APIcomponentsprovideadditionalconfigurationoptionsthrough
theuseofoptional/defaultparameters.
Weproposetheuseof weakspecifications asawayforAutoML
userstoinfluencethepipelinesproducedbyautomaticallysubset-
tingtherelevantsetofAPIcomponents,thusconstrainingthesearch
spaceforcandidatesgeneratedbytheAutoMLtool.
Definition 3.2. Weak Specification. A weak specification is an
(unordered)setofAPIcomponents,atleastoneofwhichisaregres-
sor(ifperformingaregressiontask)oraclassifier(ifperforminga
classificationtask).
ByprovidingasetofAPIcomponentsauserprovides(partial)in-
formationregardingwhattheywant:specificallyasetofalgorithms
(e.g.classifiers,preprocessors)thatshouldbeconsideredforpipeline
generation.Wecallthistypeofspecification weakasitisincomplete
alongfourkeydimensions:
(1) itdoesnotspecifywhathyperparametersarerelevant
(2)itdoesnotspecifywhatvalueshyperparameterscantakeon
(3) otherrelevantAPIcomponentsmaybemissing
(4)itdoesnotspecifyanyorderorcompositionaloperatorsused
togeneratenewpipelinesfromthesecomponents
Providingaweakspecificationallowsausertoexertinfluence
onthefinalpipelineproduced,whileatthesametimenotrequir-
ingdeepAPIormachinelearningexpertise,astheydonothaveto
manuallydetailthecompletespace.Forexample,ausercanenforce
adegreeofinterpretabilityontheoptimizedpipelinebywritinga
specificationwithasinglelinearmodel(e.g.logisticregression).Definition3.3. StrongSpecification.Let hcbeamapfromasubset
ofhyperparametersforcomponent ctoacollectionofpossiblevalues.
LetCbeamapfromcomponent itoitsrespective hi.LetPbeasearch
proceduretogeneratecandidatepipelines.Astrongspecificationis
atripleoftheform ⟨C,(h1,...,hn),P⟩.
A strong specification, in effect, defines a search space for an
AutoMLtool.Weproposethatthisspacecanbederivedfromthe
weakspecification,whichexpresses(partial)userpreferences.
In the following sections, we detail our approach to automati-
callystrengtheningweakspecificationstoinfluencetheAutoML
search process. But first, we introduce an illustrative scenario to
demonstrateausecaseforAMS.
4 ILLUSTRATIVESCENARIO
Wefollowthejourneyofaforensicscientistwhoisnotamachine
learningexpertbutwantstoclassifyglassfragments[ 9,13].The
forensicscientisthasahighlevelunderstandingofdifferentlearn-
ingandpreprocessingalgorithmsbutisnotawareofthevarious
hyperparameters,possiblevalues,orothersuitablealgorithmsto
consider.ThescientisthasheardofAutoMLandthinksthismight
be a suitable tool to explore pipelines. However, they have clear
constraints:notree-basedensemblemodels,asthepipelinesneed
tobeeasilyinterpretable.Unfortunately,AutoMLtoolsareknown
to often produce tree-based ensemble models [ 14,20], which are
challengingtointerpret[ 21].
Theyspentsometimeontheinternetandfoundarelatedtutorial
thatdetailedaScikit-Learn[ 39]pipelinethatmayworkfortheiruse
case(case1inTable 1).
Thescientistwillusethisexamplepipeline(withnohyperparam-
etersorvalues)asaweakspecification.Toevaluatetheirprogress,
theywilluseanexistingclassificationdataset,łglassž[ 13],consist-
ingofcontinuousmeasurementsfor7typesofglass.Thescientist
performs a random 80/20 split for training/testing and evaluates
pipelinesusingmacro-averagedF1score.
Thescientiststartsbynaivelyrunningtheirspecificationdirectly
asapipelinewithdefaulthyperparameters,whichresultsinaninitial
F1scoreof 0.43.Nextthescientistusesthespecificationcomponents,
withdefaulthyperparameters,asaconfigurationfortheAutoML
toolTPOT[ 37],whichusesgeneticprogrammingtogeneratecan-
didate pipelines. Applying TPOT to the weak specification (with
nohyperparametersdefinedinthesearchspace)resultsinabetter
scoreof0.51.
Afterconsultingwithamachinelearningcolleague,thescientist
setsupadefinedhyperparameterspace(i.e.whichhyperparameters
totuneandsetofpossiblevalues)foreachcomponentinthespec-
ification.Thescientistthenappliesthesamegeneticprogramming
searchtothenewconfiguration,resultinginpipelinenumber3in
Table1.Notethattheshapeoftheoptimizedpipelineisthesameas
inthepriorstep,butnowtheregularizationpenaltyanditsweight
varies.Thisstepraisedtheirscoreto0.57.
Thescientistnowgoesbacktotheoriginalweakspecification
andusesAMStoautomaticallystrengthenthisweakspecification
(ratherthanmanuallyspecifyingthefullspace).AMSextendsthe
weakspecificationusingacodecorpusandtheAPI’sdocumentation.
ApplyingthesamesearchproceduretoAMS’sspecificationnowre-
sultsinthehighestscoreof 0.75.Thefinalpipelineretainspolynomial
765ESEC/FSE’20,November8–13,2020,VirtualEvent,USA JoséP.Cambronero,JürgenCito,andMartinC.Rinard
features,butreplacesthevariancethresholdselectorwithaselector
basedonaspecifiedfalsepositiverate.Thepipelinethenstacksa
SGDClassifier(withhingeloss)anduseslogisticregressionwithan
L1penalty(toproducesparsecoefficients).Thisembodiesthespirit
oftheinitiallygivenspecification,butsubstantiallyoutperformsthe
restoftheapproaches.
Table1:Summaryofscenarioiterationsbasedonthełglassž
dataset showing the progression of score improvements.
Notethatcomponentnamesareabbreviatedforbrevity.
#Pipeline Description Score
1PolyFeatures,
MinMaxScaler,
VarianceThreshold,
LogisticRegressionInitial (naive) weak specification as a
pipelinewithdefaulthyperparameters.0.43
2StackingEstimator(
LogisticRegression
),
LogisticRegressionApplying AutoML tool TPOT (Ge-
netic Programming) to the original
specification without defining any
hyperparameters.0.51
3StackEstimator(
LogisticRegression([
Penalty:L1,
Cost:10]),
LogisticRegressionSameas#2,butwithexpert-definedhy-
perparameter space for regularization
(cost)andpenalty.0.57
4PolyFeatures,
SelectFPR,
StackingEstimator(
SGDClassifier
[Loss:Hinge]),
LogisticRegression[
Penalty:L1,
Cost:100]Applying genetic programming to
the strong specification generated by
our approach (AMS) given the weak
specification.0.75
5 AMS
WeintroduceAMS,asystemthatautomaticallystrengthensweak
pipelinespecificationsusinganexistingcodecorpus,anAPI’sdocu-
mentation,andaplug-insearchprocedure.Figure 1showsadiagram
ofthesystem.AMStakestheuser’sweakspecificationasinput.The
systemfirstextendsthesetofAPIcomponentsconsideredinthe
specification.Toperformthisextension,AMSreliesonacodecorpus,
whichexercisesthetargetAPI,andontheAPI’snaturallanguage
documentation.Afterthespecificationhasbeenextended,AMSuses
thecodecorpustoidentifykeyhyperparametersfortheAPIcompo-
nentsinthespecificationandincludesetsofpossiblevaluestheycan
takeon.AMSthenpairsthissetofcomponentconfigurationswitha
searchproceduretoproduceastrongspecification.Thesearchpro-
cedurecanthenbeusedtoiterativelysampleandevaluatecandidate
pipelines,resultinginafinaloptimizedpipeline.
WenowpresentdetailsoneachstepintheAMSsystem.
5.1 Unspecified(butUseful)APIComponents
AMSfirstextendstheinitialspecificationwithadditionalcompo-
nents,whichtheusermaynothaveincluded.Givenaspecification
Sandanewcomponent c,cmaybeaddedto Sifitsatisfiesoneof
thefollowingtwoconditions: ciscommonlyusedwithacomponent
alreadyin S,orccouldreplaceacomponentalreadyin S.
Thegoalofthefirstconditionistoidentify complementarycom-
ponents.Forexample,ifaclassifierisoftenusedwithaparticular
preprocessingstep,wesaythesecomponentsarecomplementary.
Thegoalofthesecondconditionistoidentify functionallyrelatedcomponents,whicharealternativestoeachother.Forexample,two
differentlinearclassifierswouldbeconsideredfunctionallyrelated.
AMSreliesontwodifferentsourcesofinformationtoidentify
componentsthatsatisfyeachoftheseconditions.Wefirstaddress
complementarycomponents.
5.1.1 Complementary Components. To identify complementary
components,AMSexploitsinformationfromacrowd-sourcedcor-
pusofscripts,whichexercisethetargetAPI.Eachscriptinthecorpus
waswrittentotargetasingledataset,thereforetwocomponents
usedinthesamescriptmaybecomplementary.Byusingacodecor-
pustoidentifysuchcomponents,AMScanautomaticallyproduce
andupdateitsinventoryofcomplementarycomponentstoreflect
currentMLpractices.
Fromthecodecorpus,thesystemextractsallscriptsthatcontain
acalltoourtargetAPIlibraryandrecordsthesetofAPIcomponents
usedineachscript.Theintuitionisthatthesesetscanbeusedto
measurethelikelihoodofcomponentsco-occurring,andthatcomple-
mentarycomponentsmust(bydefinition)co-occurmorefrequently.
Formally,wecomputethenormalizedpointwisemutualinfor-
mation (NPMI) [ 5] over the collection of all (unordered) pairs of
co-occurringAPIcallsinourcodecorpustoidentifycomplementary
components.Let XandYbetworandomvariables,representing
possiblecomponents,definedoverthethedomainofourtargetAPI
library.WedefineNPMIfortwocomponents x∈Xandy∈Yas
NPMI(x,y)=log2/parenleftBigp(x,y)
p(x)p(y)/parenrightBig
−log2(p(x,y))(1)
wherep(x)isthefractionofpairswhereeitherelementis xdivided
bythenumberofallpairs,similarlyfor p(y),andp(x,y)isthefraction
ofpairs(x,y)or(y,x)dividedbythenumberofallpairs.
NPMIrangesbetween-1and1,where-1meansthecomponents
never co-occur, 1 means the components always co-occur, and 0
meansthecomponentsareindependent.WecomputetheNPMIover
thesetofallpairsofco-occurringcomponents(i.e.APIcomponents
calledinthesamescript).EliminatingpairswithanNPMIlessthan
orequaltozeroyieldspairsofvaryingdegreeofcomplementarity.
When given a weak specification, we can identify all NPMI-
positivepairsthatshareacomponentwiththespecification.Foreach
suchpair,thenewpotentialcomponentcorrespondstotheelement
inthepairthatisnotintheoriginalspecification.Ifmorethanone
componentintheoriginalspecificationsupports(i.e.co-occurswith)
anewcomponent,wecomputeanaverageNPMI.Foreachpossible
newcomponent,wecomputeaweightedsumoftheaverageNPMI
andthefractionoforiginalspecificationcomponentsthatsupport
it.TheweightedsumbalancesaverageNPMIandsupportfraction
basedonauser-definedweight α∈[0,1].Wethentakethetop Kcomp
newcomponentsandaddthemascomplementarycomponentsto
theoriginalspecification.Algorithm 1describesthisprocedure.
5.1.2 Functionally Related Components. The goal of identifying
functionallyrelatedcomponentsistoincludealgorithmalternatives
inthespecification.Forexample,theuser’sweakspecificationmay
indicate that they are interested in using linear models, but they
mayhavenotexhaustivelylistedalllinearmodelalternatives.This
taskraisesthechallengeofreasoningaboutthesemanticsofAPI
766AMS:GeneratingAutoMLSearchSpacesfromWeakSpecifications ESEC/FSE’20,November8–13,2020,VirtualEvent,USA
Algorithm1 ExtractingComplementaryComponents
INPUT: Acollection PofpairsofAPIcomponentsco-occurringinacode
corpus;afunction npmithatcomputesthenormalizedpointwisemutual
informationoftwoAPIcomponents;aspecification S={c1,...,cn};a
weightα∈(0,1)tocombineNPMIandsupportfraction;andaninteger
Kcompforthemaximumnumberofcomplementarycomponentstotake.
OUTPUT: Anewspecification S′extendedwithatmost Kcompnewcom-
ponents.
procedure ComplementaryComponents
▷Mapfromco-occurringpairtoaccumulatorlistofNPMIscores
npmis←{}
forc∈S,(p1,p2)∈Pdo
if(c∈(p1,p2))∧(p1/nelementS∨p2/nelementS)then
new←p2ifc1=p1elsep2
▷Accumulatethenpmiscore
npmis[new]←npmis[new]::NPMI(p1,p2)
▷Computeaveragenpmiandsupportfraction
▷Combineusing αtocreatescore
scores←{}
n←len(S)
forcomp∈npmisdo
vals←npmis[comp]
scores[comp]←Avg(vals)∗α+/parenleftBigLen(vals)
n/parenrightBig
∗(1−α)
S′←S∪GetTopK (scores,Kcomp)
components.Ratherthanreasonaboutcomponentsemantics,we
relyonasimplernotionofsimilarity.
We would like to define a function sim(c1,c2)that computes a
scorefortwoAPIcomponents, c1andc2,suchthatahigherscore
correspondstohigherdegreeofsemanticsimilarity.Givenacompo-
nentci,wecanthensortallpossiblecomponentsinourtargetAPIin
descendingorderbasedontheirsimilarityscorewithrespectto ci.
AMSexploitsthefactthatthetargetlibraryhasnaturallanguage
documentationforeachcomponent(aspartofitsdeveloperdocu-
mentation),whichweassumedetailskeyaspectsabouttheirfunc-
tionalbehavior.ByminingtheAPI’sdocumentation,AMScanbe
usedtoautomaticallyidentifyfunctionallyrelatedcomponentsin
new target libraries or new versions of previously used libraries
withouttheneedforextensiveexpertannotation.
Wedefine sim(c1,c2)tobecomputedoverthedocumentation2for
c1andc2andinstantiateittoaclassicalrelevance/similarityscoring
technique:BM25[ 42].BM25,detailedbelow,producesascorefor
a document, given a query and a corpus of documents. A higher
scoreindicatesahigherdegreeoflexicalcorrelationbetweenthe
documentandthequery.
BM25(D,Q)=n/summationdisplay
iIDF(C,qi)f(qi,D)∗(k1+1)
f(qi,D)+k1∗/parenleftBig
1−b+b∗Len(D)
AvgLen(C)/parenrightBig
(2)
whereDisadocument, Q=(q1,...,qn)isaquerycomprisedof qi
terms,Cisacorpusofdocuments,and k1andbarescorehyperpa-
rameters3.
2Weperformstandardpreprocessingofthedocumentationstringssuchastokenization,
stemming,andextensionwiththepathofthegivencomponentinthelibrary’smodule
structure.
3We use the gensim [ 41] BM25 implementation, where k1=1.5andb=0.75are
implementation-definedconstants.Inoursetting,thedocumentationforanexistingcomponentinthe
weakspecificationcorrespondstothequery,aparticularAPIcompo-
nent’sdocumentationcorrespondstothedocument,andtheentirety
oftheAPI’sdocumentationcorrespondstothedocumentcorpus.
AMSuses simtoretrieve,andappend,thetop Krelnewcompo-
nentsforeachcomponentintheoriginalweakspecification(i.e.,we
donotconsideranycomplementarycomponentsaddedforpurposes
ofthisprocedure).Algorithm 2describesthisprocedure4.
Algorithm2 ExtractingFunctionallyRelatedComponents
INPUT: Acollection CofAPIcomponents;amap MfromAPIcomponent
todocumentation;afunction simthatcomputestheBM25scorebetween
aquerystringandadocument;aspecification S={c1,...,cn};andan
integerKrelforthemaximumnumberoffunctionallyrelatedcomponents
totakepercomponentin S.
OUTPUT: Anewspecification S′extendedwithatmost Krelfunctionally
relatedcomponentspercomponentintheoriginalspecification.
procedure FunctionallyRelatedComponents
▷SetofemptyAPIcomponents
extension←∅
forc∈Sdo
scored←{(c′,sim(M[c],M[c′]))forc′∈Cifc′/nelementS}
cK←GetTopK (scored,Krel)
extension←extension∪ck
S′←S∪extension
5.2 IdentifyingHyperparametersandValues
Machinelearningpractitionersoftenspendasignificantamount
oftimenotjustchoosingpipelinecomponents,butalsotuningthe
hyperparametersassociatedwitheachcomponent.Performancecan
significantlyincreasebyidentifyingtheappropriatehyperparameter
valuesforagivendatasetandpipeline[ 40].
AMSreliesonthecorpusofscriptsthatmakecallstothetarget
APItoidentifythesetofrelevanthyperparametersandpossibleval-
ues.ThisdesignchoicehypothesizesthatanAutoMLsystemshould
focus on tuning the set of hyperparameters and hyperparameter
valuesthathumandevelopersfocusontuning.
ForeachscriptinourcodecorpusthatimportsthetargetAPI,we
parsethesourcecodeandidentifycallstoAPIclassconstructors.
Weextractthesetofoptionalargumentsineachconstructorcalland
recordeachpairof(argumentname,argumentvalue)asahyperpa-
rametersetting.Thevaluerecordedcorrespondstoaconstantinthe
constructorcall,orpointstoanunknownplaceholder.
Whengivenaspecification,AMStakeseachAPIcomponentand
identifiesthesetoftop Kparamshyperparameternamesobservedin
theminedcodeforthatcomponent,alongwiththetop Kvalsvalues
observedforeachofthenames.AMSaddsthedefaultvalueforeach
hyperparametertothesetofpossiblevalues(obtainedbyintrospect-
ingtheclassdefinition),andthenemitsthisasthecorresponding
hyperparametersearchspace.Algorithm 3describesthisprocedure.
Figure2showsaspecification,originallyjust sklearn.linear_model
.LogisticRegression ,extendedwithacomplementarycomponent
(Algorithm 1),afunctionallyrelatedcomponent(Algorithm 2),and
hyperparametersandvalues(Algorithm 3).
4AMSalsoexposesfunctionalitytolabelaweakspecificationcomponentasłincludež
(byappending :1)orłexcludež(byappending :0),indicatingthatitmustbeincluded
orexcludedfromthestrengthenedspecification,respectively.
767ESEC/FSE’20,November8–13,2020,VirtualEvent,USA JoséP.Cambronero,JürgenCito,andMartinC.Rinard
Algorithm3 AddingAPIComponentHyperparametersandValues
INPUT: A mapPfrom API components to hyperparameter names and
frequenciesobservedincalls;amap Vfromhyperparameterstovalues
andtheirfrequenciesobservedincalls;aspecification S={c1,...,cn};an
integerKparamsforthemaximumnumberofhyperparameterstoconsider
percomponent;andaninteger Kvalsforthemaximumnumberofvalues
perhyperparametertoconsider.
OUTPUT: Anewspecification S′withatmost Kparamshyperparameters
percomponentandatmost Kvals+1(includingdefaultvalue)perhyper-
parameter.
procedure HyperParamsAndValues
▷Emptymapfromcomponenttohyperparameterspace
S′←{}
forc∈Sdo
params←GetTopK (P[c],Kparams)
▷Emptyconfigurationforcomponent c
cconfig←{}
forp∈paramsdo
values←GetTopK (V[p],Kvals)
▷Appenddefaultvalue,ifnotincluded
values←values::GetDefaultValue (p)
cconfig[p]←values
S′[c]←cconfig
{'sklearn . linear_model . LogisticRegression ': {
'C': [100000.0 , 7 , 1.0] , 'penalty ': ['l1','l2']} ,
'sklearn . feature_extraction . text . TfidfTransformer ': {} ,
'sklearn . linear_model
. SGDClassifier ': {'loss': ['log','hinge'] ,
'penalty ': ['l2','elasticnet '] } }
Figure 2: A weak specification extended with one comple-
mentary component, one functionally related components,
and two hyperparameters/values per component (plus a
potentialdefaultvalue,ifdifferent).
5.3 SearchProcedure
Tofullysatisfythedefinitionofastrongspecification,AMSmust
add in a specific search procedure to the extended specification.
AMSallowstheuseofdifferentsearchprocedures,whichcanbe
pluggedintothesystem.Inparticular,thecurrentimplementationof
AMSexposesaplug-ingeneticprogrammingsearchprocedure,us-
ingTPOT[ 37]andaconceptuallysimplerandomsearchprocedure
implementedaspartofAMS’scodebase.
5.3.1 GeneticProgramming. WeuseTPOT[ 37],ageneticprogram-
mingbasedAutoMLtool,asasearchprocedure.WhenusingTPOT,
weusethesearchspacedefinedbyAMSastheconfigurationavail-
abletotheoptimizationprocess.
5.3.2 RandomSearch. AMS’simplementationincludesahierarchi-
calrandomsearchproceduretogeneratesequential(i.e.APIcompo-
nentsarechainedinsequence)pipelines.Randomsearchisknownto
performbetterforalgorithmconfigurationthanequallysimplealter-
nativessuchasgridsearch[ 4]andhasalsobeensuccessfullyapplied
torelatedsoftwareengineeringareassuchasproductlineconfigura-
tion[35].Togenerateapipeline,thesearchmodulesamplesadepth
(uptoabound),thenforeachstepinthepipelineitsamplesanAPITable 2: NPMI-based association rules mined from our
code corpus to identify complementary API components
categorized by algorithmic role. When both components in
theassociationhavethesamerole,weelideoneforbrevity.
RuleType #Rules MeanNorm.PMI SDNorm.PMI
classifier 78 0.18 0.11
(classifier,cluster) 1 0.37 -
(classifier,decomposition) 3 0.16 0.13
(classifier,featureextraction/selection) 29 0.19 0.15
(classifier,preprocessor) 31 0.20 0.13
(cluster,decomposition) 2 0.45 0.26
(cluster,preprocessor) 1 0.27 -
(cluster,regressor) 3 0.10 0.05
(decomposition,featureextraction/selection) 7 0.17 0.19
(decomposition,preprocessor) 4 0.24 0.26
(decomposition,regressor) 3 0.20 0.25
featureextraction/selection 3 0.35 0.25
(featureextraction/selection,preprocessor) 10 0.25 0.20
(featureextraction/selection,regressor) 4 0.17 0.12
preprocessor 3 0.32 0.26
(preprocessor,regressor) 6 0.20 0.21
regressor 97 0.19 0.07
componentfromtheconfigurationspecified.Foreachhyperparam-
eterinthechosencomponent’sconfiguration,thesearchsamplesa
valueandsetsitinthatcomponent’sconstructor.Thesearchdistin-
guishesbetweenpreprocessingandclassifiercomponentstogener-
atevalidcandidatepipelines(i.e.thelaststepmustalwaysbeaclassi-
fier).Candidatepipelinesarecachedtoavoidre-training/evaluating
pipelines,however,thereisnoefforttoexhaustivelysearchthespace
andifapipelineisre-sampledagivennumberoftimes(100inour
implementation),thesearchprocedureterminates.
6 EVALUATION
Wenowpresentourexperimentalresults,whichevaluateindividual
partsofoursystem(RQ1-RQ3,RQ5)andtheoverallperformanceof
AMS(RQ4).First,wecharacterizethecomplementaryAPIcompo-
nentsextractedfromourcodecorpus(RQ1).WeevaluateAMS’abil-
itytoretrievefunctionallyrelatedAPIcomponents(RQ2).Wethen
characterizetheuseofhyperparametersandtheirvaluesinourcode
corpus,andevaluatethepossibilitiesforimprovingclassifierperfor-
mancebasedonthisinformation(RQ3).WeevaluateAMS’sability
toproducespecificationsthatresultinhigherperformance(RQ4).
Andfinally,weexploretheimpactofthecodecorpussizeonAMS’s
minedhyperparametersandcomplementarycomponents(RQ5).
Forourevaluation,weimplementedAMSanditsevaluationin
approximately5000linesofPython.WeuseScikit-Learn[ 39],apop-
ularPythonmachinelearninglibrary,asthetargetAPIforpipelines.
Tominecomplementarycomponentsandidentifyhyperparameter-
s/values,weusethemeta-Kaggle[ 28]datasetasourcodecorpus.
Themeta-Kaggledatasetcontainsover3300Pythonscripts.
6.1 RQ1:ComplementaryAPIComponents
AMSmined285normalizedPMI(NPMI)positiveassociationpairs
fromourcodecorpus.Theseassociationscover69differentcompo-
nents(39.2%ofallcomponentsinScikit-Learn).
Table2detailsthedistributionofassociationsbasedonthealgo-
rithmicroleofeachofthecomponents,alongwiththeirmeanand
standarddeviationNPMI.
768AMS:GeneratingAutoMLSearchSpacesfromWeakSpecifications ESEC/FSE’20,November8–13,2020,VirtualEvent,USA
Figure 3: NPMI-based component extension can produce at
least one complementary component for 82.68% of our test
observations, with precision of approximately 60% when
Kcomp=1.
ToevaluatetheeffectivenessoftheseNPMI-basedcomponentex-
tensions,weconductthefollowingexperiment.Wetakeeachscript
inourcodecorpus,andextractthesetofScikit-Learncomponents
used.Using10-foldcrossvalidation(CV),wesplitthiscollectionof
componentsintoatrainingfoldandtestfold.Weuseeachtraining
foldtocomputeNPMI,andweusethecorrespondingtestfoldto
evaluate.Foreachset(groundtruth)inthetestfold,wetakeeach
componentindividuallyanduseitasaquerytermtoretrievethetop
Kcomp∈[1,5]complementarycomponentsbasedonourapproach
(Algorithm 1,withα=0.5).Wethencomputeprecisionasthefrac-
tionofretrievedcomponentsthatarepresentinthefullgroundtruth
componentset.Notethatrecallisnotanappropriatemeasureof
performanceforevaluatingcomplementarycomponents,asrecall
impliesourextensionsneedtobecomplete,butbydefinitionwe
willonlybeabletocovercomponentswithstrongco-occurrence
patterns.Giventhis,wefocusonprecision.
Wefoundthat82.68%ofthesetsinthetestfoldswerecovered(i.e.
wewereabletoidentifyatleastonecomplementarycomponent).
ForKcomp=1, we found that our NPMI-based approach yields a
precisionof60%.Thisprecisiondeclinesasexpectedwhenwein-
creaseKcomp,withaprecisionofapproximately28%when Kcomp=5.
Basedontheseresults,weconfiguredAMStouse Kcomp≤3.Figure3
summarizestheseresults.Theselevelsofcomplementarycompo-
nentretrievalsufficedforimprovedperformanceonourend-to-end
benchmarks,butfurtherimprovingcomplementarycomponentpre-
cisioncoulddeliveradditionalgains.
6.2 RQ2:FunctionallyRelatedAPIComponents
ToevaluateAMS’sretrievaloffunctionallyrelatedcomponents,we
manuallyannotatedourBM25-basedrankingofAPIcomponents
foragivenquerycomponent.Todetermineiftwocomponentswere
functionallyrelated,weoutlinedasetofconditionsthattheyshould
satisfy.GivenaspecificationcomponentQ(forquery)andapossible
extensioncomponentR(forrelated),wesaytheyarefunctionally
relatediftheysatisfythefollowing:
•RcouldreplaceQinapipelinewithoutraisinganexception
forthesamedataset.
•QandRbelongtothesameclassofoperators(e.g.classifier,
regressor,valuenormalizer,decompositionalgorithm,loss
function).
Top 1
 Top 5
 Top 10
Cutoﬀ
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8Fraction Functionally Related
Approach
Embeddings
BM25
RandomFigure 4: For 50 randomly sampled query API components,
BM25 can retrieve close to 72%, 55%, and 44% functionally
relatedcomponentsbasedontop1,5,and10cutoffs.
•If Q/R are classifiers/regressors, they must respect output
shape constraints: a multi-task model can replace a single
taskmodel,butnotvice-versa.
•IfQis(non-)linear,Rmustbealways(non-)linearormust
be(non-)linearbasedonahyperparameter(e.g.SVMwitha
linearkernel)
•IfQisensemble-based,Rmustbeensemble-basedwithone
exception:Rcanbenon-ensemblebasedifitisrelated(based
ontheserules)totheweakmodelclassensembledinQ.
•IfQisnotensemble-based,Rmaybeensemble-basedifituses
aweakmodelclassrelatedtoQtocreateitsensemble.
Tocarryoutourexperiment,werandomlysampled50classes
fromScikit-Learnandusedtheseasqueries.Wechosetosample50
classesasthiscoversapproximately28%ofthecomponentsavailable
inScikit-Learnandbalancedtheneedfordetailedmanualannotation.
Foreachquery,weretrievedthetop10APIcomponentsbasedon:1)
ourBM25metric,2)cosinesimilarityusingaveragedpre-trainedneu-
ralembeddings(whichhavebeenshowntobeeffectivefortherelated
taskofcodesearch[ 6]),and3)auniformrandommetric.Weused
(2)tocomparetheuseofBM25withanotherunsupervisedapproach
tosemanticsimilarity.WeusedBERTembeddingsderivedfroma
scientifictextcorpus[ 3].Weused(3)asabaselinetocontrolforthe
extenttowhichourtargetAPI(Scikit-Learn)mayhaveredundant
componentsresultinginfunctionallyrelatedresultsthroughchance.
Figure4presentsourresults.TheBM25-basedrankingperformed
comparably(withnostatisticallysignificantdifference)totheembed-
dingsbasedapproach.Arandomrankingresultsinapproximately
10%functionallyrelatedresults,acrossthetop1,5,and10query
results.Incontrast,BM25resultsincloseto72%,55%,and44%func-
tionally related results across the same cutoffs, respectively. We
optedtouseBM25inAMS,incontrasttotheneuralembeddings
approach,giventheircomparableperformanceandtheaddedad-
vantageofavoidingtheadditionalstoragerequirementsimposed
byper-tokenembeddings.
Notethatwhileforpurposesofthisexperiment,weallowfunc-
tionallyrelatedcomponentstoincludeensembled-variantsofnon-
ensemble models, in our tool implementation users can exclude
ensemblesthroughasimplecommandlineflag.
Whileweevaluatedfunctionally-relatedAPIcomponentretrieval
usingBM25andcosine-similarityusingBERTembeddings,AMS
769ESEC/FSE’20,November8–13,2020,VirtualEvent,USA JoséP.Cambronero,JürgenCito,andMartinC.Rinard
canuseotherinformationretrievalmetrics.Wealsonotethatthe
task of specification strengthening, in the context of AutoML, is
relatedbutorthogonaltopureinformationretrieval.Inparticular,
wegeneratea newsearchspaceconfigurationbasedonaweakspec-
ification,ratherthansearchingoverastored(andpre-enumerated)
setofconfigurations.
6.3 RQ3:HyperparametersandValues
Figure5characterizesthehyperparametertuningobservedinour
codecorpus.Inparticular,wefoundthatover50%ofthecallstune
(i.e.explicitlysetavalueinthecall)forunder20%ofthehyperpa-
rametersavailable( 5a);foraboutathirdofAPIcomponentstheset
ofhyperparameterstunedissimilaracrosscalls( 5b);andforover
70%ofthehyperparametersobserved,usercallschoosefewvalues
(under10distinctvalues)( 5c).Thisalignswithourintuitionthat
humandeveloperstendtotuneasmallsetofhyperparameters,these
areconsistentacrossdatasets/pipelines,andtherearepopularvalues
thatdeveloperschooseforeach.
Todemonstratethepossibleimpactofhyperparametertuning,
weperformedthefollowingexperiment.Wecollectedfivedatasets
from the Penn Machine Learning Benchmarks (PMLB) [ 38]. The
fivedatasetsarehealthcare-relatedclassificationtasks.Wecollected
these5datasettobeindependentfromthoseusedinRQ4.Wethen
identifiedthetop5mostcommonclassifiers5fromourcodecorpus.
Foreachclassifier,weextractedthetop3hyperparametersandtop
3 values for each hyperparameter, along with the default values.
Weperformedgridsearchoverthesevaluestoevaluateallpossi-
bleconfigurations.Wethencomparedthebestmacro-averagedF1
score[16]fromthegridsearchwiththescoreobtainedunderthe
defaultconfiguration.
Figure6showsourresults.Inalmostallcases,thehyperparameter
spacedefinedbythecodeexamplesinourcorpuscontainedasetting
whichwouldhaveimprovedperformancewithrespecttothedefault
configuration.Fortheensemble-basedclassifiers,ExtraTreesClassi-
fierandRandomForestClassifier,thisimprovementcouldhavebeen
upto10%ontwoofthedatasets.
6.4 RQ4:PerformanceofStrongSpecifications
Ourperformanceexperimentscomparethefollowingapproaches:
•Weak Spec.: runs an ordered version of the original weak
specificationasapipelinedirectly.
•WeakSpec.+Search:carriesoutaspecifiedsearchprocedure
overthecomponentsdefinedintheweakspecification(with
defaulthyperparameters).
•Expert+Search:usesthesetofhyperparameters/valuesde-
finedinTPOT’sdefaultclassifierconfiguration[ 1]foreach
component in the specification, and applies the specified
searchprocedure.Thischoiceofhyperparameterspacecor-
responds to an expert AutoML developer identifying key
hyperparametersandvalues.Wealsoevaluatedwritingour
ownhyperparameterspaceandfoundthatitperformedcom-
parablyorworse,soweelideforbrevity.
5excluding SVM, which did not terminate within a reasonable computing budget
withoutadditionaldatapre-processingforthesedatasetsTable 3: Components used to produce weak specification.
Expert + Search uses TPOT’s pre-defined hyperparameter
searchspace[ 1]foreachcomponent.
Shortname Component
lr LogisticRegression
rf RandomForest
dt DecisionTree
scale Min-maxvaluescaling
poly Extractpolynomialfeatures
var Variance-basedfeatureselection
pca PCAdecomposition
•AMS+Search:appliesAMStotheweakspecificationtopro-
duceafullsearchspaceandthenappliesthespecifiedsearch
procedure.
Fortheseexperiments,weconsiderbothsearchproceduresavail-
ableinAMS:geneticprogrammingandrandomsearch.
Table3presentstheindividualcomponentsusedtocreatethe
weakspecificationsforourexperiments.Wechosecomponentsthat
coveredcommonmachinelearningoperations:valuescaling,fea-
turederivation,featureselection,datasetdecomposition,andvaried
formsofclassification.Foreachsuchcomponent,wealsooutlinea
subsetofhyperparametersidentifiedfortuningandtheirpossible
values,whichareusedinthe Expert+Search approach.
Weproduced15weakspecificationsbycombiningthefollowing5
pre-processingweakspecificationswitheachofthethreeclassifiers
(lr,rf,dt)-asoutlinedinTable 3:{}(no-preprocessing),{scale},{poly,
scale},{poly,scale,var},and{poly,scale,pca,var}.
Forourexperimentsweusedallclassificationdatasetsfromthe
originalTPOTpaper[ 37];9intotal.Thesedatasetsare:Hill-Valley-
with-Noise, Hill-Valley-Without-Noise, breast-cancer-wisconsin,
car-evaluation,glass,ionosphere,spambase,wine-quality-red,and
wine-quality-white.AlldatasetsareavailablethroughPMLB[ 38].
Ourexperimentsusedmacro-averagedF1scoreasaperformance
metric, where a higher score corresponds to better performance.
Eachsearchprocedureusesthissamescoremetricintheirinternal
searchloop.Foreachbenchmark,dataset,andsearchprocedurecom-
bination,wecarriedout5-foldcross-validation(CV)witheachofthe
approachesoutlinedpreviously.IneachCViteration,thetraining
foldisusedtofindanoptimizedpipeline,andthetestfoldisused
forevaluation.Allapproacheswereprovidedabudgetof5minutes
perCViteration(i.e.25minutesperdataset,foreachspecification
andapproachcombination).
WeevaluateAMSwiththefollowingconfiguration:aweakspeci-
ficationcanbeextendedwithatmost3complementarycomponents
(Kcomp=3),wherethenpmi/supportfractionweighingparameteris
setto0.5( α=0.5),foreachspecificationcomponentweincludeup
to4functionallyrelatedcomponents( Krel=4),andwetunethetop
3hyperparameterspercomponent( Kparams=3)bychoosingfrom
the3mostcommonvaluesperhyperparameter( Kvals=3).Weset
thedepthboundfortherandomsearchprocedureto4.
Figure7presentsacountofthewinsforeachapproachacross
both searchprocedures [ 8]. An approach winswhen the average
ofthe5-foldCVtest-foldperformancemetricisthehighestacross
approachesforagivendatasetandweakspecificationcombination,
770AMS:GeneratingAutoMLSearchSpacesfromWeakSpecifications ESEC/FSE’20,November8–13,2020,VirtualEvent,USA
0.0
 0.2
 0.4
 0.6
 0.8
 1.0
Ratio of hyperparameters tuned to available
0.4
0.5
0.6
0.7
0.8
0.9
1.0Empirical CDF (Calls)
(a)Fractionofhyperparameterstuned
0.0
 0.2
 0.4
 0.6
 0.8
 1.0
Mean Jaccard Distance of Parameters Tuned Across Calls
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0Empirical CDF (Components) (b)Distancebetweensetsofhyperparame-
ters
0
 10
 20
 30
 40
 50
Unique values in calls
0.6
0.7
0.8
0.9
1.0Empirical CDF (Component Hyperparameters)(c)Uniquehyperparametervaluesused
Figure5:Characterizinghyperparametertuninginourcodecorpus.
Figure 6: Possible improvements in macro-averaged F1
score by using hyperparameter settings in our code corpus,
comparedtotheperformanceusingdefaults.
andthescoreisatleast1%(inabsoluteterms)higherthanthenext
bestscore.Weintroducedaminimumperformancedifferencethresh-
oldtoeliminatecaseswheremultipleapproachesperformroughly
equallyonaspecification/datasetcombination.Wevariedthemin-
imumdifferencethresholdfrom1%to5%(absolute)andfoundthat
AMSobtainedmorewinsthanotherapproachesinallcases.
Whenusinggeneticprogrammingasasearchprocedure,wesee
thatWeakSpec. obtained6winscomparedto9winsfor WeakSpec.+
Search.Underrandomsearch, WeakSpec. obtained1winand Weak
Spec.+Search obtained12wins. Expert+Search obtained12wins
whenusinggeneticprogramming,and14winswhenusingrandom
search.Underbothsearchprocedures,usingAMSproducedthema-
jorityofwins:38inthegeneticprogrammingexperimentsand41
intherandomsearchexperiments.
Figure8presentsthedistributionofthetop-10Scikit-Learnopera-
torsasafractionofthetotalcountofoperatorsinpipelinesproduced
bygeneticprogrammingusingAMS’sstrengthenedspecification
fortwodifferentweakspecifications.Forcomparison,runningge-
neticprogrammingoverthefullsearchspace(asdefinedinTPOT’s
defaultclassificationconfiguration)producespipelineswhere60%
ofthemhaveanensemble-basedmodel(oneofGradientBoosting,
ExtraTrees,XGBoost,orRandomForest),andatleastonepipeline
Genetic Programming
 Random Search
Search
0
5
10
15
20
25
30
35
40Number of Wins
Approach
Weak Spec.
Weak Spec. + SearchExpert + Search
AMS + SearchFigure 7: Wins for each approach across 270 experiments.
Withinagivensearchprocedure,anapproach winswhenit
obtainsthehighestaverage5-foldCVtest-foldperformance
for a dataset and weak specification combination, andthis
score is at least 1% higher (in absolute terms) than the next
bestscore.
produced in 8 of the 9 datasets includes such a component. The
skewtowardsensemble-basedmodelshasbeenobservedinother
AutoMLtoolsaswell[ 14].ByusingAMSausercanrestricttheuse
ofensemble-basedmodels,forexample,ifdesired.
Comparisontootherprogram-miningbasedAutoMLtools. Wealso
compared AMS to AL [ 7]. AL mines dynamic program traces to
learnaprobabilisticmodelforMLpipelinesandusesthistogener-
atesequentialpipelines.AkeyadvantageofAMSisthatuserscan
strengthenspecificationswithouttheneedtocollectanewcorpus
thatreflectstheirinitialspecification.AMScanalsomineinforma-
tionfromotherwiseun-executableprogramsandwithoutaccessto
theprograms’targetdatasets,whileALrequiresprogramexecution
foritsdynamicanalysis.
TocompareALandAMS,weconsidertheweakspecificationof
Scikit-Learncomponents6:
{ LogisticRegression , LinearSVC , StandardScaler }
6namesabbreviatedforbrevity
771ESEC/FSE’20,November8–13,2020,VirtualEvent,USA JoséP.Cambronero,JürgenCito,andMartinC.Rinard
ExtraTreesClassifier
RandomForestClassifier
GradientBoostingClassifier
StackingEstimator
StandardScaler
VarianceThreshold
MinMaxScaler
MaxAbsScaler
SelectPercentile
FeatureUnion
Components
0.0
0.1
0.2
0.3
0.4% of componentsSpec: PolynomialFeatures, MinMaxScaler, VarianceThreshold, RandomForestClassifier
LogisticRegression
SGDClassifier
RobustScaler
FeatureUnion
KernelPCA
MinMaxScaler
StandardScaler
QuantileTransformer
RidgeClassifier
VarianceThreshold
Components
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40% of componentsSpec: PolynomialFeatures, MinMaxScaler, PCA, VarianceThreshold, LogisticRegression
Figure8:ExampledistributionsofScikit-Learncomponents
in pipelines produced by genetic programming, based on
AMSstrengtheningoftheinitialweakspecification.
and run experiments on our 9 datasets. We use 5-fold CV, pair
pipelinesbetweenCVfoldsinordertoappropriatelyperformcom-
parisonsafterremovingpipelinesthatdon’tsatisfytheweakspecifi-
cation,andthencomputewinsonthepairedpipelines.Ifthepipeline
forasystemdoesnotsatisfythespecification,theothersystem’s
pipelineisassignedthewin.AListrainedonthecorpuspresented
in[7],whichisrestrictedtoprogramsithasalreadyexecutedand
fromwhichithasextracteddynamictraces.
WhenAListrainedonthesubsetofprogramsthatuseatleast
oneweakspecificationcomponent,andAMSminesthissameset
ofprograms,wefindthatALcanproducepipelinesthat stilldeviate
from the weak specification (as the full program traces may con-
tainadditionalcomponents).21ofthe45pipelinesgeneratedbyAL
didnotsatisfytheweakspec,whileallofAMSdo.Afterremoving
specification-violatingpipelines,AMSobtains29winsandALob-
tains9.WhenAMSistrainedonthefullAMScorpus,AMS’swins
increase to 35 (and all pipelines continue to satisfy the specifica-
tion)andAL’sdecreaseto4.Finally,whenAMSistrainedonthe
AMScorpusandAListrainedonthefullALcorpus(withoutany
specification-relatedprogrampruning),26ofthe45ALpipelinesdo
notsatisfytheweakspecification.Afterremovingthesepipelines,
AMSobtains42winsandALobtains1win.6.5 RQ5:ImpactofCorpusSize
AMSmineshyperparameters,theircorrespondingvalues,andcom-
plementarycomponentassociationrulesfromacorpusofcodeex-
amples.ToevaluatetheimpactofvaryingcorpussizesonAMS,we
sampledfrom10%to90%(in10%increments)oftheoriginal3,300
scripts.Werepeatedthissamplingfivetimespersamplingratio.For
eachsampledcorpus,weranAMS’shyperparameterminingand
complementarycomponentmining.
Figure9ashowstheaveragefractionofhyperparametersmissing
foragivencomponent,withrespecttothehyperparametersfound
throughthefullcorpus.Forverysmallcorpora,e.g.10%(330scripts),
asexpectedthereductioninhyperparametersminedcanbesubstan-
tial.Amoderatesizedcorpus,e.g.50%(1650scripts)coversmostof
thehyperparametersfoundinthefullcorpus.
Figure9bshowstheaveragereductioninpossiblehyperparame-
tervalueswithrespecttothefullcorpus.Ifwemine5possiblevalues
forahyperparameterinthefullcorpus,andwemine3possibleval-
uesinadownsampledcorpus,wesaythatisareductionof2possible
values.Weseethatforamoderatesizedcorpus(e.g.50%)theaverage
reductionisapproximatelyonepossiblevalueperhyperparameter.
Figure9cshowsthedecreaseinnumberofcomplementarycom-
ponentsmined,whencomparedtothefullcorpus.Smallcorpora
(<30%oftheoriginalsize)displaylargedecreasesinthenumberof
associationrulesfound,butmoderatesizedcorpora(e.g.50%)mine
approximately80%asmanyrulesasthefullcorpus.Figure 9dshows
thatformoderatedsizedcorpora,therulesminedarerelativelysim-
ilar(approximately0.8jaccardsimilarity)tothoseminedfromthe
fullcorpus.
7 RELATEDWORK
Automatedmachinelearning(AutoML)hasreceivedincreasedat-
tentionintherecentpast.TPOT[ 37]usesgeneticprogrammingto
automaticallyproducetree-structuredclassificationandregression
pipelinescomposedofScikit-Learn[ 39]operators.Autosklearn[ 15]
usessequentialmodel-basedalgorithmconfiguration(SMAC)[ 26]
togenerateScikit-Learnpipelinesandtheirhyperparametersettings.
ReinBo[46]usesreinforcementlearningtogeneratepipelinecan-
didatesandBayesianoptimizationtotunetheirparameters.AL[ 7]
learnsapipelinelikelihoodmodelfromdynamicprogramtracesand
usesthistogeneratesequentialScikit-Learnpipelines,withdefault
hyperparametervalues.MLBazaar[ 45]buildsupanAutoMLsystem
throughalibraryofcomposableoperatorswithacleanandunified
interface.
Incontrasttothesesystems,AMSfocusesonprovidingAutoML
users with a simple way of influencing the pipeline generation
process:writingaweakspecification,whichcanbeautomatically
strengthened.Thisusagemodelempowersuserstoinfluencepipeline
generationonaper-specificationbasis,ratherthanrelyingondis-
tributional characteristics of a pipeline corpus (as in AL), on the
developerpre-definedsearchspacesintheoriginalAutoMLtool(as
inTPOT,ReinBoandAutosklearn),ormanuallyspecifyinganew
completesearchspacethatreflectstheirpreferences(asinTPOT’s
optionalconfigurationsorMLBazaar’stop-downtemplates/config-
urations).
Search-basedSoftwareEngineering(SBSE)[ 22]providesageneral
frameworkthroughwhichtodesignandanalyzeAutoMLsystems,
772AMS:GeneratingAutoMLSearchSpacesfromWeakSpecifications ESEC/FSE’20,November8–13,2020,VirtualEvent,USA
0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
Corpus Sampling Ratio
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7Fraction of Hyperparameters Missing per Component
(a)
0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
Corpus Sampling Ratio
0.0
0.5
1.0
1.5
2.0
2.5
3.0Reduction in Hyperparameter Values (b)
0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
Corpus Sampling Ratio
0
10
20
30
40
50
60
70% Reduction in Total Number of Mined Rules (c)
0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
Corpus Sampling Ratio
0.0
0.2
0.4
0.6
0.8
1.0Rules' Jaccard Similarity w.r.t Full Corpus Rules (d)
Figure9:Impactofdownsamplingthefullcorpus.( a)showsthefractionofmissinghyperparameterspercomponent;( b)shows
the number of missing possible values per hyperparameter; ( c) shows the percent reduction in number of complementary
componentassociationrule;and( d)showsthejaccardsimilarityofthecomplementarycomponentrules.
withthelattereffectivelybeingainstanceoftheformer.SBSEhas
beensuccessfullyappliedtoproblemssuchasautomatedtestingof
softwarewithlargetestsuites[ 32],synthesizingequivalentmethod
callsequences[ 19],andoptimizingproductlineconfigurations[ 35],
among others. AMS allows users to approach AutoML in a grey-
boxsetting,wheretheirweakspecificationcaninfluencethesearch
process. Feedback of this form can enable an łiterative process of
refinementž [22]toobtainsolutionsthatsatisfyuserpreferences.
Code corpora have enabled advances in various areas of soft-
wareengineering.Codeidiomsminedfromacorpuscanbeused
toimproveprogramsynthesisandsemanticparsing[ 44],aswell
asenablingcontext-sensitivedeveloperqueries[ 43].Atasmaller
scale,automatedexampleextraction[ 24]fromspecificscriptsallows
allowsuserstoproduceminimalworkingsnippets.Large-scalecor-
porathatexerciseparticularAPIscanbeusedtominepreconditions
formethodcalls[ 33],order-basedspecificationsforchainingcalls[ 2],
coderepairpatterns[ 31],anddrivesemanticcodesearch[ 6,25,30].
8 THREATSTOVALIDITY
We discuss potential limitations of this research based on design
choices.Inparticular,wefocusonthreatstogeneralizability.First,
ourevaluationusesaparticularMLframework(Scikit-Learn).We
believe this threat is mitigated by the fact that Scikit-Learn is a
widely-adoptedMLlibrary,usedbyover92,000GitHubrepositories
asofMarch2020.ExtendingAMStootherpopularlibraries,such
asTensorflow,maybepossibleaslongasthesehavehighquality
APIdocumentation,withrelevantkeywordsandexplanations,and
enoughonlineexamplesforacodecorpus7.
Ourcodecorpus(meta-Kaggle)representsawiderangeofscripts
writtenbydifferentuserstargetingdifferentdatasets.ApplyingAMS
tosmallercodecorporamayimpactperformance.Inourexperiments,
wefoundourcorpusofapproximately3,300scriptsdeliveredgood
performance,andourexperimentswithvaryingsizesofcodecorpus
showthatamoderatesize(approximately1650scripts)candeliver
reasonablyhighcoverageofhyperparametersandcomplementary
components when compared to our full sized corpus. Further in-
creasingthesizeofthecorpuscanhelpmitigatethisrisk.
Weevaluatedtwosearchprocedures:geneticprogrammingand
randomsearch.Othersearchproceduresmaypotentiallyfindpipelines
7e.g.asofApril28th202072,000sourcecodeprojectsonGitHubusedTensorflowwithdifferentcharacteristicsandperformance.However,bothran-
domandgeneticsearcharecommonlyusedmethodsinsearch-based
softwareengineeringandhaveshowngoodperformanceoverawide
rangeofAutoMLproblems.Thechoiceofevaluationdatasetscould
alsoinfluenceourresults.Weusedtheclassificationdatasetsfrom
theoriginalTPOTpaper,whichhavealsobeenusedintheevaluation
ofexistingAutoMLresearch[ 8,10].Theweakspecificationsinour
evaluationarenaturallyasampleofpossiblespecifications.However,
weaimedtoincorporatecommonoperationsandcomponentsin
thesespecificationstoreflectstandardusage.
Finally,weakspecificationsmustincludeatleastonetask-specific
(i.e.regression/classification)component.Webelievesatisfyingthis
requirementisfacilitatedbythewideavailabilityofonlineresources
(e.g.tutorials,blogs)describingbasiclibraryusage.
9 CONCLUSION
WeintroducedanewusagemodelforAutoML,whereauserprovides
asetofAPIcomponentsasaweakspecificationandthisspecification
canbeautomaticallystrengthened.Specificationsenableusersto
exertcontrolandexpresspreferencesovertheresultingpipeline.We
implementourstrengtheningapproachśextendingthespecifica-
tionwithcomplementarycomponentsusingnormalizedpointwise
mutualinformationonanexistingcodecorpus,functionallyrelated
componentsusingalexicalsimilarityscoreoverthetargetAPI’s
documentation,frequencydistributionsonconstructorcallsinthe
codecorpustoextractkeyhyperparametersandvalues,andasearch
procedureśintheAMSsystem.WeevaluatedAMSon9datasets
and15weakspecificationsusingtwodifferentsearchprocedures.
We show that the pipelines produced using AMS’s strengthened
specificationsoutperformpipelinesproducedusingtheinitialweak
specifications and variants of the initial specifications annotated
withexpert-definedhyperparameterspaces.
ACKNOWLEDGEMENTS
Wethanktheanonymousreviewersfortheirfeedback,whichim-
provedthepapersubstantially,particularlyon:anextendedform
ofweakspecifications,adiscussionandcomparisonwithAL,and
anexplorationoftheimpactofcorpussize.Thisworkwaspartially
fundedbyDARPAHACCS-HR001118C0059.
773ESEC/FSE’20,November8–13,2020,VirtualEvent,USA JoséP.Cambronero,JürgenCito,andMartinC.Rinard
REFERENCES
[1][n.d.]. TPOT Default Classifier Configuration.
https://github.com/EpistasisLab/tpot/blob/master/tpot/config/classifier.py.
Accessed:2020-03-05.
[2]MithunAcharya,TaoXie,JianPei,andJunXu.2007.MiningAPIpatternsaspartial
ordersfromsourcecode:fromusagescenariostospecifications.In Proceedings
ofthethe6thjointmeetingoftheEuropeansoftwareengineeringconferenceand
theACMSIGSOFTsymposiumonThefoundationsofsoftwareengineering .25ś34.
[3]IzBeltagy,ArmanCohan,andKyleLo.2019. Scibert:Pretrainedcontextualized
embeddingsforscientifictext. arXivpreprintarXiv:1903.10676 (2019).
[4]JamesBergstraandYoshuaBengio.2012. Randomsearchforhyper-parameter
optimization. Journalofmachinelearningresearch 13,Feb(2012),281ś305.
[5]GerlofBouma.2009. Normalized(pointwise)mutualinformationincollocation
extraction. ProceedingsofGSCL (2009),31ś40.
[6]JoseCambronero,HongyuLi,SeohyunKim,KoushikSen,andSatishChandra.
2019. Whendeeplearningmetcodesearch.In Proceedingsofthe201927thACM
JointMeetingonEuropeanSoftwareEngineeringConferenceandSymposiumon
theFoundationsofSoftwareEngineering .964ś974.
[7]JoséPCambroneroandMartinCRinard.2019. AL:autogeneratingsupervised
learningprograms. ProceedingsoftheACMonProgrammingLanguages 3,OOPSLA
(2019),1ś28.
[8]BoyuanChen,HarveyWu,WarrenMo,IshanuChattopadhyay,andHodLipson.
2018. Autostacker:Acompositionalevolutionarylearningsystem.In Proceedings
oftheGeneticandEvolutionaryComputationConference .402ś409.
[9]James Michael Curran, Tacha Natalie Hicks Champod, and John S Buckleton.
2000.Forensicinterpretationofglassevidence . CRCPress.
[10]AlexGCdeSá,WalterJoséGSPinto,LuizOtavioVBOliveira,andGiseleLPappa.
2017. RECIPE:agrammar-basedframeworkforautomaticallyevolvingclassifica-
tionpipelines.In EuropeanConferenceonGeneticProgramming .Springer,246ś261.
[11]IddoDrori,YamunaKrishnamurthy,RaoniLourenco,RemiRampin,Kyunghyun
Cho, Claudio Silva, and Juliana Freire. 2019. Automatic Machine Learning by
PipelineSynthesisusingModel-BasedReinforcementLearningandaGrammar.
arXivpreprintarXiv:1905.10345 (2019).
[12]IddoDrori,LuLiu,YiNian,SharathCKoorathota,JieSLi,AntonioKhalilMoretti,
Juliana Freire, and Madeleine Udell. 2019. AutoML using Metadata Language
Embeddings. arXivpreprintarXiv:1910.03698 (2019).
[13]IanWEvettandErnestJSpiehler.1989. Ruleinductioninforensicscience.In
KnowledgeBasedSystems .HalstedPress,152ś160.
[14]FabioFabrisandAlexAFreitas.2019. AnalysingtheOverfitoftheAuto-sklearn
Automated Machine Learning Tool. In International Conference on Machine
Learning,Optimization,andDataScience .Springer,508ś520.
[15]MatthiasFeurer,AaronKlein,KatharinaEggensperger,JostTobiasSpringenberg,
Manuel Blum, and Frank Hutter. 2019. Auto-sklearn: efficient and robust
automatedmachinelearning. In AutomatedMachineLearning .Springer,113ś134.
[16]AkinoriFujino,HidekiIsozaki,andJunSuzuki.2008. Multi-labeltextcategoriza-
tionwithmodelcombinationbasedonf1-scoremaximization.In Proceedingsofthe
ThirdInternationalJointConferenceonNaturalLanguageProcessing:Volume-II .
[17]Pranav Garg, Daniel Neider, Parthasarathy Madhusudan, and Dan Roth. 2016.
Learninginvariantsusingdecisiontreesandimplicationcounterexamples. ACM
SigplanNotices 51,1(2016),499ś512.
[18]PieterGijsbers,ErinLeDell,JanekThomas,SébastienPoirier,BerndBischl,and
JoaquinVanschoren.2019. AnopensourceAutoMLbenchmark. arXivpreprint
arXiv:1907.00909 (2019).
[19]Alberto Goffi, Alessandra Gorla, Andrea Mattavelli, Mauro Pezzè, and Paolo
Tonella. 2014. Search-based synthesis of equivalent method sequences. In
Proceedingsofthe22ndACMSIGSOFTInternationalSymposiumonFoundations
ofSoftwareEngineering .366ś376.
[20]Isabelle Guyon, Imad Chaabane, Hugo Jair Escalante, Sergio Escalera, Damir
Jajetic,JamesRobertLloyd,NúriaMacià,BisakhaRay,LukaszRomaszko,Michèle
Sebag,etal .2016. AbriefreviewoftheChaLearnAutoMLchallenge:any-time
any-dataset learning without human intervention. In Workshop on Automatic
MachineLearning .21ś30.
[21]SatoshiHaraandKoheiHayashi.2018. MakingTreeEnsemblesInterpretable:
A Bayesian Model Selection Approach. In Proceedings of the Twenty-First
International Conference on Artificial Intelligence and Statistics (Proceedings of
Machine Learning Research, Vol. 84) , Amos Storkey and Fernando Perez-Cruz
(Eds.).PMLR,PlayaBlanca,Lanzarote,CanaryIslands,77ś85.
[22]Mark Harman, S Afshin Mansouri, and Yuanyuan Zhang. 2012. Search-based
software engineering: Trends, techniques and applications. ACM Computing
Surveys(CSUR) 45,1(2012),1ś61.
[23]Xin He, Kaiyong Zhao, and Xiaowen Chu. 2019. AutoML: A Survey of the
State-of-the-Art. arXivpreprintarXiv:1908.00709 (2019).
[24]Andrew Head, Elena L Glassman, Björn Hartmann, and Marti A Hearst. 2018.
Interactiveextractionofexamplesfromexistingcode.In Proceedingsofthe2018
CHIConferenceonHumanFactorsinComputingSystems .1ś12.
[25]Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc
Brockschmidt.2019. CodeSearchNetChallenge:EvaluatingtheStateofSemanticCodeSearch. arXivpreprintarXiv:1909.09436 (2019).
[26]Frank Hutter, Holger H Hoos, and Kevin Leyton-Brown. 2011. Sequential
model-basedoptimizationforgeneralalgorithmconfiguration.In International
conferenceonlearningandintelligentoptimization .Springer,507ś523.
[27]Frank Hutter, Lars Kotthoff, and Joaquin Vanschoren (Eds.). 2018. Automated
MachineLearning:Methods,Systems,Challenges . Springer. Inpress,available
athttp://automl.org/book.
[28] Kaggle.2017. Meta-Kaggle .https://www.kaggle.com/kaggle/meta-kaggle/data
[29]Michael J Kane, Natalie Price, Matthew Scotch, and Peter Rabinowitz. 2014.
ComparisonofARIMAandRandomForesttimeseriesmodelsforpredictionof
avianinfluenzaH5N1outbreaks. BMCbioinformatics 15,1(2014),276.
[30]JasonLiu,SeohyunKim,VijayaraghavanMurali,SwaratChaudhuri,andSatish
Chandra. 2019. Neural query expansion for code search. In Proceedings of the
3rd acm sigplan international workshop on machine learning and programming
languages .29ś37.
[31]Xuliang Liu and Hao Zhong. 2018. Mining stackoverflow for program repair.
In2018 IEEE 25th International Conference on Software Analysis, Evolution and
Reengineering(SANER) .IEEE,118ś129.
[32]KeMao,MarkHarman,andYueJia.2016. Sapienz:Multi-objectiveautomated
testingforAndroidapplications.In Proceedingsofthe25thInternationalSymposium
onSoftwareTestingandAnalysis .94ś105.
[33]Hoan Anh Nguyen, Robert Dyer, Tien N Nguyen, and Hridesh Rajan. 2014.
MiningpreconditionsofAPIsinlarge-scalecodecorpus.In Proceedingsofthe22nd
ACMSIGSOFTInternationalSymposiumonFoundationsofSoftwareEngineering .
166ś177.
[34]João Nobre and Rui Ferreira Neves. 2019. Combining principal component
analysis,discretewavelettransformandXGBoosttotradeinthefinancialmarkets.
ExpertSystemswithApplications 125(2019),181ś194.
[35]Jeho Oh, Don Batory, Margaret Myers, and Norbert Siegmund. 2017. Finding
near-optimalconfigurationsinproductlinesbyrandomsampling.In Proceedings
ofthe201711thJointMeetingonFoundationsofSoftwareEngineering .61ś71.
[36]PedroPaulodeMagalhãesOliveiraJr,RicardoNitrini,GeraldoBusatto,Carlos
Buchpiguel,JoãoRicardoSato,andEdsonAmaroJr.2010. UseofSVMmethods
withsurface-basedcorticalandvolumetricsubcorticalmeasurementstodetect
Alzheimer’sdisease. JournalofAlzheimer’sDisease 19,4(2010),1263ś1272.
[37]RandalS.Olson,NathanBartley,RyanJ.Urbanowicz,andJasonH.Moore.2016.
Evaluation of a Tree-based Pipeline Optimization Tool for Automating Data
Science.In ProceedingsoftheGeneticandEvolutionaryComputationConference
2016(Denver,Colorado,USA) (GECCO’16) .ACM,NewYork,NY,USA,485ś492.
https://doi.org/10.1145/2908812.2908918
[38]Randal S. Olson, William La Cava, Patryk Orzechowski, Ryan J. Urbanowicz,
and Jason H. Moore. 2017. PMLB: a large benchmark suite for machine
learning evaluation and comparison. BioData Mining 10, 1 (11 Dec 2017), 36.
https://doi.org/10.1186/s13040-017-0154-4
[39]FabianPedregosa,GaëlVaroquaux,AlexandreGramfort,VincentMichel,Bertrand
Thirion,OlivierGrisel,MathieuBlondel,PeterPrettenhofer,RonWeiss,Vincent
Dubourg,etal .2011. Scikit-learn:MachinelearninginPython. Journalofmachine
learningresearch 12,Oct(2011),2825ś2830.
[40]Philipp Probst, Bernd Bischl, and Anne-Laure Boulesteix. 2018. Tunability:
Importanceofhyperparametersofmachinelearningalgorithms. arXivpreprint
arXiv:1802.09596 (2018).
[41]RadimŘehůřekandPetrSojka.2010. SoftwareFrameworkforTopicModelling
withLargeCorpora.In ProceedingsoftheLREC2010WorkshoponNewChallenges
forNLPFrameworks .ELRA,Valletta,Malta,45ś50.
[42]Stephen Robertson, Hugo Zaragoza, et al .2009. The probabilistic relevance
framework:BM25andbeyond. FoundationsandTrends ®inInformationRetrieval
3,4(2009),333ś389.
[43]NaiyanaSahavechaphanandKajalClaypool.2006. XSnippet:miningforsample
code.InProceedingsofthe21stannualACMSIGPLANconferenceonObject-oriented
programmingsystems,languages,andapplications .413ś430.
[44]EuiChulShin,MiltiadisAllamanis,MarcBrockschmidt,andAlexPolozov.2019.
Programsynthesisandsemanticparsingwithlearnedcodeidioms.In Advances
inNeuralInformationProcessingSystems .10824ś10834.
[45]MicahJSmith,CarlesSala,JamesMaxKanter,andKalyanVeeramachaneni.2020.
Themachinelearningbazaar:HarnessingtheMLecosystemforeffectivesystem
development.In Proceedingsofthe2020ACMSIGMODInternationalConference
onManagementofData .785ś800.
[46]XudongSun,JialiLin,andBerndBischl.2019. Reinbo:Machinelearningpipeline
searchandconfigurationwithbayesianoptimizationembeddedreinforcement
learning. arXivpreprintarXiv:1904.05381 (2019).
[47]CatherineWong,NeilHoulsby,YifengLu,andAndreaGesmundo.2018. Transfer
learningwithneuralautoml.In AdvancesinNeuralInformationProcessingSystems .
8356ś8365.
[48]Quanming Yao, Mengshuo Wang, Yuqiang Chen, Wenyuan Dai, Hu Yi-Qi, Li
Yu-Feng, Tu Wei-Wei, Yang Qiang, and Yu Yang. 2018. Taking human out of
learningapplications:Asurveyonautomatedmachinelearning. arXivpreprint
arXiv:1810.13306 (2018).
774