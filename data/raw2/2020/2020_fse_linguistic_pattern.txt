Automatically Identifying Performance Issue Reports with
Heuristic Linguistic Patterns
Yutong Zhao
Stevens Institute of
Technology
Hoboken, NJ, USA
yzhao102@stevens.eduLu Xiao
Stevens Institute of
Technology
Hoboken, NJ, USA
lxiao6@stevens.eduPouria Babvey
Stevens Institute of
Technology
Hoboken, NJ, USA
pouria.babvey@gmail.comLei Sun
Stevens Institute of
Technology
Hoboken, NJ, USA
lsun18@stevens.edu
Sunny Wong
Analytical Graphics, Inc.
Exton, PA, USA
sunny@computer.orgAngel A. Martinez
Analytical Graphics, Inc.
Exton, PA, USA
amartinez@agi.comXiao Wang
Stevens Institute of
Technology
Hoboken, NJ, USA
xwang97@stevens.edu
ABSTRACT
Performance issues compromise the response time and resource
consumption of a software system. Modern software systems use
issue tracking database to keep track of all kinds of issue reports,
including performance issues. However, performance issues are
largely under-tagged in practice, since the tagging process is volun-
tary and manual. For example, the performance tag rate in Apache‚Äôs
Jira system is below 1%. This paper contributes a novel, hybrid classi-
fication approach, combining linguistic patterns and machine/deep
learning techniques, to automatically detect performance issue
reports. We manually learn from 980 real-life performance issue
reports, and summarize 80 project-agnostic linguistic patterns that
recur in the reports. Our approach uses these linguistic patterns to
construct the sentence-level and issue-level learning features for
training effective machine/deep learning classifiers. We test our
approach on two new datasets, each with 980 unclassified issues
reports. We compare our approach with 31 baseline methods. Our
approach can reach up to 83% precision and up to 59% recall. The
only comparable baseline method is BERT, which is still 25% lower
in theùêπ1-score.
CCS CONCEPTS
‚Ä¢Software and its engineering ‚ÜíRisk management .
KEYWORDS
software performance, performance optimization, software
repositories mining
ACM Reference Format:
Yutong Zhao, Lu Xiao, Pouria Babvey, Lei Sun, Sunny Wong, Angel A.
Martinez, and Xiao Wang. 2020. Automatically Identifying Performance
Issue Reports with Heuristic Linguistic Patterns. In Proceedings of the 28th
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
ESEC/FSE ‚Äô20, November 8‚Äì13, 2020, Virtual Event, USA
¬©2020 Association for Computing Machinery.
ACM ISBN 978-1-4503-7043-1/20/11. . . $15.00
https://doi.org/10.1145/3368089.3409674ACM Joint European Software Engineering Conference and Symposium on the
Foundations of Software Engineering (ESEC/FSE ‚Äô20), November 8‚Äì13, 2020,
Virtual Event, USA. ACM, New York, NY, USA, 12 pages. https://doi.org/10.
1145/3368089.3409674
1 INTRODUCTION
Software performance issues can cause dissatisfaction and drive
users to switch to competitor products [ 1]. Like other types of
software bugs, performance problems are reported to developers
via issue tracking systems. Modern issue tracking systems support
tagging a specific issue report as being performance-related (e.g., via
a‚ÄúLabel‚Äù in Jira). This allows product managers to quickly find and
prioritize addressing such problems. However, manual tagging of
issue reports is tedious and leads to significant under-tagging in
practice. We find that only 5 of the 13 largest open-source Apache
projects have over 1% of issue reports tagged as performance issues‚Äî
when empirical studies [ 2] find that performance issues should be
around 4% to 16%.
Automatic tagging of performance issue reports is an ideal. How-
ever existing analysis techniques are limited to achieve this goal.
Simple techniques, such as keyword matching, tend to find exces-
sive false positives; while machine/deep learning methods struggle
due to the imbalance of issue types for training. For example, our
manual investigation, of almost 2000 randomly selected issues from
Apache‚Äôs Jira system, finds only 7% performance issues. With such
a significantly unbalanced dataset, machine/deep learning models
tend to miss the few positive cases‚Äîleading to high precision and
low recall.
This paper presents an approach to automatically tag perfor-
mance issues, based on linguistic analysis of the issue description.
Our approach stems from the observation that performance-related
issues often contain similar linguistic characteristics at the sentence-
level, agnostic of specific software product. By manually analyzing
almost 1000 issues, we contribute a set of 80 heuristic linguistic
patterns to capture these common linguistic features. Combining
these linguistic patterns with state-of-the-art machine/deep learn-
ing models, we offer a method for practitioners to automatically
identify performance-related issue reports. The advantage of our ap-
proach lies in two aspects: 1) the linguistic patterns provide specific
learning features to pin-point descriptions of performance issues,
1ESEC/FSE ‚Äô20, November 8‚Äì13, 2020, Virtual Event, USA Zhao and Xiao, et al.
while classic NLP features are mostly based on general statistical
models; and 2) our sentence-level classifier plays an important role
in extracting more accurate learning features for issue-level clas-
sification. In fact, incorporating sentence-level analysis improves
our recall by 24% on average.
We evaluated our approach on two different datasets, each con-
taining 980 unclassified issue reports, randomly selected from Apa-
che‚Äôs Jira platform. We compared the sentence-level and issue-level
classifiers in our approach, with a total of 31 baseline methods‚Äî
including keyword matching, and combinations of different ma-
chine/deep learning models and classic NLP features. The results
showed that our approach can identify performance-related issues
with up to 83% precision and 59% recall. In comparison, most state-
of-the-art methods produce only 29% recall.
The rest of this paper is organized as following. Section 2 pro-
vides fundamental background information. Section 3 details our
approach. Section 4 describes our evaluation design and Section 5
explores the evaluation results. Section 6 reviews related prior work,
and Section 7 discusses threats to validity and potential future work.
Finally, Section 8 concludes.
2 BACKGROUND
This paper is tackling a text classification problem. We will intro-
duce the background information of this problem in this section.
a) Linguistic Patterns: ‚ÄúLinguistic patterns are grammatical
rules that allow their users to speak properly in a common lan-
guage‚Äù [3]. Previous research has been using heuristic linguistic
patterns to classify texts [ 4,5]. For example, ‚ÄúAs a xx..., I want to...,
so that... " is a heuristic linguistic pattern, which captures how a user
story is usually described [ 6]. It can be used to automatically match
texts that describe users stories in a large corpus. Sometimes, the
classification is not definite due to the fuzzy nature of the problem.
In these cases, researchers combine heuristic linguistic patterns
with fuzzy logic [ 7‚Äì9]. For example, in the study of Shi et al. , the
authors combine linguistic patterns with fuzzy logic to classify texts
in issue reports into different information types, including Intent,
Benefit, Drawback, Example, Explanation andTrivia . [10]. The tricky
part is that an input text could reasonably relate to multiple infor-
mation types without being definite. Thus, they assign a confidence
value (0 to 1) to each linguistic pattern. The confidence value models
the association between this pattern and a information type. The
linguistic fuzzy model provides interpretability of the classification
process [11, 12].
In this paper, we are dealing with a binary classification: perfor-
mance related or not, but not in between. Thus, fuzziness is not
necessary in our case. The commonality is that we manually learn
heuristic linguistic patterns from a large body of developer-tagged
performance issue reports, similar to the practice of Shi et al. [10].
b) Machine/Deep Learning .Machine learning and deep learn-
ing models are commonly used for text classification [ 13]. They
gain knowledge of classification via large amounts of training based
on manually tagged datasets.
An effective classifier relies on extracting relevant features from
the data for training. A common approach is to transform input
texts into a numerical representation in the form of vectors andmatrix [ 14]. For example, Count Vector works on the frequency
of terms. It is a matrix notation of a corpus, where every row
represents a document from the corpus, every column represents
a term from the corpus, and every cell represents the frequency
count of a particular term in a particular document [15].
However, simply calculating the frequency of terms suffers from
a critical problem that all terms are considered equally important
when it comes to assessing relevancy on a query. Thus, TF-IDF
(Term Frequency - Inverse Document Frequency) is proposed to scale
down the weights of terms with high collection frequency [ 16].
TF-IDF can be generated at different levels of input tokens [ 17]: 1)
Word-Level TF-IDF is a matrix representing scores of every term in
different corpus [ 18]; 2) N-gram Level TF-IDF is the combination
ofNterms together [ 19]; 3) Character Level TF-IDF is a matrix
representing scores of character level N-gram in the corpus [20].
Unlike sparse matrix such as Count Vector ,Word Embedding is a
form of representing words and documents using a dense vector
representation [ 21]. The position of a word within the vector is
learned from text and based on the words that surround it. Word
Embedding can be generated using pre-trained embeddings, such
asGlove [22], FastText [23], and Word2Vec [24].
The used model also impacts the accuracy of the classification.
Machine learning models have been used for addressing various
classification problems. Each model has its unique feature. Naive
Bayes is a classification technique based on Bayes‚Äô Theorem with
an assumption of independence among predictors [ 25‚Äì27].Logistic
regression measures the relationship between the categorical depen-
dent variable and one or more independent variables by estimating
probabilities using a logistic/sigmoid function [ 28].Support Vector
Machine is a supervised machine learning algorithm which can be
used for both classification or regression challenges [ 29,30].Deci-
sion Tree is a flowchart-like structure that are commonly used in
operations research and management [ 31].Random Forest models
are a type of ensemble models, particularly bagging models [ 32].
Extreme Gradient Boosting is a machine learning ensemble meta-
algorithm for primarily reducing bias, and also variance in super-
vised learning, and a family of machine learning algorithms that
convert weak learners to strong ones [33].
Deep learning modesl has gained popularity in text classification,
since they are inspired by how human brain works. There are two
main deep learning models: Convolutional Neural Networks (CNN)
andRecurrent Neural Networks (RNN) . In addition, the BERT model
is the landmark of multi-head attention deep learning models. It has
shown to outperform the previous methods such as CNNs, RNNs,
and among others for a wide variety of natural language process-
ing (NLP) tasks [ 34,35]. Instead of selecting features, Bidirectional
Encoder Representations from Transformers (BERT) benefits from
transfer learning. In transfer learning, the model first is trained
on a large text set to solve some general-purpose by training the
model like language modeling and auto-encoding [ 36]. This step,
regarded as pretraining, prepares the deep learning model to rapidly
learn new downstream tasks. Thus, BERT does not use a classic
method of using features as proxies of contextual information but
the model deep learning weights encode a lot of information about
the language. We simply used token embedding as features for the
BERT model.
2Automatically Identifying Performance Issue Reports with Heuristic Linguistic Patterns ESEC/FSE ‚Äô20, November 8‚Äì13, 2020, Virtual Event, USA
c) Hybrid Approaches: Hybrid approaches combine a base clas-
sifier, i.e. the machine/deep learning model, with a rule based sys-
tem, to improve the classification [ 37]. These hybrid systems can be
easily fine-tuned by adding specific rules for conflicting tags that
haven‚Äôt been correctly modeled by the base classifier.
Our approach resides in this category: we train the classic ma-
chine/deep learning models with the heuristic linguistic patterns
derived learning features. To prove the advantage of our approach,
we will compare with a comprehensive set of baseline methods, by
combining different NLP features and machine/deep learning mod-
els discussed in the previous section. We envision the advantage of
our approach lies in that the heuristic linguistic patterns extracted
from known performance issues provide more accurate learning
features, compared to classic NLP features, which are based on
statistical modeling.
3 APPROACH
Our approach is inspired by the observation that performance-
related issues often include sentences that share similar linguis-
tic characteristics. We capture these common characteristics with
heuristic linguistic patterns, which we empirically derive from ana-
lyzing existing issue reports. Leveraging these linguistic patterns
and machine learning techniques, we can classify new issues as
performance-related or not. Therefore, our approach consists of
two main phases: a preparation/learning phase to build the linguis-
tics patterns and an issue classification phase. We first elaborate on
linguistic patterns before detailing these two phases below.
3.1 Heuristic Linguistic Patterns
Heuristic linguistic patterns allow us to approximate whether some
text relates to the topic of performance. An issue report that de-
scribes the system as ‚Äúrunning slow‚Äù or‚Äúinefficient‚Äù , for example,
would indicate that it likely concerns the system performance.
These patterns offer a project-agnostic method for us to identify
issue-report texts that describe the symptoms, root causes, solutions,
and run-time measurements of performance issues. Our patterns
operate at the sentence-level by analyzing the words and gram-
matical structure of a sentence. Following the approach of Shi et
al. [10], we define four types of linguistic patterns: lexical ,profiling ,
structural , and semantic .
Lexical linguistic patterns extend the basic keyword match con-
cept by additionally considering synonyms, negated antonyms, and
lemmatizations of that keyword/phrase. For example, an efficiency
lexical pattern would include the such terms as efficient ,efficiency ,
inefficient , and inefficiency . Another example, which we call the
infinite_loop pattern, looks for infinite ,forever , orendless ; followed
byloop oriteration .
When a performance issue is identified, developers often use a
profiler to record performance characteristics of the system. We
define profiling linguistic patterns to capture this information, as em-
bedded in an issue report. Usually, the matching issue text contains
a time or memory usage unit (e.g., milliseconds, megabytes), or the
extent of performance change‚Äîmeasured in percentage terms, com-
paring the run-time parameters of before and after a code revision.
Structural linguistic patterns operate on a higher grammatical
level of the sentence structure. Here, we are looking for phrase struc-
tures that imply a performance issue. For example, we observe that
‚Äúwhen . . . run/execute/perform . . . for [NUMBER] seconds/minutes ‚Äù isa common way in different projects to describe performance issues
that happen under a special input.
Semantic linguistic patterns extend lexical patterns by also incor-
porating sentiment analysis [ 38], to capture linguistic expressions
that imply performance issues. For example, our negative_necessary
pattern searches for a common way of describing the root cause
or solution to performance issues that happen under unnecessary
conditions. It searches for the word necessary ,required , oressential
in a sentence that has a negative sentiment.
In the following subsection, we detail how we derive our linguis-
tic patterns for classifying performance-related issues.
3.2 Preparation/Learning Phase
The goal is to extract a comprehensive heuristic linguistic pattern
setfrom known performance issue reports, which can be used
to automatically tag more performance issues in a new dataset.
The raw input is the developer-tagged performance issue reports
on Apache‚Äôs Jira system. The output is a comprehensive heuristic
linguistic pattern set . This part contains five iterative steps.
Issue 
Tracking 
DBPre-Process & 
Manual TagExtract 
HLP
HLP in PiMerge & 
Consolidate Saturated? Yes No
Rank & 
Retrieve
HLP Seti = i+1
End
Issues 
in PiSentences 
in Pii = 1
Evaluate 
& ReflectSentences 
in P1 to Pi
Added to
Sentence 
Classifier(For each)
SentenceHLP
MatcherSentence 
HLP VectorYes/No
Issue HLP 
VectorYes
/No
Issue 
ClassifierY/
N?Yes Sentence TaggingIssue Tagging
Sentence
HLP VectorsIssue
(Sentences)HLP SetML/DL 
Models
ML/DL 
ModelsHLP Set Derivation
Figure 1: Iterative HLP Set Derivation
a) Rank & Retrieve .We rank all Apache Software Foundation
projects in descending order based on the number of developer-
tagged performance issues. On Apache‚Äôs Jira system, developers
defined a special label, named ‚Äúperformance" to tag issues. In each
iteration,ùëñ(starting from 1), we retrieve all the developer-tagged
performance issue reports from the ùëñùë°‚Ñéproject, namely ùëÉùëñ. In each
iteration, issues from ùëÉùëñserve as the source for us to learn how
performance issues are described in practice. We start from projects
with the largest number of tagged performance issues to accelerate
the building process.
b) Pre-Process & Manual Tag .First, we pre-process the is-
sue reports from project ùëÉùëñ. We use the Stanford Core-NLP1to
break each issue report into sentences. For each sentence, we ap-
plied lemmatization, Part-Of-Speech (POS) tagging , Named-Entity-
Recognition (NER) tagging, dependency parsing, and sentimental
analysis using the annotators of Stanford Core-NLP . We apply stan-
dard data cleaning to remove stop words, such as ‚Äúa‚Äù, ‚Äúan‚Äù, ‚Äúthe‚Äù.
Next, we manually tag each sentence in each issue as either per-
formance related or not performance related. The reason is that a
performance issue usually also contains many sentences that do not
carry performance related information. For example, developers
may describe the general background information of an issue or
include social notes. Thus, a sentence must contain description rel-
evant to performance problems, such as the symptoms, the causes,
the optimization solutions, and performance profiling data, to be
considered as performance related. The goal is to identify sentences
that contain reusable information that can help identify similar
1https://stanfordnlp.github.io/CoreNLP/
3ESEC/FSE ‚Äô20, November 8‚Äì13, 2020, Virtual Event, USA Zhao and Xiao, et al.
issues in a different context. A team of five people worked together
on the manual tagging, including a senior researcher, a Ph.D candi-
date, a master student (with 6 years of previous working experience
as product manager), and two senior undergraduate students in
the Software Engineering major. To best avoid bias in the tagging
process, we divided the team into two groups: 1) the Ph.D candidate
and one senior undergraduate; and 2) the master student and the
other senior undergraduate. The senior researcher worked as the
mediator in case of conflicts.
The two members in each group tag the same set of sentences,
and cross-validate their results. Each tagger not only tags the sen-
tences as performance related or not, but also provides comments
that explain why a sentence should be tagged as performance re-
lated. This helps them to be more transparent and definite about
their decision. This also provides reference in the case of disagree-
ment between two taggers. In the case of disagreements, the senior
researcher examines the case, and makes a final decision based on
the grounds of whether general, reusable information for tagging
other performance issues exists in the sentence.
c) Heuristic Linguistic Pattern Extraction .Based on the
tagged performance-related sentences and the respective comments
provided by the taggers, we manually extract and summarize heuris-
tic linguistic patterns that can recur in different contexts for describ-
ing performance problems. This is a combination of automated and
manual processes.
For each sentence that is manually tagged as performance related
inùëÉùëñ, we identify the linguistic properties using the Stanford Core-
NLP. First, we use the Part-Of-Speech Tagger (POS Tagger) to assign
parts of speech tags to each word, such as noun, verb, adjective, etc.
This helps us to extract lexical, structural and semantic patterns.
For example, ‚Äúload_nn" is a lexical heuristic linguistic pattern,
indicating that a sentence must contain keyword ‚Äúload" or ‚Äúloads‚Äù
in the form of a noun, used in describing computation load(s). Sim-
ilarly, ‚Äúnn_by_nn" (structural heuristic linguistic pattern) captures
issues like ‚Äúpixel by pixel", ‚Äúbyte by byte‚Äù, which is often used to
describe a tedious computation process.
Meanwhile, we use Named Entity Recogonizer Tagger (NER Tag-
ger)to capture specific terms for describing time or memory con-
sumption of an issue. This helps to extract profiling heuristic linguis-
tic patterns such as ‚ÄúPercentage" (NER Tagger contains ‚ÄúPERCENT‚Äù)
and‚ÄúDuration‚Äù (NER Tagger contains ‚ÄúDURATION") that describe
the profiling measurements.
Furthermore, Stanford Core-NLP categorizes a sentence in senti-
ment such as positive, neutral, and negative. This helps to extract
semantic heuristic linguistic patterns. Figure 2 shows an example of
a sentence in issue report KAFKA-5512 matches ‚Äúnegative_necessary‚Äù
(semantic heuristic linguistic pattern), since it has keyword ‚Äúunnec-
essary‚Äù and its sentiment is categorized as negative.
By the end of this step, the output is a set of heuristic linguistic
patterns from the project, ùëÉùëñ.
d) Merge & Consolidate .We merge the heuristic linguistic pat-
tern set from project ùëÉùëñwith the set built from previous iterations.
The heuristic linguistic patterns from ùëÉùëñmay overlap/duplicate with
the existing heuristic linguistic pattern set. Thus, we merge overlap-
ping, duplicating, or similar patterns together. For example, ‚Äúinfi-
nite_loop" and‚Äúloop_forever" can be merged to one heuristic linguis-
tic pattern, i.e. ‚Äúloop_infinite/forever , which means that an infinite
(a) Dependencies Analysis
(b) Sentimental Analysis
Figure 2: An Example of Extracting a Semantic Heuristic Lin-
guistic Pattern
loop occurs. While merging, we also consolidate the merged heuris-
tic linguistic pattern through divergent thinking. That is, we add
possible variations of identified rules to be inclusive and predictive.
For example, the example rule, ‚Äúloop_infinite/forever , above, can be
extended to a more predicable rule, ‚Äúloop/iteration_infinite/forever .
e) Evaluate & Reflect .The last step of each iteration is to eval-
uate and reflect the updated heuristic linguistic pattern set. First,
we check whether the heuristic linguistic pattern set has grown
compared to previous iterations. If there is no (or only a few) new
patterns added in the current iteration, it indicates that the heuris-
tic linguistic pattern set is (or close to being) saturated. Next, we
evaluate the precision/recall of the heuristic linguistic pattern set
using the data from the processed projects. We use a naive match-
ing approach: as long as a sentence matches one of the heuristic
linguistic patterns from the set, we consider it as performance-
related. We calculate the precision/recall of this naive tagging by
comparing with our manual tagging. If the precision is low, it means
that the heuristic linguistic patterns can also frequently appear in
non-performance related sentences, implying that the heuristic
linguistic pattern set is irrelevant. If the recall is low, it means that
the heuristic linguistic pattern set is not comprehensive to cap-
ture all different ways that performance issues are presented. As
the heuristic linguistic pattern set grows with iterations, the recall
should increase gradually and become stable when no more new
rules can be found. We call this status as heuristic linguistic pattern
set saturation, which means the heuristic linguistic pattern building
is complete and no more iterations are needed.
3.3 Issue Classification Phase
In the second part, we develop an automatic issue tagging approach
by leveraging the heuristic linguistic pattern set from part 1. The
goal is that given an input issue report, our approach automatically
outputs Yes or No, indicating whether this issue is related to per-
formance problems. Our approach works at two progressive levels:
1) sentence tagging and 2) issue tagging (which depends on the
sentence tagging).
a) Sentence Tagging .First, given an input sentence, our ap-
proach automatically tags it as ‚ÄúYes" or ‚ÄúNo" in terms of performance-
related. For each sentence in an issue, we use a ‚Äú HLP Matcher‚Äù to
calculate a Sentence HLP Vector representing which heuristic lin-
guistic patterns are matched in this sentence. The Sentence HLP
Vector is a binary vector with ùëõdimensions, where ùëõis the total
number of heuristic linguistic patterns identified in part 1. The ùëñùë°‚Ñé
value in the vector is either 0 or 1, indicating whether the sentence
4Automatically Identifying Performance Issue Reports with Heuristic Linguistic Patterns ESEC/FSE ‚Äô20, November 8‚Äì13, 2020, Virtual Event, USA
Issue 
Tracking 
DBPre-Process& 
Manual TagExtract 
Rule
Fuzzy 
Rules in PiMerge & 
Consolidate Saturated? Yes No
Rank & 
Retrieve
Fuzzy 
Rule Seti = i+1
End
Issues 
in PiSentences 
in Pii = 1
Evaluate 
& ReflectSentences 
in P1 to Pi
Added to
Sentence 
Classifier(For each)
SentenceHLP
MatcherSentence 
HLP VectorYes/No
Issue HLP 
VectorYes
/No
Issue 
ClassifierY/
N?Yes Sentence TaggingIssue Tagging
Sentence
HLP VectorsIssue
(Sentences)HLP SetML/DL 
Models
ML/DL 
ModelsFuzzy Rule Building
Figure 3: Performance Issue Classification
matches the ùëñùë°‚Ñéheuristic linguistic pattern. We use the Sentence
HLP Vector as the learning feature to train machine learning and
deep learning algorithms. Given any sentence as input, we can use
the trained model to determine whether it is related to performance
problems.
b) Issue Tagging .Next, we perform the issue-level tagging, built
upon the sentence tagging. The input is an issue report, the output
is also ‚ÄúYes" or ‚ÄúNo" indicating whether this issue report is relevant
to performance problems. Towards this, we collect the Sentence HLP
Vectors of all the sentences that are tagged as ‚ÄúYes" above. We add
these vectors to calculate a weighted Issue HLP Vector , which also
hasùëõdimensions. The ùëñùë°‚Ñévalue in this vector indicates the number
of sentences, which are tagged as ‚ÄúYes" from the previous step and
also matches the ùëñùë°‚Ñérule. Similarly, using the Issue HLP Vector as
the learning feature, we train classic machine learning and deep
learning models to automatically tag the issue.
c) Approach Variations .The unique contribution of this work
is the heuristic linguistic pattern features, i.e. Sentence HLP Vec-
torand Issue HLP Vector . They can be combined with different
machine/deep learning models to provide the variations of our ap-
proach. In this study, we combine the heuristic linguistic pattern
features with 6 classic machine learning models and 2 deep learn-
ing models. Thus we have 8 variations in our approach as shown
in Table 1. Note that each variation can be applied either at the
sentence level, by using Sentence HLP Vector , or the issue level, by
using Issue HLP Vector .
Table 1: Sentence/Issue Tagging Approach Variations
Type Abbre. Model Name Feature
HLP+MLHLP+NB Naive Bayes
HLP+LR Logistic Regression
HLP+SVM Support Vector Machine Sentence HLP Vector
HLP+DT Decision Tree OR
HLP+RF Random Forest Issue HLP Vector
HLP+XGB Extreme Gradient Boosting
HLP+DLHLP+CNN Convolutional Neural Network
HLP+RNN Long Short Term Memory RNN
4 EVALUATION DESIGN
We design our evaluation to answer to following research questions.
RQ1: When can the heuristic linguistic pattern set become
saturated? This RQ investigates the number of projects we
need to review until no more new heuristic linguistic pattern
can be found, which indicates that the heuristic linguistic
pattern set has became saturated.RQ2: How accurate is our sentence tagging approach? We
will compare the sentence tagging precision, recall, and ùêπ1-
score of our approach with that of a comprehensive set of
baseline approaches.
RQ3: How accurate is our issue tagging approach? We will
compare the issue tagging precision, recall, and ùêπ1-score of
our approach with that of a comprehensive set of baseline
approaches.
RQ4: How much does sentence tagging impact the accuracy
of issue tagging? That is, if we directly add the Sentence
HLP Vector of all the sentences in an issue report without
tagging the sentences first to calculate the Issue HLP Vector ,
how much accuracy will be compromised?
4.1 Experiment Setup
Next, we talk about the datasets used in this study, our experiment
setting, and the comprehensive set of baseline approaches that we
compare our approach against.
a) Datasets .We extract three datasets containing real-life issues
from Apache‚Äôs Jira system, for building heuristic linguistic patterns,
as well as for training and testing our approach. Table 2 shows basic
information of the three datasets.
Table 2: Datasets
ID Purpose #Issues (P%) #Sentences (P%)
1 HLP Set Building 980 (100%) 5754 (48%)
2 Homologous Evaluation 980 (8%) 4790 (11%)
3 Heterologous Evaluation 980 (6%) 5371 (6%)
‚Ä¢Dataset 1 is iteratively collected for building the heuristic
linguistic pattern set following Section 3.2. As we will discuss
in more details later, there are 5754 sentences from 980 issues
in 13 projects.
‚Ä¢Dataset 2 is for evaluating how the heuristic linguistic pat-
tern set can identify more untagged performance issues from
the same (i.e. homologous) projects. It contains 980 randomly
selected non-tagged issues from the same projects of the
heuristic linguistic pattern building. Therefore, only 8% of
issues and 11% of sentences are verified as performance re-
lated.
‚Ä¢Dataset 3 is for evaluating whether the heuristic linguistic
patterns are general for tagging issues from heterologous
projects, independent from the heuristic linguistic pattern
building. It contains 980 issues and 5371 sentences from
seven projects other than the projects of heuristic linguistic
pattern building. 6% issues and sentences are verified as
performance related.
We followed the process in Section 3.2 to manually tag each
sentence in the three datasets. For dataset 2 and dataset 3, we also
manually tag each issue report, following similar practice. The
tagger provides comments for each issue to justify their decision
of whether to manually tag this issue as a performance issue or
not. The entire tagging process of three datasets took a total of
approximately 762 human hours for the entire group of 4 taggers
and 1 mediator. Finally, we measured the inter-reliability between
the two tagging teams using the Cohen‚Äôs Kappa agreement. The
agreement is on average 0.8 for tagging the three datasets. This
5ESEC/FSE ‚Äô20, November 8‚Äì13, 2020, Virtual Event, USA Zhao and Xiao, et al.
suggests that the two teams have substantial to almost perfect
agreement regarding whether a sentence or an issue report is related
to performance problems.
Of a particular note, we noticed that containing a performance-
related sentence in an issue report does not warrant a performance
issue. The presence of performance related sentences in an issue
could just serve as problem context or background information,
instead of being the main focus/purpose of the issue. Issue IGNITE-
7849 below is such as an example. The performance-related sentence
is underlined above. However, it is just an assumption related to
the performance of ScanQuery . The main focus of this issue is a
functional improvement related to ‚Äú continuous querying ". Thus this
issue is tag as a ‚ÄúNO".
IGNITE-7849 :‚ÄúCurrently when we want to use the same
predicate for the continuous query and initial query, it‚Äôs easy
enough to write something like this. Assumingwearefine
with theperformance ofScan Query: <CODE SNIP PET>. How-
ever, this becomes more inconvenient when we want to use
SqlQuery in the initial query to take advantage of indexing.
This is obviously not ideal because we have to specify the
predicate in two different ways. A quick Google revealed that
there are products out there that more seamlessly support
this use case of continuous querying. I understand that Ignite
isn‚Äôt built on top of SQL, unlike the commercial RDBMSes I
found, so maybe this is an out-of-scope feature. "
b) Experiment Setting .The iterative process of heuristic lin-
guistic pattern building leads to the dataset 1. In answering RQ1, we
will show how the heuristic linguistic pattern set became saturated
with the growth of dataset 1.
In answering RQ2 (sentence tagging) and RQ3 (issue tagging), we
use both the dataset 2 for the homologous evaluation and the dataset
3 for the heterologous evaluation respectively. For each dataset, we
run the experiment for 20 times. Each time, we randomly divide
the dataset into 70% for training and 30% for testing. The precision,
recall, andùêπ1-score are calculated as the average of the 20 times.
In answering RQ4 (impact of sentence tagging on the accuracy
of issue tagging), we implement a variation of our issue tagging
approach. That is, instead of adding up the Sentence HLP Vectors
for sentences that are tagged as performance-related, we directly
sum the Sentence HLP Vectors of all sentences in an issue. Then, we
compare the precision/recall and F1-score of this variation with our
original issue tagging approach.
c) Comparison Baselines .To answer RQ2 and RQ3, we com-
pare our approach with a total of 31 baseline methods, grouped into
three types of baselines listed in Table 3. Note that, each baseline
approach can also be applied at the sentence level or issue level.
For fair comparison, some baseline methods only compare with
a subset of our approach variations. For example, it is not fair to
compare a deep learning model with a machine learning model.
Thus, we will focus on three comparisons:
(1)(ML+HLP) VS. Baseline 1. We compare 6 of our approach
variations with machine learning models (ML) with baseline
1 methods. The baseline 1 contains 24 different methods. As
shown in Table 3 (from row 3 to row 8), the baseline 1 isTable 3: Comparison Baselines
Baseline 1: ML Models + NLP Features
Abbreviation ML Model NLP Feature
NB+CV|WL|NG|CH Naive Bayes
LR+CV|WL|NG|CH Logistic Regression Count Vectors (CV)
SVM+CV|WL|NG|CH Support Vector Machine Word Level TF-IDF (WL)
DT+CV|WL|NG|CH Decision Tree N-gram Level TF-IDF (NG)
RF+CV|WL|NG|CH Random Forest Character Level TF-IDF (CH)
XGB+CV|WL|NG|CH Extreme Gradient Boosting
Baseline 2: DL Models + NLP Features
Abbreviation DL Models NLP Feature
BERT BERT Classifier Token Embedding
CNN Convolutional Neural Network Word Embedding
RNN-LSTM Long Short Term Memory RNN Word Embedding
RNN-GRU Gated Recurrent Units RNN Word Embedding
Bi-RNN Bidirectional RNN Word Embedding
Baseline 3: Keyword-based Matching
Abbreviation Method
Kw LR Matching Keywords from Literature
Kw HLP Matching Keywords in HLP Set
the combination of 6 classic machine learning models and 4
classic NLP features. For each machine learning model, we
will compare the accuracy of using the heuristic linguistic
pattern features vs. using the NLP features.
(2)(DL+HLP) VS. Baseline 2. We compare 2 of our approach
variations with deep learning models (DL) with baseline 2
methods. As shown in Table 3 (row 11 to row 15), the base-
line 2 contains 5 different approaches using 5 deep learning
models and 2 NLP features.
(3)(ML/DL+HLP) VS. Baseline 3. As shown in Table 3 on the
bottom, the baseline 3 includes: matching common keywords
from literature and matching keywords derived from the lex-
ical heuristic linguistic patterns (since this type can be used
as keywords). We compare all of our approach variations
with baseline 3 for comprehensiveness.
As a particular note, for issue tagging, our approach build the
Issue HLP Vector by filtering out sentences that are not tagged as
performance related. For a fair comparison, in each baseline method
for issue tagging, we also extract the NLP features based only on
sentences that are tagged (by this method) as performance related.
For instance, when using BERT for issue tagging, the feature ‚ÄúToken
Embedding" is extracted from sentences that are tagged by BERT
as performance related.
5 RESULTS
5.1 Heuristic Linguistic Pattern Saturation
(RQ1)
RQ1 : When can the heuristic linguistic pattern set become
saturated? We went through 13 iterations to make sure that the
heuristic linguistic pattern set was saturated. Table 4 shows the in-
formation of the 13 iterations, in terms of the project being studied,
the domain of the project, the tag rate of performance issues, the
number of developer-tagged performance issues, and the number
(and percentage) of manually tagged performance sentences in each
project. We collected 5754 sentences that describe performance re-
lated information from totally 980 issues. Overall, the performance
issue tag rate is only 0.9%, and 48% of the sentences are manually
verified as performance related.
6Automatically Identifying Performance Issue Reports with Heuristic Linguistic Patterns ESEC/FSE ‚Äô20, November 8‚Äì13, 2020, Virtual Event, USA
Figure 4 shows how the overall heuristic linguistic pattern set
reaches saturation. In Figure 4a, the line on the top shows that
the entire heuristic linguistic pattern set grows fast in the first 5
iterations, and it slows down after iteration 6, and becomes sta-
ble in iteration 11. We extracted a total of 80 heuristic linguistic
patterns, with 46% lexical patterns, 38% structural patterns, 9%
semantic patterns, and 8% profiling patterns. The accumulation
process of the four types are detailed in the four lines on the bot-
tom of Figure 4a. Figure 4b shows the precision and recall of using
the updated heuristic linguistic patterns by the end of each itera-
tion to match sentences from all the reviewed data. If a sentence
can match any heuristic linguistic pattern, we consider it as per-
formance related. The precision/recall is calculated by comparing
to our manual tagging. The precision remains constantly at 94%,
indicating that the heuristic linguistic pattern set is precise. The
recall grows from 76% in the first iteration and remains at 87% from
iteration 9 to 13, indicating the the heuristic linguistic pattern set
becomes comprehensive in iteration 9.
Table 4: Heuristic Linguistic Pattern Building Iterations
Iter. Project Domain T.Rate #P.Issues # P.Sentences (%)
1 Impala Query Engine 4% 265 1339 (53%)
2 Oak Repository Tool 2% 151 832 (50%)
3 Ignite Distributed Database 2% 146 767 (44%)
4 Cassandra Database Management 0.9% 124 696 (56%)
5 Spark Analytic Engine 0.2% 52 479 (35%)
6 Hive Database Tool 0.3% 39 194 (43%)
7 SVN Revision Control 1% 38 357 (36%)
8 Mesos Computer Cluster 0.5% 36 161 (53%)
9 Sling Web Framework 0.4% 30 175 (60%)
10 PDFBox PDF Library 0.7% 28 186 (53%)
11 Tapstry-5 Web Framework 1% 27 163 (48%)
12 Lucene Retrieval Library 0.3% 22 210 (49%)
13 Kafka Distributed Streaming 0.4% 22 204 (39%)
Total 0.9% 980 5754 (48%)
Answer to RQ1: The heuristic linguistic pattern set be-
comes saturated after 11 iterations, containing 80 heuristic
linguistic patterns, in the type of lexical (44%), structural
(39%), semantic (10%), and profiling (8%). The heuristic
linguistic pattern set can match performance related sen-
tences in the rule building dataset with precision of 94% and
recall of 87%. This indicates that the heuristic linguistic
pattern set is saturated‚Äîand can precisely and compre-
hensively capture the features of how performance issues
are described in the heuristic linguistic pattern building
dataset.
5.2 Sentence Tagging Accuracy (RQ2)
RQ2: How accurate is our sentence tagging approach? We
evaluated the 8 variations of our approach (see Table 1) on both
dataset 2 and dataset 3 for homologous and heterologous evaluation.
The precision, recall, and ùêπ1-score of the 8 approach variations is
shown in Figure 5. We can make two key observations:
‚Ä¢The performance of the 8 approach variations on dataset 2
and dataset 3 is highly consistent, indicating that the heuris-
tic linguistic pattern set is generally applicable to differ-
ent datasets, including those that are independent from the
heuristic linguistic pattern building.
(a) # HLPs with Iterations
 (b) Precision and Recall
Figure 4: Heuristic Linguistic Pattern Saturation with Itera-
tions
‚Ä¢DT+HLP, RF+HLP, CNN+HLP, and RNN+HLP can quite pre-
cisely (76% to 81% precision) and comprehensively (61% to
79% recall)) capture performance related sentences.
Figure 5: Sentence Tagging Precision, Recall, and ùêπ1-Score
Next, we compare our approach variations with the three base-
line approaches as listed in Table 3. We tested on both dataset 2
and dataset 3, yielding to the same conclusions. Due to space limit,
we present the details based on dataset 2 in the paper, details based
on dataset 3 can be found here2.
(ML+HLP) vs. Baseline 1 .In Figure 6, each sub-figure shows
the comparison of the precision, recall, and ùêπ1-score of a particular
machine learning model, when 1) using the Sentence HLP Vector as
learning feature (above the horizontal dash line) vs. 2) using the four
classic NLP features (below the horizontal dash line). We can make
the following observations: Our sentence tagging is 1.2 to 19 times
better than most baseline 1 methods based on the ùêπ1-score. For ex-
ample, in Figure 6b, the ùêπ1-score in LR+HLP is 57%, comparing with
the 3%ùêπ1-score in LR+NG. The exception is NB+CV and SVM+CV,
compared to which the ùêπ1-score of our approach only outperforms
by 3%. Of a particular note, our approach always has the highest
recall. It is challenging for machine learning approaches to achieve
high recall when using unbalanced data like the performance issues
with low positive cases. For example, the recall of most baseline
1 methods is very low (between 2% and 31%). In comparison, our
DT+HLP and RF+HLP can have 62% and 61% recall, respectively,
which is significantly higher.
(DL+HLP) vs. Baseline 2 .Figure 7 shows the comparison be-
tween 1) two deep learning models based on the sentence HLP Vector
(above the horizontal dash line) vs. 2) five deep learning models
based on classic NLP features (below the horizontal dash line). We
observe that 1) our approach, RNN+HLP, has very high precision
(78%), recall (69%), and ùêπ1-score (73%) compared to the five baseline
2https://sites.google.com/view/fse-2020-hlp
7ESEC/FSE ‚Äô20, November 8‚Äì13, 2020, Virtual Event, USA Zhao and Xiao, et al.
(a) Naive Bayes Classifier
(b) Logistic Regression
(c) Support Vector Machine
(d) Decision Tree
(e) Random Forrest
(f) Extreme Gradient Boosting
Figure 6: (ML+HLP) vs. Baseline 1 on Dataset 2
Figure 7: (DL+HLP) VS. Baseline 2 on Dataset 2
2 methods. In particular, the classic DL models using the NLP fea-
tures also suffer from low recall due to the unbalanced nature of the
performance issue dataset. 2) BERT is comparable to our approach,but still with 13%, 6%, and 9% lower precision, recall, and ùêπ1-score
respectively.
Figure 8: (ML/DL+HLP) VS. Baseline 3 on Dataset 2
(ML/DL+HLP) vs. Baseline 3 .Figure 8 compares the 8 varia-
tions of our approach (above the horizontal dash line) with two
keyword matching methods (below the horizontal dash line). We
observe that: 1) the Kw matching based on keywords from previ-
ous literature yields to very low recall (21%), thus it is not a good
method to tag performance sentences; 2) the Kw HLP method based
on the lexical heuristic linguistic patterns has pretty good precision
(64%) and recall (61%), indicating that the lexical heuristic linguis-
tic patterns play a significant role in sentence tagging; and 3) our
approach, leveraging all 8 types of heuristic linguistic patterns and
combining with ML/DL models, can optimize the precision and
recall by about 15% and 7%, compared to using the lexical heuristic
linguistic pattern in a simple keyword matching manner.
Answer to RQ2: Four of our approach variations, namely
DT+HLP, RF+HLP, CNN+HLP, and RNN+HLP, can pre-
cisely (76% to 81% precision) and comprehensively (61%
to 79% recall) capture performance related sentences in
different datasets, including those that are independent
from heuristic linguistic pattern building. Our approach
is significantly (1.2 to 19 times) better than most baseline
methods. Only two baseline methods, BERT and KW HLP
(matching keywords from the lexical rules), have compa-
rable performance. However, our approach is still 13% to
15% better in precision and 6% to 7% better in recall.
5.3 Issue Tagging Accuracy (RQ3)
RQ3: How accurate is our issue tagging approach? The issue
tagging is built upon the sentence level tagging. As discussed earlier,
there are a total of 39 methods for issue tagging: 8 variations of
our approach, 2 keyword matching methods (baseline 3), 5 deep
learning methods (baseline 2), and 24 (6*4) machine learning meth-
ods (baseline 1). In our approach, we build Issue HLP Vectors as the
learning feature (see Section 3.3). In the baseline methods (used at
the issue-level), we extract the NLP features from sentences that are
tagged by the same method as performance related. For keyword
matching, we tag an issue as performance related as long as there
is one match. We repeat our experiment on three datasets: dataset
2, dataset 3, and their combination, to avoid the interference of
particular datasets and generalize our findings. Figure 9a to Fig-
ure 9c show the precision, recall, and ùêπ1-score of the issue tagging
on dataset 2, dataset 3, and the combined dataset respectively. We
ranked different methods by the ùêπ1-score. We observe that:
8Automatically Identifying Performance Issue Reports with Heuristic Linguistic Patterns ESEC/FSE ‚Äô20, November 8‚Äì13, 2020, Virtual Event, USA
(a) Dataset 2
(b) Dataset 3
(c) Dataset 2 + Dataset 3
(d) Cross Dataset Winners for Issue Tagging
Figure 9: Issue Tagging
‚Ä¢The top five methods based on the three datasets are always
from our approach variations, with up to 90% precision and
up to 64% recall. As shown in Figure 9d, the winners in dif-
ferent datasets are highly consistent. CNN+HLP, XGB+HLP,
RF+HLP, and RNN+HLP consistently rank in the top five
for the three datasets. On average, these four methods have
67% to 83% precision, 51% to 59% recall. This indicates that
these three methods can precisely identify the majority of
performance issues from different datasets.
‚Ä¢The only comparable baseline method for issue tagging is
BERT, whose ùêπ1-score is still averagely 22% to 25% lower
than CNN+HLP, XGB+HLP, RF+HLP, and RNN+HLP.
Answer to RQ3: Four of our approach variations,
CNN+HLP, XGB+HLP, RF+HLP, and RNN+HLP, con-
stantly outperform the other methods in the three datasets,
reaching 67% to 83% precision and 51% to 57% recall. The
only comparable baseline approach is BERT, whose ùêπ1-
score is still 22% to 25% lower.
5.4 Impact of Sentence Tagging (RQ4)
RQ4: How much does the sentence tagging impact the ac-
curacy of the issue tagging? This RQ examines how the preci-
sion/recall and ùêπ1-score of our approach will be impacted by not
performing sentence tagging prior to issue tagging. That is, we
calculate the Issue HLP Vector by adding the Sentence HLP Vector ofall the sentences in an issue, without filtering out sentences that
are tagged as not related to performance.
(a) Dataset 2
(b) Dataset 3
Figure 10: Impact of Sentence Tagging on Issue Tagging
We conducted two experiments on dataset 2 and dataset 3, and
the results are shown in Figure 10a and Figure 10b respectively. For
both datasets, the precision and recall of each approach variation
are improved non-trivially, on average 22% for the precision and 20%
for the recall. Of a particular note, in the experiment with dataset
2, the precision of RF+HLP and DT+HLP increased by 31% and
39% with sentence tagging. This is because the recall in sentence
tagging of these two methods have been significantly compromised
by 61% and 62% respectively. Therefore, the sentence tagging is an
important step before the issue tagging for optimizing the recall in
tagging unbalanced data with a small percentage of performance
issues.
Answer to RQ4: Sentence tagging as a pre-step can im-
prove the issue tagging in precision (averagely 22%) and
recall (averagely 20%), because it ensures that the issue-
level features are built from more accurate input.
6 RELATED WORK
6.1 Issue Categorization
Software developers, testers and customers routinely submit issue
reports to the issue tracking systems to record the problems they
observe or requests they have. Due to the large amount of issue
reports, previous research has developed different approaches to au-
tomatically categorize issue reports into different types of interests.
This helps developers to prioritize different tasks.
Antoniol et al. showed that automatic classifiers such as Decision
Trees ,Naive Bayes and Logistic Regression are able to distinguish
bugs from other kinds of issues such as enhancement and refactor-
ing [ 39]. Pandey et al. compared more classifiers for distinguishing
bugs from non-bugs, including Naive Bayes ,Logistic Regression ,
k-Nearest Neighbors ,Support Vector Machine ,Decision Tree , and Ran-
dom Forest . They found that Random Forest performs best in the
text mining of bugs [ 40]. Some prior studies aim at classifying issue
reports into more diverse categories. Ohira et al. manually reviewed
4000 issues reports in Apache‚Äôs Jira system and classified the bugs
impacted on products into security, performance, and breakage
bugs [ 41,42]. Limsettho developed an unsupervised framework
that can automatically group bug reports together based on their
9ESEC/FSE ‚Äô20, November 8‚Äì13, 2020, Virtual Event, USA Zhao and Xiao, et al.
textual similarity [ 43]. However, their framework labels issue re-
ports using the most frequently appearing words rather than the
labels that are already used by labels submitted by users of the issue
tracking system. Thung et al. [44] and Chawla et al. [45] developed
automated approaches that can automatically classify issue reports
into different types such as functional bugs, security bugs, refac-
toring improvements, etc. But performance is not considered as a
specific type by them.
In addition, some prior studies focused on internal content of
issue reports. Shi et al. leveraged linguistic fuzzy rules to classify sen-
tences in feature requests issue reports into six categories, namely
Intent, Benefit, Drawback, Example, Explanation andTrivia [10]. Ag-
garwal et al. used machine learning models to detect duplicate issue
reports [46].
Futhermore, some prior studies focused on classifying the sever-
ity, priority, and complexity of issue reports. Lamkanfi et al. pro-
posed a technique to identify bug severity [ 47]. Tian et al. used
machine learning classifiers on issue reports as well as code bases
to predict bug priorities [ 48]. Zhang et al. proposed a Markov-based
method for estimating bug fixing time [49].
6.2 Performance Issue Analysis
A rich body of prior studies have focused on performance issue
analysis from different perspectives [ 1,2,50‚Äì55]. Recent studies
that rely on real life performance issues use keyword matching and
manual verification to extract dataset. The most common keywords
include ‚Äúfast, slow, perform, latency, throughput, optimize, speed,
heuristic, waste, efficient, unnecessary, redundant, too many times, lot
of time, too much time‚Äù [51,56,57]. However, the keyword matching
largely compromise the accuracy of the retrieved data. In addition,
there could be many different ways to describe performance issues
beyond what is captured in known keywords. Thus, due to the lack
of rigor in the keyword matching approach, researchers have to
invest a large amount of time on manual verification.
In addition, prior studies usually focused on a specific type of
performance bugs based on limited number of issue reports. Nistor
et al. studied 150 performance bugs caused by inefficient loop [ 58].
Yuet al. studied 106 performance bugs caused by synchronization
bottlenecks. Selakovic et al. presented an empirical study of 98
performance bugs written in JavaScript language in both [ 53]. Our
study presented an adequate and diversified dataset of more than
1000 real-life performance issues, which can be used as a ground
truth dataset for future studies on performance bugs.
7 DISCUSSION
a) Limitations: First, we did not test our approach on datasets
from other than the Apache Jira platform, such as Bugzilla and
GNATS . However, since Apache Jira is one of the most widely used
platforms by real-world software projects. In addition, our ground
truth data covers various, common domains as shown in Table 4;
while the testing dataset 3 comes from seven new projects and
includes three new domains: a mobile development framework, a
messaging server, and an enterprise search platform. Therefore,
we believe our approach is general. Second, the heuristic linguistic
patterns capture how practitioners describe performance problems
in general terms. If issues are described only in project specific
terms, understood only by project experts, our approach will becompromised. Lastly, we have not separately evaluated the accu-
racy of our approach on datasets of different domains. We believe
that different heuristic linguistic patterns may provide difference
accuracy for different domain data. A possible solution to the last
two limitations is to tune the heuristic linguistic patterns with
project/domain specific concepts.
b) Validity: First, we acknowledge that the manual tagging of
performance related sentences and issues could pose internal threat
to validity. Any manual effort is subjective to bias derived from indi-
vidual expertise and understanding. The taggers are not intimately
familiar with the reviewed projects, but this should not compro-
mise their ability to recognize general performance problems from
reading the issue texts (e.g., ‚Äúexpensive/unnecessary recalculation‚Äù).
Actually, it is our intention not to rely on project experts, as our goal
is to derive general and transferable linguistic patterns. In addition,
we tried to mitigate the risk posed by the manual tagging of ground
truth dataset by allowing multiple taggers to cross-validate results,
and by asking for tagging comments to increase transparency. Sec-
ond, we cannot guarantee that the 80 heuristic linguistic patterns
cover all possible patterns in real-life performance issues. Lastly,
we did not test whether our approach still outperforms the state-
of-the-art approaches, when classic data balancing technique such
as data swapping [ 59] and resampling [ 60] are applied. However,
we manually controlled the density of positive issues to be 50%,
which mimics a balanced dataset. Our experiment shows that, the
advantage of our approach on balanced dataset lies in the up to 13%
higher precision. We believe that practitioners favor precision over
recall in an issue tagging approach, as excessive false positives can
lead them to stop using it.
c) Future work: We are motivated to optimize our approach
on larger scale and more diverse datasets. In particular, we plan
to polish our approach leveraging datasets from different issue
tracking platforms, such as Bugzilla andGNATS . We also plan to
leverage different types of heuristic linguistic patterns to facilitate
the understanding and management of performance issues. For
example, profiling linguistic patterns can be used to investigate the
dynamic features of different performance issues. We plan to extend
the usage of heuristic linguistic patterns to detect other types of
issues, such as security issues. We also plan to explore how the
available amount of training data will impact the performance of
our approach and the baselines.
8 CONCLUSION
In this paper, we contribute a hybrid classification approach, com-
bining classic machine/deep learning models and the heuristic lin-
guistic patterns, to automatically detect performance issues. We
learned and derived a comprehensive heuristic linguistic pattern
set with 80 patterns, from 980 developer-tagged performance issues
from the Apache‚Äôs Jira Platform. We used the heuristic linguistic
pattern set to extract learning features that can accurately pin-point
performance problem descriptions, and use these features train dif-
ferent machine/deep learning models. We evaluated our approach
on two different datasets, each containing 980 unclassified issue
reports, randomly selected from Apache‚Äôs Jira platform. The re-
sults showed that our approach can identify performance-related
issues with up to 83% precision and 59% recall, which has obvious
advantage over 31 baseline methods, including BERT.
10Automatically Identifying Performance Issue Reports with Heuristic Linguistic Patterns ESEC/FSE ‚Äô20, November 8‚Äì13, 2020, Virtual Event, USA
REFERENCES
[1]Shahed Zaman, Bram Adams, and Ahmed E Hassan. A qualitative study on
performance bugs. In Proceedings of the 9th IEEE Working Conference on Mining
Software Repositories , pages 199‚Äì208. IEEE Press, 2012.
[2]Adrian Nistor, Tian Jiang, and Lin Tan. Discovering, reporting, and fixing perfor-
mance bugs. In Proceedings of the 10th Working Conference on Mining Software
Repositories , pages 237‚Äì246. IEEE Press, 2013.
[3]Alberto Rodrigues da Silva. Linguistic patterns and linguistic styles for require-
ments specification (i) an application case with the rigorous rsl/business-level
language. In Proceedings of the 22nd European Conference on Pattern Languages
of Programs , pages 1‚Äì27, 2017.
[4]Hisao Ishibuchi, Tomoharu Nakashima, and Tadahiko Murata. Three-objective
genetics-based machine learning for linguistic rule extraction. Information sci-
ences , 136(1-4):109‚Äì133, 2001.
[5]Prerna Chikersal, Soujanya Poria, Erik Cambria, Alexander Gelbukh, and
Chng Eng Siong. Modelling public sentiment in twitter: using linguistic patterns
to enhance supervised learning. In International Conference on Intelligent Text
Processing and Computational Linguistics , pages 49‚Äì65. Springer, 2015.
[6]Laurens M√ºter, Tejaswini Deoskar, Max Mathijssen, Sjaak Brinkkemper, and
Fabiano Dalpiaz. Refinement of user stories into backlog items: Linguistic struc-
ture and action verbs. In International Working Conference on Requirements
Engineering: Foundation for Software Quality , pages 109‚Äì116. Springer, 2019.
[7] Lotfi A Zadeh. Fuzzy sets. Information and control , 8(3):338‚Äì353, 1965.
[8]Lotfi A Zadeh. The concept of a linguistic variable and its application to approxi-
mate reasoning‚Äîii. Information sciences , 8(4):301‚Äì357, 1975.
[9]Maria Jose Gacto, Rafael Alcal√°, and Francisco Herrera. Interpretability of lin-
guistic fuzzy rule-based systems: An overview of interpretability measures. In-
formation Sciences , 181(20):4340‚Äì4360, 2011.
[10] Lin Shi, Celia Chen, Qing Wang, Shoubin Li, and Barry Boehm. Understanding
feature requests by leveraging fuzzy method and linguistic analysis. In Proceedings
of the 32nd IEEE/ACM International Conference on Automated Software Engineering ,
pages 440‚Äì450. IEEE Press, 2017.
[11] Ebrahim H Mamdani. Application of fuzzy algorithms for control of simple
dynamic plant. In Proceedings of the institution of electrical engineers , volume 121,
pages 1585‚Äì1588. IET, 1974.
[12] EH Mamdani and S Assilian. An experiment in linguistic synthesis with a fuzzy
logic controller. International journal of human-computer studies , 51(2):135‚Äì147,
1999.
[13] Fabrizio Sebastiani. Machine learning in automated text categorization. ACM
computing surveys (CSUR) , 34(1):1‚Äì47, 2002.
[14] Justin Martineau, Tim Finin, Anupam Joshi, and Shamit Patel. Improving binary
classification on text problems using differential word features. In Proceedings
of the 18th ACM conference on Information and knowledge management , pages
2019‚Äì2024, 2009.
[15] C Jashubhai Rameshbhai and Joy Paulose. Opinion mining on newspaper head-
lines using svm and nlp. International Journal of Electrical and Computer Engi-
neering (IJECE) , 9(3):2152‚Äì2163, 2019.
[16] Ho Chung Wu, Robert Wing Pong Luk, Kam Fai Wong, and Kui Lam Kwok.
Interpreting tf-idf term weights as making relevance decisions. ACM Transactions
on Information Systems (TOIS) , 26(3):1‚Äì37, 2008.
[17] Matthias Eck, Stephan Vogel, and Alex Waibel. Low cost portability for statistical
machine translation based on n-gram frequency and tf-idf. In International
Workshop on Spoken Language Translation (IWSLT) 2005 , 2005.
[18] Donghwa Kim, Deokseong Seo, Suhyoun Cho, and Pilsung Kang. Multi-co-
training for document classification using various document representations:
Tf‚Äìidf, lda, and doc2vec. Information Sciences , 477:15‚Äì29, 2019.
[19] Man Li, Cheng Ling, and Jingyang Gao. An efficient cnn-based classification on
g-protein coupled receptors using tf-idf and n-gram. In 2017 IEEE Symposium on
Computers and Communications (ISCC) , pages 924‚Äì931. IEEE, 2017.
[20] Judit Acs, L√°szl√≥ Grad-Gyenge, and Thiago Bruno Rodrigues de Rezende Oliveira.
A two-level classifier for discriminating similar languages. In Proceedings of the
Joint Workshop on Language Technology for Closely Related Languages, Varieties
and Dialects , pages 73‚Äì77, 2015.
[21] Omer Levy and Yoav Goldberg. Dependency-based word embeddings. In Proceed-
ings of the 52nd Annual Meeting of the Association for Computational Linguistics
(Volume 2: Short Papers) , pages 302‚Äì308, 2014.
[22] Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global
vectors for word representation. In Proceedings of the 2014 conference on empirical
methods in natural language processing (EMNLP) , pages 1532‚Äì1543, 2014.
[23] Ben Athiwaratkun, Andrew Gordon Wilson, and Anima Anandkumar. Proba-
bilistic fasttext for multi-sense word embeddings. arXiv preprint arXiv:1806.02901 ,
2018.
[24] Yoav Goldberg and Omer Levy. word2vec explained: deriving mikolov et al.‚Äôs
negative-sampling word-embedding method. arXiv preprint arXiv:1402.3722 ,
2014.
[25] Jingnian Chen, Houkuan Huang, Shengfeng Tian, and Youli Qu. Feature selec-
tion for text classification with na√Øve bayes. Expert Systems with Applications ,36(3):5432‚Äì5435, 2009.
[26] Andrew McCallum, Kamal Nigam, et al. A comparison of event models for naive
bayes text classification. In AAAI-98 workshop on learning for text categorization ,
volume 752, pages 41‚Äì48. Citeseer, 1998.
[27] Fuchun Peng and Dale Schuurmans. Combining naive bayes and n-gram language
models for text classification. In European Conference on Information Retrieval ,
pages 335‚Äì350. Springer, 2003.
[28] David W Hosmer Jr, Stanley Lemeshow, and Rodney X Sturdivant. Applied logistic
regression , volume 398. John Wiley & Sons, 2013.
[29] Aixin Sun, Ee-Peng Lim, and Ying Liu. On strategies for imbalanced text classifi-
cation using svm: A comparative study. Decision Support Systems , 48(1):191‚Äì201,
2009.
[30] Fabrice Colas and Pavel Brazdil. Comparison of svm and some older classification
algorithms in text classification tasks. In IFIP International Conference on Artificial
Intelligence in Theory and Practice , pages 169‚Äì178. Springer, 2006.
[31] S Rasoul Safavian and David Landgrebe. A survey of decision tree classifier
methodology. IEEE transactions on systems, man, and cybernetics , 21(3):660‚Äì674,
1991.
[32] Mahesh Pal. Random forest classifier for remote sensing classification. Interna-
tional Journal of Remote Sensing , 26(1):217‚Äì222, 2005.
[33] Tianqi Chen, Tong He, Michael Benesty, Vadim Khotilovich, and Yuan Tang.
Xgboost: extreme gradient boosting. R package version 0.4-2 , pages 1‚Äì4, 2015.
[34] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. Attention is all you need.
InAdvances in neural information processing systems , pages 5998‚Äì6008, 2017.
[35] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-
training of deep bidirectional transformers for language understanding. arXiv
preprint arXiv:1810.04805 , 2018.
[36] Sebastian Ruder, Matthew E Peters, Swabha Swayamdipta, and Thomas Wolf.
Transfer learning in natural language processing. In Proceedings of the 2019
Conference of the North American Chapter of the Association for Computational
Linguistics: Tutorials , pages 15‚Äì18, 2019.
[37] Julio Villena-Rom√°n, Sonia Collada-P√©rez, Sara Lana-Serrano, and Jos√© Carlos
Gonz√°lez-Crist√≥bal. Hybrid approach combining machine learning and a rule-
based expert system for text categorization. In Twenty-Fourth International
FLAIRS Conference , 2011.
[38] Richard Tong. An operational system for detecting and tracking opinions in
on-line discussions. In Working Notes of the SIGIR Workshop on Operational Text
Classification , pages 1‚Äì6, 2001.
[39] Giuliano Antoniol, Kamel Ayari, Massimiliano Di Penta, Foutse Khomh, and
Yann-Ga√´l Gu√©h√©neuc. Is it a bug or an enhancement? a text-based approach to
classify change requests. In Proceedings of the 2008 conference of the center for
advanced studies on collaborative research: meeting of minds , pages 304‚Äì318, 2008.
[40] Nitish Pandey, Debarshi Kumar Sanyal, Abir Hudait, and Amitava Sen. Automated
classification of software issue reports using machine learning techniques: an
empirical study. Innovations in Systems and Software Engineering , 13(4):279‚Äì297,
2017.
[41] Yutaro Kashiwa, Hayato Yoshiyuki, Yusuke Kukita, and Masao Ohira. A pilot
study of diversity in high impact bugs. In 2014 IEEE International Conference on
Software Maintenance and Evolution , pages 536‚Äì540. IEEE, 2014.
[42] Masao Ohira, Yutaro Kashiwa, Yosuke Yamatani, Hayato Yoshiyuki, Yoshiya
Maeda, Nachai Limsettho, Keisuke Fujino, Hideaki Hata, Akinori Ihara, and
Kenichi Matsumoto. A dataset of high impact bugs: Manually-classified issue re-
ports. In 2015 IEEE/ACM 12th Working Conference on Mining Software Repositories ,
pages 518‚Äì521. IEEE, 2015.
[43] Nachai Limsettho, Hideaki Hata, Akito Monden, and Kenichi Matsumoto. Auto-
matic unsupervised bug report categorization. In 2014 6th International Workshop
on Empirical Software Engineering in Practice , pages 7‚Äì12. IEEE, 2014.
[44] Ferdian Thung, David Lo, and Lingxiao Jiang. Automatic defect categorization.
In2012 19th Working Conference on Reverse Engineering , pages 205‚Äì214. IEEE,
2012.
[45] Indu Chawla and Sandeep K Singh. Automatic bug labeling using semantic
information from lsi. In 2014 Seventh International Conference on Contemporary
Computing (IC3) , pages 376‚Äì381. IEEE, 2014.
[46] Karan Aggarwal, Finbarr Timbers, Tanner Rutgers, Abram Hindle, Eleni Stroulia,
and Russell Greiner. Detecting duplicate bug reports with software engineering
domain knowledge. Journal of Software: Evolution and Process , 29(3):e1821, 2017.
[47] Ahmed Lamkanfi, Serge Demeyer, Emanuel Giger, and Bart Goethals. Predicting
the severity of a reported bug. In 2010 7th IEEE Working Conference on Mining
Software Repositories (MSR 2010) , pages 1‚Äì10. IEEE, 2010.
[48] Yuan Tian, David Lo, Xin Xia, and Chengnian Sun. Automated prediction of
bug report priority using multi-factor analysis. Empirical Software Engineering ,
20(5):1354‚Äì1383, 2015.
[49] Hongyu Zhang, Liang Gong, and Steve Versteeg. Predicting bug-fixing time:
an empirical study of commercial software projects. In 2013 35th International
Conference on Software Engineering (ICSE) , pages 1042‚Äì1051. IEEE, 2013.
[50] Sebastian Baltes, Oliver Moseler, Fabian Beck, and Stephan Diehl. Navigate,
understand, communicate: How developers locate performance bugs. In Empirical
11ESEC/FSE ‚Äô20, November 8‚Äì13, 2020, Virtual Event, USA Zhao and Xiao, et al.
Software Engineering and Measurement (ESEM), 2015 ACM/IEEE International
Symposium on , pages 1‚Äì10. IEEE, 2015.
[51] Guoliang Jin, Linhai Song, Xiaoming Shi, Joel Scherpelz, and Shan Lu. Under-
standing and detecting real-world performance bugs. ACM SIGPLAN Notices ,
47(6):77‚Äì88, 2012.
[52] Yepang Liu, Chang Xu, and Shing-Chi Cheung. Characterizing and detecting
performance bugs for smartphone applications. In Proceedings of the 36th Inter-
national Conference on Software Engineering , pages 1013‚Äì1024. ACM, 2014.
[53] Marija Selakovic and Michael Pradel. Performance issues and optimizations in
javascript: an empirical study. In Proceedings of the 38th International Conference
on Software Engineering , pages 61‚Äì72. ACM, 2016.
[54] Linhai Song and Shan Lu. Statistical debugging for real-world performance
problems. In ACM SIGPLAN Notices , volume 49, pages 561‚Äì578. ACM, 2014.
[55] Yutong Zhao, Lu Xiao, Wang Xiao, Bihuan Chen, and Yang Liu. Localized or
architectural: an empirical study of performance issues dichotomy. In 2019
IEEE/ACM 41st International Conference on Software Engineering: Companion
Proceedings (ICSE-Companion) , pages 316‚Äì317. IEEE, 2019.[56] Michael Pradel, Markus Huggler, and Thomas R Gross. Performance regression
testing of concurrent classes. In Proceedings of the 2014 International Symposium
on Software Testing and Analysis , pages 13‚Äì25. ACM, 2014.
[57] Zhifei Chen, Bihuan Chen, Lu Xiao, Xiao Wang, Lin Chen, Yang Liu, and Baowen
Xu. Speedoo: prioritizing performance optimization opportunities. In Proceedings
of the 40th International Conference on Software Engineering , pages 811‚Äì821. ACM,
2018.
[58] Adrian Nistor, Po-Chun Chang, Cosmin Radoi, and Shan Lu. Caramel: Detecting
and fixing performance problems that have non-intrusive fixes. In Software Engi-
neering (ICSE), 2015 IEEE/ACM 37th IEEE International Conference on , volume 1,
pages 902‚Äì912. IEEE, 2015.
[59] Vladimir Estivill-Castro and Ljiljana Brankovic. Data swapping: Balancing pri-
vacy against precision in mining for logic rules. In International Conference on
Data Warehousing and Knowledge Discovery , pages 389‚Äì398. Springer, 1999.
[60] Weicai Zhong, Bijan Raahemi, and Jing Liu. Learning on class imbalanced data
to classify peer-to-peer applications in ip traffic using resampling techniques. In
2009 International Joint Conference on Neural Networks , pages 3548‚Äì3554. IEEE,
2009.
12