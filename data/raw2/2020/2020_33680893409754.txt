IsNeuronCoveragea Meaningful MeasureforTesting Deep
Neural
Networks?
Fabrice Harel-Canada
fabricehc@cs.ucla.edu
UCLA, USALingxiao Wang
lingxw@cs.ucla.edu
UCLA, USAMuhammadAli Gulzar
gulzar@cs.vt.edu
Virgina Tech,USA
Quanquan Gu
qgu@cs.ucla.edu
UCLA, USAMiryungKim
miryung@cs.ucla.edu
UCLA, USA
ABSTRACT
Recentefforttotestdeeplearningsystemshasproducedanintuitive
andcompellingtestcriterioncalledneuroncoverage(NC),which
resemblesthenotionoftraditionalcodecoverage.NCmeasuresthe
proportionofneuronsactivatedinaneuralnetworkanditisimplic-
itlyassumedthatincreasingNCimprovesthequalityofatestsuite.
Inanattempttoautomaticallygenerateatestsuitethatincreases
NC, we design a novel diversity promoting regularizer that can be
pluggedinto existing adversarialattack algorithms.We then assess
whethersuchattemptstoincreaseNCcouldgenerateatestsuite
that (1)detects adversarial attacks successfully, (2) produces natural
inputs, and (3) is unbiased to particular class predictions. Contrary
toexpectation,ourextensiveevaluationfindsthatincreasingNC
actuallymakesitharder to generate aneffective test suite: higher
neuroncoverageleadstofewerdefectsdetected,lessnaturalinputs,
andmorebiasedpredictionpreferences.Ourresultsinvokeskep-
ticismthatincreasingneuroncoveragemaynotbeameaningful
objective for generating tests for deep neural networks and call for
a new test generation technique that considers defect detection,
naturalness, andoutputimpartiality in tandem.
CCS CONCEPTS
·Software and its engineering →Software testing and de-
bugging;Software reliability ;·Computing methodologies →
Neuralnetworks.
KEYWORDS
Testing, Software Engineering, Machine Learning, Neuron Cover-
age, Adversarial Attack
ACM Reference Format:
Fabrice Harel-Canada, Lingxiao Wang, Muhammad Ali Gulzar, Quanquan 
Gu, and Miryung Kim. 2020. Is Neuron Coverage a Meaningful Measure for
Testing Deep Neural Networks?. In Proceedings of the 28th ACM Joint Euro-
pean Software Engineering Conference and Symposium on the Foundations 
of Software Engineering (ESEC/FSE ’20), November 8ś13, 2020, Virtual Event,
USA. ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/3368089.
3409754
ESEC/FSE ’20, November 8ś13, 2020, Virtual Event, USA
© 2020 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-7043-1/20/11.
https://doi.org/10.1145/3368089.34097541 INTRODUCTION
Extensive progress in machine learning has enabled computers
to model expected behavior with minimal human guidance and
hasledtoitsintegrationintomanysafety-criticalsystems[ 5,24].
Sinceallsoftwareispronetounanticipatedandundesirabledefects,
creating test suites and assessing their quality is an important part
of building confidence duringthe software lifecycle.
To assess the test adequacy of neural networks, prior work pro-
posed neuron coverage (NC) [ 47] and its variants [ 37,58]. This
notionofNCbuildsontheintuitionofcodecoverage,whilstrec-
ognizing the unique challenges and structures of neural networks.
NCdescribestheproportionofneuronsactivatedbeyondagiven
threshold. The intuition here is that NC captures the magnitude
of individual neuron activations independently and thus serves as
a proxy for observing model behavior. Based on the implicit as-
sumptionthatincreasingNCcanimprovetestsuitequality,NCwas
usedtoguidetestgeneration[ 47,58].Priorworkfoundpreliminary
evidencethatNCiscorrelatedwithdefectdetectioncapability[ 58].
To systematically increase NC during test generation, we de-
velopanovel diversity-promotingregularizer thatcanbeplugged
into existing adversarial attack algorithms such as PGD[ 39] and
CW[8].Thisregularizerpenalizesskewedlayer-wiseactivationsto
promotemorediverseneuronactivationdistributions.Asaresult,
our regularizer can be added to augment existing adversarial at-
tack methods so that these methods can induce previously inactive
neurons to fire and thereby increase NC. While prior work [ 47,58]
has attempted to improve a few neurons’ activation magnitudes at
each optimization step, our diversity-promoting regularizer makes
this process more systematic by incorporating NC increase and
diversification intothe optimization objective.
Wethenassessthegeneratedtestsuitesusingthreecriteria.The
first isdefect detection capability, i.e., the ability to detect adver-
sarialattacks.Thesecondisthe naturalness ofthegeneratedtest
inputsandweusetheInceptionScore(IS)[ 4,51]andtheFrèchet
Inception Distance (FID) [ 17,42] to assess how realistic the gener-
atedtestinputsare.Thethirdcriterionis outputimpartiality,the
degreetowhichmodelpredictionsarebiased(orunbiased)towards
particular class labels. Assessing impartiality is inspired by the
output-uniqueness test selection criteria [ 2], as the test suite must
exercisediverseoutputbehaviorandshouldnotpreferonlyafew
outputvalues.WequantifyoutputimpartialityviaPielou’seven-
ness [49], an entr opy-based measure [54] from the field of ecology.
* Thisresearchwas donewhile the third authorwas a graduate studentat UCLA.
851This work is licensed under a Creative Commons Attribution International 4.0 License.
ESEC/FSE ’20, November8ś13,2020,VirtualEvent, USA FabriceHarel-Canada,Lingxiao Wang,MuhammadAliGulzar,QuanquanGu,andMiryungKim
Equipped with the above evaluation metrics and the novel di-
versity promoting input generation method, we investigate the
trade-offs between neuron coverage, defect detection, naturalness
andoutputimpartiality.Westudytwoimageclassificationdatasets
(MNIST and CIFAR10), one autonomous vehicle dataset (Udac-
ity Self-Driving Car), six classification-based DNN models, two
regression-basedDNNmodels,andtwoattackalgorithms(CWand
PGD). In total, 2095 test suites, over 200,000 images, are gener-
ated. Each test suite represents a different configuration of models,
datasets,attackalgorithms,andhyperparametercombinationsused
for targeting certain layers and promoting diversity in neuron acti-
vations.ThisextensiveanalysisfindsthatincreasingNCactually
makesitharder to generatean effective test suite.
(1)Defect Detection : Only 2 out of 64 experimental results
supported the hypothesis that NC is both strongly and posi-
tivelycorrelatedwithdefectdetection(i.e.,adversarialattack
success), whereas 33 were negatively correlated, implying
that increasing NC islikely to harmdefectdetection.
(2)Naturalness :Only1outof64resultssupportedthehypoth-
esisthatNCisbothstronglyand positivelycorrelated with
therealismandnaturalnessoftheinputs,whereas44were
negativelycorrelated, implyingthat increasing NC islikely
to make the generatedinputsmore unnatural.
(3)OutputImpartiality :Only3outof64resultssupportedthe
hypothesisthatNCisbothstronglyandpositivelycorrelated
with impartiality in output predictions, whereas 21 were
negatively correlated. Certain class labels have higher NC
by default and the process of increasing NC in fact biases
perturbations towardsthoseoutputclass labels.
Our key contributionsare summarizedas follows:
•We develop a novel regularization technique that can be
seamlessly integrated into existing adversarial attack meth-
ods to promote neural activation diversity and increase neu-
roncoverageduringtest suite generation.
•WeadopttheInceptionScore(IS)[ 51]andFrèchetInception
Distance(FID)[ 17]asgeneric,scalable,andautomaticmeans
ofevaluatingnaturalness.Weare thefirsttoapplyPielou’s
evenness [ 49] to examine the previously under-investigated
issueofoutputimpartiality intest suites.
•We conduct extensive evaluations to show that NC is nei-
therpositivelynorstronglycorrelatedwithattacksuccess,
inputrealism,andoutputimpartiality,whichweargueare
importantpropertiesto considerwhen testingDL systems.
•We put forwardthe completecodeand artifactsto automati-
callygeneratetestsuitesandreplicateourempiricalanalysis
athttps://doi.org/10.5281/zenodo.4021473
Overall, our findings invoke skepticism that neuron coverage
maynotbeameaningfulmeasurefortestingdeepneuralnetworks.
Thisresultisalignedwithrecentskepticismthat,whilecodecov-
erageremainsawidelyusedtestadequacycriterion[ 6,23],code
coveragemaynotbecorrelatedwithdefectdetection[ 22]andthus
maynotbeameaningfulmetricbyitself.SimilartohowInozem-
sevaetal.[22]highlightanempiricallackofcorrelationbetween
traditionalcode coverage and defect defection, our result is about
a lack of correlation , not causation. We do not claim that NC is
useless; rather, we warn researchersabout thepotentialmisuse ofNCastheobjective fortestgenerationbecauseanaiveattemptto
increaseNC could sacrificeotherdesiredproperties.
These findings call for a new test generation method that not
only improves defect detection, but also promotes naturalness and
output impartiality to create realisticinputs and to exercise diverse
outputbehavior.Thisargumenttoincorporateadditionalobjectives
is aligned with a recent survey of testing ML-based systems [ 62]
that lists multiple desired testing properties, including correctness,
model relevance, robustness, security, efficiency, fairness, inter-
pretability,privacy,andsurpriseadequacy.Satisfyingsuchmultiple
objectivesmaynecessitatetheuseofmulti-objectivesearchtech-
niques[31]orenableuserstoeasilyadddomain-specificconstraints
toguidemeaningfulinputtransformationandoraclecheckingin
metamorphic testing[ 52].
2 RELATED WORK
ThissectionreviewsrelatedworkonDLsystems,DNNtesting,and
adversarial attacks. Work relevant to our methodology is described
ingreater detailinSection 3.
DeepLearningSystems. DNNshaveachievedmanybreakthroughs
inthefieldofartificialintelligence,suchasspeechrecognition[ 18],
imageprocessing[ 28],statisticalmachinetranslation[ 3],andgame
playing[55]. EachDNN contains basiccomputational unitscalled
neurons, which are connected with one another via edges of vary-
ing importance or weight. Neurons apply a nonlinear activation
function to the inner product of their inputs and weights to output
avalue,whichbecomestheinputtoasubsequentneuron.Layers
areusedtoorganizethedirectedconnectionsbetweenneuronsand
thereisalwaysoneormorehiddenlayersbetweenoneinputand
one output layer. Overall, a DNN can be viewed as a meta-function
that aggregates the weighted contributions from its neural sub-
functionstomapsomeinputintosometargetoutput.Suboptimally
set weights make the DL system vulnerable to erroneous behav-
iors and the opacity of these numerically-derived rules make them
difficult to understandanddebug.
DNN Testing. Withthesuccessofdeeplearning,thereemerged
a line of research into testing DNNs by leveraging the ideas in
traditional software testing methods [ 15,40]. We discuss several of
the most relevantDNNtesting methodsthatutilizetheNC-based
criteriaas follows.
DeepXplore[ 47]isawhite-boxdifferentialtestingalgorithmthat
leverages NC to guide systematic exploration of DNN’s internal
logic.Inputimagesare modifiedby severaldomain-specific trans-
formations,and a transformed imageisselected for inclusion into
a test suite if it fools at least one of several similarly trained DNNs.
TheirstudyfindsthatNCisabettermetricthancodecoverageand
increasing NC tendsto increase ℓ1-distanceamong inputs.
DeepTest [ 58] is a gray-box, NC-guided test suite generation ap-
proachusingmetamorphicrelations.Thiseffortintroducedawider
range of affine transformations to predict the steering angle of an
autonomous vehicle. DeepRoad[ 63] is a GAN-based metamorphic
testing approach that utilizes a shared latent space representation
to perform a sophisticated style transfer of some target road condi-
tion,i.e.,rain,snow,etc.,toagivensourceimage.DeepRoadmakes
noattempttosystematicallyexplorethepossibleinputspacevia
ametriclikeNCbutfindsthatGAN-basedtransformationscould
expose newfaultybehaviors.
852Is NeuronCoverage aMeaningfulMeasure forTestingDeep NeuralNetworks? ESEC/FSE ’20, November8ś13,2020,VirtualEvent, USA
DeepGaugeexpandsontheideaofNC[ 37]byintroducingthree
new neuron-level coverage criteria and two layer-level coverage
criteriatoproduceamulti-granularsetofDNNcoveragemetrics.
To argue for the utility of these metrics, DeepGauge uses standard
adversarial attack techniques [ 8,14,30,46] to generate test suites.
ItthencomparestheNCoftheoriginaltestsuiteagainstthatofthe
new, augmented test suite, boosted by the generated adversarial
examples.Bydoingso,itfindssomeevidencethataddingadversar-
ial examples tends to increase NC in terms of most of the proposed
criteria. In Section 5, we report our results that explicit effort to
increaseNCactuallydoesnotimprovedefectdetectionandisoften
harmfulinterms ofnaturalness andoutputimpartiality.
Recenton-goingwork[ 11,33,53]foundpreliminaryevidence
that the correlation between NC and DNN robustness is rather
limitedandthatsimilarstructuralcoveragemetricsforDNNscould
be misleading. Specifically, their test suites are generated using the
standardadversarialattackmethods,andtheirevaluationislimited
to defect detection only. Our study scope is more comprehensive :
weuseautomated,quantitativemeasuresofnaturalnessandout-
put impartialityin addition to defect detection and systematically
investigatethetrade-offs;wedesignanoveldiversitypromoting
regularizertoextendexistingadversarialattackalgorithms;andwe
include both classification modelsand regression models (8models
intotal),as opposedto classification models only.
While our evaluation focuses on generating test suites, others
focusonselectingexistingtestsbasedonmodeluncertainty[ 38]or
surprise adequacy( i.e.,significantly differentand adversarial ) [25].
Finally, it is worth noting that our proposed output impartiality
criteria discussed in Section 4.3is different from the concept of
fairness in machine learning [ 9]. Fairness in ML is concerned with
thebiasofanMLmodelwithrespecttosensitiveattributes,such
asgenderorrace.Alongasimilarvein,Themis,asoftwarefairness
testing tool by Galhotra et al.[12], automatically detects causal
discrimination between input-output pairs for user-specified at-
tributes.Insharpcontrastwiththesenotionsoffairness,ouroutput
impartiality isameasure ofthe bias onhow a testsuite exercises
diverseoutputbehaviors inan ML model.
Adversarial Attacks. Recent studies show that DNNs are vulner-
abletoadversarialexamples[ 14,57],i.e.,byaddingaverysmall,of-
ten visually imperceptible, perturbation to an input, a well-trained
DNN may produce misclassifications. While adversarial attacks
employ a variety of methods to induce erroneous behavior, their
effectivenessislargelymeasuredbytheattacksuccessrateofthe
perturbedinputsanditsdistortionfromtheoriginalinputs.Most
optimization-based adversarial attacks [ 8,39] are based onℓ2or
ℓ∞norm-based perturbation. Some work [ 47,58] has attempted
toimproveorsidestepthenormconstraintwithdomainspecific
transformations. In our evaluation of neuron coverage, we use the
standardattackmethodswith ℓ∞normconstraint,becausethese
methodsare efficient andcan generatenaturalexamples.
Adversarialattackalgorithmsofferbothtargetedanduntargeted
attacks for perturbing inputs to be predicted as some other class.
Untargeted attacks aim to turn the prediction into any incorrect
class,whiletargetedattacksaimtoturnthepredictionintoaspecific
class. We use untargeted attacks to give them more freedom to
perturb the input in whichever way NC maximization incentivizes.Table 1:DNN Architectural Details
DNNs DatasetPrimary
LayerType# Layers # Neurons
FCNet5 MNIST Fully Connected 5 478
FCNet10 MNIST Fully Connected 10 3,206
Conv1DNet MNIST Conv1D 4 35,410
Conv2DNet MNIST Conv2D 4 15,230
ResNet56 [16] CIFAR10 Conv2D 56 532,490
DenseNet121 [19] CIFAR10 Conv2D 121 563,210
DAVE2[5] Driving Conv2D 10 82,669
DAVE2-N [47]‘ Driving Conv2D 10 82,669
3 STUDYMETHODS
Thissectiondescribesthedatasets,DNNmodels,andadversarial
attackalgorithmsusedforourempiricalstudyanddescribesour
diversitypromoting regularizer to increaseneuroncoverage.
3.1 DatasetsandDNNs
Table1summarizesarchitecturaldetailsofalltheDNNsundertest.
CIFAR10 [27]isadatasetcontaining32x32x3RGBpixelimages
representingtenmutually exclusive classes of naturallyoccurring
entitiesthataresuitableforISandFIDrealismmeasurement.Weuse
two well-known pre-trained DNNs: a 56-layer ResNet [ 16,20] and
a121-layerDenseNet[ 19,48],bothofwhichachievecompetitive
performance onthis dataset.
MNIST[32] is a large, well-studied dataset containing 28x28x1
gray-scalepixelimagesrepresentinghandwrittendigitsfrom0to
9. For this dataset, we consider two fully connected neural net-
works:FCNet5with5hiddenlayersandFCNet10with10hidden
layers, and two convolutional neural networks: Conv1DNet and
Conv2DNet. Both convolutional neural networks have 2 convo-
lutionallayersfollowedby2fullyconnectedlayers,butvarythe
primary convolutional layer type from 1D to 2D. All MNIST DNNs
were trainedfor 10 epochsusing an Adam optimizer[ 26].
ThetworealismmetricsweemployÐIS[ 51]andFID[ 17]Ðare
tuned on the internal structures of natural images which generally
have both foregrounds and backgrounds. Because such naturalism
is not applicable to a digit recognition task, we exclude MNIST
when studyingthe relationship between NC andnaturalness.
UdacitySelf-DrivingCar [1]isadatasetcontaining480 ×640×
3 RGB pixel images extracted from video footage shot by a camera
mounted to the front of a moving vehicle and the corresponding
angle of the steering wheel ( ±25◦) for each frame. We use two pre-
trained DNNs:DAVE2 andDAVE2-Norminit(abbreviated DAVE2-
N),usedbyDeepXplore [ 47]andoriginallyfrom NVIDIA [ 5].
3.2 MeasuringNeuron Coverage
Peiet al.[47]formally defineneuroncoveragebythe following:
neuron_cov(T,x,t)=|{n|∀x∈T,out(n,x)>t}|
|N|
whereN={n1,n2,...}represents all the neurons in the DNN; T=
{x1,x2,...}represents all test inputs (i.e., those to be perturbed);
out(n,x)is a function that returns the output value of neuron n
for a given test input xscaledto be between 0 and 1 based on
the minimum and maximum neuron activations for the layer; and
853ESEC/FSE ’20, November8ś13,2020,VirtualEvent, USA FabriceHarel-Canada,Lingxiao Wang,MuhammadAliGulzar,QuanquanGu,andMiryungKim
0.5
 0.99
 0
11
(1)
4.2
(0.382)
0
(0)
0
(0)
6.90
(0.627)
0.25
➊
➋
➌
Figure 1: Single Layer DNN. ➊represents inputs (i.e., pix-
els,features,etc.). ➋representsahiddenlayerof5neurons,
where parentheses denote activations scaled between 0 and
1 for comparison against a NC threshold. ➌represents an
outputlayerof1neuron(i.e.,classlogits,probabilities,etc.).
tis the user-set threshold for determining whether a neuron is
sufficiently activated.
Figure1depicts an example neural network with a single hid-
den layer.Eachcircular node corresponds to a neuronorganized
and color-coded by layer. The hidden layer neurons also contain
theirlayer-wisescaledactivationsinparenthesesforcomparison
against a chosen threshold t. Ift=0, thenNCt=0=4/6=0.67,
or ift=0.75, thenNCt=0.75=1/6=0.17. Selecting an appro-
priatethreshold twasanopenissueinearlyNCresearch.When
measuringNC,wevaryathreshold tfortherangeusedbyprior
work [37,47,58],t∈{ 0, 0.2,0.5,0.75}.
3.3 AdversarialAttackAlgorithms
Usingadversarialattacksfortestgenerationisanalogoustofuzzing
in software testing and acts as a means of introducing targeted
perturbations. We select the following two adversarial attack algo-
rithms[8,39]dueto theirwidespread usage inthe ML literature.
Carlini-Wagner (CW) [8]constructstheadversarialexample
x+δ,wherexistheoriginalinputtoattack, δistheadversarial
perturbation, bysolving the following optimization problem:
min
δα·L/parenleftbigh(x+δ),y/parenrightbig+∥δ∥psubjectto x+δ∈ [0,1]n,
whereyisthelabelof x,Lisasuitablelossfunction, histhetarget
model,∥ · ∥pdenotes theℓp-norm such asℓ∞,ℓ0,ℓ2norms, and α
isascalingconstanttobalancethetheloss Landtheℓp-norm.The
intuition behindthe CW attack isto find somesmall perturbation
δthatwecanaddtotheoriginal input xsuchthatit willleadthe
targetmodeltochangeitsclassification.Toachievethis,theCW
attack exploits the loss function Lto guide the generation of δ
thatwillmakethetargetmodel’sclassificationon x+δdifferent
fromx.Byminimizingthe ℓp-normofδ,theCWattackcanensure
that such perturbation issmall. In this effort, we use the ℓ∞norm,
wheredistanceismeasuredbythepixelwiththegreatestmagni-
tude changefromitsoriginalvalue.As forthelossfunction L, we
usethelossfunctionprovidedbyCarliniandWagner [8]forour
classification tasks. For our regression models, we substitute the
standardCWlossfunctionforacustomlossdesignedforregression
tasksbyMeng et al.[41].
Figure 2: Neural activation before and after regularization:
our regularization significantly promotes NC at t=0.2.
Projected Gradient Descent(PGD) [39] finds the adversarial
examplex+δbysolving the following maximization problem:
max
δL/parenleftbigh(x+δ),y/parenrightbigsubjectto ∥δ∥p≤ϵ,
whereyis the label of x,hrepresents the target model, Lis the
loss function for training h,ϵis the perturbation limit. The max-
imization step will guide us to find the adversarial example and
theℓpnormconstraintwillmaketheperturbationsmall.Forthe
PGDattack,projectedgradientdescentisperformedtosolvethe
aboveconstrainedoptimizationproblem.Weconsiderthe ℓ∞norm
constraint as in the CW attack, and use the sign of the gradient
[14] to efficiently solve the maximization problem. For the loss
functionL,wechoosethecross-entropylossforclassificationtasks
and mean square error for regression tasks. We vary a different
perturbation limit ϵ∈{0.1, 0.2, 0.3} for the norm bounds to explore
its possible effectsonNC.
3.4 Extending Attacksto Increase NC
Adversarial attacks aim at creating perturbed inputs to achieve
twoprimaryobjectivesÐmaximizinglosswhilekeeping ℓp-norm
distancefromtheoriginalinputssmall.Previous research[ 37,47]
foundthatthesealgorithmsdonotproduceanysignificantvaria-
tioninNC.ToincreaseNCwhileleveragingtheskeletonofexisting
adversarialattacks,wedesignanoveladversarialattackregularizer
toincorporatethemaximizationofNCasanadditionalobjective.
Our regularizer works by penalizing skewed layer-wise activations
and thuspromotesmore diverseneural activation distributions.Di-
versity promotion has the effect gravitating all neurons toward the
average magnitude ofactivation. Here we show theextendedCW
attack, augmentedwithour newdiversity-promoting regularizer:
min
δα·L/parenleftbigh(x+δ),y/parenrightbig+∥δ∥p+λ·/summationdisplay.1
ldiv(outl(x+δ),U)
subjectto x+δ∈ [0,1]n,
whereλ>0 isa user-set diversity weight to controlhowstrongly
wewishtoinducehigherNC; div(·)isadivergencefunction; outl(·)
isa function that returns the neuralactivations from the lthlayer
of the DNN for the perturbed inputs x+δ;Urepresents a uniform
distribution;andweconsider ℓ∞norminourmethod(i.e.,choosing
p=∞). We use the Kullback-Leibler (KL) divergence [ 29] to im-
plement our div(·)function, but any other measure of the distance
betweentwoprobabilitydistributionscouldbesuitable.KLdiver-
gence measures how much information islost by approximating
theneuralactivationsasiftheywereperfectlyuniformÐthehigher
the loss, the less diverse the activations. With a sufficiently high
854Is NeuronCoverage aMeaningfulMeasure forTestingDeep NeuralNetworks? ESEC/FSE ’20, November8ś13,2020,VirtualEvent, USA
Table 2:OriginalNC, Average %Increase fromOriginal NC, andMaximum%Increase fromOriginal NC
NCt=0(%) NCt=0.2(%) NCt=0.5(%) NCt=0.75(%)
DNNs Orig Avg ↑Max↑Orig Avg ↑Max↑Orig Avg ↑Max↑Orig Avg ↑Max↑
FCNet5 96.09 0.56 0.66 61.21 22.59 61.72 15.01 45.75 173.98 4.33 38.72 171.88
FCNet10 78.52 7.65 18.63 16.79 54.68 98.90 3.70 38.27 71.63 0.89 63.77 123.91
Conv1DNet 68.08 4.82 14.50 12.67 8.77 45.33 1.26 12.85 139.58 0.48 15.16 63.38
Conv2DNet 94.96 1.77 4.18 23.89 9.11 36.47 6.66 25.48 55.69 1.23 67.88 122.04
ResNet56 95.07 0.22 0.40 26.87 4.31 11.77 5.42 6.17 21.76 1.29 6.20 15.71
DenseNet121 96.46 0.04 0.06 12.88 7.00 14.49 1.20 6.59 15.85 0.16 11.77 31.87
DAVE2 78.11 9.99 15.09 13.32 4.39 30.62 2.45 -5.78 9.28 0.72 -16.17 29.23
DAVE2-N 77.57 11.90 17.26 14.69 26.77 59.26 2.54 2.63 28.91 0.46 -1.26 37.14
Average 85.61 4.62 8.85 22.79 17.20 44.82 4.78 16.50 64.59 1.20 23.26 74.40
Table 3:ExperimentalVariables
Variable Values
Adversarial Attacks CW,PGD
DNNs FCNet5, FCNet10,Conv1DNet
Conv2DNet, ResNet56, DenseNet121
Datasets MNIST,CIFAR10
Target Layers Varies
λDiversity Weights 0,100,101,102,103,104,105
cConfidence (CW)10,20,40
ϵLimit(PGD) 0.1,0.2,0.3
regularization weight placed on this objective, diversity promotion
caninducepreviouslyinactiveneuronstofireandincreaseNC.Itis
important tonote thataddingtheregularizer doesnot necessarily
harmtheattacksuccessrateasapproximately23%ofourgenerated
suites have 100% attack success. However, there tends to be an
inverse relationship between the regularization weight ( λ) and the
attack success rate. For example, the average attack success rate is
65% when λis 0,and with increasing λto 1, 101, 102, 103, 104, and
105, the average attack success rate is 53%, 51%, 48%, 43%, 38%, and
35%,demonstratingsomedecrease.Figure 2showshowourregu-
larization promoteshigherNC byhavingmore neurons activated
byvisualizingneuronactivation at agiven layer inConv2DNet.
Table2shows our regularizer’s effectiveness in terms of the
averageandmaximumpercentincreasesinNCoverthebaseline
NCoftheoriginaltestsuiteimagesforallmodels.Naturally,already
highlyactivatedDNNsaremoredifficulttoactivatefurther,making
NCt=0undesirable for comparison purposes. On the other hand,
NCt=0.5andNCt=0.75activatesignificantlysmallerportionsofthe
network. We report primarilyonNC t=0.2for visual figures.
As an implementation note, our diversity-promoting regularizer
can target a specific layer, contiguous and non-contiguous layer
subsets,oralllayerssimultaneously.Inourexperiments,wevary
thetargetlayerone atatime, primarily toevaluatethe sensitivity
ofNCtothisregularization.FortheMNISTmodels,wetargeteach
layerinturn.However,forlargermodels,wetarget klayers(default
k=6) evenly spaced in the model, starting from the first hidden
layer andendingat the outputlayer.
Figure 3: NC t=0.2vs ASR: the results show that NC does not
consistentlycorrelate with defect detection.
4 FINDINGS
Foreachconfiguration,weconstructatestsuiteof100randomly
selected images such that each class is equally represented. This
isto ensurethat the suite has complete outputimpartiality before
perturbation. We then use the NC-augmented adversarial attack
algorithm to perturb the original tests before computing NC at
threshold t∈ {0,0.2,0.5,0.75},defectdetection,IS,FID,andoutput
impartiality. Finally, we perform an analysis of 2,095 test suitesto
measurethestrength,direction,andsignificanceofcorrelation.The
experimental conditions are listedinTable 3.
1Theparameter cencouragesthesolvertofindanadversarialinstancethatisclassified
as aspecificclass with high confidence, seeCarliniand Wagner[ 8] for detail.
855ESEC/FSE ’20, November8ś13,2020,VirtualEvent, USA FabriceHarel-Canada,Lingxiao Wang,MuhammadAliGulzar,QuanquanGu,andMiryungKim
Table 4:Correlation between NC & ASR: Grayindicates ap-value >0.05
CW - ASRCorrelations PGD - ASRCorrelations
DNNs NC t=0NCt=0.2NCt=0.5NCt=0.75NCt=0NCt=0.2NCt=0.5NCt=0.75
FCNet5 -0.20 -0.23 -0.18 0.07 -0.10 -0.52 -0.52 -0.32
FCNet10 -0.67 0.76 0.75 0.04-0.18 -0.16 -0.10 0.14
Conv1DNet NA NA NA NA 0.58 -0.37 0.10 0.05
Conv2DNet -0.16 -0.20 -0.29 -0.23 0.08 -0.04 -0.16 -0.36
ResNet56 -0.46 0.59 0.58 0.57 -0.11 0.52 0.53 0.21
DenseNet121 -0.83 -0.21 -0.06 0.13 0.19 0.18 0.20 0.11
Dave2 0.02 -0.17 -0.27 -0.21 0.30 -0.16 -0.45 -0.34
Dave2-N NA NA NA NA 0.00 -0.10 0.00 -0.08
Average -0.38 0.09 0.09 0.06 0.10 -0.08 -0.05 -0.07
Allcorrelations are presentedina tabularform andwevisualize
a sample of the NC t=0.2results for PGD for presentation purposes.
We adopt a standardized delineation of correlative significance
laidoutbyRatner[ 50]tocharacterizevaluesbetween0and ±0.3
as weak, ±0.3 to±0.7 as moderate, and ±0.7 to±1.0 as strong.
Correlation coefficients are alsocolor-coded according to whether
or not they are statistically significant. Grayindicates a p-value
>0.05andsuch valuesare discountedin oursubsequent analysis.
Emboldened valuesindicatethattheresultssupporttheassociated
hypothesisandallothersdonot.
4.1 Defect Detection
4.1.1 StudyMethod. Sinceourapproachreliesonadversarialat-
tacks to generate test suites, we equate the attack success rate
(ASR)withdefectdetectionrate(DDR)andusebothmeasuresin-
terchangeably. Let pert_accrepresent the classification accuracy
on the adversarially perturbed suite of test inputs ( T), then DDR
is simply ASR(T)=1−pert_acc. In order to use the same metric
fortheregressiondrivingmodels,wediscretizetheircontinuous
outputsinto25equal-widthintervals[ 59],eachrepresentinga2◦
difference insteeringangle.
4.1.2 Results. Figure3visualizestherelationshipbetweenNCand
ASR,brokendownbyDNN for the PGD attack, whichshowsthat
NCisvolatileandNCdoesnotconsistentlycorrelatewithdefect
detection.Evenformodelsthatsharealargedegreeofarchitectural
similarity,liketheFCNet5andFCNet10models,thecorrelationsdif-
fer in both strength and direction, reinforcing the unpredictability
ofNC.
Table4showstheresultsofallconfigurationsbrokendownby
an attack algorithm, network, and tthreshold. Only 2 out of 64
correlationssatisfythehypothesisthatNCisbothpositivelyand
strongly correlated with defect detection. Independent of direction,
58%ofexperimentalconfigurationsshowaweakcorrelation,while
25%aremerelymoderate.Thecorrelationispositiveinonly36%of
configurations,negative in52%,andnon-existent in12%.
DefectDetection. Ourfindingsrejectthehypothesisthat
NC is strongly and positively correlated with defect detec-
tion.Only3% ofthe configurationssupportedthis.4.2 Naturalness
DLsystemsaredesignedtosolvereal-worldproblemsandthere-
fore a test suite must have realistic and natural inputs. In fact,
several prior techniques are motivated by this naturalness goal
and state this requirement. For example, DeepXplore [ 47] uses
domain-specificconstraintstogeneratetestimagesthatare valid
andrealistic .DeepTestalsostatesthatitseekstoapplywell-behaved
transformations to preserve realism [ 58,63]. We explicitly inves-
tigatewhethermaximizingNCcangeneratetestsuitesreflecting
the naturalness ofthe expectedinputspace.
4.2.1 StudyMethod. Appraisingthevisualqualityofanimagecan
be highly subjective and there is still no definitive solution on how
toformalizeitsnaturalness.Fortunately,researchintogenerative
adversarial networks (GANs) [ 13] has produced several popular
metricsforthispurpose.Weselectthetwomosthighlycitedmetrics
from the GAN literature to objectivelymeasure naturalness.
TheInceptionScore (IS) [4,51]formalizesthe conceptof nat-
uralnessbydecomposing itintothe following twosub-concepts:
•Salience. Of the possible class labels that could be applied
to an individual image, only one has a highprobability and
theothersareverylow.Thiscorrespondstotheimagebeing
highly recognizable.
•Diversity. There are many different kinds of classes present
acrossallimagesinthe set.
TheFrèchet Inception Distance (FID) [17,42] is a measure
of similarity between two datasets of images. It is calculated by
computing the Frèchet distance between two Gaussians fitted to
feature representations of the final average pooling layer within
the InceptionV3 network [ 56]. The inventors, Heusel et al., find
evidence that FID captures the similarities of generated images
better than IS and that FID correlates well with human judgement
of visual quality. Unlike IS, the lower the FID value, the more real-
istic the imagesare, sincethe distance from theoriginal imagesis
smaller.Therefore,weinvestigatewhetherNChasastrong negative
correlation withFID.
In the ML community, ImageNet [ 10] is considered as a com-
prehensivedatasetforimageclassifications.Thus,theauthorsof
IS and FID derived these metrics based on the models trained on
ImageNet and demonstrated generalizability to other datasets such
asSVHN[ 43],CelebA[ 35],CIFAR10[ 27],andLSUNBedrooms[ 60].
856Is NeuronCoverage aMeaningfulMeasure forTestingDeep NeuralNetworks? ESEC/FSE ’20, November8ś13,2020,VirtualEvent, USA
Table 5:Correlation between NC & Naturalness: Grayindicates ap-value >0.05
CW - IS/ FID Correlations PGD - IS/ FID Correlations
DNNs NC t=0NCt=0.2NCt=0.5NCt=0.75NCt=0NCt=0.2NCt=0.5NCt=0.75
ResNet56 0.09/ 0.27 -0.87 / 0.76 -0.81 / 0.75 -0.59 / 0.59 0.34/-0.03-0.38 / 0.42 -0.52 / 0.46 0.06/-0.14
DenseNet121 0.57/ -0.23 0.73/0.130.63/ 0.24 0.46/ 0.35 -0.15/ 0.260.16/-0.06 -0.08 /0.160.19/-0.12
Dave2 -0.62 / 0.61 -0.32 / -0.22 0.02/ -0.31 0.21/ -0.18-0.55 / 0.49 -0.89 / 0.97 -0.53 / 0.60 -0.29 / 0.29
Dave2-N 0.56/ 0.88 -0.48 / 0.50 -0.53 / 0.41 -0.50 / 0.49 -0.94 / 0.96 -0.67 / 0.78 -0.28 / 0.38 -0.23 / 0.32
Average 0.15/ 0.38 -0.23 / 0.29 -0.17 / 0.27 -0.10 / 0.31 -0.33 / 0.42 -0.45 / 0.53 -0.35 / 0.40 -0.07 / 0.08
Figure 4: NC t=0.2vs Naturalness (IS / FID): the results show
bothstronglynegative andstronglypositive correlations.
Therefore, we usethe same method that the authorsof FID and IS
used. In our experiments, we exclude MNIST from the measure-
mentofISandFID,sinceitisinapplicabletodiscussnaturalnessof
highly,pre-processed MNISTdigitrecognition.Therefore,weuse
only CIFAR10and drivingdatasets for examiningthe relationship
between NC andnaturalness.
4.2.2 Results. Figure4depictstherelationshipbetweenNCandIS
andFID,brokendownby metric, modelfor thePGD attack.Once
again, thewide fluctuationof stronglynegative and stronglyposi-
tive correlations underscore the volatility of NC. Table 5shows the
resultsforeachattackalgorithm,model,and tthreshold.Only1out
of 64correlations satisfy thehypothesis that NC is both positively
andstronglycorrelatedwithimprovinginputnaturalness.Indepen-
dentofdirection,38%ofconfigurationsshowaweakcorrelation
whileanother45%aremerelymoderate.Independentofstrength,
the correlation ispositive inonly 31%ofcases.Unlike the mixed results for IS, increasing NC invariably in-
creases FID, making the inputs less natural. In fact, not a single
configurationinthe FIDexperiment supports the hypothesis.
MorethanhalfofthePGDresultsacrossbothISandFIDaresta-
tistically insignificant. This is because PGD attacks enforce a more
strictϵperturbationlimit,whiletheperturbationsofCWattacks
are theoretically unbounded and thus minimize the distortion as
much as possible. Since this limit tightly constrains the range of
measurements,itisdifficult to assessthe correlation withNC.
Figure 5:Test Suite #33.NC t=0.2: 0.29- IS: 1.97- FID: 0.10
Figure 6:Test Suite #140. NC t=0.2: 0.33- IS: 1.48- FID: 2.96
Figures5and6show a sample of two test suites with a 14%
NCdifference.Whilebothsetsofimagesarenoticeablydistorted,
testsuite#140isclearlymoreunnatural.Testsuite#33hasanIS
about33%higherandanFID about29x smaller,bothconfirming
the intuition that Figure 5withNC=0.29 is more natural than
Figure6withNC=0.33.Here,increasingNCmakesnoisierand
more noticeably perturbedinputs,thus aless valuable test suite.
Naturalness. Only 1.5% of all experimental results sup-
portedthehypothesisthatNCisstronglyandpositively
correlatedwithnaturalness.69%ofthetestsuitesareac-
tually negatively correlated, implying that maximizing
neuroncoverageislikely to undermine naturalness.
4.3 OutputImpartiality
The final dimension of our investigation probes the relationship
between NC and the bias in model predictions. This idea of mea-
suring the impartiality of model predictions is motivated by the
857ESEC/FSE ’20, November8ś13,2020,VirtualEvent, USA FabriceHarel-Canada,Lingxiao Wang,MuhammadAliGulzar,QuanquanGu,andMiryungKim
Table 6:Correlation between NC & OutputImpartiality: Grayindicates ap-value >0.05
CW - OI Correlations PGD - OI Correlations
DNNs NC t=0NCt=0.2NCt=0.5NCt=0.75NCt=0NCt=0.2NCt=0.5NCt=0.75
FCNet5 0.33 0.43 0.36 0.11 0.37 0.74 0.52 0.26
FCNet10 0.77 -0.70 -0.76 -0.02 0.42 0.34 0.25 0.02
Conv1DNet 0.39 0.08 0.10 0.15 -0.57 0.18 -0.28 -0.15
Conv2DNet -0.22 -0.02 0.29 0.34 0.34 0.11 -0.04 -0.08
ResNet56 -0.27 0.45 0.43 0.43 -0.02 0.07 0.09 0.08
DenseNet121 0.79 0.11 -0.04 -0.18 -0.09 -0.18 -0.06 0.08
Dave2 NA NA NA NA -0.36 0.13 0.41 0.34
Dave2-N 0.66 0.01 -0.04 0.00 -0.05 0.19 0.26 0.22
Average 0.35 0.05 0.05 0.12 0.01 0.20 0.14 0.09
Figure 7: NC t=0.2vs Output Impartiality: the results show
that increasing NC createsbias inoutputbehavior.
output-uniqueness test selection criteria [ 2] in traditional software
testing, which argues that a test suite must exercise diverse output
behaviorandshouldnotpreferonlyafewoutputvalues.Investi-
gating the relationship between NC and output impartiality is also
motivated by several observations about DNN behavior by prior
work.Ilyas etal.[21]foundthatadversarialexamplescanbecreated
byincorporating unnoticeable featuresofother classes to confuse
theDLmodel.Similarly,Pei etal.foundthatdifferentclassesare
associatedwithdistinctive neuronactivation patterns [ 47].
Considerabalancedtestsuitecomprisedofinputsevenlydrawn
from multiple classes. Suppose that the test suite is fed to a model
andthe modelpredicts alwaysthe sameclass label. This indicates
output skew. Since one important aspect of testing is to exercise as
much diverse output behavior as possible, weinvestigate the rela-
tionshipbetween NC andthe impartiality ofpredictedoutcomes.4.3.1 Study Method. We take inspiration for measuring impartial-
ityfromadjacentworkonecologicalbiodiversity[ 34].Insteadof
considering a distribution of species, we recast impartiality as a
measure of the distribution of class predictions under a uniform
inputdistribution(i.e.theinitialtestsuitecontainsanequalnumber
ofinputsfromeachclass).WeusePielou’s evennessscore[49],athe-
oreticallygroundedmeasureofbiodiversity[ 36]toassesstheskew
of the output class distribution. It uses a normalized Shannon’s
entropy[54]scaledtoarangeof0and1bydividingtheentropyof
eachtestsuite’soutputdistributionbythemaximumentropygiven
the total number of classes. A high evenness score entails high
impartiality (low bias). We define an output impartiality metric for
atest suite Twith|C|possible classes,indexedby k:
output_impartiality (T)=/summationtext.1
t∈CkPt=CklogPt=Ck
log|C|,
where|C|is the cardinality of classes and Pt=Ckrepresents the
percentageofthetestcases tpredictedtobelongtoclass Ck.For
theregressionmodels,weusethesamediscretizationmethodas
before to enable the use of this metric.
4.3.2 Results. Figure7visualizestherelationshipbetweenNCand
output impartiality by DNN for CW. The results show that increas-
ingNCcreatesbiasinoutputbehavior.Table 6showstheresultsof
allconfigurationsbyanattackalgorithmand tthreshold.Only3out
of64 configurationsshowthat NCisboth positivelyandstrongly
correlatedwithoutputimpartiality.Independentofstrength,the
correlation is negative in 33% of correlations. Independent of direc-
tion, 62%of experimentalconfigurations show a weak correlation
while32%are moderate.
4.3.3 InvestigatingOutputBiasCausedbyNC. Inadditiontothe
previoussection’scorrelationanalysis,wedesignanotherexperi-
ment to investigate which classes are likely to be over-represented
intheoutputsafteratestsuitehasbeenperturbedtomaximizeNC.
The idea of maximizing NC during test suite generation does
not take into account that different classes of inputs can already
havedifferentbaselineNClevels.Forexample,itmaybethecase
that a set of inputs containing only the łdogž class in CIFAR10
hasNCt=0=0.9whileanothersetofinputscontainingthesame
number of łcarsž has NC t=0=0.6. Increasing NC may then bias
theperturbationsÐandthereforetheoutputpredictionsÐtowards
858Is NeuronCoverage aMeaningfulMeasure forTestingDeep NeuralNetworks? ESEC/FSE ’20, November8ś13,2020,VirtualEvent, USA
Figure 8: Output Prediction Distribution Histogram (left)
andCross-ClassPrediction Heatmap (right)
the class łcarž with the higher NC baseline instead of łdogž. Below
wedescribeanexperimentconductedwiththeMNISTdatasetto
investigate this further.
Wegenerate10partitionsofthetestdataÐonepartitionforeach
classÐbyrandomlyselecting100instancesofthatclassfromthe
test set. These partitions are then used to calculate a class-specific
NC baseline. Since NC depends on the choice of t, we repeat NC
baselinecalculationfor eachclasslabel, while varying tfrom0to
0.9 in an increment of 0 .1. This process reveals which class label
hasthehighestNCbaseline,thesecondhighestNCbaseline,and
so on. In other words, we rank class labels from the highest NC
(Rank1)to the lowestNC (Rank10).
Supposethatclasslabel8hasarank{1,3,3}andclasslabel1has
arank{8,9,10},respectivelyfor t∈ {0,0.5,0.9}.Alowaveragerank
forclass8(2.3)indicatesthatclass8tendstohaveahighNCbaseline
regardlessof t.Ontheotherhand,ahighaveragerankforclass1
(9)indicatesthatclass1tendstohavealowNCbaseline.Therefore,
during NC maximization, the perturbation process may favor over-
representingclass8intheoutputpredictions.However,suppose
that class 3 has an average ranking closer to 5Ðthe midpoint of
10 possible labels. That implies that class 3 may have a high NC
baselineunderacertainthreshold,butmayhavealowNCbaseline
under another threshold, or places the fifth for all t, etc. Thus,
it would be unlikely for NC maximization to consistently prefer
over-representation of outputs associated with class label 3 in the
resultingtest suite.
Table 7:MNIST ClassandAverage Rank ofNC Baseline
Class 8 5 2 0 7 3 4 9 1 6
Average
Rank2.1 2.9 3.1 4.1 5.0 6.2 7.0 7.4 8.2 8.6
Concretely, an average rank closer to 1 indicates a greater likeli-
hood of being over-represented in the output distribution through
NC-maximization.Table 7reportstheaverageclassranksforthe
10classlabelsofMNIST.Here,wecanseethatclasslabel8tends
to have a high NC baseline and that class label 6 tends to have a
lowNCbaselineacrossdifferentthresholds.ThereforeintheNC-
maximized test suite, class 8 is likely to be over-represented and
class 6 islikely tobe under-represented in the outputdistribution.
We first construct a group of inputs with an output impartial-
ity 1 by drawing 30 inputs per class labelÐevery class is equally
represented in the output distribution, because all are correctlypredictedbytheConv2DNettrainedforMNIST.Wethenuseour
testgenerationalgorithmwithadiversity-promotingregularizer
toperturbtheinputsettoincreaseitsNC.Thehistogramonthe
left in Figure 8shows the percentage of model predictions for each
class. The heatmap on the right details how many of the inputs
belongingtoeachclassintheoriginalgroupwereperturbedinto
to predicting another class label. This perturbed test suite had a
NCt=0.2of 0.34Ðabout 40% higher than the original images, but
45% of all predictions are now for class 8, demonstrating output
skew (a low impartiality score of 0 .26). As expected, the classes
with the low NC baselines (e.g., 6, 1, and 9) are among the most
under-represented.Thisshowsthat,whenNCmaximizationisused
asaguidancecriterion, atest generation technique can easily sat-
isfy this criterion by simply perturbing inputs towards the class
label withthe highestNC baseline.
OutputImpartiality. Only5%ofallexperimentalresults
supportthehypothesisthatNCisbothstronglyandpos-
itively correlated with output impartiality. When a few
class labels have higherNC baselinesthanthe otherclass
labels,increasingNCbiasesthetestsuitetopredominantly
incorporatethe features of this preferredsubset.
5 DISCUSSION
Thissectionincludesadditionalevidenceandrationalethatques-
tionsthe meaningfulness of neuroncoverage.
5.1 DeepXplore& DeepTestComparison
It is certainly possible that another method may create a natural
test suite with high NC. Therefore we perform similar analysis on
the test suites generated by DeepXplore [ 47] and DeepTest [ 58] to
see whether similar trade-offs exist. We utilize the authors’ pub-
licly available implementations to generate tests for the MNIST
andDrivingdatasets.ForDeepXplore, notasingle correlationis
sufficientlystrongenoughtosupportthethreehypothesesthatNC
is positively related withdefect detection, naturalness, and output
impartiality.ForDeepTest,onlyonecorrelationforoutputimpar-
tiality at NCt=0.5is strongly positive. In fact, our investigation
finds that many images generated by DeepXplore and DeepTest
turnrichdrivingscenesintocompletelywhiteimages,yetretain
theiroriginallabels.Nohumanorprogramcanpredictasteering
angle from such an unnatural input.
WhiletheresultsfromDeepXploreandDeepTestmayhavebeen
sufficient to warrant skepticism about NC, our NC-maximizing
approach is easily applicable and systematic. First, it can probe the
behavior of a single model, while DeepXplore’s differential testing
requires multiple models. As DNNs become large and costly to
train [7], differential testing may become less practical. Second, by
directly extendingadversarial attacks thatmaximizedefective be-
haviorandminimizethenorm-distancefromtheiroriginalsources,
thegeneratedtestsuiteisordersofmagnitudemorenatural.For
instance, our test suites have average FID scores 458 ×and 3,887×
higherthanthosecreatedbyDeepXploreandDeepTestrespectively.
Whileitiscertainlypossiblethatyetanothermethodmaycreate
859ESEC/FSE ’20, November8ś13,2020,VirtualEvent, USA FabriceHarel-Canada,Lingxiao Wang,MuhammadAliGulzar,QuanquanGu,andMiryungKim
anaturaltestsuitewithhighNC,ourcomprehensiveexperimen-
tation of 2095 test configurations suggests that increasing NC is
unlikely to correlate with defect detection, naturalness, and out-
putimpartiality.Triangulationbetweentheseapproachesincreases
confidence aboutthe external validity ofour findings.
5.2 HowMeaningful IsaNeuron?
The viability of NC as a DNN testing metric is underpinned by
the idea that łeach neuron independently extracts a specific input
feature"[47]ratherthancollaboratingwithotherneurons.However,
recent research into DNNvisualization techniques [ 44,45,61] has
demonstratedthatthisisnotsoÐneuronindependenceandlocal
feature extraction do not accurately characterize DNN behavior.
Instead, the neurons in a layer interact with one another to pass
information to subsequent layers and NC does not capture the
richnessofsuchneuroninteractions.Whiletheprobabilitythata
neuron distinctly encodes a specific feature increases the deeper it
issituatedintheDNN,manyoftheneuronsrepresentanamalgam
ofverydifferentabstractconcepts,likethevisualizationofpixels
leadingtohighactivationsofcertainneuronsinFigure 9[45].This
observation raises serious doubts about whether neurons are even
the right semantic units for understanding DNN behavior, further
questioning the viabilityofNC as ameaningfultest metric.
Figure 9: Visualization of neuron activations shows mixed
conceptsÐcats, foxes,andcars [ 45]
5.3 DoesNC MaximizationMake Sensefor
TestingDNNs?
Assumingthebestcasescenarioofneuronsindependentlyencoding
specificfeatures,ismaximizationofNCevendesirable?Consider
eachneuronina DNN as a binaryclassifier checkingfor thepres-
enceorabsenceofaspecificfeaturewithintheinput.For nneurons
in a DNN, there are 2npossible activation patterns. In general,
establishing a single objective to maximize NC could easily tar-
get having onepossible pattern, where all neurons are activated.
As asimplifiedexample,consider twoneurons ina DNNtrained
for autonomousdriving. Supposethat one detectsthe presence of
vehicles and the other detects stop signs. NC maximization as a
single objective in test generation can be easily satisfied with a
single image containing both a vehicle and a stop sign together.
Subsequently, such limited focus on NC could easily produce a test
suite that does not cover other interesting portions of the potential
inputspace.
6 CONCLUSION
Recentefforttotestdeeplearningsystemshasproducedanintu-
itive testing adequacy metric, called neuron coverage (NC) and its
severalvariants. Prior work has also produced severaltestgenera-
tiontechniquesthatuseNCasaguidancecriterionandsomehasfound evidence that adding adversarial inputs to an existing test
suite tendsto increaseNC.
TosystematicallyincorporateNCmaximizationtoexistingadver-
sarialattackalgorithms,wedesignedanoveldiversitypromoting
regularizerthatcanbepluggedintoexistingattackalgorithmsto
increaseNC.Wethenassessedthequalityoftheresultingtestsuites
intermsofdefectdetection, naturalness,andoutputimpartiality.
Fromourevaluationof2,095experimentalconfigurationsinvolving
8 DNNs, 2 datasets, and 2 adversarial attack algorithms, we con-
clude that NC should notbe blindly trusted as a guidance metric for
DNNtesting .Whilewe do notclaimthat NCis useless,increasing
NCactuallyhasaharmfuleffectbyproducinglessnaturalinputs
andbycreatingaskewinoutputdistribution.Thisresultisaligned
withrecentskepticismthatcodecoverageintraditionalsoftware
testingisnotstronglycorrelatedwithtestsuiteeffectivenessand
thus maynot be ameaningfulmetric byitself [ 22].
Wethereforeadvocateincorporatingothertestobjectivessuchas
naturalness andoutputimpartiality andusemulti-objectivesearch
techniquesfortestingDLsystems.Ourexperienceofadaptingexist-
ingadversarialattackalgorithmsfortestgenerationhasshownthat
itisfairlyeasytocreateinputsthatleadtomispredictionbysacrific-
ing naturalness, and that it is also fairly easy to perturb a test suite
toproduceahighNCscorebyskewingtheoutputdistribution.Our
results call for more systematic research on how to generate realis-
ticinputsthatreveal meaningful ,undesiredbehaviorinDLsystems.
Sucharesearchdirectionmayrequirenewmethodstoempower
userstoeasilyspecifydomainspecificconstraintsexpressivelyand
to leveragethoseconstraintsto guide test generation.
Peropensciencepolicy,thecodeanddataisavailableat https:
//doi.org/10.5281/zenodo.4021473 .
ACKNOWLEDGEMENT
We thank anonymous reviewers for their comments. The partic-
ipants of this research are in part supported by NSF grants CCF-
1764077, CCF-1527923, CCF-1723773, SaTC-1717950; ONR grant
N00014-18-1-2037;IntelCAPA grant;Samsunggrant;GooglePhD
Fellowship;andthe Alexander vonHumboldtFoundation.
REFERENCES
[1]2016.UsingDeepLearningtoPredictSteeringAngles .https://github.com/udacity/
self-driving-car
[2]Nadia Alshahwan and Mark Harman. 2014. Coverage and Fault Detection of the
Output-UniquenessTestSelectionCriteria.In Proceedingsofthe2014International
SymposiumonSoftwareTestingandAnalysis (SanJose,CA,USA) (ISSTA2014) .
Association for Computing Machinery, New York, NY, USA, 181ś192. https:
//doi.org/10.1145/2610384.2610413
[3]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural ma-
chinetranslationbyjointlylearningtoalignandtranslate.In 3rdInternational
Conference onLearning Representations, ICLR2015 .
[4]ShaneBarratt.2018(accessedAugust8,2019). InceptionScoreforGANsinPytorch .
https://github.com/sbarratt/inception-score-pytorch
[5]Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat
Flepp,PrasoonGoyal,LawrenceD.Jackel,MathewMonfort,UrsMuller,Jiakai
Zhang, Xin Zhang, Jake Zhao, and Karol Zieba. 2016. End to End Learning
for Self-Driving Cars. CoRRabs/1604.07316 (2016). arXiv: 1604.07316 http:
//arxiv.org/abs/1604.07316
[6]BenjaminBrosgol.2011. Do-178C:TheNextAvionicsSafetyStandard.In Proceed-
ingsofthe2011ACMAnnualInternationalConferenceonSpecialInterestGroup
on the Ada Programming Language (Denver, Colorado, USA) (SIGAda ’11) . ACM,
NewYork, NY, USA,5ś6. https://doi.org/10.1145/2070337.2070341
[7]T. Brown, B. Mann, Nick Ryder, Melanie Subbiah, J. Kaplan, P. Dhariwal, Arvind
Neelakantan,PranavShyam,GirishSastry,AmandaAskell,SandhiniAgarwal,
ArielHerbert-Voss,G.Krüger,TomHenighan,R.Child,AdityaRamesh,D.Ziegler,
860Is NeuronCoverage aMeaningfulMeasure forTestingDeep NeuralNetworks? ESEC/FSE ’20, November8ś13,2020,VirtualEvent, USA
JeffreyWu, Clemens Winter,Christopher Hesse,Mark Chen,E. Sigler,Mateusz
Litwin,ScottGray,BenjaminChess,J.Clark,ChristopherBerner,SamMcCan-
dlish, A. Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are
Few-Shot Learners. ArXivabs/2005.14165 (2020).
[8]NicholasCarliniandDavidA.Wagner.2016. TowardsEvaluatingtheRobustness
ofNeuralNetworks. 2017IEEESymposiumonSecurityandPrivacy(SP) (2016),
39ś57.
[9]Alexandra Chouldechova and Aaron Roth. 2018. The frontiers of fairness in
machinelearning. arXiv preprint arXiv:1810.08810 (2018).
[10]J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. 2009. ImageNet: A
Large-Scale HierarchicalImage Database.In CVPR09.
[11]YizhenDong,PeixinZhang,JingyiWang,ShuangLiu,JunSun,JianyeHao,Xinyu
Wang,LiWang,JinSongDong,andDaiTing.2019. ThereisLimitedCorrelation
between Coverage and Robustness for Deep Neural Networks. arXiv preprint
arXiv:1911.05904 (2019).
[12]Sainyam Galhotra, Yuriy Brun, and Alexandra Meliou. 2017. Fairness Testing:
TestingSoftwareforDiscrimination.498ś510. https://doi.org/10.1145/3106237.
3106277
[13]Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-
Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative
Adversarial Nets. In Proceedings of the 27th International Conference on Neural
Information Processing Systems - Volume 2 (Montreal, Canada) (NIPS’14). MIT
Press, Cambridge, MA, USA, 2672ś2680. http://dl.acm.org/citation.cfm?id=
2969033.2969125
[14]Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. 2014. Explaining and
harnessing adversarial examples.In ICLR.
[15]KellyJHayhurst.2001. Apracticaltutorialonmodifiedcondition/decisioncoverage .
DIANE Publishing.
[16]Kaiming He, Xiangyu Zhang, Shaoqing Ren,and Jian Sun.2015. Deep Residual
LearningforImageRecognition. 2016IEEEConferenceonComputerVisionand
PatternRecognition (CVPR) (2015), 770ś778.
[17]MartinHeusel,HubertRamsauer,ThomasUnterthiner,BernhardNessler,and
SeppHochreiter.2017.GANsTrainedbyaTwoTime-ScaleUpdateRuleConverge
to a LocalNash Equilibrium. In NIPS.
[18]GeoffreyHinton,LiDeng,DongYu,GeorgeEDahl,Abdel-rahmanMohamed,
Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N
Sainath, et al .2012. Deep neural networks for acoustic modeling in speech
recognition: The shared views of four research groups. IEEE Signal Processing
Magazine 29,6 (2012), 82ś97.
[19]Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger.
2016. Densely Connected Convolutional Networks. 2017 IEEE Conference on
Computer Visionand PatternRecognition (CVPR) (2016), 2261ś2269.
[20]YerlanIdelbayev.2018(accessedAugust 8,2019). ProperResNetImplementation
forCIFAR10/CIFAR100inpytorch .https://github.com/akamaster/pytorch_resnet_
cifar10
[21]Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon
Tran,andAleksanderMadry.2019. AdversarialExamplesAreNotBugs,They
AreFeatures. ArXivabs/1905.02175 (2019).
[22]Laura Inozemtseva and Reid Holmes. 2014. Coverage is Not Strongly Correlated
with Test Suite Effectiveness. In Proceedings of the 36th International Conference
on Software Engineering (Hyderabad, India) (ICSE 2014) . ACM, New York, NY,
USA,435ś445. https://doi.org/10.1145/2568225.2568271
[23]ISO 26262-6:2011(en) 2011. Road vehicles Ð Functional safety Ð Part 6: Prod-
uctdevelopmentatthesoftwarelevel . Standard.InternationalOrganizationfor
Standardization, Geneva, CH.
[24]KyleJulian,MykelKochenderfer,andMichaelOwen.2018. DeepNeuralNetwork
Compression for Aircraft Collision Avoidance Systems. Journal of Guidance,
Control,and Dynamics 42(11 2018),1ś11. https://doi.org/10.2514/1.G003724
[25]Jinhan Kim,Robert Feldt,and ShinYoo.2019. Guiding DeepLearningSystem
TestingUsingSurpriseAdequacy. 2019IEEE/ACM41stInternationalConference
onSoftwareEngineering (ICSE) (2019), 1039ś1049.
[26]DiederikP.KingmaandJimmyBa.2014. Adam:AMethodforStochasticOpti-
mization. CoRRabs/1412.6980(2014).
[27]AlexKrizhevsky,VinodNair,andGeoffreyHinton.[n.d.]. CIFAR-10(Canadian
InstituteforAdvancedResearch). ([n.d.]). http://www.cs.toronto.edu/~kriz/cifar.
html
[28]AlexKrizhevsky,IlyaSutskever,andGeoffreyEHinton.2012. Imagenetclassifica-
tion with deep convolutional neural networks. In Advances in neural information
processingsystems . 1097ś1105.
[29]S.KullbackandR.A.Leibler.1951. OnInformationandSufficiency. Ann.Math.
Statist.22,1 (03 1951),79ś86. https://doi.org/10.1214/aoms/1177729694
[30]AlexeyKurakin,IanJGoodfellow,andSamyBengio.2018. AdversarialExamples
in the Physical World. In Artificial Intelligence Safety and Security . Chapman and
Hall/CRC, 99ś112.
[31]Kiran Lakhotia, Mark Harman, and Phil McMinn. 2007. A multi-objective ap-
proach to search-based test data generation. Proceedings of GECCO 2007: Genetic
andEvolutionaryComputationConference ,1098ś1105. https://doi.org/10.1145/
1276958.1277175[32]Yann LeCun and Corinna Cortes. 2010. MNIST handwritten digit database.
http://yann.lecun.com/exdb/mnist/. (2010). http://yann.lecun.com/exdb/mnist/
[33]Zenan Li, Xiaoxing Ma, Chang Xu, and Chun Cao. 2019. Structural Coverage
Criteria for Neural Networks Could Be Misleading. In Proceedings of the 41st
InternationalConferenceonSoftwareEngineering:NewIdeasandEmergingResults
(Montreal, Quebec, Canada) (ICSE-NIER ’19) . IEEE Press, 89ś92. https://doi.org/
10.1109/ICSE-NIER.2019.00031
[34]X. Liu, L. Zhang, and S. Hong. 2010. Global biodiversity research during
1900ś2009: a bibliometric analysis. Biodiversity and Conservation 20 (2010),
807ś826.
[35]ZiweiLiu,PingLuo,XiaogangWang,andXiaoouTang.2015. DeepLearningFace
Attributes in the Wild. In Proceedings of International Conference on Computer
Vision(ICCV) .
[36]Jost Lou. 2010. The Relation between Evenness and Diversity. Diversity 2 (02
2010).https://doi.org/10.3390/d2020207
[37]LeiMa,FelixJuefei-Xu,FuyuanZhang,JiyuanSun,MinhuiXue,BoLi,Chunyang
Chen,TingSu,LiLi,YangLiu,JianjunZhao,andYadongWang.2018. DeepGauge:
Multi-granularity Testing Criteriafor Deep Learning Systems. In Proceedings of
the33rdACM/IEEEInternationalConferenceonAutomatedSoftwareEngineering
(Montpellier, France) (ASE 2018) . ACM, New York, NY, USA, 120ś131. https:
//doi.org/10.1145/3238147.3238202
[38]Wei Ma, Mike Papadakis, Anestis Tsakmalis, Maxime Cordy, and Yves Le Traon.
2019. Test Selectionfor DeepLearningSystems. ArXivabs/1904.13195 (2019).
[39]Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
Adrian Vladu. 2018. Towards Deep Learning Models Resistant to Adversar-
ial Attacks. In International Conference on Learning Representations .https:
//openreview.net/forum?id=rJzIBfZAb
[40]William M McKeeman. 1998. Differential testing for software. Digital Technical
Journal10,1 (1998), 100ś107.
[41]Lubin Meng, Chin-Teng Lin, Tzyy-Ping Jung, and Dongrui Wu. 2019. White-
Box Target Attack for EEG-Based BCI Regression Problems. In International
Conference onNeural InformationProcessing . Springer, 476ś488.
[42]mseitzer. 2018 (accessed August 9, 2019). Frèchet Inception Distance (FID score) in
PyTorch.https://github.com/mseitzer/pytorch-fid
[43]YuvalNetzer,T.Wang, A.Coates, AlessandroBissacco, B. Wu,and A. Ng. 2011.
ReadingDigitsin Natural Images with Unsupervised FeatureLearning.
[44]Anh Mai Nguyen, Jason Yosinski, and Jeff Clune. 2016. Multifaceted Feature
Visualization: Uncovering the Different Types of Features Learned By Each
Neuronin DeepNeural Networks. ArXivabs/1602.03616 (2016).
[45]Chris Olah, Alexander Mordvintsev, and Ludwig Schubert. 2017. Fea-
ture Visualization. Distill(2017). https://doi.org/10.23915/distill.00007
https://distill.pub/2017/feature-visualization.
[46]NicolasPapernot,PatrickMcDaniel,SomeshJha,MattFredrikson,ZBerkayCelik,
and Ananthram Swami. 2016. The limitations of deep learning in adversarial
settings. In 2016 IEEE European symposium on security and privacy (EuroS&P) .
IEEE,372ś387.
[47] KexinPei,YinzhiCao,JunfengYang,andSumanJana.2017. DeepXplore:Auto-
mated Whitebox Testing of Deep Learning Systems. In Proceedings of the26th
Symposiumon Operating SystemsPrinciples (Shanghai,China) (SOSP’17) .ACM,
NewYork, NY, USA,1ś18. https://doi.org/10.1145/3132747.3132785
[48]HuyPhan. 2019 (accessed October4, 2019). PyTorch models trained on CIFAR-10
dataset.https://github.com/huyvnphan/PyTorch-CIFAR10
[49]E. C.Pielou. 1966. The measurement of diversity in differenttypes of biological
collections.
[50]BruceRatner.2009. Thecorrelationcoefficient:Itsvaluesrangebetween+1/1,or
do they? Journal of Targeting,Measurement and Analysis forMarketing 17, 2(01
Jun2009),139ś142. https://doi.org/10.1057/jt.2009.5
[51]Tim Salimans, Ian J. Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Rad-
ford, and Xi Chen. 2016. Improved Techniques for Training GANs. ArXiv
abs/1606.03498 (2016).
[52]SergioSegura,GordonFraser,AnaB.Sánchez,andAntonioRuiz-Cortés.2016. A
SurveyonMetamorphicTesting. IEEETransactionsonSoftwareEngineering 42
(09 2016),1ś1. https://doi.org/10.1109/TSE.2016.2532875
[53]JasmineSekhonandCodyFleming.2019. TowardsImprovedTestingForDeep
Learning. 2019 IEEE/ACM 41st International Conference on Software Engineering:
NewIdeas and EmergingResults (ICSE-NIER) (2019), 85ś88.
[54]C.E.Shannon.2001. AMathematicalTheoryofCommunication. SIGMOBILE
Mob.Comput.Commun.Rev. 5,1(Jan.2001),3ś55. https://doi.org/10.1145/584091.
584093
[55]DavidSilver,AjaHuang, ChrisJMaddison,ArthurGuez,LaurentSifre,George
Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershel-
vam, Marc Lanctot, et al .2016. Mastering the game of Go with deep neural
networks and treesearch. Nature529, 7587 (2016), 484ś489.
[56]Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbig-
niewWojna.2015. RethinkingtheInceptionArchitectureforComputerVision.
2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2015),
2818ś2826.
861ESEC/FSE ’20, November8ś13,2020,VirtualEvent, USA FabriceHarel-Canada,Lingxiao Wang,MuhammadAliGulzar,QuanquanGu,andMiryungKim
[57]ChristianSzegedy,WojciechZaremba,IlyaSutskever,JoanBruna,DumitruErhan,
Ian Goodfellow, and Rob Fergus. 2014. Intriguing properties of neural networks.
InICLR.
[58]YuchiTian,KexinPei,SumanJana,andBaishakhiRay.2018.DeepTest:Automated
Testingof Deep-neural-network-driven Autonomous Cars.In Proceedings of the
40thInternationalConferenceonSoftwareEngineering (Gothenburg,Sweden) (ICSE
’18).ACM,NewYork,NY,USA,303ś314. https://doi.org/10.1145/3180155.3180220
[59] LuísTorgo and JooGama. 1999. Regressionby Classification. (07 1999).
[60]JianxiongXiao,J.Hays,K.Ehinger,A.Oliva,andA.Torralba.2010. SUNdatabase:
Large-scale scene recognition from abbey to zoo. 2010 IEEE Computer Society
Conference onComputer Visionand PatternRecognition (2010), 3485ś3492.[61]Jason Yosinski,JeffClune, AnhMai Nguyen,ThomasJ.Fuchs,and HodLipson.
2015. Understanding Neural Networks Through Deep Visualization. ArXiv
abs/1506.06579 (2015).
[62]Jie M. Zhang, Mark Harman, Lei Ma, and Yang Liu. 2019. Machine Learn-
ing Testing: Survey, Landscapes and Horizons. CoRRabs/1906.10742 (2019).
arXiv:1906.10742 http://arxiv.org/abs/1906.10742
[63]MengshiZhang,YuqunZhang,LingmingZhang,CongLiu,andSarfrazKhurshid.
2018. DeepRoad: GAN-based Metamorphic Testing and Input Validation Frame-
work for Autonomous Driving Systems. In Proceedings of the 33rd ACM/IEEE
InternationalConferenceonAutomatedSoftwareEngineering (Montpellier,France)
(ASE2018) .ACM,NewYork,NY,USA,132ś142. https://doi.org/10.1145/3238147.
3238187
862