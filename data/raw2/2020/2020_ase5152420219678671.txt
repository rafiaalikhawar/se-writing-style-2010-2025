InstruGuard: Find and Fix Instrumentation Errors
for Coverage-based Greybox Fuzzing
Y uwei Liu1,2†, Y anhao Wang3†, Purui Su1,2, Y uanping Y u1,2and Xiangkun Jia1,2∗
1TCA/SKLCS, Institute of Software, Chinese Academy of Sciences
2School of Cyber Security, University of Chinese Academy of Sciences
3QiAnXin Technology Research Institute
{yuwei2018, purui, yuanping2017, xiangkun}@iscas.ac.cn wangyanhao@qianxin.com
Abstract —As one of the most successful methods at vulner-
ability discovery, coverage-based greybox fuzzing relies on the
lightweight compile-time instrumentation to achieve the ﬁne-grained coverage feedback of the target program. Researchersimprove it by optimizing the coverage metrics without ques-tioning the correctness of the instrumentation. However, instru-mentation errors, including missed instrumentation locationsand redundant instrumentation locations, harm the ability offuzzers. According to our experiments, it is a common andsevere problem in various coverage-based greybox fuzzers andat different compiler optimization levels.
In this paper, we design and implement InstruGuard, an
open-source and pragmatic platform to ﬁnd and ﬁx instrumen-tation errors. It detects instrumentation errors by static analysison target binaries, and ﬁxes them with a general solution basedon binary rewriting. To study the impact of instrumentationerrors and test our solutions, we built a dataset of 15 real-worldprograms and selected 6 representative fuzzers as targets. Weused InstruGuard to check and repair the instrumented bina-ries with different fuzzers and different compiler optimizationoptions. To evaluate the effectiveness of the repair, we ran thefuzzers with original instrumented programs and the repairedones, and compared the fuzzing results from aspects of executionpaths, line coverage, and real bug ﬁndings. The results showedthat InstruGuard had corrected the instrumentation errors ofdifferent fuzzers and helped to ﬁnd more bugs in the dataset.Moreover, we discovered one new zero-day vulnerability missedby other fuzzers with ﬁxed instrumentation but without anychanges to the fuzzers.
Index T erms—Software Security, Fuzzing, Instrumentation
I. I NTRODUCTION
Coverage-based greybox fuzzing has become one of the
most popular techniques for software vulnerability discovery
due to its ease of use and efﬁciency [35]. Taking the state-of-the-art fuzzer AFL (American Fuzzy Lop) [52] and itspopular family tools [9–11, 13, 14, 17, 18, 23, 24, 27, 36,44, 47, 50, 53, 54] as examples, the workﬂow of thesegreybox fuzzers can roughly be divided into two main stages:instrumentation and fuzzing loop, as shown in Figure 1. Theinstrumentation codes injected into compiled programs areused to capture branch (edge) coverage or other featureswhile fuzzing. It provides a simple way for fuzzer to auto-matically mutate input ﬁles towards more coverage based onthe captured feedback. In this way, coverage-based greybox
†co-leading authors.∗corresponding author.Initial Seed
Ś
Trim Caseś
Mutate Case
Execute and 
MonitorŝNext Case
Ŝ
Ş
Crash ReportFeedbackQueueQueueFuzzing Loop
Instrumentation
Instrumented
ProgramProgram
cur_location = <COMPILE_TIME_RANDOM>;
shared_mem[cur_location ^ prev_location]++;
prev_location = cur_location >> 1;
basic
blockbasic
block
basic
blockcur_location = <COMPILE_TIME_RANDOM>;
shared_mem[cur_location ^ prev_location]++;prev_location = cur_location >> 1;
basic
blockbasic
block
basic
block
Figure 1: The Workﬂow of Coverage-based Greybox Fuzzing.
fuzzing could learn the input format and test the deeper
program logic without prior knowledge.
As instrumentation feedback is crucial to greybox fuzzer,
researchers propose lots of approaches to improve it. Partsof them enhance the function of the injected code to recordmore behavior information of the program for detectingspeciﬁc vulnerabilities, such as concurrency [48] and memorycorruption bugs [10, 46, 47]. The other of them [10, 18, 49]improve the sensitivity of the instrumentation feedback forall branches to perceive subtle changes of different programstates. All of the current work strongly relies on an assump-tion that the instrumentation for getting coverage feedback iscomplete and accurate. However, according to our analysis,the assumption is not always correct.
The current greybox fuzzers provide two methods to
inserts instrumentation into each basic block to get accurateedge coverage feedback. One is the assembly-level rewritingapproach. The other is compiler-level instrumentation, whichleverages the plugin of compilers such as LL VM [22].However, some factors, such as compiler optimizations, resultin basic block merging and other impacts that bring a sideeffect of missing instrumentation locations and redundantinstrumentation locations, and affect the completeness and
5682021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)
DOI 10.1109/ASE51524.2021.000572021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE) | 978-1-6654-0337-5/21/$31.00 ©2021 IEEE | DOI: 10.1109/ASE51524.2021.9678671
978-1-6654-0337-5/21/$31.00  ©2021  IEEE
accuracy of the instrumentation feedback for both methods.
In the worst-case scenario of missing instrumentation lo-cations of some basic blocks, the fuzzers cannot perceivewhether the missing parts are executed and lose the chanceto ﬁnd the potential vulnerabilities in the lost code fragments.
In this work, we try to ﬁnd and ﬁx errors of instrumentation
and set a series of experiments to ﬁgure out the impacts onfuzzing. For explicitness of our research, we name the incor-rect instrumentation locations in the instrumented binaries as“instrumentation errors”, and break the problem into threeresearch questions as follows:
•RQ1: How serious are instrumentation errors?
•RQ2: Can we ﬁx instrumentation errors of fuzzers?
•RQ3: Does the ﬁxed instrumentation beneﬁt to fuzzing?
To answer the research questions, we built a dataset of 15
real-world programs collected from recent fuzzing papers asour targets [10, 12, 13, 18, 27, 29, 36, 46, 50] and selected 6representative greybox and whitebox fuzzers (i.e., AFL [52],FairFuzz [24], MOPT [27], Memlock [47], AFL++ [16], andAngora [10]) to study the impacts. We instrumented the targetprograms following the fuzzers’ instructions with their instru-mentation methods, including assembly-level instrumentationand compiler-level instrumentation, then checked instrumen-tation errors in the binaries. We also studied the impactsof different compiler optimization options. Concretely, wetraversed each basic block in the program based on IDAPro [2] and compared it with the speciﬁc patterns to judgewhether it was instrumented or not heuristically. The resultsshowed that the problem is common in the real world as itexists in both types of instrumentation methods of differentfuzzers and at all compiler optimization levels.
A straightforward solution to ﬁx instrumentation errors is
controlling the compiler options. However, according to ourexperiments, the compiler-level instrumentation errors couldnot be eliminated effectively. This paper presents a generalmethod based on binary rewriting and we implement a pro-totype tool called InstruGuard to ﬁnd and ﬁx instrumentationerrors of coverage-based greybox fuzzers.
For evaluating our solutions, we ran a series of fuzzing
experiments on the dataset we built. More speciﬁcally, weused code coverage, path number, and real bugs
1as the met-
rics to compare the results of original instrumented programsand the ﬁxed ones with different fuzzers. The results of 72hours×5 times running showed that our ﬁx could ensure the
correctness of the instrumentation implementation. Althoughwe are not improving coverage-feedback greybox fuzzingdirectly, our solutions are beneﬁcial for fuzzing. We foundone new vulnerability missed by other fuzzers just with theﬁxed instrumentation but without any changes to the originalfuzzer. Overall the paper makes the following contributions:
•We point out that instrumentation errors, includingmissed instrumentation locations and redundant instru-
1Real bugs represent the vulnerabilities found by fuzzers and are manually
veriﬁed. We associate each real bug with the corresponding CVE-ID or bug
issue number.mentation locations, are common for different fuzzers
and different compiler optimization options, and impactcoverage-based greybox fuzzing seriously.
•We propose a general solution for instrumentation errorsbased on binary rewriting. Based on the solution, wedesign and implement an open-source and pragmaticplatform to ﬁnd and ﬁx instrumentation errors.
•We built a dataset of real-world programs and eval-uated the effectiveness of InstruGuard by fuzzing theﬁxed programs and the original instrumented programswithout any modiﬁcation of the fuzzers. The resultsshowed that we had corrected the instrumentation ofcoverage-based greybox fuzzing and helped to ﬁnd morevulnerabilities.
To foster future research, we will release the source code
of InstruGuard and the dataset at https://github.com/Marsman1996/instruguard.
II. B
ACKGROUND
A. Coverage-based greybox Fuzzing
Fuzzing was proposed in the 1990s [28] and developed
for decades. Among kinds of fuzzers, coverage-based grey-box fuzzers (e.g., AFL [52]) attract more attention recentlybecause of their high efﬁciency and ease of use. Based on amodiﬁed form of edge coverage to effortlessly pick up subtle,local-scale changes to program control ﬂow, coverage-basedgreybox fuzzing could mutate towards more program pathsand ﬁnd more vulnerabilities.
To get the coverage feedback, coverage-based greybox
fuzzers implement instrumentation while compiling. Thecode for collecting coverage information is inserted into thetarget program by two methods when the source code isavailable, i.e., assembly-level instrumentation and compiler-level instrumentation. We will use AFL as the representationof coverage-based greybox fuzzers to introduce the details ofinstrumentation in the following paragraphs.
Assembly-level instrumentation. AFL uses wrappers
(i.e.,
afl-gcc andafl-clang ) for two normal compilers (i.e.,
gcc andclang ) to conduct assembly-level instrumentation
and produce binaries. They parse the assembly ﬁle line byline and modify it during the compilation stage accordingto the following rules. Rule1: If the line is a function label,
branch destination label, or conditional jump instruction, theywill add instrumentation. It is mainly because these labels andinstructions mark the boundaries of the basic blocks. Rule2:I f
the line is in the section other than
text2or after .p2align3,
they will leave this basic block un-instrumented even thoughthe line satisﬁes the Rule1.
Compiler-level instrumentation. The LL VM mode of
AFL leverages
afl-clang-fast to do compiler-level instru-
mentation via loading LL VM pass while compiling. It walks
2Thetext section is used for keeping the actual assembly code of a
program. Hence, AFL only inserts instrumentation into this section.
3AFL does not intend to instrument the basic blocks after .p2align
to reduce unnecessary instrumentation while compiling the program under
OpenBSD.
569 1 fread(hdr, sizeof(file_header), 1, f);
 2 if (hdr->magic != MAGIC) exit(1);
 3 entry *ent = (entry *) malloc( sizeof(entry));
 4 fread(ent, sizeof(entry), 1, f);
 5 if (ent->type == TYPEA) {
 6   if (ent[0] == 0x6c) {
 7     if(ent[1] == 0x61)                            
 8       if(ent[2] == 0x75)                          
 9         if(ent[3] == 0xde)
10           printf( "fdata = %f\n" +*(unsigned int  *)ent, 
ent->data.fdata); // crash
11     }
12 } 6   if (ent[0] == 0x6c) {
 7     if(ent[1] == 0x61)                            
 8       if(ent[2] == 0x75)                          
 9         if(ent[3] == 0xde)
10           printf( "fdata = %f\n" + *(unsigned int  *)ent, 
                    ent->data.fdata); // crash
11   }
12 } 7     if(ent[1] == 0x61)                            
 8       if(ent[2] == 0x75)                          
 9         if(ent[3] == 0xde)
(a) Source code of the simpliﬁed sample.
 1 %cmp31 = icmp eq i8 %9, 0x61              
 2 %cmp35 = icmp eq i8 %11, 0x75            
 3 %or.cond71 = and i1 %cmp31, %cmp35     
 4 %cmp39 = icmp eq i8 %5, 0xDE             
 5 %or.cond72 = and i1 %cmp39, %or.cond71 
 6 br i1 %or.cond72, label %if.then41, label %if.end58 1 %cmp31 = icmp eq i8 %9, 0x61              
 2 %cmp35 = icmp eq i8 %11, 0x75            
 3 %or.cond71 = and i1 %cmp31, %cmp35     
 4 %cmp39 = icmp eq i8 %5, 0xDE             
 5 %or.cond72 = and i1 %cmp39, %or.cond71 
 6 br i1 %or.cond72, label %if.then41, label %if.end58
(b) LL VM IR code of the nested if statements.Falseloc_400BB9:  
  Instrumentation
  cmp al, 6Ch
  jz  loc_400CFATrueloc_400BEC:    Instrumentation
  cmp dl, 61h
  jz  loc_400CFA
True
loc_400C13:  
  cmp cl, 75h   
  jz  loc_400CFA
False
loc_400CFA:
  Instrumentation  retnTrue
loc_400C1C:  
  cmp sil, 0DEh  
  jz  loc_400CFA
True
loc_400C26:  
  Instrumentation  lea  rdi, fdata_addr
  call printfFalse
False
Eh
(c) Control ﬂow graph of the assembly code.
Figure 2: Motivation example. Including the source code, the control ﬂow graph of lines 6 ∼12 of the source code compiled with -O3, and
the IR code of lines 7∼9 of the source code.
through all basic blocks at the LL VM IR (IntermediateRepresentation) level and inserts instrumentation codes at thebeginning of each basic block.
B. Motivating Example
The coverage-feedback greybox fuzzers assume that the
compilers or wrappers they use could carry out correct
instrumentation during compilation, helping them to obtainaccurate feedback from running states. However, it remainsunexplored whether the compiler could guarantee the accu-racy of the instrumentation. We will illustrate the problemthrough the simpliﬁed code snippet in Figure 2a.
The sample program
4is simpliﬁed from a real-world code
snippet of the Libxml2 library [41], which is a softwarelibrary for parsing XML documents. It ﬁrst parses the ﬁleheader and compares the checksum with the magic number.After that, it copies the content of the ﬁle to
ent. Then
the program veriﬁes the ﬁrst four bytes in ent by a nested
ifstructure one by one. Only if all the checks are passed
through, it will trigger the crash at line 10.
Based on the intuition of coverage-based greybox fuzzing,
every branch of the sample program should be instrumentedso that AFL could ﬁnd the crash easily. But it was sur-prising that for binary compiled with
-O3 (i.e., the default
compiler optimization level that AFL uses), AFL could notﬁnd any crashes after 24 hours, and failed to cover line8∼10. According to our analysis, this is because of the
incomplete instrumentation. Figure 2c illustrates the controlﬂow graph of the sample binary. As the graph shows, thenested
ifstructure in Figure 2a (i.e., line 6∼ 9) contains four
basic blocks (i.e., loc 400BB9, loc 400BEC, loc 400C13,
and loc 400C1C). To achieve complete instrumentation and
4https://groups.google.com/g/aﬂ-users/c/e89ruXs7oOc/m/JjU03GlEBQAJkeep sensitive for all branches, the greybox fuzzer shouldinstrument all four assembly-level basic blocks. However, aninstrumentation error occurs while compiling the programin the LL VM mode of AFL. As Figure 2b shows, threecompare statements (i.e., line 7∼ 9 in Figure 2a) are merged
into one LL VM IR-level basic block because of the opti-mization that the compiler applies. Hence,
afl-clang-fast
only inserts one instrumentation at the beginning of thebasic block. In the corresponding binary produced by thecompiler, two assembly-level basic blocks (i.e., loc
400C13
and loc 400C1C) miss instrumentation because of the error.
As a result, it loses the ability to perceive two missedbranches, and triggers the vulnerability after these basicblocks with a low probability.
C. Instrumentation errors of Greybox Fuzzers
According to our observation, there are mainly two types
of errors in the instrumentation of coverage-based greybox
fuzzers (we named them as “instrumentation errors” in thispaper).
MIL (missed instrumentation location) means one basic
block is missed by instrumentation. If the basic block is notinstrumented, it will not give back some key informationwhen it is executed, such as the coverage feedback [52],memory usage behavior [47], and so on. As a result, thefuzzer will not get useful feedback on this basic block andmight discard some newly generated interesting samples,which could help the fuzzer explore more program states.
RIL (redundant instrumentation location) means that there
is more than one instrumentation inserted in the same basicblock. RIL increases the path depth, which misleads thefuzzers depend on the execution depth. In addition, becausegreybox fuzzers use ﬁxed-size (e.g., 64KB) hash tables (i.e.,bitmap) to store feedback information, RIL, which expands
570Algorithm 1 The Detection Workﬂow.
Input: Instrumented Program
Output: Data about instrumentation errors of Program
1: P← DISASSEMBLE (Program)
2:for bb = P .StartBB → P .EndBB do ⊿scan each basic block
3: InstruSet ←∅ ⊿used to record all instrumentation in a bb
4: offset← 0⊿used to record the offset of instruction in the pattern sequence
5: instSequence ← empty list ⊿used to record successive instructions
6: for inst = bb.StartInst → bb.EndInst do ⊿scan each instruction
7: ifINSTRU PA TTERN MA TCH (inst, offset )then
8: instSequence. APPEND (inst )
9: ifoffset == S IZE(intruPattern) −1then
10: InstruSet ← InstruSet ∪instSequence
11: offset← 0
12: instSequence ←∅
13: else
14: offset← offset + 1
15: end if
16: end if
17: end for
18: ifSIZE(InstruSet )= =0then
19: MILNum ← 1
20: else if SIZE(InstruSet )>1then
21: RILNum ← SIZE(InstruSet )−1
22: end if
23: SAV E INSTRU INFO (bb, InstruSet, MILNum, RILNum)
24: end for
the number of instrumentation locations, could exacerbate
bitmap collision.
Some researchers [21, 25, 26] notice that instrumentation
could affect fuzzing, but few of them analyze and evaluatethe impacts. As it is an underlying problem for almost allcoverage-feedback greybox fuzzers, it motivates us to designan automatic tool to ﬁnd and ﬁx them to ensure fuzzing withcorrect instrumentation.
D. F ocus of this paper
In this paper, we focus on studying the instrumentation
errors of coverage-feedback greybox fuzzers with compile-
time instrumentation. We try to develop a tool to ﬁnd and ﬁxthese errors in the instrumented target binaries. Although weare not improving coverage-feedback greybox fuzzing fromthe fuzzing framework or strategies, our ﬁndings and toolcan cooperate with existing fuzzers and beneﬁt them as theinstrumentation is accurate and complete.
III. M
ETHOD
According to our analysis, instrumentation errors could
cause incorrect coverage feedback and further harm fuzzing.To ﬁnd and ﬁx these errors, we ﬁrstly design a methodto detect them in target binaries based on static analysis.Then we design two methods to ﬁx instrumentation errors:ﬁrstly we try a straightforward method by changing compileroptions, then we propose a general approach based on binaryrewriting named InstruGuard. It should be noted that, in thissection, the basic block represents the assembly-level basic
block.
A. Detect Instrumentation Errors
To detect instrumentation errors, we disassemble the in-
strumented program and examine each basic block heuris-
tically. As shown in Algorithm 1, for a basic block
bbof
the target program, we traverse all the instructions in itAlgorithm 2 The Repair Workﬂow.
Input: Instrumented Program
Output: Fixed Program
1: P← DISASSEMBLE (Program)
2:for bb = P .StartBB → P .EndBB do
3: InstruSet, MILNum, RILNum ← LOAD INSTRU INFO (bb)
4: ifMILNum then
5: INSERT INSTRU (bb)
6: else if RILNum then
7: for i=0→ RILNum do
8: DELETE INSTRU (bb, InstruSet[i])
9: end for
10: end if
11: end for
12: FixedProgram ← ASSEMBLE (P)
(line 6), leverage function InstruPatternMatch to check each
instruction with the speciﬁc pattern of the instrumentation,which is implemented by the target fuzzer, and judge whetherit belongs to the instrumentation (line 7). If so, we add theinstruction
inst to the instruction sequence instSequence
(line 8). If the instrumentation pattern is matched exactly(line 9), we add
instSequence toInstruSet (line 10) and
reset the correlation variables (line 11∼ 12). When ﬁnishing
the traversal, we check the number of instruction sequencesin
InstruSet . If there is no sequence in InstruSet , which
means that bbhas a MIL error, we set the MILNum to 1 (line
18∼ 19). If there is more than one sequence, this bbhas one
or more RIL errors, and we set the RILNum to the number of
RIL errors (line 20∼ 21). After that, we save the information
of the instrumentation set InstruSet , the number of MIL
errorMILNum , and the number of RIL errors RILNum (line 23).
1mov reg1, cs: __afl_area_ptr ;shared_mem
2xor reg2, cur_loc ;edge_id
3add byte ptr [reg1+reg2], 1 ;shared_mem[edge_id]++
4mov reg2, cur_loc >> 1 ;reg2 stores the prev_loc
Listing 1: The template code of the instructions instrumented by
afl-clang-fast .cur loc is a constant and is generated during the
compilation.
The patterns we used for matching are extracted from the
instrumentation codes of fuzzers. Taking AFL as an example,the patterns are as following:
1 for programs compiled with
afl-gcc , we mark the call instruction to the record function
aflmaybe log5as the feature of instrumentation; 2
for programs compiled with afl-clang-fast , we highlight
the instruction sequence of xor,add/inc , and mov, which
represents the logic of the inserted instructions of instru-mentation as shown in Listing 1. Speciﬁcally, AFL obtainstheedge
idby applying XOR operation to the cur loc and
prev loc, adds one to shared mem[edge id], and stores
the left shifted cur loc toprev loc. If InstruGuard ﬁnds
the sequence, and the second argument of instructions xor
(x2) and mov (m2) satisfy the equation: m2=x2>> 1,i t
marks the instruction sequence as the instrumentation. For
5AFL inserts a function call to aflmaybe log in each basic block, and
the parameter to that call is a different value in each basic block. Therefore,
when this instrumented code is executed, AFL can log which branch istriggered.
571Figure 3: Distribution of the number of affected instrumentation
location in different optimization ﬂags of clang.
other fuzzers, we also manually analyze the features and use
them as the prior knowledge for detection.
B. Fix Instrumentation Errors
1) Straightforward solution based on compiler options:
As both of the instrumentation methods are implemented
while compiling and the process are affected by compileroptions, a straightforward solution is to control compileroptimization options. Intuitively, there should be no instru-mentation errors if the basic blocks are not optimized duringthe compilation. We analyzed the impacts of the options oninstrumentation locations by passing different optimizationﬂags to the compiler. There are 27 ﬂags of clang and 66 ﬂagsof gcc, which could change the number of instrumentationlocations when used alone (Figure 3 displays the effect andmarks the top 10 ﬂags). We disabled all these ﬂags to ﬁx theinstrumentation errors and checked the compiled binaries.
However, according to our experiments, it is not the
proper way to ﬁx instrumentation errors. Besides losing theperformance advantage of compiler optimization, the instru-mentation errors are not mitigated effectively by changingcompiler options. As shown in Table III, the repair rate isonly 49.86 % on average.
2) General solution based on Binary Rewriting: We pro-
pose an intuitive but more general repair method that directlyﬁxes the instrumentation errors on the instrumented binariesbased on binary rewriting. After identifying whether thereare instrumentation errors and locating the position of theerrors, we ﬁx the MIL and RIL errors following the workingprocedure shown in Algorithm 2.
Firstly we disassemble the given program to the assembly
code
P(line 1). Then we traverse the basic blocks in Pto ﬁx
the instrumentation errors (line 2). For a speciﬁc basic block
bb, we load the result of Algorithm 1 to get instrumentation
setInstruSet , the number of MIL errors MILNum , and the
number of RIL errors RILNum (line 3). If this bbhas a MIL
error, we insert an instruction sequence (i.e., instrumentation)to the
bb(line 4∼5). If this bbhas one or more RIL errors,
we remove all instrumentations except the last one in this bb
(line 6∼ 9). Finally, we recompile the program Pand get the
repaired program (line 12).
Some instrumentation methods of greybox fuzzers, such
as the compiler-level instrumentation of AFL, make thePre-Processing
Basic BlocksControl Flow 
Graphs
 Instrumentation 
Error DetectionInstrumentation
Error CorrectionCompiler
RecompileError
LogAssembly
FilesInstrumented 
Executable FilesGreybox 
Fuzzers
Instrumentation
Error LogsInstrumented 
Executable FilesInstrumentation 
Pattern Analysis
Figure 4: Architecture of InstruGuard.
instrumented code efﬁcient via applying optimizations toinstrumentations during the compilation (e.g., more efﬁcientassign of registers). Hence, we rewrite the target programbased on the instrumented binary instead of the vanilla binaryand expect to retain the original correct instrumentations tokeep the performance.
C. Implementation
We implement a framework named InstruGuard to ﬁnd and
repair instrumentation errors based on the above design. The
architecture is shown in Figure 4. InstruGuard consists ofthree major components: the instrumentation error detectioncomponent, the error correction component and the compi-lation component. Before being processed by InstruGuard,the instrumented executable ﬁles are pre-processed to getthe information of basic blocks and control ﬂow graphs.The instrumentation patterns are also prepared as our priorknowledge.
The detection component detects instrumentation errors,
which is developed based on IDApython [3]. It identiﬁesinstrumentation by matching the instruction sequence withparticular patterns, and we adjust the matching patterns todetect instrumentation for different fuzzers as described inSection III-A. It is worth noting that IDA Pro sometimesidentiﬁes a basic block incorrectly due to a false branch labelwith no corresponding jump instruction. InstruGuard ﬁxes itby checking the reference of each label with IDApython API
CodeRefsTo() .
The error correction component is implemented based on
RetroWrite [15], which is a precise and efﬁcient binary-rewriting instrumentation tool. Firstly, we use it to disas-semble the input binary ﬁle to the assembly code. ThenInstruGuard modiﬁes the assembly code to repair the instru-mentation errors. For MIL errors, it inserts an instructionsequence, such as a function call to the record function(e.g.,
aflmaybe log) or a pattern sequence (e.g., as
Listing 1 shows), to conduct instrumentation. For RIL errors,it comments out the redundant instructions.
The compilation component produces binary executables
based on the modiﬁed assembly code. In detail, we write
572a wrapper for the compiler (i.e., gcc) to recompile the
program. According to our analysis in Section IV -B, gcc
itself will not introduce new instrumentation errors except
for several side-effects in afl-gcc . For example, AFL may
miss the instrumentation after .p2align due to Rule2 if it uses
afl-gcc to compile programs under a UNIX-like operating
system except for OpenBSD.
IV . E V ALUA TION
In this section, we set experiments to answer the research
questions raised in Section I. To answer RQ1, we instru-mented real-world programs with the speciﬁc implementa-tion of different coverage-based greybox fuzzers and trieddifferent optimization levels of the compilers they use. Thenwe checked the instrumented programs with InstruGuard andanalysed the root cause of the instrumentation errors. ForRQ2 and RQ3, we repaired the instrumented programs witherrors and calculated the ﬁxed rate. Then we ran fuzzers withthe original programs and the ﬁxed ones, and compared thefuzzing results.
A. Setup Experiments
1) Program Dataset: We built a dataset of real-world
programs by gathering 15 open-source Linux applications
from recent papers published during the last two years withthe corresponding version. The 15 applications are shownin Table I, including image parsing and processing libraries,text parsing tools, multimedia ﬁle processing libraries, anddeveloping tools. In addition to version information, wealso represent the default optimization option set by theirdevelopers and in which paper the application is selected asthe test bench.
2) Instrumentation Methods: The instrumentation is re-
lated to fuzzers’ implementation and compiler optimization.In our experiments, we selected 6 state-of-the-art fuzzersthat get coverage feedback through instrumentation, i.e.,AFL [52], FairFuzz [24], MOPT [27], MemLock [47],Angora [10] and AFL++ [16]. As AFL is the most pop-ular coverage-based greybox fuzzer, we chose both theassembly-level mode (
afl-gcc ) and the compiler-level mode
(afl-clang-fast ) of it. FairFuzz, MOPT, and MemLock are
three tools based on AFL, but towards different goals. Wechose them to study the AFL’s family. Chen et al. rewritethe algorithms of AFL in Angora, so we chose it as thecomparison to avoid simple implementation bugs of greyboxfuzzers. AFL++ is a union of several improvements of AFLand contains two different instrumentation methods, i.e.,pcguard mode and LTO mode. We tested the above fuzzersand their instrumentation implementation to check whetherinstrumentation errors were common.
3) Fuzzing Setting: To study the impacts of instrumenta-
tion errors on fuzzing and test our solutions, we set fuzzingexperiments with the instrumented program and the corre-sponding fuzzers. All the experiments were performed onﬁve servers running Ubuntu 16.04.2 LTS and equipping withIntel(R) Xeon(R) CPU E5-2630 v3@2.40GHz (32 cores) andTABLE I: Real-world applications used in the experiment. Paper
means where the application is selected as the test bench.
Package Program VersionDefault
OptionPaper
libwav wav gain 5cc8746 -O2 EnFuzz [13]
mp3gain mp3gain 1.5.2 -O2 MOPT [27]
libjpeg cjpeg 9a -O2 EnFuzz
lupng lupng 877a76f -O0 EnFuzz
nm 2.29 -O2 CollAFL [18]
objdump 2.29 -O2 PTrix [12]
size 2.29 -O2 Angora [10]binutils
strip 2.29 -O2 Neuzz [36]
libming listswf 0.4.8 -O2 ProFuzzer
ngiﬂib gif2tga c8488d5 -O TortoiseFuzz
catdoc catdoc 0.95 -O2 CollAFL
libpng pngﬁx 1.6.34 -O2 Angora
tiff2pdf 4.0.9 -O2 TortoiseFuzzlibtifftiff2ps 4.0.9 -O2 CollAFL
mpg321 mpg321 0.3.2 -O2 MOPT
32GB RAM. The compilers were gcc 5.4.0 and clang 6.0,
as gcc 5.4.0 was the default gcc version of Ubuntu 16.04,and clang 6.0 was widely used by related works. For onetarget program, we ran experiments on the same server andconﬁgured it with the same seeds and command. We used thetest cases of AFL as seeds that could be processed by thetarget application. Otherwise, we randomly selected ﬁles. Theprogram arguments used in the evaluation were the same asthe corresponding papers or issues. Each experiment timeoutwas set to 72 hours. Furthermore, we repeated all experiments5 times and took the average value. We collected paths,coverage, and real bugs as the fuzzing results.
B. Detect Instrumentation Errors (RQ1)
We compiled the programs in our dataset following the
instructions of different fuzzers and with different compiler
optimization levels (i.e.,
O0toO3,O3is the default option of
AFL). Then we checked them with InstruGuard. The resultsare shown in Table II. Since FairFuzz and MOPT change theseed selection and mutation strategy without modifying theinstrumentation method of the vanilla AFL, we got the sameinstrumentation results as AFL and did not put them in thetable. We also did not list the data of the programs compiledby
afl-gcc withO0andO1in the table since they almost had
no instrumentation errors.
As Table II shows, instrumentation errors are common for
different programs and different fuzzers. Programs compiledby different fuzzers all have instrumentation errors. Evensmall packages, like
libwav whose total number of instru-
mentation locations is around 100, suffer from instrumen-tation errors. About one-ﬁfth of libwav’s basic blocks areincorrectly instrumented.
Instrumentation locations and the error rate vary a lot
among different fuzzers. Programs produced by AFL witheither the assembly-level or compiler-level instrumentationhave the lowest instrumentation error rate. A deeper analysisshows that the majority of instrumentation errors causedby AFL are MIL errors. As for Memlock, it modiﬁes the
573TABLE II: Number of instrumentation locations (ILs) and the percentage of the instrumentation errors (Err-%) of packages compiled by
different fuzzers and different optimization options. The symbol -means the corresponding fuzzer could not compile the package. AFL(ASM)
column shows the results of binaries compiled by afl-gcc .AFL column shows the results of binaries compiled by afl-clang-fast .I fn o t
speciﬁed, we use -O3 as the default option.
ProgramAFL(ASM) AFL-O0 AFL-O1 AFL-O2 AFL Memlock Angora AFL++ AFL++-LTO
ILs Err-% ILs Err-% ILs Err-% ILs Err-% ILs Err-% ILs Err-% ILs Err-% ILs Err-% ILs Err-%
catdoc 1,068 14% 1,071 8% 843 12% 915 13% 1,219 12% 1,098 8% 1,003 62% 660 60% 1,060 27%
libjpeg 5,488 49% 4,026 11% 3,240 13% 3,642 14% 4,532 15% 3,523 12% 3,328 65% 2,960 58% 4,906 31%
ngiﬂib 427 13% 392 8% 317 8% 422 8% 454 8% 407 8% 463 55% 253 56% 405 26%
libming 3,874 15% 3,749 16% 3,498 9% 5,321 11% 5,776 11% 3,075 37% 4,072 70% -- 3,461 27%
lupng 4,248 11% 3,329 11% 1,940 17% 2,464 18% 3,118 19% 3,708 11% 3,044 64% 1,588 63% 1,564 28%
mp3gain 3,441 12% 2,644 8% 1,740 18% 1,870 19% 2,428 19% 547 82% 2,335 63% 1,288 60% 2,141 28%
binutils 46,556 15% 43,098 9% 31,085 15% 37,653 15% 45,809 15% 61,718 10% 39,898 64% 31,953 64% 49,866 27%
libwav 101 24% 93 25% 72 19% 72 23% 92 23% 50 52% 70 63% 59 45% 45 35%
mpg321 2,287 16% -- -- 1,753 13% 1,773 14% -- -- -- 1,111 51%
libpng 16,868 12% 9,246 6% 7,332 15% 9,114 17% 9,417 21% 9,246 6% 7,916 65% 5,148 61% 3,955 52%
libtiff 25,086 13% 16,400 12% 12,851 14% 16,127 16% 16,692 20% 14,094 13% 11,844 63% 7,739 60% 9,411 52%
Average 9,949 18% 8,405 11% 6,292 14% 7,214 15% 8,301 16% 9,747 24% 7,397 63% 5,739 58% 7,084 35%
TABLE III: The number of the instrumentation errors before and after applying three ﬁxing methods and the ﬁxing rate. Ori-Err stands for
the instrumentation errors that we found in the program compiled with vanilla fuzzer, and After-Err is the remaining errors after our ﬁxes.
The symbol -means the corresponding fuzzer could not compile the binary.
ProgramStraightforward way AFL(ASM) AFL Memlock
Ori-Err After-Err Fix Rate Ori-Err After-Err Fix Rate Ori-Err After-Err Fix Rate Ori-Err After-Err Fix Rate
catdoc 186 106 43.01% 294 0 100% 186 0 100.00% 91 0 100.00%
cjpeg 520 86 83.46% 1,038 1 99.89% 520 0 100.00% 403 0 100.00%
gif2tga 41 21 48.78% 95 0 100% 41 0 100.00% 32 0 100%
listswf 688 749 -8.87% 1,059 0 100% 688 0 100.00% 1,416 1 100%
lupng 580 132 77.24% 452 0 100% 580 0 100.00% 362 1 99.72%
mp3gain 454 -- 407 0 100% 454 0 100.00% 2,279 0 100.00%
nm 7,595 3,531 53.51% 7,729 15 99.81% 7,595 1 99.99% 4,088 1 99.98%
objdump 10,856 5,996 44.77% 10,789 39 99.64% 10,856 5 99.95% 6,524 1 99.98%
size 7,516 2,911 61.27% 7,667 19 99.75% 7,516 1 99.99% 4,084 1 99.98%
strip 8,881 -- 7,649 14 99.82% 8,881 4 99.95% 4,638 1 99.98%
wav gain 20 13 35.00% 24 0 100% 20 0 100.00% 53 0 100%
mpg321 223 -- 356 0 100% 223 0 100% - --
pngﬁx 1,616 639 60.46% 2,079 1 99.95% 1,616 2 99.88% 597 0 100%
tiff2pdf 2,809 -- 3,199 4 99.87% 2,809 16 99.43% 1,857 0 100%
tiff2ps 2,425 -- 2,752 1 99.96% 2,425 4 99.84% 1,766 0 100%
Average 49.86% 99.91% 99.93% 99.96%
instrumentation method of the AFL to get more information
about the memory and causes more instrumentation errors.On average, there exist instrumentation errors in more thanone-fourth of basic blocks. With further analysis, the pro-grams compiled by Memlock have more RIL errors thanprograms compiled by other fuzzers. Instrumentation errorsare particularly common for programs compiled by Angoraand AFL++’s pcguard mode. For almost every programcompiled by Angora and AFL++, there are instrumentationerrors in more than half of the basic blocks.
The results also show that instrumentation errors intro-
duced by compiler-level instrumentation exist in all opti-mization options. In general, no matter what optimizationoption is set, more than 8% of the basic blocks of a programexist instrumentation errors. There are fewer instrumentationerrors in the programs compiled by
afl-clang-fast withO0.
But for a speciﬁc program, compiling with O0could increase
the instrumentation errors, such as libwav whose error rate
reaches 25%.We analyzed these instrumentation errors of different
fuzzers to explore the root cause, and found out that theroot causes are different for instrumentation errors introducedby assembly-level and compiler-level instrumentation. Forassembly-level instrumentation, the errors are caused by side-effects of the implementation of
afl-gcc . In detail, AFL
misses the instrumentation after .p2align due to the Rule2.
Besides, AFL adds redundant instrumentation code afterlabels that do not have the corresponding jump instruction(Rule1). For compiler-level instrumentation, the errors arecaused by the transformation process from IR code to assem-bly code. As the example in Figure 2 shows, there is no MILor RIL in the IR code, which is conﬁrmed for other programscompiled with compiler-level instrumentation by checkingtheir IR code. The errors happen during the transformationprocess from IR code to assembly code, the IR basic blockswill be split or merged due to the optimization, which causesthe MIL or RIL.
AFL, Memlock, Angora, and AFL++ use compiler-level
574TABLE IV: Code coverage, the number of real bugs, and the number of paths of the fuzzing result of the repaired program and the original
program. The last line is average for coverage and paths, and sum for real bugs. -re stands for the result of the repaired program. AFL(ASM)
column shows the results of binaries compiled by afl-gcc .AFL column shows the results of binaries compiled by afl-clang-fast .
ProgramAFL(ASM)-O0 AFL(ASM)-O1 AFL(ASM) AFL(ASM)-re AFL AFL-re
Bugs Coverage Paths Bugs Coverage Paths Bugs Coverage Paths Bugs Coverage Paths Bugs Coverage Paths Bugs Coverage Paths
catdoc 1 50.0% 423 1 50.0% 702 1 50.0% 651 1 50.0% 684 1 50.0% 454 1 50.0% 453
cjpeg 2 14.3% 1,115 2 29.2% 1,581 2 29.5% 1,231 2 30.0% 1,199 2 29.2% 1,376 2 29.8% 671
gif2tga 4 75.3% 20,328 4 75.3% 18,247 3 75.3% 2,885 4 75.3% 42,635 2 75.3% 7,587 5 75.3% 40,453
listswf 3 18.0% 4,408 3 18.4% 5,207 3 18.4% 4,919 3 18.8% 5,625 3 21.0% 2,906 2 21.0% 2,430
lupng 0 38.7% 80 0 38.7% 75 0 38.7% 84 1 38.7% 92 0 38.7% 63 1 38.7% 71
mp3gain 5 57.3% 1,130 4 59.4% 1,341 5 59.5% 1,712 5 59.5% 1,685 6 58.8% 1,160 5 60.1% 1,118
nm 0 11.0% 2,267 0 10.5% 2,532 0 10.4% 2,563 0 10.0% 2,328 0 10.6% 2,497 0 10.9% 2,467
objdump 2 7.5% 1,980 2 7.6% 2,404 2 7.6% 2,505 2 7.6% 2,753 2 7.8% 2,190 2 7.5% 1,906
size 0 6.5% 1,818 0 6.4% 2,566 1 6.2% 2,933 0 6.8% 2,775 0 6.2% 1,690 0 6.0% 1,672
strip 1 7.9% 1,406 1 8.2% 1,688 0 8.1% 1,791 0 9.2% 2,619 0 8.0% 1,265 0 8.8% 1,716
wav gain 2 77.0% 47 2 77.0% 40 2 77.0% 55 2 77.0% 58 2 77.0% 44 3 77.0% 46
mpg321 - -- 1 18.7% 186 1 18.6% 171 1 18.6% 177 1 18.6% 168 1 18.6% 183
pngﬁx 0 17.6% 334 0 17.7% 378 0 18.3% 324 0 18.4% 365 0 18.4% 336 0 18.4% 334
tiff2pdf 0 43.0% 5,025 0 42.8% 9,298 0 43.1% 9,376 0 43.2% 5,658 0 44.2% 5,526 0 44.3% 5,562
tiff2ps 0 33.1% 5,541 0 32.8% 6,711 0 36.6% 7,169 0 36.5% 5,941 0 37.4% 5,688 0 39.0% 5,288
Sum/A ver 20 32.7% 3,279 20 32.8% 3,530 20 33.2% 2,558 21 33.3% 4,973 19 33.4% 2,197 22 33.7% 4,291
ProgramFairFuzz FairFuzz-re MOPT MOPT-re Memlock Memlock-re
Bugs Coverage Paths Bugs Coverage Paths Bugs Coverage Paths Bugs Coverage Paths Bugs Coverage Paths Bugs Coverage Paths
catdoc 1 50.0% 384 2 50.2% 385 1 50.1% 362 1 50.2% 417 1 50.0% 380 1 50.0% 385
cjpeg 2 30.3% 705 2 30.5% 715 3 30.9% 722 4 29.4% 632 0 26.1% 398 2 29.2% 648
gif2tga 3 75.3% 359 4 75.3% 695 3 75.3% 289 3 75.3% 316 2 79.4% 439 2 79.4% 391
listswf 3 64.9% 3,046 3 68.6% 3,699 0 68.6% 4,566 0 68.8% 4,640 3 66.1% 3,650 3 67.2% 4,102
lupng 1 38.7% 62 1 38.7% 67 0 38.7% 61 0 38.7% 68 0 38.7% 172 0 38.7% 171
mp3gain 6 60.7% 1,379 6 60.6% 1,390 5 57.7% 1,234 5 56.3% 1,225 5 60.8% 1,304 5 60.8% 1,315
nm 0 10.8% 3,928 0 10.0% 2,967 0 10.1% 2,015 0 10.1% 2,023 0 9.2% 5,808 0 9.4% 6,734
objdump 3 8.2% 3,029 3 8.7% 3,074 2 7.8% 2,187 2 8.2% 2,681 2 8.6% 3,397 2 8.3% 3,302
size 0 6.4% 1,973 1 6.9% 2,223 0 6.1% 1,974 0 7.0% 2,251 1 5.8% 2,929 0 5.7% 3,126
strip 0 8.4% 1,972 0 9.0% 2,279 0 9.1% 1,681 0 8.9% 1,840 0 8.1% 2,649 0 8.7% 2,744
wav gain 2 73.1% 42 2 73.1% 43 1 73.1% 40 1 73.1% 40 1 73.1% 48 2 73.1% 56
mpg321 1 18.7% 192 1 18.7% 170 1 18.6% 191 1 18.6% 164 - -- - --
pngﬁx 0 17.7% 318 0 17.8% 304 0 17.7% 285 0 17.7% 288 0 18.3% 324 0 18.3% 362
tiff2pdf 0 44.5% 6,741 0 44.2% 6,624 0 39.4% 3,316 0 41.0% 4,556 0 36.1% 2,504 0 37.5% 2,661
tiff2ps 1 38.4% 6,062 1 38.4% 6,145 0 27.2% 1,514 0 27.6% 1,378 0 30.9% 2,165 0 31.2% 1,876
Sum/A ver 23 36.4% 2,013 26 36.7% 2,052 16 35.4% 1,362 17 35.4% 1,501 15 36.5% 1,869 17 37.0% 1,991
instrumentation, however, our experiments show that pro-
grams compiled by Angora and AFL++ have far moreinstrumentation errors. We did further research and foundthat besides the instrumentation, they do more modiﬁcationsto the program during the compilation. Angora would splitthe basic block containing condition statements, such as
cmp
andswitch statement, and generate two new branches. The
two branches are the same in program logic and the branchtarget, but one of the branches will collect the informationof the condition statement. This strategy is proposed to cutdown the overhead of the constraint solving process. Similarto Angora, AFL++ employs pc-guard strategy to speed upthe fuzzing, which adds new basic blocks into the programduring the compilation process. These new basic blocks onlycontain a
mov and a jmp instruction, which are used to load
the edge id from the global memory.
As most of the coverage-based greybox fuzzers are based
on AFL, and we have found great quantities of instrumenta-tion errors caused by these state-of-the-art fuzzers, we cansee that instrumentation errors are common in real-worldcoverage-based greybox fuzzers.C. Repair Instrumentation Errors (RQ2)
We have demonstrated that there are vast numbers of MILs
and RILs in programs compiled by fuzzers like AFL andMemlock. To repair the instrumentation errors, we appliedtwo methods proposed in Section III-B to them. Table IIIshows the effect of our repairs. Since RetroWrite now couldnot handle binaries compiled by Angora and AFL++, we onlylisted 2 fuzzers in the table (More discussions in Section V).
As we mentioned in Section III-B1, we could not com-
pletely repair errors with the straightforward solution bycontrolling compiler optimization options. However, withInstruGuard, we almost eliminated all instrumentation errorswith a rate of 99.93% for programs compiled by the compiler-level mode of AFL. For the programs compiled by theassembly-level mode of AFL and Memlock, InstruGuardachieved the similar effect, with a repair rate of over 99.9%.
We manually veriﬁed each unﬁxed instrumentation error
in IDA pro by checking the assembly code of the ﬁxedprograms and did further research on them. We found theyare detection errors instead of unﬁxed errors. Most of theerrors are because of the missing branch label in the code
575loc_1600:  
  Instrumentation  cmp  eax, 20746D66H
  jnz  loc_16F9
loc_16AB:  
  Instrumentation  call  malloc  call  read_content
loc_16F9:    Instrumentation
  retnFalse
True
FalseTrue
True
False
Instrumentation
call read_headercmp  eax, 46464952hjnz  loc_16F9
 952hloc_1626:    cmp  eax, 61746164h  jz   loc_16AB
Figure 5: The simpliﬁed control ﬂow graph of the code around the
memory allocation point in wavgain .
structures like switch . The argument of the jump instruction
in the switch is a register like rax rather than a branch
label. The original programs have jump table information soIDA Pro could identify the basic blocks accurately. However,after the re-compilation process of RetroWrite, the jump tableinformation is lost, which makes IDA pro miss the label inthe destination basic block of the jump instruction.
D. Fuzz with Repaired Instrumentation (RQ3)
To evaluate the effectiveness of our repairs, we ran a
series of fuzzing experiments with the programs generated
by fuzzers’ instrumentation toolchains and the repair method.The results are shown in Table IV, and we use the number ofreal bugs, the line coverage of source code, and the numberof paths as the metrics.
We can ﬁnd that the fuzzing results of the repaired binaries
are better at bugs and paths than the fuzzing results ofthe original ones with instrumentation errors. AFL ﬁnds 1more real bugs in total and covers 0.1% more lines of codeon average for the assembly-level mode. For compiler-levelmode, AFL can trigger almost 3000 more paths, ﬁnd 3 morereal bugs, one of which is not reported before, and triggersimilar coverage on average with the repaired programs.FairFuzz and Memlock are better at all three aspects withour binary rewrite solution, while MOPT seems not to beneﬁtmuch. For speciﬁc programs, although AFL covers the sameamount of code (75.3%) when fuzzing
gif2tga compiled
with afl-clang-fast , it ﬁnds more paths (from 7587 to
40453) with the repaired instrumentation, which results inﬁnding more bugs (from 2 to 5). Memlock cannot ﬁnd anybug in the origin
cjpeg , but it ﬁnds 2 bugs in the repaired
cjpeg .
New Vulnerability. With repaired instrumentation, we for-
tunately found a new vulnerability of memory leak in
wavgain , The original AFL could not ﬁnd this bug in
the program because of its inaccurate instrumentation. Weanalyzed the vulnerability based on the bug report and foundthere were several MIL errors, which are shown in Figure 5.TABLE V: Fuzzing speed and the overview of p-values from
Mann-Whitney U Test. The Instru column is the fuzzing speed
of binaries compiled by afl-clang-fast withO3and ﬁxed by In-
struGuard. AFL(ASM) column shows the results of binaries compiled
byafl-gcc .AFL column shows the results of binaries compiled by
afl-clang-fast .mpg321 can not be compiled with -O0.
ProgramInstru AFL(ASM)-O0 AFL(ASM)-O3 AFL
Means Means P-value Means P-value Means P-value
catdoc 455 404 0.500 760 0.037 539 0.201
cjpeg 136 162 0.265 204 0.105 280 0.147
gif2tga 955 506 0.104 727 0.265 712 0.148
listswf 206 112 0.338 377 0.265 455 0.265
lupng 2995 2162 0.047 1828 0.006 2683 0.500
mp3gain 66 59 0.338 60 0.500 173 0.030
nm 1170 1225 0.072 1463 0.030 1408 0.047
objdump 486 206 0.006 358 0.047 540 0.148
size 1147 1304 0.072 1265 0.265 1231 0.338
strip 301 100 0.047 241 0.500 333 0.265
wav gain 153 158 0.417 112 0.047 131 0.104
mpg321 99 -- 148 0.337 73 0.006
libpng 907 167 0.011 770 0.265 1400 0.500
tiff2pdf 667 389 0.202 377 0.202 655 0.500
tiff2ps 366 98 0.104 472 0.338 960 0.104
AV G 715 504 0.011 644 0.105 822 0.202
Just as the example in the Section II-B, the switch code is
optimized to a series of comparisons and some basic blocksare not instrumented, so they cannot be perceived by AFL.We repaired the instrumentation errors along the vulnerablepaths, giving us the ability to explore the vulnerable pathsand discover the vulnerability. On the contrary, even thoughAFL might trigger the vulnerable path but would abandon theseed based on the wrong feedback caused by instrumentationerrors. The new vulnerability is assigned with CVE-2020-28176 [1], which shows our repair is a beneﬁt to fuzzing.
E. Performance & Overhead
To evaluate the performance of InstruGuard, we compared
the execution speed of programs before and after being re-
paired while fuzzing process, besides compared with severalcompilation optimization levels. We took the execution speedof programs, which are compiled by
afl-clang-fast with
O3and ﬁxed by InstruGuard, to represent the performance of
InstruGuard, since afl-clang-fast is recommended by the
fuzzing community and O3is the default optimization level.
Firstly, we compared the performance of binaries ﬁxed byInstruGuard with binaries compiled by
afl-gcc . It is worth
noting that binaries compiled by afl-gcc withO0andO1are
free of instrumentation errors. As the performance resultsof binaries compiled with
O1and with O3are similar, we
did not show the result of O1 in the table. As Table V
shows, the average fuzzing speed (i.e., execution times eachsecond) of binaries ﬁxed by InstruGuard is 41.9% faster thanbinaries compiled by
afl-gcc with O0, and 11.1% faster
thanO3. Compared to binaries compiled by afl-gcc with
O0, 28.6% (4 of 14) of binaries ﬁxed by InstruGuard are
signiﬁcantly faster (based on the p-values), and 20% (3 of15) of binaries are faster compared to
O3. Although binaries
576(a) aﬂ-gcc. (b) aﬂ-clang-fast.
Figure 6: The distribution of MIL and RIL errors.
compiled by afl-gcc withO0are free from the instrumenta-
tion errors, our method could achieve better performance. For
other optimization levels, binaries ﬁxed by InstruGuard arealso comparable in performance. Combining with Table IV,except InstruGuard,
O1 andO0 might be good choices if
one compiles binaries with afl-gcc and does not care about
the execution speed while using AFL to test them. Theirperformance in ﬁnding bugs is comparable with
O3, and they
cause no instrumentation errors.
We also compared the performance of binaries ﬁxed
by InstruGuard with the unﬁxed binaries compiled by
afl-clang-fast with O3. We found that binaries ﬁxed by
our tool are similar to them, regardless of the mean valuesor signiﬁcant analysis. Surprisingly, the fuzzing speed of theﬁxed
mpg321 is even faster than the unﬁxed one, which might
be due to the shrinking size of the binary after the rewriting.The shrunken parts comes from the relocation table, the
ehframe hdr section, and the ehframe section, which are
discarded by RetroWrite after rewriting.
As for the overhead, the processing time of InstruGuard
is 49 seconds on average for the tested programs, and themaximum time is 241 seconds for
mp3gain . Compared with
our entire fuzzing cycle (72 hours), the average overhead isminimum (0.02%).
F . Case Study
We took a further step based on the extended dataset
to analyze the real-world instrumentation errors and shared
some interesting observations. Figure 6 shows the distributionof the MIL and RIL errors. Since the programs compiled by
afl-gcc withO0orO1are free from instrumentation errors,
we did not present them in the ﬁgure.
1) MIL: Whether the program is compiled by afl-gcc
orafl-clang-fast , MIL accounts for the majority of in-
strumentation errors. After preliminary manual analysis, wefound that MIL occurs mostly in multiple continuous com-parison logic in IR code. To verify this discovery, we wrotean LL VM pass to identify the corresponding logic and foundthat more than 70% of the MILs happen around the multiplecontinuous comparison logic. The pass also matched themultiple continuous comparison logic in IR code with thesource code. The corresponding source code can be roughlyloc_400B70:    Instrumentation
  cmp  eax, 20
  ja   default_case         
False
True
default_case:
  Instrumentation  jmp  loc_400C85
loc_400C85:    Instrumentation
  retncase_0:    Instrumentation  jmp  loc_400C80case_19:    Instrumentation  jmp  loc_400C80case_3/5/18:    Instrumentation  jmp  loc_400C80switch_jump:    jmp ds:off_401178[rax*8]switch_jump:    jmp ds:off_401178[rax*8]
*8]
loc_400C80:    call  puts
s
Figure 7: The simpliﬁed control ﬂow graph of switch code structure
in optimization option O3inafl-clang-fast .
categorized into 5 code patterns: 1switch statement, 2
nested condition statements, such as if-if,3 continuous if-
return statement, 4 loop nested conditional statement, such
asfor-if, and 5 condition statement with multiple logical
operators, such as &&or||.
Figure 7 is an example of objdump which con-
tained two MILs. The switch jump basic block and
the other in loc 400C80 were missed with instrumenta-
tion by afl-clang-fast with O3. Without feedback from
switch jump basic block, AFL will treat two different paths,
i.e., path with basic blocks of (loc 400B70, switch jump,
default case) and the path of (loc 400B70, default case), as
the same and record only one of them. If AFL records pathof (loc
400B70, default case), it will generate fewer paths
in which eax does not exceed 20. Under this situation, AFL
will leave some numerically sensitive crashes undiscovered,and vice versa.
1std::map<std::string,std::string> longs;
2longs[ "--adjust" ]="-a" ;
3longs[ "--binary" ]="-b" ;
4...
5longs[ "--years" ]="-Y"
Listing 2: The code fragment of Params::getopt() .
2) RIL: Most RILs exist in C++ programs for AFL.
Listing 2 displays the function Params::getopt() inexiv2 ,
which has the max number of RILs with the optimizationoption
O1. Each assignment operation results in adding three
more RILs during the compilation, and it has 120 RILsin total. Similar RIL errors happen in
bento4 , which con-
tains 25 instrumentations in one basic block of function
AP4HvccAtom::UpdateRawBytes() . This basic block con-
tains 24 call instructions which call the same function withdifferent arguments, and after each call instruction there is aredundant instrumentation location.
V. T
HREA T TO VALIDITY
Internal Validity. Fuzzing is a random process that may
have an impact on the results of our evaluation. To mitigate
577the effect of randomness, we extended the timeout to 72
hours and repeated our experiments 5 times according to theevaluation suggestions [20]. Besides, the process of binaryrewriting could change the program behavior and introducenew vulnerabilities. In order to eliminate these possibleeffects, we double-checked each bug with original programscompiled with Address Sanitizer [33] while fuzzing, as wellas gathered the line coverage of source code with the originalprograms.
External Validity. Due to the limitation of the RetroWrite,
the binary rewriting tool we use, InstruGuard now could onlyﬁnd instrumentation errors for C++ programs and binariescompiled by Angora and AFL++, but could not ﬁx them. It ismainly because that RetroWrite has trouble handling binariesthat contain C++ exceptions, fails to disassemble binariescompiled by Angora, and generates non-compilable assemblycode for binaries compiled by AFL++. However, since ourrepair method has been proven effective, once RetroWriteis updated or more powerful binary rewrite tools come out,InstruGuard will be able to ﬁx instrumentation errors for allfuzzers.
In this paper, we also did not ﬁx the program instrumented
by fuzzers that use selective instrumentation. We selected 6fuzzers as the research targets since they are representativeand differ in instrumentation methods. They all try to exploreas many paths as possible. However, other types of fuzzersalso exist. For example, some fuzzers are designed forspeciﬁc types of vulnerability, they could only instrumentthe “interesting” basic blocks or paths without triggering un-related paths. For these fuzzers, we could extend InstruGuardwith additional conﬁgurations to detect instrumentation errorsalong a speciﬁc path to ensure they act as expected.
VI. R
ELA TED WORK
The most related researches and techniques are presented
in the following two parts, including greybox fuzzing andinstrumentation.
Greybox fuzzing. Researchers improve fuzzing from var-
ious aspects. Some of them are applied after the instrumen-tation process. Vuzzer [32], Skyﬁre [19], Neuzz [36], andFaster Fuzzing [30] learn the important bytes or the grammarsof the input ﬁles for more effective mutations. MOPT [27]optimizes the seed mutation scheduling strategies with theParticle Swarm Optimization (PSO) algorithm. AFLFast [5]allocates more energy to test the low-frequency paths tooptimize the path exploration. Driller [38], QSYM [51] andT-fuzz [31] integrate static and dynamic analysis to prioritizehard-to-reach deeper paths.
Others modify the instrumented code to record more
program behaviors or improve the sensitivity of the feedback.To guide the testing towards speciﬁc locations, AFLGo[6] modiﬁes the instrumentation to calculate the distancebetween the current path and the target location. Memlock[47] and UAFL [42] collect the memory behavior of theprograms to ﬁnd more memory-related bugs. Angora [10]uses call stack while AFL-sensitive [43] uses n-gram toidentify the different paths more speciﬁcally.
Instrumentation. Instrumentation approaches can be di-
vided to binary instrumentation and source instrumentation.Usually binary instrumentation is applied when the sourcecode of the tested program is unavailable, and could slowdown the program signiﬁcantly. Some fuzzers [31, 32, 38,45] obtain feedback with dynamic binary instrumentationtools such as QEMU [4], DynamoRIO [7], and hardware-accelerated Intel Processor Trace. Fuzzers like AFL-Dyninst[40] use static instrumentation tools [8] to obtain feedback.Speciﬁcally, they use code patching techniques to injectcallback events to gather coverage or other information.
AFL-family fuzzers [34, 39, 52] mainly get feedback
through source instrumentation, which inserts a piece ofspeciﬁc code to each basic block while the target programis compiled with GCC or LL VM. Only a little research isconducted about the instrumentation problem. AFL-cc [37]leverages controlled optimization to minimize the differ-ence (such as basic block layout) between the LL VM IRcode and the binary, and tries to get accurate feedbackfor fuzzers. UNIFUZZ [26] notices that the instrumentationmethod might affect the program behavior, such as whetherthe certain bug could be triggered, and thus the fuzzingevaluation. However, they do not systemically analyze theimpact of instrumentation errors and ﬁx the problem.
VII. C
ONCLUSION
In this paper, we point out two types of instrumentation
errors in coverage-based greybox fuzzers, and propose aframework named InstruGuard to ﬁnd and ﬁx instrumentationerrors. We assessed the impacts of instrumentation errors ongreybox fuzzers with a dataset of 15 real-world programs,and evaluated the effectiveness of our repairs through fuzzingfrom the aspect of the number of paths, line coverage ofsource code, and the number of real bugs. The resultsshowed that instrumentation errors are common for mostcompilation optimization levels and coverage-based greyboxfuzzers, and impact the fuzzing results. Our method ﬁxedinstrumentation errors effectively and beneﬁted coverage-based greybox fuzzers.
A
CKNOWLEDGEMENT
We thank the anonymous reviewers of this work for
their helpful feedback. This research is supported, in part,by National Natural Science Foundation of China (GrantNo. U1936211, U1836117, U1836113, and 61902384), theStrategic Priority Research Program of the Chinese Academyof Sciences, Grant No. XDC02020300. All opinions ex-pressed in this paper are solely those of the authors.
R
EFERENCES
[1] “CVE-2020-28176,” https://cve.mitre.org/cgi-bin/cvename.cgi?name
=2020-28176.
[2] “IDA Pro,” https://www.hex-rays.com/products/ida/.
[3] “IDApython,” https://www.hex-rays.com/products/ida/support/idapyt
hon docs/.
578[4] F. Bellard, “Qemu, a fast and portable dynamic translator,” in Pro-
ceedings of the Annual Conference on USENIX Annual Technical
Conference, 2005, pp. 41–41.
[5] M. B ¨ohme, V . Pham, and A. Roychoudhury, “Coverage-based greybox
fuzzing as markov chain,” IEEE Transactions on Software Engineering,
vol. 45, no. 5, pp. 489–506, 2016.
[6] M. B ¨ohme, V .-T. Pham, M.-D. Nguyen, and A. Roychoudhury, “Di-
rected greybox fuzzing,” in Proceedings of the 2017 ACM SIGSAC
Conference on Computer and Communications Security, 2017, pp.2329–2344.
[7] D. Bruening, E. Duesterwald, and S. Amarasinghe, “Design and
implementation of a dynamic optimization framework for windows,” in4th ACM Workshop on Feedback-Directed and Dynamic Optimization(FDDO-4), 2000.
[8] B. Buck and J. K. Hollingsworth, “An api for runtime code patching,”
Int. J. High Perform. Comput. Appl., vol. 14, no. 4, pp. 317–329, 2000.
[9] H. Chen, Y . Xue, Y . Li, B. Chen, X. Xie, X. Wu, and Y . Liu, “Hawkeye:
Towards a desired directed grey-box fuzzer,” in ACM Conference on
Computer and Communications Security, 2018.
[10] P . Chen and H. Chen, “Angora: efﬁcient fuzzing by principled search,”
inProceedings of the 2018 IEEE Symposium on Security and Privacy.
IEEE Computer Society, 2018.
[11] Y . Chen, P . Li, J. Xu, S. Guo, R. Zhou, Y . Zhang, T. Wei, and L. Lu,
“Savior: Towards bug-driven hybrid testing,” in 2020 IEEE Symposium
on Security and Privacy (SP). IEEE, 2020, pp. 1580–1596.
[12] Y . Chen, D. Mu, J. Xu, Z. Sun, W. Shen, X. Xing, L. Lu, and
B. Mao, “Ptrix: Efﬁcient hardware-assisted fuzzing for cots binary,”inProceedings of the 2019 ACM Asia Conference on Computer and
Communications Security, ser. Asia CCS ’19, 2019.
[13] Y . Chen, Y . Jiang, F. Ma, J. Liang, M. Wang, C. Zhou, X. Jiao,
and Z. Su, “Enfuzz: Ensemble fuzzing with seed synchronizationamong diverse fuzzers,” in Proceedings of the 28th USENIX Security
Symposium, 2019.
[14] M. Cho, S. Kim, and T. Kwon, “Intriguer: Field-level constraint
solving for hybrid fuzzing,” in Proceedings of the 2019 ACM SIGSAC
Conference on Computer and Communications Security, 2019, pp.515–530.
[15] S. Dinesh, N. Burow, D. Xu, and M. Payer, “Retrowrite: Statically
instrumenting cots binaries for fuzzing and sanitization,” in 2020 IEEE
Symposium on Security and Privacy (SP). IEEE, 2020, pp. 1497–1511.
[16] A. Fioraldi, D. Maier, H. Eißfeldt, and M. Heuse, “AFL++: Combining
incremental steps of fuzzing research,” in 14th USENIX Workshop on
Offensive Technologies (WOOT 20). USENIX Association, Aug. 2020.
[17] S. Gan, C. Zhang, P . Chen, B. Zhao, X. Qin, D. Wu, and Z. Chen,
“GREYONE: Data ﬂow sensitive fuzzing,” in 29th USENIX Security
Symposium, 2020.
[18] S. Gan, C. Zhang, X. Qin, X. Tu, K. Li, Z. Pei, and Z. Chen, “Collaﬂ:
Path sensitive fuzzing,” in Proceedings of the 2018 IEEE Symposium
on Security and Privacy. IEEE, 2018.
[19] W. Junjie, C. Bihuan, W. Lei, and L. Y ang, “Skyﬁre: Data-driven seed
generation for fuzzing,” in 2017 IEEE Symposium on Security and
Privacy, 2017, pp. 579–594.
[20] G. Klees, A. Ruef, B. Cooper, S. Wei, and M. Hicks, “Evaluating
fuzz testing,” in Proceedings of the 2018 ACM SIGSAC Conference
on Computer and Communications Security, 2018.
[21] laf-intel, “Circumventing fuzzing roadblocks with compiler transfor-
mations,” https://laﬁntel.wordpress.com/2016/08/15/circumventing-fuzzing-roadblocks-with-compiler-transformations/, [2019-1-27].
[22] C. Lattner and V . Adve, “LL VM: A compilation framework for lifelong
program analysis & transformation,” in Proceedings of the interna-
tional symposium on Code generation and optimization: feedback-directed and runtime optimization. IEEE Computer Society, 2004.
[23] C. Lemieux, R. Padhye, K. Sen, and D. Song, “PerfFuzz: Automati-
cally generating pathological inputs,” in Proceedings of the 27th ACM
SIGSOFT International Symposium on Software Testing and Analysis,2018.
[24] C. Lemieux and K. Sen, “FairFuzz: a targeted mutation strategyfor increasing greybox fuzz testing coverage,” in Proceedings of the
33rd ACM/IEEE International Conference on Automated SoftwareEngineering, 2018.
[25] Y . Li, B. Chen, M. Chandramohan, S.-W. Lin, Y . Liu, and A. Tiu,
“Steelix: program-state based binary fuzzing,” in Proceedings of the
2017 11th Joint Meeting on F oundations of Software Engineering.ACM, 2017.[26] Y . Li, S. Ji, Y . Chen, S. Liang, W.-H. Lee, Y . Chen, C. Lyu, C. Wu,
R. Beyah, P . Cheng et al., “UNIFUZZ: A holistic and pragmatic
metrics-driven platform for evaluating fuzzers,” in 30th USENIX Se-
curity Symposium (USENIX Security 21), 2021.
[27] C. Lyu, S. Ji, C. Zhang, Y . Li, W.-H. Lee, Y . Song, and R. Beyah,
“MOPT: Optimized Mutation Scheduling for Fuzzers,” in Proceedings
of the 28th USENIX Security Symposium, 2019.
[28] B. P . Miller, L. Fredriksen, and B. So, “An empirical study of the
reliability of unix utilities,” Commun. ACM, 1990.
[29] S. Nagy and M. Hicks, “Full-speed fuzzing: Reducing fuzzing over-
head through coverage-guided tracing,” in 2019 IEEE Symposium on
Security and Privacy (SP). IEEE, 2019, pp. 787–802.
[30] N. Nichols, M. Raugas, R. Jasper, and N. Hilliard, “Faster fuzzing:
Reinitialization with deep neural models,” CoRR, vol. abs/1711.02807,
2017.
[31] H. Peng, Y . Shoshitaishvili, and M. Payer, “T-fuzz: Fuzzing by program
transformation,” in 2018 IEEE Symposium on Security and Privacy,
2018, pp. 697–710.
[32] S. Rawat, V . Jain, A. Kumar, L. Cojocar, C. Giuffrida, and H. Bos,
“VUzzer: Application-aware evolutionary fuzzing,” in Proceedings of
the 24th Network and Distributed System Security Symposium, 2017.
[33] K. Serebryany, D. Bruening, A. Potapenko, and D. Vyukov, “Ad-
dresssanitizer: a fast address sanity checker,” in Usenix Conference
on Technical Conference, 2012.
[34] K. Serebryany, “Continuous fuzzing with libfuzzer and addresssani-
tizer,” IEEE, 2016.
[35] ——, “Oss-fuzz - google’s continuous fuzzing service for open source
software,” 2017.
[36] D. She, K. Pei, D. Epstein, J. Y ang, B. Ray, and S. Jana, “Neuzz:
Efﬁcient fuzzing with neural program smoothing,” in 2019 IEEE
Symposium on Security and Privacy (SP). IEEE, 2019.
[37] L. Simon and A. V erma, “Improving Fuzzing through Controlled
Compilation,” in 5th IEEE European Symposium on Security and
Privacy (EuroS&P), 2020.
[38] N. Stephens, J. Grosen, C. Salls, A. Dutcher, R. Wang, J. Corbetta,
Y . Shoshitaishvili, C. Kruegel, and G. Vigna, “Driller: AugmentingFuzzing Through Selective Symbolic Execution,” in Proceedings of
the 23rd Network and Distributed Systems Security Symposium, 2016.
[39] R. swiecki, “Honggfuzz,” https://github.com/google/honggfuzz, 2016.[40] talos vulndev, “AFL Dyninst,” https://github.com/talos-vulndev/aﬂ-dy
ninst, 2018.
[41] D. V eillard, “Libxml2: The XML C parser and toolkit of Gnome,”
http://www.xmlsoft.org, 2012.
[42] H. Wang, X. Xie, Y . Li, C. Wen, Y . Li, Y . Liu, S. Qin, H. Chen,
and Y . Sui, “Typestate-guided fuzzer for discovering use-after-freevulnerabilities,” in 2020 IEEE/ACM 42nd International Conference on
Software Engineering (ICSE), 2020, pp. 999–1010.
[43] J. Wang, Y . Duan, W. Song, H. Yin, and C. Song, “Be sensitive
and collaborative: Analyzing impact of coverage metrics in greyboxfuzzing,” in 22nd International Symposium on Research in Attacks,
Intrusions and Defenses, 2019.
[44] J. Wang, B. Chen, L. Wei, and Y . Liu, “Superion: Grammar-aware
greybox fuzzing,” in 2019 IEEE/ACM 41st International Conference
on Software Engineering. IEEE, 2019, pp. 724–735.
[45] Y . Wang, C. Zhang, X. Xiang, Z. Zhao, W. Li, X. Gong, B. Liu,
K. Chen, and W. Zou, “Revery: From proof-of-concept to exploitable,”inProceedings of the 2018 ACM SIGSAC Conference on Computer and
Communications Security, 2018.
[46] Y . Wang, X. Jia, Y . Liu, K. Zeng, T. Bao, D. Wu, and P . Su, “Not all
coverage measurements are equal: Fuzzing by coverage accounting forinput prioritization,” in NDSS, 2020.
[47] C. Wen, H. Wang, Y . Li, S. Qin, Y . Liu, Z. Xu, H. Chen, X. Xie,
G. Pu, and T. Liu, “Memlock: Memory usage guided fuzzing,” inProceedings of the ACM/IEEE 42nd International Conference onSoftware Engineering. ICSE, 2020.
[48] M. Xu, S. Kashyap, H. Zhao, and T. Kim, “Krace: Data race fuzzing
for kernel ﬁle systems,” in 2020 IEEE Symposium on Security and
Privacy (SP). IEEE, 2020, pp. 1643–1660.
[49] S. Y an, C. Wu, H. Li, W. Shao, and C. Jia, “Pathaﬂ: Path-coverage
assisted fuzzing,” Proceedings of the 15th ACM Asia Conference on
Computer and Communications Security, 2020.
[50] W. Y ou, X. Wang, S. Ma, J. Huang, X. Zhang, X. Wang, and
B. Liang, “Profuzzer: On-the-ﬂy input type probing for better zero-day vulnerability discovery,” in 2019 IEEE Symposium on Security
579and Privacy. IEEE, 2019, pp. 769–786.
[51] I. Y un, S. Lee, M. Xu, Y . Jang, and T. Kim, “QSYM: A Practical
Concolic Execution Engine Tailored for Hybrid Fuzzing,” in Proceed-
ings of the 27th USENIX Security Symposium. USENIX Association,
2018.
[52] M. Zalewski, “American fuzzy lop (AFL) fuzzer,” http://lcamtuf.core
dump.cx/afl, 2013.[53] L. Zhao, Y . Duan, H. Yin, and J. Xuan, “Send hardest problems my
way: Probabilistic path prioritization for hybrid fuzzing,” in NDSS,
2019.
[54] Y . Zheng, A. Davanian, H. Yin, C. Song, H. Zhu, and L. Sun, “Firm-
aﬂ: High-throughput greybox fuzzing of iot ﬁrmware via augmentedprocess emulation,” in 28th USENIX Security Symposium, 2019, pp.
1099–1114.
580