Automatic Extraction of Opinion-based Q&A from
Online Developer Chats
Preetha Chatterjee
University of Delaware
Newark, DE, USA
preethac@udel.eduKostadin Damevski
Virginia Commonwealth University
Richmond, V A, USA
kdamevski@vcu.eduLori Pollock
University of Delaware
Newark, DE, USA
pollock@udel.edu
Abstract ‚ÄîVirtual conversational assistants designed specif-
ically for software engineers could have a huge impact on
the time it takes for software engineers to get help. Research
efforts are focusing on virtual assistants that support speciÔ¨Åc
software development tasks such as bug repair and pair pro-
gramming. In this paper, we study the use of online chat
platforms as a resource towards collecting developer opinions
that could potentially help in building opinion Q&A systems,
as a specialized instance of virtual assistants and chatbots for
software engineers. Opinion Q&A has a stronger presence in
chats than in other developer communications, thus mining them
can provide a valuable resource for developers in quickly getting
insight about a speciÔ¨Åc development topic (e.g., What is the best
Java library for parsing JSON? ). We address the problem of
opinion Q&A extraction by developing automatic identiÔ¨Åcation of
opinion-asking questions and extraction of participants‚Äô answers
from public online developer chats. We evaluate our automatic
approaches on chats spanning six programming communities
and two platforms. Our results show that a heuristic approach
to opinion-asking questions works well (.87 precision), and a
deep learning approach customized to the software domain
outperforms heuristics-based, machine-learning-based and deep
learning for answer extraction in community question answering.
Index Terms ‚Äîopinion question-answering system, public
chats, opinion-asking question, answer extraction
I. I NTRODUCTION
Recognizing the increasing capabilities of virtual assis-
tants that use conversational artiÔ¨Åcial intelligence (AI) (e.g.,
chatbots, voice assistants), some researchers in software en-
gineering are working towards the development of virtual
assistants to help programmers. They have conducted studies
to gain insights into the design of a programmer conversational
agent [1], proposed techniques to automatically detect speech
acts in conversations about bug repair to aid the assistant
in mimicking different conversation types [2], and designed
virtual assistants for API usage [3].
While early versions of conversational assistants were fo-
cused on short, task-oriented dialogs (e.g., playing music or
asking for facts), more sophisticated virtual assistants deliver
coherent and engaging interactions by understanding dialog
nuances such as user intent (e.g., asking for opinion vs
knowledge) [4]. They integrate specialized instances dedicated
to a single task, including dialog management, knowledge
retrieval, opinion-mining, and question-answering [5]. To build
virtual assistants for software engineers, we need to providesimilar specialized instances based on the available infor-
mation from software engineers‚Äô daily conversations. Recent
studies indicate that online chat services such as IRC, Slack,
and Gitter are increasingly popular platforms for software
engineering conversations, including both factual and opinion
information sharing and now playing a signiÔ¨Åcant role in
software development activities [6]‚Äì[9]. These conversations
potentially provide rich data for building virtual assistants
for software engineers, but little research has explored this
potential.
In this paper, we leverage the availability of opinion-
asking questions in developer chat platforms to explore the
feasibility of building opinion-providing virtual assistants for
software engineers. Opinion question answering (Opinion QA)
systems [10]‚Äì[13] aim to Ô¨Ånd answers to subjective questions
from user-generated content, such as online forums, product
reviews, and discussion groups. One type of virtual assistant
that can beneÔ¨Åt from opinions are Conversational Search
Assistants (CSAs) [24]. CSAs support information seekers
who struggle forming good queries for exploratory search, e.g.,
seeking opinions/recommendations on API, tools, or resources,
by eliciting the actual need from the user through conver-
sation. Studies indicate developers conducting web searches
or querying Q&A sites for relevant questions often Ô¨Ånd it
difÔ¨Åcult to formulate good queries [25], [26]. Wizard of Oz
studies have explicitly shown the need for opinions within
CSAs [27]. A key result of our paper is the availability of
opinions on chat platforms, which would enable the creation
of a sizable opinion Q&A corpus that could actually be used
by CSAs. The opinion Q&A corpus generated from chats
by our technique can be used in a few different ways to
build a CSA: 1) matching queries/questions asked to the CSA
with questions from the corpus and retrieving the answers; 2)
summarizing related groups of opinion Q&A to generate (e.g.,
using a GAN) an aggregate response for a speciÔ¨Åc software
engineering topic.
Opinion extraction efforts in software engineering have
focused on API-related opinions and developer emotions from
Q&A forums [14]‚Äì[18], developer sentiments from commit
logs [19], developer intentions from emails and issue reports
[20], [21] and detecting software requirements and feature
requests from app reviews [22], [23]. These studies suggest
that, beyond reducing developers‚Äô effort of manual searches
12602021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)
1558-1225/21/$31.00 ¬©2021 IEEE
DOI 10.1109/ICSE43902.2021.00115
on the web and facilitating information gathering, mining
of opinions could help in increasing developer productivity,
improving code efÔ¨Åciency [16], and building better recom-
mendation systems [17].
Findings from our previous exploratory study [9] of Slack
conversations suggests that developer chats include opinion
expression during human conversations. Our current study
(in Section II) of 400 developer chat conversations selected
from six programming communities showed that 81 (20%)
of the chat conversations start with a question that asks for
opinions (e.g., ‚ÄúWhich one is the best ORM that is efÔ¨Åcient
for large datasets?‚Äù, ‚ÄúWhat do you think about the Onyx
platform?‚Äù). This Ô¨Ånding shows much higher prevalence of
questions asking for opinions in chats than the 1.6% found in
emails [20] and 1.1% found in issue reports [21].
Thus, we investigate the problem of opinion Q&A extraction
from public developer chats in this paper. We decompose the
problem of opinion Q&A extraction into two subproblems: (1)
identifying questions where the questioner asks for opinions
from other chat participants (which we call posing an opinion-
asking question), and (2) extracting answers to those opinion-
asking questions within the containing conversation.
Researchers extracting opinions from software-related doc-
uments have focused on identifying sentences containing
opinions, using lexical patterns [20] and sentiment analysis
techniques [16], [17], [28]. However, these techniques are not
directly applicable to identifying opinion-asking questions in
chats for several reasons. Chat communities differ in format,
with no formal structure and informal conversation style. The
natural language text in chats could follow different syntactic
patterns and contain incomplete sentences [9], which could
potentially inhibit automatic mining of opinions.
Outside the software engineering domain, researchers have
addressed the problem of answer extraction from community
question-answering (CQA) forums by using deep neural net-
work models [29]‚Äì[31], and syntactic tree structures [32], [33].
Compared to CQA forums, chats contain rapid exchanges of
messages between two or more developers in short bursts
[9]. A question asked at the start of a conversation may be
followed by a series of clariÔ¨Åcation or follow-up questions
and their answers, before the answers to the original question
are given. Moreover, along with the answers, conversations
sometimes contain noisy and unrelated information. Therefore,
to determine the semantic relation between a question and
answer, understanding the context of discussion is crucial.
We are the Ô¨Årst to extract opinion Q&A from developer
chats, which could be used to support SE virtual assistants as
well as chatbots, programmer API recommendation, automatic
FAQ generation, and in understanding developer behavior and
collaboration. Our automatic opinion Q&A extraction takes a
chat conversation as input and automatically identiÔ¨Åes whether
the conversation starts with an opinion-asking question, and if
so, extracts one or more opinion answers from the conversa-
tion. The major contributions of this paper are:
‚Ä¢For opinion-asking question identiÔ¨Åcation, we designed
a set of heuristics, learned from the results from ourpreliminary chat analysis, to determine if the leading
question in a chat conversation asks for opinions.
‚Ä¢For automatic answer extraction, we built upon related
work on non-SE artifacts to create a deep learning
approach customized to the SE domain. We compare
against heuristics, machine learning combining features,
and a deep learning technique based on the context of
the discussion in community question answering. This
answer extraction model could potentially be leveraged
to extract answers from other types of questions in chats.
‚Ä¢We evaluated our techniques on developer conversations
from six different programming communities on two
different platforms, Slack and IRC. Our evaluation results
show that we can automatically identify opinion-asking
questions and extract their corresponding answers within
a chat conversation with a precision of 0.87 and 0.77,
respectively.
‚Ä¢We publish the dataset and source code1to facilitate
the replication of our study and its application in other
contexts.
II. O PINION -ASKING QUESTIONS IN DEVELOPER ONLINE
COMMUNICATIONS
Since developer chats constitute a subclass of developer
online communications, we began by investigating whether
we could gain insights from work by others on analyzing the
opinion-asking questions in other kinds of developer online
discussions (emails, issue reports, Q&A forums).
Emails. The most closely related work, by Di Sorbo et
al. [20], proposed an approach to classify email sentences
according to developers‚Äô intentions (feature request, opin-
ion asking, problem discovery, solution proposal, information
seeking and information giving). Their taxonomy of intentions
and associated linguistic patterns have also been applied to
analyze user feedback in app reviews [34], [35].
In their taxonomy, Di Sorbo et al. deÔ¨Åne ‚Äúopinion asking‚Äù
as:requiring someone to explicitly express his/her point of
view about something (e.g., What do you think about creating a
single webpage for all the services? . They claim that sentences
belonging to ‚Äúopinion asking‚Äù may emphasize discussion
elements useful for developers‚Äô activities; and thus, make it
reasonable to distinguish them from more general information
requests such as ‚Äúinformation seeking‚Äù. Of their manually
labelled 1077 sentences from mailing lists of Qt and Ubuntu,
only 17 sentences (1.6%) were classiÔ¨Åed as ‚Äúopinion asking‚Äù,
suggesting that opinion-asking questions are infrequent in
developer emails.
Issue Reports. To investigate the comprehensiveness and
generalizability of Di Sorbo et al.‚Äôs taxonomy, Huang et al.
[21] manually labelled 1000 sentences from issue reports of
four projects (TensorFlow, Docker, Bootstrap, VS Code) in
GitHub. Consistent with Di Sorbo et al.‚Äôs Ô¨Åndings, Huang et
al. [21] reported that only 11 (1.1%) sentences were classiÔ¨Åed
as ‚Äúopinion asking‚Äù. Given this low percentage and that
1Replication Package: https://tinyurl.com/y3qth6s3
1261Table I: Example of Opinion-asking Question And Answers
on Slack #python-dev channel
Ques: hello everyone, I‚Äôve requirement where dataset is large like 10
millions records, i want to use django rest framework in order to provide
that data. Question: which one is the best ORM which is efÔ¨Åcient for
large datasets? 1. Django ORM 2. SQL alchemy
Ans 1: SQLalchemy is more performant especially if you‚Äôre using Core.
Ans 2: yea, you can mix sqlalchemy and django orm. it‚Äôs all just python
at the end of the day. however, if you swap out one for the other you
lose all the beneÔ¨Åts of the django orm and how it integrates with the
framework.
Ans 3: If performance is a factor than use SQLAlchemy core to work on
this large data set If deadlines are more of a factor that use Django ORM
since you‚Äôre already use DRF. Just make sure to use eager loading on
relationships where you can to optimize queries.
‚Äúopinion asking‚Äù could be a sub-category of ‚Äúinformation
seeking‚Äù, they merged these two categories in their study. To
broaden their search of opinions, Huang et al. introduced a
new category ‚Äúaspect evaluation‚Äù, deÔ¨Åned as: express opinions
or evaluations on a speciÔ¨Åc aspect (e.g., ‚ÄúI think BS3‚Äôs new
theme looks good, it‚Äôs a little Ô¨Çat style. ‚Äù, ‚ÄúBut I think it‚Äôs
cleaner than my old test, and I prefer a non-JS solution
personally.?) .‚Äù They classiÔ¨Åed 14-20% sentences as ‚Äúaspect
evaluation‚Äù. Comparing the two deÔ¨Ånitions and their results,
it is evident that although opinions are expressed widely in
issue reports, questions asking for others‚Äô opinions are rare.
Chats. Chatterjee et al.‚Äôs [9] results showing potentially more
opinions in Slack developer chats motivated us to perform
a manual study to systematically analyze the occurrence of
opinion-asking questions and their answers in developer chats.
Dataset: To create a representative analysis dataset, we iden-
tiÔ¨Åed chat groups that primarily discuss software development
topics and have a substantial number of participants. We se-
lected three programming communities with an active presence
on Slack. Within those selected communities, we focused on
public channels that follow a Q&A format, i.e., a conversation
typically starts with a question and is followed by a discussion
potentially containing multiple answers or no answers. Our
analysis dataset of 400 Slack developer conversations consists
of 100 conversations from Slack Pythondev#help channel, 100
from clojurians#clojure, 100 from elmlang#beginners, and 100
from elmlang#general, all chosen randomly from the dataset
released by Chatterjee et al. [36].
Procedure: Using the deÔ¨Ånition of opinion-asking sentences
proposed by Di Sorbo et al. [20], two annotators (authors
of this paper) independently identiÔ¨Åed conversations starting
with an opinion-asking question. We also investigated if those
questions were answered by others in a conversation. The
authors annotated a shared set of 400 conversations, which
indicates that the sample size is sufÔ¨Åcient to compute the
agreement measure with high conÔ¨Ådence [37]. We computed
Cohen‚Äôs Kappa inter-rater agreement between the 2 annotators,
and found an agreement of 0.74, which is considered to be
sufÔ¨Åcient ( >0.6) [38]. The annotators further discussed their
annotations iteratively until all disagreements were resolved.
Observations: We observed that out of our 400 developer
conversations, 81 conversations (20%) start with an opinion-asking question. There are a total of 134 answers to those
81 opinion-asking questions, since each conversation could
contain no or multiple answers.
Table I shows an opinion-asking question ( Ques ) and its
answers ( Ans) extracted from a conversation on #python-dev
channel. Each of the answers contain sufÔ¨Åcient information
as a standalone response, and thus could be paired with the
question to form separate Q&A pairs. Given that conversations
are composed of a sequence of utterances by each of the people
participating in the conversation in a back and forth manner,
the Q&A pairs are pairs of utterances.
Summary: Compared to other developer communications,
conversations starting with opinion-asking questions in de-
veloper chats are much more frequent. Thus, chats may
serve as a better resource to mine for opinion Q&A systems.
III. A UTOMATICALLY EXTRACTING OPINION Q&A FROM
DEVELOPER CHATS
Figure 1 describes the overview of our approach, ChatEO, to
automatically Extract Opinion Q&A from software developer
Chat s. Our approach takes a developer chat history as input
and extracts opinion Q&A pairs using the three major steps:
(1) Individual conversations are extracted from the interleaved
chat history using conversation disentanglement. (2) Conver-
sations starting with an opinion-asking question are identiÔ¨Åed
by applying textual heuristics. (3) Possibly multiple available
answers to the opinion-asking question within the conversation
are identiÔ¨Åed using a deep learning-based approach.
A. Conversation Disentanglement
Since utterances in chats form a stream, conversations often
interleave such that a single conversation thread is entangled
with other conversations. Hence, to ease individual conver-
sation analysis, we separate, or disentangle, the conversation
threads in each chat log. The disentanglement problem has
been widely addressed by researchers in the context of IRC
and similar chat platforms [9], [36], [39]‚Äì[41]. We used the
best available disentanglement approaches proposed for Slack
and IRC chat logs, respectively, in this paper.
‚Ä¢Slack chat logs: We used a subset of the publicly available
disentangled Slack chat dataset2released by Chatterjee et
al. [36] since their modiÔ¨Åed disentanglement algorithm
customized for Slack developer chats achieved a micro-
averaged F-measure of 0.80.
‚Ä¢IRC chat logs: We used Kummerfeld et al.‚Äôs [41] tech-
nique, a feed-forward neural network model for conversation
disentanglement, trained on 77K manually annotated IRC
utterances, and achieving 74.9% precision and 79.7% recall.
In the disentangled conversations, each utterance contains a
unique conversation id and metadata including timestamp and
author information.
2https://www.zenodo.org/record/3627124
1262     Conversation
     Answer Extraction from a Conver sation
Chat Logs Disentanglement
Heuristics-based 
Identification of Opinion 
Asking QuestionsExtracted 
Opinion Asking 
Q&A Questi on
UtterancesBiLSTM Q&A 
pairsSE-customized 
Word Embeddi ngs
CNN
 Answ er 
PredictionDeveloper         
Chat Logs
Conver sations
Conver sations 
with Opinion 
Asking Questi ons
Figure 1: Overview of ChatEO Automatic Extraction of Opinion Q&A from Developer Chats
B. Heuristics-based IdentiÔ¨Åcation of Opinion-asking Question
Di Sorbo et al. [20] claim that developers tend to use recur-
rent linguistic patterns within discussions about development
issues. Thus, using natural language parsing, they deÔ¨Åned
Ô¨Åve linguistic patterns to identify opinion-asking questions in
developer emails. First, to investigate the generalizability of
Di Sorbo et al.‚Äôs linguistic patterns, we used their replication
package of DECA3to identify opinion-asking questions in
developer chats. We found that, out of 400 conversations in
our manual analysis dataset, only 2 questions were identiÔ¨Åed
as opinion-asking questions by DECA. Hence, we conducted
a deeper analysis of opinion-asking questions in our manual
dataset in Section II, to identify additional linguistic patterns
that represent opinion-asking questions in developer chats.
We followed a qualitative content analysis procedure [42],
where the same two annotators (authors of this paper) Ô¨Årst
independently analyzed 100 developer conversations to un-
derstand the structure and characteristics of opinion-asking
questions in chats. The utterances of the Ô¨Årst speaker in each
conversation were manually analyzed to categorize if they
were asking for an opinion. When an opinion-asking question
was manually identiÔ¨Åed, the annotator identiÔ¨Åed the parts of
the utterance that contributed to that decision and identiÔ¨Åed
part-of-speech tags and recurrent keywords towards potential
linguistic patterns. Consider the question in Table I. First, the
annotator selects the text that represents an opinion-asking
question, in this case: ‚Äúwhich one is the best ORM which
is efÔ¨Åcient for large datasets?‚Äù. ‚ÄòORM‚Äù is noted as a noun
referring to the library, and ‚Äúbest‚Äù as an adjective related to
the opinion about ORM. Thus, a proposed linguistic pattern
to consider is: ‚Äú Which one <verb tobe>[positive adjective ]
[rectarget noun ]<verb phrase >?‚Äù.
Throughout the annotation process, the annotators wrote
memos to facilitate the analysis, recording observations on
types of opinions asked, observed linguistic patterns of
opinion-asking questions, and researcher reÔ¨Çections. The two
annotators then met and discussed their observations on com-
mon types of questions asking for opinions, which resulted
in a set of preliminary patterns for identifying opinion-asking
3https://www.iÔ¨Å.uzh.ch/en/seal/people/panichella/tools/DECA.htmlTable II: Most Common Pattern for opinion-asking Question
IdentiÔ¨Åcation in Chats.
Pattern code: PANY ADJ
Description: question starting with ‚Äùany‚Äù and followed by a positive
adjective and target noun.
Rule: Any [rec positive adj] [rec target noun] verb phrase
DeÔ¨Ånitions:
[rec positive adj] = ‚Äùgood‚Äù,‚Äùbetter‚Äù,‚Äùbest‚Äù,‚Äùright‚Äù,‚Äùoptimal‚Äù,...
[rec target noun] = ‚Äùproject‚Äù,‚Äùapi‚Äù,‚Äùtutorial‚Äù,‚Äùlibrary‚Äù,...
Example: Any good examples or things I need to be looking at?
questions in developer chats. Using the preliminary patterns,
the two authors then independently coded the rest of the
opinion-asking questions in our manual analysis dataset, after
which they met to further analyze their annotations and discuss
disagreements. Thus, the analysis was performed in an iterative
approach comprised of multiple sessions, which helped in
generalizing the hypotheses and revising the linguistic patterns
of opinion-asking questions.
Our manual analysis Ô¨Åndings from 400 Slack conversations
showed that an opinion-asking question in a developer chat is a
question occurring primarily at the beginning of a conversation
and could exhibit any of these characteristics:
‚Ä¢Expects subjective answers (i.e., opinions) about APIs,
libraries, examples, resources, e.g., ‚ÄúIs this a bad
style?‚Äù,‚ÄúWhat do you think?‚Äù
‚Ä¢Asks for which path to take among several paths, e.g.,
‚ÄúShould I use X instead of Y?‚Äù
‚Ä¢Asks for an alternative solution (other than questioner‚Äôs
current solution), e.g., ‚ÄúIs there a better way?‚Äù
Thus, we extended Di Sorbo et al.‚Äôs linguistic pattern
set for identifying opinion-asking questions, by adding 10
additional linguistic patterns. Table II shows the most common
pattern, P ANY ADJ, in our dataset with its description and
example question. Most of the patterns utilize a combination of
keywords and part-of-speech-tagging. The annotators curated
sets of keywords in several categories, e.g., [rec target noun],
[rec verbs], [rec positive adjective] related to nouns, verbs,
and adjectives, respectively. The complete set of patterns and
keywords list are available in our replication package.
1263C. Answer Selection from a Conversation
We build upon work by Zhou et al. [29], who designed
R-CNN, a deep learning architecture, for answer selection
in community question answering (CQA). Since R-CNN was
designed for application in CQA for the non-SE domain4, we
customize for application in chats for software engineering
and then compare to the non-customized R-CNN in our
evaluation. We chose to build on R-CNN because other answer
extraction models [43]‚Äì[45] only model the semantic relevance
between questions and answers. In contrast, R-CNN models
the semantic links between successive candidate answers in
a discussion thread, in addition to the semantic relevance
between question and answer. Since developer chats often
contain short and rapid exchanges of messages between partic-
ipants, understanding the context of the discussion is crucial to
determine the semantic relation between question and answer.
Hence, we adapt R-CNN to extract the relevant answer(s) to an
opinion-asking question based on the context of the discussion
in a conversation.
Zhou et al. [29] regarded the problem of answer selec-
tion as an answer sequence labeling task. First, they apply
two convolution neural networks (CNNs) to summarize the
meaning of the question and a candidate answer, and then
generate the joint representation of a Q&A pair. The learned
joint representation is then used as input to long short-term
memory (LSTM) to learn the answer sequence of a question
for labeling the matching quality of each answer.
To design ChatEO, we make the following adaptations
to account for both the SE domain content and speciÔ¨Åcally
software-related chats. First, we preprocess the text, apply
a software-speciÔ¨Åc word-embedding model, and use those
embeddings as input to a CNN to learn joint representation
of a Q&A pair. We use TextCNN [46] since text in chat
(utterances) are much shorter compared to CQA (post). The
representations from the CNN are then passed as input to
Bidirectional LSTM (BiLSTM) instead of LSTM to improve
prediction of the answers from a sequence of utterances in a
conversation. We detail ChatEO answer extraction as follows:
1) Preprocessing: To help ChatEO with the semantics of
the chat text, the textual content in the disentangled conversa-
tions is preprocessed. We replace url, user mentions, emojis,
and code with speciÔ¨Åc tokens ‚Äòurl‚Äô, ‚Äòusername‚Äô, ‚Äòemoji‚Äô , and
‚Äòcode‚Äô respectively. To handle the informal style of commu-
nication in chats, we use a manual set of common phrase
expansions (e.g., ‚Äúyou‚Äôve‚Äù to ‚Äúyou have‚Äù). We then convert
the text to lowercase.
2) SE-customized Word Embeddings: Text in developer
chats and other software development communication can dif-
fer from regular English text found in Wikipedia, news articles,
etc. in terms of vocabulary and semantics. Hence, we trained
custom GloVe vectors on the most recent Stack OverÔ¨Çow
data dump (as of June, 2020) to more precisely capture word
semantics in the context of developer communications. To
train GloVe vectors, we performed standard tokenization and
4http://alt.qcri.org/semeval2015/task3/preprocessing on each Stack OverÔ¨Çow post‚Äôs title and text
and trimmed extremely rarely occurring words (vocabulary
minimum threshold of 100 posts; window size of 15 words).
Our word embedding model thus consists of 123,995 words,
where each word is represented by a 200-dimensional word
vector. We applied this custom word embedding model to each
word in each utterance of a conversation.
3) Convolutional Neural Networks: In natural language
analysis tasks, a sentence is encoded before it is further pro-
cessed in neural networks. We leverage the sentence encoding
technique from TextCNN [46]. TextCNN, also used for other
dialog analysis tasks [47], is a classical technique for sentence
modeling which uses a shallow Convolution Neural Network
(CNN) that combines n-gram information to model compact
text representation. Since an utterance in a chat is typically
short ( <25 words on average), we take each utterance as a
sentence and apply word embedding, multiple convolution,
and max-pooling operations.
The input for TextCNN is the distributed representation
of an utterance, created by mapping each word index into
its pre-trained embeddings. Each utterance is padded to the
same length nwith zero vectors. Let zj2Rddenote the
d-dimensional embedding for the jth word in an utterance.
Thus, an utterance of length ncan be represented by: z1:n=
z1 z2 ...z n, where  is the concatenation operator. To
gather local information, convolution is achieved by applying
a Ô¨Åxed length sliding window (kernel) wm2Rh‚á•d, on each
word position isuch that n h+1convolutional units in the
mth layer are generated by: cm
i= (wm¬∑zi:i+h+bm),i=
0,1,...,n  h+1, where his the size of convolution kernel,
 is the activation function, and bmis the bias factor for the
mth layer. The convolution layer is followed by a max pooling
layer, which can select the most effective information with the
highest value. The Ô¨Çattened output vectors for each kernel after
max-pooling are concatenated as the Ô¨Ånal output.
4) Bidirectional LSTM: The task of identifying answers in
a conversation requires capturing the context and Ô¨Çow of in-
formation among the utterances inside a conversation. Hence,
we use Bidirectional Long Short Term Memory (BiLSTM)
[48], where the utterances of a conversation are considered as
sequential data. The input to our BiLSTM is a sequence of
utterance representations created by TextCNN. Variations of
LSTM, widely used by researchers for answer extraction tasks
[30], [49], are capable of modeling semantic links between
continuous text to perform answer sequence learning.
LSTM [50] uses a gate mechanism to Ô¨Ålter relevant infor-
mation and capture long-term dependencies. An LSTM cell
comprises of input gate (i), forget gate (f), cell state (c), and
output gate (o). The outputs of LSTM at each time step ht
can be computed by the following equations:
2
664it
ft
Àúct
ot3
775=2
664 
 
tanh
 3
775‚úì
WÔ£ø
xt
ht 1 
+b‚óÜ
ct=Àúct it+ct 1 ft
1264ht=ot tanh ( ct)
where xtis the ith element in the input sequence; Wis the
weight matrix of LSTM cells; bis the bias term;  denotes sig-
moid activation function, and tanh denotes hyperbolic tangent
activation function;  denotes element-wise multiplication.
BiLSTM processes a sequence on two opposite directions
(forward and backward), and generates two independent se-
quences of LSTM output vectors. Hence, the output of a
BiLSTM at each time step is the concatenation of the two
output vectors from both directions, h=[~h ~h], where ~h
and ~hdenote the outputs of two LSTM layers respectively,
and is the concatenation operator.
IV . E VA L UAT I O N STUDY
We designed our evaluation to analyze the effectiveness
of the pattern-based identiÔ¨Åcation of opinion-asking questions
(RQ1) and of our answer extraction technique ( RQ2).
A. Metrics
We use measures that are widely used for evaluation in
machine learning and classiÔ¨Åcation. To analyze whether the
automatically identiÔ¨Åed instances are indeed opinion-asking
questions and their answers, we use precision, the ratio of
true positives over the sum of true and false positives. To
evaluate how often a technique fails to identify an opinion-
asking question or its answer, we use recall, the ratio of true
positives over the sum of true positives and false negatives.
F-measure combines these measures by harmonic mean.
B. Evaluation Datasets
We established several requirements for dataset creation to
reduce bias and threats to validity. To curate a representative
analysis dataset, we identiÔ¨Åed chat groups that primarily
discuss software development topics and had a substantial
number of participants. To ensure the generalizability of our
techniques, we chose two separate chat platforms, Slack and
IRC, which are currently the most popular chat platforms used
by software developers. We selected six popular programming
communities with an active presence on Slack or IRC. We
believe the communities are representative of public software-
related chats in general; we observed that the structure and
intent of conversations are similar across all 6 communities.
To collect conversations on Slack, we downloaded the
developer chat conversations dataset released by Chatterjee
et al. [36]. To gather conversations on IRC, we scraped
publicly available online chat logs5. After disentanglement, we
discarded single-utterance conversations, and then created two
separate evaluation datasets, one for opinion-asking question
identiÔ¨Åcation and a subset with only chats that start with an
opinion-asking question, for answer extraction. We created
our evaluation datasets by randomly selecting a representative
portion of the conversations from each of the six programming
communities.
5https://echelog.com/Table III shows the characteristics of the collected chat logs,
and our evaluation datasets, where #OAConv gives the number
of conversations that we identiÔ¨Åed as starting with an opinion-
asking question using the heuristics described in section III-B,
per community.
The question identiÔ¨Åcation evaluation dataset consists of a
total of 400 conversations, 5153 utterances, and 489 users. Our
question extraction technique is heuristics-based, requiring
conversations that do not start with an opinion-asking question.
Thus, we randomly chose 400 from our 45K chat logs for a
reasonable human-constructed goldset.
The evaluation dataset for answer extraction consists of
a total of 2001 conversations, 23,972 utterances, and 3160
users. Our machine-learning-based answer extraction requires
conversations starting with a question, and a dataset large
enough for training. Thus, 2001 conversations starting with
opinion-asking questions were used.
RQ1. How effective is ChatEO in identifying opinion-
asking questions in developer chats?
Gold Set Creation: We recruited 2 human judges with (3+
years) experience in programming and in using both chat
platforms (Slack and IRC), but no knowledge of our tech-
niques. They were provided a set of conversations where each
utterance includes: the unique conversation id, anonymized
name of the speaker, the utterance timestamp, and the utterance
text. Using Di Sorbo et al.‚Äôs [20] deÔ¨Ånition of opinion-asking
questions i.e., requiring someone to explicitly express his/her
point of view about something (e.g., What do you think about
creating a single webpage for all the services?) , the human
judges were asked to annotate only the utterances of the Ô¨Årst
speaker of each conversation with value ‚Äò1‚Äô for opinion-asking
question, or otherwise ‚Äò0‚Äô.
The judges annotated a shared set of 400 conversations,
of which they identiÔ¨Åed 69 instances i.e., utterances of the
Ô¨Årst speaker of each conversation, containing opinion-asking
questions (AngularJS: 10, C++: 10, OpenGL: 5, Python: 15,
Clojurians: 12, Elm: 17). We computed Cohen‚Äôs Kappa inter-
rater agreement between the 2 judges, and found an agreement
of 0.76, which is considered to be sufÔ¨Åcient ( >0.6) [38], while
the sample size of 400 conversations is sufÔ¨Åcient to compute
the agreement measure with high conÔ¨Ådence [37]. The two
judges then iteratively discussed and resolved all conÔ¨Çicts to
create the Ô¨Ånal gold set.
Comparison Techniques: Researchers have used sentiment
analysis techniques [16], [17] and lexical patterns [20] to
extract opinions from software-related documents. Thus, we
selected two different approaches, i.e., pattern-matching ap-
proaches and sentiment analysis, as comparison techniques to
ChatEO. We evaluated three well-known sentiment analysis
techniques SentiStrength-SE [51], CoreNLP [52], and NLTK
[53] with their default settings. Since opinions could have
positive/negative polarities, for the purpose of evaluation, we
consider a leading question in a conversation identiÔ¨Åed with
either positive or negative sentiment as opinion-asking. DECA
[20] is a pattern-based technique that uses Natural Language
1265Table III: Evaluation Dataset
Samples(OA Question IdentiÔ¨Åcation) created from Chat Logs(#Conv) ,Samples(Answer Extraction) created from Chat Logs(#OAConv)
Source DurationChat LogsSamples
OA Question IdentiÔ¨Åcation Answer Extraction
#Conversations #OAConv #Conversations #Utterances #Users #Conversations #Utterances #Users
angularjs (IRC) Apr2014-Jun2020 181662 9175 80 1812 90 891 11218 1109
c++ (IRC) Oct2018-Jun2020 95548 841 60 630 62 127 2017 181
opengl (IRC) Jul2005-Jun2020 135536 9198 60 698 54 258 3176 390
pythondev (Slack) Jul2017-Jun2019 8887 707 80 848 93 160 1604 320
clojurians (Slack) Jul2017-Jun2019 7918 562 60 570 81 156 1422 314
elmlang (Slack) Jul2017-Jun2019 22150 1665 60 595 109 409 4535 846
Total 451701 22148 400 5153 489 2001 23972 3160
Table IV: Opinion-Asking Question IdentiÔ¨Åcation Results
Technique P R F Example FP Example TP
SentiStrength-SE [51] 0.18 0.31 0.23...I‚Äôm having a weird issue with my ng-cli based angular
app...Is there any potential issue, because my Model is
called ‚ÄùClass‚Äù and/or the method is called ‚ÄùtoClass‚Äù?Anyone know a good cross platform GUI library that
(preferably) supports CMake? I‚Äôd rather not use Qt if I
don‚Äôt have to because I don‚Äôt want to use the qt MOC
CoreNLP [52] 0.19 0.73 0.30why does ‚Äò(read-string ‚Äù07‚Äù)‚Äò return ‚Äò7‚Äò, but ‚Äò(read-string
‚Äù08‚Äù)‚Äò throws an exception?any suggestions for cmake test running strategies? or how
to organize tests expected to succeed or fail?
NLTK [53] 0.18 0.78 0.30Hi, is there a way to expose a class and a function using
c style declaration : void foo(MyClass bar)Does anyone know of a good way to ‚Äùsandbox‚Äù the
loading of namespaces?
DECA [20] 1.00 0.01 0.02 -..seems lambda with auto argument type provide separate
templated methods per used type; am i right?..
ChatEO 0.87 0.49 0.62 can someone tell me why does this give an error ?What is the best way in your opinion to convert input
(Ô¨Åle, XML, etc.) to PDF with precision to 1 mm?
Parsing to classify the content of development emails accord-
ing to their purpose. We used their tool to investigate the use of
their linguistic patterns to identify opinion-asking questions in
developer chats. We do not compare with Huang et al.‚Äôs CNN-
based classiÔ¨Åer of intention categories [21], since they merged
‚Äúopinion asking‚Äù with the ‚Äúinformation seeking‚Äù category.
Results: Table IV presents precision (P), recall (R), F-measure
(F), and examples of False Positives (FP) and True Positives
(TP) for ChatEO and our comparison techniques for opinion-
asking question identiÔ¨Åcation on our evaluation dataset.
ChatEO achieves a precision, recall, and F-measure of 0.87,
0.49 and 0.62, respectively. Results in Table IV indicate that
ChatEO achieves an overall better precision (except DECA)
and F-measure, compared to all the comparison techniques.
With high precision, when ChatEO identiÔ¨Åes an opinion-
asking question, the chance of it being correct is higher
than that identiÔ¨Åed by other techniques. We aim for higher
precision (with possible lower recall) in identifying opinion-
asking questions, since that could potentially contribute to the
next module of ChatEO i.e., extracting answers to opinion-
asking questions.
Some of the opinion-asking instances that ChatEO wasn‚Äôt
able to recognize lacked presence of recurrent linguistic pat-
terns such as ‚Äú‚ÄùHow does angular Ô¨Åt in with traditional
MVC frameworks like .NET MVC and ruby on rails? Do
people generally still use an MVC framework or just write
a web api?‚Äù . Some FNs also resulted from instances where
the opinion-asking questions were continuation of a separate
question such as ‚ÄúIs there a canvas library where I can use
getImageData to work with the typed array data? Or is this
where I should use ports?‚Äù .
We observe that the sentiment analysis tools show a high
recall at the expense of low precision, with an exception
of SentiStrengthSE, which exhibits lower values for both
precision and recall. The ‚ÄúExample FP‚Äù column in Table
IV indicates that sentiment analysis tools are often unable to
catch the nuances of SE-speciÔ¨Åc keywords such as ‚Äòexpose‚Äô,‚Äòexception‚Äô. Another example, ‚ÄúWhat is the preferred way
to distribute python programs?‚Äù , which ChatEO is able to
correctly identify as opinion-asking, is labelled as neutral by
all the sentiment analysis tools. The same happens for the
instance ‚Äúhow do I Ô¨Ålter items in a list when displaying them
with ngFor? Should I use a Ô¨Ålter/pipe or should I use ngIf in
the template?‚Äù . ChatEO is able to recognize that this is asking
for opinions on what path to take among two options, while
the sentiment analysis tools classify this as neutral. Note that
this just indicates that these tools are limited in the context
of identifying opinion-asking questions, but could be indeed
useful for other tasks (e.g., assessing developer emotions).
DECA [20] identiÔ¨Åed only one instance to be opinion-
asking, which is a true positive, hence the precision is 1.00.
Apart from this, it was not able to classify any other instance
as opinion-asking, hence the low recall (0.01). On analyzing
DECA‚Äôs classiÔ¨Åcation results, we observe that, out of 69
instances in the gold set, it could not assign any intention
category to 17 instances. This is possibly due to the informal
communication style in chats, which is considerably different
than emails. Since an utterance could contain more than
one sentence, DECA often assigned multiple categories (e.g.,
information seeking, feature request, problem discovery ) to
each instance. The most frequent intention category observed
was information seeking . During the development phase, we
explored additional linguistic patterns, but they yielded more
FPs. This is a limitation of using linguistic patterns, as they are
restrictive when expressing words that have different meaning
in different contexts.
ChatEO opinion-asking question identiÔ¨Åcation signiÔ¨Åcantly
outperforms an existing pattern-based technique that was
designed for emails [20], as well as sentiment analysis tools
[51]‚Äì[53] in terms of F-measure.
RQ2. How effective is ChatEO in identifying answer(s) to
opinion-asking questions in a developer conversation?
Gold Set Creation: Similar to RQ1, we recruited 2 human
1266judges with (3+ years) experience in programming and in
using both chat platforms (Slack and IRC), but no knowledge
of our techniques. The gold set creation for answer annotation
was conducted in two phases, as follows:
‚Ä¢Phase-1 (Annotation): The human judges were provided a
set of conversations with annotation instructions as follows:
Mark each utterance in the conversation that provides
information or advice (good or bad) that contributes to
addressing the opinion-asking question in a way that is
understandable/meaningful/interpretable when read as a
standalone response to the marked opinion-asking ques-
tion (i.e., the answer should provide information that is
understandable without reading the entire conversation).
Such utterances should not represent answer(s) to follow-
up questions in a conversation. An answer to an opinion-
asking question could also be a yes/no response. There could
be more than one answer provided to the opinion-asking
question in a conversation.
‚Ä¢Phase-2 (Validation): The purpose of Phase-2 was two-fold:
(1) measure validity of Phase-1 annotations, and (2) evaluate
if an answer would match an opinion-asking question out
of conversational context, such that the Q&A pair could
be useful as part of a Q&A system. Therefore, for Phase-
2 annotations, we ensured that the annotators read only
the provided question and answers, and not the entire
conversations from which they were extracted. The Phase-1
annotations from the Ô¨Årst annotator were used to generate
a set of question and answers, which were used for Phase-
2 annotations by the second annotator, and vice-versa. For
each utterance provided as an answer to an opinion-asking
question, the annotators were asked to indicate (‚Äúyes/no‚Äù) if
the utterance represents an answer based on the guidelines
in Phase-1. Additionally, if the annotation value was ‚Äúno‚Äù,
the annotators were asked to state the reason.
The judges annotated a total of 2001 conversations, of which
they identiÔ¨Åed a total of 2292 answers to opinion-asking
questions (AngularJS: 1001, C++: 133, OpenGL: 263, Python:
165, Clojurians: 197, Elm: 533). We found that the Ô¨Årst
annotator considered 94.6% of annotations of the second an-
notator as valid, while the second annotator considered 96.2%
annotations of the Ô¨Årst annotator as valid. We also noticed
that the majority of disagreements were due to the answer
utterances containing incomplete or inadequate information
to answer the marked opinion-asking question when removed
from conversational context (e.g., ‚Äúand then you can replace
your calls to ‚Äòf‚Äò with ‚ÄòlogArgs2 f‚Äò without touching the
function...‚Äù), and human annotation errors such as marking an
utterance as an answer when it just points to other channels.
Comparison Techniques: Since we believe this is the Ô¨Årst effort
to automatically extract answers to opinion-asking questions
from developer chats, we chose to evaluate against heuristic-
based and feature-based machine learning classiÔ¨Åcation as
well as the original R-CNN deep learning-based technique on
which we built ChatEO.
Heuristic-based (HE): Intuitively, the answer to an opinion-asking question might be found based on its location in the
conversation, the relation between its content and the question,
or the presence of sentiment in the answer. We investigated
each of these possibilities separately and in combination.
‚Ä¢Location: Based on the intuition that a question might be
answered immediately after it is asked during a conversa-
tion, we compare against the naive approach of identifying
the next utterance after the leading opinion-asking question
as an answer.
‚Ä¢Content: Traditional Q&A systems have often aimed to
extract answers based on semantic matching between ques-
tion and answers [43], [54]. Thus, to model content-based
semantic relation between question and answer, we compare
the average word embedding of the question and answer
texts. Using our word embedding model described in III-C2,
we extract utterances with considerable similarity (  0.5) to
the opinion-asking question as answers.
‚Ä¢Sentiment: Previous researchers have leveraged sentiment
analysis to extract relevant answers in non-factoid Q&A
systems [55]‚Äì[58]. Thus, based on the intuition that the
answer to an opinion-asking question might exhibit senti-
ment, we use CoreNLP [52] to extract utterances bearing
sentiment (positive or negative) as answers. We explored
other sentiment analysis tools (e.g., SentiStrength-SE [51],
NLTK [53]); however, we do not discuss them, since they
yielded inferior results.
Machine Learning-based (ML): We combine location, con-
tent, sentiment attributes as features of a machine learning
(ML)-based classiÔ¨Åer. We explored several popular ML algo-
rithms (e.g., Support Vector Machines (SVM), Random Forest)
using the Weka toolkit [59], and observed that they yielded
nearly similar results. We report the results for SVM.
Deep Learning-based (DL): We present the results for both
R-CNN and ChatEO implemented as follows.
‚Ä¢RCNN: We implemented R-CNN [29] for developer chats
using open-source neural-network library Keras [60]. R-
CNN used word embeddings pre-trained on their corpus.
Similarly, we trained custom GloVe vectors on our chat
corpus for our comparison.
‚Ä¢ChatEO: We also implemented ChatEO using Keras [60].
We used grid-search [61] to perform hyper-parameter tun-
ing. First, to obtain sufÔ¨Åcient semantic information at the
utterance level, we use three convolution Ô¨Ålters of size 2,
3, and 4, with 50 (twice the average length of an utterance)
feature maps for each Ô¨Ålter. The pool sizes of convolution
are (2,1), (2,1), (3,1), respectively. Then, a BiLSTM layer
with 400 units (200 for each direction) is used to capture
the contextual information in a conversation. Finally, we use
a linear layer with sigmoid activation function to predict
the probability scores of binary classes (answer and non-
answer). We use binary-cross-entropy as the loss function,
and Adam optimization algorithm for gradient descent.
To avoid over-Ô¨Åtting, we apply a dropout [62] of 0.5 on
the TextCNN embeddings, i.e., 50% units will be randomly
omitted to prevent complex co-adaptations on the training
1267Table V: Answer Extraction Results on Held-out Test Set
HE: Heuristic-based, ML: Machine Learning-based, DL: Deep Learning-based
TechniqueAngularJS C++ OpenGL Python Clojurians Elm Overall
P R F P R F P R F P R F P R F P R F P R F
HELocation(L) 0.66 0.47 0.55 0.68 0.77 0.72 0.69 0.74 0.71 0.59 0.62 0.60 0.52 0.40 0.46 0.65 0.45 0.53 0.63 0.55 0.59
Content(C) 0.03 0.06 0.04 0.05 0.14 0.07 0.06 0.18 0.09 0.11 0.18 0.14 0.09 0.11 0.10 0.14 0.15 0.14 0.07 0.13 0.09
Sentiment(S) 0.06 0.16 0.09 0.05 0.23 0.08 0.09 0.31 0.13 0.11 0.24 0.15 0.07 0.14 0.09 0.10 0.18 0.13 0.07 0.20 0.11
ML L+C+S 0.70 0.47 0.56 0.78 0.77 0.77 0.70 0.73 0.71 0.62 0.62 0.62 0.60 0.41 0.49 0.75 0.47 0.58 0.69 0.55 0.61
DLR-CNN 0.70 0.46 0.55 0.78 0.74 0.76 0.72 0.70 0.71 0.64 0.61 0.62 0.62 0.38 0.48 0.73 0.39 0.51 0.70 0.52 0.60
ChatEO 0.80 0.53 0.64 0.81 0.72 0.77 0.77 0.68 0.72 0.76 0.65 0.70 0.66 0.42 0.51 0.81 0.56 0.66 0.77 0.59 0.67

/RFDWLRQ &RQWHQW 6HQWLPHQW 0/ 5&11 &KDW(2$QJ-6 & 2SHQ*/ 3\WKRQ &ORMXULDQV (OPPrecision
/RFDWLRQ &RQWHQW 6HQWLPHQW 0/ 5&11 &KDW(2$QJ-6 & 2SHQ*/ 3\WKRQ &ORMXULDQV (OPRecall
Figure 2: Evaluation Measures of Answer Extraction
data. Additionally, we use a recurrent dropout of 0.1 in
the LSTM units. We also use early stopping [63], i.e., if
the performance of the classiÔ¨Åer did not improve for 10
epochs, the training process is stopped. ChatEO answer
extraction takes approximately 36 minutes to train and test
2001 conversations on a system with 2.5 GHz Intel Core i5
processor and 8GB DDR3 RAM.
Evaluation Process: Evaluation of ChatEO and the comparison
techniques was conducted using the evaluation dataset of 2000
conversations, described in Table III. We created a test set of
400 conversations (adhering to commonly recognized train-
test ratio of 80-20%). We ensured that it contains similar
number of instances from each programming community: 140
(70*2) conversations from Angular JS and Python, 260 (65*4)
conversations from C++, OpenGL, Clojurians, and Elm.
Results: Table V presents precision (P), recall (R), and F-
measure (F) for ChatEO and our comparison techniques for
automatic answer extraction on our held-out test set of 400
conversations, which contains a total of 499 answers. The best
results for P, R, F across all techniques are highlighted in bold.
Overall, Table V shows that ChatEO achieves the highest
overall precision, recall, and F-Measure of all techniques, with
0.77, 0.59, and 0.67, respectively. Overall, ChatEO identiÔ¨Åed
370 answers in the test set, out of which 285 are true positives.
R-CNN and the ML-based classiÔ¨Åer perform next best andsimilar to each other in precision and F-measure. The better
performance of ChatEO suggests that capturing contextual
information through BiLSTM can beneÔ¨Åt answer extraction
in chat messages, and that using a domain-speciÔ¨Åc word
embedding model (trained on software-related texts) accom-
panied with hyper-parameter tuning, is essential to adapting
deep learning models for software-related applications. Figure
2 shows that the performance of ChatEO is consistent (76-
81% precision) across all communities, except Clojure. One
possible reason for this is, answers are often provided in
this chat community in the form of code snippets along with
little to no natural language text, which makes it difÔ¨Åcult for
ChatEO to model the semantic links.
ChatEO‚Äôs overall recall of 0.59 indicates that it is difÔ¨Åcult
to identify all relevant answers in chats even with complex
dialog modeling. In fact, the recall of ChatEO is lower than
the heuristic-based technique location for C++ and OpenGL.
Upon deeper analysis, we observed that these two communities
contain less answers (one answer per opinion-asking question
on average) compared to the other communities, and the
answer often resides in the utterance occurring immediately
after the Ô¨Årst speaker.
The location heuristic exhibits signiÔ¨Åcantly better perfor-
mance than content or sentiment heuristics. Of the 400 conver-
sations, 278 have at least one answer occurring immediately
after the utterances of the Ô¨Årst speaker. Neither content or
sentiment is a strong indicator of answers, with precision
of 0.07 and F-measure of 0.09 and 0.11, respectively. These
heuristics cannot distinguish the intent of a response. Consider,
‚ÄúQ: Hi, Clojure intermediate here. What is the best way to read
big endian binary Ô¨Åle?; R:do you need to make something
that parses the binary contents, does it sufÔ¨Åce to just get the
bytes in an array? ‚Äù. Both content andsentiment marked this
response as an answer, without being able to understand that
this is a follow-up question.
Combining the heuristics as features to SVM, the precision
(and thus F-Measure) improved slightly over the location
heuristic with .06 increase in precision and .02 in F-measure.
As expected, location as a feature shows the highest informa-
tion gain. We investigated several classiÔ¨Åer parameters (e.g.,
kernal and regularization parameters), but observed that the
classiÔ¨Åcation task was not very sensitive to parameter choices,
as they had little discernible effect on the effectiveness metrics
(in most cases Ô£ø0.01). Since our dataset is imbalanced, with
considerably low ratio of answers to other utterances, we
explored over-sampling (SMOTE) techniques. No signiÔ¨Åcant
improvements occurred. ML-based classiÔ¨Åcation may be im-
proved with more features and feature engineering.
1268ChatEO answer extraction shows improvement over
heuristics-based, ML-based, and existing deep learning-
based [29] techniques in terms of precision, recall, and F-
measure.
V. T HREATS TO VALIDITY
Construct Validity: A threat to construct validity might arise
from the manual annotations for creating the gold sets. To limit
this threat, we ensured that our annotators have considerable
experience in programming and in using both chat platforms
and that they followed a consistent annotation procedure
piloted in advance. We also observed high values of Cohen‚Äôs
Kappa coefÔ¨Åcient, which measures the inter-rater agreement
for opinion-asking questions. For answer annotations, we
conducted a two-phase annotation procedure to ensure the
validity of the selected answers.
Internal Validity: Errors in the automatically disentangled
conversations could pose a threat to internal validity affecting
misclassiÔ¨Åcation. We mitigated this threat by humans, without
knowledge of our techniques, manually discarding poorly
disentangled conversations from our dataset. In all stages of
the pipeline of ChatEO, we aimed for higher precision over
recall as the quality of information is more important than
missing instances; chat datasets are large with many opinions
so our achieved recall is sufÔ¨Åcient to extract a signiÔ¨Åcant
number of opinion Q&A. Other potential threats could be
related to evaluation bias or errors in our scripts. To reduce
these threats, we ensured that the instances in our development
set do not overlap with our train or test sets. We also wrote
unit tests and performed code reviews.
External Validity: To ensure generalizability of our approach,
we selected the subjects of our study from the two most
popular software developer chat communities, Slack and IRC.
We selected statistically representative samples from six active
communities which represent a broad set of topics related to
each programming language. However, our study‚Äôs results may
not transfer to other chat platforms or developer communica-
tions. Scaling to larger datasets might also lead to different
evaluation results. Our technique of identifying opinion-asking
questions could be made more generalizable by augmenting
the set of identiÔ¨Åed patterns and vocabulary terms.
VI. R ELATED WORK
Mining Opinions in SE. In addition to the related work
discussed in Section II, signiÔ¨Åcant work has focused on mining
opinions from developer forums. Uddin and Khomh [16]
designed Opiner, which uses keyword-matching along with
a customized Sentiment Orientation algorithm to summarize
API reviews. Lin et al. [17] used patterns to identify and
classify opinions on APIs from Stack OverÔ¨Çow. Zhang et
al. [64] identiÔ¨Åes negative opinions about APIs from forums.
Huang et al. [65] proposed an automatic approach to distill and
aggregate comparative opinions of comparable technologies
from Q&A websites. Ren et al. [66] discovered and sum-
marized controversial (criticized) answers in Stack OverÔ¨Çow,
based on judgment, sentiment and opinion. Novielli et al. [18],[67] investigated the role of affective lexicon on the questions
posted in Stack OverÔ¨Çow.
Researchers have also analyzed opinions in developer
emails, commit logs, and app reviews. Xiong et al. [68] studied
assumptions in OSS development mailing lists. Sinha et al.
[19] analyzed developer sentiment in Github commit logs.
Opinions in app reviews [22], [23], [34], [69] have been
mined to help app developers gather information about user
requirements, ideas for improvements, and user sentiments
about speciÔ¨Åc features. To the best of our knowledge, our work
is the Ô¨Årst to extract opinion Q&A from developer chats.
Extracting Q&A from Online Communications. Outside the
SE domain, researchers have proposed techniques to identify
Q&A pairs in online communications (e.g., Yahoo Answers).
Shrestha et al. [70] used machine learning approaches to auto-
matically detect Q&A pairs in emails. Cong et al. [71] detected
Q&A pairs from forum threads by using Sequential Pattern
Mining to detect questions, and a graph-based propagation
method to detect answers in the same thread.
Recently, researchers have focused on answer selection, a
major subtask of Q&A extraction, which aims to select the
most relevant answers from a candidate answer set. Typical
approaches for answer selection model the semantic matching
between question and answers [31], [43]‚Äì[45]. These ap-
proaches have the advantage of sharing parameters, thus mak-
ing the model smaller and easier to train. However, they often
fail to capture the semantic correlations embedded in the re-
sponse sequence of a question. To overcome such drawbacks,
Zhou et al. [29] designed a recurrent architecture that models
the semantic relations between successive responses, as well
as the question and answer. Xiang et al. [49] investigated an
attention mechanism and context modeling to aid the learning
of deterministic information for answer selection. Wang et al.
[30] proposed a bilateral multi-perspective matching model in
which Q&A pairs are matched on multiple levels of granularity
at each time-step. Our model belongs to the same framework
which captures the contextual information of conversations in
extracting answers from developer chats.
Most of the these techniques for Q&A extraction were de-
signed for general online communications and not speciÔ¨Åcally
for software forums. Gottipati et al. [72] used Hidden Markov
Models to infer semantic tags (e.g., question, answer, clari-
fying question) of posts in the software forum threads. Hen√ü
et al. [73] used topic modeling and text similarity measures
to automatically extract FAQs from software development
discussions (mailing lists, online forums).
Analyzing Developer Chats. Wood et al. [2] created a
supervised classiÔ¨Åer to automatically detect speech acts in
developer Q&A bug repair conversations. Shi et al. [47] use
deep Siamese network to identify feature-request from chat
conversations. Alkadhi et al. [74], [75] showed that machine
learning can be leveraged to detect rationale in IRC messages.
Chowdhury and Hindle [76] exploit Stack OverÔ¨Çow discus-
sions and YouTube video comments to automatically Ô¨Ålter off-
1269topic discussions in IRC chats. Romero et al. [8] developed
a chatbot that detects a troubleshooting question asked on
Gitter and provides possible answers retrieved from querying
similar Stack OverÔ¨Çow posts. Compared to their work, we are
automatically identifying opinion-asking questions and their
answers provided by developers in chat forums.
Chatterjee et al.‚Äôs [9] exploratory study on Slack developer
chats suggested that developers share opinions and interest-
ing insights on APIs, programming tools and best practices,
via conversations. Other studies have focused on learning
developer behaviors and how chat communities are used by
development teams across the globe [6], [7], [77]‚Äì[81].
VII. C ONCLUSIONS AND FUTURE WORK
In this paper, we present and evaluate ChatEO, which auto-
matically identiÔ¨Åes opinion-asking questions from chats using
a pattern-based technique, and extracts participants‚Äô answers
using a deep learning-based architecture. This research pro-
vides a signiÔ¨Åcant contribution to using software developers‚Äô
public chat forums for building opinion Q&A systems, a
specialized instance of virtual assistants and chatbots for soft-
ware engineers. ChatEO opinion-asking question identiÔ¨Åcation
signiÔ¨Åcantly outperforms existing sentiment analysis tools and
a pattern-based technique that was designed for emails [20].
ChatEO answer extraction shows improvement over heuristics-
based, ML-based, and an existing deep learning-based tech-
nique designed for CQA [29]. Our replication package can be
used to verify our results [url].
Our immediate next steps focus on investigating machine
learning-based techniques for opinion-asking question identi-
Ô¨Åcation, and attention-based LSTM network for answer ex-
traction. We will also expand to a larger and more diverse
developer chat communication dataset. The Q&A pairs ex-
tracted using ChatEO could also be leveraged to generate
FAQs, provide tool support for recommendation systems, and
in understanding developer behavior and collaboration (asking
and sharing opinions).
ACKNOWLEDGMENT
We acknowledge the support of the National Science Foun-
dation under grants 1812968 and 1813253.REFERENCES
[1] S. K. Kuttal, J. Myers, S. Gurka, D. Magar, D. Piorkowski, and R. Bel-
lamy, ‚ÄúTowards designing conversational agents for pair programming:
Accounting for creativity strategies and conversational styles,‚Äù in 2020
IEEE Symposium on Visual Languages and Human-Centric Computing
(VL/HCC) , 2020, pp. 1‚Äì11.
[2] A. Wood, P. Rodeghero, A. Armaly, and C. McMillan, ‚ÄúDetecting
speech act types in developer question/answer conversations during
bug repair,‚Äù in Proceedings of the 2018 26th ACM Joint Meeting
on European Software Engineering Conference and Symposium on the
Foundations of Software Engineering , ser. ESEC/FSE 2018. New York,
NY , USA: Association for Computing Machinery, 2018, p. 491?502.
[Online]. Available: https://doi.org/10.1145/3236024.3236031
[3] Z. Eberhart, A. Bansal, and C. McMillan, ‚ÄúThe apiza corpus: Api usage
dialogues with a simulated virtual assistant,‚Äù 2020.
[4] A. Ram, R. Prasad, C. Khatri, A. Venkatesh, R. Gabriel, Q. Liu,
J. Nunn, B. Hedayatnia, M. Cheng, A. Nagar, E. King, K. Bland,
A. Wartick, Y . Pan, H. Song, S. Jayadevan, G. Hwang, and A. Pettigrue,
‚ÄúConversational ai: The science behind the alexa prize,‚Äù 2018.
[5] I. Itkin, A. Novikov, and R. Yavorskiy, ‚ÄúDevelopment of intelligent vir-
tual assistant for software testing team,‚Äù in 2019 IEEE 19th International
Conference on Software Quality, Reliability and Security Companion
(QRS-C) , 2019, pp. 126‚Äì129.
[6] B. Lin, A. Zagalsky, M.-A. Storey, and A. Serebrenik, ‚ÄúWhy Developers
Are Slacking Off: Understanding How Software Teams Use Slack,‚Äù
inProceedings of the 19th ACM Conference on Computer Supported
Cooperative Work and Social Computing Companion , ser. CSCW ‚Äô16
Companion. New York, NY , USA: ACM, 2016, pp. 333‚Äì336. [Online].
Available: http://doi.acm.org/10.1145/2818052.2869117
[7] E. Shihab, Z. M. Jiang, and A. E. Hassan, ‚ÄúOn the Use of Internet Relay
Chat (IRC) Meetings by Developers of the GNOME GTK+ Project,‚Äù
inProceedings of the 2009 6th IEEE International Working Conference
on Mining Software Repositories , ser. MSR ‚Äô09. Washington, DC,
USA: IEEE Computer Society, 2009, pp. 107‚Äì110. [Online]. Available:
http://dx.doi.org/10.1109/MSR.2009.5069488
[8] R. Romero, S. Haiduc, and E. Parra, ‚ÄúExperiences building an answer
bot for gitter,‚Äù in Proceedings of the 2nd International Workshop on
Bots in Software Engineering , ser. BotSE ‚Äô20, 2020.
[9] P. Chatterjee, K. Damevski, L. Pollock, V . Augustine, and N. Kraft,
‚ÄúExploratory Study of Slack Q&A Chats as a Mining Source for
Software Engineering Tools,‚Äù in Proceedings of the 16th International
Conference on Mining Software Repositories (MSR‚Äô19) , May 2019.
[Online]. Available: https://doi.org/10.1109/MSR.2019.00075
[10] Peng Jiang, Hongping Fu, Chunxia Zhang, and Zhendong Niu, ‚ÄúA
framework for opinion question answering,‚Äù in 2010 6th International
Conference on Advanced Information Management and Service (IMS) ,
2010, pp. 424‚Äì427.
[11] A. Balahur, E. Boldrini, A. Montoyo, and P. Mart ¬¥ƒ±nez-Barco, ‚ÄúOpinion
question answering: Towards a uniÔ¨Åed approach,‚Äù in ECAI , 2010.
[12] M. Wan and J. McAuley, ‚ÄúModeling ambiguity, subjectivity, and diverg-
ing viewpoints in opinion question answering systems,‚Äù in 2016 IEEE
16th International Conference on Data Mining (ICDM) , 2016, pp. 489‚Äì
498.
[13] X. Su, G. Gao, and Y . Tian, ‚ÄúA framework to answer questions of opinion
type,‚Äù in 2010 Seventh Web Information Systems and Applications
Conference , 2010, pp. 166‚Äì169.
[14] M. M. Rahman, C. K. Roy, and D. Lo, ‚ÄúRACK: Automatic API
Recommendation Using Crowdsourced Knowledge,‚Äù in 2016 IEEE
23rd International Conference on Software Analysis, Evolution, and
Reengineering (SANER) , vol. 1, March 2016, pp. 349‚Äì359.
[15] C. Chen, S. Gao, and Z. Xing, ‚ÄúMining Analogical Libraries in Q
A Discussions ‚Äì Incorporating Relational and Categorical Knowledge
into Word Embedding,‚Äù in 2016 IEEE 23rd International Conference
on Software Analysis, Evolution, and Reengineering (SANER) , vol. 1,
March 2016, pp. 338‚Äì348.
[16] G. Uddin and F. Khomh, ‚ÄúAutomatic summarization of api reviews,‚Äù
inProceedings of the 32nd IEEE/ACM International Conference on
Automated Software Engineering , ser. ASE 2017. IEEE Press, 2017,
p. 159?170.
[17] B. Lin, F. Zampetti, G. Bavota, M. Di Penta, and M. Lanza,
‚ÄúPattern-based mining of opinions in q&a websites,‚Äù in Proceedings of
the 41st International Conference on Software Engineering , ser. ICSE
1270‚Äô19. IEEE Press, 2019, p. 548?559. [Online]. Available: https:
//doi.org/10.1109/ICSE.2019.00066
[18] N. Novielli, F. Calefato, and F. Lanubile, ‚ÄúTowards Discovering the
Role of Emotions in Stack OverÔ¨Çow,‚Äù in Proceedings of the 6th
International Workshop on Social Software Engineering , ser. SSE 2014.
New York, NY , USA: ACM, 2014, pp. 33‚Äì36. [Online]. Available:
http://doi.acm.org.udel.idm.oclc.org/10.1145/2661685.2661689
[19] V . Sinha, A. Lazar, and B. Sharif, ‚ÄúAnalyzing developer sentiment in
commit logs,‚Äù in 2016 IEEE/ACM 13th Working Conference on Mining
Software Repositories (MSR) , 2016, pp. 520‚Äì523.
[20] A. D. Sorbo, S. Panichella, C. A. Visaggio, M. D. Penta,
G. Canfora, and H. C. Gall, ‚ÄúDevelopment Emails Content Analyzer:
Intention Mining in Developer Discussions (T),‚Äù in Proceedings
of the 2015 30th IEEE/ACM International Conference on Automated
Software Engineering (ASE) , ser. ASE ‚Äô15. Washington, DC, USA:
IEEE Computer Society, 2015, pp. 12‚Äì23. [Online]. Available:
https://doi-org.udel.idm.oclc.org/10.1109/ASE.2015.12
[21] Q. Huang, X. Xia, D. Lo, and G. C. Murphy, ‚ÄúAutomating intention
mining,‚Äù IEEE Transactions on Software Engineering , pp. 1‚Äì1, 2018.
[22] E. Guzman and W. Maalej, ‚ÄúHow do users like this feature? a Ô¨Åne
grained sentiment analysis of app reviews,‚Äù in 2014 IEEE 22nd Interna-
tional Requirements Engineering Conference (RE) , 2014, pp. 153‚Äì162.
[23] L. V . G. Carre Àúno and K. Winbladh, ‚ÄúAnalysis of user comments: An ap-
proach for software requirements evolution,‚Äù in 2013 35th International
Conference on Software Engineering (ICSE) , 2013, pp. 582‚Äì591.
[24] J. S. Culpepper, F. Diaz, and M. D. Smucker, ‚ÄúResearch frontiers
in information retrieval: Report from the third strategic workshop
on information retrieval in lorne (swirl 2018),‚Äù SIGIR Forum ,
vol. 52, no. 1, p. 34?90, Aug. 2018. [Online]. Available: https:
//doi.org/10.1145/3274784.3274788
[25] L. Nie, H. Jiang, Z. Ren, Z. Sun, and X. Li, ‚ÄúQuery expansion based
on crowd knowledge for code search,‚Äù IEEE Transactions on Services
Computing , vol. 9, no. 5, pp. 771‚Äì783, Sept 2016.
[26] N. Rao, C. Bansal, T. Zimmermann, A. H. Awadallah, and N. Nagappan,
‚ÄúAnalyzing web search behavior for software engineering tasks,‚Äù 2020.
[27] A. Vtyurina, D. Savenkov, E. Agichtein, and C. L. A. Clarke,
‚ÄúExploring conversational search with humans, assistants, and wizards,‚Äù
inProceedings of the 2017 CHI Conference Extended Abstracts on
Human Factors in Computing Systems , ser. CHI EA ‚Äô17. New York,
NY , USA: Association for Computing Machinery, 2017, p. 2187?2193.
[Online]. Available: https://doi.org/10.1145/3027063.3053175
[28] G. Uddin and F. Khomh, ‚ÄúAutomatic mining of opinions expressed about
apis in stack overÔ¨Çow,‚Äù IEEE Transactions on Software Engineering , pp.
1‚Äì1, 2019.
[29] X. Zhou, B. Hu, Q. Chen, B. Tang, and X. Wang, ‚ÄúAnswer
sequence learning with neural networks for answer selection in
community question answering,‚Äù in Proceedings of the 53rd Annual
Meeting of the Association for Computational Linguistics and the 7th
International Joint Conference on Natural Language Processing (Volume
2: Short Papers) . Beijing, China: Association for Computational
Linguistics, Jul. 2015, pp. 713‚Äì718. [Online]. Available: https:
//www.aclweb.org/anthology/P15-2117
[30] Z. Wang, W. Hamza, and R. Florian, ‚ÄúBilateral multi-perspective
matching for natural language sentences,‚Äù in Proceedings of the 26th
International Joint Conference on ArtiÔ¨Åcial Intelligence , ser. IJCAI?17.
AAAI Press, 2017, p. 4144?4150.
[31] G. Shen, Y . Yang, and Z.-H. Deng, ‚ÄúInter-weighted alignment network
for sentence pair modeling,‚Äù in Proceedings of the 2017 Conference
on Empirical Methods in Natural Language Processing . Copenhagen,
Denmark: Association for Computational Linguistics, Sep. 2017,
pp. 1179‚Äì1189. [Online]. Available: https://www.aclweb.org/anthology/
D17-1122
[32] S. Joty, A. Barr ¬¥on-Cede Àúno, G. Da San Martino, S. Filice,
L. M `arquez, A. Moschitti, and P. Nakov, ‚ÄúGlobal thread-level inference
for comment classiÔ¨Åcation in community question answering,‚Äù
in Proceedings of the 2015 Conference on Empirical Methods in
Natural Language Processing . Lisbon, Portugal: Association for
Computational Linguistics, Sep. 2015, pp. 573‚Äì578. [Online]. Available:
https://www.aclweb.org/anthology/D15-1068
[33] A. Moschitti, S. Quarteroni, R. Basili, and S. Manandhar,
‚ÄúExploiting syntactic and shallow semantic kernels for question
answer classiÔ¨Åcation,‚Äù in Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics . Prague, Czech Republic:Association for Computational Linguistics, Jun. 2007, pp. 776‚Äì783.
[Online]. Available: https://www.aclweb.org/anthology/P07-1098
[34] A. Di Sorbo, S. Panichella, C. V . Alexandru, J. Shimagaki, C. A.
Visaggio, G. Canfora, and H. C. Gall, ‚ÄúWhat would users change
in my app? summarizing app reviews for recommending software
changes,‚Äù in Proceedings of the 2016 24th ACM SIGSOFT International
Symposium on Foundations of Software Engineering , ser. FSE 2016.
New York, NY , USA: ACM, 2016, pp. 499‚Äì510. [Online]. Available:
http://doi.acm.org/10.1145/2950290.2950299
[35] S. Panichella, A. Di Sorbo, E. Guzman, C. A. Visaggio, G. Canfora, and
H. C. Gall, ‚ÄúARdoc: App Reviews Development Oriented ClassiÔ¨Åer,‚Äù in
Proceedings of the 2016 24th ACM SIGSOFT International Symposium
on Foundations of Software Engineering , ser. FSE 2016. New York,
NY , USA: ACM, 2016, pp. 1023‚Äì1027. [Online]. Available: http:
//doi.acm.org.udel.idm.oclc.org/10.1145/2950290.2983938
[36] P. Chatterjee, K. Damevski, N. Kraft, and L. Pollock, ‚ÄúSoftware-
related Slack Chats with Disentangled Conversations,‚Äù in Proceedings
of the 17th International Conference on Mining Software Repositories
(MSR‚Äô20) , May 2020.
[37] M. A. Bujang and N. Baharum, ‚ÄúA simpliÔ¨Åed guide to determination
of sample size requirements for estimating the value of intraclass
correlation coefÔ¨Åcient: a review.‚Äù Archives of Orofacial Science , vol. 12,
no. 1, 2017.
[38] J. R. Landis and G. G. Koch, ‚ÄúThe measurement of observer agreement
for categorical data,‚Äù biometrics , pp. 159‚Äì174, 1977.
[39] M. Elsner and E. Charniak, ‚ÄúYou talking to me? a corpus and algorithm
for conversation disentanglement,‚Äù in Proc. Association of Computa-
tional Linguistics: Human Language Technology , 2008, pp. 834‚Äì842.
[40] D. C. Uthus and D. W. Aha, ‚ÄúMultiparticipant chat analysis: A survey,‚Äù
ArtiÔ¨Åcial Intelligence , vol. 199, pp. 106‚Äì121, 2013.
[41] J. K. Kummerfeld, S. R. Gouravajhala, J. J. Peper, V . Athreya,
C. Gunasekara, J. Ganhotra, S. S. Patel, L. Polymenakos, and W. S.
Lasecki, ‚ÄúA large-scale corpus for conversation disentanglement,‚Äù
inProceedings of the 57th Annual Meeting of the Association for
Computational Linguistics , July 2019, pp. 3846‚Äì3856. [Online].
Available: https://www.aclweb.org/anthology/P19-1374.pdf
[42] P. Runeson, M. Host, A. Rainer, and B. Regnell, Case Study Research
in Software Engineering: Guidelines and Examples , 1st ed. Wiley
Publishing, 2012.
[43] M. Tan, B. Xiang, and B. Zhou, ‚ÄúLstm-based deep learning models for
non-factoid answer selection,‚Äù ArXiv , vol. abs/1511.04108, 2015.
[44] A. R ¬®uckl¬¥e and I. Gurevych, ‚ÄúRepresentation learning for answer
selection with LSTM-based importance weighting,‚Äù in IWCS 2017
‚Äî 12th International Conference on Computational Semantics ‚Äî Short
papers , 2017. [Online]. Available: https://www.aclweb.org/anthology/
W17-6935
[45] Z. Zhao, H. Lu, V . W. Zheng, D. Cai, X. He, and Y . Zhuang,
‚ÄúCommunity-based question answering via asymmetric multi-faceted
ranking network learning,‚Äù in Proceedings of the Thirty-First AAAI
Conference on ArtiÔ¨Åcial Intelligence , ser. AAAI?17. AAAI Press, 2017,
p. 3532?3538.
[46] Y . Kim, ‚ÄúConvolutional neural networks for sentence classiÔ¨Åcation,‚Äù in
Proceedings of the 2014 Conference on Empirical Methods in Natural
Language Processing (EMNLP) . Doha, Qatar: Association for
Computational Linguistics, Oct. 2014, pp. 1746‚Äì1751. [Online].
Available: https://www.aclweb.org/anthology/D14-1181
[47] L. Shi, M. Xing, M. Li, Y . Wang, L. Shoubin, and Q. Wang, ‚ÄúDetection
of Hidden Feature Requests from Massive Chat Messages via Deep
Siamese Network,‚Äù in Proceedings of the 42nd International Conference
on Software Engineering , ser. ICSE ‚Äô20. New York, NY , USA: ACM,
2020.
[48] A. Graves, A. Mohamed, and G. Hinton, ‚ÄúSpeech recognition with deep
recurrent neural networks,‚Äù in 2013 IEEE International Conference on
Acoustics, Speech and Signal Processing , 2013, pp. 6645‚Äì6649.
[49] Y . Xiang, Q. Chen, X. Wang, and Y . Qin, ‚ÄúAnswer selection in
community question answering via attentive neural networks,‚Äù IEEE
Signal Processing Letters , vol. 24, no. 4, pp. 505‚Äì509, 2017.
[50] S. Hochreiter and J. Schmidhuber, ‚ÄúLong short-term memory,‚Äù Neural
Comput. , vol. 9, no. 8, p. 1735?1780, Nov. 1997. [Online]. Available:
https://doi.org/10.1162/neco.1997.9.8.1735
[51] M. R. Islam and M. F. Zibran, ‚ÄúLeveraging automated sentiment analysis
in software engineering,‚Äù in Proceedings of the 14th International
Conference on Mining Software Repositories , ser. MSR ?17. IEEE
1271Press, 2017, p. 203?214. [Online]. Available: https://doi.org/10.1109/
MSR.2017.9
[52] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning,
A. Ng, and C. Potts, ‚ÄúRecursive deep models for semantic
compositionality over a sentiment treebank,‚Äù in Proceedings
of the 2013 Conference on Empirical Methods in Natural Language
Processing . Seattle, Washington, USA: Association for Computational
Linguistics, Oct. 2013, pp. 1631‚Äì1642. [Online]. Available:
https://www.aclweb.org/anthology/D13-1170
[53] C. J. Hutto and E. Gilbert, ‚ÄúVader: A parsimonious rule-based model
for sentiment analysis of social media text,‚Äù in ICWSM , 2014.
[54] W. Wu, X. Sun, and H. Wang, ‚ÄúQuestion condensing networks for
answer selection in community question answering,‚Äù in Proceedings
of the 56th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) . Melbourne, Australia: Association
for Computational Linguistics, Jul. 2018, pp. 1746‚Äì1755. [Online].
Available: https://www.aclweb.org/anthology/P18-1162
[55] Q. Ye, K. Misra, H. Devarapalli, and J. T. Rayz, ‚ÄúA sentiment based
non-factoid question-answering framework,‚Äù in 2019 IEEE International
Conference on Systems, Man and Cybernetics (SMC) , 2019, pp. 372‚Äì
377.
[56] L.-W. Ku, Y .-T. Liang, and H.-H. Chen, ‚ÄúQuestion analysis and
answer passage retrieval for opinion question answering systems,‚Äù in
International Journal of Computational Linguistics & Chinese Language
Processing, Volume 13, Number 3, September 2008: Special Issue on
Selected Papers from ROCLING XIX , Sep. 2008, pp. 307‚Äì326. [Online].
Available: https://www.aclweb.org/anthology/O08-5003
[57] F. Eskandari, H. Shayestehmanesh, and S. Hashemi, ‚ÄúPredicting best
answer using sentiment analysis in community question answering
systems,‚Äù in 2015 Signal Processing and Intelligent Systems Conference
(SPIS) , 2015, pp. 53‚Äì57.
[58] S. Moghaddam and M. Ester, ‚ÄúAqa: Aspect-based opinion question
answering,‚Äù in 2011 IEEE 11th International Conference on Data
Mining Workshops , 2011, pp. 89‚Äì96.
[59] M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, and I. H.
Witten, ‚ÄúThe weka data mining software: An update,‚Äù SIGKDD Explor.
Newsl. , vol. 11, no. 1, pp. 10‚Äì18, Nov. 2009. [Online]. Available:
http://doi.acm.org/10.1145/1656274.1656278
[60] F. Chollet et al. (2015) Keras. [Online]. Available: https://github.com/
fchollet/keras
[61] J. Bergstra and Y . Bengio, ‚ÄúRandom search for hyper-parameter op-
timization,‚Äù J. Mach. Learn. Res. , vol. 13, no. null, p. 281?305, Feb.
2012.
[62] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhut-
dinov, ‚ÄúDropout: A simple way to prevent neural networks from over-
Ô¨Åtting,‚Äù J. Mach. Learn. Res. , vol. 15, no. 1, p. 1929?1958, Jan. 2014.
[63] L. Prechelt, ‚ÄúEarly stopping - but when?‚Äù in Neural Networks: Tricks
of the Trade, volume 1524 of LNCS, chapter 2 . Springer-Verlag, 1997,
pp. 55‚Äì69.
[64] Y . Zhang and D. Hou, ‚ÄúExtracting problematic api features from
forum discussions,‚Äù in 2013 21st International Conference on Program
Comprehension (ICPC) , May 2013, pp. 142‚Äì151.
[65] Y . Huang, C. Chen, Z. Xing, T. Lin, and Y . Liu, ‚ÄúTell them
apart: Distilling technology differences from crowd-scale comparison
discussions,‚Äù in Proceedings of the 33rd ACM/IEEE International
Conference on Automated Software Engineering , ser. ASE 2018. New
York, NY , USA: Association for Computing Machinery, 2018, p.
214?224. [Online]. Available: https://doi.org/10.1145/3238147.3238208
[66] X. Ren, Z. Xing, X. Xia, G. Li, and J. Sun, ‚ÄúDiscovering, explaining
and summarizing controversial discussions in community q&a sites,‚Äù
inProceedings of the 34th IEEE/ACM International Conference on
Automated Software Engineering , ser. ASE ?19. IEEE Press, 2019, p.
151?162. [Online]. Available: https://doi.org/10.1109/ASE.2019.00024
[67] N. Novielli, F. Calefato, and F. Lanubile, ‚ÄúThe Challenges of Sentiment
Detection in the Social Programmer Ecosystem,‚Äù in Proceedings of the
7th International Workshop on Social Software Engineering , ser. SSE
2015. New York, NY , USA: ACM, 2015, pp. 33‚Äì40. [Online]. Avail-
able: http://doi.acm.org.udel.idm.oclc.org/10.1145/2804381.2804387
[68] Z. Xiong, P. Liang, C. Yang, and T. Liu, ‚ÄúAssumptions in oss develop-
ment: An exploratory study through the hibernate developer mailing list,‚Äù
in2018 25th Asia-PaciÔ¨Åc Software Engineering Conference (APSEC) ,
2018, pp. 455‚Äì464.
[69] M. Goul, O. Marjanovic, S. Baxley, and K. Vizecky, ‚ÄúManaging the
enterprise business intelligence app store: Sentiment analysis supportedrequirements engineering,‚Äù in 2012 45th Hawaii International Confer-
ence on System Sciences , 2012, pp. 4168‚Äì4177.
[70] L. Shrestha and K. McKeown, ‚ÄúDetection of question-
answer pairs in email conversations,‚Äù in Proceedings of the 20th
International Conference on Computational Linguistics , ser. COLING
‚Äô04. Stroudsburg, PA, USA: Association for Computational Linguistics,
2004. [Online]. Available: https://doi.org/10.3115/1220355.1220483
[71] G. Cong, L. Wang, C.-Y . Lin, Y .-I. Song, and Y . Sun, ‚ÄúFinding question-
answer pairs from online forums,‚Äù in Proceedings of the 31st Annual
International ACM SIGIR Conference on Research and Development in
Information Retrieval , ser. SIGIR ‚Äô08. New York, NY , USA: ACM,
2008, pp. 467‚Äì474. [Online]. Available: http://doi.acm.org/10.1145/
1390334.1390415
[72] S. Gottipati, D. Lo, and J. Jiang, ‚ÄúFinding relevant answers
in software forums,‚Äù in Proceedings of the 2011 26th IEEE/ACM
International Conference on Automated Software Engineering , ser. ASE
‚Äô11. Washington, DC, USA: IEEE Computer Society, 2011, pp. 323‚Äì
332. [Online]. Available: http://dx.doi.org/10.1109/ASE.2011.6100069
[73] S. Hen√ü, M. Monperrus, and M. Mezini, ‚ÄúSemi-automatically
extracting faqs to improve accessibility of software development
knowledge,‚Äù in Proceedings of the 34th International Conference on
Software Engineering , ser. ICSE ‚Äô12. Piscataway, NJ, USA: IEEE
Press, 2012, pp. 793‚Äì803. [Online]. Available: http://dl.acm.org/citation.
cfm?id=2337223.2337317
[74] R. Alkadhi, T. Lata, E. Guzmany, and B. Bruegge, ‚ÄúRationale in De-
velopment Chat Messages: An Exploratory Study,‚Äù in 2017 IEEE/ACM
14th International Conference on Mining Software Repositories (MSR) ,
May 2017, pp. 436‚Äì446.
[75] R. Alkadhi, M. Nonnenmacher, E. Guzman, and B. Bruegge,
‚ÄúHow do developers discuss rationale?‚Äù in 2018 IEEE
25th International Conference on Software Analysis, Evolution and
Reengineering (SANER) , vol. 00, March 2018, pp. 357‚Äì369. [Online].
Available: doi.ieeecomputersociety.org/10.1109/SANER.2018.8330223
[76] S. A. Chowdhury and A. Hindle, ‚ÄúMining StackOverÔ¨Çow to Filter
Out Off-Topic IRC Discussion,‚Äù in 2015 IEEE/ACM 12th Working
Conference on Mining Software Repositories , May 2015, pp. 422‚Äì425.
[77] E. Shihab, Z. M. Jiang, and A. E. Hassan, ‚ÄúStudying the Use of
Developer IRC Meetings in Open Source Projects,‚Äù in 2009 IEEE
International Conference on Software Maintenance , Sept 2009, pp. 147‚Äì
156.
[78] M. S. Elliott and W. Scacchi, ‚ÄúFree software developers
as an occupational community: Resolving conÔ¨Çicts and
fostering collaboration,‚Äù in Proceedings of the 2003 International
ACM SIGGROUP Conference on Supporting Group Work , ser. GROUP
‚Äô03. New York, NY , USA: ACM, 2003, pp. 21‚Äì30. [Online]. Available:
http://doi.acm.org/10.1145/958160.958164
[79] S. Panichella, G. Bavota, M. D. Penta, G. Canfora, and G. Antoniol,
‚ÄúHow developers‚Äô collaborations identiÔ¨Åed from different sources tell
us about code changes,‚Äù in 2014 IEEE International Conference on
Software Maintenance and Evolution , Sept 2014, pp. 251‚Äì260.
[80] E. Paikari and A. van der Hoek, ‚ÄúA framework for understanding
chatbots and their future,‚Äù in Proceedings of the 11th International
Workshop on Cooperative and Human Aspects of Software Engineering ,
ser. CHASE ‚Äô18. New York, NY , USA: ACM, 2018, pp. 13‚Äì
16. [Online]. Available: http://doi.acm.org.udel.idm.oclc.org/10.1145/
3195836.3195859
[81] C. Lebeuf, M. D. Storey, and A. Zagalsky, ‚ÄúHow Software
Developers Mitigate Collaboration Friction with Chatbots,‚Äù CoRR , vol.
abs/1702.07011, 2017. [Online]. Available: http://arxiv.org/abs/1702.
07011
1272