CoFI: Consistency-Guided Fault Injection for Cloud Systems
Haicheng Chen
Department of Computer Science and Engineering
The Ohio State University, United States
chen.4800@osu.eduWensheng Dou
State Key Lab of Computer Science, Institute of Software,
Chinese Academy of Sciences
University of Chinese Academy of Sciences, China
wsdou@otcaix.iscas.ac.cn
Dong Wang
State Key Lab of Computer Science, Institute of Software,
Chinese Academy of Sciences
University of Chinese Academy of Sciences, China
wangdong18@otcaix.iscas.ac.cnFeng Qin
Department of Computer Science and Engineering
The Ohio State University, United States
qin.34@osu.edu
ABSTRACT
Network partitions are inevitable in large-scale cloud systems. De-
spitedeveloper‚Äôseffortsinhandlingnetworkpartitionsthroughout
designing,implementingandtestingcloudsystems,bugscausedby
networkpartitions,i.e., partitionbugs,stillexistandcausesevere
failures in production clusters. It is challenging to expose thesepartition bugs because they often require network partitions to
startandstopat specific timings.
In this paper, we propose Consistency-Guided FaultInjection
(CoFI), a novel technique that systematically injects network parti-
tionstoeffectivelyexposepartitionbugs.Weobservethat,network
partitions can leave cloud systems in inconsistent states, where
partitionbugsaremorelikelytooccur.Basedonthisobservation,
CoFI first infers invariants (i.e., consistent states) among different
nodes in a cloudsystem. Once detecting violations to theinferred
invariants(i.e.,inconsistentstates)whilerunningthecloudsystem,CoFIinjectsnetworkpartitionstopreventthecloudsystemfromre-coveringbacktoconsistentstates,andthoroughlytestswhetherthecloudsystemstillproceedscorrectlyatinconsistentstates.Wehave
applied CoFI to three widely-deployed cloud systems, i.e., Cassan-
dra,HDFS,andYARN.CoFIhasdetected12previously-unknown
bugs, and four of them have been confirmed by developers.
CCS CONCEPTS
‚Ä¢Computer systems organization ‚ÜíCloud computing ;Reli-
ability;‚Ä¢Software and its engineering ‚ÜíSoftware testing and
debugging .
KEYWORDS
Cloud system, netwrok partition, fault injection, testing
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ASE ‚Äô20, September 21‚Äì25, 2020, Virtual Event, Australia
¬© 2020 Association for Computing Machinery.
ACM ISBN 978-1-4503-6768-4/20/09...$15.00
https://doi.org/10.1145/3324884.3416548ACM Reference Format:
Haicheng Chen, Wensheng Dou, Dong Wang, and Feng Qin. 2020. CoFI:
Consistency-GuidedFaultInjectionforCloudSystems.In 35thIEEE/ACM
International Conference on Automated Software Engineering (ASE ‚Äô20), Sep-
tember 21‚Äì25, 2020, Virtual Event, Australia. ACM, New York, NY, USA,
12pages.https://doi.org/10.1145/3324884.3416548
1 INTRODUCTION
Cloud systems are playing an increasingly important role in our
dailylife.AmajorityofFortune500companiesadoptcloudstorage
tohosttheirdata[ 6,11].Manypopularsocialmediawebsitesare
backed up by cloud services [ 15,27]. As a result, the dependability
ofcloudsystemshasbecomemoreimportantthanever.Whencloud
systemsfail,theconsequencesareusuallysevere.Forexample,a
three-hour AWS outage in 2017 led to a 150 million dollar loss
forS&P500companies[ 10].Asanotherexample,whenFacebook
service failed in 2014, many Los Angeles residents called 911 [5].
Cloud systems run on networked commodity machines, where
network partitions can occur as frequently as once a week and one
incidentmaylastforminutesorevenhours[ 2,17,19,40,41].There-
fore, dependable cloud systems must handle network partitionscorrectly.Unfortunately,thisisachallengingtasksincenetworkpartitions can happen to any node and can startandstop at any
time.Eventhoughdevelopersstrivetohandlenetworkpartitions
throughoutdesigning,implementing,andtestingacloudsystem,
network partitions can still lead to cloud system failures [2, 7].
Weuseathree-nodereplica-basedcloudsysteminFigure 1to
illustrate the typical process when a cloud system encounters a
networkpartition.Normally,differentnodesinacloudsystemkeep
their states consistent by exchanging messages (Phase 1). Whenanetworkpartitionstarts,thenodesonthedifferentsidesofthepartition become disconnected. This may occur when the three
nodes have consistent states (Phase 2). However, the two nodes on
the left side may change their states (e.g., serving a data updaterequest from the client), causing inconsistency between the two
sides(Phase2toPhase3).Similarly,anetworkpartitionmayoccur
when the left two nodes are already inconsistent with the right
node(Phase1toPhase3).Thisispossiblebecauseincloudsystemsnodestatesareupdatedasynchronously.Whenanetworkpartition
starts, cloud systems often have various built-in mechanisms to
recover from inconsistent states, e.g., repeatedly trying to connect
the partitioned nodes for recovery. Thus, cloud systems can still
5362020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE)

	 
	 
	
 
	
 
	
Figure1:Anetworkpartition‚Äôseffectonathree-nodecluster.
Each circle represents one node. Circles with the same pat-tern (i.e., solid, hollow, horizontal stripe, or vertical stripe)represent nodes that are consistent. In Phases 2 and 3, thelightnings represent network partitions, and the dotted ar-rows represent the message attempts failed by the networkpartition.
operate without the partitioned nodes. When the network heals,
thetwosidesofthepartitionmayhaveinconsistentstates(Phase4).
In this case, the cloud system will try to recover the whole cluster
back to a consistent state (Phase 5). In this paper, we refer to thecloud system bugs triggered by network partitions as partition
bugs. A partition bug can occur if the cloud system cannot handle
thenetworkdisconnectioninPhases2or3,orcannothandlethe
inconsistency caused by the network partition in Phases 3 or 4.
Fault injection is a common technique for testing cloud sys-
temsagainstadversarialconditions,e.g.,nodecrashesandnetwork
partitions. Many works inject node crashes when testing cloud
systems [ 1,29,30]. However, node crashes and network partitions
are fundamentally different. First, node crashes and network parti-
tionsoftenexercisedifferentrecoveryoperationsofacloudsystem.
Second, crashing a node will remove its in-memory state, while
partitioninganodewillnot.So,thesenode-crashinginjectiontech-
niquesareinapplicabletoexposepartitionbugsincloudsystems.
Recently,afewtoolshavebeenproposedtoinjectnetworkfailures
when testing cloud systems. For example, Namazu randomly drops
network packages with a configured probability [ 32]. However,
suchafaultmodelresemblesanunreliablenetwork,whichisdiffer-
entfromnetworkpartitions.NEAT[ 2]andJepsen[ 23]caninject
network partitions when testing cloud systems, but they rely on
developers to specify when a network partition starts and stops.
Inthispaper,wepropose Consistency-guided FaultInjection(CoFI),
an automated technique to expose partition bugs by systematically
injecting network partitions into cloud systems. We find that par-
titionbugsaremorelikelytooccurwhencloudsystemsarerunning at inconsistent states
(i.e., Phases 3 and 4 in Figure 1)
due to two main reasons. First, node communications at incon-sistent states are harder to reason about than those at consistent
states.Second,cloudsystemsstrivetorecoversystemstatesfrom
inconsistency as quickly as possible, leaving smaller time windows
fordeveloperstotestcloudsystemsatinconsistentstates.Basedonthisobservation,CoFI‚Äôsmainidea istoinject network partitionsto
thoroughly exercise cloud systems at inconsistent states.
Specifically,CoFIfirstemploysdistributedprograminvariants
[14,20] to represent the consistent states in a cloud system. A dis-
tributed program invariant (or invariant for short) is a propertythatmustholdwhenmultiplenodesareeachatcertainprogram
point.UsinginvariantstorepresentconsistentstatesallowsCoFIto
automatically identify consistent states in different cloud systems.!
  
 	" 
#





 $		" 
#
	" #!	
	
"	
		

#	

	
	
	

	
	 
		" 
#
Figure 2: The triggering process of CASSANDRA-2115 . This par-
tition bug can only be triggered when a network partition
starts and stops at specific timings.
Then,CoFImonitorsthecloudsystem‚Äôsruntimestatesandstarts
anetworkpartitionwhenaninconsistentstateoccurs,i.e.,anin-
variant is temporarily violated. Starting the network partition at
inconsistentstatescanpreventthecloudsystemfromrecovering
backtoconsistentstates,sothatCoFIcanthoroughlytestthecloud
system at inconsistent states. Finally, CoFI systematically explores
thestoppingpointofthenetworkpartitionbasedonmessagetypes.Hence,CoFIenablesmoreefficienttestingofthecloudsystemthan
exhaustively stopping the partition for every message.
WehaveimplementedaprototypeofCoFIandappliedittomulti-
pleversionsofthreepopularcloudsystems,namelyCassandra[ 35],
HDFS[39],andYARN[ 38].CoFIcansuccessfullydetect4known
bugs,and12 unknown bugsthathave neverbeenreportedbefore.
Moreover, the triggering processes of 10 out of these 16 bugs have
timingrequirementsonboththestartingandthestoppingpoints
of their network partitions. At the time of this writing, developers
have confirmed four of the 12 unknown bugs reported by CoFI.
In summary, we make the following contributions in this paper.
‚Ä¢We propose consistency-guided fault injection (CoFI), a novel ap-
proachtoexpose partitionbugsbysystematicallyinjecting net-
work partitions at inconsistent states of cloud systems. CoFI can
efficientlyexposepartition bugsthathavetiming requirements
on the starting and stopping points of network partitions.
‚Ä¢We implement a prototype of CoFI1, and evaluate CoFI using
multiple versions of three popular cloud systems, namely Cas-
sandra, HDFS, and YARN. CoFI detects 12 previously-unknown
bugs, and four of them have been confirmed by developers.
2 MOTIVATION AND CHALLENGES
In this section, we first motivate consistency-guided fault injection
(i.e., CoFI) with a real-world example. Then, we use the motivating
example to discuss how CoFI addresses its main challenges.
2.1 A Motivating Example
2.1.1 TheTriggeringProcess. Figure2showsthetriggeringprocess
ofCASSANDRA-2115 , a partition bug that manifests when two Cas-
sandranodesexchangegossipmessagesataspecificinconsistent
state. In a Cassandra cluster, each node obtains the status of itspeers through exchanging gossip messages. Updates to a node‚Äôs
statusareorderedusingvectorclocks.If C1,C2andC3arethethree
1https://hanseychen.github.io/CoFI/
537nodes in a cluster, both C2andC3will know that C1is running
normally(Step 1).Theclusterisnowataconsistentstate(Phase1
in Figure 1). When a user decommissions C1,C1will change its
status to ‚ÄúLeft‚Äù, increases its clock value (from t0tot1), and then
notifies its peers about the update. When C2receives the update
message,itwillmodifyitslocalcopyaccordingly(Step 2)sincethe
incomingmessagehasagreaterclockvalue( t1>t0).However,due
to a network partition, C3does not receive any message about the
update(Step 3).Asaresult, C3willfalselybelievethat C1isstill
runningnormally.Atthismoment,theclusterbecomespartitioned
andinconsistent(Phase3inFigure 1).Aftercertainamountoftime,
C2dropsC1‚Äôsstatus,togetherwiththeclockvalue(Step 4).Atthis
point, the network partition heals, allowing C2andC3to exchange
messages at the inconsistent state where C2forgets about C1while
C3thinksC1is running normally (Phase 4 in Figure 1). We use { ‚àÖ,
Normal} to denote this inconsistent state. C3then propagates to
C2an outdated value of C1‚Äôs status (Step 5). SinceC2now knows
nothingabout C1,itwillblindlyaccept C3‚Äôsvalue,eventhoughitis
outdated( t0<t1).Asaresult, C1reappearstoberunningnormally
after it has already been decommissioned!
2.1.2 TimingRequirementsonNetworkPartitions. Totriggerthis
bug,C2andC3need to exchange a gossip message at the inconsis-
tent state { ‚àÖ, Normal}. This requires the network partition to start
after Step 1and before Step 3, as well as to stop after Step 4and
before Step 5. First, ifthe network partition startsbefore Step 1,
C3willnotconsider C1asrunningnormally.Second,ifthenetwork
partition starts after Step 3or stops before Step 4,C2andC3will
eventuallyagreethat C1hasleft.Finally,ifthenetworkpartition
does not stop before Step 5,C2andC3will not exchange gossip
messages at state { ‚àÖ, Normal}. The bug will not be triggered if any
of the aforementioned situations happens. Such a complex timing
requirement on the network partition makes the bug difficult to be
exposed using random or developer-specified fault injection.
2.2 Challenges and Solutions
Totriggerthepartitionbuginourmotivatingexample,CoFIinjects
network partitions to thoroughly test the cloud system at inconsis-
tent states. CoFI needs to address the following three challenges.
2.2.1 Challenge 1: How to Represent and Decide Consistent States?
Theconsistencyofasystemstateiscloselyrelatedtothespecific
protocols that individual cloud system adopts. For example, Cas-sandra uses Paxos to replicate user data [
36] while HDFS uses
replication pipeline [ 39]. Moreover, even for the same protocol,
twocloudsystemsmayhavetheirownimplementationssuchasCassandra‚Äôs and Google Spanner‚Äôs Paxos implementations [
18].
CoFIemploys distributedprograminvariants [14,20]torepresent
theconsistentstatesinacloudsystem.Adistributedprogramin-
variant is a property that must hold when multiple nodes are each
atcertainprogrampoint.Foraselectedinvariant,acloudsystemstateisconsistentifitsatisfiestheinvariant.Otherwise,thestate
is inconsistent. We can use the following invariant to represent the
consistentstatesinourmotivatingexample,i.e., C2andC3agree
onC1‚Äôs status:
status[C1]@C2==status[C1]@C3FromFigure 2wecanseethat,ataconsistentstate,e.g.,Step 1,the
aboveinvariantissatisfied.Conversely,ataninconsistentstate,e.g.,
Step2, the above invariant is violated. Since the invariants can be
automatically extracted from cloud systems, such consistent states
can be easily applied to different protocols and implementations.
2.2.2 Challenge 2: When to Start a Network Partition? An intuitive
idea is to start a network partition at every point during the sys-tem execution since it simulates the real-world scenario that thenetwork can be partitioned at any time. However , this approach
isimpracticalduetoextremelyhighoverhead.Inourmotivating
example, Step 3alone lasts for more than one minute, containing
toomanyexecutionpointstoexplore.Instead,CoFIinjectsanet-
workpartitionassoonasitdetectsaninconsistentstateatruntime.
Forexample,usingtheinvariantin¬ß2.2.1 torepresentconsistent
states, CoFI will start the network partition after C2updates its
localcopyof C1‚Äôsstatusto‚ÄúLeft‚Äù(Step 2)andbefore C3updates
itslocalcopyof C1‚Äôsstatusto‚ÄúLeft‚Äù.Toprovidemaximumchances
ofexposingpartitionbugs,CoFIinjectsnetworkpartitionsatthe
messagelevel(insteadofattheuseroperationlevelasemployedby
other tools [ 2,23]) by failing the message exchanges among nodes.
Therefore,thegossipmessagesatStep 3willbefailed,keeping C2
andC3inconsistent.
2.2.3 Challenge3:WhentoStopaNetworkPartition? Toexercisea
cloudsystematinconsistentstates,onecantrytostopthenetwork
partition at every execution point (i.e., enabling message exchange
before each message is sent). However, this approach of exhaus-
tivelysearchingallpossiblepointstostopthenetworkpartitionhas
very high overhead. For instance, Step 3alone consists of more
than 100 gossip messages. Trying to stop the network partition
before each of them will be inefficient for exposing our motivating
bug.Toaddressthisissue,CoFIclassifiesmessagesintodifferent
typesandsystematicallyexploresthetimingofstoppinganetwork
partition for each type of messages, instead of each message. Af-ter the classification, the gossip messages at Step
3are grouped
into only a few types, drastically reducing the number of stopping
points to explore.
3 CONSISTENCY-GUIDED FAULT INJECTION
Inthissection,wefirstdiscussCoFI‚Äôsfaultmodel.Then,weexplain
CoFI‚Äôs workflow and explain its major steps.
3.1 Fault Model
CoFItestsacloudsystembyinjectingaperiodof temporary network
partition to onenode in the system. Here, ‚Äútemporary‚Äù means that
a started network partition will stop at certain point. Note thatstarting a network partition at inconsistent states only tests thecloud system at Phase 3 in Figure 1. To thoroughly test a cloud
system at inconsistent states, CoFI also stops the network partitiontotestthecloudsystematPhase4,i.e.,exchangingmessagesamong
inconsistent nodes.
Thespecificsofourfaultmodelareasfollows.First,inatestrun,
only one node will be partitioned. Second, the network partitioncanstartandstopatanytime(controlledbyCoFI),butCoFIwill
onlystart andstop thenetworkpartition onceper testrun.Third,
duringthenetworkpartition,allthemessagesbeingsentfromor
538 

!	
 
  
"!
 
 
Figure3:CoFI‚Äôsworkflow.Thetwosolidboxesrepresentthe
two stages of CoFI‚Äôs workflow. The three dotted boxes rep-resent the configurable inputs to each stage.
deliveredtothepartitionednodewillbefailed.CoFIfocusesonthis
simple network partition model because it is realistic, and more
importantly, cloud systems are expected to correctly handle such a
simplefaultmodelattheminimum.ItisworthnotingthatCoFIcanbeextendedtotestmorecomplicatedfaultmodels,e.g.,partitioning
multiple nodes and simplex partition [2]. We leave them as future
work.
3.2 CoFI in A Nutshell
Figure3presentsanoverviewofCoFI‚Äôsworkflow.CoFIworksin
two stages: invariant mining andfault injection. In the invariant
mining stage, CoFI first runs the cloud system using the workload
andrecordstheruntimevaluesoftheinterestingvariables.Then,
CoFI mines distributed program invariants from the recorded vari-
ablevalues.Theminedinvariantswillbeusedtoguidestartingandstoppingnetworkpartitionsinthefaultinjectionstage.Specifically,
for each invariant, which represents consistent system states, CoFI
systematically explores the scenarios of network partitions thatstart at an inconsistent state where the invariant is violated and
stop at a later execution point. During each testing run, CoFI uses
thecheckertodetectincorrectsystembehaviors,e.g.,systemdown.
When the checker fails, CoFI will generate a detailed bug report
to help developers diagnose the failure. The bug report containsinformation about the executed workload, the failure symptom,
the runtime values of the invariant-related variables, as well as the
messages failed by the network partition.
3.3 Specifying Interesting Variables
3.3.1 Which Variables are Interesting? In the invariant mining
stage, CoFI mines distributed program invariants based on the
runtime values ofsome interesting variables.Which variables are
interesting? We observe that two categories of variables can bet-
terrepresentthestateofacloudsystem,namely systemmetadata
(e.g., the status of a Cassandra node as shown in our motivating
example) and user metadata (e.g., the location of a container in
YARN).Thismeta-information iscriticalbecauseanythingwrong
with this meta-information may seriously affect the reliability of a
cloudsystem.Moreover,aninterestingvariableshould havedata
flow from or to the network so that CoFI can exercise the cloud sys-
tem in inconsistent states by controlling the timing of the network
partition. For instance, the status[C1]variable in our motivating
example is a system metadata that has data flow to and from thenetwork. By starting the network partition when
C2andC3are
inconsistent on status[C1](Step3in Figure 2) and stopping the
network partition after C2removes its status[C1](Step4), CoFI
triggers the bug.1// NameNode's status on NameNode.
2NameNode.instance.state: NameNode_Status
3// NameNode's status on DataNode.
4DataNode.instance.bpManager.bpByNameserviceId.bpServices.state:
NameNode_Status
Figure 4: Two interesting variables in HDFS.
3.3.2 How to Specify Interesting Variables? Since CoFI may access
aninterestingvariableoutsideofitsscope,werepresentaninterest-ingvariableusingthepathtoaccessthevariablefromaJava
static
field(Java‚Äôsglobalvariable).Werefertothesepathsas accesspaths.
Figure4lists two interesting variables(i.e., two accesspaths) that
refertothesamemetadata inHDFS.Specifically,thesetwovariables
bothstorethestatusofaNameNode,i.e., whethertheNameNode
isactiveorstandby.Whenspecifyinganinterestingvariable,one
should specify the access path followed by the metadata stored in
thevariable.Let‚Äôsusethefirstinterestingvariable(Line2)tofurther
explain how theaccess path works. NameNode.instance is astatic
field that refers to a NameNode object, and stateis the NameN-
ode object‚Äôs instance field that stores the NameNode‚Äôs status. At
run time, CoFIaccesses this interesting variableby first accessing
theNameNode.instance object and then accessing the statefield of
thatNameNode.instance object.Developerscanprovidetheirown
interesting variablesto represent systemstates, customizing CoFI
totestthestatestheyareinterestedin.Theeffortneededtospec-
ifyaninterestingvariabledependsontheimplementationdetails
involved in the access path. For example, it is straightforward toderive the first access path in Figure 4because it matches with
‚Äúthe NameNode‚Äôs status‚Äù. Conversely, specifying the second access
pathrequirestheknowledgethataDataNodestoresinformation
about the NameNodes in the bpManager field. The metadata after
eachaccesspathisauser-definedidentifier,whichhelpsCoFIselect
interesting invariants in the invariant mining stage (¬ß3.4.3).
3.4 Invariant Mining
In the invariant mining stage, CoFI first runs the cloud system
and records the interesting variables‚Äô values at the program points
thatlikely reflectconsistentsystem states.Then,CoFI groupsthe
values recorded on different nodes to reconstruct the consistent
states, from which invariants are mined. Finally, from the mined
invariants,CoFIselectsthe interestingones toguidefaultinjection
in the next stage.
3.4.1 Which Program Points to Collect Variable Values? To derive
consistentstatesinacloudsystem,CoFIminesdistributedprogram
invariants from the interesting variables‚Äô runtime values at certain
programpoints.Theconventionistochooseprogrampointslike
functionentrances,functionexits,andloopentrances[ 14].How-
ever,valuesattheseprogrampointsmayreflecttheintermediate
resultsofanode‚Äôslocalcomputation,whichdonotrepresentthesys-
tem‚Äôs consistent states. Instead, CoFI selects program points right
beforeamessageissent( before-sendprogrampoints )andright
afteramessageishandled( after-handleprogrampoints ).Specif-
ically,CoFIconsiderstheentranceofamessage-sendingmethodas
539#

" #	
		
$$ 
%!$

&	 '	
		
(	
)! 
		
 	
 

Figure5:Apartialexecutionof C2andC3ifthenetworkpar-
tition in Figure 2does not occur.
abefore-sendprogrampoint,andconsiderstheexitofamessage-
handling method as an after-handle program point. A before-send
program point reflects the sender node‚Äôs state when it has applied
somechangeslocally,andisreadytopropagatesuchachangetoits
peers.Similarly,anafter-handleprogrampointreflectsthereceiver
node‚Äôs state when it has finished updating its local state according
toareceivedmessage.Therefore,thesenderandthereceiverofthe
samemessageareusuallyconsistentatthepairoftheseprogram
points.
Figure5illustratessuchapropertyusingourmotivatingexample.
Itshowsapartialexecutionof C2andC3ifthenetworkpartition
does notoccur and C2sends agossip message to C3at Step 3in
Figure2. At the before-send program point (Line 3 in Figure 5),
C2has updated its copy of C1‚Äôs status (Line 2). At the after-handle
program point (Line 8), C3has also updated its copy of C1‚Äôs status
accordingtothegossipmessagefrom C2(Line7).Asaresult, C2and
C3are consistent about C1‚Äôs status at this pair of program points.
3.4.2 How to Associate Values from Different Nodes? After col-
lectingvariablevaluesatthebefore-sendprogrampointsandthe
after-handleprogrampoints,CoFIneedstogroupthesevaluestore-constructtheconsistentstates.Sincethesenderandthereceiverof
a message are usually consistent at the corresponding before-send
and after-handle program points, CoFI groups the variable values
at the pair of these program points to reconstruct the consistent
state. For the example in Figure 5, CoFI groups C2‚Äôs variable values
at Line 3 and C3‚Äôs variable values at Line 8 for the same gossip
message to reconstruct the state. The constructed state containsall the variables logged at the two before-send and after-handle
programpointinstances.CoFIconsidersthestateoftwonodes,i.e.,
pair-wise state, instead of the state of all the nodes in the cluster
for two reasons. First, many properties of distributed protocols canbecapturedinpair-wisestates.Forinstance,forCassandra‚Äôsgossipprotocol,twonodesshouldbeconsistentaftertheyhaveexchangedgossips (e.g., the invariant in ¬ß2.2.1). Second, even when a property
involves more than two nodes, breaking the sub-property between
anypairoftheinvolvednodeswillbesufficienttoviolatethewhole
property. For example, in HDFS‚Äôs replication protocol, all the repli-
casshouldhavethesamedatawhenawritesucceeds.Ifanypair
of replicas have different data, the whole property no longer holds.
When the interesting variable is status[C1]in Figure 5, grouping
C2‚Äôs value at Line 3 and C3‚Äôs value at Line 8 allows CoFI to mine
the desired invariant, i.e., the invariant in ¬ß2.2.1.
3.4.3 WhichInvariantsareMoreInteresting? Afterconstructingthe
pair-wisestates,CoFIemploysDaikon[ 14]toperformtheactual
invariantmining.Bydefault,Daikoncanminemanyinvariants[ 20].Algorithm1: Runningfaultinjectiontestsforaninvariant.
Input:invariant, workload, checker
1iStates := runWithoutPartition(invariant)
2whileiState := iStates.next() do
3foriNd‚ààiState.inconsistentNodes do
4 passedNewMsgType := true; allMsgTypes := {}
5 whilepassedNewMsgType do
6 (newIStates, passedNewMsgType) :=
runWithPartition(iState, allMsgTypes)
7 iStates.add(newIStates)
8Function runWithPartition(iState, allMsgTps) do
9workload.start()
10partition := WAITING; passedNewMsgTp := false
11whileworkload.isRunning do
12 ifpartition = WAITING ‚àßcurState = iState then
13 partition := STARTED
14 ifpartition = STARTED ‚àßcurMsg /nelementallMsgTps then
15 allMsgTps.add(curMsg)
16 passedNewMsgTp := true; partition := STOPPED
17ifchecker.failed then
18 generateBugReport()
19return(getNewInconsistentStates(),passedNewMsgTp)
Exploring all these invariants can be very time-consuming. There-
fore, CoFI further prunes the mined invariants based on a few
heuristics,allowingdeveloperstorunmoreinterestingtestswithin
a limited budget.
First, CoFI only selects invariants that involve multiple (i.e., two
in our setting) nodes. Cross-node invariants capture the consistent
statesofdifferentnodes.Whentheyareviolated,theinvolvednodes
are inconsistent. Second, CoFI removes invariants among variables
thatrefertodifferentmetadatabecauseitmaynotbemeaningfulto
comparetwodifferentmetadata.Recordthat,whenspecifyingan
interestingvariable,developersalsospecifythemetadatareferred
to by the variable (¬ß3.3.2). If an invariant involves variables thatrefer to different metadata, the invariant will be disregarded. By
default,Daikonminesmanytypesofinvariants,e.g.,theequality
ofvariablevalues(e.g., vara==varb+varc),andthemembership
relation between two variables (e.g., varelmnt‚ààvarset). CoFI
focuses on equality invariants since it is usually easier to violate
theseinvariants,i.e.,tocreateinconsistentstates,duringthetest
runs. Since theinvariant for triggering CASSANDRA-2115 asserts the
equality of the same metadata on two nodes, CoFI will select it.
Note that, CoFI‚Äôs methodology allows using any invariant to
represent consistent states, pruning out less interesting invariants
improves the test efficiency.
3.5 Fault Injection
Inthefaultinjectionstage,CoFIconductsmultipletestrunsforeach
mined invariant, as shown in Algorithm 1. More specifically, for
eachinvariant,CoFIfirstrunsthecloudsystemwithoutinjecting
networkpartitiontorecordpossibleinconsistentstatesforstarting
540the network partition in the later runs (Line 1). Then, CoFI iter-
ates over each inconsistent state as the starting point of a network
partition (Line 2). For each inconsistent state, CoFI also explores
partitioning different inconsistent nodes (Line 3). For each <incon-
sistentstate,partitionednode>pair,CoFIrepeatedlyrunsthecloud
system to systematically explore different scenarios of network
partitions described as follows.
In each test run for exploring one scenario of a network parti-
tion, CoFI first starts the workload (Line 9) and monitors the cloud
system‚Äôsruntimestate(representedbytheinvariant-relatedvari-
ables).Oncethesystemreachestheselectedinconsistentstateof
thecurrentrun,CoFIstartsthenetworkpartitionbystoppingfu-
turemessagessendingfromordeliveringtothepartitionednode
(Lines12-13).CoFIsystematicallyexploresdifferentnetworkpar-
tition stopping points by stopping the network partition before
different types of messages.
Foreachmessagetype,CoFIexplorestwopossiblecases:stop-
ping the network partition or maintaining the network partition
atthispoint.Specifically,afterthenetworkpartitionstarts,CoFI
intercepts everymessage thatis sendingfrom ordelivering tothe
partitioned node. If having not seen the type of an interceptedmessage, CoFI explores the scenario of stopping the partition atthis point, which allows the current and future messages to pass
(Lines 14-16). Otherwise, CoFI maintains the network partition by
dropping the message. CoFI simulates message drop at the applica-
tion level instead of the OS level. More details will be explained in
¬ß4.Attheendofeachtestrun,ifthecheckerfails,CoFIgeneratesa
bug report for the failure (Lines 17-18). CoFI terminates exploring
the scenarios of the network partition for the pair of <inconsistent
state, partitioned node> if there is no new message type encoun-
teredinthelatestrun(Line5),whichmeansCoFIhasexploredboth
passing and failing all the message types.
Due to the non-determinism in the cloud system‚Äôs execution,
someinconsistentstatesmaynotoccurinthefirstpartition-free
run but in the later runs. Therefore, CoFI continues collecting new
inconsistentstates ineach testrun(Line 19).The newlycollected
inconsistentstateswillbeusedasthestartingpointofthenetworkpartitioninthesuccessiveruns(Line7).Itisalsopossiblethatsome
inconsistent states CoFI initially collected may not occur in the
latertestruns.Toaddressthisissue,CoFIwillretrythetestruns
multiple times (a configurable parameter) for the inconsistent state.
3.5.1 IdentifyingInconsistentStates. Toidentifythepossibleincon-
sistentstatesduringthepartition-freerun(Line1inAlgorithm 1)
and the runs with network partitions (Line 6 in Algorithm 1), CoFI
monitorstheruntimevaluesoftheinterestingvariables.Specifically,
CoFI synchronously collects the variable values at the before-send
program points and the after-handle program points. By collecting
values at the after-handle program points, CoFI can capture the
state change that is caused by a node handling a state-changing
message. As shown in our motivating example, after C2handles
C1‚ÄôsgossipmessageatStep 2inFigure 2,CoFIwillimmediately
know that status[C1]@C2has become ‚ÄúLeft‚Äù. Sometimes, a state
change is not caused by the node handling a message. For example,
Step4in Figure 2happens after a timeout (Line 1 in Figure 2). To
capture these state changes, CoFI collects the variable values at
the before-send program points. Since cloud systems often employaheartbeatmechanism,collectingvariablevaluesatbefore-send
program points helps CoFI periodically refresh its copy of a node‚Äôs
state. When C2tries to send a gossip after Step 4in Figure 2, CoFI
will realize that status[C1]@C2has become ‚àÖ.
3.5.2 ClassifyingMessages. WhenCoFIfailsamessageduringa
network partition, the cloud system can react in two ways: Thecloud system can simply retry sending the message, or initiate a
different protocol to perform recovery (e.g., Cassandra will initiate
hinted-handoff when a data replication message fails). Stoppingthe network partition for retried messages is unnecessary since
exchanging the same type of messages will not exercise the cloud
system differently. However, stopping the network partition in the
second scenario can test if the alternative protocol will proceed
correctly at inconsistent states.
Basedonthisobservation,CoFIsystematicallyexploresstopping
the network partition before each typeof messages (Lines 14-16
in Algorithm 1), instead of each message. An ideal message type
shouldcorrelatewiththecodesegmentsthatwillbeexecutedwhensendingorhandlingthemessage.Inthisway,stoppingthenetwork
partition before different types of messages may exercise different
code segments in the cloud system.
CoFIrepresentsthemessagetypeusingthequad< stack,sender,
receiver,state>, where stackis the runtime call stack of the mes-
sagesendingmethod, senderandreceiver arethesenderandthe
receiverofthemessage,and stateisthesystemstate(i.e.,thevalues
oftheinvariant-relatedvariables)whenthemessageissent.The
messagetypeincludesthesendercallstackofamessagebecauseit
reflects the execution path on the sender side. Moreover, messages
sent at different call stacks usually belong to different protocols
(e.g., Cassandra‚Äôs gossip and hinted-handoff protocols), or differ-
entsteps ofthe sameprotocol (e.g., thecommit-requeststep and
thecommitstepofatwo-phasecommitprotocol[ 42]).Therefore,
handlingthesemessageswillexecutedifferentcodesegmentson
the receiver side. The other three elements in the message type
also correlate with the code segments that will be exercised. In our
motivating example, the following three types of messages execute
different code segments at the receiver side:
Type 1: <stack gossip,C2,C3, {Left, Normal}>
Type 2: <stack gossip,C3,C2, {Left, Normal}>
Type 3: <stack gossip,C3,C2,{‚àÖ, Normal}>
Specifically, a Type 1 message will trigger the code that checks the
message‚Äôs vector clock and updates the receiver‚Äôs state, a Type 2message will exercise the code that checks the message‚Äôs vector
clock and discards the message, and a Type 3 message will execute
thecodethatblindlyacceptsthevalueinthemessage.Forexample,
all the gossip messages that C2sends toC3at Step 3in Figure 2
belong to Type 1. As a result, CoFI will not redundantly try to
stopthenetworkpartitionbeforeeachoftheseequivalentgossip
messages.
3.6 Workload and Checker
Workloads drive CoFI to exercise a target cloud system. They
cancomefromvarioussourcesrangingfromsimpleunitteststocarefully-crafted test cases for stress testing. Although CoFI can
bedrivenbydifferentworkloads,CoFIismosteffectivewhenthe
workload includes cross-node operations that repeatedly read and
541write the interesting variables in different ways. With such a work-
load, CoFI can explore more network partition scenarios. For each
of our tested cloud system, we implement a few such workloads
usingcommonadminoperations(e.g.,ResourceManagerfailover
in YARN) and user operations (e.g., file movement in HDFS).
Moreover, developers can flexibly implement checkers to assert
thesystempropertiestheycareabout.Weprovidedefaultcheckers
inCoFI,oneperworkload.Specifically,ourcheckerscheckforbothgeneralfailures(i.e.,FATALentries,ERRORentries,andexceptions
inexecutionlogs,aswellasnodecrashes)andoperation-specific
failures (e.g., returning error code and reading stale data).
4 IMPLEMENTATION
CoFIhasthreecomponents:aninstrumentationengine,aninvariantminingengine,andafaultinjectionengine.Theinstrumentationen-gineinstrumentsthecloudsystemtoenablereadingtheinteresting
variablesatruntimeaswellasinterceptingmessagesendingand
message handling method calls. The invariant mining engine runs
thecloudsystemandminesdistributedprograminvariantsfrom
the valuesrecorded bythe instrumentedcode. Thefault injection
enginerunstheworkloadandthecheckeronthecloudsystemand
interacts with the instrumented code to inject network partitions.
4.1 Code Instrumentation
We build CoFI‚Äôs instrumentation engine using Javassist [ 8], a Java
bytecodeinstrumentation toolkit.To enableaccessinginteresting
variables at run time, the instrumentation engine adds a getter
method for each field in the target system. To intercept messagesending method calls, the instrumentation engine adds a call to
CoFI‚ÄôsbeforeSend() API at the beginning of each message sending
method(i.e.,eachbefore-sendprogrampoint).Similarly,tointer-
ceptmessagehandlingmethodcalls,theinstrumentationengine
adds a call to CoFI‚Äôs afterHandle() API at the end of each message
handlingmethod(i.e.,eachafter-handleprogrampoint).Weinte-
grate CoFI with the knowledge of the message sending methods
and the message handling methods in popular cloud systems, e.g.,
sendOneWay() in Cassandra. Developers can configure CoFI to in-
strumentdifferentmessage-passingmethods.Insidethemessage
sendingmethods,theinstrumentationenginealsoaddscodetosim-
ulatethenetworkpartition‚Äôseffectonthelocalnode.Inthefault
injection stage, this code will be executed when the fault injection
enginedecidestofailthemessage.Developerscanalsoconfigure
the effect of the network partition. By default, the instrumented
code throws an IOException .
BothbeforeSend() andafterHandle() takethreeparameters:the
sender of the message, the receiver of the message, and the classof the message. All three parameters are used to generate an ID
for themessage in theinvariant miningstage. The senderand the
receiver parameters are also sent to the fault injection engine to
decide the message type during the fault injection stage.
4.2 Invariant Mining
Intheinvariantminingstage, beforeSend() andafterHandle() per-
form similar tasks: Both APIs first record the interesting variables‚Äô
values through calling the getter methods. Then, the APIs generate
anIDforthemessage-to-sendorthehandledmessage.Finally,theyassociatethevariablevalueswiththemessageIDandwritethem
to a log, from which the invariant mining engine mines invariants.
Note that CoFI needs to pair the before-send and after-handle
program points of the same message to reconstruct a system state.
Toidentifythesamemessageonthesenderandreceiversides,CoFI
makes two assumptions: (1) Each communication channel between
twonodesisFIFO;(2)Messagesofthesameclassgothroughthe
samechannel.Takeourmotivatingbugasanexample,underthese
two assumptions, the first gossip message that C2sends toC3is
the first gossip message that C3receives from C2. All of our tested
systems satisfy these two assumptions. With these assumptions,CoFI constructs the message ID to be the concatenation of the
message‚Äôssender,receiver,class,andthecounter i.Inthisway,a
message will have the same ID on the sender and receiver sides.
Whenmininginvariants,theinvariantminingenginefirstgroups
the variable values at the before-send program point and the after-
handle program point of the same message to form a system state.
Theenginethenconcatenatesthestatesofthesameprogrampoint
pair(i.e.,samesender,samereceiver,andsamemessageclass)to
formatraceofthestatesforthatprogrampointpair.Afterwards,
theenginerunsDaikon[ 14]onthetracestomineinvariants.Finally,
the mined invariants are pruned based on the rules in ¬ß3.4.3.
4.3 Fault Injection
Inthefaultinjectionstage,the beforeSend() andtheafterHandle()
APIsrecordtheruntimevaluesoftheinvariant-relatedvariables
and report them to the fault injection engine. The beforeSend()
API also reports the pending message sending event to the fault
injectionengine,andwaitsfortheengine‚Äôsdecisiononwhetherthe
message should be failed. If the engine decides to fail the message,
thebeforeSend() API will return a false, triggering the execution
of the instrumented code in its caller (i.e., the message sendingmethod) to simulate the network partition, e.g., by throwing an
IOException to signal the caller about the network partition, or by
returning from the message sending method to simulate a silent
message drop.
5 EVALUATION
Ourevaluationaimstoanswerthreeresearchquestions:(1)Howef-fectiveisCoFIindetectingpartitionbugsincloudsystems?(2)How
does CoFI compare with other approaches for injecting network
partitions?(3)HowefficientisCoFI?Weperformourevaluationus-
ing a CloudLab [ 12] machine that runs Ubuntu 16.04. The machine
has 40 Xeon¬Æ E5-2660 processors and 157 GB memory.
5.1 Experimental Methodology
5.1.1 Target Cloud Systems. We select three widely-used open-
sourcecloudsystemsasourexperimentsubjects,i.e.,Cassandra[ 35],
HDFS [39], and YARN [ 38]. They represent different kinds of cloud
systems. First, they provide different functionalities: Cassandrais a distributed NoSQL database, HDFS is a distributed file sys-
tem,andYARNisadistributedcomputingframework.Moreover,
these systems adopt differentsystem architectures: Cassandra is a
peer-to-peersystemwhileHDFSandYARNarecoordinator/worker
systems.Finally,tocombatnetworkpartitions,thesesystemsim-
plementdifferentrecoverymechanisms,e.g.,HDFSemploysdata
542Table1:Experimentalsettingsforthetargetsystems.‚ÄúVar#‚Äùshowsthenumbersoftheinterestingvariablesforeachsystem.
System Operations in the Workloads Interesting Metadata Var #
Cassandra-3.11.5Create keyspace/column family, read/write data,Node status, node token, keyspace name 3add/remove column, decommission node
HDFS-3.3.0 Read/write file, move file/directory, DataNode ID, NameNode ID, NameNode status, 9
HDFS-2.10.0 failover NameNode, failover DataNode data block ID, data block location 9
YARN-3.3.0 Launch/stop application, failover NodeManager, NodeManager ID, container ID, container location, 9
YARN-2.10.0 failover ResourceManager ResourceManager ID, ResourceManager status 9
Table 2: The known bugs used to evaluate CoFI. ‚ÄúS‚Äù shows
whether stopping a network partition is needed to trigger
thebug.‚ÄúOperations‚Äùshowstheoperationsintheworkload
for exposing each bug.
BugID SOperations Interesting Metadata
CASSANDRA-3975 /checkWrite data, drop table Columnfamily name
CASSANDRA-2115 /checkDecommission node Node status
HDFS-14372 √óShutdown DataNode DataNode ID
YARN-4288 √óStartcluster NodeManager ID
re-replicationtorecoverinconsistentuserdata[ 39],andCassandra
uses gossip to recover inconsistent system metadata [13].
5.1.2 Detecting Partition Bugs. To evaluate CoFI‚Äôs effectiveness,
weapplyCoFItoourtargetsystemsandcheckifCoFIcandetect
both known bugs and unknown bugs.
Detecting KnownBugs. First, we collect several known parti-
tionbugsbyinspectingtherecentlypublishedbugdatasets[ 2,7,25,
30].Ifabugsatisfiesthefollowingrequirements,weselectittoeval-
uate CoFI: (1) It happens in our target systems. (2) It only requires
partitioningonenodetotrigger.(3)Wecanmanuallyreproducethe
bug. Finally, we obtained fourpartition bugs, as shown in Table 2.
These four known bugs cover all three target systems, and have
differenttimingrequirementsonthenetworkpartition(Column S).
Table2alsoshowstheoperationsineachbug‚Äôsworkloadandthe
metadatastoredintheinterestingvariableswhichwespecifyfor
eachbug.Theoperationsareextractedfromthebugreports.The
interesting variables are identified through understanding each
bug‚Äôs triggering process.
DetectingUnknownBugs. ToevaluateCoFI‚Äôseffectivenessin
detectingunknownbugs,weapplyCoFItotestthelatestversionsofourtargetsystems.ForHDFSandYARN,wetestboththeirversion
2 and version 3 since these versions are both widely deployed and
underactivedevelopment.ForCassandra,weonlytestitsversion3
sincethelatestminorreleaseofitsversion2(Cassandra-2.2)will
nolongerbesupportedafterCassandra‚Äôsnextmajorrelease[ 37].
Table1lists the selected system versions.
Wedesign several workloads for each target systemusing the
commonuserandadminoperationsasshowninTable 1.ForHDFS
and YARN, we use the same set of operations for both of their
versions. The operations in each workload follow natural order,
e.g., create a table before writing data to it. For Cassandra-3, we
implementfourworkloadsonathree-nodeclustertotestregular
data access, Paxos data access, schema update, and node decom-
mission. For both HDFS-2 and HDFS-3, we design three workloadstotestfilesystemoperations,NameNodefailover,andDataNode
failover. Both the file system operations workload and the NameN-
odefailoverworkloadrunonaclusteroftwoNameNodesandthreeDataNodeswhiletheDataNodefailoverworkloadrunsonacluster
with two NameNodes and four DataNodes. For YARN-2, we create
three workloads: Run a YARN application in a cluster of one Re-
sourceManager and one NodeManager; ResourceManager failover
inaclusteroftwoResourceManagersandoneNodeManager;Node-
Manager failover in a cluster of two ResourceManagers and two
NodeManagers. For YARN-3, we build two workloads: ResourceM-
anager failover when a YARN application is running in a cluster
oftwoResourceManagersandoneNodeManager;NodeManager
failover when a YARN application is running in a cluster of two
ResourceManagers and two NodeManagers. Moreover, each YARN
cluster also runs on top of an HDFS cluster with one NameNode
and one DataNode.
Ourcheckerscheckforbothgeneralfailures(i.e.,FATALentries,
ERRORentries,andexceptionsinexecutionlogs,aswellasnode
crashes)andoperation-specificfailures(e.g.,returninganerrorcodeandreadingstaledata).Toexploremorenetworkpartitionscenarios,
we limit at most 100 fault injection runs for each invariant.
Table1alsoshowsthemetadatastoredintheinterestingvari-
ablesthatwespecifyforeachtargetsystem.ForCassandra-3,we
specify theinteresting variable thatstores keyspace nameinstead
of column family name as for triggering CASSANDRA-3975 . This is
because keyspace names are accessed more often than column
family names (to access a column family, one needs to first access
the owner keyspace), potentially exposing more system behaviors
when two nodes are inconsistent on a keyspace name. To enable
accessingtheinterestingvariablesinHDFSandYARN,weaddtwo
staticfields to each version of HDFS and three staticfields to
each version of YARN to refer to the objects of the main compo-
nents in the system, i.e., NameNode, DataNode, ResourceManager,
NodeManager, andApplicationMaster.Intotal,this onlyinvolves
modifying22linesofcodeforallfoursystemversions.Themanualeffortsforspecifyingtheinterestingvariablesareacceptable:Oneofour authors specified all the interesting variables and implementedallthemodificationsinthetargetsystemsinafewhours,evenifheonlyhasabasicunderstandingofthesesystems.Forthedevelopersofthesesystems,specifyinginterestingvariablesshouldtakemuch
less time.
5.1.3 Comparing with an Alternative Approach. We compare CoFI
with injecting network partitions randomly. To be more specific,
we repeatedly run each workload for the same time as CoFI spends
when testing the target systems. During each test run, we inject a
543Table 3: Bugs triggered by CoFI. ‚ÄúStop‚Äù shows whether the network partition needs to stop to trigger the bug. ‚ÄúStatus‚Äù shows
whether the bug is pending for developer‚Äôs confirmation, has been confirmed by developers, or has already been fixed. ‚ÄúRan-
dom‚Äùshowswhetherthebugistriggeredbyrandomlyinjectingnetworkpartitionsduringourexperiment.Notethatthefour
known bugs are from older versions of the target systems and we only apply random injection to the latest versions of thesesystems.
Bug ID Failure Symptom Interesting Metadata StopStatus Random
Known
bugsCASSANDRA-3975 Thread keeps crashing Column family name /checkFixed N/A
CASSANDRA-2115 Decommissioned node reappears Node status /checkFixed N/A
HDFS-14372 NullPointerException DataNode ID √óFixed N/A
YARN-4288 NodeManager aborts NodeManager ID √óFixed N/A
UnknownbugsCASSANDRA-15758 Thread crashes Node status /checkPending /check
CASSANDRA-15548 A created keyspace can‚Äôt be found Node status √óConfirmed /check
CASSANDRA-15546 Data read failure Node status √óPending √ó
CASSANDRA-15437 Decommission failure Node status √óPending /check
CASSANDRA-11804‚Ä†Data access failure Node status √óConfirmed /check
HDFS-15367 File metadata inaccessible NameNode status /checkPending /check
HDFS-15235 NameNode crashes NameNode status /checkConfirmed √ó
YARN-10301 Fail to stop a YARN service ResourceManager ID /checkPending √ó
YARN-10294 Misleading error message Container‚Äôs location /checkPending √ó
YARN-10288 Invalid application state transition Container‚Äôs location /checkConfirmed √ó
YARN-10232 Invalid application state transition Container‚Äôs location /checkPending √ó
YARN-10231 Misleading error message Container ID /checkPending /check
‚Ä†Wedidnotknow CASSANDRA-11804 beforeCoFIexposedit.ThisbugispreviouslyreportedbyothersinCassandra-3.5.Buttheoriginal
bug reporter can no longer trigger it in later versions of Cassandra. We are the first one to report this bug in Cassandra-3.11.5.
network partition randomly. The scenario of the network partition
isdeterminedbeforeeachtestrun.First,werandomlyselectanode
toinjectnetworkpartition.Then,wedecidewhentostartandstop
the network partition. The starting point and the stopping point of
the network partition are represented using the time offset from
the start of a test run. We randomly choose a time offset between 0andthelongesttestdurationtobethestartingpointofthenetwork
partition, and randomly select a time offset between the starting
pointand thelongesttest durationtobe thestoppingpoint ofthe
network partition. The longest test duration will be updated when
a longer test run occurs. If a test run finishes before the startingpoint/stopping point is reached, the network partition does not
start/stop in that test run.
5.2 Detecting Partition Bugs
5.2.1 Overall Experimental Results. Table3lists the partition bugs
triggered byCoFI. In this table,we show the detailedinformation
abouteachbug,includingthebug‚ÄôsIDinJIRA( BugID),thefailure
symptomofthebug( FailureSymptom ),theinterestingmetadata
in the invariant that guides CoFI to expose the bug ( Interesting
Metadata ), whether the bug requires stopping the network par-
tition to trigger ( Stop), whether the bug has been confirmed or
fixed by developers ( Status), and whether the bug is also triggered
byrandomlyinjectingnetworkpartitionsduringourexperiment
(Random).
AsshowninTable 3,CoFIsuccessfullydetectsallfourknown
bugsusingtheworkloadsspecifiedinthebug‚ÄôsJIRAreport,demon-
strating CoFI‚Äôs effectiveness in detecting known partition bugs.
Table3alsoshowsthat,CoFIidentifies12partitionbugsusingthe
interesting variables and the simple workloads (common user andadminoperations)thatwespecifiedforeachsystem(Table 1).All12
bugsarepreviouslyunknowninthesystemversionswetest.Atthetime of writing, four of the unknown bugs have been confirmed by
developers.Theexposedunknownbugshavedifferentsymptoms,
including severe failures like node crashes and data access failures.
Notethatthesebugsonlyrelyonasmallsetofinterestingmetadata,
i.e., Cassandra‚Äôs node status, HDFS‚Äôs NameNode status, YARN‚Äôs
ResourceManager ID, container location, and container ID.
In Table3, we can also see that CoFI is effective in exposing
partitionbugsthatrequiresthenetworkpartitiontostopatcertain
points. Specifically, 10 out of 16 bugs can only be triggered bystopping the network partition at certain timing. CoFI exposesthese bugs by systematically stopping the network partition for
each type of messages. On the contrary, it is challenging to expose
these bugs using existing techniques that rely on developers to
specify when the network partition starts and stops.
5.2.2 False Positive Analysis. Table4showsthe detailed statistics
of applying CoFI to the latest versions of the target systems. In
total,CoFIreports49 uniquetestfailuresinthesesystems(Column
Failures). 15 of these failures are caused by the unknown bugs
listed in Table 3(ColumnBugs). The remaining failures are mostly
false positives (Column False Pos. ), while one failure cannot be
reproduced for diagnosis (Column Can‚Äôt Repr.).
Wefurtherinvestigatethefalsepositivesreportedby CoFI.We
find that, most (28 out of 33) of these false positives are caused by
ourcheckersassertingforoperationsuccesswhiletheoperation
has to fail. For example, in one of the false positives in Cassan-dra, a data read with quorum consistency (2 out of 3) fails with
a‚ÄúNoHostAvailable‚Äùerror.ThisfailurematcheswithCassandra‚Äôs
544Table4:Thenumberofuniquetestfailuresineachsystem.A
failurecanbeabug,afalsepositive(FalsePos.),orundecided
if it cannot be reproduced for diagnosis (Can‚Äôt Repr.).
System Failures Bugs False Pos. Can‚Äôt Repr.
Cassandra-3 10 55 0
HDFS-3 7 2‚Ä†50
HDFS-2 9 2‚Ä†70
YARN-3 17 51 2 0
YARN-2 6 1‚Ä°41
Total 49 15 33 1
‚Ä†The two failures in HDFS-3 and the two failures in HDFS-2
share the same two root causes ( HDFS-15367 andHDFS-15235 ).
‚Ä°ThefailureinYARN-2sharesthesamerootcause( YARN-10232 )
with one of the failures in YARN-3.
 		

		
		



	

	
	

	
		
	
		
	
Figure 6: The triggering process of HDFS-15235 .
specificationbecausethecoordinatornodeforthe readrequestis
partitioned from the other two nodes. As a result, it does not have
enough peers (hosts) to serve the request.
These false positives raise a challenge ingenerating oracles for
fault injection tests. Specifically, the correct system behaviors may
vary in different fault scenarios. For example, if the ‚ÄúNoHostAvail-
able‚Äùerroroccurswhenonlyanon-coordinatornodeispartitioned,
the failure is a bug because the coordinator should have enough
peerstoservethereadrequest.Automaticallygeneratingoracles
for fault injection tests will be a challenge for the future research.
5.2.3 CaseStudy. Wenowshowanexampleofhowpartitionbugs
can occur under intricate network partition scenarios and how
CoFI‚Äôschoiceofthepartitionstartingpointsandtheexplorationof
the stopping points help CoFI expose partition bugs.
Figure6showsapartitionbug HDFS-15235 ,whichistriggeredby
a temporary network partition that occurs when a failover attempt
is being rolled back. This is a previously-unknown partition bug
reported by CoFI. In a cluster with two NameNodes, NN2has just
became active in a failover attempt (Step 1). Due to a network
partition, NN2fails to respond to the failover command, which
triggers a rollback (Step 2). Normally, as the network partition
recovers, NN2shouldsafelychangebacktostandby.However,abug
in the rollback process unconditionally terminates NN2during the
rollback(Step 3).Asaresult,theclusterlosesahealthyNameNode
only because of an untimely transient network partition!
CoFItriggersthisbugwhenusingtheinvariantthat NN2and
the DataNode in the cluster ( DN) agree on NN2‚Äôs status in consis-
tent states. After NN2becomes active but before it informs DN,Table 5: The overhead of CoFI.
SystemInvariants Test Runs
Mined Selected Iterations Time
Cassandra-3 513 99 9,366 222h, 20m
HDFS-3 157 48 944 24h, 09m
HDFS-2 256 68 1,305 39h, 41m
YARN-3 51 14 1,151 240h, 11m
YARN-2 32 8 250 58h, 59m
Total 1,009 237 13,016 585h, 20m
the system is inconsistent. At this point, CoFI starts a network
partitionon NN2.DuringCoFI‚Äôssystematicexplorationofdifferent
partitionstoppingpoints,itwillteststoppingthenetworkpartitionbeforeStep
2,whichthentriggersthebug.Inourexperiment,CoFI
exposes this bug using only 15 runs when testing HDFS-3. As this
example shows, the correctness of a protocol implementation can
behardtoanalyzeunderintricatenetworkpartitionscenarios.CoFI
canhelpexposepartitionbugsintheimplementationbysystem-
atically exploring different network partition scenarios. It is also
worthnotingthattotriggerthisbugthenetworkpartitionneeds
to start and stop in the middle of one failover command issued
bytheadmin.Therefore,injectingnetworkpartitionsattheuser
operation level, i.e., starting and stopping the network partition
before or after the failover command, cannot trigger this bug.
5.3 Comparing with Random Fault Injection
Table3also lists the bugs triggered by randomly injecting network
partitions using the policy described in ¬ß5.1.3. We find that CoFI is
more effective in exposing partition bugs than random injection.
Specifically, the random injection only triggerred 6 out of 12 bugs
triggered byCoFI. Moreover, the randominjection did not trigger
any bugs that are new to CoFI. It is also worth noting that if the
randominjectiondoesnotstopthenetworkpartition,3outofthese
6 bugs will not be triggered.
To understand whyrandom injection is noteffective in trigger-
ing partition bugs, we further analyze the triggering process of
fourrepresentativebugs( CASSANDRA-15437 ,HDFS-15367 ,HDFS-15235 ,
andYARN-10232 ) to compute the probability to trigger them ran-
domly.First,weassumethatthenodetopartitioniscorrectlyse-
lected.Then,basedona concreteexecution,wecomputetheprob-
ability of selecting the right starting point of the network parti-
tionP(start)andtheconditionalprobabilityofselectingtheright
stopping point based on the selected starting point P(end|start).
Finally,theprobabilitytorandomlytriggerabugiscomputedas
P(bu–¥)=P(start)√óP(end|start). We find that the two bugs trig-
geredbybothCoFIandtherandominjectionhavehighlikelyhoods
to trigger ( P(CA-15437)=12.65%,P(HF-15367)=8.87%), while
the other two bugs only triggered by CoFI have much lower proba-
bilities(P(HF-15235)=0.08%,P(YN-10232)=0.002%).Therefore,
both our experiment and our analysis suggest that CoFI is more
effective than random injection in triggering partition bugs.
5.4 Overhead Analysis
To measure the overhead of CoFI, we record several metrics while
applying CoFI to detect unknown bugs. The metrics include the
545numberofinvariantsminedandselectedbyCoFI,aswellasthetest
iterations and wall clock time CoFI spends in testing each target
system. Table 5shows the values of these metrics.
In the fault injection stage, there are in total 13,016 test runs for
allfivesystemversions(Column Iterations ),whichtakesabout585
hourstofinish(Column Time).Specifically,theaveragetimefor
each test run (Column Time/ Column Iterations ) in Cassandra
and HDFS is about 1 to 2 minutes. On average, YARN requires
more than 10 minutes to finish one test run. This is because YARN
employs a wait-and-retry mechanism for many operations. The
testtimecanbeshortenedbyconfiguringYARNtoreducethewait
time and thenumber of max retries. Giventhat cloud systems are
complicated, the above result demonstrates that CoFI is efficient to
be used for real world cloud system testing.
Table5also shows, in the invariant mining stage, CoFI mines
1,009 distributed program invariants from the target systems (Col-
umnMined). After applying CoFI‚Äôs invariant pruning strategy,
only237invariantsremain(Column Selected).That‚Äôssaid,about
76%ofinvariantsareremovedbyCoFI‚Äôsinvariantspruningstrategy,
significantly reducing the number of invariants to test.
6 DISCUSSION
We now discuss limitations and potential threats in our approach.
CoFI‚ÄôsFaultModel. CoFI focuses on a simple fault model: one
temporary network partition occurs on one node. Hence, CoFI can
misssomepartitionbugs,e.g.,theonestriggeredbypartitioning
multiple nodes. Eventhough CoFI is not complete,our evaluation
shows that CoFI is effective in detecting partition bugs. Extending
CoFI to support more fault models can be our future work.
Identifying Interesting Variables. CoFI‚Äôs effectiveness and
efficiency depend on the quality of the specified variables. Spec-
ifying uninteresting variables, e.g., variables for local file systemdata, may prevent CoFI from injecting network partitions at in-teresting inconsistent states. Moreover, specifying variables thatare updated at the same time, e.g.,
nodeIdandnodeId_charArray ,
willcauseCoFItotestredundantnetworkpartitionscenarios.To
helpidentifyinterestingvariables,wesuggestdeveloperstospec-
ify metadata variables that interactwith network. Moreover, CoFI
provides default interesting variables for our target systems. In the
future, it is needed to automate the variable identification process.
MonitoringVariables. CoFIdoesnotusecomplicatedprogram
analysisorexpensivesynchronizationtocollectconsistentstates
ofthecloudsystem.Instead,CoFIemploysasimpleheuristic,i.e.,
usingthebefore-sendandtheafter-handleprogrampointsofthe
same message to construct system states. This heuristic may cause
CoFI to collect an inaccurate state, e.g., if the variables are asyn-
chronouslyupdatedinthemeantime.Inthefuture,thisinaccuracy
can be removed by adding synchronization across the cluster.
Threats to Validity. Due to resource limitation, we evaluate
CoFI using five versions of three popular cloud systems. Therefore,
ourexperimentalresultsmaynotreflectthesituationinothercloud
systems, e.g., distributed streaming systems. However, we strive
tobeunbiasedbyselectingsystemswithdifferentfunctionalities
(i.e.,a distributedNoSQL database,adistributed filesystem, anda
distributedcomputingframework)andarchitectures(i.e.,peer-to-
peer vs. coordinator/worker).7 RELATED WORK
Fault Injection. Fault injection is a commonly used technique for
exposing fault-triggered bugs. In recent years, many fault injection
techniques have been proposed to expose bugs in cloud systems.NEAT [
2] and Jepsen [ 23] both inject network partitions when
testing cloud systems. However, they rely on developers to specify
when a network partition starts and stops, which makes them
less desirable for exposing partition bugs that have strict timingrequirements on network partitions. Moreover, both tools injectnetwork partitions at the user operation level, which limits their
effectiveness in exposing intricate partition bugs. On the contrary,
CoFI systematicallyand smartlyexplores differentstarting points
and stopping points of network partitions at message level, which
enables CoFI to expose partition bugs effectively.
Other fault injection techniques include randomly dropping
networkpackets[ 32],injectingnodecrashes[ 1,29,30],injecting
filesystemfaults[ 16],injectingAPIfailures[ 3,4],andreordering
networkmessages[ 28].CoFIiscomplementarytothesetechniques
since it focuses on a different and important fault model for cloud
systems, i.e., network partitions.
Faultinjectionhasalsobeencommonlyusedtotesthowgeneral
software behaves at adversarial scenarios, such as power faults
[22,33,46]andadversarialinputs[ 26,34,45].Thesetechniquesdo
not focus on exposing partition bugs in cloud systems.
BugDetectionTechniquesforCloudSystems. Besidesfault
injection, many other techniques have been proposed to detectbugs in cloud systems. For example, distributed model checkersexplore all possible interleavings among network messages and
local computation to expose bugs in the cloud system implementa-
tions [21,24,31,43]. While being powerful, they still suffer from
thestatespaceexplosionproblem.Sometoolscandetectbugsby
statically analyzing the source code of the cloud systems [ 7,9,44].
Partitionbugsinvolvecomplexinteractionbetweenmultiplenodes
in the cloud system, which is challenging to analyze statically.
8 CONCLUSION
We present consistency-guided fault injection (CoFI), a novel tech-
nique that injects network partitions to expose partition bugs in
cloudsystems.CoFIisthefirstfaultinjectiontechniquethatcon-
trols both the starting point and the stopping point of the injected
networkpartition.Ourevaluationonpopularcloudsystemsshows
thatCoFIisbotheffectiveinexposingpartitionbugsandefficient
to be used in real world cloud system testing.
ACKNOWLEDGEMENTS
We would like to thank the anonymous reviewers for their thor-
oughandinsightfulcomments.Inthiswork,HaichengChenand
Feng Qin were partially supported by National Science Foundation
grants#CNS-1513120and#CCF-0953759(CAREERAward).Wen-
shengDou andDong Wangwere partiallysupported byNational
Key R&D Program of China (#2017YFB1001804), National Natural
Science Foundation of China (#61732019), Frontier Science Project
of Chinese Academy of Sciences (QYZDJ-SSW-JSC036), and Youth
InnovationPromotionAssociationatChineseAcademyofSciences.
Both Haicheng Chen and Wensheng Dou are the corresponding
authors of this paper.
546REFERENCES
[1]Ramnatthan Alagappan, Aishwarya Ganesan, Yuvraj Patel, Thanu-
malayan Sankaranarayana Pillai, Andrea C Arpaci-Dusseau, and Remzi H
Arpaci-Dusseau. 2016. Correlated crash vulnerabilities. In Proceedings of the 12th
USENIX Symposium on Operating Systems Design and Implementation. 151‚Äì167.
[2]Ahmed Alquraan, Hatem Takruri, Mohammed Alfatafta, and Samer Al-Kiswany.
2018.Ananalysisofnetwork-partitioningfailuresincloudsystems.In Proceedings
of the 13th USENIX Symposium on Operating Systems Design and Implementation.
51‚Äì68.
[3]Peter Alvaro, Kolton Andrus, Chris Sanden, Casey Sosenthal, Ali Basiri, and
Hochstein Lorin. 2016. Automating failure testing research at Internet scale. In
Proceedings of the 7th ACM Symposium on Cloud Computing. 1‚Äì16.
[4]Peter Alvaro, Joshua Rosen, and Joseph M Hellerstein. 2015. Lineage-driven
faultinjection.In ProceedingsoftheACMSIGMODInternationalConferenceon
Management of Data. 331‚Äì346.
[5]BenBrody.2014. LAresidentscall911whenFacebookgoesdown. https://money.
cnn.com/2014/08/04/news/companies/facebook-outage-911/index.html.
[6] Box. 2020. Box customers. https://www.box.com/customers.
[7]Haicheng Chen, Wensheng Dou, Yanyan Jiang, and Feng Qin. 2019. Understand-
ingexception-relatedbugsinlarge-scalecloudsystems.In Proceedingsofthe34th
International Conference on Automated Software Engineering.
[8] Shigeru Chiba. 2020. Javassist. https://www.javassist.org.
[9]Ting Dai, Jingzhu He, Xiaohui Gu, Shan Lu, and Peipei Wang. 2018. DScope:Detecting real-world data corruption hang bugs in cloud server systems. In
Proceedings of the 7th ACM Symposium on Cloud Computing. 333‚Äì325.
[10]Datapth.io.2017. RecentAWSoutageandhowyoucouldhaveavoideddown-time.https://medium.com/@datapath_io/recent-aws-outage-and-how-you-
could-have-avoided-downtime-7d9d9443d776.
[11]DropboxBusiness.2020. Customers-Dropboxbusiness. https://www.dropbox.
com/business/customers.
[12]Dmitry Duplyakin, Robert Ricci, Aleksander Maricq, Gary Wong, Jonathon
Duerig, Eric Eide, Leigh Stoller, Mike Hibler, David Johnson, Kirk Webb, Aditya
Akella, Kuangching Wang, Glenn Ricart, Larry Landweber, Chip Elliott, Michael
Zink, Emmanuel Cecchet, Snigdhaswin Kar, and Prabodh Mishra. 2019. Thedesignand operationofCloudLab.In ProceedingsofUSENIX AnnualTechnical
Conference.
[13]Dynamo. 2016. Documentation. http://cassandra.apache.org/doc/latest/
architecture/dynamo.html.
[14]Michael D. Ernst. 2000. Dynamically discovering likely program invariants. Ph.D.
University of Washington Department of Computer Science and Engineering,
Seattle, Washington.
[15] Facebook. 2020. Facebook. https://www.facebook.com.
[16]Aishwarya Ganesan, Ramnatthan Alagappan, Andrea C Arpaci-Dusseau, andRemzi H Arpaci-Dusseau. 2017. Redundancy does not imply fault tolerance:
Analysis of distributed storage reactions to single errors and corruptions. In
Proceedings of the 15th USENIX Conference on File and Storage Technologies. 149‚Äì
166.
[17]Phillipa Gill, Navendu Jain, and Nachiappan Nagappan. 2011. Understandingnetwork failures in data centers: Measurement, analysis, and implications. In
Proceedings of the ACM SIGCOMM Conference. 350‚Äì361.
[18]Google. 2020. Spanner - Replication. https://cloud.google.com/spanner/docs/
replication.
[19]RameshGovindan,InaMinei,MaheshKallahalla,BikashKoley,andAminVahdat.
2016. Evolve or die: High-availability design principles drawn from Googles
network infrastructure. In Proceedings of the ACM SIGCOMM Conference. 58‚Äì72.
[20]Stewart Grant, Hendrik Cech, and Ivan Beschastnikh. 2018. Inferring and as-serting distributed system invariants. In Proceedings of the 40th International
Conference on Software Engineering. 1149‚Äì1159.
[21]Haryadi S Gunawi, Thanh Do, Pallavi Joshi, Peter Alvaro, JosephM Hellerstein,
Andrea C Arpaci-Dusseau, Remzi H Arpaci-Dusseau, Koushik Sen, and Dhruba
Borthakur.2011. FATEandDESTINI:Aframeworkforcloudrecoverytesting.
InProceedingsofthe8thUSENIXSymposiumonNetworkedSystemsDesignand
Implementation.
[22]Yanyan Jiang, Haicheng Chen, Feng Qin, Chang Xu, Xiaoxing Ma, and JianLu. 2016. Crash consistency validation made easy. In Proceedings of the 24th
ACM SIGSOFT International Symposium on Foundations of Software Engineering.
133‚Äì143.
[23]Kyle Kingsbury. 2018. Distributed systems safety research. Retrieved Oct 10,
2018 from https://jepsen.io/[24]Tanakorn Leesatapornwongsa, Mingzhe Hao, Pallavi Joshi, Jeffrey F Lukman,
andHaryadiSGunawi.2014. SAMC:Semantic-awaremodelcheckingforfast
discovery of deep bugs in cloud systems.. In Proceedings of the 11th USENIX
Symposium on Operating Systems Design and Implementation. 399‚Äì414.
[25]TanakornLeesatapornwongsa,JeffreyFLukman,ShanLu,andHaryadiSGunawi.
2016. TaxDC: A taxonomy of non-deterministic concurrency bugs in datacen-ter distributed systems. In Proceedings of the 21st International Conference on
Architectural Support for Programming Languages and Operating Systems.
[26]Caroline Lemieux, Rohan Padhye, Koushik Sen, and Dawn Song. 2018. Perffuzz:
Automatically generating pathological inputs. In Proceedings of the 27th ACM
SIGSOFT International Symposium on Software Testing and Analysis. 254‚Äì265.
[27] Linkedin. 2020. Linkedin. https://www.linkedin.com.
[28]HaopengLiu,GuangpuLi,JeffreyFLukman,JiaxinLi,ShanLu,HaryadiSGunawi,
and Chen Tian. 2017. DCatch: Automatically detecting distributed concurrency
bugs in cloud systems. In Proceedings of the 22nd International Conference on
Architectural Support for Programming Languages and Operating Systems.
[29]Haopeng Liu, Xu Wang, Guangpu Li, Shan Lu, Feng Ye, and Chen Tian. 2018.
FCatch: Automatically detecting time-of-fault bugs in cloud systems. In Proceed-
ingsofthe23rdInternationalConferenceonArchitecturalSupportforProgramming
Languages and Operating Systems.
[30]Jie Lu, Chen Liu, Lian Li, Xiaobing Feng, Feng Tan, Jun Yang, and Liang You.2019. CrashTuner: Detecting crash recovery bugs in cloud systems via meta-
infoanalysis.In Proceedingsofthe27thACMSymposiumonOperatingSystems
Principles. 114‚Äì130.
[31]JeffreyFLukman,HuanKe,CesarAStuardo,RizaOSuminto,DaniarHKurni-
awan,DikaiminSimon,SatriaPriambada,ChenTian,FengYe,TanakornLeesata-pornwongsa,etal
.2019. FlyMC:Highlyscalabletestingofcomplexinterleavings
in distributed systems. In Proceedings of the 14th EuroSys Conference. 20.
[32]osrg.2018. Namazu:Programmablefuzzyschedulerfortestingdistributedsys-
tems.https://github.com/osrg/namazu.
[33]ThanumalayanSankaranarayanaPillai,VijayChidambaram,RamnatthanAlagap-pan,SamerAl-Kiswany,AndreaCArpaci-Dusseau,andRemziHArpaci-Dusseau.
2014. All file systems are not created equal: On the complexity of crafting
crash-consistentapplications.In Proceedingsofthe11thUSENIXSymposiumon
Operating Systems Design and Implementation. 433‚Äì448.
[34]NickStephens,JohnGrosen,ChristopherSalls,AndrewDutcher,RuoyuWang,
Jacopo Corbetta, Yan Shoshitaishvili, Christopher Kruegel, and Giovanni Vigna.
2016. Driller:Augmentingfuzzingthroughselectivesymbolicexecution.In NDSS,
Vol. 16. 1‚Äì16.
[35]TheApacheSoftwareFoundation.2016. Cassandra. http://cassandra.apache.org.
[36]TheApacheSoftwareFoundation.2016. Cassandra-Datamanipulation. https:
//cassandra.apache.org/doc/latest/cql/dml.html.
[37]TheApacheSoftwareFoundation.2016. Cassandra‚Äôsoldersupportedreleases.
https://cassandra.apache.org/download/.
[38]The Apache Software Foundation. 2019. Apache Hadoop YARN. https://hadoop.
apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html.
[39]The Apache Software Foundation. 2019. HDFS Architecture. http://hadoop.
apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html.
[40]Daniel Turner, Kirill Levchenko, Jeffrey C Mogul, Stefan Savage, Alex C Snoeren,
Daniel Turner, Kirill Levchenko, Jeffrey C Mogul, Stefan Savage, and Alex C
Snoeren.2012. Onfailureinmanagedenterprisenetworks. HPLabsHPL-2012-
101(2012).
[41]Daniel Turner,Kirill Levchenko, Alex CSnoeren, and Stefan Savage. 2010. Cali-
forniafaultlines:Understandingthecausesandimpactofnetworkfailures.In
Proceedings of the ACM SIGCOMM Conference. 315‚Äì326.
[42]Wikipedia. 2020. Two-phase commit protocol. https://en.wikipedia.org/wiki/
Two-phase_commit_protocol.
[43]Junfeng Yang, Tisheng Chen, Ming Wu, Zhilei Xu, Xuezheng Liu, HaoxiangLin, Mao Yang, Fan Long, Lintao Zhang, and Lidong Zhou. 2009. MODIST:
Transparentmodelcheckingofunmodifieddistributedsystems.In Proceedings
of the 6th USENIX Symposium on Networked Systems Design and Implementation.
[44]DingYuan, YuLuo, XinZhuang, GuilhermeRennaRodrigues, XuZhao, Yongle
Zhang,PranayUJain,andMichaelStumm.2014. Simpletestingcanpreventmost
criticalfailures:Ananalysisofproductionfailuresindistributeddata-intensive
systems. In Proceedings of the 11th USENIX Symposium on Operating Systems
Design and Implementation. 249‚Äì265.
[45] Michal Zalewski. 2017. American fuzzy lop. https://lcamtuf.coredump.cx/afl/ .
[46]MaiZheng,JosephTucek,DachuanHuang,FengQin,MarkLillibridge,ElizabethS
Yang,BillWZhao,andShashankSingh.2014. Torturingdatabasesforfunand
profit. In Proceedings of the 11th USENIX Conference on Operating Systems Design
and Implementation. 449‚Äì464.
547