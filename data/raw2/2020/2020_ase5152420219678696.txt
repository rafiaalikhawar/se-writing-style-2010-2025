VizSmith: Automated Visualization Synthesis by
Mining Data-Science Notebooks
Rohan Bavishi
University of California, Berkeley
rbavishi@cs.berkeley.eduShadaj Laddad
University of California, Berkeley
shadaj@cs.berkeley.eduHiroaki Y oshida
Fujitsu Research of America
hyoshida@fujitsu.com
Mukul R. Prasad
Fujitsu Research of America
mukul@fujitsu.comKoushik Sen
University of California, Berkeley
ksen@cs.berkeley.edu
Abstract â€”Visualizations are widely used to communicate ï¬nd-
ings and make data-driven decisions. Unfortunately creating
bespoke and reproducible visualizations requires the use ofprocedural tools such as matplotlib. These tools present a steeplearning curve as their documentation often lacks sufï¬cient usageexamples to help beginners get started or accomplish a speciï¬ctask. Forums such as StackOverï¬‚ow have long helped developerssearch for code online and adapt it for their use. However,developers still have to sift through search results and understandthe code before adapting it for their use.
We built a tool called V
IZSMITH which enables code reuse
for visualizations by mining visualization code from Kagglenotebooks and creating a database of 7176 reusable Python
functions. Given a dataset, columns to visualize and a text queryfrom the user, V
IZSMITH searches this database for appropriate
functions, runs them and displays the generated visualizationsto the user. At the core of V
IZSMITH is a novel metamorphic
testing based approach to automatically assess the reusability offunctions, which improves end-to-end synthesis performance by10% and cuts the number of execution failures by 50%.
I. I NTRODUCTION
Visualizations are increasingly being used across various do-
mains, including academic research, journalism and business
intelligence, to communicate insights and enable data-drivendecision making [1], [2]. The need for bespoke visualizationsand reproducible analytical workï¬‚ows [3] requires the use ofpowerful procedural visualization tools such as ggplot andmatplotlib [4], [5]. However these tools also have a steeplearning curve for novices and domain experts with littleprogramming background. Tool documentation pages functionwell as a reference but often lack sufï¬cient snippets orexamples to help beginners get started.
This has led to a huge surge in popularity of technical Q&A
forums such as StackOverï¬‚ow and social programming plat-forms like GitHub as they facilitate code reuse [6]. Analysts
can search for usage-examples or even complete recipes [7],[8] to incorporate directly into their workï¬‚ow.
In practice, however, code reuse in software development
has largely been sub-optimal [8], [9] due to two main reasons.First, the code results returned by StackOverï¬‚ow may beincomprehensible to relatively new users, making it difï¬cultfor them to modify and reuse that code [8]. Second, there isa proliferation of similar questions on StackOverï¬‚ow whichends up pushing the burden of selecting the right solution tothe end user, who may not be familiar with the speciï¬cs ofthe visualization tools.
Facilitating better code reuse has been a subject of active
research [9]â€“[16]. This includes improving the quality ofsearch results [14], as well adapting the code using additionalspeciï¬cations such as test-cases or type signatures of targetmethods [9], [11]. None of these are however applicable in thecontext of visualizations. Wang et al. [17], [18] use a synthesis-powered approach to generate visualization programs in a lim-ited DSL given partial or incomplete visualizations. However,
this can be insufï¬cient when a helpful partial visualization isdifï¬cult to provide, such as when visualizing the correlationmatrix of a large table.
In this paper, we present and evaluate an approach for fa-
cilitating code reuse in generating visualizations. We leveragethe fact that machine learning platforms such as Kaggle [19]host scores of executable data science notebooks that alsoinclude the raw dataset. We developed a tool V
IZSMITH that
analyzes these notebooks and mines a knowledge base ofvisualization functions, which are Python functions that takean input table and the set of columns to visualize as inputand produce a visualization as output. V
IZSMITH provides
a frontend where users can provide a dataframe and thecolumns to visualize along with a text query. V
IZSMITH ï¬nds,
ranks, and executes the functions best matching the query, anddisplays the synthesized bespoke visualizations.
At the heart of V
IZSMITH lies a novel analysis for de-
termining the quality or reusability of a mined visualization
function. The analysis allows it to discard low-quality codeupfront which greatly helps in improving both quality andspeed of synthesis. To the best of our knowledge, we are theï¬rst to provide a precise conceptual deï¬nition of reusability
in the context of visualization code. We also develop a novelapproach based on metamorphic testing that approximatesthis deï¬nition, for automatically evaluating reusability of anyarbitrary visualization function. In summary, our contributionswithin V
IZSMITH are as follows:
1) A framework for mining visualization functions from Kag-
gle that yields a knowledge base of 7126 reusable functions
mined across 1280 notebooks and 10 competitions.
1292021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)
DOI 10.1109/ASE51524.2021.000222021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE) | 978-1-6654-0337-5/21/$31.00 Â©2021 IEEE | DOI: 10.1109/ASE51524.2021.9678696
978-1-6654-0337-5/21/$31.00  Â©2021  IEEE
 !	)*"
	 

	 	 
	 
 


!"
+	!	(#)$	(#*$"
	!!)"
!"+("%
!	 +''
	 +"

!&
) "

!&
* "
!	 +	
)+&
 
*+&
& "
				
				
Fig. 1: V IZSMITH â€™s Jupyter notebook frontend. V IZSMITH is provided with a table as a Pandas dataframe along with columns
to visualize as input. It has a search bar to input text queries. (A) shows how Alice uses V IZSMITH to search for normalized
stacked bar charts for her call quality dataset. (B) and (C) show the visualization selected by Alice and its code respectively.
2) A conceptual deï¬nition of reusability in the context of
visualizations along with a novel decision procedure based
on metamorphic testing that achieves 73% precision and71% recall with respect to a ground truth obtained viamanual inspection.
3) A synthesis engine that takes as user input a dataframe and
the columns to visualize. In a cross-project experiment,the target visualization is contained in the top-10 resultsreturned by V
IZSMITH for 56% of our benchmarks.
4) A publicly available front-end and demo at https://github.
com/rbavishi/vizsmith-demo.
II. M OTIV A TING EXAMPLE
Alice is a researcher working on a project on analyzing the
voice call quality dataset released by the Indian government[20] containing customer ratings. As part of her project, Aliceneeds to build a visual dashboard that updates every time newdata comes in. She has heard about rich data transformationand visualization libraries in Python such as
pandas and
matplotlib and decides to use them for this purpose.
In her dashboard, Alice wants to include a visualization of
the distribution of customer ratings for every network operatorindividually, normalized by the number of records for everyoperator. She decides that a normalized stacked bar chart with
a bar for every operator would be appropriate for this purpose.
Alice promptly writes code to load the dataset into a
pandas
dataframe. Unsure about how to create a stacked bar chart, shevisits the
matplotlib gallery entry for this chart [21] only to
ï¬nd it insufï¬cient for her needs. She is also uncertain aboutexactly how to transform her dataframe in order to create the
bar chart. She turns to StackOverï¬‚ow for help and browsesthe results for the query â€œ
matplotlib pandas normalized
stacked bar chart â€.The top result [22] contains a visualization close to what
Alice needs, but she has trouble understanding the code, letalone adapting it for her data. This experience is in line withthe ï¬ndings of previous work [8].
Figure 1 demonstrates how Alice uses V
IZSMITH to ï¬nd
the visualization of her choice along with code to produceit. First, Alice ï¬res up the frontend of V
IZSMITH , which is
implemented as a Jupyter notebook [3] widget. Alice provides
VIZSMITH with her dataframe as well as the columns she
wants to visualize. V IZSMITH then presents a search bar where
Alice provides the same query as before.
VIZSMITH then consults its knowledge base of visualiza-
tion functions that it has mined from the machine learning
notebooks written by data scientists on Kaggle. V IZSMITH
utilizes dynamic program analysis and metamorphic testingto construct these functions. These visualization functions areregular Python functions that take a dataframe as an argumentalong with column arguments and produce a visualizationafter performing any necessary dataframe transformations.
V
IZSMITH indexes these functions using the names of the
API functions and their keyword arguments, along with thenatural language comments found in the Kaggle notebooks.Given Aliceâ€™s keywords, V
IZSMITH ï¬nds the best matching
functions, runs them and presents the resulting visualizationsin a gallery view as shown in Figure 1. V
IZSMITH allows
Alice to expand a particular visualization to a full-screen viewas well as study the code for the visualization.
Alice ï¬nds her desired visualization in this list right away,
shown in (B) in Figure 1. V
IZSMITH also produces many
similar visualizations with small styling variations. The codefor the visualization is shown in (C) and illustrates the inherentcomplexity of the task as it needs a combination of three
pandas functions, namely crosstab ,div and sum followed
130!&#!
"$	
!  !"!
#"!&
$
 "%!
 &

%!#!!
 "%!
"! " !$
$  

! 	

$
 

! 
	


 "%!
"! 
"!
! 
&#"! "%! 
Fig. 2: Overview of V IZSMITH .
by the call to plot . Alice copies the code into her workï¬‚ow,
and adjusts the title and y-axis labels.
Thus, V IZSMITH enables better code reuse by eliminating
the burden of understanding and adapting code found online.
III. O VERVIEW OF VIZSMITH
Figure 2 presents a high-level overview of V IZSMITH .I n
the ofï¬‚ine phase, V IZSMITH collects and mines visualization
functions from Python notebooks hosted on the machine learn-
ing platform Kaggle [19] (Section IV). V IZSMITH also ana-
lyzes the functions using a novel metamorphic testing scheme(Section V) to discard functions ill-suited for synthesis. In theonline phase, V
IZSMITH receives from the user, a dataframe as
well as the set of columns in the dataframe that participate inthe desired visualization along with a search query. V
IZSMITH
ï¬rst uses the query to collect a ranked list of functions toexplore (Section VI-A). Then it ï¬nds appropriate arguments tothe parameters of each function, and executes them all. Finally,
V
IZSMITH collects and displays the generated visualizations
in a Jupyter notebook user interface.
IV . M INING
We ï¬rst describe the component of our system responsible
for collecting notebooks from Kaggle, replaying them andharvesting visualization code from the notebook runs.
A. Collecting and Replaying Notebooks
We sort the list of competitions on Kaggle by the number of
teams participating in the competition. From the top-50 such
competitions, we pick those where the dataset corresponds toasingle csv/tsv ï¬le. We additionally include the
titanic and
house-prices competitions as they are the most well-known
classiï¬cation and regression tasks on Kaggle respectively,resulting in a total of 10 competitions (Table II).
Within these competitions, we only select kernels that have
an associated Docker image ID which can be downloaded fromKaggleâ€™s GCR repository
1. To conserve resources, we ignore
GPU-based kernels and impose a timeout of 10 minutes oneach kernel run. For competitions with large datasets ( >50k
rows), we take a sample of the dataset in order to reduce theexecution time.
1https://gcr.io/kaggle-images/pythonB. Instrumentation and Execution
We perform source-level instrumentation of the scripts
collected before execution to facilitate the construction ofthe dependency graph. We deï¬ne the dependency graph of
a program Pas a graph Gsuch that the nodes correspond
to the simple statements in P. A dependency edge exists
between nodes n
1andn2if the statement corresponding to
n2is data-dependent or control-dependent on the statement
atn1. Data-dependence implies that n2uses some variables
or data deï¬ned or modiï¬ed at n1. Control-dependence means
that ifn1determines whether n2executes or not, which is the
case when n1is an if-statement or a looping statement.
Our source-level instrumentation adds wrapper functions to
record essential runtime information such as variable readsand writes, as well as types and memory locations of ob-jects. This information helps us construct the dependencygraph. Note that we do not instrument code corresponding tobuilt-in or third-party libraries. Therefore, to capture librarydependencies correctly, we construct a separate database offunction specs with one entry for each built-in and API
function. For every function, we determine if it has side-effects, based on the arguments to the function. We writesuch specs for methods of inbuilt types such as lists, setsand dictionaries as well as API functions from popular datascience libraries, namely
pandas ,matplotlib ,seaborn ,numpy
and scikit-learn . These specs are quite coarse â€” given
the function call df.drop(columns=["Low"], inplace=True) ,
our spec for drop only records that the dataframe dfis mod-
iï¬ed, instead of the precise column â€œ Lowâ€ that was updated.
This keeps our implementation simple at the cost of spuriousdependency edges.
C. Visualization Objects and Visualization Slices
Over the course of execution of a program P, we collect the
Python objects corresponding to individual visualizations. In
our implementation, we focus on the
matplotlib library as
well as its wrapper library seaborn , so we track all unique
Python objects of the type matplotlib.pyplot.Figure .W e
call such an object a visualization object, or simply visual-
ization. We say that visualizations Î½1andÎ½2are the same if
the corresponding images obtained after serialization/renderingare a pixel-by-pixel match. For
matplotlib , this corresponds
to the output of the matplotlib.pyplot.Figure.savefig API
function.
For every visualization Î½seen over the execution of program
P, we construct a visualization slice deï¬ned as follows:
Deï¬nition 1 (Visualization Slice). We deï¬ne the visualization
slice of a program Pwith respect to a visualization object Î½,
denoted as VizSlice(P,Î½ ), as a program P/primethat can be obtained
by removing statements from Psuch that, when executed, P/prime
produces the same visualization Î½and onlyÎ½.
Thus, a visualization slice contains all the statements in
a program necessary for recreating a particular visualization.Figure 3 contains a slice of the linked Kaggle notebook for
1311import pandas aspd
2import seaborn assns
3 sns.set_style('white')
4 df_train =pd.read_csv("../input/train.csv")
5 df_train.fillna(df_train.mean(), inplace=True)6d f =df_train[['Age']]
7a x =sns.distplot(df['Age'], kde=False)
8a x .set(xlabel='Age', ylabel='Frequency')
Fig. 3: Example of a visualization and a corresponding slice
extracted from Kaggle.
1import pandas aspd
2import seaborn assns
3 df_train =pd.read_csv("../input/train.csv")
4 df_train.fillna(df_train.mean(), inplace=True)
5a x =sns.distplot(df_train['Age'], kde=False)
6a x .set(xlabel='Age', ylabel='Frequency')
Fig. 4: Minimized version of visualization slice in Figure 3.
the shown visualization. Furthermore, the slice should only
produce a single visualization.
We use standard dynamic program slicing [23] to obtain
a visualization slice. Speciï¬cally, we remove all statementsinPthat are not reachable via a backward-traversal of the
dependency graph of Pstarting from any of the statements in
in the set VizStmts( P,Î½ )deï¬ned below:
Deï¬nition 2 (VizStmts( P,Î½ )).VizStmts( P,Î½ )is the set of all
statements in the program Pthat directly create/modify the
visualization object Î½.
In Figure 3, the statements in lines 5-6 correspond to
the set returned by VizStmts for the program corresponding
to the parent notebook and the visualization object beingthe actual plot at the top of Figure 3. The ï¬rst creates thedistribution plot, while the second sets the labels of the axes.The remaining statements in Figure 3 modify the style, loadthe dataframe and modify it before visualization and are henceincluded in the slice.
D. Minimizing Visualization Slices
Recall that our dependency graph construction is not precise
as we use coarse speciï¬cations for third-party libraries. As a
result, the visualization slice obtained via dynamic programslicing may still contain irrelevant statements whose removalwill not affect the visualization. Consider the slice in Figure 3.The call to
set_style in line 3 is unnecessary as the style is
"white" by default. It is included in the slice because it writes
to an internal styling dictionary which is then read in the callto
distplot thereby establishing a dependency. Taking the
subset of columns in line 6 is also unnecessary as distplotonly receives the target column anyway. We can remove boththese operations to yield a simpler, minimized visualization
slice, as shown in Figure 4.
How do we obtain the minimized visualization slice in
Figure 4 from the slice in Figure 3? Note that it is not enoughto simply remove or delete code as one might do if they wereusing delta-debugging [24]; removing lines 3 and 6 in Figure3 would lead to an undeï¬ned variable error for
df. Essentially,
we need transformations that go beyond code removal.
We instantiate the generalized syntax-guided program re-
duction framework developed in PERSES [25] to enable this
minimization. In particular, we use standard statement-leveldelta-debugging to remove top-level statements whose removaldoes not change the generated visualization. Additionally,we use a transformation where we replace a usage of avariable holding a dataframe with the usage of a previouslydeï¬ned variable, also holding a dataframe. We keep alternatingbetween these two transformations until the slice cannot beminimized further without altering the visualization. Alter-nation helps here because one transformation may introduceminimization opportunities for another. Algorithm 1 describesthis procedure.
Algorithm 1 Minimization Algorithm Pseudocode
1:function MINIMIZE (PÎ½)
2: currentâ†PÎ½; change â†true
3: while change is true do
4: changeâ†false
5: variantâ† DELTA DEBUG (current)
6: ifvariant/negationslash=current then
7: currentâ†variant; change â†true
8: for variant in DFVARREPLACE (current) do
9: ifvariant produces same visualization then
10: currentâ†variant; change â†true
11: break
12:Pmin
Î½â†current
13: returnPmin
Î½
We walk through how the algorithm minimizes the slice in
Figure 3. In the ï¬rst iteration, delta-debugging (line 5) wouldremove the call to
set_style in line 3, Figure 3. Then we iter-
ate over variants returned by DFVARREPLACE .DFVARREPLACE
replaces a use of a variable holding a dataframe by a use of an-other previously deï¬ned variable holding a different dataframe.If there are many possibilities, DfV arReplace explores variants
in the descending order of the gap between the original and
replacing deï¬nitions of the variables, measured in the numberof statements. The variant where
dfis replaced with df_train
is retained. Then line 6 in Figure 3 gets removed in the seconditeration, and the algorithm exits after the third iteration asno further minimization occurred, successfully returning thedesired slice in Figure 4.
The reasons behind selecting these two transformations are
two-fold. First, data-science code has minimal control ï¬‚ow.Hence, focusing on top-level statements is sufï¬cient. Secondly,data-transformation logic almost always involves applying APIfunctions on variables holding the data (dataframes). Since
1321import pandas aspd
2import seaborn assns
3 df_train =pd.read_csv( "../input/train.csv" )
4 df_train .fillna(df_train .mean(), inplace =True) 
5 ax =sns.distplot(df_train[ 'Age'], kde=False)
6 ax.set(xlabel ='Age', ylabel ='Frequency' )


Fig. 5: Dependencies between top-level statements for code
in Figure 3. Edges labeled 1, 3, 4, 5 and 6 capture depen-dency between the use and deï¬nition of a variable (
df_train ,
df_train ,ax,sns and pdrespectively) while 2 captures the
dependency between attribute reads and writes of an object(the dataframe in
df_train ).
1def visualization(df, col1):
2 import seaborn assns
3d f .fillna(df.mean(), inplace=True)
4a x =sns.distplot(df[col1], kde=False)
5a x .set(xlabel=col1, ylabel='Frequency')
(a) Visualization function using b=3 and variable as df_train .
1def visualization(df, col1):
2 import seaborn assns
3a x =sns.distplot(df[col1], kde=False)
4a x .set(xlabel=col1, ylabel='Frequency')
(b) Visualization function using b=4 and variable as df_train .
Fig. 6: Visualization functions extracted from slice in Figure 3.
visualization slices can be slow to execute as they use heavy-
weight libraries, our restricted set of transformations strike abalance between scalability and quality of minimization.
E. Extracting Visualization Functions
In this section, we describe how V
IZSMITH creates visu-
alization functions from a visualization slice. Visualization
functions form the basic unit of V IZSMITH â€™s mined database
which it uses for synthesis. Throughout this section, whenever
we refer to a visualization slice, we assume it is minimized.
A visualization function is formally deï¬ned as follows:
Deï¬nition 3 (Visualization Functions). A visualization func-
tionfis a Python function with a single dataframe parameter
dfandmcolumn parameters col1,..., colmthat produces a
visualization.
Note that while the above deï¬nition restricts a visualization
function to a single dataframe parameter, our technique has nosuch inherent restriction. We adopt this deï¬nition to simplifythe discussion and the notation used throughout the paper.
At a high level, visualization functions can be extracted
from a visualization slice by converting variables holding ref-erences to dataframes into parameters and abstracting concretereferences to columns into column parameters. The body of thefunction contains only the statements from the slice required toreproduce its visualization given the new dataframe argument.Figure 6 shows two visualization functions from the visual-ization slice in Figure 3. Each of them has a single columnparameter
col1 . Both produce a visualization containing thedistribution plot of the supplied column, with the functionin Figure 6a performing an extra imputation step to replacemissing values by the mean of their respective columns. Wecall the slice P
Î½from which a visualization function fis
obtained as the parent slice off.
Algorithm 2 formalizes the idea. Given a visualization slice
PÎ½producing visualization Î½and its dependency graph G,
for every program point bbetween the top-level statements of
the slice (line 4) , and every variable var holding a reference
to a dataframe object val dfthat is in scope at b(line 6), we
extract a visualization function as follows. We set the bodyof the function to be a subset of the statements in P
Î½, with
the variable var renamed to df(the dataframe parameter). This
subset is the smallest such that if the function is executed withthe initial value of dfasval
dfin the exact same state it was
at program point bin the slice, the resulting visualization is
the same as Î½. This subset is obtained using backward slicing
(lines 7-10), but on a subgraph GrofG.Grhas the same set
of nodes as G, but does not contain any dependency edges in
Gthat originate before the boundary and that arise because of
the use of the variable var or the dataframe val df. This helps us
pick only the statements necessary to reproduce visualizationÎ½ifvar is already assigned to val
dfto begin with.
Algorithm 2 Extracting Visualization Functions
GETVARDF S(PÎ½,b)returns the set of dataframe variables in
scope at program point binPÎ½along with their values.
ISDATA FRAME EDGE(e,var, dfvar)returns true if the edge eis a
data-dependency edge resulting from the use of variable var
or dataframe val df.REACHABLE (si,Gr,root) checks if siis
reachable from root via a backwards traversal of Gr.
1:function EXTRACT VIZFUNCTIONS (PÎ½,Î½,G)
2:/angbracketlefts1,...,s k/angbracketrightâ†top-level statements in PÎ½
3: funcsâ†âˆ…
4: for each program point bâˆˆ[1,k]do
5:Sbâ†{s1,...,s b}
6: for each (var, dfvar)âˆˆGETVARDF S(PÎ½,b)do
7: Erâ†{e|eâˆˆEDGES (G)âˆ§SRC(e)âˆˆSb
âˆ§ISDATA FRAME EDGE(e,var, dfvar)}
8: Grâ†induced subgraph of Gby removing edges in Er
9: rootâ† VIZSTMTS (PÎ½,Î½)
10: bodyâ†{si|siâˆˆ{s1,...,s k}
âˆ§REACHABLE (si,Gr,root)}
11: Sforbidâ†{s|sâˆˆSbâˆ§var is used in s}
12: ifSforbidâˆ©body =âˆ…then
13: f.df paramâ†df
14: f.bodyâ† RENAME VAR(body, var, df)
15: f.col params,f.bodyâ† INFER COLPARAMS (f,dfvar)
16: ifVERIFY (f)then
17: funcsâ†funcsâˆª{f}
18: return funcs
For example, suppose b=3 and var= df_train and the slice
under consideration is the one in Figure 3. The graph Gr
would not contain the edges 1 and 3 in Figure 5 as they
originate right after the statement at line 3 (before b), and arise
due to the use of the dataframe variable df_train . The edge
2i s included as it originates afterb.
Lines 11-12 conï¬rm that the selected statements which
appear before the selected program point bdo not involve
1331def visualization(df):
2 import seaborn assns
3 sns.heatmap(df.corr())
Fig. 7: A visualization function taking no arguments.
the use of variable var. This prevents any dependency on a
possibly stale version of val df. Then, we infer column param-
eters by simply replacing all string constants that correspond
to a column name in val dfwith parameter variables (line 15).
In Figure 3, this corresponds to the string "Age" in lines
5 and 6. We also rewrite attribute based column-accessesof dataframes, such as
df.Column asdf["Column"] prior to
applying this procedure. We denote the mapping from thesecolumn parameters to the string constants as
ORIGCOLS (f).
We refer to the selection of val dfasORIGDF(f).
Finally, in line 16, we verify if running the visualization
function with val dfi.e. ORIGDF S(f)and ORIGCOLS (f)repro-
duces the visualization from the parent visualization slice.Figure 6 contains the two visualization functions extractedfrom the visualization slice in Figure 3. Observe that nochoices for a dataframe variable would be available if we pickthe program point bas either 1 or 2.
In this way we are able to obtain 9740 visualization func-
tions across 1188 Kaggle notebooks. Additionally, for eachvisualization function, we also have access to the original
dataframe and column arguments needed to reproduce thevisualization as seen in the parent notebook via
ORIGDF and
ORIGCOLS. We utilize this information heavily when analyzing
these functions and using them for synthesizing visualizationsin the next two sections.
F . Participating Columns vs. Column Parameters
Visualization functions have dataframe and column pa-
rameters. It is important to note that column parameters do
not necessarily correspond to the exact subset of columnsthat actually participate in the visualization. For example,
the function in Figure 7 accepts no column arguments, butproduces a correlation heatmap of all the numeric columnsin the passed dataframe. We call such columns implicitly
participating columns. Consequently, we call a column asexplicitly participating if it is passed as a column argument.
We can decide if a column is implicitly participating using
a simple mutation-based strategyâ€”for every column cin
ORIGDF(f)that is not mapped in ORIGDF S(f), we drop cfrom
ORIGDF(f)and check if the visualization is the same after
executing the function. If it is not, the column cis implicitly
participating.
We denote the set of columns visualized (explicit
or implicit) by ffor the dataframe ORIGDF(f)as
ORIGPARTICIPA TING COLS (f). This notion of participation is at
the heart of the reusability analysis as well as visualizationsynthesis as we shall see next.
V. A
NALYSIS OF MINED VISUALIZA TION FUNCTIONS
Before we use the generated visualization functions for
synthesis, we need to assess their quality. What makes a mined1def visualization(df, col1):
2 import matplotlib.pyplot asplt
3 counts =df[col1].value_counts()
4 porct =counts/1460 *100
5 label =[]
6 for iinrange(len(counts)):
7 label.append(counts.index[i] +""+'{0:.2f}
8 sizes =[1141, 286, 13, 11, 7,2]
9 colors =['steelblue', 'skyblue', 'navy',
10 'blue', 'red', 'green']
11 fig, ax =plt.subplots()
12 ax.pie(sizes, colors=colors, shadow=False,
13 startangle=0)
14 ax.axis('equal')
15 ax.legend(label, shadow=True)
Fig. 8: A visualization function with hard-coded values.
1def visualization(df, col1, col2):
2 import seaborn assns
3 sns.set(font_scale=2.5)
4 df[df[col1] == 1][col2].hist()
Fig. 9: A visualization function using a specialized predicate.
visualization function â€œgoodâ€ (or â€œbadâ€) in the context of
synthesis? Since synthesis, by its very nature, involves the con-struction of visualizations for an unseen dataframe, a visualiza-
tion function should be considered â€œgoodâ€ or reusable if, given
appropriate assignments to column parameters, it producesmeaningful visualizations for a broad class of dataframes, and
â€œbadâ€ or non-reusable otherwise. We illustrate our notions of
meaningful and broad using examples.
Consider the visualization function in Figure 8. Note that the
data-values passed to
ax.pie in line 12 are hard-coded in the
function. That is, regardless of the dataframe and categoricalcolumn passed to the function, the produced visualization willbe exactly the same. The produced visualization is thus notmeaningful. If this function is used in a visualization synthesissetting, its resulting visualization would most likely make nosense to the user, and could undermine trust in the system.Thus we deem this function to be non-reusable. This also
illustrates why a successful execution of a function does notnecessarily entail a meaningful visualization.
In contrast, we consider the function in Figure 7 as â€œgoodâ€
orreusable. It will correctly produce a correlation heatmap for
the class of dataframes that have at least one numeric column.This class clearly includes a wide variety of dataframes andhence we consider this function reusable.
Figure 9 presents a much more subtle scenario. The function
plots a histogram of the values in
col2 , but only considers the
rows where the value corresponding to col1 is1. This ï¬ltering
criteria is quite arbitrary and only meaningful for dataframesthat contain a
1. We thus deem this function non-reusable..
A. Deï¬ning Reusability
We consolidate the ideas developed in the above discussion
in the following deï¬nition of reusability
Deï¬nition 4 (Reusable Visualization Function). We consider
a visualization function freusable if there exists a set Sdfof
dataframes such that:
1341)fproduces a meaningful visualization for every dataframe
dfinSdf, given an appropriate assignment of dfâ€™s columns
tofâ€™s parameters. A meaningful visualization is non-empty
and represents all the information in dfor a ï¬ltered view
ofdfwhere the ï¬ltering criterion is independent of the
concrete data values in df.
2)Sdfcan be characterized using high-level properties of a
dataframe and its columns including types of columns and
types of data values, but excluding properties relying onarbitrary constants or values in the data.
Note that a meaningful visualization need not follow best
visualization design practices that would make it â€œmeaningfulâ€for an end-user. With reusability, we are only concerned aboutits relationship to the data and the visualization function code.
Ideally, we would like to be able to automatically clas-
sify our mined visualization functions as reusable and non-reusable and discard the non-reusable functions. However itis hard to automatically check if a visualization functionis reusable according to Deï¬nition 4 as we do not haveaccess to S
df. Essentially, we are faced with the problem of a
missing test-oracle [26]. We present a novel approach of using
metamorphic-testing [27] to alleviate this issue.
B. Metamorphic Testing for Checking Reusability
Metamorphic testing relies on a metamorphic relation (MR):
a property that must be satisï¬ed by the outputs of a function
for different inputs. Our choice of this property for a visual-ization function fis deï¬ned as follows:
Deï¬nition 5 (MR for Approximating Reusability). Visual-
izations produced by fon mutated copies of its original
dataframe i.e.
ORIGDF(f)must all be different from each other
as well as the original visualization of f.
These mutated dataframes are produced using column-level
type-aware mutation operators. Deï¬nition 5 along with these
mutation operators approximates the concept of reusability inDeï¬nition 4 in two ways. First, these operators only modifyone column, and take the column type (categorical, quantita-tive etc.) into account. This helps increase the likelihood ofstaying within the class of dataframes fis appropriate for.
It also ensures that this class is characterizable using simpleproperties like column types. Second, the mutations appliedare large enough to warrant a change in the visualizationifftruly produces a visualization that represents all the
information in the dataframe or a meaningful subset of it. Thishelps catch cases like Figure 8 and Figure 9
Algorithm 3 formalizes our metamorphic testing strategy.
For every visualized columncâˆˆ
ORIGPARTICIPA TING COLS (f),
we check if there exists a mutation operator for which themetamorphic relation is satisï¬ed for the mutated dataframes itgenerates. Every mutation operator has a guard that must betrue for it to be applicable (line 7).
Our mutation operators for columns take the type of the col-
umn into account and are listed in Table I. We recognize fourdistinct types of columns, namely categorical, quantitative, IDAlgorithm 3 Checking Reusability using Metamorphic Testing
1:function ISREUSABLE (f)
2: dforigâ† ORIGDF(f);Î½origâ† ORIGVIZ(f)
3: for each câˆˆORIGPARTICIPA TING COLS(f)do
4: successâ†false
5: for each mutation operator mfor COLTYPE(c,dforig)do
6: sâ†initialize m
7: ifGUARD (m,dforig,c,s)then
8: df1,...,dfkâ†m(dforig,c,s)
9: Î½1,...,Î½ kâ†viz produced by fondf1,...,dfk
10: if(âˆ€i. Î½i/negationslash=Î½orig)âˆ§(âˆ€i/negationslash=j. Î½i/negationslash=Î½j)then
11: successâ†true
12: break
13: ifsuccess is false then
14: return false
15: return true
and nominal. At a high-level, for each type of column, wedesign a mutation operator for each of the different ways inwhich a column of that type may participate in a visualization.We walk through the operators for the two most common typesof columns: categorical and quantitative.
a) Categorical Columns: The visualization may be a
function of either (1) the individual category labels in thecolumn, or (2) the count distribution of categories or (3)whether a value is a NaN (missing value). Note that thevisualization may represent a function of these properties,
which may not necessarily be identity. The ï¬rst operator inTable I selects a ï¬xed subset of values and replaces themwith one or more unseen categories. Thus, if a function relieson hard-coded values or too arbitrary a ï¬ltering process, theresulting visualizations should be the same and thus fail thecheck. The second operator enables the check of whether thevisualization is sensitive to whether values are NaNs or not,rather than their concrete values themselves. It also has a guardwhich checks whether substituting the same missing valueswith different categories yields the same result as the original.This ensures that cases like Figure 9 do not pass the check.
b) Quantitative Columns: The visualization may be a
function of either (1) the values, or (2) a statistical function ofthose values or (3) whether a value is a NaN. The ï¬rst operatorshifts and scales the data by different amounts and addssome Gaussian noise, thus testing (1) and (2). We add noisebecause some statistical functions such as Pearson correlationare robust to uniform scaling and shifting. The magnitude ofthe shift is at least as large as the range of values to ensurezero overlap with the original range of values. Null values arehandled similarly as in categorical columns.
The mutation operators for ID and nominal columns are
designed using similar principles. V
IZSMITH is able to discard
26% of mined functions by classifying them as non-reusablevia this approach. We evaluate how well the metamorphic test-ing approach approximates the main deï¬nition of reusabilityin Section VII-B.
135TABLE I: Column Mutation Operators
Column Type Mutation Operator Guard
Categorical Replacing a ï¬xed subset of values with new categories -
Quantitative Shifting and Scaling V alues + Gaussian Noise -
Categorical/Quantitative Replacing a ï¬xed subset of values with missing values Replacing the same subset with arbitrary values does not change visualization
ID Random Permutation -
Nominal Replace a subset of values with a sample from the remaining values -
Nominal Replacing a ï¬xed subset of values with missing values Replacing the same subset with values sampled from the column does not
change visualization
VI. V ISUALIZA TION SYNTHESIS
VIZSMITH accepts a user speciï¬cation comprising
dataframes, a list of columns in each dataframe that need
toparticipate in the visualization, and a search query.
VIZSMITH uses the search query to get a ranked list of
visualization functions from the database obtained usingthe mining and analysis components from Sections IV andV. Then for each function, V
IZSMITH determines the best
possible assignments to the dataframe and column arguments,runs the function, collects the visualizations generated andpresents them to the user after deduplication.
A. Search
V
IZSMITH associates each mined visualization function
with a text document that contains (a) the natural language
comments around the visualization statements in the parentnotebook, (b) the text in the title and axis labels of thevisualization in the parent notebook, (c) the names of theAPI functions used and (d) the API documentation of the APIfunctions used in the visualization function. We collect com-ments from the notebook under the assumption that authorsoften attach meaningful comments describing the logic in andbefore/after cells, although this may not always be true.
Given a search query, we rank documents according to their
similarity with the search query using BM25 [28]. To obtaina ranked list of visualization functions, we simply map thedocuments back to their respective visualization functions.
B. Generating Visualizations
V
IZSMITH adapts the ranked visualization functions to the
user-provided dataframe using the INSTANTIA TE function in
Algorithm 4. It takes as input the mined visualization function
f, the user supplied dataframe dfand the columns that must
participate in the visualization vcols.
In the ï¬rst phase (lines 3-4), the set of mappings from the
column parameters of fto a subset of vcols is computed. A
mapping is valid if (a) it has a non-zero score, and (b) thecolumns in vcols that have not been assigned to a parameter
as per the mapping are eligible to be visualized implicitly.
The
SCORE function computes the score of a mapping mfor
a visualization function fby comparing mtoORIGCOLS (f).
Recall that ORIGCOLS (f)is the mapping column parameters to
the string values in the parent visualization slice of f. Essen-
tially SCORE checks the compatibility between the columns
using high-level properties such as column, data-types andpresence of null values.
We consider a column eligible to participate implicitly
(
ISIMPLICIT CAND ), if there exists a column in the original setAlgorithm 4 Instantiating Visualization Functions
1:function INSTANTIA TE (f,df,vcols)
2: Vâ†âˆ… ; params â† COLPARAMS (f)
3:Mâ†set of all injective maps from params to vcols
âŠ¿Score is non-zero and every col in vcol is mapped to a
param or potentially implicit
4:Mvalidâ†{m|mâˆˆMâˆ§SCORE (f,df,m )>0âˆ§âˆ€câˆˆ
vcols. (ISIMPLICIT CAND(f,df,c)âˆ¨âˆƒpâˆˆparams.m [p]=c)}
5: for each minRANK (M valid,SCORE )do
6:Î½â†f(df,m )
7: ifÎ½is valid then
8: Vâ†Vâˆª{Î½}
9: return V
10: function SCORE (f,df,m )
11: dforigâ† ORIGDF(f);morigâ† ORIGCOLS(f); scoreâ†0
12: for each pâˆˆCOLPARAMS (f)do
13:cmâ†m[p];corigâ†morig[p]
âŠ¿Column-types must match for the mapping to be valid
14: ifCOLTYPE(dforig,corig)/negationslash=COLTYPE(df,cm)then
15: return 0
16:dorigâ† DT YPES (dforig,corig);dmâ† DT YPES (df,cm)
17: scoreâ†score + (|dorigâˆ©dm|/|dorigâˆªdm|)
18: ifHASNULLS (dforig,corig)= HASNULLS (df,cm)then
19: scoreâ†score + 1
20: return score
of implicitly participating columns of fwhich has the same
column-type and data-types. The rationale is that if a columnparticipates implicitly, the criteria determining its participatingis most often a function of the column and data types.
In the second phase (lines 5-9), the mappings are tried one-
by-one, highest-score ï¬rst. All the unique valid (non-empty)visualizations collected are returned at the end.
VII. E
V ALUA TION
We focus on three main research questions (RQs) to evaluate
VIZSMITH . In RQ1, we analyze the diversity of the mined vi-
sualization functions. Speciï¬cally, we explore the distributionsover the size of the functions, the APIs explored, and whethera function performs data pre-processing. RQ2 evaluates ourmetamorphic testing approach to computing reusability againsta ground truth established via a manual study. Finally, in RQ3,we evaluate end-to-end synthesis performance of V
IZSMITH .
A. RQ1: How diverse is the collective functionality of allvisualization functions?
As it is infeasible to manually examine each function and
classify its functionality, we approximate it as the set ofAPI functions used in the body of the visualization function.Note that we only consider functions classiï¬ed as reusable
136TABLE II: Competition Statistics. # notebooks is the number
of notebooks eligible for execution. /check,âˆ…,/latticetop,Ã—indicate that at
least one viz was mined, no visualizations mined, timeout anderror respectively. # viz. funcs is the number of visualizationfunctions mined with reusable count in brackets.
competition # viz. funcs
(passed quality assurance)
LANL-Earthquak e-Prediction 90 (86)
covid19-global-forecasting-week-1 397 (212)
house-prices 3832 (2772)
mercari-price-suggestion-challenge 120 (67)
mercedes-benz-greener-manufacturing 95 (64)
otto-group-product-classiï¬cation 68 (68)
santander-customer-satisfaction 39 (23)
santander-value-prediction-challenge 64 (47)
titanic 4745 (3604)
tmdb-box-ofï¬ce-prediction 290 (233)
total 9740 (7176)
TABLE III: Top-10 API functions in each category, and thenumber of reusable viz. functions that use the API.
plotting transform computation styling
sns.heatmap (1064) pd.groupby (367) pd.corr (687) mpl.title (948)
sns.countplot (1019) pd.drop (316) pd.isnull (352) sns.set (882)
sns.distplot (827) pd.ï¬llna (308) pd.mean (241) mpl.ylabel (707)sns.barplot (629) pd.sort
values (264) pd.sum (221) mpl.xlabel (565)
sns.boxplot (576) pd.dropna (235) pd.value counts (217) mpl.xticks (377)
sns.factorplot (370) pd.concat (72) pd.replace (126) sns.set style (344)
mpl.scatter (304) pd.reset inde x (63) pd.isna (80) mpl.set title (302)
sns.scatterplot (213) pd.pivot table (53) pd.median (67) mpl.legend (252)
mpl.hist (212) pd.get dummies (52) pd.nlargest (63) sns.add legend (207)
sns.catplot (180) pd.head (35) pd.count (61) mpl.set ylabel (176)
by V IZSMITH . We ï¬nd that all mined functions collectively
exercise a total of 289 API functions across 12 third-party
libraries. We further bucket each API function manually intofour categories using simple criteria, namely (a) plotting ifit draws a visualization, (b) transformation if it involves re-shaping or ï¬ltering operations such as transpose, groupby anddropping null rows, (c) computation if it involves mathematicaloperations such as correlation and skew and (d) styling if itonly modiï¬es the look of a visualization or the text inside it.
We ï¬nd that 100%, 27%, 38% and 80% of visualization
functions use APIs in categories (a), (b), (c) and (d) respec-tively. The top-10 API functions in each category with respectto the number of visualization functions using the API arelisted in Table III. Evidently, V
IZSMITH â€™s database covers a
wide variety of plotting, styling and transformation operations.
B. RQ2: How accurate is our metamorphic testing approach?
Section V introduced the conceptual deï¬nition of reusability
of visualizations. We also proposed an approach using meta-
morphic testing where the metamorphic relation approximatedthis concept of reusability. In this RQ we measure the accu-racy, precision and recall of this metamorphic testing approachwith respect to a ground truth obtained via manual inspectionof the visualization functions using the conceptual deï¬nition.
We sampled 50 reusable and 50 non-reusable visualization
functions as judged by our metamorphic testing approach. Wethen designed an interface that displays these 100 functionsTABLE IV: Characterization of misclassiï¬cations by our meta-morphic testing approach. FP and FN stand for false positiveand false negative respectively
ID Category Num. Cases
A Arbitrary Filtering using Multiple Columns (FP) 3
B Undetected Over-Specialization (FP) 4
C Visualization Design Choices (Bucketing/Axis-Limits) (FN) 4
D Overaggressive Mutation (FN) 14
E Adequately General Filtering Criterion (FN) 4
one-by-one in a random order. Three of the authors labelledeach function as reusable or non-reusable as per Deï¬nition 4.We computed the ground-truth label via majority vote. In par-ticular, the authors try to assess the intent of the visualization,the class of dataframes where a similar visualization would bemeaningful and whether the implementation would be able toproduce that visualization without any modiï¬cations.
We ï¬nd the accuracy of the metamorphic approach to be
71%, with a precision of 73% and recall of 71%. Therewere 7 false positives (ground-truth non-reusable, classiï¬edreusable) and 22 false negatives. We categorized these casesin Table IV. The category column summarizes the reasonfor the misclassiï¬cation of the metamorphic testing approach.Examples of these categories are shown in Figure 10.
The 7 false positives occur because our mutation operators
are only applied on one column at a time (category A), or thecode performs overly speciï¬c transforms that are not triggeredby mutations (category B), and hence pass metamorphic test-ing check. The majority of the false negatives occur because ofover-aggressive mutation (category D). The example in Figure10 uses a log function that throws an error when our mutationintroduces negative values. In 4 cases, the design choice ofusing bucketing or changing the axis limits led to the samevisualizations being produced despite the mutations (categoryC). Finally, there were 4 cases in Category E where theï¬ltering was not arbitrary (all positive values), but was judgedto be the case by our approach. All categories except E canbe handled by a more sophisticated mutation scheme or ï¬ner-grained operators. Category E would require a pre-deï¬nednotion of what is an adequately general ï¬ltering criterion.
C. RQ3: Effectiveness of Synthesis Approach
Finally, we evaluate the end-to-end synthesis performance
of V
IZSMITH . We reuse the Kaggle notebooks utilized for
mining to create benchmarks. For every visualization slice we
extracted in Section IV-C, we select a visualization functionand create a benchmark where the dataframe corresponds tothe original dataframe i.e.
ORIGDF(f)and the columns to vi-
sualize are ORIGPARTICIPA TING COLS (f). The natural language
query is set to the text document associated with fas described
in Section VI-A. We select the largest visualization function, interms of statements, whose statements all come from the samecell in the parent notebook. The rationale is that this simulatesa real usage scenario for V
IZSMITH as notebook cells often
correspond to a single semantic unit of work. We also only
137defvisualization (df, col1, col2): 
import matplotlib.pyplot asplt
train_df =df.drop(
df[
(df[col1] >4e3) & (df[col2] <3e5)
].index 
)
plt.scatter(train_df[col1], 
train_df[col2])defvisualization (df, col1):
import seaborn assns
df[col1] =df[col1] .fillna("S")
df[col1] =df[col1] .map({"S": 0,"C":1,"Q":2})
sns.heatmap(df .corr(), annot =True)
defvisualization (df, col1):
import matplotlib.pyplot asplt
df[col1] .hist(bins =5, grid =False)
plt.xlabel(col1)defvisualization (df, col1, col2):
import numpy asnp
import matplotlib.pyplot asplt
df[col1] =np.log(df[col1])
plt.scatter(df[col1],df[col2])
defvisualization (df, col1):
import seaborn assns
ms=df[df[col1] >0]
sns.barplot(ms .index, ms[col1]) 


Fig. 10: Examples of each category in Table IV.
consider reusable functions as benchmarks. This yields 3284
benchmarks in total.
For each benchmark, we create an instantiation of V IZ-
SMITH using only visualization functions mined from com-
petitions other than the one corresponding to the benchmark(leave-one-out cross-project). We then run V
IZSMITH as well
as a baseline version of V IZSMITH called V IZSMITH ALL
that searches over all visualization functions, including non-reusable functions on each benchmark till they generate 10visualizations or timeout after 60 seconds, whichever is earlier.
We ï¬nd that both V
IZSMITH and V IZSMITH ALLhave a top-
10 accuracy of 5%. That is, both have an exactly matching
visualization in the top-10 for only 5% of the cases. There aretwo possible reasons for this low performance: (a) the qualityof the natural language query is poor and (b) styling variationssuch as color schemes, rotation of tick labels and legendpositions will fail the matching visualization test. In a separatemanual study of a sample of 100 visualization functions withassociated natural language comments, we found only 17%to actually describe the kind of plot and the columns beingvisualized. Thus (a) is a distinct possibility. To mitigate theeffects of (b), we sample 50 benchmarks and examine theresults of both the tools manually. In particular, we ignorestylistic variations such as color schemes, rotations of ticklabels, legend positions etc. while comparing the visualizationswith the ground truth. The top-10 accuracy in this case is56% and 46% for V
IZSMITH and V IZSMITH ALLrespectively.
Although the numbers are close, the difference lies in thenumber of functions explored. V
IZSMITH explores 50% less
visualization functions than V IZSMITH ALLwhile still getting
slightly better accuracy as it only searches over reusablefunctions which we hypothesized to be more useful duringsynthesis than their non-reusable counterparts. Hence, wedemonstrate the utility of reusability analysis to improve end-to-end synthesis performance.
VIII. L
IMITA TIONS AND THREA TS TO VALIDITY
A. Real-World Usage
We have not performed an explicit user study to gauge the
performance of users using V IZSMITH on real-world visual-
ization authoring tasks. Hence, the results in Section VII-Cmay not apply to real use cases. Note that performing sucha study would require careful experimental design to decou-ple the techniques behind V
IZSMITH from the quality ofthe mined code as well as the associated natural languagecomments, which are often imprecise or even irrelevant. Toenable external assessment, we have released a fully func-tioning prototype of V
IZSMITH along with a simple UI at
https://github.com/rbavishi/vizsmith-demo.
B. Code Licensing and Security
VIZSMITH â€™s database is populated using code written by
data scientists and machine learning practitioners that is
publicly available on the Internet. As such, code snippetsreturned by V
IZSMITH may not be appropriate for use in
certain contexts due to the license of the parent notebookcontaining the code snippet. This can be mitigated by passingan appropriately vetted corpus to V
IZSMITH . Security may
also be a concern since V IZSMITH executes every function in
its database as part of its metamorphic testing phase. We couldmitigate this by adding extra checks to ï¬lter out functionswith excessive resource consumption, unauthorized ï¬le systemaccess, or network requests.
C. Construct V alidity
All three research questions involve manual analysis and
thus have a subjective component. For RQ1, we classiï¬ed the
functions manually. To reduce the effect of subjectivity, weprovided simple and easily reproducible criteria for arrivingat this classiï¬cation. For RQ3, we analyzed the generatedvisualizations manually because it is hard to automaticallyidentify stylistic variations in a reliable manner. We preciselylisted down the classes of stylistic variations that we ignorewhile comparing two visualizations. Judging reusability as perDef. 4 involves manual inspection of the code, the data, and thevisualization. Thus, RQ2 has a higher risk of imprecision thanRQ1 and RQ3. We mitigated this by having three reviewersindependently judge reusability and taking the majority vote.We also assessed the misclassiï¬cations qualitatively, and cameup with general characterizations of the failure cases.
IX. R
ELA TED WORK
We compare V IZSMITH against existing visualization au-
thoring systems along the dimensions of intended usage, theuse of code templates, the kind of speciï¬cations used and theaspect of learning from data. We also compare and contrastapplications of code mining and reuse in other domains.
138A. Visualization Authoring Systems
1) Exploratory Data Analysis: To facilitate data explo-
ration, systems such as V oyager [29] and Draco [30] ac-
cept partial visual speciï¬cations containing the columns to
visualize as well as wildcards to generate a collection ofvisualizations capturing different views of the data in a targetgrammar such as V ega-Lite [31]. These visualizations are ï¬l-tered and ranked based on either manually designed heuristics[29], [32] or using constraint solving [30]. These heuristicsensure conformance to best visualization design practices.These systems are very useful for quickly exploring data andgathering insights while V
IZSMITH is mostly intended for
searching for a speciï¬c visualization. Nevertheless, V IZSMITH
can also beneï¬t from incorporating the design heuristics tobetter rank its output visualizations.
2) Reusable Visualization Templates: Ivy [33] allows users
to build a speciï¬c visualization by choosing from a cata-logue of visualization templates, which are similar in spiritto the reusable functions mined by V
IZSMITH . However
these templates are manually derived while V IZSMITH uses
a combination of program analysis and metamorphic testingto collect its reusable functions.
3) Visualization Speciï¬cations: Similar to V
IZSMITH ,a
number of visualization authoring systems accepting natural-language speciï¬cations from the user have been developed[34]â€“[36]. These systems use a carefully designed grammaror automaton to parse natural language queries describingthe desired visualization and keep track of the interactioncontext. This grants users ï¬ne-grained control over the pro-duced visualization. However the use of such a ï¬xed grammarlimits the space of visualizations that can be generated. Dueto its use of mining, V
IZSMITH can target different kinds
of visualizations such as word-clouds, visualizations usingdifferent APIs and richer data transformation code such ascomputing correlations and cross-tabulations on top of thesorting, ï¬ltering and aggregation functionalities offered bythe above systems. Finally, although this work only exploresa simple keyword-based search to match snippets with userqueries, recent advances in natural-language processing (NLP)[37], [38] could be leveraged to improve the search.
Richer modes of speciï¬cation have also been explored.
Falx [17], [18] allows users to provide pieces of the targetvisualization and the system utilizes program synthesis to gen-erate the required data transformation and visualization code.This form of speciï¬cation captures a lot more informationabout the desired visualization and thus Falx can complement
V
IZSMITH in cases where a very speciï¬c visualization is
desired and cannot be described accurately with keywords.
4) Learning from Data: Data2Vis [39] trains deep learning
models on pairs of dataset and visualization speciï¬cationsobtained from the V ega-Lite corpus [40] and recommendsvisualizations given a dataset at inference time. However, itdoes not grant control over the columns or ï¬elds that arevisualized which would force the user to pick out the desiredvisualization from a large set. PlotCoder [41] is the closestto V
IZSMITH in that it generates visualization code fromnatural language that contains information about columns tovisualize. However it restricts the set of visualizations to asubset of matplotlib, and cannot generate transformation codelike V
IZSMITH .
B. Code Mining and Reuse
Code mining and reuse has been employed in a number of
other applications. The EG system [42] uses static analysisacross large code-bases to build a database of usage examplesfor APIs which can be queried. However the examples are notalways executable and thus their output cannot be shown onthe userâ€™s input. For visualization synthesis, it is essential forthe user to see the output visualization on their data to selectthe one that meets their needs, thus such an approach wouldnot work in our problem setting.
AutoType [43] and TDE [44] synthesize executable pro-
grams using mined code corpora for type-validation and stringtransformation respectively. Phoenix [45] and Getaï¬x [46]induce repair patterns from mined static analysis repairs. Allthese domains offer precise speciï¬cationsâ€”positive and nega-tive examples for a type, input/output pairs for transformationsand a pass/fail from the static analyzer. This greatly simpliï¬esthe ï¬ltering of bad mined code as one can simply check themagainst the speciï¬cation. In V
IZSMITH â€™s setting, the lack of
such a precise target necessitates the use of techniques such asmetamorphic testing to weed out bad visualization functions.
Aroma [47] takes a different approach to reusability. It
accepts a partial code snippet as input and performs codecompletion by searching over a large indexed code corpus andintersecting the search results. This intersection step ensuresthe completed code only contains elements that are commonacross a sufï¬ciently diverse set of snippets and thus reusable.
X. C
ONCLUSION
We presented V IZSMITH , a tool which accepts a dataset,
columns to visualize and a text query from the user and synthe-sizes visualization code. First, in an ofï¬‚ine phase V
IZSMITH
mines Kaggle notebooks to create a database of 7176 reusable
Python functions. It uses a novel metamorphic testing ap-proach to automatically assess reusability of functions. Whenpresented with the user query, V
IZSMITH efï¬ciently searches
this database to ï¬nd relevant functions, execute them andreturn the generated visualizations. We evaluated V
IZSMITH
and found that it can suggest the right visualization for 56% ofthe benchmarks. We also found that using reusability analysishelps improve the quality of visualizations and reduces thesearch space by 50%. V
IZSMITH is available publicly at
https://github.com/rbavishi/vizsmith-demo.
XI. A CKNOWLEDGEMENTS
We thank Caroline Lemieux, Karan Bavishi, and all our
anonymous reviewers for their invaluable feedback on thispaper. This research is supported in part by a grant fromFujitsu Research of America, and NSF grants CCF-1900968,CCF-1908870, and CNS-1817122.
139REFERENCES
[1] E. Segel and J. Heer, â€œNarrative Visualization: Telling Stories with
Data,â€ IEEE Transactions on Visualization and Computer Graphics,
vol. 16, no. 6, p. 1139â€“1148, Nov. 2010. [Online]. Available:
https://doi.org/10.1109/TVCG.2010.179
[2] S. Kandel, A. Paepcke, J. M. Hellerstein, and J. Heer, â€œEnterprise data
analysis and visualization: An interview study,â€ IEEE Transactions on
Visualization and Computer Graphics, vol. 18, no. 12, pp. 2917â€“2926,2012.
[3] T. Kluyver, B. Ragan-Kelley, F. P Â´erez, B. Granger, M. Bussonnier,
J. Frederic, K. Kelley, J. B. Hamrick, J. Grout, S. Corlay, P . Ivanov,D. Avila, S. Abdalla, C. Willing, and J. D. Team, â€œJupyter Notebooks- a publishing format for reproducible computational workï¬‚ows,â€ inELPUB, 2016.
[4] H. Wickham, ggplot2: Elegant Graphics for Data Analysis. Springer-
V erlag New Y ork, 2016. [Online]. Available: https://ggplot2.tidyverse.org
[5] J. D. Hunter, â€œMatplotlib: A 2D graphics environment,â€ Computing in
Science & Engineering, vol. 9, no. 3, pp. 90â€“95, 2007.
[6] M. Gharehyazie, B. Ray, and V . Filkov, â€œSome from Here, Some
from There: Cross-Project Code Reuse in GitHub,â€ in Proceedings of
the 14th International Conference on Mining Software Repositories,ser. MSR â€™17. IEEE Press, 2017, p. 291â€“301. [Online]. Available:https://doi.org/10.1109/MSR.2017.15
[7] C. Sadowski, K. T. Stolee, and S. Elbaum, â€œHow Developers Search
for Code: A Case Study,â€ in Proceedings of the 2015 10th Joint
Meeting on F oundations of Software Engineering, ser. ESEC/FSE 2015.New Y ork, NY , USA: Association for Computing Machinery, 2015, p.191â€“201. [Online]. Available: https://doi.org/10.1145/2786805.2786855
[8] Y . Wu, S. Wang, C.-P . Bezemer, and K. Inoue, â€œHow Do Developers
Utilize Source Code from Stack Overï¬‚ow?â€ Empirical Softw.
Engg., vol. 24, no. 2, p. 637â€“673, Apr. 2019. [Online]. Available:https://doi.org/10.1007/s10664-018-9634-5
[9] Y . Wang, Y . Feng, R. Martins, A. Kaushik, I. Dillig, and S. P . Reiss,
â€œHunter: Next-Generation Code Reuse for Java,â€ in Proceedings of the
2016 24th ACM SIGSOFT International Symposium on F oundationsof Software Engineering, ser. FSE 2016. New Y ork, NY , USA:Association for Computing Machinery, 2016, p. 1028â€“1032. [Online].Available: https://doi.org/10.1145/2950290.2983934
[10] S. Bajracharya, J. Ossher, and C. Lopes, â€œSourcerer: An Infrastructure
for Large-Scale Collection and Analysis of Open-Source Code,â€ Sci.
Comput. Program., vol. 79, p. 241â€“259, Jan. 2014. [Online]. Available:https://doi.org/10.1016/j.scico.2012.04.008
[11] S. P . Reiss, â€œSemantics-Based Code Search,â€ in Proceedings of the
31st International Conference on Software Engineering, ser. ICSEâ€™09. New Y ork, NY , USA: Association for Computing Machinery,2009, p. 243â€“253. [Online]. Available: https://doi.org/10.1109/ICSE.2009.5070525
[12] S. Wang, D. Lo, and L. Jiang, â€œActive Code Search: Incorporating
User Feedback to Improve Code Search Relevance,â€ in Proceedings of
the 29th ACM/IEEE International Conference on Automated SoftwareEngineering, ser. ASE â€™14. New Y ork, NY , USA: Associationfor Computing Machinery, 2014, p. 677â€“682. [Online]. Available:https://doi.org/10.1145/2642937.2642947
[13] R. Cottrell, R. J. Walker, and J. Denzinger, â€œSemi-Automating
Small-Scale Source Code Reuse via Structural Correspondence,â€ inProceedings of the 16th ACM SIGSOFT International Symposiumon F oundations of Software Engineering, ser. SIGSOFT â€™08/FSE-16.New Y ork, NY , USA: Association for Computing Machinery, 2008, p.214â€“225. [Online]. Available: https://doi.org/10.1145/1453101.1453130
[14] X. Xia, L. Bao, D. Lo, P . S. Kochhar, A. E. Hassan, and Z. Xing,
â€œWhat Do Developers Search for on the Web?â€ Empirical Softw.
Engg., vol. 22, no. 6, p. 3149â€“3185, Dec. 2017. [Online]. Available:https://doi.org/10.1007/s10664-017-9514-4
[15] S. Sachdev, H. Li, S. Luan, S. Kim, K. Sen, and S. Chandra, â€œRetrieval
on source code: a neural code search,â€ in ACM SIGPLAN Workshop on
Machine Learning and Programming Languages (MAPLâ€™18), 2018.
[16] J. Cambronero, H. Li, S. Kim, K. Sen, and S. Chandra, â€œWhen
deep learning met code search,â€ in Industry Track of 27th ACM Joint
European Software Engineering Conference and Symposium on theF oundations of Software Engineering (ESEC/FSEâ€™19). ACM, 2019,
pp. 964â€“974.[17] C. Wang, Y . Feng, R. Bodik, I. Dillig, A. Cheung, and A. J. Ko,
â€œFalx: Synthesis-Powered Visualization Authoring,â€ arXiv e-prints,p .
arXiv:2102.01024, Feb. 2021.
[18] C. Wang, Y . Feng, R. Bodik, A. Cheung, and I. Dillig, â€œVisualization
by Example,â€ Proc. ACM Program. Lang., vol. 4, no. POPL, Dec.
2019. [Online]. Available: https://doi.org/10.1145/3371117
[19] â€œThe kaggle data-science platform.â€ [Online]. Available: https:
//www.kaggle.com/
[20] â€œV oice call quality customer experience.â€ [Online]. Available: https:
//data.gov.in/catalog/voice-call-quality-customer-experience
[21] â€œStacked bar chart.â€ [Online]. Available: https://matplotlib.org/stable/
gallery/lines
bars and markers/bar stacked.html
[22] â€œHow can i normalize data and create a stacked bar chart?â€
[Online]. Available: https://stackoverï¬‚ow.com/questions/57337796/how-can-i-normalize-data-and-create-a-stacked-bar-chart
[23] H. Agrawal and J. R. Horgan, â€œDynamic Program Slicing,â€ SIGPLAN
Not., vol. 25, no. 6, p. 246â€“256, Jun. 1990. [Online]. Available:https://doi.org/10.1145/93548.93576
[24] A. Zeller and R. Hildebrandt, â€œSimplifying and Isolating Failure-
Inducing Input,â€ IEEE Trans. Softw. Eng., vol. 28, no. 2, p. 183â€“200,
Feb. 2002. [Online]. Available: https://doi.org/10.1109/32.988498
[25] C. Sun, Y . Li, Q. Zhang, T. Gu, and Z. Su, â€œPerses: Syntax-
Guided Program Reduction,â€ in Proceedings of the 40th International
Conference on Software Engineering, ser. ICSE â€™18. New Y ork,NY , USA: Association for Computing Machinery, 2018, p. 361â€“371.[Online]. Available: https://doi.org/10.1145/3180155.3180236
[26] E. Weyuker, â€œOn Testing Non-Testable Programs,â€ Computer Journal,
vol. 25, 11 1982.
[27] T. Y . Chen, S. C. Cheung, and S. M. Yiu, â€œMetamorphic testing: a
new approach for generating next test cases,â€ Technical Report HKUST-
CS98-01,, 1998.
[28] G. Amati, BM25. Boston, MA: Springer US, 2009, pp. 257â€“260.
[Online]. Available: https://doi.org/10.1007/978-0-387-39940-9
921
[29] K. Wongsuphasawat, D. Moritz, A. Anand, J. Mackinlay, B. Howe,
and J. Heer, â€œV oyager: Exploratory Analysis via Faceted Browsingof Visualization Recommendations,â€ IEEE Trans. Visualization &
Comp. Graphics (Proc. InfoVis), 2016. [Online]. Available: http://idl.cs.washington.edu/papers/voyager
[30] D. Moritz, C. Wang, G. L. Nelson, H. Lin, A. M. Smith, B. Howe, and
J. Heer, â€œFormalizing Visualization Design Knowledge as Constraints:Actionable and Extensible Models in Draco,â€ IEEE Transactions on
Visualization and Computer Graphics, vol. 25, no. 1, p. 438â€“448, Jan.2019. [Online]. Available: https://doi.org/10.1109/TVCG.2018.2865240
[31] A. Satyanarayan, D. Moritz, K. Wongsuphasawat, and J. Heer,
â€œV ega-Lite: A Grammar of Interactive Graphics,â€ IEEE Transactions on
Visualization and Computer Graphics, vol. 23, no. 1, p. 341â€“350, Jan.2017. [Online]. Available: https://doi.org/10.1109/TVCG.2016.2599030
[32] J. Mackinlay, â€œAutomating the Design of Graphical Presentations of
Relational Information,â€ ACM Trans. Graph., vol. 5, no. 2, p. 110â€“141,
Apr. 1986. [Online]. Available: https://doi.org/10.1145/22949.22950
[33] A. McNutt and R. Chugh, â€œIntegrated Visualization Editing via Parame-
terized Declarative Templates,â€ arXiv e-prints, p. arXiv:2101.07902, Jan.
2021.
[34] T. Gao, M. Dontcheva, E. Adar, Z. Liu, and K. Karahalios,
â€œDatatone: Managing ambiguity in natural language interfacesfor data visualization,â€ Proceedings of the 28th Annual
ACM Symposium on User Interface Software & Technology- UIST â€™15, pp. 489â€“500, 2015. [Online]. Available:http://www.scopus.com/inward/record.url?eid=2-s2.0-84958249800{&}partnerID=40{&}md5=f0eb3ceb834a66e6d0eb6b59ffc57163
[35] V . Setlur, S. E. Battersby, M. Tory, R. Gossweiler, and A. X.
Chang, â€œEviza: A Natural Language Interface for Visual Analysis,â€Proceedings of the 29th Annual Symposium on User Interface Softwareand Technology - UIST â€™16, pp. 365â€“377, 2016. [Online]. Available:http://doi.acm.org/10.1145/2984511.2984588
[36] E. Hoque, V . Setlur, M. Tory, and I. Dykeman, â€œApplying Pragmatics
Principles for Interaction with Visual Analytics,â€ IEEE Transactions on
Visualization and Computer Graphics, no. c, 2017. [Online]. Available:dx.doi.org/10.1109/TVCG.2017.2744684
[37] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, â€œBERT: Pre-training
of deep bidirectional transformers for language understanding,â€ inProceedings of the 2019 Conference of the North American Chapterof the Association for Computational Linguistics: Human LanguageTechnologies, V olume 1 (Long and Short Papers). Minneapolis,
140Minnesota: Association for Computational Linguistics, Jun. 2019, pp.
4171â€“4186. [Online]. Available: https://aclanthology.org/N19-1423
[38] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P . Dhariwal,
A. Neelakantan, P . Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-V oss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler,J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray,B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever,and D. Amodei, â€œLanguage models are few-shot learners,â€ 2020.
[39] V . Dibia and C Â¸ . Demiralp, â€œData2Vis: Automatic Generation of Data Vi-
sualizations Using Sequence to Sequence Recurrent Neural Networks,â€arXiv e-prints, p. arXiv:1804.03126, Apr. 2018.
[40] J. Poco and J. Heer, â€œReverse-engineering visualizations: Recovering
visual encodings from chart images,â€ Computer Graphics F orum,
vol. 36, no. 3, pp. 353â€“363, 2017. [Online]. Available: https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13193
[41] X. Chen, L. Gong, A. Cheung, and D. Song, â€œPlotcoder: Hierarchical
decoding for synthesizing visualization code in programmatic context,â€inProceedings of the 59th Annual Meeting of the Association for
Computational Linguistics and the 11th International Joint Conferenceon Natural Language Processing, ACL/IJCNLP 2021, (V olume 1: LongPapers), Virtual Event, August 1-6, 2021, C. Zong, F. Xia, W. Li, andR. Navigli, Eds. Association for Computational Linguistics, 2021,pp. 2169â€“2181. [Online]. Available: https://doi.org/10.18653/v1/2021.acl-long.169
[42] C. Barnaby, K. Sen, T. Zhang, E. Glassman, and S. Chandra, â€œExempla
Gratis (E.G.): Code Examples for Free,â€ in Proceedings of the 28th
ACM Joint Meeting on European Software Engineering Conferenceand Symposium on the F oundations of Software Engineering,ser. ESEC/FSE 2020. New Y ork, NY , USA: Association forComputing Machinery, 2020, p. 1353â€“1364. [Online]. Available:https://doi.org/10.1145/3368089.3417052
[43] C. Yan and Y . He, â€œSynthesizing Type-Detection Logic for Rich
Semantic Data Types Using Open-Source Code,â€ in Proceedings of the
2018 International Conference on Management of Data, ser. SIGMODâ€™18. New Y ork, NY , USA: Association for Computing Machinery, 2018,p. 35â€“50. [Online]. Available: https://doi.org/10.1145/3183713.3196888
[44] Y . He, K. Ganjam, K. Lee, Y . Wang, V . Narasayya, S. Chaudhuri,
X. Chu, and Y . Zheng, â€œTransform-Data-by-Example (TDE): ExtensibleData Transformation in Excel,â€ in Proceedings of the 2018 International
Conference on Management of Data, ser. SIGMOD â€™18. New Y ork,NY , USA: Association for Computing Machinery, 2018, p. 1785â€“1788.[Online]. Available: https://doi.org/10.1145/3183713.3193539
[45] R. Bavishi, H. Y oshida, and M. R. Prasad, â€œPhoenix: Automated
data-driven synthesis of repairs for static analysis violations,â€ inProceedings of the 2019 27th ACM Joint Meeting on EuropeanSoftware Engineering Conference and Symposium on the F oundationsof Software Engineering , ser. ESEC/FSE 2019. New Y ork, NY , USA:
Association for Computing Machinery, 2019, p. 613â€“624. [Online].Available: https://doi.org/10.1145/3338906.3338952
[46] J. Bader, A. Scott, M. Pradel, and S. Chandra, â€œGetaï¬x: Learning to ï¬x
bugs automatically,â€ Proc. ACM Program. Lang., vol. 3, no. OOPSLA,
Oct. 2019. [Online]. Available: https://doi.org/10.1145/3360585
[47] S. Luan, D. Yang, C. Barnaby, K. Sen, and S. Chandra, â€œAroma:
Code recommendation via structural code search,â€ Proc. ACM
Program. Lang., vol. 3, no. OOPSLA, Oct. 2019. [Online]. Available:https://doi.org/10.1145/3360578
141