SynGuar: GuaranteeingGeneralizationinProgramming by
Example
BoWang
National Universityof Singapore
Singapore
bo_wang@u.nus.eduTeodoraBaluta
National Universityof Singapore
Singapore
teodora.baluta@u.nus.edu
AashishKolluri
National Universityof Singapore
Singapore
e0321280@u.nus.eduPrateek Saxena
National Universityof Singapore
Singapore
prateeks@comp.nus.edu.sg
ABSTRACT
ProgrammingbyExample(PBE)isaprogramsynthesisparadigmin
whichthesynthesizercreatesaprogramthatmatchesasetofgiven
examples. In many applications of such synthesis (e.g., program
repairorreverseengineering),wearetoreconstructaprogramthat
isclosetoaspecifictargetprogram,notmerelytoproducesome
program that satisfies the seen examples. In such settings, we wish
thatthesynthesizedprogram generalizes well,i.e.,hasasfewerrors
as possible on the unobserved examples capturing the target func-
tion behavior. In this paper, we propose the first framework (called
SynGuar)forPBEsynthesizersthatguaranteestoachievelowgen-
eralizationerrorwithhighprobability. Ourmaincontributionisa
procedure to dynamically calculate how many additional examples
suffice to theoretically guarantee generalization. We show how
our techniques can be used in 2 well-known synthesis approaches:
PROSE and STUN (synthesis through unification), for common
string-manipulationprogrambenchmarks.Wefindthatoftenafew
hundredexamplessufficetoprovablyboundgeneralizationerror
below5%with high (â‰¥98%) probability on these benchmarks. Fur-
ther,we confirm this empirically: SynGuar significantly improves
the accuracy of existing synthesizers in generating the right target
programs. But with fewer examples chosen arbitrarily, the same
baseline synthesizers (without SynGuar) overfitand lose accuracy.
CCS CONCEPTS
Â·Software and its engineering â†’Software notations and
tools;General programming languages.
KEYWORDS
Program Synthesis, Generalization, Sample Complexity
ACM Reference Format:
Bo Wang, Teodora Baluta, Aashish Kolluri, and Prateek Saxena. 2021. Syn-
Guar: Guaranteeing Generalization in Programming by Example. In Pro-
ceedings of the 29th ACM Joint European Software Engineering Conference
ESEC/FSE â€™21, August 23Å›28, 2021, Athens, Greece
Â© 2021 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-8562-6/21/08.
https://doi.org/10.1145/3468264.3468621andSymposiumontheFoundationsofSoftwareEngineering(ESEC/FSEâ€™21),
August 23Å›28, 2021, Athens, Greece. ACM, New York, NY, USA, 13pages.
https://doi.org/10.1145/3468264.3468621
1 INTRODUCTION
Programsynthesisisthegoalofautomaticallygeneratingcomputer
programs for a given task. This vision has existed for over at least
four decades [ 52,56,57]. One of the mainstream approaches to-
wards this goal is programming by example (or PBE) [ 27]. In its
simplestform,aPBEsynthesizerisgivenaccesstoan oraclethat
can generate correct input-output (I/O) examples for the unknown
targetprogram. The synthesizer has to create a candidate program
ascloseaspossibletothetargetprogramfromapre-specified hy-
pothesisspace,i.e.,thespaceofallpossiblecandidateprogramsthat
thesynthesizer canreason about. Thenumberof givenI/Oexam-
plescan varydependingonthe endapplication,butthe fewerthe
better. Therein lies the challenge of generalization: If examplesare
toofew,then many possiblecandidate functions satisfy them, and
picking one arbitrarily might yield a solution that works well only
ontheseenexamples.Inotherwords,thesolutionoverfitstothe
seenexamplesandmaynotgeneralizewell.Howdoesasynthesizer
createprogramsthatareprovablyclosetothetargetprogram?This
has been a fundamental question for PBE-based program synthesis.
There are several domain-specific solutions to generalization.
In program repair as well as in inductive synthesis, for example,
inferring additional specifications from observed examples that
must be satisfied by the program is shown to help with generaliza-
tion [6,24,30,33]. Allowing the synthesizer to use more powerful
oracles that adaptively craft examples or logical invariants help
to synthesize correct programs [ 22,31]. In neural-guided program
synthesis [ 13,17,45,54], machine learning techniques to avoid
over-fitting such as regularization or structural risk minimization,
are employed implicitly [ 59]. In domains where we have prior
knowledge about the likely distribution to which the target pro-
gram belongs, synthesizers can rank solutions [ 55], guide program
search [33], and use generative models for program representa-
tion or distributional priors [ 21]. Some synthesizers favor short
programs as per Occamâ€™srazor [25].
All of the above approaches, while useful, require additional
knowledge or implicit assumptions about the target program be-
yondthatcapturedbytheoriginalPBEproblemsetup.Withoutsuch
assumptions,theseapproachesdo notprovideanyformalguarantee
677This work is licensed under a Creative Commons Attribution International 4.0 License.
ESEC/FSE â€™21, August 23â€“28, 2021,Athens,Greece BoWang,Teodora Baluta, AashishKolluri, andPrateek Saxena
thattheproducedprogramwillbecorrectorgeneralizewellonun-
seenexamples.Itisnaturaltoask:Canweguaranteegeneralization
withoutmakingany additionaldomain-specific assumptions?
In this paper, we study generalization in PBE from the perspec-
tive ofsample complexity : How many I/O examples should the
synthesizer have to see to be confident that its selected solution
is close to the target program? To answer this question, the PAC
learning theory provides a starting point [ 10,58]. A synthesizer
generalizeswellwhenthesynthesizedprogramisclosetothetar-
getprogramwithhighconfidence.Thenotionofconfidenceand
closeness to the target program can be made formal using PAC
learningtheory.Specifically,the synthesizedprogram generalizes
if itwill makeno more than a small fraction ğœ–of errorson unseen
examplestakenindependently,withhighprobability(atleast1 âˆ’ğ›¿).
OurapproachworksonanydistributionthattheI/Oexamples
are sampled from. To formally guarantee that generalization is
achievedonthedistribution,weneedaprincipleddesignforPBE
synthesizers.ExistingPBEsynthesizersarenotdesignedtoprovide
generalizationguarantees;therefore,theypickthenumberofI/O
examplestoworkwithinanad-hocfashion.Forinstance,theymay
synthesizeaprogramafterseeingonly2 âˆ’4examples[ 26,55].This
paper seeksto answer the following questions:
RQ1.How many I/Oexamples would a synthesizer need to see in
order to provablygeneralize?
RQ2.Doexisting synthesizersoverfitwith,say,2 âˆ’4examples?
As a conceptual contribution, we present the first principled
framework, SynGuar1, to provide generalization guarantees about
the synthesized programs. We propose a procedure that computes
the size of the hypothesis space dynamically during synthesis,
whichisthenusedtocalculatethesamplesizerequiredtoprovably
generalize. The challenge is therefore two-fold. First, while effi-
ciently computing the size of the hypothesis space is easy in some
existing PBE synthesizers (such as the PROSE framework [ 46]), for
others it requires careful design. An example of the latter is the
synthesisthroughunification(STUN)[ 5]approach.Asourmain
technicalcontribution,wepresent anexample ofintegrating Syn-
Guarinto synthesizers based on PROSE and STUN frameworks.
Specifically, we provide two PBE synthesizers for string manipu-
lation programs that provably generalize, one implemented in the
PROSE framework ( SynGuar-PROSE ) and one based on the STUN
approach( SynGuar-STUN ).Tothebestofourknowledge,noprior
PBEsynthesizerclaimssuchstronggeneralizationguaranteesabout
the synthesizedprograms.
Fromanempiricalperspective,ourworkprovidesthefirstexper-
imentalevidenceforthenumberofexamplessufficienttoguarantee
generalization in practice for simple string-manipulation tasks. We
runSynGuar-PROSE andSynGuar-STUN on two benchmarks in
this domain: 1) manually designed data-wrangling tasks similar to
those used in FlashFill [ 46]; and 2) the standard SyGuS 2019 bench-
mark[1],respectively.Wefindthatontheirrespectivebenchmarks,
the tools produce programs which are provably within at most 5%
generalizationerrorwithamodestnumberofexamplesÃaround
197 samples and 357 examples on average, respectivelyÃwith a
highprobability(â‰¥98%).Thisobservationalsosuggeststhatitis
1ThetoolisavailablewithDOInumber10.5281/zenodo.4883273.Thelatestversion
canbefoundat https://github.com/HALOCORE/SynGuarlanguage StrPROSE;
@input string x;
@start string program := recTerm;
stringrecTerm := catTerm | Concat(catTerm, recTerm);
stringcatTerm := ConstStr(cs) | convTerm;
stringconvTerm := term | UpperCase(term) | LowerCase(
term);
stringterm := SubStr(x, pos1, pos2);
int? pos := AbsPos(x, ka) | RegPos(x, kr);
stringcs;//constant string
intka;//absolute position
Tuple<Regex,Regex,int,int> kr;// kr=(r1,r2,k,offset)
Figure1:ADSLforstringtransformationprograms. Concat
returns the string produced by concatenating two strings
catTerm andrecTerm,SubStrreturns the substring between
pos1andpos2. TheUpperCase andLowerCase return the
string in upper case and lower case, respectively. AbsPosre-
turns the absolute position of string x. TheRegPosoperator
outputs the kğ‘¡â„position plus an offsetwhere the bound-
ariesofthestringsreturnedbyapplyingregularexpressions
r1andr2,respectively,on string xmatch.
unlikelyforPBEsynthesizerstogeneralizefromjust2 âˆ’4exam-
ples without using some implicit or explicit additional knowledge.
Weconfirmthisobservationbyrunningthevanillaversions,i.e.,
versions with SynGuar disabled, on the same benchmarks with
4 randomly chosen examples or the given seed examples in the
benchmark(2âˆ’10insize).Wefindthat SynGuar-PROSE generates
thecorrecttargetprogramfor14 /16casesfromthedata-wrangling
task benchmark and SynGuar-STUN for 53/59 cases fromthe Sy-
GuSbenchmark. Incontrast,the vanillaversionsgenerate correct
programsfor0/16and36/59cases,respectively.Thisshowsthat
withoutenough examples, synthesizedprograms often overfit.
Though we focus on string-manipulating programs, our ap-
proach makes minimal additional assumptions and thus can be
extendedtootherapplicationdomains,suchasprogramrepair[ 24],
invariantdiscovery[ 9]andsoon.Thegeneralizationguaranteefits
well in applications such as data cleaning and transformation [ 16],
where a provable accuracy matters, or automatic stub writing in
symbolicexecution[ 38,53],wherethegoalistolearnasymbolic
constraint that is approximately close enough to the target. Our
experiments suggest that the sample size to achieve generaliza-
tionistaskandbenchmarkdependent,whichleavesthequestion
of how well our presented approach works in other domains or
benchmarks open. Theseare grounds for promisingfuture work.
2 OVERVIEW
In practice, it is hard to know how many I/O examples suffice
to solve a synthesis task.The number of I/O examplesacross vari-
oustargetprograms,evenforthesamesynthesizer,varyinprior
worksandarechosensomewhatarbitrarily.Forinstance,theSy-
GuS benchmark [ 1] hasa dedicated track forthe domain ofstring-
manipulating programs. The benchmark consists of several PBE
tasks andeach of themis providedwith a differentnumber of I/O
examples. Some have as low as 2 examples whereas others have 50.
678SynGuar : Guaranteeing Generalizationin Programmingby Example ESEC/FSE â€™21, August 23â€“28, 2021,Athens,Greece
// word \w+=(A-Za-z0-9)+, digit \d+=(0-9)+
// *.?= any character
Concat(// return substr until the end of 1st word match
SubStr(x,AbsPos(x, 0), RegPos(x, (\w+, .*?, 0, 0))),
Concat(ConstStr (","),// append ","
Concat(// return substr of 2nd word match until the end
of the word
SubStr(x,RegPos(x, (.*?, \w+, 1, 0)),
RegPos(x, (\w+, .*?, 1, 0))),
Concat(ConstStr (","),
Concat(// return the substr of 3rd word match until
the end of the word
SubStr(x,RegPos(x, (.*?, \w+, 2, 0)),
RegPos(x, (\w+, .*?, 2, 0))),
Concat(ConstStr (","),
// return the substr from the 4th word until the end
of the string
SubStr(x,RegPos(x, (.*?, \w+, 3, 0)),
AbsPos(x, -1))))))))Concat(// return substr of first two characters
SubStr(x, AbsPos(x, 0), AbsPos(x, 2)), overfits
Concat(ConstStr (","),
Concat(// return the string from the start of the first
number offset by 1 till the second last separator
SubStr(x, RegPos(x, (.*?, (-?\d+)(\.\d+)?, 0, 1)),
RegPos(x, (.*?, [\,\.\;\-\|], -2, 0))), overfits
Concat(ConstStr (","),
Concat(// return the substr from second capital letter
offset +2 to offset +3
SubStr(x, RegPos(x, (.*?, [A-Z]+, 1, 2)),
RegPos(x, (.*?, [A-Z]+, 1, 3))), overfits
Concat(ConstStr (","),
// return the substr from start of second word with
offset +3 till the end
SubStr(x, RegPos(x, (\w+, .*?, 1, 3)),
AbsPos(x, -1)))))))) overfits
Figure 2: On the left, we show the correct target program ğ‘¡that tokenizes the string xon the boundaries of the first 4 words.
On theright, we show theprogram synthesized on 2examples which overfits at allthefour SubStroperations.
#Ex. Input Output #ğ‘“
1"0E-E 2|u7kuZ85" "0E,E,2,u7kuZ85" â‰ˆ1042
2" J-3bJ.9;PPm" " J,3bJ,9,PPm" â‰ˆ1020
3"tpJ|AV n0d7 6z" "tpJ,AV,n0d7,6z" â‰ˆ1012
4"R 3|6VCs Q" "R,3,6VCs,Q" 304128
5"M x cSkrw ru6" "M,x,cSkrw,ru6" 304128
6"Wk U U nZp X " "Wk,U,U,nZp X " 864
7"gsa-ub hn lpa" "gsa,ub,hn,lpa" 216
8"RO8I3 R SuM|e " "RO8I3,R,SuM,e " 144
9"q E 0 LD0 " "q,E,0,LD0 " 144
10"dZPz T.Q s " "dZPz,T,Q,s " 144
11"Ny e87e -lO 0w" "Ny,e87e,lO,0w" 36
12"FX 1 U P 1fN" "FX,1,U,P 1fN" 18
Figure 3: The I/O examples provided to the synthesis algo-
rithm, along with the number of consistent candidates (# ğ‘“).
This numberdrops from â‰ˆ1042to18injust12samples.
ToillustratetheproblemofoverfittinginPBE-basedsynthesis,
let us consider a data-wrangling task of tokenizing a given pas-
sageoftextintoindividualwords.Thetokenizationtaskinvolves
recognizingtheboundariesofthefirst4wordsandreplacingthe
characters used to separate them with commas. The user wishes
touseaprogramsynthesistooltolearnaprogramthatperforms
thistask.Here,weuseourtool SynGuar-PROSE whichisbasedon
thePROSEframework[ 46]forsynthesizingaprogram.Program
synthesizersoutputprogramsinthesyntaxofsomepre-specified
target language or domain-specific language (DSL). In order to sup-
portourtask,weimplementastringtransformationDSLinPROSE
that is similar to the FlashFill DSL [ 46], see Figure 1. We give a ref-
erence implementation of our modified DSLin the supplementary
material [ 2]. The user provides an oracle, which can be queried by
the synthesizer forI/Oexamples exemplifying the behavior of thetargetprogram. EachI/Oexampleisapairofstrings formedfrom
lower andupper caseletters, digits, spacesandseparators.
Given the problem setup as described above, the goal of the
synthesizerreducestolearningthe4correctsubstringsfromthe
providedexamples.Letusexaminehowwellthesynthesizerper-
forms on a few I/O examples, say 2. After running it on the first
2 examples given in Figure 3, as expected, the output program
overfits. For instance, instead of trying to get the first substring
untiltheendofthefirstword,itjustpickstwocharactersforthe
firstwordsinceboththeexampleshaveonlytwocharactersuntil
theirrespectivefirstseparators.Infact,thesynthesizercontinues
to overfiteven after being provided 9examples.
However, it turns out that for our running example, we can
theoretically assert a program close to the target will be picked
withhighprobabilityafter149examples!Ourproposedalgorithmic
framework SynGuar isabletocalculatethisquantityontheflyand
stop when enough examples are seen. To understand how it works,
consider Figure 3that shows the estimated size2of the hypothesis
spacethatis consistent (ormatches)withalltheseenexamplesupto
a certain point. SynGuar computes this quantity internally, which
servesasourmaintechnicalinsight.Noticethatthespaceofthe
consistentprogramsshrinksprogressivelyasmoreexamplesare
provided.Beforeseeinganyexample,thehypothesisspaceisthe
set of all the programs our target language can represent and its
size can be infinite as the DSL grammar is recursive. Now suppose
the synthesizer sees the first I/O example, then the number of can-
didateprogramswhichare consistent withthefirstexamplereduces
considerably. The reduction depends critically on the example pro-
vided. For instance, the first I/O example shown in Figure 3will
reducethespaceofconsistentprogramsto1042.Thisisstillquite
largeÃif we arbitrarily pick one program, without using any auxil-
iaryassumptionsorpriorknowledgeaboutthetargetprogram,the
odds of picking the correct program are negligibly small. However,
after seeing 12 examples, the consistent program space reduces to
2Asound upper bound of the actualsize is calculated.
679ESEC/FSE â€™21, August 23â€“28, 2021,Athens,Greece BoWang,Teodora Baluta, AashishKolluri, andPrateek Saxena
Algorithm1 Meta-synthesisAlgorithm
1:procedure MetaSyn (ğœ–,ğ›¿)
2:whilestopping condition do
3: Query userfor k examples
4: Updatethe hypothesisspace
5: returnğ‘ğ‘œğ‘›ğ‘’if emptyhypothesisspace
6:Compute the ğ‘šexamplesfor ( ğœ–,ğ›¿)guarantee
7:Query userfor ğ‘šexamples
8:Updatethe hypothesisspace
9:ifemptyhypothesisspace then
10: returnğ‘ğ‘œğ‘›ğ‘’
11:returnğ‘“inhypothesisspace
18forourrunningexample.Notethatchoosingaprogramoutof
these18atrandomdoesnothaveanyguaranteesonitsclosenessto
thetargetprogram.Foraprogramspaceof18,tochooseaprogram
thatisclosetothetargetprogramwithprovablyhighconfidence
werequire137moreexamples. SynGuar canprovablyassertthat
it has seen enough examples to stop and return a program close to
the target program after12 +137=149 examples.
Problem Setup. Similar to the setup used in existing PBE-based
synthesizers, we are given an oracle to query I/O examples and
a DSL for representing the output program. Additionally, we are
given user-specified ( ğœ–,ğ›¿) parameters that capture the desired gen-
eralization guarantee. The synthesizer queries the oracle for as
many I/O examples as it needs and terminates with either ğ‘ğ‘œğ‘›ğ‘’
or a synthesized function ğ‘“. The probability that the synthesizer
returns a function ğ‘“that might not generalize should be under
the given small ğ›¿. Letğ‘†={(ğ‘¥, ğ‘¡(ğ‘¥))}be the set of I/O examples
seen by one invocation of the synthesizer. Here, each ğ‘¥is an input
drawn independently and identically distributed (i.i.d) from an un-
knowndistribution ğ·thattheoraclecaptures.Weassumethat ğ‘“
will satisfy all given I/O examples, âˆ€ğ‘¥âˆˆğ‘†,ğ‘“(ğ‘¥)=ğ‘¡(ğ‘¥). Note that
thisisdifferentfromtheÅ‚best-effortâ€[ 42]orapproximatesynthesis
approaches[ 53]wheretheprogram ğ‘“isallowedtodifferfrom ğ‘¡on
some examples in ğ‘†. In this setup, therefore, ğ‘†is a random variable.
Thesynthesizer,denotedas A(ğ‘†),isalsoarandomvariabledefined
over I/O samples ( ğ‘†) drawn from ğ·.
We seek to designPBEsynthesizers that achieve generalization
given by a rigorous PAC-style guarantee [ 58] while computing the
required sample complexity. For an( ğœ–,ğ›¿)-synthesizer, the general-
ization error ğ‘’ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿ(ğ‘“)=ğ‘ƒğ‘Ÿğ‘¥âˆ¼ğ·[ğ‘“(ğ‘¥)â‰ ğ‘¡(ğ‘¥)]is bounded by ğœ–. The
probability ofgenerating ğ‘“withğ‘’ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿ(ğ‘“)>ğœ–isboundedby ğ›¿.
Definition 2.1 ( (ğœ–,ğ›¿)-synthesizer). A synthesis algorithm Awith
hypothesis space ğ»is an (ğœ–,ğ›¿)-synthesizer with respect to a target
classoffunctionsCiffforanyinputdistributions ğ·,forallğ‘¡âˆˆC,
ğœ–âˆˆ(0,1),ğ›¿âˆˆ(0,1),given example set ğ‘†drawn i.i.d from the ğ·,
ğ‘ƒğ‘Ÿ[A(ğ‘†)outputs ğ‘“âˆˆğ»such that ğ‘’ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿ(ğ‘“)>ğœ–]<ğ›¿
3 THE SYNGUAR FRAMEWORK
Weproposeaframeworkwithasimilaralgorithmicmeta-structure
as that of existing PBE engines. The overall procedure is shown
in Algorithm 1. Instead of synthesizing a program after seeingapre-determinednumberofI/Oexamplestheprocedurequeries
an oracle for new examples until it synthesizes a program that
generalizes. There are two key new features in our framework:
a stopping criterion and a dynamically calculated count of the
numberof samplestobe seen.The following classical resultgives
usastarting pointto compute the count precisely.
AStartingPoint. The number of examplesprovably sufficient to
achievethe( ğœ–,ğ›¿)-generalizationisgivenbyBlumeretal.[ 10].We
restatethisresult,whichcomputessamplecomplexityasafunction
of (ğœ–,ğ›¿) and the capacity (or size) of any given hypothesis space ğ».
Theorem 3.1 (Sample Complexity for (ğœ–,ğ›¿)-synthesis). For
allğœ–âˆˆ(0,1),ğ›¿âˆˆ(0,1),andhypothesisspace ğ»,asynthesisalgorithm
A(ğ‘†)which outputs functions consistent with ğ‘ši.i.d samples is an
(ğœ–,ğ›¿)-synthesizer,if
ğ‘š>1
ğœ–(ln|ğ»|+ln1
ğ›¿)
Theabovetheoremisintuitivelybasedonthefollowinganalysis.
Letussaywehavesomeinitialhypothesisspace ğ».Afterseeing
onenewI/Oexample,eachhypothesisthatisÅ‚ ğœ–-farâ€from ğ‘¡(gen-
eralization error >ğœ–) becomes inconsistent with some non-zero
probability, and is eliminated. Therefore after seeing sufficiently
manynewexamples,theprobabilityofanyÅ‚ ğœ–-farâ€hypothesisbeing
outputfalls below ğ›¿.For details,pleasesee the analysis[ 10].
3.1 KeyObservations& Challenges
WeobservethatTheorem 3.1canbeusedatanypointofthesyn-
thesisprocedure.Afterseeingsaythefirst ğ‘†examples,letthespace
of programs consistent with the examples be ğ»ğ‘†. We can plug|ğ»ğ‘†|
intoTheorem 3.1tocomputehowmanymoreexamplesaresuffi-
cienttoachievetheguaranteeprovidedinDefinition 2.1.But,there
are several key technical challenges in utilizing the classical result
of Theorem 3.1in providing end-to-end generalization guarantees.
First,applyingthisresultrequiresbeingabletocompute |ğ»ğ‘†|.We
pointoutthat thishasnotbeenan explicitalgorithmicgoalwhen
designingexistingsynthesizers.Consequently,computing |ğ»ğ‘†|is
non-trivialinsomeofthe existing synthesizers.Totackle this,we
design our own PBE synthesizer based on the STUN approach and
bottom-up explicit search with the ability to compute |ğ»ğ‘†|(see
Section4.2).Further,weshowhowtointegrate SynGuar inPROSE,
a synthesis framework where thesizeofthehypothesis space can
be easily computed.
Second,|ğ»ğ‘†|can be large and plugging in its values at the be-
ginning leads to vacuously high sample bounds in practice. For
instance,initiallythehypothesissizeinourrunningtaskisinfinite,
andevenafterseeingoneexample,thesizeis1042.Therefore,in-
stead of naively plugging in values of parameters at the beginning,
we use the idea that if |ğ»ğ‘†|decreases as the synthesizer sees more
examples then the estimated sample complexity for generalization
reduces as well. Therefore, in SynGuar â€™s design,|ğ»ğ‘†|is computed
ontheflyas thesynthesizerseesmore examples andthestopping
condition ensures that the synthesizedprogram generalizes.
Lastly,thePAClearningtheoryoffersnorecoursetopredicthow
fast|ğ»ğ‘†|reduceswithmoresamplesinpractice.Thisquestion,then,
becomes an empirical one: For which programs do we observe that a
680SynGuar : Guaranteeing Generalizationin Programmingby Example ESEC/FSE â€™21, August 23â€“28, 2021,Athens,Greece
Algorithm 2 SynGuar Synthesis returns a program with error
smallerthan ğœ–withprobability higher than 1 âˆ’ğ›¿
1:procedure SynGuar (ğœ–,ğ›¿)
2:ğ‘˜=1 // tunableparameter
3:ğ‘”â†PickStoppingCond
4:ğ‘†â€²â†âˆ…,ğ‘ â†0
5:ğ‘ ğ‘–ğ‘§ğ‘’ğ»â†ComputeSize (ğ»)
6:ğ‘›â†ğ‘”(ğ‘ ğ‘–ğ‘§ğ‘’ğ»)
7:whileğ‘ â‰¤ğ‘›do
8: ğ‘†â€²â†ğ‘†â€²âˆªsample(ğ‘˜)
9: ğ»ğ‘†â€²â†UpdateHypothesis (ğ‘†â€²)
10: ğ‘ ğ‘–ğ‘§ğ‘’ğ»ğ‘†â€²â†ComputeSize (ğ»ğ‘†â€²)
11: returnğ‘ğ‘œğ‘›ğ‘’ifğ‘ ğ‘–ğ‘§ğ‘’ğ»ğ‘†â€²=0
12: ğ‘ â†ğ‘ +ğ‘˜
13: ğ‘›â†ğ‘šğ‘–ğ‘›(ğ‘›,ğ‘ +ğ‘”(ğ‘ ğ‘–ğ‘§ğ‘’ğ»ğ‘†â€²))
14:ğ‘šğ»ğ‘†â€²=1
ğœ–(lnğ‘ ğ‘–ğ‘§ğ‘’ğ»ğ‘†â€²+ln1
ğ›¿)
15:ğ‘‡â†sample(ğ‘šğ»ğ‘†â€²(ğœ–,ğ›¿))
16:ğ‘†â†ğ‘†â€²âˆªğ‘‡
17:returnprogram ğ‘“inğ»ğ‘†orNone if ğ»ğ‘†=âˆ…
smallnumberofexamplesaresufficienttogeneralize? Ourempirical
evaluation shows that for several common string manipulation
tasks,the requirednumber ofexamplesturnoutto be modest.
Remark. Itcanbeseenthatoursolutionisamodificationtothe
existing PBEsynthesis loop,which canbe instantiatedfor several
programsynthesisengines.Ourproofsandanalysisutilizeclassical
samplecomplexityarguments,togetherwithboundsforhypoth-
esisspacesize.Itmaythereforeappearsurprisingthenthatsuch
calculationsarenotroutineinpriorprogramsynthesisworksal-
ready,despiteaccuracy/generalizationbeinganaturalobjective.
We believe that the challenges stated above offer an explanation as
towhyapplyingprevioustheoreticalresultsisnotstraightforward,
and requires a principled approach. We point out that the subtlety
is in our problem formulation itself, namely, the use of dynamic
calculation of the remaining sample size needed for generalization.
3.2SynGuar Algorithm
We start by addressing the second challenge assuming that |ğ»ğ‘†|is
computable. Recall that, our synthesis algorithm takes as input the
error tolerance parameters ğœ–and the confidence ğ›¿(Algorithm 2).
The algorithm follows the structure of Algorithm 1and consists of
two phases: the sampling phase (lines 7 âˆ’13) and the validation
phase (lines 14âˆ’17). The sampling phase addresses the second
challenge by trying to shrink the hypothesis space as much as
possible. In each iteration ğ‘˜samples are taken before updating the
hypothesisspacethatsatisfiesall ğ‘˜samplesseen.Thecrucialpartof
the sampling phase is deciding when the number of examples seen
sofar(storedintheset ğ‘†â€²andwhosecardinalityis ğ‘ )isÅ‚enoughâ€.
SynGuar stopssamplingwhenthenumberofexamplesseensofar
exceedsastoppingthreshold,representedbyvariable ğ‘›.Ineachiter-
ation,thestoppingthreshold(whichdependsonfinite |ğ»|initially)
eitherremainsthesameoritgraduallyshrinkswitheachupdate
of the hypothesis space (line 13). Hence, SynGuar dynamically
updatesthethresholdbasedonthechangeinthehypothesissize.
Tocontrolhowmuchthethresholdvariableshrinkswithrespecttothesizeoftheupdatedhypothesisspace ğ»ğ‘†â€²,SynGuar picksa
function ğ‘”(line3). Our framework allows using any ğ‘”:Nâ†¦â†’Z
that is monotonically non-decreasing, and we provide a sample
complexity analysis for such functions. We propose a particular
choiceoffunction ğ‘”asthedefault: ğ‘”(ğ‘¥)=max{0,1
ğœ–(ln(ğ‘¥)âˆ’ln(1
ğ›¿))}.
Thisğ‘”has a useful propertyÃthe required number of samples it
entails in the worst case cannot be more than twice the number
ofsamplesthatanoptimalchoiceof ğ‘”willtake.Wewillformally
state andprove this optimalityclaim inSection 3.3.
In the second phase, in addition to the samples ğ‘†â€²,SynGuar
samples a fixed number of samples according to Theorem 3.1.Syn-
Guarthen can return a program ğ‘“with provable ( ğœ–,ğ›¿) guarantees
(line14). Algorithm 2calls sub-procedures UpdateHypothesis
(line9)tofindaprogramspaceconsistentwith ğ‘†â€²andComputeSize
(line10)tocompute thesize oftheconsistent program space.The
sub-procedure UpdateHypothesis can be implemented by any ex-
istingPBEsynthesisalgorithmwhichreturnhypothesesconsistent
withğ‘†â€².Section4detailshowtoimplement ComputeSize ,which
isspecific to the underlying UpdateHypothesis sub-procedure.
Running Example. Consider the example given in Section 2.
First the user inputs ğœ–=5%,ğ›¿=2% respectively. In the sampling
phase,theuserisqueriedforoneexampleineachiteration.After
the first iteration, i.e., seeing one example, the sample size for
generalization( ğ‘šğ»ğ‘†â€²)is2018.Instead, SynGuar â€™ssamplingphase
stopsafterseeing ğ‘›=12examplesandtheadditionalsamplesizefor
generalization (Line 14) reduces to ğ‘šğ»ğ‘†â€²=137. The total sample
size of both phases sums up to 149 examples, which is 10 Ã—less
than the sample complexity after the first iteration. This is a direct
consequenceof SynGuar dynamically estimating the sample size
for generalization.
3.3 Analysis oftheAlgorithm
SynGuar â€™s design is motivated by being able to give a formal gen-
eralizationguaranteeandaboundedsamplecomplexity.Forthis
purpose,we state andprove the following properties:
(P1:Termination) SynGuar alwaysterminatesfor afinite |ğ»|.
(P2:(ğœ–,ğ›¿)guarantees) Theprobabilityof SynGuar returningan
ğ‘“that isğœ–-far issmallerthan ğ›¿.
(P3:Samplecomplexity) SynGuar â€™ssamplecomplexityisalways
within 2Ã—ofthe optimal for ğ‘˜â‰¤1
2ğœ–ln1
ğ›¿.
Theorem 3.2 (P1). SynGuar alwaysterminates for afinite |ğ»|.
Proof.Itsufficestoprovethatthesamplingphase(lines7 âˆ’13)
ofSynGuar terminates in order to show that SynGuar terminates.
In each iteration of the sampling phase, let ğ‘†ğ‘–be the queue stor-
ing the user-provided examples after each iteration, ğ‘§ğ‘¡be theğ‘¡th
example, ğ‘†ğ‘–+1=ğ‘†ğ‘–âˆª{ğ‘§ğ‘–ğ‘˜+1,...ğ‘§ğ‘–ğ‘˜+ğ‘˜}andğ‘†0=âˆ…. For each ğ‘†ğ‘–,ğ»ğ‘†ğ‘–
determinesthesetofconsistenthypothesisthatsatisfy ğ‘†ğ‘–.Letğ‘ğ‘–
bethelimitofthenumberofI/Oexamples ğ‘›forthesamplingphase
after iteration ğ‘–. For iterations ğ‘–andğ‘—whereğ‘–<ğ‘—andâˆ€ğ‘”:Nâ†’Z
such that ğ‘”ismonotonically non-decreasing,the following holds:
ğ‘†ğ‘–âŠ‚ğ‘†ğ‘—â‡’|ğ»ğ‘†ğ‘—|â‰¤|ğ»ğ‘†ğ‘–|â‡’ğ‘”(|ğ»ğ‘†ğ‘—|)â‰¤ğ‘”(|ğ»ğ‘†ğ‘–|)
ğ‘ğ‘—â‰¤min{ğ‘ğ‘–,|ğ‘†ğ‘—|+ğ‘”(|ğ»ğ‘†ğ‘—|)}â‰¤ğ‘ğ‘–(see line 13inAlg. 2)
Therefore, if ğ‘0â‰¤ğ‘”(|ğ»|) then the loop will terminate at some
iterationğ‘such that ğ‘ğ‘<|ğ‘†ğ‘|â‰¤ğ‘ğ‘+ğ‘˜â‰¤ğ‘0+ğ‘˜.â–¡
681ESEC/FSE â€™21, August 23â€“28, 2021,Athens,Greece BoWang,Teodora Baluta, AashishKolluri, andPrateek Saxena
Theorem3.3(P2). Theprobabilityof SynGuar returningasyn-
thesized program ğ‘“that isğœ–-far is smallerthan ğ›¿.
Proof.ByTheorem 3.2,weknowthatthesamplingphaseter-
minates with ğ‘†â€²samples (see line 14). In lines 14âˆ’16 SynGuar
samplesanadditionalnumberofI/Oexamplesrequiredtogener-
alize and then synthesizes a program after seeing the additional
samples.Therefore,Theorem 3.3followsfrom Theorem 3.1.â–¡
In order to prove the last property, we define a new quantity
ğœ”(ğ‘„). It is the smallest sample size taken by SynGuar (ğœ–,ğ›¿) for any
non-decreasing ğ‘”usedfor asequenceofI/O examples ğ‘„.
Definition 3.4 (Smallest dynamic sample size). For any infinite
sampledsequenceofexamples ğ‘„,letPrefix(ğ‘„,ğ‘”)betheprefixof
ğ‘„at which SynGuar (ğœ–,ğ›¿) terminates.Then,
ğœ”(ğ‘„)=inf{|ğ‘šğ‘”|:âˆ€ğ‘”, ğ‘šğ‘”=Prefix(ğ‘„,ğ‘”)}
Theorem 3.5 (P3). SynGuar uses no more than 2ğœ”(ğ‘„)examples
onanyğ‘„whenthe resultisnotNonewith ğ‘”(ğ‘¥)=max{0,1
ğœ–(ln(ğ‘¥)âˆ’
ln(1
ğ›¿))}andğ‘˜â‰¤1
2ğœ–ln1
ğ›¿.
Duetospacelimit,weprovidetheproofofthistheoreminthe
supplementary material[ 2].
4 RETROFITTING SYNGUAR INTO EXISTING
SYNTHESIZERS
We now show how to compute |ğ»ğ‘†|, the size of the consistent
programspace.Asoundupperboundof |ğ»ğ‘†|issafetouse,sincein
thiscase,ouranalysisshows SynGuar totakemoreexamplesthan
thoseneededtoguaranteegeneralization.Weshowhowtocompute
|ğ»ğ‘†|bounds for twowell-knownPBE synthesis approaches.
4.1SynGuar forthePROSEFramework
We first apply SynGuar on top of the PROSE framework [ 46], a
state-of-the-artPBEmeta-synthesisframeworkthatgeneratesan
inductive synthesizer for a given DSL. PROSE allows developers to
write DSLs and specify witness functions that capture (a subset of)
inversesemanticsfortheDSLoperators.Thesewitnessfunctions
arethedriversfortheÅ‚deductivebackpropagationâ€becausethey
specifythe inputs or properties of the input given an I/Oexample.
We implement a synthesizer named StrPROSE with the DSL in
Figure1ontopofPROSEbyspecifyingexecutablesemanticsand
witness functions for its operators. Our DSL shares most operators
with the DSL of FlashFill [ 46]. For the operators that differ, we
detail their executable semantics and witness functions in the sup-
plementarymaterial[ 2].Notethat SynGuar workswithanyDSL
expressible in PROSE, as long as each operator has its semantics
andan associatedwitness functionspecified.
PROSE uses an internal succinct representation of the program
space using a data structure called version-space algebra (VSA)
which makes it convenient to calculate |ğ»ğ‘†|[26,34,35,39]. A
VSA is a directed graph where each node corresponds to a set of
programs. The leaf nodes explicitly represent a set of programs
that can be enumerated. There are two types of internal nodes:
unionnodesthatrepresentaset-theoreticunionand joinnodesthat
represent ğ‘˜-ary operators whichare definedbythe DSL.Computing|ğ»ğ‘†|UsingVSA. PROSEreadilycomputes |ğ»ğ‘†|using
a bottom-up graph traversal on its VSA. For each leaf node, it
enumerates and counts the set of programs directly. For every
union node, to compute the corresponding number of programs
it adds up the count of all child nodes. For every join node, the
number of programs is a cross product of all applications of the ğ‘˜-
ary operator to ğ‘˜parameter programs. This soundly upper bounds
|ğ»ğ‘†|. In our implementation, we reuse the SizeAPI available in
PROSE,resultinginthe sizes showninFigure 2.
Scaling to Large Sample Size. Building VSA ona largenumber
ofexamplescanbe time-consuming. Therefore,webuildtheVSA
onasubsetoftheexampleswhichleadtothesamesetofprograms.
Morespecifically,wetaketheexamplesonebyoneanddropthe
examples that do not decreasethe VSAsize.
4.2SynGuar inStrSTUN
STUNis a well-known synthesis approach [ 5]. It was originally
proposed as an extension of the counter-example guided induc-
tive synthesis (CEGIS) approach to synthesize program from the
specification. The high-level ideaisto synthesize partial solutions
satisfyingpartsoftheinputsandunifythem.Asaninstantiationof
STUNforsynthesizingconditionalprogramsunderPBEsettings,
we work with top-level3if-then-else unification operator where
theconditioncanbeanybooleanexpressioninthehypothesisspace.
Thesubsequentsynthesisalgorithmsfollowingthisapproachdo
notcompute|ğ»ğ‘†|directly,ormakeitstraightforwardtocompute
it.Wedesignasynthesisalgorithmbasedonthisapproach,anda
proceduretosoundlycomputeanupperboundon |ğ»ğ‘†|.Wechoose
our target language as the SyGuS string-manipulating program
DSL [1]. Our synthesis algorithm isreferredto as StrSTUN .
VanillaStrSTUN : Overview. StrSTUN instantiates the previ-
ously proposed approach of bottom-upexplicit search withobser-
vational equivalence reduction [ 3]. Its algorithm consists of two
phasesatahighlevel:anenumerationphaseandaunificationphase.
In the first phase, the synthesizer enumerates candidate programs
only by repeatedly using function application. It clusters all candi-
dateprogramswhichhavethesameI/Obehavioronthegivenexam-
plesandsavesonlyoneprogramrepresentativeofeachcluster.Such
enumeratedprogramsmayonlybeconsistentwithsubsetsofthe
givenI/Oexamples.Intheunificationphase, StrSTUN composes
enumeratedprogramswithan if-then-else unificationoperator.
Thefinalsynthesizedprogram ğ‘ƒ,therefore,canbeastraight-line
program (obtained by repeated function application) or a program
withnestedcompositionsoftheform ifğ‘ƒ1thenğ‘ƒ2elseğ‘ƒ3,where
ğ‘ƒ2andğ‘ƒ3canbenestedprogramsthemselves.Thenestingdepthis
boundedinternallytolimitthesearchspace.Weexplainthecon-
structionaldetailsofthesephasesnext,andexplainhowtocompute
|ğ»ğ‘†|from the internal data-structures later. In what follows, we
denote inputsandoutputs of thegivenI/O examplesasvectors w
andorespectively.
VanillaStrSTUN : Enumeration. StrSTUN enumeratesallcan-
didateprogramsinabottom-upfashionbygeneratingprograms
throughfunctionapplication.Westartwiththe smallestsyntactic
programs,whicharejustsinglecomponents(orsyntacticterminals)
3Such if-elseconstructsisrestricted to beingat the topof the functionâ€™s AST.
682SynGuar : Guaranteeing Generalizationin Programmingby Example ESEC/FSE â€™21, August 23â€“28, 2021,Athens,Greece
inthetargetlanguage,workinguptoprogramswithmorethanone
component.Forinstance, concat(input0, input1) isacandidate
program with three components. The total hypothesis space with-
outconditionalsisfixedbasedonauser-providedconstraintonthe
maximumcomponentsize,specifiedasthemaximumnumberof
components the straight-line program contains. For each program
createdintheenumerationphase,wecomputetheoutputsofthe
program on the given I/O examples. Note that this step does not
requireexplicitlyindividuallycreatingandrunningallcandidate
programsÃit ispossibletoevaluateoutputsduringthebottom-up
construction ofthe programs [ 3].
Wecompute 2usefuldatastructuresinternallyduringenumer-
ation. The first is a consistency vector cwhich captures whether
anenumeratedprogram ğ‘ƒisconsistentwiththegivenI/Oexam-
ples represented by vector wand vector o. Specifically, the consis-
tency vector cfor program ğ‘ƒhas theğ‘–ğ‘¡â„element set toâœ“if the
ğ‘ƒ(w[ğ‘–])=o[ğ‘–],namely,theoutputof ğ‘ƒontheğ‘–ğ‘¡â„giveninputexam-
plematchesthecorrespondinggivenoutputexample.Otherwise,
c[ğ‘–]is set to â€™Ã—â€™. This data structure speeds up the search in two
ways, conceptually. First, it is calculated on top of observational
equivalence [3].Ifmanycandidateprogramsgeneratethesameout-
puts on the given input examples, then they are all observationally
equivalent,andweonlyneedtokeeponesuchprogramthathasthe
outputsforcompleteness[ 3],thustheconsistencyvectorisonly
calculatedonceforthoseprograms.Secondly,eveniftwoprograms
areobservationallynotequivalent,andbothgivedifferentincor-
rect outputs foran input example,they will have the samevalue
(Ã—) in their consistency vectors. Thus, we can effectively cluster
manyprogramsthatareobservationallynon-equivalentbuthave
the same consistency vector to speedupthe nextphase.
Thesecondusefuldatastructureisclustermap ğœ™.Itmapsconsis-
tency vectors to sets of programs. Each distinct consistency vector
ccomputed during the enumeration phase is mapped to a set of
programs ğœ™(c)that have I/O behaviorcapturedby c.
VanillaStrSTUN : Unification. In this phase, StrSTUN synthe-
sizes programs with nested if-then-else structures. The goal is
to create programs that are consistent with larger subsets of I/O
examples than enumerated programs, and ideally, correct on the
full set of I/O examples. The cluster map ğœ™allows us to quickly
find programs which match certain subsets of all the given I/O
examples,specifiedbyaconsistencyvectorvalue.Whenaprogram
ğ‘ƒ:=ifğ‘ƒ1thenğ‘ƒ2elseğ‘ƒ3is synthesized during unification, we
must carefully construct a semantically correct consistency vector
forğ‘ƒ,usingthoseforsub-programs ğ‘ƒ2andğ‘ƒ3.Here,notethat ğ‘ƒ1
needs to be a program that evaluates to a boolean value on a given
inputexample, say w[ğ‘–].Ifitevaluatesto true,then theprogram
ğ‘ƒ2mustbecorrecton w[ğ‘–]forğ‘ƒtobecorrecton w[ğ‘–].Therefore,
inthiscase,wemark ğ‘ƒasconsistentwith w[ğ‘–]ifandonlyif c[ğ‘–]
forğ‘ƒ2hasaâœ“.Analogously,if ğ‘ƒ1evaluatesto falseonw[ğ‘–],then
c[ğ‘–]forğ‘ƒ3should be set toâœ“forğ‘ƒto be marked consistent with
w[ğ‘–]; otherwise ğ‘ƒismarkedinconsistentwith w[ğ‘–].
Theabove-describedunificationproceduresynthesizesallpro-
grams with a nesting depth of up to a pre-configured maximum
(defaultof 2).Thenestingdepthcontrolsthehypothesisspacede-
siredbythe user.Theclustermap ğœ™isupdatedcontinuously withnewconsistencyvectorsdiscoveredintheunificationphase.Asuc-
cessfulsolutionisaprogramthatmatchesallgivenexamples,i.e.,
has aconsistency vector witha âœ“for allvalues.
Computing the|ğ»ğ‘†|. The vanilla StrSTUN algorithm can be
slightly modified and augmented with rules shown in Table 1to
computethe|ğ»ğ‘†|soundly.Noticethatinvanilla StrSTUN ,when
employing the observational equivalence, a program with larger
componentssizemightbediscardedifthereisasmallerprogram
thathasthesame valuevector4[3].Butweneedtocountallpro-
grams at different components sizes. To do so, we store multiple
countingvalues(andrepresentativeprograms)fordifferentcompo-
nent sizesalongwitheachvaluevector.
Duringtheenumerationphase,programsaresynthesizedbottom-
up from smallest components size to larger ones. Let ğ‘¡be the com-
ponents sizeof aprogram. We keeptrack of a Count(v,ğ‘¡)for each
valuevector vcomputedforprogramswithsize ğ‘¡.TheCount(v,ğ‘¡)
forğ‘¡=1(smallest base components) can be directly enumerated
(rule 1 in Table 1), since these are program input arguments or
constant components in our target language. For ğ‘¡>1,Count(v,ğ‘¡)
canreturn 0ifthereisnoenumeratedprogramwithcomponents
sizeğ‘¡thatoutputsvaluevector v.Samevaluevectors vmayhave
differentcounts fordifferentcomponentssizes ğ‘¡,thus weenumer-
ate on tuple (v,ğ‘¡)rather than just v. WhenStrSTUN uses function
applicationtogenerateanewprogram ğ‘ƒâ€²fromprograms ğ‘ƒğ‘–(rule
2.1inTable 1),thecount for thevaluevectoroftheresulting ğ‘ƒâ€²is
updated by addingthe product of all the counts of its arguments
ğ‘ƒğ‘–at their respective components sizes (rule 2.2 in Table 1). This
completes all the ways programs are compositionally created in
the enumeration phase from component size 1 to the maximum
componentsize.
After the enumeration on value vectors is finished, we have
alsoclusteredobservationallynon-equivalentprogramsbasedon
aconsistencyvector cinğœ™.Defineğœ“[c]asthesetofallthevalue
vectors that correspondsto aconsistency vector c,we sum upthe
Count(v,ğ‘¡)for every vinğœ“[c](rule 3 in Table 1). This way, we
compute counts for consistency vectors.
Duringunification, programs of the form ğ‘ƒ:=ifğ‘ƒ1thenğ‘ƒ2
elseğ‘ƒ3are composed. Here, counts of the consistency vectors
ofğ‘ƒ2andğ‘ƒ3have been computed after enumeration if ğ‘ƒ2andğ‘ƒ3
areprogramswithnestingdepthzero.Inthiscase,the Count(c,1)
of the program ğ‘ƒis the the product of ğ‘ƒ1, all possible ğ‘ƒ2(0 con-
dition) with ğ‘ƒ1as the condition, and all possible ğ‘ƒ3(0 condition)
withğ‘ƒ1as the condition, summed over all possible ğ‘ƒ1. Thus, we
havecomputed Countfortheconsistencyvectorsofprogramswith
nesting depthof 1. Using this, we canrecursively compute counts
forconsistencyvectorsofprograms withnestingdepth 2ormore
(rule5 inTable 1). To optimize,we memorize counts ofindividual
consistencyvectorsaswellasforsetsofconsistencyvectors.For
example, the set comprising two consistency vectors âŸ¨âœ“,Ã—,Ã—âŸ©and
âŸ¨âœ“,Ã—,âœ“âŸ©is succinctly represented as âŸ¨âœ“,Ã—,âŠ¤âŸ©, and their sum of
counts is memorized (rule 4 in Table 1). We prove that the rules in
Table1provide asound upperbound ofthesizeofthehypothesis
space|ğ»ğ‘†|insupplementary material[ 2].
4The value vector vfor a program ğ‘ƒis simply the outputs of ğ‘ƒon the given input
examples,i.e., v[ğ‘–]:=ğ‘ƒ(w[ğ‘–])for allğ‘–.
683ESEC/FSE â€™21, August 23â€“28, 2021,Athens,Greece BoWang,Teodora Baluta, AashishKolluri, andPrateek Saxena
Table 1: Count rules for StrSTUN . We define the set of all
boolean value vectors as B, and we useCto represent a suc-
cinct representation of consistency vectors, which can be a
singleton or a set of consistency vectors. The Countvalue is
calculated differently for value vector v, consistency vector
c,ortheir succinct representation C.
CountRulesfor Enumeration
Starting pointof enumeration:
Count(v,1)=number of singlecomponentsthat output v
Count(v,ğ‘¡)=0,forğ‘¡>1.(1)
When program ğ‘ƒâ€²with valuevector vâ€²is enumerated at componentsize ğ‘¡â€²:
Let,ğ‘ƒâ€²=ğ‘“(ğ‘ƒ1,ğ‘ƒ2,...,ğ‘ƒğ‘›)for some function component ğ‘“and programs ğ‘ƒğ‘–.
Ifğ‘¡ğ‘–is the componentsize for ğ‘ƒğ‘–,
andvğ‘–is the valuevector of ( ğ‘ƒğ‘–,ğ‘¡ğ‘–),ğ‘–âˆˆ{1,...,ğ‘›},
then1+/summationtext.1ğ‘¡ğ‘–=ğ‘¡â€²holds, and (2.1)
Count(vâ€²,ğ‘¡â€²)â†Count(vâ€²,ğ‘¡â€²)+ğ‘›/productdisplay.1
ğ‘–=1Count(vğ‘–,ğ‘¡ğ‘–) (2.2)
CountRulesfor Clustering
Count(ğ‘)=/summationtext.1
vâˆˆğœ“[ğ‘]/summationtext.1
ğ‘¡(Count(v,ğ‘¡)) (3)
CountRulesfor Unification
Cgoal=âŸ¨âœ“,âœ“,...,âœ“âŸ©,
the count of hypothesisspace up to ğ‘˜conditions is/summationtext.1ğ‘˜
ğ‘–=0Count(Cğ‘”ğ‘œğ‘ğ‘™,ğ‘–)
Count(C,0)=/summationtext.1
ğ‘âˆˆCCount(ğ‘) (4)
Count(C,ğ‘–)=/summationdisplay.1
bâˆˆBğ‘–âˆ’1/summationdisplay.1
ğ‘—=0(Count(Î“then(C,b), ğ‘—)Ã—
Count(Î“else(C,b),ğ‘–âˆ’ğ‘—âˆ’1)Ã—/summationdisplay.1
ğ‘¡Count(b,ğ‘¡)) (5)
whereÎ“then(C,ğ‘)andÎ“else(C,ğ‘)arealso succinct
representation of consistencyvectors,and
Î“then(C,ğ‘)ğ‘–=/braceleftbiggâœ“ifğ‘ğ‘–=ğ‘‡
âŠ¤otherwise,Î“else(C,ğ‘)ğ‘–=/braceleftbiggâœ“ifğ‘ğ‘–=ğ¹
âŠ¤otherwise
StrSTUN Augmented with SynGuar .We callStrSTUN aug-
mentedwith SynGuar asdescribedinSection 4asSynGuar-STUN .
The maximum number of nested conditions during the unification
phaseofStrSTUN is2,andwithoutmuchlossofexpressiveness,
weonlyallowthe elsebranchtohavenesting.Withthissetting,
the hypothesisspaceof StrSTUN isaunion of:
â€¢ğ»0(The setofstraight-lineprograms),
â€¢ğ»1(The programs ofform ifğ‘ƒ1thenğ‘ƒ2elseğ‘ƒ3),
â€¢ğ»2(The programs ofform ifğ‘ƒ1thenğ‘ƒ2elseifğ‘ƒ3
thenğ‘ƒ4elseğ‘ƒ5).
Fromğ»0toğ»2,thehypothesisspaceisincreasinglyexpressive.
SynGuar-STUN invokes the SynGuar loop with ğ»0first, and if it
returnsNonethenwith ğ»1,andsooninthatorder.Thishasthenice
propertythatitwillreturn ğ‘“consistentwithexistingexamplesfrom
ğ»ğ‘–whereğ‘–isthesmallestpossible.Forcorrectness,eachinvocation
with a new hypothesis ğ»ğ‘–uses a failure probability ofğ›¿
3, so the
totalfailure probability isboundedby ğ›¿(union bound).
5 EVALUATION
Wehave shown thatwhen anexistingPBE synthesizerusing Syn-
Guarreturns a synthesized program, the program generalizes, i.e.,
itisclosetothetargetwithhighprobability.Ourevaluationfocuses
ontwoempirical utilitygoalsinstring-manipulationtasks:(1)Accuracy: Do our theoretical generalization guarantees im-
prove the end accuracyof existing PBE synthesizers?
(2)Sample Size: Howmanyexamplesdoes SynGuar require
to achieve provablegeneralization?
Recall that SynGuar primarily extends existing synthesizers
to control how many examples the synthesizer sees before stop-
ping.Weevaluate(a) SynGuar-PROSE ,whichbuildsonthePROSE
framework, and (b) SynGuar-STUN , which is implemented on the
StrSTUN synthesizer we designed. The vanilla StrSTUN synthe-
sizer is around 4000 lines of C++ code. These vanilla versions of
PBE synthesizers (without the SynGuar augmentation) serve as
our baselinesto measure improvements dueto SynGuar .
We point out that PBE synthesizers for string programs often
competeoncomputationaloverheadsreportedforproducing any
programthatfitsagivensetofexamples.Ouraccuracycriterionand
ourobjectivearecompletelydifferentÃwewanttocheckwhena
synthesizerproducesaprogramclosetoafixedtargetprogram(the
numberofexamplesisnotfixed).Thisiswhywedonotcompareto
otherbaselinesolverswhichmaybecomputationallyfaster[ 48,49],
but are not designedto generalize to atarget program.
Benchmarks. ForSynGuar-PROSE , we considered 16common
string-related programming tasks as target functions to synthe-
size. These are of similar style and complexity as those reported
in FlashMeta paper [ 46] such as changing the date format, extract-
ingnumbersorabbreviatingwords.Henceforth,werefertothese
programsas PROSE-benchmark ,detailsofwhichareinthesup-
plementarymaterial[ 2].ForSynGuar-STUN ,wetaketheeuphony
benchmarkfromthePBE-StringstrackoftheSyGuS2019bench-
mark[1]5whichcontains 100PBE taskswith 2âˆ’16examples.
The target programs are not available for those 100tasks, so we
manuallywrotethemfromthegivenexamplesfromthebenchmark.
Out of the 100tasks,10are for tasks that output boolean values
which are not in the scope of our considered DSL. Further, we
experimentally observed that our StrSTUN implementation scales
uptocomponentsize 9(sizeofthelongeststraight-lineprograms
beforeunification)withinareasonablecomputationofadayforall
benchmarkstofinishonourexperimentalsetup(largercomponent
sizeincreasestheprogramsearchspace).Sofortheremaining 90,we
filtered out the ones that could not be manually constructed under
component size 9. This finally results in 59SyGuS benchmarks
whichwe call SyGuS-STUN .
The generalization error tolerance for all experiments is set
to5%(ğœ–=0.05) and confidence parameter to 98%(ğ›¿=0.02) by
default.Whencomparingsamplesizefordifferent ğœ–,wealsorun
the benchmark on ( ğœ–=0.02,ğ›¿=0.02) and (ğœ–=0.1,ğ›¿=0.02). The
defaultstepsize ğ‘˜forthesamplingphaseissetto 1forSynGuar-
PROSEand20forSynGuar-STUN . With these parameters, it is
guaranteedwithprobabilityatleast 98%thatwhen SynGuar stops,
itssynthesizedprogramisgoingtohaveageneralizationerrorof
at most5%. We ran all experiments on Amazon EC2 Ubuntu 16.04
instancewith512GBRAM,64-core3.1GHzIntelXeonprocessors
whereeachbenchmarkruns 1core.Allourexperimentsfinished
within24hours.For SynGuar-PROSE ,79%runsfinishedin1minute
5downloaded from https://github.com/SyGuS-Org/benchmarks/tree/master/comp/
2019/PBE_SLIA_Track/euphony
684SynGuar : Guaranteeing Generalizationin Programmingby Example ESEC/FSE â€™21, August 23â€“28, 2021,Athens,Greece
and 100% within 1 hour. For SynGuar-STUN , 75% runs finished in
10 minutesand97%within 2hours.
SynGuar works with any input distribution for creating I/O
examples.Wechooseadistributionthatiseasytogenerateandnot
specializedtoeachtargetprogram.Specifically,wesimulateablack-
boxfuzzerforstringinputs.Allour SynGuar-PROSE evaluation
reports an average over 32trials of each target program, and for
eachtrial,we sample astringinputas follows:
â€¢thestringlengthischosenuniformlyatrandomfrom 8âˆ’16;
â€¢each character in the string is either chosen uniformly at
randomfromthecharacterset C="A-Za-z0-9,.-;|" ,or
chosen as white-space withprobability 15Ã—larger than the
probability ofany character in C.
Weruneachsuchinputcreatedonthetargetprogramtocreatethe
output.The input-output pairsare given to the synthesizer.
We evaluate SynGuar-STUN over3trials for each target, as
theseprogramsarecomputationallyheaviertosynthesize.Foreach
trial, we simulate a basic mutation-based fuzzer as the sampling
distribution. Specifically, we take the input strings provided in the
SyGuS benchmark as seeds, and mutate them randomly as follows:
â€¢add a string of randomly chosen length up to size 10at
a randomly chosen location of the seed string, with each
character being aprintablevalue6; or
â€¢remove arandomly chosencharacter from the seedstring.
For programs with more than one input argument, we addition-
allyaddrulesspecifyingwhetheroneoftheinputsisasubstring
of another input argument. We then randomly choose a substring
fromthatinputargumentwhensynthesizingexamples.Forinteger
inputs,werandomlychoseeitheranintegerboundedbythelength
of one of the string input arguments or a randomly chosen integer
from(0,1000). This ensuresthat thetarget program can be runon
the mutatedinputswithoutresultingintype errorsorfailure.
5.1 AccuracyImprovement
Toevaluatewhetherourtheoreticalgeneralizationguaranteestrans-
late into improved correctness, we check that the synthesized pro-
gramiscorrect, i.e., syntacticallyorsemanticallyequivalent to the
targetprogram.Oursyntacticequivalenceisconfirmedautomati-
cally,andforsemanticequivalence,weresorttomanualinspection.
WhileSynGuar mightproduceprograms that are closetothetar-
get asper its ğœ–-close guarantee,it is difficultto estimate closeness
objectively.Therefore,wetakeaconservativeapproachandonly
reportwhether thesynthesizedprogram iscorrect.Programsthat
are "almost"or"close to"correctare reportedas incorrect.
SynGuar-PROSE synthesizes 14/16programssemanticallyequiv-
alenttothetargetprograminall 32runsforğœ–=0.05,ğ›¿=0.02which
showsthat SynGuar-PROSE isusefultosynthesizecommonstring
manipulation programs. For one of the remaining benchmarks, the
synthesized program is correct on 26/32runs. For 1benchmark,
thesynthesizedprogramiscorrecton 7/32runs.Intotal, SynGuar-
PROSEproduces correctprograms in 481/512(93.95%)runs.
Weobservethatthevanilla StrPROSE synthesizer(without Syn-
Guar)consistentlyoverfitswhensamplesizesarechosenarbitrarily
smallerthanmandatedby SynGuar-PROSE .Forinstance,whenwe
6In Python,weuse string.printable and removethe whitespace charactersP1 P2 P3 P4 P5 P6 P7 P8 P9 P10 P11 P12 P13 P14 P15 P16100200300400500600700800
Programin PROSE-benchmarkNumberof Samples
ğœ–=0.1,ğ›¿=0.02ğœ–=0.05,ğ›¿=0.02ğœ–=0.02,ğ›¿=0.02
Figure 4: For most programs in the PROSE-benchmark ,
SynGuar-PROSE synthesizes programs with provable gener-
alization under 400examples for ğœ–=0.02,ğ›¿=0.02. For
ğœ–=0.1,ğ›¿=0.02,the#samplesdropto 58âˆ’218(average 107.22).
useexactly 4randomlychosenexamplesineachtrial,thevanilla
StrPROSE synthesizer produces incorrect programs on most of
the32runs for all target programs. Most of the synthesized pro-
gramsoverfittheexamples:itisonlycorrecton 176/512(34.38%)
runs in total. This confirms the importance of SynGuar â€™s main
objective:takingenoughexamplesuntilthesynthesizedprogram
isguaranteedto generalize withhigh probability.
SynGuar-STUN synthesizes 53/59correctprogramsfromthe
SyGuS-STUN forğœ–=0.05,ğ›¿=0.02forall3runs.Intotal,thisleads
to159/177(89.83%) correct runs. As a point of comparison, the
vanillaversion StrSTUN synthesizer,evaluatedontheexamples
providedin the SyGuS benchmark, produces correct programsfor
36/59ofthe target benchmark. SynGuar-STUN shows a29%im-
provementoverthevanilla StrSTUN ontheSyGuSbenchmark,syn-
thesizingcorrectly an additional 17programs inalltrials.Further,
thissuggeststhata significantnumberof SyGuS-STUN programs
in the benchmarkdo nothave enough examplesin the benchmark
toprovablygeneralize.Synthesizers,therefore,mayneedadditional
hints orassumptions to solve themcorrectly.
Toanalyzevanilla StrSTUN underthesameinputdistribution
asSynGuar-STUN , we further evaluate it on a fixed number of
randomly chosen examples in 3trials. We use sample size of 4per
trialfollowingpriorwork[ 26,55].Wefindthatvanilla StrSTUN
synthesizes only 33/59correct programs in all 3runs (in total
correcton 121/177runs),confirming that itoften overfits.
5.2 SampleSizeSufficient forGeneralization
Our work provides empirical evidence that a modest number of
examplessufficeforprovablegeneralizationfortheevaluatedtasks.
Figure4shows that we need between 100-400examples (about
197on average) to achieve (ğœ–=0.05,ğ›¿=0.02)generalization for
theSynGuar-PROSE on the16target programs evaluated. For a
smallererrortolerance (ğœ–=0.02,ğ›¿=0.02),theobservedrangeof
samplesizebecomes 200-900.Notethatsamplesizeissensitiveto ğœ–
as is theoretically expectedÃdistinguishing between two functions
685ESEC/FSE â€™21, August 23â€“28, 2021,Athens,Greece BoWang,Teodora Baluta, AashishKolluri, andPrateek Saxena
1 5 9 13 17 21 25 29 33 37 41 45 49 53 571002003004005006007008009001,0001,1001,2001,3001,4001,500
Program in SyGuS-STUNNumberof Samples
ğœ–=0.05,ğ›¿=0.02
Figure 5: For most programs in the SyGuS-STUN ,SynGuar-
STUNsynthesizes programs with provable generalization
under500examples for ğœ–=0.05,ğ›¿=0.02. Only11of these
programs require 500âˆ’1400examples.
that behave almost identically using random sampling will require
manysamples.
Figure5shows that we need between 140-1400examples (about
357on average) to achieve (ğœ–=0.05,ğ›¿=0.02)generalization for
the59target programs evaluated with SynGuar-STUN . Most of
the programsin the SyGuS benchmarkrequire under 500samples,
withonly 11programs requiring 500âˆ’1400I/O examples.
5.3 Reductionin ProgramSearch Space
Note that Theorem 3.1doesnotpredict how fast the procedure
willconvergetoageneralizedprogram,i.e.,howfastthespaceof
consistent programs will shrink after each example. This varies
empirically given the task and examples seen. But, SynGuar in-
ternally estimates (conservatively) how many programs remain
consistent after seeing each example. From this, two empirical
findings which explain our other observations emerge. First, the
programspaceshrinks drastically withthefirstfewexamplesfor
nearlyallbenchmarks.Second,thenumberofexamplesrequired
to generalize depends significantly on the expressiveness of the
chosenhypothesisspace.
Figure6shows the size of the consistent program space, for
6representative programs in the PROSE-benchmark computed
bySynGuar-PROSE . The Y-axis is a logarithmic scale. The full
evaluation of the rest of the programs is in the supplementary ma-
terial [2].We choose these programsasthey representthe largest,
smallest,andaveragecasesoftheprogramspacesizeafter 25ex-
amplesaveragedover 32runs,aswellasthelargestandsmallest
programspacesizedecreaseafter 5examples.Inparticular,P 15has
the largest decreaseonaverage inthe program space sizeafter 5
examplesfrom 1050to106whileP13hasthesmallestdecreaseafter
5examples from 9.7Â·103to27. This observation explains why a
smallnumberofexamplesturnouttobesufficientforgeneraliza-
tioninthisbenchmark.Italsoshowsthatreducingtheconsistent
hypothesis space further, after the initial quick reduction, becomes
increasingly difficult withunbiasedsampling.0 5 10 15 20 251001012102410361048
NumberofSamplesProgram Space Size
P2P5P11 P12 P13 P15
Figure 6: In PROSE-benchmark , the program space shrinks
3.7Â·102âˆ’1044Ã—onaveragewiththefirst 5examples,explain-
ingwhy SynGuar-PROSE can provablygeneralizeinmodest
numberofsamplesformostprograms inthe benchmark.
The SyGuS benchmark has target programs of different com-
plexity(differentnumberofconditionals).For SynGuar-STUN ,the
numberofsamplesrequiredtogeneralizedependsontargetpro-
gramâ€™s complexityÃmore complex programs require considering a
largeroriginalhypothesisspacetoberepresented.However,theal-
gorithmdoesnotknowtheoriginalhypothesisspace.Tousefewer
samples, the algorithm chooses the smallest hypothesis space that
stillcontainsprogramsconsistentwithsamples,becauseoutputting
morecomplexprograms(moreconditionals)requireschoosinga
larger hypothesisspacefor whichmore samples are needed.
We use the target programs in ğ»0as an example to show this
phenomenoninFigure7.Thefigureshowsforeachtargetprogram
thesamplesizesufficientfor (ğœ–,ğ›¿/3)-generalizationcalculatedby
the3parallel SynGuar instanceson ğ»0,ğ»1,andğ»2.Weshowthatif
SynGuar-STUN choosesaprogramin ğ»0,thenumberofexamples
is141âˆ’419.Ifthetargetprogramisin ğ»0butthesynthesizerchooses
a program in ğ»1, the number of samples is larger by 1166.22on
average. For example, for program 20in our3runs,213samples
aresufficienttopickaprogramin ğ»0,buttoreturnaprogramfrom
ğ»1orğ»2SynGuar-STUN requiresaround 1300and2400examples,
respectively. This quantitatively shows that the sample size can
vary by a large margin when considering more complex programs.
Moreover, this result explains why choosing a simpler program
firstcan require asmallernumber of examples.
6 RELATED WORK
The overfitting problem in learning programs from examples is
known. Many different approaches have been proposed to tackle it
(see Section 1). One lineof work proposesconditioning thesearch
with program traces rather than just I/O examples [ 13,21,54].
Anotherlineofworkimprovestheinputspecification[ 18,33]us-
ingdomain-specificknowledgeaboutthehypothesisspace.Singh
et al. propose to rank the synthesized functions based on distri-
butional priors with machine learning [ 55]. Similarly, several in-
ductive synthesis techniques use deep learning to improve their
686SynGuar : Guaranteeing Generalizationin Programmingby Example ESEC/FSE â€™21, August 23â€“28, 2021,Athens,Greece
1 5 9 13 17 21 25 29 33 37 41 4501,0002,0003,000
Programin SyGuS-STUNNumberofSamples
ğ‘âˆˆğ»0ğ‘âˆˆğ»1ğ‘âˆˆğ»2
Figure 7: For target programs with no condition ( ğ‘¡âˆˆğ»0),
choosing a program ğ‘âˆˆğ»0, versus a program with one con-
dition (ğ»1) or two conditions ( ğ»2) leads to provable general-
ization inless numberofsamples.
search [7,45,61]. Broadly speaking, these approaches are comple-
mentary to ours as they either require domain-specific priors or
theylearnprogrampatternsfromadatasetofsimilarprograms[ 47].
Moreover,none ofthemclaims any generalization guarantees.
Several works suggest that larger test harnesses lead to promis-
ingimprovementsinsynthesizedprograms[ 33,44],whichisalso
observed in related domains of program repair [ 23,24] and invari-
antlearning[ 9].Ourwork,motivatedbythese,providestheformal
bridgebetweentestharnesssizeandprovablegeneralization.We
alsoshowthatittranslatesintosignificantlyimprovedaccuracyon
string-manipulating tasks,directlydueto reducedoverfitting.
Establishinggeneralizationguaranteesforsynthesizershasbeen
studied for over three decades. The PAC learnability framework
has been introduced by Valiant [ 58] foranalyzing generalization
fromacomputationalperspective.Underthistheory,thesample
complexity required for generalization has been established for
learninginpropositionallogicdomain[ 29,51,58].Theresultshave
been extended to learning logic programs i.e., predicate logic do-
main [14,15,20]. Some oftheabove boundsarelimitedtocertain
typesofhypothesisspaces.Instead,Blumeretal.havegiventwo
sample complexity bounds 1) union bound [ 10] and 2) Vapnik-
Chervonenkis Dimension bound [ 11] that are more general. The
union bound is used when the hypothesis space is finite and when
the VC dimension of a model is difficult to estimate [ 8,34]. Whilst
the above works have established the bounds in theory, their ap-
plicability in real-world synthesizers have been very limited. A
recent work uses the VC dimension argument for synthesizing lin-
ear arithmetic functions for holes given in a sketch [ 19]. For many
programmingdomains,suchas forstring-manipulatingprograms,
itisdifficult to compute VC dimensions.
BesidesthePACframework,anotherlineofworktowardsgen-
eralizationisthroughactivelearning.Someinteractivesynthesis
systemshavequestionselectionmechanismstofinddistinguishing
input [37,60].Ji et al.further approximate optimal questions to
resolveambiguityinlessnumberofsamples[ 31].However,theseapproaches do not give generalization guarantees without assum-
ing the existence of the target programs in the hypothesis space or
apriordistributionovertargetprograms,sotheyareorthogonalto
our approach whichworks underminimal assumptions.
Outside of program synthesis, generalization has been exten-
sively studied in machine learning. Our work bridges the two lines
ofinquirythathaveevolvedinparallel.ApartfromPAC-styledefini-
tionsbasedonsamplecomplexity,generalizationcanbeachievedus-
ingalgorithmicstability[ 12].Boundshavebeenestablishedforboth
convexoptimizationsandnon-convexoptimizationalgorithms,i.e.,
lowsensitivitytosmallchangesininputs[ 28,36,40,43,50].These
worksleveragethepropertiesofalgorithmslikestochasticgradient
descent(SGD)andstochasticgradientLangevindynamic(SGLD)
in order to estimate the generalization bounds for a given num-
ber of samples. Adapting the framework of algorithmic stability
to PBE-based synthesis is promising future work, but it is chal-
lenging. A direct adaptation, for example, would restrict learnt
programstobestable,forwhichsmallchangesinoutputsforsmall
changes in inputs. Generalization has been explored from other
perspectives such as by boundingnetwork capacity [ 41] and over-
parametrization [ 4,32,62] in machine learning literature, which
arealsoalternativestartingpointsforstudyinggeneralizationin
program synthesis.
7 CONCLUSION
In this work, we exploit the theoretical connection between gener-
alizationandthenumbersofexamplesusedinprogramming-by-
examplesynthesis.Weprovidethefirstprincipledapproachthat
guarantees generalization with a modest number of examples in
this regime. Key to this result is our mechanism for computing
sample complexity on the fly. We show experimentally that this
significantly reduces overfitting and improves accuracy for synthe-
sizing string-manipulation programs, compared to approaches that
use arbitrarilyfewer examples.
ACKNOWLEDGMENTS
We thankShiqi Shen,ShrutiHiray,and theanonymousreviewers
for helpful feedback on this work. This work was supported by
Crystal Centre at National University of Singapore, a Singapore
Ministry of Education Academic Research Fund Tier 1 (WBS num-
ber R-252-000-B50-114), and a research grant with WBS number
R-252-000-B14-281. All opinions in this work are solely those of
the authors.
REFERENCES
[1] 2019. SyGuS-Comp 2019 .https://sygus.org/comp/2019/
[2] 2021. SupplementaryMaterial .https://github.com/HALOCORE/SynGuar
[3]AwsAlbarghouthi,SumitGulwani,andZacharyKincaid.2013. Recursivepro-
gramsynthesis.In Computer-AidedVerification(CAV) .https://doi.org/10.1007/
978-3-642-39799-8_67
[4]ZeyuanAllen-Zhu,YuanzhiLi,andYingyuLiang.2019. LearningandGeneral-
izationinOverparameterizedNeuralNetworks,GoingBeyondTwoLayers.In
Conference onNeural InformationProcessingSystems(NeurIPS) .
[5]Rajeev Alur, Pavol ÄŒernÃ½, and Arjun Radhakrishna. 2015. Synthesis Through
Unification.In Computer-AidedVerification(CAV) .https://doi.org/10.1007/978-3-
319-21668-3_10
[6]Shengwei An, Rishabh Singh, Sasa Misailovic, and Roopsha Samanta. 2019. Aug-
mented example-based synthesis using relational perturbation properties. In
Principles ofProgrammingLanguages (POPL) .https://doi.org/10.1145/3371124
687ESEC/FSE â€™21, August 23â€“28, 2021,Athens,Greece BoWang,Teodora Baluta, AashishKolluri, andPrateek Saxena
[7]MatejBalog,AlexanderLGaunt,MarcBrockschmidt,SebastianNowozin,and
Daniel Tarlow. 2017. Deepcoder: Learning to write programs. In International
Conference onLearning Representations (ICLR) .
[8]Guy Blanc, Jane Lange, and Li-Yang Tan. 2020. Top-Down Induction of Decision
Trees:RigorousGuaranteesandInherentLimitations.In InnovationsinTheoretical
Computer Science(ITCS) .https://doi.org/10.4230/LIPIcs.ITCS.2020.44
[9]TimBlazytko,MoritzSchlÃ¶gel,CorneliusAschermann,AliAbbasi,JoelFrank,
SimonWÃ¶rner,andThorstenHolz.2020. AURORA:StatisticalCrashAnalysis
for Automated Root Cause Explanation.In USENIXSecurity .
[10]Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K Warmuth.
1987. Occamâ€™srazor. Informationprocessingletters 24,6(1987),377Å›380. https:
//doi.org/10.1016/0020-0190(87)90114-1
[11]Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K Warmuth.
1989. LearnabilityandtheVapnik-ChervonenkisDimension. JournaloftheACM
(JACM)36(1989). https://doi.org/10.1145/76359.76371
[12]OlivierBousquetandAndrÃ©Elisseeff.2002. StabilityandGeneralization. Journal
ofMachineLearning Research (JMLR) 2 (2002).
[13]Xinyun Chen, Chang Liu, and Dawn Song. 2019. Execution-Guided Neural Pro-
gram Synthesis. In InternationalConference onLearningRepresentations (ICLR) .
[14]William W. Cohen. 1994. Pac-learning recursive logic programs: efficient al-
gorithms. JournalofArtificialIntelligenceResearch(JAIR) 2,1(1994),501Å›539.
https://doi.org/10.1613/jair.97
[15]WilliamW.Cohen.1994. Pac-learningrecursivelogicprograms:negativeresults.
Journal of Artificial Intelligence Research (JAIR) 2, 1 (1994), 541Å›573. https:
//doi.org/10.1613/jair.1917
[16]Tamraparni Dasu and Theodore Johnson. 2003. Exploratory Data Mining and
Data Cleaning . Vol. 479. John Wiley & Sons. https://doi.org/10.1002/0471448354
[17]JacobDevlin,JonathanUesato,SuryaBhupatiraju,RishabhSingh,Abdel-rahman
Mohamed,andPushmeetKohli.2017. RobustFill:neuralprogramlearningunder
noisyI/O. In InternationalConference onMachineLearning (ICML) .
[18]DanaDrachsler-Cohen,SharonShoham, andEran Yahav.2017. Synthesiswith
Abstract Examples. In Computer-Aided Verification (CAV) .https://doi.org/10.
1007/978-3-319-63387-9_13
[19]Samuel Drews, Aws Albarghouthi, and Loris Dâ€™Antoni. 2019. Efficient Synthesis
with Probabilistic Constraints. In Computer-Aided Verification (CAV) .https:
//doi.org/10.1007/978-3-030-25540-4_15
[20]SaÅ¡oDÅ¾eroski,StephenMuggleton,andStuartRussell.1992. PAC-learnability
of determinate logic programs. In Annual Workshop on Computational Learning
Theory(COLT) . 128Å›135. https://doi.org/10.1145/130385.130399
[21]KevinEllisandSumitGulwani.2017. LearningtoLearnProgramsfromExamples:
Going Beyond Program Structure. In International Joint Conferences on Artificial
IntelligenceOrganization (IJCAI) .https://doi.org/10.24963/ijcai.2017/227
[22]P. Ezudheen,Daniel Neider, Deepak Dâ€™Souza, Pranav Garg, and P. Madhusudan.
2018. Horn-ICELearningforSynthesizingInvariantsandContracts.In Object-
Oriented Programming, Systems, Languages & Applications (OOPSLA) .https:
//doi.org/10.1145/3276501
[23]Xiang Gao, Shraddha Barke, Arjun Radhakrishna, Gustavo Soares, Sumit Gul-
wani,AlanLeung,NachiappanNagappan,andAshishTiwari.2020. Feedback-
Driven Semi-Supervised Synthesis of Program Transformations. In Object-
Oriented Programming, Systems, Languages & Applications (OOPSLA) .https:
//doi.org/10.1145/3428287
[24]Claire Le Goues, Michael Pradel, and Abhik Roychoudhury. 2019. Automated
ProgramRepair. Commun. ACM (2019).https://doi.org/10.1145/3318162
[25]PeterGrÃ¼nwald.2007. TheMinimumDescriptionLengthPrinciple .https://doi.
org/10.7551/mitpress/4643.001.0001
[26]Sumit Gulwani. 2011. Automating String Processing in Spreadsheets Using
Input-OutputExamples.In PrinciplesofProgrammingLanguages(POPL) .317Å›330.
https://doi.org/10.1145/1926385.1926423
[27]SumitGulwani. 2016. Programming by Examples - anditsapplicationsin Data
Wrangling. VerificationandSynthesisofCorrectandSecureSystems (2016).https:
//doi.org/10.3233/978-1-61499-627-9-137
[28]Moritz Hardt, Benjamin Recht, and Yoram Singer. 2016. Train Faster, Generalize
Better:StabilityofStochasticGradientDescent.In InternationalConferenceon
MachineLearning (ICML) .
[29]DavidHaussler.1992. DecisiontheoreticgeneralizationsofthePACmodelfor
neural net and other learning applications. Information and Computation 100
(1992).https://doi.org/10.1016/0890-5401(92)90010-D
[30]Susmit Jha, Sumit Gulwani, Sanjit A Seshia, and Ashish Tiwari. 2010. Oracle-
guided component-based program synthesis. In International Conference on Soft-
wareEngineering (ICSE) .https://doi.org/10.1145/1806799.1806833
[31]Ruyi Ji, Jingjing Liang, Yingfei Xiong, Lu Zhang, and Zhenjiang Hu. 2020. Ques-
tion Selection for Interactive Program Synthesis. In Programming Language
Design and Implementation (PLDI) . 1143Å›1158. https://doi.org/10.1145/3385412.
3386025
[32]K. Kawaguchi and J. Huang. 2019. Gradient Descent Finds Global Minima
for Generalizable Deep Neural Networks of Practical Sizes. In Annual Aller-
ton Conference on Communication, Control, and Computing (Allerton) .https:
//doi.org/10.1109/ALLERTON.2019.8919696[33]LarissaLaich,PavolBielik,andMartinVechev.2020. GuidingProgramSynthe-
sisbyLearningtoGenerateExamples.In InternationalConferenceonLearning
Representations (ICLR) .
[34]Tessa Lau,Steven A.Wolfman,PedroDomingos,andDanielS.Weld. 2003. Pro-
grammingbyDemonstrationUsingVersionSpaceAlgebra. MachineLearning
53,1 (2003), 111Å›156. https://doi.org/10.1023/A:1025671410623
[35]Tessa A. Lau, Pedro Domingos, and Daniel S. Weld. 2000. Version Space Al-
gebraanditsApplicationtoProgrammingbyDemonstration.In International
Conference onMachineLearning (ICML) .
[36]Ben London. 2017. A PAC-Bayesian Analysis of Randomized Learning with
Application to Stochastic Gradient Descent. In Conference on Neural Information
ProcessingSystems(NeurIPS) .
[37]MikaÃ«lMayer,GustavoSoares,MaximGrechkin,VuLe,MarkMarron,Oleksandr
Polozov, Rishabh Singh, Benjamin Zorn, and Sumit Gulwani. 2015. User Interac-
tionModelsforDisambiguationinProgrammingbyExample.In UserInterface
Software & Technology (UIST) . 291Å›301. https://doi.org/10.1145/2807442.2807459
[38]SergeyMechtaev,AlbertoGriggio,AlessandroCimatti,andAbhikRoychoudhury.
2018. Symbolic execution with existential second-order constraints. In ESEC/FSE .
389Å›399. https://doi.org/10.1145/3236024.3236049
[39]TomMMitchell.1982. Generalizationassearch. ArtificialIntelligence 18,2(1982),
203Å›226. https://doi.org/10.1016/0004-3702(82)90040-6
[40]Wenlong Mou, Liwei Wang, Xiyu Zhai, and Kai Zheng. 2018. Generalization
Bounds of SGLD for Non-convex Learning: Two Theoretical Viewpoints. In
Conference onLearning Theory, PMLR . 605Å›638.
[41]Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro.
2018. APAC-BayesianApproachtoSpectrally-NormalizedMarginBoundsfor
NeuralNetworks.In InternationalConferenceonLearningRepresentations(ICLR) .
[42]HilaPelegandNadiaPolikarpova.2020. PerfectistheEnemyofGood:Best-Effort
Program Synthesis. In European Conference on Object-Oriented Programming
(ECOOP).https://doi.org/10.4230/LIPIcs.ECOOP.2020.2
[43]A. Pensia, V. Jog, and P. Loh. 2018. Generalization Error Bounds for Noisy,
IterativeAlgorithms.In InternationalSymposiumonInformationTheory(ISIT) .
https://doi.org/10.1109/ISIT.2018.8437571
[44]Daniel Perelman, Sumit Gulwani, Dan Grossman, and Peter Provost. 2014. Test-
DrivenSynthesis.In ProgrammingLanguage Designand Implementation(PLDI) .
https://doi.org/10.1145/2594291.2594297
[45]Illia Polosukhin and Alexander Skidanov. 2018. Neural Program Search: Solving
ProgrammingTasks from Description and Examples.In ICLRworkshop .
[46]Oleksandr Polozov and Sumit Gulwani. 2015. FlashMeta: a framework for induc-
tiveprogram synthesis.In Object-OrientedProgramming, Systems, Languages&
Applications(OOPSLA) ,Vol.50.107Å›126. https://doi.org/10.1145/2858965.2814310
[47]Veselin Raychev, Martin Vechev, and Andreas Krause. 2015. Predicting Program
Properties from "Big Code". In Principles of Programming Languages (POPL) .
https://doi.org/10.1145/2676726.2677009
[48]Andrew Reynolds, Haniel Barbosa, Andres NÃ¶tzli, Clark Barrett, and Cesare
Tinelli. 2019. cvc4sy: Smart and Fast Term Enumeration for Syntax-Guided
Synthesis. In Computer-Aided Verification (CAV) .https://doi.org/10.1007/978-3-
030-25543-5_5
[49]Andrew Reynolds, Morgan Deters, Viktor Kuncak, Cesare Tinelli, and Clark
Barrett. 2015. Counterexample-Guided Quantifier Instantiation for Synthesis in
SMT.InComputer-AidedVerification(CAV) .https://doi.org/10.1007/978-3-319-
21668-3_12
[50]OmarRivasplata,Emilio Parrado-HernÃ¡ndez, JohnShawe-Taylor, ShiliangSun,
and Csaba SzepesvÃ¡ri. 2018. PAC-Bayes Bounds for Stable Algorithms with
Instance-Dependent Priors. In Conference on Neural Information Processing Sys-
tems(NeurIPS) .
[51]Ronald L. Rivest. 1987. Learning Decision Lists. Machine Learning 2 (1987).
https://doi.org/10.1023/A:1022607331053
[52]JeanE.Sammet.1966. TheUseofEnglishasaProgrammingLanguage. Commun.
ACM(1966).https://doi.org/10.1145/365230.365274
[53]ShiqiShen,ShwetaShinde,SoundaryaRamesh,AbhikRoychoudhury,andPra-
teek Saxena. 2019. Neuro-Symbolic Execution: Augmenting Symbolic Execution
with Neural Constraints. In Network and Distributed Systems Security (NDSS) .
https://doi.org/10.14722/ndss.2019.23530
[54]EuiChulShin,IlliaPolosukhin,andDawnSong.2018. ImprovingNeuralProgram
SynthesiswithInferredExecutionTraces.In ConferenceonNeuralInformation
ProcessingSystems(NeurIPS) .
[55]RishabhSinghandSumitGulwani.2015. PredictingaCorrectPrograminPro-
gramming by Example. In Computer-Aided Verification (CAV) .https://doi.org/10.
1007/978-3-319-21690-4_23
[56]David Canfield Smith. 1975. PYGMALION: A Creative Programming Environment .
Technical Report. Stanford University. https://doi.org/10.21236/ada016811
[57]PhillipDSummers.1977. AmethodologyforLISPprogramconstructionfrom
examples. Journal of the ACM (JACM) (1977).https://doi.org/10.1145/321992.
322002
[58]L. G. Valiant. 1984. A Theory of the Learnable. Commun. ACM 27 (1984).
https://doi.org/10.1145/1968.1972
688SynGuar : Guaranteeing Generalizationin Programmingby Example ESEC/FSE â€™21, August 23â€“28, 2021,Athens,Greece
[59]VladimirVapnik.2000. TheNatureofStatisticalLearningTheory . Springerscience
& businessmedia. https://doi.org/10.1007/978-1-4757-3264-1
[60]ChenglongWang,AlvinCheung,andRastislavBodik.2017. InteractiveQuery
SynthesisfromInput-OutputExamples.In InternationalConferenceonManage-
ment ofData (SIGMOD) . 1631Å›1634. https://doi.org/10.1145/3035918.3058738[61]AmitZoharandLiorWolf.2018. AutomaticProgramSynthesisofLongPrograms
withaLearnedGarbageCollector.In ConferenceonNeuralInformationProcessing
Systems(NeurIPS) .
[62]DifanZou, Yuan Cao, Dongruo Zhou, andQuanquan Gu.2020. Gradientdescent
optimizesover-parameterizeddeepReLUnetworks. MachineLearning 109(2020),
1Å›26.https://doi.org/10.1007/s10994-019-05839-6
689