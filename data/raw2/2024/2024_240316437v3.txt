Reasoning Runtime Behavior of a Program with
LLM: How Far Are We?
Junkai Chen∗
School of Software Technology,
Zhejiang University
Ningbo, China
junkaichen@zju.edu.cn
Zhenhao Li
York University
Toronto, Canada
zhenhao.li@ieee.orgZhiyuan Pan∗
The State Key Laboratory of
Blockchain and Data Security,
Zhejiang University
Hangzhou, China
zypan@zju.edu.cn
Ge Li
Peking University
Beijing, China
lige@pku.edu.cnXing Hu†
The State Key Laboratory of
Blockchain and Data Security,
Zhejiang University
Hangzhou, China
xinghu@zju.edu.cn
Xin Xia
Zhejiang University
Hangzhou, China
xin.xia@acm.org
Abstract —Large language models for code (i.e., code LLMs)
have shown strong code understanding and generation capabili-
ties. To evaluate the capabilities of code LLMs in various aspects,
many benchmarks have been proposed (e.g., HumanEval and
ClassEval). Code reasoning is one of the most essential abilities
of code LLMs (i.e., predicting code execution behaviors such as
program output and execution path), but existing benchmarks
for code reasoning are not sufficient. Typically, they focus
on predicting the input and output of a program, ignoring
the evaluation of the intermediate behavior during program
execution, as well as the logical consistency (e.g., the model
should not give the correct output if the prediction of execution
path is wrong) when performing the reasoning. To address
these problems, in this paper, we propose a framework, namely
REval, for evaluating code reasoning abilities and consistency
of code LLMs with program execution. We utilize existing
code benchmarks and adapt them to new benchmarks within
our framework. A large-scale empirical study is conducted and
most LLMs show unsatisfactory performance on both Runtime
Behavior Reasoning (i.e., an average accuracy of 44.4%) and
Incremental Consistency Evaluation (i.e., an average IC score
of 10.3). Evaluation results of current code LLMs reflect the
urgent need for the community to strengthen the code reasoning
capability of code LLMs. Our code, data and REval leaderboard
are available at https://r-eval.github.io.
Index Terms —Code Reasoning, Large Language Model,
Benchmark
I. I NTRODUCTION
Large language models (LLMs) attract great attention for
their exceptional performance on diverse tasks [1] including
sentiment analysis [2], logical reasoning [3], and question
answering [4]. Recently, large language models for code (i.e.,
code LLMs) have become a popular research area because
of the promising prospect of empowering humans in software
development and maintenance [5]. Hence, both academia and
industry have proposed a lot of code LLMs (e.g., CodeLlama
∗Equal contribution.†Corresponding author.family [6] and Magicoder series [7]), which are widely applied
to different tasks like code generation [8], [9].
To provide a fair and comprehensive measure of the capa-
bilities of code LLMs, many code-related benchmarks (e.g.,
HumanEval [8] and CodeXGLUE [10]) are proposed to eval-
uate the effectiveness of code LLMs in different tasks such as
code generation and vulnerability detection [11]. Given that
“executable” is a distinct feature of code compared to natural
language, and code execution provides additional information
(e.g., program output) to assist with code tasks [12], [13],
benchmarking code reasoning abilities of code models with
execution raises researchers’ interests [14], [15]. Here, code
reasoning is referred to as predicting code execution behaviors
(e.g., program outputs, execution paths and possible variable
values) without executing the code directly. For example, Gu
et al. [14] proposed CRUXEval to evaluate code LLMs by
predicting output from input and vice versa. Typically, these
works measure the model’s ability to predict and analyze the
relationship between the input and output of an executable pro-
gram. However, the intermediate information (e.g., execution
path) during code execution is ignored, posing challenges to
developers in comprehending the program’s runtime behavior.
Fig. 1 shows common concerns about the runtime behavior
during program execution. Intuitively, how a program behaves
under certain input can help developers better understand the
code and perform debugging activities. For example, if we
have concerns about the correctness of a certain statement
while debugging a snippet of code, we typically first determine
whether this statement is executed given the input (i.e., ❶in
Fig. 1); If it is executed, observing the changes in variables
before and after execution is a natural choice ❷; Sometimes
this line of code may seem fine, so the statement immediately
following it will be examined ❸; Additionally, the program
output can be used to verify whether the results match the
expectations ❹. Therefore, we argue that these kinds of
runtime behaviors (e.g., program state and execution path) arearXiv:2403.16437v3  [cs.SE]  21 Sep 2024OutputInput Code deff(var):
ifvar.isdigit ():
return"int"
elifvar.replace ('.', '', 1).isdigit():
return"float"
elifvar.count(' ') ==len(var) -1:
return"str"
eliflen(var) ==1:
return"char"
else:
return"tuple"What is the next line 
to be executed?What is type and value of var?Is the statement executed ? What is the input?
" 99 777"
'tuple'×❶√
❷
❸What is the output?
CRUXEval ❹❺ REval❶❷❸❹ + IC❹❺Fig. 1. The demonstration of code reasoning tasks in CRUXEval [14] and
REval. “IC”: Incremental Consistency.
essential for program understanding and reasoning for humans.
Meanwhile, they are also proven to be effective for an in-depth
understanding of code semantics for language models [16]. As
previous benchmarks like CRUXEval (i.e., with ❹and❺) fail
to evaluate whether LLMs can reason about these dynamic
characteristics of a program, it is necessary to measure the
code reasoning ability of LLMs with runtime behavior of
execution. In this paper, we propose our framework, REval,
to comp rehensively ( re)-evaluate the code reasoning ability
of LLMs, which consists of two evaluation components: (i)
Runtime Behavior Reasoning and (ii) Incremental Consistency
Evaluation.
Evaluation Component 1: Runtime Behavior Reasoning .
To mitigate this limitation in previous research, we make the
first attempt to systematically evaluate the code LLM’s ability
to reason about the runtime behavior of program execution.
Specifically, we propose four evaluation tasks to achieve this
goal:❶Code Coverage Prediction (CCP), i.e., whether a
statement is executed or not; ❷Program State Prediction
(PSP), i.e., what is the value and the type of a variable;
❸Execution Path Prediction (EPP), i.e., which is the next
statement to be executed; and ❹Output Prediction (OP), i.e.,
what is the output. These four tasks cover various aspects
of program execution, including control flow, data flow, and
type dependency, which are widely applied to prior research
in software engineering such as type inference [17] and code
translation [18]. Therefore, this evaluation provides a more
comprehensive measure of code model’s ability to reason
about executable programs in comparison with previous work.
Nevertheless, it is noticed that sometimes the reasoning re-
sults of a model could conflict with human logic on sequential
tasks in Runtime Behavior Reasoning. For instance, the code
model may correctly predict the next statement to be executed
(i.e., EPP) when it fails to tell the value of a variable after
the statement’s execution (i.e., PSP), which is not expected
because the control flow of the execution relies on the program
state. As this kind of inconsistency in sequentially related
tasks is unlikely to occur in humans, the trustworthiness of
AI systems built on these models (e.g., GitHub Copilot [19])
can easily suffer from these unreliable behaviors. Althoughsome previous works have discussed consistency for code
LLMs [20], [21], they are limited to semantic consistency like
back translation between NL and code and ignore the logical
consistency mentioned here. Hence, it is necessary to measure
the consistency of code LLMs on sequentially related tasks.
Evaluation Component 2: Incremental Consistency Eval-
uation .To fill the gap in evaluation, we propose a novel
metric named Incremental Consistency (IC) to measure the
extent to which the model can maintain its logical consistency
on sequentially related tasks of incremental difficulty. We
observe that the four tasks in Runtime Behavior Reasoning
are progressive and consistent with the context of IC, i.e., the
knowledge required to finish the current task is the preliminary
of the next task. Hence, we can judge how much a model is
incrementally consistent by utilizing the results of reasoning
runtime behavior (See Section III for details). Incremental
Consistency provides new sights for evaluating LLMs and the
consistency measure of AI systems beyond traditional metrics.
To construct our framework, we leverage existing executable
datasets (e.g., HumanEval [8] and ClassEval [22]) as our base
benchmarks and adapt them into an adapted benchmark within
our framework by extracting runtime behavior, constructing,
and filtering the problems. We conduct a large-scale empirical
study on various models, including general and code LLMs in
our frameworks. Evaluation results show that our framework
presents a degree of difficulty and most LLMs show poor
performance on both Runtime Behavior Reasoning and IC
Evaluation (e.g., an IC score below 20 for all open-source
LLMs we evaluate). Our research highlights the importance
of utilizing runtime behavior and incremental consistency
evaluation to measure the reasoning ability of code LLMs, and
we call for targeted efforts in subsequent research to enhance
these weaknesses.
In summary, the contributions of our paper are as follows:
•We propose a new framework, REval, to comprehensively
evaluate code LLMs’ abilities of code reasoning. To the best
of our knowledge, we are the first work to evaluate code
models to systematically reason about runtime behavior
during program execution.
•We propose a novel metric named Incremental Consistency
(IC) to measure to what extent a code LLM can maintain its
consistency across sequentially related tasks of incremental
difficulty.
•We conduct a large-scale empirical study on diverse LLMs
within our evaluation framework. Our results reveal the
limitations of reasoning runtime behavior and IC of code
models.
•We construct an adapted benchmark based on Hu-
manEval [8] and ClassEval [22] and develop an evaluation
harness for our framework. To facilitate further research of
code reasoning, our code, data, and REval leaderboard are
publicly available at https://r-eval.github.io.
II. B ACKGROUND AND RELATED WORK
In this section, we discuss the background information of
our research and the corresponding related work.A. Code Execution and Reasoning
1) Code Execution Behavior: We refer to code execution
behavior as the additional information offered by program exe-
cution compared to static analysis. According to the execution
order, we classify them into pre/post-execution information
and runtime information:
•Pre/Post-Execution Information is the content we can
obtain before or after the actual execution process of
program. For example, the input, output, and NL require-
ments belong to this category.
•Runtime Information is the intermediate state during code
execution. For instance, we are able to collect contents
like program state and execution path only when the code
is still running.
Previous research has leveraged code execution behavior to
improve the performance of various downstream tasks, e.g.,
program understanding [23], code generation [12], [13], [24],
software testing [25], [26] and vulnerability detection [16]. Ni
et al. [12] improved code generation performance with an extra
verifier, which learns the results of code execution and helps
rerank generated code candidates. Chen et al. [13] utilized
different kinds of feedback including output to help LLMs
“self-debug” the generated code. They designed a series of
prompting strategies to guide the model to refine the program
automatically. In these works, pre-/post-execution information,
such as program output, is applied to code generation. Fur-
thermore, some works found the worth of dynamic features
during execution and exploit them to train various language
models. Liu et al. [24] pre-trained a language model to
learn the execution process of the program. Specifically, they
represented the program state as a sequence that neural models
can learn from and expect the model to predict the trace.
Compared to Liu et al., Ding et al. [16] proposed a pre-training
technique combining both static and dynamic characteristics
of the program. In summary, the aforementioned works reflect
the close relationship between the behavior of code execution
and the program semantics, and emphasize the importance of
evaluating models for code reasoning with execution.
2) Code Reasoning with Large Language Models: As in-
troduced in Section I, in the task of code reasoning, an LLM
needs to predict the program behavior without execution.
Recently, some works have proposed different evaluation
approaches for the code reasoning abilities of code LLMs. For
example, Gu et al. [14] proposed CRUXEval, which requires
LLMs to reason about pre/post-execution information such as
input and output. Following this study, similar to the idea
of CRUXEval, Liu et al. [15] extended the evaluation tasks
(i.e., predicting input and output) to the natural language
specification. However, their evaluation approaches are still
limited to pre/post-execution information and ignore inter-
mediate runtime behavior. In contrast, our work goes a step
further to measure how the model learns the runtime behavior
during execution, which shows promising potential in helping
program comprehension, as mentioned above. We notice that
a recent work [27] aimed to simulate the code executionprocess with code LLMs. They used the analogy of a large
language model to a CPU to explore the process of a program
executing code, paying more attention to algorithm complexity
and structure. Different from the aforementioned studies, our
framework is not only limited to algorithm problems (e.g.,
competition-level ones), but also suitable for general program-
ming scenarios (e.g., more real-world projects). In addition, we
also explore detailed runtime behavior like code coverage and
execution path, containing more runtime information.
B. Consistency for Large Language Models
Semantic Consistency. Semantic consistency refers to the
same decisions on semantically equivalent texts of LLMs [28].
For example, the model should provide similar and even
the same answers in the face of two meaning-perserving
questions. In the realm of software engineering, this feature
is generally utilized for the unsupervised evaluation of code
LLMs [20], [21]: Min et al. [21] evaluated the self-consistency
of code LLMs by comparing the functional correctness of
two code snippets: one code is generated using a human-
written description, and the other is generated iteratively with
the summary of the previously generated. Chen et al. [29]
studied the robustness of code LLMs to the variations in
natural language descriptions for code generation. Allamanis
et al. [20] introduced round-trip correctness which aligns code
and NL to perform unsupervised evaluation for code LLMs.
The aformentioned works leveraged the back translation be-
tween NL and PL iteratively generated by the model and
conduct the semantic or functional comparison. However, they
are restricted to the context of semantic consistency in the
context of NL and PL.
Logical Consistency. If an LLM is able to make predictions
without logical contradiction, it shows the feature of logi-
cal consistency [28]. For example, if one model assumes a
proposition to be true, it should consider the negation of that
proposition to be false as well. There are lots of previous
research about how to evaluate and utilize logical consistency
for LLMs in natural language processing [28], [30]–[32],
but few works pay attention to logical consistency on code
LLMs. As the reasoning ability is highly related to its logical
consistency [33], a comprehensive code reasoning evaluation
should contains the measure of logical consistency in scope
of programming languages (PLs). Therefore, it motivates us
to propose a novel consistency metric idea named IC to fill
this gap.
C. Code LLMs and Benchmarks
1) Code LLMs: Large language models for code are LLMs
specialized for the generation and understanding of PLs. For
example, CodeLlama family models [6] inherit the architecture
of Llama2 [34] and are further pre-trained on extra code cor-
pora. Its three variant models (i.e., base,instruct , and Python-
specialized ) are designed for different programming scenarios.
StarCoder2 [35] is a series of code LLMs developed by
the BigCode Project, which achieves competitive performance
with other similar-sized models. These models are trained√
×Code Coverage Prediction
Execution Path PredictionProgram State Prediction
Output Prediction
 Other Contexts StatementBase Benchmark
Value?
Type?
?CCPCode
 Test Case
Benchmark ConstructionLarge Language Model
Exec -
utionIncremental
Consistency
Non Incremental 
Consistency
Runtime Behavior Reasoning Incremental Consistency Evaluation
PSP EPP OP
Correct Prediction
 Wrong PredictionFig. 2. Overview of our framework. Benchmark Construction : we adapt the base benchmarks to fit our framework by execution. Runtime Behavior
Reasoning : we propose four tasks including CCP, PSP, EPP, and OP, which challenge LLMs to perform code reasoning. Incremental Consistency Evaluation :
we evaluate if the model can maintain consistency on sequentially related tasks (i.e., Incremental Consistency).
on The Stack v2 dataset [35] whose data size is four times
larger than its first generation. CodeGen2.5 models [36] are
improved versions of their previous models (e.g., CodeGen2)
for program synthesis. It is claimed that their performance
gains mainly come from optimizations such as training and
sampling strategies.
Code reasoning is one of the most important capabilities of
LLMs related to code [14], [15], but few work are dedicated to
its evaluation. In this paper, we aim at comprehensively eval-
uating their reasoning capabilities for programming languages
based on execution.
2) Benchmarks: Recently, many code generation bench-
marks have been proposed to evaluate the correctness of code
snippets generated by code LLMs. HumanEval [8] is one of
the most popular benchmarks for code generation. It consists
of 164 competitive programming problems and evaluates the
functional correctness of generated samples rather than text
similarity. Apart from competitive benchmarks of which ques-
tion generally runs in a simple context, lots of context-aware
benchmarks such as CoderEval [37], ClassEval [22], and
CrossCodeEval [38] have been proposed. These benchmarks
provide more complex surrounding contexts and dependencies
(e.g., private libraries) to evaluate code generation in real-
world projects. In addition, some domain-specific benchmarks
have been proposed to evaluate the performance of code
generation in various programming languages and paradigms
(e.g., DS-1000 [39] for data science). Our work utilizes exist-
ing executable benchmarks for code generation and evaluates
code LLMs with respect to code reasoning, which makes our
framework universal and applicable to different scenarios.
Prior studies proposed a variety of code understanding tasks
including code search, type inference, and code translation.
Evaluating the ability of code understanding is essential for
code LLMs. Lu et al. [10] propose CodeXGLUE, a compre-
hensive benchmark for code models that supports 10 tasks re-
lated to code and text. Cassano et al. [40] create a hand-craftedbenchmark to evaluate the instruction following ability on code
editing. Khan et al. [41] introduce a large-scale multilingual
multitask benchmark that consists of numerous executable
coding examples. Compared to the above benchmarks for
code understanding, we propose a comprehensive framework
for code reasoning from the perspective of runtime behavior,
which provides a different point of view for the evaluation of
code LLMs.
III. RE VAL FRAMEWORK
In this section, we first introduce the overview of our evalu-
ation framework REval, and then describe the two evaluation
components in detail, namely, Runtime Behavior Reasoning
and Incremental Consistency Evaluation . In the end, we
describe how to construct the corresponding benchmark under
our framework.
A. Overview of Framework
Fig. 2 shows an overview of our framework, which aims
to challenge code LLMs to reason how the program behaves
during execution. To achieve this, we adopt two different
perspectives for the abilities of code reasoning: (1) Runtime
Behavior Reasoning ; and (2) Incremental Consistency Evalu-
ation .
For Runtime Behavior Reasoning, we focus on whether
the code model can correctly predict the intermediate states
of program execution given an executable program and input
(as well as other contexts in the base benchmark). We select
four different dimensions of runtime behavior, including code
coverage ,execution path ,program state , and output , each
of which corresponds to a specific sub-task under Runtime
Behavior Reasoning. We present the task description and the
evaluation metric in Section III-B for each sub-task.
Besides the standalone metrics that measure a single ca-
pability of the models, we propose a novel idea named In-
cremental Consistency Evaluation to assess the consistency
across a series of incremental tasks during code reasoning. Asthe knowledge required to finish the latter task contains that of
the former task in Runtime Behavior Reasoning, the difficulty
increases progressively in order, and we can utilize this char-
acteristic to evaluate Incremental Consistency of LLMs with
existing predictions (see Section III-C for details).
In addition, as our evaluation relies on existing base bench-
marks, we present how to construct an adapted benchmark of
code reasoning within our framework in Section III-D.
B.Runtime Behavior Reasoning
Runtime behavior refers to the intermediate state and in-
formation during program execution, such as code coverage
and variable values, which are widely mentioned in previ-
ous research [16], [42]. As shown in Fig. 2, to evaluate
the reasoning ability in program runtime behavior for code
models, we analyze and select four representative dimensions
of intermediate information during execution, including code
coverage ,execution path ,program state , and output predic-
tion. Corresponding to these features, we propose four sub-
tasks for Runtime Behavior Reasoning and introduce them in
detail.
1)Code Coverage Prediction (CCP): Code coverage mea-
sures the proportion of code covered by a test suite [43].
Recent research [42] utilized this idea to challenge the model
in predicting whether the statements in the program can be
executed or not. Hence, we exploit the LLM code to judge
whether a specific statement is executed given the input of a
test case.
Task Description. Given a program Pwith statements
(S1, S2,···, Sn), an input Xfor execution and a statement
index I, the model Mis required to predict whether the I-th
statement SIis executed. The ground truth can be denoted as
Coverage (I).
Evaluation Metrics. In this task, Accuracy presents the
percentage of correct coverage predictions. For our benchmark
that consists of Nnumber of (P,X, I)pairs, Accuracy can
be computed as:
Accuracy =1
NXConditional Expression. Value is 0 or 1.
JM(P,X, I) =Coverage (I)K
Besides, as this task can be considered as a binary classi-
fication task, we also use F1 score as the evaluation metric
following previous research [24].
2)Program State Prediction (PSP): The initial idea of
program state refers to values of the program counter and
the variables [44] in the context of assembly language and
instructions. Since we are mainly concerned with code models
at the source code level, we follow related work [16], [45]
and define program state as a set of variables in the current
runtime scope. Each variable has its corresponding value and
type. Program State Prediction examines the model’s ability
to reason about value and type conversion of the variable after
a statement is executed.
Task Description. Given a program Pwith statements
(S1, S2,···, Sn), an input X, a statement index Iand avariable name Vrelated to the current statement Si, the model
Mis required to predict the type and value of variable V
afterSIis executed. The ground truth of type and value can
be denoted as Ty (I, V )and Val (I, V ), respectively.
Evaluation Metrics. In this task, Accuracy (Acc.) measures
the percentage of correct value and type predictions:
Acc. =1
NX
JM(P,X, I, V ) = ( Val(I, V ),Ty(I, V ))K
With this equation, a model’s prediction is correct only if
the value and type both match the ground truth.
3)Execution Path Prediction (EPP): In this task, we refer
to the execution path of the program as ordered sequences of
statements. As the granularity of our context is statement-level,
we challenge the code model to predict the next statement to
be executed given a specific statement. A code LLM skilled in
code reasoning should be capable of telling where the control
flow of the program is going and, consequently, can predict
the next executed statement naturally.
Task Description. Given a program Pwith statements
(S1, S2,···, Sn), an input Xand a statement index I, the
model Mis required to predict the next statement to be
executed after SIis executed. The ground truth can be denoted
as Next (I).
Evaluation Metrics. In this task, Accuracy measures the
percentage of correct next statement predictions:
Accuracy =1
NX
JM(P,X, I) =Next (I)K
Note that if the number of possible answers is more than
one (i.e., several statements could be the next one to execute),
we consider the prediction correct if it hits any possible one.
4)Output Prediction (OP): This task is to directly generate
the output of a program with the given input, which is applied
in previous code reasoning work [14], [15]. To accurately
predict the output of a program, code LLMs should be capable
of controlling and simulating the whole execution process,
which places high demands on the code reasoning ability. To
evaluate the correctness of the output, we utilize test cases (i.e.,
a collection of assertion statements) in the base benchmarks.
This approach is also applied in various code benchmarks [8],
[22].
Task Description. Given a program Pand an input Xfor
execution, the model Mis required to generate the output.
The correct output can be denoted as Y.
Evaluation Metrics. In this task, Accuracy measures the
percentage of correct output predictions:
Accuracy =1
N′X
JM(P,X) =YK,
where N′equals the number of different (P,X)pairs in our
benchmark.C.Incremental Consistency Evaluation
Incremental Consistency refers to the idea of how much
the model can maintain its consistency across a series of
sequentially related tasks. Intuitively, if an LLM cannot reason
about the current task, it is not expected to finish the next task
whose preliminary depends on the current task. To clarify
this idea, we first present the description of Incremental
Consistency, and then explain how we evaluate it on code
models in practice.
1) Description of Incremental Consistency: The core idea
of Incremental Consistency is to assess code models by
leveraging the relationship where the knowledge from one task
in a series of tasks depends on the next task. In the context of
our research, four distinct sub-tasks (i.e., CCP, PSP, EPP, and
OP) in Runtime Behavior Reasoning are selected, and we can
observe some patterns from them:
i) CPP ⇐PSP: Since the execution of a statement could
lead to changes in the program state, the prerequisite for
PSP is correctly predicting if the statement is executed
(i.e., CCP).
ii) PSP ⇐EPP: The control flow of a running program is
affected by the value of some variables (e.g., “if” branch
and its conditional variable), thus the next statement to
be executed (i.e., EPP) is influenced by the program state
(i.e., PSP).
iii) EPP ⇐OP: The intermediate execution state is one
of the factors that affect the program output. Thus, the
knowledge for OP covers that of PSP.
According to the above descriptions, we find that the
knowledge required to finish the previous task is contained
by that of the following task. Intuitively, the subsequent tasks
are more difficult than the current task. Hence, if a model fails
to correctly complete a task (e.g., fails to finish CPP) but then
predicts the following tasks correctly (e.g., correct prediction
of output), we consider this model behaves inconsistently in
consecutive tasks.
2) Evaluation Approach: We analyze the results of our
Runtime Behavior Reasoning to evaluate the Incremental
Consistency. Specifically, for the i-th specific problem in our
benchmark (i.e., full program, a specific statement in it, input,
and one question to ask), we assume that the sequential results
of four tasks are:
Ri={rCPP, rPSP, rEPP, rOP},
where r∈ { 0,1}and the number indicates whether the
prediction matches the ground truth (i.e., 1) or not (i.e., 0).
Hence, if the model is incrementally consistent and completes
one task only when all its previous tasks are finished, the
binary sequence Rishould be non-declining, i.e.,
Ri∈S,whereS={{1,1,1,1},{1,1,1,0},{1,1,0,0},{1,0,0,0}}.
For the first example of Incremental Consistency Evaluation
in Fig. 2, the resulting sequence is {1,1,0,0}(/,/,/reve,/reve),
which means that Incremental Consistency is observed in thiscase. However, for the second example, the result is {0,1,1,0}
(/reve,/,/,/reve), so Incremental consistency is not observed.
In addition, depending on how many consecutive times
consistency is maintained (i.e. {1,1,1,1}means 4 times),
we assign different weights to reward models that maintain
Incremental Consistency more often. It is intuitive because
it is harder to behave incrementally consistently across more
sub-tasks.
Finally, we define Incremental Consistency Score (IC
Score) to quantitatively model the Incremental Consistency of
an LLM M. For our benchmark that contains Nnumber of
(P,X, I, V )pairs, IC Score can be computed as:
IC Score =100
NNX
i=1IC Score i,
IC Score i=

1
2j−1,ifRi=Sj, j∈ {1,2,3,4}
0, otherwise
The above formula indicates that weighted scores are given
based on the number of times the model maintains Incremental
Consistency. The higher the IC score, the higher Incremen-
tal Consistency of the model’s behavior. Specifically, for a
model’s results of a problem:
i) If the answers are completely correct, it gains a full score.
ii) If the answers are partially correct and Incremental Con-
sistency is observed, the model gains a partial score.
iii) For other cases (e.g., partially correct, but Incremental
Consistency is not observed), the model gets a zero score.
D.Benchmark Construction
As Fig. 2 illustrates, our framework utilizes existing exe-
cutable benchmarks to evaluate the code reasoning ability of
LLMs. We introduce how to adapt these base benchmarks into
our framework in two steps: (i) Runtime Behavior Extraction;
(ii) Problem Construction and Filtering:
1) Runtime Behavior Extraction: Our evaluation frame-
work requires code models to predict intermediate information
during code execution, thus, we need to extract the runtime
information as the ground truth of the problem. We use the
provided test case to execute the corresponding canonical
solution to ensure the correctness of program and input.
During the execution, we implement the customized program
tracer to record (i) the statement being executed with its
number of lines and (ii) the current program state (i.e., local
variables) for each execution step. Thus, when the execution
of code terminates, we can acquire an ordered sequence of the
runtime behavior we need for the evaluation.
2) Problem Construction and Filtering: We construct our
problem for each task with the extracted information. As there
could be a large number of combinations of different runtime
behavior and input (e.g., lots of variables in the program state
of a specific time step), we design several filtering rules to
select reasonable and representative ones:CCP and EPP. In these two tasks, we focus on whether and
when a statement is being executed. As the actual execution
sequence of statements could be very long for loops, we
analyze the control flow graph and break the program into
several blocks. We prioritize the last statement in a block as
it leads to various new blocks and is generally more difficult
to reason about.
PSP. This task challenges an LLM to predict the type and value
of a variable. For REval, we inspect the code and focus on
the following types of statements:
i)Assignment. We extract the variable(s) at the left-hand
side for assignment statements. The possible types of
variables are identifier (i.e., ordinary variables like “ x”),
subscript (i.e., array slices like “ x[0] ”), and attribute (i.e.,
fields like “ x.y”). In most cases, we extract identifiers.
Note that some naive assignments such as a = 0 orl = []
are skipped, but we keep statements like a += 1 for the
change of variable value.
ii)Return statement. In return statements, we extract the
variables in the returned object if local variables are
returned. If the returned object is a constant value, we
will select the “nearest” variable, i.e., the last variable
that is not constant.
iii)Others. For other situations, if any variable after the
current line is changed, we extract a changed variable
based on the priority of “new variable >changed vari-
ables >changed attributes (i.e., self.xxx )”. Not all
variables are used because we prefer variables that have
closer logical relationship with other tasks (e.g., EPP).
We ignore objects of non-serializable classes or complex
structures (e.g., “ self ” objects), as it is challenging
to convert them to canonical string representations and
compare ground truth with an LLM’s output.
OP.In output prediction task, we follow CRUXEval [14] and
utilize the assertion statements in the test cases of the base
benchmarks. For base benchmarks such as ClassEval [22],
where one test case contains multiple assertions, we use all the
assertions. Specifically, we replace the right operands in the
assertions with question marks (“ ??”), and challenge models
to predict the masked values.
After the above screening, we combine the results to obtain
the final adapted benchmark, ensuring that the dataset used for
each task is consistent.
IV. E XPERIMENTAL SETUP
A. Base Benchmarks
In our experiments, we first need to obtain the runtime
behavior of code such as program state, thus base benchmarks
should be executable and equipped with test cases. Moreover,
we would like to experiment with diverse types of data
(e.g., different programming scenarios). Therefore, we utilize
existing code generation benchmarks as the basis for code
reasoning evaluation. Typically, code generation benchmarks
can be categorized into two types: competition-level ones (i.e.,
with standalone functions) [9], [46] and context-aware onesTABLE I
STATISTICS OF OUR DATASET .
Description Number
# of Problems 3152
# of Avg. Tokens in Programs 408.3
# of Avg. Tokens in Selected Statements 14.0
TABLE II
FEATURES OF STUDIED LLM S. “FD”: F OUNDATION (CODE )MODELS .
“IF”: S UPPORTING INSTRUCTION FOLLOWING . “OS”: O PEN-SOURCE
MODELS .
Category Series Model Name Size FD IF OS Time
Code
LLMsCodeLlamaCodeLlama-7B-Base 7B✓ ✗ ✓ 08/2023
CodeLlama-7B-Python 7B✓ ✗ ✓ 08/2023
CodeLlama-7B-Instruct 7B✓ ✓ ✓ 08/2023
CodeLlama-13B-Instruct 13B✓ ✓ ✓ 08/2023
CodeLlama-34B-Instruct 34B✓ ✓ ✓ 08/2023
MagicoderMagicoder-CL-7B 7B✗ ✓ ✓ 12/2023
Magicoder-S-CL-7B 7B✗ ✓ ✓ 12/2023
StarCoder2StarCoder2-3B 3B✓ ✓ ✓ 02/2024
StarCoder2-7B 7B✓ ✗ ✓ 02/2024
StarCoder2-15B 15B✓ ✗ ✓ 02/2024
General
LLMsGPTGPT-3.5-Turbo - - ✓ ✗ 01/2024
GPT-4-Turbo - - ✓ ✗ 01/2024
Mistral Mistral-7B-Instruct 7B✗ ✓ ✓ 01/2024
GemmaGemma-7B-It 7B✓ ✓ ✓ 02/2024
Gemma-2B-It 2B✓ ✓ ✓ 02/2024
(i.e., with more code context like class and file dependencies)
[22], [47]. Considering the diversity of base benchmarks, we
choose HumanEval as a representative of competition-level
benchmarks and ClassEval as a representative of context-aware
benchmarks.
•HumanEval [8] is a popular competition-level bench-
mark for code generation. It consists of 164 hand-written
Python programming problems and needs models to solve
the problem given the function signature and docstring.
•ClassEval [22] is a class-level hand-written code gener-
ation benchmark. ClassEval provides different program-
ming scenarios (e.g., incremental generation) and topics
(e.g., management systems and database operations).
These benchmarks are widely used in previous re-
search [15], [48], [49]. We show the statistics of the adapted
benchmarks in Table I. In addition to our selection, we
highlight that our framework is applicable for other similar
code generation benchmarks.
B. Studied Code LLMs
To study the reasoning capabilities of diverse code LLMs,
we curate a selection of models with a variety of distinctions.
Specifically, we mainly consider these dimensions of them:
(1) general or code specified (e.g., GPT-4-Turbo [50] v.s.
CodeLlama [6]); (2) scale of parameters; (3) open-source
or closed-source (e.g., GPT-3.5-Turbo [50] v.s. Mistral-7B-
Instruct [51]) (4) foundation or further fine-tuned (e.g., CodeL-[PYTHON]
......
[/ANSWER]
[PYTHON]
def f(x):
......
f(3)
[/PYTHON]
[THOUGHT]
1. The function f is defined, which takes ...
2. ...
[/THOUGHT]ProgramYou are given a Python function ... System Message
[ANSWER]
NO
[/ANSWER][QUESTION]
Is Line 3 (        return x*2) executed when f(3) is called?
[/QUESTION]
Thoughts ( CoT only)Question
AnswerFew -shot DemonstrationsFig. 3. The prompt template for our empirical study. Note that the “Thoughts”
part is used only we leverage Chain-of-Thought (CoT) [55] prompting.
lama v.s. Magicoder-CL [7]); (5) instruct following or not
(e.g., CodeLlama-7B-Base v.s. its instruct version); (6) open-
source or not; and (7) release time.
As a result, considering these dimensions we select several
state-of-the-art code LLMs that have been applied to various
code related tasks [29], [52]–[54]. Table II presents detailed
features of them and we can see that full consideration of the
diversity of models across various features is taken to enhance
the generalization of our study.
C. Prompt Design
In our work, we utilize prompting to evaluate code LLMs
with our code reasoning tasks. We refer to a recent study [14]
on code reasoning and design our prompt templates as illus-
trated in Fig. 3. For classic few-shot prompting, our prompt
template consists mainly of five parts, including the system
message, few-shot demonstrations, program, question, and
answer. If Chain-of-Thought (CoT) [55] prompting is utilized,
the thoughts are added into the prompt template as well as the
examples in it. Both few-shot prompting and CoT prompting
are widely applied in various tasks [55]–[57], including rea-
soning tasks [14].
D. Implementation Details
Access of Models and Base Benchmarks. For open-source
models such as CodeLlama, we use the corresponding official
releases available on HuggingFace [58]. For closed-source
models (e.g., GPT-3.5-Turbo and GPT-4-Turbo), we invoke
the OpenAI API [59] to access them. Two benchmarks (i.e.,
HumanEval and ClassEval) are also publicly available on
HuggingFace [60], [61]. To help replicate our research, we
list detailed information, such as model IDs and URLs in our
replication package [62].
Environment. We run experiments on a Linux server with
8 NVIDIA A800 GPUs. For open-source LLMs, we deploy
a local API server based on vLLM [63] which is a unified
library for LLM serving and inference. All models are not
quantized and we use their original precisions.Configurations. Temperature can control the randomness in
the generated results of models [8]. Specifically, we follow Gu
et al. [14] and set the temperature to 0.8. For tasks with direct
prompting, we set the maximum length of generated tokens to
256, while for tasks with CoT prompting, we set it to 1024.
For the rest of the parameters, we use the default settings
in vLLM, to ensure a fair comparison. To obtain reliable
results, experiments for all open-source models with few-shot
prompting are repeated five times, and we report the mean
and standard deviation values in Section V. We do not repeat
experiments for closed-source models like the GPT series due
to a limited budget.
V. R ESULTS
In this section, we discuss the results of our empirical study
onREval by answering two research questions:
•RQ1 : How do LLMs perform on Runtime Behavior
Reasoning?
•RQ2 : How do LLMs perform on Incremental Consistency
evaluation?
A. RQ1: Performance of Runtime Behavior Reasoning
Table III shows the detailed results of Runtime Behavior
Reasoning. All models are evaluated with few-shot prompting
except for special annotated ones (i.e., CodeLlama-7b-Instruct
(CoT)). Below, we discuss the results from different aspects.
Overall Performance. Overall, we find that the performance
of different LLMs presents a large variation, and GPT-4-
Turbo shows superior performance in reasoning about program
execution. For example, GPT-4-Turbo achieves the best results
in all metrics of Runtime Behavior Reasoning, and its average
accuracy outperforms the second best (i.e., 55.7% of GPT-3.5)
by a large margin (i.e., an absolute improvement of 19.3%).
However, the overall performance of open-source models is
not high and the best performer among them (i.e., CodeLlama-
34B-Instruct) only achieves a level close to that of GPT-3.5
(i.e., 51.0% v.s. 55.7%) in terms of average accuracy.
Task. Runtime Behavior Reasoning consists of four distinct
evaluation tasks, i.e., CCP, PSP, EPP, and OP, and the perfor-
mance varies among different tasks. For example, all models
achieve an accuracy of more than 50% in OP (i.e., Output
Prediction), while only about half of them (i.e., 8 out of 15)
can provide the correct answers for more than 10% problems
in EPP (i.e., Execution Path Prediction). Hence, according
to the average score of tasks, the performance distribution
may suggest that EPP is the most challenging task and OP
is relatively easy among them.
Size and Category. In general, we observe that for mod-
els within the same family, the variant with larger size of
parameters shows better performance in Runtime Behavior
Reasoning. In the case of the CodeLlama-instruct series, as the
number of parameters increases (i.e., 7B →34B), the accuracy
of EPP has a relative improve by over 100% (i.e., 14.4%
→29.2%). Meanwhile, smaller models like StarCoder2-3BTABLE III
RESULTS FOR RUNTIME BEHAVIOR REASONING AND INCREMENTAL CONSISTENCY EVALUATION (RQ1 & 2). “CCP”, “PSP”, “EPP”, AND “OP”: FOUR
TASKS OF RUNTIME BEHAVIOR REASONING ; “A VG”:THE AVERAGE ACCURACY SCORE OF FOUR TASKS .
WE REPORT THE RESULTS IN THE FORM OF “MEAN ±STANDARD DEVIATION ”EXCEPT FOR TWO GPT MODELS BECAUSE OF BUDGET LIMIT .
ModelCCP PSP EPP OP Acc. Avg. IC
Acc. (%) F1 Acc. (%) Acc. (%) Acc. (%) (%) Score
CodeLlama-7B-Base 54.3±0.5 56.1±0.5 25.0±0.5 5.6±0.3 58.2±1.6 35.8 4.0±0.2
CodeLlama-7B-Python 55.5±0.7 62.7±0.6 31.3±1.0 8.7±0.7 62.3±1.0 39.4 4.8±0.2
CodeLlama-7B-Instruct 55.6±0.9 47.2±1.2 25.1±0.5 10.8±0.2 62.6±0.8 38.5 4.1±0.1
CodeLlama-13B-Instruct 61.0±0.7 66.4±0.6 32.5±0.4 14.4±0.4 64.5±1.1 43.1 6.6±0.3
CodeLlama-34B-Instruct 61.5±0.5 70.1±0.4 47.5±0.6 29.2±0.4 65.9±1.1 51.0 11.8±0.3
StarCoder2-3B 54.8±0.7 58.2±0.5 29.0±0.7 6.5±0.5 58.8±0.8 37.3 4.3±0.3
StarCoder2-7B 55.1±0.7 63.8±0.6 34.2±0.7 5.0±0.4 63.9±0.8 39.6 4.2±0.3
StarCoder2-15B 58.9±0.8 64.6±0.8 43.5±0.3 28.0±0.5 71.5±1.1 50.5 10.7±0.4
Magicoder-CL 58.7±1.4 61.2±1.8 30.1±0.5 15.5±1.1 60.4±1.4 41.2 6.2±0.3
Magicoder-S-CL 60.3±1.1 69.9±0.8 31.4±0.4 9.8±0.4 62.3±1.2 40.9 6.0±0.2
Gemma-2B-It 52.7±0.4 31.0±0.6 13.5±0.5 7.3±0.5 43.9±1.5 29.3 5.5±0.2
Gemma-7B-It 66.3±0.3 75.2±0.1 32.1±0.1 8.4±0.4 57.9±0.7 41.2 6.9±0.2
Mistral-7B-Instruct 69.5±0.2 75.9±0.2 35.2±0.3 35.8±0.4 51.5±0.7 48.0 16.3±0.3
GPT-3.5-Turbo 61.8 64.0 51.6 48.6 60.7 55.7 20.6
GPT-4-Turbo 88.4 89.8 71.4 57.7 82.6 75.0 42.5
Average 61.0 63.7 35.6 19.4 61.8 44.4 10.3
CodeLlama-7B-Instruct (CoT) 57.5 59.2 33.4 21.4 55.8 42.2 7.5
can also outperform larger models such as CodeLlama-7B-
Instruct in terms of average accuracy (i.e., 37.3% v.s. 35.8%).
The StarCoder2 series utilizes varied architectures and training
datasets compared to CodeLlama. This may demonstrate that
apart from parameter size, the model architecture and training
strategy also play an important role in code reasoning ability.
We also find that code LLMs do not exhibit an obviously
leading advantage over general LLMs of the same size.
Training Strategy. As shown in Table III, we conduct experi-
ments on three variants of CodeLlama (i.e., base, instruct, and
Python) of the same 7B size. Compared to the base model, the
“instruct” variant that leverages instruction tuning techniques
brings gains in the code reasoning ability (i.e., Avg. Acc.
from 35.8% to 38.5%), which may reflect the relationship
between understanding instructions and reasoning program.
Meanwhile, since our base benchmarks are all in Python, addi-
tional training with Python corpora (i.e., CodeLlama-Python)
leads to an improvement of performance (i.e., an absolute
improvement of 0.9%). In addition, we note that although
further fine-tuning applied to the Magicoder series improves
their performance in code generation [7], the improvement in
code reasoning ability is relatively limited compared to their
foundation model CodeLlama-7B-Python. This may indicate
that the training strategies they utilize are not well suited for
the reasoning tasks in our evaluation.
Prompting Strategy. The last row of Table III presents the
performance of CodeLlama-7B-Instruct with CoT prompting.
Compared to the model with few-shot prompting, the perfor-mance of CPP, PSP, and EPP receives varying degrees of
improvement. For instance, the EPP accuracy with CoT is
improved from 10.8% to 21.4%, surpassing the performance
of the larger 13B model (i.e., 14.4%). It may demonstrate the
effectiveness of presenting how to reason about a piece of code
step by step. However, CoT prompting fails to improve its OP
performance, with an absolute decrease of 6.8%. This may
result from the wrong thought chain that the model generates
for the whole program and eventually leads to the mistake.
Summary for RQ1: Models with different features (e.g.,
size and training strategy) exhibit notable disparities in
performance on Runtime Behavior Reasoning. Overall,
GPT-4-Turbo demonstrates a clear advantage over other
models in all four tasks in our setting.
B. RQ2: Incremental Consistency Evaluation
Fig. 4 shows the sorted average accuracy for Runtime
Behavior Reason for different models, with an additional line
indicating the IC scores. A detailed information of mean
IC scores and their standard deviations are also reported in
Table III.
Overall Performance. We find that the majority of LLMs
exhibit a low level of Incremental Consistency with scores be-
low 20, which highlights the inconsistency in model behavior
across the four tasks of Runtime Behavior Reasoning. Among
all of the models, GPT-4-Turbo stands out with the highest IC
score of 42.5, even more than double that of the second place42.5
10.3 6.0
40.944.4Fig. 4. Average Accuracy of Runtime Behavior Reasoning and Incremental
Consistency Score for different models, sorted in descending order according
to average accuracy.
GPT-3.5 (i.e., 20.6). Given that GPT-4-Turbo achieves the best
results in both Runtime Behavior Reasoning and Incremental
Consistency Evaluation (i.e., 75.0% of Avg. Acc. and 42.5
of IC score), we believe that it has both superior ability in
program reasoning and a high level of Incremental Consistency
across sequentially related tasks in our evaluation.
Trend Between IC and Runtime Behavior Reasoning. As
illustrated in Fig. 4, we find that there is an approximately
similar trend between the model’s average accuracy and its IC
score. For example, compared to CodeLlama-7B-Instruct, its
larger version CodeLlama-34B-Instruct has a noticeable higher
average accuracy (i.e., 38.5% v.s. 51.0%) and IC score (i.e., 4.1
v.s. 11.8). However, this pattern does not hold for all models.
The performance of the general LLM Mistral-7B is not as
good as that of CodeLlama-34B in terms of average accuracy
(i.e., 48.0% v.s. 51.0% in Runtime Beheavior Reasoning), but
performs better on Incremental Consistency (i.e., 16.3 v.s.
11.8).
Others. Similar to the results of RQ1, we observe that code
LLMs do not significantly outperform general LLMs in the
IC evaluation. For example, the IC score of Gemma-2B-It
(i.e., 5.5) is higher than code LLMs trained with more code
corpora like CodeLlama-7b-Instruct and StarCoder2-7B (i.e.,
4.1 and 4.2). This phenomenon may suggest that more code
data cannot help LLMs reason programs better and maintain
their Incremental Consistency. In addition, CoT prompting for
CodeLlama-7B-Instruct leads to a great increase in its IC Score
(i.e., 4.1 →7.5), and this improvement of IC Score may benefit
from explicit problem solving steps.
Summary for RQ2: In code reasoning tasks, most LLMs
behave inconsistently and their average accuracy is not
entirely associated with IC. GPT-4-Turbo achieves an IC
Score of as high as 42.5, surpassing other models by a
large margin (i.e., more than 21.9 absolute improvements).
VI. D ISCUSSION
A. Case Study
Fig. 5 shows a case from the problem of EPP. Given a
Python function that aims to return the largest prime factor of
deflargest_prime_factor (n: int):
"""Return the largest prime factor of n. 
Assume n > 1 and is not a prime.
>>> largest_prime_factor (13195)
29
>>> largest_prime_factor (2048)
2
"""
defis_prime (k):
ifk<2:
returnFalse
foriinrange(2, k-1):
ifk%i==0:
returnFalse
returnTrue
largest =1
forjinrange(2, n+1):
ifn%j==0andis_prime (j):
largest =max(largest, j)
returnlargest
Input: 15
Question: 
What is the next line to be executed after 
line “largest =max(largest, j)”?CodeLlama -
34B-InstructGPT-3.5-
TurboGPT-4-
Turbo
×√Fig. 5. A tricky problem of EPP from HumanEval/59 [8]. The prediction
of GPT-4-Turbo is correct and the other two models (i.e., GPT-3.5-Turbo
and CodeLlama-34B-Instruct) fail to finish it. The problem description is
simplified for a concise presentation.
parameter nand input 15, this problem requires the model to
predict the next statement to be executed after an assignment
statement (i.e., largest = max ... ). Here we select three
models that show competitive performance in EPP including
GPT-4-Turbo, GPT-3.5-Turbo, and CodeLlama-34B-Instruct.
For GPT-3.5-Turbo and CodeLlama-34B-Instruct, the predic-
tion is the last “return” statement of this function; while GPT-
4-Turbo chooses the above “for” loop as its prediction. We
mark a happy emoji to show that GPT-4-Turbo makes the right
choice, and the other two models fail to predict it correctly.
The explanation is that if the assignment statement is executed,
the possible value of jcan only be 3 or 5 ( n+ 1 = 16 ), which
means that the loop will continue and the “return” statement
is not the next executed line.
In our framework and the adapted benchmark, there are
many problems like this that have no obvious answer and are
challenging. If the model is not capable of reasoning about
its inherent logic of execution, it can easily be misled and
give the most “look-alike” answer (i.e., the “return” statement
in this case), indicating that our framework can effectively
measure the code reasoning capability of LLMs and present
the discrimination of them. We discuss more cases in the
appendix, which can be accessed in our replication package1.
B. Unsatisfactory Performance of Code Reasoning
According to evaluation results, we observe that many
models perform poorly in Runtime Behavior Reasoning and
Incremental Consistency Evaluation. In particular, even the
best performer GPT-4-Turbo only achieves an IC score of
42.5, reflecting the limitation of current models in maintaining
consistency in sequential-related tasks, and there is still a long
way to go to make the LLMs perform code reasoning.
1https://r-eval.github.ioTABLE IV
PEARSON CORRELATION COEFFICIENT MATRIX OF THE RESULTS OF
RUNTIME BEHAVIOR REASONING (RBR), I NCREMENTAL CONSISTENCY
(IC), AND HUMAN EVAL (HE)
Pearson Correlation RBR IC HE
RBR 1.000 0.940 0.772
IC 0.940 1.000 0.724
HE 0.772 0.724 1.000
One potential reason is that the current LLM might not
understand the program execution behavior. While it is con-
venient to obtain source code from open-source platforms
(e.g., GitHub), there is relatively less data available regarding
code execution behavior, because running the program and
collecting its runtime information require the corresponding
development environments and test suite. Therefore, if the
model is not familiar with the knowledge related to runtime
behavior, it may not perform well in code reasoning tasks.
C. Correlation between Code Reasoning & Code Generation
We utilize the experimental results and study the correlation
between code reasoning and code generation, i.e., whether
an LLM that performs well in code generation could exhibit
equally strong abilities in code reasoning.
Table IV presents the Pearson correlation coefficient matrix
of Runtime Behavior Reasoning (Avg. Acc.), Incremental
Consistency Score and HumanEval ( pass@ 1rate). According
to the matrix, we find that there is a strong positive correlation
(i.e, the Pearson correlation coefficients are larger than 0.7)
among code generation (i.e., HumanEval) and code reasoning
(i.e., Runtime Behavior Reasoning and Incremental Consis-
tency). However, the correlation between code reasoning and
code generation is relatively lower than that between two rea-
soning tasks internally (i.e., 0.724 and 0.772 v.s. 0.940), which
indicates that models with similar code generation abilities
may vary a lot in code reasoning. As the correlation may
help researchers increase the understanding of code LLMs and
improve the models’ code generation and reasoning abilities,
future research could investigate such correlation in depth.
D. Threats to Validity
Internal Threats. To construct the adapted benchmark for
different sub-tasks of Runtime Behavior Reasoning, we man-
ually establish some rules to select appropriate statements and
variables for the evaluation. However, our selection criteria
may not effectively represent the runtime state of the program.
To mitigate this threat, we take some measures to determine
problem settings that are representative and challenging, based
on the characteristics of different tasks. For instance, we
choose the last statement in the control flow (i.e., for EPP)
and variables that are modified after the execution (i.e., for
PSP). These measures help us to reasonably assess the model’s
capability to reason about code and provide meaningful dif-
ferentiation. For Runtime Behavior Reasoning, we select four
dimensions of the intermediate state of program executionwhich are widely applied in previous research [16], [42].
These four tasks are proven to effectively evaluate the code
reasoning capability of code LLMs [15], [16], [42], and are
appropriate for Incremental Consistency Evaluation for their
unique sequential relationship. However, there are still some
dynamic features such as memory allocation and exception
handling which may help measure code models, and we have
not explored yet. Further research could consider exploring the
potential for LLMs to reason about other dynamic program
features and extend REval to more scenarios.
External Threats. In the empirical study for our evalu-
ation framework, the results are restricted to the specific
collection of code models and base benchmarks. To mitigate
this threat, we choose representative code LLMs considering
several standards including their scale, popularity, and training
strategy; For the base benchmarks applied, two benchmarks
(i.e., HumanEval [8] and ClassEval [22]) are distinct from
evaluation fashion and programming scenarios, as described in
Section III-D. With the above efforts, the experimental results
are expected to be sustained in more circumstances.
VII. C ONCLUSION AND FUTURE WORK
In this paper, we propose REval, a comprehensive frame-
work for evaluating the code reasoning capability of code
LLMs. Our framework consists of two evaluation compo-
nents including Runtime Behavior Reasoning and Incremental
Consistency Evaluation: We conduct a large-scale empirical
study on several popular LLMs and two widely used base
benchmarks. Our empirical results show that the majority of
LLMs we evaluate show unsatisfactory performance in both
Runtime Behavior Reasoning and Incremental Consistency
Evaluation. To improve the code reasoning capabilities of
LLMs, future works can explore:
Training with Execution Behavior. One reason why large
models may struggle with code reasoning is possibly due to
a lack of knowledge related to program execution. Although
some general fine-tuning approaches are applied to code
LLMs [7], they fail to improve the code reasoning capabilities.
Given the demonstrated effectiveness of training models with
execution behavior in improving performance in a range of
downstream tasks [16], [24], it is reasonable to expect that
LLMs would also derive benefits from such a process.
Improving Prompting Strategy. Our evaluation results
demonstrate the effectiveness of CoT prompting in code
reasoning tasks. Apart from CoT, other prompting techniques
that have been proven effective in NL reasoning tasks (such
as Tree-of-Thoughts [64]) may also be applicable to code
reasoning tasks. Besides, the prompting approach tailored for
reasoning about program execution also warrants investigation.
ACKNOWLEDGMENTS
This research is supported by the Ningbo Natural Science
Foundation (No. 2023J292). It is also supported by the ad-
vanced computing resources provided by the Supercomputing
Center of Hangzhou City University.REFERENCES
[1] B. Min, H. Ross, E. Sulem, A. P. B. Veyseh, T. H. Nguyen, O. Sainz,
E. Agirre, I. Heintz, and D. Roth, “Recent advances in natural language
processing via large pre-trained language models: A survey,” ACM
Computing Surveys , vol. 56, no. 2, pp. 1–40, 2023.
[2] F. Barbieri, L. E. Anke, and J. Camacho-Collados, “Xlm-t: Multilingual
language models in twitter for sentiment analysis and beyond,” in
Proceedings of the Thirteenth Language Resources and Evaluation
Conference , 2022, pp. 258–266.
[3] A. Creswell, M. Shanahan, and I. Higgins, “Selection-inference: Exploit-
ing large language models for interpretable logical reasoning,” in The
Eleventh International Conference on Learning Representations , 2022.
[4] A. Rogers, M. Gardner, and I. Augenstein, “Qa dataset explosion:
A taxonomy of nlp resources for question answering and reading
comprehension,” ACM Computing Surveys , vol. 55, no. 10, pp. 1–45,
2023.
[5] A. Fan, B. Gokkaya, M. Harman, M. Lyubarskiy, S. Sengupta, S. Yoo,
and J. M. Zhang, “Large language models for software engineering:
Survey and open problems,” arXiv preprint arXiv:2310.03533 , 2023.
[6] B. Roziere, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y . Adi,
J. Liu, T. Remez, J. Rapin et al. , “Code llama: Open foundation models
for code,” arXiv preprint arXiv:2308.12950 , 2023.
[7] Y . Wei, Z. Wang, J. Liu, Y . Ding, and L. Zhang, “Magicoder: Source
code is all you need,” arXiv preprint arXiv:2312.02120 , 2023.
[8] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan,
H. Edwards, Y . Burda, N. Joseph, G. Brockman et al. , “Evaluating large
language models trained on code,” arXiv preprint arXiv:2107.03374 ,
2021.
[9] J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan,
E. Jiang, C. Cai, M. Terry, Q. Le et al. , “Program synthesis with large
language models,” arXiv preprint arXiv:2108.07732 , 2021.
[10] S. Lu, D. Guo, S. Ren, J. Huang, A. Svyatkovskiy, A. Blanco,
C. Clement, D. Drain, D. Jiang, D. Tang et al. , “Codexglue: A machine
learning benchmark dataset for code understanding and generation,”
inThirty-fifth Conference on Neural Information Processing Systems
Datasets and Benchmarks Track (Round 1) , 2021.
[11] Y . Chang, X. Wang, J. Wang, Y . Wu, L. Yang, K. Zhu, H. Chen, X. Yi,
C. Wang, Y . Wang et al. , “A survey on evaluation of large language
models,” ACM Transactions on Intelligent Systems and Technology ,
2023.
[12] A. Ni, S. Iyer, D. Radev, V . Stoyanov, W.-t. Yih, S. Wang, and X. V . Lin,
“Lever: Learning to verify language-to-code generation with execution,”
inInternational Conference on Machine Learning . PMLR, 2023, pp.
26 106–26 128.
[13] X. Chen, M. Lin, N. Sch ¨arli, and D. Zhou, “Teaching large language
models to self-debug,” arXiv preprint arXiv:2304.05128 , 2023.
[14] A. Gu, B. Rozi `ere, H. Leather, A. Solar-Lezama, G. Synnaeve, and S. I.
Wang, “Cruxeval: A benchmark for code reasoning, understanding and
execution,” arXiv preprint arXiv:2401.03065 , 2024.
[15] C. Liu, S. D. Zhang, and R. Jabbarvand, “Codemind: A framework to
challenge large language models for code reasoning,” arXiv preprint
arXiv:2402.09664 , 2024.
[16] Y . Ding, B. Steenhoek, K. Pei, G. Kaiser, W. Le, and B. Ray, “Traced:
Execution-aware pre-training for source code,” in Proceedings of the
46th IEEE/ACM International Conference on Software Engineering ,
2024, pp. 1–12.
[17] Y . Peng, C. Gao, Z. Li, B. Gao, D. Lo, Q. Zhang, and M. Lyu,
“Static inference meets deep learning: a hybrid type inference approach
for python,” in Proceedings of the 44th International Conference on
Software Engineering , 2022, pp. 2019–2030.
[18] D. Guo, S. Ren, S. Lu, Z. Feng, D. Tang, S. Liu, L. Zhou, N. Duan,
A. Svyatkovskiy, S. Fu et al. , “Graphcodebert: Pre-training code repre-
sentations with data flow,” arXiv preprint arXiv:2009.08366 , 2020.
[19] “GitHub Copilot · Your AI pair programmer,” https://github.com/
features/copilot, 2024, last accessed Mar. 2024.
[20] M. Allamanis, S. Panthaplackel, and P. Yin, “Unsupervised evaluation of
code llms with round-trip correctness,” arXiv preprint arXiv:2402.08699 ,
2024.
[21] M. J. Min, Y . Ding, L. Buratti, S. Pujar, G. Kaiser, S. Jana, and B. Ray,
“Beyond accuracy: Evaluating self-consistency of code llms,” in The
Twelfth International Conference on Learning Representations , 2023.[22] X. Du, M. Liu, K. Wang, H. Wang, J. Liu, Y . Chen, J. Feng, C. Sha,
X. Peng, and Y . Lou, “Classeval: A manually-crafted benchmark for
evaluating llms on class-level code generation,” in Proceedings of the
46th IEEE/ACM International Conference on Software Engineering ,
2024.
[23] J. Henkel, S. K. Lahiri, B. Liblit, and T. Reps, “Code vectors:
understanding programs through embedded abstracted symbolic traces,”
inProceedings of the 2018 26th ACM Joint Meeting on European
Software Engineering Conference and Symposium on the Foundations
of Software Engineering , ser. ESEC/FSE 2018. New York, NY , USA:
Association for Computing Machinery, 2018, p. 163–174. [Online].
Available: https://doi.org/10.1145/3236024.3236085
[24] C. Liu, S. Lu, W. Chen, D. Jiang, A. Svyatkovskiy, S. Fu, N. Sundaresan,
and N. Duan, “Code execution with pre-trained language models,” arXiv
preprint arXiv:2305.05383 , 2023.
[25] F. Tsimpourlas, G. Rooijackers, A. Rajan, and M. Allamanis,
“Embedding and classifying test execution traces using neural networks,”
IET Software , vol. 16, no. 3, pp. 301–316, 2022. [Online]. Available:
https://ietresearch.onlinelibrary.wiley.com/doi/abs/10.1049/sfw2.12038
[26] E. Jabbar, H. Hemmati, and R. Feldt, “Investigating execution trace
embedding for test case prioritization,” in 2023 IEEE 23rd International
Conference on Software Quality, Reliability, and Security (QRS) , 2023,
pp. 279–290.
[27] E. La Malfa, C. Weinhuber, O. Torre, F. Lin, A. Cohn, N. Shadbolt,
and M. Wooldridge, “Code simulation challenges for large language
models,” arXiv preprint arXiv:2401.09074 , 2024.
[28] M. Jang, D. S. Kwon, and T. Lukasiewicz, “Becel: Benchmark for
consistency evaluation of language models,” in Proceedings of the 29th
International Conference on Computational Linguistics , 2022, pp. 3680–
3696.
[29] J. Chen, Z. Li, X. Hu, and X. Xia, “Nlperturbator: Studying the
robustness of code llms to natural language variations,” arXiv preprint
arXiv:2406.19783 , 2024.
[30] M. Jang and T. Lukasiewicz, “Consistency analysis of chatgpt,” in
Proceedings of the 2023 Conference on Empirical Methods in Natural
Language Processing , 2023, pp. 15 970–15 985.
[31] Y . Elazar, N. Kassner, S. Ravfogel, A. Ravichander, E. Hovy, H. Sch ¨utze,
and Y . Goldberg, “Measuring and improving consistency in pretrained
language models,” Transactions of the Association for Computational
Linguistics , vol. 9, pp. 1012–1031, 2021.
[32] P. Sahu, M. Cogswell, Y . Gong, and A. Divakaran, “Unpacking
large language models with conceptual consistency,” arXiv preprint
arXiv:2209.15093 , 2022.
[33] X. Wang, J. Wei, D. Schuurmans, Q. V . Le, E. H. Chi, S. Narang,
A. Chowdhery, and D. Zhou, “Self-consistency improves chain of
thought reasoning in language models,” in The Eleventh International
Conference on Learning Representations , 2022.
[34] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y . Babaei,
N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale et al. , “Llama
2: Open foundation and fine-tuned chat models,” arXiv preprint
arXiv:2307.09288 , 2023.
[35] A. Lozhkov, R. Li, L. B. Allal, F. Cassano, J. Lamy-Poirier, N. Tazi
et al. , “Starcoder 2 and the stack v2: The next generation,” arXiv preprint
arXiv:2402.19173 , 2024.
[36] E. Nijkamp, H. Hayashi, C. Xiong, S. Savarese, and Y . Zhou, “Code-
gen2: Lessons for training llms on programming and natural languages,”
arXiv preprint arXiv:2305.02309 , 2023.
[37] H. Yu, B. Shen, D. Ran, J. Zhang, Q. Zhang, Y . Ma, G. Liang, Y . Li,
Q. Wang, and T. Xie, “Codereval: A benchmark of pragmatic code
generation with generative pre-trained models,” in Proceedings of the
46th IEEE/ACM International Conference on Software Engineering ,
2024, pp. 1–12.
[38] Y . Ding, Z. Wang, W. Ahmad, H. Ding, M. Tan, N. Jain, M. K.
Ramanathan, R. Nallapati, P. Bhatia, D. Roth et al. , “Crosscodeeval:
A diverse and multilingual benchmark for cross-file code completion,”
Advances in Neural Information Processing Systems , vol. 36, 2024.
[39] Y . Lai, C. Li, Y . Wang, T. Zhang, R. Zhong, L. Zettlemoyer, W.-
t. Yih, D. Fried, S. Wang, and T. Yu, “Ds-1000: A natural and
reliable benchmark for data science code generation,” in International
Conference on Machine Learning . PMLR, 2023, pp. 18 319–18 345.
[40] F. Cassano, L. Li, A. Sethi, N. Shinn, A. Brennan-Jones, A. Lozhkov,
C. Anderson, and A. Guha, “Can it edit? evaluating the ability of large
language models to follow code editing instructions,” arXiv preprint
arXiv:2312.12450 , 2023.[41] M. A. M. Khan, M. S. Bari, X. L. Do, W. Wang, M. R. Parvez, and
S. Joty, “xcodeeval: A large scale multilingual multitask benchmark for
code understanding, generation, translation and retrieval,” arXiv preprint
arXiv:2303.03004 , 2023.
[42] M. Tufano, S. Chandel, A. Agarwal, N. Sundaresan, and C. Clement,
“Predicting code coverage without execution,” arXiv preprint
arXiv:2307.13383 , 2023.
[43] H. Hemmati, “How effective are code coverage criteria?” in 2015 IEEE
International Conference on Software Quality, Reliability and Security .
IEEE, 2015, pp. 151–156.
[44] P. Ammann and J. Offutt, Introduction to software testing . Cambridge
University Press, 2016.
[45] B. Souza and M. Pradel, “Lexecutor: Learning-guided execution,” in
Proceedings of the 31st ACM Joint European Software Engineering
Conference and Symposium on the Foundations of Software Engineering ,
2023, pp. 1522–1534.
[46] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan,
H. Edwards, Y . Burda, N. Joseph, G. Brockman, A. Ray, R. Puri,
G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan,
S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian,
C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert, F. Chantzis,
E. Barnes, A. Herbert-V oss, W. H. Guss, A. Nichol, A. Paino, N. Tezak,
J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse,
A. N. Carr, J. Leike, J. Achiam, V . Misra, E. Morikawa, A. Radford,
M. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew,
D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba, “Evaluating
large language models trained on code,” 2021.
[47] J. Li, G. Li, Y . Zhao, Y . Li, Z. Jin, H. Zhu, H. Liu, K. Liu, L. Wang,
Z. Fang et al. , “Deveval: Evaluating code generation in practical software
projects,” arXiv preprint arXiv:2401.06401 , 2024.
[48] K. Zhang, Z. Li, J. Li, G. Li, and Z. Jin, “Self-edit: Fault-aware code
editor for code generation,” in Proceedings of the 61st Annual Meeting
of the Association for Computational Linguistics (Volume 1: Long
Papers) , A. Rogers, J. Boyd-Graber, and N. Okazaki, Eds. Toronto,
Canada: Association for Computational Linguistics, Jul. 2023, pp.
769–787. [Online]. Available: https://aclanthology.org/2023.acl-long.45
[49] X. Hou, Y . Zhao, Y . Liu, Z. Yang, K. Wang, L. Li, X. Luo, D. Lo,
J. Grundy, and H. Wang, “Large language models for software engineer-
ing: A systematic literature review,” arXiv preprint arXiv:2308.10620 ,
2023.
[50] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman,
D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat et al. , “Gpt-4
technical report,” arXiv preprint arXiv:2303.08774 , 2023.
[51] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot,
D. d. l. Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier et al. ,
“Mistral 7b,” arXiv preprint arXiv:2310.06825 , 2023.
[52] A. Z. Yang, C. Le Goues, R. Martins, and V . Hellendoorn, “Large
language models for test-free fault localization,” in Proceedings of the
46th IEEE/ACM International Conference on Software Engineering ,
2024, pp. 1–12.
[53] A. G. Shypula, A. Madaan, Y . Zeng, U. Alon, J. R. Gardner,
Y . Yang, M. Hashemi, G. Neubig, P. Ranganathan, O. Bastani, and
A. Yazdanbakhsh, “Learning performance-improving code edits,” in The
Twelfth International Conference on Learning Representations , 2024.
[Online]. Available: https://openreview.net/forum?id=ix7rLVHXyY
[54] Y . Wei, C. S. Xia, and L. Zhang, “Copiloting the copilots: Fusing
large language models with completion engines for automated program
repair,” in Proceedings of the 31st ACM Joint European Software
Engineering Conference and Symposium on the Foundations of Software
Engineering , 2023, pp. 172–184.
[55] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V . Le,
D. Zhou et al. , “Chain-of-thought prompting elicits reasoning in large
language models,” Advances in neural information processing systems ,
vol. 35, pp. 24 824–24 837, 2022.
[56] J. Chen, X. Hu, Z. Li, C. Gao, X. Xia, and D. Lo, “Code search is all you
need? improving code suggestions with code search,” in Proceedings of
the 46th IEEE/ACM International Conference on Software Engineering,
ICSE 2024, Lisbon, Portugal, April 14-20, 2024 , 2024, pp. 73:1–73:13.
[57] Y . Song, T. Wang, P. Cai, S. K. Mondal, and J. P. Sahoo, “A compre-
hensive survey of few-shot learning: Evolution, applications, challenges,
and opportunities,” ACM Computing Surveys , vol. 55, no. 13s, pp. 1–40,
2023.
[58] “Models - Hugging Face,” https://huggingface.co/models, 2024, last
accessed Mar. 2024.[59] “API Reference - OpenAI API,” https://platform.openai.com/docs/
api-reference, 2024, last accessed Mar. 2024.
[60] “openai humaneval · Datasets at Hugging Face,” https://huggingface.co/
datasets/openai humaneval, 2024, last accessed Mar. 2024.
[61] “FudanSELab/ClassEval · Datasets at Hugging Face,”
https://huggingface.co/datasets/FudanSELab/ClassEval, 2024, last
accessed Mar. 2024.
[62] “Replication package,” https://figshare.com/s/e5de95bd79ab5ddea76c,
2024, replication package.
[63] W. Kwon, Z. Li, S. Zhuang, Y . Sheng, L. Zheng, C. H. Yu, J. E.
Gonzalez, H. Zhang, and I. Stoica, “Efficient memory management for
large language model serving with pagedattention,” in Proceedings of
the ACM SIGOPS 29th Symposium on Operating Systems Principles ,
2023.
[64] S. Yao, D. Yu, J. Zhao, I. Shafran, T. Griffiths, Y . Cao, and
K. Narasimhan, “Tree of thoughts: Deliberate problem solving with large
language models,” Advances in Neural Information Processing Systems ,
vol. 36, 2024.