ROCODE: Integrating Backtracking Mechanism and
Program Analysis in Large Language Models for
Code Generation
Xue Jiang, Yihong Dong
Key Lab of High Confidence Software
Technology, MoE (Peking University)
Beijing, China
{jiangxue, dongyh }@stu.pku.edu.cnYongding Tao
University of Electronic Science and
Technology of China
Chengdu, China
yongd.tao@gmail.comHuanyu Liu
Key Lab of High Confidence Software
Technology, MoE (Peking University)
Beijing, China
huanyuliu@stu.pku.edu.cn
Zhi Jin
Key Lab of High Confidence Software
Technology, MoE (Peking University)
Beijing, China
zhijin@pku.edu.cnWenpin Jiao
Key Lab of High Confidence Software
Technology, MoE (Peking University)
Beijing, China
jwp@sei.pku.edu.cnGe Li
Key Lab of High Confidence Software
Technology, MoE (Peking University)
Beijing, China
lige@pku.edu.cn
Abstract ‚ÄîLarge language models (LLMs) have achieved im-
pressive performance in code generation recently, offering pro-
grammers revolutionary assistance in software development.
However, due to the auto-regressive nature of LLMs, they are
susceptible to error accumulation during code generation. Once
an error is produced, LLMs can merely continue to generate
the subsequent code conditioned on it, given their inability
to adjust previous outputs. Existing LLM-based approaches
typically consider post-revising after code generation, leading
to the challenging resolution of accumulated errors and the
significant wastage of resources. Ideally, LLMs should rollback
and resolve the occurred error in time during code generation,
rather than proceed on the basis of the error and wait for post-
revising after generation. In this paper, we propose R OCODE,
which integrates the backtracking mechanism and program
analysis into LLMs for code generation. Specifically, we employ
program analysis to perform incremental error detection during
the generation process. When an error is detected, the backtrack-
ing mechanism is triggered to priming rollback strategies and
constraint regeneration, thereby eliminating the error early and
ensuring continued generation on the correct basis. Experiments
on multiple code generation benchmarks show that R OCODE
can significantly reduce the errors generated by LLMs, with a
compilation pass rate of 99.1%. The test pass rate is improved by
up to 23.8% compared to the best baseline approach. Compared
to the post-revising baseline, the token cost is reduced by 19.3%.
Moreover, our approach is model-agnostic and achieves consistent
improvements across nine representative LLMs.
Index Terms ‚ÄîCode Generation, Large Language Models,
Backtracking Mechanism, Program Analysis.
I. I NTRODUCTION
As modern software architectures continue to increase in
size and complexity, the burden on developers to construct
and maintain these systems has become substantial. Given that
programs serve as the fundamental carriers of software func-
tionality, the automation of their generation is of paramountimportance. Code generation technology, which seeks to auto-
matically produce programs that align with human intentions,
has emerged as a focal area of interest within both academia
and industry fields [1]‚Äì[4]. In recent years, large language
models (LLMs) have rapidly advanced and achieved signifi-
cant success in the domain of automated code generation [5]‚Äì
[9]. A well-known tool for code generation based on LLMs is
Copilot [10], which has demonstrated its utility by generating
code that can be accepted by more than 30% of its users [11].
Fig. 1. Statistics on the types of errors in code generated by LLM. The
statistics are conducted based on the results generated by CodeLlama-7B and
CodeGen-6B on HumanEval and MBPP benchmarks using greedy decoding.
Typically, LLMs adopt an auto-regressive approach, where
the output at each step is conditioned on the outputs of
previous steps. Once an error occurs during the generation
process at any step (for example, the selection of an inap-
propriate token due to hallucinations1[14]), this error will
be included in the context of the subsequent steps. This
1The hallucination in code generation manifests as generated code that
violates programming principles, resulting in code that cannot be compiled or
executed, or that is inconsistent with user requirements or context, leading to
failed tests [12]. Recent research has demonstrated that all computable LLMs
cannot prevent themselves from hallucinating [13].arXiv:2411.07112v2  [cs.SE]  22 Mar 2025phenomenon can cause errors to accumulate and amplify their
impact, potentially causing the generated content to completely
deviate from the expected path [15], [16]. Moreover, the
generation process of LLMs differs significantly from the
common practice of reviewing and adjusting existing code
in human coding. In practice, developers are able to adjust
their code whenever necessary based on its quality and its
alignment with requirements, while LLMs can merely proceed
based on the output generated so far and are unable to adjust
previous outputs spontaneously. Recent studies [17]‚Äì[19] have
attempted to utilize the LLMs to revise their output after
generation in a post-revising manner. However, this type of
approach faces difficulties in revising the accumulated errors
[20] and can result in resource wastage [17].
Ideally, through incorporating a backtracking mechanism
into the generation process, we can expose potential errors
early and resolve them, effectively preventing error propaga-
tion. However, to effectively implement backtracking, three
key issues ought to be addressed: 1) When to roll back.
During the generation process, the rollback is triggered de-
pending on when errors are detected. Error detection during
the generation of LLMs should satisfy the following condi-
tions. First, it must be capable of performing real-time checks
on incomplete code; second, it is required to cover common
errors produced by LLMs which are shown in Figure 1; finally,
its running speed would be better to fast enough so as not to
affect the efficiency of LLM significantly. 2) Where to roll
back to. Simply rolling back to the last error-free state of the
generated code usually does not address the issue. We should
identify the initial decision point that caused the error and
roll back to that point. Determining the rollback point is a
complex decision-making process because the meaning and
behavior of the erroneous code depend not only on itself, but
also on interactions with preceding code, which are influenced
by factors such as variable scopes, state dependencies, and
logical dependencies within the program. 3) How to avoid
previous errors. After the rollback, the key task during
regeneration is to prevent the recurrence of previous errors.
However, completely prohibiting the LLMs from generating
previously erroneous code may inadvertently block benign
tokens. Thus, it is essential to impose appropriate constraints
on the regeneration process.
To address the preceding three issues, we first implement
incremental error detection using program analysis, which
enables the examination of incomplete code to identify po-
tential errors. Compilers can be used not only in code trans-
formation for execution but also as an effective tool for
program analysis. It is capable of performing numerous key
and common analyses such as syntax parsing, type checking,
and dependency analysis, and they have been optimized for
speed over many years. Moreover, by using compilers, we can
design new analyses for specific errors in LLMs‚Äô generated
code, such as checking for code repetition problems. Second,
for determining rollback points, program analysis serves as an
external inspection during the generation of LLMs, providing
essential error information. However, this error informationmay not directly pinpoint the root cause of the error. In
contrast, the inherent uncertainty of LLMs is proven to be
usable for self-assessment during generation [21]‚Äì[23], which
can aid in tracing the root cause of errors. Therefore, combin-
ing these two sources of information facilitates determining
rollback points. Third, in regeneration with constraints, we
decay the generation probability of the paths leading to error
progressively. Moreover, by modeling the entire generation
process with tree structures, it is feasible to comprehensively
account for all historical errors and to effectively superimpose
penalties for them.
In this paper, we propose R OCODE, a novel code generation
approach that integrates backtracking mechanism and program
analysis to LLMs. The core of our approach ‚Äì the backtrack-
ing mechanism detects errors in real-time, rolls back, and
regenerates with constraints during the generation process of
LLMs, thus preventing error accumulation and enhancing the
performance and efficiency of code generation. Specifically,
we employ program analysis to perform incremental error
detection during the code generation process to discover errors
timely. Based on the results of program analysis and the
observation of uncertainty in the generation of LLMs, we
design a series of rollback strategies to determine the rollback
point. To constrain the process of regeneration, we strategically
penalize the likelihood of tokens that have contributed to
previous errors. Further, given that the introduction of rollback
and regeneration makes code generation no longer follow a
linear path, we use a Trie Tree to model the whole generation
process of R OCODE. Importantly, our approach is model-
agnostic, and requires no additional training.
Our experimental results demonstrate that R OCODE con-
sistently outperforms all baselines across six code generation
benchmarks. R OCODE achieves a compilation success rate of
99.1% and surpasses the best-performing baseline by 23.8% in
pass rate. To further demonstrate its utility, we apply R OCODE
to multilingual code generation tasks and achieve a relative
improvement of 34.2% in pass rate. We also explore generaliz-
ability of R OCODE across various LLMs, revealing significant
enhancements in the performance of both general LLMs and
code LLMs, with an average improvement of 18.2% in pass
rate. In terms of cost and performance, R OCODE reduces
token costs by 19.3%, compared to the Post-revising approach.
Furthermore, the ablation studies reveal that incremental error
detection, rollback strategies, and constraint regeneration in
ROCODE all contribute to performance improvement. To the
best of our knowledge, this work is the first to introduce and
implement the rollback approach for code generation during
the decoding process in LLMs2.
II. M ETHODOLOGY
A. Overview
For a code generation task, given the requirement x, we
propose to perform R OCODE for LLMs to generate code y.
ROCODE consists of three key steps:
2Code is available at https://github.com/jiangxxxue/ROCODE.‚Ä¢Incremental Error Detection focuses on continuously
checking the generated code during the generation pro-
cess to discover errors early. By implementing program
analysis, we can detect potential errors in the generated
code such as compile errors and runtime errors.
‚Ä¢Strategic Rollback , upon detecting an error, rolls back
the generated code to an earlier error-free state. In this
step, we design a series of specific rollback strategies to
determine the rollback point.
‚Ä¢Constraint Regeneration formulates error-related con-
straints and combines them with the LLM decoding
process to prevent previous errors from happening again.
This step involves strategically penalizing the likelihood
of the generated tokens that contributed to the errors.
To track the code generation progression of R OCODE,
we employ the structure of Trie Tree and and integrate
operations of incremental error detection, strategic rollback,
and constraint generation within the Trie Tree. This structure
helps organize non-linear, tree-like code generation trajecto-
ries, allowing us to efficiently handle multiple rollback and
regeneration cycles.
B. Incremental Error Detection
Considering LLMs are generated in an auto-regressive way,
once an error occurs during the generation process, the LLMs
will continue to generate content on the basis of the errors,
leading to the propagation of errors. Since the occurred errors
are inevitable in the final outputs, the subsequent generation
derived from these erroneous contents can be almost con-
sidered redundant. Therefore, we employ incremental error
detection to detect errors during generation in a timely manner
and substantially reduce the cost of long rollbacks.
Incremental error detection employs the program analysis
tool to incrementally detect errors following the generation of
each detectable unit. Specifically, we use the statement as the
smallest unit for detection, each representing the smallest code
unit with independent functionality. The LLM Mincremen-
tally generates these statements step by step. Upon completion
of each statement, we employ the program analysis tool Cto
conduct error detection. This process can be formulated as
follows:
si=M(x, S:i‚àí1), (1)
ei=C(S:i‚àí1‚à•si), (2)
where siis the i-th statement generated by M,S:i‚àí1=
[s0, s1, . . . , s i‚àí1],‚à•denotes the concatenation of statements,
andeiis the report of incremental error detection for si, which
is defined as:
ei={result ,type,lineno ,offset}, (3)
where ‚Äòresult‚Äô indicates the detection result of whether the
generated code passes, with possible values being {success,
failure}. If ‚Äòresult‚Äô is ‚Äòsuccess‚Äô, the remaining items are not
applicable, otherwise eireturns ‚Äòfailure‚Äô along with its ‚Äòtype‚Äô,
‚Äòlineno‚Äô, and ‚Äòoffset‚Äô. Among them, ‚Äòtype‚Äô indicates the typeof error detected, ‚Äòlineno‚Äô represents the line number where
the error occurs, and ‚Äòoffset‚Äô represents the specific position
of error within the ‚Äòlineno‚Äô.
The program analysis tool determines the types of errors
that can be detected during error detection. There are various
tools designed for different errors and programming languages.
In this paper, we choose the compiler to support our program
analysis. The reason is that the compiler integrates some key
and mature analysis techniques, which can effectively detect
errors commonly found in LLMs (shown in Fig. 1). Moreover,
compilers support almost all programming languages and run
fast. In the generation process, we use a compiler either
without executing or with executing test input to check the
generated code. Without executing, we can check for syntax
errors, type mismatches, declaration errors, scope errors, and
linking errors. With executing test input, we can further in-
crease checks for runtime errors, including timeouts, recursion
errors, division by zero errors, memory access errors, index
out-of-bounds errors, and resource not found errors. More-
over, in practical scenarios, developers usually have access to
publicly available test cases to better understand and validate
requirements, we take this part into account. Once the code
is completely generated, we execute the complete set of test
cases (if available), which include both input and output, to
thoroughly verify the program‚Äôs logic.
Furthermore, we observe that repeat patterns problem occurs
during the code generation process, characterized by the repet-
itive output of the same syntactic structure but meaningless
code constructs like ‚Äòif-elif-elif...‚Äô, ‚Äòprint‚Äô, etc., resulting in
failure to generate a termination symbol ( EOS ) [12]. This
problem typically does not result in syntax and compile errors
during generation, but it can significantly affect the semantics
of the generated code, thereby introducing potential logic
errors. Therefore, we design an additional analysis to detect
repeat patterns problem. Specifically, we utilize the syntax
parsing module of the compiler to extract the syntactic struc-
ture and identify repetitive patterns. According to Abstract
Syntax Definition Language (ASDL) [24], if the same type
ofstmt appears consecutively more than a specified number
of times, it is considered as an error with the error report
including ‚Äòtype‚Äô as repetition and ‚Äòlineno‚Äô as the line number
where the first repeated stmt occurs.
C. Strategic Rollback
When the error is detected, it is necessary to undo a part of
the previously generated code to rectify the issue. To identify
the specific point that requires to roll back to, we design a
series of strategies to determine the rollback point.
Generally, for detected errors, incremental error detection
can provide an error report including the location where the
error occurs, offering an initial clue to resolve the error.
Therefore, we first attempt to resolve the error by rolling back
directly to this specific location. The rollback point ris defined
as a two-dimensional value:
re= [e.lineno , e.offset ], (4)ùëÜ!ùëÜ"ùëÜ#ùëÜ$ùëÜ%ùëÜ%ùëÜ#ùëÜ$ùê∏ùëÇùëÜùë†!ùë†"ùë†#ùë†$ùë†%1. Incremental Error Detection2. RollbackMechanismùë†!ùë†"ùë†#ùë†$ùë†%3. ConstraintRegeneration
Program Analysis
LLMs!s"s#s$s%s%s%s%s#!High uncertainty
Generate the next statementRollback PointError detection resultsùë†&=ùë¶!"#ùë¶!ùë¶|%!|. . .
Checkthe statement/Nodes with penaltyFig. 2. The Overview of R OCODE with Trie Tree.
where e.lineno refers to the line number of error report eand
e.offset represents the offset within that line, used to precisely
locate the error. However, merely reverting to remay not be
sufficient for some complex errors, since the root cause of
these errors might not actually originate from the reported
location instead of an early location. These errors usually
involve dependencies and semantics.
Given that LLMs are probabilistic models, i.e., LLMs pre-
dict the next token by calculating the conditional probability
of each possible subsequent token given the preceding context,
LLMs may exhibit high levels of uncertainty at certain points
during the generation process, leading to fluctuating outputs
[21]‚Äì[23]. Although high uncertainty increases the likelihood
of erroneous decisions, it also facilitates the redistribution
of probabilities to alter the output. Therefore, we can infer
rollback points by analyzing the model‚Äôs display of uncer-
tainty. We can calculate the entropy at each position using the
following formula:
Ht=‚àí|V|X
j=1p(yt=vj|y<t, x) logp(yt=vj|y<t, x),(5)
where p(yt=vj|y<t, x)denotes the probability of generat-
ing the t-th token ytasvjgiven the context xand previously
generated tokens y<t. The summationP|V|
j=1iterates over all
possible tokens vjin the vocabulary Vthat can be generated
at this position. We roll back to the beginning of the statement
containing the token with the highest entropy.
t‚àó=argmaxt‚àà[0,|y|]Ht (6)
rh= [ConvertToLineno (t‚àó, y),0], (7)where ConvertToLineno is a function that converts the token
position to the corresponding line number within the code,
and|y|denotes the length of generated code y. This strategy
allows for an opportunity to refresh the most relevant context
in the next generation, thereby avoiding the recurrence of this
high-entropy point. The algorithm for the strategic rollback is
shown in Algorithm 1.
Algorithm 1 Algorithm of Strategic Rollback
Require: Error Detection Reports E={ei}1:nand Generated
Statements S={si}1:n.
Ensure: Rollback Point r.
1:Assert en.result is ‚Äòfailure‚Äô.
2:Initialize y‚Üês1||¬∑¬∑¬∑|| sn.
3:ifen.lineno and enÃ∏=en‚àí1then
4: r‚Üê[e.lineno , e.offset ].
5:else ‚ñ∑ en.lineno is None or the error recurs.
6: r‚Üê[ConvertToLineno (t‚àó, y),0], where t‚àóis com-
puted via Eq. (5) and Eq. (6).
7:end if
8:return r
D. Constraint Regeneration
After error detection and rollback, we perform constraint
regeneration to prevent the LLM from reproducing the same
error. Constraint regeneration involves two parts: constructing
constraints and integrating these constraints into the LLM‚Äôs
decoding process, thereby influencing the model‚Äôs output
behavior.
Constraint Construction: We define constraints as penal-
ties applied to the LLMs‚Äô output probabilities for previouslygenerated erroneous code. In the process of code generation,
the generated code can be considered as an output path of
LLMs. To avoid the model generating incorrect paths, a naive
approach is to set the probabilities of tokens on erroneous
paths to zero, completely blocking those paths. However, this
approach can penalize some benign tokens. Instead, we adopt
a milder penalty approach, which applies an exponentially
decaying penalty to each token from the point of error back
to a rollback point,
PN(v|y<t) =(
Œªt‚àír,ifv=yt,
1, otherwise,(8)
where Œªis a decay factor between 0 and 1 and t‚àírdenotes
the number of time steps from the rollback point rto the
current token yt. This approach not only penalizes the tokens
that directly cause errors but also applies lighter penalties
to preceding tokens that may have indirectly contributed to
the mistake, thereby preventing the model from repeating
erroneous generation paths.
Decoding with Constraints: To avoid errors in the previ-
ous generation path, penalties are applied to the probability
distribution of LLMs, adjusting the generation likelihood of
each token. Then, this modified probability distribution is re-
normalized. Specifically, for each token y‚Ä≤
t, its constrained
probability distribution pc(y‚Ä≤
t|y<t)is given by:
pc(y‚Ä≤
t|y<t) =p(y‚Ä≤
t|y<t)¬∑PN(y‚Ä≤
t|y<t)P
vp(v|y<t)¬∑PN(v|y<t)(9)
E. Trie Tree Modeling
The generation process of R OCODE involves rollbacks and
regeneration, resulting in a non-linear structure. Therefore, we
use Trie Tree to model the entire process, as illustrated in
Figure 2. In the Trie Tree T= (U, E), each node u‚ààU
represents a generated token in this process, and each edge
(u, v)indicates that the token sequence from the root node to
node userve as the context to generate node v.
During the generation process of R OCODE, as each state-
ment is generated, its corresponding tokens are sequentially
appended to the Trie Tree. This addition is immediately
followed by incremental error detection. If an error is iden-
tified, the affected path within the tree is flagged and the
strategic rollback is activated, identifying the precise node to
revert to. Subsequently, all descendant nodes of this rollback
point, representing the erroneous sequence, are used to impose
constraints. This penalization process effectively discourages
the regeneration of the same erroneous sequences during
subsequent iterations of code generation. Each new, error-
free statement is integrated into the tree as a distinct branch,
aligning with existing paths that share a common prefix.
This integration not only consolidates the tree structure but
also accumulates the penalties associated with each erroneous
path, reinforcing the deterrent against repeating past mistakes.
It ensures that the generation process dynamically adapts,
minimizing the recurrence of similar errors and optimizing
code output over time.Ultimately, each path from the root node to any termi-
nal node represents an attempt at the generation process of
ROCODE, and the last path of Trie Tree represents the final
generated code y. The pseudocode of R OCODE during code
generation is shown in Algorithm 2.
Algorithm 2 The Pseudocode of R OCODE
Require: Input Requirement x, LLM M.
Ensure: Generated Code y.
1:Initialize Trie Tree T‚Üê ‚àÖ and index i‚Üê0.
2:Statement si‚Üê M (x)
3:T.update stmt(si).
4:while sidoes not include EOS token do
5: #Incremental Error Detection
6: ei‚Üê C(T.stmts )via Eq. (2).
7: T.update report( ei).
8: #Strategic Rollback
9: ifei.result is ‚Äòfailure‚Äô then
10: r‚ÜêRollBack (T.stmts, T.reports )via Alg. 1.
11: T.rollback to(r).
12: end if
13: #Constraint Generation
14: T.update pn(T.stmts, r) via Eq. (8).
15: Sample si+1‚Üê M (x, T. stmts, T.pn)via Eq. (9).
16: i‚Üêi+ 1.
17: T.update stmt(si).
18:end while
19:return T.getfinal gen code()
III. E VALUATION
ROCODE aims to effectively prevent error accumulation in
the code generation process of LLMs and improve the quality
of generated code by integrating backtracking mechanism
and program analysis into LLMs. In this section, we present
extensive experiments that span six representative code gen-
eration benchmarks, two program languages, and nine LLMs
of varying series or sizes. We aim to investigate six research
questions:
‚Ä¢RQ1: How does R OCODE perform compared to baseline
approaches on code generation benchmarks?
‚Ä¢RQ2: How effective is R OCODE in improving LLMs
in code generation tasks across different programming
languages?
‚Ä¢RQ3: How does R OCODE perform when applied to
different LLMs?
‚Ä¢RQ4: How about the cost and efficiency of R OCODE?
‚Ä¢RQ5: How does each component of R OCODE contribute
to the effectiveness?
‚Ä¢RQ6: How does the hyperparameter decay factor affect
the effectiveness of R OCODE?
A. Evaluation Setup
1) Benchmark: We perform a comprehensive evaluation on
six code generation benchmarks to demonstrate the superiority
and generality of R OCODE.HumanEval [1] consists of 164 handwritten programming
tasks, proposed by OpenAI. Each task includes a function
signature, a requirement, use cases, a function body, and
several unit tests (average of 8 per task). We use the use cases
as public test cases for our approach and baseline approaches,
while unit tests are used as private test cases for evaluation.
MBPP [25] contains 974 Python programming tasks, cover-
ing programming fundamentals, standard library functionality,
and more. The MBPP dataset does not specify public vs.
private test cases. Following previous work [26], we use one
input of the test cases for all baseline approaches and do not
involve any ground-truth test case output.
CodeForces2305 [27] comprises 90 of the competition-
level programming problems collected from the CodeForces
website. On average, each problem is accompanied by three
public test cases and three private test cases. These problems
are created after May 2023, which is after the training data
cutoff of most LLMs, such as CodeLlama [4] and CodeGen
[5], mitigating the impact of data contamination on evaluation.
HumanEval-ET andMBPP-ET [28] are expanded versions
of HumanEval and MBPP with over 100 additional test cases
per task. This updated version includes edge test cases that
enhance the soundness of code evaluation compared to the
original benchmark.
HumanEval-CPP [29] is constructed based on the
HumanEval benchmark to evaluate the code generation ability
of LLMs on C++ programming language.
2) Baselines: Our approach works on the decoding phase of
LLMs that does not require modification and training of the
model. We use the three most common decoding approaches
of LLMs and set them as baselines. Specifically,
Temperature Sampling [30] controls the randomness of
the token selection process‚Äîhigher temperatures Tlead to
more uniform distributions, while lower temperatures Tmake
high-probability tokens even more likely.
P‚Ä≤(w) =exp(log( P(w|w<t))/T)P
w‚Ä≤exp(log( P(w‚Ä≤|w<t))/T), (10)
when Tis 0,P‚Ä≤(w)is equivalent to 1(w= arg max wP(w|
w<t)), which means greedy sampling.
Topk Sampling [31] limits the next-word selection to the
top k most likely candidates as determined by the model.
P‚Ä≤(w) =(
P(w|w<t)ifw‚ààTop-k ,
0 otherwise.(11)
Nucleus Sampling [32] involves choosing from a smaller
set of plausible candidates by dynamically selecting a variable-
sized subset of tokens (the ‚Äùnucleus‚Äù) that cumulatively make
up a certain probability mass (e.g., top 90%).
P‚Ä≤(w) =(
P(w|w<t)ifP
w‚Ä≤‚ààSP(w‚Ä≤|w<t)‚â§p,
0 otherwise.(12)
We also implement two baselines, representing the
execution-based sampling approaches [26], [33], [34] and thepost-revising approaches [18], [19], [35], to demonstrate the
efficiency of R OCODE, Specifically,
Sampling+Filtering utilizes LLMs to generate a vast num-
ber of codes, which are then filtered by executing test cases.
Post-revising conducts testing after code is generated by
LLMs, and further revises codes that fail these tests based on
error messages.
Additionally, we also compare four state-of-the-art (SOTA)
code generation approaches that operate during the decoding
process, Specifically,
PG-TD [34] employs Monte Carlo Tree Search during the
LLM decoding process, formulating rewards based on testing
results to guide the generation of code.
MBR-EXEC [26] introduces the execution result-based
minimum Bayes risk decoding to select code from the samples
generated by LLMs.
MGD [36] utilizes static analysis tools to perform type
analysis at pre-defined trigger points (specifically at derefer-
ence operations) during the code generation process of LLMs,
enabling the selection of type-consistent variables.
AdapT [37] dynamically adjusts the temperature during the
LLMs‚Äô generation process, applying a higher temperature at
points of low generation probability (challenging tokens) and
a lower temperature at points of high generation probability
(confident tokens).
3) Metrics: We used three metrics to evaluate our approach,
including PassRate, AvgPassRate, and Compiler Correctness
Percentage.
PassRate [1] metric can measure the functional correctness
of the generated code by executing private test cases. For each
task, n‚â•1samples of code are generated, and the number
of samples, c‚â§n, that pass the test cases are counted. The
PassRate is then calculated using the following estimator:
PassRate = E
Problems
1‚àí
n‚àíc
1

n
1
. (13)
AvgPassRatio [38] calculates the average proportion of test
cases that generated codes y‚Ä≤
pspass, which is a milder metric
than PassRate, allowing to assess the partial correctness of the
generated codes.
1
|P|X
p‚ààP1
|Cp|X
c‚ààCpI{Eval ( yp,Ip,c) =Op,c}, (14)
where prepresents a task within the test set P, and
{(Ip,c,Op,c)}Cp
c=1is the set of test cases for p,I(¬∑)is an
indicator function, which outputs 1 if the condition is true
and 0 otherwise, and Eval ( yp,Ip,c)represents an evaluation
function that obtains outputs of code ypby way of executing
it with Ip,cas input.
Compiler Correctness Percentage (CCP) measures the
proportion of generated code samples that are compilable (i.e.,
free of syntax errors and compilation errors). It is defined as:
CCP =Ncompilable
Ntotal, (15)TABLE I
THE COMPARISON OF ROCODE AND BASELINE APPROACHES ON DIFFERENT CODE GENERATION BENCHMARKS . THEBOLD TEXT INDICATES THE
HIGHEST VALUE FOR A PARTICULAR METRIC WITHIN A GIVEN DATASET ,REGARDLESS OF THE BASELINE OR ITS CONFIGURATIONS .
Approaches HumanEval (ET) MBPP (ET) CodeForces2305
PassRate AvgPassRate CCP PassRate AvgPassRate CCP PassRate AvgPassRate CCP
PG-TD 46.3 (38.4) 64.7 (61.2) 82.3 - - - 0.0 0.0 6.7
MGD 31.4 (24.6) 57.7 (54.3) 79.5 34.9 (28.0) 39.8 (40.4) 82.4 0.0 0.0 0.0
MBR-EXEC 34.8 (28.1) 59.3 (54.7) 80.5 34.0 (27.6) 39.0 (40.0) 83.5 0.0 0.7 5.6
AdapT 31.9 (26.7) 59.2 (55.8) 78.2 32.4 (26.4) 38.6 (39.1) 80.9 0.0 0.1 5.6
Temperature Sampling ( T= 0.0) 31.1 (24.4) 57.6 (54.5) 80.5 35.5 (29.3) 41.4 (42.0) 82.0 0.0 0.0 6.7
+ Post-revising 31.3 (24.5) 57.5 (54.5) 79.9 35.7 (29.4) 41.6 (42.0) 82.1 0.0 0.0 6.7
ROCODE 53.8 (45.5) 66.3 (62.6) 95.8 40.5 (32.3) 46.7 (47.3) 98.7 8.3 21.2 85.9
Temperature Sampling ( T= 0.6) 27.1 (22.9) 52.2 (47.2) 74.9 29.1 (23.3) 34.7 (35.7) 79.1 0.0 0.5 5.5
+ Filtering 35.9 (30.5) 55.5 (51.5) 74.4 31.8 (25.4) 38.4 (38.9) 88.1 0.0 1.8 6.7
+ Post-revising 32.3 (27.0) 57.0 (51.8) 75.7 30.2 (23.9) 36.0 (36.8) 83.9 0.0 0.4 4.3
ROCODE 55.8 (48.6) 71.4 (66.7) 96.4 35.7 (27.8) 41.9 (42.4) 98.3 8.9 22.3 88.7
Temperature Sampling ( T= 0.8) 22.2 (18.9) 43.7 (38.4) 67.9 21.9 (17.6) 26.5 (27.3) 69.3 0.0 0.6 4.4
+ Filtering 31.7 (29.9) 51.7 (47.2) 72.6 24.2 (20.1) 30.0 (30.5) 85.6 0.0 0.6 4.4
+ Post-revising 29.6 (25.9) 51.8 (47.0) 70.1 22.9 (18.1) 29.4 (29.8) 84.1 0.1 0.3 3.2
ROCODE 54.4 (48.0) 70.9 (66.8) 98.2 30.3 (23.9) 39.5 (40.9) 97.9 8.4 21.4 86.8
Top-k Sampling ( k= 10 ) 22.3 (18.5) 45.0 (40.1) 69.3 23.2 (18.4) 27.7 (28.5) 70.9 0.0 3.6 5.9
+ Filtering 35.4 (30.5) 55.9 (51.2) 68.9 25.4 (20.4) 30.3 (31.6) 85.4 0.0 0.4 7.8
+ Post-revising 27.1 (22.7) 50.1 (45.6) 70.5 25.5 (20.1) 30.5 (31.3) 83.1 0.0 0.6 5.4
ROCODE 53.1 (39.7) 67.7 (65.0) 97.0 32.6 (25.2) 40.7 (41.6) 97.6 7.7 20.1 86.6
Top-k Sampling ( k= 40 ) 21.7 (19.0) 43.8 (39.2) 70.4 22.3 (17.7) 26.8 (27.6) 69.3 0.0 0.3 5.0
+ Filtering 33.5 (29.3) 56.4 (51.6) 78.0 25.4 (20.2) 30.8 (31.2) 84.8 0.0 0.0 4.4
+ Post-revising 29.3 (25.2) 52.3 (47.4) 69.1 24.5 (19.3) 29.5 (30.4) 82.1 0.1 0.4 4.4
ROCODE 55.0 (46.1) 67.6 (62.7) 96.4 31.5 (24.7) 39.8 (41.4) 98.1 8.1 20.8 87.2
Nucleus Sampling ( p= 0.8) 28.8 (23.4) 54.7 (49.4) 76.3 31.2 (25.7) 37.0 (37.7) 81.2 0.0 5.2 6.1
+ Filtering 34.8 (28.1) 63.4 (57.5) 78.7 31.9 (24.7) 38.5 (39.3) 88.8 0.0 0.8 4.4
+ Post-revising 30.0 (24.2) 56.1 (50.8) 75.3 31.9 (25.2) 37.7 (38.3) 84.1 0.1 0.5 5.6
ROCODE 55.3 (46.8) 71.2 (68.0) 98.0 38.4 (30.5) 44.7 (45.6) 98.5 8.3 21.9 87.9
Nucleus Sampling ( p= 0.9) 26.7 (22.2) 52.0 (47.5) 75.1 28.5 (22.5) 33.9 (35.0) 79.3 0.0 0.4 6.0
+ Filtering 36.0 (30.5) 59.2 (54.9) 79.8 31.0 (24.1) 36.7 (37.5) 89.4 0.0 0.4 5.6
+ Post-revising 32.5 (27.2) 58.0 (53.8) 76.3 29.4 (23.1) 34.8 (35.7) 82.8 0.1 0.4 4.9
ROCODE 57.3 (48.2) 72.4 (66.1) 97.6 36.1 (27.5) 42.1 (42.6) 99.1 8.7 23.4 88.2
where Ncompilable is the number of compilable code samples,
andNtotalis the total number of generated codes.
4) Implementation Details: In the evaluation, we use
CodeLlama-7B [4] as base model by default. The decay
factor Œªfor constraint regeneration is set at 0.9. The maximum
generation length of our approach and baselines is set to
512 on all benchmarks, except for CodeForces2305, where
it is set to 1024. To mitigate the instability of the model
sampling, we report the average results of three trials in the
experiments. Due to space limits, we only present the results
on the HumanEval dataset (other benchmarks follow similar
trends) for RQ3, RQ4, and RQ5.
B. RQ1. Comparing ROCODE to Baseline Approaches
To evaluate the effectiveness of R OCODE on code gener-
ation, we evaluate test correctness and compile correctness
across various representative code generation benchmarks,
including HumanEval, MBPP, HumanEval-ET, MBPP-ET, and
CodeForces2305.
Settings: We compare our approach with nine baselines,
including Temperature Sampling, Topk Sampling, Nucleus
Sampling, Sampling+Filtering, Post-revising, PG-TD, MBR-
EXEC, MGD, and AdapT. For the HumanEval, MBPP,HumanEval-ET, and MBPP-ET benchmarks, CodeLlama-7B
serves as our base model, while for the more challenging
CodeForces2305 benchmark, we employ CodeLlama-34B as
our base model. Since Temperature Sampling, Top-k Sam-
pling, and Nucleus Sampling are sensitive to their parameter
settings, we evaluate their performance under different set-
tings. For the temperature (T) in Temperature Sampling, we
use values of 0.0, 0.6, and 0.8. For the k value in Top-k Sam-
pling, we use 10 and 40. For the p-value in Nucleus Sampling,
we use 0.8 and 0.9. Our approach, Sampling+Filtering, and
Post-revising can be combined with these three decoding meth-
ods. We set the token budget of R OCODE during generation to
be twice the maximum generation length. Sampling+Filtering
and Post-revising maintain the same token budget as R OCODE.
Results: The experimental results are shown in Table I.
These results demonstrate that our approach outperforms all
baseline approaches across three metrics on five datasets,
demonstrating the superior performance of R OCODE. Notably,
our approach shows the best performance at 57.3% in pass rate
under the Nucleus Sampling ( p= 0.9) setting on HumanEval
benchmark, exceeding the direct generation with LLMs by
30.6% in the same setting. Specifically, our approach exceeds
those of Sampling+Filtering and Post-revising across three
commonly used decoding methods: Temperature Sampling,Top-k Sampling, and Nucleus Sampling. The fact that our
approach significantly surpasses the Sampling+Filtering ap-
proach proves that the improvement in performance is not
merely due to repetitive sampling but is greatly aided by
the backtracking mechanism. Compared to Post-revising, our
approach can resolve errors in real-time during the generation
process, which helps enhance the quality of generated code
and prevents the accumulation of errors that can compli-
cate error resolution. Among all baselines, PG-TD performs
the best; however, it requires both test inputs and outputs
for execution, limiting its applicability to benchmarks like
MBPP that do not provide public test cases. It is also worth
noting that all approaches generally perform worse on the
CodeForces2305 dataset compared to other benchmarks. This
may be due to two reasons: firstly, the code generation task
in CodeForces2305 is inherently challenging, with even the
powerful ChatGPT achieving only a 7.9% pass rate in the
original paper [27]; secondly, potential data contamination
issues might have caused the LLMs to perform exceptionally
well on other benchmarks, creating a significant disparity with
CodeForces2305. Despite this, R OCODE successfully attains
a performance level of pass rate up to 8.9%, which represents
a substantial improvement over the baselines on the Code-
Forces2305 benchmark. This enhancement underscores the
significant potential of our approach to elevate the capabilities
of LLMs in addressing complex problem-solving tasks.
C. RQ2. Performance on Multilingual Code Generation
For different programming languages, due to the unique
characteristics of each language and the distribution of training
data, there are variations in the performance of LLMs when
generating code in different languages. In this evaluation, we
examine the performance of our approach on multilingual code
generation tasks.
Settings: In addition to Python language, we also evaluate
our approach on C++ language utilizing HumanEval-CPP
[29] benchmark. The baseline approaches include Temperature
Sampling, Sampling + Filtering, and Post-revising, all of
which employ the best-performing configurations of PassRate
as shown in Table I.
TABLE II
THE PERFORMANCE OF ROCODE ON DIFFERENT PROGRAMMING
LANGUAGES (PL).
PL Approaches PassRate AvgPassRate CCP
C++Temperature Sampling 26.8 45.2 85.8
Sampling + Filtering 29.5 49.8 84.6
Post-revising 28.9 48.8 85.4
ROCODE 39.6 60.7 95.5
PythonTemperature Sampling 31.1 57.6 80.5
Sampling + Filtering 36.0 59.2 79.8
Post-revising 32.5 58.0 76.3
ROCODE 57.3 72.4 97.6
Results: The experimental results in Table II show that
our approach significantly improves performance in both
languages. Our approach achieves greater improvement onPython, which is a language where LLMs excel, compared to
C++. Nevertheless, our approach still outperforms all baselines
in C++, with a relative increase of 34.2% over the best-
performing baseline, i.e., Sampleing + Filtering, in pass rate.
Moreover, our approach achieves a 95.5% compilation pass
rate on C++ code generation tasks, significantly higher than
other baselines. Utilizing compiler-based program analysis
for error detection proves effective across various languages,
ensuring the robustness and versatility of our approach.
D. RQ3. Performance on Different LLMs
ROCODE is model-agnostic and can be applied to a variety
of LLMs. In this evaluation, we explore how R OCODE en-
hances code generation performances across different LLMs.
Settings: We employ several different series and sizes of
representative general LLMs and Code LLMs to perform
ROCODE. The general LLMs used are from the Llama series
(Llama-2-7B, 13B, and 34B [39]), while the Code LLMs
include the multi-lingual CodeGen series (CodeGen-2B, 6B,
and 16B [5]), and the CodeLlama series (CodeLlama-7B, 13B,
and 34B [4]).
01020304050607080
Llama-7BLlama-13BLlama-34BCodeGen-2BCodeGen-6BCodeGen-16BCodeLlama-7BCodeLlama-13BCodeLlama-34BPassRate
Temperature SamplingROCODE
Fig. 3. The performance of R OCODE on different LLMs.
Results: From the experimental results shown in Figure
3, we can observe that R OCODE achieves significant im-
provements over temperature sampling across all series and
on LLMs of various sizes. Our approach achieves higher
performance on code LLMs compared to general LLMs, with a
pass rate exceeding 70%. Furthermore, we observed a trend in
the enhancement across different LLMs: the stronger the base
model, the greater the improvement brought by R OCODE. This
might suggest that more powerful LLMs have greater potential
for enhancements through rollback corrections.
E. RQ4. Cost and Efficiency of ROCODE
Besides performance, cost, and efficiency also influence
whether a code generation approach will be widely adopted.
Therefore, we discuss the cost and efficiency of R OCODE.
Settings: We measure the costs by the number of tokens
consumed, since the computational resource usage for LLMsscales with the number of tokens and services that provide
LLM access typically charge based on token usage. We also
measure the efficiency by the speed of the running time
(min). We compared R OCODE with seven baseline methods:
PG-TD, MGD, MBR-EXEC, AdapT, Temperature Sampling,
Sampling+Filtering, and Post-revising. For Temperature Sam-
pling, Sampling+Filtering, and Post-revising, we configure
them according to the parameter configurations that exhibit
the best performance (PassRate) shown in Table I.
TABLE III
THE COST AND EFFICIENCY OF ROCODE,WHERE THE BOLD ITALIC
INDICATES THE HIGHEST VALUE OTHER THAN ROCODE,WHICH IS ALSO
THE BASELINE OF THE RELATIVE IMPROVEMENT .
Approaches PassRate Token Consumption Time
PG-TD 46.3 675.2 1.219
MGD 31.4 566.4 0.348
MBR-EXEC 34.8 438.5 0.314
AdapT 31.9 332.1 0.309
Temperature Sampling 31.1 445.8 0.313
Sampling + Filtering 36.0 532.2 0.334
Post-revising 32.5 623.4 0.498
ROCODE 57.3 (‚Üë23.8%) 503.1 0.622
Results: The evaluation results on HumanEval benchmark
are presented in Table III. In terms of cost (token consump-
tion), our approach shows clear advantages compared to most
baselines. Compared to Temperature Sampling that is directly
generated with LLMs, our cost increases by less than 1.1
times. Notably, our approach is substantially more efficient
compared to Post-revising, with a cost reduction of 19.3%.
More importantly, the cost of our approach is significantly
lower than the state-of-the-art (SOTA) approach, PG-TD. In
terms of time efficiency, although our approach is slightly
slower than Sampling + Filtering and Post-revising, it is still
faster than PG-TD. The additional time cost primarily stems
from calling the compiler for incremental checks. Consider-
ing the generated code is in Python language, which is an
interpreted language suitable for just-in-time execution and
dynamic typing, we can perform incremental execution rather
than starting from scratch each time to further optimize the
speed of checks.
F . RQ5. Ablation Study
ROCODE consists of three key components: incremental
error detection, strategic rollback, and constraint generation.
We evaluate the effectiveness of each component through
ablation experiments.
Settings: We modify or remove different components while
keeping the rest of R OCODE unchanged:
‚Ä¢For error detection, we replace the original program
analysis-based detection with an entropy-based detection,
which aligns with our rollback strategy, reporting errors at
locations with the highest entropy ( Entropy-based Error
Detection ).
‚Ä¢For the strategic rollback, we explore four other roll-
back strategies respectively: 1) Roll back directly to
the beginning and generate from scratch ( Full RestartRollback ). 2) Roll back only to the statement of the
reported error ( Error Statement Rollback ). 3) Roll back
only to the statement with the highest entropy ( High
Entropy Statement Rollback) ). 4) Roll back to the token
with the highest entropy and disable that token ( High
Entropy Token Disable Rollback ).
‚Ä¢For constraint generation, we remove the constraints
during generation but instead resample an output
(Constraint-Free Resampling ).
TABLE IV
ABLATION RESULTS .
Variants PassRate AvgPassRate CPP
Entropy-based Error Detection 45.1 59.0 76.2
Full Restart Rollback 50.6 63.8 95.0
Error Statement Rollback 51.2 62.9 93.2
High Entropy Statement Rollback 49.4 61.9 92.8
High Entropy Token Disable Rollback 47.5 60.7 88.3
Constraint-Free Resampling 50.7 64.4 89.4
ROCODE 57.3 72.4 97.6
Results: The experimental results on the HumanEval bench-
mark are shown in Table IV. From the experimental results, it
is evident that all components of our approach are effective.
In contrast to the entropy-based error detection approach,
our program analysis-based error detection avoids the bias
of labeling tokens as erroneous merely due to their high
entropy, as not all high-entropy tokens lead to errors. The
Full Restart Rollback cannot achieve the same performance
as our approach under the same token budget as it lacks in
efficiency. The performance decline observed in both Error
Statement Rollback and High Entropy Statement Rollback fur-
ther validates the effectiveness of combining program analysis
with LLM-based entropy assessments in rollback strategies.
Additionally, approaches that simply block high-entropy to-
kens have failed to effectively alter the entropy of the code,
thus offering limited performance enhancement. Removing
constraints during decoding also leads to a noticeable decline
in performance, which confirms the efficacy of our Constraint
Regeneration approach.
G. RQ6. Effect of Decay Factor
Since R OCODE involved one hyperparameter, the decay
factor Œª, we evaluate the impact of different values of this
hyperparameter on the performance to analyze its sensitivity.
Settings: We choose {0.5, 0.6, 0.7, 0.8, 0.85, 0.9, 0.95,
0.98, 0.99 }as test values for Œª. We conduct experiments on
HumanEval benchmarks under the setting of greedy sampling.
Results: The experimental results of this evaluation are
shown in Figure 4. From the results we can observe that
as the hyperparameter Œªincreases, the metrics PassRate and
AvgPassRate show a slight downward trend, although the
decline is not significant. On the other hand, the CCP metric,
while fluctuating across different values of r, still maintains
a high level overall, averaging over 90%. These observations
suggest that our approach demonstrates strong robustness to
adjustments in the hyperparameter Œª. The downward trend in40%50%60%70%80%90%100%
0.50.60.70.80.850.90.950.980.99pass@1AvgPassRatioCCPFig. 4. The performance of R OCODE with different values of the hyperpa-
rameter Œª. We use the gray dashed line to represent the employed hyper-
parameters.
PassRate and AvgPassRate could be due to higher values of
Œªmeaning looser constraint penalties during code generation,
which decays the likelihood of previous errors less. Specifi-
cally, a larger Œªvalue reduces the immediate penalty for errors,
requiring more iterations to correct mistakes, which may affect
the performance of generating correct code within a limited
token budget.
IV. T HREATS TO VALIDITY
There are three major threats to the validity of our work.
1) Threats to external validity concern the quality of
experimental datasets and the generalizability of our results.
First, we use six public code generation datasets for evaluation,
which are mainstream benchmarks and have been used in
many related works [34], [40]‚Äì[44]. Moreover, to prevent the
evaluation dataset from being affected by data contamination
(i.e., the test data may have been included in the training
data of LLMs), we used problems from CodeForces that were
published after the cutoff date of LLM‚Äòs training data for the
assessment. Second, R OCODE can be applied to any LLMs,
and we choose nine well-known LLMs [45]‚Äì[48] of different
series and sizes for our experiments.
2) Threats to internal validity involve the impact of hyper-
parameters. For our approach, we introduce a hyperparameter,
i.e. the decay factor in constrained regeneration. For this hyper-
parameter, we intuitively selected a specific value and observed
that it enhances performance across multiple benchmarks.
To further explore the impact of this hyperparameter, we
conducted detailed experimental studies, which showed that
this hyperparameter effectively improves experimental results
over a broad range. As for other hyperparameters, such as
maximum generation length and temperature, to ensure fair-
ness in comparison, we maintained these parameters consistent
with the baseline approaches.
3) Threats to construct validity pertain to the reliability
of evaluation metrics. We use the test pass rate as the primary
evaluation metric. However, due to the limited number of
test cases, this method cannot fully assess the functionalcorrectness of the generated code. To mitigate this issue,
we adopted extended versions of some benchmarks, which
significantly expanded the number of test cases to provide
a more comprehensive functional evaluation. For PassRate
metrics, we employ the unbiased version of PassRate [1] to
diminish evaluation errors that arise from sampling. On this
basis, each experiment is run three times, and its average result
is reported.
V. R ELATED WORK
A. Code Generation with LLMs
General LLMs, represented by ChatGPT [49], have demon-
strated significant potential in software engineering tasks such
as code generation. This led to the development of specialized
LLMs for code generation, such as AlphaCode [50], CodeGen
[5], Incoder [6], CodeGeeX [29], Starcoder [41], WizardCoder
[40] and CodeLlama [4]. These specialized LLMs are typically
developed by further training general LLMs or by training
them from scratch using code corpus. Furthermore, there is
a series of research efforts for code generation that propose
improvements to the decoding stage of general LLMs or Code
LLMs. Zhang et al. [34] proposed a planning-guided decoding
algorithm to generate higher-quality programs. This algorithm
is based on Monte Carlo Tree Search (MCTS) and explores
different branches of the search tree to examine various possi-
bilities for program generation. After generating a complete
program, it is evaluated by executing test cases to obtain
rewards. Shi et al. [26] and Chen et al. [33] generate a large
number of program samples from LLMs and subsequently re-
ranking them using public or generated test cases. Zhang et
al. [35] introduced Self-edit, which involves training another
model to modify the programs generated by LLMs based on
the results of executing test cases. Similarly, Cheng et al. [19]
also developed a post-processing technique for modifying the
outputs of models. Those approach leverages the capabilities
of LLMs to debug and correct their own errors.
B. Combining Program Analysis and LLMs
Combining emerging LLMs with traditional program anal-
ysis techniques to overcome existing technological limitations
has become a new trend. Currently, there have been some
efforts in this direction, which have been applied to various
software engineering tasks including program synthesis, for-
mal verification, and defect detection. Jain et al. [51] proposed
Jigsaw, an approach that performs several transformations
and checks during the processing steps, thereby enhancing
the program synthesis capabilities of LLMs and validating it
through the synthesis of the Python Pandas API. Agrawal et
al. [36] incorporated type-based static analysis into the code
generation process, enabling the provision of a candidate list
that constrains LLMs to produce type-correct identifiers. Wen
et al. [52] utilize static analysis techniques to decompose
programs, thereby facilitating incremental specification gen-
eration for program verification. Li et al. [53] designed LLift,
a framework that enables interaction between static analysis
tools and LLMs, using use-before-initialization (UBI) bugs asa case study to demonstrate its effectiveness. Wang et al. [54]
proposed a resource leak detection approach that combined
LLMs with static program analysis. This approach utilizes
LLMs to infer the resource-oriented intentions (resource acqui-
sition, release, and reachability verification) in code, instead
of matching predefined APIs, and then inferred intentions are
applied to enhance static resource leak detection techniques.
VI. C ONCLUSION
In this paper, we introduce R OCODE, a novel code gener-
ation approach based on LLMs that integrates backtracking
mechanism and program analysis tools to eliminate errors
in the code generation process. Our approach enables LLMs
to generate programs incrementally, followed by incremental
error detection through program analysis. When an error
is detected, we perform rollback strategies, which provide
an opportunity for LLMs to make modifications during the
generation process. Furthermore, we impose constraints on the
regeneration process to avoid repeating historical errors. Our
approach is model-agnostic and does not require training, al-
lowing for direct integration with LLMs. Experimental results
show that our approach consistently outperforms baselines
across various benchmarks, providing stable improvements for
different decoding approaches and various LLMs.
REFERENCES
[1] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan,
H. Edwards, Y . Burda, N. Joseph, G. Brockman, A. Ray, R. Puri,
G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan,
S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian,
C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert, F. Chantzis,
E. Barnes, A. Herbert-V oss, W. H. Guss, A. Nichol, A. Paino, N. Tezak,
J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse,
A. N. Carr, J. Leike, J. Achiam, V . Misra, E. Morikawa, A. Radford,
M. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder,
B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba,
‚ÄúEvaluating large language models trained on code,‚Äù CoRR , 2021.
[Online]. Available: https://arxiv.org/abs/2107.03374
[2] S. Shen, X. Zhu, Y . Dong, Q. Guo, Y . Zhen, and G. Li, ‚ÄúIncorporating
domain knowledge through task augmentation for front-end javascript
code generation,‚Äù in ESEC/SIGSOFT FSE . ACM, 2022, pp. 1533‚Äì1543.
[3] Y . Dong, G. Li, and Z. Jin, ‚ÄúCODEP: grammatical seq2seq model for
general-purpose code generation,‚Äù in ISSTA . ACM, 2023, pp. 188‚Äì198.
[4] B. Rozi `ere, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y . Adi,
J. Liu, T. Remez, J. Rapin, A. Kozhevnikov, I. Evtimov, J. Bitton,
M. Bhatt, C. Canton-Ferrer, A. Grattafiori, W. Xiong, A. D ¬¥efossez,
J. Copet, F. Azhar, H. Touvron, L. Martin, N. Usunier, T. Scialom, and
G. Synnaeve, ‚ÄúCode llama: Open foundation models for code,‚Äù CoRR ,
vol. abs/2308.12950, 2023.
[5] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y . Zhou, S. Savarese,
and C. Xiong, ‚ÄúCodegen: An open large language model for code with
multi-turn program synthesis,‚Äù in ICLR . OpenReview.net, 2023.
[6] D. Fried, A. Aghajanyan, J. Lin, S. Wang, E. Wallace, F. Shi, R. Zhong,
W. Yih, L. Zettlemoyer, and M. Lewis, ‚ÄúIncoder: A generative model
for code infilling and synthesis,‚Äù CoRR , vol. abs/2204.05999, 2022.
[7] X. Jiang, Y . Dong, L. Wang, Z. Fang, Q. Shang, G. Li, Z. Jin, and
W. Jiao, ‚ÄúSelf-planning code generation with large language models,‚Äù
ACM Trans. Softw. Eng. Methodol. , vol. 33, no. 7, Sep. 2024. [Online].
Available: https://doi.org/10.1145/3672456
[8] X. Jiang, Y . Dong, Z. Jin, and G. Li, ‚ÄúSEED: customize large language
models with sample-efficient adaptation for code generation,‚Äù CoRR , vol.
abs/2403.00046, 2024.
[9] T. Zhang, T. Yu, T. Hashimoto, M. Lewis, W. Yih, D. Fried, and
S. Wang, ‚ÄúCoder reviewer reranking for code generation,‚Äù in ICML , ser.
Proceedings of Machine Learning Research, vol. 202. PMLR, 2023,
pp. 41 832‚Äì41 846.[10] GitHub. (2022) Copilot. [Online]. Available: https://github.com/features/
copilot
[11] T. Dohmke, M. Iansiti, and G. Richards, ‚ÄúSea change in software
development: Economic and productivity analysis of the ai-powered
developer lifecycle,‚Äù arXiv preprint arXiv:2306.15033 , 2023.
[12] F. Liu, Y . Liu, L. Shi, H. Huang, R. Wang, Z. Yang, and L. Zhang, ‚ÄúEx-
ploring and evaluating hallucinations in llm-powered code generation,‚Äù
CoRR , vol. abs/2404.00971, 2024.
[13] Z. Xu, S. Jain, and M. S. Kankanhalli, ‚ÄúHallucination is inevitable: An
innate limitation of large language models,‚Äù CoRR , vol. abs/2401.11817,
2024.
[14] Z. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y . Xu, E. Ishii, Y . Bang,
A. Madotto, and P. Fung, ‚ÄúSurvey of hallucination in natural language
generation,‚Äù ACM Comput. Surv. , vol. 55, no. 12, pp. 248:1‚Äì248:38,
2023.
[15] S. Wu, X. Xiao, Q. Ding, P. Zhao, Y . Wei, and J. Huang, ‚ÄúAdversarial
sparse transformer for time series forecasting,‚Äù in NeurIPS , 2020.
[16] ¬¥A. Mart ¬¥ƒ±nez-Gonz ¬¥alez, M. Villamizar, and J. Odobez, ‚ÄúPose transformers
(POTR): human motion prediction with non-autoregressive transform-
ers,‚Äù in ICCVW . IEEE, 2021, pp. 2276‚Äì2284.
[17] Y . Dong, X. Jiang, Z. Jin, and G. Li, ‚ÄúSelf-collaboration code
generation via chatgpt,‚Äù ACM Trans. Softw. Eng. Methodol. , vol. 33,
no. 7, Sep. 2024. [Online]. Available: https://doi.org/10.1145/3672459
[18] A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe,
U. Alon, N. Dziri, S. Prabhumoye, Y . Yang, S. Gupta, B. P. Majumder,
K. Hermann, S. Welleck, A. Yazdanbakhsh, and P. Clark, ‚ÄúSelf-refine:
Iterative refinement with self-feedback,‚Äù in NeurIPS , 2023.
[19] X. Chen, M. Lin, N. Sch ¬®arli, and D. Zhou, ‚ÄúTeaching large language
models to self-debug,‚Äù CoRR , vol. abs/2304.05128, 2023.
[20] T. X. Olausson, J. P. Inala, C. Wang, J. Gao, and A. Solar-Lezama,
‚ÄúIs self-repair a silver bullet for code generation?‚Äù in The Twelfth
International Conference on Learning Representations , 2023.
[21] S. Farquhar, J. Kossen, L. Kuhn, and Y . Gal, ‚ÄúDetecting hallucinations
in large language models using semantic entropy,‚Äù Nat., vol. 630, no.
8017, pp. 625‚Äì630, 2024.
[22] L. Kuhn, Y . Gal, and S. Farquhar, ‚ÄúSemantic uncertainty: Linguistic
invariances for uncertainty estimation in natural language generation,‚Äù
inICLR . OpenReview.net, 2023.
[23] J. Kossen, J. Han, M. Razzak, L. Schut, S. A. Malik, and Y . Gal,
‚ÄúSemantic entropy probes: Robust and cheap hallucination detection in
llms,‚Äù CoRR , vol. abs/2406.15927, 2024.
[24] D. C. Wang, A. W. Appel, J. L. Korn, and C. S. Serra, ‚ÄúThe zephyr
abstract syntax description language,‚Äù in DSL. USENIX, 1997, pp.
213‚Äì228.
[25] J. Austin, A. Odena, M. I. Nye, M. Bosma, H. Michalewski, D. Dohan,
E. Jiang, C. J. Cai, M. Terry, Q. V . Le, and C. Sutton, ‚ÄúProgram synthesis
with large language models,‚Äù CoRR , vol. abs/2108.07732, 2021.
[26] F. Shi, D. Fried, M. Ghazvininejad, L. Zettlemoyer, and S. I. Wang,
‚ÄúNatural language to code translation with execution,‚Äù in EMNLP .
Association for Computational Linguistics, 2022, pp. 3533‚Äì3546.
[27] Y . Dong, X. Jiang, H. Liu, Z. Jin, B. Gu, M. Yang, and G. Li,
‚ÄúGeneralization or memorization: Data contamination and trustworthy
evaluation for large language models,‚Äù in Findings of the Association for
Computational Linguistics: ACL 2024 . Association for Computational
Linguistics, 2024, pp. 12 039‚Äì12 050.
[28] Y . Dong, J. Ding, X. Jiang, G. Li, Z. Li, and Z. Jin, ‚ÄúCodescore:
Evaluating code generation by learning code execution,‚Äù ACM
Trans. Softw. Eng. Methodol. , Sep. 2024. [Online]. Available:
https://doi.org/10.1145/3695991
[29] Q. Zheng, X. Xia, X. Zou, Y . Dong, S. Wang, Y . Xue, Z. Wang, L. Shen,
A. Wang, Y . Li, T. Su, Z. Yang, and J. Tang, ‚ÄúCodegeex: A pre-trained
model for code generation with multilingual evaluations on humaneval-
x,‚ÄùCoRR , vol. abs/2303.17568, 2023.
[30] M. Caccia, L. Caccia, W. Fedus, H. Larochelle, J. Pineau, and L. Charlin,
‚ÄúLanguage gans falling short,‚Äù in International Conference on Learning
Representations (ICLR) , 2019.
[31] A. Fan, M. Lewis, and Y . N. Dauphin, ‚ÄúHierarchical neural story
generation,‚Äù in ACL (1) . Association for Computational Linguistics,
2018, pp. 889‚Äì898.
[32] A. Holtzman, J. Buys, L. Du, M. Forbes, and Y . Choi, ‚ÄúThe curious
case of neural text degeneration,‚Äù in ICLR . OpenReview.net, 2020.
[33] B. Chen, F. Zhang, A. Nguyen, D. Zan, Z. Lin, J. Lou, and
W. Chen, ‚ÄúCodet: Code generation with generated tests,‚Äù CoRR , vol.
abs/2207.10397, 2022.[34] S. Zhang, Z. Chen, Y . Shen, M. Ding, J. B. Tenenbaum, and C. Gan,
‚ÄúPlanning with large language models for code generation,‚Äù in ICLR .
OpenReview.net, 2023.
[35] K. Zhang, Z. Li, J. Li, G. Li, and Z. Jin, ‚ÄúSelf-edit: Fault-aware code
editor for code generation,‚Äù in ACL (1) . Association for Computational
Linguistics, 2023, pp. 769‚Äì787.
[36] L. A. Agrawal, A. Kanade, N. Goyal, S. K. Lahiri, and S. K. Rajamani,
‚ÄúGuiding language models of code with global context using monitors,‚Äù
CoRR , vol. abs/2306.10763, 2023.
[37] Y . Zhu, J. Li, G. Li, Y . Zhao, J. Li, Z. Jin, and H. Mei, ‚ÄúHot or cold?
adaptive temperature sampling for code generation with large language
models,‚Äù in AAAI . AAAI Press, 2024, pp. 437‚Äì445.
[38] D. Hendrycks, S. Basart, S. Kadavath, M. Mazeika, A. Arora, E. Guo,
C. Burns, S. Puranik, H. He, D. Song, and J. Steinhardt, ‚ÄúMeasuring
coding challenge competence with APPS,‚Äù in NeurIPS Datasets and
Benchmarks , 2021.
[39] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y . Babaei,
N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, D. Bikel, L. Blecher,
C. Canton-Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes,
J. Fu, W. Fu, B. Fuller, C. Gao, V . Goswami, N. Goyal, A. Hartshorn,
S. Hosseini, R. Hou, H. Inan, M. Kardas, V . Kerkez, M. Khabsa,
I. Kloumann, A. Korenev, P. S. Koura, M. Lachaux, T. Lavril, J. Lee,
D. Liskovich, Y . Lu, Y . Mao, X. Martinet, T. Mihaylov, P. Mishra,
I. Molybog, Y . Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi,
A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E. Tan, B. Tang,
R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan, I. Zarov, Y . Zhang,
A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov,
and T. Scialom, ‚ÄúLlama 2: Open foundation and fine-tuned chat models,‚Äù
CoRR , vol. abs/2307.09288, 2023.
[40] Z. Luo, C. Xu, P. Zhao, Q. Sun, X. Geng, W. Hu, C. Tao, J. Ma, Q. Lin,
and D. Jiang, ‚ÄúWizardcoder: Empowering code large language models
with evol-instruct,‚Äù CoRR , vol. abs/2306.08568, 2023.
[41] R. Li, L. B. Allal, Y . Zi, N. Muennighoff, D. Kocetkov, C. Mou,
M. Marone, C. Akiki, J. Li, J. Chim, Q. Liu, E. Zheltonozhskii, T. Y .
Zhuo, T. Wang, O. Dehaene, M. Davaadorj, J. Lamy-Poirier, J. Monteiro,
O. Shliazhko, N. Gontier, N. Meade, A. Zebaze, M. Yee, L. K. Umapathi,
J. Zhu, B. Lipkin, M. Oblokulov, Z. Wang, R. M. V , J. Stillerman,
S. S. Patel, D. Abulkhanov, M. Zocca, M. Dey, Z. Zhang, N. Moustafa-
Fahmy, U. Bhattacharyya, W. Yu, S. Singh, S. Luccioni, P. Villegas,
M. Kunakov, F. Zhdanov, M. Romero, T. Lee, N. Timor, J. Ding,
C. Schlesinger, H. Schoelkopf, J. Ebert, T. Dao, M. Mishra, A. Gu,
J. Robinson, C. J. Anderson, B. Dolan-Gavitt, D. Contractor, S. Reddy,
D. Fried, D. Bahdanau, Y . Jernite, C. M. Ferrandis, S. Hughes, T. Wolf,
A. Guha, L. von Werra, and H. de Vries, ‚ÄúStarcoder: may the source be
with you!‚Äù CoRR , vol. abs/2305.06161, 2023.
[42] J. P. Inala, C. Wang, M. Yang, A. Codas, M. Encarnaci ¬¥on, S. K.
Lahiri, M. Musuvathi, and J. Gao, ‚ÄúFault-aware neural code rankers,‚Äù in
NeurIPS , 2022.
[43] D. Huang, Q. Bu, J. Zhang, X. Xie, J. Chen, and H. Cui, ‚ÄúBias
assessment and mitigation in llm-based code generation,‚Äù arXiv preprint
arXiv:2309.14345 , 2023.
[44] X. Wei, S. K. Gonugondla, S. Wang, W. U. Ahmad, B. Ray, H. Qian,
X. Li, V . Kumar, Z. Wang, Y . Tian, Q. Sun, B. Athiwaratkun, M. Shang,
M. K. Ramanathan, P. Bhatia, and B. Xiang, ‚ÄúTowards greener yet
powerful code generation via quantization: An empirical study,‚Äù in
ESEC/SIGSOFT FSE . ACM, 2023, pp. 224‚Äì236.
[45] D. Zan, B. Chen, Z. Lin, B. Guan, Y . Wang, and J. Lou, ‚ÄúWhen language
model meets private library,‚Äù in EMNLP (Findings) . Association for
Computational Linguistics, 2022, pp. 277‚Äì288.
[46] Y . Wen, P. Yin, K. Shi, H. Michalewski, S. Chaudhuri, and A. Polozov,
‚ÄúGrounding data science code generation with input-output specifica-
tions,‚Äù CoRR , vol. abs/2402.08073, 2024.
[47] J. Liu, C. S. Xia, Y . Wang, and L. Zhang, ‚ÄúIs your code generated by
chatgpt really correct? rigorous evaluation of large language models for
code generation,‚Äù in NeurIPS , 2023.
[48] D. Zan, B. Chen, D. Yang, Z. Lin, M. Kim, B. Guan, Y . Wang, W. Chen,
and J. Lou, ‚ÄúCERT: continual pre-training on sketches for library-
oriented code generation,‚Äù in IJCAI . ijcai.org, 2022, pp. 2369‚Äì2375.
[49] OpenAI. (2022) ChatGPT. [Online]. Available: https://openai.com/blog/
chatgpt/
[50] Y . Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond,
T. Eccles, J. Keeling, F. Gimeno, A. Dal Lago et al. , ‚ÄúCompetition-
level code generation with alphacode,‚Äù Science , vol. 378, no. 6624, pp.
1092‚Äì1097, 2022.[51] N. Jain, S. Vaidyanath, A. S. Iyer, N. Natarajan, S. Parthasarathy,
S. K. Rajamani, and R. Sharma, ‚ÄúJigsaw: Large language models meet
program synthesis,‚Äù in ICSE . ACM, 2022, pp. 1219‚Äì1231.
[52] C. Wen, J. Cao, J. Su, Z. Xu, S. Qin, M. He, H. Li, S. Cheung, and
C. Tian, ‚ÄúEnchanting program specification synthesis by large language
models using static analysis and program verification,‚Äù CoRR , vol.
abs/2404.00762, 2024.
[53] H. Li, Y . Hao, Y . Zhai, and Z. Qian, ‚ÄúThe hitchhiker‚Äôs guide to
program analysis: A journey with large language models,‚Äù CoRR , vol.
abs/2308.00245, 2023.
[54] C. Wang, J. Liu, X. Peng, Y . Liu, and Y . Lou, ‚ÄúBoosting static resource
leak detection via llm-based resource-oriented intention inference,‚Äù
CoRR , vol. abs/2311.04448, 2023.
[55] C. Snell, J. Lee, K. Xu, and A. Kumar, ‚ÄúScaling LLM test-time compute
optimally can be more effective than scaling model parameters,‚Äù CoRR ,
vol. abs/2408.03314, 2024.
[56] B. C. A. Brown, J. Juravsky, R. S. Ehrlich, R. Clark, Q. V . Le, C. R ¬¥e, and
A. Mirhoseini, ‚ÄúLarge language monkeys: Scaling inference compute
with repeated sampling,‚Äù CoRR , vol. abs/2407.21787, 2024.APPENDIX
A. Examples of ROCODE vs. Post-revising
Figure 5 presents an example of R OCODE based on CodeLlama-7B, and Figure 6 shows an example of the Post-revising
approach with the same LLMs for comparison.
even =0odd =0fori instr(num):ifint(i) %2==0:even =0odd =0fori instr(num):ifint(i) %2==0:Invalid literal for int() with base 10: '-'defeven_odd_count(num):"""Given an integer. return a tuple that has the number of even and odd digits           respectively.Example:even_odd_count(-12) ==> (1, 1)even_odd_count(123) ==> (1, 2)"""Input
LLMeven =0odd =0even =0odd =0ifnum <0:num =num *-1fordigit instr(num):ifint(digit) %2==0:even +=1else:odd +=1return(even, odd)ROCODE (Our Approach)
Roll back to the statementwithhighest uncertaintytokenUncertaintyHighUncertaintyLowUncertainty
Fig. 5. An example of R OCODE.
defeven_odd_count(num):"""Given an integer. return a tuple that has the number of even and odd digits           respectively.Example:even_odd_count(-12) ==> (1, 1)even_odd_count(123) ==> (1, 2)"""Input
LLMeven =0odd =0fori instr(num):ifint(i) %2==0:even +=1else:odd +=1return(even, odd)Error Code:defeven_odd_count(num):"""Given an integer. return a tuple that has the number of even and odd digits           respectively.Example:even_odd_count(-12) ==> (1, 1)even_odd_count(123) ==> (1, 2)"""even =0odd =0fori instr(num):ifint(i) %2==0:even +=1else:odd +=1return(even, odd)Error Message:Line 12: invalid literal for int() with base 10: '-'Fixed code:Input
LLMeven_count, odd_count =0, 0fordigit innum:ifdigit %2==0:even_count +=1else:odd_count +=1return(even_count, odd_count)Invalid literal for int() with base 10: '-'
Not Passing TestFinallyPhase 1Phase 2Post-revising Approach
Iteratephasesuntil the error detection is passed or the limit is reachedRe-generate from scratch to reviseConstruct Prompt
Fig. 6. An example of Post-revising approach.