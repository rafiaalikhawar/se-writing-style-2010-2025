arXiv:2501.15181v2  [cs.SE]  2 May 2025From Bugs to Beneﬁts: Improving User Stories by
Leveraging Crowd Knowledge with CrUISE-AC
Stefan Schwedt
Heriot Watt University
Edinburgh, Scotland
schwedt@cruise-ac.netThomas Str¨ oder
Fachhochschule der Wirtschaft (FHDW)
Mettmann, Germany
thomas.stroeder@fhdw.de
©2025 IEEE. Personal use of this material is permitted. Perm ission from IEEE must be obtained for all other uses, in any cu rrent or future media, including
reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for r esale or redistribution to servers or lists, or
reuse of any copyrighted component of this work in other work s. This work has been accepted to the 2025 IEEE/ACM Internati onal Conference on Software
Engineering (ICSE) for publication. The ﬁnal version is ava ilable at: https://doi.org/10.1109/ICSE55347.2025.002 17Abstract —Costs for resolving software defects increase expo-
nentially in late stages. Incomplete or ambiguous requirem ents
are one of the biggest sources for defects, since stakeholde rs
might not be able to communicate their needs or fail to share
their domain speciﬁc knowledge. Combined with insufﬁcient
developer experience, teams are prone to constructing inco rrect
or incomplete features. To prevent this, requirements engi neering
has to explore knowledge sources beyond stakeholder interv iews.
Publicly accessible issue trackers for systems within the s ame
application domain hold essential information on identiﬁe d weak-
nesses, edge cases, and potential error sources, all docume nted
by actual users. Our research aims at (1) identifying, and (2 )
leveraging such issues to improve an agile requirements art ifact
known as a “user story”. We present CrUISE-AC (Crowd and
User Informed Suggestion Engine for Acceptance Criteria) a s
a fully automated method that investigates issues and gener ates
non-trivial additional acceptance criteria for a given use r story by
employing NLP techniques and an ensemble of LLMs. CrUISE-
AC was evaluated by ﬁve independent experts in two distinct
business domains. Our ﬁndings suggest that issue trackers h old
valuable information pertinent to requirements engineeri ng. Our
evaluation shows that 80–82% of the generated acceptance
criteria add relevant requirements to the user stories. Lim itations
are the dependence on accessible input issues and the fact th at
we do not check generated criteria for being conﬂict-free or non-
overlapping with criteria from other user stories.
Index Terms —Agile requirements engineering, User stories,
Acceptance criteria, Issue tracker, NLP, NLP4RE, LLM
I. I NTRODUCTION
In software development, defects are a signiﬁcant driver fo r
costs. Costs for resolving defects become exponentially hi gh if
they are detected late in the development process [1]. After a
software system has been released, the costs for ﬁxing a defe ct
are up to 100 times higher compared to a defect detected in
the early requirement phase [2].
In agile software development, SCRUM is used as the
major process template which proposes to note requirements
in a textual artifact called “user story”. A user story is
typically written in the standardized Connextra format “As
a [role], I want [action], so that [beneﬁt]” and acts as a
reminder to discuss the described feature between involved
roles [3], [4]. Intensive conversation rather than documen tation
is supposed to ensure mutual understanding and agreement of
a requirement’s scope between stakeholders, product owner
and the development team [5].User stories are typically accompanied by acceptance crite -
ria (AC). These are conditions a system must meet in order to
fulﬁll a user story [4]. They can be either written in pure
natural language or in a semi-structured format known as
Gherkin scenarios. A Gherkin scenario follows a GIVEN-
WHEN-THEN pattern and serves as a solid basis to derive
acceptance tests [6].
However, agile requirement engineering comes with some
challenges. Stakeholders might not be able to communicate
their needs [7] or fail to share their domain speciﬁc knowled ge
[8]. Due to lack of knowledge, experience and inadequate
requirement analysis, the development team is likely to bui ld
wrong or incomplete features [9]. In the end, these results a re
deviations from stakeholder’s (implicit) expectations wh ich are
deﬁned as software defects [10].
All systems within a software family share a common set of
features [10]. Observing and analyzing similar systems hel ps
to get a deeper understanding about requirements [11]. Con-
sidering that the current software system falls under a cert ain
software family, public issue trackers can be an additional
source of information about possibly hidden or yet unknown
requirements.
In this paper, we address and answer the following research
question:
How can we use issue trackers of similar systems to
generate additional acceptance criteria for a given user
story and how useful are the generated criteria?
As an answer to the research question, we present CrUISE-
AC, a novel approach to enrich user stories with automati-
cally generated non-obvious acceptance criteria extracte d from
crowd-entered knowledge listed in public issue trackers. O ur
pipeline integrates a variety of AI techniques, including s uper-
vised classiﬁcation [12] for natural language documents, z ero-
shot instruction prompting using decoder-only LLMs [13] an d
ensemble learning [14].
We demonstrate the proposed approach using real-world
industrial user stories from two different application dom ains.
Our main focus is on e-commerce, from which we gathered
over 300 user stories across three distinct projects. For th is
domain, over 165,000issues were sourced from seven different
public issue trackers. Our results reveal that 82% of the AC
produced by CrUISE-AC represent signiﬁcant enhancements
to user stories, as determined by manual evaluation.To evaluate the applicability of our method across differen t
domains, we performed a similar assessment within the do-
main of content management systems (CMS). In this context,
80% of all generated AC were considered useful.
Overall, the results demonstrate that issue trackers can se rve
as a valuable knowledge repository and should be utilized in
conjunction with stakeholder interviews during the requir e-
ments elicitation process. Furthermore, CrUISE-AC provid es a
novel approach and framework to extract, transform and util ize
this data.
II. R ELATED WORK
Agile software development starts with the deﬁnition of
a high-level project scope that is broken down into smaller
requirements artifacts during reﬁnements [15]. These requ ire-
ments artifacts are deﬁned with the customer and other
stakeholders [16]. Thus, stakeholders collaborate extens ively
with the development team to establish these deﬁnitions and
continuously validate the product being delivered. Accord -
ing to McGrath et al. [17], stakeholders are described in
various ways within the literature, identifying end-users as
key stakeholders. To gather requirements and other feedbac k
from a large distributed and heterogeneous stakeholder gro up
of end-users (referred to as “crowd” [18]), Crowd-Based
Requirements Engineering (CrowdRE) promises to provide
appropriate mechanisms [19]. Numerous studies utilize use r
reviews to identify and extract feature requests. SAFE uses
part-of-speech (POS) patterns to detect features in App Re-
views [20]. Using SAFE as baseline, Motger et al. [21] traine d
different transformer based models on a corpus they built fr om
human-created feature tags in app descriptions. Wu et al. [2 2]
identiﬁed key features of an app by analyzing app descriptio ns
and user reviews. Moreover, the extraction of requirements
from various document and text types has been explored. Shi
et al. [23] examined development emails, while Abualhaija
et al. [24] analyzed textual requirement speciﬁcations. In
addition to features, the processes of extracting and trans -
forming requirements artifacts have been examined. Tiwari
et al. [25] identiﬁed use case scenarios from textual proble m
speciﬁcations. Furthermore, Ngaliah et al. [26] showed tha t
online news can be utilized as a data source for creating user
stories. All studies employ natural language processing (N LP)
techniques [27] to identify and convert text fragments into
artifacts that facilitate requirements engineering.
Issue trackers are software tools used by organizations to i n-
teract with users and stakeholders, facilitating communic ation,
documentation, and collaboration on project-related issu es
during the entire software development lifecycle [28]. Use rs
submit various types of issues, including feature requests ,bug
reports , and questions , thereby sharing their experiences and
wishes regarding the software system. Data collected from
issue trackers were the subject of investigation by several
researchers, who developed or enhanced methods for the
automated classiﬁcation of issues [29]–[31]. Ruan et al. [3 2]
suggested an approach to automatically establish missing l inks
between issues and commits. To the best of our knowledge,there is no existing work that employs issue trackers to iden -
tify, extract, and convert their data into requirements art ifacts.
Furthermore, there has been no proposed approach to associa te
issue data with the existing requirements of other projects , thus
enabling their utility in different contexts. This paper ai ms to
address this gap by presenting a novel concept on the rationa le
and methodology for utilizing data from issue trackers.
Throughout our experiments to match issues with user
stories, we investigated the usefulness of various approac hes
grounded in information retrieval (IR) [33]. In evaluating
the semantic similarity between two documents of natural
language text, functions like cosine can be employed to
quantify the semantic distance after converting the docu-
ments into vector representations [34]. Various methods ex ist
for converting documents into vectors, including statisti cally
driven approaches (e. g., term frequency – inverse document
frequency (TF-IDF) [35]) or models based on neural networks
(e. g., universal sentence encoder (USE) [36] or Sentence-
BERT (SBERT) [37]). As these methods are designed to
retrieve the most similar documents, our experiments revea led
that these approaches provided information with limited no v-
elty (or even none) to a given user story. Typically, the
issues returned directly mirrored the content of a user stor y.
Furthermore, the strategy of omitting the initial nresults to
obtain more innovative requirements was ineffective since it
was challenging to establish a clear beginning and end for th e
retrieval window.
Clustering approaches [38] especially utilizing DBSCAN
[39] produced good results in certain cases. However, to
obtain these result, we had to use a (manually crafted) domai n
speciﬁc thesaurus to identify different words with the same
meaning. Moreover, the quality of the results was highly
dependent on the complexity of the user story, thus limiting
the approach’s utility. We ﬁrst tried to classify user stori es to
predict when such a clustering approach would yield useful
results, but then we found the following approach, which is
both simpler, more powerful, and does not need any domain
speciﬁc thesaurus.
III. A PPROACH
An overview of the proposed approach is shown in Fig. 1.
Data objects are illustrated as rectangles featuring a curv ed
lower edge. Processing steps are represented as rectangles .
The upcoming sections will detail the four major steps
involved in the processing pipeline: Preprocessing, Match ing,
Generation, and Evaluation. We consider the following thre e
inputs:
•issuesIharvested from various issue trackers
•a single user story Uirequiring the generation of accep-
tance criteria
•the set of already deﬁned acceptance criteria AUifor the
user story Ui(this set might be empty)
A. Preprocessing
The goal of our issue preprocessing is to retain only infor-
mation containing business requirements. For this work, webug report #1
- description
- steps to reproduce
issuesI
description
expected result
preprocessed issues I′PreprocessingAs a user, I want to receive
an order conﬁrmation. . .
user story Ui
description
expected result
matched issues IUiMatching
acceptance criterion
GIVEN
WHEN
THEN
generated acceptance
criteriaAUigGeneration
acceptance criterion
GIVEN
WHEN
THEN
relevant acceptance
criteriaAUirEvaluation- display a “Thank you”
- the order conﬁrmation
should be easy to print
- . . .- display a “Thank you”
- the order conﬁrmation
should be easy to print
- . . .- display a “Thank you”
- the order conﬁrmation
should be easy to print
- . . .
existing acceptance
criteriaAUi
Fig. 1: CrUISE-AC processing pipeline
only consider issues written in English. We ﬁrst eliminate p ull
requests (identiﬁable by their URL or description starting with
a default pattern) and duplicates (i. e., issues with the sam e
title and description). Moreover, we only keep the caption,
description, and assigned labels of an issue, not making use
of any other ﬁelds.
No order conﬁrmation when changing the payment method
Description: If an order is placed and e.g. Paypal is selected,
you will receive an order conﬁrmation . However,
if you cancel the payment and want to complete the order
with a new payment method in the customer account , no order
conﬁrmation will be sent. For this purpose, an event was crea ted in
the Flow Builder, which is triggered when the payment method in
the order is changed (Checkout / Order / Payment Method /
Changed). As an action, a mail is then sent with the mail templ ate
“Order conﬁrmation”. The event is triggered but the mail is n ot
generated because the ISO code is not passed.
Environment: Shopware 6.4.9.0
Steps to reproduce:
•Create a trigger in the Flow Builder with “Checkout / Order /
Payment Method / Changed” and create an “Order
conﬁrmation” mail
•Order with e.g. Paypal until you come to the payment
•Cancel order
•Complete the order in the customer account and change the
payment method beforehand
Expected result:
New order conﬁrmation with the correct payment method
Current result:
Mail is not generated because the ISO code is missing
Fig. 2: Sample issue taken from Shopware 6
Issues contain various types of information. Most of them
are authored by developers for developers, aiming not only t o
convey a desired outcome (either a bug to be ﬁxed or a new
feature to be added) but also to provide detailed instructio ns on
how to reproduce the mentioned issue. Many reports include
source code to offer insights or suggestions on resolving th edescribed issue. Consequently, for our approach we need to
identify and eliminate information without business requi re-
ments. Fig. 2 illustrates an example issue1from Shopware 6.
Sentences enclosed in red boxes embody the actual business
requirement.
As an issue report is usually written in markdown language
[40], we can easily identify and remove unneeded sections
(such as environment, steps to reproduce, system status rep ort)
and embedded source code. Similarly, we eliminate URLs,
image links, HTML comments and duplicate sentences.
In the last step of issue preprocessing, we identify phrases in
the remaining sections which do not relate to any requiremen t.
Shi et al. [23] classify such phrases as trivia. Examples are “I
hope you will ﬁnd the solution” ,“Thank you” or“I think it
should be modiﬁed” . We use a supervised deep learning model
(see Sec. III-E) to identify and remove trivia phrases. Issu es
processed and reduced as described constitute the corpus I′.
B. Matching
During the next phase, we associate relevant issues with
the user story. An issue is deemed to correspond to the user
story if it includes information inﬂuencing the implementa tion
of that user story. Usually, this means the issue contains
useful additional requirements or outlines a bug describin g an
overlooked edge case. The task of matching can be reduced
to a binary classiﬁcation problem, where each user story/is sue
pair is assessed. The result is “yes” if there is a correspond ence
between the user story and the issue and “no” otherwise.
To match issues to user stories, we utilized decoder-only
large language models (LLMs). These models have proven
their superiority across a wide range of NLP tasks [41]
including the classiﬁcation of short texts [42].
Given the vast diversity of user stories and issues as well
as the fact that our data is not labeled beforehand, supervis ed
classiﬁcation is impractical in this case. Likewise, few-s hot
learning turned out to be unfeasible in our experiments sinc e
1https://issues.shopware.com/issues/NEXT-20948we did not ﬁnd good ways to generalize examples of such
pairs. However, decoder-only LLMs have proven their abilit y
to generalize well across different application domains al ong
with impressive zero-shot performance [43].
Thus, we pass user story/issue pairs along with a brief
description of the application domain to kdecoder-only LLMs
using a zero-shot instruction prompt. The prompt’s instruc tions
are as follows: Below you will ﬁnd the description of an issue
and a user story. Assess, if the issue affects the functional ity and
role covered in the user story. Return ”yes” or ”no”, nothing else.
The full prompt is available online.2
Each user story Uitogether with the m-th LLM deﬁne
a characteristic function matchUi
m:I′→ {0,1}with
matchUi
m(Ij) = 1 iff them-th LLM returns “yes” to the
prompt above.
An ensemble or combination of classiﬁers [44] is a widely
utilized technique in classiﬁcation learning, which invol ves
constructing a new classiﬁer by integrating a set of base
classiﬁers. Numerous studies have demonstrated that this a p-
proach signiﬁcantly enhances the classiﬁcation performan ce
compared to individual classiﬁers (see, e. g., [45]). A comm on-
ality among these studies is the utilization and combinatio n
of traditional approaches. To the best of our knowledge,
there is currently no research that explores ensemble metho ds
combining the outputs of multiple decoder-only LLMs.
Our approach uses a non-adaptive ensemble algorithm [46]
using majority voting [47]. The process of applying and lear n-
ing weights (e. g., as proposed in [48]) for each base model,
along with evaluating potential improvements, is deferred to
future research.
Hence, the user story Uitogether with our kLLMs deﬁne
the overall classiﬁcation result for each issue by the chara cter-
istic function matchUi:I′→ {0,1}withmatchUi(Ij) = 1
iffk/summationtext
m=1matchUi
m(Ij)≥/ceilingleftbigk
2/ceilingrightbig
.
Thus, the set of corresponding issues is IUi={Ij∈I′|
matchUi(Ij) = 1}.
C. Generation
Acceptance criteria for user stories are generally written
either as natural language sentences or as semi-structured
Gherkin scenarios. These criteria serve as a foundation for de-
veloping and supporting acceptance tests. The Gherkin form at
offers advantages over unstructured sentences by allowing for
simpler automated text generation and ensuring traceabili ty
from acceptance criteria to executable test cases [6]. The
Gherkin pattern consists of a brief description of the scena rio
itself, followed by a precondition GIVEN , a trigger WHEN and
a result THEN . An example of a simple Gherkin scenario is:
Scenario : Simple search
GIVEN a web browser is on a search engine’s page
WHEN the search phrase “cake” is entered
THEN results for “cake” are shown
2https://zenodo.org/records/14709846/ﬁles/prompt match.txtTo convert an issue Ij∈IUiinto a Gherkin-style acceptance
criterion, we use a single LLM (this time, the output is not
suitable for a simple merging mechanism like majority votin g).
The prompt contains the instruction “Use the following issue
listed under [Issue] to generate one new acceptance criteri on
in Gherkin format (GIVEN-WHEN-THEN) for the user story in
[User Story].” along with the issue text Ijand the user story
Ui. We provide the full prompt online.3
We refer to the set of generated AC for user story Ui
(considering all issues in IUi) asAUig. An example of such
an AC generation is shown in Fig. 3.
Input = preprocessed and matched issue i∈IUi
Session Not Expire After New Password Reset.
Initially the user will login in a browser then the go
with another browser and changed the password for his
account. Again he come back to his ﬁrst browser but
the application doesn’t ask new password. We need to
get the new password from the user to login for
different browsers.a
Output
Scenario: Ensure session expires in other browsers after
password reset
GIVEN I am logged in on Browser A
WHEN I change my password on Browser B
THEN I should be prompted to log in again on
Browser A with the new password
aOriginal issue: https://github.com/opencart/opencart/ issues/10449
Fig. 3: Gherkin scenario generated from issue
D. Evaluation
The standard for software and requirement quality [4]
deﬁnes several characteristics for requirements to be complete ,
consistent ,feasible ,comprehensible andable to be validated .
These criteria are supported by literature and even extende d
[49], [50]. In this paper, we have addressed the following
quality criteria so far:
•completeness by suggesting additional and potentially yet
unknown acceptance criteria for a user story,
•feasibility by generating acceptance criteria from issues
that have been solved already in other projects,
•comprehensibility and ability to be validated by using
Gherkin as a template for acceptance criteria.
Consistency demands that requirements are unique and do
not conﬂict with or overlap with other requirements in the
set. We do not address the global check for conﬂict-free and
non-overlapping conditions, as it necessitates consideri ng the
entire scope of all user stories together (we only focus on on e
user story at a time). However, we do address consistency
3https://zenodo.org/records/14709846/ﬁles/prompt generate.txt(partially) within the scope of the one user story under
consideration.
To this end, we use an LLM to determine whether the
generated acceptance criteria introduce any new requireme nts
beyond those already known. Additionally, we request that
the generated acceptance criteria must contain some non-
trivial and non-obvious knowledge. An example for a trivial
requirement (which we want to eliminate) is depicted in Fig. 4.
User story with acceptance criteria
As a logged-in user I can view and sort my favorite
products.
•Clicking on “Title A-Z” sorts the titles
alphabetically from A to Z
•Clicking on “Title Z-A” sorts the titles
alphabetically from Z to A
•. . .
Generated acceptance criterion
Scenario: Sort wishlist by product name
GIVEN I am a logged-in user
WHEN I choose to sort my wishlist by product name
THEN my wishlist should be displayed in alphabetical
order based on product name
Fig. 4: Example for trivial acceptance criterion
We prompt the model to assess the quality of all new
acceptance criteria along with an explanation for the taken
decision. It contains the instruction, the user story Ui, the
existing acceptance criteria AUi, one newly generated accep-
tance criterion a∈AUig, and the issue Ij∈IUifrom which
awas generated. The basic instruction is: “For the following
user story listed under [User Story] and the supplied existi ng
acceptance criteria listed under [Acceptance Criteria] te ll me, if
the new acceptance criterion listed as [New Acceptance Crit e-
rion] adds any unique, novel or surprising insights and cann ot be
considered as common knowledge. Your response LABEL must
be either “relevant” or “irrelevant”. “relevant” means, [N ew Ac-
ceptance Criterion] adds valuable and not common knowledge .”
The full prompt is available online.4
The model’s response for Fig. 4 was irrelevant . The decision
was explained by the already existing requirement of alpha-
betical sorting. Thus, the newly generated acceptance crit erion
does not introduce any new conditions.
We refer to all generated acceptance criteria the model
deems relevant for UiasAUir.
E. Implementation
As the issues do not carry a language ﬁeld, we employed
automatic language detection utilizing the Python package
cld3 [51] to identify issues written in English. To identify and
ﬁlter trivia phrases during preprocessing, a supervised de ep
learning model was trained on a balanced corpus that contain s
4https://zenodo.org/records/14709846/ﬁles/prompt evaluate.txt1,916phrases with an even distribution of 958 trivia and 958
non-trivia phrases.5
Our objective in constructing the corpus was to analyze
approximately 1,000 random issues. To achieve this, we se-
lected a random sample of 143 issues from each of the seven
e-commerce-related issue trackers described in Section IV -A2.
The issues were preprocessed according to the method
outlined in Section III-A, except for the ﬁnal step of ﬁlteri ng
by trivia, which was not applied. After preprocessing, 841
issues remained with a total of 2,373lines of text. Each line
was reviewed by expert E1 (see Section IV-A3) and labeled
for trivia / non-trivia.
The ﬁnal balanced corpus was split with a ratio of 9:1 for
training and test data. The best results were achieved by a
method that integrates Robustly optimized BERT approach
(RoBERTa) [52] from the Transformer family and Long
Short-Term Memory (LSTM) [53] from the Recurrent Neural
Networks family. This combined approach has been proven
to provide better results compared to training a Transforme r
model independently for various NLP classiﬁcation tasks (s ee,
e. g., [54], [55]). During experiments, we trained and compa red
BERT [56], RoBERTa [52] and XLNet models [57]. The best
score was achieved by the combined RoBERTa+LSTM model
with an accuracy and F1-score of 0.91 on the test data. We
provide the trained model online (hence, it can also be appli ed
in other settings).6
At this stage, we have not conducted a thorough evaluation
of the labeling process or the model’s outcomes, since the
model serves merely as a component in the preprocessing
phase. Instead, we refer to the results of the evaluation of
the entire process as detailed in Section IV.
For the selection of LLMs in the matching process,
we focused on smaller open-source models7and assessed
the most popular models available for download from the
Ollama website [58]. These models were (presented in ascent
order of billion parameters): phi3.5:3b ,gemma:7b ,
gwen2:7b ,mistral:7b ,llama3:8b ,llama3.1:8b ,
mistral-nemo:12b ,phi3:14b ,gemma2:27b .
The best performance in our ﬁnal LLM ensemble was
achieved using the following ﬁve models: gemma2:7b ,
mistral-nemo:12b ,llama3:8b ,llama3.1:8b , and
gemma2:27b .
To generate Gherkin-style acceptance criteria from issues ,
we use OpenAI’s GPT-4 Turbo model [59]. To get close to
deterministic results, we set the temperature (a parameter that
controls the randomness of the model’s output) to 0. The same
model is also used to check generated AC afterwards for new
and non-trivial requirements compared to already existing AC.
IV. E VALUATION AND RESULTS
We evaluated our approach within two different application
domains. In our primary domain e-commerce, we collected
5https://zenodo.org/records/14709846/ﬁles/trivia-tra iningdata.csv
6https://zenodo.org/records/12749484
7We also evaluated some commercial GPT-models, but they demo nstrated
comparatively suboptimal results.307 real-world user stories from three different projects.
Additionally, over 160,000 issues from seven different issue
trackers were mined. To apply our results to a second domain,
we obtained 34 user stories for a CMS product created by
a company based in the Netherlands. This dataset has been
used in other research before and was originally created by
Lucassen et al. [60]. We gathered data from two distinct CMS
issue trackers, mining over 145,000issues.
A. Primary Domain: E-commerce
We ﬁrst describe the data collection followed by our eval-
uation methods.
1) User stories: For the e-commerce domain, we gained
access to user stories from three distinct industry-speciﬁ c
closed-source projects. This is where our research started ,
since one of the authors was involved in these projects and we
wanted to ﬁnd improvements for the way how these projects
were carried out. All projects speciﬁcally target the book
industry in German-speaking countries and are managed by
different product owners. Therefore, user stories are writ ten
by different authors.
Project A deﬁnes a complete set of requirements for a B2C
focused online shop of a publishing house who aims to sell
its own publications directly.
Project B contains a partial set of requirements for a B2C
focused online shop of a bookseller.
Project C includes a subset of B2C and B2B requirements
for an online bookstore, supplemented by an e-procurement
module designed to provide information and automation for
industrial customers.
Since AC are optional, not all user stories have some. Table I
presents the number of user stories (US) we received by proje ct
along with the individual distribution between user storie s with
and without additional acceptance criteria (AC). We provid e
the dataset online.8
TABLE I: User stories by project
Project # US with AC Ratio
Project A 127 77 61%
Project B 140 111 79%
Project C 40 13 33%
Total 307 201 65%
250 out of 307 user stories along with their AC were written
in German language. To match them against the collected
issues written in English, user stories in German had to be
translated. For this purpose, we used the API provided by
DeepL [61].
All user stories and AC were anonymized. Company names
were replaced by neutral terms (e. g., “merchant” or “shop
owner”) and contained URLs removed.
8https://zenodo.org/records/14709846/ﬁles/User%20sto ries%20e-commerce.xlsx2) Issues: To collect a sufﬁciently large dataset of issues,
publicly accessible issue trackers were crawled. We employ ed
data supplied by the platform BuiltWith [62] to identify
relevant e-commerce systems with a certain market penetrat ion
and analyzed this list in a top-down manner. We looked for
systems with publicly accessible issue trackers containin g a
certain amount of issues. As of August 3rd, 2023, when we
started our research and accessed BuiltWith, the ranking of
the projects utilized was WooCommerce (1)9, Magento (3)10,
PrestaShop (7)11, OpenCart (8)12, and nopCommerce (33)13
out of 109 total systems listed. A crawler was utilized to
download and copy all issues that met the following criteria
to a local database:
•The issue’s state is set to “closed” to capture only issues
that have been reviewed by the developers.
•The issue is not labeled with “cannot reproduce”, “dupli-
cate”, “needs update”, “invalid”, “refactoring”, or “test ”.
Given that all the issue trackers of interest offered an
API and supported the application of ﬁlters, the process
was straightforward. As described in Section III-A, we only
downloaded the caption, description, and assigned labels o f
each issue, not making use of any attached discussion or
other ﬁelds. With this approach, over 165,000 issues could
be collected that were created between June 2011 and July
2024. For this work, we only considered artifacts written in
the English language. As the issues do not carry a language
ﬁeld, we employed automatic language detection utilizing t he
Python package cld3 [51] which identiﬁed 120,168 issues as
being written in English. Moreover, we eliminated issues ba sed
on the following criteria:
•The issue is a pull request, not a bug report or feature
request.
This type was identiﬁed by URLs beginning with /pull/
or descriptions starting with “This issue is automatically
created based on existing pull request” .
•The issue was detected as a duplicate.
Our database contained more than one issue with exactly
the same title and description. In this case, only the issue
with the earliest creation date was kept.
Table II presents the projects whose issue trackers were
crawled along with the total number of collected and the ﬁnal
count of issues remaining after preprocessing. We provide
the full dataset of issues including both the original and
preprocessed text online.14
3) Evaluation Procedure: Since we could not compare our
results to existing approaches, we had to include a manual
evaluation step at the end to determine the quality of our
results. Hence, we had to limit the number of user stories
and issues used in the evaluation to a feasible amount. We
9https://woocommerce.com
10https://magento-opensource.com
11https://prestashop.com
12https://www.opencart.com
13https://www.nopcommerce.com
14https://zenodo.org/records/14709846/ﬁles/Issues%20e -commerce.xlsxTABLE II: Data collected from issue trackers (e-commerce)
Project Issued downloaded Issues remaining
magento2133,806 13,081
nopCommerce27,131 3,745
OpenCart313,805 3,690
PrestaShop431,200 6,510
Shopware5520,283 2,740
Shopware6615,491 7,341
WooCommerce743,840 17,289
Total 165,556 54,396
1https://github.com/magento/magento2/issues
2https://github.com/nopSolutions/nopCommerce/issues
3https://github.com/opencart/opencart/issues
4https://github.com/PrestaShop/PrestaShop/issues
5https://issues.shopware.com/?products=SW-5
6https://issues.shopware.com/?products=SW-6
7https://github.com/woocommerce/woocommerce/issues
evaluated our approach on a random sample of 30 out of
307 user stories and 3,500out of54,396preprocessed issues.
For this sample, a total of 1,207 matching user story/issue-
pairs were determined. For all pairs, AC were automatically
generated (see Sec. III-C) and evaluated (see Sec. III-D). F ig. 5
presents the resulting number of AC per user story.
0≤10≤20≤30≤50>50051015
410
34 45
Number of ACNumber of User Stories
Fig. 5: Number of generated AC by user story (e-commerce)
The average number of newly generated AC was 35. The
highest number of AC generated for an individual user story
reached 273. We assume that this elevated ﬁgure is attribute d
to the fact that the associated user story includes speciﬁca -
tions related to the accurate presentation of prices, inclu ding
international tax rules, which is a very complex ﬁeld.
We took a random sample of 10 AC for every user story with
more than 10 generated AC and all generated AC for stories
with at most 10 generated AC for manual inspection by four
independent experts. In total, 198 AC for 26 user stories wer e
reviewed.
Experts 1 (E1) and 2 (E2) are seasoned professionals in the
e-commerce sector, with over 20 years of project experience
and 8 years in agile project management. Expert 3 (E3) is
a certiﬁed Professional Scrum Master with additional certi -
ﬁcation in Professional Agile Leadership and seven years of
experience. Expert 4 (E4) has 19 years of working experience
in the software industry as developer, enterprise architec t, agilecoach, people manager, and principal consultant (with mult iple
projects both from the e-commerce and CMS domain).
Every expert reviewed the results independently and de-
termined whether an acceptance criterion constituted a val id
addition to the business scope of the user story, described
edge cases, or covered other non-obvious scenarios. AC that
met the aforementioned conditions were tagged as “approved ”.
Table III presents the results.
TABLE III: AC approval by expert (e-commerce)
E1 E2 E3 E4
Approved AC 165 161 190 154
Declined AC 33 37 8 44
Approval Rate 83% 81% 96% 78%
We consider an AC as accepted by the experts if it receives
positive assessments from at least three out of four evalua-
tors. According to this deﬁnition, 82% of the generated ACs
were approved by the experts. We offer the annotations as a
downloadable dataset.15
To ensure the reliability of our annotations, we calculated
the agreement rate between all annotators as outlined by
Table IV.
TABLE IV: Agreement rate of experts (e-commerce)
E1 E2 E3 E4
E1 100.00% 92.93% 82.32% 77.27%
E2 92.93% 100.00% 81.31% 75.25%
E3 82.32% 81.31% 100.00% 77.78%
E4 77.27% 75.25% 77.78% 100.00%
Average 88.13% 87.37% 85.35% 82.58%
On average, there was an 85.85% consensus among all ex-
perts. We calculated Cohen’s Kappa [63], a commonly utilize d
metric for evaluating inter-rater reliability [64]. Despi te the
high agreement between the raters, Cohen’s Kappa returned
a relatively low value of 0.44, suggesting moderate inter-r ater
reliability [65]. The phenomenon of high agreement, but low
Kappa value is known as Kappa paradox [66]. Consequently,
Gwet’s AC1 [67] was computed as another paradox-resistant
indicator to provide a valid assessment of the inter-rater
reliability. Gwet’s AC1 yielded a value of 0.74, indicating
substantial agreement [65].
A complete example of a user story, its existing acceptance
criteria along with a newly generated and manually approved
criterion is displayed in Fig. 6.
In addition, Fig. 7 presents a counterexample of a generated
acceptance criterion that received approval from the autom ated
assessment but was rejected by all the experts.
B. Secondary Domain: CMS
To evaluate the generalizability of our method in a later
stage of our research, we applied it to the second domain of
CMS.
15https://zenodo.org/records/14709846/ﬁles/Evaluation %20e-commerce.xlsxUser story
As a customer, I would like to register as a private user in the
webshop.
Original acceptance criteria
– The user must enter at least the mandatory ﬁelds for
successful registration.
– The user will be notiﬁed if he forgets to enter mandatory
ﬁelds.
– The user must conﬁrm the e-mail address.
– The user is logged in after conﬁrmation.
– The user can log in and out with his data as soon as the
e-mail has been conﬁrmed.
Newly generated acceptance criterion
Scenario: Register as a private user during checkout and
proceed to conﬁrmation page
GIVEN I am a new customer and I am at the checkout page
WHEN I complete the registration process as a private user
THEN I should be redirected to the checkout conﬁrmation
page instead of the account site
Fig. 6: User story enriched with new relevant acceptance criterion
(e-commerce)
User story
As a punchout-user, I want to order bundle-titles in basket,
split for V AT differences all over the system.
Original acceptance criteria
– Bundles can only be ordered together.
– It should not be possible to delete only one part of bundles.
– It should not be possible to change the amount for only one
part of bundles.
– If the print part is deleted, the online part is also deleted .
– Changing the amount of print part also changes the amount
of the online part.
Newly generated acceptance criterion
Scenario: Ensure correct total price calculation with tax f or
multiple units of a product
GIVEN the tax rate is set to 21%
WHEN the system calculates the total price including tax
THEN the total price should be exactly $222.00
Fig. 7: User story with new acceptance criterion, rejected by all
experts (e-commerce)
We used all 34 user stories from a dataset that was originally
created by Lucassen et al. [60]. All user stories came withou t
additional AC. We provide the user stories for download sinc e
the original link is not available anymore.16
To identify relevant issue trackers, we visited the CMS
usage statistics of BuiltWith [68] and conducted a search fo r
publicly accessible issue trackers that offered a certain n umber
of issues along with an API for downloading. To capture a
broad spectrum of available CMS, we chose to examine issues
from one system located in the top tier and another situated
in the lower tier. As of August, 14th 2024, BuiltWith ranked
16https://zenodo.org/records/14709846/ﬁles/User%20sto ries%20CMS.xlsxMoodle17on position 10 and Umbraco18on position 206 out
of 287 total systems listed.
To download and preprocess the issues for both CMSs, we
reused the same mechanisms as outlined in Section IV-A2.
Table V displays the total number of issues collected and
the ﬁnal count of issues remaining after preprocessing for b oth
projects. We provide the complete dataset of issues featuri ng
both their original and preprocessed text.19
TABLE V: Data collected from issue trackers (CMS)
Project Issued downloaded Issues remaining
Moodle1133,632 55,752
Umbraco215,639 8,748
Total 149,271 64,500
1https://github.com/magento/magento2/issues
2https://github.com/nopSolutions/nopCommerce/issues
We again selected a random sample of 3,500 issues to
maintain consistent parameters with those used for the prim ary
domain. The matching mechanism detailed in Section III-B
resulted in 368 pairs of user stories and issues. Following t he
methodology discussed in Section III-C, AC were generated
for all these pairs and evaluated automatically as describe d
in Section III-D. This way, 163 AC across 28 of the 34 user
stories were found to be beneﬁcial. Fig. 8 details the result ing
number of acceptance criteria per user story.
0 ≤10 ≤20 ≤30 >3001020
624
21 1
Number of ACNumber of User Stories
Fig. 8: Number of generated AC by user story (CMS)
On average, 6 new acceptance criteria were generated. The
maximum number of acceptance criteria produced for a single
user story was 38.
For manual evaluation, a random sample consisting of up
to 10 AC per user story was selected (for those user stories
where there were more than 10 AC generated – otherwise, all
generated AC were considered), resulting in 113 user story/ AC
pairs in total.
The evaluation within the CMS domain was performed by
three experts. Experts E1 and E4 from the e-commerce domain
also had signiﬁcant experience in the CMS domain. Hence,
we asked them again to evaluate our results for the CMS
domain. Furthermore, an additional review was conducted by
17https://moodle.org
18https://umbraco.org
19https://zenodo.org/records/14709846/ﬁles/Issues%20C MS.xlsxexpert E5, a senior developer with 10 years of development
experience across various CMS projects.
Table VI presents the outcome of the individual manual
evaluation.
TABLE VI: AC approval by expert (CMS)
E1 E4 E5
Approved AC 86 86 88
Declined AC 27 27 25
Approval Rate 76.1% 76.1% 77.8%
Again, we consider an acceptance criterion as accepted if it
receives positive assessments from the majority of the expe rts,
speciﬁcally at least two out of the three evaluators.
Following this approach, 80% of the generated ACs were
approved by the experts. We provide the results of the expert s’
assessments as a downloadable dataset.20
The agreement rate between all annotators is shown in
Table VII.
TABLE VII: Agreement rate of experts (CMS)
E1 E4 E5
E1 100.00% 68.14% 76.99%
E4 68.14% 100.00% 80.53%
E5 76.99% 80.53% 100.00%
Average 81.71% 82.89% 85.84%
On average, all experts reached a consensus of 83.5%.
Cohen’s Kappa yielded a value of 0.54 suggesting moderate
inter-rater reliability. Gwet’s AC1 calculated a value of 0 .61,
indicating substantial agreement.
A complete example of a user story taken from the CMS
domain and enriched with a newly generated and approved
criterion is displayed in Fig. 9.
User story
As an editor, I want to crop images, so I can edit images
easily without using photo editing tools, and therefore, I c an
work more productive.
Newly generated acceptance criterion
Scenario: Ensure cropping is centered on the image when an
aspect ratio is set
GIVEN I have uploaded an image in the grid image editor
WHEN I select a speciﬁc aspect ratio and initiate cropping
THEN the cropping tool should automatically center the
crop area on the image instead of starting at the top-left
corner (0,0)
Fig. 9: User story enriched with new relevant acceptance criterion
(CMS)
Again, we provide in Fig. 10 a counterexample of an accep-
tance criterion that was generated and automatically appro ved
but subsequently rejected by all experts.
20https://zenodo.org/records/14709846/ﬁles/Evaluation %20CMS.xlsxUser story
As an editor, I want to crop images, so I can edit images
easily without using photo editing tools, and therefore, I c an
work more productive.
Newly generated acceptance criterion
Scenario: Ensure content properties with “Vary by culture” set
to true are accessible without exceptions
GIVEN a content property is set to “Vary by culture” as true
WHEN the online channel manager accesses this property
THEN the system should allow access without throwing any
exceptions
Fig. 10: User story with new acceptance criterion, rejected by all
experts (CMS)
V. D ISCUSSION AND FUTURE WORK
We have introduced CrUISE-AC as a novel approach to
automatically utilize knowledge in issue trackers and enha nce
user stories with additional acceptance criteria (AC). Thi s
way, software development can be sped up signiﬁcantly as the
amount of missing or incomplete requirements is signiﬁcant ly
reduced. Our experiments show that most of the automaticall y
generated criteria (80–82%) for such stories indeed add use ful
information.
Future work will concentrate on how to increase this number
even more. This includes usage of other LLMs having a
higher number of parameters and evaluation of other prompt
types. Another potential improvement could be assigning an d
applying weights to LLMs during ensemble learning.
When running all user story/issue pairs through ﬁve differ-
ent models, the matching performance demonstrates slow pro -
cessing speeds for extensive issue corpora. In our testing e nvi-
ronment utilizing an NVIDIA GeForce RTX3060, evaluating
30 user stories against 3,500 issues across 5 different mode ls
(totaling 525,000 prompt executions) required approximat ely
50 hours. Therefore, we intend to assess whether a completel y
new model can be trained from scratch using the output of our
LLM ensemble as training data, aiming for similar accuracy
but showing signiﬁcantly improved performance.
Since our approach is the ﬁrst of its kind, we could not
compare our ﬁndings to existing results directly and had to
assess their quality manually. Of course, this process con-
tains some limitations. In either application domain, diff erent
experts reviewed the results to determine whether the AC
could generally serve as a valuable addition to the user stor y.
The experts did not take into account the scope of individual
projects or the entirety of user stories and AC. Similarly, w e
did not evaluate the quality criterion of (interstory) conﬂ ict-
freeness during the assessment.
Hence, assessing our approach through a ﬁeld study and
within actual ongoing projects is essential to determine th e
industrial applicability of CrUISE-AC. With a larger group of
domain experts, product owners, stakeholders, and develop ers,
we will be able to either validate our ﬁndings or gain importa nt
insights for necessary adjustments.From the issues we collected, we utilize only the title,
description, and labels. Nevertheless, other signiﬁcant i nforma-
tion may be present. The discussion surrounding an issue cou ld
encompass or disclose vital aspects of a business requireme nt.
Similarly, we used closed issues only, but did not consider w hy
an issue has been closed. Maybe it was rejected by the project
team because of pointless requirements which will lead to
pointless acceptance criteria subsequently. Therefore, w e plan
to review our issue selection process and possibly consider
issues with a corresponding code commit only.
To conclude, our research advances the state-of-the-art in
requirements engineering by a new fully automated tool chai n
that supports requirements engineers, project managers, a nd
project teams by paving the avenue to crowd knowledge in
available issue trackers.
REFERENCES
[1] J. C. Westland, “The cost behavior of software defects,” Decision
Support Systems , vol. 37, no. 2, pp. 229–238, 2004.
[2] M. Dawson, D. Burrell, E. Rahim, and S. Brewster, “Integr ating software
assurance into the software development life cycle (SDLC), ”Journal of
Information Systems Technology and Planning , vol. 3, pp. 49–53, 2010.
[3] G. Lucassen, F. Dalpiaz, J. M. E. M. V . D. Werf, and S. Brink kemper,
“The use and effectiveness of user stories in practice,” in Proceedings
of the 22nd International Working Conference on Requiremen ts Engi-
neering: Foundation for Software Quality (REFSQ 2016) , ser. LNPSE,
vol. 9619. Springer International Publishing, 2016, pp. 20 5–222.
[4] ISO/IEC/IEEE 29148:2018(E), Systems and software engineering – Life
cycle processes – Requirements engineering . ISO, 2018.
[5] K. Beck, M. Beedle, A. van Bennekum, A. Cockburn, W. Cunni ngham,
M. Fowler, J. Grenning, J. Highsmith, A. Hunt, R. Jeffries, J . Kern,
B. Marick, R. C. Martin, S. Mellor, K. Schwaber, J. Sutherlan d, and
D. Thomas, “Manifesto for agile software development,” 200 1, accessed
on 2024-07-31. [Online]. Available: http://www.agileman ifesto.org/
[6] M. Wynne, A. Hellesoy, and S. Tooke, The Cucumber Book: Behaviour-
Driven Development for Testers and Developers . Pragmatic Bookshelf,
2017.
[7] M. Daneva, E. Van Der Veen, C. Amrit, S. Ghaisas, K. Sikkel , R. Kumar,
N. Ajmeri, U. Ramteerthkar, and R. Wieringa, “Agile require ments
prioritization in large-scale outsourced system projects : An empirical
study,” Journal of Systems and Software , vol. 86, no. 5, pp. 1333–1353,
2013.
[8] V . Gervasi, R. Gacitua, M. Rounceﬁeld, P. Sawyer, L. Kof, L. Ma,
P. Piwek, A. De Roeck, A. Willis, H. Yang, and B. Nuseibeh, “Un -
packing tacit knowledge for requirements engineering,” in Managing
Requirements Knowledge . Springer, 2013, pp. 23–47.
[9] I. Morales-Ramirez and L. H. Alva-Martinez, “Requireme nts analysis
skills: How to train practitioners?” in 2018 IEEE 8th International Work-
shop on Requirements Engineering Education and Training (R EET) .
IEEE, 2018.
[10] M. Fredericks and V . Basili, “Using defect tracking and analysis to
improve software quality,” IBM Journal of Research and Development ,
vol. 19, no. 10, pp. 23–26, 1998.
[11] A. Rasheed, B. Zafar, T. Shehryar, N. A. Aslam, M. Sajid, N. Ali,
S. H. Dar, and S. Khalid, “Requirement engineering challeng es in agile
software development,” Mathematical Problems in Engineering , vol.
2021, pp. 1–18, 2021.
[12] P. C. Sen, M. Hajra, and M. Ghosh, “Supervised classiﬁca tion algorithms
in machine learning: A survey and review,” in Proceedings of the
International Conference on Emerging Technology in Modell ing and
Graphics (IEM Graph 2018) , ser. AISC, vol. 937. Springer, 2020,
pp. 99–111.
[13] W. Wang, V . W. Zheng, H. Yu, and C. Miao, “A survey of zero- shot
learning: Settings, methods, and applications,” ACM Transactions on
Intelligent Systems and Technology , vol. 10, no. 2, pp. 1–37, Jan. 2019.
[14] R. Polikar, “Ensemble learning,” in Ensemble Machine Learning: Meth-
ods and Applications . Springer, 2012, pp. 1–34.[15] D. M´ endez Fern´ andez, W. B¨ ohm, A. V ogelsang, J. Mund, M. Broy,
M. Kuhrmann, and T. Weyer, “Artefacts in software engineeri ng: a
fundamental positioning,” Software & Systems Modeling , vol. 18, pp.
2777–2786, 2019.
[16] M. Cohn, Succeeding with agile: software development using Scrum .
Pearson Education, 2010.
[17] S. K. McGrath and S. J. Whitty, “Stakeholder deﬁned,” International
Journal of Managing Projects in Business , vol. 10, no. 4, pp. 721–748,
2017.
[18] C. M. Henein and T. White, “Information in crowds: The sw arm infor-
mation model,” in Proceedings of the 7th International Conference on
Cellular Automata, for Research and Industry (ACRI 2006) . Springer,
2006, pp. 703–706.
[19] E. C. Groen, J. Doerr, and S. Adam, “Towards crowd-based requirements
engineering,” in Proceedings of the 21st International Working Confer-
ence on Requirements Engineering: Foundation for Software Quality
(REFSQ 2015) , ser. LNPSE, vol. 9013. Springer, 2015, pp. 247–253.
[20] T. Johann, C. Stanik, A. M. Alizadeh B., and W. Maalej, “S AFE:
A simple approach for feature extraction from app descripti ons and
app reviews,” in Proceedings of the 25th International Requirements
Engineering Conference (RE 2017) . IEEE, 2017, pp. 21–30.
[21] Q. Motger, A. Miaschi, F. Dell’Orletta, X. Franch, and J . Marco, “T-
FREX: A transformer-based feature extraction method from m obile app
reviews,” in Proceedings of the International Conference on Software
Analysis, Evolution and Reengineering (SANER 2024) . IEEE, 2024,
pp. 227–238.
[22] H. Wu, W. Deng, X. Niu, and C. Nie, “Identifying key featu res from
app user reviews,” in Proceedings of the 43rd International Conference
on Software Engineering (ICSE 2021) . IEEE, 2021, pp. 922–932.
[23] L. Shi, C. Chen, Q. Wang, and B. Boehm, “Automatically de tecting fea-
ture requests from development emails by leveraging semant ic sequence
mining,” Requirements Engineering , vol. 26, no. 2, pp. 255–271, 2021.
[24] S. Abualhaija, C. Arora, M. Sabetzadeh, L. C. Briand, an d M. Traynor,
“Automated demarcation of requirements in textual speciﬁc ations: a
machine learning-based approach,” Empirical Software Engineering ,
vol. 25, no. 6, pp. 5454–5497, 2020.
[25] S. Tiwari, D. Ameta, and A. Banerjee, “An approach to ide ntify use
case scenarios from textual requirements speciﬁcation,” i nProceedings
of the 12th Innovations on Software Engineering Conference (ISEC ’19) .
ACM, 2019, pp. 1–11.
[26] N. Ngaliah, D. Siahaan, and I. K. Raharjana, “User story extraction
from online news with featurebased and maximum entropy meth od for
software requirements elicitation,” IPTEK The Journal for Technology
and Science , vol. 32, no. 3, p. 125, 2022.
[27] K. R. Chowdhary, “Natural language processing,” in Fundamentals of
Artiﬁcial Intelligence . Springer, 2020, pp. 603–649.
[28] L. Montgomery, C. L¨ uders, and W. Maalej, “Mining issue trackers:
Concepts and techniques,” arXiv preprint arXiv:2403.05716 , 2024.
[29] R. Kallis, A. Di Sorbo, G. Canfora, and S. Panichella, “T icket tagger:
Machine learning driven issue classiﬁcation,” in Proceedings of the In-
ternational Conference on Software Maintenance and Evolut ion (ICSME
2019) . IEEE, 2019, pp. 406–409.
[30] N. Pandey, D. K. Sanyal, A. Hudait, and A. Sen, “Automate d classi-
ﬁcation of software issue reports using machine learning te chniques:
An empirical study,” Innovations in Systems and Software Engineering ,
vol. 13, pp. 279–297, 2017.
[31] M. L. Siddiq and J. C. Santos, “BERT-based GitHub issue r eport
classiﬁcation,” in Proceedings of the 1st International Workshop on
Natural Language-based Software Engineering (NLBSE 2022) . ACM,
2022, pp. 33–36.
[32] H. Ruan, B. Chen, X. Peng, and W. Zhao, “DeepLink: Recove ring issue-
commit links based on deep learning,” Journal of Systems and Software ,
vol. 158, p. 110406, 2019.
[33] K. A. Hambarde and H. Proenca, “Information retrieval: recent advances
and beyond,” IEEE Access , 2023.
[34] A. Singhal, “Modern information retrieval: A brief ove rview,” in IEEE
Data Engineering Bulletin . IEEE, 2001, vol. 24, no. 4, pp. 35–43.
[35] T. Roelleke and J. Wang, “TF-IDF uncovered: a study of th eories
and probabilities,” in Proceedings of the 31st Annual International
ACM SIGIR Conference on Research and Development in Informa tion
Retrieval . ACM, 2008, pp. 435–442.
[36] D. Cer, Y . Yang, S.-y. Kong, N. Hua, N. Limtiaco, R. St. Jo hn,
N. Constant, M. Guajardo-Cespedes, S. Yuan, C. Tar, B. Strop e, and
R. Kurzweil, “Universal sentence encoder for English,” in Proceedingsof the 2018 Conference on Empirical Methods in Natural Langu age
Processing: System Demonstrations . Association for Computational
Linguistics, 2018, pp. 169–174.
[37] N. Reimers and I. Gurevych, “Sentence-BERT: Sentence e mbeddings
using siamese BERT-networks,” in Proceedings of the Conference on
Empirical Methods in Natural Language Processing (EMNLP 20 19).
Association for Computational Linguistics, 2019.
[38] L. Rokach and O. Maimon, “Clustering methods,” in Data Mining and
Knowledge Discovery Handbook . Springer, 2005, pp. 321–352.
[39] E. Schubert, J. Sander, M. Ester, H. P. Kriegel, and X. Xu , “DBSCAN
revisited, revisited: why and how you should (still) use DBS CAN,” ACM
Transactions on Database Systems (TODS) , vol. 42, no. 3, pp. 1–21,
2017.
[40] M. Cone, “The markdown guide,” 2024, accessed on 2024-0 7-31.
[Online]. Available: https://www.markdownguide.org/
[41] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. M atena,
Y . Zhou, W. Li, and P. J. Liu, “Exploring the limits of transfe r learning
with a uniﬁed text-to-text transformer,” Journal of Machine Learning
Research , vol. 21, no. 140, pp. 1–67, 2020.
[42] T. Obinwanne and P. Brandtner, “Enhancing sentiment an alysis with
GPT-A comparison of large language models and traditional m achine
learning techniques,” in Proceedings of the World Conference on Smart
Trends in Systems, Security, and Sustainability (WS4 2023) , ser. LNNS,
vol. 803. Springer, 2023, pp. 187–197.
[43] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. D hariwal,
A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A . Herbert-
V oss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Zieg ler,
J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gr ay,
B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. S utskever,
and D. Amodei, “Language models are few-shot learners,” in Pro-
ceedings of the 34th International Conference on Neural Inf ormation
Processing Systems (NIPS ’20) . Curran Associates Inc., 2020, pp.
1877–1901.
[44] M. Mohandes, M. Deriche, and S. O. Aliyu, “Classiﬁers co mbination
techniques: A comprehensive review,” IEEE Access , vol. 6, pp. 19 626–
19 639, 2018.
[45] Y . Bi, J. Guan, and D. Bell, “The combination of multiple classiﬁers
using an evidential reasoning approach,” Artiﬁcial Intelligence , vol. 172,
no. 15, pp. 1731–1751, 2008.
[46] R. Delgado, “A semi-hard voting combiner scheme to ense mble multi-
class probabilistic classiﬁers,” Applied Intelligence , vol. 52, no. 4, pp.
3653–3677, 2022.
[47] J. Kittler, M. Hatef, R. P. Duin, and J. Matas, “On combin ing classi-
ﬁers,” IEEE Transactions on Pattern Analysis and Machine Intellig ence,
vol. 20, no. 3, pp. 226–239, 1998.
[48] C. Fang, X. Li, Z. Fan, J. Xu, K. Nag, E. Korpeoglu, S. Kuma r, and
K. Achan, “LLM-ensemble: Optimal large language model ense mble
method for e-commerce product attribute value extraction, ” in Proceed-
ings of the 47th International ACM SIGIR Conference on Resea rch and
Development in Information Retrieval . ACM, 2024, pp. 2910–2914.
[49] P. Heck and A. Zaidman, “A systematic literature review on quality
criteria for agile requirements speciﬁcations,” Software Quality Journal ,
vol. 26, no. 1, pp. 127–160, 2018.
[50] G. Lucassen, F. Dalpiaz, J. M. E. M. Van Der Werf, and S. Br inkkemper,
“Improving agile requirements: the quality user story fram ework and
tool,” Requirements Engineering , vol. 21, no. 3, pp. 383–403, 2016.
[51] Google, “Compact language detector 3 (cld3),” 2021, ac cessed on
2024-07-31. [Online]. Available: https://github.com/go ogle/cld3
[52] Z. Liu, W. Lin, Y . Shi, and J. Zhao, “A robustly optimized BERT pre-
training approach with post-training,” in Proceedings of Chinese Com-
putational Linguistics (CCL 2021) , ser. LNAI, vol. 12869. Springer,
2021, pp. 471–484.
[53] S. Hochreiter and J. Schmidhuber, “Long short-term mem ory,” Neural
Computation , vol. 9, no. 8, pp. 1735–1780, 1997.
[54] K. L. Tan, C. P. Lee, K. S. M. Anbananthen, and K. M. Lim, “R oBERTa-
LSTM: A hybrid model for sentiment analysis with transforme r and
recurrent neural network,” IEEE Access , vol. 10, pp. 21 517–21 525,
2022.
[55] R. Mohawesh, H. B. Salameh, Y . Jararweh, M. Alkhalaileh , and
S. Maqsood, “Fake review detection using transformer-base d enhanced
LSTM and RoBERTa,” International Journal of Cognitive Computing
in Engineering , vol. 5, pp. 250–258, 2024.[56] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-
training of deep bidirectional transformers for language u nderstanding,”
arXiv:1810.04805 , 2018.
[57] Z. Yang, Z. Dai, Y . Yang, J. Carbonell, R. R. Salakhutdin ov, and
Q. V . Le, “XLNet: Generalized autoregressive pretraining f or language
understanding,” arXiv:1906.08237 , 2019.
[58] Ollama, “Ollama - most popular models,” 2024, accessed on 2024-07-
31. [Online]. Available: https://ollama.com/library?so rt=popular
[59] OpenAI, “GPT-4 Turbo,” 2024, accessed on 2024-07-31. [ Online]. Avail-
able: https://platform.openai.com/docs/models/gpt-4- turbo-and-gpt-4
[60] G. Lucassen, F. Dalpiaz, J. M. E. van der Werf, and S. Brin kkemper,
“Visualizing user story requirements at multiple granular ity levels via
semantic relatedness,” in Proceedings of the 35th International Confer-
ence on Conceptual Modeling (ER 2016) . Springer, 2016, pp. 463–478.
[61] DeepL, “DeepL translator,” 2024, accessed on 2024-07- 31. [Online].
Available: https://www.deepl.com
[62] BuiltWith, “eCommerce usage distribution in the top 1 m illion
sites,” 2023, accessed on 2023-08-03. [Online]. Available :
https://trends.builtwith.com/shop
[63] J. Cohen, “A coefﬁcient of agreement for nominal scales ,”Educational
and Psychological Measurement , vol. 20, no. 1, pp. 37–46, 1960.
[64] A. J. Viera, J. M. Garrett et al. , “Understanding interobserver agreement:
the kappa statistic,” Family Medicine , vol. 37, no. 5, pp. 360–363, 2005.
[65] J. Landis, “The measurement of observer agreement for c ategorical
data,” Biometrics , 1977.
[66] A. R. Feinstein and D. V . Cicchetti, “High agreement but low kappa:
I. the problems of two paradoxes,” Journal of Clinical Epidemiology ,
vol. 43, no. 6, pp. 543–549, 1990.
[67] K. L. Gwet, Handbook of inter-rater reliability: The deﬁnitive guide t o
measuring the extent of agreement among raters . Advanced Analytics,
LLC, 2014.
[68] BuiltWith, “CMS usage distribution in the top 1 million
sites,” 2024, accessed on 2024-08-14. [Online]. Available :
https://trends.builtwith.com/cms