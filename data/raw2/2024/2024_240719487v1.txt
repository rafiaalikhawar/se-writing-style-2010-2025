RLCoder: Reinforcement Learning for
Repository-Level Code Completion
Yanlin Wang1, Yanli Wang1, Daya Guo1, Jiachi Chen1‚àó, Ruikai Zhang2, Yuchi Ma2, Zibin Zheng1
1Sun Yat-sen University, Zhuhai, China
{wangylin36, chenjch86, zhzibin }@mail.sysu.edu.cn, {wangyli58, guody5 }@mail2.sysu.edu.cn,
2Huawei Cloud Computing Technologies Co., Ltd., Shenzhen, China
{zhangruikai1, mayuchi1 }@huawei.com
Abstract ‚ÄîRepository-level code completion aims to generate
code for unfinished code snippets within the context of a
specified repository. Existing approaches mainly rely on retrieval-
augmented generation strategies due to limitations in input
sequence length. However, traditional lexical-based retrieval
methods like BM25 struggle to capture code semantics, while
model-based retrieval methods face challenges due to the lack
of labeled data for training. Therefore, we propose RLCoder, a
novel reinforcement learning framework, which can enable the
retriever to learn to retrieve useful content for code completion
without the need for labeled data. Specifically, we iteratively eval-
uate the usefulness of retrieved content based on the perplexity
of the target code when provided with the retrieved content as
additional context, and provide feedback to update the retriever
parameters. This iterative process enables the retriever to learn
from its successes and failures, gradually improving its ability to
retrieve relevant and high-quality content. Considering that not
all situations require information beyond code files and not all
retrieved context is helpful for generation, we also introduce a
stop signal mechanism, allowing the retriever to decide when to
retrieve and which candidates to retain autonomously. Extensive
experimental results demonstrate that RLCoder consistently
outperforms state-of-the-art methods on CrossCodeEval and
RepoEval, achieving 12.2% EM improvement over previous
methods. Moreover, experiments show that our framework can
generalize across different programming languages and further
improve previous methods like RepoCoder.
Index Terms ‚ÄîRepository-Level Code Completion, Reinforce-
ment Learning, Perplexity, Stop Signal Mechanism
I. I NTRODUCTION
With the advancement of large language models for code
(code LLMs) [1]‚Äì[5], code completion has emerged as one of
the most important features in integrated development environ-
ments (IDEs) [6]‚Äì[11]. However, due to the vast size of code
repositories and the limitations of context length in models,
repository-level code completion, which involves generating
code suggestions within the context of an entire repository,
cannot practically leverage the entire repository directly as
context [12]. Therefore, previous works [12]‚Äì[17] typically
employ a retrieval-augmented-generation (RAG) strategy. In
this approach, the unfinished code in the current file serves
as a query to retrieve code candidates from the entire repos-
itory, providing cross-file context. These candidates are then
concatenated with the unfinished code before being fed into
code LLMs. To retrieve relevant code snippets from other files,
* Corresponding authorvarious retrievers are adopted. RepoFuse [15] uses lexical-
based method BM25 [18] as the retriever to retrieve code
snippets that are textually similar with the unfinished code.
RepoCoder [13] and RepoHyper [19] use the model-based
approach that encodes code candidates and unfinished code
into vectors and employs dense retrieval to find similar codes.
Although these efforts have shown promising performance
in repository-level code generation, we have identified the
following problems in retrieval.
P1 Labeled Data Dependency. Lexical-based methods such
as BM25 [18] cannot capture code semantics, while
model-based methods [12], [19], [20] are capable of
understanding code semantics but are hampered by the
lack of ground-truth candidate data for training. This
labeled data is hard to obtain, as it requires significant
effort in data parsing and expert labeling, limiting its
generalizability.
P2 Candidate Construction Issue. Previous methods of
code candidate construction mainly employ the fixed
window strategy [13] or dependency parsing [12], [21].
However, the fixed window strategy may disrupt the con-
tinuity of the code. Methods based on dependency parsing
can only focus on limited context in the dependency
graph and can not be applied to complex scenarios.
P3 Non-Selective Retrieval. Previous studies typically di-
rectly retrieve several candidates to serve as the context
for generation, neglecting when to retrieve and which
candidates to retain. Unnecessary candidates can detract
from the performance in completion scenarios that do not
require repository context.
In this paper, we propose RLC ODER , a reinforcement learn-
ing framework for repository-level code completion to address
the aforementioned problems. Firstly, we propose a code-
base construction pipeline with a simple yet effective Split-
Aggregate strategy. This approach allows better code continu-
ity of the candidates, which we refer to as natural candidates
(addressing P2). Secondly, during the training stage, we di-
verge from supervised learning methods that depend on labeled
data. Instead, we train a retriever named RLRetriever that
learns what to retrieve based on feedback from a specifically
designed evaluator, without needing labeled data (addressing
P1). Specifically, we iteratively evaluate the usefulness ofarXiv:2407.19487v1  [cs.SE]  28 Jul 2024retrieved content based on the perplexity of the target code
when provided with the retrieved content as additional context,
and provide feedback to update the retriever parameters,
which enables the retriever to learn from its successes and
failures, gradually improving its ability to retrieve relevant
and high-quality content. Moreover, to mitigate hallucinations
often observed in repository-level code completions, typically
due to incorrect identifier or API usage [16], we design a
weighted perplexity (PPL) mechanism that allocates higher
weights to certain important tokens in perplexity calculation.
Furthermore, considering that not all candidates retrieved are
useful for generation, we introduce a stop signal mechanism
to evaluate the usefulness of candidates, allowing the retriever
to decide when to retrieve and which candidates to retain
autonomously (addressing P3). Finally, in the inference stage,
given an unfinished code as input, RLCoder retrieves natural
candidates from the codebase, retains the useful candidates,
and then feeds them along with the unfinished code into the
generator (a backbone LLM) for target code generation.
We evaluate RLCoder with extensive experiments with
several LLMs on CrossCodeEval [22] and RepoEval [13].
Experimental results show that our framework achieves 12.2%
improvement of Exact Match compared with previous meth-
ods. Furthermore, RLCoder demonstrates high generalizabil-
ity, showing effectiveness across various LLMs and program-
ming languages. Additionally, experiments show that RLCoder
can be integrated into previous methods such as RepoCoder
to enhance code completion performance further.
Our main contributions are:
‚Ä¢We propose RLCoder, a reinforcement learning frame-
work for repository-level code completion. To our knowl-
edge, we are the first to train the retriever without labeled
data for repository-level code completion. Besides, we
design a mechanism that uses the weighted perplexity
of the target code as the reward to further enhance
performance.
‚Ä¢We introduce a simple yet effective Split-Aggregate
candidate construction strategy based on human pro-
gramming habits. This method avoids the disruption of
code continuity and outperforms fixed window candidates
indicated by the experimental results.
‚Ä¢We propose a stop signal mechanism to evaluate the
usefulness of candidates and discard useless candidates
for more effective code completion.
‚Ä¢We perform an extensive evaluation of RLCoder. Ex-
perimental results show that RLCoder outperforms the
state-of-the-art methods and demonstrates generalizabil-
ity and applicability. We provide the code and data at
https://github.com/DeepSoftwareAnalytics/RLCoder.
II. B ACKGROUND
A. Retrieval-Augmented Generation
Retrieval-augmented generation (RAG) [23] is an approach
that enhances the quality of generation by retrieving from
external knowledge bases. This method includes three keycomponents [24]: retriever ,generator , and augmentation tech-
niques . The retriever is used to find relevant information from
a large-scale dataset or knowledge base, including pertinent
documents, facts, or text snippets that are relevant to the
input query or prompt. The retrieved information is fed into
thegenerator , which integrates this external knowledge into
the generation stage. Augmentation techniques focus on how
retrieved information is integrated into the generation process.
To formalize the RAG process, consider a scenario where we
want to generate code based on a query qand a set of retrieved
candidates {c1, c2, ..., c n}. The process can be described by the
following formula:
Code =Generate (q,Retrieve (q,{c1, c2, ..., c n})) (1)
where the Retrieve (¬∑)function selects the most relevant
candidates based on the query qfrom the candidate set
{c1, c2, ..., c n}, and the Generate (¬∑)function then takes the
query and the retrieved candidates to generate the target code.
In recent years, researchers have conducted a substantial
amount of research related to RAG, highlighting its promising
potential for future applications [23], [25]‚Äì[28]. Many studies
have utilized RAG for code-related research [29]‚Äì[42]. In
repository-level code completion, due to the massive amount
of code in the repository and limited context of generator [12],
it is impractical to use the entire repository as the context for
generation. Therefore, most current methods employ the RAG
method to retrieve suitable candidates from the repository for
generation [12], [13], [16], [17].
    in_regex: Pattern[str],    in_style: _styles.Style,    out_style: _styles.Style,  ) -> None:    """    Initializes the :class:`.Converter` instance.    """    self._escape_start = len(escape_char) if escape_char is not None else 0    """    *_escape_start* (:class:`int`) is the offset used to  skip the escapecharacter.    """    self._expand_tuples: bool = expand_tuples    """    *_expand_tuples* (:class:`bool`) is whether to convert tuples into a    sequence of parameters.    """class Converter(object):  """  The :class:`.Converter` class is the base class for implementing the  conversion from one in-style parameter to another out-style parameter.  """  def __init__(    self,    escape_char: Optional[str],    expand_tuples: bool,Code Continuity is Disrupted!Candidate 1
Candidate 2
Candidate 3
Fig. 1. Using fixed window candidates may disrupt the continuity of code
semantics, resulting in the definition of functions being split across different
code snippets.
B. Repository-Level Code Completion
Traditional code completion [1], [43] usually focused on
generating code with in-file context. With the development
of LLMs [3]‚Äì[5], [44], repository-level code completion is
gradually gaining attention as it better reflects real-world# neo4j-python-driver/src/neo4j/_data.py
...
defkeys(self) ->t.List[str]:
""" Return the keys of the record.
:returns: list of key names
"""
returnlist(self.__keys)
defvalues(self, *keys: _K) -> t.List[t.Any]:
""" Return the values of the record, optionally filtering to
include only certain values by index or key.
:param keys: indexes or keys of the items to include; if none
are provided, all values will be included
:returns: list of values
""" Query (Unfinished Code)
ifkeys:
return[self[key] forkeyinkeys]
else:
returnlist(self) Generation without Retrieval 
ifnotkeys:
returnlist(self.__values)
return[self[key] forkeyinkeys]# neo4j-python-driver/src/neo4j/_sync/work/result.py
...
def values(
self, *keys: _ TResultKey
) -> t.List[t.List[t.Any]]:
"""Return the remainder of the result asa listof values lists.
:param keys: fields to returnforeach remaining record. 
Optionally filtering to include only certain values by index orkey.
:returns: listof values lists
:raises ResultConsumedError : ifthe transaction fromwhich this result
was obtained has been closed orthe Result has been explicitly
consumed.
Retrieved Context
Generation with Retrieval Attribute Does Not Exist!
Fig. 2. Due to the limitations of LLMs, inappropriate retrieval can mislead
generation, resulting in attempts to call an non-existent attribute.
scenarios [12], [13], [22], [45]. To formalize repository-level
code completion, we conceptualize the process as selecting the
most relevant snippets (candidates) from a code repository and
generating code based on the query. This can be encapsulated
in a formula as follows:
Code =Generate (q,Retrieve (q,codebase )) (2)
where qrepresents the query or the prompt for code com-
pletion. Codebase symbolizes code snippets from the code
repository. Retrieve (q,codebase )is the function that selects the
most relevant code snippets (candidates) from the repository
based on the query q. Generate (q,candidates )is the generation
function that generates the target code based on the query q
and the selected candidates.
Previous work highlights the importance of integrating both
the in-file and cross-file context in repository-level code com-
pletion [12]. This implies that the model needs to understand
not only the local context but also third-party libraries and
global modules [21]. Fusing analogy context and rationale
context can greatly ensure the integrity of the retrieval code-
base [15]. Iterative retrieval and generation method [13],
[16] involves concatenating the results generated from the
previous iteration with the prior context to form a query. This
query is then used for the subsequent round of retrieval and
generation. Additionally, agents [17], [46]‚Äì[48] that assist in
code completion through invoking tools or collaborating with
each other is also a remarkable approach.
RLRetriever
Evaluatorùíá(ùíô)Weighted Perplexity RewardUnfinished Code
...RetrivedCodesCodebase</>TrainingGitHubRepos
Unfinished CodeCodebase</>InferenceCurrentRepo
GeneratorRLRetriever
...RetrivedCodesTargetCode
Fig. 3. Overview of RLCoder.
Limitations: There are still some issues that need to be ad-
dressed for current repository-level code completion methods.
First , the lack of labeled data limits the generalizability of
many learning-based approaches. For example, CoCoMIC [12]
can only be used in trained repository and struggles to expand
to other languages and repositories. RepoHyper [19] uses a
subset of the benchmark as training data and sets the gold can-
didate as label. Second , previous works mostly adopted fixed
window candidates [13] or candidates based on dependency
parsing [12]. Methods based on dependency parsing only
consider the nodes in the dependency parse graph, neglecting
other code in the repository. This can lead to omitting many
potentially useful code pieces during retrieval. Methods using
fixed window candidates, as shown in Figure 1, may split
the signature of ‚Äú __init__ ‚Äù function into two different
candidates. This situation may lead to the retriever fetching the
required code snippet without capturing the full parameter list.
As a result, this partial information could confuse the generator
leading to incorrect function calls. Third , current work lacks
an evaluation of the necessity for candidates. As illustrated
in Figure 2, the task can be correctly completed using only
the in-file preceding context. However, if the context retrieved
is blindly used, it may mislead the generation results. In
this case, there is a function definition for ‚Äú keys ‚Äù in the
unfinished code, which calls the ‚Äú __keys ‚Äù attribute. The code
snippet retrieved happens to have a function definition for
‚Äúvalues ‚Äù, leading the model to mistakenly believe there is
a corresponding ‚Äú __values ‚Äù attribute defined, thus calling
a non-existent ‚Äú __values ‚Äù attribute during code generation.
III. M ETHODOLOGY
A. Overview
In this section, we introduce RLCoder, a reinforcement
learning framework for repository-level code completion. The
overview of RLCoder is shown in Figure 3, comprising
two stages: training and inference. In the training stage, the
major objective is to train the retriever RLRetriever, the key
component of our framework. First, to train RLRetriever, we
construct data from repositories collected from GitHub andFilter
Code RepositoriesDependency
Analysis 
Code Clusters
Candidates
Target Code
ÔÅµÔÅ∂Fig. 4. Data construction pipeline.
obtain unfinished code, target code, and candidate codebase.
Then, RLRetriever will retrieve from the candidate codebase
using unfinished code as the query. Finally, the retrieved code
candidates will be evaluated by the evaluator and obtain the
weight perplexity reward to update the parameters of RLRe-
triever. Through repeated iterations, RLRetriever enhances its
retrieval capability via continuous feedback and learning. In
the inference stage, given unfinished code and the current
repository context, we first construct codebase from current
repository. Then, we use the RLRetriever trained in the training
stage to retrieve from the codebase using the unfinished code.
Finally, we use the retrieved codes as context to concatenate
with the unfinished code and feed them into the generator for
target code generation.
B. Data Construction
Repository-level code completion tasks typically refer to
generating partial code within the given repository code
context, generally including a line, an API, or a part of a
function body [13]. To ensure the retriever we train meets
the requirements of repository-level code retrieval, we need to
simulate such scenarios. Figure 4 shows the pipeline of our
data construction process, which includes the following steps:
repository filtering, dependency analysis, target code selection,
and candidate construction.
1) Repository Selection: We randomly select 10,000 large-
scale Python and Java repositories from GitHub that were cre-
ated before March 2023 and meet the following requirements:
(1) have cross-file dependencies for constructing our training
dataset; and (2) not included in well-known benchmarks such
as CrossCodeEval [22] and RepoEval [13], which are used in
our evaluation. This filtering process aims at preventing po-
tential data leakage, ensuring the reliability of the evaluation.
2) Dependency Analysis: To ensure our training data con-
tains a substantial quantity of cross-file context dependencies,
we implement a dependency analysis for each repository. The
methodology is outlined in Algorithm 1. Specifically, to get
the code files that are related to each other, we analyze
theimport statements within code files and construct a
dependency graph that represents the relationships between
these files. Based on whether dependencies exist between
code files, we categorize them into clusters of interdependent
code files. Through this process, we obtain 27,919 Python
file clusters and 41,647 Java file clusters. We eliminate any
cluster that contains only a single file. For clusters comprising
multiple files, we employ a topological sorting based on the
in-degree and out-degree of files. This means that, aside fromAlgorithm 1 Dependency Analysis and Clustering.
Require: Set of code files F
Ensure: Clusters of interdependent code files Clusters
G‚ÜêConstructDependencyGraph (F)
Clusters ‚ÜêIdentifyClusters (G)
for all cluster inClusters do
ifSize(cluster ) == 1 then
Clusters ‚ÜêClusters ‚àí {cluster }
else
SortedCluster ‚ÜêTopologicalSort (cluster )
Update cluster inClusters withSortedCluster
end if
end for
return Clusters
the first file, each file in the cluster will contain code segments
that depend on one or more of other files.
3) Target Code Selection: Within the clusters of interde-
pendent code files, we designate files other than the first file
as the ones to be completed. We select a random position
within these files, excluding the beginning and end to ensure
ample context for the code to be completed. This position
serves as the starting point for the target code segment that
needs completion. To formalize this process, we define the
target code segment to be masked and completed as Ctarget ,
starting from position pstart with length l, where pstart is
chosen randomly within the constraints mentioned above. The
selection of pstart can be expressed as:
pstart =Random (pmin, pmax) (3)
where pmin andpmax define the permissible range within
the file, excluding the very beginning and ending segments
to ensure sufficient context. The length lof the target code
Ctarget is also determined randomly, with the constraint that
the entire segment Ctarget must lie within the boundary of the
code file:
Ctarget =C[pstart :pstart +l] (4)
After identifying Ctarget , we mask this segment within the file
to simulate an unfinished code scenario that needs completion.
Upon masking Ctarget , the segment designated for completion,
we intentionally exclude the file containing the masked code
when assembling candidates. To formalize this concept, we
define a binary selection function for candidate files as S(fi),
where firepresents a candidate file:
S(fi) =(
0ifCtarget‚ààfi,
1otherwise(5)
4) Candidate Construction: Unlike previous works that uti-
lized fixed window candidates [13], [16] or candidates parsed
from dependencies [12], we propose a simple yet effective
Split-Aggregate candidate construction strategy inspired by
human programming habits. We term these candidates as natu-
ral candidates. Specifically, programmers often write code with
continuous semantic information together, using blank lines asseparators to facilitate readability. As shown in the left part of
Figure 5, code and its corresponding comments are usually not
separated by blank lines. In fact, blank lines are usually used to
separate code snippets with different semantics and usage. This
practice naturally forms continuous code segments. The Split-
Aggregate strategy is outlined in Algorithm 2. Specifically,
we divide the code in a file into several mini-blocks based
on blank lines and then aggregate these mini-blocks into
candidates by a certain length. During aggregation, the mini-
blocks are concatenated to form candidates in such a way that
the length of any candidate does not exceed a preset threshold
value T.
Algorithm 2 Split-Aggregate Strategy
Require: Code File F, Threshold T
Ensure: Candidate set C
Blocks ‚ÜêSplitIntoBlocks (F)
C‚Üê ‚àÖ
for all block inBlocks do
ifLineCount (block )< T then
Aggregate ‚Üêblock
while LineCount (Aggregate )< T andblock Ã∏=
Last(Blocks )do
block‚ÜêNext(block )
Aggregate ‚ÜêAggregate +block
end while
C‚ÜêC‚à™ {CreateCandidate (Aggregate )}
else
C‚ÜêC‚à™SplitBlock (block, T )
end if
end for
return C
C. Reinforcement Learning-based RLRetriever Training
1) Design of Reward: For reinforcement learning, reward
is feedback from the external environment that assists a model
in learning specific capabilities based on the feedback. In the
scenario of repository-level code completion, the most intuitive
indicator of reward is whether the generated code can be
executed to obtain the expected results. However, obtaining
feedback through actual execution is difficult. On the one
hand, it‚Äôs challenging to set up the execution environment
for repository code. Even if the execution environment is
established, execution can be time-consuming, and there may
be a lack of corresponding test cases to evaluate the accuracy
of the execution results.
In the context of repository-level code completion, our
primary aim is to identify the optimal candidate cfrom a
set of possibilities that maximizes the likelihood of accurately
generating the target code sequence ygiven the contextual
information x. This objective can be formally articulated as:
max
cP(y|x, c) (6)It is evident that this maximization is equivalent to minimizing
the negative log-likelihood:
min
c‚àílogP(y|x, c) (7)
Perplexity (PPL), a standard measure for evaluating the pre-
dictive performance of probabilistic models, is defined as the
exponential of the average negative log-likelihood (NLL) over
a sequence. Minimizing NLL thereby directly corresponds to
minimizing the perplexity of the target code sequence y:
min
cPPL (y|x, c) =e‚àí1
NPN
i=1logP(yi|x,c,y <i)(8)
where yirepresents the i-th token in the target code sequence
y.
In the domain of code completion, the first few tokens
generated play a pivotal role in shaping the entire output.
Considering this, we give more attention to the first few tokens.
Besides, errors in repository-level code completion often occur
due to hallucinations caused by a lack of understanding of the
entire repository, such as generating incorrect or non-existent
APIs. Therefore, we assign a higher focus on the identifier
tokens. To further refine the model‚Äôs focus, we introduce a
weighted variant PPL w:
PPL w(y|x, c) =e‚àí1PN
i=1wiPN
i=1wi¬∑logP(yi|x,c,y <i)(9)
The weight wifor each token of the target code is determined
by a function that considers the token‚Äôs position in the se-
quence and whether it is an identifier, which can be represented
as:
wi=Ô£±
Ô£¥Ô£≤
Ô£¥Ô£≥wfirst ifi‚â§k,
wapi ifyi‚ààAPIs,
1 otherwise(10)
where the first ktokens are assigned by a weight wfirst to
reflect their significant impact on the overall quality of the
generated code. If the i-th token is part of an API or an
identifier, it is assigned a weight wapito acknowledge the
importance of accurate and contextually appropriate identifiers
in code completion.
We define the reward for choosing a particular candidate ci,
cjfrom the set of all candidates Cas follows:
r(ci) =(
1ifPPL w(ci)‚â§PPL w(cj),‚àÄcj‚ààC,
0otherwise(11)
where PPL w(ci)denotes the weighted perplexity of the
target code given candidate ci, serving as an abbreviation for
PPL w(y|x, ci). The reward r(ci), equivalently referred to as
reward (ci, x, C )in the formulations, is assigned a value of
1 if the candidate ciexhibits a PPL that is equal to or lower
than that of any other candidate in the set C. Conversely, a
reward of 0 is allocated to ciif it fails to meet this criterion.
Building on the concept of this reward mechanism, we
further define our objective function, L, as an aggregation
of the logarithmic probabilities of choosing each candidate,class Converter(object):  """  The :class:`.Converter` class is the base class for implementing theconversion from one in-styleparameter to another out-style parameter.  """  def __init__(    self,    escape_char: Optional[str],    expand_tuples: bool,    in_regex: Pattern[str],    in_style: _styles.Style,    out_style: _styles.Style,  ) -> None:    """    Initializes the :class:`.Converter` instance.    """    self._escape_start = len(escape_char) if escape_char is not None else 0    """    *_escape_start* (:class:`int`) is the offse used to skip the escapecharacter.    """    self._expand_tuples: bool = expand_tuples    """    *_expand_tuples* (:class:`bool`) is whether toconvert tuples into asequence of parameters.    """class Converter(object):  """  The :class:`.Converter` class is the base class for implementing theconversion from one in-styleparameter to another out-style parameter.  """  def __init__(    self,    escape_char: Optional[str],    expand_tuples: bool,    in_regex: Pattern[str],    in_style: _styles.Style,    out_style: _styles.Style,  ) -> None:    """    Initializes the :class:`.Converter` instance.    """    self._escape_start = len(escape_char) if escape_char is not None else 0    """    *_escape_start* (:class:`int`) is the offse used to skip the escapecharacter.    """    self._expand_tuples: bool = expand_tuples    """    *_expand_tuples* (:class:`bool`) is whether toconvert tuples into asequence of parameters.    """RawCodeMiniBlocksCandidates
SplitAggregateclass Converter(object):  """  The :class:`.Converter` class is the base class for implementing theconversion from one in-styleparameter to another out-style parameter.  """  def __init__(    self,    escape_char: Optional[str],    expand_tuples: bool,    in_regex: Pattern[str],    in_style: _styles.Style,    out_style: _styles.Style,  ) -> None:    """    Initializes the :class:`.Converter` instance.    """    self._escape_start = len(escape_char) if escape_char is not None else 0    """    *_escape_start* (:class:`int`) is the offse used to skip the escapecharacter.    """    self._expand_tuples: bool = expand_tuples    """    *_expand_tuples* (:class:`bool`) is whether toconvert tuples into asequence of parameters.    """CandidatesFig. 5. Candidate construction strategy.
‚Ä¶Top-k Candidates‚Ä¶Stop SignalRetainDisgard
Fig. 6. Illustration of the stop signal mechanism.
weighted by the corresponding reward. This is mathematically
represented as:
L=nX
i=1(reward (ci, x, C )√ólogp(ci|x, C)) (12)
where nis the total number of candidates in set C, and
p(ci|x, C)denotes the probability of selecting candidate ci
given the context xand the set of candidates C.
2) Stop Signal Mechanism for Candidates Selection:
Previous works often overlook when to retrieve and which
candidates to retain after retrieval. Specifically, after obtaining
the top kcandidates, traditional methods simply truncate this
list to the top icandidates, determined by a predefined context
length. However, this method overlooks the fact that not every
retrieved candidate contributes positively to the generation
process, and some may even have a negative impact. There-
fore, discerning which candidates to retain is crucial for code
completion performance. As shown in Figure 6, we design a
stop signal mechanism. For each repository, this mechanism
introduces an empty candidate serving as the stop signal into
the candidate codebase. Within the candidate list retrieved by
RLRetriever, we only retain those appearing before the stop
signal. If the stop signal appears at the beginning of the list,
it suggests that the generation task is likely to be a task that
does not require cross-file context, as illustrated in Figure 2.3) Learning from Reward: As depicted in Figure 3, RLRe-
triever is fine-tuned through a dynamic learning process where
it receives rewards from the evaluator to update its parameters.
This iterative learning process enables RLRetriever to progres-
sively improve its retrieval results, leading to the selection of
code candidates with progressively higher quality.
D. Code Completion with RLCoder
As shown in the lower half of Figure 3, during the inference
stage, given unfinished code and the current repository context,
we first construct the candidate codebase from the current
repository using the Split-Aggregate method. Then, we retrieve
code candidates using the unfinished code with the trained
RLRetriever. Inherently, the stop signal strategy mentioned
in Section III-C2 is embedded in the retrieved results to
retain only the useful candidates. Finally, the unfinished code,
together with these selected candidates, is provided as input
to the generator for code completion.
IV. E XPERIMENTAL SETUP
A. Baselines
1) For RLCoder: To evaluate the effectiveness of RLCoder,
we compare it with the RawRAG method and RepoCoder
framework. Besides, we use a popular dense retriever UniX-
coder [49] in these experiments.
‚Ä¢RawRAG refers to the standard retrieval and generation
approach in the repository-level code completion task. For
the unfinished code to generate, RawRAG uses the left
context of unfinished code as the query to find the relevant
code in the repository to build prompts for generation.
‚Ä¢RepoCoder [13] is the state-of-the-art framework for
repository-level code completion. It uses an iterative
retrieval and generation approach to generate target code.TABLE I
BENCHMARK STATISTICS .
Benchmark Category #Samples Avg. #Lines Avg. #Tokens
CrossCodeEvalPython 2665 1.00 14.45
Java 2139 1.09 16.76
RepoEvalPython (Line) 1600 1.00 15.03
Python (API) 1600 2.48 34.91
2) For RLRetriever: To evaluate the effectiveness of RL-
Retriever, we compare it with the following commonly used
retrieval methods in RAG:
‚Ä¢NoRetrieval stands for direct generation with unfinished
code, without retrieval.
‚Ä¢BM25 [18] calculates scores for code candidates based
on the frequency of query terms in each candidate. It
adjusts for candidate length and the average candidate
length across the entire database to prevent bias towards
longer candidates.
‚Ä¢UniXcoder [49] is a dense retriever that encodes both
the query and the code snippets into dense vector spaces.
This encoding facilitates the identification and retrieval of
semantically relevant code snippets from a large corpus
based on the similarity of vector representations.
‚Ä¢UniXcoder-SFT is a retriever that we trained using
supervised fine-tuning of UniXcoder. Due to the lack
of labeled data, we use the candidate with the lowest
perplexity of target code as the label to fine-tune the
retriever.
B. Benchmarks
We evaluate RLCoder on widely used benchmarks for code
completion: CrossCodeEval and RepoEval.
‚Ä¢CrossCodeEval [22] is a diverse and multilingual code
completion benchmark, and we use the Python and Java
parts of it.
‚Ä¢RepoEval [13] is a benchmark proposed simultaneously
with RepoCoder [13]. The benchmark consists of the
latest repositories that cover the line-level, API-level, and
function-level completion tasks. We use the line-level and
API-level tasks among it for evaluation.
Table I shows the statistics of the benchmarks. #Samples
stands for the number of samples in a benchmark, Avg.
#Lines andAvg. #Tokens stands for the average num-
bers of lines and tokens of the target code snippets in the
benchmark, respectively. Tokens are tokenized by the tokenizer
of DeepSeekCoder-1B.
C. Evaluation Metrics
We measure the performance of our approach using the
widely used metrics Exact Match (EM) andEdit Similarity
(ES) [50]. These metrics are widely used in previous code
completion studies [12], [13], [16], [22]. EM assesses the
precision of code completion by checking if the generated code
matches the expected code exactly. It treats the entire code
snippet as a single unit. ES measures the similarity between the
0 2 4 6 810 12 14 16 18 20
Epoch121416182022EM
Language
Python
JavaFig. 7. Performance trajectory curve during the training process.
generated code and the expected code by calculating the edit
distance. It reflects the number of edits needed to transform
the generated code into the expected code.
D. Experimental Details
All experiments are conducted on a machine with two
Tesla A100 GPUs, each with 80 GB memory. In the training
stage, we use the parameters of UniXcoder [49] to initialize
RLRetriever and use DeepSeekCoder-1B as the evaluator. The
batch size is 16 and the learning rate is 5e‚àí5. We train the
model for 20 epochs with 2000 samples per epoch and perform
early stopping. In the inference and evaluation stage, we use
five different backbone models as the generators.
V. E VALUATION RESULTS
In this section, we report and analyze the experimental
results to answer the following research questions (RQs):
‚Ä¢RQ1: How effective is RLCoder in repository-level code
completion?
‚Ä¢RQ2: How effective is RLRetriever compared to other
retrieval methods?
‚Ä¢RQ3: Does each component of RLCoder contribute to its
performance?
‚Ä¢RQ4: How is the generalizability of RLCoder?
A. RQ1: Effectiveness of RLCoder
To evaluate the effectiveness of RLCoder, we com-
pare it with the RawRAG framework [35] and Re-
poCoder [13] with five backbone LLMs, i.e., CodeLlama-
7B [3], StartCoder-7B [4], StarCoder2-7B, DeepSeekCoder-
1B [5], and DeepSeekCoder-7B. We evalaute the performance
on CrossCodeEval [22] and RepoEval [13] benchmarks.
From the experimental results shown in Table II, we can find
that the proposed RLCoder demonstrates effectiveness on all
backbone language models across the four evaluated datasets,
except for RLCoder DeepSeekCoder-7B evaluated on RepoEval
API, where its performance is on par with its corresponding
best baseline RepoCoder DeepSeekCoder-7B . We can also observe
that among all models, RLCoder DeepSeekCoder-7B achieves theTABLE II
PERFORMANCE OF DIFFERENT MODELS . THE SUPERSCRIPTS IN PERCENTAGE DENOTE THE IMPROVEMENT RATIOS OF RLC ODER OVER THE
CORRESPONDING BEST BASELINE .
ModelCrossCodeEval (Python) CrossCodeEval (Java) RepoEval (Line) RepoEval (API)
EM ES EM ES EM ES EM ES
RawRAG CodeLlama-7B 21.76 69.09 23.42 66.13 42.31 64.35 34.38 61.45
RepoCoder CodeLlama-7B 23.34 70.84 24.17 66.56 43.94 65.81 37.00 63.51
RLCoder CodeLlama-7B 26.60‚Üë14.0%72.27‚Üë2.0%26.23‚Üë8.5%67.61‚Üë1.6%46.63‚Üë6.1%67.92‚Üë3.2%37.94‚Üë2.5%64.31‚Üë1.3%
RawRAG StarCoder-7B 22.33 69.60 22.16 67.80 43.81 64.83 31.94 56.00
RepoCoder StarCoder-7B 23.15 70.71 22.53 68.22 45.69 66.90 33.44 57.81
RLCoder StarCoder-7B 25.82‚Üë11.5%72.11‚Üë2.0%24.73‚Üë9.8%69.08‚Üë1.3%47.38‚Üë3.7%68.46‚Üë2.3%34.88‚Üë4.3%58.11‚Üë0.5%
RawRAG StarCoder2-7B 22.89 70.66 23.42 69.13 44.44 65.95 34.50 58.78
RepoCoder StarCoder2-7B 24.35 71.71 23.75 69.59 45.81 67.37 36.44 59.92
RLCoder StarCoder2-7B 27.17‚Üë11.6%73.24‚Üë2.1%26.23‚Üë10.4%70.51‚Üë1.3%48.25‚Üë5.3%68.61‚Üë1.8%38.00‚Üë4.3%61.21‚Üë2.2%
RawRAG DeepSeekCoder-1B 19.74 67.68 18.89 62.47 39.31 62.04 33.00 60.41
RepoCoder DeepSeekCoder-1B 20.23 68.78 19.59 62.35 40.88 63.56 35.13 61.92
RLCoder DeepSeekCoder-1B 23.98‚Üë18.5%70.44‚Üë2.4%20.80‚Üë6.2%63.39‚Üë1.7%44.19‚Üë8.1%66.48‚Üë4.6%36.06‚Üë2.6%62.72‚Üë1.3%
RawRAG DeepSeekCoder-7B 23.30 70.84 22.49 66.78 45.69 66.67 38.00 65.66
RepoCoder DeepSeekCoder-7B 26.98 72.96 24.96 66.52 46.38 67.51 39.31 66.29
RLCoder DeepSeekCoder-7B 30.28‚Üë12.2%74.42‚Üë2.0%26.09‚Üë4.5%67.31‚Üë1.2%48.75‚Üë5.1%69.43‚Üë2.8%39.88‚Üë1.5%66.22‚Üë-0.1%
best performance with EM score of 30.28, improving its cor-
responding best baseline RepoCoder DeepSeekCoder-7B by 12.2%
on CrossCodeEval Python and 5.1% on RepoEval Line.
Furthermore, to investigate the efficacy of our training
process, we plot the performance trajectory curve across the
training epochs on CrossCodeEval, as illustrated in Figure 7.
The result shows that the EM score gradually increases with
each epoch until stabilizing, indicating the effectiveness of our
training process.
RQ1 Summary: Our approach significantly outperforms
current state-of-the-art methods for all backbone LLMs,
improving the CrossCodeEval benchmark by 12.2% and
RepoEval 5.1%. The performance trajectory further demon-
strates the efficacy of our training process.
B. RQ2: Effectiveness of RLRetriever
To assess the effectiveness of RLRetriever, the key module
of RLCoder, we conduct a comparative study. We evaluate
RLCoder equipped with different retrieval methods described
in Section IV-A, including NoRetrieval, BM25 [18], UniX-
coder [49], and our enhanced model UniXcoder-SFT. Table III
shows the experimental results on the CrossCodeEval and Re-
poEval benchmarks. The results yield the following findings:
‚Ä¢Our proposed RLRetriever consistently outperforms com-
parative baseline methods under all metrics in both bench-
marks, underscoring its superior performance.
‚Ä¢All retrieval-based methods (i.e., BM25, UniXCoder,
UniXcoder-SFT, and RLRetriever) perform better than
NoRetrieval, showing the inherent value of the retrieval
process itself.
‚Ä¢Both UniXcoder-SFT and RLRetriever show better per-
formance than UniXcoder, indicating that retrieval train-
ing can enhance the performance. Notably, our rein-
forcement learning-based training method exhibits better
performance over supervised fine-tuning.RQ2 Summary: RLRetriever consistently outperforms
other retrieval methods. Furthermore, the results affirms
the significance of the retrieval and training processes,
particularly highlighting the advantages of our reinforcement
learning-based training approach.
C. RQ3: Contributions of Each Component
To understand the contributions of each component to
RLCoder, we conduct an ablation study on RLCoder. Specif-
ically, we remove each component of RLCoder each time
and study the performance of the ablated model. The ex-
perimental results are shown in Table IV. ‚Äúw/o RL‚Äù means
using retriever without reinforcement learning. ‚Äúw/o WP‚Äù
means utilizing unweighted perplexity of the target code as
the reward. ‚Äúw/o NC‚Äù means using fixed window candidates
instead of our natural candidates, ‚Äúw/o SS‚Äù means using
retriever without the stop signal mechanism. From Table IV,
we can see that the performance of the model drops after
removing any one component, indicating that each component
contributes to the effectiveness of RLCoder. Especially, the
performance drops the most significantly for ‚ÄúRLCoder w/o
RL‚Äù, indicating that the reinforcement learning mechanism
is the most important component in RLCoder. We observe
that unweighted perplexity and no stop mechanism can be
beneficial to ES performance in some cases, but still harm
EM performance. In fact, ES mainly considers the similarity
between two pieces of code. The introduction of the stop
signal and weighted perplexity can both affect code similarity.
The stop signal reduces useless but similar candidates, while
weighted perplexity emphasizes API tokens more rather than
all tokens. This can potentially reduce the similarity between
generated and target code. Although these strategies weaken
similarity, they improve code correctness. So removing the
stop signal and weighted perplexity decreases in EM across
all benchmarks.TABLE III
EXPERIMENTAL RESULTS OF RLC ODER EQUIPPED WITH DIFFERENT RETRIEVAL METHODS . THE BACKBONE LLM USED IS DEEPSEEKCODER -7B. T HE
SUPERSCRIPTS IN PERCENTAGE DENOTE THE IMPROVEMENT RATIOS OF OUR RETRIEVAL MODEL RLR ETRIEVER OVER THE CORRESPONDING BEST
BASELINE RETRIEVER .
Retrieval MethodCrossCodeEval (Python) CrossCodeEval (Java) RepoEval (Line) RepoEval (API)
EM ES EM ES EM ES EM ES
NoRetrieval 9.46 62.79 11.41 63.81 39.63 61.95 30.44 59.40
BM25 18.31 68.38 17.48 65.23 45.94 66.67 38.25 65.04
UniXcoder 23.30 70.84 22.49 66.78 45.69 66.67 38.00 65.66
UniXcoder-SFT 27.28 72.90 25.11 66.39 46.75 67.28 37.69 65.00
RLRetreiver 30.28‚Üë11.0%74.42‚Üë2.1%26.09‚Üë3.9%67.31‚Üë1.4%48.75‚Üë4.3%69.43‚Üë3.2%39.88‚Üë5.8%66.22‚Üë1.9%
TABLE IV
ABLATION STUDY RESULTS ON CROSS CODEEVAL AND REPOEVAL.
ModelCrossCodeEval (Python) CrossCodeEval (Java) RepoEval (Line) RepoEval (API)
EM ES EM ES EM ES EM ES
RLCoder 30.28 74.42 26.09 67.31 48.75 69.43 39.88 66.22
w/o RL 23.30‚Üì23.1%70.84‚Üì4.8%22.49‚Üì13.8%66.78‚Üì0.8%45.69‚Üì6.3%66.67‚Üì4.0%38.00‚Üì4.7%65.66‚Üì0.8%
w/o WP 27.35‚Üì9.7%72.82‚Üì2.1%25.67‚Üì1.6%67.43‚Üë0.2%47.44‚Üì2.7%67.83‚Üì2.3%38.81‚Üì2.7%65.25‚Üì1.5%
w/o NC 29.31‚Üì3.2%73.91‚Üì0.7%24.03‚Üì7.9%66.49‚Üì1.2%47.13‚Üì3.3%68.11‚Üì1.9%38.63‚Üì3.1%65.56‚Üì1.0%
w/o SS 29.57‚Üì2.34%74.49‚Üë0.09%25.57‚Üì1.99%67.42‚Üë0.16%47.31‚Üì2.95%68.23‚Üì1.73%39.63‚Üì0.63%65.87‚Üì0.53%
TABLE V
ADDITIONAL ABLATION STUDY FOR THE STOP SIGNAL MECHANISM .
NOTE THAT ,CONTRARY TO EM AND ES, LOWER PPL SCORES
CORRESPOND TO BETTER PERFORMANCE .
Model EM ES PPL
RawRAG CodeLlama-7B 10.3 65.2 2.1389
RLCoder CodeLlama-7B 11.1 66.4 2.0835
w/o Stop Signal 9.9‚Üì10.81%66.1‚Üì0.45%2.1152‚Üë1.52%
RawRAG StarCoder-7B 8.9 59.2 2.7400
RLCoder StarCoder-7B 10.1 61.3 2.6695
w/o Stop Signal 9.1‚Üì9.90%60.1‚Üì1.96%2.7100‚Üë1.52%
RawRAG StarCoder2-7B 9.1 60.5 2.6422
RLCoder StarCoder2-7B 10.3 60.2 2.5716
w/o Stop Signal 9.3‚Üì9.71%60.5‚Üë0.50%2.6115‚Üë1.55%
RawRAG DeepSeekCoder-1B 9.6 64.9 2.4370
RLCoder DeepSeekCoder-1B 10.5 66.7 2.3764
w/o Stop Signal 10.0‚Üì4.76%65.9‚Üì1.20%2.4138‚Üë1.57%
RawRAG DeepSeekCoder-7B 11.5 66.6 2.3334
RLCoder DeepSeekCoder-7B 12.2 68.6 2.2824
w/o Stop Signal 11.8‚Üì3.28%67.9‚Üì1.02%2.3157‚Üë1.46%
Since CrossCodeEval and RepoEval are specifically curated
to evaluate code completion ability in scenarios requiring
cross-file context, the improvement brought by the stop signal
is expected to be minor. To further evaluate the practical
effectiveness of the stop signal mechanism, we construct a
new dataset GitHubEval that construct code completion targets
at random positions within repositories, thus incorporating
instances that may not necessitate cross-file context. Following
the same construction procedure as the training dataset, we
obtain 1000 samples for evaluation. The results shown in
Table V indicate that the stop signal is an important component
in RLCoder, without which, the EM scores drop significantly
by an average of 7.69%.RQ3 Summary: Reinforcement learning mechanism is the
most important component in RLCoder. Other components
of RLCoder also contributes to its superior performance,
with the stop signal mechanism showing further enhance-
ments in scenarios involving target code completion that
both require and do not require cross-file context.
D. RQ4: Generalizability of RLCoder
To explore the generalizability of RLCoder, we conduct
an evaluation with a setting different from the training stage.
Specifically, we train a new retriever by fusing RLCoder and
RepoCoder. Then, we evaluate the performance of the fused
model. As shown in Table VI, we find that RepoCoder trained
using the framework of RLCoder significantly outperforms
the original RepoCoder method. Specifically, the improvement
rates in EM for Python and Java are 12.4% and 8.1% on
CrossCodeEval, respectively. This result indicates that our
training framework can be integrated into other models to
further improve their performance. Note that when comparing
‚ÄúRepoCoder w/ RLCoder‚Äù to RLCoder, they have comparable
performance (with ‚ÄúRepoCoder w/ RLCoder‚Äù slightly better).
However, considering that RepoCoder requires multiple itera-
tions of retrieval and generation, we opt for RLCoder, which
accomplishes code completion in a single round, as the default
setting in this work.
RQ4 Summary: The training pipeline of RLCoder shows
generalizability on all datasets in applying to other frame-
works.
E. Case Study
We illustrate the effectiveness of RLCoder through a case
study presented in Figure 8. The left-hand side shows the
incomplete code, groundtruth code, and the gold candidateTABLE VI
EXPERIMENTAL RESULTS OF REPOCODER INTEGRATED WITH RLC ODER .
MethodCrossCodeEval (Python) CrossCodeEval (Java) RepoEval (Line) RepoEval (API)
EM ES EM ES EM ES EM ES
RawRAG 23.30 70.84 22.49 66.78 45.69 66.67 38.00 65.66
RLCoder 30.28 74.42 26.09 67.31 48.75 69.43 39.88 66.22
RepoCoder 26.98 72.96 24.96 66.52 46.38 67.51 39.31 66.29
w/ RLCoder 30.32‚Üë12.4%74.79‚Üë2.5%26.98‚Üë8.1%67.81‚Üë1.9%49.44‚Üë6.6%69.76‚Üë3.3%41.25‚Üë4.9%67.08‚Üë1.2%
# similarity_filter.pyfrom typing import Listfrom common.constants import Constantsfrom common.utils import UtilsRATE = 2class SimilarityFilter:  def __init__(self, detect_data: List[float], algorithm_type: str, anomaly_duration: int):    self.algorithm_type =algorithm_type    self.detect_data = self.minus_data(detect_data)    self.anomaly_duration =anomaly_duration  def run(self):    """    Check if the current data is similar to the historical data.    :return: True if the current data is similar to the historical data.    """    agg_list = Utils.# utils.py@staticmethoddef agg_diff_fe_calc(input_data: List[float], agg_length: int) -> list:  ...  diff_func: Callable[[Any, Any], None] = lambda a, b: np.sum(a) - np.sum(b)  diff = []  for i in range(len(input_data) - 2 * agg_length + 1):    post = input_data[i + agg_length:i + 2 * agg_length]    pre = input_data[i:i + agg_length]    diff.append(diff_func(post, pre))  return diffGold Candidateaggregate_data(self.detect_data, self.anomaly_duration)RawRAGUniXcoderagg_diff_fe_calc(self.detect_data, Constants.AGG_LENGTH)RawRAGUniXcoder-SFTget_agg_list(self.detect_data, self.anomaly_duration)agg_diff_fe_calc(self.detect_data, self.anomaly_duration)RLCoderQuery (IncompleteCode)
Code To Generate# diff_outlier_detector.pyclass DiffOutlierDetector:  def __init__(self, detect_data: List[float], algorithm_type: str):    self.algorithm_type =algorithm_type    self.detect_data = self.minus_data(detect_data)    self.default_point = 4    self.alarm_last_time = 15    self.tk_delta = 2.0    self.default_duration = 1    # output    self.real_duration = 0  def run(self):    """    Detect an anomaly using the previous difference.Bad Candidate
3rd Candidate1st CandidateNot Found
1st Candidateagg_diff_fe_calc(self.detect_data, self.anomaly_duration)Groundtruth
RepoCoderNot Found
Not Found
Fig. 8. Case study. An example sources from CrossCodeEval with the task idbeing project ccpython/210 . The highlight token represents the identical
tokens exists in both the query and the bad candidate.
we labelled for this case. We can see that RLCoder ranks the
gold candidate as the first candidate and generates the correct
code. A possible reason that other methods fail to retrieve
the correct code is that these methods rely on the surface-
level similarity between the query and the candidate. The bad
candidate, despite sharing several tokens with the query (as
highlighted in the figure), does not contribute meaningfully to
the correct code completion. In contrast to RepoCoder, which
iteratively uses generated code for retrieval and generation but
still depends heavily on query-candidate similarity, RLCoder
leverages the perplexity of generating target code from a
given candidate. This approach enables RLRetriever to bypass
candidates that are seemingly useful but actually useless ,
focusing instead on those more likely to aid in accurate code
generation. This strategic prioritization explains RLCoder‚Äôs
success in both retrieving the gold candidate and generating
the correct target code.
VI. R ELATED WORK
A. Code Completion
Code completion as one of the most important tasks in mod-
ern IDEs, has attracted the attention of many researchers [51]‚Äì
[55]. Traditional studies [56]‚Äì[58] use rule-based methods or
code examples for code completion. In recent years, deeplearning-based methods [9], [10], [54], [59]‚Äì[71] have been
explored to improve the performance of code completion.
Recent studies found that code search can enhance code
completion performance [72]. With the development of large
language models [73]‚Äì[82], many researchers have introduced
LLMs into code completion [83]‚Äì[89]. Equipped with LLMs,
many studies have employed RAG for code completion/gen-
eration [32], [34]‚Äì[37], [40]. For example, RedCoder [35]
enhances code generation and summarization by integrating
relevant past work using dense retrieval techniques. To en-
hance private library code generation, APICoder [37] was
proposed to employ API documentation to train models to
better generate these libraries. DocPrompting [36] introduces
a method to enhance code generation by using code doc-
umentation to address the challenge of generating code for
unseen functions and libraries. AceCoder [34] improves code
generation by integrating example retrieval and guided gen-
eration. ReCode [40] improves neural code generation by
incorporating subtree retrieval from existing code examples.
kNN-TRANX [32] improves code generation from natural
language by using syntax-aware retrieval, reducing noise and
computational time.B. Repository-Level Code Completion
Repository-level code completion, which leverages the
broader context of an entire code repository, has become a fo-
cal point for research in the field of code completion and many
studies have attempted to improve repository-level code com-
pletion performance [12]‚Äì[17], [19], [21], [48]. CoCoMIC [12]
and RepoHyper [19] enhance code completion capabilities
through dependency analysis and learning-based methods but
they encounter the problem of difficulty in obtaining training
data and poor generalizability. CodePlan [14], RepoFuse [15]
andA3-CodeGen [21] employ static code analysis to obtain
relevant candidates. RepoCoder [13] and De-Hallucinator [16]
adopt an approach through iterative retrieval and generation.
CodeAgent [17] and ToolGen [48] explore tool invocation to
help code completion.
Although these efforts show promising performance, RL-
Coder differs from them in that it does not require labeled
data to train and uses a novel stop signal mechanism to know
when to retrieve and which candidates to retain.
VII. C ONCLUSION
In this paper, we propose RLCoder, a novel reinforcement
learning framework for repository-level code completion. We
enable the retriever to iteratively learn by obtaining feedback
from the evaluator. Besides, unlike using fixed window can-
didates or candidates parsing from dependency, we introduce
a simple yet effective Split-Aggregate candidate construction
method based on human programming habits. Moreover, we
propose the stop signal to avoid using useless cross-file
context. Experimental results indicate that RLCoder achieves
state-of-the-art performance on repository-level code comple-
tion and demonstrates good generalizability and applicability
to further enhancing existing methods.
ACKNOWLEDGEMENTS
The work described in this paper is supported by CCF-
Huawei Populus Grove Fund CCF-HuaweiSE202301.
REFERENCES
[1] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan,
H. Edwards, Y . Burda, N. Joseph, G. Brockman et al. , ‚ÄúEvaluating large
language models trained on code,‚Äù arXiv preprint arXiv:2107.03374 ,
2021.
[2] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y . Zhou, S. Savarese,
and C. Xiong, ‚ÄúCodegen: An open large language model for code with
multi-turn program synthesis,‚Äù arXiv preprint arXiv:2203.13474 , 2022.
[3] B. Roziere, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y . Adi,
J. Liu, T. Remez, J. Rapin et al. , ‚ÄúCode llama: Open foundation models
for code,‚Äù arXiv preprint arXiv:2308.12950 , 2023.
[4] R. Li, L. B. Allal, Y . Zi, N. Muennighoff, D. Kocetkov, C. Mou,
M. Marone, C. Akiki, J. Li, J. Chim et al. , ‚ÄúStarcoder: may the source
be with you!‚Äù arXiv preprint arXiv:2305.06161 , 2023.
[5] D. Guo, Q. Zhu, D. Yang, Z. Xie, K. Dong, W. Zhang, G. Chen,
X. Bi, Y . Wu, Y . Li et al. , ‚ÄúDeepseek-coder: When the large language
model meets programming‚Äìthe rise of code intelligence,‚Äù arXiv preprint
arXiv:2401.14196 , 2024.
[6] F. Liu, Z. Fu, G. Li, Z. Jin, H. Liu, Y . Hao, and L. Zhang, ‚ÄúNon-
autoregressive line-level code completion,‚Äù ACM Transactions on Soft-
ware Engineering and Methodology , 2024.
[7] C. Wang, J. Hu, C. Gao, Y . Jin, T. Xie, H. Huang, Z. Lei, and Y . Deng,
‚ÄúHow practitioners expect code completion?‚Äù in Proceedings of the 31st
ACM Joint European Software Engineering Conference and Symposium
on the Foundations of Software Engineering , 2023, pp. 1294‚Äì1306.[8] V . J. Hellendoorn, S. Proksch, H. C. Gall, and A. Bacchelli, ‚ÄúWhen
code completion fails: A case study on real-world completions,‚Äù in
2019 IEEE/ACM 41st International Conference on Software Engineering
(ICSE) . IEEE, 2019, pp. 960‚Äì970.
[9] M. Izadi, R. Gismondi, and G. Gousios, ‚ÄúCodefill: Multi-token code
completion by jointly learning from structure and naming sequences,‚Äù
inProceedings of the 44th International Conference on Software Engi-
neering , 2022, pp. 401‚Äì412.
[10] W. Zhou, S. Kim, V . Murali, and G. A. Aye, ‚ÄúImproving code autocom-
pletion with transfer learning,‚Äù in Proceedings of the 44th International
Conference on Software Engineering: Software Engineering in Practice ,
2022, pp. 161‚Äì162.
[11] M. Izadi, J. Katzy, T. van Dam, M. Otten, R. M. Popescu, and
A. van Deursen, ‚ÄúLanguage models for code completion: A practical
evaluation,‚Äù arXiv preprint arXiv:2402.16197 , 2024.
[12] Y . Ding, Z. Wang, W. U. Ahmad, M. K. Ramanathan, R. Nallapati,
P. Bhatia, D. Roth, and B. Xiang, ‚ÄúCocomic: Code completion by jointly
modeling in-file and cross-file context,‚Äù 2023.
[13] F. Zhang, B. Chen, Y . Zhang, J. Keung, J. Liu, D. Zan, Y . Mao, J.-
G. Lou, and W. Chen, ‚ÄúRepocoder: Repository-level code completion
through iterative retrieval and generation,‚Äù 2023.
[14] R. Bairi, A. Sonwane, A. Kanade, A. Iyer, S. Parthasarathy, S. Rajamani,
B. Ashok, S. Shet et al. , ‚ÄúCodeplan: Repository-level coding using llms
and planning,‚Äù arXiv preprint arXiv:2309.12499 , 2023.
[15] M. Liang, X. Xie, G. Zhang, X. Zheng, P. Di, H. Chen, C. Wang,
G. Fan et al. , ‚ÄúRepofuse: Repository-level code completion with fused
dual context,‚Äù arXiv preprint arXiv:2402.14323 , 2024.
[16] A. Eghbali and M. Pradel, ‚ÄúDe-hallucinator: Iterative grounding for llm-
based code completion,‚Äù arXiv preprint arXiv:2401.01701 , 2024.
[17] K. Zhang, J. Li, G. Li, X. Shi, and Z. Jin, ‚ÄúCodeagent: Enhancing code
generation with tool-integrated agent systems for real-world repo-level
coding challenges,‚Äù arXiv preprint arXiv:2401.07339 , 2024.
[18] S. Robertson, H. Zaragoza et al. , ‚ÄúThe probabilistic relevance frame-
work: Bm25 and beyond,‚Äù Foundations and Trends¬Æ in Information
Retrieval , vol. 3, no. 4, pp. 333‚Äì389, 2009.
[19] H. N. Phan, H. N. Phan, T. N. Nguyen, and N. D. Bui, ‚ÄúRepohyper:
Better context retrieval is all you need for repository-level code com-
pletion,‚Äù arXiv preprint arXiv:2403.06095 , 2024.
[20] D. Wu, W. U. Ahmad, D. Zhang, M. K. Ramanathan, and X. Ma,
‚ÄúRepoformer: Selective retrieval for repository-level code completion,‚Äù
arXiv preprint arXiv:2403.10059 , 2024.
[21] D. Liao, S. Pan, Q. Huang, X. Ren, Z. Xing, H. Jin, and Q. Li, ‚ÄúContext-
aware code generation framework for code repositories: Local, global,
and third-party library awareness,‚Äù arXiv preprint arXiv:2312.05772 ,
2023.
[22] Y . Ding, Z. Wang, W. Ahmad, H. Ding, M. Tan, N. Jain, M. K.
Ramanathan, R. Nallapati, P. Bhatia, D. Roth et al. , ‚ÄúCrosscodeeval:
A diverse and multilingual benchmark for cross-file code completion,‚Äù
Advances in Neural Information Processing Systems , vol. 36, 2024.
[23] Y . Gao, Y . Xiong, X. Gao, K. Jia, J. Pan, Y . Bi, Y . Dai, J. Sun, and
H. Wang, ‚ÄúRetrieval-augmented generation for large language models:
A survey,‚Äù arXiv preprint arXiv:2312.10997 , 2023.
[24] P. Zhao, H. Zhang, Q. Yu, Z. Wang, Y . Geng, F. Fu, L. Yang, W. Zhang,
and B. Cui, ‚ÄúRetrieval-augmented generation for ai-generated content:
A survey,‚Äù arXiv preprint arXiv:2402.19473 , 2024.
[25] B. Cao, D. Cai, L. Cui, X. Cheng, W. Bi, Y . Zou, and S. Shi, ‚ÄúRetrieval
is accurate generation,‚Äù arXiv preprint arXiv:2402.17532 , 2024.
[26] Z. He, Z. Zhong, T. Cai, J. D. Lee, and D. He, ‚ÄúRest: Retrieval-based
speculative decoding,‚Äù arXiv preprint arXiv:2311.08252 , 2023.
[27] N. Nashid, M. Sintaha, and A. Mesbah, ‚ÄúRetrieval-based prompt se-
lection for code-related few-shot learning,‚Äù in 2023 IEEE/ACM 45th
International Conference on Software Engineering (ICSE) . IEEE, 2023,
pp. 2450‚Äì2462.
[28] Y . Liu, S. Yavuz, R. Meng, D. Radev, C. Xiong, and Y . Zhou, ‚ÄúUni-
parser: Unified semantic parser for question answering on knowledge
base and database,‚Äù arXiv preprint arXiv:2211.05165 , 2022.
[29] S. Lu, N. Duan, H. Han, D. Guo, S.-w. Hwang, and A. Svyatkovskiy,
‚ÄúReacc: A retrieval-augmented code completion framework,‚Äù arXiv
preprint arXiv:2203.07722 , 2022.
[30] C. Yu, G. Yang, X. Chen, K. Liu, and Y . Zhou, ‚ÄúBashexplainer:
Retrieval-augmented bash code comment generation based on fine-
tuned codebert,‚Äù in 2022 IEEE International Conference on Software
Maintenance and Evolution (ICSME) . IEEE, 2022, pp. 82‚Äì93.[31] J. A. Li, Y . Li, G. Li, X. Hu, X. Xia, and Z. Jin, ‚ÄúEditsum: A
retrieve-and-edit framework for source code summarization,‚Äù in 2021
36th IEEE/ACM International Conference on Automated Software En-
gineering (ASE) . IEEE, 2021, pp. 155‚Äì166.
[32] X. Zhang, Y . Zhou, G. Yang, and T. Chen, ‚ÄúSyntax-aware retrieval
augmented code generation,‚Äù in The 2023 Conference on Empirical
Methods in Natural Language Processing , 2023.
[33] J. Zhang, X. Wang, H. Zhang, H. Sun, and X. Liu, ‚ÄúRetrieval-based
neural source code summarization,‚Äù in Proceedings of the ACM/IEEE
42nd International Conference on Software Engineering , 2020, pp.
1385‚Äì1397.
[34] J. Li, Y . Zhao, Y . Li, G. Li, and Z. Jin, ‚ÄúAcecoder: Utilizing existing code
to enhance code generation,‚Äù arXiv preprint arXiv:2303.17780 , 2023.
[35] M. R. Parvez, W. U. Ahmad, S. Chakraborty, B. Ray, and K.-W.
Chang, ‚ÄúRetrieval augmented code generation and summarization,‚Äù
arXiv preprint arXiv:2108.11601 , 2021.
[36] S. Zhou, U. Alon, F. F. Xu, Z. Wang, Z. Jiang, and G. Neubig,
‚ÄúDocprompting: Generating code by retrieving the docs,‚Äù arXiv preprint
arXiv:2207.05987 , 2022.
[37] D. Zan, B. Chen, Z. Lin, B. Guan, Y . Wang, and J.-G. Lou, ‚ÄúWhen
language model meets private library,‚Äù arXiv preprint arXiv:2210.17236 ,
2022.
[38] A. Madaan, S. Zhou, U. Alon, Y . Yang, and G. Neubig, ‚ÄúLanguage
models of code are few-shot commonsense learners,‚Äù arXiv preprint
arXiv:2210.07128 , 2022.
[39] Y . Wang, H. Le, A. D. Gotmare, N. D. Bui, J. Li, and S. C. Hoi,
‚ÄúCodet5+: Open code large language models for code understanding
and generation,‚Äù arXiv preprint arXiv:2305.07922 , 2023.
[40] S. A. Hayati, R. Olivier, P. Avvaru, P. Yin, A. Tomasic, and
G. Neubig, ‚ÄúRetrieval-based neural code generation,‚Äù arXiv preprint
arXiv:1808.10025 , 2018.
[41] N. Beau and B. Crabb ¬¥e, ‚ÄúThe impact of lexical and grammatical
processing on generating code from natural language,‚Äù arXiv preprint
arXiv:2202.13972 , 2022.
[42] T. Ahmed, K. S. Pai, P. Devanbu, and E. T. Barr, ‚ÄúAutomatic semantic
augmentation of language model prompts (for code summarization),‚Äù in
2024 IEEE/ACM 46th International Conference on Software Engineer-
ing (ICSE) . IEEE Computer Society, 2024, pp. 1004‚Äì1004.
[43] J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan,
E. Jiang, C. Cai, M. Terry, Q. Le et al. , ‚ÄúProgram synthesis with large
language models,‚Äù arXiv preprint arXiv:2108.07732 , 2021.
[44] S. Bubeck, V . Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Ka-
mar, P. Lee, Y . T. Lee, Y . Li, S. Lundberg et al. , ‚ÄúSparks of artificial
general intelligence: Early experiments with gpt-4,‚Äù arXiv preprint
arXiv:2303.12712 , 2023.
[45] H. Yu, B. Shen, D. Ran, J. Zhang, Q. Zhang, Y . Ma, G. Liang, Y . Li,
Q. Wang, and T. Xie, ‚ÄúCodereval: A benchmark of pragmatic code
generation with generative pre-trained models,‚Äù in Proceedings of the
46th IEEE/ACM International Conference on Software Engineering ,
2024, pp. 1‚Äì12.
[46] S. Hong, X. Zheng, J. Chen, Y . Cheng, J. Wang, C. Zhang, Z. Wang,
S. K. S. Yau, Z. Lin, L. Zhou et al. , ‚ÄúMetagpt: Meta programming for
multi-agent collaborative framework,‚Äù arXiv preprint arXiv:2308.00352 ,
2023.
[47] D. Huang, Q. Bu, J. M. Zhang, M. Luck, and H. Cui, ‚ÄúAgentcoder:
Multi-agent-based code generation with iterative testing and optimisa-
tion,‚Äù arXiv preprint arXiv:2312.13010 , 2023.
[48] C. Wang, J. Zhang, Y . Feng, T. Li, W. Sun, Y . Liu, and X. Peng,
‚ÄúTeaching code llms to use autocompletion tools in repository-level code
generation,‚Äù arXiv preprint arXiv:2401.06391 , 2024.
[49] D. Guo, S. Lu, N. Duan, Y . Wang, M. Zhou, and J. Yin, ‚ÄúUnixcoder:
Unified cross-modal pre-training for code representation,‚Äù arXiv preprint
arXiv:2203.03850 , 2022.
[50] V . I. Levenshtein et al. , ‚ÄúBinary codes capable of correcting deletions,
insertions, and reversals,‚Äù in Soviet physics doklady , vol. 10, no. 8.
Soviet Union, 1966, pp. 707‚Äì710.
[51] C. Liu, X. Xia, D. Lo, C. Gao, X. Yang, and J. Grundy, ‚ÄúOpportunities
and challenges in code search tools,‚Äù ACM Computing Surveys (CSUR) ,
vol. 54, no. 9, pp. 1‚Äì40, 2021.
[52] J. Chen, C. Chen, J. Hu, J. Grundy, Y . Wang, T. Chen, and Z. Zheng,
‚ÄúIdentifying smart contract security issues in code snippets from stack
overflow,‚Äù arXiv preprint arXiv:2407.13271 , 2024.[53] Y . Wang, T. Jiang, M. Liu, J. Chen, and Z. Zheng, ‚ÄúBeyond functional
correctness: Investigating coding style inconsistencies in large language
models,‚Äù arXiv preprint arXiv:2407.00456 , 2024.
[54] Y . Wang and H. Li, ‚ÄúCode completion by modeling flattened abstract
syntax trees as graphs,‚Äù in Proceedings of the AAAI conference on
artificial intelligence , vol. 35, no. 16, 2021, pp. 14 015‚Äì14 023.
[55] W. Tao, Y . Zhou, Y . Wang, H. Zhang, H. Wang, and W. Zhang, ‚ÄúKadel:
Knowledge-aware denoising learning for commit message generation,‚Äù
ACM Transactions on Software Engineering and Methodology , 2024.
[56] M. Bruch, M. Monperrus, and M. Mezini, ‚ÄúLearning from examples
to improve code completion systems,‚Äù in Proceedings of the 7th joint
meeting of the European software engineering conference and the ACM
SIGSOFT symposium on the foundations of software engineering , 2009,
pp. 213‚Äì222.
[57] D. Hou and D. M. Pletcher, ‚ÄúTowards a better code completion system
by api grouping, filtering, and popularity-based ranking,‚Äù in Proceedings
of the 2nd International Workshop on Recommendation Systems for
Software Engineering , 2010, pp. 26‚Äì30.
[58] R. Robbes and M. Lanza, ‚ÄúHow program history can improve code
completion,‚Äù in 2008 23rd IEEE/ACM International Conference on
Automated Software Engineering . IEEE, 2008, pp. 317‚Äì326.
[59] Y . Chen, C. Gao, X. Ren, Y . Peng, X. Xia, and M. R. Lyu, ‚ÄúApi usage
recommendation via multi-view heterogeneous graph representation
learning,‚Äù IEEE Transactions on Software Engineering , 2023.
[60] C. Wang, X. Peng, M. Liu, Z. Xing, X. Bai, B. Xie, and T. Wang, ‚ÄúA
learning-based approach for automatic construction of domain glossary
from source code and documentation,‚Äù in Proceedings of the 2019 27th
ACM joint meeting on european software engineering conference and
symposium on the foundations of software engineering , 2019, pp. 97‚Äì
108.
[61] M. Liu, X. Peng, A. Marcus, Z. Xing, W. Xie, S. Xing, and Y . Liu,
‚ÄúGenerating query-specific class api summaries,‚Äù in Proceedings of
the 2019 27th ACM joint meeting on European software engineering
conference and symposium on the foundations of software engineering ,
2019, pp. 120‚Äì130.
[62] X. Wang, Y . Wang, Y . Wan, F. Mi, Y . Li, P. Zhou, J. Liu, H. Wu,
X. Jiang, and Q. Liu, ‚ÄúCompilable neural code generation with compiler
feedback,‚Äù arXiv preprint arXiv:2203.05132 , 2022.
[63] G. A. Aye and G. E. Kaiser, ‚ÄúSequence model design for code comple-
tion in the modern ide,‚Äù arXiv preprint arXiv:2004.05249 , 2020.
[64] V . J. Hellendoorn and P. Devanbu, ‚ÄúAre deep neural networks the best
choice for modeling source code?‚Äù in Proceedings of the 2017 11th Joint
meeting on foundations of software engineering , 2017, pp. 763‚Äì773.
[65] R.-M. Karampatsis, H. Babii, R. Robbes, C. Sutton, and A. Janes,
‚ÄúBig code!= big vocabulary: Open-vocabulary models for source code,‚Äù
inProceedings of the ACM/IEEE 42nd International Conference on
Software Engineering , 2020, pp. 1073‚Äì1085.
[66] S. Kim, J. Zhao, Y . Tian, and S. Chandra, ‚ÄúCode prediction by feeding
trees to transformers,‚Äù in 2021 IEEE/ACM 43rd International Conference
on Software Engineering (ICSE) . IEEE, 2021, pp. 150‚Äì162.
[67] J. Li, Y . Wang, M. R. Lyu, and I. King, ‚ÄúCode completion with neural
attention and pointer networks,‚Äù arXiv preprint arXiv:1711.09573 , 2017.
[68] S. Nguyen, T. Nguyen, Y . Li, and S. Wang, ‚ÄúCombining program anal-
ysis and statistical language model for code statement completion,‚Äù in
2019 34th IEEE/ACM International Conference on Automated Software
Engineering (ASE) . IEEE, 2019, pp. 710‚Äì721.
[69] A. Svyatkovskiy, S. K. Deng, S. Fu, and N. Sundaresan, ‚ÄúIntellicode
compose: Code generation using transformer,‚Äù in Proceedings of the
28th ACM joint meeting on European software engineering conference
and symposium on the foundations of software engineering , 2020, pp.
1433‚Äì1443.
[70] F. Wen, E. Aghajani, C. Nagy, M. Lanza, and G. Bavota, ‚ÄúSiri, write
the next method,‚Äù in 2021 IEEE/ACM 43rd International Conference on
Software Engineering (ICSE) . IEEE, 2021, pp. 138‚Äì149.
[71] Y . Yang, Y . Jiang, M. Gu, J. Sun, J. Gao, and H. Liu, ‚ÄúA language model
for statements of software code,‚Äù in 2017 32nd IEEE/ACM International
Conference on Automated Software Engineering (ASE) . IEEE, 2017,
pp. 682‚Äì687.
[72] J. Chen, X. Hu, Z. Li, C. Gao, X. Xia, and D. Lo, ‚ÄúCode search is all
you need? improving code suggestions with code search,‚Äù Proceedings of
the 46th IEEE/ACM International Conference on Software Engineering ,
2024.[73] J. Lu, W. Zhong, Y . Wang, Z. Guo, Q. Zhu, W. Huang, Y . Wang, F. Mi,
B. Wang, Y . Wang et al. , ‚ÄúYoda: Teacher-student progressive learning
for language models,‚Äù arXiv preprint arXiv:2401.15670 , 2024.
[74] F. Hu, Y . Wang, L. Du, H. Zhang, S. Han, D. Zhang, and X. Li,
‚ÄúSplit, encode and aggregate for long code search,‚Äù arXiv preprint
arXiv:2208.11271 , 2022.
[75] Y . Liu, J. Chen, T. Bi, J. Grundy, Y . Wang, T. Chen, Y . Tang,
and Z. Zheng, ‚ÄúAn empirical study on low code programming us-
ing traditional vs large language model support,‚Äù arXiv preprint
arXiv:2402.01156 , 2024.
[76] Y . Wang, Y . Huang, D. Guo, H. Zhang, and Z. Zheng, ‚ÄúSparsecoder:
Identifier-aware sparse transformer for file-level code summarization,‚Äù
arXiv preprint arXiv:2401.14727 , 2024.
[77] J. Zhou, W. Zhong, Y . Wang, and J. Wang, ‚ÄúAdaptive-solver framework
for dynamic strategy selection in large language model reasoning,‚Äù arXiv
preprint arXiv:2310.01446 , 2023.
[78] E. Shi, F. Zhang, Y . Wang, B. Chen, L. Du, H. Zhang, S. Han, D. Zhang,
and H. Sun, ‚ÄúSotana: The open-source software development assistant,‚Äù
arXiv preprint arXiv:2308.13416 , 2023.
[79] Z. Zheng, K. Ning, Y . Wang, J. Zhang, D. Zheng, M. Ye, and J. Chen,
‚ÄúA survey of large language models for code: Evolution, benchmarking,
and future trends,‚Äù arXiv preprint arXiv:2311.10372 , 2023.
[80] Z. Zheng, K. Ning, J. Chen, Y . Wang, W. Chen, L. Guo, and W. Wang,
‚ÄúTowards an understanding of large language models in software engi-
neering tasks,‚Äù arXiv preprint arXiv:2308.11396 , 2023.
[81] C. Chen, J. Su, J. Chen, Y . Wang, T. Bi, Y . Wang, X. Lin, T. Chen, and
Z. Zheng, ‚ÄúWhen chatgpt meets smart contract vulnerability detection:
How far are we?‚Äù arXiv preprint arXiv:2309.05520 , 2023.
[82] W. Zhong, R. Cui, Y . Guo, Y . Liang, S. Lu, Y . Wang, A. Saied, W. Chen,
and N. Duan, ‚ÄúAgieval: A human-centric benchmark for evaluating
foundation models,‚Äù arXiv preprint arXiv:2304.06364 , 2023.
[83] M. Liu, Y . Yang, Y . Lou, X. Peng, Z. Zhou, X. Du, and T. Yang,
‚ÄúRecommending analogical apis via knowledge graph embedding,‚Äù in
Proceedings of the 31st ACM Joint European Software Engineering
Conference and Symposium on the Foundations of Software Engineering ,
2023, pp. 1496‚Äì1508.
[84] X. Jiang, Y . Dong, Z. Jin, and G. Li, ‚ÄúSeed: Customize large language
models with sample-efficient adaptation for code generation,‚Äù arXiv
preprint arXiv:2403.00046 , 2024.
[85] Z. Yang, J. W. Keung, Z. Sun, Y . Zhao, G. Li, Z. Jin, S. Liu, and Y . Li,
‚ÄúImproving domain-specific neural code generation with few-shot meta-
learning,‚Äù Information and Software Technology , vol. 166, p. 107365,
2024.
[86] B. Li, Z. Sun, T. Huang, H. Zhang, Y . Wan, G. Li, Z. Jin, and C. Lyu,
‚ÄúIrcoco: Immediate rewards-guided deep reinforcement learning for code
completion,‚Äù arXiv preprint arXiv:2401.16637 , 2024.
[87] J. Li, G. Li, C. Tao, H. Zhang, F. Liu, and Z. Jin, ‚ÄúLarge language
model-aware in-context learning for code generation,‚Äù arXiv preprint
arXiv:2310.09748 , 2023.
[88] Y . Zhu, J. A. Li, G. Li, Y . Zhao, J. Li, Z. Jin, and H. Mei, ‚ÄúImproving
code generation by dynamic temperature sampling,‚Äù arXiv preprint
arXiv:2309.02772 , 2023.
[89] L. Guo, Y . Wang, E. Shi, W. Zhong, h. Zhang, j. Chen, R. Zhang, y. Ma,
and Z. Zheng, ‚ÄúWhen to stop? towards efficient code generation in llms
with excess token prevention,‚Äù in Proceedings of the 33st ACM SIGSOFT
international symposium on software testing and analysis , 2024.