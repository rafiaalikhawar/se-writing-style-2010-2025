Can an LLM find its way around a Spreadsheet?
Cho-Ting Lee
Thesis submitted to the F aculty of the
Virginia Polytechnic Institute and State University
in partial fulfillment of the requirements for the degree of
Master of Science
in
Computer Science and Applications
Naren Ramakrishnan, Chair
Chang-Tien Lu
John Simeone
May 2nd, 2024
Arlington, Virginia
Keywords: LLMs, data cleaning, end-user programming
Copyright 2024, Cho-Ting LeeCan an LLM find its way around a Spreadsheet?
Cho-Ting Lee
(ABSTRACT)
Spreadsheets are routinely used in business and scientific contexts, and one of the most vexing
challenges data analysts face is performing data cleaning prior to analysis and evaluation.
The ad-hoc and arbitrary nature of data cleaning problems, such as typos, inconsistent
formatting, missing values, and a lack of standardization, often creates the need for highly
specialized pipelines. W e ask whether an LLM can find its way around a spreadsheet and
how to support end-users in taking their free-form data processing requests to fruition. Just
like RAG retrieves context to answer users’ queries, we demonstrate how we can retrieve
elements from a code library to compose data processing pipelines. Through comprehensive
experiments, we demonstrate the quality of our system and how it is able to continuously
augment its vocabulary by saving new codes and pipelines back to the code library for future
retrieval.Can an LLM find its way around a Spreadsheet?
Cho-Ting Lee
(GENERAL AUDIENCE ABSTRACT)
Spreadsheets are frequently utilized in both business and scientific settings, and one of the
most challenging tasks that must be accomplished before analysis and evaluation can take
place is the cleansing of the data. The ad-hoc and arbitrary nature of issues in data quality ,
such as typos, inconsistent formatting, missing values, and lack of standardization, often
creates the need for highly specialized data cleaning pipelines. Within the scope of this
thesis, we investigate whether a large language model (LLM) can navigate its way around
a spreadsheet, as well as how to assist end-users in bringing their free-form data processing
requests to fruition. Just like Retrieval-Augmented Generation (RAG) retrieves context to
answer user queries, we demonstrate how we can retrieve elements from a Python code
reference to compose data processing pipelines. Through comprehensive experiments, we
showcase the quality of our system and how it is capable of continuously improving its
code-writing ability by saving new codes and pipelines back to the code library for future
retrieval.Acknowledgments
I am sincerely grateful to Professor Naren Ramakrishnan, my advisor, for affording me the
opportunity to engage in captivating projects and for his unwavering support, guidance, and
patience throughout the research journey . His expertise, insightful feedback, and encourage-
ment have been invaluable in shaping this thesis and enhancing its quality .
I would like to express my deep appreciation to my committee members, Professor Chang-
Tien Lu and John Simeone, for their invaluable suggestions and meticulous review of my
work. Their feedback has significantly improved the caliber of this thesis.
F urthermore, I would like to extend my gratitude to Shengzhe Xu for his invaluable guidance
throughout my research. His insightful suggestions and constructive feedback have greatly
enriched the quality of my work.
I also want to convey my gratitude for the financial support and valuable data provided by
Jade Saunders and Marigold Norman from W orld F orest ID (WFID), which were instrumen-
tal in enabling this thesis.
Special appreciation is due to Jay Katyan, Patrick Cross, and Sharanya Pathakota for their
assistance with my work. Their dedication and collaborative efforts have been essential to
the completion of the project.
Lastly , heartfelt thanks to my grandparents, my parents, and my brother for their enduring
love, encouragement, and unwavering belief in my capabilities. Their steadfast support has
empowered me with newfound confidence and enabled achievements beyond my expectations.
ivContents
List of Figures viii
List of T ables ix
1 Introduction 1
1.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
1.2 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
2 Review of Literature 4
2.1 Generating F ormulas and SQL queries . . . . . . . . . . . . . . . . . . . . . 4
2.2 LLMs with Information Retrieval . . . . . . . . . . . . . . . . . . . . . . . . 5
2.3 Data Preprocessing Pipelines . . . . . . . . . . . . . . . . . . . . . . . . . . 6
2.4 LLMs and Code Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
3 T radeSweep 11
3.1 Problem Definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
3.2 Prompt Augmentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
3.3 Code Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
3.4 Code Library . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
v4 Evaluation Methodology 19
4.1 Experimental Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
4.2 Baselines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
4.2.1 Baseline 1 : State-of-the-Art (SOT A) simulation - Code generation
purely with LLM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
4.2.2 Baseline 2 : LLM is prompted with candidate codes and the user’s
request . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
4.2.3 Baseline 3 : Provide the entire code library to the LLM without code
descriptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
4.3 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
4.4 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
4.4.1 RQ1: Can T radeSweep automatically preprocess data in an accurate
way? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
4.4.2 RQ2: T o what extent can T radeSweep independently generate valid
code proposals? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
4.4.3 RQ3: When user feedback becomes necessary , how many rounds of
feedback are required to generate a valid code proposal? . . . . . . . 28
4.4.4 Case Study: T radeSweep vs. Baselines . . . . . . . . . . . . . . . . . 29
5 Conclusions 33
5.1 Summary of Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
5.2 Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 345.3 F uture Directions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
5.3.1 Summary T ables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
5.3.2 Data Visualization . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
5.3.3 Anomaly Detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
Bibliography 38List of Figures
1.1 Potential errors and issues in a ”dirty” data. . . . . . . . . . . . . . . . . . . 3
3.1 Overview of T radeSweep . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
3.2 Prompt augmentation flowchart of T radeSweep . . . . . . . . . . . . . . . . 15
3.3 Code generation flowchart of T radeSweep . . . . . . . . . . . . . . . . . . . 16
3.4 Code library maintenance of T radeSweep . . . . . . . . . . . . . . . . . . . . 17
4.1 Comparison of an accepted code proposal of T radeSweep and Baselines . . . 30
5.1 An example table of weight summary for different HS codes. . . . . . . . . . 36
5.2 An example table of emerging trends of shipper companies. . . . . . . . . . . 36
5.3 An example of distribution analysis. . . . . . . . . . . . . . . . . . . . . . . . 37
5.4 An example of network analysis. . . . . . . . . . . . . . . . . . . . . . . . . . 37
viiiList of T ables
1.1 Cleaning performance of our approach (TS: T radeSweep ). Note the improve-
ment from 9% to 98% for the Timber dataset. . . . . . . . . . . . . . . . . . 1
2.1 F eature comparison of SOT A methods vs. T radeSweep . . . . . . . . . . . . 9
4.1 Some of the functions included in our initial code library . . . . . . . . . . . 22
4.2 Preprocessing tasks performed in the three datasets. . . . . . . . . . . . . . 24
4.3 Overall data cleaning performance of T radeSweep vs. Baselines . . . . . . . . 26
4.4 An overview of the data cleaning performance of T radeSweep vs. Baseline 1
(abstracted SOT A method). . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
4.5 First-version code acceptance rate of T radeSweep vs. Baselines . . . . . . . . 27
4.6 The average number of iterations required to generate valid code proposal for
T radeSweep vs. Baselines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
4.7 The average time (in seconds) required to generate a valid code proposal for
T radeSweep vs. Baselines. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
ixChapter 1
Introduction
Pre-trained large language models (LLMs) have been demonstrated to be adept at code
generation from natural language prompts, ushering in a new era in software development [ 10,
12]. Modern IDEs such as Visual Studio Code and IntelliJ provide access to LLMs to
support mistake correction, code recommendation, and the ability to converse in English to
understand and navigate complex codebases [ 23].
1.1 Motivation
T able 1.1: Cleaning performance of our ap-
proach (TS: T radeSweep ). Note the improve-
ment from 9% to 98% for the Timber dataset.
T eak Grain Timber
Init correct-rate 86.60% 85.94% 9.09%
TS correct-rate 97.19% 97.61% 98.17%One of the vexing forms of code that pro-
grammers deal with pertains to processing
tabular data, e.g., in the form of spread-
sheets. Due to the substantial size of the
data (Fig. 1.1 , for example), they are often
beset with issues such as typos, inconsistent
formatting, missing information, and a lack
of standardization. Not solving these issues
beforehand can also cause several research problems in future ML analyses. F or example,
uncleaned and unformatted data that contains irrelevant information can lead to over-fitting
the model, longer training times, and biased classification results. These problems state that
1preprocessing data is a crucial step to avoid less accurate or less interpretable models, cre-
ating the need for highly specialized pipelines [ 1,15] to preprocess (or clean, to be more
specific) the data before proceeding with future analysis.
Data preprocessing involves identifying relevant and information-preserving columns in any
given dataset, where a human analyst determines which categories have the potential to be
utilized for downstream analysis. Once such relevant categories have been determined, the
dataset is ready to undergo cleaning. Throughout the process, the analyst reviews the results
and suggests modifications to the data. However, manually doing this for datasets demands
significant effort, and automation often requires thorough domain knowledge and proficiency
in programming. F urthermore, data cleaning is not simply a matter of resolving inconsisten-
cies and can lead to significant shifts in results downstream. F or example, the mere act of
correcting typos in a dataset may inadvertently lead to over-clustering or under-clustering
of the pertinent column; inaccurately resolving entities can distort the true distribution of
data points. Thus, data cleaning needs to be approached with care and deliberation, with
attention paid to sequential transformations.
It is clear that an automated procedure for data interaction and a more effective way to
perform data preprocessing are urgently needed in place of direct human engagement. So in
this thesis, we ask whether an LLM can find its way around a spreadsheet and how to sup-
port end-users in taking their free-form data processing requests to fruition. Our approach,
T radeSweep (named for its focus on tabular trade datasets), takes English requests for data
cleaning and generates code proposals that can be composed and applied to targeted datasets,
achieving high performance as shown in T able 1.1 . Just like RAG (retrieval-augmented gen-
eration) retrieves context to answer users’ queries, we demonstrate how we can store and
retrieve elements from a code library to compose complex pipelines.Figure 1.1: Potential errors and issues in a ”dirty” data.
1.2 Overview
This thesis is divided into five chapters. Chapter 2surveys some related work focusing on
different aspects of interactions with data and code generation by LLMs. Chapter 3provides
a detailed explanation of how T radeSweep works to automate data preprocessing tasks with
self-enhancements. In Chapter 4, we explain our evaluation methodology on T radeSweep ;
W e design three baselines and compare their performance with T radeSweep on generating
codes for data cleaning, performing an ablation study to further discuss several research
questions. Last but not least, Chapter 5concludes this thesis and proposes potential future
directions for improving T radeSweep ’s limitations.Chapter 2
Review of Literature
2.1 Generating F ormulas and SQL queries
F ormulas and SQL queries are the lingua franca of spreadsheets. ’F ormula Language Model
for Excel’ (FLAME) [ 14] is a T5-based model trained exclusively on Excel formulas that
achieves competitive performance vs. LLMs (such as CodeT5) on facets such as formula re-
pair, formula completion, and similarity-based formula retrieval. Results show that FLAME
achieves the best results in 11 out of 16 evaluation settings and can successfully fix, complete,
and retrieve formulas. However, in data analysis use cases, users may have different tabular
formats (Excel, CSV, etc.) and hope to perform more complex tasks like correcting mis-
spellings using a customized algorithm, filling in blanks by locating external documents, or
even writing new code. This brings out the limitations of FLAME since this tool is restricted
to using pre-defined formulas in Excel.
A substantial amount of research aims to convert natural language text into executable SQL
queries. While LLMs like GPT-4 and Claude-2 demonstrate the capacity to accomplish text-
to-SQL tasks, most current benchmarks concentrate on tiny databases with few rows, which is
not the same as working on massive databases in real-world scenarios. In an effort to narrow
the gap between experimental and practical scenarios, BIRD (Big Bench for Large-scale
Databases) [ 18] is a text-to-SQL benchmark that is the first to provide more effective query
techniques in the context of large and noisy databases. It also explores three challenges:
4handling big and unclean databases, maximizing the effectiveness of SQL execution, and
evaluating outside information sources. While BIRD made it possible for users to access and
modify databases without the need for programming expertise, this approach is limited to
using SQL; As data preprocessing is usually accomplished with Python, it is troublesome to
write complex procedures in SQL.
2.2 LLMs with Information Retrieval
While LLMs are highly effective when fine-tuned for particular NLP tasks, they are inher-
ently limited by their capability to access and precisely manipulate knowledge. Retrieval-
augmented generation (RAG) [ 16] is a runaway success in how it combines retrieval and
generation techniques, leading to numerous offshoots and variations in order to improve
LLMs in understanding and producing human-like text. When compared to task-specific
retrieve-and-extract architectures and parametric sequence-to-sequence models, RAG has
shown superior performance, demonstrating a notable improvement in the factual correct-
ness, specificity , and diversity of the generated language. This line of work led to a blurring
of the lines between information retrieval and language generation.
The development of LLMs has caused a paradigm shift in human information acquisition,
from using information retrieval to search for information, to generating information with
them. As a result, information retrieval systems are now supporting LLMs rather than
just humans. F or instance, Qiaoyu T ang et al. proposed Self-Retrieval [ 24], an end-to-
end information retrieval architecture driven by LLMs. In Self-Retrieval, an information
document is internalized as a corpus into an LLM, and the retrieval process is redesigned as
a process of document generation and self-evaluation carried out using the same model.
The advent of vector database systems such as Qdrant [ 22] and Pinecone [ 21] has contributedto the rapid adoption of RAG and LLM pipelines. With the vector database storing em-
bedded vectors and data items, users can search the database using an embedded query
vector and retrieve the closest matching data results. The database employs multiple dis-
tinct approximate-nearest-neighbor search algorithms and assembles them to retrieve the
queried vector’s neighbors quickly and precisely . As generative AI models advance, the im-
portance of vector database systems continues to grow; By providing fast similarity searches
and closest match queries, it enables LLMs to learn from large datasets.
In this thesis, we adapt the RAG approach in a code composition setting. Specifically , we
retrieve only the most relevant codes and narrow the number of code candidates that are
prompted into the LLM, reducing the prompt context length and allowing the LLM to learn
with the most helpful information.
2.3 Data Preprocessing Pipelines
Data preprocessing is essential for enhancing the consistency and dependability of raw data,
which often includes ‘dirty’ data containing inconsistent formats, typos, missing values, and
outliers. In addition to requiring domain expertise, programmers often employ feature engi-
neering to overcome deficiencies in data quality .
F an et al. investigated and carried out a series of data preprocessing tasks on building oper-
ation data for additional analysis [ 7]. In order to ensure data compatibility with algorithm
analysis, they proposed tasks for building operation data, including data transformation
(encoding categorical columns to ensure data compatibility with algorithm analysis), data
reduction (reducing data dimensions both row-wise and column-wise), data scaling (scal-
ing data into similar ranges using max-min normalization and z-score standardization), and
data partitioning (dividing data into subsets for in-depth analysis). In their study , they gavea thorough overview of both traditional and advanced data preprocessing duties. F urther-
more, they addressed the necessity of automating these jobs to improve the efficiency of data
analysis, which can be solved in this thesis.
Several network-based quantitative and qualitative strategies for timber data analysis were
represented by Charvi Gopal in his work to fight against deforestation, especially illegal
harvesting and over-logging of protected forests [ 9]. F ollowing the computation of transaction
data into adjacency matrices, he analyzed the export-import distribution as well as the
primary trading partners of each export and import country . The research transformed the
matrices into heatmaps to observe differences and qualitative trends between imports and
exports, and to facilitate comparisons on smaller subsets of data. The study also computed
correlation coefficient values from normalized matrices to observe the correlation between
exports and imports, and tree cover loss. Subsequently , he proposed an interactive network-
based visualization tool to provide estimates of illicit transactions and overreported trade
data. The study explored effective techniques for data preprocessing and transaction data
analysis.
2.4 LLMs and Code Generation
Inspired by implementing deep learning techniques for automatic code generation, Jia Li et
al. proposed a sketch-based code generation approach, SkCoder, that can imitate engineers’
code reuse behavior [ 17]. Upon receiving a natural language request by the user, SkCoder
performs online scraping to search for a similar code snippet, then removes pertinent parts
to create a ’code sketch’ and modifies those parts to create the requested code. Although
users without programming experience can successfully generate codes with SkCoder, the
’editing’ feature is done automatically , which means the user is unable to provide feedbackor steer the generated code and therefore must take care to provide a thorough request at
the outset.
In order to enhance codes generated by pre-trained LLMs, Naman Jain et al. introduced
Jigsaw, a tool designed to help language models understand program syntax and semantics
and improve their performance based on user feedback [ 13]. By providing the LLM with a
natural language string representing the user’s request, along with samples of input-output
data or test case examples, Jigsaw can generate a code snippet that solves the task. After the
code is generated, Jigsaw then executes the code to ensure it passes all provided test cases
and quality checks. The process is designed to correct frequent and recurrent errors in the
code, including referencing errors, argument errors, and semantic errors. However, Jigsaw
requires users to specify the exact column in the dataset to apply the generated codes, in
addition to providing example output data. Thus, if a user makes accidental typos in column
names or if the user request is imprecise, the resulting performance will degrade.
While LLMs are powerful at writing codes in response to user prompts, they still struggle
with algorithmic challenges and typically require human verification. T o tackle the issue,
Kexun Zhang et al. presented ALGO, a framework that combines algorithmic programs
with oracles generated to guide code generation and ensure accuracy [ 25]. After creating a
reference oracle that consists of a list of correct but ponderous code programs, ALGO first
asks the LLM to search through the oracle and select a code that solves the user request.
Then, it employs a second LLM as a coder to produce a code proposal that performs faster
(but possibly wrong). Both the coder’s proposal and the oracle’s search result go through the
same test cases in order to compare outputs, and the coder further refines its code proposal
if necessary . ALGO allows users to communicate with the LLM in English to generate
codes, but they are not able to provide feedback or confirm whether the code can handle
their request. This makes it challenging to customize generated codes, even if the user onlydesires to make minor adjustments.
V ery recently (March 2024), Cognition unveiled Devin [ 4], their ’completely autonomous AI
software engineer’ that claims to manage the entire software development process. According
to the company , Devin stands out with its ability to manage the entire software development
process, from writing codes and addressing bugs to the final code execution. Cognition also
claims that Devin is powerful in reading documents and extracting useful information, writ-
ing codes and test cases, executing codes, and fixing bugs automatically . This enables data
analysts without programming experience to successfully generate data preprocessing codes
and show example executions to the user. However, since Devin is not currently publicly
accessible, we are unable to evaluate our approach against it. It is also unknown whether
Devin can interact with massive datasets; F or example, to automatically fix misspellings in a
data column without the need to define a specific algorithm. F urthermore, it is also unclear
whether Devin can be run locally; If not, then it might not be the best idea to give Devin
crucial material since the information will not remain confidential.
T able 2.1: F eature comparison of SOT A methods vs. T radeSweep
SkCoder Jigsaw ALGO Devin T radeSweep
F ully-English communication ✓ ✓ ✓ ✓
Simple request description ✓ ✓
Improve code by feedback ✓ ✓ ✓
Novel code generation ✓ ✓ ✓ ✓ ✓
Ongoing code library update ✓
Visualized execution demo ✓ ✓
While SkCoder, Jigsaw, ALGO, and Devin are all powerful code-generation tools, we have
discovered certain limitations in each. Therefore, our goal is to develop an agent to solve
these issues. Without requiring extensive programming knowledge or providing specific
details like column names or algorithms in the request, T radeSweep helps users generatecode and enhance it through English-language interactions. T radeSweep produces novel code
functions and stores them in a code library , aiming for continuous optimization. Lastly ,
T radeSweep provides users with visualized execution results on sample data to confirm the
validity of the code. T able 2.1 depicts a comparison between T radeSweep and the systems
surveyed above.Chapter 3
T radeSweep
The goal of T radeSweep is to utilize LLM’s code-writing abilities to produce executable
programs for data preprocessing tasks. Users are only required to provide a dataset in
either CSV or Excel format and enter a preprocessing request. As depicted in Fig. 3.1 ,
T radeSweep consists of three primary components:
• Prompt augmentation : we employ information retrieval techniques to select the
top-k relevant codes from the code library . (Chapter 3.2 )
• Code generation : the LLM generates a code proposal along with visualized samples
of execution results, awaiting user feedback. (Chapter 3.3 )
• Code library : we build a reference document that includes classic Python scripts for
data preprocessing tasks. (Chapter 3.4 )
3.1 Problem Definition
LetD represent a table with nrows and m columns, with column names c1, c2, ..., c m(this
set of column names is denoted as C). Each element value xij, where i∈ {1, ..., n}and
j∈ {1, ..., m }, represents the value of the j-th feature of the i-th data record.
Typically , in practical applications, it is necessary for individuals to perform cleaning (e.g.,
11Figure 3.1: Overview of T radeSweep
inconsistent formats, outliers, missing values) and preprocessing (e.g., normalization, label
encoding) on data D before applying an ML model. Let A be a subset of {c1, c2, ..., c m},
which denotes the collection of columns that require some form of processing.
T radeSweep ( M ) aims to receive a preliminary description rof the user’s data preprocessing
requirements in English and generate a Python code ˆfand a processed dataset ˆD automat-
ically .
Code Generation (Chapter 3.3 ): In order to achieve this goal, we begin by constructing
a prompting curriculum denoted as P= [r,C]. This curriculum is then inputted into an LLM
as a prompt to develop a Python code that satisfies the specified requirements r. Using the
code created by T radeSweep (code proposal G), we will additionally execute and evaluate G
on a subset of D to obtain an execution outcome O that can be presented to the user so
that they can validate the code proposal without any programming expertise. It should be
noted that in the aforementioned process, the specific columns A that require modification
are automatically determined by the model M .
A voiding LLM hallucination: In order to address hallucination within the context ofautomated data preprocessing, we expanded the existing curriculum P= [r,C]intoP=
[r,C,F], where F denotes a finite set of closely interconnected fundamental functions that
serve as a reference for LLM. It is important to recognize that this offers a dual advantage:
1) The occurrence of hallucinations is decreased; 2) LLMs can develop new code instead
of relying solely on the fundamental reference function. Our solution utilizes an adaptable
code library L to implement the aforementioned design (in Chapter 3.2 ). Initially , the
request rwill be transformed into an embedding representation emb r. Then, a set of k
relevant fundamental functions F={f1, ..., f k}will be extracted from the code database
⟨emb i,function i⟩based on the minimum cosine similarity min i∈Lcosine (emb r,emb i). Once
a new code proposal Gis successfully developed to meet complex requirements r, typically
through interaction with humans, it is then saved in the library L. This allows the library
L to continually learn and provide more accurate and advanced reference functions in the
future.
Human F eedback: Given the user’s limited programming skills, we present the code G
along with an execution result O to simplify the user’s task of verifying the code. This
allows the user to focus on determining if the processed data aligns with their domain knowl-
edge of the data, rather than examining the code in detail. If the intended outcome is not
achieved, T radeSweep will generate a continual conversation P= [r′,C,F], where r′repre-
sents a revision need. In the Results (Chapter 4.4 ), we demonstrate how T radeSweep achieves
a high probability of passing on all examined datasets on the first attempt and adapts faster
than baselines when revisions are required.3.2 Prompt Augmentation
F ollowing the submission of the user’s request for data preprocessing, T radeSweep examines
all functions contained within the code library to learn and utilize them as references (prompt
lengths are minimized using information retrieval techniques).
The code library is made up of code functions that are commonly applied in various prepro-
cessing tasks. In addition to the functions, data information is also prompted to the LLM
to facilitate its comprehension of the dataset structure. However, inputting both the entire
code library and data information could lead to a lengthy prompt containing redundant
information, not only reducing the LLM’s performance in accurately finding the most corre-
sponding function, but also significantly delaying the response time of the LLM. Therefore,
it is usually a good idea to shorten the input context. Instead of presenting all code func-
tions in the prompt, we employ information retrieval techniques to perform a more accurate
selection of code functions and provide the LLM with the most relevant and beneficial codes
to learn from.
The approach of our prompt augmentation involves embedding the user request and querying
it in the code library , which is represented as a vector database. As a result, the vector
database compares the embedded request with its vectors, which are embedded descriptions
of code functions. It then retrieves the top-k code functions that are most closely associated
with fulfilling the user’s request. Finally , these retrieved code functions are integrated with
the user’s request and data information to form a prompt for the LLM to generate code.
This process not only effectively shortens the prompt length and LLM response time but
also assists the LLM in concentrating on functions that are most related to achieving the
user’s request. (see Fig. 3.2 )Figure 3.2: Prompt augmentation flowchart of T radeSweep
3.3 Code Generation
Once the top-k relevant codes have been retrieved as candidate codes, a prompt for the LLM
is created by combining the candidate codes with data information and the user’s request.
The data information is collected based on the dataset provided by the user; It can be any
type of information that aids the LLM in understanding the data structure, such as a list of
column names or a limited number of data rows as samples.
The LLM examines the provided information and produces a code proposal that is expected
to accomplish the requested task. If the LLM does not identify any candidate code that seems
to align with the user’s request, it produces a novel code function. In addition, the process
of code generation will perform iterative code enhancements in the following scenarios after
a code proposal is generated:
1. Incorrect code proposal format : W e provided the LLM with a response template,
asserting that the complete code proposal should consist of a Python code function
that reads in a dataframe and returns it at the end, and a call function for applying
the code to the data. If either one is not generated, the LLM automatically rolls back
to re-generate.
2. Execution error : Once a code proposal is generated in the correct format, the codesare tested on some sample data. If the execution fails for any reason, including bugs,
syntax errors, exceptions, or other factors, both the code and its corresponding error
message are prompted back to the LLM for a revision of the generated code.
3. User’s feedback : In addition to the code proposal, users are also provided with a
visualized execution result of sample data. This result displays some examples of input
values and their corresponding output values if the code is applied. F ollowing the user’s
observation of the code proposal and execution outcomes, the user can ask for code
revision by giving feedback that describes a fix request to the LLM.
Finally , if the user suggests that both the code proposal and execution examples are correct,
the code will be applied to the target dataset, as shown in Fig. 3.3 . F or situations in which
the generated code is novel or when the user requests to save multiple functions into a single
pipeline, we add this newly generated code or series of codes to our code library .
Figure 3.3: Code generation flowchart of T radeSweep3.4 Code Library
Due to complicated data preprocessing tasks, such as those that require specific algorithms
or involve multiple datasets, a code library is beneficial to serve as a reference document for
the LLM to learn and follow when generating code proposals. Within T radeSweep ’s code
library , each function is responsible for a certain data preprocessing task. Each code function
also contains comments describing its usage.
Figure 3.4: Code library maintenance of T radeSweep
T o enhance the efficacy of T radeSweep , we also developed a code library capable of support-
ing the addition of new functions as interaction progresses. New functions shall be added to
the library in the following circumstances:
1. Novel code generation : When the LLM does not identify any candidate code that
corresponds to the user’s request after analyzing the retrieved functions, it develops
a novel code proposal. Given that the newly generated code is not included in the
library , we incorporate it back into the library to enhance efficiency and accuracy for
future code generations.
2. Pipeline creation request : After a series of code functions have been generated, the
user can request that the entire procedure be stored as a pipeline. During this process,
the several functions that were previously applied to the dataset are combined into asingle code function, which is then added to our code library .
As demonstrated in Fig. 3.4 , in order to save a new code into the code library , we use
the LLM to write a description of the code and add it as comments. Afterward, both the
new code function and the description are added back into our vector database code library ,
where a new query vector is constructed using the function description, and its corresponding
payload is generated by the code.Chapter 4
Evaluation Methodology
In this chapter, we explain how we setup the environment for experiments, as well as some
baselines designed to be compared and evaluated against T radeSweep .
4.1 Experimental Setup
Data : F or this study , we used shipment-level bill of lading data [ 8] that captures business-
to-business international trade and highlight the complexity of supply chains that enable our
globalized world to operate. The United States, like all countries, uses trade policy , tariffs,
and trade sanctions to enforce objectives related to foreign policy and national security , as
well as other strategic goals such as detecting and deterring trade in illegally harvested or
produced products. The ability to identify specific shipments that may be in contravention
of economic sanctions, have high tariff rates, or be consistent with suspicious or illegal
activity depends largely on how well the data is initially cleaned and preprocessed. Due to
the complexity of global trade and the incentives that companies face to reach destination
markets and circumvent sanctions in the aforementioned transactions, there is potential for
the involvement of third-party entities that may be located in neither the initial source nor
the final destination country .
Specifically , we investigate three datasets that pertain to shipments involving imports and
exports across multiple nations for commodities that have been subject to recent import
19prohibitions, high tariff rates, and sanctions, and all may contain risks associated with
origin fraud [ 2,3,5]. The datasets are given as follows:
1. T eak : contains 69,134 teakwood transactions exporting from 116 countries to the
United States, spanning from July 1, 2007, to August 10, 2023 (5,885 days in total).
(According to the source from Panjiva [ 20])
2. Grain : consists of 145,217 grain transactions from Russia to 118 global destinations,
starting from May 20, 2021, to November 30, 2022 (560 days in total). (According to
sources ExportGenius [ 6] and ImportGenius [ 11])
3. Timber : contains 3,087,822 timber exports from Russia to 173 countries, commencing
from October 20, 2021, to March 31, 2023 (528 days in total). (According to sources
ExportGenius and ImportGenius)
These transaction data span a considerable amount of time and were manually entered,
making them prone to errors such as typos, inconsistent formatting, missing information,
and other potential issues. In this case, examining data becomes a laborious and time-
consuming task for analysts, necessitating the urgency and importance of preprocessing the
data.
V ector Database : V ector databases are a popular method of interacting with data repre-
sentations, commonly referred to as vectors or embeddings. The utilization of these emerging
databases is experiencing significant growth and is widely adopted in various applications,
including semantic search and recommendation systems. In T radeSweep , we use Qdrant,
a widely recognized and rapidly expanding vector database, to serve as the information re-
trieval system. Qdrant is a vector similarity search engine that offers users an API for storing,
searching, and managing data. In comparison to other databases available on the market,it outperforms many of its competitors in terms of query speed and retrieval accuracy . In
addition, it enables customers to deploy the system locally . Given our need to maintain the
confidentiality of our transaction data, Qdrant is an ideal option for us.
Large Language Model : LLMs have recently shown remarkable proficiency in natural
language processing tasks as well as in other domains. In addition, the development of code
generation in LLMs is experiencing rapid growth. Nowadays, there is a vast selection of
robust models that are capable of efficient code generation. Among the various kinds of
models, we employed CodeLlama-Instruct [ 19] in our study and utilized it to learn from
retrieved codes, generate executable Python functions, and modify codes in accordance with
user requests. Unlike API-based models like ChatGPT, CodeLlama has the benefit of en-
abling customers to execute the model locally . Due to the fact that it guarantees the security
of the data provided to the LLM, this aspect is quite important in our study .
Initial Code Library : W e established our initial code library with a selection of 13 widely
recognized and commonly used data preprocessing functions. Moreover, the functions we
adopted are used for general data preprocessing, which means these functions can be applied
to any area of data that the user inputs and are not limited to trade interactions.
Listed in T able 4.1 , the functions we implemented in our initial code library include standard-
izing date formats, removing punctuation marks, correcting typos, filling in missing values
based on another column, etc.
Hardware Environment : Our work is carried out using a T esla P40 NVIDIA driver, which
is equipped with 8 cores, 38 GB of RAM, and 500 GB of disk memory .T able 4.1: Some of the functions included in our initial code library
T ask Explanation
Remove columns Delete an unwanted column.
Remove rows Delete rows that satisfy a certain condition.
Clean numbers Remove non-numeric symbols and convert the value to numbers.
Clean stringsRemove punctuation marks, quotation marks, and any extra
spaces.
Clean dates Standardize all date values to a YYYY-mm-dd format.
Correct misspellingsApply word-embedding and clustering to a column to cluster
similar values. Then, in each cluster group, find the most
frequent value and correct others to that.
Compare columns
and cleanBetween a to-clean column and a reference column, group the
two columns. Then, for all to-clean values that have the same
reference value, find the most frequent to-clean value and
update others on this.
Lookup documentGiven a target column in the dataset and an external CSV/Excel
document, map the target values to a reference column in the
document, then create a new column with the mapped values.
4.2 Baselines
In order to determine the efficiency of T radeSweep in generating meaningful codes, we devel-
oped three baselines to be used for comparison:
4.2.1 Baseline 1: State-of-the-Art (SOT A) simulation - Code gen-
eration purely with LLM
T o compare the effectiveness of T radeSweep in achieving our primary objective, code gener-
ation, we have abstracted the code writing components of SOT A tools (SkCoder, Jigsaw,
ALGO, etc.) and designated them as Baseline 1. This abstraction was needed in order to
accommodate the various usages and features of these tools. During this stage, we solely
rely on the LLM for code generation without applying other augmentations. In the absenceof the code library as a reference document, the LLM is required to produce code indepen-
dently . In this case, the user’s description of the request is the most crucial component. It is
possible that modifying the code proposal will require a considerable amount of effort, and
the result code may not clean the data as successfully as expected.
4.2.2 Baseline 2: LLM is prompted with candidate codes and the
user’s request
In T radeSweep , we incorporate data information, the top-k candidate codes, and the user’s
request to be inputted into the LLM. By providing data information, the LLM can adjust its
code according to the actual data. Otherwise, the LLM is entirely dependent on prompts and
can only make guesses regarding the columns or data that the user is referring to. Therefore,
we designed baseline 2, where the LLM is only given the candidate codes and the user’s
request. W e assume that the absence of data information available for the LLM may in
some way have an impact on its performance.
4.2.3 Baseline 3: Provide the entire code library to the LLM with-
out code descriptions
As the candidate codes are provided to the LLM, they are accompanied by their function
descriptions, formatted as comments. During the process of examining the descriptions, the
LLM can gain a deeper understanding of the candidate codes, enabling it to generate a code
proposal that employs a similar algorithm. So, in baseline 3, our objective is to investigate
the impact of eliminating function descriptions on the code generation results. Additionally ,
since our vector database retrieves candidate codes by comparing the user’s request to codedescriptions, we have also eliminated the prompt augmentation step of retrieving relevant
codes. Rather than giving k candidate codes, we now provide the entire code library (without
descriptions) into the LLM as part of the prompt.
4.3 Evaluation
F or each dataset, we consider various data preprocessing tasks depending on its content,
which are outlined in T able 4.2 .
T able 4.2: Preprocessing tasks performed in the three datasets.
T eak Grain Timber
Remove columns ✓ ✓
Clean dates ✓ ✓ ✓
Clean numbers ✓ ✓
Clean strings ✓ ✓
Correct misspellings ✓ ✓
Compare columns and clean ✓ ✓ ✓
Lookup documents ✓ ✓
Others (not in library) ✓ ✓ ✓
Each task is evaluated with each of the baselines as well as T radeSweep . T o conduct evalua-
tion, we input data preprocessing requests to each baseline and T radeSweep to produce codes
for all three datasets. All user requests sent to the baselines and T radeSweep are identical
when performed on the same data. W e record the first and final versions (in the event of
code revision) of generated codes, along with the amount of time it took to generate each
code and the number of revisions that were carried out. After all codes have been generated,
they are then applied to our initial uncleaned data. The execution outputs are subsequently
compared to manually preprocessed data in order to assess the performance of each baseline.A task could be applied to multiple columns in each dataset; In this case, we perform the
same task repeatedly and ask the LLM to generate codes, each time using the same algorithm
but adopting small changes, to tailor the code to certain columns in the dataset.
4.4 Results
F rom the codes generated by all baselines and T radeSweep , we consider the following research
questions:
4.4.1 RQ1: Can T radeSweep automatically preprocess data in an
accurate way?
In this chapter, we denote our initial uncleaned data as Init and the data that was manually
preprocessed as Ground T ruth (GT) for simplicity .
F ollowing the generation of the codes by baselines and T radeSweep , the functions are then
applied to the initial data for data preprocessing. As shown in T able 4.3 , the execution results
obtained from T radeSweep were the most similar to Ground T ruth (with Baselines 2 and 3
yielding similar outcomes to T radeSweep as a result of referencing and learning from the
code library for code generation). On the other hand, applying codes generated by Baseline
1 leads to outputs that differ significantly from GT; this disparity can be attributed to the
LLM writing functions independently in the baseline, which ultimately employed different
algorithms than the code library to accomplish the tasks.
After executing all generated codes by baselines and T radeSweep to the datasets, the overview
of data preprocessing performances of Baseline 1, an abstracted version of SOT A methods, is
represented in T able 4.4 . Compared to Baseline 1, T radeSweep is fully capable of generatingT able 4.3: Overall data cleaning performance of T radeSweep vs. Baselines
Init GT Baseline 1 Baseline 2 Baseline 3 T radeSweep
T eak# of cols 127 26 26 26 26 26
# of rows 69,134
NaNs (%) 48.77 7.57 10.94 7.09 7.09 7.09
incorrect
formats (%) 19.99 0 29.98 9.86 1.03 1.03
typos (%) 9.29 0 25 3.92 3.92 3.92
Grain# of cols 56 61 61 61 61 61
# of rows 145,217
NaNs (%) 66.94 62.10 3.76 63.07 66.22 63.57
incorrect
formats (%) 2.84 0 9.68 0.09 0.08 0.08
typos (%) 18.06 0 14.43 2.37 6.65 3.21
Timber# of cols 29 23 23 23 23 23
# of rows 3,087,822
NaNs (%) 3.19 4.55 8.68 4.75 8.24 4.42
incorrect
format (%) 87.93 0 64.86 0.63 12.81 0.63
typos (%) 93.19 0 74.69 4.04 6.83 2.76
suitable and executable codes that clean the data to have a similar correct-value rate as
Ground T ruth.
4.4.2 RQ2: T o what extent can T radeSweep independently gener-
ate valid code proposals?
In this chapter, we perform experiments to evaluate the ability of T radeSweep to produce
valid codes without the need for user feedback.
This research question is evaluated by the user’s acceptance of the code generated by the
LLM on its first attempt. The rate of codes approved during LLM’s first attempt is recorded
in T able 4.5 . Each value denotes the number of codes that were accepted by the user on theT able 4.4: An overview of the data cleaning performance of T radeSweep vs. Baseline 1
(abstracted SOT A method).
T eak Grain Timber
Init correct-rate 86.60% 85.94% 9.09%
GT correct-rate 100% (time-consuming)
SOT Acorrect-rate 73.09% 87.65% 29.58%
first-attempt valid rate 41.66% 27.78% 62.5%
TScorrect-rate 97.19% 97.61% 98.17%
first-attempt valid rate 83.33% 66.67% 75%
LLM’s first try , divided by the total number of accepted codes.
T radeSweep successfully generated the most user-acceptable codes in all three datasets with-
out requiring any further revision. The lowest acceptance rate was observed in Baseline 2 as
a result of the absence of column name information inputted into the LLM. Thus, despite
employing the identical algorithm as T radeSweep in the produced codes of Baseline 2, the
functions were applied to incorrect columns, and users were compelled to request revisions
and specify the desired column names.
T able 4.5: First-version code acceptance rate of T radeSweep vs. Baselines
Baseline 1 Baseline 2 Baseline3 T radeSweep
T eak 5/12 1/12 9/12 10/12
Grain 5/18 5/18 12/18 12/18
Timber 15/24 5/24 17/24 18/244.4.3 RQ3: When user feedback becomes necessary , how many
rounds of feedback are required to generate a valid code
proposal?
If the generated code does not fulfill the user’s requirements for the desired task, we invoke the
LLM again to make additional modifications to the code. The average number of iterations
that were required in these cases and the average duration (in seconds) needed are recorded
in T able 4.6 and T able 4.7 .
T able 4.6: The average number of iterations required to generate valid code proposal for
T radeSweep vs. Baselines
Baseline 1 Baseline 2 Baseline3 T radeSweep
T eak 1.92 2.08 1.42 1.16
Grain 2.11 2.17 1.39 1.33
Timber 1.5 2.13 1.42 1.29
T able 4.7: The average time (in seconds) required to generate a valid code proposal for
T radeSweep vs. Baselines.
Baseline 1 Baseline 2 Baseline3 T radeSweep
T eak 134.72 141.15 555.46 167.55
Grain 104.87 137.43 369.99 117.40
Timber 42.71 134.65 269.34 94.08
Based on the results, T radeSweep is capable of generating codes that successfully meet the
user’s demands at the LLM’s initial attempt, achieved by utilizing knowledge from the code
library . On the other hand, Baseline 1 produces unstable outcomes on the first attempt
since the LLM generated codes independently; several codes were incapable of meeting the
user’s vague request (e.g., ”Clean the numbers in net weight”) and required the user to give
a more explicit explanation (e.g., ”Clean the numbers in net weights: remove commas, thenconvert the string value to float numbers”). Moreover, due to the absence of column names in
Baseline 2, nearly all preprocessing tasks required revision; It was essential to specify accurate
column names to ensure the functionality of the generated code (for example, the prompt
should have read ”Clean numbers in the NetW eight column” instead of ”Clean numbers in
weights”).
While Baseline 3 could also generate suitable codes on the LLM’s first attempt, it needed
slightly more user involvement than T radeSweep . W e observed that in this baseline, although
the LLM selected the correct code to modify , the lack of function descriptions led the LLM to
over- or under-modify the reference code, necessitating further revision requests. Moreover,
this baseline took a considerably longer time to generate a code compared to the other two
baselines and T radeSweep . This was due to providing all codes in the library to the LLM
rather than inputting the most relevant ones. The length of the prompt sent to the LLM
exceeded that of other baselines, resulting in a much slower process and a greater expenditure
of time.
4.4.4 Case Study: T radeSweep vs. Baselines
T o gain a more comprehensive understanding of the disparity in code generation perfor-
mance between various baselines and T radeSweep , we analyze and compare the initial code
proposals produced (before the user provides any feedback) in each scenario while ensuring
all baselines were given the same user request input for each preprocessing task.
Using the example of cleaning the column ”Shipper Country”, Fig. 4.1 illustrates the code
proposal generated by the three baselines and T radeSweep . In this experiment, the user’s
request was inputted as ”Compare values in shipper country with shippers, and clean country
names with the same shipper to the most frequent value. ” The ideal procedure is to enablethe LLM to employ the ”compare_and_clean” function from the code library and apply it
to the ”Shipper Country” column while comparing with the ”Shipper” column.
Figure 4.1: Comparison of an accepted code proposal of T radeSweep and Baselines
T radeSweep : In T radeSweep , the top 3 relevant functions were retrieved from the code
library and inputted into the LLM. The code proposal demonstrates that the LLM effec-
tively learned from the ”compare_and_clean” function in the code library . F urthermore, by
inputting the data column names into the LLM, T radeSweep is able to determine the precise
columns that the user wishes to apply the function to, even when not providing accurate
names. In this scenario, the desired columns are ”Shipper” and ”Shipper Country” . In the
proposed code, the function groups the data by the ”Shipper” column. Then, within each
group, it identifies the most frequent value of ”Shipper Country”, and modifies the other
shipper countries so that they reflect this value.Baseline 1: While the user’s request remains consistent across all baselines, Baseline 1,
which lacks knowledge of the code library , assumed the user’s intended meaning of ”compare
and clean” . Although the code proposed by Baseline 1 successfully grouped the columns and
determined the most frequent shipper country , it erred by converting all strings to lowercase
and failing to account for potential scenarios where the most frequent value found was NaN.
Consequently , the resulting cleaned data exhibited a higher prevalence of NaNs and wrongly
formatted country names. On the other hand, T radeSweep was able to successfully acquire
the function defined in the code library; This function identifies the most frequent non-NaN
country value, thereby filling in missing values in Shipper Country without altering its string
format.
Baseline 2: In Baseline 2, the candidate codes and the user’s request are provided to
the LLM, while data information is not inputted. The generated code proposal shows that
Baseline 2 successfully obtained the ”compare_and_clean” function from the code library
and performed adjustments following the user’s request. Nevertheless, the function was
unable to comprehend the particular columns that the user intended to apply due to the
lack of data information. Instead of using actual column names, the LLM was only able to
depend on the user’s request and make assumptions about the column names, which ended
up using ”shippers” and ”shipper_country” . In contrast, when data information is provided,
like in T radeSweep , the LLM considers the column description provided by the user and
selects the most suitable columns to apply the function to. In this case, the correct columns
”Shipper” and ”Shipper Country” were selected.
Baseline 3: Instead of providing only the top 3 relevant functions to the LLM, the entire
code library is inputted in Baseline 3, but without function descriptions. By observing the
generated results from this baseline, the LLM is capable of comprehending the usage of each
given function, despite the absence of their descriptions. The LLM successfully utilized the”compare_and_clean” function from the code library , which was then modified and applied
to the appropriate columns based on understanding the user’s request. However, as a result
of inputting all functions in the code library into the LLM, the prompt grew excessively
long and contained redundant information. Consequently , Baseline 3 required a total of 698
seconds to read the given information and generate codes, which is significantly slower in
comparison to other baselines. This further indicates that by minimizing the number of
functions offered to the LLM, as in T radeSweep , we can effectively shorten the prompt and
therefore expedite the process.Chapter 5
Conclusions
The contributions of this thesis, T radeSweep , are as follows:
1. T radeSweep utilizes English conversations to understand and respond to users’ requests
with Python code encompassing preprocessing functions. This supports three main
capabilities: 1) It produces new code when requested for a completely new task; 2)
It can precisely modify its proposed code based on users’ feedback in English; 3) The
proposed code is automatically tailored to the target data, including accurate column
names and suitable algorithms.
2. T radeSweep develops and continuously grows a library of fundamental data prepro-
cessing codes by storing executable functions that have been successfully deployed
previously . Augmentation of the code library supports the composition and creation
of elaborate data pipelines.
3. T o incorporate feedback from users who lack programming expertise, T radeSweep offers
both code proposal and execution results on example data. This allows users to de-
termine whether to accept T radeSweep ’s proposal or to make modifications based on
output data visualizations, instead of focusing on code specifics.
4. W e perform extensive experiments on three trade datasets. Results show that T radeSweep is
capable of generating executable and efficient codes for data preprocessing, and also
significantly reduces time and effort for data analysts.
335.1 Summary of Results
The rise of automation and programming support by LLMs has rapidly decreased the turn-
around time for end-users in processing massive spreadsheets. T radeSweep can be viewed as
an LLM-driven data preprocessing agent that retrieves suitable functions from a code library
and modifies them to achieve the data preprocessing task. Our results have demonstrated the
effectiveness of T radeSweep in practical data transformation contexts. Our study conducted
experiments on three transaction datasets to perform data cleaning, and the results indicate
that T radeSweep can efficiently perform the requested task using minimal user interaction.
The ablation study also demonstrates that the utilization of a vector database for information
retrieval contributes to the enhancement of the speed of code generation. F urthermore,
providing data information and function descriptions to the LLM facilitates the generation
of more accurate codes without requiring user feedback for modification, enabling users with
less programming proficiency to be effective.
5.2 Limitations
Though T radeSweep shows promising results in code generation performance compared to
SOT A tools and baselines, there are also some limitations that await resolution.
First, when the user is performing a data preprocessing task where T radeSweep cannot find a
related function in the code library , it generates a novel code. In this case, T radeSweep performs
in the same way as SOT A tools and generates codes purely by the LLM, requiring the user
to give more iterations of feedback and modification in order for the LLM to generate valid
code functions.
Also, while T radeSweep responds with a code proposal faster and more accurately than otherbaselines, it still requires some time when the desired data preprocessing task is relatively
complicated. Apart from changing hardware requirements, finding a solution to optimize
the response time is important to enhance the user experience.
F urthermore, the sample execution might not be sufficient for users to evaluate the code
when performing some tasks. F or example, correcting misspellings in company names might
lead to over- or under-correcting, and by observing a limited number of correction examples,
users are not able to confirm whether the code successfully captures all corner cases. In this
scenario, it is helpful to perform an apply-test on the target data and generate a comparison
of names before and after correction (just like find-replace in word processing tools). However,
applying the code to the full dataset takes much more time than only applying it to sample
data; It is also important to speed up this process and provide much faster results for users.
5.3 F uture Directions
In addition to continuing to improve T radeSweep ’s performance and resolve the limitations
mentioned above, in this chapter, we discuss what can be accomplished in the future for
T radeSweep to provide a more user-friendly , efficient, and accurate data preprocessing expe-
rience.
5.3.1 Summary T ables
After preprocessing the data, we could use the LLM to write codes that generate summary
documents based on some condition. F or example, Fig. 5.1 shows an example of a weight
summary table that records the transaction net weight of different HS codes during a specific
period of time.Figure 5.1: An example table of weight summary for different HS codes.
W e can also use the LLM to generate tables for emerging trends. As shown in Fig. 5.2 , the
generated table will show information for each shipper and HS code. F or each entity , we list
consignee companies that interacted with the shipper before and after a specific date, then
highlight those that appeared in both time frames.
Figure 5.2: An example table of emerging trends of shipper companies.
5.3.2 Data Visualization
Besides generating summaries and tables, providing visualization plots or figures for these
results is often helpful for data analysts to examine the data more efficiently . Fig. 5.3 shows
an example of distribution analysis; the plot represents the top 10 countries with the highest
transaction amount (weight) for each month in the dataset. In Fig. 5.4 , we draw a world
map and connect two countries if the difference in their transaction amount (weight) with
Russia is under a specific δvalue.
5.3.3 Anomaly Detection
Finally , since one eventual outcome of trade analytics is to detect suspicious trades, it is also
desirable to implement machine learning technology into T radeSweep . It would be beneficial
to train ML models to detect and highlight anomaly entities, helping flag shipments for
further scrutiny .Figure 5.3: An example of distribution analysis.
Figure 5.4: An example of network analysis.Bibliography
[1] Sadhvi Anunaya. Data preprocessing in data mining: A hands
on guide. URL https://www.analyticsvidhya.com/blog/2021/08/
data-preprocessing-in-data-mining-a-hands-on-guide/ .
[2] AP News. Russia smuggling Ukrainian grain to help
pay for Putin’s war. URL https://apnews.com/article/
russia-ukraine-putin-business-lebanon-syria-87c3b6fea3f4c326003123b21aa78099 .
[3] Lauren Aratani. US imports of ‘blood teak’ from Myanmar continue despite sanctions.
The Guardian , May 2023. ISSN 0261-3077. URL https://www.theguardian.com/
world/2023/may/16/myanmar-teak-wood-import-sanctions .
[4] Devin. Introducing devin, the first ai software engineer. URL https://www.
cognition-labs.com/introducing-devin .
[5] European Parliament. T rade restrictions / import ban on Russian and Belarusian
wood and timber products, 2022. URL https://www.europarl.europa.eu/doceo/
document/P-9-2022-000973_EN.html .
[6] ExportGenius. Exportgenius: T rade intelligence online platform. URL https://www.
exportgenius.in/ .
[7] C. F an, M. Chen, X. W ang, J. W ang, and B. Huang. A review on data preprocessing
techniques toward efficient and reliable knowledge discovery from building operational
data, 2021. URL https://doi.org/10.3389/fenrg.2021.652801 .
38[8] Aaron B. Flaaen, Flora Haberkorn, Logan T. Lewis, Anderson Monken, Justin R.
Pierce, Rosemary Rhodes, and Madeleine Yi. Bill of Lading Data in International
T rade Research with an Application to the COVID-19 Pandemic. Finance and Eco-
nomics Discussion Series 2021-066, Board of Governors of the F ederal Reserve System
(U.S.), October 2021. URL https://www.federalreserve.gov/econres/feds/files/
2021066pap.pdf .
[9] Charvi Gopal. Network visualization and anomaly detection in international timber
trade flows, 2021.
[10] Martin Heller. Large language models and the rise of the ai code
generators. URL https://www.infoworld.com/article/3696970/
llms-and-the-rise-of-the-ai-code-generators.html .
[11] ImportGenius. Importgenius: Global trade data. URL https://www.importgenius.
com/ .
[12] Prathamesh Ingle. T op artificial intelligence (ai) tools that can generate code
to help programmers (2024). URL https://www.marktechpost.com/2024/03/14/
top-artificial-intelligence-ai-tools-that-can-generate-code-to-help-programmers/ .
[13] Naman Jain, Skanda V aidyanath, Arun Iyer, Nagarajan Natarajan, Suresh
Parthasarathy , Sriram Rajamani, and Rahul Sharma. Jigsaw: Large language mod-
els meet program synthesis, 2021.
[14] Harshit Joshi, Abishai Ebenezer, José Cambronero, Sumit Gulwani, Aditya Kanade,
V u Le, Ivan Radiček, and Gust V erbruggen. Flame: A small language model for spread-
sheet formulas, 2023.
[15] Abdullah Alka Kandilli. How is dirty data handled indata analytics? URL https://medium.com/@sweephy/
how-is-dirty-data-handled-in-data-analytics-1767fb998e37 .
[16] Patrick Lewis, Ethan Perez, Aleksandra Piktus, F abio Petroni, Vladimir Karpukhin,
Naman Goyal, Heinrich Küttler, Mike Lewis, W en-tau Yih, Tim Rocktäschel, Sebastian
Riedel, and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive NLP
tasks, 2021.
[17] Jia Li, Y ongmin Li, Ge Li, Zhi Jin, Yiyang Hao, and Xing Hu. Skcoder: A sketch-based
approach for automatic code generation, 2023.
[18] Jinyang Li, Binyuan Hui, Ge Qu, Jiaxi Y ang, Binhua Li, Bowen Li, Bailin W ang, Bowen
Qin, Rongyu Cao, Ruiying Geng, Nan Huo, Xuanhe Zhou, Chenhao Ma, Guoliang Li,
Kevin C. C. Chang, F ei Huang, Reynold Cheng, and Y ongbin Li. Can llm already serve
as a database interface? a big bench for large-scale database grounded text-to-sqls,
2023.
[19] Meta. Code-llama. URL https://github.com/facebookresearch/codellama .
[20] Panjiva. Panjiva supply chain intelligence: Data-driven insights for the global trade
community . URL https://panjiva.com/ .
[21] Pinecone. Pinecone. URL https://www.pinecone.io/ .
[22] Qdrant. Qdrant. URL https://qdrant.tech/ .
[23] David Ramel. T op 10 ai ’copilot’ tools for visual studio code. URL https://
visualstudiomagazine.com/articles/2023/06/30/vs-code-copilots.aspx .
[24] Qiaoyu T ang, Jiawei Chen, Bowen Y u, Y aojie Lu, Cheng F u, Haiyang Y u, Hongyu Lin,
F ei Huang, Ben He, Xianpei Han, Le Sun, and Y ongbin Li. Self-retrieval: Building an
information retrieval system with one large language model, 2024.[25] Kexun Zhang, Danqing W ang, Jingtao Xia, William Y ang W ang, and Lei Li. Algo:
Synthesizing algorithmic programs with llm-generated oracle verifiers, 2023.