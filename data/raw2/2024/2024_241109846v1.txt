Leveraging Propagated Infection to Crossfire
Mutants
Hang Du
University of California, Irvine
Irvine, CA, USA
hdu5@uci.eduVijay Krishna Palepu
Microsoft, Silicon Valley Campus
Mountain View, CA, USA
vijay.palepu@microsoft.comJames A. Jones
University of California, Irvine
Irvine, CA, USA
jajones@uci.edu
Abstract —Mutation testing was proposed to identify weak-
nesses in test suites by repeatedly generating artificially faulty
versions of the software ( i.e., mutants ) and determining if the test
suite is sufficient to detect them ( i.e., kill them). When the tests
are insufficient, each surviving mutant provides an opportunity to
improve the test suite. We conducted a study and found that many
such surviving mutants (up to 84% for the subjects of our study)
are detectable by simply augmenting existing tests with additional
assertions, or assertion amplification . Moreover, we find that many
of these mutants are detectable by multiple existing tests, giving
developers options for how to detect them. To help with these
challenges, we created a technique that performs memory-state
analysis to identify candidate assertions that developers can use
to detect the surviving mutants. Additionally, we build upon prior
research that identifies “crossfiring” opportunities — tests that
coincidentally kill multiple mutants. To this end, we developed
a theoretical model that describes the varying granularities that
crossfiring can occur in the existing test suite, which provide
opportunities and options for how to kill surviving mutants.
We operationalize this model to an accompanying technique
that optimizes the assertion amplification of the existing tests to
crossfire multiple mutants with fewer added assertions, optionally
concentrated within fewer tests. Our experiments show that we
can kill allsurviving mutants that are detectable with existing
test data with only 1.1% of the identified assertion candidates,
and increasing by a factor of 6x, on average, the number of killed
mutants from amplified tests, over tests that do not crossfire.
I. I NTRODUCTION
The ultimate goal of mutation testing is to allow software
developers to create stronger test suites. It does this by
injecting artificial faults ( i.e., mutations) into a program to
identify weaknesses in the test suite ( i.e.,mutations that are
not detected). A developer would then write tests to detect
(orkill) the undetected or surviving mutants. The intuition of
this approach is that by strengthening the test suite to detect
the mutations, the test suite will then be more likely to catch
future real faults before they can cause any adverse effects
upon users.
Multiple works on test generation and amplification ( e.g.,
[1], [2], [3], [4], [5], [6], [7]) focus on automating the improve-
ment of test suites. Some of these works address generating
specific parts of the test suite, such as test data and test oracles
(e.g., [5], [6], [8]), while others generate entire test suites
from scratch ( e.g., [2], [9]). Additionally, some techniques
amplify existing test suites by exploring new test data and
assertions ( e.g., [4], [7]). These approaches enhance test suitesand are evaluated based on their ability to produce higher
mutant-killing ratios. However, the usage scenarios of these
techniques often differ from typical mutation-testing practices
in either or both of the following two key ways: (1) Mutation
testing practitioners target individual surviving mutants and
incrementally improve existing test suites [10], [11], [12],
[13], and (2) they perform mutant-killing activities on pre-
constructed, human-written test suites that already contain test
data and oracles [10], [11], [12], [14].
From (1), we recognize the importance of analyzing spe-
cific surviving mutants to help practitioners target and kill
individual mutants. From (2), we acknowledge the presence
of multiple existing developer-written tests that may execute
the mutant, creating opportunities for test amplification—if a
slight improvement to an existing test can kill a surviving
mutant, there is no need to design a different test from scratch.
The classic fault-error propagation model, RIPR model [15],
[16], investigates such scenarios where a test case executes a
specific fault. The model includes four conditions: the fault
must be executed ( Reachability), infect the program states
(Infection), propagate the infection ( Propagation), and have
appropriate test oracles to reveal the fault ( Revealability).
The last condition of revealability relies on the test case
having appropriate and sufficient test oracles. If it does not,
adding additional assertions, or assertion amplification may
help reveal the fault. Following the RIPR model, Du et al.
[17] empirically investigates the end-to-end runtime effects
of mutation execution, which uncovers opportunities to kill
surviving mutants through such amplification.
Furthermore, while targeting a specific surviving mutant,
mutation-testing researchers discovered that a human-designed
test for one surviving mutant sometimes coincidentally kills
other surviving mutants—a phenomenon termed “crossfire”
[13], [18]. Understanding and leveraging these mechanisms
can strengthen each incremental test-augmentation (mutant-
killing) attempt.
In this work, we (1) offer a model to investigate the
causes and intricacies behind the crossfire phenomenon in
both the mutation-analysis and mutation-testing processes,
(2) empirically analyze existing test suites’ mutant-crossfire
capabilities at both the test and assertion granularities, (3)
systematically investigate assertion-amplification opportunities
for each surviving mutant, (4) develop techniques that recom-arXiv:2411.09846v1  [cs.SE]  14 Nov 2024mend mutant-crossfiring assertions with crossfiring goals at
varied granularities, (5) compare and evaluate our assertion-
amplification techniques of varied crossfiring strategies, and
(6) gain initial insights into mutant killing assertion candi-
dates’ characteristics.
Through our analysis, we found varied mutant-killing capa-
bilities of individual assertions and test cases across different
projects, unveiled how passing test runs exhibit propaga-
tion, and discovered overwhelming surviving mutant-killing
opportunities through assertion amplification. Our surviving
mutant-crossfiring techniques allow for a small selective set
of assertion-amplified, developer-written tests to crossfire a
substantially larger number of surviving mutants, by a factor
of6.1.
The main contributions of this paper include:
•An analysis technique that assesses the granular infected
memory locations that resulted from the propagation of
the infection caused by execution of the injected faults.
This technique offers recommendations for additional
assertions that can be used to kill surviving mutants.
•A theoretical model that dissects and analyzes propaga-
tion at fine-grained levels and illustrates the “crossfire”
effects in mutant kills and a technique that provides
optimizations for adding more effective assertions to tests.
•An empirical evaluation of our proposed techniques
showing the varied capabilities of tests and assertions for
detecting propagated state infection, assessing our ability
to recommend assertions to kill surviving mutants, and
assessing our optimizations to crossfire mutants.
•An implementation and dataset to allow for future re-
search and experimental reproducibility.
II. M OTIVATION AND CHALLENGES
Mutation testing involves both test automation and devel-
oper effort. A tool like PIT [19] or µJAVA [20] typically
handles the automated portion. These tools inject faults, rerun
the test suite for each mutant, calculate the mutation score,
and report unkilled mutants. The output of the tool provides
developers with two key benefits: (1) an assessment of the
test suite’s strength through the mutation score , and (2) a
prescription on improving the test suite via a list of surviving
mutants. With these results, developers must then strengthen
the test suite by targeting surviving mutants. The manual
process that a developer might take to kill surviving mutants
is depicted in the top row of Figure 1.
Once a mutation-testing tool is used to reveal surviving
mutants, a developer may wish to kill one surviving mutant by
first identifying a test that executes the mutation. The RIPR
model [15], [16] prescribes the execution (or Reachability ) as
the first condition to detect a fault. Next, a developer would
want to find a test that not only executes the mutation, but
also does so with test data that causes the mutation to affect,
orInfect , the state. Then, a developer would need to create a
test assertion to detect an infected state that Propagated back
to the test, in order to Reveal the mutation or fault and kill
the mutant.Each of these steps provides challenges for a developer. To
find such infections, a developer might probe the state (through
print statements or a debugger) to determine if any part of the
state that is accessible from the test case shows any infection,
as evidenced by differences with the original (unmutated)
program’s state from the same test. Even once such differences
in state are found, another challenge is determining if those
differences are caused by the infection from the mutation or
from nondeterministic values (such as a date or time-of-day
field, a hashcode, or a thread identifier). Even once a real
infection has been identified, there may be multiple ways to
access that infection, and the developer would need to make
a choice as to how to access it in order to detect it with
an assertion. Moreover, this entire process must be repeated
for each surviving mutant. Mutation-testing tools often report
hundreds or thousands of surviving mutants, even for mature
and well-tested software [10].
This last challenge of the magnitude of the problem could
be somewhat alleviated by the phenomenon of “crossfiring”
mutants [13], [18]—a test for one surviving mutant sometimes
coincidentally kills other surviving mutants. However, such a
manual, mutant-by-mutant approach would likely not exploit
the crossfire effects to their true potential.
Based on the potential opportunities and these observed
challenges for developers to perform the manual portion of
analyzing each surviving mutant, our motivation is to provide
conceptual models and analyses to help researchers understand
the various aspects of this task, as well as to create practical
techniques to help developers kill surviving mutants with
greater assistance.
III. A- MODEL : A B REAKDOWN OF MUTANT CROSSFIRE
EFFECTS
To understand the natural occurrence of the mutant “cross-
fire” phenomena, and to exploit its potential in killing surviv-
ing mutants, we developed a model in Figure 2 to illustrate
the observed mutant crossfire effects, as a result of intricate
fine-grained memory infection, in the mutation-analysis phase
and the mutation-testing phase.
The model’s structure takes the shape of the letter “A,”
which symbolizes the divergence of two mutation phases:
an existing test case is evaluated on killed/detected mutants
during the mutation-analysis phase on the left leg, and sub-
sequently, this test case may be augmented with assertions
in the mutation-testing phase to kill surviving mutants on the
right leg. We start by explaining the A-model’s left leg in the
mutation-analysis phase by breaking down an existing test,
using the example shown in the bottom row of Figure 1. The
example includes both the original test (Figure 1(c)) and the
augmented test (Figure 1(g)), and an anatomy of mutation
effects in the intermediate subfigures (Figure 1(d)-(f)).
In Block 1, an existing test case was executed on mutants
and demonstrates its capability to kill mutants. For instance,
Test 1 in Figure 1(c) killed two mutants ( m4andm5). When
a test kills mutants, it can fail by assertions (Block 2) or non-
assertion failures (such as crashes [21]) (Block 1.5). Test 2 ’s
2ManualDeveloperEﬀortCodeTestsMutation Testing  ToolSurvivingMutantsKill a Single MutantIdentify Test that Executes MutationIdentify Test that Causes State InfectionDiﬀerentiate Infection from NondeterminismWrite a New AssertionRepeat for Each Surviving Mutant without Regard for Crossﬁre OptimizationIdentify Test that Causes Detectable Infection PropagationAutomation-Assisted ApproachCodeTestsMutation Testing  ToolSurvivingMutantsInstrument Original and Mutated CodeExecute Instrumented Code to Record and AnalyzeMemory State in TestsConstruct Location-Mutant MatrixCrossﬁre-Optimized Assertion CandidatesInfection, Nondeterminism, and Access-Path Labels
@Testpublic void test1() {    var var1 = …     var var2 = …     assertEquals(3, var2.f5);}@Testpublic void test2() {    var var3 = …     assertEquals(1, var3.f7);    assertEquals(2, var3.f9);}Original Test CasesOriginal StateMutant 1 Statetest1OriginalProgramMutant 1executeN timesexecute
Observed state differences: Assessed as infection that propagated from the faultObserved state difference: Assessed as nondeterministic memory locationreturned N statesreturned statecomparecompare
Memory-State Analysisfor a Single Mutanttest1var1var2f4ﬂaky f6test2var3f1
m1f2
m1
m2f3
m3f7
m7f8
m3f9
m6f5
m4
m5Legend:
m1Infection caused by mutation m1. No assertion that checks at this location.
m1Infection caused by mutation m1. There is an assertion that checks  (i.e., test fails, detects m1) at this location.Labeled Memory Graphs to Identify all Detectable Infection Locations for all Mutantsm1m2m3f1f2f3f3f8var1var2var3test1
test2surviving mutantsmemorylocationvariabletestcase
Mutant-to-Location Infection Matrixfor Surviving Mutants@Testpublic void test1() {    var var1 = …     var var2 = …     assertEquals(3, var2.f5);    assertEquals(7, var1.f2);    assertEquals(8, var1.f3);}@Testpublic void test2() {    var var3 = …     assertEquals(1, var3.f7);    assertEquals(2, var3.f9);}Test Cases withAugmented Assertions(a)(b)(c)(d)(e)(f)(g)Fig. 1: Process to kill surviving mutants by manual developer effort (top) that does not optimize for crossfiring due to its
piecemeal approach, and an automation-assisted approach (bottom) that analyzes state infection and optimizes for crossfiring.
1. 1 test t was able to kill M  mutants2. N  assertions in t were able to kill M  mutants3. 1 assertion in t was able to kill M’ ⊂ M mutants4. 1 assertion was able to kill 1 mutant5. Detect Unrevealed Propagation & Perform Fine-grained State Analysis6. A newly added 1 assertion kills a surviving 1 mutant7. An added 1 assertion  may kill multiple surviving M’ ⊂ M  mutants8. Multiple added N  assertions in t kill surviving M  mutants 9. Produced new 1 test potentially crossﬁres M  mutantsMutation Analysis(Mutants were Killed)Mutation Testing(To Kill Surviving Mutants)
TestCaseAssertionsKill Pair
All assertionsSingle Assertion
1.5 Non-assertion test failures9.5 New test data, test codekkkkss
s
sks
Fig. 2: A-model: A Breakdown of Mutant Crossfire Effects
set of assertions cumulatively killed two mutants ( m6andm7).
As a test case may contain multiple assertions, each assertion
may be capable of killing multiple mutants (Block 3). Test 1 ’s
assertion of var2.f5 detects two mutations ( m4and m5).
As a result, an individual assertion contributes to a mutant
kill (Block 4), creating a set of assertion-mutant pairs ( e.g.,
var2.f5 -m4).
At the apex of the model, an analysis can reveal propagated
infection despite the fact that the tests pass (Block 5). As
described in Section II, performing such analysis can be
difficult and time consuming. However, an automated analysismay be able to identify infections that propagate from a
mutation back to a test, and as such suggest an assertion
candidate to kill it (Block 6). In Figure 1(e), for instance,
if we add an assertion in Test 1 that asserts the memory
location at node f3, then the surviving mutant m3will be killed.
Moreover, each added assertion may kill multiple surviving
mutants (Block 7). For instance, in Figure 1(e), an assertion
for memory location f2inTest 1 crossfires surviving mutants
m1andm2, because those two mutants trigger infection at the
same memory location. Furthermore, a test may be augmented
with multiple assertions (Block 8), and each of them may kill
some surviving mutants. As a result, the newly produced test
(e.g., Test1 in Figure 1(g)) with assertion amplification exhibits
its capability of killing multiple surviving mutants (Block 9).
Note that, such mutant kills may also be produced by crafting
a brand new test case with new test data and execution logic
(Block 9.5).
Throughout the A-model, we demonstrate the crossfire
effects of an existing test case from the individual test case
and individual assertion level, as a result of assessing granular
memory infections for multiple mutants. Also, we recognize
the potential to kill multiple surviving mutants at the test-
and assertion-level in the test-augmentation process. Moreover,
such assertion-amplification opportunities may occur across
multiple existing tests for each surviving mutant. In the next
section, we present our technique to strategically augment
an existing test suite with new assertions to kill surviving
mutants according to this crossfire-aware model, and thus offer
developer automated assistance to strengthen their test suite.
3IV. A PPROACH
In this section, we describe our technique to produce
assertion candidates for strategically killing surviving mutants.
Our technique can be decomposed into the following steps:
1) Run mutation-testing tool on program Pwith its test suite
Tto get the list of surviving mutants M.
2) Instrument Pto record all reachable memory states for
each test case in T. The output is the instrumented
program P′.
3) Execute the instrumented program P′Ntimes on its test
suite Tto produce Ncopies of memory states S.
4) Perform a comparison across all Ncopies of S. Those
memory locations that are consistent across all Ncopies
ofSare labeled as deterministic SP,d, and those locations
that show any differences are labeled as nondeterministic
SP,n.
5) For each surviving mutant MiinM, instrument mutant
Mito record all memory states for every test case. The
output is instrumented mutant program M′
i.
6) Execute instrumented mutant M′
iwith test suite Tto
record all state for all test cases SMi.
7) Filter the state of the mutant SMito remove all nonde-
terministic locations SMi,d.
8) Compare deterministic state of the original program SP,d
with the deterministic state of the mutant SMi,dto iden-
tify all locations SMi,ithat reveal infection.
9) Build a matrix Mfor all memory locations that reveal
any infections SMi,dacross all surviving mutants M.
10) Performing strategies on Mto optimize crossfiring to
produce a list of assertion candidates, and for each
assertion candidate the list of surviving mutants that it
would kill.
This process can be summarized into two primary stages:
(a) a fine-grained memory-state analysis on surviving mutants,
through which we analyze the location of state infection
in memory and the ways to access the pollution so as to
derive assertion candidates (Steps 2–8), and (b) assertion-
candidate selection, where we leverage analyzed granular
infection behaviors to design assertion-candidate selection
strategies that crossfire mutants (Steps 9–10). This process is
depicted in the second row of Figure 1 along with example
test cases, memory states, memory-location-to-mutant matrix,
and resulting augmented test cases.
A. Memory Analysis and Assertion Candidate Generation
A new assertion may be derived to check a specific, pre-
viously unchecked granular program-state behavior through
accessing variables that are directly exposed from the test-case
scope. As such, we analyzed each variable’s program states
for each passing test run (Figure 1(d)). Note, in this work,
we refer to “variables” as any local variables, fields of test-
class instances, method-return values, instantiated objects, or
static fields—that is, any memory location that is immediately
accessible from each test case’s scope. For each variable’s
program-state analysis, we performed Nrepetitive test runson the original program to identify nondeterministic memory
locations (such as node f6in Figure 1(e)), and compared
their states to their counterpart from a mutant, while ignoring
non-deterministic locations. We used breadth-first search in the
traversal of each pair of matched memory graphs to perform
such a comparison, starting from the root variable.
Each node in the object graph represents an instance of
an object (or primitive value at the leaves) and is given
a unique node ID. Each edge indicates a relation between
objects: an element from a size-changeable collection ( e.g.,
array, list) or a field of an object. During graph traversal, any
granular memory differences detected at a node is marked, and
the traversal along that path ceases any further comparisons
deeper along that path. Meanwhile, graph structural differences
may occur during comparison, which can result from null
fields and mismatches in collection sizes. If a collection-size
infection occurs, further state comparison under the collection
is stopped. Therefore, the algorithm yields a set of granular
state-difference details between two object graphs. Each of
them includes a unique node ID that specifies the location of
the infection relative to the object graph.
Such an analysis is performed on all surviving mutants’ cov-
ering test runs and individually on each variable. To mitigate
the redundant calculation and analysis of variables pointing to
the same object in memory, we apply a hashing algorithm on
variable-level object graphs. As a result, this approach yields
a set of granular state-difference details between two object
graphs. Each of them includes a unique node ID that specifies
the location of the infection retrievable from an object graph.
As a result, we produce comprehensive sets of program-
state infection locations for surviving mutants: each of them
includes the surviving mutant ID, test case ID, variable ID, and
node ID accessible via the variable pinpointing the exact pol-
lution (Figure 1(f)). This information forms a set of assertion
candidates with each specifying which surviving mutant can
be killed, which test case can be augmented, which variable
in the test case can access the pollution, which part of the
object graph should be asserted, and what are their expected
and polluted values.
B. Assertion Candidate Selection and Mutant Crossfire
To reduce the inevitable human-in-the-loop engineering
cost, only a few assertions are required to kill all of the
assertion-actionable mutants. For example, some assertions
may be written with shorter access paths than others. Specif-
ically, we measure the depth of the polluted node accessible
from the corresponding variable starting from depth of 1. In
Figure 1(e), node f3has a depth of 3 relative to var 2 and a
depth of 2 relative to var 1 . As such, an assertion-checking
memory location f3may be easier to access from variable
var 1 , thus potentially making the assertion more readable
with less comprehension and engineering cost. Moreover, we
recognize that practitioners often perform incremental aug-
mentation and validation to kill surviving mutants. Focusing
these efforts within a few locations in the test code can reduce
the engineering and validation burden, minimizing the need to
4Primary Mutant Crossﬁre Optimization Goalc1. Iteratively Search for the most capable assertionsc2. Iteratively Search for the most capable Variablesc3. Iteratively Search for the most capable Testsb. Select assertions via shortest access paths d1. For each selected variable, search for the most capable assertions d2. Within each selected test, search for the most capable assertionsa. Memory Analysis(produce assertion candidates)Strategy 1Strategy 2Strategy 3Fig. 3: Mutant Killing Strategies
switch contexts between different amplification locations in the
test code. As such, we can minimize the number of assertions
(e.g., checks for f2andf8), minimize the number of variable
checks ( e.g., checks for var 1 ) and minimize the number
of tests to perform such assertion amplification ( e.g., improve
Test 1 only rather than both tests) while achieving the same
mutation score.
As such, we presented three different mutant crossfiring
strategies shown in Figure 3. In Block b, an initial optimization
heuristic can be applied to select candidate assertions by
filtering out the assertion candidates via non-shortest access
paths for each surviving mutant. This filtering is based on the
heuristic that assertions checking attributes deeply accessible
from a variable are more likely to be brittle, which may
reduce the maintainability of the test suite. Then, we apply
greedy heuristics—iteratively search for the assertion/vari-
able/test that kills the largest number of surviving mutants (c1,
c2, and c3) among all candidate assertions until all assertion-
actionable surviving mutants are killed. Such greedy heuristics
prioritize assertions, variable checks, or tests that should be
augmented based on their ability to kill the largest number
of mutants, thereby limiting the human-engineering burden.
Similarly, greedy heuristics could be applied further on non-
finest searching scopes, i.e., variables and tests, where we
continue to iteratively search for the most capable assertions
related to a specific variable (d1) or test (d2), corresponding
to Strategy 2 and Strategy 3.
As a result, our technique produces assertion candidates
that not only kill but also crossfire surviving mutants, which
requires fewer and more concentrated updates in the existing
test suite to strengthen the test suite.
V. E VALUATION
In this section, we outline the experimental design to assess
the effectiveness of our approach for generating assertion
candidates that kill surviving mutants, while optimizing for
crossfiring assertions. We enumerate the research questions
that will guide our empirical investigations. To answer those
questions, we implemented our approach, performed exper-
imentation on 10 popular open-source Java programs, and
report our findings in Section VI.
A. Research Questions
These research questions are structured to assess both legs,
or facets, of the A-model: (1) how test assertions detect
infection ( i.e.,kill mutants); and (2) how assertions and tests
crossfire multiple mutants.RQ1 : For killed mutants, how do tests and assertions
contribute to the test suite’ fault-detection capabilities? Tests
often contain multiple assertions that together reveal infections
through test failures. However, the fault-detection capabilities
of individual assertions in a test may vary. For instance, a
single test with two assertions may kill three mutants, but with
only one assertion killing all three mutants. For a developer,
this may potentially make some assertions more useful than
others. In a different example, a single test contains three
assertions, each of which kills a unique mutant, and as such
each demonstrate their utility. We investigate this phenomenon
by measuring: (a) the count of mutants killed by each test, (b)
the count of mutants killed by each assertion, and (c) the count
of assertions in each test.
RQ2 : How many surviving mutants are detectable by existing
tests, and thus killable by our approach? Tests can only detect
infections that propagate to the test code’s execution scope.
Often, infections may propagate to a test’s execution scope,
but may go unrevealed because the test lacked an appropriate
assertion to detect the infection. If a developer can detect
faults by augmenting an existing test with an assertion, without
writing a whole new test case, it might save engineering cost to
create new test inputs and identify surviving mutants that are
guaranteed to not be equivalent mutants. We investigate this
phenomenon, and report the number of surviving mutants that
could be killed by augmenting existing tests with assertions.
RQ3 : How many tests offer the opportunity for assertion
amplification? We investigate the number of tests that do
not reveal propagated infections and offer the chance to add
newer assertions to kill surviving mutants. We suspect that
developers would ultimately want to (or perhaps even need to)
assess generated assertion candidates. And so, if we augment a
greater number of tests with assertions, then that may increase
the manual work for a developer. When manually assessing
the generated assertion candidates, the developer would need
to understand the logic for more test cases. In our experiments,
we report the number of tests wherein our memory-state
analysis was able to generate assertion candidates.
RQ4 : How effective are the crossfire strategies at reducing
the need for additional assertions to kill surviving mutants?
As explained from the right leg of the A-model, surviving
mutants could be killed and even crossfired. Our technique
searches for assertions, test variables, or test cases that guide
the killing of a maximal number of mutants to achieve the
best crossfire effects. Developers may need to manually assess
generated assertion candidates. In that event, it might be useful
if developers need to examine fewer test cases, test variables
and assertions while killing a maximal number of surviving
mutants. We evaluate the magnitude of such crossfire effects
and capabilities of three different mutant crossfire strategies.
We further compare them and discuss the trade-offs of the
three strategies.
5B. Experimental Setup
Mutation analysis with PIT. To conduct our empirical analysis
and technique evaluation, we employed PIT [19], a mutation-
testing tool for Java extensively utilized in research and
practice ( e.g., [22], [23]). We used the “DEFAULTS” group of
mutation operators provided by PIT, which contains a set of
mutation operators that has been widely adopted in practice.
Each mutation operator in this group ensures one-to-one map-
pings between the bytecode syntax and the resulting mutation,
and pre-filters for bytecode-equivalent mutants [24], thereby
mitigating equivalent and duplicate mutants.We modified PIT’s
source code to enable 10 non-mutation test runs, isolate mutant
execution in separate JVM instances, and collect final program
states for each test run. We also configured PIT to enable the
execution of all covering test runs for all mutants.
Memory Object-Graph Instrumentation. To collect final
program memory states, we instrumented test code (with
ASM [25]) to identify and record a list of test variables
and memory state accessible in the test’s execution scope.
We collect the memory object graphs for each local vari-
able, static field, and heap location that is accessible from
a test method. We use XStream [26] to record memory
object graphs, with configurations to exclude states related to
threading and logging utilities. We customized XStream to
support comparison of arrays and collections as part of our
fine-grained memory state analysis, detailed in Section IV-A.
We also employed static-field cleaners to mitigate state, or test
data pollution that might occur across two successive test runs.
These implementation details are captured in our artifact.1
Experimental Steps. We first ran our analysis to attribute test-
failure causes for failing test runs. Specifically, we count the
number of mutants that each test case and each assertion kills.
Next, we compare program states between mutated and
original runs on individual variables in the maintained variable
list on surviving mutants, which produces comprehensive
assertion candidates, as introduced in Section IV-A.
Finally, we apply the three mutant-crossfire strategies in-
troduced in Section IV-B. The greedy heuristics used in our
strategies may produce multiple equally optimal choices at a
given step, thus yielding close-to-optimal but nondeterministic
performance. As such, we ran each of our strategies 20 times
to evaluate their average performance.
Subject Programs. We ran our experiments on 10 open-source
Java projects, chosen for their use of the M AVEN build tool,
inclusion of developer-written JU NIT5 tests, no documentation
of flakiness of tests, minimal dependence on multi-threading
and external mocking libraries, and finally, compatibility with
JDK 8/11 and the XStream library. Each column of Table I
provides: (1) subject project; (2) lines of code; (3) number of
tests; (4) number of analyzed mutants; (5) number of analyzed
test runs; (6) average number of covering tests for each
analyzed mutant; (7) the mutation score for covered mutants;
and (8) time taken by our experiment for the subject. Across
the 10 subjects, we analyzed fine-grained memory data fromTABLE I: Experimental Subject Programs
avg.
Subject Project KLoC #T #Mut #Run #CT MS Time
commons-cli 6.2 381 723 36,084 49.9 92% 4h 56min
joda-money 9.2 1457 899 97,933 108.9 82% 12h 30min
cdk-data 10.6 4348 2,143 208,876 97.5 77% 49h 53min
jline-reader 13.3 180 2,920 130,700 44.8 66% 167h 36min
commons-validator 16.8 491 1,743 36,581 21.0 88% 8h 41min
commons-codec 24.1 1074 3,444 41,110 11.9 92% 14h 56min
spotify-web-api 24.4 287 1,916 20,817 10.9 85% 1h 44min
commons-text 26.6 1241 4,258 75,461 17.7 85% 24h 07min
dyn4j 66.1 2308 9,736 286,366 29.4 78% 28h 01min
jfreechart 138.4 2306 19,176 337,159 17.6 59% 155h 16min
over 1.2 million mutated test runs for 46,958 mutants. We
found no test flakiness in the subjects’ test suites.
Experiment Running Time. Our experiment ran on a 3.2Ghz
Apple M1 ARM processor with 16GB RAM, and required
18 days (approx.) to complete, cumulatively for all 10 subject
programs. We designed the experiment to answer the research
questions that we pose in this work. Answering such research
questions requires comprehensive runtime data, which we
collect through the experiment’s multiple stages: (a) running a
program’s 10 non-mutated runs; (b) running a program’s mu-
tated test run with instrumentation to collect runtime memory
graphs; and (c) analyzing memory graphs for both surviving
and killed mutants across all test runs.
Collectively, each such step incurs running-time costs. In
Table I we list these running times that vary with each program
— from as little as 1 hour 44 minutes (S POTIFY -WEB-API) to
as much as 167 hours 36 minutes (J LINE -READER ) — often
depending on the number of mutants and covering tests.
It is important to note that our experimental setup has sub-
optimal performance and is not designed for use in practice.
Our experiment was designed to test the scope and feasibility
of our approach, and includes time-consuming, redundant
steps to ensure the integrity of our experimental data. For
instance, we record memory data from test runs on both
killed and surviving mutants, which are voluminous and incur
substantial I/O and compression/decompression costs, to en-
able experimentation and exhaustive data analysis. In practice,
much of these expenses would not be needed. Moreover,
the analysis could be performed on individual or a smaller
subset of mutants. A tool developer could forego such costly
redundancies when implementing our approach as a developer
tool ( e.g., in an IDE or a CI/CD system), which assuredly
would offering significant speed-ups.
VI. RESULTS
RQ1: Test and Assertion Fault Detection for Killed Mutants
In Figure 4, we employ a small-multiples approach [27]
to illustrate varied capabilities of tests and assertions in
killing mutants across 6 representative subject projects. In each
project, we present two sub-figures at the same scale: the left
highlights the capability of test cases, while the right delves
into the granularity of individual assertions. Test cases are
visualized as regions outlined with gray borders. Assertions
are depicted as square pixels within the grey borders of their
enclosing test case. The size of a test’s enclosed area (with
the gray border) reflects the count of assertions within that
6(a) commons-cli
 (b) commons-codec
 (c) commons-validator
(d) jline-reader
 (e) joda-money
 (f) spotify-web-api
0 1 2 4-5 6-7 8-10 11-15 16-20 21-30 31-40 41-50 51-65 66-80 81-100 101-150 151-200 201-300 301-400 401-500 >500
(g) Legend: mutant killing/crossfiring capability
Fig. 4: Test and Assertion Crossfiring Capabilities
test case. The test cases and their assertions are sorted by
their names and locations.
In the right sub-figure for each project, each pixel’s color
indicates an assertion’s mutant-killing capability, with darker,
redder pixels denoting higher numbers of killed mutants ( i.e.,
greater crossfiring) by the corresponding assertion. This color
coding is also detailed in the legend in Figure 4g, where darker,
redder shades indicate that an assertion or test has killed more
mutants, while the lighter, more yellow shades suggesting
“fewer mutants killed,” and white would indicate no mutant
kills. For the left sub-figure, the entire region of a test within
a grey boundary is colored to reflect the overall strength of a
test case, i.e.,the number of mutants each test case fails on,
aggregated by test failures from all sources. In other words,
the color in these regions represents the crossfiring capability
as measured at the test level. When comparing mutant-killing
capabilities, several key takeaways stand out:
1) The patterns formed by striped, rectangular regions ob-
served in the left sub-figures for each subject reveal
that the majority of developer-written test cases contain
multiple assertions.
2) Darker shades of red dominate the left sub-figures across
all subjects. These left sub-figures show mutant kill
counts for individual test-cases. The dark red shades
suggest the aggregated effects of all of a test’s assertions’
capabilities combined with its non-assertion failures’ con-
tributions.
3) In contrast, we see lighter shades of yellow and orange
in the right sub-figures. For tests’ constituent assertions,
some carry significantly more weight than others within a
single test case or throughout the test suite. In fact, many
assertions detect zero faults (white) of the mutants used,
whereas others in the same test case detect many. Overall,
the sparse lighter-colored pixels at the assertion-level re-
veal a surprisingly smaller fraction and a more dispersedcapability of individual assertions in their contributions
to mutant kills.
Notably, testIPv6 in C OMMONS -VALIDATOR (marked with
green boundary in Figure 4c) stands out with 472 assertions
and spans more than 500 lines in test code.1This test case
demonstrates a strong mutant-killing capability as a whole
(deep red test region in the left subfigure), but only 28
assertions contribute to the mutation score. Interestingly, the
patterned bottom in Figure 4e for J ODA-MONEY shows the
use of unit tests that share similar groups of assertions, which
could be parameterized.
Answering RQ1: While a single test case may demonstrate
a strong mutant-killing capability, the constituent assertions
unevenly contribute to that capability, which reveals that
some assertions are more capable of detecting more faults.
RQ2: Killable Surviving Mutants
We present the results for both RQ2 andRQ3 in Table II,
where we present summary statistics for assertion candidates
for killing surviving mutants, as identified through our analy-
sis. We group the columns of Table II into three categories:
1) Magnitude of killable surviving mutants: a count of
surviving mutants (#Surviving) alongside those that are
killable (#Killable), as identified by our analysis.
2) Ways of killing each killable mutant: average number
of ways to kill each killable mutant through checks
by different (a) assertions ( #assert ), (b) test variables
(#var), and (c) test cases ( #test).
3) Assertion Candidate Characteristics: a count of number of
assertions to kill all killable, surviving mutants (#Assert),
along with the number of tests (#Test) and test variables
(#Var) used in devising the assertions.
1This may indicate a test smell instance because each individual assertion
can be isolated as one single unit test case and parameterized.
7TABLE II: Surviving Mutant-Killing Opportunities
Magnitude of Killable Ways of killing each Assertion Candidate
Surviving Mutants killable mutant (avg) Characteristics
Subject Project #Killable /#Surviving #assert #var#test #Assert #Var #Test
commons-cli 36/60 (60%) 16 15 13 312 298 205
commons-valida. 40/210 (19%) 224 89 37 5405 531 101
spotify-web-api 243/288 (84%) 99 25 10 17414 563 184
cdk-data 146/519 (28%) 196 119 20 27341 10126 1518
commons-text 298/644 (46%) 8 7 4 941 848 357
dyn4j 736/2188 (34%) 138 19 6 25997 3039 1000
commons-codec 65/284 (23%) 59 18 4 3082 659 175
joda-money 62/165 (38%) 6 6 6 138 138 137
jline-reader 414/1019 (41%) 509 337 76 33871 946 151
jfreechart 3464/8107 (43%) 78 20 9 33488 2263 844
The column labeled “Magnitude of Killable Surviving Mu-
tants” in Table II shows that our analysis is able to identify
19%–84% of surviving mutants as killable, depending on the
subject program. In other words, the existing tests already
provide the necessary test data to execute, infect, and cause in-
fection propagation back to at least one test; however, the tests
do not have sufficient assertions to reveal it. For instance, our
memory-state analysis shows that 60% of C OMMONS -CLI’s
60 surviving mutants are killable with assertion amplification
to existing tests. That number is as high as 84% in the case of
SPOTIFY -WEB-API, where 243 out of 288 surviving mutants
are killable.
Answering RQ2: For our subject programs, a fine-grained
memory-state analysis reveals that 19%–84% of surviving
mutants exhibit propagated infections not revealed by test
cases, which can be killed through assertion augmentation
of existing tests.
RQ3: Opportunities in Tests for Assertion Amplification
Given that many surviving mutants are indeed killable
through assertion augmentation of existing tests, we next
investigate the number of ways available (tests, test variables,
assertions) to kill such mutants.
Again, consider the data in Table II. For each mutant, our
technique is able to identify the total number of tests, test
variables, and specific candidate assertions that can be of use in
killing the surviving mutants. Consider, S POTIFY -WEB-API’s
243 killable surviving mutants. We find that, on average, to
kill a surviving S POTIFY -WEB-APImutant, our analysis can
identify 99 assertion candidates that can be written using 25
different test variables ( e.g., local variables, method return
values), which spread across 10 existing tests in S POTIFY -
WEB-API’s test suite. Further, for all 243 killable mutants,
our analysis detected 17,414 killing assertion candidates,
using 563 different test variables in 184 different existing test
methods in S POTIFY -WEB-API’s test suite.
Across all subject projects, we identify comprehensive can-
didate solutions (assertions, tests, variables) to kill surviving,
killable mutants through our analysis. The extensive mutant-
killing opportunities arise from the fact that a single surviving
mutant may produce a varied magnitude (avg. 118 locations)
of propagation on multiple test cases (avg. 14 tests), which can
be accessed through multiple variables (avg. 46 variables).Furthermore, we additionally assessed the average depth for
an assertion to access a specific field (whose value can be
asserted or evaluated) from a variable in the test code. We
found that the average depth ranges from 1.5to12.1across
all subject projects. For example, access paths that are four
nodes deep suggest accessing heap locations through complex
series of memory dereferences ( e.g.,var1.f1.f1.f3.f4 ).
Long access paths can make assertions hard to read and brittle,
leading to test anti-patterns. Therefore, it is important to filter
out assertion candidates with excessively long access paths.
Answering RQ3: Our memory-state analysis uncovers ex-
tensive assertion-amplification opportunities for each killable
surviving mutant: on average, each can be detected through
118 infected locations across 14 tests in test code, while only
one location in one test would be necessary to kill it. As such,
developers would have numerous options to kill each mutant.
RQ4: Optimizing Crossfire Strategies
In Table III, we present the performance of three mutant
killing-strategies introduced in Section IV. In the first three
columns, we present project’s name, the number of killable
surviving mutants (#Kill) as a baseline, and the average
depth (Dep) of an assertion’s access path, when accessing the
runtime state, from a variable in test code. In Figure 1 we show
examples of access paths; where for instance, var2.f4.f3
is an access path, usable in an assertion, and accessed from
the test code’s local variable var2 with a depth of 3.
For each strategy (assertion-, variable-, test-greedy), we
provide three crossfire performance metrics, including the
number of assertions (#Assert), variables (#Var), and tests
(#Test) to kill the baseline number of surviving mutants from
20 separate runs. Each strategy’s best performance metrics
are bolded with a crossfire factor included in the parentheses,
which is the average mutant-killing capability of the test-case
element.
Take S POTIFY -WEB-APIas an example: iteratively search-
ing for the most capable assertion (Strategy 1: Assertion-
Greedy) yields a solution with an average of 187 assertions
that can be devised using over 66 test variables across more
than 48 test cases. The 187 assertions would kill all 243
killable, surviving mutants where each assertion on average
kills 1.3(i.e.,the crossfire factor) surviving mutants.
Similarly, iteratively searching for the most capable test-
variable check (Strategy 2: Variable-Greedy) yields a solution
that, on average, yields the same number of assertions (187) as
in Strategy 1, but only requires a focus on 31 variables across
27 test cases. Further, devising assertions using any one of
those 31 test variables would, on average, kill 7.8surviving
mutants (crossfire factor).
Likewise, iteratively searching for the most capable test
case (Strategy 3: Test-Greedy) produces a solution that only
requires augmenting 23 test cases, where each assertion-
augmented test on average kills 10.6surviving mutants. More-
over, the average depth of assertions’ access paths is 2.5for
killable mutants as a result of filtering out all assertions with
8TABLE III: Surviving Mutant Killing Strategies
Subject Project #Kill Dep Strategy 1 (Assertion-Greedy) Strategy 2 (Variable-Greedy) Strategy 3 (Test-Greedy)
#Assert (factor) #Var #Test #Assert #Var (factor) #Test #Assert #Var #Test (factor)
commons-cli 36 1.4 27.0 (1.3) 26.9 23.1 27.0 26.0 (1.4) 23.0 27.0 26.9 14.0 (2.6)
commons-valid. 40 3.4 24.0 (1.7) 23.4 17.3 24.0 16.0 (2.5) 11.8 24.0 19.8 10.0 (4.0)
spotify-web-api 243 2.5 187.0 (1.3) 66.5 48.6 187.0 31.0 (7.8) 27.1 188.0 36.9 23.0 (10.6)
cdk-data 146 2.2 109.0 (1.3) 105.8 90.0 109.2 92.0 (1.6) 81.1 112.9 104.2 66.0 (2.2)
commons-text 298 1.2 235.0 (1.3) 234.9 171.4 235.3 233.0 (1.3) 170.4 239.4 238.8 161.0 (1.9)
dyn4j 736 1.7 348.0 (2.1) 341.0 248.8 354.2 309.0 (2.4) 236.1 357.3 337.9 219.0 (3.4)
commons-codec 65 1.4 41.0 (1.6) 40.4 39.4 41.0 40.0 (1.6) 39.4 42.0 41.4 38.0 (1.7)
joda-money 62 1.6 40.0 (1.6) 40.0 40.0 40.0 40.0 (1.6) 40.0 40.0 40.0 40.0 (1.6)
jline-reader 414 3.4 81.0 (5.1) 69.8 51.1 88.0 49.0 (8.4) 38.8 85.5 55.5 31.0 (13.4)
jfreechart 3464 2.7 555.9 (6.2) 473.2 332.2 559.2 384.6 (9.0) 309.1 567.4 446.4 298.0 (11.6)
non-shortest depths — much shorter than the access-path depth
of5.2, which is the average depth when not filtering out non-
shortest-depth access paths.
Across all subjects, we observe significant mutant-crossfire
effects enabled by our optimizing technique. Each asser-
tion can crossfire as many as 6.2surviving mutants in
JFREECHART with the assertion-greedy strategy, and each
assertion-augmented test can crossfire as many as 13.4surviv-
ing mutants in J LINE -READER with the test-greedy strategy.
Such leveraged crossfire effects can enlarge each incre-
mental assertion amplification effort and scope the overall
engineering and validation efforts within a few assertions,
variables, or tests. For example, our analysis found assertion
amplification opportunities within 4,572 tests (from RQ3)
across all subjects, while the test-greedy strategy can scope
the number down to 900tests (by aggregating the last column
in Table III). As a result, only 1,684out of 147,989assertion
candidates are selected.
Moreover, each candidate assertion to kill surviving mutants
has the shortest depth of access path to assert the state, which
on average ranges 1.2–3.4, as compared to 1.5–12.1for candi-
date solutions before any of our three different optimizations.
Such a reduction may mitigate rendering hard-to-read and
brittle assertions that check attributes deeply accessible from
a variable.
When comparing the three optimizing strategies, we find
that the test-greedy strategy (Strategy 3) is able to substantially
reduce the number of tests to be improved. Remarkably, for
SPOTIFY -WEB-API, the strategy almost halves the number
of target tests (from 48.6 to 23.0) and test variables (from
66.5 to 36.9) that may require consideration for assertion
amplification, with a negligible rise in the number of as-
sertions (from 187.0 to 188.0). Such significance reduction
in the target tests can also be observed in multiple projects,
such as C OMMONS -CLI, C OMMONS -VALIDATOR , SPOTIFY -
WEB-API, CDK-DATA, and J LINE -READER , while not signif-
icantly increasing the number of assertions or variables to be
checked for all subjects.
Answering RQ4: Our mutant-crossfire techniques with the
test-greedy strategy can scope the amplification efforts down
to1,684from 147,989assertion candidates, while retaining
the ability to kill all 5,504surviving, killable mutants across
all ten subject programs.VII. D ISCUSSION
Our experimental results from the previous section offer
several key insights into how we do test amplification, the
characteristics of killable surviving mutants and cross-firing
effects of tests, which we discuss next. We also offer a
qualitative exposition of assertion candidates that we generate,
using real examples from our experimental data.
Key Takeaways from Experimental Data. Our approach for
assertion amplification looks to kill surviving mutants. We
target surviving mutants because we find that many such
hard-to-kill mutants propagate program-state infections that go
undetected by existing tests. In our experimental subjects, we
note that 19%–84% of surviving mutants can be killed with
such additional mutant-killing assertions.
We also observe that multiple existing test cases can be
amplified with additional assertions to kill surviving mutants.
Interestingly, a single surviving mutant can exhibit multiple
state infections across various test cases: for each surviving
mutant, on average, our memory-state analysis can detect 118
different infection locations across 14 existing tests.
We speculate the reasons behind such “broad-spectrum”
killability of surviving mutants: software tests are intentional
in their design and typically do not cover every possible
fault in a program. They reflect a software tester’s intent and
priorities — e.g., the behaviors they are testing for, or the faults
they are guarding against. As such, the fact that multiple test
cases (on average) produce detectable infection from surviving
mutants may be an indication that these mutations are relevant
to the test data that were chosen by the software’s developers,
but the developers simply did not provide sufficient assertions
to detect some relevant fault-revealing attributes. We further
surmise that such surviving mutations likely mimic faults that
are meaningful to the program’s logic and semantics, as well
as its human-written test data and harnesses. Therefore, such
surviving mutants may warrant additional assertions, to guard
against faulty program behaviors.
Critically, we again observe this “broad-spectrum” killa-
bility when studying crossfiring properties of tests and their
constituent parts: in many cases, a single test assertion, test
variable, or test case can detect and kill multiple mutants.
We leverage this observation to scope the amplification
efforts within only a few test cases that we might augment,
which can bring down the number of assertion candidates from
9—————————————————from spotify-web-api————————————————assertEquals(1, audio.getSegments().length);   assertNotNull(audio.getSegments()[0]);                      (a)assertEquals(0.709, audio.getSegments()[0].getPitches()[0]) (b)———————————————————from commons-cli——————————————————Option o = OptionBuilder.hasArg(false).    withDescription("display the Groovy and JVM versions")      .withLongOpt("version").create('v');assertNotNull(o);                                           (c)assertEquals(-1, o.getArgs());                              (d)assertTrue(parsedReadableFileStream.getFD().valid())        (e)———————————————————from joda-money———————————————————           Throwable t = assertThrows(NullPointerException.class, () ->    {BigMoney.of((CurrencyUnit)null, BIGDEC_2_345);});assertEquals("Currency must not be null", t.getMessage();   (f)Fig. 5: Assertion Examples
147,989 to1,684 among 900 tests, while still being able to
kill all 5,504surviving, killable mutants across the ten subject
programs. This provides a crossfire factor of 6.1(i.e.,900tests
crossfiring 5,504surviving mutants).
Representative Examples of Assertion Candidates. In Fig-
ure 5, we enumerate six representative examples of asser-
tion candidates generated from our experiments on three
subject programs: S POTIFY -WEB-API, JODA-MONEY , and
COMMONS -CLI. The assertion candidates are highlighted in
bold and presented alongside snippets of the original test code.
Each assertion candidate is marked as (a) through (f).
These examples offer insights into the characteristics and
suitability of assertion candidates produced by our approach.
While not exhaustive, they serve as a qualitative exposition of
our technique and results. To capture a diversity of assertion
candidates, we include examples that (a) might be adopted
by developers or (b) might introduce test anti-patterns. We
also showcase how our approach safeguards against assertion
candidates that might introduce test anti-patterns.
Examples that might be adopted by developers. Consider the
Assertion Example (a) from S POTIFY -WEB-APIand (c) and
(d) from C OMMONS -CLI. We speculate that such assertions
are likely to be adopted by developers, due to their simplicity.
These assertions check a variable directly ( e.g., (c) per-
forms a null check on test variable o) or have shal-
low access paths to attributes, such as fields defined in
project production code ( e.g., o.getArgs() in (d) or
audio.getSegments()[0] in (a)).
For our experimental subjects, we find that on average,
42.1% of the assertion candidates (using the test-greedy strat-
egy) simply check a test variable directly, including checking
primitive values, string values, variable type, or their nul-
lability. We also observe that 41% of assertion candidates
access first-party attributes from production code, via fields
or accessor methods ( e.g.,getArgs() ).
Further, assertion examples in (a), (c) and (d) test variables
and states in first-party production code, without requiring
access to third-party or system code. On average, 85% of all
assertion candidates from our experiments access variables and
program states from first-party production code.Examples that might introduce test anti-patterns. Consider
the example assertions (e) in C OMMONS -CLI and (f) in
JODA-MONEY . These assertions might introduce test anti-
patterns for different reasons, and as such might not
be adopted by developers. Assertion Example (e) checks
an attribute from an external library via a deep ac-
cess path to examine the validity of a file descriptor
(.getFD().valid() ). Assertion Example (f) checks the
value of a method return ( t.getMessage() ) from a
system-level class ( Throwable ). We hypothesize that de-
velopers might avoid writing tests for external library code,
especially with deep and specific access paths, leading to a
higher likelihood of inducing test anti-patterns due to reliance
on external libraries and specificity.
Notably, as a side-effect our crossfiring-based filtering
strategy (see Section IV-B, and RQ4 experiment results in
Section VI) reduces the average depth of access-paths for
assertion candidates from 6.3 to 2.2. As such, when optimizing
for cross-firing effects, our approach mitigates the likelihood
of selecting assertion candidates with deep, complex access
paths that might lead to test anti-patterns.
For instance, between Assertion Examples (a) and (b) in
SPOTIFY -WEB-APIthat both detect the same mutant, when
optimizing for the number of tests ( i.e., test-greedy strategy
shown in results for RQ4), our approach would opt for
assertion candidate (a) — the simpler assertion candidate via
shorter access paths.
Future Work. We acknowledge that our insights into the
selected assertion candidate examples depend on the authors’
categorization methods and do not capture actual developer
sentiment or feedback. The actual suitability of each assertion
candidate is subject to the specific test case context, testing,
and project requirements. Moreover, we recognize that our
techniques provide multiple options for each surviving mutant,
while only selecting one based on our strategy. Offering
developers multiple choices allows them to select the most
suitable assertion candidates or make decisions to isolate a new
test, thus reducing the occurrence of anti-pattern assertions.
In the future, we will conduct user studies to evaluate the
suitability of these assertion candidates and design tools that
investigate human aspects in mutation-testing research.
VIII. T HREATS TO VALIDITY
The external threats stem from the generalizability of our
findings: our observations on test/assertion-level mutant de-
tection, characteristics of unrevealed propagation, and the
performance of mutant-crossfire strategies may not apply to
other mutation operators, subject projects, or programming
languages. However, we use a popular mutation-testing frame-
work, PIT [19], with all its default group of mutation opera-
tors, and select subject projects that vary in production-code
size, test-suite size, and complexity. Further, across all 10 of
our subjects, we saw consistent observations: every subject
has many surviving mutants that are killable through mere
assertion augmentation of existing tests, and that crossfiring
10was a source of substantial savings, in terms of the number of
assertions needed to kill all killable, surviving mutants.
Internal validity is challenged by the presence of duplicate
mutants, which PIT addresses through using a restrictive set of
mutation operators that prevent operator subsumption, ensur-
ing a one-to-one mapping between mutants and target syntax,
and filtering out byte-code identical equivalent mutants [24].
Execution nondeterminism is another concern. We mitigate it
by our use of 10 repetitive no-mutation test runs, followed by
analysis of test consistency through object-graph walks, and
isolation of each mutant’s runs in separate JVM instances with
static-field cleaners.
Finally, the construct validity threats come from the limita-
tions of our evaluation metrics and experimental setup. Con-
ducting human-involved studies or submitting pull requests to
project maintainers would gain further insights into the actual
suitability of our suggested assertion candidates. A challenge
to such a study of pull-request acceptance rates is motivating
the pull-request to project maintainers who have no experience
nor knowledge of mutation testing— i.e.,we must answer the
question, “why are we submitting a test that catches a bug that
does not exist?” for developers who may not be aware of such
approaches. In this study, we demonstrate the potential benefits
of using different crossfire strategies, present our assertion
candidates’ characteristics, and discuss assertion candidates’
suitability. In the future, we will conduct human studies to
further explore the suitability of our approach.
IX. R ELATED WORK
Mutant Crossfiring in Mutation Testing. Recent literature
distinguishes between mutation analysis and mutation testing
[28], [29]: mutation analysis assesses the strength of the
test suite, while mutation testing focuses on resolving each
surviving mutant to strengthen the test suite.
Smith and Williams [13], [18] documented mutant “cross-
fire,” where a new test targeting one surviving mutant coinci-
dentally kills others. Jia and Harman [30] noted that mutants
can be “collaterally” killed by tests aimed at different mutants.
This phenomenon, where a single test kills multiple mutants,
appears in mutation-testing research on mutation redundancies
(e.g., [21], [31], [32], [33], [34]), mutant-subsumption relation-
ships ( e.g., [30], [35], [36]), and mutant-ranking techniques
(e.g., [28], [37]).
We also observe and confirm the crossfire phenomenon
in mutation testing from our investigations. Unlike prior
studies that analyze mutants, we offer a model to examine
the causes of this phenomenon at fine-grained memory-state
levels. Furthermore, we use these insights to develop and
prioritize crossfiring assertion candidates to target crossfiring
mutants. Moreover, we recognize that practitioners often work
with individual surviving mutants and perform incremental
augmentation in empirical mutation-testing research, both in
academia ( e.g., [13], [14], [28], [38]) and in industry ( e.g.,
[10], [11], [12]). This focus informs our work, which differs
from other test-generation works in many aspects.Test Generation and Amplification. Past efforts in test genera-
tion include test-data generation ( e.g., [5], [8], [39]), assertion
generation in programs ( e.g., [40], [41], [41], [42]) or test code
(e.g., [43], [44], [45], [46]), and whole test-suite generation
(e.g., [2], [9], [47]). Some of these efforts are mutation-
based ( e.g., [1], [37], [45], [47], [48]). Additionally, Vera-P ´erez
proposed a mutation operator that suggests improvements into
test suites with infection and propagation analysis [48]
Previous work has also explored assertions’ roles in mutant
killing [21] and test suite effectiveness [49], brittle assertions
[50], and realistic tests [51], [52].
Many works exploit existing tests and are considered test
amplification [7]. Danglot et al. [7] conducted a snowballing
literature study and categorized test amplification into four
types: adding new tests, synthesizing tests for changes, mod-
ifying test execution, and modifying existing test code. Our
work falls into the last category but differs from prior works
in key ways.
Existing test generation and amplification work primarily
aim to achieve a broader goal of improving test suites, while
using coverage/mutation score as a proxy fitness-function. For
example, Fraser and Arcuri [2], [9] developed E VOSUITE to
generate an entire test suite from source code; Baudry et
al.[3], [4] created D SPOT to perform test amplification by
exploring more input space and assertions to enhance human-
written tests. In contrast, our approach targets individual
surviving mutants in mutation testing, and prescribes assertion
candidates to kill such unkilled mutants.
Further, to target individual surviving mutants, our approach
conducts a comprehensive memory-graph walk to identify
specific differences in memory state, yielding precise assertion
candidates ( e.g., asserting a specific element in an array
rather than the entire array). Prior test generation/amplification
techniques ( e.g., [2], [4], [53]) compare primitive or string-
type values, or entire objects to capture program states for
overall test-suite improvement. In contrast, we capture the
full object state, detecting infections in any enclosed fields,
arrays, or collections accessible from an object. Additionally,
we mitigate brittle assertions by selecting those that check
shallow attributes of a variable while rendering minimal and
precise assertion candidates.
Finally, previous techniques apply separate heuristics to
minimize tests, such as reducing the number of generated
assertions [9] and prioritizing the most effective tests [4]. Our
amplification technique targets individual mutant killing and
as such expects incremental code changes by practitioners.
Therefore, instead of selecting a single minimization approach,
we develop and compare different strategies that scope the
incremental amplification efforts to a few assertions, variables,
or test cases that ultimately achieve the same mutation score.
X. C ONCLUSION
In this work, we developed a technique to identify assertion-
amplification opportunities for surviving mutants via memory-
state analysis. We found that up to 84% of surviving mutants
can be killed by reusing existing tests and augmenting their
11assertions. Each surviving mutant offers multiple locations in
the test code for potential assertion amplification. Additionally,
we used the phenomenon of mutant crossfiring, as encountered
by practitioners, to construct a theoretical model and offer em-
pirical insights for crossfiring at various granularities. Building
on these insights, we devised a technique that optimizes the
amplification process, with which we find that we can kill
all such killable surviving mutants detected from our analysis
with fewer tests and fewer assertions, providing a crossfire
factor of 6.1x. In future work, we aim to conduct human-
centered studies and validate our approach with mutation-
testing practitioners.
ACKNOWLEDGMENTS
The second author’s opinions expressed in this publication
are solely his and do not purport to reflect the opinions or
views of his employer, Microsoft.
REFERENCES
[1] G. Fraser and A. Zeller, “Mutation-driven generation of unit tests
and oracles,” in Proceedings of the 19th international symposium on
Software testing and analysis , 2010, pp. 147–158.
[2] G. Fraser and A. Arcuri, “Whole test suite generation,” IEEE Transac-
tions on Software Engineering , vol. 39, no. 2, pp. 276–291, 2013.
[3] B. Baudry, S. Allier, M. Rodriguez-Cancio, and M. Monperrus, “Dspot:
Test amplification for automatic assessment of computational diversity,”
03 2015.
[4] B. Danglot, O. L. Vera-P ´erez, B. Baudry, and M. Monperrus, “Automatic
test improvement with dspot: a study with ten mature open-source
projects,” Empirical Software Engineering , vol. 24, pp. 2603–2635,
2019.
[5] F. C. Souza, M. Souza, M. Papadakis, V . Durelli, and M. Delamaro, “Test
data generation techniques for mutation testing: A systematic mapping,”
04 2014.
[6] E. T. Barr, M. Harman, P. McMinn, M. Shahbaz, and S. Yoo, “The
oracle problem in software testing: A survey,” IEEE Transactions on
Software Engineering , vol. 41, no. 5, pp. 507–525, 2015.
[7] B. Danglot, O. Vera-Perez, Z. Yu, A. Zaidman, M. Monperrus, and
B. Baudry, “A snowballing literature study on test amplification,”
Journal of Systems and Software , vol. 157, p. 110398, 2019.
[Online]. Available: https://www.sciencedirect.com/science/article/pii/
S0164121219301736
[8] S. Anand, E. K. Burke, T. Y . Chen, J. Clark, M. B. Cohen,
W. Grieskamp, M. Harman, M. J. Harrold, and P. Mcminn, “An
orchestrated survey of methodologies for automated software test case
generation,” J. Syst. Softw. , vol. 86, no. 8, pp. 1978–2001, aug 2013.
[Online]. Available: https://doi.org/10.1016/j.jss.2013.02.061
[9] G. Fraser and A. Arcuri, “Evosuite: automatic test suite generation for
object-oriented software,” in Proceedings of the 19th ACM SIGSOFT
Symposium and the 13th European Conference on Foundations of
Software Engineering , ser. ESEC/FSE ’11. New York, NY , USA:
Association for Computing Machinery, 2011, pp. 416–419. [Online].
Available: https://doi.org/10.1145/2025113.2025179
[10] G. Petrovi ´c and M. Ivankovi ´c, “State of mutation testing at google,” in
Proceedings of the 40th international conference on software engineer-
ing: Software engineering in practice , 2018, pp. 163–171.
[11] G. Petrovi ´c, M. Ivankovi ´c, G. Fraser, and R. Just, “Please fix this
mutant: How do developers resolve mutants surfaced during code
review?” in 2023 IEEE/ACM 45th International Conference on Software
Engineering: Software Engineering in Practice (ICSE-SEIP) , 2023, pp.
150–161.
[12] M. Beller, C.-P. Wong, J. Bader, A. Scott, M. Machalica, S. Chandra,
and E. Meijer, “What it would take to use mutation testing in industry—
a study at facebook,” in 2021 IEEE/ACM 43rd International Conference
on Software Engineering: Software Engineering in Practice (ICSE-
SEIP) . IEEE, 2021, pp. 268–277.
[13] B. H. Smith and L. Williams, “On guiding the augmentation of an auto-
mated test suite via mutation analysis,” Empirical software engineering ,
vol. 14, no. 3, pp. 341–369, 2009.[14] J. M. Rojas, T. D. White, B. S. Clegg, and G. Fraser, “Code defenders:
Crowdsourcing effective tests and subtle mutants with a mutation testing
game,” in 2017 IEEE/ACM 39th International Conference on Software
Engineering (ICSE) , 2017, pp. 677–688.
[15] N. Li and J. Offutt, “Test oracle strategies for model-based testing,”
IEEE Transactions on Software Engineering , vol. 43, no. 4, pp. 372–
395, 2017.
[16] ——, “An empirical analysis of test oracle strategies for model-based
testing,” in 2014 IEEE Seventh International Conference on Software
Testing, Verification and Validation , 2014, pp. 363–372.
[17] H. Du, V . K. Palepu, and J. A. Jones, “Ripples of a mutation
— an empirical study of propagation effects in mutation testing,”
inProceedings of the IEEE/ACM 46th International Conference
on Software Engineering , ser. ICSE ’24. New York, NY , USA:
Association for Computing Machinery, 2024. [Online]. Available:
https://doi.org/10.1145/3597503.3639179
[18] B. H. Smith and L. Williams, “An empirical evaluation of the mu-
java mutation operators,” in Testing: Academic and Industrial Con-
ference Practice and Research Techniques - MUTATION (TAICPART-
MUTATION 2007) , 2007, pp. 193–202.
[19] H. Coles, T. Laurent, C. Henard, M. Papadakis, and A. Ventresque,
“Pit: a practical mutation testing tool for java,” in Proceedings of the
25th international symposium on software testing and analysis , 2016,
pp. 449–452.
[20] Y .-S. Ma, J. Offutt, and Y . R. Kwon, “Mujava: an automated class
mutation system,” Software Testing, Verification and Reliability , vol. 15,
no. 2, pp. 97–133, 2005.
[21] H. Du, V . K. Palepu, and J. A. Jones, “To kill a mutant: An
empirical study of mutation testing kills,” in Proceedings of the
32nd ACM SIGSOFT International Symposium on Software Testing
and Analysis , ser. ISSTA 2023. New York, NY , USA: Association
for Computing Machinery, 2023, pp. 715–726. [Online]. Available:
https://doi.org/10.1145/3597926.3598090
[22] M. Kintis, M. Papadakis, A. Papadopoulos, E. Valvis, N. Malevris, and
Y . Le Traon, “How effective are mutation testing tools? an empirical
analysis of java mutation testing tools with manual analysis and real
faults,” Empirical Software Engineering , vol. 23, no. 4, pp. 2426–2463,
2018.
[23] A. Shi, J. Bell, and D. Marinov, “Mitigating the effects of flaky
tests on mutation testing,” in Proceedings of the 28th ACM SIGSOFT
International Symposium on Software Testing and Analysis , 2019, pp.
112–122.
[24] H. Coles, “Mutation operators,” 2023, mutation operators in PIT.
[Online]. Available: https://pitest.org/quickstart/mutators/
[25] E. Bruneton, R. Lenglet, and T. Coupaye, “Asm: a code manipulation
tool to implement adaptable systems,” Adaptable and extensible
component systems , vol. 30, no. 19, 2002. [Online]. Available:
http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.117.5769
[26] (2022, December) Xstream - about xstream. [Accessed 01-08-2023].
[Online]. Available: https://x-stream.github.io/
[27] E. R. Tufte, The visual display of quantitative information . Graphics
press Cheshire, CT, 2001, vol. 2.
[28] S. J. Kaufman, R. Featherman, J. Alvin, B. Kurtz, P. Ammann, and
R. Just, “Prioritizing mutants to guide mutation testing,” in 2022
IEEE/ACM 44th International Conference on Software Engineering
(ICSE) , 2022, pp. 1743–1754.
[29] M. Papadakis, M. Kintis, J. Zhang, Y . Jia, Y . L. Traon, and
M. Harman, “Chapter six - mutation testing advances: An analysis
and survey,” ser. Advances in Computers, A. M. Memon, Ed.
Elsevier, 2019, vol. 112, pp. 275–378. [Online]. Available: https:
//www.sciencedirect.com/science/article/pii/S0065245818300305
[30] Y . Jia and M. Harman, “Higher order mutation testing,” Information
and Software Technology , vol. 51, no. 10, pp. 1379–1393, 2009, source
Code Analysis and Manipulation, SCAM 2008. [Online]. Available:
https://www.sciencedirect.com/science/article/pii/S0950584909000688
[31] R. Just, G. M. Kapfhammer, and F. Schweiggert, “Do redundant mutants
affect the effectiveness and efficiency of mutation analysis?” in 2012
IEEE Fifth International Conference on Software Testing, Verification
and Validation , 2012, pp. 720–725.
[32] G. Kaminski, P. Ammann, and J. Offutt, “Improving logic-based
testing,” Journal of Systems and Software , vol. 86, no. 8, pp.
2002–2012, 2013. [Online]. Available: https://www.sciencedirect.com/
science/article/pii/S0164121212002403
12[33] R. Just, G. M. Kapfhammer, and F. Schweiggert, “Using non-redundant
mutation operators and test suite prioritization to achieve efficient and
scalable mutation analysis,” in 2012 IEEE 23rd International Symposium
on Software Reliability Engineering , 2012, pp. 11–20.
[34] M. Papadakis, Y . Jia, M. Harman, and Y . Le Traon, “Trivial compiler
equivalence: A large scale empirical study of a simple, fast and effective
equivalent mutant detection technique,” in 2015 IEEE/ACM 37th IEEE
International Conference on Software Engineering , vol. 1, 2015, pp.
936–946.
[35] M. Kintis, M. Papadakis, and N. Malevris, “Evaluating mutation testing
alternatives: A collateral experiment,” in 2010 Asia Pacific Software
Engineering Conference . IEEE, 2010, pp. 300–309.
[36] M. Papadakis, C. Henard, M. Harman, Y . Jia, and Y . Le Traon,
“Threats to the validity of mutation-based test assessment,” in
Proceedings of the 25th International Symposium on Software Testing
and Analysis , ser. ISSTA 2016. New York, NY , USA: Association
for Computing Machinery, 2016, pp. 354–365. [Online]. Available:
https://doi.org/10.1145/2931037.2931040
[37] T. Titcheu Chekam, M. Papadakis, T. F. Bissyand ´e, Y . Le Traon,
and K. Sen, “Selecting fault revealing mutants,” Empirical Software
Engineering , vol. 25, no. 1, pp. 434–487, 2020.
[38] J. M. Rojas and G. Fraser, “Code defenders: a mutation testing game,”
in2016 IEEE Ninth International Conference on Software Testing,
Verification and Validation Workshops (ICSTW) . IEEE, 2016, pp. 162–
167.
[39] T. T. Chekam, M. Papadakis, M. Cordy, and Y . L. Traon, “Killing
stubborn mutants with symbolic execution,” ACM Transactions on
Software Engineering and Methodology (TOSEM) , vol. 30, no. 2, pp.
1–23, 2021. [Online]. Available: https://www.sciencedirect.com/science/
article/pii/S0164121219301554
[40] M. Ernst, J. Cockrell, W. Griswold, and D. Notkin, “Dynamically
discovering likely program invariants to support program evolution,”
IEEE Transactions on Software Engineering , vol. 27, no. 2, pp. 99–123,
2001.
[41] M. Boshernitsan, R. Doong, and A. Savoia, “From daikon to agitator:
Lessons and challenges in building a commercial tool for developer
testing,” in Proceedings of the 2006 International Symposium on
Software Testing and Analysis , ser. ISSTA ’06. New York, NY , USA:
Association for Computing Machinery, 2006, pp. 169–180. [Online].
Available: https://doi.org/10.1145/1146238.1146258
[42] V . Terragni, G. Jahangirova, P. Tonella, and M. Pezz `e, “Gassert: A
fully automated tool to improve assertion oracles,” in 2021 IEEE/ACM
43rd International Conference on Software Engineering: Companion
Proceedings (ICSE-Companion) , 2021, pp. 85–88.[43] T. Xie, “Augmenting automatically generated unit-test suites with
regression oracle checking,” in Proceedings of the 20th European
Conference on Object-Oriented Programming , ser. ECOOP’06. Berlin,
Heidelberg: Springer-Verlag, 2006, pp. 380–403. [Online]. Available:
https://doi.org/10.1007/11785477 23
[44] L. Zamprogno, B. Hall, R. Holmes, and J. M. Atlee, “Dynamic human-
in-the-loop assertion generation,” IEEE Transactions on Software Engi-
neering , vol. 49, no. 4, pp. 2337–2351, 2023.
[45] M. Staats, G. Gay, and M. P. Heimdahl, “Automated oracle creation
support, or: How i learned to stop worrying about fault propagation
and love mutation testing,” in 2012 34th International Conference on
Software Engineering (ICSE) . IEEE, 2012, pp. 870–880.
[46] D. Tiwari, M. Monperrus, and B. Baudry, “Mimicking production behav-
ior with generated mocks,” IEEE Transactions on Software Engineering ,
pp. 1–26, 2024.
[47] G. Fraser and A. Zeller, “Mutation-driven generation of unit tests and
oracles,” IEEE Transactions on Software Engineering , vol. 38, no. 2,
pp. 278–292, 2012.
[48] O. L. Vera-P ´erez, B. Danglot, M. Monperrus, and B. Baudry,
“Suggestions on test suite improvements with automatic infection and
propagation analysis,” 2019. [Online]. Available: https://arxiv.org/abs/
1909.04770
[49] Y . Zhang and A. Mesbah, “Assertions are strongly correlated with test
suite effectiveness,” in Proceedings of the 2015 10th Joint Meeting on
Foundations of Software Engineering , 2015, pp. 214–224.
[50] C. Huo and J. Clause, “Improving oracle quality by detecting brittle
assertions and unused inputs in tests,” in Proceedings of the 22nd
ACM SIGSOFT International Symposium on Foundations of Software
Engineering , ser. FSE 2014. New York, NY , USA: Association
for Computing Machinery, 2014, pp. 621–631. [Online]. Available:
https://doi.org/10.1145/2635868.2635917
[51] M. Bozkurt and M. Harman, “Automatically generating realistic test
input from web services,” in Proceedings of 2011 IEEE 6th International
Symposium on Service Oriented System (SOSE) , 2011, pp. 13–24.
[52] G. Fraser and A. Zeller, “Exploiting common object usage in test case
generation,” in 2011 Fourth IEEE International Conference on Software
Testing, Verification and Validation , 2011, pp. 80–89.
[53] V . Terragni, G. Jahangirova, P. Tonella, and M. Pezz `e, “Evolutionary
improvement of assertion oracles,” in Proceedings of the 28th ACM Joint
Meeting on European Software Engineering Conference and Symposium
on the Foundations of Software Engineering , 2020, pp. 1178–1189.
13