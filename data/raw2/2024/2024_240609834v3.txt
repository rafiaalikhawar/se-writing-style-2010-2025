LLMs Meet Library Evolution:
Evaluating Deprecated API Usage in LLM-based
Code Completion
Chong Wang†, Kaifeng Huang‡∗, Jian Zhang†, Yebo Feng†, Lyuye Zhang†, Yang Liu†, and Xin Peng§
†School of Computer Science and Engineering, Nanyang Technological University, Singapore
{chong.wang, jian_zhang, yebo.feng}@ntu.edu.sg, zh0004ye@e.ntu.edu.sg, yangliu@ntu.edu.sg
‡School of Computer Science and Technology, Tongji University, China
kaifengh@tongji.edu.cn
§School of Computer Science and Shanghai Key Laboratory of Data Science, Fudan University, China
pengxin@fudan.edu.cn
Abstract —Large language models (LLMs), pre-trained or
fine-tuned on large code corpora, have shown effectiveness
in generating code completions. However, in LLM-based code
completion, LLMs may struggle to use correct and up-to-date
Application Programming Interfaces (APIs) due to the rapid
and continuous evolution of libraries. While existing studies have
highlighted issues with predicting incorrect APIs, the specific
problem of deprecated API usage in LLM-based code completion
has not been thoroughly investigated.
To address this gap, we conducted the first evaluation study on
deprecated API usage in LLM-based code completion. This study
involved seven advanced LLMs, 145 API mappings from eight
popular Python libraries, and 28,125 completion prompts. The
study results reveal the status quo (i.e.,API usage plausibility and
deprecated usage rate) of deprecated API and replacing API usage
in LLM-based code completion from the perspectives of model ,
prompt , and library , and indicate the root causes behind. Based
on these findings, we propose two lightweight fixing approaches,
REPLACE API and INSERT PROMPT , which can serve as baseline
approaches for future research on mitigating deprecated API usage
in LLM-based completion. Additionally, we provide implications
for future research on integrating library evolution with LLM-
driven software development.
I. I NTRODUCTION
Large language models (LLMs) [ 1], [2], [3], [4], [5],
[6] have significantly advanced various aspects of software
engineering [ 7], including code completion [ 8], [9], [10],
code understanding [ 11], [12], defect detection [ 13], [14], and
program repair [ 15], [16], [17]. These models, pre-trained or
fine-tuned with extensive knowledge of code on large corpora,
are effective for tailoring to different downstream tasks. In the
realm of code completion, the state-of-the-art has evolved from
statistics-based methods [ 18], [19] to LLM-based techniques
[20], [21], [22]. Code completion is a sophisticated task that
suggests variables, functions, classes, methods, and even entire
code blocks, which depends on developers’ practical needs.
Motivation. To accelerate development, developers heavily
rely on third-party libraries, interacting with them through
∗Kaifeng Huang is the corresponding authorApplication Programming Interfaces (APIs). However, this
reliance presents a challenge for code completion tools. Third-
party libraries constantly evolve to undergo refactorings [ 23],
fix bugs [ 24], apply security patches [ 25], or introduce new
features. This rapid evolution leads to frequent API changes,
with older APIs being deprecated and replaced by newer ones.
Deprecated APIs are discouraged to use because of their in-
compatiblity with newer features or data, which will eventually
disappear in future library updates [ 26]. Taking PyTorch [ 27], a
popular deep learning library for instance, the API torch.gels()
was deprecated in version 1.2 (August 2019) in favor of
torch.lstsq() . Then, torch.lstsq() was deprecated in version 1.9
(June 2021) in favor of torch.linalg.lstsq() . Consequently, newly
developed code should avoid using the deprecated torch.gels()
andtorch.lstsq() . Therefore, it’s crucial for code completion
tools to suggest correct and up-to-dated APIs to developers.
Literature. However, to the best of our knowledge, the
capabilities of LLM-based code completion regarding API
deprecation is understudied [ 28], [29]. Although there emerges
a substantial number of evaluation on code completion, a body
of research focused on assessing the overall accuracy across
various benchmarks [ 11], [30], [31], [32], [33]. Interestingly,
Ding et al. [34] identified undefined names and unused variables
as the most common syntactic errors produced by LLMs
in Python code completions. Izadi et al. [35] found that
incorrect function name predictions were prevalent, accounting
for 23% of all token-level errors. Furthermore, Recent studies
[36], [37] highlighted the issue of hallucinations in LLM-
generated code. Their findings indicate the prevalence and
potential risks of using unexpected APIs. Nevertheless, while
researchers have noted the prevalence of incorrect function
name predictions, they have not investigated this issue in depth.
Library APIs, which constitute an important part in predicting
external function names, are worth attached importance to.
Study. To address this gap, we conducted a study to examine
the deprecated API usage in LLM-based code completion. The
study aims to answer the primary research question:arXiv:2406.09834v3  [cs.SE]  13 Feb 2025What are the status quo (i.e., API Usage Plausibility (AUP)
and Deprecated Usage Rate (DUR)) and potential root
causes of deprecated and replacing API usage in LLM-
based code completion?
This question is explored through three detailed aspects:
Model Perspective (RQ1) investigates the status quo and
potential causes based on the performance of various LLMs;
Prompt Perspective (RQ2) examines the impact of different
prompts on the status quo and potential causes;
Library Perspective (RQ3) analyzes the status quo and
potential causes across different libraries.
To address these research questions, we conducted a series
of experiments involving various libraries and LLMs. We
collected 145 API mappings between deprecated APIs and
their replacements from eight popular Python libraries. Based
on these mappings, we retrieved 9,022 outdated functions and
19,103 up-to-dated functions using the deprecated APIs and
replacing APIs, respectively. Then, we identify the locating
lines of the deprecated or replacing APIs, mask them and
subsequent code, and use the remaining parts as the code
completion prompts. The original deprecated or replacing
API is referred as the reference API. These prompts were
then inputted into seven advanced code LLMs, including
CodeLlama [ 5] and GPT-3.5, to generate completions and
analyze the predicted API usages. If the predicted API usage
corresponds to either the deprecated or replacement version of
the reference API, it is annotated as plausible ; otherwise, it is
annotated as Others .
The study results reveal the following findings: Finding in
RQ1: All evaluated LLMs encounter challenges in predicting
plausible API usages and face issues with deprecated API
usages, due to the presence of deprecated API usages during
model training and the absence of API deprecation knowledge
during model inference. Finding in RQ2: For the two
categories of prompts derived from outdated and up-to-dated
functions, the LLMs’ performance in predicting plausible and
deprecated API usages differs significantly, influenced by the
distinct code context characteristics of these prompts. Finding
to RQ3: Across the eight libraries, the LLMs exhibit significant
differences in their use of deprecated APIs, influenced by the
characteristics of API deprecations during library evolution.
Lightweight Mitigation. Based on the study results and
findings, we explored the feasibility of using two lightweight
fixing approaches to mitigate deprecated API usage in LLM-
based code completion. Given a completion containing a dep-
recated API usage, the first approach, named REPLACE API,
directly replaces the deprecated API usage with the replacement
and regenerates the remaining parts ( e.g., argument list)
during the decoding process. The second approach, named
INSERT PROMPT , inserts an additional replacing prompt after
the original prompt to guide the LLMs to use the replacing
API and then regenerate the completions. We then evaluate
the effectiveness of the proposed approaches in terms of fixing
deprecated API usages and the accuracy in predicting line-level
completions ( RQ4 ). The evaluation results demonstrate thatREPLACE API effectively addresses deprecated API usages for
all evaluated open-source LLMs, achieving fix rates exceeding
85% with acceptable accuracy measured by Edit Similarity
and Exact Match compared to ground-truth completions.
While INSERT PROMPT does not currently achieve sufficient
effectiveness and accuracy in fixing completions containing
deprecated API usage, it shows potential for future exploration.
Contribution. This paper makes the following contributions:
•We conducted the first study that reveals the status quo
and causes of deprecated API usages in LLM-based code
completion from prespectives of model, prompt, and library.
•We proposed two lightweight approaches, named RE-
PLACE API andINSERT PROMPT , to serve as baselines for
mitigating deprecated API usage in LLM-based completion.
•We provide implications for future research on the synergy
of library evolution and LLM-driven software development.
II. R ELATED WORK
A. Library Evolution
Library evolution involves refactorings [ 23], bug fixes [ 24],
and new feature introductions. Typically, refactorings can
deprecate old APIs and introduce new replacements. Several
studies have examined the reasons that developers deprecate
APIs and how the clients react to such deprecations [ 38],
[39], [40], [41]. The reasons include improving readability,
reducing redundancy, avoiding bad code practices and fixing
functional bugs. Deprecated APIs can affect hundreds of clients
[42], particularly when clients struggle to keep pace with
rapidly evolving software [ 43], [44]. McDonnell et al. [ 45]
found that only 22% of outdated API usages are eventually
upgraded to use replacing APIs. Similarly, Hora et al. [ 46]
found that client developers consumes considerate time to
discover and apply replacing APIs, with the majority of systems
not reacting at all. When clients do not upgrade their APIs,
they silently accumulate technical debt in the form of future
API changes when they finally upgrade [ 47]. To locate the
replacing API, existing works leverage change rules written by
developers [ 48], developer recordings [ 49], similarity matching
[50], mining API usage in libraries [ 51], and in client projects
[52]. Henkel and Diwan [ 49] developed an IDE plugin that
allows library developers to record API refactoring actions
and client developers to replay them. Godfrey and Zou [ 53]
proposed a semi-automated origin analysis using similarities in
name, declaration, complexity metrics, and call dependencies.
Wu et al. [ 54] introduced a hybrid approach combining call
dependency and text similarity analysis to identify API change
rules. Recently, [ 55] proposed RepFinder to find replacing APIs
for deprecated APIs in library updates from multiple sources.
In this work, we aim to comprehend the statuses and causes
of deprecated API usage in LLM-based code completion and
provide implications for mitigating the deprecated API usages.
B. LLM-based Code Completion
Code completion is an important functionality in modern
IDEs and editors. Historically, researchers have explored
statistical models [ 18], [19]. With the advent in natural languageStudy Setup(Sec. 3)Study Results (Sec. 4)
APIMapping CollectionCode Completion Prompt Construction
RQ1: LLM PerspectiveRQ2: Prompt PerspectiveLLM-based Code Completion
Code CompletionsLibrary Docs
RQ3: Library PerspectiveStatus QuooAPI Usage PlausibilityoDeprecated Usage Rate Mitigation Approaches (Sec. 5)Approach 1: ReplaceAPIApproach 2: InsertPrompt
Code Repos
LLMs
Completion Result AnnotationRQ4: EffectivenessoFixed RateoEdit SimilarityoExact MatchRoot CausesoRoot Cause Analysis 
Fig. 1: Overview of Our Study
processing, researchers have embraced deep learning for code
completion [ 8], [9] because they are similar in token-based
prediction. To explore the capability of code completion
tools driven by LLMs [ 20], [21], [22], numerous evaluations
of LLMs have been proposed. Ciniselli et al. [ 32], [33]
conducted a large-scale study exploring the accuracies of
state-of-the-art Transformer-based models in supporting code
completion at various granularity levels, from single tokens to
entire code segments. Zeng et al. [ 11] found that pre-trained
models significantly outperform non-pre-trained state-of-the-art
techniques in program understanding tasks. They also reveal
that no single pre-trained model dominates across all tasks.
Xu et al. [ 30] evaluated the performance of LLMs on the
HumanEval dataset. Ding et al. [ 34] identified undefined names
and unused variables as the most common errors produced
by language models in Python code completions. Izadi et al.
[35] evaluated the LLMs using real auto-completion usage data
across 12 languages. They found that incorrect function name
predictions, were prevalent, accounting for 23% of all token-
level errors. Besides, Liu et al. [ 31] proposed EvaluPlus, which
benchmarks the functional correctness of LLM-synthesized
code using test cases. In addition to accuracy concerns, LLM-
based approaches face issues such as security vulnerabilities
and hallucinations. Sallou et al. [ 56] explored threats posed
by LLMs, including unpredictability in model evolution, data
leakage, and reproducibility. Liu et al. [ 36] categorized the
hallucinations brought by LLM-generated code.
The findings on incorrect function predictions partially moti-
vate our study. However, we focus on the severity of predicting
deprecated API usages in LLM-based code completion.
III. S TUDY SETUP
We chose Python, a popular programming language which
ranks first among the most popular ones based in the recent
year [ 57]. We targeted eight popular Python libraries. Five
of these libraries were used in a previous study on Python
API deprecation [ 25], including Numpy, Pandas, scikit-learn,
SciPy, and seaborn. Additionally, we added three popular deep
learning libraries, i.e.,TensorFlow, PyTorch, and Transformers.
The setup of our study is presented in Figure 1. It includes four
steps. The API Mapping Collection gathers mappings betweenTABLE I: Statistics of our Collected API Mappings
Library Version Range # Mappings# Functions
Outdated Up-to-dated
Numpy 1.16.0-1.26.4 3 567 2,988
Pandas 0.24.0-2.2.2 10 69 69
scikit-learn 0.21.3-1.5.0 18 985 1,197
SciPy 1.2.1-1.13.0 4 245 1,458
seaborn 0.9.1-0.13.2 3 904 1,329
TensorFlow 1.13.1-2.16.1 57 1,491 4,830
PyTorch 1.0.1-2.3.0 21 4,726 6,406
Transformers 1.0.0-4.40.2 29 100 63
Total – 145 9,022 19,103
deprecated APIs and their replacements from various libraries.
The Completion Prompt Construction step involves creating
completion prompts by identifying instances of deprecated
and replacing API usage in open-source Python repositories.
The LLM-Based Code Completion step uses various LLMs
to generate code completions for these prompts. Finally, the
Completion Result Annotation step automatically annotates the
generated completions and calculates relevant metrics.
A. API Mapping Collection
We identified API mappings ( i.e.,deprecated APIs and the
mapping replacements) from the documentation and change
logs from each library version following the previous study [ 25].
For each library, we selected their versions that were released
after January 2019. We chose January 2019 as the starting point
for collecting API mappings, providing a five-year window that
balances sufficient version numbers with manageable human
effort. The version ranges are presented in Table I. Specifically,
we reviewed the documentation and change logs of each
library and manually look for deprecated API occurrences
which indicate the corresponding the mapping replacements.
For instance, in the API documentation of PyTorch, version
1.9.0 [ 58], a deprecation message indicates that “torch.lstsq() is
deprecated in favor of torch.linalg.lstsq() and will be removed in
a future PyTorch release. ” , where the mapping of the deprecated
API to the replacing API is torch.lstsq →torch.linalg.lstsq . For
one-to-many mappings ( i.e.,one deprecated API mapped to
multiple replacing APIs), we split them into many one-to-one
mappings. Two authors independently collected the data, with a
third author resolving inconsistencies. The process took about
three working days, and the Jaccard coefficient between the
first two collectors’ mappings was 91.8%. In total, we collected
a preliminary number of 247 API mappings.
B. Completion Prompt Construction
We constructed code completion prompts by searching real-
world code snippets that contain usages of either deprecated
APIs or replacing APIs from collected API mappings in
open-source Python repositories, to simulate realistic code
completion scenarios.
1) Outdated and Up-to-dated Function Location.: We uti-
lized Sourcegraph [ 59], a widely used code search service
to search code snippets. It supports integration with GitHub
where we can retrieve Python source files from millions of
open-source code repositories. For each deprecated or replacingAPI, we constructed search queries using both its full qualified
name (FQN) ( e.g., torch.lstsq ) and a logical disjunction of its
constituent parts ( e.g., “torch AND linalg AND lstsq”) to ensure
comprehensive retrieval. For each retrieved Python source file,
we parsed it into an Abstract Syntax Tree (AST) and extracted
the containing functions that invoked the deprecated or replac-
ing APIs. Specifically, we located function definition nodes in
the AST and traversed its descendants. For each descendant, we
checked if it is a function call node and matched the function
call to the deprecated or replacing APIs. To correctly match
the function call via API FQNs, we performed lightweight
object type resolution and alias resolution, similar to [25].
•Object Type Resolution : In the object-oriented programming
(OOP) languages, the APIs can be encapsulated into a class
as a method. Therefore, determining the FQN of the API
invocation need to resolve the corresponding type of the
invoking object. For example, the pandas library defines a
core class DataFrame with a member method loc() and the
client creates an object of class DataFrame , assigns to a
variable dt, and invokes the method using dt.loc() . Typically,
it requires resolving the type of dt. To that end, we analyzed
theassign statements to track object definitions, enabling
us to determine the class names for objects in function
calls and infer the called APIs. For instance, if the object
“dt” in the call dt.loc() was created in a preceding assign
statement ( dt = pandas.DataFrame(...) ), we could infer that
the corresponding API was pandas.DataFrame.loc() .
•Alias Resolution : Developers can alias packages, classes,
and functions in Python using the import-as feature [ 60].
This mechanism requires resolving API aliases by analyzing
import statements. For example, the pandas package is
often imported with the alias “pd” via the statement import
pandas as pd . In this case, pd.DataFrame.loc() was resolved
topandas.DataFrame.loc() . Additionally, Python provides
afrom-import mechanism allowing developers to use
APIs with short names instead of their FQNs. For example,
through from torch.linalg import lstsq , the API in torch.linalg
can be directly called via lstsq() . These short names were
resolved by analyzing the import statements.
After the lightweight object type resolution and alias reso-
lution, we obtained the corresponding FQN for each function
call. We checked whether the corresponding FQNs matched
the APIs in the collected API mappings, identifying the first
matched API as the reference API.
We denote the containing function as an outdated function
if a deprecated API was matched. Meanwhile, we denote it as
anup-to-dated function if a replacing API was matched. We
collected 113,660 Python source files by querying SourceGraph.
We filtered out API mappings that returned either no instances
ofoutdated orup-to-dated functions. As a result, we collected
9,022 outdated and 19,103 up-to-dated functions from 145
API mappings. The statistics are presented in Table I.
2) Incomplete Code Extraction.: In the task of code com-
pletion, developers usually have started with a few lines of
code and pause in the middle, waiting for LLMs to return
the suggested content based on the upward context. Therefore,
API Mapping:   torch.lstsq  ->  torch.linalg.lstsqCompletion PromptReference APIFig. 2: Illustration of Completion Prompt Construction for An
Up-to-dated Function.
to evaluate the performance of LLMs in the scenario, we
constructed the line-level code completion prompts . For each
outdated or up-to-dated function, we located the invocation line
of the deprecated or replacing APIs, respectively. We collected
the preceding lines before the invocation line into our line-
level code completion prompts for an outdated or up-to-dated
function, which is usually incomplete.
Figure 2 represents one of our collected up-to-dated functions.
The function invokes a API of PyTorch, i.e., torch.linalg.lstsq
in the fourth line. The line-level code completion prompt for
this function is highlighted in the wine-red dotted rectangle.
After processing all outdated and up-to-dated functions, we
obtained two corresponding datasets, denoted as OandU,
respectively. Each sample in OandUwas formatted as ( pmpt ,
dep→rep), where pmpt is a code completion prompt pmpt
anddep→repdenotes an API mapping from the deprecated
API to the corresponding replacing API.
C. LLM-based Code Completion
We leverage multiple LLMs including open-source and
closed-soure with varying parameter sizes and observe their
performance on the code completion task. The complete LLM
list is presented in Table II.
•CodeGen-350m, 2b, 6b : CodeGen [ 3] is a family of LLMs
developed by Salesforce specifically for code generation.
•DeepSeek-1.3b : DeepSeek-Coder [ 6] is designed on top of
Transformer and tailored for code-related applications.
•StarCoder2-3b : StarCoder2 [ 4] is an LLM optimized for
coding tasks. It leverages large-scale pre-training on code
datasets to understand programming languages deeply.
•CodeLlama-7b : CodeLlama [ 5] is a specialized variant of
Meta’s LLaMA [61], adapted for programming tasks.
•GPT-3.5 : GPT-3.5 [ 62] is a general-purpose language model
developed by OpenAI. While it is not exclusively designed
for coding, it possesses powerful code generation capabilities
due to its extensive training on diverse text, including
programming languages.
The first six code LLMs were downloaded from Hugging
Face [ 63]. For LLMs with Python-specific versions available,
we utilized those versions to improve completion results for
Python functions. Consequently, the versions used for Code-
Gen and CodeLlama were CodeGen-{350M,2B,6B}-mono and
CodeLlama-7b-Python-hf , which were fine-tuned on additional
Python corpora. For DeepSeek and StarCoder2, the versions
employed were deepseek-coder-1.3b-instruct ,starcoder2-3b ,
respectively. For the general purpose LLM, i.e.,GPT-3.5, weTABLE II: Evaluated Large Language Models (LLMs)
Model # Params Python Fine-Tuned Open-Source
CodeGen-350m 350 M ✓ ✓
CodeGen-2b 2 B ✓ ✓
CodeGen-6b 6 B ✓ ✓
DeepSeek-1.3b 1.3 B ✗ ✓
StarCoder2-3b 3 B ✗ ✓
CodeLlama-7b 7 B ✓ ✓
GPT-3.5 175 B UNKNOWN ✗
queried the model via its official online APIs [ 64], using the
gpt-3.5-turbo version released in January 2024.
For the six LLMs specifically tailored for code-related tasks,
i.e.,three versions of CodeGen, DeepSeek, StarCoder2, and
CodeLlama, the constructed prompts can be directly fed into
the models to generate completions. Meanwhile, for the general-
purpose GPT-3.5, we provided an instruction to specify the task,
preventing the model from performing beyond code completion.
The instruction was: “Complete and output the next line for
the following Python function: pmpt ”. For all these LLMs, we
utilized greedy decoding ( i.e.,choosing the token with highest
possibility at each decoding step) to generate one completion
for each prompt in OorU. The maximal output token limit was
set to 50. The greedy decoding for GPT-3.5 was implemented
through setting the temperature parameter to 0.
The procedure of LLM-based completion is defined as:
comp←LLM (pmpt )
D. Completion Result Annotation
For each sample ( pmpt ,dep→rep), we examined the
completions generated by LLMs and determine whether the
studied API was predicted and whether the deprecated API or
replacing API was predicted. Specifically, we extract the FQN
of the API invocation in the predicted line using the same
object type resolution and alias resolution in Sec. III-B 1. The
annotation procedure is formally described as follows:
{DepC ,RepC,Others } ← anno (comp )
Specifically, DepC denotes that the LLM gives a Deprecated
Completion suggestion, and RepC denotes that the LLM gives
a Replacing Completion suggestion. We identify DepC and
RepC by matching the FQN of an invocating API to either dep
orrep.Others denotes that the LLM suggests neither of the
mapping APIs. Moreover, if a completion was annotated as
either DepC orRepC , we treat it as plausible . This indicates
that the LLM successfully understood the code context and
selected a plausible API functionality.
E. Metrics
We investigate the performance of the LLMs using the
following metrics:
•API Usage Plausibility (AUP): This metric measures the
portion of plausible completions, which were annotated as
DepC orRepC . AUP is defined as:
AUP =1
|P|X
p∈PI(anno (LLM (p))∈ {DepC ,RepC})•Deprecated Usage Rate (DUR): This metric calculates
the rate of plausible completions that were annotated as
DepC . DUR is defined as:
DUR =P
p∈PI(anno (LLM (p)) = DepC )P
p∈PI(anno (LLM (p))∈ {DepC ,RepC})
Pis the prompt set ( i.e.,OorU), and I(·)is a binary function
that returns 1 if the passed argument is true and 0 otherwise.
The API Usage Plausibility (AUP) measures how effectively
LLMs predict “accurate” APIs without considering their dep-
recation status. This metric is crucial because merely counting
deprecated API usages (DUR) does not fully capture how
LLMs manage API deprecations. For instance, a low number of
deprecated API usages might be due to the prevalence of Others
completions, suggesting influences beyond deprecation. To
provide fair comparisons across models, prompts, and libraries,
we calculate the Deprecated Usage Rate (DUR) based on AUP.
A balanced relationship between AUP and DUR, characterized
by a relatively high AUP and low DUR, indicates that LLMs
effectively predict both up-to-date and “accurate” APIs. In
contrast, an imbalance suggests either a lower rate of “accurate”
predictions or a prevalence of outdated APIs. All the LLMs
exhibited low AUPs due to the generation of many Others
predictions, likely because many APIs ( e.g., PyTorch APIs)
share similar usage contexts and offer flexible combinations.
In addition, although these Others may provide alternative
solutions for similar functionalities, they fall outside our focus
on LLMs’ handling of API deprecations. To clarify, we formally
defined both Others and AUP to avoid confusion with terms
like “wrong” or “incorrect”. Nevertheless, we recognize the
value of exploring this issue and leave a broader question
for future work: how to effectively evaluate the functional
correctness of generated completions, given the numerous
alternative implementations for the same goal.
IV. S TUDY RESULTS
We present the experimental results and key findings on the
status quo and root causes of deprecated and replacement API
usage in LLM-based code completion. As illustrated in Figure 1,
the results are categorized into three detailed aspects: Model
Perspective (RQ1), Prompt Perspective (RQ2), and Library
Perspective (RQ3). For each RQ, the results and findings are
presented by first showing the status quo through API Usage
Plausibility andDeprecated Usage Rate , followed by providing
an in-depth Root Cause Analysis .
A. RQ1: Model Perspective for Status Quo and Root Causes
1) Status Quo Analysis: Figure 3 shows the distribution
ofRepC andDepC completions by the different LLMs, and
Table III presents the AUP and DUR metrics.
API Usage Plausibility. The completion distribution and
the low AUP highlight that all the LLMs faced challenges in
predicting plausible API usages for the given prompts. The
AUP of the LLMs for overall dataset ( i.e.,All=O∪U ) ranges
from 9% to 23%, indicating that a majority of predictions
were Others . Among the LLMs, CodeLlama-7b achieved theFig. 3: Distribution of RepC ,DepC , and Others Completions
by LLMs for Prompts from OandU
TABLE III: AUP and DUR for O,U, andO ∪ U
ModelAUP (%) DUR (%)
O U All O U All
CodeGen-350m 7.9 12.4 11.0 77.7 9.0 24.9
CodeGen-2b 15.9 20.1 18.8 89.6 11.3 32.6
CodeGen-6b 20.7 23.6 22.7 90.0 11.1 34.2
DeepSeek-1.3b 9.2 11.8 11.0 69.7 11.6 27.2
StarCoder-3b 8.1 9.9 9.3 79.4 17.0 34.4
CodeLlama-7b 21.6 24.3 23.4 86.9 12.1 34.2
GPT-3.5 11.9 13.8 13.2 85.7 17.8 37.4
highest AUP (23.4%), while StarCoder2-3b had the lowest
overall AUP (9.3%). This may be attributed to the fact that
StarCoder2 was not specifically fine-tuned on additional Python
corpora, unlike other LLMs such as CodeLlama and CodeGen.
Comparing the three versions of CodeGen suggests that the
capacity of LLMs to predict plausible API usages increased
with model size ( i.e.,11.0%, 28.8%, and 23.6% for CodeGen-
350m, -2b, and -6b, respectively), given the model architecture
and training data remain consistent. However, it’s noteworthy
that the largest LLM, GPT-3.5, did not achieve a high AUP
(13.2%), possibly due to the instruction used not being finely
tuned with advanced prompt engineering techniques such as
chain-of-thought (COT) [ 65] and in-context learning (ICL) [ 66].
Summary 1: All the evaluated LLMs faced challenges in
predicting plausible API usages, with AUP ranging from
10% to 30%. Effectiveness of LLM-based code completion
generally improved with model size and language-specific
fine-tuning.
Deprecated Usage Rate. The distribution shown in Figure 3,
along with the DUR metric presented in Table III, indicates
that all LLMs faced issues with using deprecated API usages.
The DUR of the LLMs for the overall dataset ( i.e., All
=O ∪ U ) ranges from 25% to 38%, with larger models
(e.g., CodeGen-6b, CodeLlama-7b, and GPT-3.5) generallypredicting more usages of deprecated APIs. Considering the
differences among the LLMs, CodeLlama-7b and CodeGen-
6b demonstrated the best balance between AUP and DUR.
They achieved significant improvements in AUP compared to
other LLMs, with a comparable DUR of 34.2%. Conversely,
StarCoder2-3b and GPT-3.5 exhibited higher DUR ( i.e.,34.4%
and 37.4%) despite having much lower AUP. These results
indicate that the preference of LLMs for using deprecated
or replacing APIs is not closely related to their capacity for
predicting plausible completions.
Summary 2: All the evaluated LLMs faced issues with
deprecated API usages, with DUR ranging from 25% to
38%, and larger models exhibiting higher DUR. Among
the LLMs, CodeLlama-7b and CodeGen-6b demonstrated
the best balance between AUP and DUR.
2) Potential Cause Discussion: From the model perspective,
the causes of deprecated API usages consist of two main points:
Model Training: As libraries evolve, both deprecated APIs and
their replacements are commonly found in open-source code
repositories. Since the training data for LLMs are primarily
sourced from these repositories without filtering for deprecated
APIs, they often include instances of deprecated API usage.
While the training datasets for specific LLMs like CodeLlama
and GPT-3.5 are not publicly available, preventing direct
inspection, the distribution of deprecated API usages in open-
source code repositories can serve as a proxy for understanding
the nature of their training data. In this study, we identified
9,022 instances of deprecated API usage and 19,103 instances
of their replacements in open-source code repositories. These
coarse statistics suggest that a significant portion of the
training data likely includes deprecated APIs. Moreover, prior
studies have demonstrated a relationship between training data
distribution and model output behavior [ 67], [68]. When trained
on data containing deprecated APIs, LLMs may “memorize”
these APIs and associated usage patterns as part of their learned
knowledge [ 69], [70]. The different training datasets also led
to different AUP and DUR of the LLMs.
Model Inference: LLMs generated completions by predicting
token probabilities based on their learned prior knowledge
(e.g., memorized API usage contexts) and applying token
selection strategies ( e.g., greedy search or beam search). Given
certain contexts, LLMs were likely to predict deprecated API
usages due to high token probabilities, without considering
any posterior API deprecation knowledge. To support this
hypothesis, we calculated the generative likelihoods of both
depandrepfor each prompt pmpt across open-source LLMs
by analyzing predicted token probabilities during decoding. To
begin, we input pmpt into an LLM, followed by sequentially
feeding each token in dep(orrep) into the model. Throughout
this process, we recorded the predicted token probability
distributions prior to each token in dep(orrep) being input.
Next, we extracted the predicted probabilities for tokens in dep
(orrep) from these distributions using their vocabulary indices.
The generative likelihoods of depandrepwere then computedby summing the log probabilities of their tokens, a common
method in language modeling [ 71]. We performed a paired
t-test on these likelihoods. The t-statistic values indicate that
for all six open-source LLMs, the likelihoods of deprecated
APIs are significantly higher than those of replacing APIs for
prompts from O, whereas the opposite is true for prompts from
U. The pvalues are approximately zero.
Summary 3: There are two primary reasons why LLMs
predict deprecated APIs: the presence of deprecated API us-
ages in corpora during model training, and the absence of
posterior knowledge about API deprecations during model
inference.
B. RQ2: Prompt Perspective for Status Quo and Root Causes
1) Status Quo Analysis: Table III presents the AUP and
DUR for prompts from the two datasets, i.e.,OandU.
API Usage Plausibility. Between the prompts from the
two different datasets, OandU, there are some differ-
ences in AUP for all LLMs, with relative differences ( i.e.,
(AUPU−AUPO)/AUPO) ranging from 12.5% to 60.0%.
This disparity may be attributed to the imbalance in the number
of outdated and up-to-dated functions in the LLMs’ training
corpora [ 72], [73]. Indirect evidence for this is that, in this study,
the up-to-dated functions collected from open-source code
repositories were about twice as many as the outdated functions
(i.e.,19,103 vs.9,022), even though there was no collection
preference. Given that LLMs were often trained on open-
source code repositories, they likely learned more up-to-dated
functions than outdated functions, leading to better AUP for
theUdataset. Additionally, larger LLMs showcased smaller
AUP differences ( e.g., 16% for CodeLlama-7b), possibly due
to the better generalizability.
Summary 4: The LLMs showcased difference in AUP
between the two datasets OandU. This difference is
possibly attributed to the different distribution of outdated
and up-to-dated functions in the training corpora of LLMs.
Deprecated Usage Rate. When considering OandU
separately, all LLMs consistently demonstrated extremely high
deprecated usage rates for O(70%-90% DUR) and relatively
low rates for U(9%-18% DUR). This significant difference is
also evident in the distribution of RepC andDepC completions
shown in Figure 3. This observed discrepancy can be attributed
to the tendency of LLMs to predict reference APIs (i.e., the
APIs used in the original functions) for most prompts in
both the OandUdatasets, reflecting the unique contextual
characteristics of each prompt. To quantify this, we define an
auxiliary metric, the Reference Usage Rate (RUR) , as follows:
RUR =P
p∈PI(LLM (p) =reference )P
p∈PI(anno (LLM (p))∈ {DepC ,RepC}),
which measures the proportion of reference APIs in plausible
completions. RUR is equal to DUR for Oand (1−DUR )
forU. Thus, no significant difference exists between OandU
in terms of RUR, which is consistently within the 70%-90%range for both. Nonetheless, the 9%-18% DUR for Uindicates
that LLMs still predicted the usage of deprecated APIs, even
for the prompts from up-to-dated functions.
Summary 5: The LLMs consistently exhibited a significant
difference in DUR between the two datasets, with extremely
high deprecated API usage rates for O(70%-90% DUR)
and relatively low rates for U(9%-18% DUR).
2) Potential Cause Discussion: The contextual characteris-
tics of the input completion prompts can significantly influence
LLMs’ use of deprecated APIs. Since the completions were
generated based on the input prompts, specific contexts can
lead LLMs to use deprecated APIs. As presented above, the
LLMs showed significantly different DUR for prompts from
OandU. Upon comparing the prompts from the two datasets,
we found that contextual characteristics of the prompts, such
as specific variables and function calls, lead to the discrepancy
of LLMs’ predictions, i.e.,whether to use deprecated APIs or
their replacements.
To quantitatively assess differences in contextual characteris-
tics, we conducted lightweight contextual feature extraction for
prompts from OandUusing static analysis. Specifically, we
grouped prompts according to their reference API, denoted as
api, with each group represented as P(api). For each prompt in
P(api), we identified the function used to construct it ( cf.Sec-
tion III-B ) and extracted the variable and function call names
in this function. After analyzing all prompts within P(api),
the extracted variable and function call names were compiled
into a contextual feature set, represented by Feat (P(api)),
for the prompt group. Through this approach, for each API
mapping dep→rep, we obtained two paired prompt groups
P(dep)⊂ O andP(rep)⊂ U , along with their feature
sets,Feat (P(dep))andFeat (P(rep)). We then assessed
the contextual similarity between P(dep)andP(rep)using
|Feat (P(dep ))∩Feat (P(rep ))|
|Feat (P(dep ))∪Feat (P(rep ))|. The overall contextual similarity
between OandUwas determined by averaging the cxt-sim
scores of paired prompt groups across all API mappings.
The final similarity is 0.266. This low similarity highlights a
significant difference in the contextual characteristics between
the prompts from OandU, influencing the model’s reference
decoding direction ( cf.Section IV-A2).
Summary 6: The contextual characteristics of the input
prompts, such as the defined variables and function calls,
contribute a significant influence to the deprecated API
usage.
C. RQ3: Library Perspective for Status Quo and Root Causes
We conducted a detailed analysis to examine the LLMs’
completions across different libraries.
1) Status Quo Analysis: We observe how the AUP and DUR
metrics vary with different libraries.
API Usage Plausibility. The results of API usage plausibility
are illustrated in the scatter plots depicted in Figure 4, where
each data point signifies the AUP of an LLM for a specific
library. Across the 8 libraries, most LLMs exhibited relativelyFig. 4: AUP by Different LLMs across Eight Libraries
low API usage plausibility for both OandU, with AUP below
30%. Notable exceptions were Pandas and TensorFlow, where
CodeLlama-7b, CodeGen-6b, and CodeGen-2b (represented
by symbols “ ★”, “ ”, and “ ”, respectively) achieved
better AUP (around or greater than 40%). This aligns with the
results presented in Model Perspective, where CodeLlama-7b,
CodeGen-6b, and CodeGen-2b achieved the best results for
the overall dataset. On the other hand, among the libraries,
completion prompts from SciPy and seaborn posed the most
difficulty for LLMs in predicting plausible API usages, with
AUP consistently below 15%.
Summary 7: Most LLMs exhibited relatively low API us-
age plausibility across the 8 libraries, with AUP below 30%.
CodeLlama-7b, CodeGen-6b, and CodeGen-2b achieved
better AUP for Pandas (about 45%-65%) and TensorFlow
(around 40%).
Deprecated Usage Rate. The results are presented in the
scatter plots shown in Figure 5, where each data point represents
the DUR of a particular LLM for a specific library. The results
reveal significant differences in the usage of deprecated APIs
across the libraries, with DUR ranging from approximately
0% to 100%. Specifically, LLMs generally showed low DUR
(around or below 20%) for Numpy, scikit-learn, and TensorFlow.
In contrast, LLMs exhibited consistently high DUR for SciPy
(approximately 30%-50%) and PyTorch (approximately 50%-
70%), and unstable DUR for Pandas (approximately 30%-
80%), seaborn (approximately 20%-60%), and Transformers
(approximately 40%-100%).
Summary 8: LLMs showed significant differences in the
usage of deprecated APIs across various libraries, with DUR
ranging from approximately 0% to 100%. LLMs exhibited
consistently high DUR for SciPy and PyTorch, and unstable
DUR for Pandas and Transformers.
2) Potential Cause Discussion: After analyzing the com-
pletion prompts and LLMs’ predictions, we found that the
AUP differences between these libraries were primarily due
to the characteristics of the API usage context. More specif-
Fig. 5: DUR by Different LLMs across Eight Libraries
ically, the usage contexts of certain APIs followed common
patterns and were surrounded by related APIs, allowing
advanced LLMs like CodeLlama-7b to infer the desired API
functionality based on the completion prompt. For example,
many utility APIs in TensorFlow, such as the deprecated
tensorflow.compat.v1.initialize_all_variables and its replace-
ment tensorflow.compat.v1.global_variables_initializer , were
often used in recognizable patterns that were easier for LLMs
to predict. Conversely, some APIs were used more flexibly
in diverse contexts and lacked obvious combinations with
other APIs, leading to low AUP for LLMs in predicting such
APIs. For instance, many APIs in SciPy, like the deprecated
scipy.misc.comb and its replacement scipy.special.comb , can
be used in diverse contexts to produce combinations for a data
sequence. Since the data sequence can originate from numerous
sources and be structured in various ways ( e.g., Numpy array
and Pandas Series), it is challenging for LLMs to predict these
APIs based on the completion prompt without additional hints.
The characteristics of API deprecations during library
evolution also significantly impacted the use of deprecated APIs
in LLM-based code completions. Some APIs were deprecated
due to simple package refactoring, leading to similar usage
patterns for the deprecated APIs and their replacements. For
example, in the version 1.9.0 release of PyTorch, many APIs
for tensor linear algebra were moved from the torch package to
thetorch.linalg package without changes to the API parameters
or usage patterns ( e.g., torch.lstsq →torch.linalg.lstsq ). In such
cases, LLMs found it more difficult to distinguish between
deprecated APIs and their replacements, resulting in more
frequent use of deprecated APIs in the predicted completions.
To further validate this attribution, we gathered prompts
associated with each library from OandU, denoted as Ol
andUl. We then performed feature extraction and calculated
the contextual similarity between OlandUlfollowing the
same approach as in Section IV-B 2. A higher contextual
similarity indicates that OlandUl, before and after API
deprecation, share more common contextual characteristics and
usage patterns. Higher Ol-Ulcontextual similarity suggests
that it is more challenging for LLMs to discern whether toTABLE IV: Relationship between Ol-UlContextual Similarity
and DUR Range across Eight Libraries
Library Similarity DUR Range
Numpy 0.056 <20%
Pandas 0.193 30%-80%
scikit-learn 0.060 <20%
SciPy 0.094 30%-50%
seaborn 0.054 20%-60%
TensorFlow 0.078 <25%
PyTorch 0.126 50%-70%
Transformers 0.672 40%-100%
use deprecated or replacing APIs based on the given prompts.
Table IV shows the relationship between Ol-Ulcontextual
similarity and DUR ranges across the eight libraries, indicating
that the DUR range generally increases with higher Ol-Ul
contextual similarity.
Summary 9: The characteristics of API deprecations
during library evolution significantly impacted the use of
deprecated APIs in LLM-based code completions. Minor
changes between deprecated APIs and their replacements,
such as simple package refactoring, often led to more
pronounced issues with deprecated API usages.
V. L IGHTWEIGHT MITIGATIONS
Based on the findings regarding the causes of deprecated
API usage, we proposed two lightweight mitigation approaches,
which can serve as baselines for future investigation.
A. Motivation
As analyzed in the RQs, the causes of deprecated API usage
in LLM-based completions can be attributed to Model Training,
Model Inference, Prompt, and Library aspects. In this section,
we further explore the feasibility of mitigating deprecated API
usage issues from these four perspectives.
During model training, a direct mitigation is to clean up
the code containing deprecated APIs from the training corpora.
However, this is impractical due to the constant library evolu-
tion. New API deprecations would cause repeatedly training
corpora rebuilding and model retraining. From the library
perspective, we cannot control the evolution of the library
and API deprecation. Therefore, mitigation should focus on
decoding strategies in model inference and prompt engineering.
B. Two Lightweight Approaches
Our basic idea is illustrated in Algorithm 1. Given an LLM,
its generation process can generally be formulated as follows:
TO=LLM (TI),
where TIandTOare the input token sequence and output
token sequence, respectively. In the context of LLM-based
code completion, TIcorresponds to the input prompt pmpt ,
andTOcorresponds to the predicted completion comp (line 1).
When comp contains a deprecated API dep(line 3), we need to
perform fixing approaches, i.e.,theFIXprocedure, to replace
depwith the corresponding replacement rep(line 4). Note thatAlgorithm 1: Deprecation-Aware Code Completion
Input: Prompt pmpt , API Mappings M
Output: Completion comp
1comp←LLM (pmpt )
2for(dep→rep)∈ M do
3 if C ONTAINS (comp, dep )then
4 comp←FIX(pmpt, comp, dep, rep )
5 break
Algorithm 2: Approach 1 - R EPLACE API
1Procedure F IX(pmpt, comp, dep, rep ):
2 prefix ←REPLACE DEP(comp, dep, rep )
3 suffix ←LLM (pmpt⊕prefix )
4 comp∗←prefix ⊕suffix
5 return comp∗
during the CONTAINS procedure in line 3, alias resolution is
conducted similarly to the process in Section III-B1.
The FIXprocedure includes reconstructing input, through
either (i) replacing the deprecated API tokens or (ii) inserting
additional replacing prompts, and then regenerating output:
•Approach 1 - REPLACE API: Replacing Deprecated API
Tokens then Regenerating. As shown in Algorithm 2,
REPLACE DEPremoves the tokens corresponding to dep
and any subsequent tokens from comp , and then appends the
tokens of rep(line 2). This results in a prefix prefix that
includes rep. The prefix is concatenated with the pmpt ,
and the LLM generates the suffix (line 3), i.e.,arguments
forrepand the remaining tokens to complete the code line.
This concatenation forms the fixed completion comp∗.
•Approach 2 - INSERT PROMPT : Inserting Additional
Replacing Prompt then Regenerating. As shown in
Algorithm 3, CREATE DEPPMPT constructs an additional re-
placing prompt pmpt′(line 2), formatted as inline comments
to guide the LLM to use repinstead of dep. By expressing
the replacing instruction into inline comments, the re-written
prompt ( i.e.,pmpt⊕pmpt′) can be naturally processed
by the LLM to continue writing the code. The replacing
prompt is structured as follows, where “ {comp} ”, “{dep} ”,
and “ {rep} ” are placeholders for the original completion,
deprecated API, and replacing API, respectively, and “ ...”
represents the indentation to ensure syntax correctness.
...# {comp}
...# {dep} is deprecated, use {rep} instead and
,→revise the return value and arguments.
The created pmpt′is then concatenated with pmpt and fed
into the LLM to generate a new completion comp∗(line 3).
Algorithm 3: Approach 2 - I NSERT PROMPT
1Procedure F IX(pmpt, comp, dep, rep ):
2 pmpt′←CREATE REPPMPT (comp, dep, rep )
3 comp∗←LLM (pmpt⊕pmpt′)
4 return comp∗TABLE V: Evaluation Results of Proposed Approaches
ModelREPLACE API I NSERT PROMPT
FR(%) ES(%) EM(%) FR(%) ES(%) EM(%)
CodeGen-350m 92.1 82.3 30.8 25.7 58.7 8.9
CodeGen-2b 88.2 84.6 38.8 66.1 66.0 23.3
CodeGen-6b 85.2 85.3 43.5 77.4 72.9 35.3
DeepSeek-1.3b 99.6 80.9 31.7 93.5 77.7 24.4
StarCoder-3b 90.9 85.0 42.2 85.3 72.2 29.1
CodeLlama-7b 99.5 85.7 48.1 95.5 82.0 43.3
GPT-3.5 – – – 97.2 76.2 20.5
C. Evaluation
We conducted experiments to address the following research
question:
•RQ4: How effectively can the proposed approaches fix
deprecated API usage in completions?
1) Evaluation Procedure: For each LLM, we selected up-
to-dated samples from Uwhere the LLM predicted DepC
completions using deprecated APIs, i.e.,
T={(pmpt, dep →rep)∈ U :anno (LLM (pmpt )) = DepC},
to constitute the evaluation data. These samples were chosen
because they have corresponding ground-truth completions
(e.g., the line following the prompt in Figure 2), which are
essential for assessing the effectiveness of the proposed fixing
approaches. For each sample, we employed the deprecation-
aware code completion illustrated in Algorithm 1 with the
two fixing approaches REPLACE API and INSERT PROMPT
to prompt the LLM to generate a completion. We used the
following three metrics to assess their effectiveness:
•Fixed Rate (FR) : This metric indicates the proportion of
RepC completions predicted by the fixing approaches.
•Edit Similarity (ES) [74]: This metric measures the
similarity between the predicted completions and the
ground-truth completions by analyzing the edit operations
required to transform one into the other.
•Exact Match (EM) : This metric calculates the rate of
predicted completions that exactly match the ground-truth
completions after normalizing the return values of function
calls ( i.e.,replacing each element in return value with “_”).
2) Results and Analysis: The evaluation results of the
proposed fixing approaches are presented in Table V.
REPLACE API.Using the REPLACE API fixing approach
(Algorithm 2), all the LLMs achieve high fixed rates (FR),
with values exceeding 85%. Failures in fixing are mainly
due to syntax errors or incorrect function calls caused
by erroneous tokens following the replaced APIs. For ex-
ample, consider a DepC completion: “meta_graph_def =
tf.saved_model.loader.load(...)” predicted by the original com-
pletion procedure (Algorithm 1, line 1). REPLACE API re-
places the deprecated API tf.saved_model.loader.load with
its replacement tf.saved_model.load , producing a prefix
of “meta_graph_def=tf.saved_model.load” (line 2 of Algo-
rithm 2). However, CodeGen-2b then predicts a suffix of
“_meta_graph_def(...)” (line 3 of Algorithm 2), resulting in an
erroneous function call: tf.saved_model.load_meta_graph_def() .This issue arises because the replacement operation in RE-
PLACE API can disrupt the naturalness of the code context [ 75]
and the LLMs’ decoding process. An additional interesting find-
ing is that for the three versions of CodeGen, the FR decreases
as the model size increases, suggesting that larger models might
be more sensitive to interventions during decoding.
The completions generated by LLMs using the REPLACE API
approach exhibit high ES of over 80%, and EM rates between
30% and 50%, with these rates increasing alongside model size.
The inaccuracies in the completions often involve incorrect
return values and arguments for the replacing APIs. Incorrect
return values arise because the replacing API might include
different elements compared to the deprecated API, and the
REPLACE API approach cannot resolve such inconsistencies in
theprefix (line 2 of Algorithm 2). The incorrect arguments
are primarily due to the LLMs’ limitations in correctly utilizing
replacing APIs, especially those with complex argument lists.
INSERT PROMPT .When applying the I NSERT PROMPT fix-
ing approach (Algorithm 3), the LLMs exhibited significantly
varied fixed rates, ranging from 25.7% to 97.2%. This variation
suggests that larger models generally possess a stronger capacity
to interpret and utilize inserted prompts formatted as inline
comments. An exception to this trend is DeepSeek-1.3b, which
achieved a notably high FR of 93.5%. This success can be at-
tributed to using the deepseek-coder-1.3b-instruct
version, which has robust zero-shot instruction-following
capabilities. Moreover, the FR differences among various LLMs
highlight their sensitivity to prompt construction [ 76], [77],
which indicates that different LLMs may require specialized
additional prompts in INSERT PROMPT for optimal performance.
Considering edit similarity and exact match, the completions
generated by GPT-3.5 and DeepSeek-1.3b using the INSERT -
PROMPT approach do not perform as well as their fixed rates
suggest. Despite being fine-tuned with an instruct-tuning corpus,
they are not specifically fine-tuned on Python code. As a result,
they can follow the instructions in the additional prompt to
use replacing APIs but struggle to use those APIs correctly.
Comparison. The comparison between REPLACE API and
INSERT PROMPT suggests that direct interventions in the
decoding process are more effective than zero-shot prompt
engineering. However, the results also reveal the potential of
INSERT PROMPT . First, REPLACE API cannot be applied to
black-box LLMs like GPT-3.5, as their decoding processes
cannot be controlled by users. Second, the additional prompt
employed by INSERT PROMPT was not carefully tuned for each
LLM and was performed in a zero-shot manner. In the future,
fine-tuning the LLMs with instructions specifically designed
for fixing deprecated usage could enhance effectiveness.
Summary 10: REPLACE API effectively addresses depre-
cated API usage for all open-source LLMs, achieving fix
rates exceeding 85%. The fixed completions demonstrate ac-
ceptable accuracy. While INSERT PROMPT does not achieve
sufficient effectiveness and accuracy in fixing completions
containing deprecated API usage, it provides future potential
of exploration.VI. D ISCUSSION
A. Implications
Validating LLM-Generated Code Completions and Is-
suing Alerts for Deprecated API Usages. Our evaluation
study reveals that LLMs frequently use deprecated APIs
during code completion. Such deprecated API usages can be
easily overlooked [ 25], potentially introducing bugs or security
vulnerabilities into software projects. Therefore, implementing
a validation mechanism for deprecated API usage in LLM-
generated code completions is crucial to ensure the reliability of
the code. Such a validation mechanism can be further integrated
into the post-processing of LLM-based code completion, such
as issuing alerts to developers.
Fixing and Updating Outdated API Knowledge in LLMs
by Model-Level Repair. The current fixing approaches mitigate
the issues by intervening in decoding process and rewriting
prompts, without addressing the outdated knowledge about
deprecated API usages stored in the LLMs. Given the constant
evolution of libraries, lightweight model repair techniques
are potential solutions for fixing outdated knowledge. Model
Editing is one such direction, which can be categorized into
the following main categories: Memory-based approaches [ 78],
[79], [80], Locating-then-editing approaches [ 81], [82], [83],
and Meta-learning approaches [ 84], [85]. Compared to the
proposed fixing approaches, model editing can directly update
the outdated knowledge about deprecated API usages, even
incorporating the information of entirely new replacing APIs
(i.e.,the replacing APIs introduced after model training and
unseen in the training corpora) into the LLMs.
Leveraging Retrieval-Augmented Generation for Up-to-
dated Code Completion. As discussed in our study findings,
a key cause of the deprecated API usage in LLM-based
completions is the lack of posterior knowledge about API
deprecations. Retrieval-Augmented Generation (RAG) is a
suitable technique that can perfectly align with the need for
posterior knowledge [ 86]. We can explore the possibility of
adopting RAG to mitigate deprecated API usage in LLM-based
code completion by retrieving related knowledge pieces, such
as documentation and usage examples.
Designing Agent & Multi-Agent Systems for Incor-
porating Library Evolution into LLM-Driven Software
Development. In modern software development driven by
LLMs, the issues brought by library evolution are encountered
not only in code completion. With advancements in AI
agents [ 87], [88], [89], we can potentially develop autonomous
agents or multi-agent systems capable of automatically discov-
ering deprecated API usages, identifying correct replacements,
upgrading dependent libraries, and fixing the code [ 90]. To
ensure comprehensive recognition of deprecated API usage
and up-to-date fixes, we should design an effective multi-agent
collaboration pipeline. One agent should scan the generated
code to identify all pieces related to API usage. Another
agent should continuously fetch information online, checking
the latest official API documentation to aid in discovery andcorrection. Finally, a dedicated agent should be responsible for
implementing the necessary library upgrades and code fixes.
B. Threats to Validity
Internal Threats. The primary threat to our study is the
soundness of the static analysis used for function location and
result annotation. Given that Python is a dynamic programming
language, the lightweight object type resolution and alis
resolution we employed may have missed some function calls
of deprecated and replacing APIs during the matching process.
Nevertheless, they did not contribute bias to deprecated APIs
or replacing APIs in our extracted function calls. In the future,
we aim to address this issue by implementing advanced type
inference techniques. Additionally, our study currently focuses
on function-level API deprecation, overlooking parameter-level
deprecations. Future research would investigate a broader type
of deprecated APIs to provide more comprehensive analysis.
External Threats. A primary threat lies in the choice of
Python libraries and LLMs. To mitigate this threat, we reused
libraries examined in previous studies and introduced three pop-
ular deep learning libraries to ensure diversity and timeliness.
For the LLMs, we selected models covering various architec-
tures, model sizes, training corpora, and training strategies to
ensure the generalizability of our findings. Another external
threat is that the study was conducted solely on the Python
language, which may limit the applicability of our findings to
other languages such as Java and C#. In the future, we plan to
explore the impact of library evolution on LLM-based code
completion across a broader range of programming languages.
VII. C ONCLUSION
In this work, we conducted an evaluation study to investigate
the statuses and causes of deprecated API usages in LLM-
based code completion. The study results all evaluated LLMs
encounter challenges in predicting plausible API usages and
face issues with deprecated API usages, influenced by the
distinct code context characteristics of the prompts and the
characteristics of API deprecations during the evolution of
these libraries. We propose two lightweight fixing approaches
to mitigate the deprecated API usages and can serve as baselines
for future research. We also provide implications for the
research directions for the combination of library evolution and
LLM-driven code completion and software development. We
released the code and data of our study at the website [91].
ACKNOWLEDGEMENT
This research project is supported by the National Key R&D
Program of China (2023YFB4503805), the National Research
Foundation, Singapore, and the Cyber Security Agency under
its National Cybersecurity R&D Programme (NCRP25-P04-
TAICeN), DSO National Laboratories under the AI Singapore
Programme (AISG2-GC-2023-008), National Natural Science
Foundation of China under Grant No. 62402342, Shanghai
Sailing Program (No. 24YF2749500), and the Ministry of
Education, Singapore, under its Academic Research Fund Tier
1 (RG96/23).REFERENCES
[1]M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan,
H. Edwards, Y . Burda, N. Joseph, G. Brockman, and Others, “Evaluating
large language models trained on code,” CoRR , vol. abs/2107.03374,
2021. [Online]. Available: https://arxiv.org/abs/2107.03374
[2]D. Fried, A. Aghajanyan, J. Lin, S. Wang, E. Wallace, F. Shi, R. Zhong,
W.-t. Yih, L. Zettlemoyer, and M. Lewis, “Incoder: A generative model
for code infilling and synthesis,” arXiv preprint arXiv:2204.05999 , 2022.
[3]E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y . Zhou, S. Savarese,
and C. Xiong, “Codegen: An open large language model for code with
multi-turn program synthesis,” arXiv preprint arXiv:2203.13474 , 2022.
[4]R. Li, L. B. Allal, Y . Zi, N. Muennighoff, D. Kocetkov, C. Mou,
M. Marone, C. Akiki, J. Li, J. Chim et al. , “Starcoder: may the source
be with you!” arXiv preprint arXiv:2305.06161 , 2023.
[5]B. Rozière, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y . Adi,
J. Liu, T. Remez, J. Rapin, A. Kozhevnikov, I. Evtimov, J. Bitton,
M. Bhatt, C. Canton-Ferrer, A. Grattafiori, W. Xiong, A. Défossez,
J. Copet, F. Azhar, H. Touvron, L. Martin, N. Usunier, T. Scialom, and
G. Synnaeve, “Code llama: Open foundation models for code,” CoRR ,
vol. abs/2308.12950, 2023.
[6]D. Guo, Q. Zhu, D. Yang, Z. Xie, K. Dong, W. Zhang, G. Chen, X. Bi,
Y . Wu, Y . K. Li, F. Luo, Y . Xiong, and W. Liang, “Deepseek-coder:
When the large language model meets programming - the rise of code
intelligence,” CoRR , vol. abs/2401.14196, 2024.
[7]X. Chen, X. Hu, Y . Huang, H. Jiang, W. Ji, Y . Jiang, Y . Jiang, B. Liu,
H. Liu, X. Li et al. , “Deep learning-based software engineering: progress,
challenges, and opportunities,” Science China Information Sciences ,
vol. 68, no. 1, pp. 1–88, 2025.
[8]A. Svyatkovskiy, S. K. Deng, S. Fu, and N. Sundaresan, “Intellicode
compose: Code generation using transformer,” in Proceedings of the
28th ACM joint meeting on European software engineering conference
and symposium on the foundations of software engineering , 2020, pp.
1433–1443.
[9]H. Le, Y . Wang, A. D. Gotmare, S. Savarese, and S. C. H. Hoi,
“Coderl: Mastering code generation through pretrained models and deep
reinforcement learning,” Advances in Neural Information Processing
Systems , vol. 35, pp. 21 314–21 328, 2022.
[10] C. Wang, J. Zhang, Y . Feng, T. Li, W. Sun, Y . Liu, and X. Peng, “Teaching
code llms to use autocompletion tools in repository-level code generation,”
arXiv preprint arXiv:2401.06391 , 2024.
[11] Z. Zeng, H. Tan, H. Zhang, J. Li, Y . Zhang, and L. Zhang, “An extensive
study on pre-trained models for program understanding and generation,”
inProceedings of the 31st ACM SIGSOFT international symposium on
software testing and analysis , 2022, pp. 39–51.
[12] C. Wang, J. Zhang, Y . Lou, M. Liu, W. Sun, Y . Liu, and X. Peng, “Tiger:
A generating-then-ranking framework for practical python type inference,”
arXiv preprint arXiv:2407.02095 , 2024.
[13] C. Wang, J. Liu, X. Peng, Y . Liu, and Y . Lou, “Boosting static resource
leak detection via llm-based resource-oriented intention inference,” arXiv
preprint arXiv:2311.04448 , 2023.
[14] J. Li, Z. Dong, C. Wang, H. You, C. Zhang, Y . Liu, and X. Peng, “Llm
based input space partitioning testing for library apis,” arXiv preprint
arXiv:2501.05456 , 2024.
[15] Z. Fan, X. Gao, M. Mirchev, A. Roychoudhury, and S. H. Tan,
“Automated repair of programs from large language models,” in 2023
IEEE/ACM 45th International Conference on Software Engineering
(ICSE) . IEEE, 2023, pp. 1469–1481.
[16] Y . Wei, C. S. Xia, and L. Zhang, “Copiloting the copilots: Fusing large
language models with completion engines for automated program repair,”
inProceedings of the 31st ACM Joint European Software Engineering
Conference and Symposium on the Foundations of Software Engineering ,
2023, pp. 172–184.
[17] J. Zhang, C. Wang, A. Li, W. Wang, T. Li, and Y . Liu, “Vuladvisor:
Natural language suggestion generation for software vulnerability repair,”
inProceedings of the 39th IEEE/ACM International Conference on
Automated Software Engineering , 2024, pp. 1932–1944.
[18] A. T. Nguyen and T. N. Nguyen, “Graph-based statistical language model
for code,” in 2015 IEEE/ACM 37th IEEE International Conference on
Software Engineering , vol. 1. IEEE, 2015, pp. 858–868.
[19] V . Raychev, M. Vechev, and E. Yahav, “Code completion with statistical
language models,” in Proceedings of the 35th ACM SIGPLAN conference
on programming language design and implementation , 2014, pp. 419–
428.[20] S. Ugare, T. Suresh, H. Kang, S. Misailovic, and G. Singh, “Improving
llm code generation with grammar augmentation,” arXiv preprint
arXiv:2403.01632 , 2024.
[21] D. Guo, S. Lu, N. Duan, Y . Wang, M. Zhou, and J. Yin, “Unixcoder:
Unified cross-modal pre-training for code representation,” arXiv preprint
arXiv:2203.03850 , 2022.
[22] (2023) Github copilot. [Online]. Available: https://github.com/features/
copilot
[23] R. G. Kula, A. Ouni, D. M. German, and K. Inoue, “An empirical study
on the impact of refactoring activities on evolving client-used apis,” Inf.
Softw. Technol. , vol. 93, no. C, pp. 186–199, 2018.
[24] M. Hu and Y . Zhang, “An empirical study of the python/c api on evolution
and bug patterns,” Journal of Software: Evolution and Process , vol. 35,
no. 2, p. e2507, 2023.
[25] J. Wang, L. Li, K. Liu, and H. Cai, “Exploring how deprecated python
library apis are (not) handled,” in Proceedings of the 28th acm joint
meeting on european software engineering conference and symposium
on the foundations of software engineering , 2020, pp. 233–244.
[26] Api lifecycle stages. [Online]. Available: https://developers.meetmarigold.
com/engage/terms/versioning-deprecation/#api-lifecycle-stages
[27] Pytorch: A python package that provides tensor computation and deep
neural networks. [Online]. Available: https://pytorch.org/
[28] D. Zan, B. Chen, D. Yang, Z. Lin, M. Kim, B. Guan, Y . Wang, W. Chen,
and J.-G. Lou, “Cert: continual pre-training on sketches for library-
oriented code generation,” arXiv preprint arXiv:2206.06888 , 2022.
[29] K. Zhang, H. Zhang, G. Li, J. Li, Z. Li, and Z. Jin, “Toolcoder:
Teach code generation models to use api search tools,” arXiv preprint
arXiv:2305.04032 , 2023.
[30] F. F. Xu, U. Alon, G. Neubig, and V . J. Hellendoorn, “A systematic
evaluation of large language models of code,” in Proceedings of the
6th ACM SIGPLAN International Symposium on Machine Programming ,
2022, pp. 1–10.
[31] J. Liu, C. S. Xia, Y . Wang, and L. Zhang, “Is your code generated by
chatgpt really correct? rigorous evaluation of large language models for
code generation,” Advances in Neural Information Processing Systems ,
vol. 36, 2024.
[32] M. Ciniselli, N. Cooper, L. Pascarella, A. Mastropaolo, E. Aghajani,
D. Poshyvanyk, M. Di Penta, and G. Bavota, “An empirical study on the
usage of transformer models for code completion,” IEEE Transactions
on Software Engineering , vol. 48, no. 12, pp. 4818–4837, 2021.
[33] M. Ciniselli, N. Cooper, L. Pascarella, D. Poshyvanyk, M. Di Penta,
and G. Bavota, “An empirical study on the usage of bert models for
code completion,” in 2021 IEEE/ACM 18th International Conference on
Mining Software Repositories (MSR) . IEEE, 2021, pp. 108–119.
[34] H. Ding, V . Kumar, Y . Tian, Z. Wang, R. Kwiatkowski, X. Li,
M. K. Ramanathan, B. Ray, P. Bhatia, S. Sengupta et al. , “A static
evaluation of code completion by large language models,” arXiv preprint
arXiv:2306.03203 , 2023.
[35] M. Izadi, J. Katzy, T. Van Dam, M. Otten, R. M. Popescu, and
A. Van Deursen, “Language models for code completion: A practi-
cal evaluation,” in Proceedings of the IEEE/ACM 46th International
Conference on Software Engineering , 2024, pp. 1–13.
[36] F. Liu, Y . Liu, L. Shi, H. Huang, R. Wang, Z. Yang, and L. Zhang,
“Exploring and evaluating hallucinations in llm-powered code generation,”
arXiv preprint arXiv:2404.00971 , 2024.
[37] Z. Zhang, Y . Wang, C. Wang, J. Chen, and Z. Zheng, “Llm hallucinations
in practical code generation: Phenomena, mechanism, and mitigation,”
arXiv preprint arXiv:2409.20550 , 2024.
[38] A. A. Sawant, M. Aniche, A. van Deursen, and A. Bacchelli, “Under-
standing developers’ needs on deprecation as a language feature,” in
ICSE , 2018, pp. 561–571.
[39] A. A. Sawant, G. Huang, G. Vilen, S. Stojkovski, and A. Bacchelli,
“Why are features deprecated? an investigation into the motivation behind
deprecation,” in ICSME , 2018, pp. 13–24.
[40] A. Mirian, N. Bhagat, C. Sadowski, A. P. Felt, S. Savage, and G. M.
V oelker, “Web feature deprecation: a case study for chrome,” in ICSE-
SEIP , 2019, pp. 302–311.
[41] A. A. Sawant, R. Robbes, and A. Bacchelli, “To react, or not to react:
Patterns of reaction to api deprecation,” Empirical Software Engineering ,
vol. 24, no. 6, pp. 3824–3870, 2019.
[42] R. Robbes, M. Lungu, and D. Röthlisberger, “How do developers react
to api deprecation? the case of a smalltalk ecosystem,” in FSE, 2012,
pp. 1–11.[43] M. Linares-Vásquez, G. Bavota, C. Bernal-Cárdenas, M. Di Penta,
R. Oliveto, and D. Poshyvanyk, “Api change and fault proneness: A
threat to the success of android apps,” in ESEC/FSE , 2013, pp. 477–487.
[44] Y . Wang, B. Chen, K. Huang, B. Shi, C. Xu, X. Peng, Y . Wu, and Y . Liu,
“An empirical study of usages, updates and risks of third-party libraries
in java projects,” in 2020 IEEE International Conference on Software
Maintenance and Evolution (ICSME) . IEEE, 2020, pp. 35–45.
[45] T. McDonnell, B. Ray, and M. Kim, “An empirical study of api stability
and adoption in the android ecosystem,” in ICSM , 2013, pp. 70–79.
[46] A. Hora, R. Robbes, N. Anquetil, A. Etien, S. Ducasse, and M. T. Valente,
“How do developers react to api evolution? the pharo ecosystem case,”
inICSME , 2015, pp. 251–260.
[47] A. A. Sawant, R. Robbes, and A. Bacchelli, “On the reaction to
deprecation of 25,357 clients of 4+ 1 popular java apis,” in ICSME ,
2016, pp. 400–410.
[48] I. Balaban, F. Tip, and R. Fuhrer, “Refactoring support for class library
migration,” in OOPSLA , 2005, pp. 265–279.
[49] J. Henkel and A. Diwan, “Catchup! capturing and replaying refactorings
to support api evolution,” in ICSE , 2005, pp. 274–283.
[50] Z. Xing and E. Stroulia, “Api-evolution support with diff-catchup,” IEEE
Transactions on Software Engineering , vol. 33, no. 12, pp. 818–836,
2007.
[51] B. Dagenais and M. P. Robillard, “Semdiff: Analysis and recommendation
support for api evolution,” in ICSE , 2009, pp. 599–602.
[52] T. Schäfer, J. Jonas, and M. Mezini, “Mining framework usage changes
from instantiation code,” in ICSE , 2008, pp. 471–480.
[53] M. W. Godfrey and L. Zou, “Using origin analysis to detect merging
and splitting of source code entities,” IEEE Transactions on Software
Engineering , vol. 31, no. 2, pp. 166–181, 2005.
[54] W. Wu, Y .-G. Guéhéneuc, G. Antoniol, and M. Kim, “Aura: a hybrid
approach to identify framework evolution,” in ICSE , 2010, pp. 325–334.
[55] K. Huang, B. Chen, L. Pan, S. Wu, and X. Peng, “Repfinder: Finding
replacements for missing apis in library update,” in ASE, 2021.
[56] J. Sallou, T. Durieux, and A. Panichella, “Breaking the silence: the
threats of using llms in software engineering,” in Proceedings of the
2024 ACM/IEEE 44th International Conference on Software Engineering:
New Ideas and Emerging Results , 2024, pp. 102–106.
[57] Most popular programming languages. [Online]. Available: https:
//www.orientsoftware.com/blog/most-popular-programming-languages/
[58] (2023) Pytorch documentation 1.9.0. [Online]. Available: https:
//pytorch.org/docs/1.9.0/
[59] Openai api reference. [Online]. Available: https://sourcegraph.com/search
[60] Pep 221 – import as. [Online]. Available: https://peps.python.org/
pep-0221/
[61] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y . Babaei,
N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, and Others, “Llama 2:
Open foundation and fine-tuned chat models,” CoRR , vol. abs/2307.09288,
2023.
[62] Gpt-3.5 turbo. [Online]. Available: https://platform.openai.com/docs/
models/gpt-3-5-turbo
[63] Hugging face - host git-based models, datasets and spaces on the
hugging face hub. [Online]. Available: https://huggingface.co/models
[64] Sourcegraph: Code search and an ai assistant with the context of
the code graph. [Online]. Available: https://platform.openai.com/docs/
api-reference/chat
[65] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. H.
Chi, Q. V . Le, and D. Zhou, “Chain-of-thought prompting elicits
reasoning in large language models,” S. Koyejo, S. Mohamed, A. Agarwal,
D. Belgrave, K. Cho, and A. Oh, Eds., 2022.
[66] A. K. Lampinen, I. Dasgupta, S. C. Chan, K. Matthewson, M. H.
Tessler, A. Creswell, J. L. McClelland, J. X. Wang, and F. Hill, “Can
language models learn from explanations in context?” arXiv preprint
arXiv:2204.02329 , 2022.
[67] S. M. Xie, S. Santurkar, T. Ma, and P. S. Liang, “Data selection
for language models via importance resampling,” Advances in Neural
Information Processing Systems , vol. 36, pp. 34 201–34 227, 2023.
[68] E. Nijkamp, H. Hayashi, C. Xiong, S. Savarese, and Y . Zhou, “Codegen2:
Lessons for training llms on programming and natural languages,” arXiv
preprint arXiv:2305.02309 , 2023.[69] F. Petroni, T. Rocktäschel, P. Lewis, A. Bakhtin, Y . Wu, A. H. Miller,
and S. Riedel, “Language models as knowledge bases?” arXiv preprint
arXiv:1909.01066 , 2019.
[70] B. Cao, H. Lin, X. Han, L. Sun, L. Yan, M. Liao, T. Xue, and
J. Xu, “Knowledgeable or educated guess? revisiting language models
as knowledge bases,” arXiv preprint arXiv:2106.09231 , 2021.
[71] Y . Bengio, R. Ducharme, and P. Vincent, “A neural probabilistic language
model,” Advances in neural information processing systems , vol. 13, 2000.
[72] J. M. Johnson and T. M. Khoshgoftaar, “Survey on deep learning with
class imbalance,” J. Big Data , vol. 6, p. 27, 2019.
[73] X. Liu, J. Wu, and Z. Zhou, “Exploratory undersampling for class-
imbalance learning,” IEEE Trans. Syst. Man Cybern. Part B , vol. 39,
no. 2, pp. 539–550, 2009.
[74] A. Svyatkovskiy, S. K. Deng, S. Fu, and N. Sundaresan, “Intellicode
compose: Code generation using transformer,” in Proceedings of the
28th ACM joint meeting on European software engineering conference
and symposium on the foundations of software engineering , 2020, pp.
1433–1443.
[75] A. Hindle, E. T. Barr, M. Gabel, Z. Su, and P. Devanbu, “On the
naturalness of software,” Communications of the ACM , vol. 59, no. 5,
pp. 122–131, 2016.
[76] T. Gao, A. Fisch, and D. Chen, “Making pre-trained language models
better few-shot learners,” arXiv preprint arXiv:2012.15723 , 2020.
[77] Z. Jiang, F. F. Xu, J. Araki, and G. Neubig, “How can we know
what language models know?” Transactions of the Association for
Computational Linguistics , vol. 8, pp. 423–438, 2020.
[78] E. Mitchell, C. Lin, A. Bosselut, C. D. Manning, and C. Finn, “Memory-
based model editing at scale,” in International Conference on Machine
Learning . PMLR, 2022, pp. 15 817–15 831.
[79] S. Murty, C. D. Manning, S. Lundberg, and M. T. Ribeiro, “Fixing model
bugs with natural language patches,” arXiv preprint arXiv:2211.03318 ,
2022.
[80] A. Madaan, N. Tandon, P. Clark, and Y . Yang, “Memory-assisted
prompt editing to improve gpt-3 after deployment,” arXiv preprint
arXiv:2201.06009 , 2022.
[81] K. Meng, D. Bau, A. Andonian, and Y . Belinkov, “Locating and editing
factual associations in gpt,” Advances in Neural Information Processing
Systems , vol. 35, pp. 17 359–17 372, 2022.
[82] K. Meng, A. S. Sharma, A. Andonian, Y . Belinkov, and D. Bau, “Mass-
editing memory in a transformer,” arXiv preprint arXiv:2210.07229 ,
2022.
[83] X. Li, S. Li, S. Song, J. Yang, J. Ma, and J. Yu, “Pmet: Precise model
editing in a transformer,” in Proceedings of the AAAI Conference on
Artificial Intelligence , vol. 38, no. 17, 2024, pp. 18 564–18 572.
[84] N. De Cao, W. Aziz, and I. Titov, “Editing factual knowledge in language
models,” arXiv preprint arXiv:2104.08164 , 2021.
[85] E. Mitchell, C. Lin, A. Bosselut, C. Finn, and C. D. Manning, “Fast
model editing at scale,” arXiv preprint arXiv:2110.11309 , 2021.
[86] P. Lewis, E. Perez, A. Piktus, F. Petroni, V . Karpukhin, N. Goyal,
H. Küttler, M. Lewis, W.-t. Yih, T. Rocktäschel et al. , “Retrieval-
augmented generation for knowledge-intensive nlp tasks,” Advances
in Neural Information Processing Systems , vol. 33, pp. 9459–9474, 2020.
[87] S. Gronauer and K. Diepold, “Multi-agent deep reinforcement learning:
a survey,” Artificial Intelligence Review , vol. 55, no. 2, pp. 895–943,
2022.
[88] Y . Talebirad and A. Nadiri, “Multi-agent collaboration: Harnessing the
power of intelligent llm agents,” arXiv preprint arXiv:2306.03314 , 2023.
[89] B. Ellis, J. Cook, S. Moalla, M. Samvelyan, M. Sun, A. Mahajan,
J. Foerster, and S. Whiteson, “Smacv2: An improved benchmark for
cooperative multi-agent reinforcement learning,” Advances in Neural
Information Processing Systems , vol. 36, 2024.
[90] C. Wang, Z. Chen, T. Li, Y . Zhao, and Y . Liu, “Towards trustworthy llms
for code: A data-centric synergistic auditing framework,” arXiv preprint
arXiv:2410.09048 , 2024.
[91] (2024) Replication package. [Online].
Available: https://anonymous.4open.science/r/
Replication-Deprecated-API-Usage-in-LLM-based-Code-Completion/
README.md