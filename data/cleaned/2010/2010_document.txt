understanding myths and realities of test suite evolution leandro sales pinto politecnico di milano pinto elet.polimi.itsaurabh sinha ibm research india saurabhsinha in.ibm.comalessandro orso georgia inst.
of technology orso cc.gatech.edu abstract test suites once created rarely remain static.
just like the application they are testing they evolve throughout their lifetime.
test obsolescence is probably the most known reason for test suite evolution test cases cease to work because of changes in the code and must be suitably repaired.
repairing existing test cases manually however can be extremely time consuming especially for large test suites which has motivated the recent development of automated test repair techniques.
we believe that for developing effective repair techniques that are applicable in real world scenarios a fundamental prerequisite is a thorough understanding of how test cases evolve in practice.
without such knowledge we risk to develop techniques that may work well for only a small number of tests or worse that may not work at all in most realistic cases.
unfortunately to date there are no studies in the literature that investigate how test suites evolve.
to tackle this problem in this paper we present a technique for studying test suite evolution a tool that implements the technique and an extensive empirical study in which we used our technique to study many versions of six realworld programs and their unit test suites.
this is the first study of this kind and our results reveal several interesting aspects of test suite evolution.
in particular our findings show that test repair is just one possible reason for test suite evolution whereas most changes involve refactorings deletions and additions of test cases.
our results also show that test modifications tend to involve complex and hard to automate changes to test cases and that existing test repair techniques that focus exclusively on assertions may have limited practical applicability.
more generally our findings provide initial insight on how test cases are added removed and modified in practice and can guide future research efforts in the area of testsuite evolution.
categories and subject descriptors d. .
testing and debugging keywords test suite evolution test suite maintenance unit testing permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
sigsoft fse november cary north carolina usa.
copyright acm ... .
.1public void testdiscardsemicolons throws throwable tokenizer t new javatokenizer sourcecode sourcecode new sourcecode s t r i n g data p u b l i c class foo p r i v a t e i n t x tokens tokens new tokens t .
tokenize sourcecode tokens new stringreader data broken statement assertequals tokens .
size a 1public void testdiscardsemicolons throws throwable tokenizer t new javatokenizer sourcecode sourcecode new sourcecode s t r i n g data p u b l i c class foo p r i v a t e i n t x tokens tokens new tokens sourcecode .
readsource new stringreader data added statement t .
tokenize sourcecode tokens modified statement assertequals tokens .
size b figure two versions of a test case from pmd s unit test suite a version .
broken and b version .
repaired.
.
introduction test cases form the first line of defense against the introduction of software faults especially when retesting modified software .
with the availability of convenient testing frameworks such as junit and the adoption of agile development methodologies in particular writing unit test cases is an increasingly common practice nowadays.
developers routinely create test suites that consist of unit tests and that they run periodically on their code.
test suites are not static entities they constantly evolve along with the application they test.
in particular changes in the application can break test cases in some cases even a small change in the application code can affect a large number of tests.
in general a test failure on a new version of the software can either expose application faults or result from a problem with the test itself.
on observing a failure the first task for the developer is to determine which of these two is the cause of the failure and once this distinction is made either fix the problem in the code or fix the broken test.
a broken test if it covers a valid functionality should ideally be repaired.
alternatively if the repair is unduly complex to perform or if the test was designed to cover a functionality that no longer exists in the application the test should be removed from the test suite.
to illustrate with an example figure shows two versions of a unit test case from the test suite of pmd one of the programs used in our empirical study.
a change in pmd s api broke the original version of the test case which had to be fixed by adding a call to method sourcecode.readsource and removing one parameter from the call to method tokenizer.tokenize lines and in figure b respectively .
1in this paper we use the terms test test case and unit test case interchangeably to indicate a pair input expected output .because test repair can be an expensive activity automating it even if only partially could save a considerable amount of resources during maintenance.
this is the motivation behind the development of automated test repair techniques such as the ones targeted at unit test cases and those focused on gui or system test cases .
we believe that to develop effective techniques for assisting manual test repair we must first understand how test suites evolve in practice.
that is we must understand when and how tests are created removed and modified.
this is a necessary preliminary step because it can provide evidence that test cases do get repaired support the hypothesis that test repairs can be at least partially automated and suitably direct research efforts.
without such understanding we risk to develop techniques that may not be generally applicable and may not perform the kind of repairs that are actually needed in real world software systems.
to the best of our knowledge to date there are no studies in the literature that investigate how unit test suites evolve.
to address this issue we defined a technique that combines various static and dynamic analysis techniques to compute the differences between the test suites associated with two versions of a program and categorize such changes along two dimensions the static differences between the tests in the two test suites and the behavioral differences between such tests.
we implemented our technique in a tool t estevol which enables a systematic study of test suite evolution.
t estevol targets java programs and junit test suites as java is a widely used language and junit is the de facto standard unit testing framework for java.
given two versions of a program and its test suite t estevol automatically computes differences in the behavior of the test suites on the two program versions classifies the actual repairs performed between the versions and computes the coverage attained by the tests on the two program versions.
we used t estevol to conduct an extensive empirical study on how test suites evolved over a number of years for six real world open source software systems.
in the study we investigated several questions on test evolution such as what types of test suite changes occur in practice and with what frequency?
how often do test repairs require complex modifications of the tests?
why are tests deleted and added?
overall we studied program versions tests and test changes test modifications test deletions and test additions .
this is the first study of this kind and magnitude and our results reveal several interesting aspects of test suite evolution.
in the first part of our study we focus on test repair.
we provide evidence that although test repairs are a relatively small fraction of the activities performed during test evolution they are indeed relevant.
we also show that repair techniques that just focus on oracles i.e.
assertions are likely to be inadequate and not applicable in many cases.
finally our findings can be used to guide future research efforts in the development of new repair techniques that are grounded in the realities of how tests actually evolve.
because our results show that test repair is only part of the picture in the second part of our study we investigate the characteristics of deleted and added test cases.
our results provide evidence that many test cases are not really deleted and added but rather moved or renamed.
we also show that test cases are rarely removed because they are difficult to fix but rather because they have become obsolete.
finally we discover that test cases are not only added to check bug fixes and test new functionality as expected but also to validate changes in the code.
this result supports the argument that the development of techniques for test suite augmentation is useful and can have practical impact.the main contributions of this paper are the identification of the problem of understanding how test suites evolve as a prerequisite for performing research in the area of test repair and test evolution in general .
a technique for studying test suite evolution and a publicly available tool t estevol 2that implements the technique for java programs and junit test cases.
an extensive analysis performed using t estevol of the evolution of six real world systems and their unit test suites over a number of years versions and test changes.
results and findings that allow for assessing existing techniques for supporting test evolution and can guide future research in the broader area of test maintenance.
the rest of the paper is organized as follows.
the next section presents definitions and terminology.
section describes our approach for studying test suite evolution.
section presents the testevol tool.
section presents our empirical study and results.
section discusses related work.
finally section summarizes the paper and lists possible directions for future research.
.
our approach for studying testsuite evolution .
definitions and terminology asystem s p t consists of a program pand a test suite t. atest suite t ft1 t2 t ngconsists of a set of unit test cases.
test p t is a function that executes test case ton program pand returns the outcome of the test execution.
a test outcome can be of one of four types pass the execution of pagainst tsucceeds.
failce the execution of pagainst tfails because a class or method accessed in tdoes not exist in p. failre the execution of pagainst tfails due to an uncaught runtime exception e.g.
a null pointer exception .
failae the execution of pagainst tfails due to an assertion violation.
we use the generic term fail to refer to failures for which the distinction among the above three types of failures is unnecessary.
cov p t is a function that instruments program p executes test case tonp and returns the set of all statements in pcovered byt.cov p t returns the cumulative coverage achieved on p by all the tests in test suite t. given a system s p t a modified version of s s0 p0 t0 and a test case tint t0 there are three possible scenarios to consider texists in tandt0 texists in tbut nott0 i.e.
twas removed from the test suite and texists in t0but not in t i.e.
twas added to the test suite .
these scenarios can be further classified based on the behavior of tinsands0 as summarized in figure and discussed in the rest of this section.
2testevol can be downloaded at edu orso software testevol.html .
3these failures can obviously be detected at compile time.
for the sake of consistency in the discussion however we consider such cases to be detected at runtime via class not found or no such method exceptions.
in fact our t estevol tool detects such failures at runtime by executing the tests compiled using the previous version of ponp.
a test texists in sands0and is modified test p0 t fail tis repaired test p0 t0 pass test p0 t pass test p0 t0 passtis refactored updated to test a different scenario or is made more less discriminating b test tis removed in s0 test p0 t failrejfailaetis too difficult to fix test p0 t failcetis obsolete or is too difficult to fix test p0 t passtis redundant c test t0is added in s0 test p t0 failrejfailaet0is added to validate a bug fix test p t0 failcet0is added to test a new functionality or a code refactoring test p t0 passt0is added to test an existing feature or for coverage based augmentation figure scenarios considered in our investigation.
given two system versions s p t ands0 p0 t0 the three scenarios are a texists in tandt0and is modified b texists intbut not in t0 c t0exists in t0but not in t. .
test modifications figure a illustrates the scenario in which tis present in the test suites for both the old and the new versions of the system.
to study different cases we consider whether tis modified to t0 and if so whether the behaviors of tandt0differ.
we have not considered the cases in which tis not modified because they are irrelevant for studying test evolution.
for behavioral differences there are two cases shown in the two rows of the table either tfails on p0and t0passes on p0or both tandt0pass on p0.
.
.
category testrep repaired tests the t estrepcategory corresponds to the case where tis repaired so that after the modifications it passes on p0 as discussed in the introduction figure shows an example of such a test repair.
the code fragments in listings and present another example of repair that involves a simpler code modification than the one in figure .
the example is taken from gson one of the programs used in our empirical study and involves a test case from gson version .
that was fixed in the subsequent version .
.
section .
describes the programs used in the empirical study.
test testnullfield had to be fixed because constructor fieldattributes class ?
declclazz field f from version .
was modified in version .
to take only one parameter of typefield .
listing unit test for class fieldattributes gson v2.
public void t e s t n u l l f i e l d throws exception try new f i e l d a t t r i b u t e s foo .
class null f a i l f i e l d parameter can not be n u l l catch nullpointerexception expected listing unit test for class fieldattributes gson v2.
public void t e s t n u l l f i e l d throws exception try new f i e l d a t t r i b u t e s null f a i l f i e l d parameter can not be n u l l catch nullpointerexception expected for this category we wish to study the types of modifications that are made to t. a test repair may involve changing the sequence of method calls assertions data values or control flow.
based on our experience for method call sequence changes we consider five types of modifications .method call added a new method call is added.
.method call deleted an existing method call is removed.
.method parameter added a method call is modified such that new parameters are added.
.method parameter deleted a method call is modified such that existing parameters are deleted.
.method parameter modified a method call is modified via changes in the values of its actual parameters.
a test repair may involve multiple such changes.
for example the repair shown in figure involves the addition of a method call in line and the deletion of a method parameter in line .
the repair illustrated in listings and also involves the deletion of a method parameter.
for assertion changes we consider cases in which an assertion is added an assertion is deleted the expected value of an assertion is modified or the assertion is modified but the expected value is unchanged.
finally we currently group together data value changes andcontrol flow changes .
the rationale underlying our classification scheme is that different classes of changes may require different types of repair analyses.
although this is not necessarily the case at least the search strategy for candidate repairs would differ for the different categories of changes.
consider the case of method parameter deletion for instance for which one could attempt a repair by simply deleting some of the actual parameters.
whether this repair would work depends on the situation.
for the code in figure for example it would not work because the deletion of one of the parameters in the call to tokenize is insufficient by itself to fix the test a new method call to readsource has to be added as well for the test to work correctly.
similarly for the case of method parameter addition one could conceivably attempt straightforward fixes by constructing equivalence classes for the new parameters and selecting a value from each equivalence class e.g.
positive negative and zero values for an integer parameter .
in our study we found cases where such an approach would in fact repair broken tests.
in this case too however such a solution is in general not enough.
.
.
category testmodnotrep refactored tests the t estmodnotrepcategory captures scenarios in which a testtis modified in s0even though tpasses on p0.
listings and show an example of one such change from commons math .
unit testtestdistance was refactored to invoke method sqrt in class fastmath a newly added class in the new release instead of the same method in java.lang.math .
there are different reasons why changes in this category might occur as we discuss in section .
.
listing unit test from class vector3dtest commons math v2.
public void testdistance vector3d v1 new vector3d vector3d v2 new vector3d assertequals .
vector3d .
distance vector3d .
minus i vector3d .
minus i assertequals math .
s q r t vector3d .
distance v1 v2 .0e assertequals v1 .
s u b t r a c t v2 .
getnorm vector3d .
distance v1 v2 .0e listing unit test from class vector3dtest commons math v2.
public void testdistance vector3d v1 new vector3d vector3d v2 new vector3d assertequals .
vector3d .
distance vector3d .
minus i vector3d .
minus i assertequals fastmath .
s q r t vector3d .
distance v1 v2 .0e assertequals v1 .
s u b t r a c t v2 .
getnorm vector3d .
distance v1 v2 .0e .
test deletions figure b illustrates the scenario in which a test tis deleted.
to study the reasons for this we examine the behavior of ton the new program version p0and consider three types of behaviors.
.
.
category testdel aejre hard to fix tests this category includes tests that fail on p0with a runtime exception or an assertion violation.
these may be instances where the tests should have been fixed as the functionality that they test in p still exists in p0 but the tests were discarded instead.
one plausible hypothesis is that tests in this category involve repairs of undue complexity for which the investigation of new repair techniques to aid the developer might be particularly useful.
we performed a preliminary investigation of this hypothesis by manually examining ten randomly selected tests in this category and found that all the examined tests were in fact obsolete we further discuss this point in section .
.
.
.
category testdel ce obsolete tests a test that fails with a compilation error on the new program version is obsolete because of api changes.
listing illustrates a test in this category taken from jodatime .
this test was deleted because the tested method chronology.getbuddhist was removed in the subsequent version of jodatime .
listing unit test from class testchronology jodatime v2.
public void testgetbuddhist assertequals buddhistchronology .
getinstance chronology .
getbuddhist although for this category of deletion too one could postulate that the tests were removed because they were too difficult to fix we believe this not to be the case in most practical occurrences.
instead the more likely explanation is that the tests were removed simply because the tested methods were no longer present.
also in this case we investigated our hypothesis by manually examining ten randomly selected cases.
indeed our manual investigation confirmed that for the cases we analyzed the tested functionality was either removed or provided through alternative methods which requires the development of new tests rather than fixes to the existing ones.
.
.
category testdel p redundant tests this category includes tests that are removed even though they pass on p0.
listing illustrates an example of one such test taken from commons lang .
in the new version this test was replaced by a more sophisticated one shown in listing .
listing unit test from class matrixindexexceptiontest commons lang v2.
public void testconstructormessage s t r i n g msg message matrixindexexception ex new matrixindexexception msg assertequals msg ex .
getmessage listing unit test implemented to replace the removed one commons lang v2.
public void testparameter matrixindexexception ex new matrixindexexception index out of range assertequals ex .
getarguments assertequals ex .
getarguments assertequals ex .
getarguments .
test additions figure c illustrates the cases of test suite augmentation where a new test t0is added to the test suite.
the behavior of t0on the old program can indicate the reason why it may have been added.
.
.
category testadd aejre bug fix tests this category includes added tests that fail on pwith a runtime exception or an assertion violation.
in this case the functionality that the added test t0was designed to test exists in pbut is not working as expected most likely because of a fault .
the program modifications between pandp0would ostensibly have been made to fix the fault which causes t0to pass on p0.
thus t0is added to the test suite to validate the bug fix.
listing illustrates a test that fits this profile as it was added to commons math version .
to validate a bug fix browse math .
in the previous version of arrayrealvector the creation of a zero length vector results in a runtime exception.
listing unit test from class arrayrealvectortest commons math v2.
public void testzerovectors assertequals new arrayrealvector new double .
getdimension assertequals new arrayrealvector new double true .
getdimension assertequals new arrayrealvector new double false .
getdimension .
.
category testadd ce new features tests the added tests in this category fail on pwith a compilation error which indicates that the api accessed by the tests does not exist inp.
thus the added test t0is created to test new code in p0 where the code could have been added as part of a refactoring or more likely to add new functionality.
listing illustrates a test from jfreechart that covers a new functionality method getminy did not exist in previous versions of class timeseries .
listing unit test from class timeseriestests jfreechart v1.
.
public void testgetminy timeseries s1 new timeseries s1 asserttrue double .
isnan s1 .
getminy s1 .
add new year .
assertequals .
s1 .
getminy epsilon s1 .
add new year .
assertequals .
s1 .
getminy epsilon s1 .
add new year .
assertequals .
s1 .
getminy epsilon .
.
category testadd p coverage augmentation tests this category considers cases where the added test t0passes on p. clearly t0would have been a valid test in the old system as well.
one would expect that the addition of t0increases program coverage i.e.
cov p0 t0 cov p0 t0 ft0g .
moreover ift0covers different statements in pandp0 assuming that there is a way of matching statements between pandp0 the plausible explanation is that t0was added to test the changes made between pandp0.
however if t0covers the same statements in both program versions it would have been added purely to increase code coverage and not to test any added or modified code .
listing illustrates an added test case that increases code coverage the new test method testfindrangebounds covers the case where the parameter of method findrangebounds is null.
listing unit test from class xyerrorrenderertests jfreechart v1.
.
public void testfindrangebounds xyerrorrenderer r new xyerrorrenderer a s s e r t n u l l r .
findrangebounds null figure high level architecture of t estevol.
.
implementation we implemented a tool called t estevol which facilitates a systematic study of test suite evolution for java programs and junit test suites.
t estevol analyzes a sequence of versions of a software system where each version consists of application code and test code.
the versions can be actual releases or internal builds.
testevol consists of five components as illustrated in the architecture diagram shown in figure .
the compiler component builds each system version and creates two jar files one containing the application classes and the other containing the test classes.
the test execution engine analyzes each pair of system versions s s0 where s p t and s0 p0 t0 .
first it executes ton program pandt0on programp0 i.e.
it runs the tests on the respective program versions .
then it executes t0onpandtonp0.
for each execution it records the test outcome pass failce failae orfailre.
the differencing component compares tandt0to identify modified deleted and added tests.
this component is implemented using the wala analysis infrastructure for java wala.sourceforge.net .
the test outcomes collected by the test execution engine and the test suite changes computed by the differencing component are then passed to the test classifier which analyzes the information about test outcomes and test updates to classify each update into one of the eight test evolution categories presented in section .
for each pair of broken test case and its repaired version the test classifier also compares the test cases to identify the types of repair changes different types of method sequence changes and assertion changes as discussed in section .
.
this analysis is also implemented using wala .
testevol performs a further step for the test cases in categories testdel p and t estadd p .
for these tests the test classifier leverages the coverage analyzer to compute the branch coverage achieved by each test this facilitates the investigation of whether the deleted or added tests cause any variations in the coverage attained by the new test suite.
.
empirical study using t estevol we conducted an empirical study on several java open source software systems to investigate different aspects of test suite evolution.
in this section we discuss the results of our analysis of this information and the insight on test suite evolution that we gained.
specifically we discuss our experimental setup analyze the overall occurrence in the programs and versions considered of the categories of test changes presented in section and investigate the different categories i.e.
tests modified deleted and added in detail.
.
programs tests and versions investigated table lists the programs that we used in our study which are all real world open source programs that we selected from popular websites such as sourceforge .
the general criteria that we used in selecting the programs were that they had to be popular be actively maintained and have a junit test suite.
for each of the programs selected we downloaded all of its official releases with the exception of pmd as explained below .
columns and of the table show the number of versions of each program and the ids of these versions respectively.
columns show various metrics for the first version of the program number of classes number of methods number of junit tests and release date.
columns show the same data for the last version considered.
finally the last column in the table shows the cumulative number of tests for each program over all of its versions.
note that one of the programs pmd appears twice in the table the second time indicated as pmd .
for this program we studied two sets of versions the official releases from november to november and versions from january to december .
the versions are between one and six weeks apart and were selected using kenyon so as to contain at least one test change.
because of the way we selected them the sets of versions forpmd andpmd may be completely disjoint.
we used this second set of versions to assess whether our findings depended on the granularity of the changes considered as further discussed in section .
.
we ran t estevol on each pair of subsequent versions of each program.
t estevol analyzed each pair and produced the information described in section .
this information together with the actual programs and test versions were the basis for our study.
.
occurrence of test change categories in the empirical evaluation we first studied how often tests change during program evolution and how they change.
to do this we used the information computed by t estevol to answer the following research question rq1.
how often do the different categories of test suite changes that we consider occur?
to answer rq1 we used the data about the distribution of testchange categories in the programs which are reported in tables and .
table shows the distribution in absolute numbers which gives a sense of the relevance of each category throughout the lifetime of a program.
for example the table shows that for commons math our study analyzed more than test changes overall.
although looking at absolute numbers is useful to get an idea of the magnitude of the number of test changes it makes it difficult to compare the results across programs as different programs could have different versions and lifetime spans.
we thus normalized the distribution of test changes with respect to the number of studied versions by considering the test differences between each pair of versions of the programs.
we report these numbers in table .
the columns in both tables contain analogous data.
column reports the number of test updates modifications deletions or additions whereas columns report the distribution of the eight categories of test changes.
let us first consider the total number of test suite updates.
over all programs we observed test changes of which were test modifications were test deletions and were test additions.
the average number of test updates per version pair ranged from for pmd to for commons math .
consider now the data for test modifications columns of tables and .
among the modifications only table programs used in our empirical study.
number of first version last version svn v1testsprogram description versions versions classes methods tests release classes methods tests release commons lang extra utilities for the java.lang api111.
.
.
.
.
.
.
.
.
.
.
.
dec nov commons math java mathematics and statistics library71.
.
.
.
.
.
.
dec mar gson library for converting java objects into their json representation and viceversa161.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
may dec jfreechart chart creation library .
.
.
.
.
.
.
.
.
.
.
.08a .
.
.
.
.
.
.
.
.
.
.
.
nov nov jodatime replacement for the java date and time api150.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
nov feb pmd java source code static analyzer271.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
nov nov pmd 17r2515 r2559 r2567 r2628 r2679 r2723 r2750 r2769 r2781 r2832 r2846 r2911 r2936 r2962 r2975 r3016 r3065288 jan dec total table distribution of the test change categories in the programs we analyzed absolute numbers.
number of test modification test deletion test addition program test updates testrep testmodnotrep testdel aejre testdel ce testdel p testadd aejre testadd ce testadd p commons lang .
.
.
.
.
.
.
.
commons math .
.
.
.
.
.
.
.
gson .
.
.
.
.
.
.
.
jfreechart .
.
.
.
.
.
.
jodatime .
.
.
.
.
.
.
pmd .
.
.
.
.
.
.
.
total .
.
.
.
.
.
.
.
of the modifications and of all test changes were performed to fix broken tests i.e.
they were actual test repairs .
the remaning modifications of the modifications and of all test changes were changes made to passing tests that is changes made for reasons other than fixing broken tests.
this data supports the observation that test repairs are only one type of change in test suite evolution and definitely not the most frequent.
it may therefore be worth investigating techniques that support other kinds of test modifications and test changes in general.
test repairs are only part of the story test repair and test modifications in general are a minority of all test changes.
looking at the data in more detail however we can also observe that test repairs although not prevalent are not irrelevant either.
if we ignore the case of added and deleted tests which we will discuss in detail in section .
test repairs represent a significant fraction of test modifications the percentage of actual test repair instances over all test modifications varies in fact from for jfreechart to for gson .
this data seems to provide a fair amount of motivation for the development of automated unit testrepair techniques .
test repairs occur often enough in practice to justify the development of automated repair techniques.
based on the above observations we further analyzed the data to get a better understanding of the kinds of both repair and non repair modifications that occur in practice.
we discuss our findings in the next section.
.
test modifications the goal of our in depth analysis of test modification was twofold.
our main goal was to study the types of repairs that are performed in practice so as to gauge the applicability of existing repair techniques.
another secondary goal was to understand what kinds of non repair modifications occur in real programs so as to assess the feasibility of developing automated techniques that can support these types of modifications too.
the starting point for the first part of our analysis were existing test repair techniques.
to the best of our knowledge most existing techniques in this arena at the time of this study e.g.
focus on repairing the assertions associated with the failing test cases.
the main exception is the technique recently presented by mirzaaghaei pastore and pezz which aims to handle changes in method signatures and which we discuss in section .
daniel and colleagues in particular present seven repair strategies for automatically fixing broken junit test cases.
six of these focus on modifying the failing assert statement in different ways such as by replacing the expected value with the actual value inverting a relational operator or expanding object comparisons.
the final strategy applies in cases where a test fails with a runtime exception and works by surrounding the failing method call with a try catch block that catches a particular exception type.
to study the applicability of assertion focused repair we investigated the following research question rq2.
how often do test repairs involve modifications to existing assertions only?
how often they require more complex modifications of the tests instead?table distribution of the test change categories in the programs we analyzed averaged over version pairs.
number of test modification test deletion test addition program test updates testrep testmodnotrep testdel aejre testdel ce testdel p testadd aejre testadd ce testadd p commons lang .
.
.
.
.
.
.
.
commons math .
.
.
.
.
.
.
.
gson .
.
.
.
.
.
.
.
jfreechart .
.
.
.
.
.
.
jodatime .
.
.
.
.
.
.
pmd .
.
.
.
.
.
.
.
pmd .
.
.
.
.
.
.
average .
.
.
.
.
.
.
.
table types of modifications made to repair broken tests.
assertion method call changes assertion changes number of changes parameter parameter parameter assertion assertion assertion exp value program test repairs only call added call deleted added deleted modified added deleted modified modified commons lang .
commons math .
gson .
jfreechart .
jodatime .
pmd .
pmd .
total .
.
.
.
.
.
.
.
.
.
table presents the analysis data relevant for the investigation of rq2.
column of the table shows the overall number of test repairs for each program this is the same information that is shown in column of table .
column reports the number of test repairs that involved changes to assertions only.
columns list the number of occurrences of the five types of changes in method calls that we discussed in section .
.
similarly columns list the number of occurrences of the four types of changes in assert statements.
the data in the first column of the table clearly shows that for the programs considered test repairs that involve only changes to assertions are rare less than overall for all programs except commons math .
looking at the remaining columns we can also clearly observe that test repairs tend to involve changes to method calls considerably more often than changes to assertions.
over all programs considered nearly of the repairs involved the addition of a method call the deletion of a method call or the deletion of a method parameter.
assertion additions and assertion deletions which occurred in and of the repairs respectively are also fairly common but much less frequent than methodcall changes.
also in this case the only exception to this trend iscommons math for which assertion changes in total occurred nearly as often as method call changes in total .
overall this data provides strong indications that existing testrepair techniques although useful in some cases must be extended to be widely applicable e.g.
by including the capability of generating and adapting method calls .
based on the data we can therefore make the following observation test repair techniques that focus exclusively on assertions can target only a small subset of all broken repairable test cases and must be extended to achieve a wider applicability of automated test repair.
as stated earlier in this part of the study we were also interested in gaining a better understanding of non repair modifications.
to this end we analyzed the occurrences of such changes in our programs and discovered that there are different reasons why changes in this category are performed.
in some of the cases that we ob served test cases were simply refactored.
this was the case for instance for the example shown in section .
.
listings and .
in other cases a test was updated to cover a different program functionality which indicates that the old functionality either was removed or did not need to be tested by this particular test case.
to distinguish between these two cases we used coverage information because intuitively we would expect a test refactoring not to alter the coverage of the program achieved by the original and the modified tests.
therefore if cov p0 t cov p0 t0 we consider the change as a test refactoring otherwise we conclude that the test has been changed to cover a different functionality.
although these two cases were prominent we also observed other cases in which tests were made more discriminating by the addition of assertions or less discriminating by the deletion of assertions.
for these three types of test changes it seems difficult to define techniques that could automatically perform the changes.
all three types of changes we observed in fact seem to involve a considerable amount of human judgement.
moreover these changes are typically not strictly necessary at least when compared to fixing a broken test.
providing support for them seems therefore to be less of a priority.
finally standard refactoring and advanced editing tools such as the ones provided in modern ides may already provide enough support for performing these changes.
investigating techniques for automated support of non repair changes does not appear to be a promising research avenue.
these techniques would require considerable manual guidance and may end up being similar to traditional refactoring tools.
.
test deletions although studying test modifications and in particular test repairs was the main focus of our study we also investigated the characteristics of deleted and added test cases.
in this section we describe our findings with respect to test deletions for which we investigated the following research question rq3.
why are tests deleted?
are tests in different categories deleted for different reasons?to answer rq3 we use the data provided in columns of tables and .
as the data shows test deletions occur often.
moreover failing tests are more often deleted instances of deletions than repaired instances of repairs .
among the deleted failing tests tests fail predominantly over with compilation errors whereas the remaining ones fail with assertion or runtime errors.
this phenomenon indicates that in most cases the deleted tests cover obsolete functionality as the application api exercised by the tests no longer exists.
the interesting question however is whether this is true also for tests in the testdel aejre category.
given that these tests could be run but failed on the modified system an interesting additional question arises.
were these tests truly obsolete and served no purpose in the new version of the system?
or could they have been fixed instead but fixing was too complex?
to address this question we randomly examined instances from category testdel aejre and found that in all the observed cases the tests were indeed obsolete and it did not make sense to repair them.
to illustrate listing shows a test from commons lang that belongs to this category.
after examining the history of deleted testtestbigdecimal we discovered that it was added in version .
to test a new functionality in class equalsbuilder and that the functionality was later removed due to a side effect see .
in this case and in the other cases that we examined the test was indeed obsolete and was correctly removed.
listing unit test from class equalsbuildertest commons lang v2.
public void testbigdecimal bigdecimal o1 new bigdecimal .
bigdecimal o2 new bigdecimal .
asserttrue new equalsbuilder .
append o1 o1 .
isequals asserttrue new equalsbuilder .
append o1 o2 .
isequals as another example listing shows a code fragment from jfreechart version .
.
.
in this version any year before is considered invalid and method year.previous returns null for such a year.
this is no longer true for the following version of jfreechart therefore in this case too the test was correctly removed.
listing unit test from class yeartests jfreechart v1.
.
public void test1900previous year c u r r e n t new year year previous year c u r r e n t .
previous a s s e r t n u l l previous tests that fail in the new version of a program because of a compilation error a runtime exception or a failed assertion tend to be deleted not because they are difficult to repair but because they are obsolete.
another interesting fact highlighted by the data is that many tests that pass and are valid tests in both the old and new systems are deleted.
in fact category testdel p with members accounts for over of all test updates and over of all test deletions.
in general if tpasses on p0 it is not obvious why it would be removed.
one possibility is that many of these deletions are actually cases where tests were renamed or moved to a different class.
to investigate this hypothesis we studied the effects of the deletions of passing tests on code coverage.
specifically we investigatedtable effects of deleted passing tests on branch coverage.
same reduced program testdel p branch coverage branch coverage commons lang .
.
commons math .
.
gson .
.
jfreechart .
.
jodatime .
.
pmd .
.
total .
.
whether the removal of each deleted test tresulted in any loss in coverage.
cov p0 t0 cov p0 t0 ftg indicates the loss in coverage if any that results from the deletion of t. the effects on coverage can in fact strongly indicate whether a test is truly deleted if the removal of the test from the test suite reduces the coverage of the suite or simply renamed or moved if the removal of the test causes no change in coverage .
table presents data on the effects of the deletion of passing tests on branch coverage.
as the table shows in most cases over the removal of a test does not reduce branch coverage a consistent result across all programs except commons math .
this data suggests that many of the deletions that occur in category testdel p may not be actual deletions.
it is worth noting that the tests might have been deleted and might have simply been redundant coverage wise.
the manual examination of ten randomly selected samples however seemed to eliminate this possibility.
as an example listing illustrates a test that was moved and renamed in gson .
initially in version .
.
the test was part of class jsonserializertest .
in version .
however it was moved to class arraytest and renamed to testarrayofstringsserialization .
listing unit test from class jsonserializertest gson v1.
.
public void t e s t a r r a y o f s t r i n g s s t r i n g t a r g e t hello world assertequals gson .
tojson t a r g e t the data in table also shows that test deletions that cause reduction in branch coverage are a minority but do occur of the cases .
although we could not find any definite explanation of the phenomenon we conjecture that this may simply be due to differences in the structure of the code that cause the moved or renamed test to cover different instructions in some parts of the program.
to assess the validity of this conjecture we plan to investigate this category further in the future.
tests that would pass in the new version of a program but appear to have been deleted have in most cases simply been moved or renamed.
.
test additions in this final part of our investigation we study the characteristics of added tests.
similar to what we did for deleted tests in this case we investigate the following research question rq4.
why are tests added?
are tests in different categories added for different reasons?
among the three types of test updates we studied test additions are the ones that occur most frequently.
this is somehow expected if we consider that all the programs we studied grew significantly over the time period of the study as demonstrated by the number of classes and methods in the first and last versions of the systems see columns and of table .
of the instancestable effects of the added passing tests on branch coverage.
same increased program testadd p branch coverage branch coverage commons lang .
.
commons math .
.
gson .
.
jfreechart .
.
jodatime .
.
pmd .
.
total .
.
of test additions failed in the previous version of the program with a runtime exception or an assertion failure.
as we discussed and illustrated with an example in section .
.
these are tests that were added to validate a bug fix.
however the majority of the added tests simply did not compile for the old program.
this fact indicates that they were most likely added to validate newly added code.
new test cases that would fail with a runtime error for the old version of the program are added to validate bug fixes new tests that would not compile against the old version of the program are likely added to validate new functionality.
the remaining added tests represent an interesting case as they would both compile and pass on the previous version of the program as shown by the column for testadd p in table .
similar to what we did for the deleted tests we used coverage information to investigate these added tests further.
table presents the branch coverage data that we collected and used to perform this investigation.
as the table shows and somehow unsurprisingly a large portion of the added tests did not increase branch coverage.
this is particularly significant for commons math jodatime and pmd.
considering our findings for the case of deleted tests see section .
some of these added tests must correspond to old tests that were moved or simply renamed.
as the table also shows the added tests did increase coverage in many cases.
the increase in percentage ranges from about forjodatime to over for commons lang with the average being .
to better understand this phenomenon we collected additional coverage data and studied whether these tests traversed modified parts of the code.
interestingly in most cases over the tests exercised one or more changes in the code.
albeit this is just preliminary evidence and would require a more in depth analysis it seems to indicate that developers do perform test augmentation not only to cover newly added code but also to exercise the changes they made to existing code.
if confirmed by further studies this result would justify existing research in test suite augmentation e.g.
and could provide input for further efforts in that area.
a significant number of new tests are added not necessarily to cover new code but rather to exercise the changed parts of the code after the program is modified.
.
summary and threats to validity our empirical analysis of real world programs versions tests and test changes allowed us to make a number of interesting observations.
in this section we summarize the most relevant of such observations.
as far as test repair is concerned our results show that test repairs are needed although they are not the majority of the test changes that we observed and techniques that just focus on assertions may not be applicable in a majority of cases.as for test additions and deletions we found initial evidence that in many cases tests are not really deleted and added but rather moved or simply renamed failing tests are deleted not because they are difficult to fix but because they are obsolete and tests are added to check bug fixes test new functionality and validate changes made to the code.
like any empirical study there are threats to the validity of our results.
threats to internal validity may be caused by errors in our implementation of t estevol that might have produced incorrect results.
to mitigate this threat we have thoroughly tested testevol with a number of small examples that contained the test changes targeted by our analysis.
moreover in our analysis of the results we have manually inspected many real cases of test changes computed by t estevol and found them to be correct which further increases our confidence in the tool.
there are also threats to the external validity of our results related to the fact that our findings may not generalize to other programs or test suites.
one of the potential external threats in particular is the granularity at which we selected the program versions that of external official releases.
our results could change if evolution were to be studied at a finer grain by analyzing internal builds with greater frequency.
to perform a preliminary investigation of this issue we obtained internal builds of pmd as mentioned in section .
and analyzed them using t estevol.
the data for these builds shown in tables and under the label pmd illustrates that a finer grained study of test evolution can indeed produce some variations in the results.
for example we can observe a larger percentage of test repairs which is probably due to the fact that tests can be repaired multiple times in different internal builds between two releases.
in such cases a coarser release level analysis cannot identify the multiple repairs whereas a finer build level analysis can do so.
these preliminary results show that there may be value in performing further studies at different levels of version granularity.
overall we considered applications from different domains many versions for each application large test suites and numerous realworld test changes.
although more studies are needed to confirm our results we believe that our initial findings provide a solid starting point on which we and other researchers can build.
.
related work daniel and colleagues presented the first automated technique and tool called reassert for repairing broken junit tests.
reassert employs different repair strategies whose common goal is to fix the failing assert statements to make the tests pass.
for example one repair strategy replaces the expected value of an assertion with the actual value observed and another strategy inverts the relational operator in the assert condition.
in subsequent work daniel and colleagues presented a symbolic analysisbased repair technique that overcomes some of the limitations of reassert e.g.
its ineffectiveness in the presence of conditional control flow .
however this technique also focuses exclusively on repairing assertions.
as our results indicate assertion focused repair may have limited applicability in practice test repairs predominantly involve changes such as synthesis of new sequences of method calls that leave the assertions unmodified.
more recently mirzaaghaei pastore and pezz presented a repair technique for fixing junit test cases that are broken because of changes in method signatures that is addition deletion or modification of parameters.
their technique identifies the broken method call and attempts to create a modified call in which new parameters are initialized with suitable values.
using data flow analysis program differencing and runtime monitoring the techniquesearches for initialization values from existing data values generated during the execution of the test case against the original application.
although its general effectiveness is unclear the technique may work well for specific types of changes e.g.
where a formal parameter is replaced with a type that wraps the parameter .
this technique by trying to handle changes in method signatures is a right step in the direction of developing more widely applicable automated repairs.
based on our evidence however the effectiveness of the technique is still limited and the development of more sophisticated approaches is required.
in particular the technique attempts to fix a broken method call by adding deleting and modifying parameters but it does not synthesize new method calls.
as the data from our study shows the synthesis of new method calls is often needed when repairing test cases in practice see column of table .
the test repair illustrated in figure for instance cannot be handled by this technique.
test repair techniques have also been developed for gui tests e.g.
such tests are sequences of events on an application user interface.
memon and soffa present a repair technique that constructs models of the original and modified gui components where the models represent the possible flow of events among the components.
their technique compares the models to identify deleted event flows and attempts to repair the tests that traverse such invalid flows.
the repair strategy deletes one or more events from a test sequence or splices one or more events with a different event so that the resulting sequence can be executed on the modified gui.
grechanik xie and fu present a similar approach for repairing broken gui test scripts their approach constructs models of the original and modified guis compares the models to identify modified gui elements and identifies the testscript actions that access such modified gui elements.
choudhary and colleagues present a repair technique for web application test scripts.
their technique collects runtime data for test execution on the original and modified web application.
by analyzing the broken script commands and the runtime data it attempts to repair the commands to make the broken test scripts pass on the modified application.
an empirical study similar to ours but that focuses on the evolution of gui test scripts would be useful in assessing the practical applicability of these techniques and in identifying interesting research problems in gui test repair.
recently several techniques for automated program repair have been developed e.g.
.
some of these techniques rely on formal specifications of correct program behavior whereas others use a suite of passing tests as the specification of intended program behavior .
it would be worth investigating whether these techniques and especially the ones based on symbolic analysis e.g.
could be adapted to be used in the context of automated support for test repair.
our study revealed that in many cases changes in the application cause tests to fail with compilation errors.
such failures are similar in nature to the failures that result in client code when library apis evolve in ways that are incompatible with the api clients.
therefore ideas from existing research on automated api migration and adaptation e.g.
could also be fruitfully leveraged for developing repair techniques for tests.
dagenais and robillard for instance presented an approach for assisting with migrating client code to updated versions of framework apis.
their key idea is to guide the client changes based on the framework code s own adaption to its api changes.
nguyen and colleagues described a related approach where the adaptation of a client is guided by how other clients have adapted to the evolved api.
similar techniques could be developed for fixing unit tests based on the application s adaptation to api level changes.the notion of generating abstract edit scripts e.g.
that apply a given program transformation in similar but not identical contexts may also be useful for developing test repair techniques.
finally techniques for constraint based synthesis of method sequences e.g.
could be leveraged for developing repair techniques that generate method sequences to satisfy the failing assertions in broken test cases.
.
conclusion and future work in this paper we presented an extensive empirical study of how test suites evolve.
our study which focuses on junit test suites is thefirst of its kind in terms of its goals and magnitude.
although software evolution has been extensively studied test suites which can be large software systems in their own right and can also require continuous development and maintenance have not been the objective of large empirical studies to date.
such studies however are essential if we are to develop automated techniques and tools for supporting test suite evolution that are grounded in real world scenarios.
our goal for this work was therefore to study test suite evolution in a systematic and comprehensive manner to understand how and why tests evolve i.e.
how and why tests are modified added and deleted .
toward that goal we studied the evolution history over a substantial period of time of several open source software systems that have large junit test suites.
our study has provided several relevant insights.
first we found that test repair does occur in practice on average we observed instances of test repairs per program version and a total of test repairs.
therefore automating test repair can clearly be useful.
second test repair is not the only or even the predominant reason why tests are modified for the programs studied non repair test modifications occurred nearly four times as frequently as test repairs.
third test repairs that focus mainly on fixing assertions i.e.
oracles as most existing techniques do may have limited applicability less than of the test repairs in our study involved fixes to assertions only.
fourth test repairs frequently involve changes to method calls and method call sequences.
thus the investigation of automated repair techniques targeted toward synthesizing new method sequences for making existing assertions pass is likely to be a worthwhile research direction to pursue.
fifth we observed that test deletions and additions are often refactorings.
finally a considerable portion of real additions occur for the purpose of augmenting a test suite to make it more adequate which lends support for research on the topic of test suite augmentation.
we are considering two main future research directions for this work.
one direction involves extending our empirical study by considering additional programs and especially additional static and dynamic analysis techniques that can help refine our results.
in particular we plan to use code clone detection techniques to better distinguish tests that are truly deleted or added from tests that are renamed or moved to different classes.
the second direction involves research on actual test repair driven by our empirical results.
a particularly interesting research problem in this context is the development of repair techniques that are intent preserving that is techniques that ensure that the repaired tests preserve the intent behind the original tests.
often tests can be repaired in different ways and selecting the repair that most closely mirrors the original test intent is obviously desirable.
the intriguing question is to what extent test intent can be characterized formally and accurately.