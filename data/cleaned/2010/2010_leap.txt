leap lightweight deterministic multi processor replay of concurrent java programs jeff huang peng liu and charles zhang department of computer science and engineering the hong kong university of science and t echnology smhuang lpxz charlesz cse.ust.hk abstract the technique of deterministic record and replay aims at faithfully reenacting an earlier program execution.
for con current programs it is one of the most important techniquesfor program understanding and debugging.
the state ofthe art deterministic replay techniques face challenging efficiency problems in supporting multi processor executions due to the unoptimized treatment of shared memory accesses.
we propose leap a deterministic record and replay technique that uses a new type of local order w.r.t.
the shared memory locations and concurrent threads.
com pared to the related work our technique records much less informationwithoutlosingthereplaydeterminism.
thecor rectness of our technique is underpinned by formal modelsand a replay theorem that we have developed in this work.
through our evaluation using both benchmarks and real world applications we show that leapis more than 10x faster than conventional global order based approaches and in most cases 2x to 10x faster than other local order based approaches.
our recording overhead on the two large open source multi threaded applications tomcatandderbyis less than .
moreover as the evidence of the deterministicreplay leapis able to deterministically reproduce out of 8r e a lb u g si n tomcatand derby out of benchmark bugs in ibm contest benchmark suite and of therandomly injected concurrency bugs.
categories and subject descriptors d. .
testing and debugging debugging aids tracing diagnostics general terms algorithms performance reliability .
introduction as concurrency becomes the major programming model of the performance improvement of software in the multicore era it is also the culprit of many so called heisenbugs such as data races deadlocks and atomicity violations that permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies arenot made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specificpermission and or a fee.
fse november santa fe new mexico usa.
copyright acm ... .
.are easy to hatch but difficult to detect and to fix.
one of the most effective ways for combating these bugs is the technique of record and replay .
the record and replay technique aimsat fully reenacting the problematic execution of concurrent programs thus giving the programmers both the context and the history information to dramatically expedite the process of bug fix.
a crucial design factor of record and replay solutions is the degree of recording fidelity i.e.
the amount of data to be recorded for the sufficient reproduction of problematic program executions.
simply speaking the degree of record ing fidelity is proportional to the degree of faithfulness inreplay unfortunately also to the runtime overhead of us ing the technique.
this characteristic is less problematic for the hardware based record and replay solutions in which special chips share the cost of the record ing computation.
for the software only solutions on uni processors the replay of concurrent programs can be achieved deterministically with low overhead by capturing the thread scheduling decisions.
however for software onlysolutions on multi processors making the best trade off between how much to record and how faithful to replay is stilla very challenging problem drawing intense research attentions .
ourresearchisalsoconcernedwiththesoftware onlyrecord and replay solutions.
our general observation is that the state of the art does not achieve the recording efficiency and the replay determinism 1at the same time.
conventional deterministic multi processor replay techniques usually incur a significantruntimeoverheadof10xto100x mak ing them unattractive for production use or even for testingpurposes.
for instance dejavu is aglobal clock based approach that is capable of deterministically replaying concurrent systems on multi processors by assigning a global order to all critical events including both the synchronizationpoints and the shared memory accesses.
as indicated by the authors the enforcement of the global order on variable accesses across multiple threads incurs a large runtime over head on multi processors.
the research of lightweight recordand replay techniques has successfully low ered the recording overhead however at the cost of sacrificing the determinism in reproducing buggy runs.
jarec and recplay abolish the idea of global ordering and 1we define replay determinism as the faithful reenactment of all program state transitions experienced by a previous execution.
a more complete and formal model is presented in section .
207uselamport clock to maintain partial thread access orders w.r.t.
only the monitor entry and exit events thus making the recording process lightweight.
however without tracking the shared memory accesses their approaches cannot deterministically reproduce problematic runs for thefact that a large majority of shared memory accesses are notsynchronized either due to programming errors or becausethey are harmless .
as also pointed out in to deterministically replay a concurrent system on multi processors it is necessary torecord the thread access orders of the shared memory loca tions a method commonly believed to be too expensive to bepractical .
inthispaper wedemonstrate that it is possible to achieve efficiency in this approach by observing that given the same program input it is sufficientto deterministically replay the program execution by record ing partial thread access information local to the individual shared variables.
based on this observation we have designed and implemented leap a replay tool that addresses both the recording efficiency and the replay determinism.the replay determinism is underpinned by a semantic model and formal theorems.
to achieve efficiency we use a fieldbased approach to statically identify shared variables thus avoiding the cost of runtime identification.
in addition wemake extensive use of static analysis to provide a close ap proximation of the necessary program locations that need to be monitored and thus to prune away a large percentage of otherwise redundant recording operations.
the idea of the local order based recording can be rooted back to instantreplay which enables the deterministic replay by recording the access history of all the shared ob jects w.r.t.
a particular thread.
this technique does not suitour design objectives of being both deterministic and effi cient.
first instantreplay requires the unique identification of shared objects dynamically a task hard to be efficiently and correctly implemented in practice.
second instantreplayuses a complex computation model based on the crew protocol making the recording process very costly.
third there are important soundness issues with the local order based approaches that must be formally proved.
another local order based approach is the use of lamport clock thattracks the partial order of critical events that each thread sees .
our technique tracks the order of thread accesses that each shared variable sees which is operationally simpler than the use of lamport clock.
we evaluate the runtime performance of leapby comparing to the related techniques including the use of globalclock instantreplay andlamportclock.
ourmicro benchmark shows that leapis more than 10x faster than the global clock based approach more than 5x faster than instantreplay and at least 2x faster than the use of lamport clock.
on real world large open source multi threaded applicationssuch as tomcatandderby leapis 5x to 10x faster than the related approaches measured by third party benchmarks.the average runtime overhead of leapis less than on tomcatand derby.
moreover as the evidence of the replay correctness leapis able to deterministically reproduce out of real concurrency bugs in tomcatand derby 3o u t of benchmark bugs in ibm contest benchmark suite and of the randomly injected concurrency bugs.
in summary this paper makes the following contributions .
we present a new local order based deterministic and efficient record and replay technique leap.
we provide afigure example code with races thread t1 x thread t2 x y if x y if y if x error x execution schedule x.vec t1 g3t2 g3t1 212access vectors y.vec g3t2 g3t1 g3t2 formal model of the concurrent program execution and use it to prove the soundness of our technique.
.
we describe the implementation of leapthat uses static analysis and bytecode instrumentation to transparently provide the capability of the deterministic replay forjava programs without any user intervention.
.
we evaluate leapby first quantifying its differences compared to the state of the art recording techniques including global clock instantreplay and lamport clock.
we then conduct thorough experiments to evaluate the correctness of leapby reproducing real and randomly injected bugs using popular and computation intensive concurrentapplications.
the rest of the paper is organized as follows section presents the technical details of leap section presents the semantic model and proofs section describes the implementation of leap section evaluates leap s e c t i o n6 reviews related work and section summarizes this paper.
.
leap local order based deterministic replay leapprovides a general technique for the deterministic replay of concurrent programs on multi processors.
the main idea of leapis that each shared variable tracks the order of thread accesses it sees during execution.
in this section we present the core techniques of leap.
.
leap overview we first use a simple example to show the main technique ofleapand draw its differences as compared to the conventional global order based approach to the deterministic replay.
in figure we show a race condition that triggers anerrorat line following the interleaved execution order .
the global order based approaches record this schedule and use it to re execute the program at the cost of global synchronization operations.
our observation is that thread accesses to different shared vari ables need not to be tracked together.
instead of enforcinga global order we claim that it is sufficient to record thethread access order that each shared variable sees.
in our example instead of the global order vector we use two access vectors x.vecand y.vec for the shared variables x and yand record t1 t2 t1 and t2 t1 t2 respectively.
we require zero global synchronization operations and two groups of local synchronization operations executed in parallel.
during replay we associate xand ywith conditional variables to enforce the access order of threads be identicalto what is recorded in their respective access vectors.
althoughourtechniquecanbeeasilyillustrated toensure 208determinism and efficiency there are many tough challenges that we must tackle .static shared variable localization.
how to effectively locate shared variables statically?
what will happen if we miss some shared variables or some local variables are mis takenly recognized to be shared?
.consistent shared variable and thread identification across runs.how to match the identities of shared variables and of threads between the recording run and the replay run?
forexample the deterministic replay would fail if the sharedvariable xat record is incorrectly recognized as yat replay or the thread t1is mistakenly recognized as t2.
.non unique global order.
keen readers may point out that by only recording the thread access orders each variable sees leapwill permit a global thread schedule that is different from the recording run.
for instance in our exam ple leapalso permits the global order .
will this affect the faithfulness of the replay?
in the rest of the section we focus on discussing the first two issues.
the soundness of our approach associated withthethirdissueisfundamentaltoourtechnique.
insection3 we provide a formal semantic model and proofs to show this phenomenon does not affect the faithfulness of the replay.
.
locating shared variable accesses precisely locating shared variables is generally undecidable .
we therefore compute a complete over approximation using a static escape analysis in the soot2framework calledthreadlocalobjectanalysis .threadlocalobjectanalysis provides on demand answers to whether a variable can be accessed by multiple threads simultaneously or not.
however there are a few important issues with thisanalysis.
first static analysis is inherently conservative aslocal variables might be reported as shared.
we show in section corollary that this type of conservativeness does not affect the correctness of the deterministic replay.second threadlocalobjectanalysis does not distinguish between read and write accesses.
for shared immutable vari ables of which the values never change after initialization we do not need to address them for they cannot cause nondeterminism.
third we discover that static variables are all conservatively reported as escaped in threadlocalobjectanalysis .
since the static variables might also be accessed only by one thread we wish them to be analyzed in thesame way as the instance variables in order to obtain amorepreciseresult.
thus wemaketwoenhancementstothethreadlocalobjectanalysis .
we further refine the analysis results of threadlocalobjectanalysis so that we do not record accesses to shared immutable variables .
we modifythreadlocalobjectanalysis to treat static variables in the same way as instance variables.
.
field based shared variable identification for java programs since the standard jvms do not support the consistent object identification across runs we cannot use the default object hash code.
we use a static field based shared variable identification scheme applied to the following three categories of variables which are collectively referred to as the shared program elements spe .
variables that serve as monitors .
class variables .
threadescaped instance variables.
these spes include both java the instrumentation of spe accesses class account spe name index intbalance1 intbalance2 balance1 account.
balance1 balance2 account balance2 intbalance2 bl ...balance2 account .balance2 getbalance1getbalance1 tmp balance1 getbalance1 thread id getthreadid tmp balance1 return tmp g get lock accessspe thread id bl setbalance2 tmp balance1 release lock return tmp ... balance2 value return tmp monitorsandsharedfieldvariablesthatmaycausenondeterminism.
spes are uniquely named as follows for category1 it is the name of the declaring type of the object variable for category and it is the variable name combined withthe name of the class in which the variable is declared.
after obtaining all the spes in the program leapassignsofflineto each spe a numerical index as its runtime identifier.
for example in figure suppose the two field variable balance1 and balance2 of the account class are identified as shared they are mapped to the numerical ids1and .
the static field based shared variable identification remains consistent across runs and does not incur runtimeoverhead.
moreover compared to the object level identification approaches this approach is more fine grained as different fields of the same object are mapped to differ ent indices.
consequently accesses to different fields of thesame object do not need to be serialized at the runtime.
there are a few issues with our field based shared variable identification.
first our approach does not staticallydistinguish between different instances of the same type.
asa result accesses to the same shared field variable of dif ferent instances of the same type would be serialized and recorded into the same access vector.
for this concern we formally prove in section corollary that the deter ministic replay is also guaranteed if the thread accesses todifferent shared variables are recorded globally into a single access vector.
second we cannot uniquely identify scalar variables that are alias to shared array variables.
to dealwith this issue we perform an alias analysis for all of thescalar array variables in the program and represent all thealiases with the same spe ignoring the indexing operations.
this treatment guarantees that the nondeterminism caused by array aliases can be correctly tracked however at thecost of reducing the degree of concurrency.
fortunately inour experiment we find very few such cases in large java multi threaded applications.
a good object oriented program rarely manipulates shared array data directly so theyare rarely escaped.
.
unique thread identification since the thread identity is the only information recorded into the access vectors we must make sure that a thread at 209the recording phase is correctly recognized during replay.
a naivewayistokeepamappingbetweenthethreadnameandthe thread id during recording and use the same mapping for replay.
however different parent threads can race with each other on creating their child threads.
therefore thethread id assignment is not fixed across runs.
to address this problem leapintroduces additional synchronizationoperationsatthethreadcreationtimetoensure the same thread creation order across the recording run and the replay run.
more specifically leapmaintains a list that records the ids of the parent threads in the global order oftheir thread creation operations.
the list is used to direct the thread creation order at replay.
.
handling early replay termination our local order based approach permits different global schedules for threads that do not affect each other s program states.
one caveat of this approach is that it gives rise to the possibility of early termination a program crash action compared to the original one might be performed earlier in the replay execution thus making the replayedrun not fully identical to the recording run in terms of its behavior.
to faithfully replay all the thread execution actions we ensure that every thread in the replay executionp e r f o r m st h es a m en u m b e ro fs p ea c c e s s e sa si td o e si nt h erecording execution.
consequently we guarantee that thereplay execution does not terminate until all the recorded actions in the original execution are performed thus making the final state of the replayed execution the same as that of the original one.
.
a theorem of local ordering in this section we formally prove the soundness of our local order based approach for the deterministic replay of shared memory races.
we also use two corollaries to showthe soundness the field based shared variable identification approach and the soundness of using an unsound but complete static escape analysis for the deterministic replay.
.
modeling concurrent program execution to provide a basis for our proof we first define the execution semantics of concurrent programs in a style similar to .
we consider a program comprised of a set of concurrently executing threads t t1 t2 ... communicating through a global store .
the global store consists of a set of variables s s1 s2 ... that are shared among threads and we use to denote the value of the shared variable son the global store.
each thread has also its own local store consisting of the local variables and the program counter to the thread.
each thread executes by performing a sequence of actions on the global store and its own local store.
each action is a single indivisible access3on a single variable.
we use to denote the owner thread of action andvar the variable accessed by .i fvar is a shared variable we call aglobal action otherwise it is alocal action .
the program state is defined as where is the global store and is a mapping from thread identifiers tito the local store iof each thread.
3the access could be a read a write a lock acquisition a lock release a message sending or a message receiving .
nevertheless in our setting we do not necessarily need to distinguish between different access types.the program execution is modeled as a sequence of transitions defined over the program state .
let kbe the kthaction in the global order of the program execution and k 1be the program state just before kis performed 0 is the initial state the state transition sequence is 0 1 1 2 2 3 ... given a concurrent system described above we next formally define the execution semantics of action .t og i v ea precise definition we first introduce some additional notations similar to i si d e n t i c a lt o except that it maps the variable sto the value v. i si d e n t i c a lt o e x c e p tt h a ti tm a p st h e thread identifier tito i. let the relation primemodels the effect of performing an action on the global store a n d primemodels the effect of performing on the local store .
the execution semantics of performing are defined as var s ti i prime i ti prime i var s s ti i prime i prime ti prime i these semantics simply take different kinds of actions into consideration.
the first case means that when a local actionis performed by a thread only the local store of the thread is changed to a new state determined by its current state.the global store and the local stores of other threads remain the same.
the second case means that when a global action is performed by a thread t ion the shared variable s o n l ys and iare changed to new states.
the states of other shared variables on the global store as well as the local stores of other threads remain the same.
also consider the action sequence local to each thread since the program counter is included in the local store the next action of tishould be determined by ti s current local store i. the execution semantics defined above conform to a general concurrent execution model with deterministic input.
although dynamic thread creation and dynamic shared vari able creation are not explicitly supported by the semantics they can be modeled within the semantics in a straightforward way .
.
equivalence of execution schedules the action sequence angbracketleft k angbracketrightof a program execution is called anexecution schedule denoted by .
suppose there is an execution schedule of sizenthat drives the program state to n our goal is to have another execution schedule primethat is able to produce the same program state as n. obviously this can be achieved if prime holds.
however this is too strong a condition.
we show a relaxed and sufficient condition based on the access vectors of all the shared variables.
to state precisely let sbe the sequence of actions w.r.t.
sprojected from sbe the sequence of thread identifiers picked out from s a n d be the mapping from sto sfor s s is the access vectors of all the shared variables we prove 210theorem .under the execution semantics defined in section .
two execution schedules and primeof the same concurrent program have the same final state n primenif 0 prime0 prime.
the core of the proof is to prove the following lemma lemma .for any action primek k n in the replay execution prime suppose it is the pthaction on a shared variable s t h e n primekis equal to the pthaction on sin the original execution .
for two actions to be equal here they need to read and write the same values not just do the same operation on the same shared variable.
next we first define a notion of happened before and then we prove lemma using this notion.
consider the happened before order of the original execution.
the happened before relationisdefinedasfollows a if action iimmediately preceded action jin the same thread then ihappened before j b if action iand action jby different threads are consecutive actions on a shared variable s without any intervening actions on s t h e n ihappened before j c happened before is reflexive and transitive.
more accurately rules a and b define happened immediately before and happened before is the reflexive transitive closure of happened immediately before .
proof.
let s say the happened before tree of an action is the tree of all the actions that happened before it we next prove lemma by induction on the depth of the happened before tree.
base case consider an action on the shared variable s with a happened before tree of depth .
this means that the current action does not depend on anything that happened before it involving shared variables.
because the first action on a shared variable is performed by the samethread in both the original and the replay execution and be cause that thread is deterministic the replay action should be identical to the one in the original execution.
induction now assuming that lemma holds for all actions with happened before depth n we prove it for n .
consideranaction ionasharedvariable s w h e r e ihas a tree of happened before depth n .
let ssay iis thepth action on s.t h e p thaction on shas a lower happenedbefore depth so it is an equal action in both the original and the replay execution.
additionally every action jthat happened immediately before ihas a happened before tree of depth n therefore it is equal to a similarly numbered action in the original execution i.e.
if jis thekthaction on a shared variable v t h e n jis equal to the kthaction onvin the original execution .
now action ionly depends on all the jactions.
so since our approach enforces that thepthaction on sis performed by the same thread in both executions and since the thread is deterministic and everyvalue that ic a nd e pe n do nh a st obee q u a lt oe a c ho t h e r i t follows that action iis also equal in the original and replay executions.
lemma is proved.
if we apply lemma to the last action primenin the replay execution we can get primen n. thus theorem is proved.with theorem we have proved the soundness of localorder based approaches for the deterministic replay that isable to reach the same program state as the original execution by only recording the access vectors for all the shared variables.
while primeis a rather relaxed condition we can surely add more information that also guarantees the deterministic replay.
for example if the local variable accesses are recorded the deterministic replay is still guaranteed as long as we do not miss any shared variable accesses.
followingwe derive two corollaries corollary .the deterministic replay holds as long as prime regardless of whether accesses to local variables are recorded or not.
corollary .recording different shared variable accesses into a single access vector does not affect the correctness ofthe deterministic replay.
as noted in section .
the static escape analysis is conservative such that local variables might be mistakenly cat egorized as shared.
corollary ensures that this conserva tiveness does not affect the correctness of the determinis tic replay as long as all the shared variables are correctly identified.
corollary is easy to understand as the thread access orders on different shared variables can be consideredas a global order on a single variable abstracted from these shared variables.
to be more clear assuming all thread accesses are recorded into a global access vector it is a global order of the execution schedule and the determinism musthold.
asnotedinsection2.
corollary2ensuresthesound ness of our field based shared variable identification.
.
leap implementation we have implemented leapusing the soot2framework.
figure3showstheoverviewofthe leapinfrastructure consisting of the transformer the recorder and the replayer.
the transformer takes the bytecode of an arbitrary javaprogram and produces two versions the record version and the replay version.
started by a record driver leapcollects the access vector for each spe during the execution of the record version.
when the recording stops leapsaves both the access vectors and the thread creation order information figure the overview of leap infrastructure tfrecorder trans former spe access recorder rd spe locatorthread creation order recorderrecord version spe access instrumentorreplay driver generatororiginal program instrumentor record versionaccessvectorreplay driver thread creation order record version generator replayer replay version replay version generatortrace loader thread schedulerversion thread scheduler 211and generates a replay driver.
to replay the leapreplayer uses the generated replay driver as the entry point to run the replay version of the program together with recorded information.
the replayer takes control of the thread scheduling to enforce the correct execution order of the threads w.r.t.the spes.
we now introduce each of the components inturn.
.
the leap transformer the leaptransformer performs the instrumentation on jimple an intermediate representation of java bytecode in thethree addressform.
fortherecordversion afterlocatingall the spes in the program the transformer visits eachjimple statement and performs the following tasks instrumenting spe accesses if the spe is not a java monitor object we insert a leapmonitoring api invocation before the jimple statement to collect both the thread id and the numeric spe id.
both the api call and the spe access are wrapped by a lock specific to the accessed spe to ensure we collect the right thread accessing order seen bythe spe.
if the spe is a java monitor object we insert themonitoring api call afterthemonitorentry andbeforethe monitorexit instructions.
the api call is also inserted before notify notifyall thread start operations and after wait thread join operations.
figure2showsasource code equivalent view of the instrumentation on the read write accessestothesharedfieldvariables.
theboxontheleftshows the original method getbalance1 inside of which the shared variable balance1 is read.
the box on the right shows the transformed version of getbalance1 .
for multiple shared variable accesses in a method the thread id needs only to be obtained once.
also to remove the unnecessary recording overhead we do not need to instrument the spes that are always protected by the same monitor.
instrumenting thread initialization to capture the thread identity information as described in section .
the transformer inserts the instrumentation code inside the thread constructor to synchronize and to collect the threadcreation order.
instrumenting recording end points to enable the deterministic replay we insert the recording end points to save the recorded runtime information and to generate the replaydriver.
currently leapsupportsthreetypesofrecording end points.
first we add a shutdownhook to the jvm runtimeintherecorddriverasarecordingendpoint.
when the program ends the shutdownhook will be invoked to perform the saving operations.
second we insert a try catch block into the mainthread and the runmethod of each java runnable class.
we then add a method invo cation in the catch block to capture the uncaught runtime exceptions as the recording end points.
third leapalso supports the user specified recording end points by allowing the annotation based specification of end points.
during thetraversal of the program statements the transformer will replace the annotation with a method invocation indicating the end of recording.
to generate the replay version the transforming process is largely identical to the record version with a few differ ences .
since the order of synchronization operations on e a c hs p ei sc o n t r o l l e db yt h e leapreplayer during replay we need to insert the api call beforethe original synchronization operations in the program i.e monitorenter and wait to avoid deadlock .
the inserted api call is boundto a different implementation from the one used during the recording phase .
since we need to ensure that the replayexecution does not terminate until all recorded actions inthe original execution have been executed see section .
we insert extra api invocations aftereach spe access so that we can check whether a thread has performed all its recorded actions in the original execution or not.
.
the leap recorder when executing the record version of the target program theleapmonitoring api will be invoked on each critical event to record the id of the executing thread into the access vector of the accessed spe.
to reduce the memoryrequirement we use a compact representation of the access vectors by replacing consecutive and identical thread ids with a single thread id and a corresponding counter.
for example suppose the access vector of a spe contains t1 t1 t2 t2 t2 i ti sr e p l a c e db y t1 t2 and a corresponding counter .
this compact representation produces much smaller log size compared to the related approaches in our experiment.
onceanewthreadiscreated weaddtheparentthreadid to the thread creation order list.
once a program end point is detected the leaprecorder will then save the recorded data i.e the recorded access vectors and the thread creation order list and generate the replay driver.
.
the leap replayer the leapreplayer controls the scheduling of threads to enforce a deterministic replay using both the access vectors andthethreadidentityinformation.
toenabletheuserlevel thread scheduling the replayer associates each thread in re play with a semaphore maintained in a global data structure so that each thread can be suspended and resumed ondemand.
to replay the replay driver first loads both the saved access vectors and the thread creation order list and startsexecuting the replay version of the program.
before a newthread is created the id of the parent thread is compared to the id at the head of the thread creation order list.
if they are the same the new thread is allowed to be createdand the head of the list is removed.
in this way the ident i fi c a t i o no fe a c ht h r e a di sg u a r a n t e e dt ob et h es a m ea sthat of the recording phase.
before each spe access the threads use their semaphores to coordinate with each other in order to obey the access order defined in the access vec tor of the spe.
also to make sure that the replay executiondoes not terminate early the thread also counts the total number of spe accesses it has performed so far after each spe access.
the thread suspends itself if it finds that it hasalready executed all its spe accesses in the original execu tion as recorded in the access vector until all threads havefinished their recorded actions.
since the threads accessing different spes can execute in parallel the replaying process is also faster than that of a global order scheduler whichc a no n l ye x e c u t eo n et h r e a de a c ht i m e .
.
ev aluation .
evaluation methodology we assess the quality of leapby quantifying both its recording overhead and the correctness of the deterministic replay.
to properly compare our technique to the state 212of the art we have also implemented the following techniques the dejavuapproach based on the global clock t h et e c h n i q u ep r e s e n t e db y instantreplay and the jarec approach based on the lamport clock .
because none of these tools are publicly available we faithfully implementedthem according to their representative publications.
sincejarecis not a deterministic replay technique we extended its capability to tracking shared memory races in order to make it comparable to our technique.
our implementations are publicly available .
for the evaluation we first design a micro benchmark to conduct controlled experiments for quantifying various runtime characteristics of the evaluated techniques.
we then use real complex java server programs and third partybenchmarks to assess the recording overhead of leapin comparison to the related approaches.
we use the bug re producibility as a way to verify if our technique can faithfully and deterministically reproduce problematic concurrent runs.
all experiments are conducted on two core3.00ghzintelxeonmachineswith16gbmemoryandlinuxversion .
.
.
we now present these experiments in detail.
figure the runtime characteristic of leap and other techniques on our microbenchmark with the number of spe ranges from to .
the mi crobenchmark starts threads running on pro cessors.
5000123456x number of spetime ms processor number thread number 10base leap lamport global instant .
.
micro benchmarking we have designed a micro benchmark to quantify the runtime characteristics of leapa n dt h er e l a t e dr e c o r da n d replay techniques.
the benchmark consists of concurrent threads that randomly update shared variables in a loop.
for each experiment we can control the number of threads and shared variables.
in our experiments we set the numberof threads ranging from to and the number of sharedvariables ranging from to we then measure the timeneeded for all the threads to finish a fixed total number of updating operations under different settings.
figure and show the runtime characteristics of leap and the related techniques on our micro benchmark.
in the figures baserefers to the native execution.
global lam4 the runtime characteristic of leap and other techniques on our microbenchmark with the number of threads ranges from to running on processors.
the number of spe is set to .
.
.
.
.54x number of threadstime ms processor number spe number base leap lamport global instant portandinstantrefer to the recorded execution using global clock lamport clock and instantreplay respectively.
figure shows that the performance of the leapinstrumented version is close to the base version.
by fixing the number of threads to as the number of spe increases from to leapis more than 10x faster than global clock more than 5x faster than instantreplay and at least 2x faster than lamport clock.
global clock is the slowest among the four techniques.
the main reason is that the use of global clock requires a global synchronization on every shared variable access which significantly affects the degree of concurrency.figure shows a similar performance trend as the numberof threads increases from to and the number of spesis fixed to .
.
.
benchmarking with third party systems to perform the unbiased evaluation we first use leap on two widely used complex server programs derbyand tomcat w i t ht h e poleposition5database benchmark and the specweb 20056web workload benchmark.
each benchmark starts with threads and we measure the time for finishing a total number of operations.
we also selected a suite of third party benchmarks among which avroraand lusearch are from the dacapo .
bach benchmark suite7 and moldyn montecarlo and raytracer are from the java grande multi thread benchmark suite.
table shows some of the relevant static attributes of the benchmarked programs as well as the associated runtime overhead of the evaluated record and replay techniques.
we report the total number of field variable accesses in the program total the total number instrumented spe accesses spe the number of spes spesize the log size kb sec of the related approaches log the log size of leap logcmp and the runtime overhead leap lamport instantandglobal .
overall the percentage of spe 213table the runtime overhead of leap and the state of the art techniques application loc total spe spesize leap lamport instant global avrora 93k16003 lusearch 69k11497 .
derby .51m .
.
tomcat 535k .
.
moldyn montecarlo .
.
.
.
raytracer accessesoverthetotalnumberoffieldvariableaccessesvaries f r o ml e s st h a n3 o n derbyand tomcatto around on avroraand lusearch.a s moldyn montecarlo and raytracer are relatively small applications dedicated for multi threaded benchmarking the percentage of their spe accesses is large.
log size by using our compact representation of the access vectors the log size of leapis much smaller than the related approaches from 3x in moldyn to as large as 164x inderby.
we recognize that the log size in leapis still considerable ranging from to kb sec.
with theincreasing disk capacity and disk write performance as alsoobserved by other researchers moderate log size doesnot pose serious problem.
for long running programs we can reset logs through the use of checkpoints.
recording overhead leapis the fastest on all the evaluated applications.
it is even more than 150x faster thanglobal clock on moldyn.f o r derbyand tomcat leapis 5x to 10x faster than all the related approaches.
the sheerruntime overhead of leaponderbyandtomcatis less than .
and .
respectively .
leap s overhead is large onavrora the reason is that there are several spes inavrorathat are frequently accessed in hot loops.
.
.
concurrency bug reproduction one of the major motivating forces for the record and replay technique is to help reproducing so called heisenbugs.
we believe that the ability of deterministically reproducing a concurrency related bug is a strong indicator of the replay correctness because it requires the program stateto be correctly restored for the bug to be triggered.
tocompare the bug reproducibility we have also implemented jarecforthecomparison.
wefirstcompare leapandjarec for their capabilities of reproducing real world concurrency b u g sh a p p e n e di nc o m p l e xs e r v e rs y s t e m sa sw e l la san u m ber of benchmark bugs widely used in concurrency testing.to proper quantify the bug reproducibility we have alsodesigned a bug injection technique that injects atomic setviolations into our micro benchmark.
we then assess how many of the violations can be deterministically replayed byleapand jarec.
random bug injection our bug injection technique is based on the problematic thread interleaving patterns presented in .
we intro duce dummy shared variables into the program and divide them into groups each group representing an atomic set as defined in .
during the recording phase on eachcritical event the thread also randomly performs a writeor readaccess on one of the introduced variables.
we use the same random seed for each thread across record and replay.after each random access if one of the problematic thread interleaving patterns occurs the program stops and the re play data are exported.
given the same program input adeterministic replay technique should be able to recreate theoccurred bug pattern.
to compare the concurrency bug reproducibility between leapandjarec we use different random seeds to inject concurrency bugs into our micro benchmark.
for eachrun we initialize threads in the program.
leapis able to deterministically reproduce of these bugs while jarec cannot deterministically reproduce any of them.
the reasonis that jarecdoes not record shared memory races while all these bug patterns are generated on shared memory ac cesses.
table summary of the evaluated real bugs bug id version loc exception type derby230 derby .
.34m duplicatedescriptor derby1573 derby .
.52m nullpointerexception derby2861 derby .
.51m nullpointerexception derby3260 derby .
.52m sqlexception tomcat728 tomcat .
150k nullpointerexception tomcat4036 tomcat .
184k numberformatexception tomcat27315 tomcat .
361k concurrentmodification tomcat37458 tomcat .
535k nullpointerexception table summary of the evaluated benchmark bugs bug name loc bug description bubblesort 362not atomic orphaned thread allocationvector 286weak reality two stage access airlinetickets not atomic interleaving pingpong not atomic bufferwriter wrong or no lock randomnumbers blocking critical section loader initialization sleep pattern account wrong or no lock linkedlist not atomic boundedbuffer notify instead of notifyall mergesort not atomic critical not atomic deadlock deadlock deadlockexception deadlock filewriter not atomic manager not atomic real and benchmark concurrency bugs table and show the description of the real concurrency bugs and the benchmark bugs used in our experiments.
allthe real bugs in table are extracted from the derbyand 214tomcatbug repositories8that were reported by users.
the benchmark bugs in table are from the ibm contest benchmark suite which cover the major types of concurrency bugs including data races atomicity violation order violation and deadlocks.
we also run both jarecandleap on these buggy programs to compare the bug reproducibilitybetween them.
for the real world concurrency bugs leapis able to deterministically reproduce of them except the bugtomcat4036 a n d jarecreproduced none ofthem.
forthe benchmark bugs leapcan reproduce of them except bufferwriter loader a n d deadlockexception while jareccan only reproduce one of them deadlock .
the reason for leapto miss tomcat4036 is that the bug is triggered by races of the internal data of the underlying jdk library java.text.dateformat w h i c h leapdoes not instrument.
and because all these real bugs are related to shared memory races jarecare not able to reproduce any of them.
for the three benchmark cases leapcannot reproduce two of them are related to random numbers and the other one makes leap outofmemory because too many threads are involved in loops.
.
discussion the evaluation results have clearly demonstrated the superior runtime performance of leapas well as its much higher concurrency bug reproducibility compared to existing approaches.
through our experiments with real world large multi threaded applications we observed several limitations of leapthat we plan to address in our future work input nondeterminism asleaponlycapturesthenondeterminism brought by thread inter leavings it may not re produce executions containing input nondeterminism e.g.
programs with nondeterministic i o. the two benchmark bugsthat leapcannotreproducebothcontainrandomnumber generators that use the current system time as the ran dom seed.
since it is not likely to keep the random numbers the same across record and replay without saving them leapmay not reproduce executions that contain such random issues.
a way to overcome these issues is to save theprogram states of some key nondeterministic events e.g.
the value of random seeds.
we set this as our future work.
jdk library leapdoes not record shared variable accesses in the underlying jdk library.
if an execution con tains races of the internal data of these apis leapmight not be able to reproduce it.
the bug tomcat4036 is an example of this limitation.
in fact we can also instrument theunderlying java runtime but as the jdk library is usedfrequently it would incur large runtime overhead.
an im plementation of leapon the jvm should relieve this issue as the jvm environment enables efficiently tracing the in ternal data of the jdk library.
we also set this as our future work.
long running programs leapcurrently has to replay from the beginning of the program execution.
for long running programs it might not be convenient to replay the whole program execution concerning the long replay timeand the large log size.
we plan to extend leapto use a lightweight checkpoint scheme that only replays the pro gram from the last checkpoint to the recording end point.
related work as the deterministic replay of concurrent programs is of such significant importance there have been enumerable research efforts on this topic.
in this section we briefly review some of the other key software only related work.
record replay pres and odr are two recent projects that use record replay for the reproduction of concurrency bugs.
presproposes a novel technique that uses a feedback replayer to explore thread interleaving space which reduces the recording overhead at the price of morereplay attempts.
odrproposes a new concept outputdeterministic replay that focuses on replaying the same pro gram output and uses a similar idea as presthat depends on offline inference to help recording less online.
smp revirt makes use of hardware page protection to detect shared memory accesses aiming at replaying multi processor virtual machines but its overhead can increase upto 10x onmulti processors.
to avoid the overhead of recording mem ory races recplay and kendo provide deterministic multi threading of concurrent programs that perfectly synchronized using locks.
unfortunately most real world concurrent applications may contain benign or harmful dataraces making these approaches unattractive.
though recplayand kendoboth use a data race detector during replay toensurethedeterministicreplayupuntilthefirstrace theysuffer from the limitation that they cannot replay past thedata race.
for instance while debugging using a replayer a programmer might want to understand the after effects ofa benign data race which is not possible with recplay and kendo.
deterministic by default there are also approaches to the nondeterminism in concurrency by making concur rent programs deterministic by default.
in this direction there have been language design approaches as well as hardware ones .
for example languages such asdpj guarantee deterministic semantics by providing a type and effect system to perform compile time type check ing.
the problem with language level approaches is that they often require nontrivial programmer annotations orhave a limited class of concurrency semantics.
hardware approaches such as dmp make inter thread communication fully deterministic by imposing a deterministic commit order among processors.
pset eliminates untested thread inter leavings by enforcing the runtime to follow a tested interleaving via processor support.
because hardware ap proaches rely on non standard hardware support they arelimited to proprietary platforms.
though dmp also proposes a software only algorithm its overhead is more than10x.
code analysis tools another line of approaches is to use code analysis tools or model checkers to try to eliminate concurrency bugs offline.
code analysis tools suffer from inaccuracies and false positives.
model checkersstatically explore all thread schedules which is hard to scaleto large programs.
though chess employs a contextbounded way to reduce the search space it may miss most of the concurrency bugs in theory.
racefuzzer is another representative technique that given a potential race pair it controls a race directed random thread scheduler to activelycreaterealraces.
as racefuzzer hasonlypartialinformation oftheraces itsuffersfromthelimitationofnondeterminism.
.
conclusion we have presented leap a new local order based approach that deterministically replays concurrent program executions on multi processors with low overhead.
our basic idea is to capture the thread access history of each shared variable and we use theoretic models to guarantee its correctness.
we have implemented leapas an automatic program transformation tool that provides the deterministic replay support to arbitrary java programs.
to evaluate ourtechnique we make use of both benchmarks and real worldconcurrent applications.
we extensively quantified the runtime overhead of using leapas well as the correctness of the leap based replay through reproducing concurrency bugs.
our evaluation shows that compared to the state of the art leapincurs much lower runtime overhead and has much superior capability of correctly reproducing concurrency bugs.
for real world applications that we evaluated the overheadof using leapis under exhibiting the great potential for the production use.
we have also discussed some limi tations that we have observed during our experimentation and these limitations are the focus of our future work.
.
acknowledgement the authors wish to express deep appreciation to the anonymous fse reviewers for their insightful and constructive comments on an early draft of this paper.
this research has been supported by rgc grf grant .
the authors are very grateful for this support.
.