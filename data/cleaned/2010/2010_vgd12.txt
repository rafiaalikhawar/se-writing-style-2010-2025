green reducing reusing and recycling constraints in program analysis willem visser stellenbosch university stellenbosch south africa wvisser cs.sun.ac.zajaco geldenhuys stellenbosch university stellenbosch south africa jaco cs.sun.ac.zamatthew b. dwyer university of nebraska lincoln lincoln ne usa dwyer cse.unl.edu abstract the analysis of constraints plays an important role in many aspects of software engineering for example constraint satisability checking is central to symbolic execution.
however the norm is to recompute results in each analysis.
we propose a di erent approach where every call to the solver is wrapped in a check to see if the result is not already available.
while many tools use some form of results caching the novelty of our approach is the persistence of results across runs across programs being analyzed across di erent analyses and even across physical location.
achieving such reuse requires that constraints be distilled into their essential parts and represented in a canonical form.
in this paper we describe the key ideas of our approach and its implementation the green solver interface which reduces constraints to a simple form allows for reuse of constraint solutions within an analysis run and allows for recycling constraint solutions produced in one analysis run for use in other analysis runs.
we describe how we integrated green into two existing symbolic execution tools and demonstrate the reuse we achieve in the di erent settings.
categories and subject descriptors d. .
software program veri cation d. .
testing and debugging general terms veri cation experimentation keywords symbolic execution path feasibility constraint solving nosql .
introduction trading o space for time is a classic approach to improving the time e ciency of software analysis tools.
the logic permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
sigsoft fse november cary north carolina usa.
copyright acm ... .
.is simple if you can retrieve a result faster than it takes to recompute and you don t pay too high a penalty for the storage then storing and reusing will improve analysis time.
needless to say the bene t becomes greater when applying the approach to problems that are computationally expensive.
satis ability checking the world s most famous np complete problem where the problem runs in time exponential in the size of the constraints being checked is a prime example of where storing the result is better than recomputing it.
using symbolic execution to nd errors and generate tests is an active area of research .
satis ability checking is the cornerstone of symbolic execution.
storing the result of satis ability checking during symbolic execution and then reusing it later is not novel .
however this is most commonly done within one analysis where results are cached and then reused later in the same analysis.
how about caching results across analyses?
in this paper we consider many variations for reusing such persistent results across runs of the same program similar programs di erent programs but also the same program being analyzed with di erent tools.
we show that the reuse of results is substantial even across di erent programs and di erent analyses.
we also consider storing even more expensive computations namely model counting results that are used to calculate path execution probabilities .
our framework also supports reuse across di erent locations by allowing the user to con gure a list of stores to check if the result is not found locally.
once the result of an analysis is known it is pushed to all the stores on the list.
as the underlying technology to handle storage we use the redis1key value store an in memory database in the nosql style.
since redis is in memory local lookups are extremely fast but over the network lookups can pay a network latency penalty.
in this paper we only address symbolic execution as the technology we try to improve with the constraint results storage but we consider both classic symbolic execution and dynamic symbolic execution dse .
symbolic execution builds a path condition that is a conjunction of all the branching conditions in terms of the program inputs to reach a particular point in the code this path condition or a slightly modi ed version in the case of dse is checked for satis ability.
here we only consider linear integer arithmetic constraints for which satis ability is decidable but our approach will also work for other theories even undecidable theories and the more expensive the check the better .
if 1we check all the branching conditions to reach a location one would think there will be very limited reuse across runs from di erent programs and by de nition no reuse in the same program since all path conditions are unique .
to address this we rstly obtain the minimal path condition by a process we refer to as slicing required to check satis ability and secondly we transform path conditions into a canonical form.
the canonical form contains a renaming phase which allows us to match constraints that only di er in variable names.
for example the canonized constraints for the expressions x x and y y will be identical.
we evaluate our approach to reuse by considering a number of programs all previously studied in the symbolic execution literature and measuring how much reuse we get in a number of di erent settings.
to measure reuse across similar programs we consider mutations of each program and then also one actual regression example where we corrected a bug in one of our examples .
we then evaluate reuse across different programs by considering a sample of random orders of execution between programs and measuring the reuse.
lastly we show that without slicing and canonization this approach of persisting results is futile.
we make the following contributions in this paper show that constraint analysis results reuse can be generalized from in program caching to persistent stores that allows reuse across programs and across analyses show that constructing the minimal path condition and creating a canonical form for path conditions enable more results reuse describe an implementation of the green solver interface to which existing program analyses and constraint solvers can be attached.
green is freely available2 and has been adopted by existing projects for example it has been integrated with symbolic pathfinder spf .
.
overview in this section we provide an overview of the green framework for reusing constraint solving results which is sketched in figure and we illustrate several di erent forms of reuse that it makes possible.
green is designed to work with a wide range of program analyses and other algorithmic techniques that formulate a query constraint c0that is to be evaluated in a context de ned by a conjunctive constraint c1 c2 c n. analyses typically take the approach of simply conjoining the query and context constraints translating them to existing solvers such as cvc3 choco or yices and invoke the solver to determine the satis ability of the constraint and to extract related information such as a model of a satisfying assignment.
in contrast our framework consists of four phases slicing canonization reuse and translation.
first we calculate the minimal set of context constraints on which the query depends.
this constraint slice has the potential to signi cantly reduce both the number of constraints and the number of variables in the problem.
second constraints are canonized by reformulating each individual analysis slice co canonize reuse translate yices cvc3 choco latte green solution storec0 c1 c2 cn c0 c1 cn c0 c0 c0 n 1get c0 c0 c0 n not foundsat c0 c0 c0 n c0 c0 c0 n s sput c0 c0 c0 n sat figure path sensitive analysis with solver reuse constraint into a canonical form indicated as primed constraints in the gure and by reordering constraints.
the canonization process also renames variables.
third a solution store is queried with the canonized constraint.
if a solution is found it is returned immediately to the invoking analysis thereby avoiding the need to call a solver.
fourth if a solution is not found the canonized constraint is translated into the format of a selected solver and passed to the solver the format for solver sis denoted s. a solver returns its results which are translated back to correspond to the canonized constraint and those results are inserted into the solution store and returned to the invoking analysis.
one might imagine that a simpler solution for reusing solver results is possible but the nature of path sensitive analyses is to construct distinct constraints representing distinct program paths.
as we illustrate below and demonstrate through our evaluation in section both slicing and canonization are essential for achieving signi cant reuse and analysis performance gains.
.
reuse within an analysis run to illustrate the operation of our framework we consider the small example program in figure and the application of a symbolic execution analysis.
the bottom of the gure shows the symbolic execution tree where nodes are labeled by the branch conditions in the source code and edges are labeled by symbolic constraints encoding conditions under which branch outcomes are taken.
we represent the input values for xandywithxandy respectively.
the lled shapes represent the exit points of the method we do not explicitly show the e ects of individual statements here but their e ects are accounted for in the symbolic constraints.
for a call m the program would take the path ending at the star node.
the path condition pc for this path is x y x y the execution of statement x x on this path gives rise 2to xin subsequent constraints along the path the same goes for y. there are paths in this program each with a distinct path condition.
to determine whether these paths are feasible a symbolic execution tool will pass the conjunction of constraint accumulated along each path pre x to a satis ability sat solver a sat result implies path feasibility.
this will lead to solver calls for this example.
while modern solvers have increased dramatically in terms of performance in recent years they still are quite expensive and many analyses that use them have had to adopt a number of innovative and custom optimizations in order to scale .
one of the goals of our framework is to provide generic support for some of those optimizations and to go further by reusing previous solver results.
consider the true outcome of initial branch x .
the pc isx and there are two potential outcomes of the next branch y y and y .
rather than simply conjoin a branch constraint with the pc and check for satis ability as explained in figure our framework slices the pc with respect to the branch constraint.
in either case this eliminates x sincex is independent of y. the resulting sliced formula e.g.
y is canonized which involves restructuring each clause into a canonical form and then reordering those clauses and renaming variables.
in the case of the constraints described above both x and y would yield the canonized constraint v0 .
slicing and canonization expose a wide range of constraint equivalences as syntactic equivalence and this permits our framework to look up existing results and reuse them.
when our solver is used to symbolically execute the example in figure just rather than solver calls are needed.
for the pcs that require solving in the rst two levels of the tree in figure only solver calls are made.
at the third level the new pcs lead to new solver calls.
at the fourth again just solver calls are needed to handle the new pcs.
the path leading to the star node illustrates how the longer paths are able to be reused.
solving for the third constraint on that path yields a sliced pc of x x which is canonized to v0 v0 .
solving for the fourth constraint on that path yields a sliced pc of y ywhich is canonized to the same constraint as encountered one step earlier on the path so the solution to that constraint is reused.
a nal but important bene t of slicing is that when a constraint solution cannot be reused the constraint that must be solved is often signi cantly smaller and has fewer variables.
.
reuse across programs analyses solvers the simple example in figure illustrates what might be achieved by results caching within a single analysis.
the solution store in our framework is by default persisted when an analysis run exits.
this makes the constraint solver results accumulated from a speci c analysis on a speci c program using a speci c solver available for subsequent reuse.
importantly the canonical forms in our framework are independent of analysis program or solver and so a wide range of reuse scenarios are possible.
one obvious scenario that o ers the potential for significant reuse is to improve the e ciency of techniques that perform path sensitive regression analysis such as dise .i n t m i n t x i n t y f i f x x x i f y y y i f x f return ge l s e i f y f modification y return ge l s ef return g g x y x x y y y x y 0x x y y y x y x 0y x x y y y x y 0x x y y y x y x fig ure example and symbolic execution tree consider a modi cation of the branch yin the example above to y. this would change each of the constraints at depth in the symbolic execution tree but all constraints solved at depth or less would be reused from the analysis of the original program.
moreover for those new constraints only solver calls are needed because of reuse within the analysis of the modi ed program.
in total for analyzing this modi ed program only of the possible solver calls are needed.
in section we will present results that demonstrate that even for di erent programs there are signi cant reuse opportunities.
for example we studied three di erent container implementations and found that more than half of the solver calls can be reused from an analysis of one of the others.
we also present our experience refactoring a dynamic symbolic execution technique developed by other researchers to use our framework.
the refactoring was straightforward and immediately sped up the other analysis.
it is a matter of con guration to switch green among a set of di erent solvers.
we have experimented primarily with cvc3 and choco but green provides an opportunity for expensive solvers to be integrated into more common use since their results can be computed once and reused many times.
we have demonstrated this bene t in our latest work on probabilistic symbolic execution where we have refactored a caching scheme for single runs of our analysis to usegreen .
this has proven to dramatically reduce analysis times which are strongly dependent on the execution of costly model counting solvers.
in the next section we present more of the details of the green framework and the slicer and canonizer that it includes.
section presents a broad evaluation of the potential bene ts of green and discusses opportunities for future 3enhancement.
we discuss related work in section and conclude in section .
.
storing results the results that we compute must be stored so that they can be reused.
for this we employ two techniques slicing of path conditions section .
and path condition canonization section .
.
although we focus mostly on the problem of satis ability checking both these techniques are also used to enable more reuse for other calculations such as model counting .
a design goal was to make our storagecentric approach as simple as possible to integrate into existing tools.
to this end we discuss what is required for the back end storage component section .
and the interface our system provides to enable integration section .
.
.
path condition slicing any path condition conjunction of constraints can be split into a part for which the satis ability result is known knownpc to hold and a new part newpc that when added to the other the satis ability result is unknown.
for example in classic symbolic execution every branch introduces a new constraint newpc to be added to the existing path condition knownpc that is known to be satis able .
whereas in dynamic symbolic execution one negates one of the constraints newpc from the path condition and the rest is the known satis able part knownpc .
slicing is based on the fact that the satis ability of adding the newpc is not dependent on all of the knownpc .
slicing makes sure that only the part that is required is used in conjunction with the new part before satis ability is checked.
if one considers the constraints within a path condition to be represented as a constraint graph v e where the vertices vare the symbolic variables and the edges eindicate whether two variables are part of the same constraint then slicing can be de ned as a graph reachability problem .
build a constraint graph for knownpc newpc .
find all variables rreachable from variables in newpc .
return the conjunction of all the constraints containing variables r. slicing has two bene ts.
it rstly reduces the size of path conditions to be checked for satis ability but in this work this bene t is not our main aim.
we are more interested in the second bene t it allows one to get more matching of path conditions both within the same program but also across other programs.
complete path conditions are unlikely to ever match a complete path condition of a di erent program and never one from the same program.
however once one only looks at a subset of the path condition as produced by slicing matching becomes a real possibility and in section we will show that it actually happens quite often.
.
canonization we use a standard approach to constructing the normal form for linear integer arithmetic but we believe one must augment this step by adding heuristic steps before and after the normal form calculation to maximize the chance of nding matches.
pre heuristic currently we have one heuristic in the pre stage namely variable reordering based on the lexicographic order of the variable names.
this reordering allowsone to match constraints that in one constraint might be x y and in another y x since they will both become x y .
this reordering is quite weak at the moment since we don t attempt to reorder expressions such as x y z versus z x y we only reorder constraints with single variables on both sides of the operator.
belying its simplicity this heuristic makes a substantial di erence to the amount of reuse achieved in our examples.
our system allows one to easily attach custom heuristics to control canonization.
normal form finding a normal form for linear integer arithmetic path conditions is well studied and in this step we follow the standard approach of converting each constraint to the form ax by cz k op where op2f g we handle and constraints by themselves but all other inequalities are transformed into constraints over only .
the rst step is to transform each constraint such that all variables and constants appear only on the left hand side of the equation with a on the right hand side this step is enough to convert and to their nal state.
however for we need potentially two more steps to multiply each side by to transform and to respectively and and for we must add on the left hand side to transform it to a inequality.
finally we join all the separately transformed canonical constraints by adding conjunctions to recreate a path condition.
note we always place the constant after all variables on the left hand side and all simple arithmetic operations over constants are performed throughout to ensure just one constant in each constraint.
for example the path condition x y z x z x y is transformed as follows.
x y z becomes x y z then x y z whereas x zbecomes x z and lastly x yis converted to x y then to x y and nally to x y .
thus the nal output of this phase is x y z x z x y post heuristic there are currently two heuristics in this phase.
the rst one orders the constraints from the previous phase normal form calculation in a lexicographic order returning a new ordered path condition.
then follows a renaming phase that is based simply on the left to right order that the variables appear in the path condition.
for example if we take the path condition at the end of the example above namely x y z x z x y the reordering produces the transformed path condition x y x y z x z since the string x y comes lexicographically before x y z etc.
.
renaming then replaces xwith v0 since it is the rst variable from the left ywith v1andzwith v2resulting in the nal path condition v0 v1 v0 v1 v2 v0 v2 .
storage the requirement on the storage component is that it must be fast but also easy to integrate into the rest of the system.
a system that meets both requirements is the redis key value store.
it is fast since it is actually an in memory database but it does persist its contents to disk after a congurable time or number of updates .
we use the jedis 4java interface code to allow communication directly to redis from our java implementation.
redis runs on most posix systems but we used it only on ubuntu linux and mac os x. however there is no requirement that the redis server runs on the same system as our framework and it can be accessed by connecting to it over the internet.
since it is in memory the only penalty is the network latency and throughput.
although our current implementation only supports redis any database can be used.
however in memory databases should always be preferred otherwise the storage retrieval latency will reduce the advantage of reuse.
as can be seen from figure we simply require getand putoperations on the store.
these are the core operations supported by redis with get key and put key value calls.
redis is optimized to handle strings as keys and as such we use the string representation for a path condition as the key.
for the value eld we also use strings true and false for satis ability and the string representation of integers for model counting.
the system supports a list of redis stores to be con gured at startup which then allows an analysis to search them in order to nd a solution.
when more than one redis store is con gured all putoperations are made immediately to the local store but asynchronously also to all the other stores in the list this allows the main system to continue while results are written to potentially slow stores .
in addition when a remote lookup returns a result it is then also pushed asynchronously to all the other stores in the list.
this allows results to propagate around the network of stores.
in turn this allows reuse across locations since results that were pushed to a store can now be reused by another analysis that uses the same store.
we believe this is a viable community analysis model to the best of our knowledge the rst of its kind where people di erent analyses now have the opportunity to bene t from each others results.
.
the green solver interface at the core of green is the solver class.
it allows clients to register the six components used to answer queries a decision procedure a constraint solver often the same as the decision procedure a model counter for counting the number of solutions a canonizer a slicer and a store.
a seventh component is required to translate path conditions to instances of our instance class.
users are free to supply their own extensions of these components but the goal is to centralize not only the query results but also as much as possible of the hard work.
there is therefore a defaultcanonizer and defaultslicer aredisstore support for cvc3 and choco as both decision procedures and constraint solvers and for latte as a model counter.
if a feature is not needed the component can be omitted.
changing symbolic pathfinder spf to use green instead of its traditional decision procedure infrastructure took hours of work.
similarly to integrate it into a dynamic symbolic execution system took hours.
in both cases the integration was done by people other than the main developers of the respective tools.
we believe this illustrates that the integration is quite manageable and for tool developers it might be even easier.
.
evaluation below we evaluate the performance of our framework for four di erent scenarios across runs of the same pro gram or very similar programs across di erent programs across di erent tools with a focus on the contribution of slicing and canonization.
all experiments were conducted on a apple imac with a ghz intel processor with cores and gb of memory.
it runs the mac os x .
.
operating system and the darwin .
.
kernel.
the experiments that follow are based on six programs trityp implements demillo and o utt s solution figure of myers s triangle classi cation problem euclid implements euclid s algorithm for the greatest common divisor using only addition and subtraction tcas is a java version of the classic anti collision avoidance system available from the sir repository binomialheap comes from bintree implements a binary search tree with element insertion deletion and search from and treemap uses a red black tree to implement a java map like interface also from .
we use four performance metrics a for symbolic execution we measure the running time t when our framework is not used and the running time t when our framework is used and we compute the time ratio ts t t .
b we also count the number of sat queries n that are given to the decision procedure when our framework is not used and the number of queries n when our framework isin use.
we then calculate the reuse ratio rs n n n .
c for model counting sat we again measure the running times u u when our framework is used not used and calculate the time ratio t u u .
d we count the number of sat queries m m when our framework is used not used and calculate the reuse ratio r m m m .
the time ratios give an indication of how much our framework speeds up the analysis.
for instance if ts the analysis is ten times faster.
lower values are better.
the reuse ratios indicate what proportion of queries are avoided .
if rs three quarters of all queries are reused and we only need to compute of the original set of queries.
higher numbers are better.
when rsorr is all queries are found in the store and no invocations of the decision procedure or model counter are needed.
unless otherwise stated all the experiments reported below were run using green interfaced with symbolic pathfinder spf .
.
across runs we consider three degrees of changing a program small increments using mutations scaling the behavior of the program and a larger regression that modi es the behavior but not the intent of the program.
.
.
small increments program changes are often small.
we use rst order mutations to illustrate the savings achieved when the results from previous analyses of earlier variations of a single program are reused.
jumble was used to produce a number of mutations either by changing a comparison operator or by changing an addition operator inside an integer expression .
note that all possible mutations 5tcas mut ation ts mut ation rs bi nomialheap mut ation ts mut ation rs tr ityp mut ation ts mut ation rs fig ure reuse during mutation analysis not only a subset are investigated.
figure shows two charts for each of three input programs the left hand chart shows ts and the right hand chart shows rs.
we start with an empty store and then analyze each mutation in turn all the while populating the store.
we do not show t orr their graphs follow roughly the same pattern except that the improvements are even more dramatic.
all three cases exhibit similar behavior as more and more mutations are analyzed the time consumption falls to a minimum of in the case of trityp while the number of sat queries reused increases to practically .
for some programs as exempli ed by trityp the early values oftsare greater than this happens because initially there is little reuse and the extra overhead cannot be recouped by the savings made.
other cases cause outliers because the mutated code leads to an exception and the running time of the mutant is so small that the natural variance in execution times leads to a high value for ts.
.
.
scaling a second aspect of programs that result in very similar variants is scaling.
for example each of the container classes binomialheap bintree treemap is parameterized with n the number of insert remove operations.
when n spf analyses four possible sets of operations insert x insert y insert x delete y delete x insert y table large regression symbolic execution ts rs self reuse bintreebug bintree model counting t r self reuse bintreebug bintree and delete x delete y .
the xandyvalues are symbolic.
table shows how our approach deals with scaling.
each container was analyzed for n starting with an empty cache.
for n the time ratio for symbolic execution is quite high because there are fewer sat queries and the time consumption with and without the store is dominated by the overhead of the analysis.
for larger n ts improves and the best results in this section is the ve fold speed increase for bintree when n .
for sat the n values are lower better because the sat queries are more expensive even though the number of sat and sat queries are the same.
for binomialheap andn there is a ten fold increase in execution speed and the value oft forbintree and treemap may be even higher but those experiments exceeded a pre set time limit.
.
.
larger regression the remove operation of bintree the binary search tree implementation has two variants the original code contains a number of subtle but serious bugs that make it impossible to remove the root element of the tree and that incorrectly deletes subtrees under very special conditions.
the remove operation has been replaced by a correct implementation and we use it for the other experiments in this section but it is instructive to look at how our approach fares when dealing with such a large regression where a substantial piece of code has changed but the general purpose of the program remains the same.
the correct version of remove contains loc compared to loc in the buggy implementation and the structure of these methods di er signi cantly.
table shows the results of analysis of the incorrect bintreebug with an initially empty store followed by the correct version bintree .
as the analysis proceeds the store is populated with query results.
the rst time a query is encountered its results are calculated and stored.
when the same result is found again it is already in the store and does not need to be recalculated.
we call this self reuse.
as the rst line of table shows this is the only kind of reuse available during the analysis of bintreebug in this scenario.
on the other hand the subsequent analysis of bintree performs no self reuse because all of its queries have already been calculated and stored by the bintreebug analysis.
.
across programs can we achieve a similarly high level of reuse even when the programs are not the same?
a natural starting point is to consider programs that while not the same share some common functionality.
the container programs binomial6table performance on scaling examples symbolic execution ts rs n n n n n n n n binomialheap bintree treemap model counting t r n n n n n n n n binomialheap bintree treemap sat queries41 133binom ialheap treem ap bintre e sa t queries40 8binom ialheap treem ap bintr eelat te internal queries 532binom ialheap treem ap bintr ee fig ure shared keys for binomialheap bintree and treemap heap bintree and treemap are good examples of such programs since they record a collection of integer values and provide operations to add remove and check for containment of values among other operations.
can the results from one of these programs be reused by the others?
the three venn diagrams in figure show that this is indeed the case.
the lower left diagram shows that of of the sat queries oftreemap are shared by binomialheap and bintree .
for bintree the gure is while for binomialheap it is only .
this share is a similarly measly for the number of sat queries as shown in the right lower diagram.
however the model counting process breaks each sat query into a number of subqueries and their results are also stored.
this subresults reuse is shown in the upper diagram treemap shares of its subqueries with theothers bintree shares subqueries and binomialheap shares subqueries.
of course one should keep in mind that even relatively small changes to the programs can reduce the potential for reuse dramatically.
nevertheless we believe that the reuse data shown here raises some interesting questions about the commonality of constraints encountered across programs and we plan to conduct a large and broad study to better understand reuse opportunities.
.
.
matrix of programs taking our six programs and some of the variants of the containers in pairs we evaluated the reuse that occurs when starting with an empty store the rst program is analyzed and then the second.
the values of rsfor this exercise are shown in table .
the rst run programs are shown in the leftmost column and the second run programs in the top row.
at one extreme there are many pairs with no shared constraints.
at the other end of the spectrum are the related programs such as the pair bintree treemap3 with a high level of reuse as we expected and demonstrated above.
encouragingly there are many pairs in the middle ground that are not related but nevertheless have many common constraints.
.
.
trends over sequences even unrelated programs may still share some even many constraints.
there are possible orderings of our six programs.
from this we selected a random sample of and for each ordering we analyze the six programs one after another starting with an empty store.
naturally the rst program in such a sequence cannot reuse any previous work since there is none .
we measure the reuse that occurs when the second third fourth and so on program is run.
the minimum maximum median and lower and upper quartile of the rsvalues are presented as box plots in figure .
first note that the distribution is quite broad stretching from to close to .
it is encouraging that all of the quartiles move towards indicating that reuse is taking place and that the store is being populated with computed queries.
.
across tools to explore the generality of the green framework we performed a case study that involved refactoring an existing analysis to use green .
we then compared the performance 7table 2matrix of across program reuse bintree 3bintree 4treemap 3treemap 4binoheap 3binoheap 4euclid trityp tcas bintree .
.
.
.
.
.
.
.
bintree .
.
.
.
.
.
.
.
treemap .
.
.
.
.
.
.
.
treemap .
.
.
.
.
.
.
.
binoheap .
.
.
.
.
.
.
.
binoheap .
.
.
.
.
.
.
.
euclid .
.
.
.
.
.
.
.
trityp .
.
.
.
.
.
.
.
tcas .
.
.
.
.
.
.
.
figure sequences of the solver based implementation with reuse and running in combination with traditional symbolic execution.
.
refactoring a dse we selected an analysis that has been implemented in the third author s research group but with which none of the authors are involved.
the analysis performs an o ine form of dynamic symbolic execution dse .
a program is analyzed using the soot analysis framework and instrumentation is inserted to record traces which encode symbolic constraints that correspond to the branch outcomes along a program execution.
an o ine component processes the trace encoding a path condition and converts it into a set of constraints which are then solved.
more speci cally for path condition c1 c2 c kthe following constraints are generated and solved c c c2 c1 c2 c k ck .
the o ine dse analysis component consists of lines of java source code.
the choco solver is used to solve constraints.
the core component of the analysis is the trace interpreter it consists of lines of java code.
the interpreter reads in the recorded trace le constructs a representation of the constraints and uses the choco api to produce a choco speci c representation of the constraints.
each of those constraints is then solved by calling a pair of methods in the choco api.
it took us approximately hours to understand the analysis and approximately .
hours totable multiple analysis performance analysis order time dp time dp dp sec sec calls reuse dse original 1st .
.
dse solver 1st .
.
symexec 2nd .
.
symexec 1st .
.
dse solver 2nd .
.
refactor the lines needed to convert the choco speci c processing to use green s apis.
we did not talk to the developers of the dse analysis until after we had completed the refactoring.
we discussed our changes with them and showed them the output of the system tests we ran on the refactored analysis.
they conrmed that the changes looked correct and the results were as expected.
we note that in the refactored analysis we can shift between choco and other solvers by modifying a single constructor parameter.
in the results discussed below we used cvc3 as the decision procedure.
.
performance of solver based dse we explored the performance of the original and refactored analysis using the java version of the tcas program.
the dse analysis requires a test suite to generate traces and we used a pre existing branch adequate test suite consisting of tests.
table shows the performance of several variants of the analysis in isolation and in combination with a solver based symbolic execution.
dse original is a version of the dse analysis that neither slices constraints nor reuses solver results we used cvc3 as the solver to enable the performance resulting from reuse to be apparent.
dse solver enables slicing canonization and reuse.
the second and third rows were run in order after clearing the results store similarly for the fourth and fth rows.
enabling slicing and reuse reduced the solver time significantly by over and the overall runtime by more than .
this is due to the fact that more than of the constraints had solutions that were reused.
running dse before symbolic execution resulted in a modest improvement in the performance of symbolic execution since an additional out of constraints had solutions computed by dse that could be reused.
switching the order of the analysis had a more signi cant e ect on dse which saved the computation of out of constraints.
we expected that dse 8table e ect of slicing canonization and the store store store canon canon canon canon slice slice store store canon canon canon canon slice store store slice slice slice slice canon slice slice canon canon canon canon store run after symbolic execution would have perfect reuse but when analyzing the constraints that could not be reused we discovered that they are all related to dse dropping a single conjunct when processing a particular branch we have reported the problem to the developers .
.
slicing and canonization table presents a more detailed analysis of the contribution of slicing canonization and storage.
the binomialheap program was run eight times to investigate all the possible combinations of these three components.
in each run the parameter value n explained in section .
.
is used.
the eight values in the top two data rows of the table give the time in milliseconds spent in the decision procedure.
recall that we are using cvc3.
the leftmost column and the two rows in the heading specify the components that were in use x xmeans that component xis inactive active.
the three lower parts of the table summarize the impact of each of the three components.
for instance when canonization is on but the store is not switching slicing on reduces the time from to milliseconds and the impact factor is .
the analysis of the program produced path conditions of which were unique.
altogether there were non distinct conjuncts which slicing reduced by to conjuncts.
the rst thing to notice is that without slicing and canonization the storage takes more time since there is no results reuse but there is some overhead to use the store.
however with both slicing and canonization enabled the improvement from using a store is almost an order of magnitude.
storing actually also bene ts from just the canonization by almost a factor of .
however slicing by itself improves reuse almost fold.
if we look at the impact of each optimization individually it is clear that slicing has the biggest impact on the performance of the decision procedure.
this is a worthwhile result by itself since it is quite easy to implement slicing in a symbolic execution framework and clearly the bene t is substantial.
canonization didn t help the decision procedure at all.
in theory this step could produce constraints in a form that bene ts the decision procedure but we in tended it to increase reuse i.e.
when storage is enabled .
since we are only considering reuse within the same analysis run here self reuse storage must be used with one of the other optimizations.
the exact ratios reported here are clearly a function of the particular decision procedure and how it copes with the nature of the constraints that a particular problems produces as well as the structure and complexity of the path conditions.
we have selected a representative but not the best possible program and parameter size.
.
related work researchers have been exploring the application of constraint solving to program analysis for over three decades building on the work on nelson and oppen .
during this time tremendous advances have been made both in constraint solving and in program analyses that have made their combination e ective in reasoning about large complex software systems e.g.
see .
for the most part these advances have been developed within a speci c system and this has limited their broad adoption by the research community.
we discuss several of these systems below and highlight the novel techniques that they introduced.
green incorporates those techniques adds additional capability in the form of canonization and disk based caching and importantly presents this suite of capabilities in a form that is both analysis and solverindependent.
.
slicing and caching when presented with a complex constraint to solve it is natural to factor it into independent subproblems.
this strategy has long been applied in research on constraint solving and it is present today in modern smt solvers which purify formulas into theory speci c sub formulas for solution.
this kind of decomposition can enable customized algorithms to be applied to the independent subformulas as in smt or can simply decrease the time to compute a solution because there are fewer clauses or variables.
several researchers have observed an additional bene t of problem decomposition decomposed constraints recur across the space of possible solutions.
an example of this strategy applied within a solver is the integration of component caching into an extension of a dpll based sat solver zcha to perfom model counting .
model counting as discussed earlier is a particularly expensive form of constraint solving that requires the exhaustive consideration of the set of satis able solutions.
the insight in this work is that by identifying independent fragments of a formula that recur across the solution space one can count the solutions for a fragment a single time cache that count then retrieve it when the fragment is encountered later in the search.
this amounts to achieving the kind of reuse we provide but within a single analysis run only .
it is important to note that when attempting to generalize this approach to count models over richer theories such as lia there are additional opportunities for reuse when one canonizes constraints as we do.
in our own recent work we see these bene ts play out when applying model counting to a variant of symbolic execution that calculates path probabilities .
in fact the inspiration for green was our experience developing this probabilistic symbolic execution.
while we did not store model 9counts across program runs the bene ts of canonization and reuse were very apparent.
since the development of green we have refactored our own probabilistic symbolic execution system3to use green rather than implement its capabilities as part of the process of invoking a speci c external tool.
an example of constraint solver reuse applied outside of the solver within an analysis tool is the exe symbolic execution system .
the key insight here is that the structure of a symbolic execution is such that when a program has independent branches the path condition will be comprised of independent constraints.
decomposing those constraints o ers the opportunity for reducing the number of calls to a constraint solver.
this is a key feature of exe and its successor klee that allow it to scale to large programs.
our work builds on this insight and takes it a step further.
whereas exe can decompose independent sub problems it can only determine sub problem equivalence through syntactic equivalence.
our framework exposes a wide range of additional equivalences and expresses them as syntactic equivalence on the canonized constraints.
we believe that state of the art tools like exe and klee could further bene t from canonization.
.
canonizing constraints canonization of logical formulas and its use in supporting decision procedures has been studied e.g.
.
an important application of these ideas is its support for identifying equivalences within a given formula .
the key insight here is that if one can de ne a canonical form then formulas converted to that form expose semantic equivalence as syntactic equivalence.
this is precisely the same insight we leverage except that we exploit syntactic equivalence to improve reuse rather than judge two formulas equivalent.
there are many settings in which the general strategy of equivalence based reduction has been developed and applied to improve the performance of program analyses.
for example the ideas of state and thread symmetries have been used to perform property preserving state space reductions in software model checking .
regression symbolic execution in recent years there have been a number of e orts focused on program equivalence checking or di erencing that exploit deep program analyses of the type provided by symbolic execution.
in such analyses one attempts to co analyze two program versions which are often very similar.
this rst work that we are aware of to consider this problem is that of siegel et al.
.
while not strictly about regression analysis this analysis constructed a driver that placed a sequential version of a program rst followed by a parallel version of the same program.
the path conditions accumulated along the analysis of the rst e ectively focus the analysis along the second program.
this approach uses a custom built constraint solver that implements value numbering a classic compiler optimization technique that computes a canonical form for expressions and is used to detect equivalences.
like other constraint solvers this technique can ignore redundant constraints.
they do not perform anything like slicing and we believe that their technique could be improved by so.
our own work used a form of overapproximating symbolic execution to skipportions of the program that are prov3 identical across the versions.
in followup work person et al.
developed a variant of this approach that targets the spf symbolic execution system at representative paths through the regions of the programs that have been potentially impacted by a change .
while these approaches clearly exploit the similarity between programs after all they are explicitly regression analyses they do not exploit the fact that the constraint solver calls performed on the two versions are likely to be very similar.
this could easily be added to their analyses using green .
in light of the recent work on regression symbolic execution the developers of exe have targeted their techniques at the problem .
as with the other work along these lines their technique achieves a kind of reuse across the analysis of two program versions by integrating them into a single analysis run.
while we don t focus on regression analysis it is clear from the results of our evaluation that green permits a similar kind of reuse and not just for pairs of program versions but across entire version histories.
finally we have recently become aware of an alternative approach to symbolic execution that memoizes portions of the symbolic execution tree .
this approach records a much richer body of information the tree of symbolic constraints computed by the analysis which can be exploited in subsequent analyses.
when applied to regression analysis this would o er the potential to completely skip exploration of portions of the program behaviors whereas a symbolic execution using our solver would only skip the solver calls in those portions.
when symbolic execution does need to be performed we believe that our solver can o er reuse opportunities to speed up their analysis.
moreover green is more broadly applicable across di erent analyses because it is not tied directly to symbolic execution.
.
conclusion and future work we have presented green a framework that we believe o ers bene ts for a wide range of program analyses that use constraint solvers.
we have shown evidence that constraint analysis results can be reused both within the run and across runs and that this can signi cantly reduce analysis time.
perhaps the most surprising nding is that one can reuse constraint solutions from analyses of completely di erent programs.
we also showed the intriguing possibility of reusing results between di erent tools.
in our case it was on the same program but of course it could also be across programs.
our future work involves extending the range of theories and solvers supported by green developing strategies for e ciently sharing constraint solutions in a network of stores shared by the community and exploring the extent to which di erent programs share common constraints.