detecting memory leaks through introspective dynamic behavior modeling using machine learning sangho lee college of computing georgia institute of technology atlanta ga usa slee431 cc.gatech.educhanghee jung department of computer science virginia tech blacksburg va usa chjung cs.vt.edusantosh pande college of computing georgia institute of technology atlanta ga usa santosh cc.gatech.edu abstract this paper expands staleness based memory leak detection by presenting a machine learning based framework.
the proposed framework is based on an idea that object staleness can be better leveraged in regard to similarity of objects i.e.
an object is more likely to have leaked if it shows signi cantly high staleness not observed from other similar objects with the same allocation context.
a central part of the proposed framework is the modeling of heap objects.
to this end the framework observes the staleness of objects during a representative run of an application.
from the observed data the framework generates training examples which also contain instances of hypothetical leaks.
via machine learning the proposed framework replaces the error prone user de nable staleness predicates used in previous research with a model based prediction.
the framework was tested using both synthetic and realworld examples.
evaluation with synthetic leakage workloads of spec2006 benchmarks shows that the proposed method achieves the optimal accuracy permitted by stalenessbased leak detection.
moreover by incorporating allocation context into the model the proposed method achieves higher accuracy than is possible with object staleness alone.
evaluation with real world memory leaks demonstrates that the proposed method is e ective for detecting previously reported bugs with high accuracy.
categories and subject descriptors d. .
software program verication reliability statistical methods d. .
software engineering testing and debugging debugging aids monitors tracing d. .
processors memory management run time environments general terms languages measurements performance reliability permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
icse may june hyderabad india copyright acm ... .00keywords memory leak detection machine learning runtime analysis .
introduction for unmanaged programming languages memory leaks are a common bug that undermines the quality of the program.
a memory leak occurs when an application omits the deallocation of an allocated memory object that has no future use.
because memory leaks gradually exhaust available system memory they are often the root cause of performance degradation and a sudden hang crash failure of software systems.
moreover memory leaks can be intentionally exploited by adversaries to launch denial of service attacks .
one notable example showing the severity of memory leakage is the amazon web services outage .
in amazon replaced a data collection server.
unfortunately this seemingly harmless maintenance action caused an incorrect con guration of some servers which led to memory leaks.
due to the failure of a monitoring alarm the memory leaks went out of control eventually and the a ected servers came to a stop.
consequently millions of users were a ected by the memory leaks.
what makes memory leaks hard to detect and x at an early stage of development is their input and environmental sensitivity.
since the number of possible execution paths is potentially in nite covering every possible execution path and con guration is not plausible even for extensive in house testing.
as a result only the obvious leaks are discovered and xed during the testing stage.
the remaining leaks therefore are highly susceptible to the execution environment and are highly elusive.
the amazon memory leak incident is a clear demonstration of how tricky memory leaks are.
in response to the insidious nature of memory leaks researchers have proposed low overhead dynamic memory leak detection techniques.
these techniques are based on object staleness which represents how long an object remains unaccessed during program execution.
staleness based leak detection relies on an intuition that leaking objects must be stale whereas live objects with pending uses are unlikely to remain unaccessed for a long period of time.
in light of this the leak detection method assumes that memory leaks can be e ectively identi ed by using a proper staleness predicate which decides whether an object has leaked based on the staleness of the object.permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
icse may june hyderabad india copyright acm ... .
this work is an extension of staleness based memory leak detection in that it also leverages object staleness as an indication of memory leaks.
however this work obviates using object staleness as is and use of user de nable staleness predicates unlike previous research.
instead the memory leak detection framework proposed in this work is based on introspective reasoning about how leaking objects may behave or how stale the objects would be and on modeling the behavior using machine learning.
the key idea behind using machine learning is that a leaking object is discernible by observing the lifetimes of other similar objects.
that is an object can be regarded as having leaked when it accrues a high degree of staleness that is not observed from other supposedly similar objects i.e.
objects with the same allocation context.
to make matters easier once expected behavior of an object is known introspecting what may happen if the object has leaked is surprisingly easy i.e.
the object becomes only more and more stale.
this leads to a key insight of this work that memory leaks can be e ectively addressed by observing modeling the behavior of heap objects.
in light of this this paper presents a machine learningbased memory leak detection framework which is intended to be used throughout the development cycle.
the proposed framework enables observation of heap objects during the testing phase e.g.
regression testing and application of this knowledge during the production phase.
to this end the framework runs an application during testing with a memory access sampling to store the heap activity samples.
the framework then uses a trace simulation to replay the heap activities and to generate training examples.
by using these examples which also contain hypothetically injected leaks the framework constructs a model.
the framework uses this model during the production run to detect anomalous behavior indicating a memory leak.
the machine learning based framework has several advantages over the previous approach relying solely on staleness predicates.
first because the new approach is a framework it can be con gured to use any memory access sampling method for estimating staleness.
depending on the runtime requirement e.g.
a custom memory allocator is not allowed an alternative sampling method can be used with no change to the whole framework.
also compared to the previous approach the machine learning based approach by virtue of being application tailored o ers automation of staleness predicate determination.
moreover the new approach provides increased accuracy by incorporating allocation context information into leak detection.
the proposed framework was rst evaluated on synthetic leakage workloads which were generated by dynamically removing of deallocations during the execution of the spec cpu2006 benchmark suite.
the results on the workloads show that the proposed framework achieves the best accuracy permitted by staleness based leak detection and improves upon it by additionally incorporating allocation context into input features of the trained model.
for applications out of applications tried the addition of allocation context achieved meaningful accuracy improvement.
in addition to the synthetic leakage workloads the proposed framework was also tested on publicly available open source programs lighttpd .
.
and bash .
to demonstrate how the machine learning based leak detection framework can be applied to real world scenarios.
the empiricalevaluations show that the proposed framework achieves very accurate results and succeeds in correctly identifying the reported memory leaks of the tested programs.
contributions of this work include memory leak modeling using machine learning encoding scheme for object staleness for use in machine learning incorporation of allocation context into input features to improve accuracy of leak detection evaluation of the proposed method on synthetic leakage demonstration of the technique on real world examples lighttpd .
.
and bash .
.
the rest of this paper is structured as follows section lays out the motivation of this work.
section describes the machine learning based memory leak detection framework and section evaluates performance of the proposed framework on both synthetic leakage and on real world examples.
section discusses related work.
finally section concludes with the lessons learned from the work.
.
motivation memory leaks are distinct from any other program bugs in that they do not a ect program behavior directly by in uencing the execution path.
only accumulated leaks through a form of performance degradation are visible to the outside world.
due to this non interactivity with program semantics predicting what may happen to leaking objects is relatively easy i.e.
once memory objects become lost they cannot be accessed any more by a program.
in fact a recent generation of dynamic memory leak detection technique based on staleness tracking is an exploitation of this idea .
staleness based leak detection is an intuitive view of the memory leakage problem which attributes stale objects as the symptom of memory leaks.
the leak detection method is built around the idea that leaking objects must be necessarily stale whereas other objects that are currently in use are unlikely to be so.
leaking objects by de nition do not get accessed after they become lost to an application and become stale.
objects that are in use on the other hand are likely to be accessed frequently to utilize their memory space usage otherwise the objects bloat the memory footprint which wise programmers avoid through various ways .
figure is an illustration of how the staleness assumption relates to memory leak detection.
initially objects are allocated at a program point simultaneously.
at a later time the leak detection method takes a heap snapshot to query whether any object residing in memory has leaked.
object is deallocated prior to the heap snapshot point and is irrelevant to the leak query.
object is a normal object that has accrued a certain degree of staleness since its last access.
object unlike the previous objects becomes unreachable long before the snapshot point and is highly stale at the point.
using a staleness threshold that is higher than the degree of staleness of object and smaller than the degree of staleness of object staleness based leak detection can successfully identify the leaking object.815allocation staleness snapshot object object allocation became unreachable last accessed allocation object deallocation time leak?
leak?
staleness figure staleness assumption leaking objects are more likely to be stale than others.
in previous research this assumption is encoded as a userde nable predicate e.g.
the user may specify that an object has leaked if it remains idle for million memory accesses.
however relying on staleness predicates for accurate detection su ers from shortcomings of the approach.
for one thing appropriateness of a predicate heavily depends on behavior of an application.
hence the user has to nd a proper staleness predicate for every new application.
for another staleness estimation methods are usually based on sampling.
it is unrealistic to assume that the user can account for sampling factor in con guring a predicate.
to work around this this work takes inspiration from previous research on object lifetime prediction .
according to these works the lifetime of an object is strongly correlated with its allocation context.
since the staleness of an object is bounded by its lifetime object staleness is transitively correlated with allocation context.
for example it becomes more convincing that object in gure has leaked if the other objects are from the same allocation context and if they are supposed to be similar in their behavior.
with this in mind this work assumes that an object can be considered to have leaked with high probability if it shows a signi cantly high degree of staleness not observed from other similar objects which have the same allocation context.
to leverage this insight the leak detection framework proposed in this work observes behavior of heap objects from a representative run of an application.
the observed patterns are then modeled using supervised learning to construct a model which bases the decision whether an object has leaked or not on the similarity dissimilarity of objects in terms of their lifetimes and staleness behaviors.
with this approach the proposed framework replaces user de nable staleness predicates with a model based prediction.
a central question that needs to be addressed in using supervised learning is how to obtain training examples.
that is a supervised learning algorithm is essentially a process of iteratively re ning a model by using both positive and negative examples.
therefore to train a model of leaking objects and live objects with pending uses instances of both cases exhibiting their di erences has to be supplied to a learning algorithm.
however getting an instance of a leaking object is in general extremely di cult because no a priori information is given during testing of an application i.e.
an execution path that can trigger a potential memory leak is not yet identi ed at this stage otherwise it should have already been xed .
on the contrary a testing environment is supposed to be a representative usage scenario of an application.
hence the objects observable during testing show only how normal objects will should behave during production.
time allocation deallocation what if leak staleness last access snapshot figure what if leak staleness corresponds to the time interval between last access point of an object and the heap snapshot point.
the solution comes from the characteristic of memory leaks i.e.
they do not alter the execution of a program.
suppose an object is deallocated prior to a speci c time when the heap state is examined.
since the only side e ect of a memory leak is in object staleness the only thing that may have happened if the deallocated object had been lost is a higher degree of staleness of the object.
in this work this hypothetical staleness of an object is termed what if leak staleness gure .
using instances of hypothetically leaking objects with what if leak staleness as training examples therefore works as an approximate solution.
the model trained using instances of hypothetically leaking objects is a model of what may happen if an object has leaked.
as such the trained model is able to handle almost all possible leakage scenario by re ecting what if leak staleness of all objects observed during testing.
as long as the testing environment remains a representative of the production run the model must be an accurate classi er of leaking objects.
.
machine learning based leak detection framework this section presents the machine learning based memory leak detection framework.
as machine learning is the central part of the framework this section discusses background information on support vector machines svms and relevant details such as encoding object staleness as input features with respect to memory leak detection.
.
overview figure is an overview of the machine learning based memory leak detection framework.
in the envisioned usage scenario the framework is deployed during the testing phase and runs a target application with a memory access sampling method con gured by the user to generate heap activity traces.
in turn a trace simulator uses the generated traces and outputs training examples that are comprised of both non leaking objects at a point of a heap snapshot and hypothetically leaking objects with what if leak staleness.
with the training examples the framework trains multiple svm models to probe in uence of svm parameters.
among the constructed models the framework selects one which is expected to achieve the best accuracy according to its cross validation score.
during the production phase the selected svm model is applied to heap snapshots at which leak reports are requested.
.
trace simulation and training example generation the proposed framework relies on trace simulation to generate training examples from a speci c heap snapshot which the user marks as the representative of a target application816application machine learning algorithm feature vectors svm model leak report heap activity sampling trial run heap activity sampling production run heap activity traces heap activity replay hypothetical leaks feature vectors normal objects leak?
during the testing phase trace simulator during the production phase figure overview of the machine learning based memory leak detection framework.
trace simulator time event address 0x10 malloc 0x80 0x11 read 0x41 0x20 write 0x40 0x21 free 0x40 heap activity traces training examples heap state is represented as a search tree figure trace simulation replays sampled heap activities to obtain training examples for model construction.
heap state.
trace simulation provides extra exibility by enabling replay of the heap activities happened during a trial run of an application without running it again.
figure shows how the trace simulation works.
after a testing run of an application malloc free traces and sampled memory accesses are stored on the disk in chronological order.
the trace simulator reads these traces and replays the operations.
in so the trace simulator represents current heap state as a binary search tree where each node corresponds to a currently live object.
each event in the traces causes an operation on the tree.
a malloc event triggers an insertion of a new node into the tree.
likewise a free event triggers a deletion of the corresponding node from the tree.
each memory access forces the degree of staleness of the accessed object to .
for other unaccessed objects their degrees of staleness increase by the amount of time lapsed allocation snapshot object last access allocation object deallocation staleness what if leak staleness future access staleness figure training example generation by the trace simulator from a deallocated object and a live object.
between each operation.
for e cient mapping of an address to the corresponding object the tree data structure implements the range query with a typical binary search1.
figure shows how training examples are generated by the trace simulator given a heap snapshot point.
for an object that was deallocated prior to the snapshot point training examples are generated to account for facts the object is supposed to be deallocated prior to the snapshot point and if the object were leaking the what if leak staleness is the time interval between the last access of the object and the snapshot point.
for an object that is still live at the snapshot the trace simulator generates only one example to represent that an object less stale than the live object is not likely to have leaked.
although the training examples are generated at a speci c snapshot point choosing the point is not a hurdle to the user.
the kinds of applications staleness based leak detection targets are long running applications e.g.
servers or interactive games .
since such applications tend to perform repetitive computations over a long period of time taking a representative heap snapshot or specifying a snapshot point should come naturally to the user.
.
support vector machines among many supervised learning algorithms models this work selected svms as the key part of the framework.
svms are machine learning models that represent linearly separating hyperplanes.
for example when the dimensionality of an input data is a svm model corresponds to a line that divides the input space into mutually disjoint regions i.e.
data points that lie above the line are classi ed as one category whereas data points that lie below the line are classi ed as the other category.
applied to memory leak detection a svm model is conceptually a staleness predicate which bisects the input space representing staleness of allocated objects into regions respectively for leaking objects and innocent objects.
the choice of svms among various machine learning models is due to the computational capability of the models.
with more complex models such as arti cial neural networks this work found that there is a danger of learning a contradictory function i.e.
the trained model may identify a lowly stale object as non leaking a modestly stale object as leaking and a highly stale object as non leaking.
.
input features 1we implement the data structure using a specially modi ed redblack tree whose asymptotic complexities remain the same i.e.
o logn time .
this fast range search is essential for accelerating simulation.
a staleness allocation time b staleness only .
.
.
.
.
.
.
.
1ntstaleness ntallocation .
.
.
.
.
.
.
.
1ntstaleness ntallocation figure examples of memory leak indicator functions that can be expressed using the normalized input features.
if an object falls on the shaded area it is regarded as having leaked.
instead of using absolute staleness values the framework encodes staleness of an object as input features to normalize the input range to be from to .
without normalization or scaling input features in greater numeric ranges dominate those features in smaller numeric ranges and may jeopardize the overall accuracy .
the input features for an object oare as follows.
ntallocation o tallocation o tsnapshot ntstaleness o ifois deallocated prior to tsnapshot tsnapshot tlast accessed o tsnapshot tallocation o otherwise where tallocation o represents the allocation time of o andtlast accessed o represents the last access time of o2.
both features have the range of to and express relative allocation time or relative staleness as percentiles of the execution time or the lifetime of an object.
figure shows examples of memory leak indicator functions that can be expressed using the normalized input features.
due to the computational capability of svms these functions are linear classi ers.
the rst leak indicator function uses the input features.
for this function even a small ntstaleness is enough evidence for an object which was allocated early to be declared to have leaked.
for an object that was allocated late the indicator function is more lenient by permitting a higher degree of staleness.
on the other hand the second indicator function uses only the normalized staleness.
it declares an object to have leaked if ntstaleness is above some threshold regardless of the allocation time.
these functions roughly correspond to the staleness predicates used in the previous work .
using the normalized input features requires special care however because the features are relative to a snapshot point i.e.
tsnapshot .
since the absolute value of tsnapshot is di erent for each snapshot taken during production runs adjusting for that fact is necessary.
otherwise if tsnapshot of a heap snapshot is much larger than tsnapshot of the training examples the input features may look insigni cant to the model i.e.
even if an object has the same degree of staleness and the same lifetime the object may be seen as less stale old if it was observed during a longer run.
the purpose of using the relative input features is to normalize the ranges not to distort the scale.
to address this issue the framework performs time scaling using a polynomial function.
the scaling function maps a 2they are in units a con gured staleness estimation method uses e.g.
the number of memory accesses observed.
.
10snapshot snapshot snapshot snapshot 10figure scaling execution time of production runs using polynomials for the heap snapshots.
time of an event during a production run to a likely time when the same event might have happened during a trial run.
the polynomial function is de ned as r tsnapshot ttrain snapshot ts t ttrain snapshot t tsnapshot r note that the slope of tsattsnapshot is .
therefore for events that happened near the heap snapshot point the scaling factor is i.e.
original scale is preserved.
on the other hand for events that happened much earlier the scaling factor is very small which makes the events look as if they happened very early.
figure depicts a situation where snapshots are taken during a production run.
for the heap access events that happened along the x axis their respective times of occurrences are scaled by the respective polynomial for each snapshot.
for snapshot the original scale is preserved because the execution time is equal to that of the trial run.
on the other hand moderate compaction of time occurs for snapshot and as the execution time becomes longer.
for snapshot since the execution time is times longer the time is highly compacted with some level of scale preservation near the end.
to express similarity of objects based on allocation context the framework uses allocation site and allocation size as additional input features.
much the same way as in encoding object staleness allocation size is normalized using the maximum object size observed during a trial run.
nsize o size o max obj2objects size obj encoding allocation site on the other hand uses a vector of binary variables.
formally the vector nsite o is de ned as nsite o site o site o site n o site n o site k o if ois allocated from allocation site k otherwise .
polynomial kernel inclusion of allocation context into input features is based on the assumption that the context has a correlation with the estimated staleness.
however using the input features as they are does not express this relationship i.e.
the features are independent.
to handle this issue the framework uses a kernel trick which conceptually maps the input features into a higher dimension where the interaction can be expressed.818the polynomial kernel is a commonly used method in conjunction with svms.
when input data are vectors in r2 the polynomial kernel with degree maps an input vector to a vector in r3using the following mapping .
x x2 x1x2 x2 a since nsite o is a vector of binary variables applying the polynomial kernel results in a conceptual feature vector where most of the elements are zero and only the terms representing normalized staleness or the terms expressing interaction between allocation context of oand staleness of oare given non zero values.
using the polynomial kernel is purely for the sake of computational e ciency.
instead of using the kernel trick the input features can be explicitly mapped into a higher dimension.
compared to explicit mapping the kernel trick has one drawback.
terms like x2 1introduced by the polynomial kernel distort the shape of the indicator functions i.e.
they become curves not lines.
however the framework prefers using the kernel trick in favor of reducing the computational load which is expensive even with the trick.
.
cross validation and grid search svms have a handful of parameters that in uence the accuracy of the trained model.
one recommended way to determine the parameters is to use cross validation and gridsearch .
given a speci c set of parameters cross validation computes a predicted accuracy of a model trained using the parameters.
in v fold cross validation the training set is rst divided into vsubsets of equal size.
subsequently one subset is used as a testing set while the classi er is trained on the other subsets.
the process takes turns until all inputs are used as a testing instance once.
the averaged accuracy obtained thus is the expected accuracy of a model given a set of svm parameters.
the cross validation procedure has a bene t of preventing the over tting problem which occurs when a model ts itself to the training data too much instead of learning the general trend in the data.
to provide a set of model parameters for cross validation the framework uses the grid search.
the grid search is a naive method that enumerates all the possible sets of parameters given the ranges of individual parameters.
although there are several advanced methods that may save computation cost the methods are not necessarily more accurate.
additionally the grid search has a bene t of being highly amenable to parallelization.
.
evaluation this section discusses the design and results of the experiments that were performed to test the machine learningbased memory leak detection framework.
.
accuracy metrics to measure and compare the accuracy of the proposed method this work uses the classi cation accuracy as well as the precision and recall that are the standard metrics capturing performance of an information retrieval system.
in the context of memory leak detection they are de ned as follows.accuracy jcorrectly identified objects j jobjectsj precision jreported leaks realleaksj jreported leaksj recall jreported leaks realleaks jrealleaksj since the classi cation accuracy is the metric used during the cross validation it is the primary metric in comparing the accuracy of the proposed framework.
the precision and recall are of special importance because the leak report is given to the user who has to examine the cause.
the rst priority of memory leak detectors is to provide precision while still identifying many true instances of memory leaks.
.
implementation to evaluate the performance of the proposed method this work implemented adaptive burst tracing abt .
abt is the memory access sampling method proposed by chilimbi and hauswirth in their work called swat which pioneered the category of staleness based memory leak detection .
however unlike the original implementation abt was implemented using the llvm infrastructure as an ir pass and it was modi ed to write a log of sampled heap activities onto the disk for both trial and production runs .
the abt implementation used the default con guration for swat which was reported to have less than of runtime overhead .
to train svm models this work used libsvm with fold cross validation.
.
synthetic leakage generally evaluating accuracy of staleness based leak detection is a daunting task for reasons.
for one thing there is no standard set of applications containing memory leaks with which the accuracy can be measured and compared.
for another a staleness predicate can be applied on a set of allocated objects at any time during the execution.
this means the accuracy is snapshot point dependent.
this work addresses the rst problem by synthetically injecting leaks during the execution of spec cpu2006 benchmarks to generate leakage workloads.
among the total c c application in the benchmark suite this work selected applications based on the number of objects during the application execution as well as the object deallocation pattern in order to prevent misleading results.
additionally this work also took into consideration heap snapshot size for ease of training and experiment.
in generating leakage workloads the leakage percentage was an important factor in designing such workloads.
too much leakage tends to in ate the precision of the resulting detection.
if the portion of the leaks is too large compared to the portion of innocent objects at a program point blindly identifying an object as leaking becomes probabilistically a correct decision e.g.
if there are leaking objects and innocent object even an inaccurate leak detector which identi es every object as leaking attains a very high precision.
on the other hand workloads with too little leakage also end up with a bias result.
for example if there is only leaking object and innocent objects even an inaccurate81900.
.
.
.
.2accuracy higher is better manual base model full modelfigure accuracy of the machine learning based memory leak detection framework on the synthetic leakage workloads.
leak detector which identi es no object as leaking attains a high accuracy.
thus both very high and very low percentage of leaks endangers evaluation of leak detection by producing biased results.
considering such impact of the leak percentage on the accuracy of a leak detection method we decided to inject leaks by randomly removing of the total deallocations for each application.
to address the second problem this work xed of the execution as the snapshot point to account for the fact that the applications eventually terminate.
unlike long running applications such as server applications that are the targets of staleness based leak detection the benchmark applications pass through several program phases which may also interact with model training.
using a model that trained on examples gathered during a speci c program phase may be inadequate to detect leaks during a di erent program phase.
in consideration of this this work gathered the heap snapshots from the of the execution points from the runs of the applications on train inputs and applied the trained model on the corresponding execution point from the runs of the applications on reference inputs.
to see how well the machine learning based framework works this work additionally conducted experiments on the synthetic leakage workloads to obtain the optimal accuracy achievable by using the previously used approach i.e.
we used several staleness predicates based only on object staleness and picked the highest accuracy attained.
moreover this work also constructed base models which use only the normalized allocation time and the normalized staleness to see whether leveraging similarity of objects via allocation context improves the detect accuracy.
the full model as opposed to the base model was trained on the training data including allocation context.
figure is the resulting accuracy of using the machine learning based framework.
it shows the classi cation accuracies of the methods.
except for h264ref the accuracy of the base model matches that of the manual staleness predicate selection.
this result matches the intuition that the accuracy of a machine learning model is only as good as the predictive power of the input features of the model.
close inspection of the result on h264ref revealed that the trial run on train input was not representative of the production run on reference input i.e.
even the best staleness predicates were di erent for the runs.
this resulted in a biased model and impacted the leak detection accuracy.
.
.
.
.
.2precision higher is better base model full model .
.
.
.
.2recall higher is better base model full modelfigure the precision and recall of the machine learning based memory leak detection framework on the synthetic leakage workloads.
compared to the base model the full model achieved better results in applications i.e.
milc h264ref gcc libquantum.
these results are a evidence showing usefulness of leveraging allocation context in memory leak detection.
figure shows the resulting precision and recall of the machine learning based leak detection framework.
notably the recalls are almost perfect for both the base and full models.
however due to the staleness over approximation of abt as discussed in the precision is limited for h264ref andastar .
the objects allocated in these applications tend to have relatively longer lifetime compared to other applications.
such objects impacted the accuracy.
compared to the base model the full model achieves better precision by incorporating allocation context as shown in milc h264ref libquantum .
the drastic precision gain in libquantum is because the heap snapshot contained very few leaking objects and the base model reported a few additional false positives.
.
case studies to demonstrate how the proposed memory leak detection technique applies to real world leakage detection problem we experimented the technique on open source applications lighttpd andbash.
these applications have reported memory leaks that can be triggered only through a certain con guration input.
we compiled the applications statically to obtain the allocation context information for their libraries.
the alternative is to instrument the dynamic loader ld.so so that it can leave the information in which the libraries are loaded .
and we used synthesized inputs for both applications.
the inputs for model construction exercise relevant functionalities of the applications but do not introduce memory leaks.
on the other hand the inputs for testing the proposed technique periodically trigger execu p c r e f r e e l i s t hctx h a n d l e r c t x i n i t con plugi n ctx hctx 4i f rule once hctx s t a t e rewrite state finished 7return handler comeback mod rewrite.c figure lighttpd mod rewrite memory leak.
tion paths that cause memory leaks.
all features described in section were used for the experiments.
.
.
lighttpd .
.
lighttpd is a popular web server that is optimized for low memory footprint .
in version .
.
lighttpd has a memory leak that can be triggered by a tricky con guration using mod rewrite .
figure is the source code related to the memory leak.
at line mod rewrite creates a memory object that stores current context information.
reference to the temporary context object is stored at line .
however this code region can be visited multiple times during a processing of a request if url.rewrite repeat rule intervenes.
when visited again mod rewrite creates another context object that is redundant at line and overwrites reference to the old context object by a reference to the newly created one.
since the reference to the old context object is lost by the rewriting this results in a memory leak.
one plausible explanation why the developers failed to see the leak inducing execution path is because mod rewrite is a plug in that is usually executed only once per service request.
so the developers may have assumed that the lifetime of a handler context object is nished after processing amod rewrite rule and have also assumed that the object will be deallocated with a connection object when the connection is reset.
to catch the leaks we ran lighttpd for about an hour and let it process random service requests during the time frame for generation of training examples.
among the requests we included urls that would be processed by mod rewrite .
however no leak inducing urls were given to lighttpd .
so the heap activities observed contained only the expected operations.
on the other hand we triggered the leak inducing execution path during the testing run.
only out of every requests contained a harmful url.
figure is the accuracy achieved by the machine learningbased detection framework.
the accuracy measures are averages geometric means of the metrics at heap snapshots that were taken after lighttpd had been run for a long enough amount of time i.e.
longer than the trial run.
the precision is above and almost perfect recall was achieved without using the time scaling the precision drops by about .
obviously the machine learning based memory leak detection framework captured the developer s assumption by learning that the lifetime of a handler context must be short and should not span multiple service requests.
grouping the reported objects by allocation context and ranking the allocation contexts revealed the problematic allocation site line as the potential cause of the mismatch with the trained model.
t o f r e e null 2i f i n p u t s t r i n g 3f t1 i n p u t s t r i n g t get word from string i n p u t s t r i n g .
.
.
i f i n p u t s t r i n g t o f r e e i n p u t s t r i n g t elsef i n p u t s t r i n g s t r i p t r a i l i n g i f sw h i t e s p a c e t1 .
.
.
13g 14g 16i f saw escape 17f t de q uote s tr ing i n p u t s t r i n g var b i n d r e a d v a r i a b l e .
.
.
x f r e e t 22g 23else var b i n d r e a d v a r i a b l e .
.
.
i n p u t s t r i n g .
.
.
free t o f r e e read.def figure bash read built in memory leak.
.
.
bash .
bash .
has a memory leak in the read built in when the number of elds read from an input is not the same as the number of variables passed as arguments to the built in .
figure is the relevant part of the source code that binds the last variable with the remaining input string.
at line a temporary memory object is created inside getword from string and input string is advanced to the next character in the original input string to check for the end.
if the next character is null the reference to the temporary object is stored in tofree line and the object gets deallocated at line .
however if the next word is not null line the temporary object tgets lost.
to model the staleness pattern of objects created at line we ran a bash script that exercises the readbuilt in by taking synthesized inputs.
the inputs for training were all wellformed and had matching number of elds as the number ofreadvariables.
thus no memory leak was observed during the trial run.
during the testing run we supplied di erent inputs to a bash script.
the inputs were crafted so that of them contain more elds than the number of read variables to trigger the execution path that causes memory leaks.
figure is the accuracy of the machine learning based leak detection.
the achieved precision is and the recall is without using the time scaling the recall drops by about .
although the detection accuracy is high enough because the allocation site of the leaked object is insidegetword from string tracking down the cause involved more labor.
manual inspection of the source code revealed82100.
.
.
.
lighttpd .
.
bash .0precision higher is better .
.
.
.
lighttpd .
.
bash .0recall higher is better figure accuracy of the machine learning based memory leak detection on real world examples.
that among the callers of getword from string only the part listed in gure has a potential memory leak.
.
limitations one limiting factor of the machine learning based memory leak detection framework is the training time.
during the experiments the longest training time took several days.
there are several unexplored solutions to this.
the rst one is to parallelize the learning algorithms .
additionally sampling training examples to reduce the size of the training data is also a viable option.
lastly instead of the naive grid search a guided search through the space of svm parameters may be used to reduce the training time.
.
related work there is a large body of existing research addressing the issue of memory leaks.
memory leaks are of two types unreachable memory i.e.