whose fault automatic developer to fault assignment through fault localization francisco servant department of informatics university of california irvine irvine ca u.s.a. fservant ics.uci.edujames a. jones department of informatics university of california irvine irvine ca u.s.a. jajones ics.uci.edu abstract this paper describes a new technique which automatically selects the most appropriate developers for fixing the fault represented by a failing test case and provides a diagnosis of where to look for the fault.
this technique works by incorporating three key components fault localization to inform locations whose execution correlate with failure history mining to inform which developers edited each line of code and when and expertise assignment to map locations to developers.
to our knowledge the technique is the first to assign developers to execution failures without the need for textual bug reports.
we implement this technique in our tool whose fault and describe an experiment where we utilize a large open source project to determine the frequency in which our tool suggests an assignment to the actual developer who fixed the fault.
our results show that of the time whose fault produced the same developer that actually fixed the fault within the top three suggestions.
we also show that our technique improved by a difference between and the results of a baseline technique.
finally we explore the influence of each of the three components of our technique over its results and compare our expertise algorithm against an existing expertise assessment technique and find that our algorithm provides greater accuracy by up to .
keywords developer assignment fault localization mining software repositories expertise assignment i. i ntroduction today when a test suite is run and failures are found a project manager or someone equally familiar with the software attempts to examine the symptoms of the failures and assign them to developers to find and fix the faults causing them.
each developer assignment is made utilizing the symptoms of failure mental inference by the assigner of which functionality was likely to have been the root cause of that symptom and an experiential knowledge of which developers are responsible for which features.
similarly in the process of bug triage faults are represented by bug reports in a bug tracking system.
in this domain we can also see evidence of the abundance of errors in the manual assignment of developers to faults.
often a bug report is assigned and then re assigned sometimes repeatedly until it finds the proper developer that has the necessary expertise to understand find and fix the problem .we present in this paper a technique that automatically chooses the developers to fix failures represented by erroneous executions.
utilizing such a technique we anticipate a better assignment sooner thus saving the project manager s time in the assignment.
we also anticipate reducing the amount of reassignments necessary saving developer time.
some existing techniques for developer assignment focus on finding the right developer for resolving a bug report e.g.
.
however bettenburg et al.
performed a study of developers and found that bug reports are often incomplete and poorly written.
although in many cases bug reports are adequate sources for performing developer assignments additional techniques are warranted when they do not exist or are inadequate.
moreover even if bug reports exist for a fault the mapping from existing failures to bug reports may not have been identified.
irrespective of the absence or presence of quality bug reports testing failures or erroneous executions often provide the first evidence of program faults that need to be fixed.
testing failures can exist for extended intervals spanning many revisions of the program and may or may not have a related bug report.
such failures can and are often directly assigned to developers for them to fix.
however simple solutions such as assigning the failure fixing tasks to developers who most recently committed changes are often not appropriate.
as evidence researchers have developed techniques which apply complex algorithms to finding the right developer for answering questions about source code artifacts e.g.
.
in this paper we present w hose fault a technique which automatically chooses expert developers to fix the fault represented by a failing test case and provides them with a diagnosis of the location of the fault causing the failure.
such a technique provides a series of improvements over existing techniques that assign developers to bug reports or source code artifacts in the event of a test case failure the test case can be directly assigned to the expert developer without the need to write a detailed and unambiguous bug report describing the failure it provides a diagnosis for where faults may reside in the code which brings an additional saving of developer time due to more directed fault finding tasks it assesses developer expertise not only in the faulty code but also in other areas of the source code which interact with it in order capture knowledge about the context of the fault.
our approach leverages source code history that can be found in revision control systems and fault localization techniques that point to locations in the source code that are likely faulty.
from the source code history we capture every change to each line of code the type of that change the time of the change and who made the change.
our technique couples this history information with the diagnosis information about the location of faults in the source code from automated fault localization techniques.
our expertise assessment algorithm combines these two pieces of information to provide a ranked list of developers in terms of expertise in these locations.
suggesting a list of ranked developers instead of a single expert developer is a common practice in expertise finding techniques and it has a series of advantages.
the most obvious one is that if the person in the first position of the rank is unavailable we can assign the next person in the list for the resolution of the fault.
additionally we envision a benefit of automatically suggesting collaborators or mentors for fixing faults.
examples of such situations include faults that involve cross component logic or the introduction of a new developer who will need guidance from an existing and knowledgeable superior.
the main contributions of this paper are a novel approach for developer to test case assignment that uses the results of fault localization techniques and the source code history of a software system.
this approach is applicable for use with any faultlocalization technique and any source code historymining technique.
to our knowledge our technique is the first to assign developers to failing test cases.
experimental designs that enable the evaluation of future developer to fault assignment techniques and their constituent components.
the first experiment evaluates the effectiveness of the automated technique to predict assignments made by real developers.
the second experiment compares the effectiveness of the automated technique to a simple technique that only considers the distribution of commits over the source code.
the last experiment gauges the influence of the separate components that comprise the technique under evaluation.
these experiments evaluate the effectiveness of the automated technique to predict assignments made by real developers.
an implementation of our approach w hose fault and its evaluation which demonstrates that it can effectively recreate historical assignment choices made by real developers shows that w hose fault improves the results of a simple technique which onlyconsiders the distribution of commits over the source code and reveals the influence of the three component techniques on our experimental results.
in this evaluation we show that our expertise algorithm improves upon an existing expertise assessment technique.
ii.
r elated work to provide the necessary background to motivate and explain our technique we first overview three areas of related work source history mining section ii a expertisefinding techniques section ii b and automated faultlocalization techniques section ii c .
a. source history mining in order to track the history of individual lines of code zimmerman et al.
proposed annotation graphs .
annotation graphs are multipartite graphs in which each part is represented as a column of nodes which represent lines of code.
edges are drawn between line nodes to represent their evolution.
however regions of contiguously changed lines are only mapped at that region level.
these regions of difference introduce inaccuracies and imprecision.
several researchers e.g.
have investigated and proposed techniques that provide a finergrained mapping of lines of code between consecutive revisions.
we use such techniques in this work to provide a precise history of the evolution of each line.
b. expertise finding techniques existing work in the area of automatically determining developer expertise generally falls into two categories those that leverage the natural language bug reports in a bugtracking system to assign a developer and those that can identify the most knowledgeable developer given a location in the source code.
bug report expertise the existing work that assigns a developer to a bug report e.g.
utilizes the natural language terms from the bug reports.
the history of which developers were assigned to past bug reports that also used the same terms as in a current bug report description is used to guide the selection.
such approaches do not use any information about the source code or test case behavior other than the description of the input and symptoms described inside the bug report.
a number of researchers have investigated this area of using natural language analyses e.g.
.
regarding effectiveness anvik et al.
achieved a precision between and when suggesting up to three developers for a bug report and jeong et al.
improved these results with additional historical bug reassignment information to achieve precision of up to when suggesting three developers.
other similar but divergent techniques include matter et al.
who assign developers to bug reports based on the text of the bug reportsand developer supplied expertise profiles kagdi et al who link change requests to a source code location using natural language similarities between the change requests and source code and then find expertise based on those locations and baysal et al.
who combines several factors such as natural language matching and current developer workload to inform their recommendations.
these techniques also rely upon past and current bug reports to exist to be well written and to contain enough description to perform a sufficient matching.
moreover even with well written and descriptive bug reports these techniques are sensitive to word and phrasing choices made by the various bug report contributors.
one bug report contributor may refer to terms such as gui while others may refer to user interface presentation and window.
additionally these approaches are incapable of providing suggestions of where in the code the fault may reside.
source code expertise another area of existing work automatically selects a developer who has expertise in a specific area of the code.
in these techniques a person chooses a location in the source code and the technique is able to assess the developer who has the most expertise for that location.
this work mines the history of the source code capturing all changes to the system and the developers who make those changes.
a number of researchers have investigated this area of mapping expert developers to components of source code e.g.
.
each such research endeavor has examined different factors in mapping developers to components.
for example mcdonald and ackerman suggest that the developer who most recently changed a file is the expert for the entire file and mockus and herbsleb suggest that the expert is the developer who made most changes to a file.
another example includes fritz et al.
who additionally consider factors such as frequency of reading the code.
if such techniques were to be applied to finding the most appropriate developer to fix a fault they would require that a person can determine which parts of the program are faulty which is a task that itself requires specific expertise.
moreover the granularity of such approaches is typically at the file class or method level tracking developer expertise by method or by file instead of tracking it for a set of lines of code which could be located in different parts of the system.
also these approaches can only point to the developers that have expertise in one particular area of code at a time that is they do not take a number of locations that may be related in some way and determine the developer who has the best expertise across this group.
for example for a single location developer a may have the most expertise and for another location developer b may have the most expertise.
however if we are seeking the developer who has the most expertise in both of those locations developer c may be the best choice.
localizefaultsminehistoryassess expertisetest casessource locationsinstructionlineagesource code repository developersfigure .
process diagram of our technique.
c. automatic fault localization to provide an approach that has benefits of each of these classes of expertise assessment while ameliorating their limitations we utilize automated fault localization techniques.
specifically in this presentation we will leverage statistical coverage based techniques.
however any fault localization techniques can be utilized as long as it locates areas in the code that are determined to be likely to cause failure.
to inform the locations in the program where to query developer expertise we will utilize a statistical coveragebased fault localization technique.
a number of such techniques have been proposed by researchers e.g.
.
these techniques utilize information gathered about the internal behavior of the software.
for example one such technique t arantula analyzed the correlation between instruction execution and failing test cases.
the intuition is that instructions that are executed primarily by failing test cases are more suspicious of being the fault causing those failures than instructions that are primarily executed by passing test cases.
t arantula assigns a suspiciousness score to each instruction in the program according to this correlation.
such a statistical coverage based fault localization technique is used to identify areas of high suspiciousness that we can query for developer expertise.
in the next section we describe how the results from fault localization techniques can be utilized to identify expertise in finding developers best suited for the debugging task.
iii.
a pproach in order to assign a developer to a failing test case we follow three main steps to produce a list of the most suited developers to fix the fault represented by it.
first we utilize a fault localization technique to determine the likely locations of the fault.
in parallel we mine the sourcecode repository to leverage its history determining for every individual instruction its lineage and all the developers that were instrumental in its development.
utilizing the results of the fault localization technique we weigh both the suspiciousness of each instruction and the developers who have knowledge about them to produce a list ofcandidate developers with a measure of expertise.
this list of developers is then suggested as the best candidates to fix the fault represented by the test case.
figure shows these steps.
the following sections describe them in more detail.
a. localize faults the first step of our approach consists of utilizing a faultlocalization technique to determine the areas of the source code that will most likely need to be modified in order to fix the fault.
any fault localization technique can be utilized to assess which locations are suspicious of causing failures.
we can utilize approaches that identify a subset of the program such as a slice or approaches that assess a degree of suspiciousness for each component.
in our implementation and evaluation in section iv we use t arantula .
as a consequence of using a statistical coverage based fault localization technique like t arantula our approach takes as its input the coverage of a test case or test cases that tests for a fault.
the coverage of a test case contains all the locations in the source code that were executed by the test case.
we define a location in the source code as an executable line of code.
once we have collected the coverage for all test cases we can use the fault localization technique.
we use as input the coverage of the failing test case together with the coverage of all the passing test cases in the test suite.
as a result the fault localization technique will return a suspiciousness value for all the locations in the source code.
this suspiciousness value represents how likely a location is to contain the fault these values range from for the lowest suspiciousness to for the highest suspiciousness.
the set of all locations in the source code with their suspiciousness value is one of the inputs for the last step of our approach.
b. mine history as well as the localization of the fault our approach uses the history of the source code in order to find the most suited developers to fix a fault.
therefore we also mine the entire history of a software project to determine the lineage of each individual instruction in the code.
previous expertise finding projects e.g.
suggest experts for software artifacts at the methodlevel or higher levels.
we however choose a fine grained level for mining since it directly maps to the results of our fault localization technique and as such is likely to give a more precise expertise assessment.
therefore we mine changes in the source code at the individual instruction level by building a history graph for the software project.
a history graph is a variation of an annotation graph .
an annotation graph is a multipartite graph in which each part represents a revision of a file in the source code and each node in a part represents a line of code in that revision.
in a history graph each node is labeled with the actionthat produced it and is linked to only one other node in the previous revision.
in order to build the history graph we first collect the meta data including authorship and commit time for all of the revisions that the repository contains for each file belonging to the software project.
we then compare each revision of each file with its previous revision in time by using the annotate and diff features of the source code repository.
we use the results of this comparison to map a line in each specific revision of a file with its corresponding line in the previous revision of the file.
however the output of diff is not a perfect line to line correlation.
since diff uses a textual difference algorithm it returns regions of difference called hunks .
figure shows a modification hunk in which line was changed to become the region that now spans lines to .
2c2 i j i x i j figure .
modification hunk.
by observing figure we realize that line in the old revision was actually modified to become line in the new revision and that lines and were added.
in order to provide this finer grain mapping for lines between two revisions we use the line mapping technique proposed by williams and spacco for all modification hunks.
we first obtain each possible pair of one line in the old revision and one line in the new revision.
then we assign a weight to each of the pairs equal to their levenshtein distance .
after that we apply the kuhn munkres algorithm to obtain the set of pairs with optimal global similarity score.
finally we use a threshold over the levenshtein distance of each of the selected pairs.
previous work e.g.
has determined the adequacy of a standard threshold of for this purpose which we also use.
if the distance of an assigned pair is lower than the threshold we classify them as a changed line.
if their distance is higher than than the threshold we classify the old line as being deleted and the new one as being added.
figure illustrates an example of a small history graph.
c. assess expertise in the third step of our approach we combine the suspiciousness information described in section iii a with the history of the source code described in section iii b in order to infer the most experienced developers in the suspicious locations of the code.
in order to determine the expertise of a developer in a particular line of code we consider two aspects that we ............version .0sallyversion .1sallyversion .2edversion .3sallylegendevolution of an instructionunchanged instructionchanged instructionadded instructiondeleted instructionpublic void run 2public void run i 2public void run j 2public void run j i i x i j 35figure .
example of a small history graph.
estimate to be the most important amount of changes and recency of changes for that developer in that line of code.
our intuition is that as a developer makes more changes to a line of code he or she becomes more familiar with it.
this intuition has already been expressed and verified by other authors .
additionally we consider that developers will be most familiar with their most recent changes and less familiar with less recent changes.
this intuition has also been previously identified by other authors .
we then define the recency of a change cas such recency c today datec today beginning date datec beginning date today beginning date in formula the recency of the change cis the amount of time that has passed since the beginning of the software project until the change was committed compared to the total age of the software project.
we calculate the recency metric as a distance from the beginning of the project as opposed to a distance from now so that an increase in time from the change to now is reflected as a decrease in the recency metric.
the recency value ranges from to .
figure depicts the recency calculation.
timerecency010.8beginning ofsoftware projectchangetoday figure .
depiction of the calculation of the recency metric.
the value of the recency metric increases as the difference of the change date and now decreases.once recency has been defined formula represents the expertise of a developer for fixing a fault.
we define the expertise value of a developer dfor fixing a fault as the sum of the recency of all changes cd lmade by that developer dto any suspicious line lin the source code multiplied by the suspiciousness value of that line.
in this formula we reward the suspiciousness of lines recency of changes and amount of changes.
as any of these variables increases the total expertise value of a developer also increases.
expertise d mx cd l 1nx l 1recency cd l suspiciousness l we implement our expertise algorithm by first applying history slicing to the set of suspicious lines returned by the fault localization step.
thus we walk the history graph that we built during the mining of the source code repository obtaining all the equivalent lines to the suspicious lines in all the past versions of the software project in which they were modified.
finally we apply formula to this output in order to obtain an expertise score for each developer that made changes to the suspicious lines.
this expertise metric produces values for each developer for a given program and fault localization diagnosis.
the value of this metric for a specific developer is only meaningful relative to the values of the metric for the other developers we only use it as a means to compare the relative expertise of all developers for a specific bug.
once formula has been applied for all developers they are sorted from highest to lowest expertise value.
the developer in position of the list is the most appropriate to be in charge of fixing the fault according to our technique.
the next most appropriate developer is in position and so on.
developers that did not make any changes to the suspicious lines of code and therefore have no expertise value are not included in the list.iv.
e valuation to evaluate our technique for automatic developer assignment to test cases we implemented it in our tool whose fault and conducted two experiments over a realworld project.
with these experiments our goal is to answer the following research questions rq1 how often does the automatic assignment of developers to test cases find the right developer and in which position?
rq2 how much does our automatic technique improve over a na ve approach?
rq3 how does each component of our automatic technique affect its results?
a. experimental subject in order to perform our evaluation we need access to a software project s test cases and source code repository.
first we need a test suite to execute for any revision of the software and identify the test cases that fail.
second we need a source code repository to mine the entire history of the project and obtain the history of every line of source code as described in section iii b. we selected the aspectj open source project as the subject for our experiments since it provides access to both of the previously mentioned resources.
aspectj is an aspectoriented extension to the java programming language.
we mined the source code history of aspectj and extracted revisions which had been created over .
years of development.
we also extracted aspectj s test suite which contained an average of test cases per revision.
to assist in the selection compilation and execution of test cases we utilized the ibugs project .
the ibugs project processes the bug tracking system and source code repository of a software project in order to identify the source code changes that fixed a bug and the test cases that were committed with those changes.
its original purpose is to determine the changes that fixed bugs in real software projects in order to provide an oracle to benchmark faultlocalization techniques.
b. experimental setup in order to perform our experiments we need a set of failing test cases and an oracle to tell us who is the developer with the most expertise to fix the fault represented by each of them.
we select this set of failing test cases by exploring the source code repository of aspectj and identifying committed changes that fixed the fault represented by a test case.
we refer to these changes as the fixfor that failing test case.
we use the developer who committed the fixto the source code repository as the oracle for who is the developer with most expertise to fix the fault represented by a test case.
we refer to this developer as the expert for the test case.
in order to identify committed changes that fix a failing test case we used the ibugs project.
ibugs reports bugsfor aspectj and the changes committed to its source code repository that fixed each of these bugs.
ibugs also provides the revision of the source code of aspectj just before prefix and just after post fix each of the bug fixing changes was committed to the source code repository.
these bugfixing changes often include a test case to test for the bug in the future.
therefore we can expect some failing test cases in a pre fix revision to become passing test cases in the post fix revision.
we selected our set of failing test cases from the pre fix revisions of aspectj provided by ibugs.
we copy the test suite from each of the post fix revisions to their corresponding pre fix revision.
then we execute the test suite in both the pre fix and post fix revisions.
finally we select all test cases that fail in the pre fix revision and pass in the post fix revision as candidate test cases for our experiments.
the expert for each of these test cases is the developer who committed the bug fixing changes for their corresponding pre fix revision.
out of the pre fix revisions reported by ibugs contained test cases that fail in the pre fix revision and pass in the post fix revision.
in order to be able to use a fault localization technique we apply a source code instrumenter cobertura over each compiled pre fix revision.
this is so that we can capture the coverage of each test case when running the test suite.
we performed the instrumentation over the pre fix revisions that contained test cases that fail in the pre fix revision and pass in the post fix revision.
in of these pre fix revisions the instrumentation process failed for the files involved in the fix for the failing test cases.
this happened because the fixes were contained in a library that had been compiled without debugging information and the source code was not available.
this situation is similar to using test cases that test third party libraries for which neither debugging information or source code is available.
our approach is not applicable to such situations because if the faulty source code were not available to the parties performing the instrumentation they could neither instrument nor fix the fault.
as a consequence we discarded these revisions.
finally we executed the test suite on the remaining pre fix revisions for which we could successfully instrument the files involved in the fix.
we also executed the test suite on their corresponding post fix revision.
some failing test cases did not produce coverage information because they crashed as soon as their execution started.
we cannot provide a developer recommendation for these test cases since they practically do not interact with the source code of the application.
thus we selected as candidate test cases for our experiments a total of test cases which failed in one of the pre fix revisions passed in its corresponding post fix revision and produced some coverage information.
for each individual candidate failing test cases we applied a fault localization technique to its coverage plus that of allthe passing test cases.
as a result of this step we obtained for each individual failing test case a suspiciousness value for each line of code.
in parallel we mined the sourcecode history of the project as explained in section iii b and obtained the history of each line of code.
afterward we ran our expertise algorithm and stored the expertise value for each candidate developer for each test case.
then we ranked the list of developers in terms of expertise values from highest to lowest which was our prediction for each failing test case.
if the actual developer who fixed the failing test case the author that committed the changes that fixed it was within the top ranked developers we considered our approach successful.
c. experimental variables the primary object of our experiments is to assess the degree to which our technique automatically selects the developer that actually fixed a failing test case according to historical record.
to this end we assess the position in the ranked list of developers for the actual developer that wrote and committed the changes that fixed the failing test case.
a position of in our ranked list indicates that our technique precisely chose the correct developer according to the commit log of the version control system.
we also evaluate the further positions in the ranked list a position of in our ranked list indicates that we chose the correct developer within the top two positions and so on.
we note here that our assessment of the correct developer is based on the actual developer that wrote and committed the changes that fixed the failing test case which is an undoubtedly subjective and imperfect measure of true expertise.
the developer with the greatest expertise in a particular failing test case could have been busy working on other tasks on vacation or otherwise unavailable.
in addition other developers could have volunteered to fix failing test cases to learn more about the project.
nonetheless we believe using the historical artifact as our oracle for the correct assignment is a reasonable one as it was chosen by the people most knowledgeable about the project the actual developers.
d. experiment for our first experiment we mined the source code history at the line level using the levenshtein distance for our linemapping technique.
then we used the t arantula faultlocalization technique and our expertise algorithm for the test cases that we identified as candidates.
after running the experiment we obtain the results in figure .
these results show that w hose fault identifies the expert in the first position of its ranked list for .
of the failing test cases that we studied.
this result increases up to .
when we consider the first two positions of the suggestions and up to .
when we consider the first three.
we considered as candidate developers all developers who had committed changes to the source code repository accuracy .
.
.
.
.
.
figure .
effectiveness of our developer assignment technique.
before the date of the pre fix revision of the test case considered.
for each test case considered w hose fault considered an average of .
candidate developers with a standard deviation of .
.
to put our results into context consider that random selection over all active developers would yield results of only about and i.e.
the probability of randomly choosing the one correct developer out of an average of .
developers is and so on .
thus the results of w hose fault provide a considerable improvement over a random selection.
note that for .
of the considered test cases the expert was not found inside the ranked list of developers.
in these cases the person who fixed the failing test case had not committed any changes to any of the suspicious lines in the past.
a possible explanation could be that our fault localization technique was not successful at finding the context of the fault.
another possibility is that the person who fixed the failing test cases was actually not the most appropriate one given that our oracle is not perfect.
we describe this and other threats to validity in section v. rq1 our automatic developer assignment technique finds the expert within the top candidate developers for .
of the considered test cases which indicates that our automatic recommendations can be valuable and accurate.
e. experiment in our second experiment we compare the results of our automatic technique to a baseline of the distribution of expertise throughout the source code.
by this comparison we check whether the success of our technique is due to a skewed distribution of expertise.
if a small number of developers commit most changes to the source code history then any technique that depends on the amount of commits made by a developer can be successful at finding the expert.
such a scenario would imply that a technique based on simply ranking all developers in terms of the amount of commits that they made to the source code repository would obtain a high accuracy.
whosefault .
.
.
.
.
.
distribu on of commits .
.
.
.
.
.
.
.
.
.
.
figure .
effectiveness of our developer assignment technique compared to the distribution of changes committed by developers.
we obtained the distribution of expertise by building a simple technique that retrieves all developers who committed changes to the source code repository before the date of the pre fix revision of the test case.
then it sorts this list of developers in terms of the number of commits that each developer made from high to low.
we executed this simple technique for the candidate test cases.
figure displays the accuracy obtained by this simple technique in comparison with the results of our automatic assignment technique in w hose fault .
these results show that when considering only the first or the first two suggested developers our automatic technique provides more accurate results than the distribution of commits technique by finding the expert in .
and .
more test cases respectively.
moreover when considering the first three four and five developers our automatic technique is more accurate at finding the expert for .
.
and .
more test cases respectively than the distribution of commits technique.
our results also show a sudden increment in the accuracy of the distribution of commits technique when the first six developers are considered.
upon investigation of this phenomenon we discovered a significant number of failing test cases were fixed within a relatively short period of time by a developer who was the sixth most active developer.
as a consequence the accuracy of the distribution of commits technique increases significantly when including the sixth position.
however our automatic technique recommended the correct developer at a higher position for these failing test cases because its localization algorithm causes it to focus on a particular area of the code as opposed to the distribution of commits technique which considers equally all parts of the entire program.
additionally our automatic technique weighs developer expertise according to our expertise formula assigning higher importance to more recent changes.
this phenomenon demonstrates one strength of our technique its resilience to such uneven expertise and fault distribution.we should note that high accuracy obtained after considering more than four positions does not have a high impact given that an average of eight developers were considered as candidates.
considering more than four positions would mean on average considering more than of the candidate developers.
as such although we can see that the distribution of commits finds the expert for more test cases than our automatic technique when considering the first six suggested developers we interpret these results to not have a high impact because it would mean having to consider of the candidate developers.
finally for some test cases .
in our experiment our automatic technique could not find the expert in any of the suggested positions which means that it could not reach accuracy no matter how many positions were considered.
in contrast the distribution of commits technique always suggests all developers which allows it to reach accuracy even if it means inspecting all positions in its ranked list of developers our experiment considered a maximum of candidate developers .
rq2 our automatic developer assignment technique performs better than a na ve approach based on the distribution of commits among developers.
it produces recommendations that are more accurate by at least when considering the top two ranks and remarkably more accurate when the recommendations include between three and five developers up to .
f .
experiment in our third experiment we evaluated the influence over our results of each of the individual steps that comprise our approach.
for that goal we individually substituted the techniques that we selected in each of the steps described in section iii for a simple technique.
then we collected the results for each of these variations of our approach and checked how much they vary in comparison with the original configuration.
the results of this experiment are displayed in figure .
the w hose fault label represents the results of our original configuration.
whosefault .
.
.
.
.
.
mining file level .
.
.
.
.
.
.
.
coverage .
.
.
.
.
.
experience atoms .
.
.
.
.
.
.
.
figure .
effectiveness of different variants of our developer assignment technique when each step is substituted by a simple one.
mining history at the file level first we substituted our original technique of mining the history of the sourcecode repository at the line of code level for a simpler technique that retrieves changes in files but does not store which lines were modified by each change.
figure displays the results for this variation under the label mining file level .
as a consequence of this modification our expertise algorithm includes any author that committed changes to any of the files identified by the fault localization technique as suspicious regardless of whether they modified suspicious lines or any other lines of code.
this is also represented in the results by the fact that mining file level finds the expert for a higher number of test cases than w hose fault when considering all suggested developers .
vs. .
respectively .
however this advantage has low impact because it involves considering a high percentage of the candidate developers as described in section iv e .
the results obtained for the variant mining file level although lower are very similar to those obtained by our original configuration w hose fault for the first one and two positions.
however they are quite worse when we consider the first three and four positions by .
and .
respectively .
therefore we conclude that mining the source code history at the line of code level is important to achieve the results of our approach.
coverage fault localization in our second variant we substitute the t arantula fault localization technique for a simpler technique that only captures those lines that were executed by the failing test case and assigns the highest suspiciousness level to all such lines.
since all the lines executed by the failing test case have the same suspiciousness value all of them acquire the same importance when they are processed by our expertise technique.
the results for this variation are displayed in figure under the label coverage .in this case the results for the variant coverage are practically the same as for w hose fault less than difference in all positions .
this result implies that the suspiciousness value of the t arantula fault localization technique had little impact on the results of w hose fault beyond the coverage of the failing test cases that inform it.
as such it seems that the passing test cases did not significantly contribute to the results.
we speculate that unlike this evaluation in which only a single failing test case was used the inclusion of multiple failures may be helpful to allow the suspiciousness metric to differentiate the failing coverage and thus improve the results.
however further studies need to be performed to verify this hypothesis.
experience atoms finally we substitute our expertise algorithm with the simpler technique used by expertise browser .
by this technique the developer with the highest amount of experience atoms is the expert for a source code entity.
experience atoms are measured by atomic changes that a developer commits to the source code repository.
we implemented this technique by obtaining all developers who had committed changes in the source code repository to any of the suspicious files and sorting them by the number of commits that each developer made from high to low.
we display the results of this variant in figure under the label experience atoms .
our original configuration achieves much more accurate results than those achieved by this variant up to .
for the first positions in all positions except for the case of considering the first six developers suggested.
because experience atoms is based on the distribution of commits over a set of files it shows the same peculiarities described for the distribution of commits technique in section iv e. therefore we can conclude that our original expertise algorithm played an important role in our results.
the unique features of our original expertise algorithm that arenot included in the experience atoms approach are the consideration of only commits that modified suspicious lines and the measure of recency of each commit.
rq3 the results obtained by our automatic developer assignment technique were mostly influenced by our expertise algorithm and also influenced by mining the history of the source code at the lineof code level but not significantly influenced by the suspiciousness values provided by the fault localization technique.
v. t hreats to validity threats to internal validity arise when factors affect the dependent variables without the researchers knowledge.
it is possible that some implementation flaws could have affected the results.
however we are confident in the correctness of our results given that we repeated our technique for test cases and the results were consistent among them.
in addition we exercised diligence in testing and manually checking our results at each step of the process.
threats to external validity arise when the results of the experiment are unable to be generalized to other situations.
in this experiment we evaluated the technique on only one program and thus we are unable to definitively state that our findings will hold for programs in general.
however we conducted our experiment on a real project that is used world wide.
the test cases on which we evaluated represent actual faults with actual failures.
the aspectj project is over eight years old of which the test cases that we used spanned five years.
the results may not generalize to other programs development teams and their practices and test cases.
however we have a strong evidence that our approach has promise in practice.
threats to construct validity arise when the metrics used for evaluation do not accurately capture the concepts that they are meant to evaluate.
in our case we measure the location of the correct developer in a ranked list.
however correct in our experiment is defined as the developer who actually fixed the fault represented by the test case this could have been the developer performing the fix for any number of reasons.
for example the second best developer could have been chosen because the most experienced developer was unavailable an inexperienced developer was chosen because she wanted to learn about that function in the code or simply the wrong choice was made by the person the assignment.
nonetheless the actual person chosen by the people who know the most about the codebase the actual development team is likely to be among the most experienced most of the time.
vi.
c onclusions in this paper we presented a novel technique that automatically assigns a developer to a fault represented by a failing test case.
the technique is more automated than previous techniques that relied upon a person to first determine thelocation in the codebase that needed an expert.
it also works at a finer level of granularity at the individual line level.
the technique is more automated than previous techniques that relied upon well written and consistent bug reports to describe the witnessed failures to find an expert.
in addition we provide a diagnosis of the locations in the code for the expert developer to begin her search for the fault.
we implemented our approach in a tool called w hose fault and demonstrated its effectiveness on a well known real world application that has been in active development for over eight years.
our results show that w hose fault chooses the correct developer in the first suggested position for of the test cases studied for when considering the top two positions and for when considering the top three positions.
we also evaluated w hose fault against a simple distribution of the commits performed by each developer to check whether our results are caused by a skewed distribution of commits over the source code.
w hose fault improved by a difference between and the results obtained by a simple technique that sorts the developers in terms of number of commits performed in the source code repository.
finally we explored the influence of each of the components of our approach over its results.
we found that our results were mostly influenced by our expertise algorithm and moderately influenced by the algorithm that we chose to mine the history of the source code.
we also found that the fault localization technique that we utilized did not have a high influence in our results.
as a consequence we found that by using a simpler fault localization technique which utilizes only the coverage of the failing test case we could retain our effectiveness and require less input and potentially less runtime overhead.
as part of this experiment we also compared our expertise algorithm against an existing expertise assessment technique and found that our algorithm provided greater accuracy by up to .
we envision w hose fault being used with automated regression testing or continuous integration environments for use as a recommendation for which developer to assign a guide for the eventually assigned developer to find other experts to consult and collaborate and a diagnosis for areas to explore during the debugging process.
in the future we will expand our studies to additional subjects and test cases.
we also intend to experiment with applying different techniques in each of the components of our approach using different line mapping techniques fault localization techniques and expertise techniques and using multiple failures to inform the approach.
vii.
a cknowledgements this material is based upon work supported by the national science foundation under award ccf and by a google research award.