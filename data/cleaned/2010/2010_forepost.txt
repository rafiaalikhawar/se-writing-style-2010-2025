automatically finding performance problems with feedback directed learning software testing mark grechanik accenture technology lab and u. of illinois chicago chicago il drmark uic.educhen fu qing xie accenture technology lab chicago il chen.fu qing.xie accenture.com abstract a goal of performance testing is to find situations when applications unexpectedly exhibit worsened characte ristics for certain combinations of input values.
a fundamenta l question of performance testing is how to select a manageabl e subset of the input data faster to find performance problems in applications automatically.
we offer a novel solution for finding performance problems in applications automatically using black box software te sting.
our solution is an adaptive feedback directed learning te sting system that learns rules from execution traces of applicati ons and then uses these rules to select test input data automatic ally for these applications to find more performance problems when compared with exploratory random testing.
we have implemented our solution and applied it to a medium size application at a major insurance company and to an open source application.
performance problems were found automatical ly and confirmed by experienced testers and developers.
i. i ntroduction a goal of performance testing is to find performance problems when an application under test aut unexpectedly exhibits worsened characteristics for a specific workload .
for example effective test cases for load testing which is a variant of performance testing find situations where an aut suffers from unexpectedly high response time or low throughput .
test engineers construct performance test cases and these cases include actions e. g. interacting with gui objects or invoking methods of exposed interfaces as well as input test data for the parameters of these methods or gui objects .
it is difficult to construct effective performance test cases that can find performance problems in a short period of time since it requires test engineers to test many combinations of actions and data for nontrivial applications.
depending on input values an application can exhibit different behaviors with respect to resource consumption.
some of these behaviors involve intensive computations tha t are characteristic of performance problems .
naturally testers want to summarize the behavior of an aut concisely in terms of its inputs so that they can select input data that will lead to significantly increased resource consumption thereby revealing performance problems.
unfortunate ly finding proper rules that collectively describe properties ofsuch input data is a highly creative process that involves deep understanding of input domains .
descriptive rules for selecting test input data play a significant role in software testing where these rules approximate the functionality of an aut.
for example a rule for an insurance application is that some customers will pos e a high insurance risk if these customers have one or more prior insurance fraud convictions and deadbolt locks are no t installed on their premises.
computing insurance premium may consume more resources for a customer with a highrisk insurance record that matches this rule versus a custom er with an impeccable record since processing this high risk customer record involves executing multiple computationa lly expensive transactions against a database.
of course we us e this example of an oversimplified rule to illustrate the idea .
even though real world systems exhibit much more complex behavior useful descriptive rules often enable testers to build effective performance fault revealing test cases.
we offer a novel solution for feedback oriented performance software testing forepost for finding performance problems automatically by learning and using rules that describe classes of input data that lead to intensive computations.
forepost is an adaptive feedback directed learning testing system that learns rules from aut executio n traces and uses these learned rules to select test input data automatically to find more performance problems in applications when compared to exploratory random performance testing .
forepost uses runtime monitoring for a short duration of testing together with machine learning techniques and automated test scripts to reduce large amounts of performance related information collect ed during aut runs to a small number of descriptive rules that provide insights into properties of test input data that lea d to increased computational loads of applications.
this paper makes the following contributions.
forepost collects and utilizes execution traces of the aut to learn rules that describe the computational intensity of the workload in terms of the properties of input data.
these rules are used by the adaptive automated test script automatically in a feedback loop to steer the execution of the aut by selecting inputdata using these learned rules.
we know of no testing approach that uses this idea to find performance problems in real world applications.
we give a novel algorithm that identifies methods that lead to performance bottlenecks orhot spots which are phenomena where the performance of the aut is limited by one or few components .
we have implemented forepost and applied it to an application at a major insurance company.
performance problems were found automatically in the insurance application and confirmed by experienced testers and developers who work at this company.
after implementing a fix the performance of this application was improved by approximately seven percent.
we also applied forepost to an open source application benchmark jpetstore.
forepost automatically found rules that steer the executions of jpetstore towards input data that increases the average execution time by the order of magnitude when compared with exploratory random testing .
ii.
b ackground and the problem in this section we describe the state of the art and practice in performance testing show a motivating example and formulate the problem statement.
a. state of the art and practice performance testing of enterprise applications is manual laborious costly and not particulary effective.
current ly a prevalent method for performance testing is intuitive testing which is a method for testers to exercise the aut based on their intuition and experience surmising probable erro rs .
intuitive testing was first introduced in 1970s as an ap proach to use experience of test engineers to focus on errorprone and relevant system functions without writing timeconsuming test specifications thus lowering pre investmen t and procedural overhead costs .
when running many different test cases and observing application s behavior te sters intuitively sense that there are certain properties of test cases that are likely to reveal performance bugs.
distilling these properties automatically into rules that describe how thes e properties affect performance of the application is a subgo al of our approach.
b. a motivating example consider renters insurance program or simply renters designed and built by a major insurance company.
a goal of this program is to compute quotes for insurance premiums for rental condominiums.
renters is written in java and it contains close to methods that are invoked more than three million times over the course of a single end to end pass through the application.
its database contains approx imately 78mil customer profiles which are used as test input data for renters.
inputs that cause heavy computations aresparse and random test selection often does not perform a good job of locating these inputs.
a fundamental question of performance testing is how to select a manageable subset of the input data for performance test cases with which performance problems can be found faster and automatically .
rules for selecting test input data that quickly lead to finding performance problems are notoriously difficult to capture since these rules are buried in the source code and thus they are hard to locate manually.
test engineers must intimately know the functionality of the subject application under test understand how programmers designed and implemented the application and hypothesize on how application behavior matches requirements for this application.
without having useful rules that summarize these requirements it is difficult to define objectives that lead t o selecting good test cases .
currently the state of the practice of finding useful rul es is to use experience and intuition of performance test engineers who spend time observing the behavior of auts when running manually constructed test cases.
there is little au tomated support for discovering problems with performance testing where a recent work by hassan et.
al.
is the first that can automatically detect performance problems in the load testing results by analyzing performance logs however test inputs are not located that cause performance problems .
experience and intuition are main tools that performance test engineers use to surmise probable errors .
c. automated performance test scripting approaches typically performance testing is accomplished using test scripts which are programs that test engineers write to automate testing.
these test scripts performs actions i.e .
invoking methods of exposed interfaces or mimicking user actions on gui objects of the aut to feed input data into aut and trigger computation.
test engineers write code in test scripts that guide selection of test inputs typical ly it is done using randomly selected input values or by using algorithms of combinatorial design interactions .
it is impossible to performance test applications without test scripts since it is not feasible to engage hundreds of thousands of testers who simulate multiple users who call multiple methods with high frequency manually .
test scripts are written with either of two different frameworks a gui testing framework e.g.
quicktestpro from hp corp or a backend server directed performance tool such as jmeter an open source software that is widely used to load test functional behavior and measure performance of applications.
these frameworks are the basis on which performance testing is mostly done in industry.
performance te st scripts imitate large numbers of users to create a significan t load on the aut.
jmeter provides programming constructs that enable testers to automatically generate a large numbe r of virtual users who send http requests directly to webservers of auts thereby creating significant workloads.
natural measures of performance include throughput that is the number of executed requests per second and the average response time it takes to execute a request.
a goal of performance testing is to determine what combinations of requests lead to higher response times and lower throughput which are helpful to reveal performance bugs in auts.
d. the problem statement our goal is to automate finding performance problems by executing the aut on a small set of randomly chosen test input data and then inferring rules with a high precision for selecting test input data automatically to find more performance problems in the aut.
specifically these are if then rules that describe properties of input data that result in good performance test cases that lead to increased computational workload on applications when compared to bad performance test cases when computational workload is much smaller.
for example a rule may say if inputs convictedfraud istrue anddeadboltinstalled isfalse then the test case is good.
in this work we supply automatically learned rules using a feedback mechanism to test scripts and these scripts parse these rules and use them to guide test input data selection automatically to steer execution of the aut towards code that has exposes performance problems.
in this paper we accept a performance testing definition of what constitutes a good test case .
in general a main goal of performance testing is to select a small subset of good test cases with which different testing objectives can be achiev ed specifically more performance problems can be found in a shorter period of time.
one of the goals of performance testing is to find test cases that worsen response time or throughput of the aut or its latency.
it can be achieved by adding more users to the aut thereby leading to intensive computations and increased computational workloads and by finding input data that make the aut take more resources and time to compute results.
conversely bad test cases are those that utilize very few resources and take much less time to execute compared to good test cases.
a main goal is to automatically produce rules that describe good and bad test cases and then use these rules also automatically to select input data for further testing.
the system should also correct itself by applying learned rules on test input data that are selected using these rules a nd verifying that these test data lead to predicted performanc e results thereby increasing the probability that the learn ed rules express genuine causation between input values and performance related workloads rather than simply correla ting them.
finally no performance testing is complete without providing sufficient clues to performance engineers where in th e aut problems can lurk.
a main objective of performance analysis is to find bottlenecks a single method that dragsdown the performance of the entire application is easy to detect using profilers however it is a difficult problem to find bottlenecks when there are hundreds of methods whose elapsed execution times are approximately the same which often is the case in large scale applications .
a problem that we solve in this paper is that once the input space is clustered into good and bad performance test cases using learned rules we want to find methods that are specific to good performance test cases and that are most likely to contribute to bottlenecks.
iii.
t heforepost a pproach in this section we explain the key ideas behind our approach give an overview of feedback oriented performance software testing forepost describe its architecture and show two types of test scripts that are used to automate performance testing.
a. an overview of forepost in this section we describe two keys ideas on which forepost is built extracting rules from execution traces that describe relations between properties of input data and workloads of performance tests that are executed with this data and identifying bottleneck methods using these rules.
obtaining rules as part of the first key idea the instrumented aut is initially run using a small number of randomly selected test input data and its execution profiles are collected and clustered using machine learning ml techniques automatically into different groups that collectively describe different performance results of th e aut.
for example there can be as few as two groups that correspond to good and bad performance test cases.
the set of values for aut inputs for good and bad test cases is the input to an ml classification algorithm.
this input is described as implications of the form vi1 ... vik t where vimis the value of the input imandt g b gand bstanding for good and bad test case correspondingly.
in fact tis the summarized score for an execution trace that describes summarily whether this execution has evidence of performance problems.
the ml classification algorithm learns the model and outputs rules that have the form ip vip iq viq ... ik vik t where is one of the relational operators and stands for logical connectors and andor.
these rules are instrumental in guiding selection of the input test data in test scripts.
a feedback loop is formed by supplying these learned rules that are obtained using the ml classification algorith m are supplied back into the test script to automatically guid e selection of test input data.
using newly learned rules tes t input data is partitioned and the cycle repeats.
the test scr ipt selects inputs from different partitions the aut is execut ed again and new rules are relearned from collected execution traces.
if no new rules are learned after some time of testing it means that the partition of test inputs is stable with a high degree of probability.
at this point instrumentation c an be removed and testing can continue and test input data is selected using the learned rules.
identifying bottlenecks our goal is to help test engineers to identify bottlenecks automatically as method calls whose execution seriously affects the performance of the whole aut.
for example consider a method that is periodically executed by a thread to check to see if the content of some file is modified.
while this method may be one of the bottlenecks it is invoked in both good and bad test cases thus its contribution to resource consumption as the necessary part of the application logic does not lead to any insight that may resolve a performance problem.
our second key idea is to consider the most significant methods that occur in good test cases and that are not invoked or have littl e to no significance in bad test cases where the significance of a method is a function of the resource consumption that its execution triggers.
b. blind source separation large applications implement multiple requirements each of these requirements is implemented using different methods.
each aut run involves thousands of its methods that are invoked millions of times.
the resulting execution trac e is a mixture of different method invocations each of which address a part of some requirement.
these traces are very large.
in otder to identify most significant methods we need an approach with which we can compress information in these traces and break these traces automatically into components that match high level requirements and then we identify methods with most significant contributions to the se components.
unfortunately using transactional boundari es to separate information in traces is not always possible fo r example when dealing with file operations or gui frameworks.
we reduce the complexity of collected execution traces by categorizing them into components that roughly correspond to different requirements.
we draw an analogy between separating method invocations in execution traces into components that represent high level requirements and a well known problem of separating signals that represent different sources from a sign al that is a mixture of these separate signals.
this problem is known as blind source separation bss .
the idea of bss is illustrated using a model where two people speak at the same time in a room with two microphones m1andm2.
their speech signals are designated assource andsource .
each microphone captures the mixture of the signals source andsource that is the corresponding signal mixtures from m1andm2 respectively.
the original signals source andsource 2are separated from the mixtures using a technique called independent component analysis ica which we describe in section iii c. ica is based on the as sumption that different signals from different physical pr ocesses are statistically independent.
for example differ ent requirements are often considered independent since they implemented in applications as separate concerns .
when physical processes are realized e.g.
different peop le speak at the same time or stocks are traded or an application is run and its implementations of different requirements are executed in methods these different signals are mixed and these signal mixtures are recorded by some sensors.
using ica independent signals can be extracted from these mixtures with a high degree of precision.
in this paper we adjust the bss model for breaking execution traces automatically into components that approximat ely match high level requirements and then identifying metho ds with most significant contributions to these components.
nontrivial applications implement quite a few high level r equirements in different methods that are executed in differ ent threads often concurrently.
we view each requirement as a source of a signal that consists of method calls.
when an application is executed multiple requirements are realiz ed and method invocations are mixed together in a mixed signal that is represented by the execution profile.
microphones ar e represented by instrumenters that capture program executi on traces multiple executions of the application with differ ent input data is equivalent to different speakers talking at the same time as a result multiple signal mixtures i.e.
execution traces for different input data with mixed realiz ed requirements are produced.
with ica not only it is possible to separate these signal mixtures into components but also to define most significant constituents of these signals i.e.
method calls .
we choose ica because it works with non gaussian distributions of data which is the case with forepost.
c. independent component analysis a schematics of ica matrix decomposition is shown in figure .
the equation bardblx bardbl bardbla bardbl bardbl s bardbldescribed the process where bardblx bardblis the matrix that contains the observed signal mixtures and bardbla bardblis the transformation or mixing matrix that is applied to the signal matrix bardbls bardbl.
in our case the matrix bardblx bardblis shown in figure on the left hand side of the equal sign and its rows correspond to application execution traces from different input data with its columns corresponding to method invocations that are observed for each trace.
each element of the matrix bardblx bardblis calculated as xj i n nj i t tj i a aj i d dj i m mj i where nj iis the number of times that the method jis invoked in the trace i tj iis the total elapsed time of these invocations minus the elapsed time of all methods that are invoked from this method in this trace aj iis the number of attributes that this method accesses in the databases dj iis the amount of data that this method transfers between the aut andx gid16 gid10 gid1 gid15 gid1 gid8 gid1 gid8 gid5 gid14 gid6 gid9 gid4 gid13 gid1 gid14 gid12 gid2 gid3 gid5 gid13 gid1 gid17 gid1 gid1 gid1 gid1 gid1a gid16 gid10 gid1 gid15 gid1 gid7 gid1 gid12 gid5 gid11 gid13 gid1 gid14 gid12 gid2 gid3 gid5 gid13 gid1 gid15 gid1s gid16 gid7 gid1 gid15 gid1 gid8 gid1 gid8 gid5 gid14 gid6 gid9 gid4 gid13 gid1 gid12 gid5 gid11 gid13 gid1 figure .
schematics of the ica matrix decomposition.
the databases mj iis the number of methods that are invoked from this method and finally are normalization coefficients computed for the entire matrix bardblx bardblto ensure xj i .
naturally xj i means that the method iis not invoked in the trace j while xj i means that the given method makes the most significant contribution to the computation in the given trace.
using ica the matrix bardblx bardblis decomposed into a transformation and a signal matrices that are shown on the right hand side of the equal sign in figure .
the input to ica is the matrix bardblx bardbland the number of source signals that in our case is the number of requirements reqs in the figure implemented in the application.
elements of the matrix bardbla bardbl aq pspecify weights that each profile pcontributes to executing code that implements the requirement q and elements of the matrix bardbls bardbl sk qspecify weights that each method kcontributes to executing code that implements the requirement q. methods that have the highest weights for given requirements are thought to be the most significant and interesting for troubleshooting performance problems .
this is a hypothesis that we evaluate and describe results in section v. d. forepost architecture and workflow the architecture of forepost is shown in figure .
solid arrows show command and data flows between components and numbers in circles indicate the sequence of operations in the workflow.
the beginning of the workflow is shown with the fat arrow that indicates that the test scrip t executes the application by simulating users and invoking methods of the aut interfaces.
the test script is written by the test engineer as part of automating application testing as we described in section ii c. once the test script starts executing the application its execution traces are collected by the profiler and these traces are forwarded to the execution trace analyzer which produces the trace statistics.
we implemented the profiler using the tptp framework1.
this statistics contains information on each trace such as the number of invoked methods the elapsed time it takes to complete the end to end application run the number of threads and the number of unique methods that were invoked in this trace.
the trace statistics is supplied to the module last checked sept gid13 gid17 gid27 gid28 gid1 gid1 gid12 gid15 gid26 gid21 gid25 gid28 gid1 gid10 gid26 gid24 gid18 gid22 gid17 gid26 gid1 gid5 gid34 gid17 gid15 gid31 gid29 gid24 gid23 gid1 gid13 gid26 gid14 gid15 gid17 gid1 gid2 gid23 gid14 gid22 gid35 gid36 gid17 gid26 gid1 gid13 gid26 gid14 gid15 gid17 gid1 gid4 gid22 gid31 gid27 gid28 gid17 gid26 gid21 gid23 gid19 gid1 gid11 gid31 gid22 gid17 gid27 gid1 gid13 gid26 gid14 gid15 gid17 gid1 gid12 gid28 gid14 gid29 gid27 gid29 gid15 gid27 gid1 gid6 gid24 gid24 gid16 gid1 gid28 gid17 gid27 gid28 gid1 gid28 gid26 gid14 gid15 gid17 gid27 gid1 gid3 gid14 gid16 gid1 gid28 gid17 gid27 gid28 gid1 gid28 gid26 gid14 gid15 gid17 gid27 gid1 gid8 gid17 gid14 gid26 gid23 gid17 gid26 gid1 gid9 gid17 gid28 gid20 gid24 gid16 gid1 gid1 gid14 gid23 gid16 gid1 gid16 gid14 gid28 gid14 gid1 gid27 gid28 gid14 gid29 gid27 gid29 gid15 gid27 gid1 gid7 gid4 gid2 gid1 gid6 gid1 gid9 gid17 gid28 gid20 gid24 gid16 gid1 gid33 gid17 gid21 gid19 gid20 gid28 gid27 gid1 gid2 gid16 gid32 gid21 gid27 gid24 gid26 gid1 gid7 gid1 gid8 gid1 gid9 gid1 gid10 gid1 gid12 gid1 gid11 gid1 gid5 gid1 gid13 gid1 gid5 gid4 gid1 gid5 gid5 gid1 gid5 gid6 gid1 gid5 gid7 gid1 gid5 gid9 gid1 gid5 gid10 gid1 gid5 gid11 gid1 gid5 gid8 gid1 figure .
the architecture and workflow of forepost.
trace clustering which uses an ml algorithm jrip to perform unsupervised clustering of these traces into two groups that correspond to good and bad test traces.
the user can review the results of clustering and reassign clustered traces if a need exists.
these clustered traces are supplied to the learner that uses them to learn the classification model and output rules that we described in section iii a. the user can review these rules and mark some of them as erroneous if the user has sufficient evidence to do so.
then the rules are supplied to the test script.
once the test script receives a new set of rules it partitions the input space into blocks according to these rules and starts forming test inputs by selecting one input from each block.
the profiler collect traces of these new test runs and thus the cycle repeats wit h new rules that can be learned at each several passes and the input space is repartitioned adaptively to accommodate the se rules.
we implemented the ml part of forepost using weka .
the input test data comes from existing repositories or databases it is a common practice in industry we confirmed it with different performance testing professionals.
reca ll that the application renters has a database that contains approximately 78mil customer profiles which are used as the test input data for different applications including renters itself.
we repeatedly run the experiment with the randomly selected initial seeds from the input space which are different each time.
new values are selected from the input space either randomly if rules are not available or based on learned rules.
finally recall from section ii d that once the input space is partitioned into clusters that lead to good and bad test cases we want to find methods that are specific to good performance test cases and that are most likely to contribut e to bottlenecks.
this task is accomplished in parallel to computing rules and it starts when the trace analyzerproduces the method and data statistics that is used to construct two matrices bardblxb bardbland bardblxg bardblfor bad and good test cases correspondingly.
constructing thes e matrices is done as we described in section iii c. once these matrices are constructed ica decomposes them into the matrices bardblsb bardbland bardblsg bardblfor bad and good test cases correspondingly.
recall that our key idea is to consider the most significant methods that occur in good test cases and that are not invoked or have little to no significance in bad test cases.
crossreferencing the matrices bardblsb bardbland bardblsg bardbl which specifies method weights for different requirements the advisor determines top methods that performance testers should look at to debug possible performance problems.
this step completes the workflow of forepost.
algorithm the algorithm for identifying bottlenecks.
forepost execution traces t signal threshold u b cgood cbad initialize values for the set of bottlenecks the set of clusters that contain execution traces that are matched to good and bad test cases.
clustertraces t mapsto cgood mapsto tg cbad mapsto tb tg tb t tb tb createsignalmixturematrix cgood mapsto bardblxg bardbl createsignalmixturematrix cbad mapsto bardblxb bardbl ica bardblxg bardbl mapsto bardblag bardbl bardblsg bardbl mapsto lg mapsto mg rg sg ica bardblxb bardbl mapsto bardblab bardbl bardblsb bardbl mapsto lb mapsto mb rb sb for all eg mapsto mi g rj g si j g lgdo for all eb mapsto mk b rl b skl b lbdo ifmi g mk b rj g rl b si j g u skl b uthen b b eg end if end for end for return b iv.
t healgorithm for identifying bottlenecks in this section we describe our algorithm for identifying bottlenecks using forepost.
the algorithm forepost is shown in algorithm .
this algorithm takes as its input the set of captured execution traces t and the signal threshold u that is used to select methods whose signals indicate their significant contribution in execution traces.
the set of methods that are potential bottlenecks bis computed and returned in line 15of the algorithm.
in step2the algorithm initializes to the empty set the values for the set of bottlenecks the set of clusters that contain execution traces that are matched to good and bad test cases.
in step 3the procedure clustertraces is called that automatically clusters execution traces from t he set tinto good and bad test case clusters cgood andcbadrespectively.
next in steps 4and5the procedure createsignalmixturematrix is called on clusters of traces that correspond to good and bad test cases respective ly to to construct two matrices bardblxb bardbland bardblxg bardblfor bad and good test cases correspondingly as we described in section iii d. in step 5and6 the procedure ica decomposes these matrices into the matrices bardblsb bardbland bardblsg bardblfor bad and good test cases correspondingly as we described in step in section iii d. this algorithm runs the double nested loop between steps which terminates after enumerating all methods for all requirement components in the decomposed matrices.
that is for each method whose signal is greater than the threshold uin the transformation and signal matrices that correspond to good cases we compare if this method in the same component rdoes not occur in the counterpart matrices for bad test case decompositions or if the same method for the same component occurs then its signal in the bad test case matrices should be less than the threshold u. if this condition is evaluated to true in the step this method is added to the set bthat is returned to line as the algorithm terminates.
v. e valuation in this section we state our research questions rqs and we describe how we evaluated forepost on two applications commercial application renters that we described as our motivating example in section ii b and open source application jpetstore that is frequently used as an industr y benchmark.
a. research questions in this paper we make one meta claim forepost is better than exploratory random performance testing which is a popular industry approach.
we define better in two ways finding inputs that lead to significantl y higher computational workloads and finding performance bottlenecks.
we seek to answer the following research questions.
rq1 how effective is forepost in finding input test data that steer applications towards more computationally intensive executions?
rq2 how effective is forepost in identifying bottlenecks with a high degree of automation?
b. subject auts we evaluate forepost on two subject applications renters and jpetstore.
the former is a commercial mediumsize application that is built and deployed by a major insurance company.
renters serves over daily customers in the u.s. and it has been deployed for over seven years.
the latter is an open source application that is often used a s an industry benchmark since it is highly representative of enterprise level three tier database centric applicati ons.table i characteristics of the insurance application renters .
size lines of code loc noc number of classes nom number of methods noa number of attributes mcc a verage mccabe cyclomatic complexity nop number of packages .
renters size noc nom noa mcc nop component authorization .
utils .
libs .
eventing .
appweb .
total renters insurance application is a j2ee application that calculates the insurance premiums for rental condominium.
its software metrics are shown in table i. the backend database is db2 running on the ibm mainframe its schema contains over tables that include close to attributes that contain data on over 78mil customers.
the application accepts input values using gui objects.
the total number of combinations of input data is approximately making it infeasible to comprehensively test renter.
we used renters in our motivating example in section ii b. jpetstore is a java implementation of the petstore benchmark where users can browse purchase pets and rate their purchases.
this sample application is typical in usin g the capabilities of the underlying component infrastructu res that enable robust scalable portable and maintainable e business commercial applications.
it comes with full sourc e code and documentation so we used it to experiment with forepost and demonstrate that we can build scalable security mechanisms into enterprise solutions.
we used ibatis jpetstore .
.
.
jpetstore has lines of code methods classes in packages with the average cyclomatic complexity of .
it is deployed using the web server tomcat and it uses derby as its backend database.
c. inputs we measure inputs as transactional units where one transactional unit for jpetstore is a combination of differ ent urls that map to different functional units of the application.
these urls are selected into a transactional unit randomly at first but as rules are learned that describe limi ts on specific urls some urls will be given more preference for inclusion in transactions.
for renters a transactiona l unit is end to end run of the application with an input that is a customer profile that comes from the application database.
d. methodology our goal is to determine with which approach good performance test cases can be found faster.
given the complexity of subject applications it is not clear with wha t data performance can be worsened significantly for these applications and given a large space of input data it is not feasible to run these applications on all inputs to obtai n the most worsened performance execution profiles.
these limitations dictate the methodology of our experimental design specifically for choosing competitive approaches to forepost.
specifically we select random exploratory performance testing as the main competitive approach to forepost since it is widely used in industry and it is reported as consistently outperforming different systema tic testing approaches.
random testing approach as the name suggests involves random selection of test input data for input parameter values and in that it showed remarkably effective and efficient for exploratory testing and bug finding .
a seemingly stupid idea of random testing proved often more effective than systematic sophisticated testing approaches .
to prove our claims in this paper our goal is to show under what conditions forepost outperforms random testing with strong statistical significance.
most nontrivial applications have enormous spaces of test input data objects that are constructed by combining values of different input parameters.
even though it is infeasible to select a large subset of customer profiles for testing renters it is possible to create combinations of values of customer records that will result in a smaller number of customer profiles.
to do that testing practitioners freque ntly use combinatorial design algorithms .
most prominent are algorithms for t wise combinatorial interaction testing cit which requires every possible allowed combination of values of tparameters be included in some test case in the test suite .
of course these combinations of values should match to valid customer records.
pairwise testing is when t and every unique pair of values for each pair of input parameters is included in at least one test case in the test suite.
we assemble customer profiles for our experiment s with renters using pairwise testing from the range of input data that was given to us by the insurance company that owns renters.
in designing the methodology for this experiment we aligned with the guidelines for statistical tests to assess randomized algorithms in software engineering .
our goal is to collect highly representative samples of data whe n applying different approaches perform statistical tests on these samples and draw conclusions from these tests.
since our experiments involve random selection of input data it is important to conduct the experiments multiple times to pick the average to avoid skewed results.
for each subject application we run each experiment at least times with each approach on the same aut to consider collected data a good representative sample.
jpetstore is based on the client server architecture wher e its gui front end is web based and it communicates with the j2ee based backend that accepts http requests in the form a renters application.
gid15 gid16 gid13 gid13 gid13 gid1 gid1 gid1 gid1 gid1 gid1 gid1 gid1 gid1 gid1 gid1 gid1 gid16 gid13 gid13 gid13 gid13 gid1 gid1 gid1 gid1 gid1 gid1 gid1 gid1 gid1 gid1 gid1 gid1 gid17 gid16 gid13 gid13 gid13 gid1 gid1 gid1 gid1 gid1 gid1 gid1 gid1 gid1 gid1 gid14 gid13 gid13 gid13 gid13 gid13 gid1 gid1 gid1 gid1 gid1 gid1 gid1 gid1 gid1 gid1 gid14 gid15 gid16 gid13 gid13 gid13 gid1 gid5 gid8 gid10 gid4 gid9 gid8 gid11 gid12 gid1 gid10 gid2 gid7 gid3 gid8 gid6 gid1 b jpetstore application.
figure .
the summary of the results of the experiments with s ubject applications.
the box and whisker plot for renters is shown in figure a where the time for end to end runs is measured in seconds.
th e central box represents the values from the lower to upper qu artile to percentile .
the middle line represents the median.
th e thicker vertical line extends from the minimum to the maxim um value.
the filled out box represents the values from the minimum to t he mean and the thinner vertical line extends from the quart er below the mean to the quarter above the mean.
an outside value is defined as a value that is smaller than the lower quartile minus .
ti mes the interquartile range or larger than the upper quartile plus .
times the interquartile range inner fences .
the bar gr aph for jpetstore is shown in figure b where the bars represent average times per transaction in milliseconds for random and forepost app roaches for different numbers of concurrent transactions ranging f rom to .
of urls containing address to different components and parameters for those components.
for example a url can contain the address to the component that performs checkout and its parameters contain the session id.
we define a set of url requests that comes from a single user as a transaction .
jpetstore backend can serve multiple url requests from multiple users concurrently.
depending on types of url requests in these transactions and their frequencies some transactions may cause the backend server of jpetstore to take longer time to execute.
to obtain url requests that exercise different components of jpetstore we used the spider tool in jmeter to traverse the web interface of jpetstore and recorded urls that are sent to the backend during this process.
in random testing multiple urls are randomly selected to form a transaction.
in forepost url selection process is guided by learned rules.
various constraints must be observed when forming transactions.
for example the user action checkout is permitted only when the user already logged into the system and the shopping chart is not empty.
thus if a url representing checkout action is selected into a transaction then two other urls representing user logged in and add to shopping cart must also be added into the same transaction.
we limit the number of urls in each transaction to this number was chosen experimentally based on our observations of jpetstore users who explored approximatel y urls before switching to other activities.
increasing the number of certain url requests in transactions at theexpense of not including other url requests may leads to increased workloads and the goal of our experimental evalu ation is to show that forepost eventually selects test input data i.e.
customer profiles for renters or combinations of urls for jpetstore that lead to increased workloads when compared to the competitive approaches.
when testing jpetstore urls in a transaction are issued to the backend consecutively to simulate a single user.
multiple transactions are randomly selected and issued in parallel to simulate concurrent users.
during the testing we use different numbers of concurrent transactions and measure the average time it takes jpetstore backend to execute a transaction.
a goal of its performance testing is to find combinations of different urls in transactions for different concurrent users that lead to significant increas e in the average time per transaction which is often correlated with the presence of performance faults.
experiments for renters were carried out at the premises of the insurance company using dell precision t7500 with six core intel xeon processor x5675 .06ghz 12m l3 .4gt s 24gb ddr3 rdimm ram 1333mhz.
experiments for jpetstore were carried out using one dell xps laptop with intel core i7 2820qm processor .
ghz with turbo boost .
up to .
ghz 6gb shared dual channel ddr3 ram.
e. variables the main independent variable is the subject auts and approaches with which we experiment i.e.
random pair wise random and forepost .
dependent variables are the throughput or the average number of transactions or runs tha t the subject auts can sustain under the load the average time that it takes to execute a transaction or run the aut end to end.
thus if an approach achieves a lower throughput or higher average time per transaction with some approach it means that this particular approach finds test input data which are more likely to expose performance.
the effects of other variables the structure of aut and the types and semantics of input parameters are minimized by the design of this experiment.
f .
threat to validity the main threat for our experimental design is that we experiment only with two subject auts.
the results may vary for auts that have different logic or different source code structures.
this threat makes it difficult for us to generalize the result that we obtained through experimentatio n. however since both applications are highly representativ e of enterprise level applications that they come from differe nt domains we suggest that our result is generalizable for a larger population of applications.
g. results in this section we describe and analyze the results that we obtained from our experiments with renters and jpetstore.
finding test inputs for increased workloads the results for renters are shown in the box and whisker plot in figure a that summarize execution times for end toend single application runs with different test input data.
to test the null hypothesis that there is no difference between execution times for random pairwise and forepost approaches we applied two t tests for two paired sample means.
the results of this test allow us to reject the null hypotheses and we accept the alternative hypothesis with strong statistical significance p .
that states that forepost is more effective in finding input test data that steer applications towards more computationally intensive executions than random exploratory performance testing and random exploratory performance testing with pairwise selection of input data thus addressing rq1.
this conclusion is confirmed by the results for jpetstore that are shown in figure b for average execution times for five runs.
while performing random testing it takes an average .
seconds to execute transactions.
wit h forepost executing transactions takes an average .
seconds.
this implies that forepost outperforms random testing by more than the order of magnitude.
random testing is evaluated with the instrumented jpetstor e so that the cost of instrumentation is evenly factored into experimental results.
forepost has large overhead close to of the baseline execution time however once rules are learned and stabilized they can be used to partition the input space without using instrumentation.
identifying bottlenecks when applying the algorithm to renters for identifying bottlenecks that we give in section iv we obtain a list of top methods that the algorithm identified as potential performance bottlenecks out of approximately methods.
to evaluate how effective this algorithm is we ask the insurance company to allocate the most experienced developer and tester for renters to review this list and provide feedback on it.
according to the management of the insurance company it was the first time when a developer and a tester were in the same room together to review results of testing.
reviewing started with the top bottleneck method checkwildfirearea .
the developer immediately said that forepost did not work since this method could not be a bottleneck for a simple reason this method computes insurance quotes only for u.s. states that have wildfires and forepost selected input test data for northern states like minnesota that never have wildfires.
we explained that forepost automatically selected the method checkwildfirearea as important because its weight is significant in execution traces for good test cases and it wa s absent in traces for bad test cases.
it meant that this method was invoked many times for the state of minnesota and other northern states even though its contribution in computing insurance quotes was zero for these states.
invoking this method consumes more resources and time in addition to significantly increasing number of interactions with backe nd databases.
after hearing our arguments the developer and the tester told us that they would review the architecture documents and the source code and get back to us.
a day later they got back with a message that this and few other methods that forepost identified as bottlenecks were true bottlenecks.
it turned out that the implementation of the visitor pattern in renters had a bug which resulted in incorrect invocations of the method checkwildfirearea .
even though it did not contribute anything to computing insurance quote it consumed significant resources.
after implementing a fix based on the feedback from forepost the performance of renters went up by approximately seven per cent thus addressing rq2 that forepost is effective in identifying bottlenecks .
learned rules examples of learned rules are shown in table ii.
when professionals from the insurance company looked at these and other rules in more depth they identified certain patterns that indicated that these rules were logic al and matches some requirements.
for example the rules r andr point out to strange and inconsistent insurance quote inputs where low deductible goes together with very high coverage limit and it is combined with the owner of the condo taking prescribed medications and with the condo having fewer than two residents.
all these inputs point to situation that are considered higher risk insurance polici es.
these classes of input values trigger more computations tha t lead to significantly higher workloads.table ii selected rules that are learned for renters and jpetstore where the first letters of the names of the aut s are used in the names of rules to designate to which aut s these rules belong .
the last column cons designates the consequent of the rule that corresponds to good and bad test cases that these rules d escribe .
rule antecedent cons r customer.numberofresidents coverages.limitperoccurrence good preeligibility.numberofwildanimals r adjustments.homeautodiscount adjustments.fireorsmokealarm local plus central bad dwelling.construction masonry veneer coverages.limiteachperson r coverages.deductibleperoccurrence adjustments.burglarbarsquickrelease y good nursedetails.prescribemedicine y coverages.limitperoccurrence j viewitem est viewcategory cats viewitem est good checkout updatecart additem est viewcategory est j viewitem est viewcategory cats bad for jpetsore rules j andj describe inputs as the number of occurrences of urls in transactions where urls are shown using descriptive names e.g.
checkout for the url that enables customers to check out their shopping carts .
it is important that rules for both applications are input specific.
while we do not expect that rules learned for one system to apply to a completely different system training a new set of rules using the same algorithm should deliver similar benefits.
vi.
r elated work partition testing is a set of strategies that divides the program s input domain into subdomains subsets from which test cases can be derived to cover each subset at least once .
closely related is the work by dickinson et al which use clustering analysis executi on profiles to find failures among the executions induced by a set of potential test cases.
although we both used clusterin g techniques our work differs in that we cluster the executio n profiles based on the length of the execution time and number of methods have been invoked and we target the performance bugs instead of functional errors.
finally learning rules helps stakeholders to reconfigure distributed systems online to optimize for dynamically changing workloads .
it is similar to forepost in using the learning methodology to to learn rules from only low level system statistics which of a set of possible hard ware configurations will lead to better performance under the current unknown workload.
in contrast forepost uses feedback directed adaptive performance test scripts to locate most computationally intensive execution profiles a nd bottlenecks.
related to forepost is a technique for automatically classifying execution data collected in the field as comin g from either passing or failing program runs .
this technique attempts to learn a classification model to predict if an application run failed using execution data.
forepost on the other hand learns rules that it uses to select input test data that steer applications towards computationally intensive runs to expose performance problems.in their recent work zhang elbaum and dwyer generate performance test cases using dynamic symbolic execution .
similar to forepost they use heuristics that guide generation of test cases by determining paths of executions that can introduce higher workloads.
unlike forepost white box testing approach is used thus requiring access to source code while forepost is a black box approach.
it is also unclear how this approach will scale to industrial applications with over 100kloc.
we view these approaches as complementary where a hybrid approach may combine the benefits of both approach in a gray box performance testing.
it is a subject of future work.
vii.
c onclusion we offer a novel solution for finding performance problems in applications automatically using black box software testing.
our solution is an adaptive feedback direct ed learning testing system that learns rules from execution traces of applications and uses these rules to select test input data for these applications automatically to find more performance problems when compared with random testing.
we have implemented our solution and applied it to a nontrivial application at a major insurance company and to an open source application.
performance problems were found automatically in both applications and confirmed by experienced testers and developers.