privacy and utility for defect prediction experiments with morph fayola peters tim menzies lane department of computer science and electrical engineering west virginia university morgantown usa fpeters mix.wvu.edu tim menzies.us abstract ideally we can learn lessons from software projects across multiple organizations.
however a major impediment to such knowledge sharing are the privacy concerns of software development organizations.
this paper aims to provide defect data set owners with an effective means of privatizing their data prior to release.
we explore morph which understands how to maintain class boundaries in a data set.
morph is a data mutator that moves the data a random distance taking care not to cross class boundaries.
the value of training on this morphed data is tested via a way within learning study and a cross learning study using random forests naive bayes and logistic regression for ten object oriented defect data sets from the promise data repository.
measured in terms of exposure of sensitive attributes the morphed data was four times more private than the unmorphed data.
also in terms of the f measures there was little difference between the morphed and unmorphed data original data and data privatized by data swapping for both thecross and within study.
we conclude that at least for the kinds of oo defect data studied in this project data can be privatized without concerns for inference efficacy.
keywords privacy defect prediction data mining i. i ntroduction within company defect prediction is the means by which organizations predict the number of defects in their software.
cross company defect prediction looks at the feasibility of learning defect predictors using data from other companies.
recent studies show that defect and effort predictors built from cross company data can be just as effective as predictors learned using within company data caveat the cross company data must be carefully filtered when being applied locally .
the success of such cross company learning experiments suggests that there exist general principles of software engineering that transcend project boundaries and which can be used to learn general models of software engineering.
however before we conduct widespread cross company learning experiments we must first address the privacy concerns of data owners who are willing to share their data.
unless we can address these concerns continued progress in this promising area will be stalled.
extracting project data from organizations is often very difficult due to the business sensitivity associated with the data.
for example for eleven years the second author worked at nasa s software iv v center.
nasa divides its iv v work between multiple contractors who all work on similar projects.
every threeyears all the contracts are subject to competitive bidding.
hence those contractors are very reluctant to share data about say the effort associated with finding mission critical errors in manned spacecraft lest that data is used against them during the bidding process.
consequently researchers and nasa managers suffer from a severe data drought that inhibits their ability to learn effective defect detection models for life threatening software errors .
for these reasons many researchers doubt the practicality of data sharing for the purposes of research.
in a personal communication barry boehm reports he can release none of the data that his cocomo team collected after .
similarly at a recent keynote address at esem elaine weyuker doubted that she will ever be able to release the at t data she used to build defect predictors .
these examples are a clear indication of data owners who are willing to share their data for research purposes but are unable to do so because of privacy concerns.
one way to convince data owners to publish their data is to offer them a means to privatize their data in such a way as to prevent the competition from learning specific sensitive metric values from their released data and ensuring that this privatized data remain useful for research purposes such as cross company learning.
with cross company learning where the goal of a researcher is to find defects in a software system before deployment all that is required of the data is that the relationship between the class values and attribute values remain intact.
on the other hand the attacker s goal is to seek out sensitive information without concerns for defect prediction.
we find we can exploit the difference between the goals of an attacker and the goals of defect prediction.
we show that it is possible to secure against attackers while preserving the relationships required for effective defect prediction.
morph1is a privacy algorithm designed to reduce the attacker s trust in the released data.
in contrast a researcher looking for defects in their software systems will still find the privatized data useful .
in the context of this paper we will say that a data set is useful if it can be used to learn defect predictors.
1the term morph in this work refers to data transformation via perturbation to create synthetic data.this paper proposed morph as a sharing mechanism for privacy sensitive data.
using morph we can ensure privacy while supporting defect prediction since morph as an instance mutator perturbs all instance values by a random amount.
this amount is selected to move an instance to a new location in n space without moving it across the boundaries between classes.
potentially morph increases the privacy of all mutated individuals since their original data is now distorted.
in support of the adoption of morph we address three specific research questions rq1 does morph improve privacy?
rq2 does morph improve privacy better than anything else?
rq3 does morph degrade performance?
to the best of our knowledge this is the first report of a data mining method that increases the privacy of a data set without damaging inference power.
this is a significant result since prior studies have reported that the application of standard privacy methods such as k anonymity l diversity or t closeness both damaged inference power and offered little overall improvements in privacy.
hence for learning from data that must be privatized we recommend morphing over k anonymity l diversity or t closeness.
ii.
b ackground in this section we provide background on the following a defect prediction b the cross company defect prediction process c privacy preserving data publishing d problems with privacy and provide e attack models and f utility models used in this paper.
a. defect prediction boehm papaccio advise that reworking software e.g.
to fix bugs is cheaper earlier in the life cycle than later by factors of to .
other research makes the same conclusion.
a panel at ieee metrics concluded that finding and fixing severe software problems after delivery is often times more expensive than finding and fixing them during the requirements and design phase .
defect prediction allows software companies to take advantage of early defect detection.
models made for defect prediction are built with within company data sets using common machine learners.
the data sets are comprised of independent variables such as the code metrics used in this work and one dependent variable or prediction target with values labels to indicate whether or not defects are present.
a prediction model created from a defect data set can then take a new unlabeled instance and label it as defective or not defective .
when evaluating the prediction models we use the cross validation technique.
here the defect data is divided into ten folds.
one fold is used as a testset representing new unlabeled instances while the other nine folds are combined and act as the training set usedto create prediction models.
the test instances are then labeled according to the models and these new labels can be compared to the original labels using various performance measures discussed in section ii f .
b. cross company defect prediction when data can be shared between organizations defect predictors from one organization can generalize to another.
for example defect predictors developed at nasa have also been used in software development companies outside the us in turkey .
when the inspection teams focused on the modules that trigger the defect predictors they found up to of the defects using just of their qa effort measured in staff hours .
such cross company learning is useful since as zimmermann et.
al.
observed defect prediction via local data is not always available to many software companies as the companies may be to small.
the product might be in its first release and so there is no past data.
kitchenham et al.
also see problems with relying onwithin company data sets.
they note that the time required to collect enough data on past projects from a single company may be prohibitive.
additionally collecting withincompany data may take so long that technologies used by the company would have changed and therefore older projects may no longer represent current practices.
initial experiments with cross company learning were either very negative or inconclusive .
recently we have had more success using better selection tools for training data but this success was only possible if the learner had unrestricted access to all the data.
as discussed below this is a problem.
c. privacy preserving data publishing data sharing across companies exposes the data provider to unwanted scrutiny.
some of these concerns reflect the low quality of our current anonymization technologies.
for example the state of massachusetts once released some healthcare data anonymized according to hippa regulations when this supposedly anonymized data was joined to other data voters lists and census data it was possible to find data of prominent individuals such as former massachusetts governor william weld .
we say that re identification occurs when an attacker with external information such as census data can identify an individual from a privatized data set.
for the kinds of data used in this study which are aggregated at the project level but not at the developer level these data sets do not contain personnel information.
hence re identification of individuals is not explored further in this study.
on the other hand sensitive attribute value disclosure is of great concern with the data used in this study.
this is where an individual in a data set can be associated with asensitive attribute value e.g.
to identify the quality attributes of corporations in data sets.
such sensitive attribute value disclosure can prove problematic.
some of the metrics contained in defect data can be considered as sensitive to the data owners.
these can include any size measures such as lines of code loc or cyclomatic complexity max cc or avgcc .
if these size measures are joined to development time it would be possible for rivals competing on a bid to discredit their opposition by revealing say slower development times amongst their competitors.
the goal of privacy preserving data publishing ppdp is to ensure that the published data can preserve individual privacy while remaining useful for workloads such as data mining or defect prediction.
the data is looked at as having three components quasi identifiers qids such as age and zip code that help identify an individual in a data set sensitive attributes s that are a privacy threat when matched to an individual e.g.
medical diagnosis and non sensitive attributes.
as discussed next the goals of ppdp are hard to reach.
d. problems with privacy many researchers comment on how privatization algorithms can distort data.
for example consider privatization via these three methods replacing exact numeric values with intervals that cover a range of values e.g.
might become replacing symbols with more general terms e.g.
date of birth becomes month of birth or engineer or lawyer becomes professional suppressing exact values by replacing them e.g.
replace specific values with don t know or perturbing specific values by an amount selected randomly from a distribution.
according to these methods hide potentially important details in the qid that can confuse a data miner.
worse these transforms may not guarantee privacy.
for example consider privacy via perturbation e.g.
data swapping .
suppose an attacker has access to multiple independent samples from the same distribution from which the original data was drawn.
in that case a principal component analysis could reconstruct the transform from the original to privatized data .
widely used privatization approaches include kanonymity l diversity and t closeness .
k anonymity makes each record in the table be indistinguishable with k other records by suppression or generalization .
the limitations of k anonymity as listed in are many fold and include the fact that it does not hide whether a given individual is in the database.
also in theory k anonymity hides uniqueness and hence identity in a data set thus reducing the certainty that an attacker has uncovered sensitive information.
however in practice k anonymity does not ensure privacy if the attacker has background knowledge of the domain .machanavajjhala et al.
proposed l diversity .
the aim of l diversity is to address the limitations of k anonymity by requiring that for each qid group2 there are at least ldistinct values for each sensitive attribute value.
in this way an attacker is less likely to guess the correct sensitive attribute value of any member of a qid group.
work by li et al.
later showed that l diversity was vulnerable to skewness andsimilarity attacks making it insufficient to prevent attribute disclosure.
hence li et al.
proposed t closeness to address this problem.
t closeness focuses on keeping the distance between the distributions of a sensitive attribute in a qid group and that of the whole table no more than a threshold tapart.
the intuition is that even if an attacker can locate the qid group of the target record as long as the distribution of the sensitive attribute are similar to the distribution in the whole table any knowledge gained by the attacker cannot be considered as a privacy breach because the information is already public.
however with t closeness information about the correlation between qids and sensitive attributes is limited and so causes degradation of data utility .
in practice the above issues with privacy algorithms are very real problems.
grechanik et al.
found that kanonymity greatly degraded the test coverage of data centric applications.
furthermore brickell and shmatikov report experiments where to achieve privacy using the above methods requires almost complete destruction of the data mining capability .
they concluded that depending on the privatization parameter the privatized data provided no additional utility vs. trivial privatization.
worse they also reported that simplistic trivial privatization provides better privacy results than supposedly better methods like l diversity t closeness and k anonymity.
the reason for this poor performance of such widely explored privacy algorithms is unclear.
we speculate that the empirical basis for certifying these standard privacy methods may not be very strong.
fung et al.
report that one dataset the records of adult see is the de facto benchmark for testing anonymization algorithms and list papers that use it as the only test case for their algorithms.
in this regard our results have more external validity since we base our experiments on ten different data sets caveat they have some similarities in that they are all are open source object oriented ja v a projects .
e. attack models to precisely define privacy we must first define how we model an attacker trying to access data.
in this section we review three standard attack models from the literature and present the one used in this work which was adapted from brickell and shmatikov we will use this last one since it includes and extends the others .
2a qid group is a set of instances whose qid values are the same because of generalization or suppression.before continuing note that examples used to explain the following attack models and later on sensitive attribute disclosure section ?
?
are based on the top andbottom tables of figure .
the data set used is an abbreviated ant13defect data set from the promise data repository in the examples to follow top represents an original data set while bottom is the anonymity version of top.
machanavajjhala et al.
s attack models the homogeneity attack and the background knowledge attack are defined in and are seen as an attack on k anonymity.
the homogeneity attack results in groups that leak information due to the lack of diversity in the sensitive attribute.
for an example of this kind of attack consider figure .
if an attacker knows that a target file has the value of forrfc and zero for lcom q rfc11 lcom0 the bottom table produces two possibilities for the target file types.enumeratedattribute and nobannerlogger .
since both have the same sensitive attribute value the attacker is sure that the target file has lines of code.
the background knowledge attack is explained by the following example.
using the anonymity data of figure suppose the attacker has the following knowledge about a target file q cbo8 .
here q says that the query size is and cbois a qid with the value .
from the results of that query the attacker knows that the target is contained in the first group of files taskdefs.executeon taskdefs.cvs and defaultlogger with sensitive values of and respectively.
assuming that the attacker has additional knowledge that files with cbovalues 8tend to also have locless than then the attacker can be almost certain that the target file has lines of code.
li et al.
similarity attack unlike the previous attack models the similarity attack explored by li et al.
is considered as an attack on l diversity.
this attack occurs when sensitive attribute values in a qid group are distinct but semantically similar for instance if the sensitive attribute values fall within a narrow range.
consider the following example suppose the anonymity data in figure consisted of all but the last two records.
this will leave a table that conformed to diversity.
if an attacker poses the query q cbo8 the first group of files is returned as before.
with this result an attacker will know with guarantee that the target has either or lines of code.
although this may seem like a good result in terms of privacy since locis a numerical attribute this narrow range of results can be considered as sensitive information being revealed .
this is because the attacker can infer that the target file has a relatively high number for loc.
brickell and shmatikov attack this attack model is based on the previous models.
to investigate how well the original defect data is privatized we assume the role of an attacker armed with some background knowledge from the original data set and also supplied with the private data set.id quasi identifiers qids s name wmc dit noc cbo rfc lcom ca ce loc taskdefs.
executeon11 default logger14 taskdefs.
taskoutputstream3 taskdefs.
cvs12 taskdefs.
copyfile6 types.
enumerated attribute5 nobanner logger4 id quasi identifiers qids s name wmc dit noc cbo rfc lcom ca ce loc taskdefs.
executeon11 taskdefs.
cvs11 default logger11 taskdefs.
taskoutputstream taskdefs.
copyfile types.
enumerated attribute nobanner logger figure privatization example using an abbreviated ant13dataset from the promise data repository the number of lines of code loc is the sensitive attribute s .
top this represents the original data set.
bottom this is the original data set after anonymity using generalization and suppression.
in order to keep the privatized data set truthful brickell and shmatikov kept the sensitive attribute values as is and privatized only the qids.
however in this work in addition to privatizing the qids with morph we apply equal frequency binning efb 3to the sensitive attribute to create ten sub ranges of values in order to easily report on the privacy level of the privatized data set.
f .
utility model utility is the measure of the usefulness of a data set after privatization.
an inherent understanding in the field of ppdp is that there exists a trade off between privacy and utility in that with increased privacy the utility of a data set decreases .
therefore a measure of privacy alone is not enough to judge the success of a privacy algorithm we also need to judge the utility of the result.
like brickell and shamtikoc we measure the utility of our privatized data set empirically.
using random forests naive bayes and logistic regression we create defect 3a column of data is divided at the ... th percentile to create ten bins with equal frequency.prediction models for the privatized and original data sets.
these learners were chosen since we wanted to test the impact of our privacy tools on a wide range of learners including iterative dichotomizers such as random forests statistical learners such as naive bayes neighbors and parametric methods such as logistic regression.
we then compare the performance of our defect predictors using f measures .
an f measure is the harmonic mean of precision andrecall .
these three measures are defined and calculated as follows let a b c and d represent true negatives false negatives false positives and true positives respectfully recall which reveals how much of the target was found is the result of true positives divided by the sum of false negatives and true positives d b d precision reveals how many of the rows that triggered the detector actually contained the target concept.
it is the result of true positives divided by the sum of false positives and true positives d c d .
f measure allows for a dual assessment of both recall andprecision precision recall precision recall .
it has the property that if either precision or recall is low then the fmeasure is decreased.
note that in those comparisons we set aside some test data privatize the remaining training data then perform our tests on the privatized data.
iii.
morph d esign and privacy framework this section introduces morph a privacy algorithm fordefect data .
it is based on a nearest unlike neighbor nun approach explained in section iii a coupled with randomization.
morph has two goals achieve a proficient level of privacy while maintaining the utility of the defect data.
in other words defect predictors built from the privatized data must be as good as those from the original data set.
further the privatized data should reveal little or no excess information than the original data.
to fulfill these requirements morph changes each row of the original data just enough to avoid a change in its outcome that is its class defects label must not change.
in section iii a we describe morph in detail.
this is followed by the privacy metric used to measure its performance.
a. privacy algorithm morph acts to preserve the general topology of the space of the instances in the data while changing the location of specific instances within that space.
morph is an instance mutator that changes the attribute values of each instance by replacing these original values to morphed values.
morph takes care never to change an instance such that it crosses the boundary between the original instance and instances of another class.the morphed instances are created by applying equation to each attribute value of the instance.
yi xi xi zi r letx dbe the original instance to be changed ythe resulting morphed row and z dthe nearest unlike neighbor nun of x. nun is the nearest neighbor of x whose class label is different from x s class label and distance is calculated using the euclidean distance .
the random number ris calculated with the property r delta where .15and .
.
a simple hashing scheme lets us check if the new instanceyis the same as an existing instance and we keep morphing xuntil it does not hash to the same value as an existing instance .
hence we can assert that none of the original instances are found in the final privatized data set.
b. privacy measure in this work we measure privacy in terms of sensitive attribute disclosure sad .
here we determine if an attacker s query will allow them to identify a group of individual files in a data set and associate them with a sensitive attribute value sub range.
in the following sections we describe how the attacker s knowledge is modeled using random queries to measure sad.
the query generator before discussing the query generator a few details must be established.
first to maintain some truthfulness to the data a selected sensitive attribute and the class attribute are not used as part of query generation.
here we are assuming that the only information an attacker could have is information about the qids in the data set.
as a result these attribute values are unchanged in the privatized data set.
for the purposes of experimentation we will assume that each query can be of length or denoted q q andq respectively and are made up of randomly selected attribute value pairs.
to represent the attacker s knowledge we first discretize the original data set using efb.
for these experiments we will create equal frequency bins.
after binning is complete queries of lengths and are randomly generated.
for each query length we generate up to queries because it is not practical to test every possible query with these data sets the number of possible queries with arity and no repeats is .
each query must also satisfy the following sanity checks they must not include attribute value pairs from neither the designated sensitive attribute nor the class attribute they must return at least instances after a search of the original data set they must not be the same as another query no matter the order of the individual attribute value pairs in the query.
sensitive attribute disclosure once a query is generated it is used on the original and privatized data sets to select instances that adhere to the query.
sad is then measured for each of the ten sensitive attribute subranges using the following formula sadi vi g where viis the number of rows with a specific sensitive attribute sub range and gis the number of rows selected by a query.
the intuition here is that an attacker will use a best guess approach i.e.
find the sensitive attribute sub range with the highest sad using equation .
if a query results in the same sub range being selected for both the original and privatized data sets then we consider this to be a breach of privacy.
on the other hand if they are different then the attack is unsuccessful and privacy is preserved.
here is an example for one query using the tables in figure we create sensitive attribute sub ranges and .
assume that an attacker has the following query from our original data set top q cbo8 .
applying this query to top produces the following sad results sad0 3andsad1 .
here the highest sad returns the range .
applying this query to the privatized data set bottom produces the following sad results sad0 1andsad1 .
here the highest sad returns the range .
since the ranges returned are different we say that the attacker s query did not breach the privacy of the data set and so the attacker s best guess was wrong.
increase privacy ratio finally we can define the ipr.
when all the sensitive attribute sub ranges for each query are selected via the attacker s best guess strategy they are compared.
if the query returns the same sub range for the original and the privatized data sets this is counted as a privacy breach.
we then find the total number of breaches from all the queries and return the percentage.
next for the purposes of reporting the effects of morph on privacy we say that the ipr is the ratio of the baseline of full disclosure i.e.
the ability of the attacker to guess correctly for all queries i.e.
to the percent of actual correct guesses by the attacker ipr correct based on our discussions with business users on different methods for defining privacy we prefer equation to say the entropy measures used by clause et al.
.
our users find the simplicity of the above expression easier to understand than shannon s entropy.
iv.
e xperimental evaluation in this section we evaluate morph using a within and cross data study.
to show how our privacy frameworkperforms against a state of the art privacy algorithm we compare our work to data swapping a standard perturbation technique used for privacy discussed in section iv a .
in our experiments the weka implementation of random forests logistic regression and naive bayes are used to create defect models.
in order to assist replication our data comes from the on line promise data repository figure lists those data sets and figure describes their attributes.
the class labels are made categorical denotes no defects and denotes more than zero defects.
using these learners we conducted two studies for the within a cross validation experiment was conducted where the defect data was shuffled and then separated into ten bins.
for each bin morph is applied to nine of the ten bins and tested on the remaining bin.
in order to avoid order effects where the conclusions are a result of some serendipitous ordering of the data this process is repeated times using different random shuffles of the rows in the data sets.
for the cross study for all data sets each acts as a testset while the remaining data sets are training sets used as cross company data.
recall from the introduction that our results are geared toward answering the following research questions rq1 does morph improve privacy?
rq2 does morph improve privacy better than anything else?
rq3 does morph degrade performance?
rq1 and r2 accounts for the privacy level of the privatized defect data sets.
this is measured via an attack model to determine if an attacker can associate an instance in the privatized data set to a sensitive attribute value range.
with rq1 and r2 we directly address the main concern of data owners will privacy be preserved after data has been published?
unlike rq1 and r2 in rq3 we examine the performance of defect models created from privatized data sets based of the original data.
the goal here is to show data owners and researchers that privatized data can be used to make data symbol instances attributes class defect ant13 ant .
arc arc .
camel10 cam poi15 poi .
redaktor red .
skarbonka ska tomcat tom velocity14 vel xalan24 xal .
xerces12 xer .
figure defect data set characteristicsamc average method complexity e.g.
number of ja v a byte codes avg cc average mccabe average mccabe s cyclomatic complexity seen in class ca afferent couplings how many other classes use the specific class.
cam cohesion amongst classes summation of number of different types of method parameters in every method divided by a multiplication of number of different method parameter types in whole class and number of methods.
cbm coupling between methods total number of new redefined methods to which all the inherited methods are coupled cbo coupling between objects increased when the methods of one class access services of another.
ce efferent couplings how many other classes is used by the specific class.
dam data access ratio of the number of private protected attributes to the total number of attributes dit depth of inheritance tree ic inheritance coupling number of parent classes to which a given class is coupled includes counts of methods and variables inherited lcom lack of cohesion in methods number of pairs of methods that do not share a reference to an instance variable.
locm3 another lack of cohesion measure ifm a are the number of methods attributes in a class number and a is the number of methods accessing an attribute then lcom3 a summationtexta j aj m m .
loc lines of code max cc maximum mccabe maximum mccabe s cyclomatic complexity seen in class mfa functional abstraction number of methods inherited by a class plus number of methods accessible by member methods of the class moa aggregation count of the number of data declarations class fields whose types are user defined classes noc number of children npm number of public methods rfc response for a class number of methods invoked in response to a message to the object.
wmc weighted methods per class defects defects number of defects per class seen in post release bug tracking systems.
figure the c k metrics of the data sets used in this work see figure .
the last row is the dependent variable.
adequate defect models which produce comparable and or negligibly degraded results to the original data.
a. data swapping in order to benchmark our approach we need to compare it against some alternative.
since grenchanik et al.
and brickell and shmatikov report that k anonymity l diversity and t closeness damage the utility of a data set we elect to use another privacy algorithm.
since morph is a perturbation technique for generating synthetic data for our comparisons we implemented data swapping a standard perturbation technique used for privacy .
this is a permutation approach that deassociates the relationship between a qid and a numerical sensitive attribute.
in our implementation of data swapping for each qid a certain percent of the values are swapped with any other value in that qid.
for our experiments these percentages are and .
b. defect predictors many machine learning techniques have been used for the purpose of defect prediction .
for this work we focus on random forest an ensemble method and two statistical methods naive bayes and logistic regression.
recall these were chosen since they are very different algorithms iterative dichotomization statistical parametric .
the weka implementation of these methods are used along with any default values .
below is a brief description of these methods.
detailed descriptions can be found elsewhere .
random forests rf breiman describes rf as a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest.
each tree in the collection is used to classify a new instance.
the forest then selects a classification by choosing the majority result.
naive bayes nb lewis describes nb as a classifier based on baye s rule.
it is a statistical based learning scheme which assumes that attributes are equally important and statistically independent.
to classify an unknown instance nb chooses the class with the maximum likelihood of containing the evidence in the test case.
logistic regression lr afzal describes lr as a method to be used when the dependent variable is dichotomous e.g.
either fault prone or non faultprone .
the method avoids the gaussian assumption used in standard naive bayes.
c. methodology in the field of privacy preserving data publishing it is often remarked that there is a tension between gain in privacy results and degradation in the utility of that data .
therefore one cannot report on the privacy of a dataset without considering its effects on data utility i.e.
the usefulness of the privatized data.
to that end we report two sets of results the increased privacy ratio measured via equation theutility of the learning measured via the f measure .algorithms symbol meaning morph m data privatized by morph swappedx sx s10 s20 s40 xrepresents the percentage of the original data swapped figure algorithm characteristics note that prior results reported that attempts to increase privacy tended to damage utility .
d. results this section presents the results of our experiments.
before going forward figure shows the notation and meaning of the algorithms used in this work.
rq1 does morph improve privacy?
figure shows the increase privacy ratios seen in these experiments.
to generate that figure we first privatize our data sets using morph and data swapping guided by the policy that the sensitive attribute values of locand thedefects attribute remain unchanged after privatization.
next using the query generator described in section ii queries the attackers background knowledge are drawn from the original datasets.
applying equation we then calculated the increased privacy ratio by comparing the original and privatized data sets.
the sub ranges with the highest sads from each are chosen and compared using equation so our results are in that sense worst case since we focus on the largest sads and the least increased privacy ratios .
the results of those calculations are shown in figure .
interestingly the more knowledgeable the attacker the less they can learn from morphed data or data privatized by data swapping.
as we increase the background knowledge of the attacker from one attribute to two to four the increased privacy ration increases .
this is due to the randomization of the data provided by morph.
as we push data into random corners it becomes less lucky that larger and larger conjunctions will find items of interest in the morphed data.
the same can be said for the data swapping algorithms.
from these results the worse case scenario for privacy is the attacker with the most general knowledge i.e.
fewest constraints smallest queries .
in that worse case the size of the query is one and the median increased privacy ratio for morph is .4while the data swapping algorithms s10 s20 and s40 are .
.3and4.2respectively.
that is even in the worst case morph is able to a make data set at least four times more private than the original data set.
rq2 does morph improve privacy better than anything else?
it is important to note that rq2 says anything else and not everything else .
while we have shown that morph performs better than data swapping for s10 and s20 and has comparable results for s40 see figure we have not explored the space of all possible data privacy algorithms.
this paper has compared to privacy methods that have some successful track record i.e.
we compared to dataswapping but not say k anonymity since the brickell and shamtikoc results were so negative on that approach .
in future work we will explore a broader range of methods.
rq3 does morph degrade performance?
figure and figure report high f measures for both cross and within learning with two exceptions the within naive bayes experiment in figure 6b with f measures below for the poi15 poi data set the f measures forpoi15andvelocity14 vel for the cross experiment which uses random forests for defect prediction see figure .
also shown on those figures are the f measures seen in the privatized data.
both morph and the data swapping algorithms perform just as well as the original data sets in most cases for both the within andcross experiments.
with morph however poi15 is somewhat different to the other data sets since when it was used as a training set for crosscompany learning with redaktor red andskarbonka ska see figure it performed worse than all the other results.
the reason for this outlier is unknown and will be subject of our future work.
otherwise for nine out of ten training sets those privatized by morph remain as useful as the original data.
v. t hreats to validity as with any empirical study biases can affect the final results.
therefore any conclusions made from this work must be considered with the following issues in mind first sampling bias threats any data mining experiment i.e.
what matters there may not be true here.
for example the data sets used here comes from the promise repository and supplied by one individual.
the best we can do is define our methods and publicize our data so that other researchers can try to repeat our results and perhaps point out a previously unknown bias in our analysis.
hopefully other researchers will emulate our methods in order to repeat refute or improve our results.
second another source of bias in this study is the learner used for the defect prediction studies.
data mining is a large and active field and any single study can only use a small subset of the known data mining algorithms.
in this work results for naive bayes random forests and logistical regression are published.
third the field of privacy preserving data publishing is active and so it would be difficult compare the performance of morph to all of them.
in this work we chose to use data swapping as a baseline to judge the performance of our method.
last the utility of a privatized data set can be measured semantically where the workload is unknown or empirically known workload e.g.
classification or aggregate query answering .
in this work we measure utility empirically for defect prediction.
as a result we cannot guarantee positive outcomes for other utility measures.figure increased privacy ratio ipr of query sizes and .
figure within results f measures compared among the original and privatized data seen in way cross validation experiments.
vi.
r elated work a. privacy research in software engineering to the best of our knowledge this is the first paper to address the issue of privacy in cross company defect prediction.
however it is closely related to privacy work for software testing and debugging .
here although the work uses within company data the privacy becomes an issue when it involves outsourcing the testing to third parties as is the case with or collecting user information after a software system has been deployed .
in the former case since companies do not wish to release actual cases for testing they anonymize the test cases before releasing them testers.
in this situation if the test cases are not able to find the bugs like the original data then the field of outsourced testing is in danger.
similarly cross company prediction can suffer the same fate.
clause et al.
presented an algorithm which anonymizes input sent from users to developers for debugging.
since the information from the user is likely to contain sensitive information clause et al.
proposed a method which relies on the premise that there is more than one path to a bug .
in other words their aim is to supply the developer with anonymized input which causes the same failure as the original input.
to accomplish this they first use a novel path condition relaxation technique to relax the constraints in path conditions thereby increasing thenumber of solutions for computed conditions.
next the author applied a technique called breakable input conditions to ensure that the constraints do not select values from the original input data.
in contrast to the work done in taneja et.
al.
proposed priest.
unlike our work which privatizes data randomly within nun border constraints the privacy algorithm in priest is based on data swapping where each value in a data set is replaced by another distinct value of the same attribute.
this is done according to some probability that the original value will remain unchanged.
an additional difference to our work is in the privacy metric used.
they make use of a guessing anonymity technique that generates a similarity matrix between the original and privatized data.
the values in this matrix are then used to calculate three privacy metrics mean guessing anonymity fraction of records with a guessing anonymity greater than m 1and unique records which determine if any records from the original data remains after privatization.
morph takes a different approach to the above research.
firstly the techniques of clause et al.
assumes detailed connection knowledge between parts of a system.
such detailed connection knowledge is not present in defect datasets like figure .
as to priest we have some reservations to data swapping methods that use fixed distributions since zhang zhao argue that that distribution can be reverse engineered from the privatized data .figure cross results the y axis shows f measures compared among the original and privatized data.
here random forests is chosen to build the defect models.
naive bayes and logistic regression have similar results but these are left out due to the space constraint.
key original m s10 s20 s40 vii.
c onclusion studies have shown that early detection and fixing of defects in software projects is less expensive than finding de fects later on .
organizations with local data can take full advantage of this early detection benefit by withincompany defect prediction.
when an organization does not have enough local data to build defect predictors they might try to access relevant data from other organizations in order to perform cross company defect prediction.
that access will be denied unless the privacy concerns of the data owners can be addressed.
current research in privacy seek to address one issue i.e.
providing adequate privacy for data while maintaining the efficacy of the data.
however reaching an adequate balance between privacy and efficacy has proven to be a challenge since intuitively the more data is privatized the less useful the data becomes.
to address this issue in this paper we presented morph a privacy algorithm designed to privatize defect data sets for cross company defect prediction.
unlike previous studies we show that morph increases data privacy of data without damaging our ability to reason about that data.
note that this is a significant result since prior work with the standard privatization technologies could not achieve those two goals.
our work was framed in terms of three research questions rq1 does morph improve privacy?
measured in terms of exposure of sensitive attributes it was shown in figure that morphed data is four times or more private than the original data.
rq2 does morph improve privacy better than anything else?
figure shows that morph does better than s10 and s20.
however it is comparable to s40.
rq3 does morph degrade performance?
we showed in figure and figure that for the purposes of learning defect predictors training on the morphed data is just as effective as training on the unmorphed data i.e.
morphing does notdegrade inference efficacy.
we hope that this result encourages more data sharing more cross company experiments and more work on building software engineering models that are general to large classes of systems.
our results suggest the following future work the experiments of this paper should be repeated on other data sets.
the current nun algorithm is o n2 .
we are exploring ways to optimize that with some clustering index method e.g.
k means .
while figure showed that we can increase privacy they also showed that we cannot guarantee it.
at this time we do not know the exact levels of privacy required in industry or if the results of figure meet those needs.
this requires further investigation.