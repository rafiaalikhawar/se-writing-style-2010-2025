leveraging test generation and specification mining for automated bug detection without false positives michael pradel department of computer science eth zurichthomas r. gross department of computer science eth zurich abstract mining specifications and using them for bug detection is a promising way to reveal bugs in programs.
existing approaches suffer from two problems.
first dynamic specification miners require input that drives a program to generate common usage patterns.
second existing approaches report false positives that is spurious warnings that mislead developers and reduce the practicability of the approach.
we present a novel technique for dynamically mining and checking specifications without relying on existing input to drive a program and without reporting false positives.
our technique leverages automatically generated tests in two ways passing tests drive the program during specification mining and failing test executions are checked against the mined specifications.
the output are warnings that show with concrete test cases how the program violates commonly accepted specifications.
our implementation reports no false positives and true positives in ten well tested java programs.
keywords bug detection specification mining false positives i. i ntroduction programs often access libraries and frameworks via application programming interfaces apis .
while this approach reduces development effort by reusing existing functionality it entails challenges for ensuring the program s correctness.
many apis impose protocols that is temporal constraints regarding the order of the calls of api methods.
for example calling peek onjava.util.stack without a preceding push gives an emptystackexception and calling next onjava.util.iterator without checking whether there is a next element with hasnext can result in a nosuchelementexception .
api clients that violate such protocols do not obtain the desired behavior and may even crash the program.
this paper presents a fully automatic approach to reveal violations of api protocols.
our work builds upon three existing techniques that on their own do not fully address the problem.
first we build upon automatic test generation .
while test generators achieve good coverage their usefulness is bounded by the test oracle that decides when to report a potential bug.
for example the randoop test generator comes with a small set of manually specified generic test oracles that do not detect all bugs triggered by the generated tests.second we build upon dynamic protocol mining .
protocol miners infer likely api protocols from executions of api clients.
the usefulness of a dynamic protocol miner is bounded by the executions it analyzes .
a protocol miner can infer a protocol only if there is input that exercises paths of an api client that expose the protocol.
third we build upon dynamic protocol checking which verifies whether a program execution violates a given protocol.
similar to a protocol miner the usefulness of a protocol checker is bounded by the executions it analyzes.
the checker can report a protocol violation only if some input triggers the illegal behavior.
as a motivating example consider a simplified version of a problem our analysis found in xalan a program shipped with the dacapo benchmarks a class coffers a public method report .
calling report on a fresh instance of cgives an emptystackexception which is not declared in the public interface of report .
the problem is that c uses the stack api in an unsafe way that exposes users of cto exceptions caused by violations stack s api protocol.
a class using an api should ensure that the api is used safely that is without the risk to throw an exception due to a protocol violation.
a user of ccannot foresee that cmay crash the program unless she inspects c s implementation which however contradicts the idea of modular objectoriented programming.
can test generation protocol mining and protocol checking help finding the problem in class c?
a test generator can produce tests that cover many paths through c including a path that triggers the emptystackexception .
unfortunately an exception alone is an unreliable indicator for a bug generated tests produce many exceptions due to illegal usage for example by passing illegal method arguments.
a protocol miner can infer the protocol for using stack s but requires input that drives clients of the stack class.
finally a runtime protocol checker can detect that cviolates thestack protocol but requires the protocol and input that drives cinto the protocol violation.
we present a fully automatic approach to reveal api protocol violations by combining test generation protocol mining and protocol checking.
our approach consists of three steps.
first we generate tests for the classes of aprogram.
some tests fail because they result in an uncaught exception while other tests pass.
second we analyze executions of passing tests with a protocol miner giving likely specifications of how to use api classes.
third we verify executions of failing tests against the mined protocols and report a warning if a test fails because of a protocol violation.
the approach is easily applicable in practice because it has a high benefit cost ratio.
all the input required by the analysis is the program itself.
the approach does not require specifications as these are extracted by the protocol miner.
furthermore it does not require any input to run the program because all the input is automatically generated.
the output of the analysis is precise and concrete.
it is precise because each warning reveals a protocol violation that certainly leads the program to an exception.
it is concrete because each warning comes with a test case demonstrating how to trigger the problem.
a main benefit of the approach is that all reported warnings are true positives.
the analysis reports a warning only if a protocol is violated and if the protocol violation causes an uncaught exception.
as a result each warning exposes an unsafe api usage where a class uses an api in a way that exposes clients of the class to the risk of unintentionally violating the api s protocol.
in principle the approach might report a false warning when two conditions hold.
first an api method mmust be implemented incorrectly and throw an exception despite being called legally.
second a mined protocol must forbid calling mat a state where a permissive protocol would allow it.
one of the two conditions alone does not lead to false warnings.
in particular incomplete protocols alone which may result from dynamic protocol mining do not cause false positives.
in practice both conditions together are very unlikely to occur and we did not encounter this situation during our experiments.
we implement the approach based on two existing tools a dynamic protocol miner to extract api protocols and a random test generator randoop .
to evaluate the approach we apply it to ten real world java programs .
the analysis reports no false positives and true bugs all of which are illegal api usages that can be triggered via public methods of the programs.
applied to the same programs the best existing approach to dynamically find violations of api protocols without a priori known specifications has reported three warnings two of which are false positives .
as an optional heuristic optimization of our approach we modify randoop by adding a static analysis that guides the random test generation towards those methods that use the api in question.
without this guidance the test generator randomly chooses the methods to call from all methods of a program.
if only a small part of the program uses the api most of the testing effort is wasted.
our analysis statically prioritizes methods that are likely to contribute to calls from the program to the api so that api usage bugs are revealed method prioritization random test generation protocol mining protocol checkingprogram api prioritiespassing tests failing testsprotocols unsafe api usages figure .
overview.
faster.
compared to standard randoop this optimization increases the number of api calls by a factor of and reveals bugs with five times less testing effort on average.
in summary this paper makes the following contributions no false positives.
an analysis to detect bugs based on mined specifications without reporting false positives.
mining and checking driven by generated tests.
our approach drives both specification mining and program checking with automatically generated input.
guided random test generation.
we reduce the time to trigger an api usage bug by guiding random test generation towards a set of api methods.
ii.
l earning from the good to find the bad in this section we describe our approach to leverage generated tests to reveal api protocol violations without reporting false positives.
figure illustrates the major steps.
given a program and an api as input we use a random test generator to generate tests for the program.
the generated tests can be classified into two groups failing tests that lead the program to an exception or that violate an assertion and passing tests that execute without any obvious error.
we analyze executions of passing tests with a dynamic protocol miner which produces protocols describing common api usage patterns section ii b .
then we analyze executions of failing tests with a dynamic protocol checker section ii c .
this last step filters the various failing tests by selecting those that fail because the program violates a protocol removing all tests that fail for other reasons such as illegal input section ii d .
finally the protocol violations are reported to the developer.
figure also shows method prioritization a static analysis that heuristically computes how relevant each of the program s methods is for triggering calls into the api.
method prioritization is an optional optimization and can be ignored for the description of our general approach.
section iii explains how method prioritization makes our technique even more useful in practice.
a. background protocol mining specification mining extracts formal specifications from existing software.
this work focuses on protocols that describe in which order api methods1 3stack !ss.push s.push s.pop s.peek s.removeallelements figure .
mined protocol for java.util.stack .
states with gray background are liable states.
are commonly called.
several techniques for mining such protocols from an api or from programs using an api have been proposed.
we use a scalable protocol miner that infers protocols involving one or more interacting objects .
figure shows an example of a mined protocol for stack .
an empty stack allows only calls to push .
once elements have been pushed on the stack one can call push pop and peek in an arbitrary order until calls to removeallelements empty the stack again.
this protocol is a regular language approximation of the more precise property that push andpop are balanced.
despite this approximation the protocol can reveal illegal uses of stacks as we show subsequently.
a protocol consists of a finite state machine and a set of typed protocol parameters that represent the objects involved in the protocol.
the states of a protocol represent states of the involved objects.
the transitions represent method calls where receiver and optionally method parameters and the return value are protocol parameters.
a call r m a1 a n !oencodes the method identifier m the receiver r the arguments a1 a n and the return value o. the protocols created by the protocol miner distinguish two kinds of states setup states andliable states .
the setup states establish which objects interact in the protocol by binding objects to protocol parameters.
the liable states describe constraints on method calls that a programmer ought to respect.
a protocol must be followed only when the current state is a liable state.
the protocol miner requires an execution trace that is a sequence of method calls observed during an execution.
the protocol miner splits the execution trace into small subtraces that contain calls to one api object or to a set of interacting api objects.
for a subtrace sub types sub denotes the set of receiver types occurring in sub.
the miner summarizes all subtraces with the same types sub for example all subtraces for stack s into a protocol.
whenever a subtrace contains a method call sequence not yet accepted by the protocol the protocol is extended to accept the subtrace.
the resulting protocol accepts all observed subtraces.
random test generation there are various approaches to automatically generate tests section vi b .
this work builds upon feedback directed random test generation implemented in randoop .
we use randoop because it requires no input except the program under test scales to large programs and generates tests with highcoverage.
randoop randomly selects methods and chains them into call sequences.
if executing a call sequence neither produces an exception nor violates an assertion the sequence is used to construct further call sequences.
the output are passing and failing junit tests.
b. driving protocol mining with passing tests dynamic protocol mining is inherently limited by the execution traces given to the miner which in turn is limited by the input used to execute the program.
most programs come with a finite set of inputs for example given as unit tests.
producing more input requires manual effort reducing the overall usefulness of protocol mining.
we address this limitation by combining protocol mining with test generation.
the idea is to analyze an execution of a generated test in the same way as traditional specification mining analyzes executions driven by otherwise available input.
a benefit of this approach is that generated tests are a large and diverse source of inputs.
random test generation provides fresh inputs for different random seeds which can trigger behavior not yet triggered with other random seeds .
to mine the api usage of a program driven by generated tests we proceed as follows.
at first we use aspectj to instrument the program so that it writes an execution trace with all api calls to a file.
we create one file per generated sequence of method calls.
then the protocol miner analyzes the execution traces and extracts protocols describing in which order the program calls api methods.
call sequences of different tests that trigger the same api class contribute to the same protocol.
the main difference to traditional specification mining is that we drive the program with generated tests instead of other input.
a potential problem of generating tests for protocol mining is that artificial input may trigger illegal api call sequences that can lead to incorrect protocols.
we address this problem in two ways.
first we feed only execution traces of passing tests into the protocol miner not execution traces of failing tests.
while a passing test does not guarantee to use the program in the intended way we found that in practice most illegal inputs are filtered because they would lead to an exception.
second the execution trace contains only api calls from the program not calls from the generated tests a generated test can call api methods for example to create an api object before passing it as a parameter .
that is each api call the miner learns from is part of the program and not a call that was randomly generated by randoop.
c. checking failing tests against mined protocols a dynamic protocol checker can verify whether a program violates commonly accepted constraints by checking the program against mined protocols.
for a given programexecution a checker guarantees soundness and completeness all reported protocol violations provably occur in the execution and all protocol violations in the execution are reported.
unfortunately this approach can only reveal a problem if the problem occurs in the analyzed program execution that is if the problem is triggered by the given input.
for example checking the execution of a manually created unit test suite against a set of protocols is unlikely to reveal many protocol violations because the test suite exercises well tested paths of the program.
we address this limitation by driving the program with generated tests.
in contrast to the mining step of our approach we now use generated tests that fail.
these tests are classified as failing because they trigger a problem in the program that leads to an exception or an assertion violation.
many failing tests do not expose any bug but fail because the generated test uses the program incorrectly.
for example a test may fail with a nullpointerexception or anillegalargumentexception because the test passes null or another illegal value as a method argument.
such tests are not relevant for programmers as they do not reveal bugs in the program.
to focus on tests that fail because of a bug in the program we check whether the program violates a protocol during test execution as those tests expose an unsafe api usage in the program and therefore are relevant for programmers.
that is we use protocol checking as a filter that identifies true bugs among all failing tests.
the checker verifies the execution trace of each failing test against the set of mined protocols.
conceptually the approach is similar to existing runtime verification tools such as javamop .
our implementation uses the infrastructure built for mining protocols.
at first the execution trace is split into subtraces in the same manner as for protocol mining .
then each subtrace subis verified against all protocols that have protocol parameters with types equal to types sub .
the checker assigns each receiver object in the subtrace to a protocol parameter of matching type.
for subtraces with multiple receiver objects of the same type the checker creates multiple protocol instances one for each possible assignment of objects to protocol parameters.
at the beginning of a subtrace the protocol is in the initial state.
the checker goes through all calls in the subtrace and analyzes all calls that appear in the alphabet of the protocol.
each such call is matched against the outgoing transitions of the current state.
as all mined protocols are deterministic each call matches at most one transition.
if a transition matches the target state of the transition becomes the current state.
if no transition matches and the current state is a liable state the checker reports a protocol violation.
non matching transitions in setup states are ignored.
for example consider the source code in figure 3a which is the problem described in section i. the class cuses a stack in an unsafe way because a client of ccan trigger a violation of the stack protocol via c s public interface.class c private stack s newstack public string report return get .
tostring private object get s.peek public void fill s.push .. a source codenew stack !s s.peek emptystackexception at stack.peek at c.get at c.report .
.
.
b execution trace figure .
example code that can violate the stack protocol and execution trace exposing the violation.
we consider this to be an api usage bug because c s public interface does not reveal this risk and understanding it requires knowledge of c s implementation.
our approach finds this problem because randoop generates a test that triggers an api protocol violation by calling c s methods.
the test calls report without a preceding call to fill and therefore triggers a call to peek while the stack is empty.
this call causes an emptystackexception and fails the test.
figure 3b shows the execution trace of the failing test.
the trace contains all calls to stack that were observed during the test execution.
the checker finds a violation of the protocol in figure because the second call in the trace tries to call peek at state but peek is only allowed at state .
d. warnings without false positives reporting few ideally no false positives is crucial to make a bug finding technique applicable in practice .
by using runtime checking our approach avoids a problem typically encountered with static checkers namely false positives due to approximations of execution paths.
another potential source of false positives are mined protocols that disallow legal behavior because this behavior was not observed during protocol mining.
our approach eliminates this problem by only reporting those protocol violations that certainly lead to an exception thrown by the api that is to undesired behavior.
for example the call to peek in figure results in an emptystackexception a clear indication that the api is used incorrectly.
to filter protocol violations that cause an exception we need several pieces of information.
for each failing test the analysis logs the reason for the failure which consists of two parts the type texcof the thrown exception and the stack trace s loc1 loc loc n when throwing it.
each stack trace element lociindicates a source code location.
for each protocol violation the protocol checker reports the source code location locviol where the violating api call occurs.
furthermore the analysis statically extracts the set of declared exception types dapi of the api method that is called at locvioland the set of declared exception typesdprogram of the method containing locviol.
an exception can be declared using the throws keyword or using the throws or exception javadoc tags.
our analysis reports a protocol violation to the user only if the following three conditions are true texc2dapi locviol2s texc 2dprogram the first two conditions ensure that the call that violates the protocol is responsible for failing the test.
the third condition avoids warnings for methods that deliberately pass on exceptions thrown by the api to their own clients.
in this case the protocol violating class is implemented safely because it explicitly passes on responsibility for using the api correctly to its clients.
for figure all three conditions hold and our analysis reports an unsafe api usage along with a test case that illustrates how to trigger the problem.
iii.
api g uided testgeneration section ii describes how randomly generated tests can drive a program for mining api protocols and for checking the program against mined protocols.
with a random test generation tool such as randoop many generated tests do not trigger api calls.
the reason is that even though only a subset of all methods of a program uses an api randoop chooses which method to call randomly from all available methods.
this section presents a heuristic to optimize our approach by guiding random test generation towards methods that trigger api calls.
given infinite time standard randoop triggers the same behavior as a guided approach focusing on methods relevant for calling the api.
in practice increasing the number and variety of api calls triggered by generated tests within a finite amount of time is important for two reasons.
first random test generators require a stopping criterion typically wall clock time or the number of generated method call sequences.
second random test generators execute arbitrary code making them susceptible to crashes that are hard to avoid in general .
whatever stops the test generator triggering api calls earlier than a purely random approach not only saves time but also is likely to trigger more api usage bugs before the test generator terminates.
to guide randoop towards methods relevant for triggering api calls we statically analyze the program and prioritize its methods.
the priorities influence the random decisions of the test generator so that calling a method with higher priority is more likely.
methods that do not contribute at all to calling the api get a priority of zero and are ignored altogether by the test generator.
a. computing method priorities we build upon two well known graph representations of the program under test an inverted context insensitive call graph and a parameter graph .
the inverted callgraph is a directed graph where vertices represent methods and edges represent a may be called by relationship between methods.
we extract call graphs with soot .
the parameter graph is a directed graph where vertices also represent methods and where edges represent a can use the result of relationship between methods.
there is an edge from m1tom2ifm1requires an object of type t as receiver or as argument and if m2 s return type is equal to tor a subtype of t. test generators call methods for three reasons.
first because a method is among the methods under test .
in our case these are all methods that call the api.
second because the method returns an object that can be passed as an argument to another method that is the method is a parameter provider .
third because the method may change the internal state of an object that is used for another call afterwards that is the method is a state changer .
our prioritization technique considers these reasons and computes for each method three priority values that indicate how relevant the method is for each reason.
afterwards the priorities are combined into a single priority value per method.
algorithm describes how we compute the priority of each method.
the algorithm takes as input the call graph gc the parameter graph gp the set of all methods m and the set mapi of api methods.
there are four main steps.
algorithm compute priorities for calling methods during random test generation input inverted call graph gc parameter graph gp methodsm api methods mapi output map p method!rassigning priorities to methods priorities for methods under test initialize pmut method!rto zero for all m2mdo for all mapi2mapi withdc mapi m dmax c do pmut m relevance mapi dc mapi m priorities for parameter providers initialize pparam method!rto zero for all m2mwithpmut m 0do for all m02reachable gp m do pparam m0 pmut m nbproviders rettype m0 priorities for state changers initialize pstate method!rto zero for all m2mwithpmut m 0do for all t2inputtypes m do for all m02methods t do pstate pmut m jmethods t j p merge pmut pparam pstate priorities for methods under test at first we compute how relevant a method is depending on whether it calls any of the api methods.
methods calling an api method directly are the most relevant ones.
in addition methods calling api methods indirectly that is via other methods are given a lower priority.
the reason for considering indirect calls is that some methods should be called at a state built up by another method.
for instance a method m1may create a file and call another method m2that writes into the file.
by calling m1 the test generator can trigger successful writes into the file even though m1calls the file api only indirectly.
we limit the level of indirection up to which to consider indirect callers of api methods to dmax c which is set to three in our experiments.
the priority gain of a method calling an api method depends on two factors.
first it increases with the relevance of the called api method relevance mapi jfmj mapi m 2gcgj less frequently called methods are more relevant so that all api methods get an equal chance of being triggered.
second the priority gain decreases with the minimal distance dc mapi m ingcbetween the api method mapi and the calling method m. the rationale for this decrease is that the call graph contains may call and not must call edges fewer indirections to an api call increase the chance that the api is actually called during an execution.
priorities for parameter providers we compute how relevant a method is for providing parameters for calling methods under test.
therefore we consider all methods reachable gp m that are reachable from a method under testmin the parameter graph.
these are all methods that directly or indirectly contribute parameters for calling the method under test.
the gain of priority for such a parameter provider depends in two factors.
first it increases with the priority of the method it provides parameters for.
second the gain of priority decreases with the overall number of providers for the type required by the method under test.
the rationale for this decrease is to give higher priority to methods providing an uncommon type than to methods providing a type returned by many other methods.
priorities for state changers the third kind of priority is for methods changing the internal state of objects used for calling methods under test.
the types inputtypes m calling a method mare the receiver type and all argument types of m. we consider all methods of input types for methods under test as potentially state changing and increase their priority.
the priority gain depends on two factors.
first it increases with the priority of the method under test that the state changer may influence.
second the priority gain decreases with the overall number of methods of the input type.
the rationale for this decrease is to give objects of each input type an equal chance of being changed independently of the number of methods of the type.class a api api a b b .. void init api apipool.some void api.use class b b c c .. void m d d .. class c class d method priority a. a b c c a.init b.m d d figure .
example for prioritizing methods towards using the api.
merging the final step is to normalize the three kinds of priorities and to combine the three priorities of each method in a weighted sum.
the weights indicate how important calling methods under test calling parameter providers and calling state changers are respectively.
in our experiments the test generator devotes of all calls to calling state changers and to each calling methods under test and to calling parameter providers.
b. example figure shows a minimal example to illustrate how prioritizing methods guides random test generation towards calling api methods.
the example contains seven methods and constructors of which one calls the api.
algorithm finds this method as the only method under test and assigns a high priority to it.
to call the test generator requires an object of type a which in turn requires objects of types bandc.
our algorithm finds the constructors of these classes as parameter providers and gives them a medium priority.
the api call in results in a nullpointerexception unless init is called beforehand.
this method is found to be a state changer and also gets a non zero priority.
the remaining two methods b.m and d do not contribute to calling the api and get a priority of zero.
given the priorities the random test generator selects only methods that eventually contribute to calling the api and probabilistically leads to more calls to the api than a purely random approach.
we show this claim to be valid for real world programs and apis in section iv c. iv.
e valuation our evaluation was driven by two main questions.
how effective is our approach in finding unsafe api usages?
we find unsafe api usages in ten welltested java programs.
manual inspection shows all of them to be true positives.
how much does the testing effort reduce compared to a purely random approach by guiding test generation towards an api?
our heuristic optimization reveals bugs with five times fewer generated call sequences than the unguided approach on average.
this improvement istable i analyzed programs number of failing tests out of and api usage bugs found .
program classes failing tests bugs found average per run average per run total coll iter vector enum coll iter vector enum antlr bloat chart .
eclipse fop .
.
hsqldb .
jython .
lucene .
.
pmd .
.
xalan .
.
sum .
.
table ii analyzed api s. api classes coll iter all public methods of java.util.collection java.util.iterator and their subclasses.
vector enum all public methods of java.util.vector java.util.enumeration and their subclasses.
possible because method prioritization increases the number of api calls by a factor of and decreases the sequences required to call a particular api method by a factor of seven.
a. setup we run our analysis on all programs from the dacapo benchmarks version mr2 table i .
the analysis focuses on api classes of the java library table ii .
we run the analysis in two modes unguided andguided towards an api.
unguided runs use randoop s standard test generation technique.
guided runs use method prioritization section iii .
to evaluate guided runs we run randoop for each program api pair.
to evaluate unguided runs we run randoop on each program.
since randoop is based on random decisions we perform each experiment with ten different random seeds .
unless otherwise mentioned the reported results are average values of all ten runs.
randoop requires a stopping criterion.
we use a maximum number of generated method call sequences of .
we prefer this criterion over a maximum running time because it is easily reproducible whereas randoop s execution time depends on the environment used for the experiments.
b. detection of protocol violations the analysis finds a total of protocol violations.
each static source code location with a protocol violation counts only once even if it occurs in multiple failing tests.
table ipublic class mifdocument protected bookcomponent bookcomponent public mifdocument bookcomponent newbookcomponent public void settextrectprop intleft inttop intwidth intheight bookcomponent.curpage .
curtextrect .settextrectprop left top width height public void createpage bookcomponent.pages.add newpage class bookcomponent arraylist pages newarraylist private page curpage return page pages .get pages .size a incorrect usage of arraylist infop.
public class internaltables2 protected iterator iter public void begincanonical iter tableprovid2 classes .
values .
iterator public object next if iter.hasnext cur iter.next return pyjavaclass cur return null public void flushcurrent iter.remove classesdec pyjavaclass cur .
name b incorrect usage of iterator injython .
figure .
examples of bugs found.
shows the number of violations detected for each programapi pair along with the total number of violations found over all runs.
the table also shows the number of failing tests in which the protocol checker finds the bugs.
on average of tests fail too much for a programmer to inspect.
the protocol checker filters these tests and presents only those that expose a protocol bug.
we inspect all protocol violations manually to verify that no false positives are reported.
indeed all reported violations show an illegal api usage that can be triggered via the public methods of a class in the program.
furthermore we manually check whether the comments of the buggy methods contain any natural language warnings about the exceptions that may result from these methods.
none of the reported methods has such a comment.
that is all reported bugs are real problems in the programs.
figure shows two examples of bugs found.
the first example figure 5a was found in fop.
the class mifdocument contains an inner class bookcomponent that stores pages in a list which initially is empty.
theinner class provides a method curpage that tries to return the last page in the list.
this method is called in the public method settextrectprop which raises an indexoutofboundsexception when the list of pages is empty.
this risk is not apparent to clients of mifdocument and hence the class uses arraylist in an unsafe way.
our analysis finds this bug because the mined protocol for arraylist disallows get directly after creating the list.
the second example figure 5b is taken from jython .
the class has an iterator field that is accessed by multiple public methods.
one of them flushcurrent calls remove on the iterator.
the iterator protocol allows this call exactly once after each call to next .
this constraint is not respected in the class causing a potential illegalstateexception when calling flushcurrent .
similar to the first example the class uses an api in an unsafe way and does not make the potential problem apparent to its clients.
our analysis infers the iterator protocol and finds this violation of it.
descriptions of all detected bugs are available at http mp.binaervarianz.de icse2012 leveraging.
an obvious question about bug reports is how to fix the bugs.
during our experiments we have seen two strategies to ensure safe usage of the api and conformance to its protocols.
first an api using method m can ensure to follow the protocols of all api objects that m uses and not to propagate any exception from the api to callers of m .
often such methods return a special value for example null when they cannot return another value.
second an api using method m can propagate api exceptions and declare this in its signature.
this approach indicates to callers of m that calling the method at an illegal state or with illegal input may result in an exception and hence m explicitly passes the responsibility on to its callers.
interestingly many of the detected bugs are surrounded by attempts to shield users of a class from protocol violations.
for example in figure 5b the next method checks by calling hasnext whether a call to next is legal and returns null otherwise.
that is the programmers protect clients of the class from some violations of the iterator protocol but unfortunately forgot about another violation.
comparison to existing approaches we directly compare our analysis to ocd a combined dynamic protocol miner and checker.
ocd also searches for protocol violations by mining and checking protocols at runtime.
two main differences are that ocd relies on existing input to drive the program and that the analysis combines mining and checking into a single run of the program under test.
gabel and su used ocd to analyze the usage of the java standard library in the dacapo programs.
their technique reports three warnings matching gabel and su s expectation to find few problems in these well tested benchmarks.
manual inspection showed two warnings to be clear false positives and one warning to be a potential problem.applied to the same programs our analysis reports warnings that are all true positives.
what are the reasons for these differences?
first our analysis does not report false positives by construction as explained in section ii d. second ocd is limited to the program paths triggered by available input here dacapo s benchmark input .
in contrast the generated tests used by our analysis exercise less frequently tested parts of the programs and trigger api usage bugs not exposed with the benchmark inputs.
c. api guided test generation the goal of guiding test generation towards an api is to reduce the overall time required to reveal api usage bugs.
the most important factor for reducing this time is the number of generated call sequences which controls the running times of test generator protocol miner and protocol checker.
therefore we use the number of generated call sequences as a metric for testing effort.
we compare guided runs of our analysis to unguided runs table iii .
do guided runs trigger more api calls than unguided runs within a fixed number of generated call sequences?
the first block of columns in table iii compares the number of calls to an api.
each value is the average over ten runs of randoop with different random seeds.
for all but one program api pair guided runs trigger more api calls.
for example the number of calls to vector enum triggered in tests for antlr increases from to .
on average over all programs apis and runs guidance improves the number of api calls by .7x median .2x .
increasing the number of api calls is important to give the miner different examples to learn from and to increase the chance to hit a bug.
it is also important to trigger calls to different api methods to expose a broad variety of api protocols and their violations.
the second block of columns in table iii compares the number of distinct api methods called.
in most cases there is no significant difference between guided and unguided runs.
the reason is that our stopping criterion is large enough to give even unguided runs a realistic chance to hit each api method.
how long does it take the test generator to hit an api method?
the third block of columns in table iii shows after how many generated sequences an api method is called the first time.
for all but one program api pair api methods are called earlier in guided runs than in unguided runs.
on average over all programs apis and runs the guided approach calls a method .
times faster than the unguided approach.
that is even though after enough time the same set of api methods is called guided runs achieve this goal much faster than unguided runs.
figure 6a illustrates an unguided and a guided run.
the graphs show the cumulative number of api calls and the number of distinct called api methods depending on how many sequences randoop has generated respectively.
the first graph shows that the number of api calls increasestable iii comparison of unguided and guided test generation .
the last row summarizes the results from all runs .
program api calls called api methods sequences to first call coll iter vector enum coll iter vector enum coll iter vector enum ung.. guid.
ung.. guid.
ung.. guid.
ung.. guid.
ung.. guid.
ung.. guid.
antlr bloat chart eclipse fop hsqldb jython lucene pmd xalan avg.
med.
improvement x56.
x5.
improvement x1.
x1.
reduction x6.
x1.
10000api calls triggered generated call sequencesunguided guided 10000api methods called generated call sequences a effects of guiding test generation using xalan and vector enum as an example.
x1x10x20x30 90improvemt.
of seqs.
to bug runs sorted by improvement x74x109 average x5.
median x1.
b improvement of guided runs over unguided runs in the number of call sequences required to trigger a bug.
figure .
graphical comparison of guided and unguided test generation.
much faster with guidance than without guidance.
the second graph illustrates that the guided run triggers api methods much faster than the unguided run even though both arrive roughly at the same number of api methods after about sequences.
finally does guided test generation trigger bugs earlier than unguided test generation?
figure 6b compares the number of generated sequences required to trigger a bug.
the graph shows the improvement through guidance for each run where a bug was found both with and without guidance.
in most of the runs bugs are triggered faster with guidance with up to 109x improvement.
for some runs our heuristic triggers bugs slower with a slowdown up to 8x.
on average guided runs improve upon unguided runs by 5x.
there is one outlier in table iii for jython and the coll iter api guidance decreases the number of api calls and increases the sequences needed to trigger an api method.
the reason is that almost all methods in jython are relevant for using the coll iter api which skews the method priorities.
after all our guidance technique is a heuristic that even though successful in most cases cannot guarantee improvements for all programs and apis.d.
scalability and performance the total execution time of our analysis is the sum of three components the time for generating tests and executing them the time for mining execution traces and the time for checking execution traces.
all three scale well to large programs.
randoop s execution time is the product of the number of sequences to generate and some program specific factor because randoop executes code from the program .
the performance of the protocol miner scales linearly with the size of the execution traces which is also true for the checker because it builds upon the same infrastructure.
on a standard pc analyzing a program api pair takes between less than a minute and several minutes which we consider to be acceptable for an automatic testing tool.
v. d iscussion our analysis reveals unsafe api usages in classes of a program but does not guarantee that a crash is feasible when using the program as a whole for example via its graphical user interface.
nevertheless all reported bugs are real problems because a programmer can trigger them via the public interface of a class.
that is even if a bug is infeasiblein the current program changes in using the buggy class can make it feasible.
programmers can avoid this risk by ensuring safe api usage.
currently the analysis is limited to detecting violations of api protocols.
future work may apply the approach to other kinds of dynamically inferred specifications such as invariants .
another limitation is that the analysis focuses on bugs that manifest through an exception.
while this approach guarantees all bugs to be severe we might miss more subtle problems that lead to incorrect but not obviously wrong behavior.
vi.
r elated work a. bug finding with mined specifications mined specifications can be used for bug finding by searching for anomalies within the extracted specifications or by feeding the specifications into program checkers .
in contrast to our work these approaches either require input to run a program produce false positives or both.
we present a static bug finding technique based on mined protocols in .
it can analyze those parts of a program that are not covered by generated tests but reports false positives and does not address the problem of exercising programs for protocol mining.
b. test generation this work builds upon random test generation in general and the feedback directed random test generator randoop in particular.
the bugs found by our analysis complement the bugs reported by randoop based on builtin generic test oracles.
our approach extends the built in oracles with inferred protocols that reveal problems randoop is oblivious of otherwise.
jaygarl et al.
modify randoop s random method selection by creating parameters before calling a method and by favoring methods with low coverage in the existing tests.
zheng et al.
propose to select methods that are likely to affect the method under test where influence between two methods is estimated based on whether the methods access a common field.
these approaches increase the coverage of methods under test whereas our guidance technique intensifies the api usage of the tested program.
in contrast to them our guidance technique is more general as it considers both parameters needed to call a method and methods that may influence the state of objects.
a technique for regression testing by jin et al.
guides randoop towards methods that have been modified between two versions.
test generation can be enhanced by mining call sequence models .
xie and notkin propose to combine test generation and specification mining for iteratively enhancing both activities.
dallmeier et al.
concretize this idea by generating tests that explore not yet specified sequences of a protocol and by using the result of executingthese tests to refine the protocol.
the main goal of these approaches is to enhance test generation with specification mining or vice versa whereas we combine test generation and specification mining into an automatic bug finding technique.
xie and notkin integrate invariant inference with test generation to guide the test generator and to select generated tests that are more likely to expose a bug .
in contrast to their work our approach does not require an existing test suite and reports bugs without false positives.
c. comparing passing and failing tests two kinds of approaches leverage the differences between passing and failing tests.
the first group of approaches compares features of passing and failing tests to identify classes and methods that are likely to cause a known failure .
the second group of approaches compares passing and failing tests to generate fixes for a known problem .
in contrast to both approaches our work discovers unknown bugs and leverages test generation instead of relying on existing input such as manually written tests.
d. runtime protocol checking the checking step of our analysis is closely related to runtime verification of protocols .
our checker is conceptually similar to the online checker javamop but works on execution traces and thus avoids some of the challenges faced by javamop for example because we know which objects interact during an execution.
vii.
c onclusions this paper shows how to integrate test generation specification mining and runtime checking into a fully automatic bug finding technique.
we draw several conclusions.
first generated tests are not only useful to reveal bugs but they can also serve as input to drive other dynamic analyses.
this allows our approach to benefit from the precision of dynamic analysis without requiring any input to run a program.
second leveraging passing and failing tests for mining and checking specifications respectively allows for detecting bugs without reporting false positives.
third focusing the efforts spent during random test generation on those parts of a program relevant for the problem at hand increases the effectiveness of the overall approach compared to purely random test generation.