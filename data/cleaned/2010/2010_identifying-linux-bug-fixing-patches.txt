singapor e management univ ersity singapor e management univ ersity institutional k nowledge at singapor e management univ ersity institutional k nowledge at singapor e management univ ersity resear ch collection school of computing and information systems school of computing and information systems identifying linux bug fixing p atches identifying linux bug fixing p atches yuan ti an singapor e management univ ersity davidlo smu.edu.sg julia l awall inri a david l o singapor e management univ ersity davidlo smu.edu.sg follow this and additional works at https ink.libr ary.smu.edu.sg sis r esear ch part of the softwar e engineering commons citation citation tian y uan l awall julia and l o david.
identifying linux bug fixing p atches.
.
34th a cm ieee international conf erence on softwar e engineering icse .
.
available at available at https ink.libr ary.smu.edu.sg sis r esear ch this conf erence pr oceeding ar ticle is br ought t o you for fr ee and open access b y the school of computing and information systems at institutional k nowledge at singapor e management univ ersity .
it has been accepted for inclusion in resear ch collection school of computing and information systems b y an authoriz ed administr ator of institutional k nowledge at singapor e management univ ersity .
for mor e information please email cher ylds smu.edu.sg .
identifying linux bug fixing patches yuan tian1 julia lawall2 and david lo1 1singapore management university singapore 2inria lip6 regal france yuan.tian.
davidlo smu.edu.sg julia.lawall lip6.fr abstract in the evolution of an operating system there is a continuing tension between the need to develop and test new features and the need to provide a stable and secure execution environment to users.
a compromise adopted by the developers of the linux kernel is to release new versions including bug fixes and new features frequently while maintaining some older longterm versions.
this strategy raises the problem of how to identify bug fixing patches that are submitted to the current version but should be applied to the longterm versions as well.
the current approach is to rely on the individual subsystem maintainers to forward patches that seem relevant to the maintainers of the longterm kernels.
the reactivity and diligence of the maintainers however varies and thus many important patches could be missed by this approach.
in this paper we propose an approach that automatically identifies bug fixing patches based on the changes and commit messages recorded in code repositories.
we compare our approach with the keyword based approach for identifying bug fixing patches used in the literature in the context of the linux kernel.
the results show that our approach can achieve a .
improvement in recall as compared to keyword based approaches with similar precision.
i. i ntroduction for an operating system reliability and continuous evolution to support new features are two key criteria governing its success.
however achieving one is likely to adversely affect the other as supporting new features entails adding new code which can introduce bugs.
in the context of linux development these issues are resolved by regularly releasing versions that include new features while periodically designating some versions for longterm support.
development is carried out on the most recent version and relevant bug fixes are backported to the longterm code.
a critical element of the maintenance of the longterm versions is thus the identification of bug fixing patches.
in the linux development process contributors submit patches to subsystem maintainers who approve the submissions and initiate the process of integrating the patch into the coming release.
a maintainer may also forward the patch to the maintainers of the longterm versions if the patch satisfies various guidelines such as fixing a real bug and making only a few changes to the code.
this process however puts an extra burden on the subsystem maintainers implying that necessary bug fixing patches could be missed.
thus a technique that automatically labels a commit as a bug fixing patch would be valuable.in the literature there are furthermore many studies that rely on identifying links between commits and bugs.
these include work on empirical study of software changes bug prediction and bug localization .
all of these studies employ a keyword based approach to infer commits that correspond to bug fixes typically relying on the occurrence of keywords such as bug or fix in the commit log.
some studies also try to link software repositories with a bugzilla by the detection of a bugzilla number in the commit log.
unfortunately these approaches are not sufficient for our setting because not all bug fixing commit messages include the words bug or fix indeed commit messages are written by the initial contributor of a patch and there are few guidelines as to their contents.
linux development is mostly oriented around mailing lists and thus many bugs are found and resolved without passing through bugzilla.
a similar observation was made by bird et al.
who performed an empirical study that showed bias could be introduced due to missing linkages between commits and bugs.
in view of the above limitations there is a need for a more refined approach to automatically identify bug fixing patches.
in this work we perform a dedicated study on bug fixing patch identification in the context of the linux kernel.
the results of our study can also potentially benefit studies that require the identification of bug fixes from commits.
we propose a combination of text analysis of the commit log and code analysis of the change description to identify bug fixing patches.
we use an analysis plus classification framework which consists of the extraction of basic facts from the text and code that are then composed into features.
the learning of an appropriate model using machine learning and its application to the detection of bug fixing commits.
in a typical classification task appropriately labeled training dataset is available.
however this is not the case in our setting.
for positive data i.e.
bug fixing patches we can use the patches that have been applied to previous linux longterm versions as well as patches that have been developed based on the results of bug finding tools.
there is however no corresponding set of independently labeled978 .
c ieee icse zurich switzerland 386negative data i.e.
non bug fixing patches.
to address this problem we propose a new approach that integrates ranking and classification.
we have tested our approach on commits from the linux kernel code repository and compare our results with those of the keyword based approach employed in the literature.
we can achieve similar precision with improved recall our approach s precision and recall are .
and .
while those of the keyword based approach are .
and .
.
our contributions are as follows we identify the new problem of finding bug fixing patches to be integrated into a linux longterm release.
we propose a new approach to identifying bug fixing patches by leveraging both textual and code features.
we also develop a suitable machine learning approach that performs ranking and classification to address the problem of unavailability of a clean negative dataset i.e.
non bug fixing patches .
we have evaluated our approach on commits in linux and show that our approach can improve on the keyword based approach by up to .
in terms of recall while maintaining similar precision.
section ii provides more information on linux longterm and stable kernels.
section iii gives an overview of our bug fixing patch identification framework.
section iv describes the data acquisition and feature extraction processes.
section v describes the learning and application of discriminative models to detect bug fixing patches.
our experiments are discussed in section vi.
we discuss interesting issues in section vii.
related work is presented in section viii.
we conclude and discuss future work in section ix.
ii.
l inux longterm and stable kernels the linux operating system is widely used across the computing spectrum from embedded systems to desktop machines to servers.
from its first release in until the release of linux .
.
in two versions of the linux kernel were essentially maintained in parallel stable versions for users receiving only bug fixing patches over a number of years and development versions for developers only receiving both bug fixes and new features.
since the release of linux .
.
there has been only a single version which we refer to as the mainline kernel targeting both users and developers which includes both bug fixes and new features as they become available.
since the rate of these releases has been roughly one every three months.
the current frequent release model is an advantage for both linux developers and linux users because new features become quickly available and can be tested by the community.
nevertheless some kinds of users value stability over support for new functionalities.
nontechnical users may prefer to avoid frequent changes in their working environment while companies may have a substantial investment in linux .
.
6mo linux .
.
6mo linux .
.
molinux .
.
molinux .
.
mo02004006008001000patchescoccinelle mainline patches smatch mainline patches coverity mainline patches bugzilla mainline patches non bugzilla stable patches bugzilla stable patchesfigure .
various kinds of patches applied to the stable kernels .
.
and .
.
and to the mainline kernel in the same time period software that is tuned for the properties of a specific kernel and may require the degree of security and reliability that a well tested kernel provides.
accordingly linux distributions often do not include the latest kernel version.
for example the current stable debian distribution squeeze and the current ubuntu long term support distribution lucid rely on the linux .
.
kernel released in december .
for industrial users the same kernel is at the basis of suse enterprise linux red hat enterprise linux and oracle unbreakable linux.
in recognition of the need for a stable kernel the linux development community maintains a stable kernel in parallel with the development of the next version and a number of longterm kernels that are maintained over a number of years.
for simplicity in the rest of this paper we refer to all of these as stable kernels.
stable kernels only integrate patches that represent bug fixes or new device identifiers but no large changes or additions of new functionalities.1such a strategy is possible because each patch is required to perform only one kind of change.2developers and maintainers may identify patches that should be included in the stable kernels by forwarding the patches to a dedicated e mail address.
these patches are then reviewed by the maintainers of the stable kernels before being integrated into the code base.
figure compares a very conservative approximation of the number of bug fixing patches accepted into the mainline kernel left solid bars with the number of patches accepted into the stable kernels linux .
.
maintained between february and august and linux .
.
maintained between october and december right open bars .
bug fixing patches are approximated as those where the log message mentions a bug finding tool coccinelle smatch 3or coverity4 or where it mentions 1linux .
.
documentation stable kernel rules.txt 2linux .
.
documentation submittingpatches.txt .
overall framework bugzilla.
stable patches are separated into those that do not mention bugzilla and those that do.
in any of the considered month periods at least twice as many patches mentioning bugzilla were integrated into the mainline kernel as into the stable kernel.
and after year the number of patches based on bug finding tools in the mainline kernel outstripped the number of patches integrated into the stable kernel.
indeed fewer than patches based on each of the considered bugfinding tools were integrated into the stable kernel in each of the considered time periods.
while it is ultimately the stable kernel maintainers who decide whether it is worth including a bug fixing patch in a stable kernel the very low rate of propagation of the considered types of bugfixing patches from the mainline kernel to the stable kernels suggests that automatic identification of bug fixing patches could be useful.
iii.
o verall framework our approach is composed of the following steps data acquisition feature extraction model learning and bug fixing patch identification.
these steps are shown in figure .
the data acquisition step extracts commits from linux code repository.
some of these commits represent bug fixing patches while others do not.
not all bug fixing patches are well marked in linux code.
furthermore many of these bug fixes are not recorded in bugzilla.
thus they are simply hidden in the mass of other commits that do not perform bug fixing.
non bug fixing commits may perform code cleaning feature addition performance enhancement etc.
the feature extraction component then reduces the dataset into some potentially important facets.
each commit contains a textual description along with code elements that are changed by the commit.
the textual description can provide hints whether a particular commit is fixing a bugs or is it only trying to clean up some bad coding style or poor programming practice.
code features can also help identify the kind of patch.
many bug fixes involve a change at a single location while many non bug fixing commits involve substantially more lines of code.
to obtain a good collective discriminative features we need to leverage both text and code based features.
next the extracted features are provided to a model learning algorithm that analyzes the features corresponding to bug fixing patches and tries to build a model that discriminates bug fixing patches from other patches.
various algorithms have been proposed to learn a model given a sample ofits behavior.
we consider some popular classification algorithms supervised and semi supervised and propose a new framework that merges several of them.
the final step is the application of our model to the unlabeled data to obtain a set of bug fixing patches.
a challenge in our work is to obtain adequate training data consisting of known bug fixing patches and known non bug fixing patches.
for the former we may use the patches that have already been applied to linux stable versions as well as patches that are derived from the use of bug finding tools or that refer to bugzilla.
but there is no comparable source of labeled non bug fixing patches.
accordingly we propose a hybrid machine learning algorithm that first uses a ranking algorithm to identify a set of patches that appear to be quite distant from the set of bug fixing patches.
these patches are then considered to be a set of known non bug fixing patches.
we then use a supervised classification algorithm to infer a model that can discriminate bug fixing from non bug fixing patches in the unlabeled data.
we describe the details of our framework in the following two sections.
in section iv we describe our approach to collect data and to extract features from the collected data corresponding to the first two blocks in figure .
in section v we describe our new framework that integrates ranking via semi supervised classification and supervised classification.
iv.
d ata acquisition f eature extraction a. data acquisition linux development is managed using the version control system git.5git makes available the history of changes that have been made to the managed code in the form of a series of patches .
a patch is a description of a complete code change reflecting the modifications that a developer has made to the source code at the time of a commit.
figure shows an example.
a patch consists of two sections a log message followed by a description of the code changes.
our data acquisition tool collects information from both of these sections.
the collected information is represented using xml to facilitate subsequent processing.
the log message of a patch as illustrated by lines of figure consists of a commit number sha code author and date information a description of the purpose of the patch and a list of names and emails of people who have been informed of or have approved of the patch.
the data acquisition tool collects all of this information.
the change description of a patch as illustrated by lines of figure appears in the format generated by the command diff using the unified context notation .
a change may affect multiple files and multiple code fragments within each file.
for each modified file the diff output first indicates the file name lines of figure 45d787b8a946313b73e8a8fc5d501c9aea3d8847 2author johannes berg johannes .berg intel .com 3date fri sep 5wext fix potential private ioctl memory content leak 7commit df6d02300f7c2fbd0fbe626d819c8e5237d72c62 upstream .
9when a driver doesn t fill the entire buffer old heap contents may remain .
.
.
reported by jeff mahoney jeffm suse.com signed off by johannes berg johannes.berg intel.com signed off by john w. linville linville tuxdriver.com signed off by greg kroah hartman gregkh suse.de 17diff git a net wireless wext.c b net wireless wext.c 18index d98ffb7.
.6890b7e a net wireless wext.c b net wireless wext.c static int ioctl private iw point .
.
.
else if !iwp pointer return efault extra kmalloc extra size gfp kernel extra kzalloc extra size gfp kernel if !extra return enomem figure .
a bug fixing patch applied to stable kernel linux .
.
and then contains a series of hunks describing the changes lines of figure .
a hunk begins with an indication of the affected line numbers in the old and new versions of the file which is followed by a fragment of code.
this code fragment contains context lines which appear in both the old and new versions removed lines which are preceded by a and appear only in the old version and added lines which are preceded by a and appear only in the new version.
a hunk typically begins with three lines of context code which are followed by a sequence of zero or more removed lines and then the added lines if any that replace them.
a hunk then ends with three more lines of context code.
if changes occur close together multiple hunks may be combined into one.
the example in figure contains only one hunk with one line of removed code and one line of added code.
given the different information in a patch our data acquisition tool records the boundaries between the information for the different files and the different hunks.
within each hunk it distinguishes between context removed and added code.
it does not record file names or hunk line numbers.
b. feature extraction analysis of the text a commit log message describes the purpose of the change and thus can potentially provide valuable information as to whether a commit represents a bug fix.
to mechanically extract information from the commit logs we represent each commit log as a bag of words.
in these words we perform stop word removal and stemming .
stop words such as is are am would etc are used very frequently in almost all documents and thus they provide little discriminative power.
stemming reduces a word to its root for example writing and writes could all be reduced to write .
stemminggroups together words that have the same meaning but only differ due to some grammatical variations.
this process can potentially increase the discriminative power of root words that are good at differentiating bug fixing patches from other commits as more commits with logs containing the root word and its variants can potentially be identified and associated together after stemming is performed.
at the end of this analysis we represent each commit as a bag of root words.
we call this information the textual facts that represent the commit.
analysis of the code to better understand the effect of a patch we have also incorporated a parser of patches into our data acquisition tool .
parsing patch code is challenging because a patch often does not represent a complete top level program unit and indeed portions of the affected statements and expressions may be missing if they extend beyond the three lines of context information.
thus the parsing is necessarily approximate.
the parser is independent of the line based and annotations only focusing on the terms that have changed.
in the common case of changes in function calls it furthermore detects arguments that have not changed and ignores their subterms.
for example in the patch in figure the change is detected to involve a function call i.e.tokmalloc which is replaced by a call to kzalloc .
the initialization of extra is not included in the change and the arguments to kmalloc and kzalloc are detected to be identical.
based on the results of the parser we collect the numbers of various kinds of constructs such as loops conditionals and function calls that include removed or added code.
we call these the code facts that represent the commit.
feature engineering based on the textual andcode facts extracted as described above we pick interesting features that are compositions of several facts e.g.
the difference between the number of lines changed in the minus and plus hunks etc.
.
table i presents some features that we form based on the facts.
features f1tof52are those extracted from code facts.
the other features i.e.
features f53 f55and features w1town are those extracted from textual facts.
for code features we consider various program units changed during a commit including files hunks loops ifs contiguous code segments lines boolean operators etc.
for many of these program units we consider the number of times they are added or removed and also the sum and difference of these numbers.
our initial investigation suggests that often bug fixing patches and other commits e.g.
feature additions performance enhancements etc have different value distributions for these code features.
for text features we consider stemmed non stop words appearing in the logs as features.
for each feature corresponding to a word we take its frequency i.e.
number of times it appears in a commit log as its corresponding feature value.
we also consider two composite families of389table i extracted features id feature f1 number of files changed in a commit.
f2 number of hunks in a commit.
f3 loops added f4 loops removed f5 f3 f4 f6 f3 f4 f7 f13 f f8 f12 similar to f3tof7for ifs f13 f17 similar to f3tof7for contiguous code segments f18 f22 similar to f3tof7for lines f23 f27 similar to f3tof7for character literals f28 f32 similar to f3tof7for paranthesized expressions f33 f37 similar to f3tof7for expressions f38 f42 similar to f3tof7for boolean operators f43 f47 similar to f3tof7for assignments f48 f52 similar to f3tof7for function calls f53 one of these words exists in the commit log robust unnecessary improve future anticipation superfluous remove unused f54 one of these words exists in the commit log must needs to remove has to don t fault error have to need to f55 the word warning exists in the commit log w1to w n each feature represents a stemmed non stop word in the commit log.
each feature has a value corresponding to the number of times the word appears in the commit i.e.
term frequency .
words each conveying a similar meaning one contains words that are likely to relate to performance improvement feature addition and clean up another contains words that are likely to relate to a necessity to fix an error.
we also consider the word warning not stemmed as a separate textual feature.
v. m odel learning b ugfixidentification a. model learning we propose a solution that integrates two classification algorithms l earning from p ositive and u nlabeled examples lpu 6and support vector machine svm .
these learning algorithms take in two datasets training and testing where each dataset consists of many data points.
the algorithms each learn a model from the training data and apply the model to the test data.
we first describe the differences between these two algorithms.
lpu performs semi supervised classification .
given a positive dataset and an unlabelled dataset lpu builds a model that can discriminate positive from negative data points.
the learned model can then be used to label data with unknown labels.
for each data point the model outputs a score indicating the likelihood that the unlabeled data is positive.
we can rank the unlabeled data points based on this score.
svm on the other hand performs supervised classification.
given a positive dataset and a negative dataset svm builds a model that can discriminate between them.
while lpu only requires the availability of datasets with positive labels liub lpu lpu download.html figure .
model learning svm requires the availability of datasets with both positive and negative labels.
lpu tends to learn a weaker discriminative model than svm because it takes in only positive and unlabeled data while svm is able to compare and contrast positive and negative data.
to be able to classify well we propose to combine lpu and svm.
first we use lpu to rank how far an unlabeled data point is from the positive training data.
for this we sort the data points based on their lpu scores in descending order indicating the likelihood of a data point being positive.
the bottom kdata points where kis a user defined parameter are then taken as a proxy for the negative data.
these negative data along with the positive data are then used as the input to svm.
the steps in our model learning process are shown in figure .
in the problem of identifying bug fixing patches each data point is a commit.
we have a set of positive data points i.e.
bug fixing patches and a set of unlabeled data points i.e.
arbitrary commits.
we first apply lpu to sort commits such that bug fixing patches are listed first and other patches which may correspond to innocuous changes performance improvements or feature additions are listed later.
according to this ordering the bottom kcommits are likely to be non bug fixing patches.
we then take the bottom kcommits to be a proxy of a dataset containing non bug fixing patches.
we use the original bug fixing patch dataset and this data to create a model using svm.
b. bug fix identification once we have learned the model for bug fix identification we apply the same feature extraction process to a test dataset with unknown labels.
we then represent this test dataset by a set of feature values.
these feature values are then fed to the learned model as described in section v a. based on these features the model then assigns either one of the following two labels to each commit bug fixing or non bug fixing.
vi.
e xperiment we first describe the datasets used for our evaluation and then present a number of research questions.
then we present experimental results that answer these questions.390table ii properties of the considered black datasets .
loc lines of code includes both the log and the changed code .
source dates patches loc stable .
.
.
.
29k stable .
.
.
.
116k coverity .
.
22k coccinelle .
.
54k smatch .
.
31k bugzilla .
.
275k a. dataset our algorithm requires as input black data that is known to represent bug fixing patches and grey data that may or may not represent bug fixing patches.
the grey data may contain both black data and white data i.e.
non bugfixing patches .
as there is no a priori definition of what is a bug fixing patch in linux we have created a selection of black data sets from varying sources.
one source of black data is the patches that have been applied to existing stable versions.
we have considered the patches applied to the stable versions linux .
.
8released in february and maintained until august and linux .
.
9released in october and maintained until december .
we have taken only those patches that include c code and where this code is not in the documentation section of the kernel source tree.
another source of black data is the patches that have been created based on the use of bug finding tools.
we consider uses of the commercial tool coverity 10which was most actively used prior to and the open source tools coccinelle and smatch 11which have been most actively used since and respectively .
the coverity patches are collected by searching for patches that mention coverity in the log message.
the coccinelle and smatch patches are collected by searching for patches from the principal users of these tools which are the second author of this paper and dan carpenter respectively.
the coccinelle data may contain both bug fixes and simple refactorings.
the coverity and smatch patches should contain only bug fixes.
all three data sets are taken from the complete set of patches between april and august .
our final source of black data is the set of patches that mention bugzilla taken from the same time period.
table ii summarizes various properties of these data sets.
the grey data is taken as the complete set of patches that have been applied to the linux kernel between versions .
.
and .
.
.
to reduce the size of the dataset we take only those patches that can apply without conflicts to iii properties of the considered grey dataset bylinux version .
source dates patches .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
the linux .
.
code base.
table iii summarizes various properties of the data sets.
b. research questions our study addresses the following four research questions rq1 rq4 .
in rq1 we investigate the effectiveness of our approach.
factors that influence its effectiveness are investigated in rq2 and rq3.
finally rq4 investigates the benefit of our hybrid classification model.
rq1 does our approach identify bug fixing patches as well as the existing keyword based method?
to evaluate the effectiveness of our approach as compared with existing keyword based methods we consider the following criteria criterion precision and recall on sampled data.
we randomly sample commits and manually label each as a bug fix that could go to stable or not.
we then compare the human assigned labels with the labels assigned by each bug fix identification approach and compute the associated precision and recall .
criterion accuracy on known black data.
we take commits that have been identified by linux developers as bug fixing patches and split this dataset into equal sized groups.
we train on groups and use one group to test.
we evaluate how many of the bug fixing patches are correctly labeled.
the process is iterated times.
for each iteration we compute the number of bug fixing patches that are correctly identified we refer to this as accuracyblack and report the average accuracy.
the goal of the first criterion is to estimate the accuracy of our approach on some sampled data points.
one of the authors is an expert on linux development and has contributed many patches to linux code base.
this author manually assigned labels to the sampled data points.
the goal of the second criterion is to address the experimenter bias existing in the first criteria.
unfortunately we only have known black data.
thus we evaluate our approach in terms of its accuracy in labeling black data as such.
rq2 what is the effect of the parameter k on the results?
our algorithm has one parameter k which specifies the number of bottom ranked commits that we take as a proxy of a dataset containing non bug fixing patches.
as a default value in our experiments we fix kto be .
the number391of black data that are known bug fixing patches.
we vary this number and investigate its impact on the result.
rq3 what are the best features for discriminating if a commit is a bug fixing patches?
aside from producing a model that can identify bug fixing patches we are also interested in finding discriminative features that could help in distinguishing bug fixing patches and other commits.
we would like to identify these features out of the many textual and code features that we extract from commits.
we create a clean dataset containing all the known black data the manually labeled black data and the manually labeled white data.
we then compute the fisher score of all the features that we have.
specifically we compute a variant of the fisher score reported in and implemented in libsvm12.
the fisher score and its variants have been frequently used to identify important features .
rq4 is our hybrid approach i.e.
ranking supervised classification using lpu svm more effective than a simple semi supervised approach i.e.
lpu ?
our dataset only contains positively labeled data points i.e.
bug fixing patches .
to address this sort of problem machine learning researchers have investigated semi supervised learning solutions.
many of these techniques still required a number of negatively labeled data points.
however lpu which is one of the few semi supervised classification algorithms with an implementation available online only requires positively labeled and unlabeled data points.
our proposed solution includes a ranking and a supervised classification component.
the ranking component makes use of lpu.
thus it is interesting to investigate if the result of using lpu alone is sufficient and whether our hybrid approach improves the results of lpu.
c. experimental results we present our experimental results as answers to the four research questions rq1 rq4.
rq1 effectiveness of our approach we compare our approach to the keyword based approach used in the literature .
the result of the comparisons using the two criteria are discussed below.
precision and recall on sampled data.
table iv compares the precision and recall of our approach to those of the keyword based approach.
our precision is comparable with that of the keyword based approach.
on the other hand we increase the recall of the keyword based approach from .
to .
this is an improvement of .
.
to combine precision and recall we also compute the fmeasure which is a harmonic mean of precision and recall.
the f measure is often used to evaluate whether an improvement in recall outweighs a reduction in precision cjlin libsvmtoolstable iv precision and recall comparison approach precision recall ours .
.
keyword .
.
table v f m easures comparison approach f1 f2 f3 f5 ours .
.
.
.
keyword .
.
.
.
improvement .
.
.
.
and vice versa .
the f measure has a parameter that measures the importance of precision over recall.
the formula is 2 precision recall 2 precision recall if precision and recall are equally important is set to one.
this computes what is known as f1.
if beta is set higher than then recall is preferred over precision similarly if beta is set lower than then precision is preferred over recall.
in the context of bug fixing patch identification recall i.e.
not missing any bug fixing patch is more important than precision i.e.
not reporting wrong bug fixing patch .
missing a bug fixing patch could potentially cause system errors and even expose security holes.13for these cases a standard ir book recommend setting equal to or .
other studies recommend setting to .
table v shows the f measures for the different values of .
for all values of our approach has better results than the keyword based approach.
the f1 f2 f3 and f5 scores are improved by .
.
.
and .
respectively.
from the randomly sampled commits we notice that a very small number of the commits that are bug fixing patches reference bugzilla.
thus identifying these patches are not trivial.
also as shown in table iv about of bug fixing patches do not contain the keywords considered in previous work .
accuracy on known black data.
for the given data our approach increases accuracyblackby .
as compared to the keyword based approach from .
to .
.
these results show that our approach is effective in identifying bug fixing patches as compared to keyword based approach used in existing studies.
the known black data is unbiased as we do not label it ourselves.
however this experiment does not provide any information about the rate of false positives as all our known test data are black.
13recall is more important in other related settings.
manning et al.
describe scenarios where recall is more important than precision e.g.
paralegals and intelligence analysts potentially looking for violations of a legal clause or for terrorists .392table vi effect of varying konperformance .
tp t rue positive fn false negative fp f alse positive tn t rue negative .
k tp fn fp tn prec.
recall f2 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
the high accuracy of the keyword based approach is due to the large number of bugzilla patches in the clean bug fixing patch dataset.
in practice however most bug fixing patches are not in bugzilla these bug fixing patches are hidden in the mass of other non bug fix related commits.
rq2 effects of varying parameter k when we vary the parameter k as a proportion of the amount of black data the number of false positives and false negatives changes.
the results of our experiments with varying values forkis shown in table vi.
as we increase the value of kthe number of false negatives fn increases while the number of false positives fp decreases.
indeed as we increase the value of k the pseudo white data i.e.
the bottom kcommits in the sorted list after ranking using lpu gets dirtier as more black data are likely to be mixed with the white data in it.
thus more and more black data are wrongly labeled as white i.e.
an increase in false negatives .
however the white data are still closer to the dirty pseudo white data than to the black data.
also more and more borderline white data are closer to the dirtier pseudo white data than before.
this reduces the number of cases where white data are labeled black i.e.
a reduction in false positives .
we illustrate this in figure .
rq3 best features from the bug reports we extract thousands of features corresponding to the presence or absence of words in commit logs and the various code facts.
we report the top 20features sorted based on their fisher scores in table vii.
we note that among the top features there are both textual and code features.
this highlights the usefulness of combining both textual features in commit logs and code features in changed code to predict bug fixing patches.
we notice however that the fisher score is low the highest possible value is which highlights that one feature alone is not sufficient to discriminate positive from negative datasets i.e.
bug fixing patches versus other commits .
some keywords used in previous approaches e.g.
fix bugzilla etc.
are also included in the top20 features.
due to tokenization some of these features are split into multiple features e.g.
http bug.cgi and bugzilla.kernel.org.
the word blackfin which is the name of a family of microprocessors is in the top as many commits containing this keyword are non bug fixing patches.
figure .
effect of varying k. the pseudo white data is the bottom kcommits that we treat as a proxy to non bug fixing patches.
the three boxes corresponding to pseudo white of them and black data represent the aggregate features of the respective pseudo white and black data in our training set respectively.
the squares and triangles represent test data points whose labels i.e.
bug fixing patches or not are to be predicted.
table vii top m ost discriminative features based on fisher score rank feature desc.
fisher score http .
bug.cgi .
blackfin .
show .
bugzilla.kernel.org .
fix .
commit .
f18 i.e.
lines removed .
upstream .
id .
f20 i.e.
lines added removed .
unifi .
f38 i.e.
boolean operators removed .
f44 i.e.
assignments removed .
checkpatch .
spell .
f46 i.e.
assign.
removed added .
f37 i.e.
boolean operators added .
f6 i.e.
loops added removed .
f48 i.e.
function calls added .
many other features in the list are code features these include the number of times different program elements are changed by a commit.
the most discriminative code element is the number of lines of code being deleted ranked 8th .
next come features such as the number of lines added and deleted ranked 11th the number of boolean operators added ranked 13th the number of assignments removed ranked 14th the number of assignments added and removed ranked 17th the number of boolean operators added ranked 18th the number of loops added and removed ranked 19th and the number of function calls made ranked 20th .393table viii comparisons with lpu.
prec.
p recision acc.
a ccuracyblack approach prec.
recall f1 ours .
.
.
lpu only .
.
.
improvement .
.
.
approach f2 f3 f5 acc.
ours .
.
.
.
lpu only .
.
.
.
improvement .
.
.
.
rq4 our approach versus lpu we have run lpu on our dataset and found that the results of using lpu alone are not good.
results are shown in table viii.
the precision of lpu alone is slightly higher than that of our approach but the reported recall is much lower.
our approach can increase the recall by more than times i.e.
improvement .
when we trade off precision and recall using f measure we notice that for all our approach is better than lpu by .
.
.
and .
for f1 f2 f3 and f5 respectively.
theaccuracyblackvalues of our approach and that of lpu alone are comparable.
notice that the black data inaccuracyblackare similar to one another with many having the terms bugzilla http etc.
the black data in the random sample are more challenging and better reflect the black data that are often hidden in the mass of other commits.
the above highlights the benefit of our hybrid approach of combining ranking and supervised classification to address the problem of unavailability of negative data points i.e.
the non bug fixing patches as compared to a simple application of a standard semi supervised classification approach.
in our approach lpu is used for ranking to get a pseudo negative dataset and svm is used to learn the discriminative model.
vii.
d iscussion threats to validity.
as with other empirical studies there are a number of threats to the validity of our results.
threats to internal validity corresponds to the relationship between the independent and dependent variables in the study.
one such threat in our study is experimenter bias as we have personally labelled each commits as a bug fixing patch or as a non bug fixing patch.
this labelling might introduce some experimenter bias.
however we have tried to ensure that we label the commits correctly according to our experience with linux code .
also we have labelled the commits before seeing the results of our identification approach to minimize this bias.
threats to external validity refers to the generalizability of the result.
we have manually checked the effectiveness of our approach over commits.
although is not a very large number we still believe it is a good sample size.
past studies e.g.
investigate a similar amountof manually labeled data.
we plan to reduce this threat of external validity in the future by investigating an even larger number of manually labeled commits.
we have also only investigated patches in linux.
although we analyze one system it is large and contains diverse components.
linux is one of the largest open source project in terms of code size number of contributors and configurations.
thus we believe that its development is worth study.
the size of linux loc is indeed larger than the sum of that of systems investigated in related prior studies e.g.
.
we believe our approach can be easily applied to identify bug fixing patches in other systems but leave this to future work.
threats to construct validity deals with the appropriateness of the evaluation criteria.
we use the standard measures precision recall and f measure to evaluate the effectiveness of our approach.
thus we believe there is little threat to construct validity.
automated tuning of k.if there exist manual user labels on some representative samples of commits then we can use this information to automatically tune the value of k. as a default initial value kcould be set as .
the number of black data.
the performance of a classifier based on this initial value could be evaluated based on the manual user labels.kcould be reduced or increased to improve recall and precision on the input representative sample of commits.
the best value of kis one that allows learning from the most white example data points without mixing too many black data points with the white ones.
in an additional experiment we take of the manually labeled commits and use it to fine tune k. the best value of kto optimize f1is again .
.
forf2 at k .
its score is not much different than that of the other values of kin the range of .
.
that we try.
we still notice that in terms of f2 taking either .
.
.
or .
does not impact f2much less than .
difference .
benefit of including unlabeled data.
labeling is expensive and time consuming.
on the other hand unlabeled data can be obtained easily.
as there are many variants of bug fixing and non bug fixing patches many labeled data are needed to characterize all these variants.
producing these labels would cost much time and effort.
unlabeled data already contain many different variants of bug fixing and non bug fixing patches.
in this study we leverage unlabeled data and show that they can be used to identify bug fixing patches well.
characteristics of non bug fixing patches.
from the nonbug fixing patches that we label and inspect we notice that they fall into several categories performance enhancement commits.
a commit of this type often tries to make something run faster.
often this performance issue is not a bug that need to be patched in a stable version.
code clean up.
a commit of this type improves the394structure of the code without changing its semantics to make the code easier to read etc.
stable versions target users not developers so readability is not a priority.
feature addition or removal.
a commit of this type adds a new feature or remove an old one.
users of stable versions are likely to find such changes undesirable.
warnings.
warnings are not errors.
the code can still run well even in the existence of these warnings.
eliminating such a warning does not fix a bug.
viii.
r elated work we describe some studies that are related to our approach.
we start with those that are most related and then consider some that are marginally related.
identification of bug fixes.
a number of studies have searched for keywords such as bug and fix in log messages to identify bug fixing commits .
our approach on the other hand is not based on a fixed set of keywords.
rather we automatically infer keywords that are good at discriminating bug fixing patches from other commits.
furthermore we consider not only commit logs but also some features extracted from the changes made to the source code.
we then built a discriminative machine learning model that is used to classify commits as either bug fixing or not.
experimental results show that our approach can identify more bug fixing patches with similar precision but improved recall as compared to a fixed keyword based approach.
bird et al.
have observed that the lack of clearly identified bug fixing patches itself has caused potential bias in many prior studies .
thus we believe our approach could not only benefit linux stable release maintainers but also help other studies involving the analysis of bug fixes.
there are two recent studies that are related to ours.
wuet al.
propose relink which links bug reports to their associated commits .
relink only captures tracked bugs bugs described only in mailing lists etc.
are mentioned as future work.
our work considers a different problem and does not require the availability of bug reports which may be absent or incomplete.
bird et al.
propose linkster which integrates information from various sources to support manual link recovery .
our approach is based on machine learning for more automation.
studies on bug reports.
there have been a number of studies that analyze bug reports .
similar to these studies we also analyze textual data found in software artifacts.
we focus on commit logs made by developers while these studies focus on bug reports made by users.
bug localization tries to locate methods that are responsible for a bug given the corresponding bug report .
bug localization approaches require as input linkages between code responsible for the bugs and the bug reports.obtaining these linkages is often hard due to poor commit log comments.
our approach of inferring bug fixing patches could potentially help infer these links.
anvik et al.
investigate the problem of automatic bug triaging which tries to recommend developers suitable to fix a bug .
with more bug fixing patches identified possibly a better recommendation of suitable developers could be made.
other studies on textual software engineering data.
there have been other studies that analyze software and its related textual artifacts.
closest to our approach is the work by antoniol et al.
which analyzes change requests and classify them as either bugs or enhancements .
change requests contain text with additional fields such as severity and are written by ordinary users.
commits considered in this work contains both text and code.
they are written by experts but do not include fields such as severity.
thus the type of input is different.
there are many other studies.
for example tan et al.
analyze source code comments to find concurrency bugs .
zhong et al.
infer program specifications from api documentation .
gottipati et al.
build an effective search engine over software forum posts .
wang et al.
extract paraphrases of technical terms from bug reports .
there are also a number of techniques that trace requirements expressed in natural language including the work of port et al.
sultanov et al.
etc.
ix.
c onclusion f uture work linux developers periodically designate a release as being subject to longterm support.
during the support period bug fixes applied to the mainline kernel need to be back ported to these longterm releases.
this task is not trivial as developers do not necessarily make explicit which commits are bug fixes and which of them need to be applied to the longterm releases.
to address this problem we propose an automated approach to infer commits that represent bug fixing patches.
our approach first extracts features from the commits that describe those code changes and log messages that can potentially distinguish bug fixing patches from regular commits.
a machine learning approach involving ranking and classification is employed.
experiments on linux commits show that we can improve on the existing keyword based approach obtaining similar precision and improving recall by .
.
in the future we plan to further improve the accuracy of our approach.
we also plan to apply our approach to work in bug prediction and related areas that suffer from bias due to unidentified bug fixes.