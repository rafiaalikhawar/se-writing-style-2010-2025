see discussions st ats and author pr ofiles f or this public ation at .researchgate.ne t public ation detecting similar software applications article in proceedings int ernational conf erence on softw are engineering june .
ic se.
.
citations 115reads author s including mark gr echanik univ ersity of illinois chic ago publica tions citations see profile denys p oshyv anyk william mar y publica tions citations see profile all c ontent f ollo wing this p age was uplo aded b y mark gr echanik on sept ember .
the user has r equest ed enhanc ement of the do wnlo aded file.detectingsimilar softwareapplications collin mcmillan college of william and mary williamsburg va cmc cs.wm.edumark grechanik accenture technology labs and u. of illinois chicago il drmark uic.edudenys poshyvanyk college of william and mary williamsburg va denys cs.wm.edu abstract although popular textsearch engines allow users to retrieve similar web pages source code search engines do not have this feature.
detecting similar applications is a notoriously difficult problem since it implies that simila r highlevel requirements and their low level implementations ca n be detected and matched automatically for different applicat ions.
we created a novel approach for automatically detecting closely related applications clan that helps users detect similar applications for a given java application.
our main contributionsare an extension to a framework of relevance a nd a novel algorithm that computes a similarity index between java applications using the notion of semantic layers that correspond to packages and class hierarchies.
we have built clan and we conducted an experiment with participants to evaluate clan and compare it with the closest competitive approach mudablue.
the results show with strong statistic al significance that clan automatically detects similar appli cations from a large repository of java applications wit h a higher precision than mudablue.
i. introduction retrieving similar or related web pages is a feature of popular search engines e.g.
google ask.com hotbot .
after users submit search queries google displays links to relevant web pages along with a link labeled similar next to each result.
these similar links point to web pages that the google similarity algorithm computes by aggregatingmany factors that include but are not limited t o the popularity scores of the retrieved pages links among th e pages and the links positions and sizes .
for example for the main acm sigsoft page google returns three top similar web sites ieee computer society software engineering institute and esec fse .
detecting similar applications is a notoriously difficult problem since it means automatically detecting that highlevel requirementsfor these applications match semantica lly .
this situation is aggravated by the fact that many application repositories are polluted with poorly functioning projects a match between words in requirement documents with words in the descriptions or in the source code of applicationsdoes not guaranteethat thes e applications are relevant to the requirements.
applicatio ns may be highly similar to one another at a low level of the implementations of some functions even if they do not 1last time checked september .perform the same high level functionality .
rarely do programmers record any traceability links between softwar e artifacts which belong to different applications to esta blish their functional similarity.
knowing similarity between applications plays an important role in assessing reusability of these application s improving understanding of source code rapid prototyping and discovering code theft and plagiarism .
enabling programmers to compare automatically how different applicationsimplement the same requirement s greatly contributes to knowledge acquisition about these requirements and subsequently to decisions that these developers make about code reuse.
retrieving a list of similar applications provides a faster way for programmers to concentrateonrelevantaspectsoffunctionality thussaving time and resources for programmers.
programmers can spend this time understanding specific aspects of functionality i n similar applications and see the complete context in which the functionality is used.
a fundamental problem of detecting closely related applications is in the mismatch between the high level intent reflected in the descriptions of these applications and lowlevel implementation details.
this problem is known as the concept assignment problem .
for any two applications it is too imprecise to establish their similarity by simply matching words in the descriptions of these applications comments in their source code and the names of program variables and types.
since programmers typically invest a significant intellectual effort i.e.
they need to overcom e a highcognitivedistance tounderstandwhetherretrie ved applications are similar existing code search engines do n ot alleviate the task of detecting similar applications becau se they return only a large number of different code snippets.
we createdanovelapproachfordetecting closely related applications clan .
this paper makes the following contributions a major contribution of our approach is that clan uses complete software applications as input not only natural language queries.
this feature is useful when a developer needs to find similar applications to a known software application.
we introduce a new abstraction that is relevant to semantic spaces that are modeled as existinginheritance hierarchies of application programming interface api classes and packages.
we extended a well established conceptual framework of relevance with our new abstraction.
the intuition behind our approach is that if two applications contain functional abstractions in a form of inheritance hierarchies and packages that contain api calls whose semanticsaredefinedprecisely andthesecallsimplement the same requirement e.g.
different api calls from a data compression library then these applications have a higherdegreeof similarity than those that do not have api calls that are relatedto somerequirement.the idea ofusingapicallsto improvecodesearchwasproposed and implemented elsewhere however this idea has never been used to compute similarities between software applications.
based on this extension we designed a novel algorithm that computes a similarity index between java applications and we implementedthis algorithmin clan and applied to java applications that we downloaded from sourceforge.
clan is available for public use2.
we conductedanexperimentwith33javaprogrammers to evaluate clan.
the results show with strong statistical significance that users find more relevant applicationswithhigherprecisionwithclanthanthosebased on the closest competitive approach mudablue3 and a system that combines clan and mudablue that we implemented4.
ii.
ourhypothesis andtheproblem in this section we use a conceptual framework for relevance to define the concept of similarity between applications formulatea hypothesis and describe problemsthat w e should solve to test this hypothesis.
a. a motivating scenario a motivating scenario for detecting similar application is based on a typical project lifecycle in accenture a global software consulting company with over employees as of february .
at any given time company consultants are engaged in over software projects.
since its first project in accenture s consultants delivered te ns of thousand of projects and many of these projects are similar in requirementsand their implementations.
knowing th e similarity of these applications is important for preservi ng knowledge experience winning bids on future projects an d successfully building new applications.
a typical lifecycle of a large scale project involves many stages that start with writing a proposal in response to a bid from a company that needs an application.
a major part of writing a proposal and developing a prototype is to elicit from different stakeholders.
there are quite a fewcompetingcompaniesforeachbid ibmcorp hpcorp tata consultancy services to name a few.
a winning bid proposal has many components well elicited requirements preliminary models and design documents proof of experience of building and delivering similar applications in the past.
clearly a company that submits a bid proposal that contains these components as closely matching a desired application as possible will win the bid.
it is important to reuse these components from successfully delivered applications in the past so will save time and resources and increase chances of winning the bid.
it is shown that over a dozen different artifacts can be successfully reused from software applications pages3 .the processof findingsimilar applicationsstar ts with code search engines that return code fragments and documents in response to queries that contain key words from elicited requirements.
however returned code fragments are of little help when many other non code artifacts are required e.g.
different non functional requiremen ts documents uml models design documents .
matching words in queries against words in documents and source code is a good starting point however it does not help stakeholders to establish how applications are similar at a bigger scale.
in this paper we refer application as a collection of all source code modules libraries and programs that when compiled result in the final deliverabl e that customers install and use to accomplish certain busine ss functions.
applications are usually accompanied by noncode artifacts which are important for the bidding process .
establishing their similarity at large from different simi lar components of the source code is a goal of this paper.
the concept of similarity between applications is integrated in the software lifecycle process as follows.
after obtaining the initial set of requirements the user enters keywords that represent these requirements into a search engine that returns relevant applications that contain the se keywords.
in practice it is unlikely that the user finds an application that perfectly matches all the requirements i f it happens then the rapid prototyping process is finished.
otherwise the user takes the returned applications and studies them to determine how relevant they are to the requirements.
after examining some returned application the user determines what artifacts are relevant to requirements and which ones are missing.
at this point the user wants to find similar applications that contain the missing artifact s while retaining similarity to the application that the user has found.
that is using the previously found application the initial queryis furtherexpandedtoincludeartifactsfrom this application that matched some of requirements as the user determined and similar applications would contain artifa cts that are similar to the ones in the found application.b.
similarity between applications we define the meaning of similarity between applications by using mizzaro s well established conceptual framework for relevance .
in mizzaro s framework similar documents are relevant to one another if they share some commonconcepts.
once these conceptsare known a corpus of documents can be clustered by how documents are relevant to these concepts.
subsequently all documents in each cluster will be more relevant to one another when compared to documents that belong to different clusters.
this is the essence of the cluster hypothesis that specifies that documents that cluster together tend to be relevant to the same concept .
two applications are similar to each other if they implement some features that are described by the same abstraction.
for example if some applications use cryptographic services to protect information then these applications ar e similar to a certain degree even though they may have other different functionalities for different domains.
an other example is text editors that are implemented by different programmers but share manyfeatures copyand paste undo and redo saving data in files using standard formats.
a straightforward approach for measuring similarity betwee n applicationsisto matchthenamesoftheirprogramvariable s and types.
the precision of this approach depends highly on programmers choosing meaningful names that reflect correctly the concepts or abstractions that they implement but this compliance is generally difficult to enforce .
c. our hypothesis in mizzaro s framework a key characteristic of relevance is how information is represented in documents.
we concentrate on semantic anchors which are elements of documents that precisely define the documents semantic characteristics.
semantic anchors may take many forms.
for example they can be expressed as links to web sites that havehighintegrityandwell knownsemantics e.g.
cnn.co m or whitehouse.gov or theycan refer to elements of semantic ontologies that are precisely defined and agreed upon by different stakeholders.
this is the essence of paradigmatic associations where documents are considered similar if they contain terms with high semantic similarities .
our hypothesis is that by using semantic anchors and dependencies among them it is possible to compute similarities between documents with a higherdegreeof accuracywhen comparedto documentsthat have no commonly defined semantic anchors in them.
without semantic anchors documents are considered as bags of words with no semantics then the relevance of these documents to user queries and to one another can be determined by matches between these words.
this is the essence of syntagmatic associations where documents are considered similar when terms i.e.
words in these documentsoccurtogether .forexample thesimilarityengi nemudablue uses syntagmatic associations for computing similarities among applications .
the problem with thi s approach is that computed relevance is relatively imprecis e when compared with clan as we show in section v. d. semantic anchors in software since programs contain api calls with precisely defined semantics these api calls can serve as semantic anchors to compute the degree of similarity between applications by matching the semantics of these applications that is expressedwiththeseapicalls.programmersroutinelyuseapi calls from third party packages e.g.
the java development kit jdk to implement various requirements .
api calls from well known and widely used libraries have precisely defined semantics unlike names of program variables and types and words that programmers use in comments.inthis paper we use api calls as semantic anchors to compute similarities among applications.
e. challenges our hypothesis is based on our idea that it is better to compute similarity between programs by utilizing api calls as semantic anchors that come from jdk and that programmers use to implement various requirements.
this idea has advantages over using vector space model vsm where documents are represented as vectors of words and a similarity measure is computed as the cosine between these vectors .
one main problem with vsm is that different programmers can use the same words to describe differentrequirements i.e.
thesynonymyproblem andth ey can use different words to describe the same requirements i.e.
the polysemy problem .
this problem is a variation of the vocabulary problem which states that no single word can be chosen to describe a programming concept in the best way .
this problem is general to information retrieval ir but somewhat mitigated by the fact that different programmers who participate in the projects use coherentvocabulariesto write code and documentation thu s increasing the chance that two words in different applications may describe the same requirement.
thesheernumberofapicallssuggeststhatmanyofthese callsarelikelytobesharedbydifferentprogramsthatimpl ementcompletelydifferentrequirementsleadingtosignific ant imprecision in calculating similarities.
our study shows t hat outof2 080randomlychosenjava programsinsourceforge over of these programs use string objects and over contain collection objects these programs invoke api calls that these string and collection classes exports .
if similarity scores are computed based on these common api calls most java programs would be similar to one another.
on top of that it is not computationally feasible t o compute similarity scores with high precision for hundreds of thousands of api calls.
it is an instance of a problem known as the curse of dimensionality which is a problemcaused by the exponential increase in processing by adding extra dimensions to a representational space .
graphically programs are represented as dots in a multidimensional space where dimensions are api calls and coordinates in this space reflect the numbers of api calls in programs.
the jdk contains close to api calls that are exported by a little more than classes and interfaces that are contained in packages.
computing similarity scores between programs using vsm in a space with hundreds of thousands of dimensions is not always computationally feasible it is imprecise and difficult to interpret.
we need to reduce the dimensionality of this space while simultaneously revealing similarities betwee n implemented latent high level requirements.
iii.
ourapproach in this section we describe our key idea provide background material on lsi that we use in clan and explain its architecture.
a. key idea our key idea is threefold.
first if two applications share some semantic anchors e.g.
api calls then their similar ity indexshouldbehigherthanforapplicationsthatdonotshar e anysemanticanchors.sharingsemanticanchorsmeansmore than the exact syntactic match between the same two api calls it also means that two different api calls will match semantically if they come from the same class or package.
this idea is rooted in the fact that classes and packages in jdk contain semantically related api calls for example the package java.security contains classes and api calls that enable programmers to implement security relat ed requirements and the package java.util.zip exports classes that contain api calls for reading and writing the standard zipandgzipfile formats.
thus we exploit relationships between inheritance hierarchies in the jdk t o improve the precision of computing similarity.
this idea is related to semantic spaces where concepts are organized in structured layers and similarity scores between documents are computedusing relationsbetweenlayers .moreover recent work hasshown that api classes and packagescan be used to categorize software applications using those class es and packages .
second different api calls have different weights.
recall that many applications have many api calls that deal with collections and string manipulations.our idea is to automa ticallyassignhigherweightstoapicallsthatareencounter ed infewerapplicationsand converselytoassignlowerweigh ts to api calls that are encountered in a majority of applications.
there is no need to know what api calls are used in applications this task should be done automatically.
g it will improvethe precision of our approach since api calls that come from common packages like java.lang will have less impact to skew the similarity index.finally we observed that a requirement is often implemented using combinations of different api calls rather than a single api call.
it means that co occurrences of api calls in different applications form patterns of implementing different requirements.
for example a requiremen t of efficiently and securely exchanging xml data is often implemented using api calls that read xml data from a file compress and encrypt it and then send this data over the network.
even though different ways of implementing this requirement are possible detecting patterns in cooccurrencesofapi callsandusingthesepatternsto compute the similarity index may lead to higher precision when compared with competitive approaches.
b. latent semantic indexing lsi to implement our key idea we rely an ir technique called latent semantic indexing lsi that reduces the dimensionality of the similarity space while simultaneously reveali ng latent concepts that are implemented in the underlying corpus of documents .
in lsi terms are elevated to an abstract space and terms that are used in similar contexts are considered similar even if they are spelled differently .
lsi automatically makes embedded concepts explicit using singular value decomposition svd which is a form of factor analysis used to reduce dimensionality of the space to capture most essential semantic information.
the input to svd is an m n term document matrix tdm .
eachof mrowscorrespondsto a uniqueterm which in our case is either a class or a package name that contains a corresponding api call that is invoked in a corresponding application i.e.
document .
columns correspond to uniqu e documents which in our case are java applications.
each element of the tdm contains the weight that shows how frequently this api call is used in this application when compared to its usage in other applications5.
we cannot use a simple metric such as the api call count since it is biased it shows the number of times a given api call appears in applications thus skewing the distribution of these cal ls toward large applications which may have a higher api call count regardless of the actual importance of that api call.
svd decomposes tdm into three matrices using a reduced number of dimensions r whose value is chosen experimentally.
the number of dimensions for lsi is commonly chosen r .
one of these matrices contains document vectors that describe weights that documents i.e.
applications havefordifferentdimensions.
each column in this matrix is a vector whose elements specify coordinates for a given application in the r dimensional space.
computing similarities between applications means computing the cosines between vectors i.e.
rows of this matrix.
5note that we do not consider the number of times each api call i s executed e.g.
in a loop.
instead we count occurrences of a pi calls in source code.c.
clan architecture and workflow the architecture for clan is shown in figure .
the main elements of the clan architecture are the java applications apps archive and the api call archive the metadata extractor the search engine the lsi algorithm and the term document matrix tdm builder.
in tdm rows represent packages or classes that contain jdk api calls that are invoked in java applications and columns represent java applications.
applications metadata descr ibes different api calls that are invoked in the applications and their classes and packages.
the input to clan i.e.
a user query is shown in figure with a thick solid arrow labeled .
the output is shown with the thick dashed arrow labeled .
clan worksas follows.
the metadata processor takes as its inputs the apps archive with java applications and api archivethat containsdescriptionsofjdk api calls.
the metadata processor outputs the application metadata which is the set of tuples package class api call a linking api calls and their packages and classes to java applications athat use these api calls.
term document matrix tdm builder takes application metadata as its input and it uses this metadata to produce two tdms package application matrix tdmp and class application matrix tdmc that contain tfidfs for jdk packages and classes whose api calls are invoked in respective applications.
the lsi algorithm is applied separately to tdmpandtdmcto compute class and package matrices bardblc bardbland bardblp bardbl.
that is each row in these matrices contain coordinates that represent it s corresponding application in a multidimensional space wit h respect to either classes or packages of api calls that are invoked in this application.
class level and package level similarities are different since applications are often more similar on the package levelthanontheclasslevelbecausetherearefewerpackage s than classes in the jdk.
therefore there is the higher probability that two applications may have api calls that are located in the same package but not in the same class.
matrices bardblc bardbland bardblp bardblare combined into the similarity matrix using the following formula bardbls bardbl c bardbls bardblc p bardbls bardblp where is the interpolation weight for each similarity matrix and matrices bardbls bardblcand bardbls bardblpare similarity matrices for bardblc bardbland bardblp bardblrespectively.
these similarity matrices are obtained by computing the cosine between the vector for each application i.e.
a corresponding row in the matrix and vectors for all other applications.
weights pand care determined independently of applications.
adjusting these weights enables experimentation with how underlying structural and textual information in applicat ion affects resulting similarity scores.
in this paper we selec ted c p .
thus stating that class and package level scores contribute equally to the similarity matrix.
9metadata extractor api archive apps archive applications metadata 4tdm builder tdm ptdm c p lsi algorithm7 search enginesimilarity matrix c figure .
clan architecture and workflow.
the similarity matrix bardbls bardblis a square matrix whose rows and columns designate applications.
for any two applicationsaiandaj each element of bardbls bardbl sijis the similarity score between these applications that is defined as follows sij braceleftbigg s ifi negationslash j ifi j. it took us close to three hours to construct the tdm for mudablue using intel xeon cpu w3540 .93ghz with 2gbram lessthanonehourfortdmforthepackage and class level tdms for clan.
running svd on these tdms took less than three hours for mudablue and less than minutes for the package and class level tdms for clan.
for all three tdms we used the same corpus of java projects from sourceforge with api calls.
when the user enters a query it is passed to the searchenginethatretrievesrelevantapplications with ranksin the descendingorderusing the similarity matrix.
i n addition the search engine uses the application metadata to extract a map of api calls for each pair of similar applications.
this map shows api calls along with their classes and packages that are shared by similar application s and this map is given to the user .
iv.
experimental design typically search and retrieval engines are evaluated usin g manualrelevancejudgmentsby experts .
to determine how effective clan is we conducted an experiment with participants who are java programmers.
our goal is to evaluate how well these participants can find similar applications to the ones that are considered highly relevant to given tasks using three different simila rity engines mudablue clan and an integrated similarity engine that combines mudablue and clan.
a. background on mudablue and combined mudablue is the closest relevant work to clan since it provides automatic categorization for applications .
thecluster hypothesis specifies that documents that cluster together tend to be relevant to the same concept .
to the best of our knowledge there is no other system that is competitivetoclan inthat it findssimilar applications.wefaithfully reimplemented mudablue for our experiment as it is described in the original paper .
the original mudablue was implemented and evaluated on a small repositoryof c applicationsthat were selected from five different categories from sourceforge.
comparing two similarity searchenginesthat do notworkwith thesame code base or different granularity levels i.e.
applicati ons vs. code fragments might introduce considerable threats to validity.
sourceforge has a popular search engine and contains a large java repository online apps archive is populated with all java projects from this repository and we applied mudablue as baseline approach to this archive thus making its set of applications comparable with those of clan.
since similarity matrices of mudablue and clan have the same dimensions it is possible to construct a combined matrix whose values are the average of the values of the mudablue and clan matrix elements at the corresponding position.
the intuition behind this combine d approach lies in integrating two approaches mudablue where everyword in the source code of applicationsis taken into consideration versus the clan approach where only api calls with precisely defined semantics are considered.
a research question is whether this integration produces a superior result when compared to each of the constituent approaches.
experimenting with this combined similarity matrix allows us to seek an answer to this question about the benefit of the combined approach.
b. methodology we used a cross validation study design in a cohort of participants who were randomly divided into three groups.
the study was sectioned in three experiments in which each group was given a different engine to find similar applications to the ones that we provided for given tasks.
each participant used a different task in each experiment.
participantstranslated tasks intokey words searchedfor relevant applications using a code search engine and selected an application that matched their key words the best.
we call this application the source application .thena similarity enginereturneda list of top ten target applications that were most similar to the source application.
thus each participa nt used each subject engine on different tasks and different applications in this experiment.
before the experiment we gave a one hour tutorial on using these engines to find similar applications.
the next step was to examine the retrieved applications and to determine if they are relevant to the tasks and the source application.
each participant accomplished thi s step individually assigning a confidence level c to the examined applications using a four level likert scale.
sin ce this examination is time consuming manual and laborious we asked participants to examine only top ten applications that resulted from searches.the guidelines for assigning confidence levels are the following.
completely dissimilar there is absolutely nothing in the target application that the participant finds similar to the source application nothing in it is related to the task and the functionality of the subject application.
mostly dissimilar only few remotely related requirements are located in source and target application.
mostly similar a somewhat large number of implemented requirements are located in the target application that are similar to ones in the source application.
highly similar the participant is confident that the source and the target applications share the same semantic concepts expressed in the task.
all participants were computer science students from the university of illinois at chicago who had at least six months of java experience.
twelve participants were upperlevel undergraduate students and the other participant s were graduate students.
out of participants had programmingexperiencewithjavarangingfromonetothree years and participants reported more than three years of experience writing programs in java.
sixteen participan ts reportedprior experiencewith search engines andeight sa id that they never used code search engines before.
c. precision two main measures for evaluating the effectiveness of retrieval are precision and recall .
the pre cision pr of retrieved applications that are similar total of retrieved applications i.e.
the precision of a ranking method is the fraction of the toprranked target applications that are relevant to the source application where r in this experiment which means that each similarity engine returned top ten similari ty matches.
relevant or similar applications are counted only if they are ranked with the confidence levels 4or3.
the precision metrics reflects the accuracy of the similarity search.
since we limit the investigation of the retrieved applications to top ten the recall is not measured in this study.
we created the variable precision pas a categorization of the response variable confidence c. we did it for two reasons improve discrimination of subjects in the resulting data and additionally validate statistical evaluation of results.
precision pimposes a stricter boundary on what is considered reusable code.
for example consider a situatio n where one participant assigns the level two to all returned applications and another participant assigns level three to half of these applications and level one to the other half.
even though the average of c in both cases the second participant reports much higher precision p .
while the precision that is reported by the first participant is zero.
achieving statistical significance with a stricter discriminative response variable will give assurance that the result is not accidental.d.
hypotheses we introduce the following null and alternative hypotheses to evaluate how close the means are for the cs andps for control and treatment groups where candpare the confidence level and the precision respectively.
unless we specify otherwise participantsof the treatment groupuse either mudablue or combined approaches and participants of the control group use clan.
we evaluate the following hypotheses at a .
level of significance.
h0the primary null hypothesis is that there is no difference in the values of confidence level and precision per task between participants who use mudablue combined and clan.
h1an alternative hypothesis to h0is that there is statistically significant difference in the values of confidencelevelandprecisionbetweenparticipants who use mudablue combined and clan.
once we test the null hypothesis h0 we are interested in the directionality of means of the results of control and treatment groups.
we are interested to compare the effectiveness of clan cn versus the mudablue mb andcombined mc withrespecttothe valuesofconfidence level c and precision p. h1 c of clan versus mudablue.
h2 p of clan versus mudablue.
h3 c of clan versus combined.
h4 p of clan versus combined.
h5 c of mudablue versus combined.
h6 p of mudablue versus combined.
the rationale behind the alternative hypotheses to h1and h2is that clan allows users to quickly understand why applications are similar by reviewing visual maps of their common api calls classes and packages.
the alternative hypotheses to h3 and h4 are motivated by the fact that if all words from source code are used in the analysis in additionto api calls it will worsenthe precisionwith whic h users evaluate retrieved similar applications.
finally h aving the alternative hypotheses to h5andh6ensures that the combined approach still allows users to quickly understand how similar applications share the same semantic concepts using their common api calls classes and packages.
e. task design we designed tasks that participants work on during experimentsin a way that these tasks belong to domainsthat are easy to understand and they have similar complexity.
the authors of this paper visited various programming forums and internet groups to extract descriptions of tasks from the questions that programmers asked.
in addition we interviewed a dozen programmers at accenture who explained what tasks they worked on in the past year.
additional criterion for these tasks is that they should represent real world programming tasks and should not bebiased towards any of the similarity search engines that are used in this experiment.
descriptions of these tasks should be flexible enough to allow participants to find different matching applications for similarity search.
this criteri on significantly reduces any bias towards evaluated similarit y search engines.these tasks andthe resultsof the experimen t are available for download6.
f. tasks the following two tasks are examples from the set of tasks we used in our experiment.
create an application for sharing viewing and exploring large data sets that are encoded using mime.
the data sets may represents blogs or genom sequences.
the data can be stored using key value pairs.
the application should support retrieving data items by mapping keys to values.
implement a library for checking xpath expressions.
the checker should support compiling xpath expressions evaluating xpath expressions in the context of the specified xml document and returning the results as the specified type.
g. threats to validity in this section we discuss threats to the validity of this experimentaldesignandhowwe addressandminimizethese threats.
internal validity participants.
since evaluating hypotheses is based on the data collected from participants weidentifythreethreatstointernalvalidity javaprofici ency motivation and the uniformity among participants.
even though we selected participants who had working knowledge of java we did not conduct an independent assessment of how proficient these participantswere in java .
the dangerof having poor java programmersas participants of our experiment is that they can make poor choices of which retrieved applications have higher similarity to the sourceapplication.thisthreatismitigatedbythefacttha tall participants from uic have documentedexperienceworking on course projects that required writing java code taking classes on programming with java and having experience working as java programmers for commercial companies.
tasks.impropertaskspose abigthreatto validity.iftasks are too general or trivial e.g.
open a file and read its data into memory then every application that has file related api calls will be retrieved thus inundating participants w ith results that are hard to evaluate.
on the other hand if application and domain specific keywords describe a task e.g.
astronomy andcosmic vacuum only a few applications will be retrieved that contain these keywords thus creating a bias towards mudablue.
to avoid this threat we based the task descriptions on specifications follow the experiment link.
a confidence level c. b precision p. figure .
statistical summary of the results of the experime nt forcandp.the central box represents the values from the lower to upper quartile to percentile .
the middle line represents the median .
the thicker vertical line extends from the minimum to the ma ximum value.
the filled out box represents the values from the minimum to t he mean and the thinner vertical line extends from the quart er below the mean to the quarter above the mean.
of different software systems that were written by differen t people including professional programmers at accenture.
while this diversification of tasks does not completely eliminate this threat to validity it reduces it significant ly.
external validity to make results of this experiment generalizable we must address threats to external validity which refer to the generalizability of a casual relationship beyond the circumstances of our experiment.
thefactthatsupportsthevalidityofthisexperimentaldes ign is that the participants are representative of professiona l java programmers since some of them have already joined workforce and others will do soon.
a threat to external validity concerns the usage of search tools in the industria l settings where applications may not use third party api call libraries.
however it is highly unlikely that modern large scale software projects can be effectively develope d maintained and evolved without this reuse.
v. results in this section we report the results of the experimentand evaluate the null hypotheses.
a. results of hypotheses testing weuseone wayanovaandt testsforpairedtwosample for means to evaluate the hypotheses that we stated in section iv d. variables a main independentvariable is the similarity engine mudablue clan combined that participants useto findsimilar javaapplications.dependentvariablesa re the values of confidence level c and precision p. testing the null hypothesis we used anova to evaluate the null hypothesis h0that the variation in an experiment is no greater than that due to normal variation of individuals characteristics and error in their measureme nt.
the results of anova confirm that there are large differences between the groups for cwithf .
fcrit withp .
6which is strongly statistically significant.
the mean cfor the mudablue approach is .
with the variance .
which is smaller than the mean cfor combined .
with the variance .
and it is smaller than the mean cfor clan .
with the variance .
.
based on these results we can reject the null hypothesis and we accept the alternative hypothesis h1.
however the results of anova confirm that there are insignificant differences between the groups for pwithf .
fcrit .
with p .
.
the mean pfor the mudablue approach is .
with the variance .
which is smaller than the mean pfor combined .
with the variance .
and it is smaller than the mean pfor clan .
with the variance .
.
aggregating the values of c intopchanges the results of the statistical test making it difficult to reach a conclusion and it requires more precise statistical tests specifically t tests for paired two sam ple for means which we describe below.
a statistical summary of the results of the experiment forcandt median quartiles range and extreme values are shown as box and whisker plots in figure a and figure b correspondingly with confidence interval for the mean.table i results of t tests of hypotheses h for paired two sample for means for two tail distribution for dependent variable specified in the column var eithercorp whose measurements are reported in the following columns .
extremal values median means variance 2 degrees of freedom df and the pearson correlation coefficient pc are reported along with the results of the evaluation of the h ypotheses i.e.
statistical significance p and the tstatistics .
adecision to accept or reject the null hypothesis is shown in the last column decision.
hvarapproach samples minmaxmedian 2dfpc p ttcritdecision h1cclan .
.
.
.
.
.
rejectmudablue .
.
h2pclan .
.
.
.
.
.
.
.
rejectmudablue .
.
.
.
h3cclan .
.
.
.
.
.96acceptcombined .
.
h4pclan .
.
.
.
.
.
.
.04acceptcombined .
.
.
h5cmudablue .
.
.
.
.
.
rejectcombined .
.
h6pmudablue .
.
.
.
.
.
.
.
rejectcombined .
.
.
comparing mudablue with clan to test the null hypothesis h1 and h2 we applied two t tests for paired two sample for means for candpfor participants who used mudablue and clan.
the results of this test for cand forpare shown in table i. the column samples shows that the number of samples for clan is smaller than the obtained number of samples for mudablue because three participants missed one experiment.
we replaced missing valueswith the averagevaluefor cforclan for thisexperiment.
based on these results we reject the null hypotheses h1 and h2 and we accept the alternative hypotheses that states that participants who use clan report higher relevance and precision on finding similar applications than those who use mudablue .
comparing mudablue with combined to test the null hypothesesh5 and h6 we appliedtwo t tests for paired two sample formeans for candpforparticipantswho used the baseline mudablue and combined.
the results of this test forcand forpare shown in table i. based on these results we accept the alternative hypotheses h5 and h6 that say thatparticipants who use combined report higher relevance and precision on finding similar applications than those who use mudablue .
comparing clan with combined to test the null hypotheses h3 and h4 we applied two t tests for paired two sample for means for candpfor participants who used the baseline clan and combined.
the results of this test forcand forpare shown in table i. based on these results we accept the null hypotheses h3 and h4 that say thatparticipants who use clan do not report higher relevance and precision on finding similar applications than those who use combined .
the result of comparing clan with combined is somewhat surprising.
we expected that combining two differentmethodsofcomputingsimilaritieswouldyielda betterresu lt than each of these methods alone.
we have a possible explanation based on debriefing of the participants.
after the experiment a few participants expressed confusion abou t using the combined engine which reported similar applications even though these applications had no common api calls classes or packages.
naturally this phenomenon is a result of the mudablue s component of combined that computes a high similarity score based on word occurrences while the clan s component provides a low score because of the absence of semantic anchors.
at this point it is a subject of our future work to investigate this phenomenon in more detail.
while combining clan and mudablue did not producenoticeableimprovements combiningtextua l andstructuralinformationwas successfulfortasksof feat ure location and detecting duplicate bug reports .
vi.
discussion duringthe experiment programmersidentified more relevant applications using clan than when using mudablue see section v .
this result points to a key advantage of clan we help programmers effectively compare two applications by elevating highly relevant details of thes e applications.
without clan programmers must examine the whole source code of different applications in order to compare them.
consider the example in figure .
clan returned the application mboxas the most similar application to midiquickfix for the task of recording music data into a midi file.
clan marked these applications as similar because they share important elements of the api e.g.
com.sun.media.sound .
for the same task mudablue did not place mboxeven in the top ten similar applications to midiquickfix .
this example illustrates how clan improves over the state of the art.figure .
part of the clan interface showing the api calls co mmon to two applications.
clan shows these calls in order to help pro grammers concentrate on highly relevant details when comparing app lications.
vii.
r elatedwork the five most related tools to our work are those based on codeweb by michail and notkin mudablue by kawaguchi et al.
hipikat by cubranic and murphy and codebroker by ye and fischer and ssi by bajracharya ossher and lopez .
codeweb is an automated approach for comparing and contrasting software libraries based on matching similar classes and functions cross libraries via name and similarity matching .
th is work was inspirational for us in extending the relevance framework with semantic anchors.
in contrast to codeweb clan also uses advanced dimensionality reduction techniques based on lsi and svd and computes similarities among applications in the context of the complete software repository.
ssi creates an index of code based on the keywords extracted from that code and then expands that index with keywords from other code that uses the same api calls .
clan is different from ssi for three reasons clan locates the applicationssimilar to a given application and does not require a natural language query clan is independent of the keywords chosen in the code and clan has been evaluated using a standard methodology with programmers against a state of the art approach mudablue .
source code search engines have become an active research area in the recent years.
while these approaches are different from clan we believe that majority of these approaches may benefit from the ideas implemented in clan.
among these source code engines are codefinder mica exemplar sniff prospector suade starthcona xsnippet parseweb spars j portfolio sourcerer s6 and spotweb .
while none of these approachesretrieve similar applications to a given candidate software applicatio n these approachesare effective in retrieving relevant soft ware components from open source repositories.ourpreviousworksuccessfullyusestheideaoffunctional abstraction in a search enginecalled exemplarto find highly relevantapplications.however thisideahasneverbeenus ed to compute similarities between software applications.
un like exemplar clan uses a novel combination of semantic layers that correspondto packagesand class hierarchies a nd basedonourextensiontomizzaro srelevanceframeworkwe designed a novel algorithm based on lsi that computes a similarity index between java applications.
other related approachesidentify programs that are likely tosharethesameoriginrelyondynamicanalysisandknown as api birthmarks .
however our approach uses static information and assumes that similar applications may have been implemented by different software developer teams.
likewise software bertillonageis a techniquefor compari ng software components based on the dependencies of those components .
bertillonage is designed to locate duplica te code however and does not compute the similarity of software which may be related but is not duplicated.
viii.
c onclusion we created a novel search system for finding closely related applications clan that helps users find similar or related applications.
our main contribution is in using a framework for relevance to design a novel approach that computes similarity scores between java applications.
we have built clan and we conducted an experiment with participants to evaluate clan and compare it with the closest competitive approach mudablue and a system that combines clan and mudablue.
the results show with strong statistical significance that clan finds similar applications with a higher precision than mudablue.