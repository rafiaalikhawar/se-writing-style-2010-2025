itree efficiently discovering high coverage configurations using interaction trees charles song adam porter and jeffrey s. foster computer science department university of maryland college park u.s.a. email csfalcon aporter jfoster cs.umd.edu abstract software configurability has many benefits but it also makes programs much harder to test as in the worst case the program must be tested under every possible configuration.
one potential remedy to this problem is combinatorial interaction testing cit in which typically the developer selects a strength tand then computes a covering array containing all t way configuration option combinations.
however in a prior study we showed that several programs have important highstrength interactions combinations of a subset of configuration options that cit is highly unlikely to generate in practice.
in this paper we propose a new algorithm called interaction tree discovery itree that aims to identify sets of configurations to test that are smaller than those generated by cit while also including important high strength interactions missed by practical applications of cit.
on each iteration of itree we first use low strength cit to test the program under a set of configurations and then apply machine learning techniques to discover new interactions that are potentially responsible for any new coverage seen.
by repeating this process itree builds up a set of configurations likely to contain key high strength interactions.
we evaluated itree by comparing the coverage it achieves versus covering arrays and randomly generated configuration sets.
our results strongly suggest that itree can identify high coverage sets of configurations more effectively than traditional cit or random sampling.
keywords empirical software engineering software configurations software testing and analysis i. i ntroduction many modern software systems are highly configurable.
while this increases extensibility reusability and portability it also greatly complicates many software engineering tasks such as software testing.
this is because the number of possible configurations grows exponentially in the number of configuration options and in the worst case each configuration may require separate treatment.
specifically since any configuration might harbor a distinct error each configuration should in theory be tested separately something that is impossible in practice.
to alleviate this problem researchers have proposed combinatorial interaction testing cit which identifies a small but systematic set of configurations under which to test.
for example with one cit approach developers choose an interaction strength tand compute a covering array which is a set of configurations such that all possible t way combinations of option settings appear at least once.
the assumption underlying cit is that configuration sets constructed in this way are small in size while providing good coverage of the program s behavior.
thus the approach cost effectively increases the likelihood of finding faults.
however our prior work challenges this assumption in several ways.
specifically we hypothesized that in practice a system s effective configuration space the minimal set of configurations needed to achieve a specific goal typically comprises only a tiny subset of the full configuration space and that subset of configurations is not well approximated by t way covering arrays.
to test this hypothesis we used symbolic execution to discover a subject program s interactions which are conjunctions of option settings needed to achieve specific testing goals given a particular test suite.
in our case the testing goal was particular forms of coverage line block edge and condition .
to illustrate the concept consider a hypothetical program with four binary valued configuration options a b c and d. let s assume that when the goal is line coverage this program has exactly three interactions a b c a b anda d. each interaction is associated with line coverage that is guaranteed to occur in any configuration that contains the interaction.
for example suppose a b cguarantees coverage of lines .
in that case the configuration a setting of all options a b c dis guaranteed to cover at least those same lines.
additionally the union of the lines covered by all three interactions is the maximal coverage achievable across all possible configurations.
see our prior publication for more details .
among other things our prior work found that for our subject programs and test suites a most of the interactions needed to achieve maximum coverage were strength involved option settings or greater and b the largest interactions needed to achieve maximum coverage were strength .
these findings suggest cit approaches which are typically applied at t or t are likely missing key high strength interactions.
put another way while cit at low strength can yield significant coverage it is very unlikely to achieve the maximum possible coverage and achieving such coverage with cit would require prohibitively large covering arrays e.g.
strength .
another finding from our prior work was that interactions were quite rare.
only a handful of specific options setting combinations had to be exercised to maximize coverage even under a comprehensive criteria such as path coverage.
this suggests cit s insistence on testing every t way combination of option settings may be unnecessarily expensive.
to improve the current situation in this paper we propose a new algorithm that addresses the shortcomings of traditional cit.
our algorithm aims to discover sets of configurations to test that are smaller than those chosen by cit while also having higher coverage.
to achieve this aim we developed itree an interaction tree discovery algorithm that combines low strength covering arrays runtime instrumentation and machine learning ml techniques to construct an interaction tree for the subject program.
an interaction tree is a hierarchical representation of what we call proto interactions which are potential interactions or subsets of potential interactions.
in an interaction tree each node is labeled with one or more option settings and a node represents the proto interaction given by the conjunction of all labels on the path from the root to the node.
itree constructs an interaction tree as follows.
initially the tree consists of one root node containing the empty interactiontrue.
at each step we pick a leaf node use cit at low strength to generate a systematic sample of configurations consistent with the proto interaction for that node run the system s test suite under each of those configurations and identify any newly covered program entities.
if there are any we use ml to heuristically identify new proto interactions that are likely responsible for that new coverage which we then add to the interaction tree.
the process continues until one of several stopping criteria is met.
see section iii.
the key intuition behind itree stems from a third finding from our prior work.
there we observed that higher strength interactions were usually just lower strength interactions with one or more additional constraints.
itree exploits this observation by performing an iterative search based process in which the current iteration s sample configurations are based on the last iteration s proto interactions.
in this way the set of configurations constructed as itree executes have the potential to provide higher coverage than correspondingly sized configuration sets produced from traditional cit.
we evaluated itree in several ways.
first we compared itree s performance under various combinations of ml algorithms and cit sampling criteria.
in each case we computed how quickly itree reached maximum possible coverage on two subject programs vsftpd and ngircd studied in our prior work by using symbolic execution and significant computing power the prior work was able to determine all possible program executions achievable for vsftpd and ngircd while varying up to configuration options under a fixed test suite.
we found that the best choice for itree is to use a voting protocol to combine multiple ml classifiers and an adaptive sampling approach that uses higher strength covering arrays initially and lower strength covering arrays below the top level of the interaction tree.
see section iv.
second we compared itree against traditional cit and against similarly sized sets of randomly selected configurations.
again we used vsftpd and ngircd as subject programs.
our results show that itree is more likely to find high coverage configuration sets and it does so more rapidly than the other approaches.
see section v. note that while symbolic execution gave us a powerful and precise baseline for the first two experiments that technology is not scalable to large systems.
for example for the 10k loc systems we studied complete analysis used computers working round the clock for several days.
in a final experiment we evaluated the scalability of itree to a large scale system for which symbolic evaluation is infeasible specifically the 1m loc mysql database system.
we found that itree easily scaled up to mysql and was again more efficient and effective than either cit or random sampling see section vi .
these results suggest itree is an important advance in the testing of highly configurable systems.
our approach will enable developers to test their systems to higher levels of coverage at less cost than currently possible with traditional cit.
we also believe the itree process can be adapted to document a system s configuration related structure to better support a range of software engineering tasks such as impact analysis reverse engineering bug isolation and more.
ii.
e ffective configuration spaces our previous work empirically studied how and why the execution behavior of two configurable systems vsftpd and ngircd changes in relation to how those systems are configured.
as discussed earlier the results of our previous study strongly suggest that for a given test suite maximal levels of coverage can be achieved with a very small number of carefully chosen configurations i.e.
systems effective configuration spaces with respect to maximal coverage are indeed small.
for developers to exploit this insight however they need cost effective and time sensitive techniques for choosing the specific configurations to test.
unfortunately we currently know of no such techniques the approaches we used in our previous work are computationally very expensive and symbolic execution cannot be run to exhaustion on large practical systems.
key observations thus the goal of the current work is to begin creating a practical method to identify or approximate a system s effective configuration space.
our particular focus is on finding small sets of configurations for testing that yield a high degree of coverage but we believe our approach generalizes to other software engineering tasks.
toward this aim we reexamined the specific interactions we found in our initial study and made a number of observations that suggest the design of such a method.
we illustrate our observations using the code in figure which contains a highly simplified snippet of vsftpd s source code.
the code includes two traditional program variables dsa cert file andone process mode which are initialized on lines and .
in practice dsa cert file is a program input whose value would come from a test case but we have hard coded it here for simplicity.
the program also contains 1int dsa cert file null test input 2intone process mode 3if listen if accept timeout r1 listen accept timeout else r2 listen accept timeout else r3 listen if sslenable if !dsa cert file die r4 sslenable if one process mode if local enable sslenable die r5 sslenable local enable if !
local enable !
anonymous enable die r6 lots of code sslenable local enable anonymous enable if dual logenable r7 sslenable local enable anonymous enable dual logenable else r8 sslenable local enable anonymous enable dual logenable figure .
an example program and its interactions.
six binary configuration options highlighted in bold whose values depend on the system s runtime configuration.
in the actual source code each option s name is prefixed with tunable but we have omitted the prefixes to save space.
figure includes eight regions of code marked r1 whose coverage we are interested in.
the coverage of these regions of course depends on the values of the configuration options and program variables.
for each region we list the interaction that controls coverage of that region for this particular test case.
for example at the beginning of the program the coverage of r1 r3 depends on configuration variables listen andaccept timeout .
more interestingly for execution to reach the large amount of code in r6 several options must be set in specific ways.
first to reach r4 and any code thereafter sslenable must be set to because this test case sets dsa cert file to be null .
next consider reaching r5 .
since one process mode is set to to reach r5 the condition on line must be false and since as just discussed sslenable is if we reach this line localenable must also be .
finally to continue on to reach r6 we need the condition on line to be false and since localenable 0if we reach that line we must have anonymous enable .
putting this together any configuration that reaches r6 for this testcase needs at least sslenable localenable and anonymous enable .
finally the coverage of r7 andr8 also depends on the value of duallog enable .
note that although in this example we were able to reach all code regions and coverage of each was guaranteed by a distinct interaction in practice this is not usually the case.
in actual systems some regions are unreachable with the given test suite and some regions have more than one interaction that guarantees their coverage.
we found that the configuration option patterns just described are common in vsftpd and ngircd.
from these patterns we make three observations observation interactions are relatively rare.
the code shown in figure includes six binary options so in the worst case there could be different interactions .
in the example code however there are only eight interactions.
since some of these interactions can be simultaneously satisfied in a single configuration only three configurations are needed to cover all eight regions.
for vsftpd and ngircd respectively there were only and interactions.
observation most coverage is explained by low strength interactions.
in the example five of the eight interactions involve only one or two option settings one interaction involves three settings and the remaining two involve four settings each.
while the example is highly simplified we found the same trend in the actual systems.
for the systems and test suites we examined over of the achievable coverage could be achieved with lower strength size four or less interactions.
full coverage however required a handful of higher strength interactions up to strength seven .
observation higher strength interactions tend to be built on lower strength ones.
as shown in the example the higher strength interactions guaranteeing coverage of r7 and r8 are refinements of the interaction at r6 which is itself a refinement of r5 .
in implementation terms interactions tend to arise because control flow guards effectively stack up on each other not because complex guards appear directly in the code.
that is higher strength interactions often add additional constraints to existing lower strength interactions.
iii.
i nteraction tree discovery based on the insights discussed in section ii we developed the interaction tree discovery algorithm itree .
itree s goal is to automatically discover and test a small set of high coverage configurations.
itree works as follows.
first it instruments the system under test to measure some desired type of coverage.
this paper focuses on line coverage but the algorithm should apply to any type of coverage.
next itree repeats the following steps until particular stopping criteria are met.
first it computes a small sample of configurations under which to test the system.
as we 1computed as summationtext6 i c i 2i i.e.
the sum of all ways of picking option subsets times the number of settings plus the interaction true.true local enable ssl enable local enable ssl enable local enable dual log enable dual log enable figure .
the interaction tree for the example program.
shall see later the sample is chosen to select configurations that are likely to exercise previously uncovered entities.
next itree runs the system s test suite on each of the sampled configurations and records coverage information.
using this coverage data itree then attempts to learn protointeractions conjunctions of option settings that cause the new coverage and that may warrant further exploration in the next iteration of itree.
we represent itree s behavior as an interaction tree which is a hierarchical representation of proto interactions.
the nodes of the tree represent proto interactions rather than interactions because they may not in fact be full fledged interactions because itree is heuristic in nature some nodes may represent only portions of interactions or may represent full interactions with additional constraints.
figure shows the interaction tree for figure .
each node is labeled with a set of option settings with true at the root node corresponding to the empty setting .
a node represents the proto interaction that is the conjunction of settings along the path from the root to the node.
for example the interaction localenable sslenable duallog enable is represented by the left node on the lowest level of the tree.
we also see that localenable sslenable is in the interaction tree but does not correspond to an actual interaction that guarantees coverage of particular code regions.
thus in this case itree has created a proto interaction that will not lead to useful higher strength interactions.
algorithm figure gives the pseudocode for the itree algorithm.
itree runs in a loop iterating until developersupplied stopping criteria are met e.g.
no more coverage is achieved or a time limit has expired .
the itree algorithm begins with an interaction tree itree containing just one node true.
as the itree progresses it also records in runs the set of all configurations executed so far and their corresponding coverage information.
at the beginning of each iteration findbestleafnode uses various heuristics to pick a leaf node to explore next.
this heuristic is important because we do not expect to fully explore the interaction tree as that would be too expensive.
next the proto interaction represented by the path to the selected node is passed to generateconfigset .
this method 1itree tree containing root true 2runs config coverage set 3do node findbestleafnode itree runs configset generateconfigset node.proto interaction newruns executeconfigset configset if cov newruns cov runs continue runs runs newruns interactions discoverprotointers node.proto interaction runs if !
interactions.empty add newly discovered interactions to tree updatetree itree node interactions while !stoppingcriteriamet figure .
pseudocode for the interaction tree discovery.
creates a sample set of configurations that are consistent with the proto interaction represented by the selected node while the set of configurations broadly samples all options not participating in the proto interaction.
currently itree leverages cit for this step but other sampling techniques could be substituted.
next executeconfigset compiles instruments and executes the system s test suite under each configuration in the sample.
the data from the resulting executions is then added to runs .
then runs andnode .proto interaction the proto interaction represented by node are passed to discoverprotointers which uses machine learning to discover additional proto interactions that account for any newly covered entities.
note that by design any protointeractions discovered at this step must include the settings in node .proto interaction .
finally updatetree adds the newly discovered proto interactions to the interaction tree as children of the current node.
we now discuss each step of the algorithm in more detail.
findbestleafnode since itree aims to find highcoverage configurations findbestleafnode prioritizes nodes by the amount of coverage achieved by configurations containing the node s proto interaction.
the assumption is that proto interactions corresponding to high coverage configurations are more likely to lead to uncovered code with further exploration.
itree computes a node s priority as follows.
first let conf runs node be the subset of runs whose configurations are consistent with node s proto interaction.
for a run r conf runs node define cov r as the number of entities covered by r. then node s priority is given by priority node summationtext r conf runs node cov r conf runs node and the highest priority node is chosen.
the formula simply computes a slightly biased average coverage for all configurations that are consistent with the node s proto interaction.
the bias of one added in the denominator means that nodes corresponding to few runs will have lower priority than their average but has little effect on nodes corresponding sslloc lis acc anon dual c1 c2 c3 c4 c5 c6 a initial covering array ssl loc lis acc anon dual c7 c8 c9 c1000 c1100 b covering array with ssl loc ssl ssl enable loc local enable lis listen acc accept timeout anon anonymous enable dual dual log enable figure .
example way covering arrays.
to many runs since then conf runs node is high .
we found this adjustment useful in that it leads to a slight but beneficial preference for nodes that correspond to multiple high coverage configurations over nodes that correspond to fewer high coverage configurations.
generateconfigset this function generates a sample set of configurations each of which is consistent with its parameter node .proto interaction .
to do this we use a cit tool called casa to generate a low strength covering array over only the remaining options.
we then combine those partial configurations with the settings from node .proto interaction .
in our experiments we used both and way covering arrays in this step and found the performance was not sensitive to this choice.
figure shows two covering arrays created by generateconfigset as itree discovered the interaction tree in figure .
in this case we chose to generate way covering arrays.
figure a gives the covering array picked in the first iteration of itree.
interestingly our way covering array happened to include both the way interaction see figure sslenable localenable anonymous enable in c5 needed to reach r6 and beyond and the way interaction sslenable localenable anonymous enable duallog enable also in c5 needed to reach r7 .
after the data from these configurations was analyzed itree added the three children of true shown in figure .
the next iteration of itree expanded the middle of the three nodes since this node covered r6 which contains many lines and generateconfigset created the covering array shown in figure b .
note that in this covering array sslenable andlocalenable are fixed.
as a result the way covering array of the remaining options is very effective and includes both way interactions the one mentioned plus the one needed to reach r8 in c7 and c10 .
at this point itree has covered all the marked regions of the program.
executeconfigset in this step we instrument the system under test and execute its test suite on each configuration in the sample.
we compute line coverage with gcov.
we ran the instrumented programs on skoll a distributed continuous quality assurance system running on a grid comprising cpus .
as we will see in section vi skoll allowed us to scale up itree running and analyzing many jobs at once.
discoverprotointers finally we use a two step process to discover proto interactions to add to the interaction tree first we statistically cluster configurations according to their coverage data and second we try to find proto interactions responsible for differences in execution.
in the first step we find all runs involving configurations consistent with the proto interaction we are exploring.
note that we extract this subset from all of runs not just those newly explored in the current iteration this way we get better information as itree progresses.
we then cluster these runs using weka s implementation of clope a clustering algorithm that groups together similar transactional data records with high dimensionality.
as input to clope we represent each line as a boolean attribute set to true if covered in a run and false otherwise.
then we use clope to cluster together configurations that execute many of the same lines.
in the second step we use decision trees to discover commonalities among configurations in each of the clusters.
in our implementation each configuration option is an attribute and the cluster that a configuration belongs to is the class.
the decision tree algorithm then builds a model for classifying the cluster to which a configuration belongs based on the configuration s option settings.
if the resulting model identifies specific option settings that predict cluster membership then we treat them as new proto interactions and append them to the interaction tree to form higher strength proto interactions.
otherwise no new proto interactions are added and exploration of this path stops.
in our experiments we evaluated several decision tree algorithms and found each to be adequate for this task.
we should mention that clope requires a special parameter called repulsion which ranges from .
to .
and which controls the ease with which clusters form.
to make itree completely automated we implemented a voting system to adaptively select an appropriate repulsion value.
each time discoverprotointers is called clope is run multiple times with repulsion values ranging from .
to .
in increments of .
.
we perform the second step of the interaction discovery process using the clusters generated under each repulsion value.
at the end of discoverprotointers we keep the most frequently occurring unique proto interactions generated under the range of repulsion values.
stoppingcriteriamet itree allows its users to plug in their own criteria for determining when to halt execution.
our default is to halt execution when the interaction tree vsftpd ngircd version .
.
.
.
lines sloccount run time opts.
boolean enum full config space .
.
test cases max coverage figure .
program statistics for vsftpd and ngircd.
note that we removed some unreachable code before measuring lines of code.
has no more unexplored proto interactions.
the experiments in the following sections include other criteria as well e.g.
in some experiments we stop execution when a maximum number of configurations have already been tested.
another possibility is to use wall clock time as a stopping criteria e.g.
when nightly testing.
iv.
e valuating i tree parameters we explored itree s cost effectiveness in a series of experiments described in this and the next two sections.
our first experiment presented next aims to determine two key algorithmic parameters the covering array strength to use in generateconfigset and the decision tree implementation to use in discoverprotointers .
our second experiment explores modifying itree to adaptively select covering array strength and use multiple decision tree approaches simultaneously.
our remaining experiments compare the coverage achieved by itree random sampling and a single high stength covering array sections v and vi .
a. experimental setup subject programs for our first experiment we used vsftpd a widely used secure ftp daemon and ngircd the next generation irc daemon.
we studied these systems extensively in prior work .
from our prior work we have test suites for these programs and detailed information about the programs configuration spaces with respect to those test suites.
figure gives descriptive statistics for each system.
they have roughly 13kloc and are written in c. the figure details the total number of configuration options we analyzed broken down by type boolean or integer .
this is the same set of options and settings we used in our prior work.
the values we used for the integer options also came from our previous work and were chosen to maximize path coverage for these subject programs and test cases.
finally the last rows list the size of the full configuration space for the options the total number of different possible configurations the number of test cases in our test suite and the maximum possible number of lines covered if we execute every test case under every possible configuration.
covering array strengths each itree iteration begins by creating a sample of configurations derived from a t way covering array.
the value of tdetermines the size of each sample and may also influence the speed with which itree terminates.
in this study we use either t or t .vsftpd number of confs bf cart j48 vote t bf cart j48 vote t vote a t adapt ngircd number of confs bf cart j48 vote t bf cart j48 vote t vote a t adapt figure .
interaction tree effort for different itree parameters.
decision tree algorithms many different decision tree classifiers have been proposed in the machine learning literature.
we used three algorithms in our experiment j48 cart and best first all as implemented in weka .
we picked j48 and cart because they are the most popular decision tree implementations.
we chose best first because it is designed to produce compact classifications which may be well suited to itree s incremental search approach.
experimental design we ran itree times on both subject programs under each possible combination of decision tree and covering array strength.
for each run we continued execution until we reached the maximum possible coverage as determined from our prior work .
the number of configurations executed is our metric for finding the best parameter settings the lower the number the faster the algorithm achieves full coverage.
note that in these experiments rather than actually run the executeconfigset step we instead used the code coverage data we had already computed in our prior work which gave us a mapping from configurations to their coverage .
b. data and analysis figure shows boxplots of our experimental results for vsftpd and ngircd.
the left half of each chart shows the results of the decision trees for t and the right half shows the results for t .
the y axis reports the number of configurations required to achieve full coverage.
the number in parentheses under each box indicates the number of runs out of in which full coverage was achieved.
we defer discussion of vote andvote ato section iv b3.
covering array strengths figure shows that increasing the tstrength of the covering arrays did not greatly change the cost of running itree for vsftpd.
it did have some effect for ngircd where the average size of the configuration sets increased across all decision tree algorithms.
however we also see that the number of runs in which itree reached maximal coverage is substantially higher when t than when t .
we looked in more detail at the individual runs and observed that at each iteration both the likelihood of discovering proto interactions and the accuracy of the discovered proto interactions dramatically improved as tincreased.
however there was a trade off while increased sample size meant more cost at each iteration it also resulted in fewer overall iterations for our subject systems.
in the end the total cost did increase for ngircd.
we note that variance in the number of configurations tested appears unrelated to covering array strength.
instead it seems more tied to the system tested.
in particular for vsftpd the range of the number of configurations tested is fairly stable while for ngircd it fluctuates considerably.
based on further analysis we believe this occurs because the configuration space model for ngircd taken from our previous work contained many redundant option settings from the perspective of line coverage i.e.
a given line of code could be hit in many different configurations.
therefore in a sense ngircd s way covering arrays already enjoyed the benefits of larger configuration samples.
decision trees in figure the first three columns for eacht value show the effect of the best first cart and j48 decision trees on itree.
the data shows no systematic performance differences across the algorithms.
looking at the individual iterations of itree however we did find some differences best first and cart fail to discover any proto interactions in the configuration sample more often than j48 does but j48 tends to produce less accurate classifications it more often includes option settings that are not part of the actual interactions.
both situations can have negative consequences.
for instance failing to discover a real interaction will cause itree to improperly abandon the currently selected node and continue with a lowerpriority proto interaction.
this can delay or even prevent coverage of some necessary higher strength interactions.
discovering inaccurate proto interactions on the other hand can be worse.
because itree builds higher strength protointeractions on top of lower strength ones inaccurate option settings can propagate through an itree path and ultimately could prevent itree from achieving complete coverage.
hybrid approaches when we examined the worstperforming runs from the previous experiments we found they suffered from inaccurate early classification that rippled through subsequent iterations of itree.
to avoid this problem we took two steps.
first we developed a voting system similar to bagging that creates an ensemble classifier out of several simple classifiers.
the voting algorithm filters out option setting combinations unless at least out of decision trees produce them as classifiers.
the results using the voting system are shown in the vote column.
second we developed an adaptive sampling approach in which we create three way covering arrays in the first iteration and then one way covering array in each remaining iteration.
also if an interaction tree path is about to terminate because no new proto interactions have been found we generate and test one new covering array before abandoning that path.
the results using both voting and adaptive sampling are shown in the vote acolumn.
we can see from the figure that vote ais an attractive choice overall its average cost is lower or only slightly worse than the best of the other algorithms and it yields full coverage on every or almost every run.
v. c omparing i tree to other approaches as mentioned earlier both cit and random sampling are popular approaches for generating configuration samples and produce relatively good results in practice.
to better understand how itree compares with these existing approaches we conducted a series of experiments.
experimental design for these experiments we again used vsftpd and ngircd and ran each technique times.
one problem with cit and random sampling is that developers cannot know a priori how large a sample is necessary.
for cit developers must pick a tvalue and for random sampling developers must guess a sample size.
in this experiment we created covering arrays using a range of different tstrengths.
for each strength testing ran until either the maximum possible coverage was achieved or until no more configurations remained.
using and way covering arrays for vsftpd and ngircd respectively often produced maximal coverage so we used those as our largest sample sizes.
we next tested the systems with random samples sized equal to the average size of these largest covering arrays.
we also tested the systems using itree.
for this experiment we used vote aas described in the previous section and set itree to stop when either no new coverage is achieved on any path or the number of configurations tested exceeds the size of the random configuration samples.
we measure performance using two criteria whether complete coverage was reached and if so the number of configurations needed to reach the complete coverage.
we ignore the time needed to generate the configurations because in our experiments this time is dramatically smaller than the time required to run the test cases.
data and analysis figure shows the results of these experiments.
the x axis is the number of configurations tested so far in each run and the y axis is the median number of lines covered at that point across all runs.
here we are assuming that configurations are tested in the order they are generated although in actuality the work can be done in parallel across multiple cpus.
the data points plotted in each figure divide the time line into equal epochs corresponding to or configurations tested for vsftpd vsftpd number of confs cumulative coverage itree rand 5wayca 4wayca 3wayca ngircd number of confs cumulative coverage itree rand 4wayca 3wayca figure .
comparing itree against covering arrays and random sampling.
and ngircd respectively.
note that itree runs can terminate before executing the number of configurations of the other methods.
in that case we simply treat subsequent time points as unchanged from the previous time point.
the figures also include a vertical line indicating the epoch in which of the runs achieved maximal coverage.
the numbers in parentheses in the legend indicate the total number of runs out of each that reached full coverage for each approach.
in the top portion of figure showing the vsftpd results we see that itree way covering arrays and random sampling eventually reached full coverage in almost all runs and respectively but way only reached full coverage in a third of the runs and way never reached full coverage.
moreover looking at the vertical lines we see that of the itree runs reached full coverage with just over half the number of configurations on average of 5way covering arrays which themselves did noticeably better than random sampling.
we see a similar trend for ngircd in the bottom figure for which itree way covering arrays and random sampling achieved full coverage in all or almost all runs and respectively but way covering mysql version .
lines sloccount compile time opts.
boolean enum run time opts.
boolean enum full config space .
test cases figure .
mysql program statistics.
arrays only reached full coverage in just over half the runs .
again of the itree runs reached full coverage faster than way covering arrays of which reached full coverage faster than random sampling.
while itree s benefit for ngircd is not quite as stark as for vsftpd it still provides noticeable improvement over the other approaches.
discussion overall these results showed itree performing better than t way covering arrays and random sampling at substantially less cost.
this conclusion of course depends on how those approaches are actually used.
for example if developers used high strength covering arrays or large random samples they would be likely to get most of the available coverage but would do so at large cost.
as we know from our previous research this is not a very efficient approach because few of those configurations are really necessary to achieve specific types of coverage such as line coverage.
for instance it would require a way covering array with thousands of configurations to guarantee complete line coverage for vsftpd and ngircd.
if developers instead used a low strength way covering array the cost would be much lower but so would the coverage.
vi.
e valuating scalability using itree we were able to achieve maximal coverage while executing on average about configurations for both vsftpd and ngircd.
this is encouraging but after all we had already solved this problem using symbolic execution albeit at a far higher cost.
however ultimately our goal is to handle much larger systems written in a variety of languages with compile time as well as run time configuration options.
none of these issues can currently be addressed using symbolic execution but we believe that itree may be the right tool for this problem.
to better understand this issue we evaluated the scalability of itree by running it on mysql a popular open source database.
we are not aware of any current symbolic execution system that can fully handle this system.
mysql has more than 900k lines of code.
it is written in a combination of c and c and its configuration space includes a large number of run time as well as compiletime configuration options.
as in our previous experiments in section v our evaluation compared itree against covering arrays and randomly sampled configurations.subject program figure gives descriptive statistics for mysql.
the top two rows list the version we used and the lines of code it contains as computed by sloccount .
next the figure lists the number and types of configuration options we selected for our experiment.
we give the numbers of compile time and run time configuration options separately and each number is also broken down by type boolean or enumeration .
all told we are focusing on configuration options.
we selected configuration options and settings that enabled the test suite to exercise the major configurable features of mysql such as default storage engines sql modes and transaction isolation modes.
all other mysql options were left with their default values.
the next row in figure lists the number of unique configurations that can be generated given the number of distinct settings of configuration option.
all told the full configuration space given the subset of mysql options we are considering includes roughly 600k configurations.
finally the last row in the figure lists the number of test cases comprising the regression test suite that comes with mysql which we used for our experiment and focused on improving its line coverage.
we should note that this is not a comprehensive high coverage test suite the default configuration achieves 90k loc.
we also note that not every test case runs in every configuration.
experimental design our experimental design is similar to that of section v. specifically we compare way covering arrays way covering arrays random sampling and itree.
on average way coverings contained configurations way covering arrays contained configurations and random sampling also selected configurations.
we executed each approach times and computed how much line coverage was achieved under each.
for itree we used the vote approach.
one key difference between this experiment and the last is that we cannot know the maximal possible coverage achievable by the test suite and so we only discuss observed line coverage.
we executed the experiment on the skoll cluster using up to cpus at a time.
executing the mysql test suite takes approximately .
hours.
the process involves downloading the mysql source tree from source code repository compiling an instance according to the compile time option settings for the configuration to be tested instrumenting the instances with gcov starting the instance with the run time option settings dictated by the configuration to be tested running the test suite and collecting the execution data.
data and analysis figure summarizes the experimental results.
the figure shows the growth in median coverage over time under each of the four approaches used measured at equally spaced intervals.
the y axis is the number of covered lines and the x axis indicates the number of configurations tested.
we can see from these results that itree covered more lines of code on average than the other methods after running the same number of configurations.
mysql number of confs cumulative coverage itree rand 4wayca 3wayca figure .
comparing the number of configurations and coverage achieved using different testing approaches.
interestingly the traditional methods have very similar performance profiles.
thus with respect to this data it appears that at every level of effort itree selected samples included configurations with unique coverage patterns that were not found by more traditional approaches.
the absolute difference in line coverage ranges from a high of around .
700loc early on down to about .
loc near the end of the experiment.
to better understand why these lines were found by itree but not by the other methods we manually inspected mysql s source code.
we observed that the extra lines covered with itree involved many small pockets of code scattered across numerous files methods and blocks and are apparently only executed in very specific circumstances.
we further attempted to determine what interactions control the lines that are covered by itree and not the other approaches but were unable to decide this because of mysql s size and complexity.
however we generated a way covering array executed its configurations and found that none of these configurations covered those lines either.
this implies that the interactions controlling the lines in question are of strength or higher.
vii.
t hreats to validity like any empirical study our observations and conclusions are limited by potential threats to validity.
for example in this work we used widely used subject programs.
two are medium sized one is quite large.
to balance greater external validity against higher costs and lessened experimental control we focused on subsets of configuration options that we determined to be important.
the size of these sets was substantial but did not include every possible configuration option to keep our analyses tractable.
the structural coverage criteria was line coverage.
other program behaviors such as data flows or fault detection might lead to different performance trade offs.
our test suites taken together have reasonable but not complete coverage.
individually the test cases tend to focus on specific functionality rather than combining multiple activities in a single test case.
in that sense they are more like a typical regression suite than a customer acceptance suite.
we intend to address each of these issues in future work.
viii.
r elated work combinatorial interaction testing.
covering array based sampling for software testing is a specification based technique that was originally proposed as a way to ensure even coverage of combinations of input parameters to programs .
in more recent work covering arrays have been used to model configurations that should be selected for testing where the covering array defines atest schedule and each configuration is tested with an entire test suite.
covering arrays have also been used to test graphical user interfaces and in model based testing .
empirical research suggests testing with covering arrays witht 6can potentially find a large proportion of interaction faults .
further studies suggest covering arrays can be effective in practice and can yield good structural coverage during testing .
machine learning in software engineering.
many researchers have proposed using dynamic analysis with machine learning techniques to analyze program executions.
haran et al.
developed three techniques association trees random forests and adaptive sampling association trees to automatically classify fielded software system executions.
podgurski and colleagues used treebased strategies and random sampling to classify program faults in order to prioritize software failure reports.
brun and ernst use machine learning to classify program invariants that manifest themselves in failing program runs.
we are not aware of any other work that applies machine learning to testing software configurations.
test case prioritization.
the itree approach is similar to some test prioritization techniques which try to find effective orderings of test cases that reveal faults earlier in the testing process.
many such techniques also utilize structural coverage as the surrogate criteria for prioritization .
leon et al.
evaluated distribution based cluster filtering on the execution profiles as prioritization scheme and found that it detects different faults than coverage based prioritization.
yoo et al.
also applied clustering of test case dynamic runtime behavior to aid test prioritization.
li et al.
evaluated greedy metaheuristic and evolutionary search algorithms for prioritization.
ix.
c onclusions and future work we have presented a new and scalable technique called itree to support the testing of highly configurable systems.
itree s goal is to select a small set of configurations in which the execution of the system s test suite will achieve high coverage.
this technique is based upon insights gained from our previous empirical studies in which we precisely quantified the relationships between software configuration and program execution behaviors.
these insights led us to create a heuristic process that effectively searches out configurations in which high coverage is likely.
to evaluate itree we conducted several sets of experiments.
keeping existing threats to validity in mind we tentatively drew several conclusions.
all of these conclusions are specific to our programs test suites and configuration spaces further work is needed to establish more general trends.
the first set of studies evaluated the basic itree approach and its parameters.
based on these efforts we developed several optimizations such as adaptive voting that improve robustness while also removing many issues that must be handled manually with current techniques.
the second set of studies compared itree with t way covering arrays and random sampling both existing techniques.
the studies suggested that itree produced higher coverage then the other techniques while testing fewer configurations.
the third set of studies focused on scalability.
this study applied itree to mysql a large and popular database system.
the results strongly suggested that itree achieved higher coverage at lower cost than existing techniques.
taken together our results strongly suggest that itree is a promising technique that can scale to practical industrial systems.
based on this initial work we plan to pursue several research directions.
first we will extend our studies to include more systems with larger and more complex configuration spaces.
second we plan to enhance itree to incorporate new kinds of coverage.
we will also examine how information gained as itree operates might be incorporated into itree s heuristics.
finally we will explore post processing the information and artifacts that itree creates to support other software engineering tasks such as impact analysis reverse engineering and automatic architecture documentation.