see discussions st ats and author pr ofiles f or this public ation at .researchgate.ne t public ation comparing multi point stride coverage and dataflow coverage conf erence paper in proceedings int ernational conf erence on softw are engineering may .
ic se.
.
citations 19reads author s including mohammad mahdi hassan qassim univ ersity publica tions citations see profile all c ontent f ollo wing this p age was uplo aded b y mohammad mahdi hassan on dec ember .
the user has r equest ed enhanc ement of the do wnlo aded file.comparing multi point stride coverage and dataflow coverage mohammad mahdi hassan computer science department university of western ontario london canada email mhassa47 csd.uwo.cajames h. andrews computer science department university of western ontario london canada email andrews csd.uwo.ca abstract we introduce a family of coverage criteria called multi point stride coverage mpsc .
mpsc generalizes branch coverage to coverage of tuples of branches taken from the execution sequence of a program.
we investigate its potential as a replacement for dataflow coverage such as def use coverage.
we find that programs can be instrumented for mpsc easily that the instrumentation usually incurs less overhead than that for def use coverage and that mpsc is comparable in usefulness to def use in predicting test suite effectiveness.
we also find that the space required to collect mpsc can be predicted from the number of branches in the program.
keywords software testing control flow coverage data flow coverage partial execution pattern.
i. i ntroduction structural coverage measures measure how much of a program s structure a test suite exercises.
some form of coverage criteria have been used in software testing for decades to judge whether test suites are thorough enough .
in broad terms structural coverage criteria can be divided into two different domains control flow coverage cfc and data flow coverage dfc .
cfc includes measures like branch coverage which requires that each direction of each branch be taken by some test case.
dfc includes measures like def use coverage which requires that every def use path path from an assignment of a value to a variable to a use of that assigned value is taken by some test case.
in general it is accepted that dfc is superior to cfc in terms of finding and locating faults.
there is a tradeoff between them as dfc is complicated to implement and its measurement incurs more overhead than cfc so in practice some form of cfc is accepted and widely used in the software industry.
in this paper we bridge the gap between the measures by introducing a family of cfc criteria which we call multipoint stride coverage mpsc .
informally instrumentation for mpsc with gap gandppoints records the coverage of tuples b1 b p of branches taken where each branch in the tuple is the one taken gbranches after the previous one.
our research addresses four main research questions rq1.
how many mpsc tuples typically need to be collected for a program and how is that number related to other program metrics?
rq2.
what is the relationship between mpsc and dfc criteria such as def use?
rq3.
does mpsc lead to a more accurate assessment of test suite effectiveness than def use?
rq4.
what is the performance overhead of collecting mpsc compared to the uninstrumented program and to dfc?
we addressed these questions by experimentation on programs of various sizes in three commonly used programming languages.
our main findings on these programs are summarized below.
every program is associated with a constant cwhich allows us to accurately predict how much storage to allocate for mpsc data collection given the number of branches in the program.
the def use coverage of test suites is strongly correlated with their branch coverage raising questions about how distinct def use coverage is from branch coverage in practical settings.
mpsc allows us to predict the effectiveness of a test suite with a similar or higher level of accuracy than def use.
the instrumentation for mpsc is usually more efficient that that for def use coverage.
the simplest member of the mpsc family of criteria is often the most useful member.
however the family as a whole provides the user with a wide choice of useful coverage criteria.
we organize the paper as follows.
in section we give some basic definitions.
in section we discuss related work.
in section we describe the design and results of our empirical study of the basic properties of mpsc rq1 .
in section we describe the design and results of experiments to determine the relationships between mpsc and def use coverage rq2 and rq3 .
in section we describe the design and results of experiments we did to measure the performance overhead of mpsc compared to def use rq4 .
in section we present an overall discussion of the results.
in section we conclude and suggest future work.
ii.
d efinitions in this section we give precise definitions of the coverage criteria we consider in order to facilitate later discussion.abranch in a program is one direction of an if for orwhile decision or one case of aswitch statement1.
the branch coverage of a test suite set of test cases is the proportion of branches that are executed by at least one test case .
a statement is a definition ordefof a variable xif it assigns xa new value.
a statement is a useof a variable xif it contains a reference to the current value of x. two statements s1and s2constitute a def use pair forxifs1is a def ofx s2is a use ofx and there is a path froms1tos2that does not pass through any other defs of x .
the def use coverage of a test suite is the proportion of def use pair paths that are executed by at least one test case.
we do not discuss in detail here issues like the treatment of function parameters and the distinction between p use and c use the reader is referred to rapps and weyuker and horgan and london for more detail.
we propose a family of coverage criteria which is a generalization of branch coverage.
if b1andb2are branches we say that a test case executes gap gbranch stride b1 b2 if it executesb1 and thengbranches later interprocedurally it executesb2.
we define the gap gbranch stride coverage of a test suite as the proportion of gap gbranch strides that are executed by at least one test case.
we generalize this to multi point stride coverage mpsc by extending the pair b1 b2 to a sequence of ppoints b1 b2 b p each separated from the next by gbranches interprocedurally .
we refer to the sequence of ppoints as an mpsc tuple .
in order to handle the beginning and end of the execution sequence we assume a sequence of begin pseudobranches at the beginning of the sequence and endpseudobranches at the end.
for instance if a small program executes only the four branches b1 b2 b3 andb4in that order then there are six mpsc tuples with gap and points executed by the program begin begin b begin b b2 b1 b2 b3 b2 b3 b4 b3 b4 end and b4 end end .
mpsc is thus a family of coverage criteria one for each value of gap size gand number of points pin a tuple.
if p then mpsc is the same as branch coverage if g then mpsc is equivalent to branch coverage since all the points in an mpsc tuple with g are the same.
mpsc can be extended to other kinds of coverage features e.g.
statements or conditions the key consideration is that those features must be collected according to their execution sequence.
in this paper we concentrate on mpsc based on branch coverage since branch coverage is a moderately strong and commonlyused coverage measure.
we use covset s t g p to mean the set of unique mpsc tuples covered by test case tfor program swhen using gap g andppoints.
we define ncov s t g p jcovset s t g p j. we extend this to test suites i.e.
sets tof test cases by defining covset s t g p as the union of all the sets covset s t g p for allt2t and ncov s t g p 1we restrict our attention to c c and java.
the concepts can be easily extended to other languages.jcovset s t g p j. we use maxcov s g p to mean the maximum value that ncov s t g p can be for any test suite t. the precise value of maxcov s g p cannot be determined in general since whether a given branch in program scan be taken is undecidable.
an obvious upper bound is np wherenis the number of branches in the program.
static analysis analysis of source executable or intermediate code could provide a tighter upper bound and dynamic analysis execution of test cases can provide a lower bound.
in this paper we approximate maxcov s g p by the size of the union of all the sets covset s t g p for all test cases tin a test pool for the program s. iii.
b ackground and related work a. code coverage measures structural code coverage measures such as branch coverage have long been studied as a means for evaluating the thoroughness of a test suite .
tools that automatically evaluate given code coverage measures on a test suite are now commonly used in industry .
they are now increasing in importance since they are used not only in test case construction by humans but also in automatic test input generation test suite minimization and prioritization and fault localization .
many coverage measures have been proposed zhu et al.
give a comprehensive survey.
coverage measures can be grouped into the two broad classes of controlflow based and dataflow based measures.
the former focus on the flow of control from one statement to another such as the branch coverage mentioned above.
dataflow based measures in contrast focus on the flow of data from definition def statements which assign a value to a variable to statements where that value of the variable is used.
for instance the statement x y is a def for variable x. if that statement is on line of a program and line of the program is if x and it is possible for control to flow from line to line without executing any other defs of x then the statements on line and form a def use pair the dataflow equivalent of a branch of branch coverage.
if line and line are in different conditional blocks then defuse coverage would require that the def free path be executed while branch coverage might not require this.
we say that coverage measure a subsumes coverage measure b if every test suite that achieves a must necessarily achieve b. frankl and weyuker proved subsumption and similar relations among a wide variety of coverage measures including the fact that def use coverage subsumes branch coverage.
ammann and offutt extended this to many other coverage measures .
ball showed that predicate complete test coverage subsumes statement branch multiple condition and predicate coverage .
the use of coverage measures is based on the belief that a test suite that achieves higher coverage or a stronger coverage measure is more effective at exposing faults in the software under test.
some research and real world case studies have borne out this belief.
however a large number of different test coverage measures have been proposed and research is needed to study which are most closely connected with effectiveness.
b. effectiveness and coverage measures if coverage measure a subsumes coverage measure b then a test suite that achieves a coverage also achieves b coverage.
however achieving the stronger coverage measure often involves more effort.
if when applied to actual programs achieving of a does not result in a test suite that is more effective exposes more faults than achieving of b then the effort of achieving of a is not worth it.
hutchins et al.
showed that test suites that achieved higher coverage were more effective and that test suites that achieved high def use coverage were more effective than those that achieved high branch coverage.
to compare effectiveness they hand seeded faults into seven subject programs the siemens programs and measured the effectiveness of a test suite as the percentage of faulty versions that the test suite exposed.
these results raised the possibility that the effort of achieving measures stronger than branch coverage could pay off.
hutchins et al.
s research left open the question of whether the test suites that achieved higher coverage or subsuming coverage measures were more effective because of some intrinsic quality of the coverage measure or simply because the test suites they required were bigger contained more test cases .
siami namin and andrews found that both size of a test suite number of test cases and coverage were good predictors of effectiveness.
they also found that a linear combination of log size and coverage yielded a good numerical prediction of effectiveness.
they did not however compare coverage measures against each other.
in this paper we follow hutchins et al.
s general experimental design using mutation to generate faulty versions automatically.
we then compare mpsc to def use coverage in order to measure how well each predicts the effectiveness of a test suite when test suite size is taken into account.
c. implementation of def use coverage hutchins et al.
s research suggests that def use coverage is useful.
unfortunately the practical implementation of defuse coverage poses several difficulties.
the first is that it is apparently non trivial to build a tool set for correctly recording and reporting def use coverage.
we were able to find only one working compilable tool set that implemented def use coverage for c one for java and none for c .
the c tool atac did not manage to successfully instrument and record coverage for two of the large c utility programs that we used as subjects.
recently yang et al.
reported on the coverage criteria measured by a wide range of commercial tools none of the tools surveyed implemented any form of def use coverage.
although the source code analysis needed for def use coverage instrumentation does not seem to be very complex the above evidence suggests that its complexity isdeceptive and or is high enough to discourage tool vendors from implementing it.
a second difficulty associated with def use coverage is that the needed instrumentation slows down the system under test more than the instrumentation for controlflow based coverage.
for some systems this may make testing infeasible and or introduce timing bugs that do not appear in the uninstrumented system.
santelices and harrold addressed this concern in their study of more efficient ways of implementing def use coverage but concluded that more work was needed to improve the efficiency of def use coverage.
a final difficulty concerns arrays.
when the program under test contains arrays the def use tool implementor must decide whether to consider the whole array as one variable or each array entry as a separate variable.
if the whole array is considered one variable then a def use pair for the variable can be measured as executed even if one entry is assigned in the def and a different entry is used in the use.
if each array entry is considered a separate variable then the number of defuse pairs is greatly multiplied.
in our research test cases for one of the subject programs jtopas when instrumented for def use coverage yielded bytecode files that were too big to be executed on the jvm cutting down the size of an array in the code cured the problem but resulted in tests that did not do the same thing.
d. mpsc in response to the above issues researchers seek coverage measures that are more predictive of test suite quality than branch coverage but not as difficult or expensive to instrument or collect as def use coverage.
the candidate measures that we explore in this paper are the multi point stride coverage or mpsc measures defined above.
we had four main motivations for considering mpsc as a candidate coverage measure.
first it generalizes branch coverage as noted above.
second because mpsc tracks sequences of branches it may capture some of the same information that def use coverage captures.
some def use pairs connect statements in distant branches similar to mpsc tuples with g .
it is easy to show that mpsc with a given value of gandpis incomparable to neither subsumes nor is subsumed by defuse coverage.
however the more practical question is whether when applied to actual programs mpsc is as predictive of high test suite quality as def use coverage is.
we address this question through experimentation which we report on here.
third mpsc is relatively easy to instrument for in widelyused procedural languages like c c and java.
we built a simple source code transformation package called jqxz that searches for if while for andswitch statement constructs and instruments the associated decisions and cases.
we believe that similar simple transformations could be done on other source languages and on bytecode and native code.
fourth we believed that the instrumentation for collecting mpsc data could be made efficient by storing the data in a hash table of appropriate size and using an appropriate hashfunction.
our performance experiments reported on in this paper bore out this belief.
why did we not consider other established measures shown to subsume branch coverage such as condition decision multi condition or modified condition decision coverage mc dc ?
the problem here is the surprising and important results of rajan et al.
who showed that it is possible to subvert the intent of such coverage measures by restructuring programs.
rajan et al.
showed that programmers can restructure a program into one which does the same thing but whose mc dc coverage test suites are not as effective in exposing bugs.
the same analysis holds for all three of the coverage measures mentioned above.
we believe that mpsc will not be as vulnerable to source code transformation as mc dc and related measures because it is an interprocedural coverage measure defined in terms of dynamically executed sequences of branches rather than the static structure of decisions.
studying this belief in more detail however is a subject for future work.
e. antecedents to mpsc we have not been able to find any mentions of measures equivalent to mpsc with g 1andp .
however we do not claim any novelty for mpsc with g .
measures similar to mpsc with g andp 1have appeared in the literature in the past although curiously there has not been much research on them in recent years and we believe that we are the first to investigate such measures empirically.
pimont and rault defined a hierarchy of coverage measures equivalent to mpsc with g .
chow extended their work to coverage of state machine abstractions of programs by test cases in particular switch cover which is similar to mpsc with g andp .
woodward et al.
defined the notion of lcsaj or linear code sequence and jump as a body of code through which the flow of control may proceed sequentially and which is terminated by a jump .
they also defined an infinite hierarchy of coverage criteria ter i for which ter n 2wheren 1is coverage of distinct subpaths of length nlcsajs.
there is no discussion of whether the criterion is meant to be inter or intraprocedural.
over years later woodward and hennell showed that a coverage criterion equivalent to all intraprocedural lcsaj sequences subsumes mc dc under certain assumptions about program structure.
path coverage is usually taken to mean coverage of all possible paths through a program.
even at the intraprocedural level it has long been recognized that the number of such paths can be infinite .
zhu et al.
defined a simple path as one that does not repeat edges while ammann and offutt defined a simple path as one that does not repeat nodes except that the first node can be the last a refinement of zhu et al.
s notion of elementary path .
simple path coverage under either definition does not subsume mpsc since an mpsc tuple can contain a given branch an arbitrary number of times.
however mpsc with g and a high enough p does subsume simple path coverage.finally ntafos defined required k tuples coverage as coverage of all possible linked chains of kdef use pairs.
in such a chain a decision may be followed by another decision that is not the next to be executed as in mpsc tuples with g .
the required k tuples criterion presumably has the same implementation and efficiency problems as def use coverage which is a special case of it.
iv.
b asic properties of mpsc our first research question rq1 above deals with the basic properties of mpsc how many mpsc tuples typically need to be collected for a program and how is that number related to other program metrics?
data structures for collecting mpsc data for instance hash tables can be made more efficient if we can approximate how many entries will be needed.
the upper bound for this number is the measure defined above maxcov s g p for subject program s gapgand number of points p since this is the size of the set of all g p tuples that can possibly be collected for s. because mpsc with g 0orp 1is equivalent to branch coverage an upper bound for maxcov s p for anypcan be determined statically by simply counting the number of branches in the program.
we therefore wanted to see if there is a relationship between maxcov s p and maxcov s g p for generalgandp.
because maxcov s p is the same for anyp andp was the smallest pfor which we collected data we use maxcov s in this paper.
we use maxcovratio s g p to mean the ratio maxcov s g p maxcov s .
we can think of this as the memory multiplication factor needed to collect mpsc coverage at gap gandppoints.
a. subject programs we addressed our research questions and in particular rq1 by collecting data on subject programs in three languages c c and java .
we obtained the subject programs and corresponding test pools from sir the software artifact infrastructure repository except concordance which was converted to a subject program at our university.
there are eleven c three java and one c program used in our research.
we chose these programs as they are used in similar research and accepted as standard.
in table i we give the subject programs names and relevant information collected from sir.
we included the small siemens programs in our research programs because of the thoroughness and size of their pools of test cases.
in table i we also quantify the benefits this thoroughness and size give us.
the column branch cov gives the source branch coverage achieved by the entire test pool of each subject program.
the siemens program test pools are the ones with the highest coverage in addition rothermel et al.
state that the pools cover every feasible branch at least times.
larger test pools allow experimenters to select experimental testsuites that differ more from each other are more diverse .table i list of subject programs .
branch cov branch coverage of whole test pool .
tsd test suite diversity measure .
pepc partial execution pattern constant .
test branch tsd pepc program type language sloc cases cov c concordance text processor c .
.
.
flex lexer generator c .
.
.
gzip data compression c .
.
.
grep string matching c .
.
.
sed stream editor c .
.
.
printtokens lexical analyzer c .
.
.
printtokens2 lexical analyzer c .
.
.
replace pattern matcher c .
.
.
schedule data structure c .
.
.
schedule2 data structure c .
.
.
tcas hardware control c .
.
.
totinfo information measure c .
.
.
jtopas tokenizer library java .
.
.
nanoxml xml parser java .
.
.
xml security security library java .
.
.
larger test suite diversity allows us to take a more representative sample from the space of possible test suites for the program.
to quantify this test suite diversity we define tsd x as the expected fraction of the test cases of one randomlychosen test suite of size xthat would not appear in another randomly chosen test suite of size x. this is equivalent to x n wherenis the size of the test pool.
in table i we give the value of tsd for each program since the largest random test suites that we generated were of size .
again the siemens programs show the highest values of test suite diversity.
despite the lower coverage and or lower test suite diversity of the other subject programs we retained them in our set of subject programs because they are larger and represent realistic situations in which the available test suites achieve lower than maximal feasible coverage.
b. procedure we instrumented the programs using our instrumentation tool jqxz so that they recorded to a disk file the full sequence of branches followed.
we then processed the file to extract all the mpsc tuples executed by each test case forg to10and forp to5.
we chose this range of values based on what we expected would be feasible to record.
we computed the value of maxcov s g p for each subject programsand each value of gandp.
we then visualized the results using graphs and investigated the relationships between g p and maxcov using linear regression.
c. results and analysis figure shows the relationship between g p and maxcov for the subject program replace .
as expected maxcov increases with increasing gandp.
for other programs the lines are straighter or concave down rather than concave up but the relationships between the lines are similar.
using the statistical package r we searched for linear relationships between g p and maxcov .
we find that the 150000subject program siemens replace gap size vs maxcov line graph for and points gmaxcov2 points points points pointsfig.
.
graph of maxcov for given values of gandp for subject program replace following linear model can be used to describe mpsc for a subject program log maxcovratio s g p c log g log p wherecis a constant for the subject program s which we call the partial execution pattern constant .
we find that this model works for all of our subject programs with high accuracy3.
2for the program tcas we omit all points with g .
we do this because there are only a small number of unique paths possible in tcas and each of the paths is short producing anomalous results when g .
3the adjusted r2value a measure of the predictive accuracy of a linear model is greater than .
for all programs and greater than .
for most.
the predicted coefficients have high statistical significance with p for all programs.
log maxcov ratio c log g log p fig.
.
actual vs. predicted values of log maxcovratio for all programs.
interestingly each program has its own partial execution pattern constant c. the value of cfor each program is given in the last column of table i. this constant does not seem closely related to any other program metrics.
we attempted to find a model for cthat worked for all of our subject programs taking into account sloc and number of modules.
sloc and number of modules did not have a statistically significant relationship to c. for instance the smallest programs the siemens programs which also have the highest test pool coverage include the program with the highest value of c and the program with the lowest value of c. in figure we show the actual value of log maxcovratio compared to the value predicted by the model for all values ofgandpand all subject programs where each program s predicted value is based on its own partial execution pattern constantc.
we can simplify equation to predict the value of maxcov as follows maxcov s g p maxcov s p c log g using the above equations we can estimate the number of mpsc tuples that typically need to be collected for a program.
as noted in section ii maxcov s is equivalent to the number of branches in the program which can be determined by static analysis.
to estimate c we can use the highest value ofcobserved so far for any program which is .
.
a hypothetical mpsc tool could obtain an upper bound for maxcov s using source code analysis and then use equation to find an upper bound for c. v. mpsc and def use research questions rq2 and rq3 have to do with the relationship between mpsc and def use coverage.
rq2 askswhether mpsc with some value of gandpis similar to defuse in order to answer the direct question of whether mpsc can be used in place of def use.
rq3 asks a question which is more relevant to software engineers considering the use of coverage criteria whether mpsc is as predictive as def use of the quality of a test suite.
a. data collection we instrumented as many of our subject programs as possible with tools that measured def use coverage.
we used atac to instrument the c programs all seven of the siemens programs and two of the large unix utilities flex andgrep could be instrumented successfully.
atac could not successfully instrument the c program concordance or the remaining two unix utilities gzip andsed .
we modified atac slightly in order to print unique identifiers for each of the def use pairs covered by the program.
we obtained santelices tool duaf and instrumented our three java subject programs with it.
we then ran each test case on each of the instrumented programs and recorded which def use pairs were covered by which test cases this supplemented the information we collected earlier see above on which mpsc tuples were covered by which test cases.
we were not able to run of the test cases for jtopas due to the problem with arrays and duaf noted above.
for measuring effectiveness we generated a full set of mutant descriptions for each of the source programs using andrews tool mutgen .
we then randomly selected mutant descriptions generated the mutant program compiled it and ran the full suite of test cases on the mutated program until we obtained non equivalent mutants of each subject program4.
we recorded which test case killed which mutant.
finally we generated randomly chosen test suites for each size of test suite from to test cases for a total of test cases per subject program.
using the information about which test case killed which mutant we computed the effectiveness of each test suite as the percentage of mutants killed by the test suite.
we also computed the cumulative mpsc for each collected gandp and def use coverage of each test suite based on the sets of mpsc tuples and def use pairs covered by each test case.
b. relationship between the criteria in figure we show a scatter plot of def use vs mpsc for the subject program xml security where gap size g and number of points p equivalent to branch coverage each point in the plot represents one test case.
4an equivalent mutant is one which is not killed detected by any test case.
equivalence of mutants is undecidable in theory and often difficult in practice.
here we follow standard practice and approximate by considering every mutant not killed by any test case in the test pool as equivalent.
5in all scatter plots the straight line shows the best linear fit and the curved line shows the lowess locally weighted best fit smoothing line.
we use number of def use pairs and or mpsc tuples covered rather than percentages because we are interested in relationships which do not depend on the scales of the axes.
6000subject program apache xml security du vs mpsc for gap size and number of points mpscdufig.
.
mpsc vs. du for g andp subject program xml security .
6000subject program apache xml security du vs mpsc for gap size and number of points mpscdu fig.
.
mpsc vs. du for g andp subject program xml security .
other programs showed the same strongly linear relationship.
we measured the pearson correlation of branch coverage and def use coverage for all programs finding that it was greater than .
for all programs except jtopas where it was .
.
this result indicates that simple branch coverage is very strongly correlated with def use coverage.
we discuss the implications of this in section vii.
as we increase the value of gandp the linear relationship between mpsc and def use coverage deteriorates.
figure shows a similar scatter plot to figure but for g and p .
we note that mpsc distinguishes between test suites 80subject program nanoxml number of mutants detected vs mpsc scatter plot nop5 gap10 mpscno of killed mutantsfig.
.
mpsc vs. number of mutants detected for g andp subject program nanoxml .
table ii accuracy of effectiveness models .
n s not statistically significant .
n a not available due to atac limitations .
branch best best du program adjr2 g p adjr2adjr2 concordance .
.
n a flex .
.
.
gzip n s .
n a grep .
.
.
sed .
.
n a printtokens .
.
.
printtokens2 .
.
.
replace .
.
.
schedule .
.
.
schedule2 .
.
.
tcas .
.
.
totinfo .
.
.
jtopas .
.
.
nanoxml .
.
.
xml security .
.
.
assigns different coverage values more than def use does at these levels.
c. predicting effectiveness we first visualized the effectiveness of the test suites that we ran for each program.
figure shows a typical such visualization each point is one test suite the x axis shows cumulative number of mpsc tuples covered for g and p and the y axis shows number of mutants killed.
siami namin and andrews found that both size and coverage contributed to an accurate prediction of test suite effectiveness.
we therefore refine rq3 to the following does using the size of a test suite and mpsc lead to a more accurate model of test suite effectiveness than using the size of the test suite and def use coverage?
this formulation of the question accounts for the confounding factor of size in test suite effectiveness.
when a test case is added to a test suite it cannot decrease the suite s effectiveness on average it increases it by a given nonzero amount.
the refined rq3 essentially factors out this coverageneutral amount of added effectiveness allowing us to measure how much additional value we get out of increases in coverage.
to answer the refined question we used r to fit models of the form am c1 c2 log size c3 coverage to our test suite data where am adequacy on mutants is the percentage of mutants killed by the test suite size is the number of test cases and coverage is the coverage of the test suite in mpsc tuples or def use pairs.
we did this for each value ofgandpand for def use.
we measured the accuracy of the resulting models by the adjusted r2measure.
the results are summarized in table ii.
column gives the setting ofgandpthat resulted in the most accurate linear model for mpsc.
columns and show the adjusted r2 value of the models from branch coverage g p from the best setting and from def use coverage.
the boldface number in each row is the adjusted r2of the most accurate model.
we consider only cases in which both factors had a statistically significant effect on effectiveness p .
for gzip in the branch coverage case log size did not have a statistically significant effect on effectiveness.
for out of the subjects the most accurate model was the one yielded by branch coverage for others it was the one yielded by some other setting of mpsc and for the remaining it was the one yielded by def use coverage.
however for out of the subject programs including the two for which def use was the most accurate the accuracy of the models resulting from branch coverage from the best setting of mpsc and from du coverage were all within .
of each other indicating that there was little practical difference among the criteria.
for of the subjects the accuracy of all models ranked as high or very high on the standard guilford scale .
for one subject totinfo the accuracy of all models ranked as low and for the other three subjects the accuracy of all models ranked as low or moderate between .
and .
.
this indicates that for some subjects there were confounding factors other than test suite size and coverage that affected test suite effectiveness.
vi.
i mplementation and performance the jqxz utility instruments source code to make calls to a library which collects coverage information.
the implementations of the library used for the experiments above simply recorded each branch executed in a file.
we also developed two other prototype implementations for java a hash set implementation and a bitset implementation which were intended to be closer to implementations that could be used in production environments.in this section we describe these implementations and give the results of performance experiments we ran to measure the overhead of mpsc collection and compare it to the overhead of def use coverage.
a. prototype implementations both prototype implementations of the java library depended on a hash function for mpsc tuples.
we implemented several hash functions but we found in exploratory performance experiments that the best used a simple algorithm of the same form as that of the java.lang.string hash function.
the implementations maintain a circular buffer representing the lastg p branches executed.
the branches are identified by source file line and character number and for decisions whether true or false.
each branch causes an element to be added to the circular buffer and the newly completed mpsc tuple to be recorded as covered.
the hash set implementation stores each covered tuple in a hash set.
the initial size of the hash set can be computed based on the maximum expected number of tuples needed as outlined in section iv c. the advantage of this implementation is that the exact set of tuples is recorded.
the disadvantage compared to the bitset implementation is that a larger amount of storage is needed.
the bitset implementation computes the hash code hof each covered tuple and stores it by setting the h th bit in a java.util.bitset to .
the advantage of this implementation over the hash set is that much less storage is needed.
the disadvantages are that it is impossible to extract which tuples have been covered and that two or more tuples may hash to the same value losing information about precisely how many tuples have been covered.
this implementation might still be useful for instance for automated test input generation schemes which only have to compute whether one test case has covered some line of code not covered earlier.
b. performance to measure the performance of our implementations and compare it to that of def use we chose our three java subject programs and added two programs used by santelices and harrold tcas a java translation of the siemens program and scimark2 a jvm performance benchmark .
we ran the test suite for each program on the uninstrumented program the program instrumented for def use coverage by the state of the art tool duaf developed by santelices and the program instrumented for mpsc by our tool.
we were not able to successfully instrument scimark2 for du coverage with the current version of duaf.
for our tool we ran the program for every setting of gfrom to and pfrom to .
we ran each program times except for scimark2 which we ran times because it uses a random number generator to generate some of its test data.
we measured cpu time by the user time reported by the unix time facility in bash .
we then calculated the average time to run one test case or for scimark2 the average timetable iii performance data .
uninst du overhead mpsc overhead subject cpu sec sh ha hashset bitset jtopas .
.
.
nanoxml .
.
.
.
xml sec .
.
.
.
tcas .
.
.
.
.
scimark2 .
.
.
.
to run the program as a whole.
we averaged the results for all values ofgandpto get a summary value for mpsc.
table iii shows the results.
results are reported for the uninstrumented program in cpu seconds and for the other columns in percentage overhead extra time needed for the instrumentation.
the experiments were run on a sun ultrasparc iiii with a .593ghz processor and 4gb of memory.
for def use coverage we give the overhead reported in where it is available column sh and also the overhead we calculated column ha since cpus and jvms can vary.
table iii shows that the overhead for our prototype bitset implementation was always less than that of duaf and that the overhead for our prototype hash set implementation was less than that of duaf for every program except nanoxml .
the overhead for duaf was greater than both the average overhead for mpsc across all gandp and the average overhead for any individual gandp except in the case of nanoxml using the hash set implementation.
in the case of scimark2 paradoxically not only did the mpsc instrumentation have very little effect on average the software instrumented with the bitset implementation took less time than the uninstrumented version.
we attribute this to random noise resulting from choice of random seed by different runs and possibly to operating system effects due to amount of memory allocated for a process.
the average time varied with different values of gandp but not in any clear pattern.
this was probably due to different rates of hash collisions.
c. accuracy of bitset implementation as mentioned above the bitset implementation may lose information about how many tuples have been covered due to hash collisions.
we therefore studied the question of how much information was lost.
for each subject program each setting of gandp and each test case we collected the number of mpsc tuples reported as covered by the hash set and the bitset implementation.
for each subject program we then calculated the percentage decrease in number of tuples reported as covered as an average across all test cases and all settings of gandp.
we found that the average loss of accuracy ranged from .
to .
for all programs or less than out of tuples lost due to hash collisions.
this result indicates that the bitset implementation could be useful in situations where high accuracy is not needed.vii.
d iscussion a. threats to validity and their mitigation internal validity we checked and visualized data and the results of statistical analyses in various ways.
we collected data forg and several values of pto confirm that they were the same in order to increase our confidence in our experimental procedures.
the use of many randomly chosen test suites was intended to increase internal validity.
construct validity mutation adequacy of randomly constructed test suites has been shown to be a good measure of effectiveness but other measures are possible.
for subject programs with low coverage test suites we may have judged some mutants to be equivalent which were not however as noted below our results were consistent across subjects.
external validity our results do not extend to other variants of mpsc such as intraprocedural variants or variants based on statement block or condition coverage.
we hope to study such variants in the future.
the subjects that we used may not be representative of real world software.
our subjects have complementary strengths and weaknesses the small siemens programs have large thorough test pools and the programs with smaller less thorough test pools are larger in sloc.
we are not aware of any subject programs that are larger than the unix utilities we used here and also have test pools approaching the size and thoroughness of the siemens programs.
our results are consistent across program sizes and test pool sizes and across three related but distinct programming languages.
b. def use and branch coverage our most unexpected result was the high correlation of defuse coverage with branch coverage and the lack of benefit of def use coverage over branch coverage for predicting test suite effectiveness.
the hutchins et al.
study had led us to expect that def use would distinguish between test suites more than branch coverage and that def use would be a better predictor of test suite effectiveness.
our results however show that when test suite size is taken into account def use coverage performs very similarly to branch coverage.
these results are consistent with those reported in .
together they may indicate that the improved effectiveness of high def use coverage test suites in the hutchins et al.
study is an artefact of the fact that high def use coverage test suites need to be bigger than high branch coverage test suites.
we should note that these results do not extend to other problems in software testing.
santelices et al.
for instance showed that du coverage and combinations of du coverage with other forms of coverage yielded more accurate fault localization than branch coverage .
more experimentation would be needed to show whether mpsc coverage can replace du coverage for fault localization.
c. mpsc and branch coverage we found that branch coverage was often the most accurate predictor of test suite effectiveness when combined with test suite size.
however for some subject programs mpsc withsome value of g 0andp 1was a better predictor.
we also found that our instrumentation for mpsc was efficient even withg 0andp .
these results taken together suggest that users seeking a coverage criterion stronger than decision coverage can use mpsc with g and highp.
since this form of coverage subsumes decision coverage is usually more efficient to collect than def use coverage and sometimes yields a better prediction of test suite effectiveness than def use coverage it may present a better option than def use coverage.
viii.
c onclusion and future work in this paper we defined and presented the results of empirical studies on multi point stride coverage mpsc a form of coverage that subsumes branch coverage.
we compared it to def use coverage a well known dataflow coverage criterion.
we found that the maximum number of mpsc tuples that need to be collected is highly predictable given a constant which is characteristic of a program.
we found that def use coverage is strongly correlated with branch coverage and often does not yield a better prediction of test suite effectiveness than branch coverage when test suite size is taken into account.
we also found that mpsc with g 0andp 1sometimes yielded a better prediction of effectiveness than branch coverage.
finally we developed prototype implementations of mpsc data collection libraries that performed well compared to a state of the art def use coverage tool.
future work includes improving the efficiency of our prototype implementations of mpsc and exploring variants of mpsc and applications to other problems in software testing such as fault localization and test suite prioritization.
acknowledgment thank you to gregg rothermel and the university of nebraska lincoln for the sir repository the source of our subject programs.
many thanks to ra ul santelices for sharing and discussing his java du coverage tools and to michael ernst for his comments on an earlier draft.
this research is supported by nserc and by an ontario government ogs scholarship to mohammad mahdi hassan.