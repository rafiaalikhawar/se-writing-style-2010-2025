a performance comparison of contemporary algorithmic approaches for automated analysis operations on feature models richard pohl kim lauenroth and klaus pohl paluno the ruhr institute for software technology university of duisburg essen essen germany richard.pohl kim.lauenroth klaus.pohl paluno.uni due.de abstract the formalization of variability models e.g.
feature models is a prerequisite for the automated analysis of these models.
the efficient execution of the analysis operations depends on the selection of well suited solver implementations.
regarding feature models on the one hand the formalization with boolean expressions enables the use of sat or bdd solvers.
on the other hand feature models can be transformed into a constraint satisfaction problem csp in order to use csp solvers for validation.
this paper presents a performance compari son regarding nine contemporary high performance solvers three for each base problem structure bdd csp and sat .
four operations on feature models are run on each solver.
the results will in turn clear the way for new improvements regarding the automatic verification of software product lines since the efficient execution of analysis operations is essential to such automatic verification approaches.
keywords software product line feature model automated reasoning techniques performance measurement i. i ntroduction software product line spl engineering empowers companies to develop individualized software systems in a more cost efficient way with a shorter time to market and at a higher quality.
the variety of different products developed in a spl must already be considered in the development process.
to achieve this variability modeling techniques are central element of product line engineering.
a. feature models and automated analysis feature models are a common technique for modeling commonality and variability in spls.
the basic concepts of feature models are mandatory and optional features groups of child features and constraints.
efficient analysis operations on feature models are necessary because verification and validation techniques for software product lines often perform analysis operations directly on feature models.
for example one operation is to check a feature model for validity i.e.
to check whether a product can be derived from it.
another example is given a set of feature models to determine whether the feature models are semantically equivalent i.e.
whether they represent the same set of products.
these analysis operations can reveal problems with models that may not be detected during a manual review because of the high complexity of feature models.
one real example for a complex feature model is the web shop model from with features .
products can be derived from this model.
different techniques for the automated analysis of feature models have been developed during the last years .
as described in feature models can be represented by boolean expressions to perform automated reasoning.
bdd binary decision diagram and sat satisfiability solvers can be used to analyze boolean representations.
a bdd is a representation of a boolean function as a directed acyclic graph that is mainly used to perform analysis operations on it.
bdd solvers are tools to perform such analysis operations automatically.
a sat represents the question whether a boolean expression is satisfiable i.e.
whether some assignment of values to the variables of a boolean formula exists such that the formula evaluates to true.
sat solvers are tools that compute a possible solution to a sat.
an alternative approach is given in where a feature model is transformed into a csp constraint satisfaction problem .
a csp in contrary to a sat includes constraints containing integer and interval values.
a csp solver is a tool that computes a possible solution to a csp.
b. problem statement performing typical analysis operations e.g.
verifying variability models according to formalizations as described in often leads to a high number of calls to solver operations in practice.
minimizing solver calls in product line analy sis is an open research challenge .
this in addition to the potentially high complexity of the feature model itself requires the solver operations to be performed as efficiently as possible.
the state of the art in feature model verification presents three types of solvers bdd solvers csp solvers and sat solvers.
nevertheless the problems bdd csp and sat are known as np hard and therefore may have an exponential worst case runtime cf.
.
however depending on the structure of the actual problem efficient heuristics to solve these problems exist in practice.
due to the variety of implementations used to solve the different problems the following questions arise.
first which approach is generally more promising with regard to efficiency?
second which approach is suitable in certain defined situations for example when deal .
c ieee ase lawrence ks usa313 ing with extraordinarily small or large feature models or when running specific analysis operations?
according to the problem described we define the following two research questions rq1 is there a significant difference between the three approaches to transform and solve feature models bddbased csp based and sat based ?
rq2 are there specific situations in which one approach is faster with regard to performance than the other approaches?
if yes which approach is the fastest in which situation?
c. overview of this paper in order to answer the presented questions this paper presents an experiment nine contemporary high performance solvers of different types three bdd three csp and three sat solvers were used to perform four different operations on feature models of real systems.
our central finding is that there is a significant influence of the choice of the solver but none of the solver types regarded is significantly better than the others with regard to automated analysis operations on feature models in general.
nevertheless we identified conditions under which a certain type of solver is best suited.
in section ii we present the current state of the art and explain how our contribution is related to existing work.
in section iii we describe the plan for our experiment to evaluate the performance of different contemporary solvers.
in section iv we describe the preparation and execution of the experiment.
section v presents the results of the experiment.
in section vi the results are interpreted with regard to the expectations.
section vii contains the conclusion of our work and an outlook on further studies.
ii.
s tate of the art this section gives an overview of the state of the art regarding research on performance analysis of automated reasoning techniques using the boolean and csp formalization in general and specifically in conjunction with feature models.
in addition to this the fundamental concepts and definitions of feature model formalization are presented.
a. related work on the efficiency of problem solving the development of efficient heuristics to solve boolean expressions has been a topic in research since the s. a variety of highly efficient heuristics exists.
a contemporary state of the art analysis for general formal verification techniques also including techniques that use bdd csp and sat solvers is given in .
a more exhaustive analysis only regarding sat solvers with an empirical evaluation is presented in .
beyond this there are well known regular competitions like the sat competition for sat solvers or the cpai competition for csp solvers.
the problem of all studies regarding the efficiency of problem solvers in general is that heuristics used to solve np hard problems are highly dependent on the problem structure and that only a subset of all possible problem structures can be regarded.
for example the sat competition regards random problems deliberately hard problems e.g.
n queens problem and industrial problems e.g.
digital circuit verification .
the results of these studies are valuable for general performance studies and aim at identifying the practical limits of the considered solver technology.
b. related work on feature model analysis a comparison between java based csp bdd and sat solvers was already presented by benavides et al.
in .
their comparison focused on the operations derive one product from a feature model and number of products .
it included the javabdd bdd solver the jacop csp solver and the sat4j sat solver.
the choice of these relatively simple operations made it possible to run their tests on feature models with up to features.
their feature models were randomgenerated.
their findings on the runtime of the number of products operation were similar to ours as the bdd solver shows a lower runtime increase on larger models.
their tests include a memory usage analysis that showed an exponential growth in memory usage for the bdd solver on larger models.
in a performance analysis of a csp solver is performed based on the all products operation.
the test was run with models containing up to features and the results were generally good but with a clearly visible exponential growth especially with highly variable models.
an extension to the fama framework a framework for the automated analysis of feature models with performance measurement tools is provided in .
with regard to performance testing their approach aims to reveal tool specific deficiencies by the construction of specific input models.
the extension includes test data generators that support the evalua tion of analysis tools for feature models as well as tools to generate feature models that are deliberately hard to solve.
regarding the existing work there are two remaining unsolved issues.
first there is no possibility to detect whether performance issues detected on generated feature models are significant for the modeling of real systems.
this requires a study on feature models representing real systems.
the second aspect is to provide an analysis of solver performance independent of implementation aspects like the programming language and platform since all operations run on the fama framework are based on java implementations.
our contribution resolves both issues.
c. fundamentals of feature model formalization this section provides a short overview of the fundamental concepts of boolean formalization of feature models and the mapping of feature models to csp.
moreover notations and formal concepts used in the following sections are defined here.
usually feature models are trees with a root feature that is decomposed into subfeatures.
there are different notations for feature models.
we use the notation from known as feature diagram which is a common notation for feature models.
there are several possible relationships between a feature and its subfeatures and between a feature and a set of subfeatures mandatory feature subfeatures are required optional feature subfeatures are optional alternative group only one of the subfeatures in the group may be selected or group one or more of the subfeatures in the group can be selected.
a cardinality constraint may be given to re 314strict the minimum and maximum number of selectable features in an or group.
further relationships between features in feature models mentioned in are requires if a feature requires another feature.
thereby the inclusion of the feature implies the inclusion of the required feature in the product.
excludes if a feature excludes another feature.
thereby the two features cannot be part of the same product.
fig.
contains a small example of a feature diagram representing a simplified car entertainment system and explains the graphical notation of feature diagrams.
the relationships between features impose certain constraints on the products derived from the feature model.
in addition to this proposes the specification of more generic constraints on feature models.
d. boolean representation of feature models the boolean representation of feature models used in this paper is based on .
as an example the boolean formalization of the car entertainment system from fig.
is given below g4666 g1830 g1861 g1871 g1868 g1864 g1853 g1877 g3640 g1829 g1853 g1870 g1831 g1866 g1872 g1857 g1870 g1872 g1853 g1861 g1866 g1865 g1857 g1866 g1872 g1845 g1877 g1871 g1872 g1857 g1865 g4667 g3 g3 g1512 g4666 g1845 g1868 g1857 g1853 g1863 g1857 g1870 g1871 g1374 g1829 g1853 g1870 g1831 g1866 g1872 g1857 g1870 g1872 g1853 g1861 g1866 g1865 g1857 g1866 g1872 g1845 g1877 g1871 g1872 g1857 g1865 g4667 g3 g3 g1512 g4666 g1842 g1860 g1867 g1866 g1857 g1829 g1867 g1866 g1866 g1857 g1855 g1872 g1867 g1870 g1372 g1829 g1853 g1870 g1831 g1866 g1872 g1857 g1870 g1872 g1853 g1861 g1866 g1865 g1857 g1866 g1872 g1845 g1877 g1871 g1872 g1857 g1865 g4667 g3 g3 g1512 g4666 g1839 g1857 g1856 g1861 g1853 g1842 g1864 g1853 g1877 g1857 g1870 g1372 g1829 g1853 g1870 g1831 g1866 g1872 g1857 g1870 g1872 g1853 g1861 g1866 g1865 g1857 g1866 g1872 g1845 g1877 g1871 g1872 g1857 g1865 g4667 g3 g3 g1512 g4666 g1830 g1861 g1871 g1868 g1864 g1853 g1877 g1374 g1828 g1864 g1853 g1855 g1863 g1849 g1860 g1861 g1872 g1857 g1838 g1829 g1830 g3 g27 g18 g21 g3 g1829 g1867 g1864 g1867 g1873 g1870 g1838 g1829 g1830 g4667 g3 g3 g1512 g4666 g1839 g1857 g1856 g1861 g1853 g1842 g1864 g1853 g1877 g1857 g1870 g1374 g1829 g1830 g1842 g1864 g1853 g1877 g1857 g1870 g1513 g1830 g1848 g1830 g1842 g1864 g1853 g1877 g1857 g1870 g1513 g1839 g1842 g885 g1842 g1864 g1853 g1877 g1857 g1870 g4667 g3 g3 g1512 g4666 g1830 g1848 g1830 g1842 g1864 g1853 g1877 g1857 g1870 g1372 g1829 g1867 g1864 g1867 g1873 g1870 g1838 g1829 g1830 g4667 g3 g3 g1512 g3411 g4666 g1830 g1848 g1830 g1842 g1864 g1853 g1877 g1857 g1870 g1512 g1842 g1860 g1867 g1866 g1857 g1829 g1867 g1866 g1866 g1857 g1855 g1872 g1867 g1870 g4667 g3 e. mapping of a feature model to a csp benavides et al.
have established a representation of a feature model as a constraint satisfaction problem csp .
for the definition of the transformation of a feature model into a csp see .
for the algorithm to transform a feature model into a csp see .
as an example the csp formalization for the car entertainment system from fig.
is given below g1829 g1853 g1870 g1831 g1866 g1872 g1857 g1870 g1872 g1853 g1861 g1866 g1865 g1857 g1866 g1872 g1845 g1877 g1871 g1872 g1857 g1865 g3404 g1830 g1861 g1871 g1868 g1864 g1853 g1877 g3 g1512 g1829 g1853 g1870 g1831 g1866 g1872 g1857 g1870 g1872 g1853 g1861 g1866 g1865 g1857 g1866 g1872 g1845 g1877 g1871 g1872 g1857 g1865 g3404 g1845 g1868 g1857 g1853 g1863 g1857 g1870 g1871 g3 g1512 g4666 g1829 g1853 g1870 g1831 g1866 g1872 g1857 g1870 g1872 g1853 g1861 g1866 g1865 g1857 g1866 g1872 g1845 g1877 g1871 g1872 g1857 g1865 g3404 g882 g1436 g1842 g1860 g1867 g1866 g1857 g1829 g1867 g1866 g1866 g1857 g1855 g1872 g1867 g1870 g3404 g882 g4667 g3 g1512 g4666 g1829 g1853 g1870 g1831 g1866 g1872 g1857 g1870 g1872 g1853 g1861 g1866 g1865 g1857 g1866 g1872 g1845 g1877 g1871 g1872 g1857 g1865 g3404 g882 g1436 g1839 g1857 g1856 g1861 g1853 g1842 g1864 g1853 g1877 g1857 g1870 g3404 g882 g4667 g3 g1512 g4666 g1830 g1861 g1871 g1868 g1864 g1853 g1877 g3404 g882 g1438 g1828 g1864 g1853 g1855 g1863 g1849 g1860 g1861 g1872 g1857 g1838 g1829 g1830 g3397 g1829 g1867 g1864 g1867 g1873 g1870 g1838 g1829 g1830 g3404 g882 g4667 g3 g1512 g4666 g1830 g1861 g1871 g1868 g1864 g1853 g1877 g3408 g882 g3643 g1828 g1864 g1853 g1855 g1863 g1849 g1860 g1861 g1872 g1857 g1838 g1829 g1830 g3397 g1829 g1867 g1864 g1867 g1873 g1870 g1838 g1829 g1830 g3404 g883 g4667 g3 g1512 g4666 g1839 g1857 g1856 g1861 g1853 g1842 g1864 g1853 g1877 g1857 g1870 g3404 g882 g3 g1438 g1829 g1830 g1842 g1864 g1853 g1877 g1857 g1870 g3397 g1830 g1848 g1830 g1842 g1864 g1853 g1877 g1857 g1870 g3397 g1839 g1842 g885 g1842 g1864 g1853 g1877 g1857 g1870 g3404 g882 g4667 g3 g1512 g4666 g1830 g1848 g1830 g1842 g1864 g1853 g1877 g1857 g1870 g3408 g882 g1436 g1829 g1867 g1864 g1867 g1873 g1870 g1838 g1829 g1830 g3408 g882 g4667 g3 g1512 g4666 g1830 g1848 g1830 g1842 g1864 g1853 g1877 g1857 g1870 g3408 g882 g1436 g1842 g1860 g1867 g1866 g1857 g1829 g1867 g1866 g1866 g1857 g1855 g1872 g1867 g1870 g3404 g882 g4667 g484 f. analysis operations on feature models among others the following operations can be performed to analyze feature models number of products cardinal determines how many valid products can be derived from a feature model.
the number of potential products for a feature model m is also called cardinal m .
for the example from fig.
this value is .
the variability specified in a feature model defines the number of products which can be derived from a product line.
at one extreme only one product can be derived.
this is the feature model with the lowest variability.
at the other extreme regarding a specific feature model m the theoretically highest variability would be given by a feature model m v which contains only the leaf features of m and no relationships requires excludes alternative group constraints etc.
among them.
based on this the variability factor is defined g1848 g1832 g4666 g1839 g4667 g3404 g1855 g1853 g1870 g1856 g1861 g1866 g1853 g1864 g4666 g1839 g4667 g1855 g1853 g1870 g1856 g1861 g1866 g1853 g1864 g4666 g1839 g3023 g4667 all products determines all products that can be derived from a feature model.
valid a feature model m is valid if there are valid products that can be derived from it i.e.
cardinal m .
a feature model containing two mandatory features f1 and f2 where f1 excludes f2 is an example for an invalid or void feature model.
more advanced operations include additional input parameters e.g.
other feature models products or product configurations see .
iii.
e xperiment planning in this section we describe the performance comparison between nine implementations regarding basic operations on feature models.
the comparison is planned executed and analyzed as an experiment following the guidelines from .
a. selection of example models the intended test set includes feature models from splot a generally accepted source for example feature models that is used extensively for research work.
at the time of our study splot contained models of them were used for our test.
a pre test was run with all models to check the compatibility with the solvers.
some models had to be excluded from the test because the valid operation was cancelled on at least one of the solvers after three hours.
in summary mod els were selected for the test including the web shop model the largest feature model available from splot feafigure .
feature diagram example and notation 315tures cardinal .
.
the valid operation was run on all models the other operations were performed on subsets on which a termination within three hours could be expected on most of the solvers.
b. selection of operations during the test four operations were performed on feature models including the number of products operation the variability operation the all products operation and the valid operation.
these four operations are basic operations that represent different categories of operations with different degrees of complexity.
the valid operation is a simple satisfiability check on a feature model.
performing the number of products operation products are counted i.e.
satisfying variable assignments to the formalization of a feature model.
the variability operation additionally considers relations among features because different types of features leaf features and inner features have to be considered to calculate the result.
all products is an operation that computes all valid products.
this operation is a more complex operation which tends to have longer runtimes and even becomes infeasible on larger feature models .
c. selection of solvers solvers representing the current state of the art were selected for the test.
as a first step three solvers used in the fama framework were selected for the experiment.
fama is a contemporary framework for the automated analysis of feature models.
the fama framework uses the sat4j sat solver the choco csp library and the javabdd bdd library .
the other two csp solvers selected for the test are cream and mistral .
mistral won in of categories in the csp competition a well recognized competition for csp solvers.
regarding sat solvers there is a huge variety of solvers.
a well recognized competition for sat solving is the sat competition that takes place every two years.
two different solvers were selected from the competitions in and that won gold medals in two different categories picosat and clasp .
picosat is a highly optimized solver that won a medal in solving industrial problems it won its medal in the category of random sat instances.
for bdd solving we included buddy and cudd two bdd libraries that are widely used in many research works and deal with performance analysis on bdds.
shortly summarized we selected three bdd three csp and three sat solvers.
from these nine solvers four are written in java sat4j choco javabdd and cream.
the other five solvers are written in c or c .
influences of the programming language are considered in the analysis of the experiment.
d. experiment design the dependent variable of our experiment is the runtime of the solver.
the independent variables are the solver used in terms of 1a the solver problem structure i.e.
bdd csp sat and 1b the solver platform i.e.
native or java the operation run on the solver where we always regarded solvers or operations in combination with the solvers used and the cardinal value of the model as a measure for model size.
the effects of independent variables are determined by separating the runtime measurements into groups e.g.
runs from bdd csp or sat solvers etc.
to determine the effects of the model size on the runtimes the models were divided into three groups resulting in a proportional experiment design.
models with cardinal up to form the small group models with cardinal form the large group.
all models in between form the medium group.
at this point a prerequisite for this approach was to assume that cardinal is a representative measure for the size of feature models with large models being models that tend to have higher runtimes during analysis operations.
to confirm this we aim at rejecting the hypotheses that high runtimes are not correlated to high cardinal values.
in summary we define the null hypotheses h a. the mean runtimes will be equally distributed among the tested solvers.
b. the mean runtimes will be equally distributed among the different tested solver problem structures i.e.
bdd csp and sat .
c. the mean runtimes will be equally distributed among the different tested solver platforms i.e.
native java .
d. there will be no interaction between solver and operation performed.
e. there will be no significant correlation between cardinal and the runtime of a model.
f. there will be no interaction between solver by operation performed and model size.
our experiment aims at rejecting h0 which helps to resolve the defined research questions.
iv.
e xperiment operation this section provides an overview of the actions taken to prepare the environment and to run the experiment.
the experiment was run on an intel core2duo machine with a suse linux bit operating system and .00gb of ram.
for the java based solvers the java virtual machine was used with an initial heap size of .
gb and a maximum heap size of .
gb.
each solver performed one operation and was then restarted to guarantee equal conditions for each operation.
moreover the solver was changed after each run to equalize technical influences that may occur during a short period of time.
five runs for each operation were performed on each model to minimize random deviations from the system.
a. preparation of solvers to run a specific operation on a solver the feature models and the operations need to be mapped to the solver specific problem structures that are bdd csp or sat.
we provide a straight ahead implementation of the known mappings from current research work.
our implementation transforms the feature model into the solver problem input language and calls the solver to compute the desired solution.
moreover it measures the time between the beginning of the transformation into the solver specific problem and the termination of the solver operation.
all native code was compiled using the gnu c c compilers with the highest optimization level o3 .
b. input format we used the splot xml format as input for our test.
for the native implementations we developed a parser for this format in c. the java implementations were used in connection with the java parser provided by splot.
all sat solvers 316are compatible with a special exchange format for boolean expressions in conjunctive normal form cnf the dimacs cnf format.
therefore this format extended with the information of the number of leaf features needed for the variability operation was used as input for the sat solvers.
a cnf was generated from each model using a java implementation based on the java parser provided by splot.
the performance of our java implementation was very low compared to the runtimes of the native c solvers.
our expectation was that an optimized native implementation would show much better runtimes.
for this reason adding these times to the results of the operations would destroy the solver specific distributions for these solvers.
the time for generating a cnf varied be tween .
ms and ms increasing on larger models.
there is a positive correlation of .
between the number of features and the conversion time and of .
between cardinal and the conversion time both are significant at the significance level.
this is an indication of a linear increase of this time with larger models.
v. d ata collection and validation all solvers were prepared to output a summary of the operations performed to the standard output which was then redirected to a file.
the summary includes the solver called the operation performed the measured runtime and the operation result.
in the case of the all products operation we did not collect the output of all product lists instead we checked the results by comparing the size of the all products list with the result from the number of products operation.
vi.
d ata analysis in this section the results from the experiment are briefly described and then analyzed.
table i presents the descriptive statistics of the results.
a. results from the valid operation the overall maximum runtime value is at about .
s with the web shop feature model run on the javabdd bdd solver.
the sat solvers picosat and clasp were the fastest solvers for all models except for an example with only one valid product where the mistral csp solver was slightly faster than the two native sat solvers.
all solvers produced the highest runtimes on the web shop feature model.
the native csp and sat solvers as well as the cream java csp solver had quite constant runtimes the standard deviations were .
or lower with a slight runtime increase on high cardinal models.
the other java based csp and sat solvers showed a relative ly low increase between small and large models that was less than .
.
all bdd solvers show a high deviation in runtime compare table i .
in addition four noticeable peak models were observed on which the bdd solvers show long runtimes.
one of them is the web shop model.
b. results from the number of products operation fig.
shows the average runtimes of the number of products operation.
there are two models with a cardinal value far above the regular range.
these models are excluded from the figure to improve readability.
on small models with cardinal g148 the clasp sat solver is the fastest solver.
on these models the native sat and csp solvers generally produce fast runtimes up to .
ms. this outperforms the java based sat and csp solvers.
on to cardinal models clasp and the buddy bdd solver have approximately equal runtimes in the interval .
on larger models buddy becomes the fastest solver on all models except for the web shop model where cudd performs better.
generally all bdd solvers are faster than other solvers on models with a cardinal value of at least .
for csp and sat solvers at a certain threshold the runtime begins to increase up to a point where termination cannot be achieved within reasonable time.
runs were cancelled after three hours.
on picosat the models with cardinal had to be cancelled.
models with cardinal could not be run on sat4j.
models with cardinal were cancelled on all except the bdd solvers which terminated on all models within .
s. some noticeable runtime peaks up to seconds appear on the bdd solvers with no obvious relation to cardinal .
c. results from the variability operation the variability operation was run on a set of models.
the operation is quite similar to the number of products operation and all solvers show a similar runtime distribution.
because of the number of only three results all from the sat4j solver observed in on the number of products operation runs longer than seconds were cancelled on the variability operation.
these were all runs with the highest cardinal models on the picosat and sat4j solvers and all runs of the highest cardinal models on all non bdd solvers.
on smaller models up to a cardinal value of the native sat and csp solvers produce the best runtimes.
the java based sat and csp solvers produce the longest runtimes on these models.
on larger models the runtimes of the sat and csp solvers show a strong increase.
on all models with cardinal all bdd solvers were faster than all other solvers.
the buddy bdd solver was always the fastest bdd solver with one exception cudd wins the race on the web shop model.
regarding the results of the low performers on small models choco the java csp solver is the solver with the longest runtimes.
though the runtime increase of choco on high cardinal models is relatively low.
on models with a cardinal value above the runtimes of choco and the mistral native csp solver are comparable.
d. results from the all products operation fig.
shows the average runtimes of the all products operation.
on this operation the clasp sat solver is fastest on small models with cardinal g148 .
above this threshold the buddy bdd solver is the fastest solver up to cardinal g148 .
on all larger models the cudd bdd solver is the fastest solver with buddy on the second place.
up to a cardinal value of the clasp sat solver is faster than at least one bdd solver.
the choco csp solver behaves similar to the cardinal operation its increase in runtime is less strong than the increase on other sat and csp solvers.
another observation is that the picosat solver did not terminate within reasonable time on the models with the highest cardinal value.
the javabdd solver showed high deviations that occur inde pendently from cardinal .
figure .
runtime of the number of products operation all solvers based on cardinal table i. s ummary of results operationsolver type lan guagebuddy bdd ccudd bdd cjavabdd bdd javachoco csp javacream csp javamistral csp c clasp sat c picosat sat csat4j sat java valid minimum maximum average standard dev.
small models medium models large models cancelled products minimum maximum average standard dev.
small models medium models large models cancelled variabilit y minimum maximum average standard dev.
small models medium models large models cancelled all products minimum maximum average standard dev.
small models medium models large models cancelled small models is the average of models with cardinal g148 models medium models is the average of models with cardinal g1488 models large models is the average of models with cardinal models on valid on cardinal on variability on all products cancelled refers to cancellations afte r a runtime of hours .
hours on variability operation 318the standard deviations of the runtimes for each solver ranked from .
to .
.
the native bdd solvers produced runtimes with low standard deviations between .
cudd and .
buddy .
the other solvers except for the sat4j solver produced standard deviations up to .
.
the sat4j sat solver produced a standard deviation of .
.
as explained in the all products operation is often infeasible on larger models due to the high memory consumption of the operation.
we restricted the operation test set to models with a cardinal value of or less.
e. general significance of runtime measurements the analysis of variance anova is a statistical standard procedure for analyzing effects of independent variables on a dependent variable with regard to a certain hypothesis.
anova can be applied in situations of independent cases with a normal distribution of the residuals and homogeneity of variances it is tolerant to slight departures from these preconditions cf.
.
in our case independence and homogeneity of variances is achieved through the experiment design.
we assume a normal distribution of the residuals from the high number of real models.
moreover we assume that the collected models are representative.
in the experiment a x x repeated measures two way anova was conducted.
the goal was to determine whether the choice of the operation and the solver influenced the runtimes of the five repeated measures.
the results indicated a main effect for the choice of the solver with f .
p .
.
moreover a main effect for the solver by operation was indicated with f .
p .
.
on the valid operation the clasp sat solver had the shortest runtime m .
sd .
.
regarding the number of products operation the cudd bdd solver had the shortest runtime m .
sd .
.
on the variabil ity operation the cudd bdd solver had the shortest runtime m .
sd .
.
on the all products operation the cudd bdd solver had the shortest runtime m .
sd .
.
f. significance of problem structure and platform a repeated measures two way anova was conducted to determine whether the solver problem structure bdd csp or sat had a significant influence on the runtimes.
the result of f .
p .
indicates a significant influence of the solver problem structure.
though regarding the solver problem structure by operation a result of f .
p .
.
indicates no main effect at the significance level.
a second anova was conducted to determine whether there is an effect of the solver platform native or java on the runtimes.
the result of f .
p .
indicates a main effect.
regarding the platform by operation a result of f .
p .
indicates a main effect.
g. correlation analysis to determine influencing factors to runtime some complexity measures were calculated for all test models.
the set of complexity measures includes the number of features cardinal the extra constraint representativeness ecr and two well known complexity measures from graph theory the cyclomatic number and the number of cliques.
pearson s correlation coefficient between each of the complexity measures and the runtime measurements was calculated and tested for significance at the significance level.
significant correlations were found for all solvers between the number of features and between cardinal and the mean runtimes.
the correlation between cardinal and the mean runtimes were extraordinarily high.
for the all products number of products and variability operation the correlation coefficients rank from .
to .
they are within .
.
for the valid operation.
the valid operation shows a figure .
runtime of the all products operation all solvers based on cardinal 319higher correlation between the number of features and the mean runtime for all non bdd based solvers ranking from .
to .
whereby all bdd solvers showed higher correlations between cardinal and the mean runtime on the valid operation.
there was no significant correlation between ecr and mean runtime for any solver operation combination.
a significant correlation between cyclomatic number and the mean runtime was found on the valid operation for all solvers with the correlation coefficients in .
the three csp solvers produced the three highest correlation coefficients in .
considering the variability operation the correlation between cyclomatic number and mean solver runtime is significant for all but the sat4j and picosat solvers.
the significant values are in .
on the number of products operation only the bdd solvers correlation coefficients in are significant.
the correlation coefficient between the number of cliques and the mean runtime was significant for all solvers on the valid operation with correlation coefficients in and solely the correlation coefficients of the bdd solvers in .
on the number of products and variability operation significant correlation coefficients were found for all solvers except sat4j and picosat.
the correlation coeffi cients for these operations are within the correlation coefficients for the bdd solvers on both operations within .
h. significance of model size the initial x x repeated measures two way anova indicated a main effect of model size with f .
p .
regarding the division into three groups as mentioned above.
therefore in addition one anova was conducted for each of the three groups to determine whether there are significant influences within these groups.
on the small models group f .
p .
indicates a main effect for the choice of solver f .
p .
indicates a main effect for solver by operation.
regarding the medium models group f .
p .
indicates a main effect for the choice of solver f .
p .
indicates a main effect for solver by operation.
on the large models group f .
p .
.
indicates no main effect for the choice of solver.
moreover f .
p .
.
indicates no main effect for solver by operation.
we assume that this effect results from the low number of results considered due to the high number of cancellations in this group especially on complex operations where some solvers delivered only results.
therefore we repeated the anova considering the results of only the valid and number of products operation and the solvers that produced at least on these operations.
the results of the repeated analysis indicated a main effect of solver with f .
p .
solver by operation with the same results because only two operations were tested .
for the descriptive statistics of the three groups see table i. vii.
i nterpretation of results this section provides an interpretation of the results based on the hypotheses defined in the experiment design.
the inter pretation provides an answer to our two research questions.
in general we can reject h0 at the significance level leaving only a small chance of error.
in subsection a we answer rq1.
in the following subsections b and c we regard influences on the solver performance to resolve rq2.
subsection d presents threats to validity.
a. choice of solver based on the results from the three anovas performed to determine the significance of influences to the runtime we reject part a d of the null hypothesis and conclude that there is a significant influence of a the solver in general b the solver problem structure and c the solver platform on the runtime partially depending on the operation performed.
therefore we can conclude that there is a significant difference between the runtimes of bdd csp and sat solvers.
b. specific influences on the solver performance the result of the operation having an influence slightly below the significance level in the large models group may be caused by the similarities in the runtime distributions of the variability and cardinal operations and by excluding some very large models due to runtimes above hours.
this leads to a smaller number of results in the large models group.
the mean runtime of bdd solvers is significantly shorter than the mean runtime of csp solvers which itself is significantly shorter than the mean runtime of sat solvers.
the mean runtime of the native solvers is significantly shorter than the mean runtime of the java based solvers.
regarding the four operations the bdd solvers show a higher standard deviation than the native csp and sat solvers on the least complex valid operation.
while the runtimes of the native csp and sat solvers are very stable the runtimes of the bdd solvers are less predictable depending on the cardinal value of the model.
the bdd solvers in the test produced the lowest average runtime on large models on all except for the valid operation on which they were outperformed by the native sat solvers.
on all more complex operations the bdd solvers showed the lowest runtimes at least within the large models group.
small to medium models were solved by a native sat solver clasp in the shortest time.
though the overall maximum runtime of the bdd solvers during the test was below one minute.
the correlation analysis indicates a correlation between runtime and several graph complexity measures depending on solver and operation.
furthermore the bdd solvers showed a different behavior in the correlation analysis compared to sat and csp solvers regarding the valid operation.
the results of the correlation analysis as well as the peaks in runtime in conjunction with the high standard deviations of the bdd solvers lead us to assume that there is a more complex relation between specific properties of feature models and the runtime of the bdd solver.
considering the model size we can reject parts e f of the null hypotheses.
the anova performed on the three groups of different sized models indicates a significant influence of the model size.
beyond this the results from the correlation analysis indicate a significant correlation between cardinal and runtime that is higher than the correlation between any other 320tested factor and runtime.
this confirms that the choice to form categories of models based on cardinal is reasonable.
c. threats to validity the validity of an experiment can be expressed in terms of internal and external validity where the internal validity refers to possible confounders affecting independent variables without the researcher s knowledge e.g.
factors in the environment affecting the performance of one solver.
although the experiment was designed to minimize these factors one threat to the internal validity remains.
as discussed above we used different input formats for the bdd csp and sat solvers.
while the bdd and csp solvers use the splot xml format the sat solvers process a dimacs cnf.
an implementation of the sat solvers solely based on the splot xml format may show slightly different results.
this may be a factor when comparing high performance sat and csp solvers in situations with similar runtimes which particularly occur between the two sat solvers clasp and picosat and the csp solver mistral.
this does not affect the java based solvers because the runtimes of our java based implementation of a format converter are well below the minimum runtime of the javabased solvers.
the external validity refers to the generalizability of the results.
for example it may be possible that some real feature models contain modeling constructs that are not considered in the models obtained from but affect performance.
we considered a large number of feature models to minimize this factor.
beyond this there are two aspects of the solver implementations.
first the memory structure for the results of the all products operation can be implemented in a more or less efficient way.
second the transformation algorithms for transforming a feature model into a bdd or a csp can be more or less efficient.
viii.
d iscussion and conclusion a. conclusion from the results the results of the experiment indicate that different solvers can be superior with regard to performance on specific models or performing specific operations with the bdd solvers producing the best results in most situations.
in particular the bdd solvers produced the best performance in the case of larger models as well for more complex operations which generally tended to produce longer runtimes.
native implementations of csp and sat solvers like mistral clasp and picosat produced superior results either on small models or on the relatively simple valid operation.
moreover native csp and sat solvers produced relatively low standard deviations on this operation.
this makes their runtimes more predictable compared to the bdd solvers with noticeable runtime peaks even for simple operations and small models.
csp and sat solvers are especially a good choice for smaller models in a time critical environment but a potentially high number of solver calls.
when analyzing larger feature models typically advanced analysis operations e.g.
the detection of dead features consist of a large number of calls of basic operations.
for example in an interactive modeling environment with automaticallytriggered checks that are performed to give immediate feed back to the modeler this can lead to a time critical situation.
in this case the differences between native csp and sat solvers on the one hand and bdd solvers on the other hand are particularly considerable.
on the valid operation for bdd solvers the correlation between the actual cardinal value and runtime was higher while for non bdd solvers the correlation between the num ber of features and runtime was higher.
the maximum value for cardinal in a feature model with n features is n .
this suggests an exponential runtime increase with n for non bdd solvers on the valid operation.
b. summary and outlook we performed an experiment considering four analysis operations.
these operations are representative for different types of operations typically performed on feature models.
we described statistically significant effects on the runtime of differ ent solver implementations that use different base problem structures and platforms.
the outcome was that java implementations produced significantly higher runtimes than native implementations.
our central finding is that neither the boolean nor the csprepresentation is superior in all situations with regard to the runtime of the corresponding bdd sat or csp solvers.
the results confirm the findings from related work by showing that bdd solvers outperform other solvers on large models even on real instead of generated models.
compared to general studies on the efficiency of problem solving techniques the results of our experiment show that for feature model analysis the potentially exponential growth of the bdd size is not a noticeable problem for the practical feasibility of the analysis compared to the runtime increase experienced by csp and sat solvers.
in addition the experiment showed that csp and sat solvers in general behave similar but the solver implementation can lead to a significant difference especially between native and java implementations.
in general the results of our study facilitate the choice of the appropriate solver depending on the size of the models to be analyzed and the in tended analysis operation.
we envision further studies to analyze the applicability of the representations and solvers in a specific context.
in particular we plan to apply advanced analysis techniques from graph theory to the models in order to determine further possible influences on the performance results and to investigate the cause of the correlations found.
as a long term objective we plan to include our findings in the development of a tool framework for the automated analysis of variability models to support the automated selection of suitable solvers.
a cknowledgment this work has been funded by the dfg under grant po kopi.