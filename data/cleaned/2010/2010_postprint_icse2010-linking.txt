linking e mails and source code artifacts alberto bacchelli michele lanza reveal faculty of informatics university of lugano alberto.bacchelli michele.lanza usi.chromain robbes pleiad dcc reveal university of chile university of lugano rrobbes dcc.uchile.cl abstract e mails concerning the development issues of a system constitute an important source of information about high level design decisions low level implementation concerns and the social structure of developers.
establishing links between e mails and the software artifacts they discuss is a non trivial problem due to the inherently informal nature of human communication.
di erent approaches can be brought into play to tackle this traceability issue but the question of how they can be evaluated remains unaddressed as there is no recognized benchmark against which they can be compared.
in this article we present such a benchmark which we created through the manual inspection of a statistically signi cant number of e mails pertaining to six unrelated software systems.
we then use our benchmark to measure the e ectiveness of a number of approaches ranging from lightweight approaches based on regular expressions to fulledged information retrieval approaches.
categories and subject descriptors d. .
restructuring reverse engineering and reengineering .
introduction it is estimated that up to percent of software maintenance is spent on program comprehension .
this is because the knowledge about a system is often expressed implicitly and thus is di cult to retrieve .
for this reason many approaches extracting information from the source code and the program structure have been presented in a process known as reverse engineering .
however the development of any software system sees the creation of artifacts beyond the actual source code such as requirements and design documents system documentation mailing list discussions bug reports etc.
while such non source artifacts enclose valuable information about the system they permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
icse may cape town south africa copyright acm ... .
.document they are often informal free text natural language documents and therefore non trivial to process.
free form natural language artifacts e.g.
documentation wikis forums e mails intended to be read both by system developers and by users often reference implicitly or explicitly other artifacts such as source code or bug reports.
however reliably nding the traceability link between artifacts is an issue that the software engineering research community is still trying to address.
moreover it is di cult to compare the e ectiveness of such approaches as often there is no established benchmark.
we focus on recovering the traceability link between emails and source code artifacts.
mailing lists are employed by developers and users to discuss various topics ranging from low level concerns e.g.
bug xes refactoring to highlevel resolutions e.g.
future planning design decisions .
emails provide additional metadata author date and time threading etc.
that enable further analyses such as the social interaction between participants geographic analysis the behavior of developers and users the correlation between mailing lists development activity .
in a preliminary work we devised a set of lightweight methods based on regular expressions to establish the link between e mails and software artifacts.
we evaluated them in terms of precision and recall considering one single java system.
in this paper we overcome a number of limitations of our previous work resulting in the following contributions an extensive and publicly available1benchmark and toolset for recovering traceability links between e mails and source code artifacts.
we created our benchmark by analyzing the mailing lists of six di erent software systems written in four di erent programming languages.
for each system we manually annotated a statistically signi cant number of e mails.
a comprehensive evaluation of linking techniques.
we evaluated and compared in terms of precision and recall di erent linking methods ranging from lightweight grep style approaches to more complex approaches from the information retrieval ir eld.
structure of the paper.
in section we review related work.
in section we present our benchmark how we created it and miler its supporting tool infrastructure.
in section we illustrate the lightweight linking approaches while in section we present the information retrieval techniques.
in section we show and discuss the results achieved by each technique.
we draw our conclusion in section .
1see related work researchers proposed several techniques to deal with the issue of traceability between source code and non source artifacts.
we focus on methods that use information retrieval ir techniques to automatically retrieve the missing links.
probabilistic and vector space model vsm .
antoniol et al.
experimented with two di erent ir models to retrieve the links between code and documentation a probabilistic ir model and a vector space ir model .
the former computes a ranking score based on the probability that a document is related to a speci c source code component while the latter calculates the distance between the vocabulary of all the documents and a code component.
they analyzed two small software systems the rst is leda library of e cient data types and algorithms a c library of classes totaling kloc.
the documents to link were manual pages automatically generated through scripts that extract comments from the source les names of functions parameters and data types present in those les were also present in the manual pages.
each class was described by at most one manual page while of the classes were not described at all.
out of manual pages were relevant the remaining ones contained basic concepts and algorithms.
there were links overall.
the second system was implemented by students and had classes of which were considered in the study.
the classes were matched with functional requirements written beforehand.
there were links overall.
they used the collection of the documents as the corpus in which to nd the missing links and every single source code le as the query .
each document was pre processed to ease the linking task they converted any letter to lowercase removed stop words i.e.
articles punctuation numbers common words etc.
and performed stemming on the result i.e.
converting plurals into singulars transforming verbs into their in nitive form etc.
.
in addition to this they also extracted the list of identi ers from the source code removed the language keywords and split compounded words e.g.
classname intoclass name .
they chose to disregard comments.
in both case studies results revealed a better performance overall of vsm over the probabilistic approach.
latent semantic indexing lsi .
marcus et al.
proposed a solution based on lsi .
lsi is based on a vector space ir model that takes into consideration that a word always appears in a context.
this additional information provides a set of mutual constraints that determines meaning similarity in sets of words.
they evaluated the e ectiveness of their technique on the same systems considered by antoniol et al.
adopting an inverse approach they used the collection of the source code les as the corpus in which to nd the link and each document as the query .
as text pre processing they removed non textual tokens from documents converted any letter to lowercase and split compounded words in the source les while also keeping the original form i.e.
classname becomes classname class name .
they considered source code comments inside as relevant.
finally as lsi does not use a prede ned vocabulary or a prede ned grammar it was not deemed necessary to perform the stemming process i.e.
there was no morphological analysis.
the results were slightly improved with respect to the ap proach by antoniol et al.
especially for leda there were more documents in the corpus and the same entity identi ers were used in both the source code and the documents.
hayes et al.
asserted that ir techniques must not substitute the human decision maker in the linking process but should be used to generate an appropriate list of candidate links .
they show how they used the three ir algorithms proposed by antoniol et al.
and marcus et al.
vector space model vector space model with a simple thesaurus and lsi to trace requirements to requirements and aggregate candidate links to be evaluated by software analysts.
they validated the algorithms on two systems of similar size to the ones used by antoniol et al.
one of circa kloc of c code and documents and the other with documents .
lormans et al.
used lsi to nd traceability relations between requirements design documents and test cases .
they evaluated the e ectiveness of lsi in terms of precision and recall on two small case studies.
natural language processing nlp .
baysal et al.
tried to correlate discussion archives i.e.
the e mails in mailing lists and source code .
they looked for a correlation between discussions and software releases.
first they recovered information about the system applying data mining techniques on its release history and the discussion archives.
then they used natural language processing methods to search for a correlation.
they presented two signi cant case studies a visualization tool a java system with les and an archive of e mails and apache ant a java system with java les and an archive of e mails .
baysal et al.
did not manually inspect the systems of their case studies to verify the quality of their results.
re ections.
while all approaches led to relevant results they incur an inevitable bias because of the small number of analyzed systems the small size of the systems and the low number of artifacts.
the only exception is the work by baysal et al.
where however there was no manual inspection of the results to verify their quality.
we argue that for any approach to have a solid validation a benchmark is needed against which any approach can be tested.
.
benchmark the areas in which ir techniques have proven useful e.g.
management of scienti c and legal literature web searches are supported by a set of well designed robust and universally accepted benchmarks.
such benchmarks are publicly available and distributed via the infrastructure of the text retrieval conference series trec sponsored by the national institute of standards and technology nist and the us department of defense darpa .
they keep evolving and now include retrieval tasks for many di erent kinds of information e.g.
spam genomic data .
however software systems have traits that distinguish them from the standard ir domains.
for example software artifacts form document collections orders of magnitude smaller than standard ir corpuses.
also although the developers knowledge is contained in identi er names and code comments developers write them in a terse technical language.
as a consequence we cannot assume without prior veri cation that ir techniques would work with the same performances in the software engineering eld.
speci c benchmarks for software engineering need to be devised.
112samplee mailssizee mails with a linkargoumlargouml.tigris.orgfreenetfreenetproject.orgjmeterjakarta.apache.org jmetersystemurla uml modeling tool developed over the course of approximately years.a peer to peer software for anonymous file sharing and for browsing and publishing freesites web sites accessible only through freenet .a desktop application designed for load and stress testing of web applications.
the first release was done in .javajan 2000apr 2000feb 2001totallinksdescriptionlanguagecreationmailing listsjavajavaaugeasaugeas.neta configuration file editing tool which parses configuration files and store them into a tree for successive modifications.cfeb 2008away3daway3d.coma realtime 3d engine for flash written in actionscript an object oriented programming language compliant with the ecmascript language specification.actionscript 3may 2007habarihabariproject.orga blogging platform written in object oriented php .php 5oct 2006table the software systems considered for the benchmark .
data set to create our benchmark to validate the e ectiveness of automatic linking techniques we analyzed six unrelated software systems written in four di erent languages.
they are all open source and both the source code and the mailing lists are freely accessible.
table details the systems.
we release this benchmark so that other researchers can bene t from it to analyze new techniques.
it does not require any special infrastructure to be used it can be improved and is easily extensible with additional data.
e mail archives .
all the projects have active mailing lists discussing di erent topics.
we focus on development mailing lists because they have the highest density of information about code entities and thus of the links we strive to nd.
as we had no prior details about the distribution of traceability links in e mail archives we employ random sampling without replacement as opposed to other techniques e.g.
strati ed random sampling to extract reliable sample sets from the populations of the e mails.
we establish the size n of such sets with the following formula n n p q z n e2 p q z since the proportion p of the e mails referring a speci c entity of the source code is not known a priori we consider the worst case scenario i.e.
p q .
we have populations that from a statistical point of view are relatively small so we included the nite population correction factor in the formula it allows us to take the population size n into account e.g.
e mails for jmeter .
we keep the standard con dence level of and error e of i.e.
if a speci c source code entity is cited in the f of the sample set e mails we are con dent it will be cited in the f of the population messages.
this only validates the quality of this sample set as an exempli cation of the population it is not directly related to the precision and recall values presented later which are actual values based on manually analyzed elements.this resulted in the sample sizes n shown in table .
the column e mails with a link counts the number of e mail with at least one reference to a code entity.
total links sums all the links retrieved from these e mails.
source code .
the other ingredient of our benchmark is the source code of the systems.
we consider all the e mails since the inception of the mailing lists so we also consider any system release throughout the system s history.
when available we took o cial releases as our milestones e.g.
for jmeter or argouml otherwise we used the checkout by date feature of the version control system i.e.
we retrieved the committed source code in months intervals starting months after the repository creation .
we are interested in linking e mails with source code entities i.e.
classes in object oriented systems functions and structures in procedural language systems so we do not consider source les as the unit for documents as opposed to antoniol et al.
and marcus et al.
but we strive for a ner granularity.
we parse the source code extract the model and nd the links between model entities and e mails.
linkedsystem271387192209537411 396822releasesnumber of entitiesfirstreleaselastreleasetotalargoumlfreenetjmeterhabariaugeasaway3d table source code entities per software system table lists the collected data.
.
tool infrastructure miler linked modelssystem modeltarget systemmailing list archivesourcecode repositoryfamix compliantsystem model e mailmodelbenchmarkmanual analysis message linking vector space modelslatent semantic indexinglightweight techniquesparse e mailscomparison keyworkd addaliquam ac leo quis dictum convallis odio sit.
sed vel nisi libero.
in hac habitasseclearremove lastmauris dictum convallis odio sit.doneis reported parse source code figure infrastructure for this experiment we created miler the infrastructure shown in figure .
for each system we extract the e mails from the mailing list archive in which they reside and import them in our infrastructure according to the e mail metamodel we implemented.
we parse the source code of each release and create a system model complying to famix a language independent meta model for procedural and objectoriented code .
once the models are ready we link the messages and the source code.
message linking produces famix models in which each entity is annotated with the reference to any email treating it.
the model containing manually inferred links is our oracle it is compared to the models produced by the automatic techniques.
.
creation of the benchmark the creation of the benchmark consisted in reading all the e mails in the sample set and annotating them with the source code entities they treat.
we built a web application in miler to assist this task.
figure shows its main page after a user logs in.
iiiiiiivv figure benchmark creation web applicationit has the following components thesystems panel i shows the list of the software systems that are loaded in the application and must be analyzed for creating the benchmark.
themails panel ii keeps the user updated on the number of e mails for each system that have been read over the total number of e mails to analyze.
thenavigation panel iii lets the user retrieve any email by its permalink displayed in the e mail header .
the main panel iv contains the e mail header i.e.
subject author date mailing list and its body.
sentences quoted from other e mails are colored according to the quotation level this increases the e mail readability and the quality of the analysis.
theannotation panel v contains the list of already related entities and an autocompletion eld figure .
the autocompletion eld helps the user when annotating the e mail the user can see any entity whose name includes the letters she inserted and the autocompletion avoids typos since only entities actually in the system can be related.
the panel shows how entity names are colored folowing a special convention entities are black if belonging to the last release before the e mail date light grey if belonging to an older release blue if implemented in the rst release after the e mail date and light blue if released later.
for example consider the user typing objectcontainer3d figure .
the autocompletion menu shows the homonymous entities in three colors proto objectcontainer3d is displayed lightgray because it is older than the current release containers objectcontainer3d is blue it will be released in the next version scene objectcontainer3d is black as in the current release.
this helps the reader in the choice of the most appropriate entity.
figure web application autocompletion menu six members of our research group with several years of programming experience inspected the sample set.
the emails were randomly divided in overlapping sets resulting in of the messages analyzed by two people.
a complete agreement was reached on of these messages with the remaining annotations featuring small di erences almost all the divergences were caused by one of the two reviewers missing to annotate a link that was actually present in the e mail.
all the errors were corrected.
annotators did not di erentiate between links only present in text quoted from previous messages and present in the new content of the e mail.
this allows the usage of this benchmark as a general case of textual information containing source code identi ers and discussions.
.
evaluation to compare the e ectiveness of all the approaches we measure two well known ir metrics for the quality of the results namely precision precision jtp j jtp fp j and recall recall jtp j jtp fn j .tp true positives are elements correctly retrieved fn false negatives correct elements not retrieved and fp false positives elements incorrectly presented as correct.
we can describe precision as the fraction of the retrieved links that are correct and recall as the fraction on the total number of correct links.
the union of tpandfnis empty in those benchmark e mails that have no reference to source code entities.
in these cases the recall value cannot be calculated.
likewise automatic approaches can nd no link between an e mail and source code so the precision value cannot be evaluated.
to overcome these issues we calculate the average of tp fp andfn on the entire dataset and measure the average precision and recallfrom those values.
this solution also takes into account the impact of false positives on precision when the set of benchmark