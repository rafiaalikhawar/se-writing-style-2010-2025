penelope weaving threads to expose atomicity violations francesco sorrentino university of illinois urbana champaign sorrent1 illinois.eduazadeh farzan university of toronto azadeh cs.toronto.edup .
madhusudan university of illinois urbana champaign madhu illinois.edu abstract testing concurrent programs is challenged by the interleaving explosion problem the problem of exploring the large number of interleavings a program exhibits even under a single test input.
rather than try all interleavings we propose to test wisely to exercise only those schedules that lead to interleavings that are typical error patterns.
in particular in this paper we select schedules that exercise patterns of interaction that correspond to atomicity violations.
given an execution of a program under a test harness our technique is to algorithmically mine from the execution a small set of alternate schedules that cause atomicity violations.
the program is then re executed under these predicted atomicity violating schedules and verified by the test harness.
the salient feature of our tool is the efficient algorithmic prediction and synthesis of alternate schedules that cover all possible atomicity violations at program locations.
we implement the tool p enelope that realizes this testing framework and show that the monitoring prediction and rescheduling with precise repro are efficient and effective in finding bugs related to atomicity violations.
categories and subject descriptors d. .
testing and debugging d. .
software program verification general terms algorithms reliability verification keywords atomicity violation concurrency dynamic analysis predictive analysis schedule selection testing this work was funded partly by the universal parallel computing research center upcrc at the university of illinois at urbana champaign sponsored by intel corp. and microsoft corp by nsf award and by an nserc discovery grant.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
fse november santa fe new mexico usa.
copyright acm ... .
.
.
introduction concurrency errors are notoriously hard to find and are characterized by subtle interleaving patterns that tend to manifest in the field while passing extensive randomized testing in development.
furthermore they are hard to reproduce record and repair primarily because the programmer faces the difficulty of considering all the possible interleavings exercised by a nondeterministic scheduler.
testing all possible interleavings even under a single test input is infeasible and this interleaving explosion problem is one of the most important challenges that needs to be overcome to achieve effective testing of concurrent programs.
with the advent of multicore hardware concurrent and parallel software feature prominently in the future making this an important problem to solve.
the current technology of testing concurrent programs on a test input is stress testing to test the program under random schedules by creating a large number of threads and strewing the code with sleep commands for random time intervals and letting the system run and re run for days together.
this methodology is highly unsystematic and does not seem to exploit the fact that most concurrency errors can be explained using a simple schedule that uses a very few threads but an intricate interleaving of them.
the focus of research in this area has hence moved to testing systematically a small subset of schedules.
for instance the c hess tool from microsoft expects programs to use a small number of threads and tests all schedules that use a bounded number preemptions unforced context switches .
atomicity violating schedules.
the thesis of this paper is that one can do interleaving selection more effectively by only exercising those interleavings that are symptomatic of common error patterns of interactions among threads.
in this paper we focus on the common error pattern of atomicity violations and explore schedule selection algorithms that systematically choose and schedule those that violate atomicity.
a programmer writing a procedure say a method of a class often desires uninterfered access to certain shared data that will enable him her to reason about the procedure locally.
the programmer puts together a concurrency control mechanism to ensure this atomicity often achieved by taking locks associated to the data accessed or implicitly taking locks e.g.
using synchronized blocks in java.
this is however extremely error prone not acquiring all required locks for the data leads to errors non uniform ordering of locking can cause deadlocks and naive ways of granular locking can inhibit concurrency which force programmers to invent intricate ways to achieve concurrency and correctness at the same time.
when the procedure interacts with another concurrent thread inconsistent state configurations could occur leading to unexpected behaviors and errors.
such errors due to violationof atomicity are in fact as far as we know the most prevalent of concurrency errors a recent study of classifying concurrency errors shows that a majority of errors are atomicity violations about more are caused by ordering violations and together with atomicity capture almost all concurrency errors .
this motivates our choice in selecting executions that violate atomicity as the criterion for choosing interleavings to execute.
our philosophy of schedule selection is to exercise only those schedules that lead the execution to violate atomicity.
atomicity violations are defined with respect to specially marked sequential blocks of code in the program which we expect are reasoned sequentially and locally by the programmer.
in our framework we choose these blocks automatically to be obvious syntactically delineated blocks of code such as the methods of a class bodies of loop iteration etc.
while our framework does allow this markup to be refined by the tester manual markup is not necessary and is not done in our experiments .
the penelope framework.
the p enelope framework works by taking a concurrent programpunder a test harness t. the test harness tcallsp feeding it inputs as and when required and verifies that the output or reactive behavior of the program is correct.
p enelope first executes and observes an arbitrary concurrent execution rofpon t. this execution ris then abstracted to an abstract execution r0that records only the reads and writes to shared variables synchronization events such as the acquisition and release of locks thread creations and barriers and the begin and end of the sequential blocks of code local computation conditional checks etc.
are suppressed .
the abstracted execution r0is then subject to an algorithmic analysis that checks whether there are alternate ways to schedule the events in r0to obtain atomicity violating schedules where the violation involves two threads t1andt2 where a block oft1gets interrupted non trivially by a statement of t2due to an interaction involving a single variable.
this algorithmic analysis issound and complete in predicting atomicity violating schedules at this level of abstraction .
it also generates a set sof schedules that cover all possible atomicity violations involving two program locations in every thread t1that can be interrupted non trivially by a program statement in t2.
the predicted schedules though feasible at the abstract level of observation involving locks barriers and thread creation may not be actually feasible in the program p and even if feasible may not lead to actual errors the test harness attests to.
p enelope hence reschedulespunder the predicted atomicity violating schedules in s and checks whether the test harness verifies these executions to be correct.
p enelope hence has no false positives it executes predicted schedules and reports errors only when it finds one that violates the test harness.
algorithms for predicting schedules that violate atomicity.
the algorithms to select atomicity violating schedules and to predict with high accuracy only feasible schedules that respect the concurrency control mechanism in the program is non trivial to realize efficiently in comparison selecting all schedules with only kpreemptions as in c hess is algorithmically trivial .
in this paper we exhibit carefully crafted algorithmic techniques that can examine a single arbitrary execution of a concurrent program on a test and predict alternate ways of scheduling the same events to cause atomicity violations.
our technique generates schedules that violate atomicity with respect to any two threads and any single variable according to the marked boundaries.
the restriction to two threads and one variable is deliberate and pragmatically motivated most atomicity errors occur due to two threads and onevariable and our algorithms scale well in this case.
the predicted schedules however may involve the scheduling of all threads as these may be necessary to enable the violation in two threads.
the prediction phase of our technique examines the monitored run and algorithmically checks if they can be rescheduled to violate atomicity.
intuitively an atomicity violation is characterized by three events e1 f ande2 occurring in that order where e1and e2are in the same block of one thread and fis in a different thread ande1ande2are both in conflict withf.
the first part of this phase consists in identifying cut points which are pairs of events eandf such thatebelongs to the same thread as e1ande2 occurs between e1ande2 and can be scheduled concurrently with f. hence any schedule that executes eandfconcurrently will expose the atomicity violation.
finding atomicity violation using cut points runs in linear time using a static analysis of the execution by examining thelocksets andacquisition histories at every point and is adapted from our earlier work .
after finding the cut points we develop new techniques to generate alternate schedules that reach these cut points concurrently hence exposing atomicity violations.
this is perhaps the most important technical contribution of this paper.
schedules that reach the cut points concurrently are theoretically possible indeed the proof of the cut point generation relies on the existence of such a schedule .
however our algorithm synthesizes a schedule that also heuristically ensures maximum conformance to the original observed execution.
building schedules that adhere as much as possible to the causal order of the original observed execution is crucial to building feasible schedules the program under test may after all have many causal orderings such as communication barriers and even creation of threads that need to be respected to ensure feasibility.
we also propose several heuristics to reduce the number of contextswitches in the predicted schedules using a combination of escape analysis and preventing context switches between a contiguous set of read blocks of a thread.
this reduction helps controlling the overhead in the rescheduling phase as context switching is expensive to control our predicted schedules can have hundreds of thousands of context switches .
notice that our prediction algorithms are designed on purpose notto completely adhere to the happens before causal relation.
in the predictive world we do not wish to generate schedules that have the same partial order as the original run atomicity is a property of the partial order not the exact linearization hence adhering to the exact causal order will not lead to an atomicity violating schedule .
implementation and evaluation.
we have implemented the above algorithms and heuristics in a framework called p enelope that tests java programs.
p enelope observes an execution and weaves the events in them in different ways to exercise atomicity violating patterns subjecting the resulting executions against a given test harness.
p enelope achieves the monitoring and scheduling phases using java bytecode transformations that inserts code to monitor and orchestrate the scheduling of threads.
we show that with a standard automatic marking of atomic blocks p enelope accurately predicts schedules that violate atomicity is effective in reducing the number of alternate schedules to be tested to a small fraction of all possible interleavings and is effective in not only finding bugs but actually accurately scheduling them with accurate playback to expose errors detectable by the test harness.
while there have been several tools recently that aim to find errors using atomicity violations most of them including v elodrome and our own earlier framework reported in only monitor one observed execution for atomicity violations.
in con trast our tool observes an execution predicts a small set of alternate executions that violate atomicity and runs them against the test harness to check for actual errors.
related work.
there are two main streams of work that use predictive analysis for concurrent programs that are relevant.
in two papers wang and stoller study the prediction of runs that violate serializability from a single run similar to our prediction algorithm.
in recent work by us we also propose a similar prediction algorithm for detecting serializability violations.
the predictive algorithm of is better as it works with little memory overhead while that of wang and stoller keeps track of a large graph which doesn t scale as the size of executions increases.
the prediction algorithm in p enelope is adapted from the algorithm described in farzan et al .
note that while both these tools predict possible atomicity violations they do not actually synthesize alternate schedules nor try to reschedule the program to expose these errors and have a lot of false positives.
in the prediction algorithm of in combination with smt solvers was used to remove false positives.
a recent work related to ours is the tool ct rigger that has similar motivation as ours in finding and scheduling atomicity violations.
ct rigger works by examining a few executions finding points where atomicity violations could occur prunes away many schedules that are infeasible due to mutually excluded blocks or ordering constraints and tries to schedule a selected subset of the rest by scheduling those that are less likely to manifest.
the algorithms for pruning are however entirely based on heuristics in contrast the algorithms for pruning in p enelope and in particular algorithms it uses for synthesizing schedules are accurate with respect to the nested locking present in the program.
an experimental comparison was not possible first because ct rigger works for c while p enelope works for java and also because ct rigger tool was not available for comparison.
there has also been recent work on active randomized testing for atomicity in the tool a tom fuzzer that uses randomization and guidance techniques that executes and hold threads at strategic points to try to manifest atomicity errors.
we found the holding of threads at strategic points dynamically and randomly are a bit unpredictable in our experience with the tool this technique also does not have the capability of handling nested locks and interrupts threads at wrong positions which lead it to not find errors.
there has also been work on finding atomicity violations by using a generalized dynamic analysis of an execution.
s idetrack is a new tool that finds atomicity violations by a generalized analysis of the observed run.
note that this technique does not examine runs that are causally different as p enelope does and hence does not do any rescheduling.
moreover this technique only detects and reports atomicity violations for a programmer to examine and cannot produce error traces that violate the test harness a more liberal notion of generalized dynamic analysis of a single run have also been studied in a series of papers by chen et al who use static analysis of the program to build a causal map to analyze for atomicity.
the run time monitoring algorithms and tools for atomicity violations that check violations in just the observed run are well understood and the tool by farzan and madhusudan reported in provides the most space efficient algorithm known for this problem as the space overhead is bounded when the number of threads and variables are fixed the algorithm used in v elodrome also can perform sound and complete serializability detection but the space overhead is not bounded .
these techniques work for anynumber of threads and variables .
monitoring of course is a much simpler problem than prediction.
a vio defines access interleaving invariants certain patterns of access interactions on variables and learns the intended access interleaving using tests and monitors runs to find errors.
a variant of dynamic two phase locking algorithm for detection of serializability violations is used in the atomicity monitoring tool developed in .
apart from the related work discussed above atomicity violations based on serializability have been suggested to be effective in finding concurrency bugs in many works .
lipton transactions have been used to find atomicity violations in programs .
chess from microsoft and c ontestfrom ibm are two mature testing tools.
c ontest uses random injection of sleep and wait statements in programs to find errors.
c hess systematically exercises all executions that involve only a few contextswitches guided by the intuition that many errors manifest with a few context switches preemptions.
however c hess is constrained by the number of executions it must explore which even with two context switches grows quadratically in the length of the executions making it infeasible for long tests.
some of the benchmarks handled by p enelope in this paper will overwhelm c hess as there are too may interleavings it must exercise.
on the other hand chess has one distinct advantage over execution based analysis techniques including p enelope and ct rigger in that it explores allexecutions with two context switches and is not constrained by the events that occur in a particular observed schedule.
we believe that p enelope and c hess are complementary in the their coverage of the interleaving space.
there is a subtlety in our definition of serializability only accesses to data and notsynchronization events like acquisition and release of locks are considered in determining if a block in an execution is serially executed.
for example if a thread reads the field fof an object twice in an atomically marked section each time acquiring an releasing the lock on the object and another thread accesses a different field gof the same object in between these two accesses under the same object lock we will declare the execution as a serializable one this is the definition we follow in our earlier papers as well .
however several papers including the ones on s idetrack and a tom fuzzer will declare this execution as non serializable because they take into account the edges caused by the lock acquisitions and releases in the two threads.
our definition is in fact the more classical definition followed in database theory and we believe is the more accurate and useful definition for checking for errors in concurrent programs.
.
overview assume that we are given a concurrent program p a test inputi and a corresponding setof expected correct outputs o we consider a set of outputs as concurrent programs can be nondeterministic .
the input output pair i o constitutes a test harness.
we run the program pon the input i and monitor observe an execution r. in most cases we will get an expected output o2o at the end of the execution r. we would like to know if there is an alternative schedule that leads to an execution r0of the program which manifest some concurrency bug in p. in other words we would like to find an alternate execution r0that generates an output o0that does not belong to the set of expected outputs o signalling the existence of a bug.
the interleaving explosion problem prevents us from trying all possible runs as there are too many of them to find a bad one.
here we show how we come upwith likely candidates for such runs and how we schedule these candidate runs against the harness to check if it causes a real bug.
public synchronized boolean addall collection c modcount int numnew c.size ....... possible interference .... ensurecapacityhelper elementcount numnew iterator e c.iterator for int i i numnew i elementdata e.next return numnew !
figure method addall of concurrent java class vector .
we use the following example to illustrate how p enelope comes up with alternative runs that violate atomicity which can find concurrency bugs in a program.
consider the implementation of the method addall from the built in java library class vector in figure .
the purpose of this method is to add all elements of the parameter collection say another vector to the end of the current vector.
the method size which returns the number of elements of the source is a synchronized method.
the problem in this program is that after safely retrieving the number elements that are to be copied there can be a concurrent thread that modifies the source vector before the method finishes for example a concurrent thread could remove all the elements from the source vector .
therefore when the execution of the addall method happens the information about the number of the items that are being copied is stale and an exception will be raised when it tries to access elements that are not there anymore.
e1e2ftt prime......... triangleleft ............ testing this program is unlikely to reveal these kinds of bugs in small methods such as addall as scenarios where the second thread gets interleaved in between the events of the first thread is unlikely.
one has to actively make it happen and that is exactly what we do.
this problematic scenario can be generalized using the following pattern of shared variable accesses .
thread reads the value of a shared variable c.size in the above example .
.
thread writes modifies the value of that same shared variable.
.
thread reads the value of the shared variable again in c.iterator in the above example .
we refer to such a pattern as a rwr pattern read write read .
there are four more patterns that can capture other forms of undesired interference wrw write read write rww read writewrite wwr write write read and www write write write .
penelope focuses on executions that contain one of these patterns as good candidates for finding hidden concurrency bugs.
penelope works in three phases.
phase i monitoring.
in this phase we execute the program on the input from the test harness and observe a run r. penelope focusses its attention to read and write accesses to the potentially shared variables and acquisitions and releases of locks thread creation and barriers and ignores all the other events.
phase ii prediction.
in phase ii p enelope constructs several runsr0 based on run r by carefully reordering the events in run rso that each of the runs r0contain at least one of the suspect access patterns given above.
phase iii rescheduling.
in the last phase p enelope forces the program to execute r0by weaving the threads using a single processor if this is successful then the generated outputo0is checked to be in the set of expected outputs o if this fails a bug is reported.
if p enelope fails to schedule a runr0 i.e.
ifr0is not a feasible schedule of the program penelope discards it and moves to another predicted execution.
we describe in detail how the main algorithms in phase ii work in section interleaving selection and comparison with chess.
our approach differs from that of the c hess tool in the rationale that is used for selecting candidate runs.
c hess limits the number of explored runs by bounding the number of context switches actually pre emptions allowed in a single run.
it is assumed that the number of runs with up to a few context switches is small enough that one can check all of them.
this is not quite true.
in a program with nthreads where each thread executes k steps out of which at most bare potentially blocking there can be up to nk nb c !
executions when the number of context switches is limited to c .
this can amount to a huge number of executions for very simple programs.
the number of executions explored by c hess is in fact higher because of the way they define pre emptions where context switching caused by a thread encountering a yield statement is not counted as a pre emption even though it may be possible to continue executing the thread.
the number of pre emptions is hence not polynomially bounded in nandk even for a fixedc.
consider for example a program that has two threads t1andt2 that manipulate a 10matrix where t1updates the first 5rows andt2updates the latter 5rows.
each thread sequentially iterates over the cells in its portion and for each cell cadds up the value of all the 8neighboring cells and the cell citself and writes the result into the cell c this example is typical of particle computations and parallel algorithms in graphics.
the computations of the two threads overlap at the two middle rows.
assume that there is a lock associated to each cell that is acquired before a thread accesses that cell.
since there are about 400accesses to lock protected data by each thread c hess even with a preemption bound of sets out to explore approximately 4002of the interleavings and failed to finish even after hours.
p enelope on the other hand explores a much smaller set of interleavings at most one atomicity violating schedule for each of the variables accessed by both threads and finishes in under minutes.
.
preliminaries a concurrent shared memory program during its computation does local computation may read and write to shared entities memory locations dynamically create threads and use synchronization primitives such as locks .
the framework we build will observe only an abstraction of this computation that ignores local computation ignores precise values read or written but keeps track precisely of the read and write operations to shared memory locations as well as synchronization primitive usage.
this abstract view of a computation which we call an execution will be the one that is algorithmically analyzed in order to predict and schedule alternate executions.
recall that we assume that the program has been already automatically annotated with atomicity transaction boundaries to indicate rough logical sequential units of computation.
this is done by p enelope automatically by marking obvious units such as thecode for every method etc.
note that these annotations have no semantic value in particular they are certainly not respected by the compiler in any way.
these transactional boundaries are also part of the execution that we observe.
hence given an execution each thread executes in it a series of transactions .
a transaction is a sequence of actions each action can be a read or a write to a global variable or a synchronization action.
let us now define the notation to talk about executions they must satisfy the property that they contain transactions and that they respect the semantics of locks .
we assume an infinite set of thread identifiers t ft1 t2 g. we also assume an infinite set of shared entity names or just entities x fx1 x2 gthat the threads can access.
let us also fix a set of global locks l. the set of actions that a thread tcan perform on a set of entities x x is defined as t x ft t g ft read x t write x j x2xg ft acquire l t release l jl2lg.
actionst read x andt write x correspond to thread treading and writing to entityx actionst andt correspond to the beginning and the end of transaction blocks in thread t and actions t acquire l andt release l correspond to acquiring and releasing the lock l. define x s t2t t x actions on entities xby all threads t s x x t x actions by thread ton all entities and s x x t2t t x all actions .
for a word ?
let jtbe a shorthand notation for j t which includes only the actions of thread tfrom .
a word ?islock valid if it respects the semantics of the locking mechanism.
formally let l ft acquire l t release l jt2tg denote the set of locking actions on a lock l. then is lock valid if for every l2l j lis a prefix of s t2t t acquire l t release l ?.
lettran t x t ft read x t write x jx2xg?
t .
atransaction trof a threadtis a word in tran t x. let tran t tran t x ?denote the set of all possible sequences of transactions for a thread t and let tran denote the set of all possible transaction sequences.
definition .
.
anexecution over a set of threads t entities x and locksl is a word t x ?such that for each t2t jtbelongs to tran t and is lock valid.
let exect xdenote the set of all executions over threads tand entitiesx.
in other words a execution is a lock valid sequence of actions such that its projection to any thread tis a word divided into a sequence of transactions where each transaction begins with t is followed by a set of reads and writes and ends with t .
when we refer to two particular actions and in we say they belong to the same transaction if they are actions of the same thread t and they are in the same transaction block in jt i.e.
if there is some tsuch that 2at and there is no i0 i i0 jsuch that t .
throughout this paper we will assume that acquisitions and releases of locks are nested .
in other words locks are released in the reverse order of how they were acquired.
this is true in most of our examples and in fact using synchronized commands in java naturally give nested acquisitions and releases for the most part.
in any case even if lock acquisitions and releases are not nested our algorithms and tool will work it is only the accuracy of predicting feasible schedules that will get mildly affected.
.
access patterns for serializability violations there are fiveaccess patterns that correspond to simple atomicity serializability violations of two threads andone variable .
each access pattern corresponds to the existence and relative ordering of three events in an execution.
a pattern consists of two events e1ande2which belong to a thread t1 and a third event fwhich belongs to thread t2.
these events should appear in an execution such that e f e in other words foccurs aftere1 ande2occurs afterfin .
moreover e1 f ande2should be all accesses to the same shared variable x and should be of one formats given below.
note that these exhaust all patterns where e1andfare dependent as well as fande2are dependent two events are dependent if they correspond to two accesses to a single location where one of them is a write .
t1 t2 read x write x rwrrwwwwrwwwwrwread x ......................................................................t1 t2 read x write x ......................................................................write x t1 t2 read x write x ......................................................................write x t1 t2 read x write x ......................................................................write x t1 t2 write x ......................................................................write x write x e1 e2 f e1 e2 f e1 e2 f e1 e2 f e1 e2 f .
locksets and acquisition histories let be an execution and let f tgt2tbe its set of local executions.
consider t for anyt .
the lockset held after tis the set of all locks tholds lockset t fl2lj9i t t acquire l and there is no j i and t t release l g. the acquisition history of trecords for each lock lheld by tat the end of t the set of locks that tacquired and possibly released by tafter the last acquisition of the lock l. formally the acquisition history of t ah t lockset t !2l whereah l is the set of all locks l02l such that9i t t acquire l and there is no j i such that t t release l and9k i t t acquire l0 .
synchronize l1 point asynchronize l2 point b synchronize l3 point c consider the program structure on the right.
we have lockset a fl1g lockset b fl1 l2g lockset c fl1 l3g also we have ah a f l1 fg g ah b f l1 fl2g l2 fg g and ah c f l1 fl2 l3g l3 fg g. two acquisition histories ah1andah2are said to be compatibleif there are no two locks landl0such thatl02ah1 l and l2ah2 l0 .
at at the level of abstraction we observe executions namely observing reads writes and lock acquisitions and releases it turns out that we can precisely capture when two sequences of events of threadst1andt2can be combined to a lock valid execution using just locksets and acquisition histories.
more precisely let be an execution and let w1be a prefix of jt1andw2be a prefix of jt2.
then there is an execution 0such that 0jt1 w1and t2 w2 if and only if the locksets of t1andt2afterw1andw2are disjoint and the acquisition histories at the end of w1andw2are compatible this technical result is due to kahlon et al .
this result will underpin our algorithms for predicting and scheduling as it accu rately captures those schedules that are feasible in a program at our level of abstraction.
.
prediction of atomicityviolating schedules phase ii of our framework which is the predictive phase forms the crux of our algorithms and we describe it in detail below.
this phase is divided into two sub phases phase iia deals with the problem of identifying cut points of the form e f such that a schedule that reaches the two events in each cut point would trigger an atomicity violation.
this is followed by phase iib the schedule generation phase that actually synthesizes schedules based on the cut points.
.
phase iia cut point generation the first part of phase ii of p enelope is one that generates a set ofcut points for the observed execution.
this algorithm essentially determines whether it is possible to reschedule the original execution to one that violates atomicity and further provides witnesses in terms of a set of cut points.
this algorithm is adapted from an algorithm from our earlier work that determines cut points accurately and extremely efficiently in linear time in the length of the execution.
we define the notion of cut points and recap the algorithm here as it is involves concepts that will help in understanding our scheduling algorithm.
e1e2ftt prime.................. prime prime... consider a typical execution that we are looking for one where e1occurs first then f and thene2 wheree1ande2occur in one transaction block while foccurs in another thread and the three events form one of the patterns violating atomicity.
the basic argument in our earlier work is that such an abstract lock valid execution is possible if and only if there is an intermediate eventebetweene1ande2 ecan bee1but note2 such that there is an execution that reaches exactly up to executingeandfin the two threads.
to see why notice that we can roll back fand play it last as it is an access to a variable and hence we can execute e1followed by f and let the run proceed to executee2eventually.
e1e2ftt prime......... triangleleft ......... ...e...ah1ah2ls2ls1 ls1 ls2 ah1compatible withah2 it now turns out that checking whether there is a lock respecting run that reaches simultaneously e andfis easily solvable simply by examining the locksets and acquisition histories at these two points due to a result by kahlon et al which gives a necessary and sufficient condition for it the locksets ateandfmust be disjoint and the acquisition histories at eand fmust be compatible see section .
for definitions of locksets acquisition histories and compatibility .
more precisely a cut point is a pair of events in the execution e f such that there is an alternate schedule that can reach exactly up toeandfsimultaneously and furthermore any schedule that does that gives an atomicity violation.
the above arguments give a characterization of all cut points that lead to atomicity violations.
the algorithm to detect and compute cut points works as follows.
we iterate over the events of each thread gathering four sets r w ww andaa which are events along with locksets and acquisition histories.
the sets randwfor a threadtrecord forevery shared variable x one witness pair of lockset and acquisition history among all reads and writes to x respectively.
the sets ww andaaare more complex.
ww records a witness for every pair of lockset and acquisition history an event in between two writes to x. similarly aarecords events between any two accesses reads or writes to x. finally the sets ww andrare examined together to check for a compatible pair and so are the sets aa andw these capture all the five possible atomicity patterns.
for a fixed set of shared memory locations and locks the time taken by the algorithm grows linearly in the length of the execution quadratic in the number of threads as every pair of threads must be examined and at most quadratic in the size of the program as all possible program locations involved in atomicity violations are identified .
furthermore the algorithm works very well in practice scaling to work in minutes on executions of hundreds of thousands of events.
.
phase iib schedule generation the scheduling of alternate executions that violate atomicity is technically and practically the hardest aspect of this paper and we will try to explain all the ideas behind it.
we first describe a theoretically motivated algorithm that builds a simple execution from scratch from a cut point e f that causes an atomicity violation using locksets and acquisition histories.
this algorithm assumes that the program is made of nconcurrent threads that do not use any mode of communication but just interact using nested locking the algorithm is accurate in predicting under this assumption .
despite its promised accuracy it cannot itself be used in practice because the assumption is false programs do indeed have causal ordering of events and communication one thread could wait for another for a signal to proceed and threads do get created and destroyed which also gives a causal ordering to events if t1createst2 we can t commute events of t2before the event in t1that created it .
our main idea is to modify the algorithm to make the predicted run adhere to the causal ordering of events in the originally observed run as much as possible.
the effectiveness of this adherence is not captured in a theoretical assurance but its proof is in the pudding as we show that this strategy reduces infeasible executions from being predicted to a large degree in our experiments.
finally because of the adherence to the original execution the predicted executions share the complexity of the original one.
in particular the predicted schedules that p enelope synthesizes can have hundreds of thousands of context switches .
the number of context switches affects the time required to schedule the predicted execution as context switches take relatively more time than local executions.
we provide two heuristic but technically sound transformations of the schedule that reduce the number of contextswitches the transformations do not break any causal links that may be there in the program .
we now outline the theoretical algorithm the adherence algorithm and the two heuristics to reduce the number of context switches.
.
.
the theoretical scheduling algorithm let us in this subsection assume that the program consists of a static set ofnthreads all already active view them as nsequential programs interacting with each other and further assume that the threads do not communicate with each other explicitly using data.
let us assume the threads are t1 t nand further that eandf occur in threads t1andt2 and the locksets at eandfare disjoint and the acquisition histories at eandfare compatible.
our goal is to find a locking respecting execution that precisely executes up untileandfint1andt2.
first note that since we assumed there is no communication executing events from t3 t ncannever help in the execution oft1andt2 i.e.
never enable events in t1andt2 in fact they can only hinder finding an execution as they could acquire locks that t1 andt2may require.
consequently we can completely focus only on scheduling events in threads t1andt2.
the idea is that we are rescheduling up until a cut point e f in threadst1andt2 whereefalls in between e1ande2in a single thread and where the sequence e1followed by ffollowed by e2 realizes the atomicity violation.
in this alternate schedule assume that a lock lis held byt2atf and assume that the same locklis acquired and released by thread t1 beforee.
then we must schedule this block in t1before the last acquire of lock lby t2beforef ast1will have no chance to acquire it once t2has made this acquisition.
computing the above efficiently and in linear time is non trivial and this is where the acquisition history helps as it exactly captures the above information.
let x1andx2be the lastevents int1and t2 respectively that are before eandf respectively with locksets empty.
then we can easily schedule without violating locking firstt1up untilx1and thent2up untilx2.
note that the first events afterx1 andx2 either must be e f or must be acquisitions of locks that are never released till e f is reached.
hence the crux of the scheduling is to schedule from x1andx2tilleandf.
the algorithm works by building a graph of causal edges between events.
for every lock lin the lockset of f ifloccurs in the acquisition history of ewith respect to some lock l0 then we know that after the last acquisition of l0byt1 there was an acquisition followed by a release of the lock l. hence we know that we must schedule the last release of lock l0int1 say eventu before the last acquisition of lint2 sayv .
we capture this by adding a causal edge fromutov.
symmetrically we examine the lockset of eand the acquisition history of fand throw in causal edges.
it turns out that since the acquisition histories are compatible this graph will byacyclic and hence there is a schedule that respects these orderings.
the algorithm simply takes a linearization of partial order to obtain a schedule.
the above argument is adapted from the proof that the prediction algorithm works correctly which appears in farzan et al and in turn depends on a proof of a theorem by kahlon et al .
our contribution here is using it in the scheduling algorithm.
we have also augmented the above construction by using vector clocks to rule out cut points that are clearly infeasible.
we maintain vectorclocks for events that get updated during thread creation and at barriers only and eliminate cut points e f whereeandfare not concurrent with respect to the vector clocks.
this greatly reduces the number of infeasible interleavings p enelope generates.
.
.
algorithms to adhere to original execution note that the theoretically synthesized schedule described above blindly executes t1tillx1andt2tillx2 i.e.
to the last point where thread have empty locksets and further ignores all other threads.
this actually causes the scheduling algorithm to break in practice when we implemented this most predicted schedules were not feasible in the program.
the reason is that the theoretical assumption that there is a set of nthreads from the beginning that do not communicate is not real in practice.
penelope gets around this problem by scheduling a large prefix of the run accurately according to the original observed schedule.
adhering to the observed schedule lets us keep the causal conditions that exist in the actual program and yet allows us to get past it without modeling it or analyzing in our prediction model.
the scheduling algorithm implemented in p enelope works as follows.
consider a cut point e f in threadst1andt2with disjoint locksets and compatible acquisition histories and let x1andx2be the last events in threads t1andt2 beforeeandf respectively with empty locksets.
without loss of generality assume x1 occurs before x2in the original schedule.
now for every thread ti includingt1andt2 lethibe the last event in tithat occurs beforex1in the original schedule with its lockset empty .
note that h1 x1 buth2need not be equal to x2.
the idea now is to schedule all events of each thread tiup until each hi and moreover in the exact order as they occurred in the original schedule.
more precisely for an observed schedule we say event j j j j is in the prefix setpiff ti aand this event occurs before or is the same as the event hi.
the prefix predicted schedule is then simply the concatenation of all in increasing order of j wherej2p.
in other words we run through the original schedule and pick an event uto occur provided it is an event of thread ti for somei and occurs before hi.
the rationale of why this prefix has the desired properties goes as follows.
region iiiregion iiregion it1t2t3t4h2h3h4x1 h1x2ef first notice that if an eventuof threadtioccurs in the prefix schedule then so does every event of tibefore u hence the events we choose to execute certainly have their predecessor events in the same thread scheduled.
second we argue that the schedule is lockrespecting.
assume that there is an acquisition of a lock lat an eventuoftiin the prefix.
then the only way this lock is not feasible to acquire is that another thread tjacquired it but did not release it.
but since the original observed execution was feasible there must have been a release of this lock by tjin the original execution before u. this release must be included in the predicted prefix execution as well because we have chosen each hkto be an event at which the lockset is empty.
consequently the release must have happened before uin the predicted prefix schedule as well.
finally note that since this schedule adheres to the original thread to a large extent it is likely to be feasible in the real program as well.
this prefix is represented as region i in the figure above.
the next middle phase of the predicted execution is to execute the program from h2tox2int2 marked as region ii in the figure .
since no other thread holds any lock this is at least lock feasible.
finally we come to the last phase of the predicted execution region iii which schedules events in between x1andeand between x2andf.
this part is done according to the theoretical schedule synthesized in the previous section that uses locksets and acquisition histories to go back and forth between the two threads t1 andt2only to reach eandf.
once we reach eandf we let all threads go free to execute this will hopefully make t1executee2 eventually which will cause the atomicity violation .
in practice events in the prefix region i constitute the majority of events compared to regions ii and iii.
hence adhering to the original observed execution accurately in this region greatly increases the chance for the schedule being feasible and as we show in the experiments leads to most predicted executions being schedulable.
.
.
heuristics for reducing context switches while the above described algorithms focus on generating executions that are likely to be feasible they ignore the overhead complexity of actually scheduling the runs.
the executions we observeinstrumenterjava classesobserved runmonitorcut pointconfirmed bugs test harnessschedulegeneratorset of cut pointspredicted schedulespassed the test harnessgeneratorclasses instrumentedfor schedulingscheduleinstrumenterof the form e f instrumentedclassesphase i monitoringphase ii predictionphase iii reschedulingfigure penelope architecture .
have a large number of context switches hundreds of thousands in some of our examples and since our predicted executions try to adhere to the observed runs for feasibility they also have a large number of context switches.
we hence employ two heuristics to bring down the number of context switches while at the same time preserving feasibility.
both heuristics rely on the idea that if a sequence of actions is provably non interfering among the threads then we can execute the sequence in any order and in particular execute them with the least number of context switches.
escape analysis to reduce context switches the first idea is to observe which of the non local accesses to memory locations are actually shared in the run in actual runs several shared variables may be touched by only one thread.
let us call these memory locations unshared in the observed run.
unshared variables are not known in advance and we do observe them and they do play a role when we try to reschedule executions.
however accesses to unshared variables by a thread cannot affect another thread and hence we can shuffle these to obtain fewer context switches without affecting feasibility.
identifying read blocks to reduce context switches the second idea is that even if there are a large number of contiguous reads to shared variables these accesses do not cause real interference between the threads.
hence the reads in this contiguous block can be shuffled in any way without affecting feasibility of the schedule.
again we schedule these reads in a way that minimizes the number of context switches and reduce it to at most the number of active threads in the block.
.
implementation in this section we explain the key implementation details of penelope .
figure illustrates the structure of p enelope .
for more information on penelope and the experiments presented in section see sorrent1 penelope .
phase i monitoring we implemented our monitoring instrumenter using the bytecode engineering library bcel .
every class file in bytecode is automatically transformed so that a call to a global monitor is made after each relevant action is performed.
these relevant actions include field and static field reads writes entry exits to synchronized blocks and methods array reads writes etc.
but excludes actions such as accesses to local variables.
the global monitor communicates with all threads.
each thread informs the monitor when it is performing an action that needs to be observed.
phase ii a cut point generation in this phase the algorithm in section .
is used by the cut point generator to identifies all possible cut points off line i.e.
pairs of events e f such that a schedule that reaches these points concurrently will violate atomicity.
note that our algorithm computes only one representative violation for each pair of threads each entity each program location each pair of events with a compatible set of locksets and acquisition histories and each pattern of violation w r w and a w a .
since recurrent locks multiple acquisitions of the same lock by the same thread are typical in java the tool is tuned to handle them by suppressing from the analysis the subsequent acquisitions of the same lock by the same thread.
a simple automatic escape analysis unit written as a perl script excludes from the execution all accesses to thread local entities by replacing them with skips nops which enables faster prediction.
phase ii b schedule generation theschedule generator synthesizes a schedule for each predicted cut point using the algorithms described in sections .
.
the idea is that schedules are first synthesized up to the points where locksets are empty using the original observed schedule and then the accurate theoretical scheduling algorithm is used to execute the events of t1andt2 to cause an atomicity pattern violation.
also we implemented the heuristics to reduce the number of context switches by rearranging events that are not truly shared using the escape analysis as well as blocks of reads to reduce the number of context switches.
phase iii rescheduling predicted schedules our scheduler is also implemented using bcel we instrument the scheduling algorithm into the java classes using bytecode transformations so that the same events that were monitored now interact with a global scheduler.
the scheduler at each point looks at the predicted schedule and directs the appropriate thread to perform a sequence ofnsteps.
the threads stop at the first point with a relevant access and wait for a signal from the scheduler to proceed and only then they execute the number of observable events they were asked to execute.
after this the threads communicate back to the scheduler relinquishing the processor and await further instructions.
the scheduler uses two vectors sandnto communicate with program threads.
for each thread ti n will contain the number of steps that tishould take next and s contains a boolean value that allowstiprogress when true.
only sis used to communicate with threads and is protected by global locks one for each cell .
threadtican safely access n without synchronization as the synchronization in sis used to provide the synchronization for n as well .
the thread t hence does a wait ons to become true and the scheduler wakes up the thread when it is time for the thread to proceed.
the scheduler then waits on s to become true.
when the thread completes its designated number of steps fromt it setss to true notifies the scheduler and then relinquishes control and waits on s to become true again.
once the execution reaches the point that the eandfevent from the violation pattern are executed the scheduler releases all threads to execute as they please.
there is also a timeout mechanism that detects when the scheduler is trying to schedule an infeasible run.
once the scheduler reaches eandfand lets all threads go if a timeout occurs we signal a deadlock note that at this point if all threads are stuck then this is a real deadlock in the concurrent program andmonitoring prediction scheduling application input exec threads entities locks execution execution context number of execution patterns context average total errors loc ution length time switches predicted time a n fswitches time per time time schedules feasible reschedulschedule ing vector .3k vectortest .22s .29s .01s .19s .10s vectortest1 .20s .20s .01s .33s .61s vectortest2 .18s .20s .01s .23s .57s vectortest3 .27s .27s .01s .21s .27s vectortest4 .33s .35s .01s .35s .27s stack .4k stacktest .21s .24s .01s .32s .06s stacktest1 .48s .58s .01s .56s .35s stacktest2 .29s .28s .01s .35s .77s stacktest3 .17s .27s .01s .18s .20s stacktest4 .18s .18s .01s .19s .04s elevator data .37s 26k .94s .24s .78s 30m42s data2 .77s 54k .26s .87s .12s 11m44s data3 .37s 329k .93s 50k 6m48s 39k .13s 2h52m tsp map14 .19s 32m 3m41s 72k 1m36s 60k .85s 50m01s map14 .15s 14m 1m58s 22k .98s 18k .16s 23m41s raytracer .5k sizea .38s .57s .01s .57s 5m52s sizea .94s .5k .03s .03s .14s 26m20s sizea .52s .9k .08s .09s .75s 1h38m sizeb .50s .62s .01s .46s 58m16s colt 29k dgemm .27s 12k 286k .42s .7s 50x50 pool .
.8k pooltest .19s .19s .01s .17s .18s pooltest1 .20s .24s .01s .24s .20s pooltest2 .12s .17s .01s .20s .48s pooltest3 .12s .18s .01s .20s .76s pool .
7k pooltest .25s .21s .01s pooltest1 .18s .25s .02s .78s pooltest2 .12s .17s .01s .06s pooltest3 .12s .88s .01s .31s apache ftpserver 22k lgn script 1m02s 1m02s .02s 1m03s 2h16m table experimental results.
a n and f of patterns indicate number of schedules that are respectively already appeared in observed execution not feasible and feasible .
hence corresponds to a real bug though not an atomicity related bug .
p enelope detects these kinds of deadlocks as well.
.
evaluation we ran p enelope on a benchmark suite of programs against several test harnesses and input parameters and we evaluated it under several criteria whether it is successful in predicting alternative schedules with violation patterns whether it is able to execute the predicted schedules the time it takes to predict and schedule executions and whether it is able to discover errors by checking the result of the execution of schedules with violations on test harnesses.
benchmarks.
the benchmarks used are all concurrent java programs that use synchronized blocks and methods as means of synchronization using synchronized blocks automatically ensures nested locking for the most part .
they include raytracer from the java grande multithreaded benchmarks elevator and tsp from vector andstack from java libraries pool two different releases from apache commons colt andapache ftpserver .
the concurrent program elevator simulates multiple lifts in a building tsp is a parallel program that solves the traveling salesman problem for a given input map raytracer renders a frame of an arrangement of spheres from a given view point pool is an object pooling api in the apache commons suite colt is an open source library for high performance computing apache ftpserver is a ftp server and by apache and vector and stack are java libraries that respectively implement the concurrent vector and the concurrent stack data structures.test suites.
in table we provide all the relevant information about the conditions under which the tests were run such as input files and parameters.
for elevator andtsp the input files were included in the benchmarks and the table indicates which input file was used for the results.
for elevator the number of threads was also specified in the input files and there were no additional parameters to be provided by the user.
the test harness fortsp includes an input file a given number of threads and a script would compare the minimum tour computed by the program against the minimum tour computed by a single thread execution.
forvector respectively stack we wrote test harnesses with two threads and two small vectors respectively stacks where each thread executes exactly one method from class vector respectively stack .
all the scheduled runs which manifested real errors raised exceptions during the execution.
the test cases for pool were designed similarly that is for each test case two different methods were run concurrently on the same object pool.
there was no particular test harness check and the error in pool .
manifested as an exception.
the same bug was fixed in pool .
.
forcolt thedgemm command was used which invokes a matrix multiplication routine on matrices of size .
the test harness was designed to check the result from a multi threaded execution against the result from a single threaded execution.
for raytracer all the data is already incorporated in the benchmark which comes in two sizes a and b. the only parameter that the user specifies is the number of threads.
raytracer has a built in validation test.
in the case of ftpserver we wrote a test harness with a client and a server where the client logs in and requests a connection the errors manifested as exceptions.experimental evaluation.
table demonstrates the result of the evaluation of penelope on the benchmarks.
the table provides information about all three phases monitoring prediction and scheduling.
for the monitoring phase the number of threads entities variables locks and the length of execution is reported as well as how long the execution takes and how many context switches exist in the observed run.
for the prediction phase we report how many violations were discovered total over all patterns and how long the prediction phase takes.
in the scheduling part we report the number of violation patters out of the total reported in the prediction column existed in the original run observed a the number of violations for which the schedules were not feasible n and the number of violations which appeared in a successful alternative schedule generated by p enelope f .
we also report how many context switches on average there are in the alternative feasible schedules what is the average time per feasible schedule a reasonable indication of overhead and finally and most importantly how many realerrors were found.
penelope finds several bugs in the benchmarks.
the bug in raytracer is caused by an atomicity violation involving the field jgfraytracerbench.checksum1 due to wrong synchronization.
the error in pool .
which was fixed in pool .
is caused by an atomicity violation on the variable factory in methods borrowobject returnobject ran in parallel with method close and in methods addobject andborrowobject ran in parallel with method setfactory .
note that these are different errors and they all manifested as exceptions during alternative feasible schedules exercised by p enelope .
all bugs in vector andstack are the result of an atomicity violation that causes the size of a parameter collection to go stale in the middle of an operation such as addall that is using the parameter collection as a source of information while only the destination vector stack is synchronized properly.
there were discovered errors for ftpsever each giving a different exception which correspond to variables m currconnections m writer m name m currlogins m request all accessed in method requesthandler.run of the server while the timer thread interrupts by closing the connection as a result of a timeout.
penelope predicts several atomicity violating schedules in tsp andelevator but they all pass the test harness and in fact are not errors the violation of atomicity was intended and correct .
observations here are some observations we gather from these experiments on the effectiveness and performance of p enelope the number of predicted schedules is small in fact a tiny fraction of all possible executions.
this is true even compared to the number of all runs limited to just two context switches or preemptions as c hess would do.
for instance in elevator data3 there are close to points releases of locks in threads where preemptions can happen giving around billion possible schedules involving just preemptions!
penelope is effective in finding bugs .
we ran the programs under the test harness several times and did not find almost any of the reported bugs in any of these benchmarks by merely running tests randomly.
it is clear that a more focused approach is absolutely necessary in finding errors on these benchmarks.
despite its small selection of schedules to test p enelope was able to identify bugs in these programs.
reasonable time overhead .
the runtime overhead in precisely scheduling the alternate executions is not prohibitively high and is in fact very minimal in most examples.
this is despite the large number of context switches that are being exercised 284k context switches in elevator data3 .
penelope finds bugs under complex scenarios .
note that the number of context switches scheduled in the predicted executions are very high.
we believe that this allows p enelope to dig deep into the search space of the runs.
tools like c hess execute all runs with a few context switches and offer a complementary search strategy .
zero false positives .
if a bug is reported by p enelope it is a real bug i.e.
an execution that violates the test harness .
a significant amount of violations found did not correspond to real bugs and are not reported as bugs.
this is in contrast to similar tools based on atomicity checking by wang and stoller farzan et al and the tools s idetrack a tom fuzzer and velodrome .
v elodrome in fact reports atomicity violations for benchmarks elevator tsp andcolt though they donotcorrespond to bugs.
.