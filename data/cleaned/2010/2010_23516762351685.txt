to what extent could we detect field defects?
an empirical study of false negatives in static bug finding t ools ferdian thung1 lucia1 david lo1 lingxiao jiang1 foyzur rahman2 and premkumar t. devanbu2 1singapore management university singapore 2university of california davis usa ferdianthung lucia.
davidlo lxjiang smu.edu.s g mfrahman ptdevanbu ucdavis.edu abstract software defects can cause much loss.
static bug finding tools are believed to help detect and remove defects.
these tools are designed to find programming errors but do they in fact help prevent actual defects that occur in the field and reported by users?
if these tools had been used would they have detected these field defects and generated warnings that would direct programmers to fix them?
to answer these questions we perform an empirical study that investi gates the effectiveness of state of the art static bug findi ng tools on hundreds of reported and fixed defects extracted from three open source programs lucene rhino and aspectj.
our study addresses the question to what extent could field defects be found and detected by state of the ar t static bug finding tools?
different from past studies that are concerned with the numbers of false positives produced by such tools we address an orthogonal issue on the numbers of false negatives.
we find that although many field defects could be detected by static bug finding tools a substantial proportion of defects could not be flagged.
we also analyze the types of tool warnings that are more effective in finding field defects and characterize the types of missed defects.
categories and subject descriptors d. .
debugging aids testing tools general terms experimentation measurement reliability keywords static bug finding tools field defects false negatives .
introduction bugs are prevalent in many software systems.
the national institute of standards and technology nist has permission to make digital or hard copies of all or part of thi s work for personal or classroom use is granted without fee provided th at copies are not made or distributed for profit or commercial advantage an d that copies bear this notice and the full citation on the first page.
to cop y otherwise to republish to post on servers or to redistribute to lists re quires prior specific permission and or a fee.
ase september essen germany copyright acm ... .
.estimated that bugs cost the us economy billions of dollars annually .
bugs are not merely economically harmful they can also harm life property when mission critical systems malfunction.
clearly techniques that can detect and reduce bugs would be very beneficial.
to achieve this goal many static analysis tools have been proposed to find bugs.
static bug finding tools such as findbugs jlint and pmd have been shown to be helpful in detecting many bugs even in mature software .
it is thus reasonable to believe that such tools are a useful adjunct to other bug finding techniques such as testing and inspection.
although static bug finding tools are effective in some settings it is unclear whether the warnings that they generate are really useful.
two issues are particularly important to be addressed first many warnings need to correspond to actual defects that would be experienced and reported by users.
second many actual defects should be captured by the generated warnings.
for the first issue there have been a number of studies showing that the numbers of false warnings or false positives are too many and some have proposed techniques to prioritize warnings .
while the first issue has received much attention the second issue has received less.
many papers on bug detection tools just report the number of defects that they can detect.
it is unclear how many defects are missed by these bug detection tools.
while the first issue is concerned with false positives the second focuses on false negatives.
we argue that both issues deserve equal attention as both have impact on the quality of software systems.
if false positives are no t satisfactorily addressed this would make bug finding tools unusable.
if false negatives are not satisfactorily addres sed the impact of these tools on software quality would be minimal.
on mission critical systems false negatives may even deserve more attention.
thus there is a need to investigate the false negative rates of such tools on actual field defects .
our study tries to fill this research gap by answering the following research question and we use the term bug and defect interchangeably both of which refer to errors or flaws in a software to what extent could state of the art static bug finding tools detect field defects?
to investigate this research question we make use of abundant data available in bug tracking systems and software repositories.
bug tracking systems such as bugzilla or ji ra record descriptions of bugs that are actually experienced a nd reported by users.
software repositories contain informat ion on what code elements get changed removed or added atpermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
ase september essen germany copyright acm ... .
different periods of time.
such information can be linked together to track bugs and when and how they get fixed.
jira has the capability to link a bug report with the changed code that fixes the bug.
also many techniques have been employed to link bug reports in bugzilla to their correspondin g svn cvs code changes .
these data sources provide us descriptions of actual field defects and their treatments .
based on the descriptions we are able to infer root causes of defects i.e.
the faulty lines of code from the bug treat ments.
to ensure accurate identification of faulty lines of code we perform several iterations of manual inspections to identify lines of code that are responsible for the defect s. then we are able to compare the identified root causes with the lines of code flagged by static bug finding tools and to analyze the proportion of defects that are missed or capture d by the tools.
in this work we perform an exploratory study with three state of the art static bug finding tools findbugs pmd and jlint on three reasonably large open source java programs lucene rhino and aspectj.
lucene rhino and aspectj have and committers respectively1.
we use bugs reported in jira for lucene version .
and the ibugs dataset provided by dallmeier and zimmermann for rhino and aspectj.
our manual analysis identifies real life defects that we can unambiguously locate faulty lines of code.
we find that many of these defects could be detected by findbugs pmd and jlint but a number of them remain undetected.
the main contributions of this work are .
we examine the number of real life defects missed by various static bug finding tools and evaluate the tools performance in terms of their false negative rates.
.
we investigate the warning families in various tools that are effective in detecting actual defects.
.
we characterize actual defects that could not be flagged by the static bug finding tools.
the paper is structured as follows.
in section we present introductory information on various static bug finding tool s. in section we present our experimental methodology.
in section we present our empirical findings and discuss interesting issues.
in section we describe related work.
we conclude with future work in section .
.
bug finding tools in this section we first provide a short survey of different bug finding tools that could be grouped into static dynamic and machine learning based.
we then present the static bug finding tools that we evaluate in this study namely findbugs aspectj and jlint.
.
categorization of bug finding tools many bug finding tools are based on static analysis techniques such as type systems constraint based an alysis model checking abstract interpreta tion or a combination of various techniques .
they often produce various false positives and in theory they should be free of false negatives for the kinds of defects they are designed to detect.
however due to implementation limitations and the fact that a large program often contains defect types that are beyond the designed ca1these are counted based on information available on their respective websites and github on june .pabilities of the tools such tools may still suffer from fals e negatives with respect to all kinds of defects.
in this study we analyze several static bug finding tools that make use of warning patterns for bug detection.
these tools are lightweight and can scale to large programs.
on the downside these tools do not consider the specifications of a system and may miss defects due to specification violations .
other bug finding tools also use dynamic analysis techniques such as dynamic slicing dynamic instrumentation directed random testing and invaria nt detection .
such tools often explore particular par ts of a program and produce no or few false positives.
however they seldom cover all parts of a program they are thus expected to have false negatives.
there are also studies on bug prediction with data mining and machine learning techniques which may have both false positives and negatives.
for example sliwerski et al.
analyze code change patterns that may cause defects.
ostrand et al.
use a regression model to predict defects.
nagappanet al.
apply principal component analysis on the code complexity metrics of commercial software to predict failure prone components.
kim et al.
predict potential faults from bug reports and fix histories.
.
findbugs findbugs was first developed by hovemeyer and pugh .
it statically analyzes java bytecode against various famil ies of warnings characterizing common bugs in many systems.
code matching a set of warning patterns are flagged to the user along with the specific locations of the code.
findbugs comes with a lot of built in warnings.
these include null pointer dereference method not checking for nu ll argument close invoked on a value that is always null tes t for floating point equality and many more.
there are hundreds of warnings these fall under a set of warning families including correctness bad practice malicious code vuln erability multi threaded correctness style internation alization performance risky coding practice etc.
.
jlint jlint developed by artho et al.
is a tool to find defects inconsistent code and problems with synchronizati on in multi threading applications .
similar to findbugs jlint also analyzes java bytecode against a set of warning patterns.
it constructs and checks a lock graph and does data flow analysis.
code fragments matching the warning patterns are flagged and outputted to the user along with their locations.
jlint provides many warnings such as potential deadlocks unsynchronized method implementing runnable interface method finalize not calling super.finalize null refere nce etc.
these warnings are group under three families synchronization inheritance and data flow.
.
pmd pmd developed by copeland i.e.et al.
is a tool that finds defects dead code duplicate code sub optimal code and overcomplicated expressions .
different from findbugs and jlint pmd analyzes java source code rather than java bytecode.
pmd also comes with a set of warning patterns and finds locations in code matching these patterns.
pmd provides many warning patterns such as jumbled incrementer return from finally block class cast exceptio n51with toarray misplaced null check etc.
these warning patterns fall into families such as design strict exceptions clone unused code string and stringbuffer security code etc.
which are referred to as rule sets .
.
methodology we make use of bug tracking version control and stateof the art bug finding tools.
first we extract bugs and the faulty lines of code that are responsible for them.
next we run bug finding tools for the various program releases before the bugs get fixed.
finally we compare warnings given by bug finding tools and the real bugs.
.
extraction of faulty lines of code we analyze two common configurations of bug tracking systems and code repositories to get historically faulty li nes of code.
one configuration is the combination of cvs as the source control repository and bugzilla as the bug tracking system.
another configuration is the combination of git as the source control repository and jira as the bug tracking system.
we describe how these two configurations could be analyzed to extract root causes of fixed defects.
data extraction cvs with bugzilla.
for the first configuration dallmeier and zimmermann have proposed an approach to automatically analyze cvs and bugzilla to link information .
their approach is able to extract bug reports linked to corresponding cvs entries that fix the corresponding bugs.
they are also able to download the code before and after the bug fix.
the code to perform this has been publicly released for several software systems.
as our focus is on defects we remove bug reports that are marked asenhancements i.e.
they are new feature requests and the cvs commits that correspond to them.
data extraction git cum jira.
git and jira have features that make it preferable over cvs and bugzilla.
jira bug tracking systems explicitly links bug reports to the revisions that fix the corresponding defects.
from these fixes we use git diff to find the location of the buggy code and we download the revision prior to the fix by appending the symbol to the hashcode of the corresponding fix revision number.
again we remove bug reports and git commits that are marked as enhancements .
identification of faulty lines.
the above process gives us a set of real life defects along with the set of changes tha t fix them.
to find the corresponding root causes we perform a manual process based on the treatments of the defects.
kawrykow and robillard have proposed an approach to remove non essential changes and convert dirty treatments to clean treatments.
however they still do not recover the root causes of defects.
our process for locating root causes could not be easily automated by a simple diffoperation between two versions of the systems after and prior to the fix due to the following reasons.
first not all changes fix the bug some such as addition of new lines removal of new lines changes in indentations etc.
are only cosmetic changes that make the code aesthetically better.
figure shows such an example.
second even if all changes are essential it is not straightforward to identify the defective lines from the fix es.
some fixes introduce additional code and we need to find the corresponding faulty lines that are fixed by the additional code.
we show several examples highlighting the processof extracting root causes from their treatments for a simple and a slightly more complicated case in figures .
figure describes a bug fixing activity where one line of code is changed by modifying the operator to !
.
it is easy for us to identify the faulty line which is the line that gets changed.
a more complicated case is shown in figure .
there are four sets of faulty lines that could be inferred from the diff one is line marked with where an unnecessary method call needs to be removed a similar fault is at line the third set of faulty lines are at lines and they are condition checks that should be removed the fourth one is at line and it misses a pre condition check.
for this case the diffis much larger than the faulty lines that are manually identified.
figure illustrates the difficulties in automating the identification of faulty lines.
to ensure the fidelity of identifie d root causes we perform several iterations of manual inspec tions.
for some ambiguous cases several of the authors discussed and came to resolutions.
some cases that are still deemed ambiguous i.e.
it is unclear or difficult to manually locate the faulty lines are removed from this empirica l study.
we show such an example in figure .
this example shows a bug fixing activity where an if block is inserted into the code and it is unclear which lines are really faulty.
at the end of the above process we get the sets of faulty lines faulty for all defects in our collection.
notationwise we refer to these sets of lines using an index notation faulty .
we refer to the set of lines in faulty that correspond to the ithdefect as faulty .
.
extraction of warnings to evaluate the static bug finding tools we run the tools on the program versions before the defects get fixed.
an ideal bug finding tool would recover the faulty lines of the program possibly with other lines corresponding to other defects lying within the software system.
some of these warnings are false positives while others are true positiv es.
for each defect we extract the version of the program in the repository prior to the bug fix.
we have such information already as an intermediate result for the root cause extraction process.
we then run the bug finding tools on these program versions.
because .
of these versions could not be compiled we remove them from our analysis which is a threat to validity of our study see section .
.
as described in section each of the bug finding tools takes in a set of rules or the types of defects and flags them if found.
by default we enable all rules types of defects available in the tools except that we exclude two rule sets from pmd one related to android development and the other whose xml configuration file could not be read by pmd i.e.
coupling .
each run would produce a set of warnings.
each warning flags a set of lines of code.
notation wise we refer to the sets of lines of code for all runs as warning and the set of lines of code for the ithwarning as warning .
also we refer to the sets of lines of code for all runs of a particular tooltaswarning t and similarly we have warning t .
.
extraction of missed defects with the faulty lines faulty obtained through manual analysis and the lines flagged by the static bug finding tools warning we can look for false negatives i.e.
actual reported and fixed defects that are missed by the tools.52aspectj file name org.aspectj modules org.aspectj.
ajdt.core src org aspectj ajdt internal compiler as t thisjoinpointvisitor.java buggy version fixed version line line insert a line between line and in the buggy v ersion line system.err.println done method line system.err.println isref expr binding line is deleted line else if isref ref thisjoinpointstaticpart dec line else if isref ref thisjoinpointst aticpartdec figure example of simple cosmetic changes lucene .
file name src java org apache lucene se arch scorer.java buggy version fixed version line return doc no more docs line r eturn doc !
no more docs figure identification of root causes faulty lines from t reatments aspectj file name org.aspectj modules weaver src o rg aspectj weaver bcel lazymethodgen.java buggy version fixed version line lng.setstart null line is deleted line lng.setend null line is deleted line if i instanceof localvariableinstr uction line int index localvariableinstr uction i .getindex line if lvt.getslot index line if localvariablestarts.get lv t null line localvariablestarts.put lvt jh line line localvariableends.put lvt jh line line line if localvariablestarts.get lvt null line localvariablestarts.put lvt jh line line localvariableends.put lvt jh line keys.addall localvariablestarts.keyset line collections.sort keys new comparator line public int compare object a object b line localvariabletag taga localvariab letag a line localvariabletag tagb localvariab letag b line return taga.getname .compareto t agb.getname line line for iterator iter keys.iterator it er.hasnext line keys.addall localvariablestarts.keyset line these lines are commented codes line collections.sort keys new comparator line public int compare object a object b line localvariabletag taga localvari abletag a line localvariabletag tagb localvari abletag b line if taga.getname .startswith a rg line if tagb.getname .startswit h arg line return taga.getname .co mpareto tagb.getname line else line return whatever tag b is it must come out before arg line line else if tagb.getname .starts with arg line return whatever ta ga is it must come out before arg line else line return taga.getname .c ompareto tagb.getname line line line line these lines are commented codes line for iterator iter keys.iterator iter.
hasnext figure identification of root causes faulty lines from t reatments aspectj org.aspectj modules weaver src org aspectj weaver patterns signaturepattern.java buggy version fixed version line line if !modifiers.matches sig.getmodifiers return false line line if kind member.static initialization insert these lines between line 87and in th e buggy version line if kind member.advice return true line new line figure identification of root causes faulty lines from t reatments to get these missed defects for every ithwarning we take the intersection of the sets faulty and warning from all bug finding tools and the intersection between faulty with each warning t .
if an intersection is an empty set we say that the corresponding bug finding tool misses theithdefect.
if the intersection covers a true subset of the lines in faulty we say that the bug findingtool partially captures theithdefect.
otherwise if the intersection covers all lines in faulty we say that the bug finding tool fully captures theithdefect.
we differentiate the partial and full cases as developers might be able to recover the other faulty lines given that some of the faulty lines have been flagged.
.
overall approach our overall approach is illustrated by the pseudocode in figure .
our approach takes in a bug repository e.g.
bugzilla or jira a code repository e.g.
cvs or git and a bug finding tool e.g.
findbugs pmd or jlint .
for each bug report in the repository it performs three steps mentioned in previous sub sections faulty lines extracti on warning identification and missed defect detection.
the first step corresponds to lines .
we find the bug fix commit corresponding to the bug report.
we identify the version prior to the bug fix commit.
we perform a diff to find the differences between these two versions.
faulty lines are then extracted by a manual analysis.
the second step corresponds to lines .
here we simply run the bug finding tools and collect lines of code flagged by the various warnings.
finally step three is performed by lines .
here we detect cases where the bug finding tool misses partially captures or fully captures a defect.
the final statistics is output at line .
procedure identifymisseddefects inputs bugrepo bug repository coderepo code repository bftool bug finding tool output statistics of defects that are missed and captured fully or partially method let stats for each bug report brinbugrepo step extract faulty lines of code let fixc br s corresponding fix commit in coderepo let bugc revision before fixc incoderepo let diff the difference between fixc andbugc extract faulty lines from diff let faulty br faulty lines in bugc step get warnings run bftool onbugc let warning br flagged lines in bugc bybftool step detect missed defects let common f aulty br w arning br if common add an bracketle tbr miss an bracketri httostats else if common faulty br add an bracketle tbr full an bracketri httostats else add an bracketle tbr partial an bracketri httostats output stats figure identification of missed defects.
.
empirical ev aluation in this section we present our research questions datasets empirical findings and threats to validity.
.
research question dataset we would like to answer the following research questions rq1 how many real life reported and fixed defects from lucene rhino and aspectj are missed by state of the art static bug finding tools?
rq2 what types of warnings reported by the tools are most effective in detecting actual defects?
rq3 what are some characteristics of the defects missed by the tools?
we evaluate three static bug finding tools namely findbugs jlint and pmd on three open source projects lucene rhino and aspectj.
lucene is a general purpose text search engine library .
rhino is an implementation of javascrip t written in java .
aspectj is an aspect oriented extensio n of java .
the average sizes of lucene rhino and aspectj are around and lines of code loc respectively.
we crawl jira for defects tagged for lucene version .
.
for rhino and aspectj we analyze the ibugs repository prepared by dallmeier and zimmermann .
we show the numbers of unambiguous defects that we are able to manually locate root causes from the three datasets in table together with the total numbers of defects available in the datasets and the average faulty lines per defect.
table number of defects for various datasets dataset of of avg of unambiguous defects faulty lines defects per defect lucene .
rhino .
aspectj .
.
experimental results we answer the three research questions as follows.
.
.
rq1 number of missed defects we show the number of missed defects by each and all of the three tools for lucene rhino and aspectj in table .
we do not summarize across software projects as the numbers of warnings for different projects differ greatly and any kind of summarization across projects may not be meaningful.
we first show the results for all defects and then zoom into subsets of the defects that span a small number of lines of code and those that are severe.
our goal is to evaluate the effectiveness of state of the art static bug finding too ls in terms of their false negative rates.
we would also like to evaluate whether false negative rates may be reduced if we use all three tools together.
.
.
.
all defects.
lucene.
for lucene as shown in table we find that with all three tools .
of all defects could be fully identifie d i.e.
all faulty lines faulty of a defect iare flagged .
an addition of .
of the defects could also be partially identified i.e.
some but not all faulty lines in faulty are flagged .
still of the defects could not be flagged by the tools.
thus the three tools are effective but are not very successful in capturing lucene defects.
among the tools pmd captures the most numbers of bugs followed by findbugs and finally jlint.
rhino.
for rhino as shown in table we find that with all three tools of all defects could be fully identified.
only of the defects could not be captured by the tools.
thus the tools are very effective in capturing rhino defects .
among the tools findbugs captures the most numbers of defects followed by pmd and finally jlint.
aspectj.
for aspectj as shown in table we find that with all three tools .
of all defects could be fully captured .
also another .
of the defects could be partially captured.
only .
of the defects could not be captured by any of the tools.
thus the tools are very effective in cap 54table percentages of defects that are missed partially c aptured and fully captured.
the numbers in the parentheses indicate the numbers of actual faulty lines captured if any.
tools vs. lucene rhino aspectj programs miss partial full miss partial full miss partial full findbugs .
.
.
.
.
.
.
.
jlint .
.
.
.
.
.
.
.
.
pmd .
.
.
.
.
.
.
.
.
all .
.
.
.
.
.
.
.
turing aspectj defects.
pmd captures the most numbers of defects followed by findbugs and finally jlint.
also to compare the bug finding tools we show the average number of lines in one defect program version over all defects that are flagged by the tools for our subject programs in table .
we note that pmd flags the most numbers of lines of code and likely produces more false positives th an others while jlint flags the least numbers of lines of code.
with respect to the average sizes of programs see column avg loc in table the tools may flag .
to .
of the whole program as buggy.
table average numbers of lines flagged per defect version by various static bug finding tools.
tools vs. programslucene rhino aspectj findbugs .
.
.
jlint .
.
.
pmd .
.
.
all .
.
note that there may be many other issues in a version besides the defect in our dataset in a program version and a tool may generate many warnings for the version.
these partially explain the high average numbers of lines flagged i n table .
to further understand these numbers we present more statistics about the warnings generated by the tools for each of the three programs in table .
column avg warning provides the average number of reported warnings.
column avg flagged line per warning is the average number of lines flagged by a warning.
column avg unfixed is the average number of lines of code that are flagged but are not buggy lines fixed in a version.
column avg loc is the number of lines of code in a particular software system across all buggy versions .
we notice that the average number of warnings generated by pmd is high.
findbugs reports less warnings but many warnings span many consecutive lines of code.
many flagged lines do not correspond to the buggy lines fixed in a version.
these could either be false positives or bugs found in the future.
we do not check the exact number of false positives though as they require much manual labor i.e.
checking thousands of warnings in each of the hundreds of program versions and they are the subject of other studies e.g.
.
on the other hand the high numbers of flagged lines raise concerns with the effectiveness of warnings generated by the tools are the warnings effectively correlated with actual defects ?
to answer such a question we create random bug finding tools that would randomly flag some lines of code as buggy according to the distributions of the number of the lines flagged by each warning generated by each of the three bug finding tools and compare the bug capturing effectiveness of such random tools with the actual tools.
for each version of the subject programs and each of the actual tools we run arandom tool times each time the random tool would randomly generate the same number of warnings by following the distribution of the numbers of lines flagged by each warning then we count the numbers of missed partially captured and fully captured defects by the random tool finally we compare the effectiveness of the random tools with the actual tools by calculating the p values as in table .
a value xin each cell in the table means that our random tool would have x chance to get at least as good results as the actual tool for either partially or fully capturing th e bugs.
the values in the table imply the actual tools may indeed detect more bugs than random tools although they may produce many false positives.
however some tools for some programs such as pmd for lucene may not be much better than random tools.
if there is no correlation between the warnings generated by the actual tools with actual defects the tools should not perform differently from the random tools.
our results show that this is not the case at least for some tools with some programs.
table p values among random and actual tools.
tool program full partial or full findbugslucene .
.
rhino .
.
aspectj .
.
jlintlucene .
.
rhino .
.
aspectj .
.
pmdlucene .
rhino .
aspectj .
.
.
.
.
localized defects.
many of the defects that we analyze span more than a few lines of code.
we further focus only on defects that can be localized to a few lines of code at most five lines of code which we call localized defects and investigate the effectiveness of the various bug finding tools.
we show the numbers of missed localized defects by the three tools for lucene rhino and aspectj in tables .
lucene.
table shows that the tools together fully identify .
of all localized defects and partially identify anoth er .
.
this is only slightly higher than the percentage for all defects see table .
the ordering of the tools based on their ability to fully capture localized defects is the same .
rhino.
as shown in table all three tools together could fully identify .
of all localized defects.
this is sligh tly lower than the percentage for all defects see table .
aspectj.
table shows that the tools together fully capture .
of all localized defects and miss only .
.
these are better than the percentages for all defects see table .55table unfixed warnings by various defect finding tools software tool avg warning avg flagged line per warning avg unfixed avg loc rhinofindbugs .
.
.
.8jlint .
pmd .
.
.
all .
.
.
lucenefindbugs .
.
.
.75jlint .
.
pmd .
.
.
all .
.
.
aspectjfindbugs .
.
.
.94jlint .
pmd .
.
.
all .
.
.
table percentages of localized defects that are missed p artially captured and fully captured tools vs. programslucene rhino aspectj miss partial full miss partial full miss partial full findbugs .
.
.
.
.
.
.
.
jlint .
.
.
.
.
.
.
.
pmd .
.
.
.
.
.
.
.
all .
.
.
.
.
.
.
.
.
.
.
severe defects.
different defects are often labeled with different severity levels.
we further focus only on defects at severe levels blocker critical and major which we collectively call severe defects and investigate the effectiveness of the various bug finding tools on detecting severe defects.
we show the numbers of missed severe defects by the three tools for lucene rhino and aspectj in table .
lucene.
as shown in table all three tools together could fully identify .
of all severe bugs.
this is higher than the percentage for all bugs see table .
considering sever e defects in lucene only findbugs is equally good as jlint.
rhino.
as shown in table all three tools together could fully identify all severe defects.
note that although the pe rcentage is very good there are only two severe defects in the rhino dataset and jlint misses both of these two defects.
aspectj.
as shown in table all three tools together could fully capture .
of all severe defects and only miss .
.
the relative performance of the three tools is the same as the relative performance for all defects see table pmd followed by findbugs followed by jlint.
jlint could not fully capture any severe defect in the aspectj dataset.
.
.
.
stricter defects.
we also perform a stricter analysis that requires not only that the faulty lines are covered by the warnings but also the type of the warnings must be directly related to the faulty lines.
for example if the faulty line is a null pointe r dereference then the warning must explicitly say so.
again we manually analyze to see if the warnings strictly captures the defect.
we focus on defects that are localized to one line of code due to limited manpower.
there are bugs bugs and bugs for lucene rhino and aspectj respectively that can be localized to one line of code.
we show the results of our analysis for lucene rhino and aspectj in table .
we notice that under this stricter requirement very few of the defects fully captured by thetools see column full are strictly captured by the same tools see column strict .
thus although the faulty line s might be flagged by the tools the warning messages from the tools may not have sufficient information for the developers to understand the defects.
table numbers of one line defects that are strictly captured versus fully captured tool lucene rhino aspectj strict full strict full strict full findbugs pmd jlint .
.
rq2 effectiveness of different warnings we show the effectiveness of various warning families of findbugs pmd and jlint in flagging defects in tables and respectively.
we highlight the top warning families in terms of their ability in fully capturing the roo t causes of the defects.
findbugs.
for findbugs as shown in table in terms of low false negative rates we find that the best warning families are style performance malicious code bad practice and correctness.
warnings in the style category include switch statement having one case branch to fall through to the next case branch switch statement having no default case assignment to a local variable which is never used unread public or protected field a referenced variable contai ns null value etc.
warnings belonging to the malicious code category include a class attribute should be made final a class attribute should be package protected etc.
furthermore we find that a violation of each of these warnings would likely flag an entire class that violates it.
thus a lot of lines of code would be flagged making it having a higher chance of capturing the defects.
warnings belonging to the bad practice category include comparison of string ob jects using or !
a method ignores exceptional return value etc.
performance related warnings include method56table percentages of severe defects that are missed part ially captured and fully captured tools vs. programslucene rhino aspectj miss partial full miss partial full miss partial full findbugs .
.
.
.
.
.
jlint .
.
.
.
.
pmd .
.
.
.
.
.
.
all .
.
.
.
.
.
.
concatenates strings using instead of stringbuffer etc.
correctness related warnings include a field that masks a superclass s field invocation of tostring to an array etc.
table percentages of defects that are missed partially captured and fully captured for different warning families of findbugs warning family miss partial full style .
.
.
performance .
.
.
malicious code .
.
.
bad practise .
.
.
correctness .
.
.
pmd.
for pmd as shown in table we find that the warning categories that are the most effective are code size design controversial optimization and naming.
co de size warnings include problems related to a code being too large or complex e.g.
number of acyclic execution paths is more than method length is long etc.
code size is correlated with defects but does not really inform the types of defects that needs a fix.
design warnings identify suboptimal code implementation these include the simplification o f boolean return missing default case in a switch statement deeply nested if statement etc which may not be correlated with defects.
controversial warnings include unnecessary constructor null assignment assignments in operands et c. optimization warnings include best practices to improve th e efficiency of the code.
naming warnings include rules pertaining to the preferred names of various program elements.
table percentages of warnings that are missed partially captured and fully captured for different warning families of pmd warning family miss partial full code size .
.
.
design .
.
.
controversial .
.
.
optimization .
.
.
naming .
.
.
jlint.
for jlint as shown in table we have three categories inheritance synchronization and data flow.
we find that inheritance is more effective than data flow which in turn is more effective than synchronization in detecting defects.
inheritance warnings relate to class inheritance issues such as new method in a sub class is created with identical name but different parameters as one inherited from the super class etc.
synchronization warnings relate to er roneous multi threading applications in particular probl ems related to conflicts on shared data usage by multiple threads such as potential deadlocks required but missing synchronized keywords etc.
data flow warnings relate to problems that jlint detects by performing data flow analysis on javabytecode such as null referenced variable type cast misus e comparison of string with object