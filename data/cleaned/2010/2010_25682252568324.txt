a critical review of automatic patch generation learned from human written patches essay on the problem statement and the evaluation of automatic software repair martin monperrus university of lille inria france martin.monperrus univ lille1.fr abstract at icse there was the rst session ever dedicated to automatic program repair.
in this session kim et al.
presented par a novel template based approach for xing java bugs.
we strongly disagree with key points of this paper.
our critical review has two goals.
first we aim at explaining why we disagree with kim and colleagues and why the reasons behind this disagreement are important for research on automatic software repair in general.
second we aim at contributing to the eld with a clari cation of the essential ideas behind automatic software repair.
in particular we discuss the main evaluation criteria of automatic software repair understandability correctness and completeness.
we show that depending on how one sets up the repair scenario the evaluation goals may be contradictory.
eventually we discuss the nature of x acceptability and its relation to the notion of software correctness.
categories and subject descriptors d. .
testing and debugging general terms veri cation reliability experimentation keywords bugs faults error recovery automatic patch generation automatic program xing automatic software repair .
introduction what one would like ideally is the automatic detection and correction of bugs r. j. abbott permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
icse may june hyderabad india copyright acm ... .
.the automatic detection of bugs has been a vast research eld for decades with a large spectrum of static and dynamic techniques.
active research on the automatic repair1 of bugs is more recent.
a seminal line of research started in with the genprog system and at the international conference on software engineering there was the rst session ever dedicated to automatic program repair.
the par system was presented there it is an approach for automatically xing bugs of java code.
the repair problem statement is the same as genprog given a test suite with at least one failing test generate a patch that makes all test cases passing .
par introduces a new technique to x bugs based on templates.
each of par s ten repair templates represents a common way to x a common kind of bug.
for instance a common bug is the access to a null pointer and a common x of this bug is to add a nullness check just before the undesired access this is template null pointer checker .
we strongly disagree with kim et al.
s paper on par.
this is our motivation to present this critical review of their work.
we think that a respectful critical reading and debate is very important in the scienti c process.
this usually happens within technical contributions e.g.
in the related work section but this is often shallow and biased towards the new approach.
explicit criticism may sometimes be better to clearly see the opposing ideas in the rst place see for instance the strong opposition in a debate on teaching computer science in the communications of the acm .
our critical review has two goals.
first we aim at explaining why we disagree with kim and colleagues and why the reasons behind this disagreement are important for automatic software repair in general.
second we aim at contributing to the eld with a clari cation of the essential ideas behind automatic software repair.
we will discuss neither the motivation of automatic software repair we extensively work on automatic repair nor the technical contribution we believe in the e ciency of templates for xing certain bugs and the soundness of par s ones .
we start by discussing the concept of defect class which is missing in par s paper.
we show that ignoring this concept has serious consequences on the conclusiveness of evaluation protocols in general and par s one in particular.
we propose three dimensions for de ning a defect class the root cause the symptom and the kind of x. this is broad 1or correction xing patching .
.
.permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
icse may june hyderabad india copyright acm ... .
enough to revisit in the related work recent papers on automatic repair.
then we will elaborate on the problem statement of automatic repair.
beyond the canonical problem statement which we call test suite based program repair we show that there are diverse repair problems depending on whether repair happens online at runtime or o ine at maintenance time and whether repair happens on the state of programs or on the behavior expressed in their code.
naturally this diversity of problem statements leads to a diversity of evaluation criteria.
for test suite based program repair we emphasize on the need to characterize the inner quality of test suites.
then depending on whether one considers software repair as fully automatic code synthesis or as a recommendation system problem there are di erent evaluation criteria to be considered.
those evaluation criteria may even be contradictory.
for instance bug x recommendation systems are expected to synthesize understandable patches but are allowed to provide partial solutions.
fully automatic repair systems have no understandability constraints but should always yield a executable solution.
eventually we explore the foundations of the x acceptability question whether a x is more acceptable than another .
through a thought experiment we claim that under certain circumstances the question is patch a more acceptable than patch b?
is fundamentally unanswerable.
we nish the discussion by revisiting the relation between repair and software correctness.
to sum up we contribute to the eld of automatic repair with new perspectives using kim et al.
s paper as starting point.
our contributions are the highlighting of pitfalls in automatic software repair research and the uttermost importance of explicit defect classes the presentation of kinds of software repair and their characteristics in particular state repair and behavioral repair the identi cation of important and sometimes contradictory evaluation criteria in the eld understandability correctness completeness.
the paper reads as follows.
section gives some background information on automatic software repair and par.
section is the critical review per se it exposes what we think is wrong in kim et al.
s paper.
section elaborates on the problem statement of automatic software repair.
section discusses the evaluation of automatic software repair techniques and presents a thought experiment on the notion of x acceptability .
section discusses the related work.
section concludes this paper.
.
background before discussing in details kim et al.
s approach called par let us rst brie y discuss what we mean by automatic software repair and then how par works.
.
automatic software repair automatic software repair is the process of xing software bugs automatically.
this de nition seems broad enough toencompass diverse approaches from di erent elds of computer science and engineering.
in particular it accepts different notions of xing e.g.
on source code on binary code on execution data and di erent kinds of bugs independently of the severity and the type of oracle that asserts the presence of the bug .
in a nutshell automatic software repair consists of several overlapping phases failure detection something wrong has happened bug diagnosis why this has happened fault localization what the root cause is where the initial faulty module statement is repair inference what should be done to x the problem .
for instance an archetypal automatic software repair system takes a bug report as input and outputs a source code patch that xes the bug.
it closely relates to what is called automatic debugging the ultimate goal of debugging is to x bugs and to that extent automatic software repair is perfect automatic debugging.
however as said above the term automatic software repair is broader in scope than automatic debugging in the sense that debugging connotes more the diagnosis phase of repair than the combination of the aforementioned phases.
automatic software repair relates to the traditional eld of software dependability and in its terminology it spans both fault tolerance and fault removal.
indeed automatic software repair can bene t from the clear de nitions of error failure and fault coming from dependable computing because the term bug is vague and refers to those three concepts indistinctly.
automatic software repair also much relates to software testing w.r.t.
the techniques used to detect and diagnose the bugs to program synthesis when it comes to create a correct piece of code and to data mining and machine learning for software engineering when the repair knowledge is extracted from existing data version control systems execution traces etc.
.
this list of related elds is not meant to be exhaustive.
.
par template based software repair par is an automatic software repair technique invented by kim and colleagues and presented at the international conference on software engineering .
as previous work it states the problem of automatic repair as follows a bug is detected by a failing test case and the goal is to change the application code so that rst the failing test case now passes and second the other test cases forming a test suite still pass.
to do so par uses an existing fault localization technique and for each suspicious statement it tries di erent repair templates.
if the application of a repair template makes the test suite passing the bug is considered to be xed.
each of the ten repair templates represents a common way to x a common kind of bug.
for instance a common bug is the access to a null pointer and a common x of this bug is to add a nullness check just before the undesired access template null pointer checker .
some of par s templates use the intrinsic redundancy of software and pick some code elsewhere in the program under repair to x the bug .
.
a critical review of par in our group we extensively work on automatic software repair .
in the last months we have spent much time in studying kim et al.
s paper on par and its235table defect classes can be de ned along several dimensions.
automatic repair approaches can be compared only if they address similar defect classes.
defect class according to.
.
.
examples of defect class the root cause incorrect variable initialization incorrect con guration .
.
.
the symptom segmentation faults null pointer exceptions memory exhaustion .
.
.
the x adding an input check changing a method call restoring an invariant .
.
.
dataset.
over weeks we have isolated two points on which we strongly disagree.
beyond par only this discussion aims at being insightful for automatic software repair as a whole.
.
what are the addressed defect classes?
by defect class we mean a family of bugs that have something in common we equate bug and defect the former being the colloquial name for the latter .
we have an open understanding of in common they can share the same root cause e.g.
a programmer mistake the same symptom e.g.. an exception or the same kind of x e.g.
changing the conditional of an if expression .
table gives examples of such de ned defect classes.
there is often a relation between the root cause and the kind of kind.
for instance an incorrect initialization can obviously be xed changing the initialization.
but it can be also be xed by inserting an assignment later.
there can be several di erent x locations e.g.
at line or at line and kinds for the same root cause e.g.
changing an assignment or changing a return value .
the core of par is a collection of templates.
what are the addressed defect classes?
par doesn t address or clearly identify one or several defect classes.
by looking at the template names and descriptions they seem to address many di erent classes a null pointer bug is of di erent nature compared to a incorrect cast bug .
there is no apparent principle behind the collection of templates.
as far as we understand the templates seem to have been collected by browsing bug xes and see whether they would t in their overall approach.
why is this ad hoc approach a problem?
to us a contribution on automatic software repair should answer the following questions for which defect class does it work?
this names the enemy and enables the community to answer the related questions what are the repairable defect classes why is a defect class easy hard to repair?
identifying a defect class in automatic repair is as important as de ning the fault model of a testing technique2.
identifying the target defect classes enables the community to answer the questions what are the repairable defect classes why is a defect class easy or hard to repair?
these are academic questions.
the real impact of automatic repair on practitioners is founded are two other questions what s the abundance of this defect class?
what is its criticality?
automatically xing a common defect class would save a lot of maintenance resources xing critical crashes would save a lot of production loss.
on the contrary the automatic repair of minor and rare bugs would only be 2our goal is not to articulate those two terms.
for us a fault model consists of several defect classes and may identi y some relations between the root causes and the symptoms.of academic interest.
the lack of explicit defect classes in par s paper hinders the answering to all those questions.
note that the seminal paper on automatic repair by weimer and colleague genprog does not explicitly address those questions as well.
however it was a paper opening a new eld of research.
but we note that in the conference presentation3and in subsequent papers the notion of defect class is clearly present and hints that genprog works best for manipulating defensive code against memory errors in particular segmentation faults and bu er overruns .
.
how conclusive is the evaluation?
let us now discuss the conclusiveness of par s evaluation .
using real bugs they evaluate whether par xes more bugs than genprog and whether the generated xes are of better quality.
we think that the experimental methodology has several issues.
note that we are not saying that their approach does not work actually we think that their ast template based approach may well address some common bugs .
we are only saying that the conclusiveness of the evaluation as presented is questionable.
.
.
on the relation between the dataset creation methodology and conclusiveness the rst part of the evaluation is about the number of xed bugs.
in par s paper there are templates and in the presented evaluation par is able to x bugs.
what is the distribution of xed bugs by template?
mathematically the templates and their associated defect class are evaluated on average on or bug instances .
we asked the authors about this distribution i.e.
which template xes which bug id?
it has been lost.
according to our replication experiments with the xed bugs it actually seems that most bugs are xed by the same templates in particular null pointer checker and expression adder remover replacer and that others e.g.
class cast checker only x one bug.
many empirical evaluations including some of ours are biased or over conclusive with the proposed approach.
the bias often lies in the way the datasets are constructed.
however beyond the magnitude of a validation the dataset construction can impact the internal validity.
do par and genprog address the same defect classes?
if no this raises doubts about the conclusiveness of the results.
let s assume that par address defect classes a and b and genprog defect classes b and c. in this case depending on how one builds the dataset the results would be totally different for instance a dataset a b would much favor par.
this fallacy is partly due to the absence of the concept of defect class.
in par s paper there is no presentation on how the dataset was built and no characterization 3slide of papers weimer icse2009 genprog presentation.pdf236of the kind of bugs it contains.
to address this fallacy one needs to characterize how the dataset is built and what it contains.
the way one builds evaluation datasets for automatic repair has a great impact on the conclusiveness of the results.
finally is par better than genprog?
according to our arguments as long as one does not clarify the underlying defect classes and build a well formed dataset we do not know.
if par and genprog are proven to address di erent defect classes the question can be considered as ill formed.
the question that then arises is how to build a valid evaluation dataset for automatic repair?
at this point in the life of the research eld there is no de nitive answer.
however we tend to think that having a explicit target defect class is again a key to answering this question.
the dataset should contain only bugs from the same defect class.
within a defect class the dataset sampling should be strati ed from easy bugs to complex ones.
with such a dataset subsequent approaches on the same defect class can be meaningfully compared.
having yet un xed complex bugs in the dataset would even foster creativity in other teams who would invent new ways to x them in an automated manner.
.
.
on the meaning of evaluating patch acceptability let us now examine the second evaluation question on whether the synthesized patches are of better quality compared to ones synthesized with an other approach or compared to the real ones.
the experimental protocol consists of asking developers to blindly assess synthesized patches.
they are also asked to assess the original human written patch.
the experiment says it evaluates patch acceptability .
the subjects of the user study are students and developers of local companies.
none of them are developers of the software packages for which bugs are xed.
as a result of this evaluation it is claimed that par generates more acceptable patches than genprog does .
what are the underlying assumptions of this experiment?
first it is that a developer is able to rate the quality of a patch without any knowledge about the codebase and domain of the bug.
within reasonable time in the experiment say minutes per bug we think it is hardly possible.
for many bugs understanding the bug report itself understanding the causality chain4 and understanding whether the patch is correct are all di cult tasks.
given only the patch and an hyperlink to the bug report this analysis on an unknown codebase seems really hard.
understanding the inner quality of a patch requires far more domain speci c knowledge than the subjects of the user study have.
to us the subjects of the experiment do not rate the inner quality of the patch but more on whether the code looks good or not kim et al.
use is acceptable but as explained above to us we think that looks good ts more the reality of the experiment .
4this often requires executing the program and not only looking at the program as done in this setup.evaluating the inner quality of a patch requires a thorough process involving understanding the bug report itself understanding the causality chain of the bug and understanding the potential consequences and sidee ects of the patch.
.
on the problem statement of automatic software repair now let us step back and discuss the core problem statement of the eld of automatic software repair.
.
on patch prettiness versus alien code as said above and especially in section .
.
to our understanding par s evaluation implicitly reformulates the automatic repair problem statement as generate a patch that makes the test suite passing and that looks good .
beyond looks good we would say looks like humanly written good code .
automatic software repair is one branch of code synthesis according to the literature and our experience code synthesis often generates surprising code a kind of alien code.
this is normal since the processes to create this are completely di erent biological versus arti cial .
let us dwell on this.
automatic software repair is about xing bugs automatically it is not about xing bugs as humans are used to .
we should not be afraid of alien ways of reasoning on and modifying programs alien in the sense of fundamentally different .
par s implicit reformulation of the problem statement of automatic repair puts up barriers on the way we x bugs and on the way we design automatic bug xing techniques.
we should not only aim at techniques that mimic human bug xing.
we should not be afraid of alien ways of writing code.
.
kinds of software repair more generally the problem statements of automatic software repair are not yet clearly identi ed.
in the following we rst aim at clarifying the canonical repair problem as stated by weimer et al.
and we call it test suite based program repair section .
.
.
we then broaden the scope of automatic repair and propose to distinguish two families of repair techniques state repair and behavioral repair section .
.
.
.
.
test suite based program repair in both par and genprog the primary problem statement is given a test suite with one failing test generate a patch that makes them all passing .
the failing test case is the oracle for the bug.
the rest of the test suite is the oracle for regression.
this is what can be called test suite based program repair .
in the patch quality experiment of par all the assessed patches pass the whole test suite.
it means that the primary problem statement is solved.
asking users whether a patch is better than another one implicitly breaks the well formedness of the problem statement.
asking the question means that passing the test suite may not be su cient.
many have raised such points since237table two examples of software repair scenarios.
depending on the problem the evaluation criteria of automatic software repair are di erent and the evaluation goals may be contradictory.
facet fully automatic system recommendation system who the repair robot or agent the repair system then the human when mostly at runtime mostly at development and maintenance time longevity solutions may be temporary disposable solutions should sustain time understandability anything goes incl.
alien code solutions must be understandable by humans correctness fully automated procedure pre ltering then human validation completeness solutions must be executable solutions may be partial a human would ll the gaps genprog s break up.
with hindsight we also agree that this is an important weakness of the original problem statement as done by weimer and colleagues.
however as shown above the answer of par yields an implicit reformulation of the automatic repair problem statement as generate a patch that makes the test suite passing and that looks good .
we disagree with it.
we think that a better way to reformulate it is to put the emphasis on assessing the test suite quality to what extent is a test suite good?
does it well specify its domain?
is it appropriate for automatic repair?
if the research community is able to characterize what a good test suite is we can simply clarify the problem statement as follows given a good and trustable test suite generate a patch that makes the test suite passing .
it may happen that the concept of test suite as we understand it today with test cases and assertions will never prove appropriate for automatic repair.
more generally in test suite based program repair there is an important asymmetry between test cases.
one single failing test case is enough to express the bug while the other ones must cover as completely as possible the speci ed behavioral space.
to this extent the point given a good and trustable test suite is simply a concrete instance of the more general problem of being able to characterize the speci cation of a program or software component .
.
.
behavioral repair versus state repair genprog focuses on synthesizing code to x bugs ofine the code being meant to be committed into a version control system.
the visibility and impact of this work tends to associate the term repair with this kind of repair.
however repair has a broader sense than just synthesizing source code.
it may mean repairing a data structure repairing the register values and memory locations etc.
we think that we can actually distinguish two kinds of automatic software repair state repair and behavioral repair.
state repair consists in modifying the program state during the execution the registers the heap the stack etc.
.
demsky and rinard s work on data structure repair is an example of such state repair.
state repair can be seen as a kind of data repair as opposed to code repair in the spirit of the data diversity techniques in fault tolerance .
behavioral repair consists in modifying the program executable code.
according to this de nition synthesizing a source code patch is indeed behavioral repair.
behavioral repair is also relevant on binary code when no source code is available.
behavioral repair can also happen at runtime when one changes the code part of the memory .
to sum up state repair is only online at runtime while behavioral repair can be either online or o ine at maintenance time.
we will come back on this point in section .
the problem statement of automatic software repair can be decomposed in state repair that consists in modifying the program state during the execution and behavioral repair that consists in modifying the program code.
.
on the evaluation of automatic software repair according to the broad de nition given in section .
automatic software repair can be declined in di erent scenarios.
repair involves failure detection bug diagnosis fault localization and repair inference.
even though automatic software repair contains the word automatic it is not reasonable to state that automatic software repair systems are only those that fully automatically cover all those phases.
a system that produces a patch on which a developer would build on to write the nal patch indeed goes in the direction of automatic software repair.
one can actually imagine a broad range of repair scenarios fully automatic repair agents at runtime repair bots taking care of some bug reports and modifying the source code base automatically or repair recommendation systems proposing tentative patches that developers would improve.
as we shall see now depending on the scenario the evaluation criteria change and may even be contradictory.
.
evaluation criteria we see at least three dimensions of evaluation for which the evaluation goals di er depending on the automatic repair scenario.
understandability.
let us consider a repair robot that automatically commits patches to a code base where conventional human based maintenance takes place.
some complex bugs would be unxable by the repair robot and they would consequently be handled by a human developer.
to x those tough bugs she may have to understand a patch previously generated by the repair robot or even a set of superimposed synthesized patches which may be rather di cult.
when human based maintenance and automatic repair are interleaved the generated patches have to be clearly documented and the repair approach could also generate an explanation of the repair.
le goues et al.
refer to this issue as patch maintainability .
on the contrary if the repair happens at runtime as a temporary solution in order not to crash there is no need for documentation at all.
in the former case alien code is problematic in the latter code alien code is welcome.
state repair at runtime have no understandability requirements and depending of the scenario of behavioral repair understandability is not mandatory.
synthesizing maintainable repairs is antagonist to runtime disposable xes.
correctness.
those di erent repair scenarios have a direct impact on the correctness evaluation criteria.
to some extent a repair system that generates patches is a recommendation system for software engineering.
in the mindset of using a recommendation system the developer would use the system as follows.
she would consider a synthesized patch perform additional correctness and understandability assessment and then decide for the nal patch to be committed to the repository.
in such a scenario the repair system is allowed to synthesize partially correct patches.
on the contrary a fully automatic repair system is liable for synthesizing fully correct repairs5 according to its correctness oracle .
the latter has a much larger scope and seems more di cult in general.
the value of the repair system may come either from the fully automated correctness decision procedure or from the help it provides to the developers.
completeness.
the same argument applies to the completeness of the patch.
a repair recommendation system can only provide a partial repair say of the nal repair or even a sketch of the repair.
this can nonetheless be very valuable to guide the developer in writing the nal patch.
on the other hand at runtime the repair must be executable and partialness is not an option.
partial repair is sometimes a valuable option and in other cases an unacceptable solution.
depending on the repair problem statement the evaluation criteria are di erent and the evaluation goals may even be contradictory.
table sums up those points.
for instance bug x recommendation systems are expected to synthesize understandable patches but are allowed to provide partial solutions.
fully automatic repair systems have no understandability constraints but should always yield a executable solution.
.
on fix acceptability let us consider again the question of x acceptability.
we now go beyond looks good and humanly written and more generally beyond understandability correctness and completeness.
beyond those three evaluation criteria we think that the question of x acceptability is related to the foundations of software.
let us assume that we have a good and 5interestingly there is asymmetry between state repair and behavioral repair with respect to correctness assessing the correctness of a new state inferred synthesized corresponds to assessing one point on the contrary assessing the correctness of a behavioral patch requires to assess the correctness of the path under all anticipated inputs many points .
f i x a code i n s e r t i o n at l i n e i f x ffoo x g f i x b code i n s e r t i o n at l i n e i f x ffoo x g listing two possible xes of the same bug both satisfy all test cases .
there is no unique way to say that one is more acceptable than the other.
trustable test suite.
this test suite completely speci es the expected behavior in the sense of if the repair technique mixes up the software then the introduced bugs will be detected .
now let us assume that for a given bug and its failing test case one has two possible xes which are shown in listing .
obviously both patches x a test case where the value is involved.
this value has a semantics in its domain.
according to the setup either foo is idempotent for all values or no values are tested.
in the latter case since we assume one has a good test suite it means that for values the behavior is unspeci ed.
this can also be formulated as the values are outside the speci ed domain neither a nominal value nor an expected incorrect value .
in both cases there is no direct answer to the question which patch is more acceptable than the other .
let us now consider the topology of the output domain.
the rst x a with does nothing outside the specied point of the failing test case.
the second x b with with has an impact on the behavior within the unspeci ed input domain.
some tend to prefer x a because it minimizes the impact which is a well known engineering value.
however the rst x a with introduces an irregularity something happens only for one point of the input domain.
on the contrary the second x b with with is more regular it introduces a kind of phase transition at x .
this ts more to the idea that x does not represent an exception but a boundary which often happens in input domains.
some tend to prefer x b because it minimizes the number of irregularities which is also a well known engineering value.
even within our research group there is no consensus on which patch is better.
what we want to show with this made up example is two fold.
first the notion of x acceptability is actually founded on deep concepts and beliefs on the nature of software.
it may depend on the domain itself.
second there may be many concurrent xes for which there is simply no answer.
in other terms asking the question which patch is more acceptable than the other must be done with great care by explicitly stating that none is a valid and common answer.
this has been done in par s experiment the subjects could answer both are acceptable .
however as discussed above the subjects absence of knowledge about the domain tends to show that when the subjects answered this this had little to do with answering the patches are incommensurable .239fix acceptability may be an unanswerable question.
it is now clear that the question of x acceptability is directly related to what can be considered as correct or not directly related to the nature of software correctness.
program repair lies at the conjunction of two dimensions of software correctness.
it needs an oracle of what is incorrect an oracle for the bug.
it also needs an oracle on what behavior should be kept correct for sake of non regression.
the conventional common sense notion of software correctness is binary there is a decision procedure that says whether the software is correct the procedure outputs true or not the procedure outputs false .
dijkstra says a program with an error is just wrong .
boolean assertions in programming languages and testing frameworks embody this notion.
in this perspective the problem statement of program repair is easy the binary oracle of the bug should be negated and the other binary oracles should be kept passing.
this is the canonical problem statement of program repair as stated by genprog.
however binary software correctness is no longer the norm.
software correctness exists at di erent scales e.g.
at the level of expressions e.g.
arithmetic expressions functions modules systems etc.
let us di erentiate between the two extremes as local correctness a few lines of code and global correctness at the system level up to several millions of lines of code .
small scale correctness is often binary.
however when the scale increases a di erent kind of correctness emerges.
this emerging correctness o ers two new facets it may be partial as when a system passes test cases out of and it may be continuous as when one considers the quality of service many quality of service attributes are continuous such as the performance .
both facets partialness and continuousness replace an is correct binary predicate by a more correct than relation.
in other terms there is no direct induction between local binary correctness and global binary correctness .
this broadening of software correctness has a direct impact on software repair.
one may accept a x that partially solves a bug or that partially breaks the existing behavior.
those new dimensions of software correctness have been called acceptability envelope and approximate correctness by rinard et al.
controlled uncertainty by locasto et al.
and su cient correctness by shaw .
however both classical binary correctness and those unconventional kinds of correctness share a common characteristic with respect to repair they all implicitly de ne x acceptability.
a x is acceptable if the system stays in the correctness envelope.
this perspective also gives a new light on the evaluation criteria discussed in section the de nition of the correctness envelop de nes the evaluation criterion.
this point has been extensively exploited by rinard and colleagues for instance to automatically x security bugs or quality ofservice bugs .
note that this perspective is independent from whether repair is state based of behavior based and whether it happens online or o ine.
.
related work we structure the discussion of the related work on the following points the notion of defect class the evaluation and the risk of fallacy and the problem statement of automatic repair.
.
on defect classes the notion of defect class or fault class is really important in the eld of fault tolerance and software testing.
in fault tolerance according to foundations of the eld the dependability security speci cation of a system must include the requirements for speci ed classes of faults .
indeed one is tolerant with respect to a certain class of fault.
avizienis et al.
s elementary dimensions of fault classes provide a coarse grain framework for characterizing the bugs addressed by an automatic repair approach.
in software testing and in particular in the eld of mutation testing a fault class or fault model describes kinds of programmer mistakes in a particular language domain etc .
each mutation operator is intended to simulate one of those mistakes.
tolerating bugs simulating faults repairing bugs in all cases there is a real need to describe the classes of bugs that are handled by a novel technique.
in the research on self healing software which is close to automatic software repair the need for a fault model has been clearly stated by koopman self healing systems must have a fault model in terms of what injuries faults they are expected to be able to self heal.
without a fault model there is no way to assess whether a system actually can heal itself in situations of interest.
.. let us now analyze other recent papers on automatic repair under the perspective of the addressed defect class.
sem x is an automatic repair approach by nguyen and colleagues based on symbolic execution.
as in par s paper there is no clearly addressed defect class.
as said above and summarized in table a defect class can be de ned in terms of causes symptoms or kinds of x. with respect to the last point kinds of x sem x targets two clear defect classes it xes faulty integer initialization and faulty conditionals that use arithmetic relational and boolean operators.
carzaniga et al.
proposed a repair approach at runtime.
in this paper the addressed defect class is very clear it is unhandled exceptions.
hosek and cadar also address a defect class at runtime segmentation faults as de ned by the reception of unix sigsegv signal .
the paper of logozzo and ball s paper clearly conveys the notion of defect classes up to its structure e.g.
the section entitled repair of initialization and o by one errors .
however they repair statically generated warnings which are virtual bugs and not real ones.
although those papers address defect classes it is not always explicit.
we note that having an underlying defect class does not remove the risk of evaluation fallacy if the evaluation is conducted on a biased dataset with respect to the defect class or against a inappropriate competitor idem .
compared to this related work our paper explicitly states the importance of defect classes in automatic software repair.
.
on fallacies in software engineering having sound evaluation methods is essential for science.
in many elds di erent fallacies have been described e.g.
in medicine .
in software engineering many authors discussed potential fallacies such as glass in his book .240recently bird and colleagues have extensively discussed the biases of datasets used in bug predication research.
posnett filkov and devanbu have published a paper on the presence of ecological fallacies in empirical software engineering research focusing on sample size zonation and class imbalance .
both papers discuss the intimate relation between the dataset construction and the conclusiveness of the evaluation.
our paper makes the same point in a di erent context automatic software repair.
.
on the problem statement of automatic software repair along the two dimensions of state repair and runtime repair let us now survey important related work.
as early as taylor and colleagues introduced robust data structures which are able to repair their own state at runtime.
demsky and rinard proposed a similar approach for data structure repair perkins et al.
invented a complex repair strategies for register values and memory locations of x86 binary programs friedrich et al.
focused on repairing service oriented software.
lewis and whitehead s paper also performs state repair by runtime modi cation of the state of event driven programs.
on behavioral repair beyond the now classical work by weimer and colleagues there is also earlier e.g.
and concurrent work on this topic e.g.
.
those contributions focus on synthesizing source code to x bugs the code being meant to be committed into a version control system.
however behavioral repair is also relevant on binary code .
moreover as stated above behavioral repair can also happen at runtime the application communities of locasto and colleagues for instance share behavioral patches at runtime for xing faults.
.
conclusion automatic software repair is a eld of research with some momentum.
it poses hard and interesting problems and may have a great impact on practitioners.
taking as stepping stone kim et al.
s paper published at icse on this topic we have discussed the foundations of automatic repair.
first a meaningful evaluation in automatic software repair requires one to identify and characterize a defect class.
otherwise there is a great risk of stating a fallacy.
second the apparently harmless question is the synthesized patch correct?
has actually deep roots on how to de ne the problem statements of automatic repair and how to set up evaluation criteria.
we are only at the beginning of automatic software repair.
we are yet only able to automatically repair some bugs in some contexts where it is easy to have a well formed problem statement e.g.
test suite based program repair .
but let us open any issue tracker how many issues can be xed in an automated manner?
let us have a look at any bug x of less than lines in a source code repository for instance on github.
how many of those small changes can be synthesized in an automated manner?
i would say very few.
there are great inventions to be done on de ning bug oracles reproducing eld failures guiding the search for a correct repair solution assessing the impact of synthesized changes for repair etc.. and all this has to be done at the scale and complexity of today s software.
.