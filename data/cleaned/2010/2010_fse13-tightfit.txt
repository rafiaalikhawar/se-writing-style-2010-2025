tightfit adaptive parallelization with foresight omer tripp tel aviv university omertrip post.tau.ac.ilnoam rinetzkyy tel aviv university maon cs.tau.ac.il abstract irregular applications often exhibit data dependent parallelism di erent inputs and sometimes also di erent execution phases enable di erent levels of parallelism.
these changes in available parallelism have motivated work on adaptive concurrency control mechanisms.
existing adaptation techniques mostly learn about available parallelism indirectly through runtime monitors that detect pathologies e.g.excessive retries in speculation or high lock contention in mutual exclusion .
we present a novel approach to adaptive parallelization whereby the e ective level of parallelism is predicted directly based on input features rather than through circumstantial indicators over the execution environment such as retry rate .
this enables adaptation with foresight based on the input data and not the run pre x. for this the user speci es input features which our system then correlates with the amount of available parallelism through o ine learning.
the resulting prediction rule serves in deployment runs to foresee the available parallelism for a given workload and tune the parallelization system accordingly.
we have implemented our approach in tightfit a general framework for input centric o ine adaptation.
our experimental evaluation of tightfit over two adaptive runtime systems and eight benchmarks provides positive evidence regarding tightfit s e cacy and accuracy.
categories and subject descriptors d. .
concurrent programming general terms algorithms performance experimentation measurement supported by the european research council under the european union s seventh framework programme fp7 erc grant agreement no ysupported by the eu project advent grant number permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
esec fse august 18 a s26 saint petersburg russia copyright acm ... .
.keywords o ine learning irregular applications data dependent parallelism adaptive software parallelization stm .
introduction this paper addresses the problem of parallelizing irregular applications whose available parallelism is data dependent.
contrary to regular applications e.g.
scienti c programs that manipulate dense arrays where dependencies between computations are identical for all inputs an irregular application has a di erent dependence structure per di erent data inputs which leads to uctuating parallelism across inputs and sometimes also execution phases.
as an illustrative example minimum spanning tree mst algorithms typically have more parallelism over dense graphs where it is less likely for tasks to work on common nodes or edges.
irregular applications are widespread .
common examples include graph algorithms such as boruvka s and kruskal s mst algorithms and dijkstra s single source shortest path algorithm scienti c applications like the barnes hut and discrete event simulation algorithms machine learning and data mining algorithms such as agglomerative clustering and survey propagation and applications in the area of computational geometry including delaunay s mesh re nement and triangulation algorithms.
e ective parallelization of irregular applications is challenging because the available parallelism is input sensitive.
fixing a single synchronization scheme treating all inputs uniformly might either i exploit the available parallelism poorly for high parallelism inputs if synchronization is conservative e.g.
coarse grained locking or ii yield high overheads for low parallelism inputs if synchronization is permissive e.g.
many retries in speculative execution of con icting transactions .
adaptive concurrency control.
a natural means of addressing data dependent parallelism is adaptation whereby the parallelization system changes its behavior in response to changes in available parallelism.
recent studies have proposed a variety of techniques for identifying such changes and more speci cally estimating available parallelism on the y. examples include monitoring abort commit ratio in software transactional memory stm or tracking access patterns to shared data structures at the early stages of execution .
see section for a comprehensive discussion.
these as well as other existing techniques estimate the parallelism enabled by the input workload indirectly based on properties of the execution environment rather than prop erties of the input.
the motivation is clear deriving useful input characteristics require semantics understanding of the application at hand whereas pro ling system behaviors like abort commit ratio or lock contention provides a general and tractable basis for automatic adaptation.
the disadvantage however is that adaptation is guided by hindsight judgments over the behavior of the execution environment during the run pre x. this can potentially lead to poor performance at the beginning of the run as well as misguided adaptation if parallelism changes across phases i.e.when the future is not predictable from the past .
ideally the runtime system would make adaptation decisions directly with foresight given an input workload the system would know in advance based on inspection of the input itself how much parallelism that workload permits and tune its behavior accordingly .
designing foresight guided adaptation mechanisms is challenging it requires uncovering the relationship between inputs and available parallelism at runtime and with low overhead .
our approach.
we present a novel approach for enabling foresight based adaptation.
the main idea is to utilize ofine analysis and more speci cally a heavyweight learning algorithm to characterize the parallelism permitted by different inputs.
the runtime system then makes use of the artifacts from o ine analysis to derive adaptation decisions from the current state of the workload at hand.
what complicates learning is that the input data is often a complex data structure like a graph.
to address this challenge we ask the user to provide a feature extraction function mapping the concrete input to a set of features that are amenable to learning e.g.the number of nodes in the graph the number of edges the graph s density etc .
we require this speci cation because feature extraction is a challenging problem for an automated algorithm.
it is our impression and experience that providing the speci cation places low burden on a user intimate with the target application as the entailed reasoning is relatively simple.
furthermore the speci cation is concise and most importantly the o ine learning system is robust to user errors.
see section for further discussion.
based on the speci cation of input features our approach decomposes into the following three phases .per system learning.
for the given adaptive parallelization system converge once per all applications on the most favorable execution mode of the system per di erent pro les of available parallelism.
.o ine learning.
for every application build a prediction function from input features to available parallelism using o ine learning over representative workloads.
.runtime parallelization.
at runtime the system periodically calls the user provided feature extraction function to obtain the features of the current data set.
it then performs a lookup to nd which mode of the parallelization system best matches these features and sets the system to that mode.
for the second phase of estimating available parallelism which is the main focus of this paper we analyze data dependencies.
a data dependency arises between two statements during sequential execution if both access a common memory location and at least one of them writes it.
intuitively this is an indication that the two statements might interfere with each other during concurrent execution.traditionally compile time parallelization transformations have been governed by qualitative analysis of data dependencies requiring absence of data dependencies between code blocks as the precondition for their parallel execution.
a fundamental observation of this paper is that dynamic data dependencies expose ne grained information about the available parallelism.
this includes not only the volume of data dependencies between tasks that can potentially run in parallel i.e.the density of the dependence graph but also the structure of such dependencies which discloses e.g.
whether two tasks have cyclic dependencies.
this provides a principled basis for o ine estimation of available parallelism that abstracts away low level deployment details such as the size of the cache the number of cores the number of executing threads etc .
an important emphasis of our approach is on e ective user interaction the application designer who is best aware of pertinent workload features communicates this information through a concise and lightweight speci cation.
the adaptation algorithm in turn leverages this information to build a bridge using statistical analysis between workload characteristics and available parallelism which is known as a challenging task for fully automated systems .
scope and limitations.
we expect the application designer to provide candidate features as well as representative workloads for pro ling.
we assume that the program is already parallel or at least contains atomicity annotations and so the notion of atomic tasks is speci ed over the program s code.
we further assume that the criterion for correct parallel execution is serializable execution of atomic blocks or transactions which tightfit guarantees.
a dependent and more subtle assumption is that parallel tasks communicate only by modifying the shared memory as is the case e.g.
with applications using stm and thus data dependencies are a reliable model of runtime con icts.
a main source of complexity in tightfit is the learning process.
this process entails o ine analysis as well as user involvement but in return it enables low overhead inputcentric adaptation.
at present our prototype has limited inference capabilities and so the user has to choose which statistical learning algorithm to apply though this again is a question of performance accuracy and not correctness .
we intend to reduce this con guration burden in the future by introducing inference capabilities into tightfit .
contributions.
the principal contributions of this paper are the following adaptation with foresight.
we present a novel solution for the problem of adaptive parallelization focusing on the direct connection between input features and parallelism.
this is achieved via expensive ofine analysis of training runs backed by user provided input features alongside low overhead input centric runtime adaptation.
adaptation based on input features.
we make the relationship between the input data and available parallelism amenable to learning by abstracting the data as a set of features according to a user provided specication.
we believe that this novel idea can be reused in other contexts.
parallelism characterization via analysis of dependencies.
we derive quantitative as well as structural characteristics of data dependencies to estimate the available parallelism thereby abstracting awaygraph g read input graph graph mst g.getnodes list worklist g.getnodes atomic foreach node nd in worklist node nbr minweight nd g.getadjacent nd node nnd edgecontract nd nbr mst.addedge nbr nnd worklist.add nnd figure the boruvka mst algorithm n1 n2 n5 n3 n6 n4 n7 a b c d n2 c1 n3 n6 c2 n2 c1 c3 n6 c1 c4 n6 figure di erent revisions of an input graph under the boruvka mst algorithm implementation speci c details.
this goes beyond the standard approach of treating data dependencies qualitatively as a clear cutting criterion for disjoint parallelism.
implementation and evaluation.
we have implemented our approach in tightfit .
we describe our experiments with tightfit over two di erent adaptive parallelization systems and a suite of eight benchmarks.
the results provide solid evidence in support of our approach.
tightfit is available online at .
.
overview in this section we motivate our approach with reference to boruvka s mst algorithm.
we then walk the reader through figure which summarizes the entire ow of the tightfit system.
.
boruvka s algorithm figure shows boruvka s algorithm.
the algorithm computes a minimum spanning tree by reducing the input graph to a single node through successive application of edge contraction.
in each iteration a graph node ndis selected nondeterministically a minimum weight neighbor nndofndis obtained and the edge between ndand nndundergoes contraction becoming part of the mst .
for a sparse input graph this permits a high level of parallel work at the beginning where di erent contractions are applied to disjoint regions of the graph but ultimately as the graph grows smaller the available parallelism gradually decays .
these two cases are illustrated in figure the transition from state a of the graph to state b is via two parallel non overlapping edge contractions n1 n5 !c1 and n4 n7 !c2.
however the application of contraction c2 n3 !c3 shown as state c cannot run in parallel with its succeeding contraction n2 c3 !c4 which appears in d .
both access a common region in the graph.features graph g nnodes g.nnodes density .
zero.noslash g.nedges g.nnodes g.nedges avgdeg .
zero.noslash g.nedges g.nnodes .... figure fragment of the tightfit speci cation for the graph type cf.figure .
flow thetightfit system breaks the parallelization process into several phases which we discuss in turn.
parallelization modes.
for a given adaptive parallelization system tightfit needs to establish a mapping from parallelism levels to e ective execution modes of the adaptive system.
this is done once per each system.
the parallelism to mode mapping shown as the sys.
level para.
!mode box in figure can either be speci ed by the user or it can be learned automatically.
in this paper we place more emphasis on the problem of estimating application speci c per input parallelism and so to compute the parallelism to mode mapping we resorted to the standard solution of running the system on a synthetic benchmark whose available parallelism is parametric to nd e ective threshold values for switching between parallelization modes .
the underlying assumption is that di erent modes correspond to di erent parallelism levels.
we expand on this step in section .
o ine adaptation.
the main focus of this paper is on learning an input centric application speci c adaptation rule.
to make the learning setting induced by o ine adaptation feasible especially in the case of complex inputs like a graph we ask the user to simplify the input description by providing a feature extraction function that reduces the input to a vector of simple features.
this is summarized as the double framed input !features box in figure .
figure illustrates the speci cation format for the example of agraph data structure.
o ine learning of adaptation rules.
input7!features for every application we learn an input centric adaptation rule.
to make the learning procedure feasible especially in the case of complex inputs like a graph we ask the user to simplify the input description by providing a feature extraction function that reduces the input to a vector of simple features.
this is summarized as the doubleframed input7!features box in figure .
figure illustrates the speci cation format for the example of a graph data structure.
the required speci cation puts little responsibility on the user s shoulders in that even if the speci cation is incomplete or simply irrelevant correctness i.e.serializability of parallel execution is guaranteed.
the user can only a ect the e cacy of adaptation decisions at most degrading performance if providing a bad speci cation.
moreover the specication is not application speci c. all graph algorithms for example can share the speci cation in figure .
tightfit favors a comprehensive speci cation including features that may prove useless or irrelevant rather than excluding them.
this is because tightfit s learning algorithm applies regression seeking correlations between input features and estimated parallelism levels which automatically prunes irrelevant i.e.weakly correlated features as input7!features features7!prof.11feature sampling mode selection sys.
level prof. !mode oo o ine per app.
sensor runtime actor o ine per sys.
figure outline of the complete tightfit ow signing them low weight in the adaptation rule.
this makes such features harmless and thus the more exhaustive the speci cation is the better the adaptation rule becomes.
since feature extraction is done not only o ine but also online during parallel runs features should be e ciently computable.
if at runtime feature computation requires too many cycles e.g.counting how many cycles or simple paths a graph contains then the performance bene ts of adaptation are obviated.
as an example the cost of methods nnodes and nedges in figure should be low.
in addition to ensure the statistical signi cance of the regression algorithm correlating input features with parallelism we ask the user to characterize the application s input space by providing legal ranges for input parameters.
tightfit features a built in harness for sampling inputs at random based on the user s characterization of parameter ranges.
features7!parallelism pro le the next step in o ine adaptation given the availability of input features is to estimate available parallelism over the training runs.
our main observation here is that pro le guided analysis of data dependencies between tasks designated for concurrent execution permits e ective measurement of available parallelism.
this is because at runtime data dependencies translate into con icting accesses to shared memory which mandate synchronization thereby limiting parallelism.
moreover compared to more concrete measures of available parallelism like direct measurement of running time over di erent inputs data dependencies are less coupled with low level deployment details like the hardware architecture number of threads caching etc which makes the learned input toparallelism correlations more signi cant as well as robust to changes in the parallelization system and or deployment con guration.
for the example of boruvka there are indeed no data dependencies between the rst two loop iterations but the next two iterations performing overlapping contractions generate data dependencies over c3.
that is there is an increase in the number of data dependencies as the computation evolves which is compatible with the observation that the available parallelism in boruvka gradually decays.
quantitative abstraction of data dependencies enables ofine generation of input centric parallelism pro les the target application is run on di erent inputs a dependence graph is computed for each trace and the density and shape of the graph are analyzed to derive a general rather than deployment sensitive measure of available parallelism.
this exposes the relationship between input properties and parallelism level allowing prediction of the available parallelism for new inputs as summarized in the features !prof.
box in figure .
this analysis is the subject of sections and .
.
parallelism pro le !mode the remaining task is to correlate between parallelism pro les with di erent execu tion modes of a particular adaptive parallelization system.
this correlation is achieved via a synthetic benchmark suite that enables parametric control over the amount of parallelism in a run.
this task is summarized in the sys.
level prof. !mode box in figure and is the subject of section .
.
runtime parallelization.
the nal stage in the tightfit ow is online parallelization.
here the o ine adaptation stragey is utilized by sampling at the beginning of the run and possibly also throughout the run when starting an atomic block if there are phase transitions the feature values for the input at hand.
this is done based on the same feature extraction function serving for o ine analysis by the sensor module which ows this information to the actor component.
the actor module then makes a mode recommendation the mode selection box based on the composition of the features !prof.
and sys.
level prof. !mode functions computed o ine.
in section we discuss two adaptive parallelization systems with which we evaluated tightfit .
.
predicting a vailable parallelism from data dependencies in this section we explain how the available parallelism in a given sequential execution trace is estimated by analysis of data dependencies.
.
atomicity aware dependence graphs a sequential trace is a sequence of transitions where sis a primitive statement and and 0are the prestate and poststate respectively.
abstracting a trace as a dependence graph is standard .
for the purpose of this paper we de ne a slight variant which we refer to as anatomicity aware dependence graph aadg .
we assume that the program is annotated with atomicf gsections and that the semantics records entry and exit events corresponding to entering and leaving atomic sections.
this yields for a given sequential run of the program a partitioning of the transitions coming from atomic sections into distinct atomic blocks .
note that di erent executions or instances of the same atomic section correspond to different atomic blocks.
trace is abstracted as an aadg as shown in figure .
every transition t s occurring within an atomic block bis mapped to a node t b .
.
for each memory location lread byt we draw edges to all nodes t0 b0 such that block b0di ers from bandl is written by t0 i.e.there is ow dependence between tandt0 and these transitions occur in di erent atomic blocks .
.
for each location lwritten by t we draw edges to all nodes t0 b0 such that block b0di ers from bandlis either read or written by t0 i.e.
there is either anti dependence or output dependence between tandt0 o ine estimation of parallelism by analysis of dependencies input execution trace with atomicity annotations offlineestimate letg for each transition toccurring in atomic block bin insert node t b intog for each mem.
loc.
l2r t for each node t0 b0 2gs.t.l2w t0 b6 b0 insert edge t0 b0 t b intog for each mem.
loc.
l2w t for each node t0 b0 2gs.t.l2r t0 w t0 b6 b0 insert edge t0 b0 t b intog return densityg cdepg figure o ine alg.
for parallelism estimation which occur in di erent atomic blocks .
note that the algorithm is parametric in the speci cation of readsets and writesets randw as well as in the grain of trace transitions .
we explain the algorithm s output soon.
the above steps yield a standard dependence graph modulo two adjustments first nodes point to their enclosing atomic block and second there are no intra block dependence edges.
intuitively this enables qualitative checking as well as quantitative measurement of dependencies between statements belonging in distinct atomic blocks designated for parallel execution.
note importantly that tightfit computes aadgs solely over sequential traces and not parallel ones .
however assuming the parallelization system guarantees serializability there is no loss of generality here.
this is because a parallel trace can be serialized and thus its aadg is the same as that of its corresponding sequential run allowing o ine analysis to consider only sequential executions.
.
quantifying dependencies given an aadg g we measure two aspects of the parallelism it permits contention.
contention is interpreted as the level of potential interference between tasks designated for parallel execution.
this corresponds naturally to the density of the aadg when viewed as an undirected graph densityg jg ej jg vj jg vj we obtain a normalized measure of the intensity or proportion of dependencies between atomic blocks.
data dependencies between transitions in the same atomic block are suppressed to concentrate on inter task dependencies.
cyclic dependence.
another measure of parallelism is the level to which tasks exhibit cyclic dependencies.
this situation illustrated in figure arises when distinct atomic blocks access two or more memory locations in opposing orders.
in figure this is shown for four transitions from two atomic blocks.
the upper left and bottom right transitions are dependent due to memory location l1 output dependence whereas the other two transitions are dependent due tol2 ow dependence .
measuring cyclic dependencies beyond contention is useful for synchronization because some protocols work well under high contention but cope poorly with cyclic dependencies .
x b1 y b2 y b1 z x b2 ll figure illustration of a sitaution where two atomic blocks b1andb2 are cyclically dependent to de ne this measure we need an auxiliary de nition we say that atomic blocks bandb0arecyclically dependent in aadggover trace denoted by bg b0 if there are two memory locations landl0 and four transitions t1 b t2 b t3 b0 t4 b0 where is the order of appearance of transitions in such that t1andt4are the rst transitions in bandb0 respectively to access l t2andt3are the rst transitions in bandb0 respectively to access l0 and t1 b t4 b0 and t2 b t3 b0 2g e. at runtime this translates into a cyclic con ict if the executions ofbandb0are interleaved i.e.t1andt3are rst executed which forces cyclic dependence between bandb0 .
based on the above de nition we quantify cyclic dependence as the following normalized measure cdepg fbg b0g nblksg nblksg where nblksgis the number of distinct atomic blocks in g. the algorithm in figure outputs a pair of real numbers in the range which denote estimates of contention and cyclic dependencies.
together these estimates provide a characterization of the available parallelism in trace which we refer to as the parallelism pro le of .
.
adaptation via offline learning given the mechanisms of section to estimate parallelism for an execution trace we now describe an o ine learning system that synthesizes based on a nite number of representative traces for an application a specialized adaptation strategy for that application.
the learning process is independent of the parallelization system.
.
learning per input parallelism a high level view of the input to parallelism learning algorithm is given in figure .
this algorithm accepts as input a collection of sequential execution traces m along with their respective input workloads i1 im .tightfit derives a parallelism pro le pjfrom jusing quantitative analysis by applying the o algorithm explained in section to j and records the relationship between the features of inputijand pj.
recall that pjestimates the available parallelism in j. the connection between inputs and their respective parallelism pro le is lifted via regression analysis into a predictor of from input features to an estimated parallelism pro le as shown in figure the free variables are the feature values and the dependent variables are the estimates of contention and cyclic dependencies.
by default tightfitapplies linear regression analysis though the choice of regression model is parametric and con gurable e.g.cubicinputfeatures trace para profile i1 f1 f1 n 1o p1 vv im fm fm n mo pm rrregression i f f n of p figure abstract view of the o ine input toparallelism learning alg.
or quartic regression .
to make the mapping from inputs to parallelism pro le amenable to learning inputs are modeled according to the user provided features.
regression analysis makes statistical assumptions over its inputs.
speci cally the training set s size should be proportional to the number of independent variables and the inputs should be sampled independently at random.
to meet these requirements tightfit provides a learning harness to the user.
the user is asked to specify an admissible range of values for each application parameter.
the harness then picks parameter values at random to craft training inputs.
by default tightfit samples maxf150 50mginputs wheremis the number of features.
this is a conservative approximation of several popular guidelines including n m n 40mandn 8m .
the user can set other bounds if desirable.
our experiments con rm these bounds as e ective cf.section .
.
threshold values for mode transitions a remaining task after learning the predictor of is to correlate between parallelism estimates and modes of given adaptive parallelization systems.
building this additional bridge yields a direct relationship between input features and parallelization modes which concludes the o ine adaptation process with an adaptation strategy that chooses between system behaviors according to input features.
we assume that the runtime adaptation system has several distinct modes totally ordered according to parallelism level.
thus correlating between parallelism pro les and parallelization modes is essentially the problem of setting thresholds for transitioning between modes.
tightfit computes the thresholds for every parallelization system irrespectively of the particular application at hand.
similarly to other adaptive systems this is achieved via a synthetic benchmark suite that enables parametric control over the amount of parallelism in a run.
the benchmarks manipulate a shared concurrentmap object using a random client loop where each iteration is an atomic task.
the proportion of read write operations range of keys and number of operations are all parametric.
within this setting tightfit induces a large number of di erent parallelism pro les checking for each which mode works best i.e.results in shortest execution time .
this yields empirical cuto values for transitioning between modes as a function of the parallelism pro le tightfit s contention and cyclicity estimates .o ine learning of adaptation strategy inputs f igm i execution traces with atomicity annotations features feature extraction function parameters adaptive para.
system with modes m1 mk mode mapping from para.
estimate d c to mode mi regress regression algorithm offlineadapt returns actor function ff7!fmigk i 1g let for each trace i letfi features let di ci offlineestimate i insertfi7!
di ci into letof ff7!
ed ec g regress returnff7!
mode of fg figure o ine alg.
for synthesizing the actor module for an adaptation scheme .
putting it all together the complete learning system is shown in pseudocode in figure where mode is the thresholding algorithm.
note that in figure the o ine learning algorithm is parameterized by the system speci c mapping from parallelism pro les to modes of the parallelization system .
the offlineestimate algorithm developed in section is a subroutine of offlineadapt .
the output is the actor module for parallelization system formed as the composition of mode and the parallelism predictor of.
this enables the runtime system outlined in figure .
the sensor module realizes the reading of feature values o the application s state initial state in general and intermediate states to account for phase transitions and the actor module issues if needed a mode selection request based on the feature values sampled by the sensor first the actor applies ofto obtain a parallelism pro le thus estimating the available parallelism and then it invokes the mode function to obtain the recommended mode.
.
adaptive runtime systems research on data dependent parallelism has resulted in a wide range of adaptive concurrency control mechanisms.
we have realized two such mechanisms to evaluate the e cacy of tightfit which we describe in this section.
in both cases the initial adaptation decision is made at the beginning of a run.
the user can con gure tightfit to perform additional adaptation steps during the run to account for phase transitions.
the user then also con gures the frequency of adaptation decisions.
a value nmeans that once every n transaction starts an adaptation decision will be taken.
.
switching between stm protocols di erent stm protocols have di erent strengths and weaknesses.
some protocols work better under high contention whereas others are specialized for high parallelism workloads .
this leads to an adaptation method whereby the runtime system switches between protocols according to the estimated available parallelism.
we have designed such a system with three underlying protocols the retry protocol retry applies eager specula tion aborting a transaction immediately as it attempts con icting access to a memory location owned by another live transaction i.e.at least one of the transactions needs write access to the location .
this works well for low contention pro les.
dependence aware transactional memory datmfg is e ective for high contention pro les with scarce cyclic dependencies between transactions.
datm fg lets a transaction depend on another transaction owning a needed memory location instead of aborting and retrying e ectively stalling the dependent transaction as long as no cyclic dependencies arise.
this reduces concurrency as well as retries which is desirable for high contention pro les.
finally for high contention pro les with a high proportion of cyclic dependencies a coarse grained variant ofdatm fg dubbed datm cg is reserved.
this variant e ectively simulates a global lock by viewing all memory locations as one and the same which obviates the threat of rollbacks due to cyclic dependencies.
the resulting actor module has the following form actor f retry ifgcon f tcon datm fg ifgcon f tcon fcyc f tcyc datm cg otherwise because retry anddatm fg are friendly protocols switching between these protocols is permitted even when there are live transactions synchronizing according to the old protocol.
to switch to from datm cg however a barrier is required.
otherwise serializability is not guaranteed.
our implementation instead makes an opportunistic attempt to enforce the selection but gives up if the attempt fails due to live transactions.
a backo mechanism improves the probability that the next attempt will succeed.
.
active threads another adaptation strategy is to set concurrency level as dictated by the number of active threads acrroding to the available parallelism .
this is facilitated by parallelization systems with a con gurable thread pool whose size can be adjusted during the run such as clients of the java threadpoolexecutor library.
the number of active threads can be adapted at any point in the run without any constraints due to live transactions and without a ecting the correctness of the run.
in our implementation the actor follows the template actor f ceil ncores gcon f fcyc f which yields an integral value in the range for real coe cients .
that is contention and cyclicity are given distinct weights in deciding the concurrency level up to the number of available cores.
.
implementation evaluation in this section we describe our protoype implementation of the tightfit system and report on experiments measuring the e ect of o ine adaptation according to our approach in comparison with several other alternatives.
.
prototype implementation tightfit is implemented as a java library.
our prototype loads user speci cations at runtime from a designated location.
it additionally inserts instrumentation hooks at thebeginning of code blocks designated for atomic execution as speci ed by an annotation to perform feature sampling and enforce adaptation requests.
for instrumentation we use the javassist bytecode instrumentation library .
for o ine analysis our prototype exposes a con gurable policy prescribing which regression algorithm to apply.
we use algorithms from the weka library for regression analysis the default being the linearregression class.
the user can also control sampling parameters.
for sampling the default interval is atomic blocks.
we used these default settings in our experiments.
.
experiments benchmarks.
our experimental suite included eight benchmarks.
these are described in table .
all benchmarks excluding boruvka and elevator were taken from jstamp the java port of the stamp benchmark suite which is distributed as part of the deuce stm implementation .
the boruvka benchmark is based on a sequential implementation of boruvka s algorithm that is available as part of the java version of the lonestar suite .
elevator is taken from the pjbench suite of parallel java benchmarks .
elevator and bank are lock based parallel applications.
matrixmultiply is embarrassingly parallel admitting zero con icts on any input matrix but is included in the evaluation for sanity checking and overhead assessment.
the remaining ve benchmarks are irregular and thus use stm.
our speci cation of workload features over these benchmarks was straightforward for all benchmarks except boruvka we set the command line arguments as the candidate features.
this is because the benchmarks are all parametric per command line values.
for boruvka we speci ed the three features in figure namely number of nodes density and average node degree.
experimental setup.
we designed two experiments corresponding to the adaptive systems described in section .
in the rst experiment we implemented an adaptive system that can switch between stm protocols cf.section .
and used the boruvka algorithm as well as the ve jstamp stm applications as benchmarks.
in the second experiment we investigated adaptation by means of changing concurrency levels cf.section .
.
this mechanism works uniformly both for stm and for lock based synchronization and thus we considered all eight benchmarks.
the stm benchmarks used the datm fg protocol.
in the rst experiment we compared tightfit with three non adaptive stms each using one of the protocols underlying the adaptive system retry datm fg and datmcg .
in the second experiment we compared tightfit against three xed concurrency levels thr.s thr.s and thr.s where thr serves as a reference as well as an online adaptation algorithm online that switches between the protocols by tracking per thread abort commit ratio .
in both experiments we also compared the e cacy of the two step o ine learning technique featured in tightfit where the application speci c predictor is computed separately from the system speci c thresholding function with two o ine adaptation algorithms o and o that learn a predictor from input features to system modes directly.
this is achieved by correlating between input features and the mode yielding the shortest execution time forbenchmark domain description inputs boruvka scienti c computes mst nodes avg.
degree between genome bioinformatics performs gene sequencing g s n intruder security detects network intrusions a l n s kmeans data mining implements k means clustring m n s matrixmultiply scienti c performs matrix multiplication n nmatrices where n vacation online tx.
processing emulates a travel reservation sys.
n q u r t bank online tx.
processing emulates a banking sys.
n i m elevator discrete event simulator simulates a sys.
of elevators oors random bias over from to oor numbers table benchmarks including parameter ranges for random workload generation that input where o utilizes four threads for this task and o trains with eight threads.
we conducted the experiments on a bit linux machine with an intel core i7 processor combining four dual cores at .93ghz each multiplexing hardware threads for a total of threads.
the host vm was java se development kit update jdk6u25 .
the input ranges we used both for o ine analysis and for the deployment runs are listed in column three of table .
the number of traces we considered for training is discussed in section .
.
for the production runs we selected at random inputs per benchmark.
for each input and concurrency level ranging from to threads we ran the benchmark times and recorded the average across the last runs excluding the rst cold run.
the measurements reported in section .
represent the average across all of the selected inputs.
.
performance results speedup and retry trends for the rst experiment the system of section .
going from one to eight threads are shown in figures and figures respectively.
we omit retry statistics for matrixmultiply which does not admit any con icts cf.section .
.
we also omit retry statistics for the datm cg protocol since this protocol simulates a global lock and thus prevents retries completely.
average speedup and retries for eight threads are summarized below speedup retries all wo.
mmul all wo.
mmul retry .
.
.
.
datm fg .
.
.
.
datm cg .
.
tightfit .
.
.
.
online .
.
.
.
o .
.
.
.
o .
.
.
.
tightfit achieves better speedup and less retries than the xed alternatives i.e.
retry datm fg and datmcg as well as online adaptation online .
as for direct o ine learning o and o at rst glance the results seem to suggest that these are comparable if not superior to tightfit o has similar speedup and retry statistics to tightfit over eight threads and o is approximately faster more accurately with matrixmultiply and without matrixmultiply .
however more careful analysis of the results and in particular the speedup and retry trends visualized in figures and figures respectively reveals that on average tightfit is as e ective as o and o on benchmarks with high variance in parallelism such as genome and kmeans across the entire range of concurrency levels .
we return to this point in section .
.the speedup results we obtained in the second experiment the system of section .
are listed in figure .
for some of the benchmarks including elevator kmeans and genome tightfit is superior to alternatives xing the number of active threads throughout all inputs and the entire duration of the run.
retries and memory usage statistics for the lock based benchmarks as well as nonzero retry stm benchmarks under datm fg are reported below moderetries memory genome boruvka vacation bank elevator thread threads .
.
.
.
.
threads .
.
.
.
.
threads .
.
.
.
.
tightfit .
.
.
.
.
o .
.
.
.
.
o .
.
.
.
.
again here o and o appear slightly better thantightfit on some benchmarks and concurrency levels though consistently with the results of the rst experiment tightfit achieves comparable speedups across all concurrency levels on benchmarks with highly dynamic parallelism namely boruvka genome and kmeans .
.
discussion overall the experimental results provide support for the main thesis of this paper which puts forward the direct connection between input features and available parallelism as the basis for adaptation.
for the adaptive stm system section .
tightfit s o ine adaptation algorithm is measurably better than its online alternative also yielding better thr thr.s thr.s thr.s tightfit off off figure speedup as function of active threads11.
.
.
.
.
.
.
8retry datm fg datm cg tightfit online off off 8figure speedup intruder .
.
.
.
.
.
.
8retry datm fg datm cg tightfit online off off figure speedup genome .
.
.
.
.
.
.
8retry datm fg datm cg tightfit online off off figure speedup boruvka .
.
.
.
.
.
.
8retry datm fg datm cg tightfit online off off figure speedup vacation .
.
.
.
.
.
.
8retry datm fg datm cg tightfit online off off figure speedup kmeans .
.
.
.
.
.
.
8retry datm fg datm cg tightfit online off off figure speedup mmul results than the three baseline stm algorithms.
a similar trend is seen with the system of section .
where adaptation is achieved by controlling concurrency level.
moreover our analysis of the o ine artifacts con rms that the learning algorithm is able to converge on the features that e ectively control parallelism.
as an example tightfitwas able to converge on qand nas the signi cant features in vacation.
these two parameters control transaction duration and query range respectively and are indeed the determining factors of available parallelism in vacation where rand tare global parameters a ecting the overall duration of the run as the documentation for this benchmark con rms.
the improvement thanks to tightfit is more noticeable on benchmarks whose available parallelism is highly dynamic such as kmeans and genome and to a lesser extent also boruvka.
benchmarks with less variance in parallelism pro les such as vacation provide less room for optimization thanks to o ine adaptation leaving tightfit similar to the online adaptation algorithm in performance.
this observation also connects to the comparison between tightfit and the two other o ine adaptation techniques o and o .
while benchmarks whose available parallelism is more stable across di erent workloads seem to favor o and o tightfit is competitive with both of these alternatives on the benchmarks whose parallelism changes signi cantly across inputs throughout the entire spectrum of concurrency levels.
in particular while o is more e ective in the neighborhood of four threads and o performs better when concurrency level is closer to eight threads tightfit is more stable than both of these alternatives and thus achieves comparable speedups on average across all concurrency levels.
the experiments indicate the superiority of the o ine adaptation technique suggested in this paper.
however it appears that if the deployment setup i.e.concurrency level cache sizes processor architecture etc is known a priori then direct learning in the style of o and o is potentially preferable.
if on the other hand the deployment setup is subject to variation e.g.due to usage of several dif ferent hardware con gurations then we expect tightfit s learning algorithm to provide better results as it abstracts away all deployment details.
another advantage of the o ine learning algorithm of tightfit over direct learning is that it allows for separation of concerns the application designer provides useful input features and the designer of the adaptive system separately decides which execution mode best ts every parallelism pro le.
recall that tightfit automatically learns the mapping from parallelism pro les to system modes.
we believe that if this mapping could be speci ed by an expert then tightfit would achieve even better results.
asking for such a speci cation is reasonable as it is done once per system.
we leave research on this topic for future work.
.
related work for space constraints we restrict our discussion to closely related research on adaptive parallelization.
a more detailed review of the related work appears in .
in the broader scope of pro le guided specialization we refer the reader to for adaptive garbage collection for pro le guided compile time parallelization and for pro le based specialization of static heap abstractions.
the transactional concurrency tuning system uses a control theoretic model to adapt the number of active threads to the available parallelism where the percentage of committed transactions out of all executed transactions in a sample period provides a measure of available parallelism.
a closely related approach presented in is to stall a thread if its abort commit history indicates low parallelism.
a similar heuristic is implemented in the galois system .
the main distinction is that tightfit estimates available parallelism directly based on input features instead of tracking indirect monitors related to the execution system s behavior.
the shrink system utilizes the run pre x to predict transactional memory accesses.
prediction is based on the access patterns of past transactions from the same thread.
the scheduler then tries to prevent transactions whose pre .
.
.
.
8retry datm fg tightfit online off off 8figure retries intruder .
.
.
.
8retry datm fg tightfit online off off figure retries genome .
.
.
.
8retry datm fg tightfit online off off figure retries boruvka .
.
.
.
8retry datm fg tightfit online off off figure retries vacation .
.
.
.
8retry datm fg tightfit online off off figure retries kmeans dicted access sets overlap from running in parallel.
tracking memory acccesses is reminiscent of our analysis of data dependencies though it is not clear how to cast shrink into our setting of o ine learning.
another form of adaptation proposed in is adaptive locking the parallelization algorithm monitors the execution and based on the collected statistics decides whether to execute a critical section cs speculatively or using mutual exclusion.
in hot variables which cause large numbers of transactions to abort are identi ed at runtime for these variables the transactional manager selectively switches to pessimistic concurrency control where reader writer locks mediate access to locations.
adaptstm collects stm internal statistics like the number of collisions in the hashtable mapping memory to locks to tune stm internal data structures.
the system of goes beyond such ne grained adaptivity that optimizes a speci c stm implementation to also support coarse grained adaptivity where a system wide policy speci es when to switch between stm implementations.
the sambamba system monitors the application s behavior and specializes functions on they for actual usage pro les.
one of the supported optimizations is automatic parallelization using speculation where deciding whether to apply parallelization depends on the available compute resources.
all of these systems adapt their behavior online and could bene t from integration with tightfit so that the choice of parallelization mode e.g.
con guration of internal data structures in adaptstm or cs execution mode in adaptive locking is made o ine based on input characteristics.
a mechanism for dynamic pro ling of a running transactional program is presented in .
the obtained pro le is then used in conjunction with a machine learning ml algorithm to select an optimal stm implementation at runtime.
the ml algorithm is trained o ine by measuring the running time of parameterized microbenchmarks for all available stm algorithms at multiple thread levels.
then during program execution a xed number of transactions is pro led to guide the ml algorithm s choice of stm algorithm.
o ine learning records certain code level character istics of the microbenchmarks such as whether transactions contain loops.
the online prediction rule is parameterized by these distinctions.
recall that tightfit estimates the available parallelism as a function of user speci ed properties of the data.
in contrast the predictor in relies on prede ned syntactic features.
in the authors utilize o ine analysis to discover seminal behaviors .
these are behaviors that typically manifest at an early stage of execution and are correlated with many other behaviors of the program permitting e ective prediction of the program s behavior.
seminal behaviors are extracted automatically to support proactive optimizations in the jikes vm.
a similar approach developed in trades training for incremental adaptation across production runs.
the authors of apply this idea to predict the likelihood of successful speculation where predictions account for input properties indirectly using classi cation trees.
tightfitshares the motivation of these works but supports direct learning of the relationship between input features and parallelism rather than passing through seminal behaviors thanks to o ine learning.
the janus algorithm built on top of the hawkeye algorithm for abstract level dependence tracking uses o ine training to improve the precision of con ict detection during speculative parallelization.
the key idea is to enforce sequence based detection where sequences of operations rather than individual operations are tested for commutativity.
the training phase is used to observe which sequences occur in the client application and check o ine whether they commute so that the runtime overhead of sequencebased detection becomes negligible.
tightfit is similar to janus in applying o ine learning but the learning process including its scope ow and techniques is quite di erent.
.
conclusion and future work we have presented a novel approach for foresight guided adaptation which permits low overhead input centric runtime adaptation by shifting most of the cost of predicting per input parallelism to an expensive o ine analysis.
twoaspects of our approach are of particular interest i specication of workloads in terms of useful features which permits direct learning of the connection between input characteristics and available parallelism and ii quantitative and structural analysis of data dependencies as a means of estimating available parallelism while abstracting away deployment speci c details.
our approach is implemented in the tightfit system which is publicly available and shows promising results in our experiments.
in the future we intend to make tightfit more automatic by introducing auto tuning capabilities similarly to e.g.to decide on e ective threshold values for mode transitions .
we also plan to develop inference capabilities to automatically converge on useful workload features.
.