how to effectively use topic models for software engineering tasks?
an approach based on genetic algorithms annibale panichella1 bogdan dit2 rocco oliveto3 massimilano di penta4 denys poshynanyk2 andrea de lucia1 1university of salerno fisciano sa italy 2the college of william and mary williamsburg v a usa 3university of molise pesche is italy 4university of sannio benevento italy abstract information retrieval ir methods and in particular topic models have recently been used to support essential software engineering se tasks by enabling software textual retrieval and analysis.
in all these approaches topic models have been used on software artifacts in a similar manner as they were used on natural language documents e.g.
using the same settings and parameters because the underlying assumption was that source code and natural language documents are similar.
however applying topic models on software data using the same settings as for natural language text did not always produce the expected results.
recent research investigated this assumption and showed that source code is much more repetitive and predictable as compared to the natural language text.
our paper builds on this new fundamental finding and proposes a novel solution to adapt configure and effectively use a topic modeling technique namely latent dirichlet allocation lda to achieve better acceptable performance across various se tasks.
our paper introduces a novel solution called lda ga which uses genetic algorithms ga to determine a near optimal configuration for lda in the context of three different se tasks traceability link recovery feature location and software artifact labeling.
the results of our empirical studies demonstrate that lda ga is able to identify robust lda configurations which lead to a higher accuracy on all the datasets for these se tasks as compared to previously published results heuristics and the results of a combinatorial search.
index terms textual analysis in software engineering latent dirichlet allocation genetic algoritms.
i. i ntroduction a significant amount of research on applying information retrieval ir methods for analyzing textual information in software artifacts has been conducted in the se community in recent years.
among the popular and promising ir techniques used we enumerate latent semantic indexing lsi and latent dirichlet allocation lda .
the latter is a probabilistic statistical model that estimates distributions of latent topics from textual documents.
it assumes that these documents have been generated using the probability distribution of these topics and that the words in the documents were generated probabilistically in a similar manner.
a number of approaches using lsi and lda have beenproposed to support software engineering tasks feature location change impact analysis bug localization clone detection traceability link recovery expert developer recommendation code measurement artifact summarization and many others .
in all these approaches lda and lsi have been used on software artifacts in a similar manner as they were used on natural language documents i.e.
using the same settings configurations and parameters because the underlying assumption was that source code or other software artifacts and natural language documents exhibit similar properties.
more specifically applying lda requires setting the number of topics and other parameters specific to the particular lda implementation.
for example the fast collapsed gibbs sampling generative model for lda requires setting the number of iterations nand the dirichlet distribution parameters and .
even though lda was successfully used in the ir and natural language analysis community applying it on software data using the same parameter values used for natural language text did not always produce the expected results .
as in the case of machine learning and optimization techniques a poor parameter calibration or wrong assumptions about the nature of the data could lead to poor results .
recent research has challenged this assumption and showed that text extracted from source code is much more repetitive and predictable as compared to natural language text .
according to recent empirical findings corpus based statistical language models capture a high level of local regularity in software even more so than in english .
this fundamental new research finding explains in part why these fairly sophisticated ir methods showed rather low performance when applied on software data using parameters and configurations that were generally applicable for and tested on natural language corpora.
this paper builds on the finding that text in software artifacts has different properties as compared to natural language text thus we need new solutions for calibrating and configuring lda and lsi to achieve better acceptable performance on software engineering tasks.
this paper introduces lda ga a novel solution that uses a genetic algorithm ga to determine the near optimal configuration for lda in the context of three different software engineering tasks namely traceability link recovery feature location and software artifacts labeling.
the contributions of our paper are summarized as follows we introduced lda ga a novel and theoretically sound approach for calibrating lda on software text corpora using a ga and we show that it can be applied successfully on three software engineering tasks traceability link recovery feature location and software artifact labeling we conducted several experiments to study the performance of lda configurations based on lda ga with those previously reported in the literature to perform such a comparison we replicated previously published case studies we compared lda ga with existing heuristics for calibrating lda the empirical results demonstrate that our proposed approach is able to identify lda configurations that lead to better accuracy as compared to existing heuristics we make publicly available in our online appendix1all the data results and algorithms used in our studies for replication purposes and to support future studies.
the paper is organized as follows.
section ii provides background notions for lda and overviews its applications to se problems.
section iii describes the lda ga approach.
section iv describes the empirical study aimed at applying lda ga in the context of traceability link recovery feature location and software artifact labeling.
results are reported and discussed in section v while section vi discusses the threats to validity that could have affected our study.
finally section vii concludes the paper and outlines directions for future work.
ii.
b ackground and related work this section provides i background information about lda ii discussions about its applications to software engineering tasks and iii discussions about related approaches aiming at determining the best configuration for lda.
a. lda in a nutshell latent dirichlet allocation lda is an ir model that allows to fit a generative probabilistic model from the term occurrences in a corpus of documents.
specifically given a collection of documents the ir process generates a m n term by document matrix m wheremis the number of terms occurring in all artifacts and nis the number of artifacts in the repository.
a generic entry wijof this matrix denotes a measure of the weight i.e.
relevance of the ithterm in the jthdocument .
one of the most used weighting schemas which we also applied in this work is the tf idf since it gives more importance to words having high frequency in a document and appearing in a small number of documents .
the term by document matrix is taken as an input by lda which identifies the latent variables topics hidden in the data and generates as output a k nmatrix called topic by document matrix wherekis the number of topics andnis the number of documents.
a generic entry ijof such a matrix denotes the probability of the jthdocument to belong to the ithtopic.
since typically k m lda is mapping the documents from the space of terms m into a smaller space of topics k .
the latent topics allow us to cluster them on the basis of their shared topics.
more specifically documents having the same relevant topics are grouped in the same cluster and documents having different topics belong to different clusters.
lda requires as input a set of hyper parameters i.e.
a set of parameters that have a smoothing effect on the topic model generated as output .
in this work we used the fast collapsed gibbs sampling generative model for lda because it provides the same accuracy as the standard lda implementation yet it is much faster .
for such an implementation the set of hyper parameters are k which is the number of topics that the latent model should extract from the data.
to some extent this is equivalent to the number of clusters in a clustering algorithm n which denotes the number of gibbs iterations where a single iteration of the gibbs sampler consists of sampling a topic for each word which influences the topic distributions per document.
a high value results in a better smoothing of the topics for each document i.e.
the topics are more uniformly distributed for each document which affects the term s distribution per topic.
a high value results in a more uniform distribution of terms per topic.
note that k and are the parameters of any lda implementation while nis an additional parameter required by the gibbs sampling generative model.
b. lda applications to software engineering some recent applications of lda to se tasks operate on models of software artifacts e.g.
source code rather than directly on those artifacts.
approaches that generate these models require as input a corpus i.e.
a document collection that represents the software artifacts being analyzed.
the corpus is constructed from textual information embedded in the artifacts including identifiers and comments.
while a number of different se tasks have been supported using advanced textual retrieval techniques such as lda the common problem remains the way lda is commonly configured is based on the assumption that the underlying corpus is composed of natural language text.
in our survey of the literature the following se tasks have been supported using lda and all of these papers and approaches used adhoc heuristics to configure lda perhaps resulting in suboptimal performance in virtually all the cases feature location bug localization impact analysis source codelabeling aspect identification expert identification software traceability test case prioritization and evolution analysis .
c. approaches for estimating the parameters of lda finding an lda configuration that provides the best performance is not a trivial task.
some heuristics have been proposed however these approaches focus only on identifying the number of topics that would result in the best performance of a task while ignoring all the other parameters that are required to apply lda in practice.
moreover such approaches have not been evaluated on real se applications or have been defined for natural language documents only thus they may not be applicable for software corpora.
one such technique is based on a heuristic for determining the optimal number of lda topics for a source code corpus of methods by taking into account the location of these methods in files or folders as well as the conceptual similarity between methods .
however the utility of this heuristic was not evaluated in the context of specific se tasks.
on a more theoretical side a non parametric extension of lda called hierarchical dirichlet processes tries to infer the optimal number of topics automatically from the input data.
griffiths and steyvers proposed a method for choosing the best number of topics for lda among a set of predefined topics.
their approach consists of i choosing a set of topics ii computing a posterior distribution over the assignments of words to topics p z w t iii computing the harmonic mean of a set of values from the posterior distribution to estimate the likelihood of a word belonging to a topic i.e.
p w t and iv choosing the topic with the maximum likelihood.
in their approach the hyper parameters and are fixed and only the number of topics is varied which in practice is not enough to properly calibrate lda.
in our approach we vary all the parameters i.e.
k n and to find a near optimal configuration for lda.
iii.
f inding a near optimal lda c onfiguration for software as explained in section ii lda and in particular its implementation based on fast collapsed gibbs sampling generative model requires the calibration of four parameters k n and .
without a proper calibration or with an ad hoc calibration of these parameters lda s performance may be sub optimal.
finding the best configuration of these parameters poses two problems.
firstly we need a measure that can be used to assess the performances of lda before applying it to a specific task e.g.
traceability link recovery .
this measure should be independent from the supported se task.
in other words we cannot simply train an lda model on the data for one particular task since obtaining such data means solving the task.
for example for traceability link recovery if we identify all the links to assess the quality of the lda model for extracting the links themselves then there is no need to have an lda based model to recover these links anymore.
in other words we need to build such a model onraw data e.g.
source code and documentation without having additional information about the links.
secondly we need an efficient way to find the best configuration of parameters as an exhaustive analysis of all possible combinations is impractical due to i the combinatorial nature of the problem i.e.
the large number of possible configuration values for the lda parameters as well as ii the large amount of computational time required for even such a configuration.
in the next section we present our approach that addresses these problems called lda ga which is able to find a near optimal configuration for the parameters of lda.
a. assessing the quality of an lda configuration lda can be considered as a topic based clustering technique which can be used to cluster documents in the topics space using the similarities between their topics distributions.
our conjecture is that there is a strong relationship between the performances obtained by lda on software corpora and the quality of clusters produced by lda.
thus measuring the quality of the produced clusters could provide some insights into the accuracy of lda when applied to software engineering tasks.
indeed if the quality of the clusters produced by lda is poor it means that lda was not able to correctly extract the dominant topics from the software corpus because the documents which are more similar to each other are assigned to different clusters i.e.
lda assigns different dominant topics to neighboring documents .
in our paper we use the concept of a dominant topic to derive the textual clustering generated by a particular lda configuration applied on a term by document matrix.
formally the concept of a dominant topic can be defined as follows definition .
let be the topic by document matrix generated by a particular lda configuration p .
a generic document djhas a dominant topic ti if and only if i j max h j h ...k .
starting from the definition of the dominant topic we can formalize how lda clusters documents within the topic space the number of clusters is equal to the number of topics as follows definition .
let be the topic by document matrix generated by a particular lda configuration p .
a generic document djbelongs to the ithcluster if and only if tiis the dominant topic of dj.
thus we can define a cluster as a set of documents in which each document is closer i.e.
shares the same dominant topic to every other document in the cluster and it is further from any other document from the other clusters.
it is worth noting that the concept of a dominant topic is specific to software documents only.
collections of natural language documents are usually heterogeneous meaning that documents can contain information related to multiple topics.
in source code artifacts heterogeneity is not always present especially when considering single classes.
more specifically a class is a0 1 2 3 4 5 67 8 9 10 0 1 2 3 4 5 6 7 8 9 10 term 2 term 1 di c2 c1 fig.
.
example of the silhouette coefficient.
crisp abstraction of a domain solution object and should have a few clear responsibilities.
hence software documents should be clustered considering only the dominant topic assuming that each document is related to only one specific topic.
different lda configurations provide different clustering models of the documents.
however not all clustering models that can be obtained by configuring lda are good.
there are two basic ways to evaluate the quality of a clustering structure internal criteria based on similarity dissimilarity between different clusters and external criteria which uses additional and external information e.g.
using judgement provided by users .
since the internal criterion does not require any manual effort and it is not software engineering task dependent in our paper we use the internal criteria for measuring the quality of clusters.
more specifically we use two types of internal quality metrics cohesion similarity which determines how closely related the documents in a cluster are and separation dissimilarity which determines how distinct or well separated a cluster is from other clusters .
since these two metrics are contrasting each other we use a popular method for combining both cohesion and separation in only one scalar value called silhouette coefficient .
the silhouette coefficient is computed for each document using the concept of centroids of clusters.
formally let cbe a cluster its centroid is equal to the mean vector of all documents belonging to c centroid c di cdi c .
starting from the definition of centroids the computation of the silhouette coefficient consists of the following three steps for document di calculate the maximum distance from dito the other documents in its cluster.
we call this valuea di .
for document di calculate the minimum distance from dito the centroids of the clusters not containing di.
we call this value b di .
for document di the silhouette coefficient s di is s di b di a di max a di b di the value of the silhouette coefficient ranges between 1and .
a negative value is undesirable because it corresponds to the case in which a di b di i.e.
the maximum distance to other documents in the cluster is greater then the minimum distance to other documents in other clusters.
for measuring the distance between documents we used the euclidean distance since it is one of the most commonly used distance functions for data clustering .
fig.
provides a graphical interpretation of the silhouette coefficient computed for a document di.
in particular it represents an example of a good silhouette coefficient since diis close to the furthest document situated in its cluster and far from the centroid of the nearest cluster.
in the end the overall measure of the quality of a clustering c c1 ... c k can be obtained by computing the mean silhouette coefficient of all documents.
letc c1 ... c k be the clustering obtained using a particular lda configuration and let mbe anm nterm bydocument matrix.
the mean silhouette coefficient is computed as s c nn i 1s di in this paper we used the mean silhouette coefficient as the measure for predicting the accuracy of lda in the context of specific software engineering tasks.
b. finding a near optimal lda configuration based on the conjecture that the higher the clustering quality produced by lda the higher the accuracy of lda when used for software engineering tasks we present an approach to efficiently identify the lda configuration p that maximizes the overall quality measured using the mean silhouette coefficient of the clustering produced by lda.
for solving such an optimization problem we applied genetic algorithms ga which is a stochastic search technique based on the mechanism of a natural selection and natural genetics.
since the introduction of ga by holland in the 1970s this algorithm has been used in a wide range of applications where optimization is required and finding an exact solution is nphard.
the advantage of ga with respect to the other search algorithm is its intrinsic parallelism i.e.
having multiple solutions individuals evolving in parallel to explore different parts of the search space.
the ga search starts with a random population of solutions where each individual i.e.
chromosome from the population represents a solution of the optimization problem.
the population evolves through subsequent generations and during each generation the individuals are evaluated based on the fitness function that has to be optimized.
for creating the next generation new individuals i.e.
offsprings are generated by i applying a selection operator which is based on the fitness function for the individuals to be reproduced ii recombining with a given probability two individuals from the current generation using the crossover operator and iii modifying with a given probability individuals using the mutation operator .
more details about ga can be found in a book by goldberg .in our paper we used a simple ga with elitism of two individuals i.e.
the two best individuals are kept alive across generations .
individuals solutions are represented as an array with four floats where each element represents k n and respectively.
thus an individual or chromosome is a particular lda configuration and the population is represented by a set of different lda configurations.
the selection operator is the roulette wheel selection which assigns to the individuals with higher fitness a higher chances to be selected.
the crossover operator is the arithmetic crossover that creates new individuals by performing a linear combination with random coefficients of the two parents.
the mutation operator is the uniform mutation which randomly changes one of the genes i.e.
one of the four lda parameter values of an individual with a different parameter value within a specified range.
the fitness function that drives the ga evolution is the silhouette coefficient described in section iii a. our ga approach can be briefly summarized as i generating lda configurations ii using them to cluster documents iii evaluating the cluster quality using the silhouette coefficient and iv using that value to drive the ga evolution.
iv.
e mpirical study definition this section describes the design of the empirical studies that we conducted in order to evaluate lda ga in the context of three software engineering tasks.
the studies aim at answering the following research questions rq1 what is the impact of the configuration parameters on lda s performance in the context of software engineering tasks?
this research question aims at justifying the need for an automatic approach that calibrates lda s settings when lda is applied to support se tasks.
for this purpose we analyzed a large number of lda configurations for three software engineering tasks.
the presence of a high variability in lda s performances indicates that without a proper calibration such a technique risks being severely under utilized.
rq2 does lda ga our proposed ga based approach enable effective use of lda in software engineering tasks?
this research question is the main focus of our study and it is aimed at analyzing the ability of ldaga to find an appropriate configuration for lda which is able to produce good results for specific software engineering tasks.
we address both research questions in three different scenarios representative of se tasks that can be supported by lda traceability link recovery feature location and software artifact labeling.
lda was previously used in some of these tasks .
for our data and results please visit our online appendix.
a. scenario i traceability links recovery in this scenario we used lda to recover links between documentation artifacts e.g.
use cases and code classes.
the experiment has been conducted on software repositories from two projects easyclinic and etour.
easyclinic is a systemtable i characteristics of the systems used in the three scenarios .
scenario i traceability link recovery system klocsource target correct artifacts artifacts links easyclinic uc cc etour uc cc uc use case cc code class scenario ii feature location system kloc classes methods features jedit argouml scenario iii software artifact labeling system kloc classes sampled classes jhotdraw exvantage used to manage a doctor s office while etour is an electronic touristic guide.
both systems were developed by the final year master s students at the university of salerno italy .
the documentation source code identifiers and comments for both systems are written in italian.
the top part of table i reports the characteristics of the considered software systems in terms of type number of source and target artifacts as well as kilo lines of code kloc .
the table also reports the number of correct links between the source and target artifacts.
these correct links which are derived from the traceability matrix provided by the original developers are used as an oracle to evaluate the accuracy of the proposed traceability recovery method.
to address rq1 we compared the accuracy of recovering traceability links using different configurations for lda.
specifically we varied the number of topics from to with step on easyclinic and from to with step on etour.
we varied and from to with .
increments and we exercised all possible combinations of such values.
we fixed the number of iterations to which resulted to be a sufficient number of iterations for the model to converge.
thus the total number of trials performed on easyclinic and etour were and respectively.
clearly although combinatorial such an analysis is not exhaustive as it considers a discrete set of parameter values and combinations.
for rq2 we compared the accuracy achieved by lda when the configuration is determined using ldaga with i the best accuracy achieved by lda determined when answering rq and ii the accuracy achieved by lda on the same system in the previously published studies where an ad hoc configuration was used .
while the former comparison is more of a sanity check aimed at analyzing the effectiveness of the ga in finding a near optimal solution the latter comparison was aimed at analyzing to what extent lda ga is able to enrich the effectiveness and usefulness of lda in the context of traceability link recovery when properly calibrated.
when addressing rq1 we evaluated lda s recovery accuracy using the average precision metric which re turns a single value for each ranked list of candidate links provided.
for rq2 we used two well known ir metrics precision and recall .
the precision values achieved for different configurations over different levels of recall are then pairwise compared using the wilcoxon rank sum test.
since this requires performing three tests for each system we adjusted the p values using holm s correction procedure .
this procedure sorts the p values resulting from ntests in ascending order multiplying the smallest by n the next by n and so on.
b. scenario ii feature location in this scenario we used lda to locate features within the textual corpus of source code.
the context of this scenario is represented by two software systems jedit v4.
andargouml v0.
.
jedit is an open source text editor for programmers while argouml is a well known uml editor.
table i reports the characteristics of the considered software systems in terms of number of classes number of methods as well as kloc and the number of features to be located.
these software systems have been used in previous studies on feature location .
for more information about the datasets refer to .
to answer rq1 we compared the effectiveness measure of lda using different configurations.
specifically we varied the number of topics from to with step for both argouml and jedit.
we varied and from to with .
increments.
similarly to the traceability task we fixed the number of iterations to .
we exercised all possible combinations of such values.
thus the total number of trials performed on both software systems consisted of different lda combinations.
for rq2 similarly to the previous scenario we compared the performance achieved by ldaga with i the best performance achieved by lda when answering rq1and ii the performance obtained by lda using the source locality heuristic proposed by grant and cordy for the feature location task .
the performance of lda in this scenario was analyzed using the effectiveness measure em .
given a feature of interest this measure estimates the number of methods a developer needs to inspect before finding a method relevant to that feature the list of methods are ranked by their similarity to the description of the feature .
a lower value for the em indicates less effort i.e.
fewer methods to analyze before finding a relevant one .
the em computed for different configurations on different queries i.e.
feature descriptions were then pairwise compared using a wilcoxon rank sum test similarly to the evaluation from scenario i and also in this case the p values were adjusted using holm s procedure.
c. scenario iii software artifact labeling in this scenario we used lda to automatically label source code classes using representative words.
specifically we extracted topics from a single class using lda and then we ranked all the words characterizing the extracted topics according to their probability in the obtained topic distribution.
the top words belonging to the topic with the highestprobability in the obtained topic distribution were then used to label the class .
the study was conducted on classes from jhotdraw and classes from exvantage.
the former is an opensource drawing tool and the latter is a novel testing and generation tool.
their characteristics are summarized in the bottom part of table i. for the sampled classes we had user generated labels from a previously published work and these represented our ideal labels.
after obtaining the lda labels we compared them to the user generated ones and computed the overlap between them.
the overlap was measured using the asymmetric jaccard measure .
formally let k ci t1 ... tm andkmi ci t1 ... th be the sets of keywords identified by subjects and the technique mi respectively to label the class ci.
the overlap was computed as follows overlap mi ci k ci kmi ci kmi ci note that the size of k ci might be different from the size ofkmi ci .
in particular while the number of keywords identified by lda is always by construction we set h the number of keywords identified by subjects could be more or less than generally it is but there are few cases where the number is different .
for this reason we decided to use the asymmetric jaccard to avoid penalizing too much the automatic method when the size of k ci is less than .
also in this scenario in order to address rq1we compared the recovery accuracy of lda using different settings.
specifically we varied the number of topics from to with step for both jhotdraw and exvantage.
as for and we varied them between and by increments of .
.
we fixed the number of iterations to as in the previous two tasks.
we exercised all possible combinations of such values.
thus the total number of trials performed on jhotdraw and exvantage was on both systems.
for rq2 we compared the accuracy achieved by lda ga with i the best accuracy achieved by lda while iterating through the parameters and ii the accuracy achieved by lda reported by de lucia et al.
.
d. lda ga settings and implementation the lda ga has been implemented in r using the topicmodels andgalibraries.
the former library provides a set of routines for computing the fast collapsed gibbs sampling generative model for lda while the latter is a collection of general purpose functions that provide a flexible set of tools for applying a wide range of ga methods.
for ga we used the following settings a crossover probability of .
a mutation probability of .
a population of individuals and an elitism of individuals.
as a stopping criterion for the ga we terminated the evolution if the best results achieved did not improve for generations otherwise we stopped after generations.
all the settings have been calibrated using a trial and error procedure and some of themeasyclinic etour0.
.
.
.
.5average precision a traceabilityargouml jedit200 1200mean effectiveness measure b feature locationjhotdraw exvantage0.
.
.
.
.
.7mean overlap c labeling fig.
.
variability of performance achieved by lda configurations for a traceability link recovery b feature location and c labeling.
i.e.
elitism size crossover and mutation probabilities were the values commonly used in the community.
to account for ga s randomness for each experiment we performed ga runs and then we took the configuration achieving the median final value of the fitness function i.e.
of the silhouette coefficient .
v. e mpirical study results this section discusses the results of our experiments conducted in order to answer the research questions stated in section iv.
we report the results for each lda application scenario.
a. scenario i traceability link recovery as for rq1 fig.
a shows boxplots summarizing the average precision values obtained using the and different lda configurations described in section iv on easyclinic and etour respectively.
we used these boxplots to highlight the variability of the average precision values across different configurations.
as shown the variability of lda s performance is relatively high the average precision ranges between and on easyclinic and between and on etour.
for easyclinic more than of the different lda configurations obtained an average precision lower than see first three quartiles in fig.
a .
moreover only a small percentage of the configurations executed in the combinatorial search about .
obtained an average precision greater than .
in the end only one of them achieved the highest value .
similarly for etour the configurations placed in the first three quartiles about of the set obtained an average precision lower than while less than of the total amount of executed configurations in the combinatorial search configurations achieved an average precision greater than the .
only one configuration achieved the highest average precision .
in summary for rq1we can assert that for traceability recovery lda shows high variability.
thus lda s efficiency for establishing links between software artifacts depends on the particular configuration p used to derive latent topics.
indeed bad configurations can produce poor0 10 20 30 40 50 60 70 80 90 100 precision recall a easyclinic 10 20 30 40 50 60 70 80 90 100 precision recall b etour fig.
.
traceability recovery precision recall graphs.
results while optimal configurations which represent a small portion of all possible lda configurations can lead to very good results.
regarding rq2 fig.
reports the precision recall graphs obtained by lda using i the best configuration across and different configurations executed in the combinatorial search ii the configuration identified by lda ga and iii an ad hoc configuration used in a previous study where lda was used on the same repositories .
for both easyclinic and etour lda ga was able to obtain a recovery accuracy close to the accuracy achieved by the optimaltable ii results of the wilcoxon test for traceability recovery .
easyclinic etour lda ga combinatorial lda ga oliveto et al.
.
.
combinatorial oliveto et al.
.
.
combinatorial lda ga .
configuration across and different configurations executed in the combinatorial search.
in particular for easyclinic lda ga returned exactly the configuration identified by the combinatorial search i.e.
the two curves are perfectly overlapped while on etour the two curves are comparable.
moreover the average precision achieved by the configuration provided by lda ga is about which is comparable with the average precision achieved with the the optimal configuration which is about only a small difference of .
among different configurations tried for the combinatorial search only five configurations obtained an average precision comparable or greater than the one achieved by lda ga i.e.
the configurations obtained by lda ga belong to the percentile for the distribution reported in fig.
a. finally comparing the performance achieved by lda ga with the performance reached by other lda configurations used in previous work we can observe that the improvement is very substantial for both software systems.
table ii reports the results of the wilcoxon test i.e.
the adjusted p values for all combinations of the techniques statistically significant results are highlighted in bold face .
as we can see there is no statistically significant difference between the performance obtained by lda ga and the combinatorial search for easyclinic.
however for etour the combinatorial search performs significantly better than lda ga. however considering the precision recall graph reported in fig.
we can observe that the difference is relatively small.
b. scenario ii feature location fig.
b shows the boxplots summarizing the variability of the average effectiveness measure em values obtained using different lda configurations as explained in section iv.
as in the previous task the feature location results show high variability in their em which ranges between and for argouml and between and for jedit.
for argouml we observed that more than of different configurations produced an average em ranging between and while only a small percentage about produced an optimal average em lower than .
within this small number of optimal configurations only one configuration obtains the lowest i.e.
the best em of .
similarly for jedit of different configurations produced an average em that ranges between and while only one achieved the smallest average em of .
these results for rq1suggest that without a proper calibration the performance of lda risks of being unsatisfactory.
forrq2 fig.
a shows boxplots for argouml of the em values achieved by three different configurations i the a argouml b jedit fig.
.
box plots of the effectiveness measure for feature location for argouml and jedit.
table iii results of the wilcoxon test on feature location performances .
jedit argouml lda ga combinatorial .
lda ga source locality heuristic .
.
combinatorial source locality heuristic .
.
best configuration obtained by a combinatorial search across different lda configurations combinatorial search ii the configuration obtained using lda ga and iii the best configuration obtained using the source locality heuristic .
first we can note that the configuration obtained via lda ga is exactly the same as the one obtained from the combinatorial search thus lda ga was able to find the best configuration i.e.
with the lowest average em .
comparing the performance of lda ga with the source locality heuristic we can observe that for the first two quartiles there is no clear difference the median values are and for ldaga and source locality heuristic respectively .
considering the third and fourth quartiles the difference becomes substantial the third quartile is for lda ga and for the previous heuristic while for the fourth quartiles we obtained for lda ga and for source locality heuristic.
overall lda ga reached an average em equal to as opposed to em equal to obtained using the source locality heuristic.
table iii reports the results of the wilcoxon test i.e.
the adjusted p values for all combinations of the techniques statistically significant results are shown in bold face .
as we can see there is no statistical difference between the performance obtained by lda ga and the combinatorial search.
based on the results of the statistical tests we can assert that lda ga is able to find the optimal or the near optimal configurations.
moreover lda ga significantly outperforms the previously published source locality heuristic p value .
.
c. scenario iii software artifact labeling forrq1 fig.
c shows boxplots for the average percentage overlap ao values obtained using different lda configurations as explained in section iv.
even if in this case the corpus of documents the total number of classes and the vocabulary size is really small as compared to thetable iv average overlap between automatic and manual labeling .
exvantage lda de lucia et al.
lda ga combinatorial n m n m n max 3rd quartile median 2nd quartile min mean st. deviation jhotdraw lda de lucia et al.
lda ga combinatorial n m n m n max quartile median quartile min mean st. deviation size of the repository considered for the other tasks lda also shows a high variability of performances ranging between and on jhotdraw and between and on exvantage.
for jhotdraw it can be noted how more than of the different configurations obtained an ao value ranging between and while only a small percentage about obtains an optimal ao greater than .
within this small number of optimal configurations only one achieves the highest ao of .
similarly for exvantage the majority about of the different configurations obtained an ao ranging between and while only one configuration achieved the highest ao of .
for rq2 table iv reports the statistics of the overlap between the user based labeling and the automatic labeling obtained using i lda ga ii the best configuration achieved using the combinatorial search i.e.
the configuration which has the higher percentage overlap among different configurations and iii the lda configuration used in the previous work for the same task.
for both systems ldaga obtains a percentage overlap with the user labeling that is close to the combinatorial search with a difference from the best lda configuration obtained by the combinatorial search of about for exvantage and for jhotdraw.
for exvantage among the different lda configurations computed in the combinatorial search only configurations have an average overlap greater or equal to .
.
we can also observe that there are only small differences for the median and second quartile between lda ga and the global optimum while for the other quartiles there is no difference.
similarly among different configurations evaluated for jhotdraw only one configuration is comparable with ldaga.
by comparing the quartile values obtained for jhotdraw we can note that the difference between lda ga and the combinatorial search optimum is about on average.
finally we can observe how the performances of lda configured using lda ga are significantly better than those reported in the previous work where and were set to defaultof kand .
respectively .
for exvantage we obtain an improvement in terms of mean overlap of about while for jhotdraw we get an improvement of about .
vi.
t hreats to validity threats to construct validity concern the relationship between theory and observation.
for tasks such as traceability link recovery and feature location we used well established metrics i.e.
precision recall and effectiveness and oracles used in previous studies thereby ensuring that the error proneness is limited.
for the labeling task we compared lda based labeling with a user generated labeling using again the dataset previously verified and published .
threats to internal validity can be related to co factors that could have influenced our results.
we limited the influence of ga randomness by performing ga runs and considering the configuration achieving the median performance.
we also observed that the configuration that we obtained did not substantially vary across ga runs.
threats to conclusion validity concern the relationship between treatment and outcome.
wherever appropriate we used non parametric statistical tests the wilcoxon test rank sum test in particular to support our claims.
threats to external validity concern the generalization of our results.
firstly it is highly desirable to replicate the studies carried out on three scenarios on other datasets.
secondly although the proposed approach can be applied in principle to other lda based solutions to support se tasks specific studies are needed to evaluate their feasibility and performances.
vii.
c onclusion and future work in this paper we proposed lda ga an approach based on genetic algorithms that determines the near optimal configuration for lda in the context of three important software engineering tasks namely traceability link recovery feature location and software artifact labeling.
we also conducted several experiments to study the performance of lda configurations based on lda ga with those previously reported in the literature i.e.
existing heuristics for calibrating lda and a combinatorial search.
the results obtained indicate that i applying lda to software engineering tasks requires a careful calibration due to its high sensitivity to different parameter settings that ii lda ga is able to identify lda configurations that lead to higher accuracy as compared to alternative heuristics and that iii its results are comparable to the best results obtained from the combinatorial search.
overall our empirical results warn the researchers about the dangers of ad hoc calibration of lda on software corpora as was predominantly done in the se research community or using the same settings and parameters applicable only to natural language texts.
without a sound calibration mechanism for lda on software data which might require using approaches such as the one proposed in this paper the potential of such a rigorous statistical method as lda can be seriously undermined as shown in our empirical study.future work will be devoted to corroborating the results reported in this paper on other datasets.
we also plan to apply lda ga on other se tasks that rely on text analysis using topic models.
viii.
a cknowledgments this work is supported in part by nsf ccf cns and nsf ccf awards.
any opinions findings and conclusions expressed here are the authors and do not necessarily reflect those of the sponsors.