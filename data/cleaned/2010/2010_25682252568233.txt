analyze this!
questions for data scientists in software engineering andrew begel microsoft research redmond wa usa andrew.begel microsoft.com thomas zimmermann microsoft research redmond wa usa tzimmer microsoft.com abstract in this paper we present the results from two surveys related to data science applied to software engineering.
the first survey solicited questions that software engineers would like data scientists to investigate about software about software process es and practices and about software engineers .
our analys es resulted in a list of questi ons grouped into categories.
t he sec ond survey asked a different pool of software engineers to rate these questions and identify the most important ones to work on first .
respondents favored questions that focus on how customers typically us e their application s. we also s aw opposition to questions that assess the performance of individual employees or compare them with one another .
our categorization and catalog of questions can help researchers practitioners and educators to more easily focus their efforts on topics that are import ant to the software industry .
categories and subject descriptors d. .
general terms management human factors m easurement keywords data science software engineering analytics .
introduction due to the increas ed availability of data and computing power over the past few years data science and analytics have become important topics of investigation .
businesses of all types commonly use a nalytics to better reach and understand their customers .
even sporting teams use analytics to improve their performance as described in the book moneyball .
many software engineering researchers have argued for more use of data for decision making .
as more and more companies start to analyze their software data the demand for data scientists in software projects will grow rapidly .
though harvard business review named the job of data scientist as the sexiest job of the 21st century by the u.s. may face a shortage of as many as people with analytical expertise and of .
million manager s and analyst s with the skills to make data driven decisions according to a report by the mckinsey global institute .
several people have offered advice on the important questions that academic and industry data scientists should focus.
in his two solitudes keynote at the mining software repositories vision event in kingston g reg wilson presented a list of ten questions for empirical researchers that a moz illa developer sent to him in response to the following request i m giving a talk on monday to a room full of softwa re engineering researchers who are specialists in data mining software repositories among other things .
if you could get them to tackle any questions at all well any related to software or software development what would you want them to do and why ?
in an introduction to a n empirical software engineering panel at esec fse bertrand meyer emphasized the need for the software engineering community to become more data driven and to shed folkloric advice and anecdotal evidence .
he presented a list of questions crying for evidence whose answers should be empirical credible and useful .
by useful meyer meant providing answers to questions of interest to practitioners .
in this paper we present a ranked list of questions that software engineers want to have answered by data scientists .
the list was compiled from two survey s that we deployed among professional software engineers at microsoft section .
.
in the first survey we asked a random sample of microsoft engineers a question similar to greg wilson s .
we asked please list up to five questions you would like a team of data scientists who specialize in studying how software is developed to answer .
after received response items from software engineers we filtered and grouped them into questions in categories .
we then distilled thes e into descriptive questions section .
.
.
we deployed a second survey to a new sample of microsoft engineers to help us prioritize the descriptive questions by indicat ing the most important ones to work on .
we received ratings from microsoft engineers.
these ratings additionally enabled us to identify differences of opinion between various demographic groups for example questions that were more important to testers than to developers sections .
and .
.
our findings suggest that engineers favor questions that focus on how customers typically use their application s. we also observe opposition against the use of analytics to assess the performance of individual employees or compare them with one another.
our catalog of questions is relevant for research industry practice and software engineering education section .
for researchers the descriptive question s outline opportunities to collaborate with industry and influence their software development processes practices and tools.
for those in industry the list of questions identifies particular data to collect and analyze to find answers as well as the ne ed to build collection and analysis tools at industrial scale.
lastly for educators the questions provide guidance on what analytical techniques to teach in courses for future data scientists as well as providing instruction on topics of importance to industry which students always appreciate .
as pointed out earlier there will be huge demand for people who are educated enough to know how to make use of data.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee .
icse may june hyderabad india .
copyright acm ... .
.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
copyright is held by the author owner s .
publication rights licensed to acm.
icse may june hyderabad india acm by in this paper we make the follow ing contributions we provide a catalog of questions that software engineers would like to ask data scientists about software.
section .
we rank the questi ons by importance and opposition to help researchers practitioners and educators focus their efforts on topics of importance to industry.
sections .
and .
we issue a call to action to other industry companies and to the academic community .
please replicate our methods and grow the body of knowledge from this modest start.
to support replication we provide the survey instructions and the full list of questions with ratings in a technical report .
.
related work the work related to this paper falls into software analytics and empirical collections of software engineering questions .
software analytics is a subfield of analytics with the focus on software data .
davenport harris and morison define analytics as the use of analysis data and systematic reasoning to make decisions.
software data can take many forms such as source code changes bug reports code reviews execution data user feedback and telemetry information.
analysis of so ftware data has a long tradition in the empirical software engineering software reliability and mining software repositories communities.
still according to an accenture survey of us manager s in industry up to percent of major decisions are based on gut feel rather than facts .
with the big data boom in recent years several research groups pushed for more use of data for decision making and shared their experiences collaborating with industry on analytics projects .
software analytics has been the dedicated topic of tutorials and panels at the icse conference as well as special issues of ieee software july and september .
zhang et al.
emphasized the trinity of software analytics in the form of three research topics development process system users as well as three technology pillars information visualization analysis algorithms large scale computing .
buse and zimmermann argued for a dedicated analyst role in software projects and presented a n empirical survey with professionals on guidelines for analytics in software development .
they identified seven typical scenarios and ranked popular indicato rs among professionals.
buse and zimmermann s focus was on the usage of analytics in general i.e.
how people make decisions rather than on collecting and ranking individual questions which is the focus of this paper.
information needs .
several researchers have identified information needs and created catalogs of frequently asked questions.
none of them focused on questions for data scientists however.
sillito et al.
observed developers to identify a catalog of questions that programmers ask d uring software evolution tasks .
phillips et al.
interviewed seven release managers in a large company to identify information needs related to integration decisions when releasing software.
ko et al.
observed developers at microsoft and identified types of information that they need.
fritz and murphy interview ed professional developers to identify questions developers want to ask but for which support is lacking due to lack of integration between different kinds of project information .
begel et al.
prioritize d different information needs about inter team coordination in a survey among software profe ssionals.
breu et al.
identified eight categories of information needs related to the interaction between developers and users based on a sample of bug reports.
roehm e t al.
observed developers to identify strategies they followed information they needed and tools they used in program comprehension tasks .
the scale of our study is larger the findings in this paper are based on input from software professionals in many different roles across three engineering disciplines development testing and program management .
.
methodology the findings in this paper come from two survey s .
an initial survey to solicit questions that software professionals would like data scientists to answer.
.
a second survey to rank and prioritize questions as well as identify questions that are more popular among certain groups of software professionals.
this section describes the surveys and the analysis of the responses in more detail.
we summarize our process in figure to which we refer with the circled numbers to .
.
initial survey our initial survey consisted of a single question suppose you could work with a team of data scientists and data analysts who specialize in studying how software is developed.
please list up to five questions you would like them to answer.
why do you want to know?
what would you do with the answers?
we additionally asked why do you want to know?
and what would you do with the answer ?
in order to gain context for the response.
we sent out two pilot surveys to and microsoft engineers and used the res ponses to improve our questions .
the pilot figure .
overview of the research methodology.
suppose you could work with a team of data scientists and data analysts who specialize in studying how software is developed .
please list up to five questions you would like them to answer .
survey participants response items r ..r728 categories questions in categories c ..c12 descriptive questions questions q ..q145 r1 r111 r432r544 r42r439r99 r528r488 r134 r355r399r380 r277r505 r488r409r606 r500r23 r256r418 r645r220 r214r189c1 c2 c3 c4 c5 c6 c7 c8 c9 c10 c11 c12 r369 r169r148 r567r88 r496r256 r515r601 r7 r12r599 q22q23q21 we used a split questionnaire design .
each participant received a subset of the questions q ..q145 on average .
and was asked in your opinion how important is it to have a software data analytics team answer this question ?
essential worthwhile unimportant unwise i don t understand survey ratings by participants top bottom ranked questions differences in demographicswe used an open card sort to group questions into categories .
we summarized each category with a set of descriptive questions .
also demonstrated the need to seed the survey with data analytics questions .
the following seed question s came from the pilot responses greg wilson s list and from our own ex perience s here are examples of questions that others have already asked.
do unit tests save more time in debugging than they take to write run keep updated?
do distributed version control systems offer any advantages over centralized version control systems?
is it really twice as hard to debug as it is to write the code in the first place?
when does it make sense to reinvent the wheel vs. use an existing libr ary?
what impact does code quality have on our ability to monetize a software service?
we sent the initial survey to microsoft software engineers randomly chosen by discipline in september .
since monetary incentives have been found to increase the participation in surveys we offered survey recipients the opportunity to enter a raffle for a visa check card.
we received items in responses for a response rate of .
.
this rate is comparable to the to and to reported for other surveys in software engineering .
the respondents had a median experience of years in the software industry by discipline .
were developers .
testers and .
program managers.
at microsoft program managers gather customer requirements write design specificat ions and manage the project schedule.
developers turn design specifications into implementation specifications and write the code.
testers turn design specifications int o testing specifications write test harnesses and conduct black box testing.
to gro up items into categories we used an open card sort .
card sorting is a technique that is widely used to create mental models and derive taxonomies from data.
in our case card sorting also helps us to deduce a higher level of abstraction and identify common themes.
cart sorting has three phases in the preparation phase we create cards for each question written by the respondents in the execution phase cards are sorted into meaningful groups wi th a descriptive title finally in the analysis phase abstract hierarchies are formed in order to deduce general categories and themes.
our card sort was open meaning we had no predefined groups instead we let the groups emerge and evolve during the sorting process see figure .
by contrast a closed card sort has predefined groups which is typically used when the themes are known in advance .
both authors jointly sorted cards over a series of several sessions until we agree d on a set of categories and subcategories .
we decided to discard out of the cards because they only made general comments on software development and did not inquire about any topic at all.
ultimately this resulted in questions in categories each with one to thirteen subcategories table breaks this down in detail .
in this paper we focus only on the main categories you may find the list of subcategories in a technical report .
we discuss t he individual categories in section .
.
after each one we list representative examples of respondent provided questions that were sorted into that category .
the respondents questions were not easy to work with many were the same or similar to one an other most were quite verbose and others were overly specific.
we distilled them into a set of descriptive questions that more concisely described each category and sub category .
we offer an example of respondent provided questions and the more concise descriptive question we created.
respondent provided questions how does the quality of so ftware change over time does software age?
i would use this to plan the replacement of components.
how do security vulnerabilities correlate to age complexity code churn etc.
of a code base?
identify areas to focus on for in depth security review or re architecting.
what will the cost of maintaining a body of code or particular solution be?
software is rarely a fire and forget proposition but usually has a fairly predictable lifecycle.
we rarely examine the long term cost of projects and the burden we place on ourselves and se as we move forward.
descriptive question that we distilled how does the age of code affect its quality complexity maintainability and security?
the number of descriptive questions for each category is shown in table in total we created descriptive questions .
the full list is available in our technical report .
.
the rating survey in order to rank and prioritize the descriptive questions we created a second survey.
however we felt that as king respondents to rate all of the questions would have put too much burden on them result ing in a low response rate and high drop off rate .
therefore we chose a split questionnaire survey design in which the questionnaire is split into component blocks .
each individual respondent is administered just a subset of the components .
comparisons between split questionnaires and complete surveys have found that little information is lost .
we defined our components to be the categories we identified in our card sort.
to make t hem similar in size we divided the development practice category in half and combined small categories together bug measurements with development best practices figure .
photo of some of the subcategories that emerged during the open card sort.
table .
the categories identified in the card sort.
category subcategoriesdescriptive questions bug measurements bug development practices dp development best practices best testing practices tp evaluating quality eq services svc customers and requirements cr software development lifecycle sl software development process proc productivity prod teams and collaboration tc reuse and shared components rsc discarded cards total cards 145cards14 productivity with reuse and shared components and services with software development lifecy cle .
this resulted in components blocks of descriptive questions.
each participant in our survey was present ed with two blocks chosen at random and asked to rate each descriptive question within.
in your opinion how important is it to have a software data analy tics team answer this question?
essential worthwhile unimportant unwise i don t understand we employed an asymmetric survey response scale inspired by prof. noriaki kano who created a model for product development and customer satisfact ion .
it measures three aspects must have essential nice to have worthwhile and detraction unwise .
we sent the rating survey to engineers randomly chosen by discipline who had not b een invited to the initial survey.
program managers were oversampled vs. because of their lower response in the initial survey.
respondents were enticed to respond with the opportunity to enter into a raffle for a visa check card.
we received responses to this survey for a response rate of .
.
this rate was better than our initial survey possibly because of the simpler multiple choice format .
.
of the respondents were developers .
were testers and .
were program managers.
the survey respondents submitted ratings.
on average each respondent rated .
questions while each question was rated by .
respondents.
the respondents used most of the scale on average .
of the five possible answers .
of respondents used four or all five answers and only three respondents gave the same answer for all questions that they rated.
for the response analysis we focused on the top and bottom rated questions as well as differences in ratings by demographics.
top rated bottom rated questions.
we followed the advice of kitchenham and pfleeger to dichotomize the ordinal kano scale to avoid any scale violations.
we computed the following percentage s for each descriptive question percentage of essential responses among all responses essential essential worthwhile unimportant unwise percentage of essential and worthwhile responses among all responses to which we refer as worthwhile essential worthwhile essential worthwhile unimportant unwise percentage of unwise responses among all responses unwise essential worthwhile unimportant unwise in addition we computed the rank of each question based on the above percentages with the top rank having the highest percentage in a dimension essential worthwhile or unwise .
in section .
of this paper we report the descriptive questions that are the most desired top essential top worthwhile and the most opposed top unwise .
the survey results for all questions are available as a technical report .
rating differences by demographics.
our survey was not anonymous so we could enrich the response data with the following demographics from the employee database discipline development testing program management region asia europe north america other number of full time employees who directly or indirectly report to the respondent.
current role manager if the respondent has reports number of full time employees individual contributor ic if the respondent has no reports.
years as manager the number of years that the respondent has ever worked in a management role over his tenure at microsoft decimals ok .
note some ics can also have management experience if they had been a manager at microsoft in the past but are not managers now .
has management experience yes if years as manager otherwise no.
years at microsoft the number of years that the respondent has been working at microsoft decimals ok .
to identify the demographics that influenced the responses for individual questions we built stepwise logistic regression models.
for each of the descriptive questions we built a model with the presence of an essential response yes no as dependent variable and the demographics above as independent variables.
we built similar models for the presence of worthwhile and unwise responses respectively.
in total we built models three for each of the descriptive question s. we used stepwise regression to eliminate demographi cs that did not improve the model for a given question and a given response.
in addition we removed demographics for which the coefficient in the logistic regression model was not statistically significant at p .
.
for exam ple for question how can we make it easier for people to find and use commonly used tools?
and the response essential after stepwise regression the logistic model included the demographics discipline testing discipline software development number of full time employees and has reports yes .
however the coefficient was only statistically significant at the .
level for discipline testing .
in this case we report the diff erence for the discipline demographics.
our analysis revealed cases where demographics responded diff erently to questions which we discuss in section .
.
.
limitations threats to validity program managers responded to the initial survey at a lower rate than the rating survey generating fewer questions than they rated.
to address any concerns related to sampling we report statistically significant differences in the ratings split by discipline .
drawing general conclusions from empirical studies in software engineering is difficult because any process depends on a potentially large number of relevant context variables .
since o ur surveys were conducted with microsoft engineers we cannot assume that the results will generalize outside of microsoft.
however there is nothing specific or different in the study that prevents replication at other companies or in the open source domain.
replicati ng our study in different organizational contexts will help generaliz e its results and build an empirical body of knowledge.
to facilitate replication both of our surveys and our list of descriptive questions are available in a technical report .
there are some frequent misconception s about empirical research within one company it is not good enough provides little value for the academic community and does not contribute to scientific develo pment.
historical evidence shows otherwise.
flyvbjerg provides several examples of individual cases that contributed to discovery in physics economics and social science s .
beveridge observed that for social sciences more discoveries have arisen from intense observation than from statistics applied to large groups as quoted in kuper and kuper .
note t his should not imply that research focusing on large samples or entire populations is not important .
on the contrary f or the development of an empirical body of knowledge both types of research are essential .
.
results the goal of this research was to uncover the kinds of software data analytics questions that professional software engineers want to ask.
while the research literature has informed our own understanding the microsoft software engineers we surveyed are informed by their own experiences design ing planning building testing and deploy ing very large software products.
in this section we report on the results from our two surveys and subsequent data analyses.
.
question categories from the questions we received during our initial survey we found out that our respondents are curious about a wide variety of topics that they would like to have a team of data scientists answer.
our card sort enabled us to group these questions into categories each described below.
afte r each category we list several of the respondents questions to illustrate the diversity of their interests.
bug measurements bug respondents were curious about many aspects of software bugs where they are found in code and in the product design w hich bugs are most commonly made by developers where in the application lifecycle they are caught both before and after the product ships how much it costs to fix bugs and how well tool automation helps to mitigate them.
in addition some respondents w ere curious about whether bug counts should be used to measure product quality and developer effectiveness.
for each bug at what stage in the development cycle was the bug found at what stage was it introduced at what stage could it have been found?
are there categories of mistakes that people make that result in hotfixes after the fact?
for example date time mistakes or memory allocation mistakes etc.
have you ever had your spec document worked through thoroughly by qa or a computer scientist th eorist to detect every single edge case beforehand?
did it help?
was it worth the effort?
development practices dp respondents asked about all kinds of code oriented practices including debugging performance profiling refactoring branching in repo sitories code reviewing commenting and documenting code.
in addition respondents expressed concern about development costs due to legacy code execution telemetry customer telemetry inaccurate effort estimation and product risk caused by code change s. how can i judge when adding telemetry instrumentation to code will pay off vs. when it will just tell me things i already know?
we are often told that code comments are important to assist others understand code but is this really true?
do many eyes make bugs shallow?
where is the break even?
what is the net cost of refactoring legacy code so that it can be covered with unit tests compared to leaving legacy code uncovered and making changes in it?
what are the provably best metrics to trac k during design planning to determine project complexity and cost?
what is the expected margin of error?
development best practices best this category resembles the development practices category above but respondents were specifically interested in the best or worst way to conduct a software practice.
for example respondents want to know the right technique for migrating between software versions the best way to track work items the right time to use formal methods for software analysis or whic h criteria should influence the decision to use a particular programming language or api.
what are the best and worst practices teams that miss deadlines or slip beyond their release schedule call out?
it would be great to note what practices team s adopt and or cut when they are feeling pressured to meet certain deadlines.
could machine learning be used to provide better coding guidelines and best practices?
how many times does a piece of code re written?
is it better to write code quick ly and then iterate on it or do a longer process of planning and just write it once ?
have you ever written a loop invariant to prove that your algorithm terminates?
did it help?
what s the recommended way to detect locate pii personally identifiable information inside arbitrary data?
are there any guidelines on how to detect it e.g.
shared rulesets ?
what are the time benefits to strict coding standards when code maintenance is inherited by another dev?
which coding guidelines patte rns have the most effect on code quality e.g.
small functions lower complexity metrics removal of duplicate code ?
how long does it take to gain a payoff from moving to new software writing tools?
testing practices tp testing was the most diverse category covering test automation testing strategies unit testing test driven development test coverage and test case elimination.
in addition respondents were interested in test processes for example writing tests who should do it and when shou ld it happen sharing tests and test infrastructures across teams and measuring the effectiveness of particular kinds of tests in finding impactful bugs.
what is the cost benefit analysis of the different levels of testing i.e.
unit testing vs. compone nt testing vs. integration testing vs. business process testing?
how much percentage of development time needs to be spent on unit testing to ensure quality?
at what point does it become too much time spent on writing test for every line?
what s the us eful lifetime of a test?
at what point does the test become so unlikely to uncover an issue that it isn t worth the maintenance and infrastructure cost of keeping it running?
how many spec code issues were found thanks to the creation of a test design sp ec?
what value is there from investigating regression data to improve future code quality if the regressions could not be found internally?
evaluating quality eq respondents questions included code optimization tradeoffs the best metrics for decid ing whether to ship software complexity metrics code duplication the relationship between test coverage and customer feature usage legacy and aging codebases and the most common causes of poor software quality.
this category is perhaps the most similar to what one would find in the research literature.
some means to get a relation defined between crashes found in real world and tests in our collateral which hit the same code path.
knowing we hit this code path but still we have a crashing bug in there would be good to know.
is there a difference in code coverage and number of actionable code bugs with respect to tests developed in native win32 vs. managed code?
do teams who leverage code coverage for more than reporting coverage percent to ma nagement deliver higher quality software?
how much self hosting beta time does a feature of a certain complexity need before it can be considered reliable?
how much do existing bugs in a code base affect the velocity with which new features can be adde d to that code base?
services svc many questions concerned developing software for the cloud.
some concerned how to change development and testing practices when migrating software to the cloud while others referenced operations and performance measu rement.
a set of respondents were interested in the effects on customer retention and monetization caused by moving to more frequent smaller releases from larger less frequent releases.
what are the kpis that are used in managing our on line services today?
this should be as standardized as possible.
do distributed systems scale better than centralized systems with vertical horizontal scaling?
when does it make sense to change a helper library to a service of its own?
what s the best approach for testing in production without affecting the live production data and compromising the performance while being able to test and regress all the functionality?
for online service agility and code quality are always what we want to achieve.
which is more important?
customers and requirements cr this category contained the majority of the concerns about customer interests ultimately focusing on how customers might react to various development tradeoffs for example the most liked and important featu res the necessity of backward compatibility or the impact of testing in production.
respondents were also interested in the payoff that arises from investing in specifications to various degrees and of the costs associated with increasing customer input into software design.
how many features of the s w are used by people and at what percentage?
it will help me understand what is the impact roi on adding a new complex feature in terms of the investment we put to add new feature vs. its impact on user.
how do new free online versions of apps impact sales and revenue of the full pay versions?
does testing in production hurt the reputation or perception of quality of product?
what factors should be considered before deprecating features?
how do we determine what problems our customers are having with our software?
i would like to know what is failing in our software when our customers use it.
is there a correlation between the level of detail in a functional requirements spec and the accuracy of dev estimates based on it?
software development lifecycle sl all of the questions in this category regarded time as a vital factor in designing the software lifecycle.
for example respondents wanted to find out how development time should be allocate d between planning design coding and testing and the impact of this allocation could have on the software.
from an efficiency standpoint how detailed should dev design docs be before just starting to code?
how do we monetize the intended savings when a decision is taken to make a change in product design?
for example if we mitigate a launch risk how do we figure out the money saved if the risk never become an issue?
i.e.
if i have to spend should i spend it on back up sensors or bigger mir rors?
which investment would save me more money?
how do i determine that?
what has a bigger impact on overall product quality up front standards and best practices for dev or longer testing and bake time before product release?
do shorter release cy cles result in a better quality product?
a more feature filled product?
software development process proc the choice of software methodology was on respondent s minds when they answered this question.
in what ways is agile better than waterfall?
what are the benefits of pair programming?
do particular methodologies work better for cloud services than for shrink wrapped products?
others were interested in whether microsoft would benefit from a company wide software process and tool chain and how to man age it .
how is agile process model more or less efficient in when it comes to number of bugs meeting deadlines customer satisfaction work life balance of dev test pm?
which software development model s are most successful in producing great software ?
is it more efficient for the company to have one consistent process for building software that all projects use rather than each team division project it their own way?
in an organization that is running scrum or any agile flavor development cycles how does it meet the challenge of leaving time for innovation?
productivity prod for many respondents this category is what they think of when they hear the term software data analytics and what many of them feel is the fear of the conseq uences caused by being unfairly judged by management.
questions in this category included investigating the quality of many different proxy metrics for measuring the productivity of software developers and understanding the relationship between individual productivity and team or product success.
some respondents wanted to know how to monitor their own or their team s productivity for instance when learning a new codebase or working with a new team member or dealing with the issues caused for a team mem ber leaving.
are there measurable differences in productivity between experienced developers with cs degrees and developers with unrelated degrees or no degrees?
are daily scrum status meetings effective?
does peer programming improve code qual ity and throughput?
how does productivity vary between open work spaces and traditional offices?
does churn in organizations cause software development to slow?
as people move between orgs do we see a slowdown in productivity?
are more senior staff actually more productive?
how do we measure the productivity of our engineers?
which engineer is more productive?
how do we measure the productivity or business savings with a specific feature or toolset?
does a person s language age experience o r gender affect the bugs s he may introduce find?
how well am i utilizing my resources?
what trace is left by activities per person?
not just code changes but bug database changes doc repository changes etc.
teams and collaboration tc respondents here cared about team makeup i.e.
how many people of various development roles should be on a team and how big it should be for a given feature size.
a second aspect of the questions wanted to learn which practices could improve sharing and collaboratio n within and between teams especially relating to shared knowledge code ownership task status and commonly used tools.
how can we share knowledge more effectively to code faster?
there re a lot of people who already have the knowledge but don t have the time to share it or just don t care to share it.
is integrated engineering i.e.
elimination of the test organizations a better use of budgets and resources than having dev program management and test?
how can i find the best developer contact f or a piece of apparently abandoned code?
what is the optimal team size n for a feature of scope m?
how much time week do you spend updating sending reporting status to other people?
how does one improve relations when cross org collaborations?
reuse and shared components rsc this was the smallest category to come from our card sort and concerned one main issue when should code be written from scratch vs. reused from another codebase?
respondents wanted to know the cost of searching for the right code and the potential impact caused by just linking to it vs. copying it wholesale.
reusing an existing library is always good and easy.
what s the best way to reuse an existing library if it matches the functionality we are looking for but does not match the architecture of our project?
when is it best to create your own ui model rather than adopt a standard that might be familiar but not fi t your use exactly right?
do shared codebases actually reduce developer productivity and innovation rates at the company?
how can i find out if someone else has already solved a similar problem before e.g.
caching datatype conversion etc.?
category relationship s the categories exist in a set of relationships having to do with people and the activities that connect them.
in figure we show an affinity chart with five job roles drawn as boxes that were mentioned in the respondents ques tions developer tester program manager customer and data analyst .
nearby each role are ovals listing popular activities done by people in those roles the attributes of which drew the attention of the respondents.
the lines drawn from the data analyst link her investigation and analyses to various development activities some of the activities are linked together themselves .
the three diamonds in the center of the figure indicate that collaboration knowledge sharing and training are three aspects th at affect personnel in all job roles.
finally shapes drawn with a double border indicate crosscutting attributes they apply across all of the roles the activities and the entire software product and development process itself.
.
top rated bottom rated questions table shows the questions with the highest percentages for the essential and worthwhile or higher worthwhile responses.
in the table we show the top questions for both essential and worthwhile however they overlap significantly .
as a result we only show questions.
in addition to the question the table shows the percentage of respondents who selected essential worthwhile and unwise as well as the ir corresponding ranks.
the table is sorted in decreasing order by the essential percentages.
one thing we noticed about the top list is the striking fraction of questions that are concerned about customers .
nine questions marked with icon q27 q18 q28 q66 q19 q131 q q25 and q17 including the top two demonstrate a concern by program managers developers and testers that they plan design implement test and profile the features that match what the customers expect and use and benefit the customers the most.
another eight questions marked with icon q115 q74 q84 q100 q83 q2 q3 and q102 focus on the engineer and the effects of software development practices and processes on her work.
the remaining seven questions marked with icon q50 q86 q40 q q59 q1 and q60 are chiefly concerned with product quality issues.
in table we show the questions with the most opposition by which we mean the highest percentages for the unwise response sorted in decreasing order by the unwise percentage .
of the questions with the most opposition in table the top five are about the fear that respondents had of being ranked and rated.
this may be due to the impact these ratings could have on the employee s yearly performance review which include the p ossibility of job termination for poor performance.
this fear has been seen before in companies that employ employee metrics collection and evaluation systems .
the other questions deemed unwise to be answered explore situations deemed improbable and unwise for example achieving test coverage is not often possible or helpful even when it happens.
second microsoft s feature teams employ a freedom of software tools and process that enables them to develop their own practices to suit their needs without requiring adherence to top down engineering management mandates.
this explains why q112 and q113 are on the list as well.
finally the last two questions could be perceived as tautological sta tements by respondents they might think that there is only one correct answer to each e.g.
q34 always and q24 as much as you can afford which may or may not be true.
looking at the percentages of essential in the top rankings we notice that there is a steep drop from the top questions to the remainder.
however considering worthwhile responses as well we see a much smoother decline in fact for questions more than of responses were worthwhile or higher worthwhile indicating that m any of the questions on the survey were worth having a team of data scientists look into the answers.
.
rating differences by demographics table shows demographics for which we identified statistically significant difference s in the ratings of the questions.
the table is grouped by discipline developer tester and program manager management role manager and individual contributor work region north america europe and asia years of experience as a manager and years working at the company.
in addition to the question we list the survey response for which the difference was observed in column response and show in the final column how the question was rated differently by the respective demographic .
the demographic with the highest rati ng for the first three demographics is highlighted in bold.
to explain further please look at the first entry of the table .
question how many new bugs are introduced for every bug that is fixed?
was rated by each discipline differently with respect to the survey response essential .
respondents in the test discipline were more likely to rate the question as essential .
than respondents in the development .
or program management .
disciplines.
for demographics related to years of experience years as manager years at company we report the change in odds per year .
look at the first entry under years as manager.
for every year that someone spent as a manager the odds that they rated question developer tester program manager customercode reviewrefactoringtest strategy testing methodsdevelopment methodsdev best practices coding architecture services version control planning writing specs gathering requirementscollaboration cross role cross org training hiring data analyst profiling telemetry knowledge sharingbugstools software lifecycle software development process figure .
relationship between card sort categories.
how much clo ned code is ok to have in a codebase?
with the response essential increased by in the survey.
discipline.
many of the differences here agree with common sense.
testers are more interested in test suites bugs and product quality while developers ca re about migrating code between versions of a library as also noted by mccamant and ernst .
testers also cared about helping people find commonly used tools the cost of customer input and identifying someone to unify the company s tools and processes.
management role.
more individual contributors than managers are interested in finding the legacy code in their systems and discover ing the optimal time in the development cycle to test their application s performance.
it was surprising that more individual contributors than mana gers want to find out how to measure an employee s productivity the fourth ranked unwise question .
we would have expected to see the opposite result.
managers are interested in finding the most commonly used tools on software teams possible to reduce cost and to improve their team s productivity .
region.
half of the respondents working in asia mainly india and china reported they wanted data scientists to look into employee productivity.
one possible explanation may be that employees in asia believe that striving for top personal performance is the best table .
questions with the highest essential and worthwhile or higher percentages.
table .
questions with the highest unwise percentage opposition .
percentages rank question category essential worthwhile unwise essential worthwhile unwise q27 how do users typically use my application?
dp .
.
.
q18 what parts of a software product are most used and or loved by customers?
cr .
.
.
q50 how effective are the quality gates we run at checkin?
dp .
.
.
q115 how can we improve collaboration and sharing between teams?
tc .
.
.
q86 what are best key performance indicators kpis for monitoring services?
svc .
.
.
q40 what is the impact of a code change or requirements change to the project and tests?
dp .
.
.
q74 what is the impact of tools on productivity?
prod .
.
.
q84 how do i avoid reinventing the wheel by sharing and or searching for code?
rsc .
.
.
q28 what are the common patterns of execution in my application?
dp .
.
.
q66 how well does test coverage correspond to actual code usage by our customers?
eq .
.
.
q42 what tools can help us measure and estimate the risk associated with code changes?
dp .
.
.
q59 what are effective metrics for ship quality?
eq .
.
.
q100 how much do design changes cost us and how can we reduce their risk?
sl .
.
.
q19 what are the best ways to change a product s features without losing customers?
cr .
.
.
q131 which test strategies find the most impactful bugs e.g.
assertions in circuit testing a b testing ?
tp .
.
.
q83 when should i write code from scratch vs. reuse legacy code?
rsc .
.
.
what is the impact and or cost of findings bugs at a certain stage in the development cycle?
q1 what is the impact and or cost of findings bugs at a certain stage in the development cycle?
bug .
.
.
q92 what is the tradeoff between releasing more features or releasing more often?
svc .
.
.
q2 what kinds of mistakes do developers make in their software?
which ones are the most common?
bug .
.
.
q25 how important is a particular requirement?
cr .
.
.
q60 how should we use metrics to help us decide when a feature is good enough to release or poor enough to cancel ?
eq .
.
.
q17 what is the best way to collect customer feedback?
cr .
.
.
q3 in what places in their software code do developers make the most mistakes?
bug .
.
.
q102 what kinds of problems happen because there is too much software process?
proc .
.
.
percentages rank question category essential worthwhile unwise essential worthwhile unwise q72 which individual measures correlate with employee productivity e.g.
employee age tenure engineering skills education promotion velocity iq ?
prod .
.
.
q71 which coding measures correlate with employee productivity e.g.
lines of code time it takes to build the software a particular tool set pair programming number of hours of coding per day language ?
prod .
.
.
q75 what metrics can be used to compare employees?
prod .
.
.
q70 how can we measure the productivity of a microsoft employee?
prod .
.
.
q6 is the number of bugs a good measure of developer effectiveness?
bug .
.
.
q128 can i generate test coverage?
tp .
.
.
q113 who should be in charge of creating and maintaining a consistent company wide software process and tool chain?
proc .
.
.
q112 what are the benefits of a consistent company wide software process and tool chain?
proc .
.
.
q34 when are code comments worth the effort to write them?
dp .
.
.
q24 how much time and money does it cost to add customer input into your design?
cr .
.
.
way to get ahead in one s career and uncovering the secret to improved productivity is one way to improve their performance.
testing questions also interested the employees from asia likely due to the large fraction of software testing work th ey do there .
years as manager .
in the survey responses m anagers with more experience care d less about investigating the impact of old code on product quality than those with less experience.
one confounding factor here is that teams that work on older software products e .g.
windows office usually have more experienced managers leading them while less experienced managers may be more involved in newer products e.g.
various web services .
years at company.
finally the more experience an employee has at microsoft the less they are interested in having a data science team explore the managed native code debate often a religious type argument with no consensual answer possible test driven development another religious battle or the preferred functions of people in testing and development roles again a religious battle .
employees who have been at microsoft longer have better knowledge sharing networks and thus would not need to rely on specialized tools as much to help them.
finally the more experience a person has at the company the less likely they are to want someone to look into who should create and maintain a standardized process and tool chain a tacit acceptance of the status quo.
table .
statistically significant rating differences by demographics.
the demographic with the highest rating is highlighted in bold.
questions that are also in table are shown in italics.
discipline question category response dev test pm q5 how many new bugs are introduced for every bug that is fixed?
bug essential .
.
.
q10 when should we migrate our code from one version of a library to the next?
best essential .
.
.
q20 how much value do customers place on backward compatibility?
cr essential .
.
.
q21 what is the tradeoff between frequency and high quality when releasing software?
cr essential .
.
.
q42 what tools can help us measure and estimate the risk associated with code changes?
essential dp essential .
.
.
q121 how can we make it easier for people to find and use commonly used tools?
tc essential .
.
.
q24 how much time and money does it cost to add customer input into your design?
unwise cr worthwhile .
.
.
q113 who should be in charge of creating and maintaining a consistent company wide software process and tool chain?
unwise proc worthwhile .
.
.
q132 how should we handle test redundancy and or duplicate tests?
tp worthwhile .
.
.
q134 should we develop a separate test suite for servicing a product after we ship it?
tp worthwhile .
.
.
management role manager ind.
contributor q29 how much legacy code is in my codebase?
dp worthwhile .
.
q31 when in the development cycle should we test performance?
dp worthwhile .
.
q70 how can we measure the productivity of a microsoft employee?
unwise prod worthwhile .
.
q120 what are the most commonly used tools on a software team?
tc worthwhile .
.
region asia europe north america q70 how can we measure the productivity of a microsoft employee?
unwise prod essential .
.
.
q104 how do software methodologies affect the success and customer satisfaction of shrinkwrapped and service oriented products?
proc essential .
.
.
q128 can i generate test coverage?
unwise tp essential .
.
.
q129 what is the effectiveness reliability and cost of automated testing?
tp essential .
.
.
years as manager change in odds per year q85 how much cloned code is ok to have in a codebase?
rsc essential q69 how does the age of code affect its quality complexity maintainbility and security?
eq worthwhile years at company change in odds per year q16 what criteria should we decide when to use managed code or native code e.g.
speed productivity functionality newer language features code quality ?
best essential q119 what are the best tools and processes for sharing knowledge and task status?
tc essential q142 should we do test driven development?
tp essential q95 how much time went into testing vs. into development?
sl worthwhile q123 how much distinction should there be between developer tester roles?
tc worthwhile q136 who should write unit tests developers or testers?
tp worthwhile q142 should we do test driven development?
tp worthwhile q144 how should we do test driven development while prototyping?
tp worthwhile q113 who should be in charge of creating and maintaining a consistent company wide software process and tool chain?
unwise proc unwise .
implications the results from our surveys can serve to guide future work agendas for academic researchers industry practitioners and data science educators.
.
research one of the challenges faced in academic research is to choose a research topic that is novel feasible and impactful .
while sel ection for novelty and feasibility are well understood and controllable impact often requires a partnership with extern al players from the software industry.
when the partnership is successful these players can serve as test beds for research validation experimentation new tools and ul timately amplification of the researcher s ability to spread his or her ideas and tools widely to software practitioners.
numerous studies have reviewed the performance of such partnerships to describe the impact of rese arch and glean the attributes that make for successful technology transfer .
one of the main findings is that to be successful the researchers need to discover and work on the problems that industry needs to be solved.
in this paper we have done the first part of that critical task by identifying the questions that microsoft a ve ry large globally distributed company working on devices and services would like to see answered by data scientists knowledgeable about how software is developed.
the questions in this paper can help guide academic researchers towards novel research problems to be studied and analyzed as well as algorithms processes and t ools to be built and tested .
by building new tools r esearchers can easily disseminate their ideas and m ake it possible for others to understand and discuss the data analyses that support them.
this should provide researchers a head start on the path to arrange academic industry collabo rative projects that address industry s software analysis needs.
as other software companies replicate our study the potential to increase the diversity of new avenues of research will grow and provide increasing opportuni ties for researchers to enjoy impactful research .
.
practice software p ractitioners often face the challenge of building the right set of tools to help their business grow without incurring too much cost to gather the required data analyze it and build tools to act on the results.
our results point at the kinds of data that should be regularly and sustainably collected in order to conduct data analyses that answer the engineers questions.
we do wish to stress that just because the survey respondents asked a question does not mean that microsoft or academia does not have tools and techniques to find the answe rs.
however any preexisting tools that could solve these problems may have been unknown to or unusable by some of those respondents.
thus even the presence of a question indicates an opportunity to build a better simpler more applicable tool and or find another way to distribute it to engineers.
industrial practitioners from large long lived institutions have an added challenge of answering all of these questions at scale.
for example a recent study at microsoft looked at failure and usage data from over a million computers to discover the difference in usage characteristics between pre release and post release failures .
an older study explored the linguistic attributes of identifiers in sourc e code by analyzing over million lines of code for windows server .
mockus analyzed years of change management system data and years of organizational reporting data at avaya to discover the effects of organizational volatility on software reliability .
academic researchers can help with these tasks by applying their work to large corpora but it is rare to find objects of study outside of industry that eve n come close to the size of typical industrial projects.
industry practitioners working together with academics could enable each to scale their data analyses and tools to work at the scale that industry requires.
.
education the set of descriptive questions that we include in this paper can be useful to undergraduate and graduate educators who teach computer science statistics and data science courses.
computer science students commonly hold wildly inaccurate preconceptions about the character of the work they will encounter in their future software engineering careers .
even beginning professional software engineers struggle to adapt the practices and processes they learned in university to the realities they find in industry .
this has not gone unnoticed by the industry either .
the categories and diversity of the questions collected in this paper can be used in software engineering cours es to illustrate real life challenges that professional software developers face when building large long lived products for a variety of customers including consumers enterprises and government institutions.
in addition the particular scenarios envis ioned in the questions can be used to contextualize the pedagogical materials created and shared by educators to motivate and excite students with challenges faced by professional software engineers rather than the typical problems faced by alyssa p. hack er and ben bitdiddle .
our corpus of questions can inform data science and statistics educators of particular analyses and techniques that are vitally importan t to the workings of for example deci sion support systems that help managers ensure a successful product release or of employee productivity measurement tools that can accurately and fairly evaluate and motivate an engineering workforce .
one can see from the examples of the respondents qu estions a range of sophistication in their phrasing and clarity.
if these were students one would hope for a coordinated set of less on plans that would result in an increased ability to pose high quality specific and actionable questions as well as write coherent report s and follow up with the teams that needed the answers to help them continuously implement and measure process and or tool improvements.
.
conclusion to understand the questions that software engineers would like to ask data scientists about software we conducted two surveys the first survey solicited questions and the second survey ranked a set of questions.
the result of our analysis of the survey responses is a catalog of questions grouped into categories as well as a ranking of the importance of each question.
this catalog can help researchers practitioners and educators to improve their focus and efforts on topics that are important to l arge software compan ies.
we hope that this paper will inspire similar research projects.
in order to facilitate replication of this work for additional engineering disciplines e.g.
user experience operations build release technical support and compan ies we provide the full text of both surveys as well as the questions in a technical report .
with the growing demand for data scientists m ore research is needed to better understand how peop le make decisions in software projects and what data and tools they need .
there is also a need to increase the data literacy of future software engineers.
lastly we need to think more about the consumer of analy ses and not just the producers of them data scientists empirical researchers .
.