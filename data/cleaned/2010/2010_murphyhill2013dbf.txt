the design of bug fix es emerson murphy hill department of computer science north carolina state university raleigh north carolina usa emerson csc.ncsu.edu thomas zimmerma nn christian bird nachiappan nagappan and michael barnett microsoft research redmond washington usa tzimmer cbird nachin mbarnett microsoft.com abstract when software engineers fix bugs they may have se veral options as to how to fix those bugs .
which fix is chosen has many implications both for practitioners and researchers what is the risk of introducing other bugs during the fix?
is the bug fix in the same code tha t caused the bug?
is the change fixing the cause or just covering a symptom?
in this paper we investigate the issue of alternative fixes to bugs and present an empirical study of how engineers make design choices about how to fix bugs .
based on qualitat ive interviews with engineer s working on a variety of products bug triage meetings and a survey filled out by engineer s we found that there are a number of factors many of them non technical that influence how bugs are fixed such as how clos e to release the software is .
we also discuss several implications for research and practice including ways to make bug prediction and localization more accurate.
keywords component bugs faults empirical study design i. introduction as the software sys tems we create and maintain continue to grow in capability and complexity software engineer s must ensure that these systems work as intended.
when systems do not software engineer s fix the bugs that cause this uninten ded behavior.
traditionally resear chers and practitioners have assumed that where in the software an engineer fixes a bug is where an error was made .
for example endes makes such an assumption in a study but cautions the reader that there is of course the initial question of how we can d etermine what the error really was.
to dispose of this que stion immediately we will say right away t hat in the mat erial described here normally the actual error was equa ted to the correction made.
this is not always quite acc urate because sometimes the real error lies too deep thus the expenditure in time is too great and the risk of intr oducing new errors is too high to attempt to solve the real error.
in these cases the correction made has probably only remedied a consequence of the error or circumven ted the problem.
to obtain greater accuracy in the anal ysis we really should instead of consideri ng the corre ctions made make a comparison between the originally i ntended implementation and the implementation actually carried out.
for this however we usually have neither the means nor the base material.
although the software engineering community h as suspected that this assumption is sometimes false there exists little evidence to help us understand under what circumstances it is false.
the consequences of this lack of understanding are manifold.
let us provide several examples.
for researchers studying bug prediction and bug localization models of how developers have fixed bugs in the past may not capture the true cause of failures but may instead only capture workarounds.
for practitioners when a software engineer is evaluated based on how many bugs are fixed the evaluation may not a ccurately affect that engineer s effect on software quality.
for educators without teaching future engineers the contextual factors that go into deciding which fix to apply as engineers they may choose inappropriate fixes.
however to our knowledge there has been no empirical research into how bugs fixes are designed.
in this paper we seek to understand the design of bug fixes .
when we say that bug fixes are designed we mean that there are a number of potential fixes for a single bug and choosing b etween those fixes is a matter of human judgment.
as with any software change an engineer must deal with a number of competing forces when choosing exactly what change to make.
the task is not always straightforward.
to fill this gap we seek to answer two research questions rq1 what are the different ways that bugs can be fixed?
figure characterizing the design of bug fixes rq2 what factors influence which fix an engineer chooses ?
this paper s primary contribution t he first systematic characterization of the design of bug fixes .
it analyzes the design space of bug fixes and describes how developers navigate that design space to understand the decisions that go in to choosing a bug fix see figure .
ii.
related work several researchers have investigated bug fixes.
perhaps the most relevant is leszak perry and stoll s study of the causes of defects where the authors classified bug reports by real defect location real location characterizes the fact that ... some defects are not fixed by correcting the real error causing co mponent but rather by a ... work around somewhere else.
while the authors co llected real defect locations the data was not analyzed or reported.
our work explains why one fix would be selected over another or in other words why an engineer would choose a workaround instead of a fix at a real location .
ko and chilana studied contentious open source bug r eports to investigate how engineer s make decisions about bugs .
while this paper did investigate design dimensions du ring bug fixing by design they meant the design of software for example whether a fix make s the software mo re usable rather than design in the sense we mean it which is the d esign of the fix itself.
our study also complements this study by improving our understanding of the decision making process when fixing bugs specifically for commercial software and for decisions that get made outside of the bug report itself .
breu and colleagues observed in a study of bug reports that .
of discussions in bug reports are spent on the co rrection itself discussions involving suggestions feedback r equests and understanding files .
our study complements this work by exploring the design space of bug fixes.
several other researchers have investigated the bug fixing .
in a manual inspection of bug fixes lucia and colleagues found that some fixes are spread over many lines of code .
bird and colleagues found that bug fixes reported in bug databases have different characteristics than fixes not reported in dat abases .
yin and colleagues investigated why bugs are fixed incorrectly that is require a later bug fi x to the source code changed by the original fix .
aranda and venolia investiga ted closed bugs and surveyed engineer s about bug coo rdination patterns at mi crosoft .
spinellis and colleagues attempted to correlate code metrics such as number of bugs fixed to evaluate the quality of open source software .
storey and colleagues investigated the interaction of bugs and code annotations .
anvik and colleagues investigated which engineer s get assigned to fix bugs .
in contrast to these papers our paper seeks to understand in what way bug fixes differ and why one fix is chosen over another.
iii.
methodology to answer our two research questions we conducted a mixed method study.
we used several research methods rather than a single one both to study our research questions in as broad a way as p ossible and to triangulate the answers to improve their accuracy .
while we feel that our methods are thorough and rigorous some threats still exist as we discuss in sect ion v. we now discuss our four research methods opportunistic interviews firehouse interviews triage meeting observations and surveying.
for each method we discuss the goal of using that method how we recruited participants the protocol we used how we analyzed data and a brief summary of the shape of the data we collected.
a. opportunistic interviews with our first method w e randomly asked engineer s about a recent bug they had been involved in fixing.
goal .
our goal in performing opportunistic interviews was to rapidly obtain qualitative answers to our research questions in a way that was minimally obtrusive to interviewees .
protocol.
we conducted these inter views by having the first author go to a building that housed a particular product group.
armed with a list of office numbers for software engineer s the interviewer walked to each engineer s office.
if the engineer s door was closed was wearing headphone s or was talking to someone else the interviewer went to the next office.
othe rwise the interviewer introduced himself said that he was a study and asked if the interviewee had to minutes to talk.
if the engineer consented the interviewer asked a series of semi structured questions regarding the last bug that the engineer was involved in fixing.
although interviewees were not offered an incen tive before the interviewer left intervie wees were compensated with a gift card for lunch.
we performed p ilot interviews to identify potential pro blems and rectify them prior to the main study.
in so we noticed that pilot interviewees could remember the fix they made but had difficulty recalling the alternative fixes they did not make.
some pilot interviewees stated that they fixed the bug the only way that it could have been fixed even though there clearly were other fixes even from our perspective as outsiders.
we sought to reduce this hindsight bias in our interviews using two different techniques.
for every odd numbered interview the first the thir d and so on we gave the interviewee an example of three bugs and multiple ways of fixing each bug.
for the other half of the interviewees we pr esented a small program containing a simple bug and then asked the interviewee to talk us through how she mi ght fix the bug interviewees typically mentioned several alternative fixes .
comparing the results obtained after starting interviews with these two methods we noticed no qualitative differences in the responses received suggesting that both methods were about equally effective.
comparing p ilot interview results against real interview results we feel that this technique significantly helped interviewees think broadly about the design space.
after this introductory exercise the interviewer asked the interviewee about the most recent bug that they fixed.
the inte rviewer asked about the software that the bug appeared in the symptoms the causes and whether they considered more than one way to fix the bug.
if an interviewee did consider multiple fixes we asked her to briefly explain each one and justify their final choice .
the full interview guide can be found online.
participants .
to sample a wide variety of engineer s we recruited interviewees using a stratified sampling technique sampling acr oss several dimensions of the products that engineers create.
we first postulated what factors might influence how engineer s design fixes we list those factors in table i. factor values domain desktop web application ente rprise backend embedded product type boxed service bug fix types pre release post release number of ve rsions shipped to continuous release phase planning and milestone quality main d evelopment stabilization and maintenance table i. factors for selecting product groups.
using these factors we select ed a cross section of microsoft products that spanned those factors.
we chose four products from which to recruit engineer s because we estimated that four products would balance two competing requirements that we sample enough engineer s from each product team to get a good feeling for what bug fixing is like within that team and to sa mple enough product teams that we could have reasonable gene ralizability.
the four product teams that we se lected spanned each of the values in the table.
for example one team we talked to worked on desktop software one web applications another enterprise backend and the last embedded systems.
within each product team we aimed to talk to a total of softw are engineer s six were what microsoft calls so ftware development engineers developers for short and two were software d evelopment engineers in test testers for short .
we intervie wed more developers as developers spend more time fixing bugs than t esters .
once we reached our quota of engineer s in a team we moved on to the next product team.
in total we completed opportunistic interviews with engineer s. data analysis .
we prepared the interviews for analysis by transcribing them.
we then coded the transcripts using the atlas.ti2 software .
before beginning coding we defined se veral base codes including codes to identify symptoms the fix that was applied alternative fixes and reasons for discrimina ting between fixes.
the first author did the coding .
additionally our research group consisting of full time researchers and interns analyzed the coded transcripts again to determine if any other notable themes emerged.
each person in the group analyzed to transcripts over hour .
we regard the first author s coding as methodical and thorough while the team s analysis was brief and serendipitous .
we derived most of the results described in this paper from the first author s coding .
we use the codes about fixes to describe the design space se ction iv.a and codes about discriminating between fixes to describe how engineer s navigate that spa ce section iv.b .
data characterization .
overall we found software engineers very willing to be interviewed.
to obtain interviews we visited engineer s offices.
most offices were empty or the engineer s appeared busy.
in only a few cases engineer s explicitly declined to be interviewed largely because the engineer was too busy.
interviews lasted between and minutes.
in this paper we refer to part icipants as p1 through p32.
most participants reported multiple possible fixes for the bug that they discussed.
in a few cases participants were un able to think of alternate solutions however the interviewer despite being unfamiliar with the bug was a ble to suggest an alternative fix.
in these cases the engineer agreed that the fix was possible but never consciously considered the alternative fix due to external project constraints .
interestingly this opportunistic methodology allowed us to interview three engineer s who were in the middle of conside ring multiple fixes for a bug.
b. firehouse interviews using the firehouse research method we interviewed engineer s immediately after they fixed a bug.
firehouse r esearch is so called because of the unpredictable nature of the events under study if one wants to study social dynamics of victims during and immediately after a fire one has to literally live in the fireh ouse waiting for fires to occur.
alternatively one can purposefully set fires although this research metho dology is generally discouraged.
in our case we do not know exactly when a n engineer is considering a fix but we can o bserve a just completed fix in a bug tracker and rush to the sc ene so that the event is fresh in the engineer s mind .
goal .
our goal was to obtain qualitative answers to our r esearch questions in a way that maximized the probability that engineer s could accurately recall their bug fix design decisions.
protocol .
we first picked one product group at microsoft went into the building where most development for that pro duct takes place and monitored that group s bug tracker watching for bugs an engineer marked as fixed within the last ten minutes .
if the engineer was not located in the building we moved on to the next most recently closed bug.
otherwise the interviewer went immediately to the engineer s office.
when approaching engineer s for this stu dy we were slightly more aggressive than in the opportunistic interviews if the engineer s door was closed we knocked on the door.
if the engineer was not in her office by the time we arrived we wai ted a few minutes.
these interview s were the same as the opportunistic interviews except that the interviewer insisted that he engineer focused on the bug that they had just closed.
participants .
our options for choosing a product group to study was fairly limited because we had to have a personal contact within that team that was willing to allow us to have live read only access to their bug tracker.
we chose on e prod uct which will remain anonymous the product group was di fferent from any of those chosen in the opportunistic interviews.
we aimed to talk to software engineer s in total for these interviews.
while we interviewed fewer people than with the opportunistic interviews these firehouse interviews tended to take much longer to orchestrate mostly because we had speci fic people that we wanted to talk to.
in retrospect we did not notice any qualitative differences in engineer s responses to the two interview types so for the remainder of the paper we do not distinguish between these two groups of participants.
non etheless you may do so if you w ish participants of the firehouse interviews are labeled p33 through p40.
data analysis .
we analyzed d ata in the same way as with the opportunistic interviews.
data characteristics .
we also found engineer s to be rece ptive to being interviewed although th ey were usually surprised we asked about a bug they had just fixed .
we reassure d them that we are from microsoft research and are there to help.
in total we went to offices and were able to interview engineers .
two of these we mistakenly interview ed one b ecause his office mate actually closed the bug and one because the interviewer misread the bug report.
we compensated these engineer s for their time but we exclude them from analysis.
c. triage meetings we hypothesized that not only do individual engineer s make decisions about the design of bug fixes but perhaps that bug fix designs happen during bug triage meetings as well.
goal .
our goal was to obtain qualitative answers to our r esearch questions with respect to how engineer s work together to find good bug fix designs.
protocol and participants .
we attended six bug triage meetings across four product groups.
five of these groups were the same groups that we did interviews with.
to ensure engineers were comfortable we did not record these meeting s rather we took notes and observed in silence.
data analysis and data characteristics .
it became clear that there was very little data we could gather in these triage meetings for two reasons.
the first is that participants rarely discussed how to fix a bug beyond whether to fix it and when to do so.
second when participants did discuss how to fix bugs the team was so tightly knit that very little explanation was needed this terseness made bug fix decisions basically impossible for us to understand w ithout the context that the team members had.
as a result we were able to glean few i nsights from the meetings.
for the few observations that we could make we label these meetings as t1 to t6.
because there was little usable data from these meetings we did not perform any data analysis beyond reading through our notes.
d. survey goal .
our goal was to quantify our observations made du ring the interviews and triage meetings.
protocol .
after we performed the interviews and triage meetings we sent a survey to software engineer s at microsoft.
as in the interviews the survey started by giving examples of bugs that could be fixed using different techniques where the examples were drawn from real bugs described by intervie wees.
as suggested by kitchenham and pfle eger we constructed the survey to use formal notations and limit responses to multiple choice likert scale s and short free form answers.
at the beginning of the survey we suggested that the r espondent browse bugs that they had recently closed to ground their answers.
in section iv we discuss these questions inte rmixed with engineer s responses.
after piloting the survey w e estimate that it took respondents about minutes to fill out the survey .
the full text of this survey can be found online.
participants .
we sen t the survey to participants by selecting employees of mic rosoft who had development in their job title and were not interns or contractors.
this fo llowed kitchenham and pfleeger s advice to understand whet her respondents had enough knowledge to answer the questions appropriate ly .
we incentivized participation by giving amazon.com gift certificates to two respondents at random .
data analysis .
we analyzed our data with descriptive st atistics for exam ple the median where appropriate.
we did not perform inferential statistics for example the t test because our research questions do not necessitate them.
when reporting survey data we omit not applicable question r esponses so percentages may not add up to .
data characteristics .
engineer s completed the survey a response rate of about within the range of other sof tware engineering surveys .
respondents were from all eight divisions of microsoft.
respondents reported between .
and years of experience in the software industry med ian .
with a median of years of experience at microsoft.
reported being developers while reported being tes ters.
one respondent reported being a product manager.
iv.
results we next characterize the design options that engineer s have when selecting a bug fix section iv.a and then describe how engineer s choose which fix to implement section iv.b .
a. description of the design space in our interviews we asked participants to estimate what percentage of their bugs for which there were multiple possible solutions.
the median was with a wide range of variance with individual responses ranging from to .
this su ggests that many bug s can be fixed in multiple ways although this numb er should be interpreted as a rough estimate.
with respect to the dimensions of the design space we obtained answers to this research question by ask ing interviewees to explain the different fixes that they considered when fixing a single bug.
in bold below we present several dimensions on which bugs may be fixed a description of each dimension and an example from our interviews .
note that a single fix can be considered a point in this design space for example a fix may have low error surfacing and high refactoring and simult a3 ph3 experiments bugfixdesignsurvey.pdf neously be placed in the other dimensions.
these dimensions are not intended to be exhaustive yet we believe that the nu mber of interviews we performed suggests that the list represents a solid foundation on which to build a theory of bug fix design.
data prop agation across components .
this dimension captures how far information is allowed to prop agate across a piece of software where the engineer has the option of fixing the bug by intercepting the data in any of the compone nts.
at one end of the dimension data is corrected at its source .
as an example p25 worked on software with a layered a rchitecture with at least four layers the top most being the user interface.
the bug was that the user interface was reporting disk space sizes far too large and the engineer found that the problem could be traced back to the lo west level layer which was reporting values in kilobytes when the user interface was expecting values in megabytes.
the interviewee had the option of fixing the bug by correcting the calculation in the lowest layer or by transforming the data multiplying by a thousand as it is passed through any of the intermediate layers.
error surfacing .
this dimension describes how mu ch information is revealed to users whether that information is for end users or other engineers .
at one end of the dimension the user is made aware of detailed error information at the other the existence of an error is not revealed .
p28 described a b ug where the software he was developing crashed when the user deleted a file.
when fixing the bug the engineer decided to catch the exception to prevent the crash but also was considering whether or not the user should be n otified that an exceptional sit uation had occurred.
as another example p6 described a bug where she was calling an api that returned an empty collection where she expected a non empty collection.
the problem was that she passed an incorrect argument to the api and the empty colle ction signif ied an error.
however an empty collection could also signify no results.
as part of the fix the engineer considered changing the api so that it threw an error when an u nexpected argument was passed to the api.
she anticipated that this woul d have helped future engineer s avoid similar bugs.
behavioral alternatives .
this dimension relates to whether a fix is perceptible to the user.
at one one end of the dimension the fix does not require the user to do anything differently at the other end she must significantly modify her behavior.
one exa mple is p11 who described a bug where the back button in a mobile application was occasionally not working.
as part of the fix he made the back button work but had to simultaneously disable another feature when the application first loads.
p11 stated th at having both the back button and the other feature working at the same time was simply not poss ible he had to choose which one should be enabled initially.
functionality removal .
this dimension relates to how much of a feature is removed during a bug fi x. at one end of the dimension the whole software product is eliminated at the other no code is removed at all.
as an example p18 described a bug in which a crash o ccurred.
rather than fixing the bug p18 considered removing the feature that the bug wa s in altogether.
we were initially quite surprised when we heard this story because the notion that an engineer would remove a feature just to fix a bug seems quite extreme.
however removal of features was me ntioned repeatedly as a fix for bug s during ou r interviews.
to quantify functionality removal w e asked survey r espondents to estimate how often they remove or disab le features rather than alleviating a symptom of a bug .
about of respondents said they had removed features from their software to fix bugs in the past.
refactoring .
this dimension expresses the degree to which code is restructured in the process of fixing a bug while preserving its behavior.
a bug may be fixed with a simple one line change or it may entail significant code restruct uring.
as an example p5 considered refactoring to remove some copy and paste duplication so you re not only fixing the bug but you also are kind of improv .
in our survey we asked respondents to report on refacto ring frequency when fixing bugs as shown in table ii.
in the table should be refactored indicates how often participants notice code that should be refactored when fixing bugs .
for example of respondents indicated that they usually n otice code th at should be refactored.
the is refactored row indicates how often participants refactor this code that should be refactored .
for example reported rarely refactoring code that should be refactored.
these results suggest that al though engineer s ap pear to regularly encounter code that should be refactored much of this code remains unchanged.
internal vs external .
this dimension relates to how much internal code is changed versus external code is changed as part of a fix.
on one end of this dimensi on the engineer makes all of her changes to internal code that is code for which the engineer has a strong sense of ownership.
on the other end the bug is fixed by changing only code that is external that is code for which the engineer has no ownershi p. one example is p33 who maintained a testing framework for devices used by several other teams.
the bug was that many devices were not reporting data in a preferred manner causing undesirable behavior in the p33 s framework.
part of the fix was immediate and internal changing the testing framework but part of it was deferred and external changing each of the other teams device code .
never rarely sometimes usually always should be refactored is refactored table i i. survey respondents refactoring behavior accuracy .
this dimension captures the degree to which a fix utilizes accurate information.
one one end of this dimension the engineer uses highly accurate information and on the other he uses heuristics or guesses.
an example is p29 who was working on a bug wher e web browser printing was not working well.
an accurate fix would be one where his print driver retrieves the available fonts from the printer then modifies the browser s output based on the available fonts.
a less accurate fix was to use a heuristic tha t produces better but not optimal print output.
hardcoding .
this dimension captures to what degree a fix hardcodes data.
on one end of the dimension data is specified explicitly and on the other data is generated dynamically.
one e xample of fixes on t his dimension is p24 who was writing a test harness for a system that received database qu eries.
the bug was that some queries that his harness was ge nerating were malformed.
he considered a completely hardco ded solution to the problem removing the query generator and using a fixed set of queries instead.
a more dynamic solution he considered was to modify the generator itself to either filter out malformed queries or not to generate them at all.
b. navigating the design space while the previous section described the design space of bug fixes it said nothing about why engineer s implement particular fixes within that design space.
for instance when would an engineer refactor while fixing a bug and when would she avoid refactor ing?
in an ideal world we would like to think that engineer s make decisions based completely on technical factors but realistically a variety of external factors come into play as engineer s navigate this bug fixing design space.
in this section we desc ribe those external factors.
risk management by development phase .
a common way that interviewees said that they choose how to design a bug fix is by considering the development phase of the project .
speci fically participants noted that as software approaches release their changes become more conservative .
conversely partic ipants reported taking more risks in earlier phases so that if a risk materializes they would have a longer period to compe nsate.
two commonly mentioned risk s were the risk that new bugs would be introduced and the risk that spending significant time fixing one bug comes at the expense of fixing other bugs.
p12 provided an example of taking a more conservative approach when he had to fix a bug by either fixing an existing implementation of the double checked locking pattern or r eplace the pattern with a simpler synchronization mechanism.
he eventually chose to correct the pattern even though he though t the use of the pattern was ques tionable because it was the least disruptive way to fix the bug.
he noted that if he had fixed the bug at the beginning of the development cycle he would have remov ed the pattern altogether.
in our survey we asked engineer s several questions rela ting to risk and development phase as shown in table i iia.
here we asked engineer s how often do the following factors influence which fix you choose?
where each factor is listed at left.
the table lists the percentage of respondents who choose that frequenc y level.
note that the factors are not necessarily linked for instance an engineer could choose to change very few lines of code for a reason other than the product is late in development.
however our qualitative interviews suggested that these factors are typically linked together and thus we feel justified in presenting these four factors as a whole.
these r esults suggest that for most respondents risk mitigation usually plays an important role in choosing how to fix a bug .
never rarely sometimes usually always optimal fix should be reconsidered actually are fixed optimally table i v. survey respondents optimal fix never rarely sometimes usually always a phase of the release cycle changes few lines of code requires little testing effort takes little time to i mplement b doesn t change inte rfaces or break backwards compatibility c maintains the integrity of the original design d frequency in practice table i ii.
factors that influence engineers bug fix design one of the findings that emerged from our interviews is that if engineer s are frequently making conservative changes then they may be incurring technical debt.
as p15 put it i wish to do it better but i m it this way because blah blah blah.
but then i don t know if we ever go back and kind of oh okay we had to do this now we can change it.
and i feel that code never goes away right?
we verified this statem ent by asking survey respondents how often they think bug s that are initially fixed suboptimally should be reconsidered for a more optimal fix in the future.
we asked how many o f these bugs actually are fixed optimally after the initial fix .
table iv displays the results.
these results suggest that engineer s often feel that optimal fixes should be reconsidered in the future but that those bugs rarely get fixed optimally.
as one respondent noted a lthough we talk about the correct fix in the next ver sion it never happens.
interface breakage .
another factor that participants said i nfluenced their bug fix es is to what degree a fix breaks existing interfaces.
if a fix breaks an interface that is used by external clients then an engineer may be less inclined to implement that fix because it entails changes in those external clients.
one example comes from p16 who was working on a bug r elated to playing music and voice over bluetooth devices.
he said that a better fix for the problem would be to change the bluetooth standard but too many clients already depend on it.
we also asked survey respondents how often the following factor influences which fix they choose doesn t change e xternal interfaces or breaks backwards compatibility.
reported that usually or always suggesting that changing external interfaces is a significant determinant in choosing which bug fix to implem ent table ii ib .
consistency .
this factor describes to what degree a fix will be consistent with the existing software or existing practices.
a fix that is not consistent with the existing code may compromise the design integrity of that code leading to code rot.
one example is p10 who fix ed a performance bug in his build system.
p10 fixed the bug by using the build system in a way consistent with how it was being used by other teams.
however he felt that a change that was inconsistent with the way the build system currently worked would have produced better build performance at least for his product.
table iiic lists survey respondents attitudes towards the importance of maintaining design consistency when fixing bugs.
user behavior .
this factor desc ribes the effect of how users of the software behave on the fix.
if users have strong opinions about the software or use a certain part of the software heavily engineer s may choose a fix that suits the user better.
one example is from t1 where the team discussed bugs in a code analysis tool .
the team wondered how often a certain code pattern was used in practice.
they acknowledged that their analysis did not work when the pattern was used but how they fixed the bug depended on how often users actually wrote code in that pattern .
they judged apparently based either on intuition or experience that several of these bugs were so u n likely to arise in practice that the effort to implement a co mprehensive fix for the problem was not justified.
after hearing about t1 and some interviewees talk about frequency of user behavior we became interested in how engineers know what users actually do.
thus we asked two que stions in the survey.
in the first we asked how often fixes d epended on usage frequency table iiid .
these results suggest that how frequently a situation occurs in practice sometimes influence s how engineer s design fixes.
the second question was a multiple choice question about how engineer s most often determine frequency table v .
in this table sqm refers to a usage data collector used in a variety of microsoft products.
the most common none of the above answer was asking the product manager.
in table v we were somewhat surprised to find that so many engineer s write queries over usage data.
however it still appears that many engineer s use ad hoc met hods for estimating user behavior including convenience sa mpling estimation and guessing.
cause understanding .
this factor describes how thoroughly an engineer understands why a particular bug occurs .
in inte rviews we were surprised how often engineer s fixed bugs wit hout understanding why those bugs occurred .
without thoroug hly understanding a bug the bug may re appear at some point in the future.
on the other hand complete understanding of w hy a bug is occurring can be an extremely time intensive task.
p3 provided an example of fixing a bug without a full u nderstand ing of the problem.
the symptom of his bug was that occasionally an error message appeared to the user whenever his software subm itted a particular job.
rather than understan ding why the error was occurring he fixed the job by simply resubmitting the job which usually completed without error.
rather than understanding the problem as he explained it my time is better spent fixin g the other ten bugs that i had .
we asked survey respondents why they do not always make an optimal fix for a bug indicated that they have not had time to figure out why the bug occurred.
this suggests that lack of cause understanding is sometimes a problem.
guess estimate bas ed on my past experience as a user of the software i develop estimate based on my past experience interac ting with users collect data by taking a quick convenience sa mple e.g.
ask devs on my team collect data by external polling e.g.
ask readers of my blog estimate based on existing usage data that i remember seeing in the past e.g.
sqm write a query over existing usage data e.g.
sqm none of the above table v. the most frequent mechanisms used by engineers to determine usage frequencies social factors .
a variety of social factors appear to play a role in how bugs are fixed including mandates from supervisors ability to find knowledgeable people and code ownership.
one example of this was p22 who was fixing a bug in a datab ase system where records were not sorting in memory causing reduced performance.
the engineer proposed a fix based on one week of discussions and bringing new ideas discussing manager .
other interviewees discussed their bugs with men tors p28 peer engineer s p28 testers p39 and development leads p34 .
in the survey we asked how communication with people helps inform the bug fixe design table vi .
the results su ggest that peer software development engineers sdes and the people who originally wrote the code related to where the fix might be applied tend to play the most important role in deci ding how a bug gets fixed.
we also ask ed survey participants about who decides on which bug fix design to implement .
most participants said they themselves usually decide while others said it was sometimes a group decision.
only said their manager usually or always decides.
we also asked survey respondents how they communicate with others about bug design.
respondents indicated t hat they most often communicate by email in unplanned mee tings planned meetings and in the bug report itself .
a few respondents also indicated that they discussed design during online code review and with instant messaging.
however in a study run in parallel with this one we i nspected online code review threads at microsoft but found no su bstantial discuss ions of bug fix design .
we postulate that by the time a fix is reviewed engineer s have already discussed and agreed upon the basic design of that fix.
we asked survey respondents how many people including themselves were typically involved in the bug fixing process.
table vii shows the results.
these results suggest that while finding the cause of a bug and implementing a solution are generally or person activities choosing a solution tend s more often to be a collaborative activity.
one of the more surprising things we heard from some i nterviewees was that when they made sub optimal changes they were sometimes hesitant to file new bug reports so that the optimal changes were reconsidered in the future.
the rationale for not so seemed to be at least partly social respon dents were not sure whether other engine ers would find a more optimal fix useful to them as well.
for instance p2 said the optimal fix to his bug would be a change to the way mobile applications are built in the build system but he wasn t sure that he would advocate for this change unless othe r teams would find it useful as well.
ideally this is what feature e nhancement bug reports with engineer voting should help with.
however p2 didn t fill out a bug report for this enhancement at all because he judged the time he spent filling out the r eport would be wasted if other engineer s didn t need it.
as he put it if i had more data ... that other teams did it ... if i could ... eyeball it quickly... then i d hey you know other teams are this.
c learly it s a scenario.
this led us to become curious why engineer s avoid filing bug reports so we asked survey respondents to estimate the frequency of several possible rationales that we heard about during the interviews table viii .
these results suggest that survey respondents rare ly avoid filing bugs for reasons that the interviewees discussed.
we view these somewhat contradictory findings as inconclusive more study likely using a different study methodology is necessary to better understand how often and why engineer s do not fi le bug reports.
v. limitations although our study provides a unique look at how engineers fix bugs several limitations of our study must be consi dered when interpreting our results.
an important limitation is that of generalizability beyond the population we studied external validity .
while our results may represent the practices and attitudes at microsoft it seems unlikely that they are completely representative of software developm ent practices and attitudes in general.
however because microsoft makes a wide variety of software products uses many of development methods and employs an intern ational workforce we believe that our random and stratified sampling techniques improved g eneralizability significantly.
giving interviewees and survey respondents example bugs and multiple fix examples may have biased participants towards providing answers that aligned with those examples a form of expectancy bias internal validity .
howev er we judged the threat of participants unable to recall implicit or never rarely sometimes usually always peer sdes peer sdets my manager my product manager the people who wrote the code other experts e.g.
architects table vi.
who is helpful to communicate with when choosing an optimal fix to to finding the cause of a bug choosing a solution implementing the solution table vii.
how many people are involved in bug fixing activities explicit design decisions outweighed this threat .
future r esearchers may be able to confirm or refute our results by using a research method that is more robust to expectancy bias.
still some interviewees struggled with remembering the design decisions they made and were generally unable to arti culate implicit decisions.
this type of memory bias is inherent in most retrospective research methods.
however we attempted to control memory bias by asking opportunistic interviewees to recall their most recently fixed bugs asking firehouse inte rviewees to discuss a bug they just fixed and asking survey respondents to look at bugs they had recently fixed .
to meet our goal of not significantly interrupting partic ipants workdays we kept our interview and survey short which means w e were unable to collect contextual information that may have helped us better explain the results.
for example in the interviews we did not ask questions about the gender or team structure which may have some effect on bug fix design s. similarly a consequence of keeping the survey short is that participants may have misunderstood our questions.
for example in our survey we asked engineer s whether they ever avoided filing a bug report this question could be interpreted conservatively to mean whe n do you not report software fai lures?
when our intent was for bug reports to be interpreted broadly to include enhancements.
while we tried to minimize this threat by piloting our survey as with all surveys we may still have miscommunicated with our respondents .
vi.
implications the findings we present in this paper have several implic ations a handful of which we discuss in this section .
additional factors in bug pred iction and localization .
previous research has investigated several approaches to pr edicting future bugs based previous bugs including our own .
the intuition behind these approaches appears reasonable how engineer s have fixed bugs in the past is a good predictor of how they should fix bugs in the future.
however the empirical results we present in this paper suggest a host of factors can cause a bug to be fixed in one way at one point in time but in a completely different way at another.
for exa mple a bug fixed just before release is likely to be fixed diffe r ently than a bug fixed during the planning phase.
as a result future research in prediction and localization may find it useful to incorporate when possible these factors into their models.
limits of bug prediction and localization .
although i ncorporating some factors such as development phase into hi storical bug prediction may improve the accuracy of these mo dels some factors appear practically outside the reach of what automated predictors can consider.
for example when analy zing past bugs it seems unlikely that an automated predictor can know whether or not a past fix was made with an engineer s full knowledge of why the bug occurred.
refactoring while fixing bugs .
the results of our study suggest that engineer s frequently see code that should be refa ctored yet still avoid refactoring.
one way that this problem could be alleviated is through wider spread use of refactoring tools which should help engineer s refactor without spending excessive time so and at minimal risk of introducing new bugs.
at the same time such tools remain buggy and diff icult to use so more research in that area is necessary .
usage analy tics.
in our study it appeared that engineer s often made decisions about how to fix bugs without a data driven understanding of how real users use their software.
while a better understanding would clearly be beneficial gat hering and querying that data appears to be time consuming.
microsoft like many companies has been gathering software usage data for some time but querying that data requires engineers to be able to find and combine the right data sources and write complex sql queries.
we envision a future where engineers while deciding the design of a bug fix can quickly query existin g usage data with an easy to use tool.
to build such a tool research is first needed to discover what kinds of que stions engineer s ask about their usage data beyond existing questions engineer s ask studies .
fix reconsideration .
engineer s in our study reported needing to reconsider bug fixes in the future but sometimes used ad hoc mechanisms for so such as writing todos in code.
some of these mechanisms may be difficult to keep track of for example which todos should be considered sooner rather than later .
engineers need a better mechanism to reconsider fixes in the future as well the time to do so.
never rarely sometimes usually always the bug is unlikely to ever be fixed whether or not the bug gets fixed has little impact on the software i m developing i don t know where to file the bug or who to report it to filing this bug dilutes the urgency of bugs i think are more important to fix a bug puts pressure on a colleague to fix the problem i don t want to add to his or her workload adding another report makes it look like the software is of poor quality or that the team is behind table viii .
frequency of reasons for not filing bugs vii.
conclusion in this paper we have described a study that combined o pportunistic interviews firehouse interviews meeting observ ation and a survey.
our results describe a multi dimensional design space for bug fixes a space that engineer s navigate by for example selecting a fix that is least disruptive whenever a release looms near.
while our study has not investigated a new practice we have taken the critical first step towards unde rstanding a practice that engi neers have always engaged in an understanding that will enable researchers practitioners and educators to better understand and improve bug fixes.
acknowledgment emerson murphy hill was a visiting researcher and m icrosoft when this work was carried out.
thanks to all partic ipants in our study as well as alberto bacchelli andy begel nicolas bettenburg rob de line jeff huang ekrem koc aguneli tamara lopez pat rick morrison shawn phillips jul iana saraiva and jonathan silitto .
viii.