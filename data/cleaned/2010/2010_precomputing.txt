precomputing possible configuration error diagnoses ariel rabkin and randy katz eecs department uc berkeley berkeley california usa fasrabkin randy g cs.berkeley.edu abstract complex software packages particularly systems software often require substantial customization before being used.
small mistakes in configuration can lead to hard todiagnose error messages.
we demonstrate how to build a map from each program point to the options that might cause an error at that point.
this can aid users in troubleshooting these errors without any need to install or use additional tools.
our approach relies on static dataflow analysis meaning all the analysis is done in advance.
we evaluate our work in detail on two substantial systems hadoop and the jchord program analysis toolkit using failure injection and also by using log messages as a source of labeled program points.
when logs and stack traces are available they can be incorporated into the analysis.
this reduces the number of false positives by nearly a factor of four for hadoop at the cost of approximately one minute s work per unique query.
i. i ntroduction studies have shown that human administrators almost inevitably make mistakes even when given explicit step bystep directions .
given an unfamiliar program errors and fumbling are inevitable.
for widely used programs web search can bring up user reports of problems and solutions.
but configuration debugging is still a thorny problem for specialized open source programs.
projects can accumulate configuration options that were useful for solving some particular problem at a particular site at some point in time.
without centralized management documentation can be sparse out of date or simply wrong .
as a result the set of configuration options can be large and incompletely documented .
human support may not be readily available.
developer time is a limited resource and inspecting and diagnosing error messages displayed when users misconfigure software is a low priority.
consider hadoop a widely used open source filesystem and mapreduce implementation.
it is a well established open source project with dozens of active developers and ample documentation including several books.
users range from large software companies with dedicated administration teams to hobbyists attempting to configure a small cluster at home.
despite this maturity configuration error checking is still weak.
if a user attempts to launch a recent version .
.
of the hadoop filesystem with the default out of the box configuration it will crash with a null pointer exception affording users little guidance about what to fix.
we will use this as a running example.
this paper describes a technique that can help users troubleshoot this sort of startup error.
when a user faces an unhelp ful error message such as hadoop s nullpointerexception at line of netutils.java our approach points them to a configuration option that if changed will make the error go away.
our goals are to give users an answer as quickly as possible without users installing any new debugging or analysis tools without them having to understand the program s source code and without exposing sensitive site specific configuration.
yinet al.
have studied configuration errors in five significant applications they found that mistaken parameter values are of all misconfigurations and that invalid values of the sort our technique could potentially address are of these .
brown et al.
suggest that typos are omnipresent and are a major problem .
hence looking for a specific wrong option will catch a large fraction of real world misconfigurations.
our chief tool is static analysis which can be done independent of the user s query and whose results can be shared across users.
most prior work on configuration debugging has relied on large user communities or on modifying the program s execution environment.
both of these are deployment challenges.
in contrast our approach lets developers shield users from the complexity of diagnosis.
a. our contributions the core of our approach is to analyze the program in question producing a table mapping each line in the program s source code to the set of relevant configuration dependencies at that point.
we envision this being done by the developers at release time.
when a user encounters an error they can use the error message to query this table perhaps via a web service.
previous work has shown how to map log messages back to the origin line in the source code .
our approach requires no reconfiguration new tools or program modifications on the part of the user unlike replay based approaches or delta debugging.
it requires no alterations to the jvm or standard library.
this distinguishes our work from competing techniques such as dynamic taint tracking.
in managed environments such as the java runtime unhandled errors often result in a stack trace.
we show that these stack traces can significantly improve the precision of analysis.
our technique which we call failure context sensitive analysis fcs re analyzes the call chain corresponding to the stack trace pruning out irrelevant paths.
by reusing the results from a prior static analysis the run time for fcs can be keptlow.
for hadoop the cost of these queries is approximately a minute with our current implementation.
the results of these queries can of course be cached reducing the time to answer for subsequent queries with the same stack trace.
our vision is that this analysis would be performed by a web service users would need only to paste in a stack trace or log file to get back a diagnosis.
we have a prototype implementation targeting java bytecode programs.
we use the jchord analysis toolkit .
all the code used for the measurements in this paper is publicly available in the jchord source code repository1.
we chose jchord because it was relatively easy for us to understand and modify its source code and because its datalog heavy programming style facilitated rapid prototyping.
this was purely an implementation choice.
other dataflow or slicing engines should work comparably well.
more broadly we do not claim that our analysis algorithms are the best possible rather we show they are effective enough to be of use in solving an important practical problem while still running quickly on large programs.
in previous work we described how static analysis can find and categorize configuration options in many programs .
the analysis presented here is similar in spirit and uses the techniques presented there for a preliminary part of the analysis.
explaining errors however requires a substantially more complex analysis than merely finding types.
none of the techniques described here were present in our prior work.
b. methodology and organization our analysis is targeted to large complex software systems such as hadoop.
in these systems data will flow in and out of the system via the network and the filesystem.
there may be native language code.
these data flows are difficult to capture dynamically and even harder to model statically.
as a result we accept that our analysis will be imprecise and will miss some configuration dependencies.
there will be both false positives and false negatives.
we believe this is acceptable so long as the analysis performs well in the common case giving a correct diagnosis and not too many wrong guesses.
while our focus is on static analysis we evaluate the benefits from several kinds of run time instrumentation.
this lets us gauge the sources of imprecision in our static approach.
we show that tracking which options are read by the program can substantially improve analysis precision.
this information can be recorded by the program and incorporated into the analysis cheaply.
only normal logging is required not any sort of dynamic tracing or taint tracking.
the rest of this paper is organized as follows.
we begin by describing our model for configuration options and give an overview of the analysis techniques we propose.
in section iii we present the details of our analyses.
our evaluation is in section iv.
section v discusses limitations and sources of experimental error.
section vi describes related work.
we give our conclusions in section vii.
analyzernetutils.java socketaddress getnamenodeaddress return createsaddr getdefaulturi .getauthority ... uri getdefaulturi return conf.get fs.default.name file ... socketaddress createsaddr string t int colonindex t.indexof .... fig.
.
simplified hadoop code that produces null pointer exception with default configuration.
... netutils.java depends on fs.default.name netutils.java depends on fs.default.name ... fig.
.
analysis output for code in figure .
note that reading a variable as on line is not a use of the variable.
ii.
m odel and overview we model configurations as a set of key value pairs where the keys are strings and the values have arbitrary type.
this is a convenient abstraction for analysis as well as a close match for many standard configuration apis.
it is the abstraction offered by the posix system environment the java properties api and the windows registry.
it is also used by many applications for their own configuration data.
it is not the only configuration model in use.
some systems use structured xml formats or other more complex models.
at least sometimes though these more complex models can be approximated by treating an xpath to the option as its name.
in previous work we attained high accuracy using this xpath based reasoning for cassandra which uses structured xml for configuration .
a. an example this section gives an example of how our analysis can diagnose a configuration error.
we continue our running example hadoop when started with default configuration prints nullpointerexception at line of netutils.java .
figure is a simplified version of the code in question.
the problem arises as follows.
the getnamenodeaddress method attempts to construct an address from the authority server portion of the filesystem uri.
the default filesystem uri controlled by option fs.default.name isfile .
this uri has no authority portion and so uri.getauthority returns null.
this null then propagates to createsaddr which attempts to dereference it and then crashes.
in the real implementation this code is scattered across three different classes.
tracking down the problem in the source code would be a substantial task particularly for new users.our approach builds a table mapping each line in the program to the options most directly associated with it.
in this case there will be an entry saying that fs.default.name affects line of netutils.java.
this table can then be presented to users via a web service or a general purpose search engine letting them discover that fs.default.name is a relevant option at the point where the exception was raised.
our analysis does not tell users what value for the option will resolve the problem but it can avoid wasted time tinkering with irrelevant options.
before describing our analysis in detail we give a sketch of how our analysis ties together the option and the relevant source code line.
this is intended to give the overall flavor of the approach.
the analysis marks the call to conf.get as an option read.
its return value is therefore assigned the label fs.default.name .
dataflow analysis tracks the flow of this label into createsaddr .
line of netutils.java uses this value and so the analysis outputs that the line in question depends on fs.default.name .
figure depicts this.
note that reading a configuration option as happens on line is not a useof the option.
b. overview of approach the next section will discuss our analysis algorithms in detail.
here we give an overview of the approach.
we define a label for each configuration option.
we then use dataflow analysis to determine which values and program points are associated with each label.
this analysis happens at the bytecode level.
at the end we map these results back to line numbers.
since the analysis is being done at development time we assume that debugging information is available to supply the line numbering.
these error attribution maps are small enough to easily fit in memory even for large programs as discussed in section iv c. the steps in our analysis are listed below.
the first step is a standard points to analysis.
the second step is shared with our previous work .
the next three steps are responsible for mapping program points to data dependencies.
last we do a method local control flow analysis.
below is an outline of the approach points to analysis and call graph construction.
find configuration read points and associated names.
create flow summaries for all reachable methods.
dataflow analysis of configuration labels.
optional demand driven failure context sensitive dataflow analysis of failure path using stack trace.
method local control flow analysis using results of either whole program or failure context sensitive dataflow analysis.
the basic analysis can all be computed statically and shared across all errors to be diagnosed.
only the optional failurecontext sensitive analysis needs to be repeated for each distinct error message.
our approach is similar to taint tracking in that we are concerned with tracking the flow of labels through a program.labels are introduced via configuration reads and propagate via assignment and via library calls.
unlike taint tracking we are not trying to find all possible dependencies but only the most relevant ones for troubleshooting.
to avoid the wellknown problem of taint explosion we apply a number of heuristics discussed below.
our analysis can also be thought of as an application of thin slicing .
we are effectively computing a forward thin slice from each configuration option read point and recording the set of slices that each program point belongs to.
several design choices were forced on us by the programs we sought to analyze.
we cannot assume that analysis will find an allocation site for every object.
the programs we analyze are large complex frameworks.
objects can be allocated in native code or in user code not present at analysis time.
hence we apply labels to variables and to fields of objects not to allocation sites.
this is similar to the approach taken in racerx .
second we do not analyze inside the standard library.
instead we treat library methods as opaque and treat