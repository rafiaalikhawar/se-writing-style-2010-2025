sedge symbolic example data generation for dataflow programs kaituo li christoph reichenbachy yannis smaragdakisz yanlei diao christoph csallnerx computer science department university of massachusetts amherst usa yinstitute of informatics goethe university frankfurt germany zdepartment of informatics university of athens greece xcomputer science and engineering university of texas at arlington usa abstract exhaustive automatic testing of dataflow esp.
mapreduce programs has emerged as an important challenge.
past work demonstrated effective ways to generate small example data sets that exercise operators in the pig platform used to generate hadoop map reduce programs.
although such prior techniques attempt to cover all cases of operator use in practice they often fail.
our sedge system addresses these completeness problems for every dataflow operator we produce data aiming to cover all cases that arise in the dataflow program e.g.
both passing and failing a filter .
sedge relies on transforming the program into symbolic constraints and solving the constraints using a symbolic reasoning engine a powerful smt solver while using input data as concrete aids in the solution process.
the approach resembles dynamic symbolic a.k.a.
concolic execution in a conventional programming language adapted to the unique features of the dataflow domain.
in third party benchmarks sedge achieves higher coverage than past techniques for out of pigmix benchmarks and out of sdss benchmarks and with equal coverage for the rest of the benchmarks .
we also show that our targeting of the high level dataflow language pays off for complex programs state of the art dynamic symbolic execution at the level of the generated map reduce code instead of the original dataflow program requires many more test cases or achieves much lower coverage than our approach.
i. i ntroduction dataflow programming has emerged as an important data processing paradigm in the area of big data analytics.
dataflow programming consists of specifying a data processing program as a directed acyclic graph.
internal nodes of the graph represent operations on the data for example using relational algebra primitives such as filter project and join or functional programming primitives such as map applications of userdefined local functions and reduce operations that collect values over sets of data.
the edges in the graph represent data tables or files passed between operators nodes in the graph.
many recently proposed data processing languages and systems such as pig latin dryadlinq and hyracks asterix resemble dataflow programming on datasets of enormous sizes.
a user can develop dataflow programs by either writing the programs directly using the above languages or compiling queries written in declarative languages such as sql and hive .
when a user writes a dataflow programs he she will typically employ example data or test cases to validate it.
validating with large real data is impractical both for reasonsof efficiency running on large data sets takes a long time and for reasons of ease of validation it is hard to tell whether the result is what was expected .
one alternative is to sample the real data available.
the sample data need to thoroughly exercise the program covering all key behavior of each dataflow operator.
this is very hard to achieve via random sampling however.
for instance equi joining two sample data tables of small size is likely to produce an empty result if the values being joined are distributed arbitrarily.
another alternative is to synthesize representative data.
such data synthesis is complicated by the complexity of dataflow language operators as well as by the presence of user defined functions.
current state of the art in example data generation for dataflow programs is of limited help.
such techniques can generate high coverage data for dataflow programs with simple constraints.
however for dataflow programs with complex constraints e.g.
with numerous filters arithmetic operations and user defined functions the generated data are incomplete due to shortcomings in constraint searching and solving strategies.
in this paper we address the problem of efficient example data generation for complex dataflow programs by bringing powerful symbolic reasoning to bear on the process of sample data generation.
we present the first technique and system for systematically generating representative example data using dynamic symbolic execution dse of dataflow programs.
our concrete setting is the popular pig latin language .
our dse technique analyzes the program while executing it using sampled data determines whether the sampled input data are complete and if not attempts to synthesize input tuples that result in the joint sampled and synthesized data being a complete example data set for the program.
we have implemented this approach in s edge short for symbolic example data generation .
sedge is a reimplementation of the example generation part in the apache pig dataflow system which currently implements the closest comparable past research by olston et al.
.
illustration for a simple demonstration consider an application scenario in computational astrophysics.
we surveyed queries in the sloan digital sky survey1for analyzing star galaxy observations and rewrote them using the pig latin language.
the most complex query contains filters and joins.
for ease of exposition we show a simple example query in listing by combining features from two actual queries and will use it as a running example in the paper.
for more details on the real queries see the evaluation section.
1a load filea using pigstorage as name chararray value int 3b load fileb using pigstorage as u double class int 5c filter a by value and value 6d filter b by math.pow u .
.
7e join c on value d on class listing .
an example pig latin program the program begins by loading tables a and b from files containing measurements.
both kinds of measurements need to be filtered.
the first filter keeps only measurements in a certain value range and the second filters low uvalues.
the tuples that survive the filtering get joined.
imagine that we execute the program for a small number of sampled input tuples from filea andfileb .
if we want to achieve perfect coverage on random sampling of actual data alone we are unlikely to be successful if the sample is small.
the data from the two tables need to pass filters and even more unlikely have their value andclass fields coincide.
this is a case where targeted test data generation can help.
past techniques for example data generation cannot handle this example well.
olston et al.
s technique will synthesize data by considering operators one by one in reverse order in the pig latin program.
it will attempt to create data to satisfy the join first without concern for the filter conditions that the same data have to satisfy.
this will likely fail to satisfy even the first filter operator the range will have to be hit purely by chance.
the problem for the olston technique is thatvalue is not a free variable once the join constraint is satisfied it is limited to the values that the system arbitrarily chose in order to have the join operator produce output.
even more importantly the second filter operator is hard to process.
it contains a user defined function math.pow .
although this function is simple it will still befuddle an automatic test data generation system.
furthermore an essential part of dataflow programming is the ability to use user defined functions freely however complex these functions may be.
the large volume of work on automatic data generation in other settings e.g.
sql databases does not address user defined functions.
our approach overcomes such problems by modeling the entire program in a powerful reasoning engine handling complex conditions and dealing with user defined functions with the aid of concrete values observed over sample data.
we process the program using a domain with symbolic variables such as value class etc.
a symbolic variable columnname represents the value of one column of an input table for a set of tuples.
we start with a concrete execution of the program using small samples of real input data.
during such concrete execution we observe first which program cases are covered and second what are the values of user defined functions for real data.
e.g.
a tuple of table b will register the value pair u math.pow u .
for the userdefined function.
this value will later help when trying to solve symbolic constraints.
after the concrete execution our approach uses symbolic reasoning in order to cover program cases that were not already covered by the concrete execution.
the approach performs a symbolic execution of the program gathering constraints along each path to the sources.
we use the z3 smt solver a powerful symbolic reasoning engine to solve the constraints.
concrete values for user defined functions are supplied to the solver.
that is the user defined function is treated as a black box and the solver is supplied extra constraints of the form u math.pow u .
these can aid the solver in producing satisfying assignments.
essentially we try to make an educated guess whenever we do not know how to generate example data for a constraint that depends on a user defined function we can always simplify this constraint by replacing the symbolic representation of the user defined function with concrete values.
contributions in brief the contributions of our work are as follows we detail a translation of dataflow operators into symbolic constraints.
these constraints are subsequently solved using a powerful smt solver.
we adapt the technique of dynamic symbolic execution to the domain of dataflow languages.
by so we exploit the unique features of this domain thus enabling high coverage.
specifically we exploit the absence of side effects in order to perform a multiple path analysis observations on the values of a user defined function on different execution paths can help solve constraints involving the user defined function.
as a result of the above we produce an example data generation technique that achieves higher coverage than past literature managing to produce data that exercise all operators of a dataflow program.
we show extensive measurements to confirm our approach s advantage.
our technique achieves full coverage in all benchmark programs with a boost in performance for most benchmarks.
ii.
b ackground and context we next discuss some pertinent background on dataflow programming as well as on concepts and mechanisms introduced in closely related past work.
a. dataflow program a dataflow program is a directed bipartite graph separating computations i.e.
operators in one partition and computational intermediate results i.e.
data tables in the other partition.
in other words it is a graph in which data tables flow into operators and operators flow into data tables.
a data table is a collection of tuples with possible duplicates.
a tuple is typically a sequence of atomic values integer long float chararray etc.
or complex types tuple bag map .
an operator usually has some input tables and one output table.
we say that a data table is an input table of an operator in a dataflow program if the data table flows into the operator.
similarly we say that a data table is an output table of an operator in a dataflow program if the operator flows into the data table.
if operator a s output table is one of operator b s input tables a is said to be an upstream neighbor of b and b is said to be a downstream neighbor of a. an operator without any upstream neighbor is called a leaf operator and an operator without any downstream neighbor is called a root operator the root operator generates the final output.
b. pig latin pig latin is a well known dataflow programming language and the language front end of the apache pig infrastructure for analyzing large data sets.
the pig compiler translates pig latin programs into sequences of map reduce programs for hadoop.
a pig latin program is a sequence of statements where each statement represents a data transformation.
in a pig latin statement an operator processes a set of input tables and produces an output table.
following are the core operators of pig latin .
load read the contents of input data files.
filter discard data that do not satisfy a built in logic predicate or a user defined boolean function.
cogroup divide one or more sets of input tuples into different groups according to some specification.
each resultant output tuple consists of a group identifier and a nested table containing a set of input tuples satisfying the specification.
group a special case of cogroup when only one set of input tuples is involved.
transform apply a transformation function to input tuples.
transformation functions include projection builtin arithmetic functions e.g.
incrementing a numeric value user defined functions and aggregation.
an aggregation is implemented by first invoking cogroup or group and then transformation group by group.
for example average is an aggregation that averages the values in each group of input tuples.
join equijoin tuples from two input tables.
union vertically glue together the contents of two input tables into one output table.
foreach apply some processing to every tuple of the input data set.
foreach is often followed by a generate clause to pick a subset of all available fields.
distinct remove duplicate tuples from the input data set.
split split out the input data set into two or more output data sets.
a condition argument determines the partition that each tuple of the input data goes into.
store write the output data set to a file.c.
equivalence class model our work tries to maximize branch coverage in pig latin programs.
an interesting question is what constitutes full coverage of a pig latin operator.
in some cases the answer is clear the filter operator for instance is well covered when its input contains tuples that satisfy the filter condition and tuples that fail the filter condition.
in other cases the definition of coverage is not as simple.
for instance do we consider a union operator sufficiently covered if all its output tuples come from a single input table i.e.
if one of its input tables is empty ?
the choice is arbitrary but the more reasonable option seems to be to require that both inputs of a union operator be non empty.
furthermore whether an operator is covered may be more convenient to discern in some cases by observing its input and in others by observing its output.
for instance a join is well covered when the output is nonempty while a union is well covered when its inputs are both non empty.
to specify the coverage of operators we inherit the definition ofequivalence classes from olston et al.
the research work that has formed the basis of the example generation functionality in apache pig.
each pig latin operator yields a set of equivalence classes for either its input or its output tuples .
equivalence classes partition the actual set of tuples each tuple can belong to at most one equivalence class.
to generate example data with coverage the input or output table of each operator when the program is evaluated with the example data must contain at least one tuple belonging to each of the operator s equivalence classes.
we summarize the equivalence class definitions for the operators of pig latin below.
the definitions are from olston et al.
s publication and implementation in apache pig.
load store foreach transform every input tuple is assigned to the same class e1.
i.e.
the operator is always covered as long as its input is non empty.
filter every input tuple that passes the filter is assigned to a class e1 all others are assigned to a class e2.
the intention is to show at least one record that passes the filter and one that does not pass.
group cogroup every output tuple is assigned to the same class e1.
for every output tuple the nested table for every group identifier should contain at least two tuples.
the purpose of e1is to illustrate a case where multiple input records are combined into a single output record.
join every output tuple is assigned to the same class e1.
the intention is to illustrate a case of two input records being joined.
union every input tuple from one input table is assigned toe1 tuples from the other input table are assigned to e2.
the aim is to show at least one record from each input table being placed into the unioned output.
distinct every input tuple is assigned to the same class e1.
for at least one input tuple to distinct there should be a duplicate to show at least one duplicate record is removed.
every input tuple that passes condition iis assigned to class ei1 input tuples that do not pass iare assigned to a class ei2.
the number of equivalence classes of a split depends on how many conditions the split has.
if a split hasnconditions it yields 2nequivalence classes.
the aim is to show for each split condition at least one record that passes the condition and one that does not pass.
d. quantitative objectives we use two metrics to describe the quality of example data and follow earlier terminology completeness the average of per operator completeness values.
the completeness of an operator is the fraction of the equivalence classes of the operator for which at least one example tuple exists.
an ideal algorithm should make example data exist for every equivalence class of every operator in a pig latin program.
conciseness the average of per operator conciseness values.
the conciseness of an operator is the ratio of the number of operator equivalence classes to the total number of different example tuples for the operator with a ceiling of .
an ideal algorithm should use as few example tuples as possible to illustrate the semantics of an operator.
the completeness metric is clearly a metric of coverage as defined earlier.
specifically it corresponds to branch coverage in the program analysis and software engineering literature.
branch coverage counts the percentage of controlflow branches that get tested.
iii.
s edge design our system s edge uses a three step algorithm to generate example data in pig latin programs.
downstream propagation execute programs using sampled real data record values of user defined functions see section iii b pruning pass eliminate redundant data so that each covered equivalence class only contains a single member upstream pass generate constraints and synthesize data for equivalence classes that the sampled test data do not explore by performing dse.
the last pass upstream pass it the key new element of our approach and is described next.
a. constraint generation the essence of our approach is to represent equivalence classes symbolically and to produce symbolic constraints that describe the data tuples that belong in each equivalence class.
solving the constraints i.e.
producing data that satisfy them yields our test inputs.
our constraint generator steps through the dataflow graph to compute all equivalence classes for each pig latin operation starting at root i.e.
final operators.
we assume that each root operator is of the form store w without loss of generality the analysis enters dummy nodes of this form when they are implicit .
similarly we assume that all variable names in our program are unique.we represent the set of constraints one for each equivalence class of a statement v asc v .
we consider two kinds of equivalence classes terminating equivalence classes which represent paths of tuples that end at a given operator e.g.
filtered out and binding equivalence classes which represent paths through which tuples continue downstream.
for illustration consider our running example reproduced here for ease of reference.
a load filea using pigstorage as name chararray value int b load fileb using pigstorage as u double class int c filter a by value and value d filter b by math.pow u .
.
e join c on value d on class here our root node consumes variable e. our analysis considers eas if it were flowing upstream from a store operation.
we invent a symbolic name p for the single equivalence class induced by the store.
its constraint is satisfied by all tuples c e fpg where8t p t note the use of .
a dataflow node could receive constraints from several operators both its upstream and downstream neighbors so our equivalence class inference is using subset reasoning we know that c e includes at least p but it could include other equivalence classes as well.
in this example it does not.
c e is then propagated to the join statement that constructs e. joins require tuples to agree on particular fields value andclass here so we enforce this property by encoding it in our constraints for all p2c e 9ex c c fp0 valueg c d fp0 classg where p0 value t p t t value ex andp0 class t p t t class ex note that for all refers to iteration that generates multiple constraints where as the 9quantifier is part of the generated constraint.
in our example c e contains only one equivalence class hence c c andc d also end up with one equivalence class each.
continuing the propagation process we pass the above constraints on to the filter operators of our example.
for instance consider the statement d filter which eliminates all elements for which math.pow u .
25does not hold.
this statement first introduces a binding equivalence class for each of the equivalence classes flowing upstream viac d .
the statement also introduces a single terminating equivalence class p to capture the case of tuples that do not pass the filter c b fp g where p t math.pow t u .
and for all p2c d c b fp0g where p0 t p t math.pow t u .
25pig latin code equivalence class constraints cardinality constraints store a c a fpg where8t p t t p a filter b by q c b fp g where p t b t for all p2c a c b fp0g where p0 t p t b t t p0 t p0 t p a union b c for all p2c a c b fpgandc c fpg a join b by x c by yfor all p2c a 9af c b fp0 xgandc c fp0 yg where p0 x t p t t x af andp0 y t p t t y af t p0 x t p t p0 y t p a distinct bfor all p2c a 9at c b fp0g where p0 t p t t at t p0 t p a group b by xfor all p2c a 9af c b fp0g where p0 t p t t x af t p0 t p fig.
.
summary of representative translations from pig latin statements into equivalence classes manifested as constraints.
the above constraints are all binding constraints except for the terminating p in filter and for p0in distinct which is both terminating and binding.
in the above btranslates boolean pig expressions into our term language and t p is the set of sample tuples for constraint p. every rule introduces fresh symbolic names for equivalence classes we use fresh variables afto refer to individual values and atto refer to tuples.
in our representation we have preserved the user defined function math.pow as an example of a function that the theorem prover cannot handle directly see section iii b .
we handle the other filter statement similarly and reach the load statement which completes the analysis.
the resultingcsets contain symbolic names for all equivalence classes and our symbolic constraints can be used to define members of these classes.
table gives the general form of our reasoning for representative constructs also including distinct statements and cardinality constraints discussed below .
for all operators for which our first two analysis passes observed insufficient coverage we collect constraints using the above scheme to generate the constraints pthat represent each insufficiently covered equivalence class.
for each pwe attempt to add elements to its corresponding set of samples t p .
we synthesize such tuples tas follows pass pto the theorem prover and query for witnesses for the existentially qualified fields.
if there are no witnesses abort either the equivalence class is empty not satisfiable due to conflicting requirements or the theorem prover lacks the power to synthesize a representative tuple.
otherwise extract the witnesses into tuple t0.
for any field f required by the type constraints over tinp extract t2 f from randomly chosen t2from our observed samples.
combine t0with all the t2 fintot00.
for any still missing fields i.e.
if no matching t2exists fill the field with randomly synthesised data yielding t. if t2t p already repeat the previous two steps as needed otherwise insert tintot p .
as the last steps and table show there is another dimension in our sample generation namely generating the right amount of sample data.
specifically recall that our binding equivalence class for f group b by x requires at least two tuples.
to capture this constraint we permit constraints on the cardinality of our sets of witness tuples notation t p0 where predicate p0representsthe binding equivalence class in the above.
all such constraints are greater than or equal constraints and we always pick the minimum cardinality that satisfies all constraints.
another subtlety of our constraint notation comes from the distinct statement as in g distinct b this statement eliminates duplicate tuples.
since set semantics have no notion of duplicates we extend all of our tuples with a unique identity field that does not occur in the pig program.
we write t1 t2iff the tuples t1andt2have the same fields ignoring the identity field.
the difference between our t1 t2 andt1 t2is analogous to reference comparison t1 t2 and value comparison t1.equals t2 .
to support aggregation operations in sample synthesis we further permit reasoning about our sampled tuples.
for example pig latin allows us to write a load ... sum sum a.x b filter a by count sum we translate aggregations such as sum sum a.x into aggregations over our sets of samples.
whenever we synthesize samples for one of a s binding equivalence classes e.g.
represented by p we simply set sum p t2t p t x. the translation is analogous for other aggregators a vg max etc.
.
aggregators enforce t p .
b. user defined function concretization in earlier sections we classified our approach as dynamicsymbolic following other similar work in different settings .
the important aspect of a dynamicsymbolic execution approach to test generation is that dynamic i.e.
concrete observations are used to help the symbolic solving process.
the foremost aspect where this benefit is apparent in our setting is when dealing with user defined functions udfs .
a user defined function is any side effectfree operator that has a definition external to the language.
in the pig latin world this typically means a java functionpublic class hash extends evalfunc integer public integer exec tuple input if input null input.size return null integer y integer input.get int hash y y return hash .. fig.
.
the implementation of function hash.
used to process values e.g.
in a filter.
what a dynamicsymbolic execution engine can do is to treat a udf as a blackbox function.
inside a constraint a use of a udf is replaced by a set of function values from the concrete semantics under the assumption that some invocations of udfs and return values thereof have already been observed.
consider the example pig latin program shown in listing .
our objective is to generate complete example data with one tuple passing and one tuple not passing the filter.
this program s key step is the application of the udf hash to perform filtering which takes an integer as argument and returns its hash value.
1a load filea using pigstorage as x int y int 3b filter a by x hash y and x listing .
example pig latin program calling user defined function hash .
a simplified implementation of hash is shown in figure .
in this implementaion hash extends the evalfunc class which is required by pig latin to construct java user defined functions3 .
assume that we run the program with two input tuples and that do not pass the filter since and are not the hash values of and respectively .
on these two executions we obtain two evaluations of hash y hash y and y hash y .
for our technique to have completeness we need to generate example data for a x y such that x hash y x .
using the two evaluations of hash we construct two simplified versions of the constraint x y x and x y x .
in the simplified constraints the function call hash y has been concretized to the observed values and respectively .
the second simplified constraint is satisfiable while the first is not.
using the satisfying assignment we derive a new example input for a x y .
thus our approach records concrete values for udfs during the downstream pass concretizes constraints and solves them using automatic constraint solvers in the upstream pass.
we use uninterpreted functions to encode a concretized constraint.
an uninterpreted function uf is a black box with no 3see pig s implementation guide for user defined functions at apache.org docs r0.
.
udf.htmlsemantic assumptions other than the obligation that it behave functionally equal parameters yield equal function values.
to encode udfs as uninterpreted functions for our constraint solver we supply concrete observations as implications using the if then else operator ite over boolean formulas and concrete values.
consider the example of listing again in which we need to find assignments to x y to satisfy the constraint x hash y x .
we supply the constraint solver z3 with concrete observations on the hash udf by the following commands declare const x int declare const y int define fun hash x!
int int ite x!
ite x!
assert not hash y assert hash y x assert x the first two declare const commands declare two integer variables.
the define fun command creates a uf that takes a parameter representing an integer and returns a constant value.
x1!
is the argument of the uf.
we have observed two invocations of the function hash hash applied to y yields and hash applied to y yields .
to complete the definition of the uf we need to relate unknown parameter values with a default return value which in this case we arbitrarily choose to be zero.
still we assert that hash y is not zero to avoid accidental satisfaction.
finally we provide the constraint x hash y x that we want to solve.
using three assert commands the system pushes three formulas into z3 s internal constraint stack.
we solve the concretized constraints by asking z3 to produce a satisfying assignment for variables in the constraints.
of course when the observations of the udf are not sufficient to obtain the desired coverage z3 will deem a concretized constraint to be unsatisfiable or unknown.
to increase the chance of finding a satisfying assignment for an abstract constraint we also try a second constraint solver coral when z3 returns unsatisfiable or unknown for a concretized constraint.
the distinction between z3 and coral concerns the kind of formulas that they can solve z3 can derive models and check satisfiability of formulas in decidable theories while coral can deal with numerical constraints involving undecidable theories.
as a consequence of supporting undecidable theories coral can solve constraints involving udfs in the form of common math functions e.g.
power function directly without concretization.
if neither concretization andemploying z3 nor calling coral can solve a constraint sedge will be unable to obtain perfect coverage.
note that our approach to solving udfs reasons about all observed values of the udf in parallel.
these udf observations may be produced in different paths through the program including executions of different test cases.
still theobservations can be used together i.e.
we can assume that all of them hold because of the lack of side effects in a dataflow program.
in contrast in dynamic symbolic execution of an imperative program only values of user defined functions observed during the current dynamic execution can be leveraged at a given constraint solving point.
iv.
i mplementation the s edge system has required non trivial implementation effort in the support of different data types the interfacing with the z3 constraint solver and the integration of string generation capabilities.
a. symbolic representation of values sedge maintains an intermediate level of abstract syntax trees for communication between z3 and pig latin constraints.
each node of the tree denotes a symbolic variable occurring in the pig latin constraints.
the high level idea is that s edge maps an execution path to a conjunction of arithmetic or string constraints over symbolic variables and constants.
each symbolic variable has a name and a data type such as int and long mapping to a field of a table in a pig latin script with the same name and data type.
s edge then invokes z3 to find a solution to that constraint system.
if the constraint solver finds a solution s edge maps it back to input tuples tuples from load .
s edge supports mapping all pig latin data types into a symbolic variable with support for overflow and underflow checked arithmetic.
a int integers are represented as bit signed bitvectors since the z3 constraint solver has better support for bit vector arithmetic than for integer arithmetic.
arithmetic calculations over integers are thus simulated with arithmetic calculations over bit vectors.
the simulation is accurate and takes into account the java apache pig is written in java representation of values of type int as bit vectors.
additional constraints are created to check that the bit wise computation does not overflow and underflow.
standard library conversion functions e.g.
java.lang.long.parselong string are used to translate back from bit vectors into integers.
b long similar to int long integers are represented by bit signed bit vectors since z3 does not support long integer arithmetic.
c float double floating point numbers are represented by real numbers in the form of fractions of long integers.
no current constraint solvers have good support for floating point arithmetic.
calculations with floating point numbers are thus approximated by real valued calculations.
a real number in the form of fractions of long integers can be translated to a floating point number by first representing the fraction using bigfraction from apache common math library 4and invokingbigfraction.floatvalue orbigfraction.doublevalue to get the fraction as a float or double respectively .
d chararray a character array is represented by java.lang.string which is also the inner representation of a character array in pig latin.
bytearray we do not support byte arrays directly.
we try to identify the type that the byte array can convert to at runtime and cast it.
f boolean a boolean variable is represented by an integer with 3values 1forfalse 0forundef 1for true .
arithmetic and string constraints are typically expressed over fields of simple types as listed above.
therefore we do not define complex types tuple bag map for symbolic variables.
b. arithmetic and string constraint solving as mentioned earlier s edge uses z3 to solve arithmetic constraints.
since z3 provides a c interface and s edge is implemented using java to have access to z3 s c api from java we employ swig.5we wrap z3 s c api using java proxy classes and generate java native interface wrapper code automatically.
a common problem in wrapping c programs for java is that values may be returned in function parameters in c but in java values are typically returned in the return value of a function.
s edge uses typemaps in swig a code generation rule that is attached to a specific c data type to overcome the problem.
given the data type dof a value returned in function parameters s edge constructs a structure scontaining a member variable of type d. it also registers a typemap such that any occurrence of a function parameter of type din a function call in z3 is converted into s the return parameter s s value can be read after returning from the function in java.
for string constraints the main new element of our implementation concerns reasoning about string constraints containing regular expressions.
our approach is based on xeger6a java library for generating a sample string for a regular expression.
xeger builds a deterministic finite automaton dfa for a string constraint in the form of a regular expression and follows the edges of the dfa probabilistically until it arrives at an accepting state of the dfa.
xeger is suboptimal for two reasons first it may keep visiting the same state until a stack overflow error happens second it does not support union concatenated repetition intersection concatenation or complement of regular expressions.
to avoid the stack overflow error our approach keeps a map from state id to the number of times a state has been entered and reduces the probability of re entering that state proportionally.
to support union concatenated repetition intersection concatenation and complement of regular expressions we add an intermediate step between building the dfa and following dfa edges to return a new deterministic automaton for the appropriate regular expression.
for example when generating a satisfying assignment that maps a string to a value so that the constraint matches .
apache.
and matches .
commons.
is satisfied we intersect the automaton representing .
apache.
with the automaton representing .
commons.
.
e valuation in this section we evaluate the implementation of s edge by running a wide spectrum of actual pig latin programs.
we measured both the completeness of generated example tuples and the run time of example generation for s edge and the original pig example data generator abbreviated to olston s system in our discussion .
compared to olston s system our experiments confirm that s edge achieves higher completeness.
in most experiments s edge also incurs a lower running time.
a. benchmark programs to evaluate our system we applied it to two benchmark suites we use the entirety of the pigmix benchmark suite consisting of pig programs designed to model practical pig problems.
we use eleven sample sql queries the first ten in the list and an th selected for being complex from the sloan digital sky survey sdss set7and hand translated them directly into pig code.
the complex query contains filter operations and join operations.
the pigmix benchmark provides pig latin programs for testing a set of features such as data with many fields but only a few are used and merge join .
the sdss sample queries typically search for an astronomical object based on some criteria.
for example program of the sdss set is below 1a load galaxy3 using pigstorage 2as colc g float colc r float cx float cy float 4b filter a by .
cx .
cy .
6and .
cx .
cy .
listing .
sdss program the program finds galaxies in a given area of the sky using a coordinate cut in the unit vector cx cy cz.
as can be seen these benchmark programs are typically short with only a handful of them exhibiting interesting complexity.
for each query set we used the input data that accompany the relevant queries.
pigmix ships with a tuple synthesizer that generates such data.
the sdss benchmark suite is designed for the digital sky survey data from the sdss data release .
we selected a random sampling of tuples from the database of this benchmark in which the total amount of data is gb and the total number of rows exceeds .
billion.
b. methodology and setup since the importance of tuple synthesis varies not only by benchmark but also by the size of the tuples supplied to the first analysis pass in the ideal case tuple synthesis is entirely unnecessary we ran our benchmarks for sample input tuple sizes of and tuples using our system and olston s system.
for each sample input tuple size we executed benchmark program times with different randomly sampled input tuples.
all experiments were performed on a four core .
ghz machine with gb of ram.
we configured our system to compare directly to olston s system which is implemented as the illustrate command in pig latin.
unfortunately the current implementation of olston s system has some limitations not mentioned in the published paper.
we wanted to evaluate against the approach and not against the implementation.
to that end we addressed such limitations or tweaked the benchmark programs so that the problems do not manifest themselves.
the first issue is that the downstream pass would discard all sample input containing null fields.
in the upstream pass if all input tuples happen to contain null fields and thus all of them are discarded there would be a nullpointerexception .
we sidestep the null field issue by ensuring that at least one sample input does not contain null fields.
in addition the system can only handle filter conditions at most as it encodes pertinent equivalence classes for filter conditions as individual bits in a bit integer index variable.
this problem affects one pigmix benchmark program intended for scalability testing.
the benchmark has a very large set of filter conditions.
we sidestepped the problem by making changes to the pigmix program so that the resulting program has only filter conditions.
also when reasoning about a join in the upstream pass a nullpointerexception is thrown if no data are observed in the input side of a join typically because one of its upstream neighbors is a highly selective operator .
we address this issue by skipping the join if its input has no data and then attempt to continue upstream propagation.
moreover by mistakenly setting a non tuple field to a tuple in a method involved in upstream propagation a type casting error arises which impedes the ability of olston s system to reason over the join and foreach operations if their downstream neighbor is a filter operator.
we disallow assigning the non tuple field to a tuple in the problematic method.
c. results we ran each experiment times and averaged the completeness of runs since the completeness may theoretically vary due to different random choice of initial samples .
the size of the input data has little effect.
the results are almost the same for all sample input sizes.
figures and figure show the average completeness for each pig latin program in the pigmix and sdss sets respectively for a sample input size of tuples.
every bar corresponds to one program with the exception of program l12 in figure in which there are three subprograms and example data were generated for three different root operators corresponding to the three subprograms for a total of programs.
as can be seen we improve on completeness for out of pigmix benchmark programs and out of sdss benchmark programs.
although the benchmark programs are small and much of their coverage is achieved with random sampling of real inputs they demonstrate clearly the benefits of ours1l1l2l3l4l5l6l7l8l9l 10l 11l 1l 2l 3l 13l 14l 15l 16l .
.
.
.
.
sedge olston pig latin illustrate completenessfig.
.
completeness of sample data generation for the pigmix benchmarks .
.
.
.0completeness sedge olston pig latin illustrate fig.
.
completeness of sample data generation for the sdss benchmarks approach.
practically every program in the two benchmark sets that has any kind of complexity either more than one operator in the same path or a user defined function or complex filter conditions is not fully covered by olston s approach.
for example olston s system cannot generate data that fail the filter in the presence of grouping projecting udf invocation in the following program program s1in figure .
1a load widerow using pigstorage u0001 as name chararray c0 int c1 int ... c31 int 2b group a by name parallel parrallelfactor 3c foreach b generate group sum a.c0 as c0 sum a.c1 as c1 ... sum a.c31 as c500 4d filter c by c0 and c1 and c2 ... and c31 listing .
pigmix program s1 in fact s edge achieves perfect coverage i.e.
full completeness for all benchmark programs.
compared to olston s approach our improved coverage is due to stronger constraint solving ability for programs in figure to udf handling ability for programs in figure and also to inter related constraints and global reasoning for programs s1 l5 l12 l12 l12 in figure and programs in figure .
we also recorded how long it took s edge and olston s system to finish example generation.
we include the infrastructure bootstrap time on each benchmark program.
both s edge and olston s system need to prepare the hadoop execution environment for new executions.
s edge needs to load its constraint solver z3 and coral as well.
as can be seen in figure and figure s edge is fasteron average than olston s system in out of pigmix benchmark programs and out of sdss benchmark programs.
for the rest of benchmark programs s edge incurs a little higher running time than olston s system.
from these numbers we can infer that although we have to conduct path exploration and constraint solving there are even time savings in most cases due to avoiding the step of pruning redundant tuples after the upstream pass because our approach does not generate redundant data .
s1l1l2l3l4l5l6l7l8l9l 10l 11l 1l 2l 3l 13l 14l 15l 16l .
.
.
.
.
.
.
.
.
.
.
.4running time seconds sedge olston pig latin illustrate fig.
.
running time of sample data generation for the pigmix benchmarks .
.
.
.
.
.
.
.
.
.
.
.
.
.
.0running time seconds sedge olston pig latin illustrate fig.
.
running time of sample data generation for the sdss benchmarks vi.
d iscussion w hyhigh level dse a natural qualitative comparison is between a dynamic symbolic execution dse engine at the level of the pig latin language and dse engines for imperative languages since pig latin code is eventually compiled into imperative code that uses a map reduce library.
the expected benefits from our approach are a simplicity b conciseness of the generated test cases i.e.
the same coverage with fewer tests and c completeness an imperative dse engine may have trouble solving constraints over the logically more complex generated code rather than the original pig latin code.
furthermore an imperative dse engine cannot take advantage of the lack of side effects in order to better concretize user defined functions as discussed in section iii b. we compared s edge with the pex state of the art dse engine in a limit study.
pex accepts c input hence we hand translated pig latin programs into c programs.8the resulting c programs are single threaded without any call to the mapreduce api in order to test the applicability of pex in the ideal case.
the inclusion of the map reduce library complicates the control flow of the imperative program even more and can easily cause the dse engine to miss a targeted branch of test execution leading to low coverage of generated test cases .
for our translation we inspected the java code generated by the pig compiler and made a best effort attempt to replicate it inc without map reduce calls.
we translated programs from our pig latin benchmark suites.
since pex has no knowledge of the original input it accepts concrete values only when passed into the test method as parameters with primitive types we enable just the 3rd pass upstream pass of s edge for a fair comparison i.e.
s edge also does not benefit from sampled real data this also disadvantages sedge as it removes the advantage of better udf handling .
the results confirm our expectation.
the conciseness of the test suite generated by pex is low since pex needs to examine a lot of irrelevant low level branches or constraints that are not necessary for equivalence class coverage of the high level pig latin control flow.
for example for step a in the pig latin program in listing pex generates tuples within tables of which tuples pass the filter in step b while s edge generates tuples within exactly table of which tuple passes the filter in step b. the conciseness of the test suite generated by pex is .
while the conciseness of the test suite generated by s edge is .
.
furthermore for specific complex constructs we also get much higher completeness although quite often pex also gets perfect coverage.
in our experience for pig latin programs containing filter statements after co group or join statements the test suites yielded by pex lack in completeness.
for instance in the sdss program with filter operations and join operations the pex completeness is only .
.
vii.
r elated work dataflow languages such as pig can been seen as a compromise between declarative languages such as sql and imperative languages such as c and java.
that is pig combines the declarative feature of straightforward parallel computation with the imperative feature of explicit intermediate results.
there is little work discussed in earlier sections that addresses test data generation for dataflow languages.
instead the related work from various research communities has focused on the extreme ends of this spectrum i.e.
either on sql or on java like programming languages.
specifically related work in the software engineering community has focused on traditional procedural and objectoriented database centric programs both using static symbolic execution and dynamic symbolic execution .
while our work is inspired by such earlier dynamic 8although there are dse engines for java e.g.
dsc they do not match the industrial strength nature of pex.
dsc for instance does not support programs with floating point numbers which are common in pig latin.symbolic execution approaches we adapted this work to dataflow programs and their execution semantics.
at the other end there is work that automatically generates database data that satisfy external constraints but there is no coverage or conciceness goal and no application to dataflow languages.
other work has introduced the idea of code coverage to sql queries.
for our purposes we reused the concept of coverage for pig latin as defined by olston et al.
.
in the formal methods community qex is generating test inputs for sql queries .
similar to our work qex maps a sql query to smt and uses the z3 constraint solver to infer data tables.
however qex differs from our work in that qex does not have a dynamic program analysis component and therefore cannot observe how a query processes existing example data.
earlier work in the software engineering community on dynamic symbolic execution has shown that dynamic analysis can make such program analysis more efficient and enable it to reason about user defined functions which we leverage in our work.
in the database community a common methodology for testing a database management system or a database application is to generate a set of test databases given target query workloads.
overall our problem differs in that instead of a whole database we aim to generate a small or minimum if desired set of tuples that have perfect path coverage of a given dataflow program.
the recent work on reverse query processing takes an application query and a result set as input and generates a corresponding input database by exploiting reverse relational algebra .
in comparison our work focuses on dataflow programs for big data applications where many operators are non relational e.g.
map reduce and arbitrary user defined functions and hence a reverse algebra may not exist.
the qagen system further takes into account a set of constraints usually cardinality and data distribution in input and operator output tables and aims to generate a database that satisfies these constraints.
analogously to earlier work in the formal methods community this work performs a static symbolic analysis and does not obtain additional information from a dynamic analysis.
viii.
c onclusions and future work generating example input data for dataflow programs has emerged as an important challenge.
we presented s edge an approach and tool for generating example data of dataflow programs using dse in order to achieve high coverage.
s edge builds symbolic constraints over the equivalence classes induced by dataflow programming language constructs and can reason over constraints on user defined functions by exploiting dynamic values as hints.
we implemented our technique for the pig dataflow system and compared it empirically with the most closely related prior work.
while we currently focus on the pig latin programming language the principles are quite general.
the same high level technique can be applied to other dataflow programming languages such as dryadlinq and hyracks asterix and relational algebra such as relational division and anti join.