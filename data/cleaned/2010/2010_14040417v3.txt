mining idioms from source code miltiadis allamanis charles sutton school of informatics university of edinburgh edinburgh eh8 9ab uk m.allamanis csutton ed.ac.uk abstract we present the rst method for automatically mining code idioms from a corpus of previously written idiomatic software projects.
we take the view that a code idiom is a syntactic fragment that recurs across projects and has a single semantic role.
idioms may have metavariables such as the body of a forloop.
modern ides commonly provide facilities for manually de ning idioms and inserting them on demand but this does not help programmers to write idiomatic code in languages or using libraries with which they are unfamiliar.
we present haggis a system for mining code idioms that builds on recent advanced techniques from statistical natural language processing namely nonparametric bayesian probabilistic tree substitution grammars.
we apply haggis to several of the most popular open source projects from github.
we present a wide range of evidence that the resulting idioms are semantically meaningful demonstrating that they do indeed recur across software projects and that they occur more frequently in illustrative code examples collected from a q a site.
manual examination of the most common idioms indicate that they describe important program concepts including object creation exception handling and resource management.
.
introduction programming language text is a means of human communication.
programmers write code not simply to be executed by a computer but also to communicate the precise details of the code s operation to later developers who will adapt update test and maintain the code.
it is perhaps for this reason that source code is natural in the sense described by hindle et al.
.
programmers themselves use the term idiomatic to refer to code that is written in a manner that other experienced developers nd natural.
programmers believe that it is important to write idiomatic code.
this is evidenced simply by the amount of time that programmers spend telling other programmers how to do this.
for example wikibooks has a book devoted to c idioms and similar guides are available for java and javascript .
a guide on github for writing idiomatic javascript has more stars and forks.
a search for the keyword idiomatic on stackover ow yields over hits all but one of the rst hits are questions about what the idiomatic method is for performing a given task.
the notion of code idiom is one that is commonly used but seldom de ned.
we take the view that an idiom is a syntactic fragment that recurs frequently across software projects and has a single semantic role.
idioms may have metavariablesthat abstract over identi er names and code blocks.
for example in java the loop for int i i n i ... is a common idiom for iterating over an array.
it is possible to express this operation in many other ways such as a do while loop or using recursion but as experienced java programmers ourselves we would nd this alien and more difcult to understand.
idioms di er signi cantly from previous notions of textual patterns in software such as code clones and api patterns .
unlike clones idioms commonly recur across projects even ones from di erent domains and unlike api patterns idioms commonly involve syntactic constructs such as iteration and exception handling.
a large number of example idioms all of which are automatically identi ed by our system are shown in figures and .
major ides currently support idioms by including features that allow programmers to de ne idioms and easily reuse them.
eclipse s snipmatch and intellij idea live templates allow the user to de ne custom snippets of code that can be inserted on demand.
netbeans includes a similar code templates feature in its editor.
recently microsoft created bing code search that allows users to search and add snippets to their code by retrieving code from popular coding websites such as stackover ow.
the fact that all major ides include features that allow programmers to manually de ne and use idioms attests to their importance.
we are unaware however of methods for automatically identifying code idioms.
this is a major gap in current tooling for software development which causes signi cant problems.
first software developers cannot use manual ide tools for idioms without signi cant e ort to organize the idioms of interest and then to manually add them to the tool.
this is especially an obstacle for less experienced programmers that do not know which idioms they should be using.
indeed as we demonstrate later many idioms are library speci c so even an experienced programmer will not be familiar with the code idioms for a library that they have just begun to use.
therefore the ability to automatically identify idioms is needed.
in this paper we present the rst method for automatically mining code idioms from an existing corpus of idiomatic code.
at rst this might seem to be a simple proposition simply search for subtrees that occur often in a syntactically parsed corpus.
however this naive method does not work well for the simple reason that frequent trees are not necessarily interesting trees.
to return to our previous example for loops are much more common than forloops that iterate over arrays but one would be hard pressed to argue that for ... ... on its own that is with no expressions orarxiv .0417v3 jun 2014body is an interesting pattern.
instead we rely on a di erent principle interesting patterns are those that help to explain the code that programmers write.
as a measure of explanation quality we use a probabilistic model of the source code and retain those idioms that make the training corpus more likely under the model.
these ideas can be formalized in a single theoretically principled framework using a nonparametric bayesian analysis.
nonparametric bayesian methods have become enormously popular in statistics machine learning and natural language processing because they provide a exible and principled way of automatically inducing a sweet spot of model complexity based on the amount of data that is available .
in particular we employ a nonparametric bayesian tree substitution grammar which has recently been developed for natural language but which has not been applied to source code.
because our method is primarily statistical in nature it is language agnostic and can be applied to any programming language for which one can collect a corpus of previouslywritten idiomatic code.
our major contributions are we introduce the idiom mining problem section we present haggis a method for automatically mining code idioms based on nonparametric bayesian tree substitution grammars section we demonstrate that haggis successfully identi es cross project idioms section for example of idioms that we identify from one set of open source projects also appear in an independent set of snippets of example code from the popular q a site stackoverow examining the most common idioms that haggis identi es figure we nd that they describe important program concepts including object creation exception handling and resource management to further demonstrate that the idioms identi ed by haggis are semantically meaningful we examine the relationship between idioms and code libraries subsection .
nding that many idioms are strongly connected to package imports in a way that can support suggestion.
.
problem definition acode idiom is a syntactic fragment that recurs across software projects and serves a single semantic purpose.
an example of an idiom is shown in figure b .
this is an idiom which is used for manipulating objects of type android.database.cursor which ensures that the cursor is closed after use.
this idiom is indeed discovered by our method.
as in this example typically idioms have parameters which we will call metavariables such as the name of the cursor variable and a code block describing what should be done if the movetofirst operation is successful.
an android programmer who is unfamiliar with this idiom might make bad mistakes like not calling the close method or not using a finally block causing subtle memory leaks.
many idioms like the close example or those in figure are speci c to particular software libraries.
other idioms are general across projects of the same programming language such as those in figure including an idiom for looping over an array or an idiom de ning a string constant.
again all of the idioms in these gures are discovered automatically by our method.
idioms concerning exception handling andresource management are especially important to identify and suggest because failure to use them correctly can cause the software to violate correctness properties.
as these examples show idioms are usually parameterized and the parameters often have syntactic structure such as expressions and code blocks.
we de ne idioms formally as fragments of abstract syntax trees which allows us to naturally represent the syntactic structure of an idiom.
more formally an idiom is a fragment t v e of an abstract syntax tree ast by which we mean the following.
let gbe the context free grammar of the programming language in question.
then a fragment tis a tree of terminals and nonterminal from gthat is a subgraph of some valid parse tree from g. an idiomtcan have as leaves both terminals and nonterminals.
non terminals correspond to metavariables which must be lled in when instantiating the idiom.
for example in figure c the shaded lines represent the fragment for an example idiom notice how the block node of the ast which is a non terminal corresponds to a body metavariable in the pattern.
idiom mining current ides provide tools for manually de ning idioms and inserting them when required but this requires that the developer incur the required setup cost and that the developer know the idioms in the rst place.
to eliminate these di culties we introduce the idiom mining problem namely to identify a set of idioms automatically given only a corpus of previously written idiomatic code.
more formally given a training set of source les with abstract syntax treesd ft1 t2 tng the idiom mining problem is to identify a set of idioms i ftigthat occur in the training set.
this is an unsupervised learning problem as we do not assume that we are provided with any example idioms that are explicitly identi ed.
each fragment tishould occur as a subgraph of every tree in some subset d ti d of the training corpus.
what idioms are not idioms are not clones.
a large amount of work in software engineering considers the problem of clone detection some of which considers syntactic clones which nd clones based on information from the ast.
clones are contiguous blocks of code that are used verbatim or nearly so in di erent code locations usually within a project and often created via copy paste operations.
idioms on the other hand typically recur across projects even those from very di erent domains and are used independently by many di erent programmers.
additionally idioms are typically not contiguous instead they have metavariables that bind to expressions or entire code blocks.
finally idioms have a semantic purpose that developers are consciously aware of.
indeed we hypothesize that programmers chunk idioms into single mental units and often type them in to programs directly by hand although the psychological research necessary to verify this conjecture is beyond the scope of the current paper.
also idiom mining is not api mining.
api mining is an active research area that focuses on mining groups of library functions from the same api that are commonly used together.
these types of patterns that are 1as a technicality programming language grammars typically describe parse trees rather than ast but as there is a mapping between the two we will assume that we have available a cfg that describes asts directly.... if c !
null try if c.movetofirst number c.getstring c.getcolumnindex phonecolumn finally c.close ...ifstatement expression c!
null then block trystatement body ifstatement expr methodinvocation expr var android.database.cursor name c name movetofirst then block number c.getstring c.getcolumnindex phonecolumn finally block expressionstatement methodinvocation expr var android.database.cursor name c name closee!
e t f f e t t prob d a try if cursor .movetofirst body finally cursor .close b c figure an example of how code idioms are extracted from asts.
a a snippet of code from the phonenumberutils in the github project android.telephony .
b a commonly occurring idiom when handling android.database.cursor objects.
this idiom is successfully discovered by haggis .
c a partial representation of the ast returned by the eclipse jdt for the code in a .
the shaded nodes are those that are included in the idiom.
d an example of a ptsg rule for a simple expression grammar.
see text for more details.
inferred are essentially sequences or sometimes nite state machines of method invocations.
although api patterns have the potential to be extremely valuable to developers idiom mining is a markedly di erent problem because idioms have syntactic structure.
for example current api mining approaches cannot nd patterns such as a library with a tree class that requires special iteration logic or a java library that requires the developer to free resources within a finally block.
these are exactly the type of patterns that haggis identi es.
simple methods do not work a natural rst approach to this problem is to search for ast fragments that occur frequently for example to return the set of all fragments that occur more than a user speci ed parameter mtimes in the training set.
this task is called frequent tree mining and has been the subject of some work in the data mining literature .
unfortunately our preliminary investigations found that these approaches do not yield good idioms.
instead the fragments that are returned tend to be small and generic omitting many details that to a human eye are central to the idiom.
for example given the idiom in figure c it would be typical for tree mining methods to return a fragment containing the try if and finally nodes but not the crucial method call to cursor.close .
the reason for this is simple given a fragment tthat represents a true idiom it can always be made more frequent by removing one of the leaves even if it is strongly correlated with the rest of the tree.
so tree mining algorithms will tend to return these shorter trees resulting in incomplete idioms.
in other words frequent patterns can be boring patterns.
to avoid this problem we need a way of penalizing the method when it chooses notto extend a pattern to include a nodethat co occurs frequently.
this is exactly what is provided by our probabilistic approach.
.
mining code idioms in this section we introduce the technical framework that is required for haggis 2our proposed method for the idiom mining problem.
at a high level we approach the problem of mining source code idioms as that of inferring of commonly reoccurring fragments in asts.
but as we have seen simple methods of formalizing this intuition do not work see section we resort to methods that are not as simple.
we apply recent advanced techniques from statistical nlp but we need to explain them in some detail to justify why they are appropriate for this software engineering tasks and why technically simpler methods would not be e ective.
we will build up step by step.
first we will describe two syntactic probabilistic models of source code probabilistic context free grammars and probabilistic tree substitution grammars ptsg .
we will explain why ptsgs provide a straightforward framework for augmenting a simple cfg to represent idioms.
the reason that we employ probabilistic models here rather than a standard deterministic cfg is that probabilities provide a natural quantitative measure of the quality of a proposed idiom a proposed idiom is worthwhile only if when we include it into a ptsg it increases the probability that the ptsg assigns to the training corpus.
at rst it may seem odd that we apply grammar learning methods here when of course the grammar of the programming language is already known.
we clarify that our aim isnotto re learn the known grammar but rather to learn 2holistic automatic gathering of grammatical idioms from software.probability distributions over parse trees from the known grammar.
these distributions will represent which rules from the grammar are used more often and crucially which rules tend to be used contiguously.
the ptsg provides us with a way to represent idioms but then we still need a way to discover them.
it is for this purpose that we employ nonparametric bayesian methods a powerful general framework that provides methods that automatically infer from data how complex a model should be.
after describing nonparametric bayesian methods we will nally describe how to apply nonparametric bayesian methods to ptsgs which requires a particular approximation known as markov chain monte carlo.
.
probabilistic grammars aprobabilistic context free grammar pcfg is a simple way to de ne a distribution over the strings of a context free language.
a pcfg is de ned as g n s r where is a set of terminal symbols na set of nonterminals s2nis the root nonterminal symbol and ris a set of productions.
each production in rhas the form x!y wherex2nandy2 n .
the set is a set of distributions p rjc wherec2nis a non terminal and r2ris a rule with con its left hand side.
to sample a tree from a pcfg we recursively expand the tree beginning ats and each time we add a non terminal cto the tree we expand cusing a production rthat is sampled from the corresponding distribution p rjc .
the probability of generating a particular tree tfrom this procedure is simply the product over all rules that are required to generate t. the probability p x of a string x2 is the sum of the probabilities of the trees tthat yieldx that is we simply considerp x as a marginal distribution of p t .
tree substitution grammars a tree substitution grammar tsg is a simple extension to a cfg in which productions expand into tree fragments rather than simply into a list of symbols.
formally a tsg is also a tuple g n s r where n s are exactly as in a cfg but now each productionr2rtakes the form x!t x wheretxis a fragment.
to produce a string from a tsg we begin with a tree containing only s and recursively expanding the tree in a manner exactly analogous to a cfg the only di erence is that some rules can increase the height of the tree by more than .
a probabilistic tree substitution grammar ptsg g augments a tsg with probabilities in an analogous way to a pcfg.
a ptsg is de ned as g n s r where is a set of terminal symbols na set of non terminal symbols s2nis the root non terminal symbol ris a set of tree fragment productions.
finally is a set of distributions ptsg txjx for allx2n each of which is a distribution over the set of all rules x!t xinrthat have left hand sidex.
the key reason that we use ptsgs for idiom mining is that each tree fragment txcan be thought of as describing a set of context free rules that are typically used in sequence.
this is exactly what we are trying to discover in the idiom mining problem.
in other words our goal will be to induce a ptsg in which every tree fragment represents a code idiom if the fragment has depth greater than or a rule from the language s original grammar if the depth equals .
as asimple example consider the pcfg e!e e prob t!f f prob e!t prob t!f prob f!
e prob f!id prob wherexandyare non terminals and ethe start symbol.
now suppose that we are presented with a corpus of strings from this language that include many instances of expressions likeid id id andid id id id perhaps generated by a group of students who are practicing the distributive law .
then we might choose to add a single ptsg rule to this grammar displayed in figure d adjusting the probabilities for that rule and the e!t tande!t rules so that the three probabilities sum to .
essentially this allows us to a represent a correlation between the rules e!t tandt!f f. finally note that every cfg can be written as a tsg where all productions expand to trees of depth .
conversely every tsg can be converted into an equivalent cfg by adding extra non terminals one for each tsg rule x!tx .
so tsgs are in some sense fancy notation for cfgs.
this notation will prove very useful however when we describe the learning problem next.
learning tsgs now we de ne the learning problem for tsgs that we will consider.
first we say that a ptsg g1 n1 s1 r1 p1 extends a cfgg0if every tree with positive probability under g1is grammatically valid according to g0.
given any settof tree fragments from g0 we can de ne a ptsg g1that extends g0as follows.
first set n1 s1 n0 s0 .
then set r1 rcfg rfrag wherercfg is the set of all rules from r0 expressed in the tsg form i.e.
with right hand sides as trees of depth andrfrag is a set of fragment rules xi!t i for all ti2tand wherexiis the root ofti.
the grammar learning problem that we consider can be called the cfg extension problem .
the input is a set of treest1 tnfrom a context free language with grammar g0 n0 s0 r0 .
the cfg extension problem is simply to learn a ptsg g1that extends g0and is good at explaining the training set t1 tn.
the notion of good is deliberately vague formalizing it is part of the problem.
it should also be clear that we are not trying to learn the cfg for the original programming language instead we are trying to identify sequences of rules from the known grammar that commonly co occur contiguously.
a na ve idea is to use maximum likelihood that is to nd the ptsgg1that extends g0and maximizes the probability thatg1assigns tot1 tn.
this does not work.
the reason is that a trivial solution is simply to add a fragment rule e!
tifor every training tree ti.
this will assign a probability of nto each training tree which in practice will usually be optimal.
what is going on here is that the maximum likelihood grammar is over tting.
it is not surprising that this happens there are an in nite number of potential trees that could be used to extend g0 so if a model is given such a large amount of exibility over tting becomes inevitable.
what we need is a strong method of controlling over tting which the next section provides.
.
nonparametric bayesian methods at the heart of any application of machine learning is the need to control the complexity of the model.
for example in a clustering task many standard clustering methods such ask means require the user to pre specify the number of clusterskin advance.
if kis too small then each cluster will be very large and not contain useful information about the data.
if kis too large then each cluster will only contain a few data points so the again the cluster centroid will not tell us much about the data set.
for the cfg extension problem the key factor that determines model complexity is the number of fragment rules that we allow for each non terminal.
if we allow the model to assign too many fragments to each non terminal then it can simply memorize the training set as described in the previous section.
but if we allow too few then the model will be unable to nd useful patterns.
nonparametric bayesian methods provide a powerful and theoretically principled method for managing this trade o .
to explain how this works we must rst explain bayesian statistics.
bayesian statistics is alternative general framework to classical frequentist statistical methods such as con dence intervals and hypothesis testing.
the idea behind bayesian statistics is that whenever one wants to estimate an unknown quantity from a data set x1 x2 xn the analyst should choose a prior distribution p that encodes any prior knowledge about if little is known this distribution can be vague and then a model p x1 xnj .
once we de ne these two quantities the laws of probability provide only one choice for how to infer which is to compute the conditional distribution p jx1 xn using bayes rule.
this distribution is called the posterior distribution and encapsulates all of the information that we have about from the data.
bayesian methods provide powerful general tools to combat over tting as the prior p can be chosen to encourage simpler models.
if is a nite dimensional set of parameters such as the mean and the variance of a gaussian distribution then it is easy to construct an appropriate prior p .
constructing a prior becomes more di cult however when does not have a xed number of dimensions which is what occurs when we wish to infer the model complexity automatically.
for example consider a clustering model where we want to learn the number of clusters.
in this case would be a vector containing the centroid for each cluster but then because before we see the data the number of clusters could be arbitrarily large has unbounded dimension.
as another example in the case of the cfg extension problem we do not know in advance how many fragments are associated with each non terminal and so want to infer this from data.
nonparametric bayesian methods focus on developing prior distributions over in nite dimensional objects which are then used within bayesian statistical inference.
bayesian nonparametrics have been the subject of intense research in statistics and in machine learning with popular models including the dirichlet process and the gaussian process .
applying this discussion to the cfg extension problem what we are trying to infer is a ptsg so to apply bayesian inference our prior distribution must be a probability distribution over probabilistic grammars .
we will bootstrap this from a distribution over context free fragments which we de ne rst.
let g0be the known context free grammar for the programming language in question.
we will assume that we have available a pcfg for g0 because this can be easily estimated by maximum likelihood from our training corpus call this distribution over trees pml.
now pmlgives us a distribution over full trees but what we will require is a distribution over fragments .
we de ne this simply as p0 t pgeom jtj p y r2tpml r wherejtjis the size of the fragment t pgeomis a geometric distribution with parameter p andrranges over the multiset of productions that are used within t. now we can de ne a prior distribution over ptsgs.
recall that we can de ne a ptsg g1that extends g0by specifying a set of tree fragments fxfor each non terminal x. so to de ne a distribution over ptsgs we will de ne a distribution p fx over the set of tree fragments rooted at x. we need p fx to have several important properties.
first we need p fx to have in nite support that is it must assign positive probability to all possible fragments .
this is because if we do not assign a fragment positive probability in the prior distribution we will never be able to infer it as an idiom no matter how often it appears.
second we want p fx to exhibit a rich get richer e ect namely once we have observed that a fragment txoccurs many times we want to be able predict that it will occur more often in the future.
the simplest distribution that has these properties is the dirichlet process dp .
the dirichlet process has two parameters a base measure 3which in our case will be the fragment distribution p0 and a concentration parameter 2r which controls how strong the rich get richer e ect is.
one simple way to characterize the dirichlet process is the stick breaking representation .
using this representation a dirichlet process de nes a distribution over fxas pr 1x k k ft tkgtk p0 uk beta k uk k 1y j 1uj to interpret this recall that the symbol is read is distributed as and the beta distribution is a standard distribution over the set as becomes large the mean of a beta distribution will approach .
intuitively what is going on here is that a sample from the dp is a distribution over a countably in nite number of fragments t1 t2 .
each one of these fragments is sampled independently from the fragment distribution p0.
to assign a probability to each fragment we recursively split the interval into a countable number of sticks .
the value uk de nes what proportion of the remaining stick is assigned to the current sample tk and the remainder is assigned to the in nite number of remaining trees tk tk .
this process de nes a distribution over fragments fxfor each non terminal x and hence a distribution p g1 over the set of all ptsgs that extend g0.
we will refer to this distribution as a dirichlet process probabilistic tree substitution grammar dpptsg .
this process may seem odd for two reasons a each sample from p g1 is in nitely large so we cannot store it exactly on a computer b the fragments from g1are sampled randomly from a pcfg so there is no reason to think that they should match real idioms.
fortunately the 3the base measure will be a probability measure so for our purposes here we can think of this as a fancy word for base distribution .stt ts figure sampling an ast.
nodes with dots show the points where the tree is split i.e.zt .
nodes with double border represent terminal nodes.
answer to both these concerns is simple.
we are notinterested in the fragments that exist in the prior distribution but rather of those in the posterior distribution.
more formally the dp provides us with a prior distribution g1over ptsgs.
but g1itself like any ptsg de nes a distribution p t1 t2 tnjg1 over the training set.
so just as in the parametric case we can apply bayes s rule to obtain a posterior distribution p g1jt1 t2 tn .
it can be shown that this distribution is also a dpptsg and amazingly that this posterior dpptsg can be characterized by a nite set of fragmentsf0 xfor each non terminal.
it is these fragments that we will identify as code idioms section .
.
inference what we have just discussed is how to de ne a posterior distribution over grammars that will infer code idioms.
but we still need to describe how to compute this distribution.
unfortunately the posterior distribution cannot be computed exactly so we resort to approximations.
the most commonly used approximations in the literature are based on markov chain monte carlo mcmc which we explain below.
but rst we make one more observation about ptsgs.
all of the ptsgs that we consider are extensions of an unambiguous base cfgg0.
this means that given a source le f we can separate the ptsg parsing task into two steps rst parse fusingg0 resulting in a cfg tree t second group the nodes intaccording to which fragment rule in the ptsg was used to generated them.
we can represent this second task as a tree of binary variables zsfor each node s. these variables indicate whether the node sis the root of a new fragment zs or if node sis part of the same fragment as its parent zs .
essentially the variables zsshow the boundaries of the inferred tree patterns see figure for an example.
conversely even if we don t know what fragments are in the grammar given a training corpus that has been parsed in this way we can use the zsvariables to read o what fragments must have been in the ptsg.
with this representation in hand we are now ready to present an mcmc method for sampling from the posterior distribution over grammars using a particular method called gibbs sampling.
gibbs sampling is an iterative method which starts with an initial value for all of the zvariables and then updates them one at a time.
at each iteration the sampler visits every tree node tof every tree in the training corpus and samples a new value for zt.
letsbe the parent oft.
if we choose zt we can examine the current valuesof thezvariables to determine the tree fragment ttthat containstand the fragment tsfors which must be disjoint.
on the other hand if we set zt thensandtwill belong to the same fragment which will be exactly tjoin ts tt.
now we set ztto with probability p zt ppost tjoin ppost tjoin ppost ts ppost tt where ppost t count t p0 t count h t hreturns the root of the fragment and count returns the number of times that a tree occurs as a fragment in the corpus as determined by the current values of z. intuitively what is happening here is that if the fragments tsandttoccur very often together in the corpus relative to the number of times that they occur independently then we are more likely to join them into a single fragment.
it can be shown that if we repeat this process for a large number of iterations eventually the resulting distribution over fragments will converge to the posterior distribution over fragments de ned by the dpptsg.
it is these fragments that we return as idioms.
we present the gibbs sampler because it is a useful illustration of mcmc but in practice we nd that it converges too slowly to scale to large codebases.
instead we use the type based mcmc sampler of liang et al.
details omitted .
.
sampling a tsg for code hindle et al.
have shown that source code presents some of the characteristics of natural language.
haggis exploits this fact by using ptsgs originally devised for natural language to infer code idioms.
here we describe a set of necessary transformations to asts and ptsg to adapt these general methods speci cally to the task of inferring code idioms.
ast transformation for each .java le we use the eclipse jdt to extract its ast a tree structure of astnode objects.
each astnode object contains two sets of properties simple properties such as the type of the operator if astnode is an in x expression and structural properties that contain zero or more child astnode objects.
first we construct the grammar symbols by mapping each astnode s type and simple properties into a terminal or nonterminal symbol.
the transformed tree is then constructed by mapping the original ast into a tree whose nodes are annotated with the symbols.
each node s children are grouped by property.
the transformed trees may contain nodes that have more than two children for a single property e.g.block .
this induces unnecessary sparsity in the cfg and tsg rules.
to reduce this sparsity we perform tree binarization .
this process common in nlp transforms the original tree into binary by adding dummy nodes making the data less sparse.
it will also help us capture idioms in sequential statements.
note that binarization is performed only on structural properties that have two or more children while an arbitrary node may have more than two children among its properties.
one nal hurdle for learning meaningful code idioms are variable names.
since variable names are mostly project orp cde ba a no binarizationp a b c de b binarized figure tree binarization for nodes with multiple children.
square nodes represent the dummy nodes added.
class speci c we abstract them introducing an intermediate metavariable node between the simplename node containing the string representation of the variable name and its parent node.
metavariable nodes are also annotated with the type of the variable they are abstracting.
this provides the ptsg with the exibility to either exclude or include variable names as appropriate.
for example in the snippet of figure a by using metavariables we are able to learn the idiom in figure b without specifying the name of the cursor object by excluding the simplename nodes from the fragment.
alternatively if a speci c variable name is common and idiomatic such as the iin a forloop the ptsg can choose to include simplename in the extracted idiom by merging it with its parent metavariable node.
training tsgs and extracting code idioms training a ptsg happens o ine during a separate training phase.
after training the ptsg we then extract the mined code idioms which then can be used for any later visualization.
in other words a user of a haggis ide tool would never need to wait for a mcmc method to nish.
the output of a mcmc method is a series of approximate samples from the posterior distribution each of which in our case is a single ptsg.
these sampled ptsgs need to be post processed to extract a single meaningful set of code idioms.
first we aggregate the mcmc samples after removing the rst few samples as burn in which is standard methodology for applying mcmc.
then to extract idioms from the remaining samples we merge all samples tree fragments into a single multiset.
we then prune the multiset by removing all tree fragments that have been seen less than cmintimes to ensure that the mined tree fragments are frequent enough.
we also prune fragments that have fewer that nminnodes to get a set of non trivial i.e.su ciently large code idioms.
finally we reconvert the fragments back to java code.
the leaf nodes of the fragments that contain non terminal symbols represent metavariables and are converted to the appropriate symbol that is denoted by a pre x. additionally to assist the sampler in inducing meaningful idioms we prune any import statements from the corpus so that they cannot be mined as idioms.
we also exclude some nodes from sampling xing zi 0and thus forcing some nodes to be un splittable.
such nodes include method invocation arguments quali ed and parametrized type node children non block children of while forand ifstatement nodes parenthesized post x and in x expressions and variable declaration statements.
.
code snippet evaluation we take advantage of the omnipresence of idioms in sourcetry regions computeprojections owner catch runtimeexception e e.printstacktrace throw e if elem instanceof iparent ijavaelement children iparent owner .getchildren for int fromposition i children.length i ijavaelement achild children set childregions findannotations achild result removecollisions regions childregions constructannotations elem result regions figure synthetic code randomly generated from a posterior ptsg.
one can see that the ptsg produces code that is syntactically correct and locally consistent.
this e ect allows us to infer code idioms.
it can be seen that as expected the ptsg cannot capture higher level information such as variable binding.
code to evaluate haggis on popular open source projects.
we restrict ourselves to the java programming language due to the high availability of tools and source code.
we emphasize however that haggis is language agnostic.
before we get started an interesting way to get an intuitive feel for any probabilistic model is simply to draw samples from it.
figure shows a code snippet that we synthetically generated by sampling from the posterior distribution over code de ned by the ptsg.
one can observe that the ptsg is learning to produce idiomatic and syntactically correct code although as expected the code is semantically inconsistent.
methodology we use two evaluation datasets comprised of java open source code available on github.
the projects dataset figure contains the top java github projects whose repository is at least 100mb in size according to the github archive .
to determine popularity we computed thez score of forks and watchers for each project.
the normalized scores were then averaged to retrieve each project s popularity ranking.
the second evaluation dataset library figure consists of java classes that import i.e.use popular java libraries.
for each selected library we retrieved from the java github corpus all les that import that library but do not implement it.
we split both datasets into a train and a test set splitting each project in projects and each library leset in library into a train and a test set.
the projects will be used to mine projectspeci c idioms while the library will be used to mine idioms that occur across libraries.
to extract idioms we run mcmc for iterations for each of the projects in projects and each of library lesets in the library allowing su cient burn in time of iterations.
for the last iterations we aggregate a sample posterior ptsg and extract idioms as detailed in section .
a threat to the validity of the evaluation using the aforementioned datasets is the possibility that the datasets are not representative of java development practices containing solely open source projects from github.
however the selected datasets span a wide variety of domains including databases messaging systems and code parsers diminishing any such possibility.
furthermore we perform an extrinsic evaluation on source code found on a popular online q a website stackover ow.name forks stars files commit description arduino electronics prototyping atmosphere a0262bf websocket framework bigbluebutton e3b6172 web conferencing elasticsearch ad547eb rest search engine grails core 15f9114 web app framework hadoop f68ca74 map reduce framework hibernate d28447e orm framework libgdx 0c6a387 game dev framework netty 3f53ba2 net app framework storm cdb116e distributed computation vert.x 9f79416 application platform voldemort 9ea2e95 nosql database wild y 043d7d5 application server figure projects dataset used for in project idiom evaluation.
projects in alphabetical order.
package name files description android.location android location api android.net.wifi android wifi api com.rabbitmq messaging system com.spatial4j geospatial library io.netty network app framework opennlp nlp tools org.apache.hadoop map reduce framework org.apache.lucene search server org.elasticsearch rest search engine org.eclipse.jgit git implementation org.hibernate persistence framework org.jsoup html parser org.mozilla.javascript javascript implementation org.neo4j graph database twitter4j twitter api figure library dataset for cross project idiom evaluation.
each api leset contains all class les that import a class belonging to the respective package or one of its subpackages.
evaluation metrics we compute two metrics on the test corpora.
these two metrics resemble precision and recall in information retrieval but are adjusted to the code idiom domain.
we de ne idiom coverage as the percent of source code ast nodes that can be matched to the mined idioms.
coverage is thus a number between and indicating the extent to which the mined idioms exist in a piece of code.
we de ne idiom set precision as the percentage of the mined idioms found in the test corpus.
this metric shows the precision of mined set of idioms.
using these two metrics we also tune the concentration parameter of the dpptsg model by using android.net.wifi as a validation set yielding .
.
top idioms figure shows the top idioms mined in the library dataset ranked by the number of les in the test sets where each idiom has appeared in.
the reader will observe their immediate usefulness.
some idioms capture how to retrieve or instantiate an object.
for example in figure the id for iterator iter methodinvoc iter.hasnext body a iterate through the elements of an iterator.
private final static log name logfactory.getlog type.class b creating a logger for a class.
public static final string name stringlit c de ning a constant string.
while string bufferedreader .
readline !
null body d looping through lines from a bufferedreader .
figure sample java language idioms.
stringlit denotes a user de ned string literal name a freely de ned variable name methodinvoc a single method invocation statement ifstatement a single ifstatement and body denotes a user de ned code block of one or more statements.
nameprecision coverage avg size nodes haggis .
.
.
nmin cmin 2libraryhaggis .
.
.
nmin cmin deckard .
.
.
mintoks stride sim 1projectshaggis .
.
.
nmin cmin haggis .
.
.
nmin cmin figure average and standard deviation of performance in library test set.
standard deviation across projects.
iom 8a captures the instantiation of a message channel in rabbitmq 8r retrieves a handle for the hadoop le system 8e builds a searchsourcebuilder in elasticsearch and 8l retrieves a url using jsoup.
other idioms capture important transactional properties of code idiom 8h uses properly the memory hungry revwalk object in jgit and 8i is a transaction idiom in neo4j.
other idioms capture common error handling such as 8d for neo4j and 8p for a hibernate transaction.
finally some idioms capture common operations such as closing a connection in netty 8m traversing through the database nodes 8n visiting all ast nodes in a javascript le in rhino 8k and computing the distance between two locations 8g in android.
the reader may observe that these idioms provide a meaningful set of coding patterns for each library capturing semantically consistent actions that a developer is likely to need when using these libraries.
in figure we present a small set of java related idioms mined across all datasets.
these idioms represent frequently used code patterns that would be included by default in tools such as eclipse s snipmatch and intellij s live templates .
de ning constants figure 7c creating loggers figure 7b and iterating through an iterable figure 7a are some of the most common language speci c idioms in java.
all of these idioms have been automatically identi ed by haggis .channel connection.
createchannel a elements name element .
select stringlit b transaction tx connectionfactory.
getdatabase .begintx c catch exception e transaction .failure d searchsourcebuilder builder getquerytranslator .build contentindexquery e locationmanager name locationmanager getsystemservice context.location service f location.distancebetween location .getlatitude location .getlongitude ... g try body finally revwalk .release h try node name methodinvoc body finally transaction .finish i connectionfactory factory new connectionfactory methodinvoc connection connection factory.newconnection j while modelnode !
null if modelnode limit break ifstatement modelnode modelnode .getparentmodelnode k document doc jsoup.connect url .
useragent mozilla .
header accept text html .
get l if connection !
null try connection .close catch exception ignore m traverser traverser node .traverse for node name traverser body n toast.maketext this stringlit toast.length short .show o try session session hibernateutil .currentsession body catch hibernateexception e throw new daoexception e p catch hibernateexception e if transaction !
null transaction .rollback e.printstacktrace q filesystem name filesystem.get path .touri conf r token xcontentparser .nexttoken !
xcontentparser.token.end object s figure top cross project idioms for library projects figure .
here we include idioms that appear in the test set les.
we rank them by the number of distinct les they appear in and restrict into presenting idioms that contain at least one library speci c i.e.api speci c identi er.
the special notation typename denotes the presence of a variable whose name is unde ned.
body denotes a user de ned code block of one or more statements name a freely de ned variable name methodinvoc a single method invocation statement and ifstatement a single ifstatement.
all the idioms have been automatically identi es by haggiswe now quantitatively evaluate the mined idiom sets.
figure shows idiom coverage idiom set precision and the average size of the matched idioms in the test sets of each dataset.
we observe that haggis achieves better precision and coverage in projects .
this is expected since code idioms recur more often in a similar project rather than across disparate projects.
this e ect may be partially attributed to the small number of people working in a project and partially to project speci c idioms.
figure also gives an indication of the trade o s we can achieve for di erent cminandnmin.
.
code cloning vs code idioms previously we discussed that code idioms di er significantly from code clones.
we now show this by using a cutting edge code clone detection tool deckard is a state of the art tree based clone detection tool that uses an intermediate vector representation for detecting similarities.
to extract code idioms from the code clone clusters that deckard computes we retrieve the maximal common subtree of each cluster ignoring patterns that are less that of the original size of the tree.
we run deckard with multiple parameters stride2 f0 2g similarity2f0 0g mintoks2f10 20g on the validation set and picked the parameters that achieve the best combination of precision and coverage.
figure shows precision coverage and average idiom size in number of nodes of the patterns found through deckard andhaggis .
haggis found larger and higher coverage idioms since clones seldom recur across projects.
the di erences in precision and coverage are statistically signi cant paired t test p .
we also note that the overlap in the patterns extracted by deckard andhaggis is small less than .
.
it is important to note these results are not a criticism ofdeckard which isa high quality state of the art code clone detection tool but rather these results show that the task of code clone detection is di erent from code idiom mining code clone detection is concerned with nding pieces of code that are not necessarily frequent but are maximally identical.
in contrast idiom mining is not concerned with nding maximally identical pieces of code but mining common tree fragments that trade o between size and frequency.
.
extrinsic evaluation of mined idioms now we evaluate haggis extrinsically by computing coverage and precision in the test sets of each dataset and the stackover ow question dataset an extrinsic set of highly idiomatic code snippets.
stackover ow is a popular q a site containing programming related questions and answers.
when developers deem that their question or answer needs to be clari ed with code they include a code snippet.
these snippets are representative of general development practice and are usually short concise and idiomatic containing only essential pieces of code.
we rst extract all code fragments in questions and answers tagged as javaorandroid ltering only those that can be parsed by eclipse jdt .
we further remove snippets that contain less than tokens.
after this process we have partial java snippets.
then we create a single set of idioms merging all those found in library and removing any idioms that have been seen in less than ve les at the test portions of library .
we end up with small but high precision set of idioms across all apis inlibrary .
figure shows precision and coverage of haggis s idiomstest corpus coverage precision stackover ow projects figure extrinsic evaluation of mined idioms.
all idioms were mined from library .
comparing stackover ow library andprojects .
using thelibrary idioms we achieve a coverage of and a precision of on stackover ow compared to a much smaller precision and coverage in projects .
this shows that the mined idioms are more frequent in stackover ow than in a random set of projects.
since we expect that stackover ow snippets are more highly idiomatic than average projects source code this provides strong indication that haggis has mined a set of meaningful idioms.
we note that precision depends highly on the popularity of library s libraries.
for example because android is one of the most popular topics in stackover ow when we limit the mined idioms to those found in the two android libraries haggis achieves a precision of .
at a coverage of in stackover ow.
this evaluation provides a strong indication that haggis idioms are widely used in development practice.
.
idioms and code libraries previously we found code idioms across projects and libraries.
as a nal evaluation of the mined code idioms semantic consistency we now show that code idioms are highly correlated with the packages that are imported by a java le.
we merge the idioms across our library projects and visualize the liftamong code idioms and import statements.
lift commonly used in association rule mining measures how dependent the co appearance of two elements is.
for each imported package p we compute the lift score lof the code idiom tasl p t p p t p p p t wherep p is the probability of importing package p p t is the probability of the appearance of code idiom tandp p t is the probability that package pand idiomtappear together.
it can be seen that l p t is higher as package pand idiomtare more correlated i.e.
their appearance is not independent.
figure shows a covariance like matrix of the lift of the top idioms and packages.
here we visualize the top most frequent train set packages and their highest correlating code idioms along with the top most frequent idioms in library .
each row represents a single code idiom and each column a single package.
on the top of figure one can see idioms that do not depend strongly on the package imports.
these are language generic idioms such as the exception handling idiom in figure 7c and do not correlate signi cantly with any package.
we can also observe dark blocks of packages and idioms.
those represent library or project speci c idioms that co appear frequently.
this provides additional evidence that haggis nds meaningful idioms since as expected some idioms are common throughout java while others are api or project speci c. suggesting idioms to further demonstrate the semantic consistency of the haggis idioms we present a preliminary approach to suggesting idioms based on package imports.
we caution that our goal here is to develop an initial proof of concept not the best possible suggestion method.
first we score each idiom tiby computing s tiji max p2il p ti where iis the set of all imported packages.
we then return a ranked list ti ft1 t2 gsuch that for all i j packagesidioms figure lift between package imports and code idioms.
a darker color signi es higher lift i.e.more common co occurrence.
each row shows the spectrum of an idiom.
darker blue color shows higher correlation between a package and an idiom.
one can nd idioms generic language idioms top and others that are package speci c dark blocks on the right .
idioms and packages are only shown for the android.location android.net.wifi and org.hibernate apis for brevity.
.
.
.
.
.
.
suggestion frequency0.
.
.
.
.
.
.
.
.
.0recall at rank k k k k figure the recall at rank kfor code idiom suggestion.
s ti i s tj i .
additionally we use a threshold sthto control the precision of the returned suggestions showing only those idioms tithat haves ti i sth.
thus we are only suggesting idioms where the level of con dence is higher thansth.
it follows that this parameter controls suggestion frequency i.e.the percent of the times where we present at least one code idiom.
to evaluate haggis s idiom suggestions we use the library idioms mined from the train set and compute the recall at rank kon the library s test set.
recall at rank k evaluates haggis s ability to return at least one code idiom for each test le.
figure shows that for suggestion frequency of we can achieve a recall of at rank k meaning that in the top results we return at least one relevant code idiom of the time.
this results shows the quality of the mined idioms suggesting that haggis can provide a set of meaningful suggestions to a developer by solely using the code s imports.
further improvements in suggestion performance can be achieved by using more advanced classi cation methods which we leave to future work which could eventually enable an ide side pane that presents a list of suggested code idioms.
.
related work source code has been shown to be highly repetitive and non unique rendering nlp methods attractive for the analysis of source code.
n gram language models have been used to improve code autocompletion performance learn coding conventions and nd syntax errors .
models of the tree structure of the code have also been studied with the aim of generating programs by example and modeling source code .
however none of this work has tried to extract non sequential patterns in code or mine tree fragments.
the only work that we are aware of that uses language models for detecting textual patterns in code isjacob and tairas that usen grams to autocomplete code templates.
code clones are related to code idiom mining since they aim to nd highly similar code but not necessarily identical pieces of code.
code clone detection using asts has also been studied extensively .
for a survey of clone detection methods see roy et al.
.
in contrast as we noted in section code idiom mining searches for frequent rather than maximally identical subtrees.
it is worth noting that code clones have been found to have a positive e ect on maintenance .
another related area is api mining .
however this area is also signi cantly di erent from code idiom mining because it tries to mine sequences or graphs of api method calls usually ignoring most features of the language.
this di erence should be evident from the sample code idioms in figure .
within the data mining literature there has been a series of work on frequent tree mining algorithms which focuses on nding subtrees that occur often in a database of trees.
however as described in section these have the di culty that frequent trees are not always interesting trees a di culty which our probabilistic approach addresses in a principled way.
finally as described previously bayesian nonparametric methods are a widely researched area in statistics and machine learning which have also found many applications in nlp .
.
discussion conclusions in this paper we presented haggis a system for automatically mining high quality code idioms.
we found that code idioms appear in multiple settings some are project speci c some are api speci c and some are language speci c. an interesting direction for future work is to study the reasons that code idioms arise in programming languages apis or projects and the e ects idioms have in the software engineering process.
it could be that there are good and bad idioms.
good idioms may arise as an additional abstraction layer over a programming language that helps developers communicate more clearly the intention of their code.
bad idioms may compensate for de ciencies of a programming language or an api.
for example one common java idiom mined by haggis is a sequence of multiple catch statements.
this idiom is indeed due to java s language design that led java language designers to introduce a new multi catch statement in java .
however other idioms such as the ubiquitous for int i i n i cannot be considered alanguage limitation but rather a useful and widely understandable code idiom.
a more formal study of the di erence between these two types of idioms could be of signi cant interest.