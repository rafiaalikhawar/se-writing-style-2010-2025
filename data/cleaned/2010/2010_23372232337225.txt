a systematic study of automated program repair fixing out of bugs for each claire le goues michael dewey v ogt computer science department university of virginia charlottesville va legoues mkd5m cs.virginia.edustephanie forrest computer science department university of new mexico albuquerque nm forrest cs.unm.eduwestley weimer computer science department university of virginia charlottesville va weimer cs.virginia.edu abstract there are more bugs in real world programs than human programmers can realistically address.
this paper evaluates two research questions what fraction of bugs can be repaired automatically?
and how much does it cost to repair a bug automatically?
in previous work we presented genprog which uses genetic programming to repair defects in off the shelf c programs.
to answer these questions we propose novel algorithmic improvements to genprog that allow it to scale to large programs and find repairs more often exploit genprog s inherent parallelism using cloud computing resources to provide grounded humancompetitive cost measurements and generate a large indicative benchmark set to use for systematic evaluations.
we evaluate genprog on defects from open source programs totaling .
million lines of code and involving test cases.
genprog automatically repairs of those defects.
to our knowledge this evaluation is the largest available of its kind and is often two orders of magnitude larger than previous work in terms of code or test suite size or defect count.
public cloud computing prices allow our runs to be reproduced for a successful repair completes in minutes and costs .
on average.
keywords genetic programming automated program repair cloud computing i. i ntroduction program evolution and repair are major components of software maintenance which consumes a daunting fraction of the total cost of software production .
automated techniques to reduce their costs are therefore especially beneficial.
developers for large software projects must confirm triage and localize defects before fixing them and validating the fixes.
although there are a number of tools available to help with triage e.g.
localization e.g.
validation e.g.
and even confirmation e.g.
generating repairs remains a predominantly manual and thus expensive process.
at the same time cloud computing in which virtualized processing power is purchased cheaply and on demand is becoming commonplace .
research in automated program repair has focused on reducing defect repair costs by producing candidate patches for validation and deployment.
recent repair projects include clearview which dynamically enforces invariants topatch overflow and illegal control flow transfer vulnerabilities autofix e which can repair programs annotated with design by contract pre and post conditions and afix which can repair single variable atomicity violations.
in previous work we introduced genprog a general method that uses genetic programming gp to repair a wide range of defect types in legacy software e.g.
infinite loops buffer overruns segfaults integer overflows incorrect output format string attacks without requiring a priori knowledge specialization or specifications.
genprog searches for a repair that retains required functionality by constructing variant programs through computational analogs of biological processes.
the goal of this paper is to evaluate dual research questions what fraction of bugs can genprog repair?
and how much does it cost to repair a bug with genprog?
we combine three important insights to answer these questions.
our key algorithmic insight is to represent candidate repairs as patches rather than as abstract syntax trees.
these changes were critical to genprog s scalability to millions of lines of code an essential component of our evaluation.
we introduce new search operators that dovetail with this representation to reduce the number of ill formed variants and improve performance.
our key performance insight is to use off the shelf cloud computing as a framework for exploiting search space parallelism as well as a source of grounded cost measurements.
our key experimental insight is to search version control histories exhaustively focusing on open source c programs to identify revisions that correspond to human bug fixes as defined by the program s most current test suite.
we combine these insights and present a novel scalable approach to automated program repair based on gp and then evaluate it on real world defects taken from open source projects totaling .
mloc and including test cases.
the main contributions of this paper are genprog a scalable approach to automated program repair based on gp.
new gp representation mutation and crossover operators allow genprog to scale to large programs and take advantage of cloud computing978 .
c ieee icse zurich switzerland parallelism.
we evaluate directly against our previous approach on its own benchmarks and find that the improved algorithm finds repairs more often.
a systematic evaluation of genprog on defects from .
mloc of open source projects equipped with test cases.
we generate a benchmark set by exhaustively searching inclusive version ranges to help address generalizability concerns all reproducible bugs in a time window are considered and all defects considered were important enough for developers to test for and fix manually.
this evaluation includes two orders of magnitude more source code than autofix e or our previous work two orders of magnitude more test cases than clearview and two orders of magnitude more defects than afix in addition to being strictly larger than each of those four previous projects on each of the those metrics separately .
results showing that genprog repairs of those defects.
because our experiments were conducted using cloud computing and virtualization any organization could pay the same rates we did and reproduce our results for or .
per successful run on this dataset.
a successful repair takes minutes of wallclock time on average while an unsuccessful run takes .
hours including cloud instance start up times.
ii.
m otivation this section motivates automated program repair and identifies monetary cost success rate and turnaround time as important evaluation metrics.
the rate at which software bugs are reported has kept pace with the rapid rate of modern software development.
in one mozilla developer noted everyday almost bugs appear far too much for only the mozilla programmers to handle bugzilla.mozilla.org gives similar bug report numbers for .
since there are not enough developer resources to fix all defects programs ship with both known and unknown bugs .
in light of this problem many companies have begun offering bug bounties to outside developers paying for candidate repairs.
well known companies such as mozilla1 and google2offer significant rewards for security fixes with bounties raising to thousands of dollars in bidding wars.
although security bugs command the highest prices more wide ranging bounties are available.
consider tarsnap.com an online backup provider.
over a four month period tarsnap paid for fixes for issues ranging from cosmetic errors e.g.
typos in source code comments to general software engineering mistakes e.g.
data corruption to bug html bug calls raises mozilla sbug bounty forchrome flaws full fitness predicate fullfitness patch b input sampled fitness samplefit patch r input mutation operator mutate patch patch input crossover operator crossover patch2 patch2 input parameter popsize output patch that passes fullfitness letpop map mutate over popsize copies of angbracketleft angbracketright repeat letparents tournselect pop popsize samplefit letoffspr map crossover over parents pairwise pop map mutate over parents offspr until candidate pop.fullfitness candidate return candidate figure .
high level pseudocode for the main loop of our technique.
security vulnerabilities.
of the approximately candidate patches submitted to claim various bounties about addressed spelling mistakes or style concerns while about addressed more serious issues classified as harmless or minor .
one issue was classified as major.
developers at tarsnap confirmed corrections by manually evaluating all submitted patches.
if we treat the nontrivial repairs as true positives and the trivial reports as overhead tarsnap paid an average of for each non trivial repair and received one about every hours.
despite the facts that the bounty pays a small amount even for reports that do not result in a usable patch and that about of all non trivial submissions fixed harmless bugs the final analysis was worth the money?
every penny.
bug bounties suggest that the need for repairs is so pressing that companies are willing to pay for outsourced candidate patches even though repairs must be manually reviewed most are rejected and most accepted repairs are for low priority bugs.
these examples also suggest that relevant success metrics for a repair scheme include the fraction of queries that produce code patches monetary cost and wall clock time cost.
we now present an automated approach to program repair with a use case similar to that of the outsourced bug bounty hunters.
the method is powerful enough to fix over half of the defects it tackles and we evaluate it using these and other metrics.
iii.
a utomated repair method in this section we describe genprog an automated program repair method that searches for repairs to off theshelf programs.
we highlight the important algorithmic and representational changes since our preliminary work that enable scalability to millions of lines of code improve performance and facilitate implementation on a cloud computing service.
a. genetic programming genprog uses genetic programming gp an iterated stochastic search technique to search for program bugs.html4repairs.
the search space of possible repairs is infinitely large and genprog employs five strategies to render the search tractable coarse grained statement level patches to reduce search space size fault localization to focus edit locations existing code to provide the seed of new repairs fitness approximation to reduce required test suite evaluations and parallelism to obtain results faster.
high level pseudocode for genprog s main gp loop is shown in figure it closely resembles previous work .
fitness is measured as a weighted average of the positive i.e.
initially passing encoding required functionality and negative i.e.
initially failing encoding a defect test cases.
the goal is to produce a candidate patch that causes the original program to pass all test cases.
in this paper each individual or variant is represented as a repair patch stored as a sequence of ast edit operations parameterized by node numbers e.g.
replace angbracketright see section iii b .
given a program and a test suite i.e.
positive and negative test cases we localize the fault section iii d and compute context sensitive information to guide the search for repairs section iii e based on program structure and test case coverage.
the functions samplefit andfullfitness evaluate variant fitness section iii c by applying candidate patches to the original program to produce a modified program that is evaluated on test cases.
the operators mutate andcrossover are defined in section iii f and section iii g. both generate new patches to be tested.
the search begins by constructing and evaluating a population of random patches.
line of figure initializes the population by independently mutating copies of the empty patch.
lines correspond to one iteration or generation of the algorithm.
on line tournament selection selects from the incoming population with replacement parent individuals based on fitness.
by analogy with genetic crossover events parents are taken pairwise at random to exchange pieces of their representation two parents produce two offspring section iii g .
each parent and each offspring is mutated once section iii f and the result forms the incoming population for the next iteration.
the gp loop terminates if a variant passes all test cases or when resources are exhausted i.e.
too much time or too many generations elapse .
we refer to one execution of the algorithm described in figure as a trial.
multiple trials are run in parallel each initialized with a distinct random seed.
the rest of this section describes additional algorithmic details with emphasis on the important improvements on our preliminary work including a new patch based representation large scale use of a sampling fitness function at the individual variant level fix localization to augment fault localization and novel mutation and crossover operators to dovetail with the patch representation.b.
patch representation an important genprog enhancement involves the choice of representation.
each variant is a patch represented as sequence of edit operations cf.
.
in the original algorithm each individual was represented by its entire abstract syntax tree ast combined with a weighted execution path which does not scale to large programs in the cloud computing setting.
for example for at least of the defects considered in this paper a population of asts did not fit in the .
gb of main memory allocated to each cloud node.
in our dataset half of all human produced patches were lines or less.
thus two unrelated variants might differ by only 25lines with all other ast nodes in common.
representing individuals as patches avoids storing redundant copies of untouched lines.
this formulation influences the mutation and crossover operators discussed below.
c. fitness evaluation to evaluate the fitness of a large space of candidate patches efficiently we exploit the fact that gp performs well with noisy fitness functions .
the function samplefit applies a candidate patch to the original program and evaluates the result on a random sample of the positive tests as well as all of the negative test cases.
samplefit chooses a different test suite sample each time it is called.
fullfitness evaluates to true if the candidate patch when applied to the original program passes all of the test cases.
for efficiency only variants that maximize samplefit are fully tested on the entire test suite.
the final fitness of a variant is the weighted sum of the number of tests that are passed where negative tests are weighted twice as heavily as the positive tests.
d. fault localization genprog focuses repair efforts on statements that are visited by the negative test cases biased heavily towards those that are not also visited by positive test cases .
for a given program defect set of tests t test evaluation function pass t b and set of statements visited when evaluating a test visited t p stmt we define the fault localization function faultloc stmt rto be faultloc s braceleftbigg0 t t. s negationslash visited t .
t t. s visited t pass t .1otherwise that is a statement never visited by any test case has zero weight a statement visited only on a bug inducing test case has high .
weight and statements covered by both buginducing and normal tests have moderate .
weights this strategy follows previous work .
on the defects considered here the total weight of possible fault locations averages .
other fault localization schemes could potentially be plugged directly into genprog .5e.
fix localization we introduce the term fix localization orfix space to refer to the source of insertion replacement code and explore ways to improve fix localization beyond blind random choice.
as a start we restrict inserted code to that which includes variables that are in scope at the destination so the result compiles and that are visited by at least one test case because we hypothesize that certain common behavior may be correct .
for a given program and defect we define the function fixloc stmt p stmt as follows fixloc d braceleftbigg s t t. s visited t varsused s inscope d bracerightbigg the previous approach chose an ast node randomly from the entire program.
as a result an average of of generated variants did not compile usually due to type checking or scoping issues.
for larger programs with long compilation times this is a significant overhead.
for the defects considered here less than of the variants failed to compile using the fix localization function just defined.
f .
mutation operator earlier work used three types of mutation delete insert andswap .
however we found swap to be up to an order of magnitude less successful than the other two .
we thus remove swap in favor of a new operator replace equivalent to a delete followed by an insert to the same location .
in a single mutation a destination statement d is chosen from the fault localization space randomly by weight .
with equiprobability genprog either deletes d i.e.
replaces it with the empty block inserts another source statementsbefored chosen randomly from fixloc d or replacesdwith another statement s chosen randomly from fixloc d .
as in previous work inserted code is taken exclusively from elsewhere in the same program.
this decision reduces the search space size by leveraging the intuition that programs contain the seeds of their own repairs.
g. crossover operator the crossover operator combines partial solutions helping the search avoid local optima.
our new patch subset crossover operator is a variation of the well known uniform crossover operator tailored for the program repair domain.
it takes as input two parents pandqrepresented as ordered lists of edits section iii b .
the first resp.
second offspring is created by appending ptoq resp.qtop and then removing each element with independent probability one half.
this operator has the advantage of allowing parents that both include edits to similar ranges of the program e.g.
parent pinsertsbafteraand parent qinsertsc aftera to pass any of those edits along to their offspring.
previous uses of a one point crossover operator on the fault localization space did not allow for such recombination e.g.
each offspring could only receive one edit to statement a .table i subject cprograms test suites and historical defects tests were taken from the most recent version available in may defects are defined as test case failures fixed by developers in previous versions.
program loc tests defects description fbc legacy coding gmp precision math gzip data compression libtiff image processing lighttpd web server php web programming python general coding wireshark packet analyzer total iv.
e xperimental setup this section describes how we selected a set of subject programs and defects for our systematic evaluation and it describes the parameter settings used for the experiments.
a. subject programs and defects our goal was to select an unbiased set of programs and defects that can run in our experimental framework and is indicative of real world usage.
we required that subject programs contain sufficient c source code a version control system a test suite of reasonable size and a set of suitable subject defects.
we only used programs that could run without modification under cloud computing virtualization which limited us to programs amenable to such environments.
we required that subject defects be reproducible and important.
we searched systematically through the program s source history looking for revisions that caused the program to pass test cases that it failed in a previous revision.
such a scenario corresponds to a human written repair for the bug corresponding to the failing test case.
this approach succeeds even in projects without explicit bug test links and it ensures that benchmark bugs are important enough to merit a human fix and to affect the program s test suite.
table i summarizes the programs used in our experiments.
we selected these benchmarks by first defining predicates for acceptability and then examining various program repositories to identify first acceptable candidate programs that passed the predicates and second all reproducible bugs within those programs identified by searching backwards from the checkout date late may .
the next subsection formalizes the procedure in more detail.
b. selecting programs for evaluation a candidate subject program is a software project containing at least lines of c code viable test cases and versions in a revision control system.
we consider all viable versions of a program defined as a version that checks out and builds unmodified on bit fedora linux a lowest common denominator os available on the ec2 cloud computing framework .
a program builds if it produces its primary executable regardless of the exit status of make .6we define test cases to be the smallest atomic testing units for which individual pass or fail information is available.
for example if a program has major areas which each contain minor tests and each minor test can pass or fail we say that it has test cases.
we define a viable test case as a test that is reproducible non interactive and deterministic in the cloud environment over at least trials .
testsuite i denotes the set of viable test cases passed by viable version iof a program.
we use all available viable tests even those added after the version under consideration.
we exclude programs with test suites that take longer than one hour to complete in the cloud environment.
we say that a testable bug exists between viable versions iandjof a subject program when testsuite i subsetnoteqltestsuite j and there is no i prime i orj prime j with the testsuite j testsuite i testsuite j prime testsuite i prime and the only source files changed by developers to reach versionjwere .c .h .yor.l the second condition requires a minimal i j .
the set of positive tests i.e.
encoding required behavior is defined as testsuite i testsuite j .
the negative tests i.e.
demonstrating the bug are testsuite j testsuite i .
note that the positive and negative tests are disjoint.
given a viable candidate subject program its most recent test suite and a range of viable revisions we construct a set of testable bugs by considering each viable version i and finding the minimal viable version j if any such that there is a testable bug between iandj.
we considered all viable revisions appearing before our start date in late may as a potential source of testable bugs.
however we capped each subject program at defects to prevent any one program from dominating the results.
given these criteria we canvassed the following sources the top c foundry programs on sourceforge.net the top c programs on google code the largest non kernel fedora source packages programs in other repair papers or known to the authors to have large test suites many otherwise popular projects failed to meet our criteria.
many open source programs have nonexistent or weak test suites opaque testing paradigms non automated gui testing or are difficult to modularize build and reproduce on our architecture e.g.
eclipse firefox ghostscript handbrake openjpeg openoffice .
for several programs we were unable to identify any viable defects according to our definition e.g.
gnucash openssl .
some projects e.g.
bash cvs openssh have inaccessible or unusably small version control histories.
other projects were ruled out by our test suite time bound e.g.
gcc glibc subversion .
some projects have many revisions but few viable versions that compile and run against recent test cases e.g.
valgrind .
earlier versions of certain programs e.g .
gmp require incompatible versions of automake andlibtool .
the set of benchmark programs and defects appears in table i. the authors acknowledge that it is not complete and that other additions are possible.
while it is certainly best effort to our knowledge it also represents the most systematic evaluation of automated program repair to date.
c. experimental parameters we ran genprog trials in parallel for each bug.
we chose popsize and a maximum of generations for consistency with previous work .
each individual was mutated exactly once each generation crossover is performed once on each set of parents and of the population is retained with mutation on each generation known as elitism .
each trial was terminated after generations hours or when another search found a repair whichever came first.
samplefit returns of the test suite for all benchmarks.
we used amazon s ec2 cloud computing infrastructure for the experiments.
each trial was given a high cpu medium c1.medium instance with two cores and .
gb of memory.6simplifying a few details the virtualization can be purchased as spot instances at .
per hour but with a one hour start time lag or as on demand instances at .
per hour.
these august september prices summarize cpu storage and i o charges.
v. e xperimental results this section reports and analyzes the results of running genprog on our benchmark suite of defects.
we address the following questions how many defects can genprog repair and at what cost?
section v a what determines the success rate?
section v b what is the impact of alternative repair strategies?
section v c how do automated and human written repairs compare?
section v d a. how many defects can genprog repair?
table ii reports results for defects in .
mloc from subject programs.
genprog successfully repaired of the defects including at least one defect for each subject program.
the non repairs met time or generation limits before a repair was discovered.
we report costs in terms of monetary cost and wall clock time from the start of the request to the final result recalling that the process terminates as soon as one parallel search finds a repair.
results are reported for cloud computing spot instances and thus include a one hour start lag but lower cpu hour costs.
for example consider the repaired fbc defect where one of the ten parallel searches found a repair after .
ii repair results of the defects were repaired successfully and are reported under the cost per repair columns.
the remaining are reported under the non repair s columns.
hours columns report the wall clock time between the submission of the repair request and the response including cloud computing spot instance delays.
us columns reports the total cost of cloud computing cpu time and i o. the total cost of generating the results in this table was .
defects cost per non repair cost per repair program repaired hours us hours us fbc .
.
.
.
gmp .
.
.
.
gzip .
.
.
.
libtiff .
.
.
.
lighttpd .
.
.
.
php .
.
.
.
python .
.
.
.
wireshark .
.
.
.
total .22h .60h wall clock hours.
this corresponds to .
hours of cloud computing cpu time per instance.
the total cost for the entire bug repair effort for that to repair that defect is thus .
hours .
hour .
see section iv c .
the successful repairs return a result in .
hours each on average.
the unsuccessful repairs required .
hours each on average.
unsuccessful repairs that reach the generation limit as in the first five benchmarks take less than hours.
the total cost for all attempted repairs is or .
per successful run.
these costs could be traded off in various ways.
for example an organization that valued speed over monetary cost could use on demand cloud instances reducing the average time per repair by minutes to minutes but increasing the average cost per successful run from .
to .
.
table ii does not include time to minimize a repair an optional deterministic post processing step.
this step is a small fraction of the overall cost .
we view the successful repair of of defects from programs totaling .
million lines of code as a very strong result for the power of automated program repair.
similarly we view an average per repair monetary cost of .
as a strong efficiency result.
b. what determines the success rate?
this section explores factors that may correlate with genprog s success in repairing a given defect.
we first quantitatively analyze the algorithmic changes we made to genprog s program representation and genetic operators.
we next investigate the relationship between genprog success and defect complexity using several external metrics including developer reported defect severity and the number of files touched by developers in a repair.
we also consider internal metrics such as localization size.
representation and genetic operators we compare our new representation and operators to the previous approach using the benchmarks from first to allowtable iii new algorithm the final column reports the ratio of successful repairs found by our enhanced algorithm to those found by the originally published algorithm on that work s benchmarks higher is better .
program fault loc ratio of repairs found gcd infinite loop .
uniqw utx segfault .
look utx segfault .
look svr infinite loop .
units svr segfault .
deroff utx segfault .
nullhttpd buffer exploit .
indent infinite loop .
flex segfault .
atris buffer exploit .
average .
for a direct comparison and second because the previous approach does not scale to our new benchmarks.
we held population size number of generations mutation rate and fault localization strategy constant changing only the internal representation and genetic operators.
we ran random repair trials per benchmark.
success rate is the number of trials that find a repair as in .
table iii shows results.
the new representation outperformed the old on all benchmarks except atris where success drops slightly and look where both approaches succeed on all trials.
averaged over these benchmarks the new representation allows genprog to find repairs more frequently than the original method.
this result is consistent with our hypothesis that the new representation would enable a more efficient search for solutions.
correlating repair with external metrics one concern is that genprog might succeed only on unimportant or trivial bugs.
we investigated this hypothesis by analyzing the relationship between repair success and external metrics such as human time to repair human repair size and defect severity.
with one exception we were unable to identify significant correlations with these external metrics.
we manually inspected version control logs bug databases and associated history to link defects with bug reports.
although all of our benchmarks are associated with source control and bug tracking databases not all defectassociated revisions could be linked with a readily available bug report .
we identified publicly accessible bug or security vulnerability reports in out of of our cases.
all bug reports linked to a defect in our benchmark set were eventually marked confirmed by developers.
we measure developer time as the difference between when the bug report was marked assigned and when it was closed which we know is a rough approximation.
we extracted developer reported defect severities on a scale.
we assigned phpsecurity bug reports marked private a severity of .
.
ultimately we identified severity information for of the defects.
results on this subset are comparable to those on the full dataset genprog repaired of the 528defects associated with bug reports and of the associated with severity ratings.
we investigated both linear and non linear relationships between repair success and search time and the external metrics.
correlation values are pearson s unless otherwise noted.
we found a significant correlation in only one case.
the number of files touched by a human generated patch is slightly negatively correlated with genprog success r .
p .
the more files the humans changed to address the defect the less likely genprog was to find a repair although we note that the correlation is not very strong .
we were unable to identify a significant relationship between either human time to repair or human patch size indiff lines and genprog s repair success.
we found no significant correlation between bug report severity and genprog s ability to repair.
exploring further we found no significant difference between the mean severity of repaired and unrepaired defects student t test and wilcoxon rank sum test at .
.
these results suggest that the defects that genprog can and those that it cannot repair are unlikely to differ in human provided severity.
we note that no defect associated with a severity report has lower than normal priority in our scheme .
recall that by definition our dataset restricts attention to bugs important enough for developers to fix see section iv a .
correlating repair with internal metrics we define the space of possible program repairs by both the fault section iii d and fix section iii e space.
previous work reported that the time to repair scaled roughly linearly with the size of the weighted path or fault localization size fig.
.
fix space size has not been previously studied.
we find a statistically significant though not very strong relationship between the log of the fault weight and repair success r .
p .
as well as the log of the number of fitness evaluations to repair r .
p .
.
as fault space size increases the probability of repair success decreases and the number of variants evaluated to a repair increases.
this result corroborates our previous findings.
we additionally find a significant negative correlation between the log of the fix space size and the log of the number of fitness evaluations required to find a repair r .
p .
.
one possible explanation for these results is that while bad fault localization can preclude a repair e.g.
the variable xmust be zeroed just before this function call imprecise fix localization may make it difficult but still possible e.g.
there are many ways to set xto0 without using x .
a larger fix space may include more candidate repair options reducing the time to find any one even if it does not appear to correlate with actual success.
c. what is the impact of alternative repair strategies?
in this subsection we evaluate two alternative repair strategies searching for multiple repairs and using annotations.table iv alternate defect repair results .
unique patches counts the number of distinct post minimization patches produced if each of the parallel searches is allowed to run to completion.
the final column reports that more defects can be repaired via our technique if human localization annotations are provided.
defects unique patches repaired program repaired patches per repair w annotat.
fbc .
gmp .
gzip .
libtiff .
lighttpd .
php .
python .
wireshark .
total .
search for multiple repairs diverse solutions to the same problem may provide multiple options to developers or enable consideration of multiple attack surfaces in a security context.
to investigate genprog s utility in generating multiple repairs we allowed each of the ten independent trials per bug to run to completion instead of terminating early when any trial found a repair.
to identify unique patches we convert each repair into a tree structured expression level edit script using the diffx algorithm and minimize the edit script using delta debugging effectively removing unnecessary edits .
we consider a repair unique if the result of using this patch is textually unique.
table iv shows how many different patches were discovered in this use case.
genprog produced unique patches for repairs or an average of .
distinct patches per repaired bug.
the unique patches are typically similar often involving different formulations of guards for inserted blocks or different computations of required values.
because all trials including successful ones must now run to completion the total cost increases from to for all runs.
include human annotations genprog is fully automated.
however we might instead use programmer annotations to guide a repair search similar in spirit to programming by sketching .
in sketching a programmer specifies high level implementation strategies a sketch of general structure as well as details such as likely relevant variables invariants or function calls but leaves low level details to a program synthesizer.
the synthesizer uses these inputs to generate the complete code.
in these experiments we relax our assumption of full automation and assume that humans provide an unordered superset of statements that may be used to construct a patch i.e.
fix localization information and pinpoint critical areas where patch actions mights be applied i.e.
fault localization .
such annotations are easier to provide than a concrete patch but are not automatic.
we are interested in annotations to explore the upper limits of our fully automated method and to explore what a hybrid humanmachine approach might achieve.
we use the actual human9repairs for our defect set as the source of our annotations.
we say that a defect can be repaired with annotations if it can be repaired automatically or it can be repaired with fault and fix information restricted to those lines and changes made by the human developers.
the final column of table iv shows results.
with annotations the statement level repair method can address out of bugs .
annotations also reduce time to first repair by on this dataset data not shown .
this is consistent with the relationship between search space size and repair success section v b3 and suggests that benefits might be gained from improved localization.
these results also illuminate our decision to use only statement level changes.
human developers used at least one extra statement level change e.g.
introducing a new global variable in of the subject defects.
however the unannotated statement level approach can repair of those defects.
for example we observed that humans often introduce new variables to hold intermediate computations or to refactor buggy code while repairing it.
genprog achieves the same effect by reusing existing variable definitions to hold intermediate results.
the statement level technique is less likely to repair such defects addressing only of them vs. overall repair rate .
statistically whether a human repair restricts attention to statement only changes moderately correlates with whether our technique can repair that same bug r .
p .
.
restricting attention to statements reduces the search space by one to two orders of magnitude.
these results suggest that is a good trade off.
however they also suggest that more powerful or finer grained operators might allow genprog to address many other real world defects.
d. comparing automated and human written repairs in this subsection we compare the repairs produced by humans with those produced by genprog for two indicative defects.
we have not inspected all unique repairs manually a user study of patch quality is left as future work.
python date handling in one bug six python tests failed based on whether the date maps to or .
the human patch removed a global dictionary lines of processing using that dictionary and a flag preserving that dictionary during y2kchecking.
the automated repair removes the lines of special processing but leaves untouched the empty dictionary and unused flag.
this retains required functionality but increases run time memory usage by one empty dictionary.
the patch is thus as functionally correct as the human patch but degrades some non functional aspects maintainability and memory footprint neither of which are tested.
this normal priority issue8was open for days and involved developer messages and two different candidate patches submitted for review by human developers.
php global object accessor crash php uses reference counting to determine when dynamic objects should be freed.
phpalso allows user programs to overload internal accessor functions to specify behavior when undefined class fields are accessed.
version .
.
had a bug related to a combination of those features.
at a high level the read property function which handles accessors always calls a deep reference count decrement on one of its arguments potentially freeing both that reference and the memory it points to.
this is the correct behavior unless that argument points to this when this