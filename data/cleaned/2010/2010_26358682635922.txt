a large scale study of programming languages and code quality in github baishakhi ray daryl posnett vladimir filkov premkumar devanbu bairay dpposnett filkov cs.
devanbu cs.
ucdavis.edu department of computer science university of california davis ca usa abstract what is the effect of programming languages on software quality?
this question has been a topic of much debate for a very long time.
in this study we gather a very large data set from github projects million sloc authors .
million commits in languages in an attempt to shed some empirical light on this question.
this reasonably large sample size allows us to use a mixed methods approach combining multiple regression modeling with visualization and text analytics to study the effect of language features such as static v.s.dynamic typing strong v.s.weak typing on software quality.
by triangulating findings from different methods and controlling for confounding effects such as team size project size and project history we report that language design does have a significant but modest effect on software quality.
most notably it does appear that strong typing is modestly better than weak typing and among functional languages static typing is also somewhat better than dynamic typing.
we also find that functional languages are somewhat better than procedural languages.
it is worth noting that these modest effects arising from language design are overwhelmingly dominated by the process factors such as project size team size and commit size.
however we hasten to caution the reader that even these modest effects might quite possibly be due to other intangible process factors e.g.
the preference of certain personality types for functional static and strongly typed languages.
categories and subject descriptors d. .
language constructs and features general terms measurement experimentation languages keywords programming language type system bug fix code quality empirical research regression analysis software domain permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
fse november 16 a s22 hong kong china copyright acm ... .
.
.
introduction a variety of debates ensue during discussions whether a given programming language is the right tool for the job .
while some of these debates may appear to be tinged with an almost religious fervor most people would agree that a programming language can impact not only the coding process but also the properties of the resulting artifact.
advocates of strong static typing argue that type inference will catch software bugs early.
advocates of dynamic typing may argue that rather than spend a lot of time correcting annoying static type errors arising from sound conservative static type checking algorithms in compilers it s better to rely on strong dynamic typing to catch errors as and when they arise.
these debates however have largely been of the armchair variety usually the evidence offered in support of one position or the other tends to be anecdotal.
empirical evidence for the existence of associations between code quality programming language choice language properties and usage domains could help developers make more informed choices.
given the number of other factors that influence software engineering outcomes obtaining such evidence however is a challenging task.
considering software quality for example there are a number of well known influential factors including source code size the number of developers and age maturity .
these factors are known to have a strong influence on software quality and indeed such process factors can effectively predict defect localities .
one approach to teasing out just the effect of language properties even in the face of such daunting confounds is to do a controlled experiment .
some recent works have conducted experiments in controlled settings with tasks of limited scope with students using languages with static or dynamic typing based on experimental treatment setting .
while type of controlled study is el camino real to solid empirical evidence another opportunity has recently arisen thanks to the large number of open source projects collected in software forges such as github.
github contains many projects in multiple languages.
these projects vary a great deal across size age and number of developers.
each project repository provides a historical record from which we extract project data including the contribution history project size authorship and defect repair.
we use this data to determine the effects of language features on defect occurrence using a variety of tools.
our approach is best described as mixed methods or triangulation approach.
a quantitative multiple regression study is further examined using mixed methods text analysis clustering and visualization.
the observations from the mixed methods largely confirm the findings of the quantitative study.permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
fse november hong kong china copyright acm ... .
in summary the main features of our work are as follows.
we leverage a categorization of some important features of programming languages that prior knowledge suggests are important for software quality strong v.s.weak typing dynamic v.s.. static typing memory managed v.s.unmanaged and scripting vs. compiled to study their impact on defect quality.
we use multiple regression to control for a range of different factors size project history number of contributors etc.
and study the impact of the above features on defect occurrence.
the findings are listed under rq1 and rq2 in section .
we use text analysis and clustering methods to group projects into domains of application and also the defects into categories of defects we then use heat maps to study relationships of project types and defect types to programming languages.
the findings from this study rq3 and rq4 in section are consistent with statistical results.
while the use of regression analysis to deal with confounding variables is not without controversy we submit that a couple of factors increase the credibility of our results a fairly large sample size and use of mixed methods to qualitatively explore and largely confirm the findings from the regression model.
.
methodology here we describe the languages and github projects that we collected and the analysis methods we used to answer our research questions.
.
study subjects to understand whether choice of programming languages have any impact on software quality we choose the top programming languages from github.
we disregard css shell script and vim script as they are not considered to be general purpose languages.
we further include typescript a typed superset of javascript .
then for each of the studied languages we retrieve top projects that are primarily written in that language.
table shows top three projects in each language based on their popularity.
in total we analyze projects spanning different languages.
.
data collection to retrieve the top programming languages and their corresponding projects from github we used github archive a database that records all public github activities.
the archive logs eighteen different github events including new commits fork events pull request developers information and issue tracking of all the open source github projects on an hourly basis.
the archive data is uploaded to google bigquery service to provide an interface for interactive data analysis.
identifying top languages.
the top languages in github are measured by first finding the number of open source github projects developed in each language and then choosing the top languages with the maximum number of projects.
however since multiple languages are often used to develop a project assigning a single language to a project is difficult.
github linguist can measure such a language distribution of a github project repository.
since languages can be identified by the extension of a project s source files github linguist counts the number of source files with different extensions.
the language with the maximum number oftable top three projects in each language language projects c linux git php src c node webkit phantomjs mongo c signalr sparkleshare servicestack objective c afnetworking gpuimage restkit go docker lime websocketd java storm elasticsearch actionbarsherlock coffeescript coffee script hubot brunch javascript bootstrap jquery node typescript bitcoin litecoin qbittorrent ruby rails gitlabhq homebrew php laravel codeigniter symfony python flask django reddit perl gitolite showdown rails dev box clojure lighttable leiningen clojurescript erlang chicagoboss cowboy couchdb haskell pandoc yesod git annex scala play20 spark scala source files is assigned as primary language of the project.
github archive stores this information.
we aggregate projects based on their primary language.
then we select the top languages having maximum number of projects for further analysis as shown in table .
retrieving popular projects.
for each selected language we retrieve the project repositories that are primarily written in that language.
we then count the number of stars associated with that repository.
the number of stars relate to how many people are interested in that project .
thus we assume that stars indicate the popularity of a project.
we select the top projects in each language.
to ensure that these projects have a sufficient development history we filter out the projects having less than commits where is the first quartile commit number of all the projects.
this leaves us with projects.
table shows the top three projects in each language.
this includes projects like linux mysql android sdk facebook sdk mongodb python ruby source code etc.
retrieving project evolution history.
for each of these projects we downloaded the non merged commits along with the commit logs author date and author name using the command git log no merges numstat .
the numstat flag shows the number of added and deleted lines per file associated with each commit.
this helps us to compute code churn and the number of files modified per commit.
we also retrieve the languages associated with each commit from the extensions of the modified files.
note that one commit can have multiple language tags.
for each commit we calculate its commit age by subtracting its commit date from the first commit of the corresponding project.
we also calculate some other project related statistics including maximum commit age of a project and the total number of developers we use them as control variables in our regression model as discussed in section .
we further identify the bug fix commits made to individual projects by searching for error related keywords error bug fix issue mistake incorrect fault defect and flaw in the commit log using a heuristic similar to that in mockus and v otta .
table summarizes our data set.
since a project may use multiple languages the second column of the table shows the total number of projects that use a certain language at some capacity.
we further exclude some languages from a project that have fewer than commits in that language where is the first quartile value of the total number of commits per project per language.
for example we find projects that use more than commits in c. this ensures that the studied languages have significant activity within the projects.
in summary we study projects developed in lan 156table study subjects project details total commits bugfix commits language projects authors sloc period commits insertion bug fixes insertion kloc kloc kloc c to c to c to objective c to go to java to coffeescript to javascript to typescript to ruby to php to python to perl to clojure to erlang to haskell to scala to summary to guages with years of parallel evolution history.
this includes thousand different developers .
million commits and bug fix commits.
.
categorizing languages we define language classes based on several properties of the language that have been thought to influence language quality as shown in table .
the programming paradigm indicates whether the project is written in a procedural functional or scripting language.
compile class indicates whether the project is table different types of language classes language classescategories languages programming paradigmprocedural c c c objective c java go scripting coffeescript javascript python perl php ruby functional clojure erlang haskell scala compilation classstatic c c c objective c java go haskell scala dynamic coffeescript javascript python perl php ruby clojure erlang type class strong c java go python ruby clojure erlang haskell scala weak c c objective c coffeescript javascript perl php memory class managed others unmanaged c c objective c statically or dynamically typed.
type class classifies languages based on strong and weak typing based on whether the language admits type confusion .
we consider that a program introduces type confusion when it attempts to interpret a memory region populated by a datum of specific type t1 as an instance of a different type t2 and t1 and t2 are not related by inheritance.
we classify a language as strongly typed if it explicitly detects type confusion and reports it as such.
strong typing could happen by static type inference within a compiler e.g.
withjava using a type inference algorithm such as hendley milner or at run time using a dynamic type checker.
in contrast a languageis weakly typed if type confusion can occur silently undetected and eventually cause errors that are difficult to localize.
for example in a weakly typed language like javascript adding a string to a number is permissible e.g.
while such an operation is not permitted in strongly typed python .
also cand c are considered weakly typed since due to type casting one can interpret a field of a structure that was an integer as a pointer.
finally memory class indicates whether the language requires developers to manage memory.
we treat objective c as unmanaged though objective c follows a hybrid model because we observe many memory errors in objective c codebase as discussed in rq4 in section .
.
identifying project domain we classify the studied projects into different domains based on their features and functionalities using a mix of automated and manual techniques.
the projects in github come with project descriptions andreadme files that describe their features.
first we used latent dirichlet allocation lda a well known topic analysis algorithm on the text describing project features.
given a set of documents lda identifies a set of topics where each topic is represented as probability of generating different words.
for each document lda also estimates the probability of assigning that document to each topic.
table characteristics of domains domain domain example total name characteristics projects proj application end user programs.
bitcoin macvim app database sql and nosql mysql mongodb db databases codeanalyzer compiler parser ruby php src ca interpreter etc.
middleware operating systems linux memcached mw virtual machine etc.
library apis libraries etc.
androidapis lib opencv framework sdks plugins ios sdk coffeekup fw other arduino autoenv oth 157we detect distinct domains i.e.
topics and estimate the probability of each project belonging to these domains.
for example lda assigned the facebook android sdk project to the following topic with high probability .
facebook .
swank slime .
framework .
environments.
.
transforming .
here the text values are the topics and the numbers are their probability of belonging to that domain.
for clarity we only show the top domains.
since such auto detected domains include several project specific keywords such as facebook swank slime as shown in the previous example it is hard to identify the underlying common functionalities.
hence we manually inspect each of the thirty domains to identify project nameindependent domain identifying keywords.
manual inspection helps us in assigning a meaningful name to each domain.
for example for the domain described earlier we identify the keywords framework environments and transforming to call it development framework .
we manually rename all the thirty auto detected domains in similar manner and find that the majority of the projects fall under six domains application database codeanalyzer middleware library and framework.
we also find that some projects like online books and tutorials scripts to setup environment hardware programs etc.
do not fall under any of the above domains and so we assign them to a catchall domain labeled as other .
this classification of projects into domains was subsequently checked and confirmed by another member of our research group.
table summarizes the identified domains resulting from this process.
in our study set the framework domain has the greatest number of projects while the database domain has the fewest number of projects .
.
categorizing bugs while fixing software bugs developers often leave important information in the commit logs about the nature of the bugs e.g.
why the bugs arise how to fix the bugs.
we exploit such information to categorize the bugs similar to tan et al.
.
first we categorize the bugs based on their cause andimpact .
root causes are further classified into disjoint sub categories of errors algorithmic concurrency memory generic programming and unknown.
the bug impact is also classified into four disjoint sub categories security performance failure and other unknown categories.
thus each bug fix commit has a cause and a impact type.
for example a linux bug corresponding to the bug fix message return if prcm base is null.... this solves the following crash 1was caused due to a missing check programming error and impact was crash failure .
table shows the description of each bug category.
this classification is performed in two phases keyword search.
we randomly choose of the bug fix messages and use a keyword based search technique to automatically categorize the messages with potential bug types.
we use this annotation separately for both cause and impact types.
we chose a restrictive set of keywords and phrases as shown in table .
for example if a bug fix log contains any of the keywords deadlock race condition or synchronization error we infer it is related to the concurrency error category.
such a restrictive set of keywords and phrases help to reduce false positives.
supervised classification.
we use the annotated bug fix logs from the previous step as training data for supervised learning techniques to classify the remainder of the bug fix messages by treating them as test data.
we first convert each bug fix message to a bag ofwords.
we then remove words that appear only once among all of the bug fix messages.
this reduces project specific keywords.
we stem the bag of words using standard natural language processing nlp techniques.
finally we use a well known supervised classifier support vector machine svm to classify the test data.
to evaluate the accuracy of the bug classifier we manually annotated randomly chosen bug fixes equally distributed across all of the categories.
we then compare the result of the automatic classifier with the manually annotated data set.
the following table summarizes the result for each bug category.
precision recall performance .
.
security .
.
failure .
.
memory .
.
programming .
.
concurrency .
.
algorithm .
.
average .
.
the result of our bug classification is shown in table .
in the cause category we find most of the bugs are related to generic programming errors .
.
such high proportion is not surprising because it involves a wide variety of programming errors including incorrect error handling type errors typos compilation errors incorrect control flow and data initialization errors.
the rest .
appears to be incorrect memory handling .
is concurrency bugs and .
is algorithmic errors.
analyzing the impact of the bugs we find .
are related to security vulnerability .
is performance errors and .
causes complete failure to the system.
our technique could not classify .
of the bug fix messages in any cause or impact category we classify these with the unknown type.
.
statistical methods we use regression modeling to describe the relationship of a set of predictors against a response.
in this paper we model the number of defective commits against other factors related to software projects.
all regression models use negative binomial regression ornbr to model the non negative counts of project attributes such as the number of commits.
nbr is a type of generalized linear model used to model non negative integer responses.
it is appropriate here as nbr is able to handle over dispersion e.g.
cases where the response variance is greater than the mean .
in our models we control for several language per project dependent factors that are likely to influence the outcome.
consequently each language project pair is a row in our regression and is viewed as a sample from the population of open source projects.
we log transform dependent count variables as it stabilizes the variance and usually improves the model fit .
we verify this by comparing transformed with non transformed data using the aic and vuong s test for non nested models .
to check that excessive multi collinearity is not an issue we compute the variance inflation factor vif of each dependent variable in all of the models.
although there is no particular value of vif that is always considered excessive we use the commonly used conservative value of .
we check for and remove high leverage points through visual examination of the residuals vs leverage plot for each model looking for both separation and large values of cook s distance.
we employ effects orcontrast coding in our study to facilitate interpretation of the language coefficients .
effects codes differ from the more commonly used dummy or treatment codes that compare a base level of the factor with one or more treatments .
with effects coding each coefficient indicates the relative effect158table categories of bugs and their distribution in the whole dataset bug type bug description search keywords phrases count count causealgorithm algo algorithmic or logical errors algorithm .
concurrancy conc multi threading or multi processing related issuesdeadlock race condition synchronization error.
.
memory mem incorrect memory handling memory leak null pointer buffer overflow heap overflow null pointer dangling pointer double free segmentation fault.
.
programming prog generic programming errors exception handling error handling type error typo compilation error copy paste error refactoring missing switch case faulty initialization default value.
.
impactsecurity sec correctly runs but can be exploited by attackersbuffer overflow security password oauth ssl .
performance perf correctly runs with delayed responseoptimization problem performance .
failure fail crash or hang reboot crash hang restart .
unknown unkn not part of the above seven categories5792 .
of the use of a particular language on the response as compared to the weighted mean of the dependent variable across all projects.
since our factors are unbalanced i.e.
we have different numbers of projects in each language we use weighted effects coding which takes into account the scarcity of a language.
this method has been used previously in software engineering to study the impact of pattern roles on change proneness .
as with treatment coding it is still necessary to omit a factor to compute the model.
since the use of effects codes compares each level to the grand mean however we can easily extract the missing coefficient from the other model coefficients or more practically we simply re compute the coefficients using a different level of the factor as a base .
to test for the relationship between two factor variables we use a chi square test of independence .
after confirming a dependence we use cramer s v an r cequivalent of the phi coefficient for nominal data to establish an effect size .
.
results prior to analyzing language properties in more detail we begin with a straightforward question that directly addresses the core of what some fervently believe must be true namely rq1.
are some languages more defect prone than others?
we evaluate this question using an nbr model with languages encoded with weighted effects codes as predictors for the number of defect fixing commits.
the model details are shown in table .
we include some variables as controls for factors that will clearly influence the number of defect fixes.
project age is included as older projects will generally have a greater number of defect fixes.
trivially the number of commits to a project will also impact the response.
additionally the number of developers who touch a project and the raw size of the project are both expected to grow with project activity.
the sign and magnitude of the estimate in the above model relates the predictors to the outcome.
the first four variables are control variables and we are not interested in their impact on the outcome other than to say in this case that they are all positive as expected and significant.
the language variables are indicator or factor variables for each project.
the coefficient compares each language to the grand weighted mean of all languages in all projects.
the language coefficients can be broadly grouped into three general categories.
the first category are those for which the coefficient is statistically insignificant and the modeling procedure could not distinguish the coefficient from zero.
these languages may behave similarly to the average or they may have wide vari ance.
the remaining coefficients are significant and either positive or negative.
for those with positive coefficients we can expect that the language is associated with ceteris paribus a greater number of defect fixes.
these languages include c c javascript objective c php andpython .
the languages clojure haskell ruby scala andtypescript all have negative coefficients implying that these languages are less likely than the average to result in defect fixing commits.
df deviance resid.
resid.
pr chi df dev null .
log commits .
.
.
log age .
.
.
log size .
.
.
log devs .
.
.
language .
.
.
one should take care not to overestimate the impact of language on defects.
while these relationships are statistically significant the effects are quite small.
in the analysis of deviance table above we see that activity in a project accounts for the majority of explained deviance.
note that all variables are significant that is all table some languages induce fewer defects than other languages.
response is the number of defective commits.languages are coded with weighted effects coding so each language is compared to the grand mean.
aic bic log likelihood deviance num.
obs.
defective commits model coef.
std.
err.
intercept .
.
log commits .
.
log age .
.
log size .
.
log devs .
.
c .
.
c .
.
c .
.
objective c .
.
go .
.
java .
.
coffeescript .
.
javascript .
.
typescript .
.
ruby .
.
php .
.
python .
.
perl .
.
clojure .
.
erlang .
.
haskell .
.
scala .
.
p .
p .
p .05159of the factors above account for some of the variance in the number of defective commits.
the next closest predictor which accounts for less than one percent of the total deviance is language.
all other controls taken together do not account for as much deviance as language.
although we expressed the impact of language in this model as a percentage of deviance we hasten to remind the reader that although interpretation is similar in a rough sense to a percentage of the total variance explained in an ordinary least squares regression it is not accurate to say that the measures are synonymous .
about the best we can do is to observe that it is a small affect.
we can read the coefficients as the expected change in the log of the response for a one unit change in the predictor with all other predictors held constant i.e.
for a coefficient i a one unit change in iyields an expected change in the response of e i. for the factor variables this expected change is normally interpreted as a comparison to a base factor.
in our case however the use of weighted effects coding allows us to compare this expected change to the grand mean i.e.
the average across all languages.
thus if for some number of commits a particular project developed in an average language had four defective commits then the choice to usec would mean that we should expect one additional buggy commit since e0.
.
.
for the same project choosinghaskell would mean that we should expect about one fewer defective commit as e .
.
.
the accuracy of this prediction is dependent on all other factors remaining the same a challenging proposition for all but the most trivial of projects.
all observational studies face similar limitations and we address this concern in more detail in section .
result some languages have a greater association with defects than other languages although the effect is small.
in the remainder of this paper we expand on this basic result by considering how different categories of application defect and language lead to further insight into the relationship between languages and defect proneness.
software bugs usually fall under two broad categories domain specific bug specific to project functionalities and do not depend on the underlying programming language.
for example we find a bug fix in linux with log message fix headset mic support for asus x101ch .
the bug was due to a missing functionality in asus headset2and less to do with language feature.
previous researches term these errors as software component bug .
generic bug more generic in nature and have less to do with project functionalities.
for example type error concurrency error etc.
consequently it is reasonable to think that the interaction of application domain and language might impact the number of defects within a project.
moreover some languages are believed to excel at some tasks more so than others e.g.
cfor low level work or java for user applications.
a less than ideal language for an application might lead to a greater number of defects.
to compare error proneness of different languages we ideally should ignore the domain specific bugs and focus only on the generic ones since they are more likely to depend on the programming language feature.
however since a domain specific bug may also arise due to a generic programming error it is difficult to separate the two.
a possible workaround is to study the defect proneness of languages while controlling the domain.
to this end we would like to confirm spe2 linux bug 1169138cific interactions between language and domain with respect to defects within projects.
statistically this goal poses challenges for regression.
interaction terms in a regression model might yield some insight however with 17languages across 7domains this will yield an excessive number of terms and a challenging and most likely spurious interpretation.
given this we first consider testing for the dependence between domain and language usage within a project using a chi square test of independence.
unfortunately out of cells in our data set i.e.
are below the value of .
this exceeds the recommendation that no more than of the counts should be below .
we include the value here for completeness3 however the low strength of association of .
as measured by cramer s v suggests that although there is likely some relationship between domain and language in our data set including domain in regression models is unlikely to lead to meaningful models.
one option to address this concern would be to remove languages or combine domains however our data here presents no clear choices.
alternatively we could combine languages this choice leads to a related but slightly different question.
rq2.
which language properties relate to defects?
rather than considering languages individually we aggregate them by language class as described in section .
and analyze the relationship between defects and language class.
broadly each of these properties divides languages along some line that is often discussed in the context of errors drives user debate or have been the subject of prior work.
to arrive at the six factors in the model we combined all of these factors across all of the languages in our study.
ideally we would want to include each of the separate properties in the regression model so that we can assert with some assurance that a particular property is responsible for particular defects.
unfortunately however the properties are highly correlated and models with all properties are not stable.
to avoid this issue we model the impact of the six different language classes which result from combining the language properties on the number of defects while controlling for the same basic covariates that we used in the model inrq1.
table functional languages have a smaller relationship to defects than other language classes where as procedural languages are either greater than average or similar to the average.
language classes are coded with weighted effects coding so each language is compared to the grand mean.
aic deviance num.
obs.
defective commits intercept .
.
log commits .
.
log age .
.
log size .
.
log devs .
.
functional static strong managed .
.
functional dynamic strong managed .
.
proc static strong managed .
.
script dynamic strong managed .
.
script dynamic weak managed .
.
proc static weak unmanaged .
.
p .
p .
p .
as with language we are comparing language classes with the average behavior across all languages.
the model is presented in table .
as with the previous model the first four variables are control variables and we are not interested in their impact on the outcome other than to say in this case that with the exception of size they are all positive and significant.
their it s clear that 3chi squared value of .6with96d.f.
andp .394e 15160script dynamic strong managed class has the smallest magnitude coefficient.
the coefficient is insignificant implying that the z test for the coefficient cannot distinguish the coefficient from zero.
given the magnitude of the standard error however we can reasonably assume that the lack of significance is not related to wide variance or insufficient data rather it is because the behavior of languages in this class is very close to the average behavior across all languages.
we confirm this by recoding the coefficient usingproc static weak unmanaged as the base level and employing treatment or dummy coding that compares each language class with the base level.
in this case script dynamic strong managed is significantly different with p .
.
we note here that while choosing different coding methods affects the coefficients and z scores the models are identical in all other respects.
when we change the coding we are rescaling the coefficients to reflect the comparison that we wish to make .
comparing the other language classes to the grand mean proc static weak unmanaged languages are more likely to induce defects.
this implies that either weak typing or memory management issues contribute to greater defect proneness as compared with other procedural languages.
among scripting languages we observe a similar relationship between weak and strong typing.
this is some evidence that weak vs strong typing is more likely responsible for this difference as opposed to memory management we cannot state this conclusively given the correlation between factors.
it is possible that among script languages strong versus weak typing is driving the relationship and among procedural languages memory management is driving the difference.
however as a group strongly typed languages are less error prone than average while the weakly typed languages are more error prone than the average.
the contrast between static and dynamic typing is also visible in functional languages.
the functional languages as a group show a strong difference from the average.
compared to all other language types both functional dynamic strong managed andfunctional static strong managed languages show a smaller relationship with defects.
statically typed languages have substantially smaller coefficient yet both functional language classes have the same standard error.
this is strong evidence that functional static languages are less error prone than functional dynamic languages however the z tests only test whether the coefficients are different from zero.
in order to strengthen this assertion we recode the model as above using treatment coding and observe that the functional static strong managed language class is significantly less defect prone than the functional dynamic strong managed language class with p .
.
df deviance resid.
df resid.
dev pr chi null .
log commits .
.
.
log age .
.
.
log size .
.
.
log devs .
.
.
lang.
class .
.
.
as with the relationship between language and defects the relationship between language class and defects is based on a small effect.
the deviance explained is shown in the anova table above and is similar although smaller to the deviance related to language and consequently has a similar interpretation.
having discussed the relationship between language class and defects we revisit the question of application domain.as before we ask whether domain has an interaction with language class.
does the choice of e.g.
a functional language have an advantage for a particular domain?
for this pair of factors the contingency table confirms to assumptions.
as above a chi square test for therelationship between these factors and the project domain yields a value of .
anddf withp .622e 09allowing us to reject the null hypothesis that the factors are independent.
cramer s v yields a value of .
a weak level of association.
consequently although there is some relation between domain and language there is only a weak relationship between domain and language class.
result there is a small but significant relationship between language class and defects.
functional languages have a smaller relationship to defects than either procedural or scripting languages.
it is somewhat unsatisfying that we do not observe a strong association between language or language class and domain within a project.
an alternative way to view this same data is to aggregate defects over all languages and domains disregarding the relationship to projects.
since we cannot view this data as independent samples we do not attempt to analyze it statistically rather we take a descriptive visualization based approach.
we define defect proneness as the ratio of bug fix commits over total commits per language per domain.
figure illustrates the interaction between domain and language using a heat map where the defect proneness increases from lighter to darker zone.
we investigate which language factors influence defect fixing commits across a collection of projects written across a variety of languages.
this leads to the following research question rq3.
does language defect proneness depend on domain?
a first glance at figure a reveals that defect proneness of the languages indeed depends on the domain.
for example in the middleware domain javascript is most defect prone .
defect proneness .
this was little surprising to us since javascript is typically not used for middleware domain.
on a closer look we find that javascript has only one project v8 google s javascript virtual machine in middleware domain that is responsible for all the errors.
also scala is most defect prone in the application domain with a defect density number of bug fix commit over total commit of .
.
however a single project zipkin with defect density of contribute most of defects.
similarly perl andc are the two top most buggy languages in database domain with defect proneness .
and .
respectively.
however we find thatmysql project is solely responsible for such high defect count where overall defect density of mysql is .
.
thus it turns out that variation of defect density of the domains with languages may be an attribute of individual projects.
to verify this we re evaluate domain language interaction after ignoring the outliers.
we filter out the projects that have defect density below percentile and above percentile.
figure b shows the result.
note that the outliers effects are also controlled in all our regression models as we filter them out as high leverage points as discussed in section .
.
the variation of defect proneness and languages per domain is much subdued in the new heat map.
the remaining variation comes from the inherent defect proneness of the languages as we have seen in rq1.
to ensure this we measure the pairwise rank correlation spearman correlation between the language defect proneness for each domain with the overall.
for all the domains the correlation is positive and p values are significant .
except for database domain.
this shows that w.r.t.
defect proneness language ordering in each domain except database is correlated161scalahaskellerlangclojureperlpythonphprubytypescriptjavascriptcoffeescriptjavagoobjective cc c c application codeanalyzer database framework library middleware overall domainlanguage204060bug pcent a variation of defect proneness across languages for a given domainscalahaskellerlangclojureperlpythonphprubytypescriptjavascriptcoffeescriptjavagoobjective cc c c application codeanalyzer database framework library middleware overall domainlanguage10203040bug pcent b variation of defect proneness across languages for a given domain after removing the outliers figure interaction between language s defect proneness and domain each cell in the heat map represents defect proneness of a language row header for a given domain column header .
the overall column represents defect proneness of a language over all the domains.
the cells with white cross mark indicate null value i.e.no commits were made corresponding to that cell.
with the overall language ordering.
hence domain has less significance when it comes to defect proneness of the languages.
app ca db fw lib mw spearman .
.
.
.
.
.
corr.
p value .
.
.
.
.
.
result there is no general relationship between domain and language defect proneness.
we have shown that different languages induce a larger number of defects and that this relationship is not only related to particular languages but holds for general classes of languages however we find that the type of project doesn t mediate this relationship to a large degree.
we now turn our attention to categorization of the response.
we want to understand how language relates to specific kinds of defects and how this relationship compares to the more general relationship that we observe.
we divide the defects into categories as described in table and ask the following question rq4.
what is the relation between language bug category?
we use a mixed method analysis to understand the relation between languages and bug categories.
first using a descriptive visualization based approach similar to rq3 we study the relation between bug categories and language class.
a heat map see figure shows aggregated defects over language classes and bug types and illustrate an overall relationship between language class and the bug categories.
to understand the interaction between bug categories with individual languages we use a separate nbr model as discussed in rq1 for each bug category.
for each of these models we use the same control factors as rq1 as well as languages encoded with weighted effects as predictors with the number of defect fixing commits belong to that category as a response.
the results are shown in table .
for these models we present only a subset of the anova coefficients as the distribution of deviance explained is very similar to the language model presented in table and its associated anova table.
most importantly the number of commits still commands the largest proportion of the deviance and the deviance explained by the remaining control variables is substantially smaller than that explained by the language.
the overall deviance for each model is substantially smaller however and the proportion explained by language for a specificdefect type is similar in magnitude for most of the categories.
we interpret this relationship to mean that language has a greater impact on specific categories of bugs than it does on bugs overall.
in the following section we elaborate the result for bug categories with significant bug count as reported in table .
however our conclusion generalizes for all categories.
func dynamic strong managedfunc static strong managedproc static strong managedproc static weak unmanagedscript dynamic strong managedscript dynamic weak managed algo concurrency failure memory performance programming security bug typelanguage class figure relation between bug categories and language class each cell represents percentage of bug fix commit out of all bug fix commits per language class row header per bug category column header .
the values are normalized column wise.
programming errors.
generic programming errors account for around .
of all bug fix commits and occur in all the language classes.
the regression analysis draws a similar conclusion as of rq1 see table since programming errors represent the majority of the studied fixes.
all languages incur programming errors such as faulty error handling faulty object and variable definitions incorrect data initialization typos etc.. still some programming errors are more language specific.
for example we find runtime errors injavascript that are not present in typescript .
in contrast typescript has more type related errors since typescript compiler flags them during development.
thus although generic programming errors occur in all languages some kinds relate to specific language features.
memory errors.
memory errors count for .
of all the bug fix commits.
the heat map in figure shows a strong relationship between proc static weak unmanaged class and memory errors.
this is obvious as languages with unmanaged memory type are known for memory bugs.
regression analysis in table also confirms that languages with unmanaged memory type e.g.
c c andobjective c introduce more memory162table while the impact of language on defects varies across defect category language has a greater impact on specific categories than it does on defects in general.
for all models above the deviance explained by language type has p .031e .
memory concurrency security failure intercept .
.
.
.
.
.
.
.
log commits .
.
.
.
.
.
.
.
log age .
.
.
.
.
.
.
.
log size .
.
.
.
.
.
.
.
log devs .
.
.
.
.
.
.
.
c .
.
.
.
.
.
.
.
c .
.
.
.
.
.
.
.
c .
.
.
.
.
.
.
.
objective c .
.
.
.
.
.
.
.
go .
.
.
.
.
.
.
.
java .
.
.
.
.
.
.
.
coffeescript .
.
.
.
.
.
.
.
javascript .
.
.
.
.
.
.
.
typescript .
.
.
.
.
.
.
.
ruby .
.
.
.
.
.
.
.
php .
.
.
.
.
.
.
.
python .
.
.
.
.
.
.
.
perl .
.
.
.
.
.
.
.
scala .
.
.
.
.
.
.
.
clojure .
.
.
.
.
.
.
.
erlang .
.
.
.
.
.
.
.
haskell .
.
.
.
.
.
.
.
aic .
.
.
.
bic .
.
.
.
log likelihood .
.
.
.
deviance .
.
.
.
num.
obs.
residual deviance null .
.
.
.
language type deviance .
.
.
.
p .
p .
p .
errors than the average with statistical significance.
among the managed languages java has significantly more memory errors than the average though its regression coefficient is less than the unmanaged languages.
though java has its own garbage collector having memory leak is not surprising since unused object