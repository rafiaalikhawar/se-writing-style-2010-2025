portfolio finding relevant functions and their usages collin mcmillan college of william mary williamsburg va cmc cs.wm.edumark grechanik accenture technology lab chicago il mark.grechanik accenture.comdenys poshyvanyk college of william mary williamsburg va denys cs.wm.eduqing xie chen fu accenture technology lab chicago il qing.xie chen.fu accenture.com abstract different studies show that programmers are more intereste d in finding definitions of functions and their uses than variable s statements or arbitrary code fragments .
therefore programmers require support in finding relevant functions and d etermining how those functions are used.
unfortunately existi ng code search engines do not provide enough of this support to devel opers thus reducing the effectiveness of code reuse.
we provide this support to programmers in a code search system called portfolio that retrieves and visualizes relevant functions and their usages.
we have built portfolio using a combinatio n of models that address surfing behavior of programmer and shari ng related concepts among functions.
we conducted an experime nt with professional programmers to compare portfolio to go ogle code search and koders using a standard methodology.
the results show with strong statistical significance that users fi nd more relevant functions with higher precision with portfolio th an with google code search and koders.
categories and subject descriptors d. .
productivity d. .m reusable software general terms algorithms experimentation keywords code search portfolio pagerank function call graph ran king.
.
introduction different studies show that programmers are more intereste d in finding definitions of functions and their uses than variable s statements or arbitrary fragments of source code .
more spec ifically programmers use different tools including code sea rch engines to answer three types of questions .
first pro grammers want to find initial focus points such as relevant functi ons that permission to make digital or hard copies of all or part of thi s work for personal or classroom use is granted without fee provided th at copies are not made or distributed for profit or commercial advantage an d that copies bear this notice and the full citation on the first page.
to cop y otherwise to republish to post on servers or to redistribute to lists re quires prior specific permission and or a fee.
icse may honolulu hawaii usa copyright acm ... .
.implement high level requirements.
second programmers m ust understand how a function is used in order to use it themselve s. third programmers must see the chain of function invocatio ns in order to understand how concepts are implemented in these fu nctions.
it is important that source code search engines suppo rt programmers in finding answers to these questions.
in general understanding code and determining how to use it is a manual and laborious process that takes anywhere from t o of programmers time .
short code fragments that ar e returned as results to user queries do not give enough backgr ound or context to help programmers determine how to reuse these c ode fragments and programmers typically invest a significant i ntellectual effort i.e.
they need to overcome a high cognitive dis tance to understand how to reuse these code fragments.
on the other hand if code fragments are retrieved as functions it makes it easier for developers to understand how to reuse these funct ions.
a majority of code search engines treat code as plain text whe re all words have unknown semantics.
however applications co ntain functional abstractions that provide a basic level of c ode reuse since programmer define functions once and call them from dif ferent places in source code.
the idea of using functional abstr actions to improve code search was proposed and implemented elsewhe re however these code search engines do not aut omatically analyze how functions are used in the context of ot her functions despite the fact that understanding the chains o f function invocations is a key question that programmers ask.
unfortu nately existing code search engines do little to ensure that they re trieve code fragments in a broader context of relevant functions th at invoke one another to accomplish certain tasks.
our idea is that since programmers frequently ask various qu estions about functions a code search engine should incorpor ate information about these functions that is used to answer the pr ogrammers questions.
browsing retrieved functions that are rel evant to queries means that programmers follow function calls and re view declarations definitions and uses of these functions to co mbine them in a solution to a given task.
that is programmers want t o accomplish the whole task quickly rather than obtain multi ple examples for different components of the task.
for example consider the query mip map dithering texture image graphics which we use as an example query throughout this paper.
programmers don t want to just see examples that implement mip map techniques and others that render texture and others that manipulate graphic images.
a pr ogrammer wants to accomplish the complete task of dithering mip ma p images that accompany a texture.
however among relevant re sults there are functions that implement mipmapping funct ions that manipulate texture and there are multiple functions that d eal with graphic images.
typically programmers investigate these functionsto determine which of them are relevant and determine how to c ompose these functions to achieve the goal that is expressed wi th the query.
that is a programmer wants to see code for the whole ta sk of how to mip map images that accompany a texture in computer graphics.
a search engine can support programmers efficient ly if it incorporates in its ranking how these functions call one a nother and displays that information to the user.
we created a code search system called portfolio that supports programmers in finding relevant functions that implement hi ghlevel requirements reflected in query terms i.e.
finding in itial focus points determining how these functions are used in a wa y that is highly relevant to the query i.e.
building on found focus points and visualizing dependencies of the retrieved fun ctions to show their usages.
portfolio finds highly relevant function s in close to millions loc in projects from freebsd ports1by combining various natural language processing nlp and indexing techniques with pagerank andspreading activation network san algorithms.
with nlp and indexing techniques initial focus p oints are found that match key words from queries with pagerank w e model the surfing behavior of programmers and with san we elevate highly relevant chains of function calls to the top of s earch results.
we have built portfolio and conducted an experimen t with professional c programmers to evaluate portfolio and c ompare it with the well known and successful engines google co de search and koders.
the results show with strong statistical significance that users find more relevant code with higher preci sion with portfolio than those with google code search and koders .
to the best of our knowledge we are not aware of any existing code search engines that have been evaluated against and sho wn to be more accurate than widely used commercial code search e ngines with strong statistical significance and over a large codebase and using a standard information retrieval methodology pages .
portfolio is free and available for public use2.
.
the model the search model of portfolio uses a key abstraction in which the search space is represented as a directed graph with node s as functions and directed edges between nodes that specify usa ges of these functions i.e.
a call graph .
for example if the function gis invoked in the function f then a directed edge exists from the node that represents the function fto the node that represents the functiong.
since the main goal of portfolio is to enable programmers to find relevant functions and their usages we need models th at effectively represent the behavior of programmers when navig ating a large graph of functional dependencies.
these are navigati on and association models that address surfing behavior of program mers and associations of terms in functions in the search graph.
.
navigation model when using text search engines users navigate among pages b y following links contained in those pages.
similarly in por tfolio programmers can navigate between functions by following ed ges in the directed graph of functional dependencies using port folio s visual interface.
to model the navigation behavior of progr ammers we adopt the model of the random surfer that is used in popular search engines such as google.
following functional depend encies helps programmers to understand how to use found functions.
the surfer model is called random because the surfer can jump t o a new url or in case of source code to a new function.
these random jumps are called teleportations and this navigation model is the basis for the popular ranking algorithm pagerank .
figure example of associations between different functi ons.
in the random surfer model the content of functions and quer ies does not matter navigations are guided only by edges in the g raph that specifies functional dependencies.
accordingly page rank reflects only the surfing behavior of users and this rank is base d on the popularity of a function that is determined by how many fu nctions call it.
however the surfing model is query independen t since it ignores terms that are used in search queries.
taking into consideration query terms may improve the precision of code sea rching.
that is if different functions share concepts that are related to query terms and these functions are connected using functio nal dependencies then these functions should be ranked higher.
we need a search model that should automatically make embedded conc epts explicit by using associations between functions that shar e related concepts and then we combine this model with the surfing mode l in portfolio.
.
association model the main idea of an association model is to establish relevan ce among facts whose content does not contain terms that match u ser queries directly.
consider the query mipmap dithering texture image graphics .
among relevant results there are functions that implement mip map techniques and others that render texture and there are multiple functions that manip ulate graphic images.
this situation is schematically shown in fi gure where the function fcontains the term mip map the function g contains the term dithering the function pcontains the terms graphics andimage and the term texture is contained in the function q. functionfcalls the function g which in turn calls the function h which is also called from the function q which is in turn called from the function p. the functions f p andqwill be returned by a search engine that is based on matching query te rms to those that are contained in documents.
meanwhile the fun ctionhmay be highly relevant to the query but it is not retrieved since it has no words that match the search terms.
in addition the functiongcan be called from many other functions since its dithering functionality is generic however its usage is most val uable for programmers in the context of the function that is related to query terms.
a problem is how to ensure that the functions handgend up on the list of highly relevant functions.
to remedy this situation we use an association model that is based on a spreading activation network san .
in sans nodes represent documents while edges specify properties that connect these documents.
the edges direction and weight re flect the meaning and strength of associations among documents.
f or example an article about clean energy and a different artic le about the melting polar ice cap are connected with an edge that is la beled with the common property climate change.
once applied to s an spreading activation computes new weights for nodes i.e.
ranks that reflect implicit associations in the networks of these n odes.
in portfolio we view function call graphs as sans where node s represent functions edges represent functional dependen cies and weights represent a strength of associations which includes the number of shared terms.
after the user enters a query a list o f functions is retrieved and sorted based on the score that refl ects thefigure portfolio architecture.
match between query terms and terms in functions.
once portf olio identifies top matching functions it computes san to propag ate concepts from these functions to others.
the result is that e very function will have a new score that reflects the associations between concepts in these functions and user queries.
.
the combined model the ranking vectors for pagerank bardbl bardblprand spreading activation bardbl bardblsanare computed separately and later are linearly combined in a single ranking vector bardbl bardblc f bardbl bardblpr bardbl bardblsan .
pagerank is query independent and is precomputed automatically f or a function call graph while bardbl bardblsanis computed automatically in response to user queries.
assigning different weights in the l inear combination of these rankings enables fine tuning of portfo lio by specifying how each model contributes to the resulting scor e. .
our approach in this section we describe the architecture of portfolio an d show how to use portfolio.
.
portfolio architecture the architecture for portfolio is shown in figure .
the main elements of the portfolio architecture are the database hol ding software applications i.e.
the projects archive the metada ta builder the function graph builder the san and pagerank algorithms the visualizer and the key word search engine.
applications metadata describes functions that are declared defined and invo ked in the applications and words that are contained in the source c ode of these functions and comments.
portfolio is built on an int ernal extensible database of c c projects that contain c lose to .3mil files with close to .6mil functions that contain indexed words.
portfolio indexes and searches close to 270m il loc in these c c projects that are extracted from freebsd s source code repository called ports3.
it is easy to extend portfolio by adding new projects to the projects archive.
the user inpu t to portfolio is shown in figure with the arrow labeled .
the output is shown with the arrow labeled .
portfolio works as follows.
the input to the system is the set of applications from the projects archive that contain variou s functions .
the function graph builder analyzes the source code of these applications statically and it outputs thefunction call graph fcg that contains functional dependencies.
this operation is imprecise since resolving dynamic dispatch calls an d function pointers statically is an undecidable problem .
si nce this last checked august .is done offline precise program analysis can be accommodate d in this framework to achieve better results in obtaining corre ct functional dependencies.
we conduct the sensitivity analysis o f portfolio and its constituent algorithms in section .
.
.
next t he algorithm pagerank is run on the fcg and it computes the rank vector bardbl bardblpr in which every element is a ranking score for each function in the fcg.
the metadata builder reads in the source code of applications applies nlp techniques such as stemming and identifie r splitting and indexes the source code as text resulting in projects metadata.
when the user enters a query it is passed to the key word search component along with the projects metadata .
the key word search engine searches the metadata using the wo rds in the query as keys and outputs the set of relevant functions whose source code and comments contain words that match the words from the query.
these relevant functions along with the fcg serve as an input to the algorithm san.
the algorithm san computes spreading activation vector of scores bardbl bardblsan for functions that are associated with the relevant functions .
ranking vectors bardbl bardblpr and bardbl bardblsan are combined into the resulting vector bardbl bardbl that contains ranking scores for all relevant functions.
the visualizer takes the list of relevant functions that are sorted in descending order us ing their ranking scores and the metadata in order to present the resulting visual map to the user as it is shown in figure .
.
portfolio visual interface after the user submits a search query the portfolio search e ngine presents functions relevant to the query in a browser wi ndow as it is shown in figure .
the left side contains the ranked li st of retrieved functions and project names while the right side contains a static call graph that contains these and other functions.
edges of this graph indicate the directions of function invocatio ns.
hovering a cursor over a function on the list shows a label over th e corresponding function on the call graph.
font sizes reflect the combined ranking the higher the ranking of the function th e bigger the font size used to show it on the graph.
clicking on the l abel of a function loads its source code in a separate browser wind ow.
.
ranking in this section we discuss our ranking algorithm.
.
components of ranking there are three components that compute different scores in the portfolio ranking mechanism a component that computes a sc ore based on word occurrences wos a component that computes a score based on the random surfer navigation model pagerank described in section .
and a component that computes a sco re based on san connections between these calls based on the ass ociation model described in section .
.
wos ranking is used t o bootstrap san by providing rankings to functions based on qu ery terms.
the total ranking score is the weighted sum of the page rank and san ranking scores.
each component produces results from different perspectives i.e.
word matches navigati on associations .
our goal is to produce a unified ranking by putting th ese orthogonal yet complementary rankings together in a singl e score.
.
wos ranking the purpose of wos is to enable portfolio to retrieve functio ns based on matches between words in queries and words in the sou rce code of applications.
this is a bootstraping ranking proced ure that serves as the input to the san algorithm.figure a visual interface of portfolio.
the left side contains a list of ranked retrieved functions f or the motivating example query and the right side contains a call graph that contains these functio ns edges of this graph indicate the directions of function i nvocations.
hovering a cursor over a function on the list shows a label over the correspondi ng function on the call graph.
font sizes reflect the score th e higher the score of the function the bigger the font size used to show it on the graph .
clicking on the label of a function loads its source code in a separate browser window.
the wos component uses the vector space model vsm which is a ranking function typically used by search engines to ran k matching documents according to their relevance to a given search query.
this function is implemented in the lucene java framework wh ich is used in portfolio.
vsm is a standard bag of words retriev al function that ranks a set of documents based on the relative proxi mity of query terms e.g.
without dependencies appearing in ea ch document.
each document is modeled as a vector of terms containe d in that document.
the weights of those terms in each document are calculated using the term frequency inverse document frequency tf idf formula.
using tf idf the weight for a term is calc ulated as t f n knkwhere nis the number of occurrences of the term in the document and knkis the sum of the number of occurences of the term in all documents.
then the similarities among the documents are calculated using the cosine distance between each pair of documents cos d1 d2 bardbld1 bardbl bardbld2 bardblwhere d1andd2are document vectors.
.
pagerank pagerank is widely described in literature so here we give i ts concise mathematical explanation as it is related to portfo lio .
the original formula for pagerank of a function fi denoted r fi is the sum of the pageranks of all functions that invoke fi r fi fj bfir fj fj where bfiis the set of functions that invoke fiand fj is the number of functions that the function fjinvokes.
this formula is applied iteratively starting with r0 fi n where nis the number of functions.
the process is repeated until pag erank converges to some stable values or it is terminated after some number of steps.
functions that are called from many other fu nctions have a significantly higher score than those that are us ed infrequently or not at all.
.
spreading activation spreading activation computes weights for nodes in two step s pulses and termination checks.
initially a set of starting nodes is selected using a number of top ranked functions using the wos ranking.
during pulses new weights for different nodes are transitively computed from the starting nodes using the formula nj if niwi j where the weight of the node njis equal to the sum of all nodes nithat are incident to the node njwith edges whose weights are wi j. this edge weight serves to give a reduced value to nodes further away from the initial nodes.
therefore the weight is a value between and .
the function fis typically called the threshold function that returns nonzero value only if the va lue of the argument is greater than some chosen threshold which ac ts as a termination check preventing flooding of the san.
.
example of san computation consider an example of san computation that is shown in figure .
this example is closely related to the motivating exam ple query mip map dithering texture image graphics .
the first ranking component wos assigned the weights .
and .
to the two functions tiledtexture andimagetexture correspondingly.
we label these functions with circlecopyrt.
all weights are to the right rounded off to the second digit .
their subs cripts indicate the order in which weights are computed from the firs t function weights.
for example the weight is computed for th e functioncreatetexturefromimage by multiplying the wos weight for the function tiledtexture by the san edge weight .
.
several functions e.g.
load initrendered get different weights by following different propagation paths from the i nitial function nodes.
in these cases we use the highest value for e ach node the final value assigned to initrenderer is .
.
.
combined ranking the combined rank is s pr bardbl bardblpr san bardbl bardblsan where is the interpolation weight for each type of the score.
these we ightsfigure example of san weight computation wi j .
.
are determined independently of queries unlike the scores w os and san which are query dependent.
adjusting these weight s enables experimentation with how underlying structural and t extual information in application affects resulting ranking scor es.
experimentation with pagerank involves changing the teleportat ion parameter that we briefly discussed in section .
.
.
experimental design typically search engines are evaluated using manual relev ance judgments by experts .
to determine how e ffective portfolio is we conducted an experiment with par ticipants who are c c programmers.
our goal was to evaluate how well these participants could find code fragments or functio ns that matched given tasks using three different search engines g oogle code search or simply google koders5and portfolio6.
we chose to compare portfolio with google and koders because th ey are popular search engines with the large open source code re positories and these engines are used by tens of thousands of pro grammers every day.
.
methodology we used a cross validation experimental design in a cohort of participants who were randomly divided into three groups.
t he experiment was sectioned in three experiments in which each gr oup was given a different search engine i.e.
google koders o r portfolio to find code fragments or functions for given tasks.
ea ch group used a different task in each experiment.
the same task was performed by different participants on different engines i n each experiment.
before the experiment we gave a one hour tutorial on using these search engines.
in the course of each experiment participants translated t asks into a sequence of keywords that described key concepts they needed to find.
once participants obtained lists of code fragments o r functions that were ranked in descending order they examined th ese functions to determine if they matched the tasks.
each parti cipant accomplished this step individually assigning a confidenc e level to the examined code fragments or functions using a four le vel likert scale.
we asked participants to examine only the top t en code fragments that resulted from their searches since the t ime for each experiment was limited to two hours.
the guidelines for assigning confidence levels are the follo wing.
.
completely irrelevant there is absolutely nothing that the participant can use from this retrieved code fragments not hing in it is related to keywords that the participant chose based on the descriptions of the tasks.
.
mostly irrelevant a retrieved code fragment is only remo tely relevant to a given task it is unclear how to reuse it.
.
mostly relevant a retrieved code fragment is relevant to a given task and participant can understand with some modest effort how to reuse it to solve a given task.
.
highly relevant the participant is highly confident that code fragment can be reused and s he clearly see how to use it.
forty four participants are accenture employees who work on consulting engagements as professional programmers for di fferent client companies.
five participants are graduate stude nts from the university of illinois at chicago who have at least six mo nths of c c experience.
accenture participants have differen t backgrounds experience and belong to different groups of the t otal accenture workforce of approximately employees.
out of participants had programming experience with c c ran ging from six months to two years and participants reported mo re than three years of experience writing programs in c .
ten p articipants reported prior experience with google code searc h and three participants with koders which are used in this exper iment thus introducing a bias toward these code search engines n ine participants reported frequent use of code search engines and said that they never used code search engines.
all participants h ave bachelor degrees and have master degrees in different tec hnical disciplines.
.
precision two main measures for evaluating the effectiveness of retri eval are precision and recall .
the precision i s calculated as pr of retrieved functions that are relevant total of retrieved functions i.e.
the precision of a ranking method is the fraction of the topr ranked documents that are relevant to the query where r in this experiment.
relevant code fragments or functions are c ounted only if they are ranked with the confidence levels 4or3.
the precision metrics reflects the accuracy of the search.
since we l imit the investigation of the retrieved code fragments or functi ons to top ten the recall is not measured in this experiment.
we created the variable precision pas a categorization of the response variable confidence c. we did it for two reasons improve discrimination of subjects in the resulting data and additi onally validate statistical evaluation of results.
precision pimposes a stricter boundary on what is considered reusable code.
for example c onsider a situation where one participant assigns the level tw o to all returned functions and another participant assigns level three to half of these functions and level one to the other half.
even t hough the average of c in both cases the second participant reports much higher precision p .
while the precision that is reported by the first participant is zero.
achieving statistical sign ificance with a stricter discriminative response variable will give assurance that the result is not accidental.
a confidence level c. b precision p. figure statistical summary of the results of the experime nt for cand p.the central box represents the values from the lower to upper quartile to percentile .
the middle line represents t he median.
the thicker vertical line extends from the minimu m to the maximum value.
the filled out box represents the values from the minimum to t he mean and the thinner vertical line extends from the quart er below the mean to the quarter above the mean.
an outside value is defined as a val ue that is smaller than the lower quartile minus .
times the interquartile range or larger than the upper quartile plus .
times the interquart ile range inner fences .
a far out value is defined as a value t hat is smaller than the lower quartile minus three times the interquartile range or larg er than the upper quartile plus three times the interquartil e range outer fences .
.
variables the main independent variable is the search engine portfol io google code search and koders that participants use to find relevant c c code fragments and functions.
the other indepen dent variable is participants c experience.
dependent varia bles are the values of confidence level c and precision p. we report these variables in this section.
the effects of other variables t ask description length prior knowledge are minimized by the des ign of this experiment.
.
hypotheses we introduce the following null and alternative hypotheses to evaluate how close the means are for the cs and ps for control and treatment groups.
unless we specify otherwise participan ts of the treatment group use portfolio and participants of the cont rol group use either google or koders.
we seek to evaluate the followin g hypotheses at a .
level of significance.
h0the primary null hypothesis is that there is no difference in the values of confidence level and precision per task between participants who use portfolio google and koders.
h1an alternative hypothesis to h0is that there is statistically significant difference in the values of confidence and precision between participants who use portfolio google and koders .
once we test the null hypothesis h0 we are interested in the directionality of means of the results of control and treatment groups.
we are interested to compare the effectiveness of po rtfolio versus google code search and koders with respect to the valu es of confidence level c and precision p. h1 c of portfolio versus google the effective null hypothesis is that port c g c while the true null hypothesis is that port c p c. conversely the alternative hypothesis is port c g c.h2 p of portfolio versus google the effective null hypothesis is that port p g p while the true null hypothesis is that port p g p. conversely the alternative hypothesis is port p g p. h3 c of portfolio versus koders the effective null hypothesis is that port c k c while the true null hypothesis is that port c k c. conversely the alternative is port c k c. h4 p of portfolio versus koders the effective null hypothesis is that port p k p while the true null hypothesis is that port p k p. conversely the alternative is port p k p. the rationale behind the alternative hypotheses to h1 h4is that portfolio allows users to quickly understand how queries ar e related to retrieved functions.
these alternative hypotheses are m otivated by our belief that if users see visualization of functional d ependencies in addition to functions whose ranks are computed hi gher using our ranking algorithm they can make better decisions about how closely retrieved functions match given tasks.
.
task design we designed tasks for participants to work on during exper iments in a way that these tasks belong to domains that are easy to understand and they have similar complexity.
the authors o f this paper visited various programming forums and internet grou ps to extract descriptions of tasks from the questions that progr ammers asked.
in addition we interviewed several programmers at a ccenture who explained what tasks they worked on in the past year.
additional criteria for these tasks is that they should repr esent realworld programming tasks and should not be biased towards any of the search engines that are used in this experiment.
these ta sks and the results of the experiment are available for download7.
follow the experiment l ink.h var approach samples min max median stddev 2df pcc p t tcrit h1 cportfolio .
.
.
.
.
.96google .
.
.
h2 pportfolio .
.
.
.
.
.
.97google .
.
.
.
h3 cportfolio .
.
.
.
.
.
.96koders .
.
.
h4 pportfolio .
.
.
.
.
.
.97koders .
.
.
.
table results of t tests of hypotheses h for paired two sample for means for two tail distributio n for dependent variable specified in the column var either corp whose measurements are reported in the following columns.
extremal values median means standard deviation stddev variance 2 degrees of freedom df and the pearson correlation coeffic ient pcc are reported along with the results of the evaluat ion of the hypotheses i.e.
statistical significance p and the tstatistics.
.
tasks the following three tasks are examples from the set of task s we used in our experiment.
implement a module for reading and playing midi files8.
implement a module that adjusts different parameters of a picture including brightness contrast and white balance9.
build a program for managing usb devices.
the program should implement routines such as opening closing writin g and reading from an usb device10.
.
threats to validity in this section we discuss threats to the validity of this ex periment and how we address these threats.
.
.
internal validity internal validity refers to the degree of validity of statem ents about cause effect inferences.
in the context of our experi ment threats to internal validity come from confounding the effe cts of differences among participants tasks and time pressure.
participants.
since evaluating hypotheses is based on the data collected from participants we identify two threats to int ernal validity c proficiency and motivation of participants.
even though we selected participants who have working knowl edge of c as it was documented by human resources we did not conduct an independent assessment of how proficient these pa rticipants are in c .
this threat is mitigated by the fact that out of participants from accenture have worked on successful c ommercial projects as c programmers for more than two years.
the other threat to validity is that not all participants cou ld be motivated sufficiently to evaluate retrieved code fragment s or functions.
we addressed this threat by asking participants to ex plain in a couple of sentences why they chose to assign certain confide nce level to retrieved and we discarded results for all searc h engines that were not properly explained.
time pressure.
each experiment lasted for two hours.
for some participants this was not enough time to explore all retr ieved code fragments for five tasks ten results for each of five task s .
therefore one threat to validity is that some participants could try to accomplish more tasks by shallowly evaluating retrieved code iread midi file.aspx and functions.
to counter this threat we notified p articipants that their results would be discarded if we did not see sufficient reported evidence of why they evaluated retrieve d code fragments and functions with certain confidence levels.
sensitivity of portfolio.
recovering functional dependencies automatically introduces imprecision since it is an undec idable problem to recover precise functional dependencies in the p resence of dynamic dispatch and functional pointers .
since the precision of portfolio depends on the quality of recovered functi onal dependencies we conducted an evaluation of these recovered d ependencies with twelve graduate computer science students at d epaul university.
we randomly selected a representative sample o f different projects in portfolio and we asked these students to m anually inspect source code of these projects to determine the preci sion of fcg computed in portfolio.
the results of this evaluation show that the precision of rec overed functional dependencies is approximately .
while t he precision appears to be somewhat lower than desired it is known that pagerank is resilient to incorrect links.
link farms for ex ample are web spam where people create fake web sites that link to on e another in an attempt to skew the pagerank vector.
it is estim ated that close to of all links on the internet are spam .
however it is shown that the pagerank vector is not affected significantly by these spam links since its sensitivity is contr olled by different factors one of which is teleportation parameter .
to evaluate the effect of incorrect links on pagerank vector we conducted experiments where we randomly modified and of links between functions.
our results show that the metric le ngth of the pagerank vector computed as the square root of the sum of squares of its components changes only by approximately for of perturbed functional dependencies.
a brief explanat ion is that by adding or removing a couple of links to functions that are either well connected or not connected at all their pagera nk score is not strongly affected.
investigating the sensitivity of portfolio as well as improving recovery of functional dependencies is th e subject of future work.
.
.
external validity to make the results of this experiment generalizable we mus t address threats to external validity which refer to the gen eralizability of a casual relationship beyond the circumstances o f our experiment.
the fact that supports the validity of this experi mental design is that the participants are highly representative o f professional c c programmers.
however a threat to external val idity concerns the usage of search tools in the industrial setting s where requirements are updated on a regular basis.
programmers us ec c cs level cs level cs level cs level 4totalexperts google koders portf google koders portf google koders portf google koders portf yes no total table the numbers of the different levels of confidence cfor participants with and without expert c c experience.
these updated requirements to refine their queries and locat e relevant code fragments or functions using multiple iterations of working with search engines.
we addressed this threat only parti ally by allowing programmers to refine their queries multiple times .
in addition participants performed multiple searches usi ng different combinations of keywords and they select certain re trieved code fragments or functions from each of the search results.
we believe that the results produced by asking participants to decide on keywords and then perform a single search and rank code fra gments and functions do not deviate significantly from the sit uation where searches using multiple refined queries are perform ed.
another threat to external validity comes from different si zes of software repositories.
koders.com claims to search more th an billion loc which is also close to the number of loc reported by google code search.
even though we populated portfolio s repository with close to mil loc it still remains a threa t to external validity.
.
results in this section we report the results of the experiment and e valuate the hypotheses.
we use one way anov a t tests for paired two sample for means and 2to evaluate the hypotheses that we stated in section .
.
.
testing the null hypothesis we used anov a to evaluate the null hypothesis h0that the variation in an experiment is no greater than that due to norm al variation of individuals characteristics and error in the ir measurement.
the results of anov a confirm that there are large differ ences between the groups for cwith f .
fcrit with p 108which is strongly statistically significant.
the mean cfor the google code search is .
with the variance .
which is smaller than the mean cfor koders .
with the variance .
and it is smaller than the mean cfor portfolio .
with the variance .
.
also the results of anov a confirm that there are large differences between the groups for pwith f .
fcrit .
with p .
22which is strongly statistically significant.
the mean pfor the google code search is .
with the variance .
which is smaller than the mean pfor koders .
with the variance .
and it is smaller than the mean pfor portfolio .
with the variance .
.
based on these results we reject the null hypothesis and we accept the alternative hypothesis h1.
a statistical summary of the results of the experiment for cand t median quartiles range and extreme values is shown as bo xand whisker plots in figure a and figure b correspond ingly with confidence interval for the mean.
even though the num bers of sample sizes are slightly different since some users missed one experiment we replaced missing values with their avera ges.
even though replacing missing data introduces an error giv en extremely low values of p this error is highly unlikely to affect our results.
.
comparing portfolio with google to test the null hypothesis h1andh2we applied two t tests for two paired sample means in this case candpfor participants who used google code search and portfolio.
the results of this te st for cand for pare shown in table .
the column samples shows different values that indicate that not all participants participated in all experiments three different participants missed tw o different experiments .
based on these results we reject the null hypo theses h1andh2and we accept the alternative hypotheses that states that participants who use portfolio report higher relevance and precision on finding relevant functions than those who use googl e code search .
.
comparing portfolio with koders to test the null hypotheses h3andh4 we applied two t tests for two paired sample means in this case candpfor participants who used portfolio and koders.
the results of this test for cand for pare shown in table .
based on these results we reject the null hypotheses h3andh4that say that participants who use portfolio report higher relevance and precision on finding relev ant functions than those who use koders .
.
experience relationships we construct contingency tables to establish a relationshi p between cfor participants with years and without less than years expert c experience who use different search engin es.
these tables are retrieved from the table that is shown in tab le that shows the numbers of the different levels of confidence cfor participants with and without expert c c experience.
to t est the null hypotheses that the categorical variable cis independent from the categorical variable java experience we apply three 2 tests 2 g 2 k and 2 pfor the search engines google koders and portfolio respectively.
we obtain 2 g .
for p .
2 k .
for p .
and 2 p .
for p .
.
the insignificant values of 2and large values of p .
allow us to accept these null hypotheses suggesting that there is no statistically strong relationship between expert c programming experiences of partici pants and the values of reported cs for the code search engines google code search koders and portfolio .
.
usefulness of visualization thirty three participants reported that the visualization of functional dependencies in portfolio is useful and helped them t o evaluate potential reuse of retrieved functions while respon dents did not find this visualization useful.
out these participant s who found it useful had more than one year of c experience while out of these participants who did not find this visual ization useful only two had more than one year of c experience.
.
related work different code mining techniques and tools have been propos ed to find relevant software components as it is shown in table .codefinder iteratively refines code repositories in order t o improve the precision of returned software components .
unlike portfolio codefinder heavily depends on the descriptions often incomplete of software components to use word matching while po rtfolio uses pagerank and sans to help programmers navigate an d understand usages of retrieved functions.
codebroker system uses source code and comments written by programmers to query code repositories to find relevant arti facts .
unlike portfolio codebroker is dependent upon the de scriptions of documents and meaningful names of program variable s and types and this dependency often leads to lower precisio n of returned projects.
even though it returns code snippets rather than functions mica is similar to portfolio since it uses api calls from java deve lopment kit to guide code search .
however mica uses help do cumentation to refine the results of the search while portfol io automatically retrieves functions from arbitrary code reposit ories and it uses more sophisticated models to help programmers evaluat e the potential of code reuse faster and a with higher precision.
exemplar sniff and mica use documentation for api calls fo r query expansion .
sniff then performs the interse ction of types in these code chunks to retain the most relevant and common part of the code chunks.
sniff also ranks these pruned chunks using the frequency of their occurrence in the indexe d code base.
in contrast to sniff portfolio uses navigation and as sociation models that reflect behavior of programmers and improve the precision of the search engine.
in addition portfolio offe rs a visualization of usages of functions that it retrieves automati cally from existing source code thus avoiding the need for third part y documentation for api calls.
web mining techniques have been applied to graphs derived f rom program artifacts before.
notably inoue et al.
proposed co mponent rank as a method to highlight the most frequently u sed classes by applying a variant of pagerank to a graph composed of java classes and an assortment of relations among them.
qual ity of match qom ranking measures the overall goodness of matc h between two given components which is different from p ortfolio in many respects one of which is to retrieve functions based on surfing behavior of programmers and associations between concepts in these functions.
gridle also applies pagerank to a graph of java classes.
in portfolio we apply pagerank to a graph with nodes as functio ns and edges as call relationships among the functions.
in addi tion we use spreading activation on the call graph to retrieve cha ins of relevant function invocations rather than single fragmen ts of code.
programming task oriented tools like prospector hipikat strathcona and xsnippet assist programmers in writing complicat ed code .
however their utilities are not applicable when searching for relevant functions given a query containing h igh level concepts with no source code.
robillard proposed an algorithm for calculating program el ements of likely interest to a developer .
portfolio is si milar to this algorithm in that it uses relations between function s in the retrieved projects to compute the level of interest rankin g of the project however robillard does not use models that reflect the surfing behavior of programmers and association models that improve the precision of search.
we think there is a potential i n exploring connections between robillard s approach and port folio.
s6is a code search engine that uses a set of user guided program transformations to map high level queries into a subset of r elevant code fragments not necessarily functions.
like portf olio s6 uses query expansion however it requires additional low level details from the user such as data types of test cases.approach granularity search result unit usage method amc u n w t codebroker p u y w q t codefinder f u y w q t codegenie p n w t exemplar a y w q t google code search u n w t gridle u n w t hipikat p y w q t koders u n w t krugle u n w t mapo f n w q t mica u f y w q t parseweb u f n w q t portfolio f p y p s w g prospector f n t t s6 f p u y w q t sniff f u y t w t sourceforge a n w t sourcerer f p u y p w t spars j f y p t spotweb u n w t strathcona f y w t xsnippet f y t w t table comparison of portfolio with other related approaches.
columngranularity specifies how search results are returned by each approach p rojects f unctions or u nstructured text and if the usage of these resulting code units is shown y es or n o .
the columnsearch method specifies the search algorithms or techniques that are used in the code search engine i.e.
p agerank s preading activation simple w ord matching parameter t ype matching or q uery expansion techniques.
finally the last column tells if the search engine shows a list of code fragments as t ext or it uses a g raphical representation of search results to illustrate code usage for progra mmers.
.
conclusion we created an approach called portfolio for finding highly re levant functions and projects from a large archive of c c sou rce code.
in portfolio we combined various natural language processing nlp and indexing techniques with a variation of pagerank andspreading activation network san algorithms to address the need of programmers to reuse retrieved code as functional ab stractions.
we evaluated portfolio with professional c c pr ogrammers and found with strong statistical significance that it p erformed better than google code search and koders in terms of reporti ng higher confidence levels and precisions for retrieved c c code fragments and functions.
in addition participants expres sed strong satisfaction with using portfolio s visualization techni que since it enabled them to assess how retrieved functions are used in co ntexts of other functions.