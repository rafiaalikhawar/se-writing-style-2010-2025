code coverage for suite evaluation by developers rahul gopinath oregon state university corvallis or usa gopinath eecs.orst.educarlos jensen oregon state university corvallis or usa cjensen eecs.orst.edualex groce oregon state university corvallis or usa agroce gmail.com abstract one of the key challenges of developers testing code is determining a test suite s quality its ability to find faults.
the most common approach is to use code coverage as a measure for test suite quality and diminishing returns in coverage or high absolute coverage as a stopping rule.
in testing research suite quality is often evaluated by a suite s ability to kill mutants artificially seeded potential faults .
determining which criteria best predict mutation kills is critical to practical estimation of test suite quality.
previous work has only used small sets of programs and usually compares multiple suites for a single program.
practitioners however seldom compare suites they evaluate one suite.
using suites both manual and automatically generated from a large set of real world open source projects shows that evaluation results differ from those for suite comparison statement not block branch or path coverage predicts mutation kills best.
categories and subject descriptors d. .
testinganddebuggingtesting tools general terms measurement verification keywords test frameworks evaluation of coverage criteria statistical analysis .
introduction the purpose of software testing is to improve the quality of software and the primary route to this goal is the detection of faults.
unfortunately the problem of finding all faults in a program or proving their absence for any meaningfulprogram isessentiallyunsolvable.
testingistherefore permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
icse may june hyderabad india copyright acm ... .
.always a trade off between the cost of further testing and the potential cost of undiscovered faults in a program.
in order to make intelligent decisions about testing develope rs need ways to evaluate their testing efforts in terms of their ability to detect faults.
the ability given a test suite to predict whether it is effective at finding faults is essential to rational testing efforts.
theidealmeasure of fault detection is naturally fault detection.
in retrospect using the set of defects discovered during a software product s lifetime the quality of a test suite could be evaluated by measuring its ability to detect those faults faults never revealed in use might reasonably have little impact on testing decisions .
of course this is not a practical method for making decisions during development and testing.
software engineers therefore rely on methods that predict fault detection capability based only on the suite itself and the current version of the software under test sut .
the most popular method is the use of code coverage criteria .
code coverage describes structural aspects of the executions of an sut performed by a test suite.
for example statement coverage indicates which statements in a program s source code were executed branch coverage indicates which branches were taken and path coverage describes typically in a slightly more complex way to account for loops the paths explored in a program s control flow graph.
in software testing research the gold standard for suite evaluation is generally considered to be actual faults detected but this is again in practice difficult to apply even in a research setting .
the second most informative measure of suite quality is usually held to be mutation testing which measures the ability of a test suite to detect small changes to the source code.
mutation testing subsumes many other code coverage criteria and has been shown to predict actual fault detection better than other criteria in some settings but never shown to be worse than traditional code coverage measures.
unfortunately mutation testing is both difficult to apply and computationally expensive which has led to the search for next best criteria for predicting suite quality by researchers .
this effort is highly relevant to real software developers who almost never apply mutation testing due to its complexity expense and the lack of tool support in many languages.
from the point of view of actual software developers and test engineers rather than researchers however most studies of suite evaluation are not focusing on their actual needs.permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
copyright is held by the author owner s .
publication rights licensed to acm.
icse may june hyderabad india acm first researchers typically consider the effectiveness of criteria for predicting which of multiple suites for a sut will detect the most faults.
for research purposes this is the right focus the primary use of coverage criteria is to compare test generation methods where the fundamental question is typically which of the following approaches will detect the most faults?
in typical development settings however the question is more often should we devote more effort to improving this test suite?
and applying completely different testing methods is not an option.
in fact the suites being evaluated are usually produced manually and the only mitigation for poor effectiveness is adding more tests manually.
testing research often focuses on comparing suites for one sut practitioners more often need to simply evaluate a single suite.
it is not clear that the best criteria for multisuite comparison where suites are expected to be generated by automated methods are the most effective for evaluating a manually generated suite.
second researchers are far more likely to apply novel coverage criteria than practitioners.
many recent papers comparing criteria either introduce a novel criteron or implement a previously proposed but never implemented criteria .
while most even moderately popular languages e.g.
java c c python haskell have multiple tools for measuring statement and branch coverage relatively few non research tools offer any variant of path coverage much less data flow based or more esoteric measures such as predicate complete test coverage measures .
this limitation is particularly important for open source development where expensive commercial coverage tools are unlikely to be applied and developers working together may lack a common development environment.
even criteria as widelyadoptedasthemc dccoveragerequiredinaerospace code development lack open source or free tools.
this paper examines the question of coverage criteria as suite quality predictors from the perspective of the nonresearcher audience developers interested in suite evaluation rather than comparison lightweight widely availabl e tools and well known coverage criteria.
given the constraints under which real software projects operate which widelyavailablecoveragecriteriaprovidethebestestimation of fault detection?
this paper draws from the evaluation of hundreds of open source projects.
while the results are based only on open source java programs hosted on github and using the popular maven build system it is likely that our findings apply at minimum to many other java projects and may well apply to other languages as well.
as a silver standard for evaluating suite quality mutation testing is used as identifying real faults of hundreds of java projects was clearly infeasible.
our findings based on real world conditions show that in contrast to the results of some studies conducted in a research context statement coverage is generally the most effective predictor of suite quality.
this is not an accident of the nature of manually produced test suites the same relationship also holds for test suites generated by the randoop tool which uses feedback directed random testing to generate suites.
while branch coverage or some variant of path coverage may be most useful for many research contexts in the context of typical java open source projects at least a focus on the simple and easily understood measure of statement coverage is probably most useful for predict ing suite quality even if developers are using an automated testing tool.
theprimarycontributionsofthispaperaretwofold first the existence of popular open source repositories makes it possible to investigate the effectiveness of coverage criteria in a more unbiased large scale and systematic way than previous studies.
such repositories provide a large body of very different suts limiting selection effects.
using actual test suites from real projects also ensures that results are relevant to actual testing practices.
the additional availability of automated testing tools mature enough to apply to this set of projects enables us to draw conclusions about both human generated and automatically generated suites and to show that results do not depend on this property of test suites.
this enables our second contribution a practical proposal to developers wishing to evaluate test suites for open source java projects.
finally by showing that the best criteria for research purposes differ from those for practitioners this paper shows that the preferences and abilities of software testing researchers may lead to less than optimal advice to developers whose focus is not on evaluating testing methods but on producing quality software.
.
related work a large body of work considers the relationship between coverage criteria and fault detection.
the most closely related work to ours which considers some of the same questions from a different perspective that of researchers is the recent work of gligoric et al.
.
their work uses the same statistical approach as our paper measuring both andr2 to examine correlations to mutation kill for a set of criteria and both studies consider realistically non adequate suite s. however their workconsidersonlyasetof15javaprograms and c programs selected not randomly but primarily from container classes used in previous studies and the classic siemens sir subjects.
their larger projects jodatime jfreechart sqllite yaffs2 were chosen opportunistically.
our study is on a much larger scale in terms of subjects and uses a more principled selection process.
most importantly however we consider correlation of criteria across all suts to answer the question given a suite for an sut which criteria best predicts mutation kills for that sut?
rather than to determine within each sut which criteria best ranks various suites for that sut.
gligoric et al.
report that branch coverage does the best job overall of predicting the best suite for a given sut but thatacyclicintra proceduralpathcoverageishighlycompetitive and may better address the issue of ties which is important in their research comparison context.
inozemtseva et al.
investigates the relationship of various coverage measures and mutation score for different random subsets of test suites.
they found that when the test suite size is controlled only low to moderate correlation is present between coverage and effectiveness.
this conclusion holds for all kinds of coverage measures used.
the difference in subjects and focus yields substantially different results than ours as we discuss below.
budd et al.
proposed mutation testing as a stronger criteria than other methods for evaluating test suites.
offut et al.
showed that mutation coverage subsumes many othercriteria includingthebasicsixproposedbymyers .
frankl et al.
compared the effectiveness of mutation testing with all uses coverage and found that at highest cov 73project size in locfrequency 1e 1e all selected cyclomatic complexityfrequency .
.
.
all selected figure log project parameters distribution before and after selecti on.
is the mean is the median and is the standard deviation of the selected parameter.
table symbols used mmutation score sstatement coverage sblock coverage bbranch coverage ppath coverage kproject size in loc ttest suite size in loc ccyclomatic complexity erage levels mutation testing was more effective.
andrews et al.
compared the fault detection ratio and the mutation kill ratio of a large number of test suites finding that the ratios were very similar and hence the faults induced by mutation representative of the real faults in programs.
a follow up study using a large number of test suites from a single program space.cfound that the mutation detection ratio and the fault detection ratio are related linearly with similar results for other coverage criteria .
to .
.
linear regression on the mutation kill ratio and fault detection ratio showed a high correlation .
.
li et al.
compared four different criteria mutation edge pair all uses and prime path and showed mutationadequate testing was able to detect the most hand seeded faults while other criteria were similar to each other in the range of detection .
similarly mutation coverage required the fewest test cases to satisfy the adequacy criteria while prime path coverage required the most.
therefore while there are no compellingly large scale studies of many suts selected in a non biased way to support the effectiveness of mutation testing it is at least highly plausi ble as a better standard than other criteria.
frankl and weiss performed a comparison of branch coverage and def use coverage showing that def use is more effective than branch coverage for fault detection and there is stronger correlation to fault detection for def use than branch coverage.gupta et al.
compared the effectiveness and efficiency of block coverage branch coverage and condition coverage with mutation kill of adequate test suites as their evaluation metric.
they found that branch coverage adequacy was more effective killed more mutants than block coverage in all cases and condition coverage was better than branch coverage for methods having composite conditional statements.
the reverse however was true when considering the efficiency average number of test cases required to detect a fault of suites.
kakarla and inozemtseva demonstrated a linear relationship between mutation detection ratio and coverage for individual programs.
inozemtseva s study used machine learning techniques to come up with a regression relation and found that effectiveness is dependent on the number of methods in a test suite with a correlation coefficient in the range .
r .
.
the study also found a moderateto high correlation with kendall s in the range .
.
between effectiveness and block coverage when test suite size was ignored which reduced when test suite size was accounted for.
kakarla found that statement coverage was correlated to mutation coverage in the range of .
r .
and .
.
.
wei et al.
examined branch coverage as a quality measureforsuitesfor14eiffelclasses showingthatforrandomly generated suites branch coverage behavior was consistent across many runs while fault detection varied widely.
early in random testing where branch coverage rises rapidly current branch coverage has high correlation to fault detection but branch coverage eventually saturates while fault detection continues to increase the correlation at this point became very weak.
cai et al.
investigated correlations between coverage criteria under different testing profiles whole test set functional test random test normal test and exceptional test.
they investigated block coverage decision coverage c use and p use criteria.
curiously they found that the relationship between block coverage and mutant kills was not always positive.
they found that block coverage and mutant kills74 .
.
.
.
.
.
.
.
.
.
.
.0original statement s mutation m k k .
.
.
.
.
.
.
.
.
.
.
.0generated statement s mutation m k k figure relation between statement coverage and mutation ki lls.
the circle represents the magnitude of project size.
had a correlation of r2 .
when considering the whole test suite but as low as .
for normal testing and as high as .
for exceptional testing.
the correlation between decision coverage and mutation kills was higher than statement coverage for the whole test suite .
ranging from normal test .
to exceptional test .
.
namin and andrews also showed that fault detection ratio non linearly correlated well with block coverage decision coverage and two different data flow criteria.
their researchsuggestedthattestsuitesizewasasignificantfactor in the model.
in general none of this work considered a large representative set of open source projects and many studies considered the within sut suite comparison problem not the problem of determining if a single suite provides effective testing for an sut as we do.
the variety of reported rankings and correlations of criteria can be highly confusing to even a researcher wishing to compare suites much less a typical open source developer seeking to decide if current testing for a project is effective for fault detection.
many studies do not even include all of branch statement and block coverage the most readily available criteria.
our contribution over related work is a study that uses a large set of open source projects uses both manually and automatically generated tests includes all the criteria of most interest to developers and focuses on the critical question of single suite evaluation correlation.
.
methodology our methodology was driven by two primary concerns we wanted our results to be applicable to the largest set of real world programs possible and based on a diverse set of actual test suites constructed by developers not testing researchers.
oursecondconcernwastostriveforastatistically significant result preferring to keep as many experimental variables constant as possible.
one result of this constraint was to restrict the study to java programs.
java is one ofthe most widely used programming languages and choosing a single language allows us to ensure a consistent definition for coverage criteria and avoid any difficulties due to variance in mutation operators.
as a consequence our results are only directly applicable to projects written in java a large portion of the code written today .
however the results are likely applicable to other programming languages with similar structures.
previous studies do not show majordifferencesbetweencriteriaeffectivenessbetweenjava and c programs despite java s object oriented nature inclusion of exceptions the different kinds of programs that tend to be written in c and java etc.
inferring criteria effectiveness for projects not based on c like languages such as java c c and c e.g.
for functional languages would be less justified.
projects were taken from github one of the largest public repositories of java projects.
as a concession to ease of analysis only projects using the popular maven build system were considered.
github provided an initial set of projects meeting this criterion.
while far from the entire set of java projects hosted by github there is no reason it should be biased in terms of test suites.
note that it is also not the full set of maven projects since github only returns pages of search results.
the process used by github to select projects is not public but we believe it is orthogonal to our concerns and likely based on popularity andrecency.
aftereliminatingprojectsaggregatingmultiple projects which are difficult to properly analyze a set of projects remained.
in order to ensure that our results remain free of systematic bias we conducted our analysis in two phases.
in the first phase test suites present in the projects mostly manually produced though there may have been some automatically generated tests included were used for coverage and mutation analysis.
in the second phase we generated test cases using randoop and performed the same statistical analysis using these suites.
finally we compared the results of our first phase to that of the corresponding75 .
.
.
.
.
.
.
.
.
.
.
.0original branch b mutation m k k .
.
.
.
.
.
.
.
.
.
.
.0generated branch b mutation m k k figure relation between branch coverage and mutation kil ls.
the circle represents the magnitude of project size.
analysis in the second phase.
our aim in conducting this cross validation was to ensure that our results would not be affected by possible bias in manually generated suites.
automatically generating suites allowed us to have a second independent measure.
inbothphases wegatheredcoveragemetricsfromemma statement coverage cobertura statement coverage branchcoverage codecover statementcoverage branch coverage jmockit statement coverage path coverage and pit mutation kills .
pit is a tool aimed at developers rather than researchers actively developed and supported with some penetration in open source testing.
while it is not explicitly mentioned in the jmockit project page the path coverage provided by jmockit is similar to the acyclic intra method path aimp coverage that performed well in the study by gligoric et al.
.
out of our projects only projects had test suites.
these were selected for the first phase of analysis.
the analysis included running each of the coverage tools over the selected projects with some effort expended on fixing trivial errors.
in places where the compilation did not succeed after some effort we discarded the project.
further we specified a maximum timeout of hour per project for a single tool.
in the end using original test cases we had results from cobertura results from emma results from codecover from jmockit and from pit.
as an example of the selection process consider mutation testing.
starting from projects had compilation errors dependency resolution problems language version problems or other fundamental issues preventing a build.
oftheremainingprojects 37timedout requiringmorethan an hour for mutation testing with pit.
an additional projects had test failures that prevented pit from running and pit failed to produce any output for projects possibly due to no coverage or mutants produced even though the build completed successfully.
for the second phase of analysis we used randoop a feedback directedrandomtestingtool oneachoftheprojects and discarded those where it failed to complete successfully or timed out.
this produced test suites for projects.
from these suites following the previous procedure we obtained results from emma results from cobertura results from codecover results from jmockit and results from pit.
from the projects for which we were able to generate randoop suites again had compilation etc.
errors preventing analysis.
of the remaining projects timed out after over an hour with pit and had test failures that prevented pit from running.
pit failed to produce output for of the randoop suites.
the symbols used to indicate various metrics collected are given in table .
the use of sfor block coverage is motivated by the observation that block coverage is a kind of weighted statement coverage given source code and a cfg block coverage can be computed given statement coverage details and vice versa.
since the coverage and mutation process resulted in a rather drastic reduction in sample space we compared the distributions for code size and complexity before and after selection to verify that our procedure did not inordinately skew the sample space in at least these dimensions.
the size distribution histograms for both before and after selection is provided in figure .
another important dimension in which a bias could appear is the complexity of programs perhaps rejection is much more common with more complex or simpler programs which could bias results since coverage metrics are intimately tied to code complexity for very simple programs e.g.
statement and path coverage are quite similar .
wemeasuredmccabecyclomaticcomplexity whichprovides a measurement of program complexity by counting the number of linearly independent execution paths through a program.
the distributions before and after selections are given in figure .
these graphs suggest that selection did not unduly bias the sample in these two key dimensions.
.
.
.
.
.
.
.
.
.
.
.
.0original path p mutation m k k .
.
.
.
.
.
.
.
.
.
.
.0generated path p mutation m k k figure relation between path coverage and mutation kills .
the circle represents the magnitude of project size.
to account for the effects of nondeterminism we ran each coverage measurement times and computed the average.
we also made use of multiple tools as noted above to verify that the coverage reported was accurate e.g.
emma cobertura codecover and jmockit all give statement coverage.
further cobertura and codecover provided branch coverage and jmockit provided path coverage.
thus we could compare most coverages provided by most tools and ensure they had high correlation to other tools.
further we could ensure that the tools were processing all classes and test cases by examining statement coverage results.
this was important because early on we found that jmockit was not including classes that were not covered by any tests in its calculation of coverage1.
further we have also removed a few observations in original suites in randoop suites where the statement coverage reported by other tools was zero and mutation or path coverage was non zero as these indicate some incorrect value from some tool.
our dataset which includes metrics for all projects before elimination of outliers is available for perusal in dataverse .
.
analysis thepurposeofouranalysisistodeterminewhichcoverage criteria that are likely to be used by real world developers best predict mutation kill ratios.
our analysis also considers project and test suite size and cyclomatic complexity to determine if these factors affect the utility of coverage criteria.
the scatter plots for mutation kills and statement coverage for both original test suites and randoop generated test suites are shown in figure with pairs for original test suites and pairs for generated test suites.
similarly the scatter plots for mutation kills and branch coverage for both original test suites and generated suites is given in figure with pairs for original test suites and pairs for generated test suites.
the scatter plots for path coverage in figure contain pairs for original test suites 5and pairs for generated test suites.
finally the scatter plots between blockcoverage and mutation kills are given in figure .
the diameter of the circles in all scatter plots correspond to the magnitudes of the project sizes log k .
the central result of these experiments is generally visible in these plots statement coverage appears to give the best prediction of mutation kills of all criteria developers are likely to use and this holds for both original and generated test suites.
we use regression analysis and significance testing to ascertain the contribution of different factors to test suite effectiveness.
the correlation coefficient r2indicates the effectiveness of a model i.e how much of the variation found in data is explainable by the parameters of the model.
the factors that were found insignificant were eliminated to obtain reduced models.
.
statement coverage and mutation kills in this section we try to find the significant factors that in combination with statement coverage predict mutation kills with a high degree of confidence.
we begin with the saturated model consisting of all variables.
these are mutation score project size in loc test suite size cyclomatic complexity and statement coverage.
weremovedthetestsuitesizetoavoidmulticollinearitywith project size after noticing that it correlated very strongly with the project size.
performing the same analysis with test suite size in place of project size gave the same results as below except that the weak effect for branch coverage become a stronger effect but for other criteria suite size did not matter .
this gives the regression relation m k c s 0 1 log k 2 c 3 s 0was set to zero since we had sufficient coverage data near the zero point and zero statement coverage should indicate zero mutation coverage too.
this is given in table .
.
.
.
.
.
.
.
.
.
.
.
.0original block s mutation m k k .
.
.
.
.
.
.
.
.
.
.
.0generated block s mutation m k k figure relation between block coverage and mutation kill s. the circle represents the magnitude of project size.
table saturated model original statement estimate std.
error t value pr t log k .
.
.
.
log c .
.
.
.
s .
.
.
.
further we noticed that project size itself did not have a significant contribution to the response variable.
once we removed project size our table was updated to table .
table second model estimate std.
error t value pr t log c .
.
.
.
s .
.
.
.
since cyclomatic complexity was also clearly not significant removing it resulted in the equation m s 1 s and the result of this equation is in table .
table original mutation statement r2 .
estimate std.
error t value pr t s .
.
.
.
there is no significant effect of project size or program complexity on mutation coverage in a model based on statement coverage.
.
branch coverage and mutation score in this analysis we follow the same path we took for statement coverage with mutation coverage.
project size had a very weak evidence of having an effect on mutation coverage at p .
when compared to statement coverage the effect for suite size here was stronger at .
.
0 was set to zero since we had sufficient coverage data near thezero point and zero branch coverage again should indicate zero mutation kills.
m b 1 b table original mutation x branch r2 .
estimate std.
error t value pr t b .
.
.
.
.
path coverage and mutation score following the same analysis steps we removed code size and cyclomatic complexity as they were not significant.
0is again set to zero for the same reasons.
m p 1 p table original mutation x path r2 .
estimate std.
error t value pr t p .
.
.
.
.
comparing the criteria after determining that project size suite size and cyclomatic complexity were essentially irrelevant for our purposes we turned to comparing correlation statistics for all criteria for both original and randoop generated tests.
in keeping with the most recent and extensive studies we report both r2and kendall correlations.
r2in our context is the most useful correlation measure since ideally developers would like to predict the actual mutation killing effectiveness of a test suite.
kendall is a rank correlation statistic that is non parametric and therefore should be reliable even if underlying relationships are not linear it aims to answer the question given that the ranking between two coverage criteria for suites for projects is such that c x .
.
.
.
.
.
.
.
.
.
.
.0original statement s path p k k .
.
.
.
.
.
.
.
.
.
.
.0generated statement s path p k k figure relation between statement coverage and path cover age.
the circle represents the magnitude of project size.
c y what is the chance that the ranking of mutation kills is in agreement with this ranking?
the results from computing r2 adjand for each of the coverage metrics with mutation coverage are given in table .
o indicates values for original test suites and r indicates randoop generated suites.
table correlation coefficients mutation p .
r2 o o r2 r r m s0.
.
.
.
m s0.
.
.
.
m b0.
.
.
.
m p0.
.
.
.
the results are clear across both original and generated suites statement coverage has the best correlation for both r2and .
for predicting mutation kills for test suites included with projects branch statement and block coverage all provide a satisfactory method predictions for randoopgenerated suites are more difficult but statement coverage still performs relatively well with sufficient power to be useful in practice.
.
why statement coverage?
some previous research e.g.
as recently as suggests the use of branch coverage as the best method for predicting suite quality.
however this study was conducted on a small set of programs a majority of which were algorithms and data structure implementations and based on comparing suites for the same sut rather than predicting the quality of testing for each sut in isolation.
moreover allpreviousstudiestendtoincludesomewhatartificialsuite s produced by testing researchers rather than focusing on real developer produced test suites for a large variety of projects.
that statement coverage performs so well in fact agrees with the conclusion of gligoric et al.
that for nonfigure unbalanced branching adequate suites criteria that are stronger in terms of subsumption for adequate suites donot necessarily have better ability to predict mutation scores.
the superiority of statement to branch coverage however requires some further examination.
figure shows a simple portion of a cfg that may explain this result.
a test suite that covers either of branches a or b would result in branch coverage of .
however there are more mutations of branch b than branch a and under the assumptions that guide mutation testing more chances for coding errors.
modeling the potential impact of the fact that of the popular coverages only statement coverage takes into account the size of a code block predicts the observed results.
assume there are nlines of code in a sut and let si be the mutability number of mutants of the ithstatement.
for the sut there are then summationtextn i 1 si mutants.
this linear relationship is also suggested by our data which shows a high correlation r2 .
between number of statements andnumberofmutantsproduced.
ifweassumethatamutation is detected every time a test suite covers the statement and we have a constant mutability k then we can see that of79 .
.
.
.
.
.
.
.
.
.
.
.0original branch b path p k k .
.
.
.
.
.
.
.
.
.
.
.0generated branch b path p k k figure relation between branch coverage and path coverag e. the circle represents the magnitude of project size.
then kmutants produced n c kwould be detected by a test suite with cas statement coverage ratio.
under ideal conditions mutation kills and statement coverage share a simple relationship.
this formulation also suggests that branch coverage alone could not be as closely correlated with mutation kills as statement coverage unless a model includes some way to incorporate the difference in mutability of program segments or the assumption that coverage usually results in detection is very far from reality and branch execution is a major factor in actual detection ratios for most mutants.
for the first possibility we draw the readers attention to the fact that there was a weak project size effect and fairly strong suite size effect when we considered branch coverage models.
this also suggests that if we consider basic block coverage which is basically statement coverage without mutability information the correlation should also be lower than the coverage reported by ordinary statement coverage.
this is again borne out by the lower values of r2and for block coverage m b in table .
why though is block coverage sometimes better correlated than branch coverage when both criteria ignore mutability of code segments?
branch coverage can compensate for missing a block which always contains at least one mutable statement by taking a branchthatcontains nomutablecode.
infact missingelse detection is what distinguishes block and branch coverage.
.
statement coverage and path coverage oneresultundercuttingthissimpleexplanationforthesuperiority of statement coverage however is that statement coverage also better predicts pathcoverage than branch coverage.
table shows correlations between path coverage and statement branch and block coverage.
scatter plots of statement coverage and branch coverage against path coverage are provided in figure and figure .
these support the conclusion that statement coverage is better than branch for this purpose also the winner betweenblock and statement coverage is less obvious since block coverage performs better for generated suites .
table correlation coefficients path p .
r2 o o r2 r r p s0.
.
.
.
p s0.
.
.
.
p b0.
.
.
.
we do not have any explanation for this effect at this time since path coverage like branch and block coverage ignores the size of code blocks.
it is possible that executing more statements leads to producing more unusual execution states which results in more covered paths but this is hard to model or investigate.
.
correlation at high coverage levels an additional interesting question to consider is whether the superiority of statement coverage for our purposes is an artifact of the fairly weak test suites for many suts.
while statement coverage is most predictive for projects actual utility could be lower than expected if statement coverage works best for suites where the question is the testing effective for finding faults?
is fairly obviously answered with no.
perhaps the better results for branch coverage in previous studies are due to considering mostly effective test suites.
in order to examine this possibility we computed correlationsforonlythosesuiteswithatleast80 statement coverage a reasonable threshold for good testing.
we have less confidence in our results for this requirement as only suts have original suites with this level of coverage and only randoop generated suites manage it however this is larger than the set of subjects in many previous studies.
table shows that even at high levels of statement coverage for the original suites statement coverage is a better predictor of mutation kills than branch coverage.
in fact statement coverage s r2value is slightly increased for the original suites.
the ability to rank projects in terms of test 80ing effectiveness however is unsurprisingly considerably diminished.
and block coverage becomes a better predictor in this sense but statement coverage remains better than branch coverage.
table10showscorrelationswhentherequirementischanged to branch coverage being at least which yields only original suites and randoop generated suites.
interestingly in this case statement coverage has a very high r2 for the original suites and branch coverage has a negative rank correlation for randoop suites.
on the whole given the small number of suts meeting strict coverage requirements the claim that statement coverage is more useful in predicting quality than branch and block coverage at least fororiginaltestsuites doesnotseemtodependonlowcoverage.
our scatter plots show a vertical line at coverage to aid in visualizing the correlations at high coverage levels.
we also show the best fit lines for manual test suites in the randoop test suite scatter plots as dashed lines for easy comparison.
table correlation coefficients s .
r2 o o r2 r r m s0.
.
.
.
m s0.
.
.
.
m b0.
.
.
.
m p0.
.
.
.
table correlation coefficients b .
r2 o o r2 r r m s0.
.
.
.
m s0.
.
.
.
m b0.
.
.
.
m p0.
.
.
.
.
threats to v alidity one of the concessions we were forced to make while measuring various coverage ratios was to restrict the amount of time each test suite was allowed to run to one hour.
while this did not result in a significant elimination of projects there is a possibility that it may have biased us against larger or more complex projects with extremely large and thorough test suites.
we selected only those projects that compiled and passed all tests.
this may bias us against small one time projects where the authors did not have sufficient time to do a thorough job of testing or even against projects under active development where some tests represent open bugs.
our findings are dependent on the metrics reported by our tools.
we have taken a number of steps including cross verification and performing multiple experiment runs to ensure that we are not led astray by random noise in results.
however we do rely on a single mutation testing tool which is central to our results.
path coverage results are also somewhat less definite as jmockit is less mature than the other tools and we do not have a second path coverage tool to cross validate results.
however manual examination of results on multiple simple examples seems to show that the path coverage reported is correct.
path coverage is in any case fairly trivial to produce given the ability to produce other coverages.finally our findings are restricted to projects using the javalanguage themavenframework andastandardproject layout.
further the samples came from a single repository github and are all open source projects.
while none of these factors are obviously confounding they may limit applicability in radically different settings e.g.
haskell de velopment or a commercial software project with an extremely large user base and a dedicated qa team operating independently of developers .
.
conclusion mutation testing is one of the best predictors of test suite quality in terms of ability to detect actual faults.
however it is also computationally expensive to run complex to apply and is generally not used by real world developers with any frequency.
a long term goal of the testing community has therefore been to develop alternative methods for predicting suite quality for software development purposes and perhaps primarily for use in evaluating competing testing techniques.
unfortunately the large body of previous studies on this topic have largely considered only a small set of programs selected opportunistically sometimes focused on coverage criteria used as rarely as mutation testing in real world projects and often been constructed around the question of predicting the best among multiple suites for a single sut.
in reality software developers seldom have the luxury of applying esoteric coverage criteria or choosing between competing test suites.
rather given an existing test suite they want to estimate whether that suite is likely effective at detecting faults or if more testing effort may be justified given the cost of faults.
this paper finds a correlation between lightweight widely available coverage criteria statement block branch and path coverage and mutation kills for hundreds of java programs for both the actual test suites included with those projects and suites generated by the randoop testing tool.
for both original and generated suites statement coverage is the best predictor for mutation kills and in fact does a relatively good r2 .
for original tests and .
for generated tests job of predicting suite quality.
sut size code complexity and suite size do not turn out to be important.
a simple model of mutation and mutation detection predicts the higher effectiveness of statement coverage but does not explain why statement coverage even predicts path coverage better than branch coverage does a highly counter intuitive result.
moreover while block coverage became more competitive at high statement coverage levels statement coverage still appeared to be the best method for evaluating high coverage statement coverage suites with an r2of .
for suites with branch coverage .
the lesson for software developers is somewhat comforting statementcoverageisthemostwidelyavailableandeasily interpreted coverage criteria and is also the best coverage criteria for predicting test suite quality in their context.
the lesson for software testing researchers is that the question of how coverage correlates to suite effectiveness likely has no single correct answer but must pay careful attention to the context of application and the selection of a proper population of subjects and suites to examine.
.