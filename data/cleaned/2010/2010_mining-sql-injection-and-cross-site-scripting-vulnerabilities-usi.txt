singapor e management univ ersity singapor e management univ ersity institutional k nowledge at singapor e management univ ersity institutional k nowledge at singapor e management univ ersity resear ch collection school of computing and information systems school of computing and information systems mining sql injection and cr oss site scripting vulner abilities using mining sql injection and cr oss site scripting vulner abilities using hybrid pr ogram analysis hybrid pr ogram analysis lwin khin sh ar singapor e management univ ersity lkshar smu.edu.sg hee beng k uan t an lionel c. bri and follow this and additional works at https ink.libr ary.smu.edu.sg sis r esear ch part of the softwar e engineering commons citation citation shar l win khin t an hee beng k uan and bri and lionel c.. mining sql injection and cr oss site scripting vulner abilities using hybrid pr ogram analysis.
.
proceedings of the 35th a cm ieee international conf erence on softwar e engineering icse san f rancisco ma y .
.
available at available at https ink.libr ary.smu.edu.sg sis r esear ch this conf erence pr oceeding ar ticle is br ought t o you for fr ee and open access b y the school of computing and information systems at institutional k nowledge at singapor e management univ ersity .
it has been accepted for inclusion in resear ch collection school of computing and information systems b y an authoriz ed administr ator of institutional k nowledge at singapor e management univ ersity .
for mor e information please email cher ylds smu.edu.sg .
mining sql injection and cross site scripting vulnerabilities using hybrid program analysis lwin khin shar and hee beng kuan tan block s2 school of electrical and electronic engineering nanyang technological university nanyang avenue singapore shar0035 ibktan ntu.edu.sg lionel c. briand snt centre university of luxembourg rue alphonse weicker l luxembourg lionel.briand uni.lu abstract in previous work we proposed a set of static attributes that characterize input validati on and input sanitization code patterns.
we showed that some of the proposed static attributes are significant predictors of web application vulnerabilities related to sql injection and cross site scripting.
static attributes have the advantage of reflecti ng general properties of a program.
yet dynamic attr ibutes collected from execution traces may reflect more specific code characteristics that are complementary to static attributes.
hence to improve our initial work in this paper we propose the use of dynamic attributes to complement static attributes in the prediction of vulnerabilities.
furthermore since existing work relies on supervised learning it is dependent on the availability of training data labeled with known vulnerabilities .
this paper pr esents prediction models that are based on both classification and clustering in order to predict vulnerabilities working in the presence or absence of labeled training data respectively.
in our experiments across six applications our new supervised vulnerability predictors based on hybrid static and dynamic attributes achieved on average recall and precision that is a sharp increase in recall when compared to static analysis based predictions.
though not nearly as accurate our unsupervised predictors based on clustering achieved on average recall and precision thus suggesting they can be useful in the absence of labeled training data.
index terms defect predi ction vulnerability input validation and sanitization static and dynamic analysis empirical study .
i. introduction sql injection sqli and cross site scripting xss are the two most common and serious web application vulnerabilities threatening the privacy and security of both clients and applications nowadays .
to mi tigate the two threats many vulnerability detection approaches such as static taint an alysis and concolic testing have been proposed.
these approaches have been shown to be effective in finding many security vulnerabilities.
static taint analysis approach es are generally easy to be implement ed and adopt ed but are inefficient in practice due to high false positive rates low precision .
concolic testing techniques are highly precise but could be impractical for large systems due to state space explosion.
alternative or complementary vulnerability detection solutions that are both practical for use and precise would be beneficial to security teams.
input validation and input sanitization methods are commonly implemented in web applications to guard against application level attacks such as sqli xss path traversal and buffer overflow.
hence intuitively an application is vulnerable if the implementation of input validation and input sanitization methods is inadequate or incorre ct. in our initial work we mined static code patterns that implement such methods to build vulnerability predictors based on supervised learning.
we showed that those predictors provide an alternative effective solution for sqli and xss vulnerabilities .
although these result s were encouraging our earlier work suffers from two major drawbacks though proposed static attribu tes are useful predictors they are limited in terms of the prediction accuracy they can yield the predictive capability of these attributes is depende nt on how precise is the classification of the input validation and sanitization code patterns being a supervised learning based approach its effectiveness is dependent on the availability of sufficient training data labeled with manually checked se curity vulnerabilities .
this paper addresses the above limitations and provides a more extensive empirical study than that of our previous work.
it presents a pattern mining approach based on dynamic analysis that classifies input validation and sanitizati on functions through the systematic execution of these functions and the analysis of their execution traces .
we also use both supervised learning methods and unsupervised learning methods to build vulnerability predictors so as to determine the effectivene ss of the predictors with or without labeled training data.
in existing vulnerability prediction studies supervised learning methods are generally used.
we have no knowledge of vulnerability prediction models built using unsupervised learning methods .
our goal is to make vulnerability prediction both more practical and accurate than in previous work .
we evaluated our proposed approach based on experiments with a set of open source php based web applications with known xss and sqli vulnerabilities.
we impl emented a tool called phpmineri for extracting relevant data from php programs.
we trained two types of vulnerability prediction models for predicting sqli vulnerabilities and xss vulnerabilities using the extracted data.
in our cross validation experime nts supervised vulnerability predictors built from hybrid attributes achieved on average over data sets recall and precision on predicting xss and sqli vulnerabilities.
these new predictors improved the recall by and the precision by compared to predictors built from static analysis attributes alone .
our unsupervised predictors also achieved on average over data sets recall false alarm rate and precision.
this paper is limited to php programming language and focuses on classifying php functio ns and operations.
sqli and xss vulnerabilities are commonly found in many php based web applications.
however our approach could easily be extended to other programming language s. the outline of the paper is as follows.
section discusses our motivation.
section defines the static dynamic hybrid attribute s and presents the two hypotheses that we shall formerly investigate in this paper .
section presents our vulnerability prediction models and evaluates their accuracy based o n the proposed hybrid attributes.
section discusses related work .
section concludes our study.
ii.
motivation both sqli and xss vulnerabilities arise from improper handling of inputs in web application programs.
hence they are application level vulnerab ilities.
in a typical web program user inputs are accessed via forms urls cookies and xml files.
those inputs are often processed and propagated to various program points to accomplish the application s objectives.
some of those inputs may then be stor ed in the application s persistent data stores such as databases and session objects for further processing of the application s required functionalities.
the operations carried out in those processes often include security sensitive operations sinks such as html response outputs and database accesses.
when user inputs referenced in such operations have not been sanitized or validated vuln erabilities arise.
xss vulnerabilities arise when an unrestricted user input is used in a html response output sta tement.
sqli vulnerabilities occur when a user input is used in a sql statement without proper checks.
hence to prevent web application vulnerabilities developers often em ploy input validation sanitization methods along the paths propagating the inputs t o the sinks.
these methods can be broadly categorized in to escaping meta character matching removal string length truncating and data type checking conversion .
the methods typically employed are language provided functions e.g.
htmlentities tru sted t hird party libraries e.g.
enterprise security apis provided by owasp or custom functions developed for specific security or data integrity requirements by developer himself or a group of security experts.
from the analysis of many vulnerabil ity reports in security databases such as cve we derived the following observations first many sqli and xss vulnerability reports show that most of these vulnerabilities arise from the misidentification of input s. that is developers may implement adequate input validation and sanitization methods but yet they may fail to recognize all the data that could be manipulated by external users thereby missing some of the inputs for validation .
therefore in security analysis it is important to first id entify all the inputs and the sinks that use them .
second when an input to be used in security sensitive program statements is considered to be a numeric type it is most effective to use a numeric type check or numeric type conversion from string since inputs are originally strings .
third we observed that individual developers often write their own piece of validation and sanitization code to protect the specific programs that they are responsible for.
but as also observed by jovanovic et al.
and xie and aiken many of such customized functions are incorrect often due to insufficient expertise in security.
thus for most cases the use of language provided sanitization validation functions widely accepted third party security libraries or sec urity functions developed by a group o f security experts is typically the most effective defense method.
lastly different defense methods are generally required to prevent differen t types of vulnerabilities.
for example to prevent sqli vulnerabilities escaping characters t hat have special meaning to sql parser is required whereas escaping characters that have special meaning to client script interpreters is needed to prevent xss vulnerabilities.
thus care m ust be taken to use appropriate methods.
iii.
propo sed approach the above observations lead us to our first hypothesis h1 exce pt for those cases that involve only numeric input s that can always be validated through simple validation in general it is not straightforward for developers to implement defe nse against sqli and xss vulnerabilities from scratch by only relying on basic operations provided by the programming language.
hence developers should use functions pre developed by security experts to implement these defenses.
from h1 we derive the fol lowing attributes to build vulnerability predictors.
a. hybrid attributes data d ependence graph our unit of measurement is a sink.
a sink is a node in a control flow graph of a web program that may cause sqli or xss attacks.
basically a sink represent s a p rogram statement that interacts with a database denoted as sql sink or web client denoted as html sink .
given a sink k we compute its data dependence graph ddg k using data flow analysis.
the graph provides reachable definitions for the variables use d in the sink that is it contains the nodes on whi ch the sink is data dependent .
as such any input validation and sanitization operations implemented for the sink k can be found in the nodes in ddg k. the first step of our method is to classify the nodes in ddg k according to their security related properties and then to capture these classifications in a set of attributes on which vulnerability predictors are to be built.
basically our approach attempts to answer the following research question given the data dependence graph of a sink from the number of inputs and the numbers and types of input validation and sanitization functions found on the nodes in the graph can we predict the sink s vulnerability?
.
to classify nodes in ddg k we use a hy brid approach that combines static analysis and dynamic analysis techniques.
from the language built in functions that have specific security purposes e.g.
addslashes the language operators e.g.
string concatenation operator .
or the predefined l anguage parameters e.g.
get used in a given node n in ddg k n is classified statically .
but it is classified dynamically if it invokes user defined functions or some built in functions such as string replacement and string matching functions.
as a con trol flow node n may contain a variety of program operations there may be multiple classifications for n see example in section .
.
we shall address the attributes on which the classification schemes will rely as hybrid attributes .
the attributes are l isted in table and presented next.
static analysis based classification some of the language built in functions and operations can be statically and precisely classified from their properties or specific purposes.
the classification can be carried out b y simply checking the properties of the function or operation.
attributes in table characterize the functions and operators to be classified statically.
these attributes are similar to those proposed in our initial work .
hence we shall only briefly present them.
depending on the nature of sources we categorize the inputs into five types as explained by attributes in table .
attributes basically involve language built in functions and operators that could be used in input validati on and sanitization procedures.
attribute and correspond to language provided sqli and xss sanitization routines e.g.
htmlspecialchars respectively.
functions that invoke stored procedures or prepared statements e.g.
query prepare are also cla ssified as sqli sanitization routines.
attribute involves type casting built in functions or operations e.g .
a double b c that cast the input string into a numeric type data.
attribute corresponds to language provided numeric data type chec k functions e.g.
is numeric .
attribute corresponds to encoding functions.
an input variable may be properly sanitized using encoding functions e.g.
a href login.php?name .urlencode input .
attribute match es to functions or operations that return predefined information or information not extracted from the input string e.g.
mysql num rows .
we include the attribute boolean as a type of valida tion and sanitization because a boolean value returned from a user defined or built in function is definitely safe for use in the concerned sink.
and such a function can be classified statically by checking its function protocol.
clearly nodes in ddg k may also include ordinary operations that ma y or may not serve any security purpose.
they may simply propagate the input .
consequently we use the attribute propagate to characterize functions and operations that are not classified as any of the rest of types via either static analysis or dynamic analysis discussed in the following .
dynamic analysis based classification when a node invokes a user defined function or a language built in string replacement matching function such as str replace the type or purpose of the function cannot be easily inferred from static analysis.
because inputs to web app lications are naturally strings string replacement matching functions are generally used to implement input validation and sanitization procedures.
a good security function generally consists of a set of string functions that allow only valid strings or f ilter unsafe strings.
a filtering action entails character removal or escaping.
in our earlier work we simply characterized such string functions with attributes such as match e.g.
strcmp and regex replacement e.g.
preg replace .
this is too general and thus our earlier work could not discriminate correct and incorrect string functions e.g.
it treats all preg replace functions as correct or as incorrect .
hence to improve the accuracy of classification in this paper dynamic analysis is u sed if a node in ddg k invokes a user defined function or a language built in string replacement matching function.
the dynamic analysis attributes are defined as follows numeric functions that return only numeric mathemat ic and or dash charac ters e.g.
func tions that validate inputs such as mathematic equations postal code or credit card number .
limitlength functions that limit the length of an input string to a specified number .
url functions that filter directory paths or urls e.g.
a href src .
eventhandler functions that filter event handlers such as onload .
htmltag functions that filter html tags e.g .
strings between and the first white space or .
delimiter functions that filter delimiters that could disrupt the syntax of intended html documents or sql queri es e.g.
string delimiters such as single quote and double quote comment delimiters such as and and some other special characters such as parenthesis semi colon backslash null b yte and new line .
alternateencode functions that filter alternate character encodings e.g.
char 0x27 .
note that though the attribute numeric is similar to static analysis attributes and th ose two attributes characterize the nodes th at invoke language built in specific numeric type casting operations and numeric type checking functions respectively.
we believe that the above attributes reflect the types of input validation and sanitization methods that are commonly used to prevent sq li or xss attacks.
clearly a user defined function or a string replacement matching function may correspond to more than one attribute.
i f a function corresponds to attribute s a and b then both the values of a and b are to be incremented say a and b are numeric attributes .
in detail we maintain seven sets of test inputs derived from xss and sqli cheat sheets prov ided by owasp and rsnake .
these two security specialists provide a comprehensive table i. static dynamic hybrid attributes attribute id attribute name description static analysis attributes client the number of nodes that access data from http request parameters file the number of nodes that access data from files database the number of nodes that access data from database text database boolean value true if there is any text based data accessed from database false otherwise other database boolean value true if there is any data except text based data accessed from database false otherwise session the number of nodes that access data from persistent data objects uninit the number of nodes that reference un initialized program variable sqli sanitization the number of nodes that apply standard sanitization functions for preventing sqli issues xss sanitizat ion the number of nodes that apply standard sanitization functions for preventing xss issues numeric casting the number of nodes that type cast data into a numeric type data numeric type check the number of nodes that perform numeric data type chec k encoding the number of nodes that encode data into a certain format un taint the number of nodes that return predefined information or information not influenced by external users boolean the number of nodes which invoke functions that return boolean value propagate the number of nodes that propagate the tainted ness of an input string dynamic analysis attributes numeric the number of nodes which invoke functions that return only numeric mathematic or dash characters limitlength the number of nodes that invoke string length limiting functions url the number of nodes that invoke path filtering functions eventhandler the number of nodes that invoke event handler filtering fun ctions htmltag the number of nodes that invoke html tag filtering functions delimiter the number of nodes that invoke delimiter filtering functions alternateencode the number of nodes that invoke alternate character encoding filtering funct ions target attribute vulnerable?
indicates a class label vulnerable or not vulnerable coverage of xss and sqli attack vectors that could filter many types of input vali dation and sanitization routines.
each set of test inputs denoted as test attr set tests for each dynamic analysis attribute e.g.
a test input p test p tests for attribute htmltag as it could discriminate functions that accept or reject html tags and for a test attr set t that tests for an attribute a we execute the concerned function with a test input t1 from t and check if the function corresponds to a from the returned result.
if the function cannot be classified as a we choose a different test input t2 and repeat the process until it is classified as a or all the test inputs from t have been used step is iterated for all the seven test attr sets each set testing for each dynamic analysis attribute .
not all function arguments are as sociated with user inputs.
some arguments are assigned with literal values in the program.
such literal arguments can be easily identified from the nodes in ddg k. test inputs are only assigned to arguments that are derived from user inputs and literal argu ments are assigned with their own literal values extracted using data flow analysis .
more than one value is also possible for a literal argument if there are conditional branches.
it is logical as the same function can be used to sanitize a variable differ ently depending on the path along which the variable is propagated.
for each possible value of a literal argument we repeat the above dynamic classification process.
as explained we expect some functions to match multiple classifications.
attributes in table represent the classifications presented above.
we shall provide more details on the classification methods in our example section.
target attribute the last attribute vulnerable?
is the target attribute which is used to indicate the class l abel to be predicted.
b. classification and clustering our goal is to build accurate vulnerability prediction models supervised vulnerability prediction from the hybrid attributes presented above.
since the proposed attributes are designed to reflect h1 if h1 is true we should expect that given a sufficient sample of vulnerability data classifiers learnt from such data be accurate at vulnerability predictions.
although classifiers can be effective a sufficient number of instances with known vulnerabili ty information is required to train a classifier .
it is usually tedious and labor intensive to tag many instances with vulnerability labels.
sometimes the vulnerability information is not even yet known.
in such situations supervised training i.e.
wher e training instances need to be labeled with vulnerability information is simply not feasible.
cluster analysis on the other hand is a type of unsupervised learning methods in which no class labels are required for training with instances.
intrusion de tection studies have shown that cluster analysis could identify numerous anomalies intrusions in their context based on the two assumptions that normal instances are much more frequent than anomalies and anomalies have different characte ristics from normal instances.
if in our context the same two assumptions hold cluster analysis could be used for identifying vulnerable sinks as well.
this leads us our second hypothesis h2 vulnerable sinks can be distinguished from non vulnerable sinks based on the hybrid attributes proposed above.
if h2 is true we would observe that cluster analysis on the unlabeled instances containing the data of hybrid attributes can predict vulnerabilities.
hence when classification based vulnerability pred iction models are not a feasible option our approach also include making use of clustering for building vulnerability prediction models from our hybrid attributes when the above assumptions are met .
c. example in this section we explain in detail the class ification methods and the attribute collection process using the program in fig.
.
s tatement is a class of input because it accesses an http session parameter.
it can be statically classified via checking the accessed predefined parameter session .
statement can be classified as xss sanitization because it invokes a standard escaping routine.
again it can be statically classified via checking the invoked function name that is we predefine the function htmlspecialchars as a xss sanitization type.
statement is an un taint type.
figure 1b shows the data dependence graph of html sink in fig.
1a.
node invokes a user defined function and it is clear that it could not be precisely classified by just looking up the predefined classifications.
we cl assify such nodes via dynamic analysis .
in node a customized security function pma backquote is invoked with two arguments trg db and sqlescape .
by a data flow analysis t he literal value for sqlescape is extracted from no de .
from node trg db is identified as an input variable.
it is then assigned with a value obt ained from test attr sets.
and t he function is executed multiple times each time selecting a different value from test attr sets to determine if it can be classified with one or more dynamic analysis attributes see dynamic analysis based classification .
classifications are carried out based on the types of input values used and the contents of the resulting outputs.
when a test input such as or is used the returned resu lt shall be or and the function would be classified as delimiter as it escapes a string delimiter .
nodes and shall not be fig.
.
a sample vulnerable php code extracted from phpmyadmin server synchronize.php sligh tly modified for illustration purpose .
the code cleanses an input using standard and customized sanitization functions.
b data dependence graph of sink statement .
classified as the nod es are contained in the user defined function th at has already been classified.
based on the above classifications the attribute vectors for html sinks and could be extracted from their respective data dependence graphs as ... not vulnerable and ... vulnerable respectively according to attribute vector session html xss sanitization un taint delimiter propagate ... vulnerable?
.
as we propose hybrid attributes each sink would be represented by a dimensio nal attribute vector.
iv.
evaluation we conducted two types of experiments in order to assess the accuracy of the predictors learnt from the proposed hybrid attributes in terms of vulnerability prediction.
in the first experiment section .
we evaluated two different types of classifiers on the data sets with class labels vulnerable or not vulnerable.
in the second experiment section .
we removed the class labels and evaluated a clustering algorithm on the data sets without class labels.
performance m easures to evaluate the vulnerability predictors we computed recall or probability of detection pd probability of false alarm pf and precision pr .
we can use the following contingency table to define these standard measures.
actual vulnerabl e not vulnerable predicted vulnerable true positive tp false positive fp not vulnerable false negative fn true negative tn recall pd tp tp fn measures how good our prediction model is in finding actual vulnerable sinks.
precision pr tp tp fp measures the actual vulnerable sinks that are correctly predicted in terms of a percentage of total number of sinks predicted as vulnerable.
false alarms pf fp fp tn is generally used to measure the cost of using the model .
increasing pd by tun ing the prediction model may in turn cause more false alarms or reduce precision .
ideally the model should neither miss actual vulnerabilities pd nor throw false alarms pf pr .
a. data collection for data collection we modified the tool phpminer i which was used in our earlier work .
phpmineri is based on an open source php program analysis tool called pixy .
for classification via static analysis php built in functions and php operators are predefined in phpmineri which computes d ata dependence graph for each sink and collects static analysis attributes.
in this work we modified the tool to incorporate dynamic analysis classification.
dynamic analysis is used when a node in ddg k invokes user defined functions or language built in string replacement matching functions.
no classification is made for nodes in ddg k that are contained in dynamically classified user defined functions to avoid unnecessary or overlapp ing classifications.
to identify function arguments i.e.
literals or i nputs static data flow analysi s is used.
test inputs are generated from our predefined test suite which reflects the dynamic classificatio n scheme proposed in section .
.
function s are executed using the apis from a trg db session echo table tr th target database .
htmlspecialchars trg db html sink .
.
.
sqlescap e query update .
pma backquote trg db sqlescape .
set ... if display true echo p .
query .
p html sink rs mysql query query sql sink function pma backquote a name replace if strlen a name a name !
return .
str replace replace a name .
else return a name php java bridge java package provide d in javabridge.sourceforge.net pjb .
f unction return results are then analyzed to determine the intended validation and sanitization scheme.
experiments were conducted on six real world php based web applications obtained from sourcef orge .
table shows relevant statistics for these test subjects.
the last column in table shows the security advisories such as cve from which the test subjects vulnerability information is obtained.
some of these test subjects have also been benchma rked for the evaluation of some vulnerability detection approaches .
table shows the data sets collected by phpmineri .
as shown in table we extracted two types of data sets one corresponds to html sinks and another corresponds to sql sinks.
in total we collected data sets only sets of sql sinks were used as we have not tagged the vulnerability labels for sql sinks in phpmyadmin and utopia systems yet .
column in table shows the number and percentage of vulnerable sinks in each data set manually ins pected and tagged by the first author .
on our web site we provide implementation details of phpmineri and the data sets.
b. data preprocessing normalization to generalize the results our vulnerability predictors must be able to handle data of arbitrary distributions.
excluding the target attribute we have hybrid attributes.
twenty attributes take on numeric values and two attributes are binary .
from our preliminary tests we observed that different numeric attributes are defined on different scales and most of the attributes distributions are highly skewed.
this may cause bias toward some attributes e.g.
attributes with large scale values especially in the context of clustering where similarity measurement combines multiple attribute scales.
we use a data standardizing technique called min max normalization to avoid this problem as described in witten and frank .
min max normalization enables our predictors to work in a standardized data space instead of a raw data space.
an attribute is normalized when its value is scaled so as to fall within a small specified range we used the range of zero to one .
as the normalized value is a linear transformation from the orig inal data value the relationships among the original data values are preserved .
the min max normalization is to be made for all the instances of every numeric attribute.
this shall result in a set of values within the range of zero to one.
the binary attr ibutes do not need to be transformed .
principal component analysis principal component analysis pca is a useful technique to identify linearly uncorrelated dimensions in a large datasets with possibly many inter correlated attributes.
multivariate data mining and statistical techniques used to build classifiers such as logistic regression see their performance negatively impacted in the presence of numerous inter correlat ed attributes.
pca results in a new set of attributes principal components each of wh ich is a linear combination of s ome of the original attributes.
the number of principal components is usually much smaller.
in our experiments we applied pca to every data set after min max normalization and used a subset of principal component s as attributes such that the selected explain at least table ii.
statistics of the test subjects test subject description loc security advisories schoolmate .
.
a tool for school administration vulnerability information in faqforge .
.
document creation and management bugtraq utopia news pro .
.
news management system bugt raq phorum .
.
message board software cve cve cutesite .
.
content manage ment framework cve cve phpmyadmin .
.
mysql database management pmasa pmasa table iii.
data sets data set html sinks vuln .
sinks vuln.
principal components schmate html faqforge html utopia html phorum html cutesite html myadmin html data set sql sinks vuln .
sinks vuln.
principal components schmate sql faqforge sql phorum sql cutesite sql of the data variance.
the last column in table shows the numbers of principal components selected for building supervised and unsupervised vulnerability predictors.
c. supervised vulnera bility prediction based on h1 we evaluated two different classifiers learnt from our proposed hybrid attributes.
classifiers classification is a type of supervised learning methods because the class label of each training instance has to be provided.
we built logistic regression lr and multi layer perceptron mlp models for this experiment.
these classifiers were benchmarked as among the top classifiers in recent studies .
mlp is a type of neural networks.
lr is a type of statistical regression models.
details about these classification techniques are provided by witten and frank .
we used two very different techniques in an attempt to optimize accuracy .
training and testing we used a standard sampling method called fold cross validation set up.
the data is divided into ten sets.
a classifier is trained on nine sets and then tested on the remaining set.
this process is repeated ten times each time testing on a different set .
the order of training and test set is randomized.
this test design o vercomes the ordering effects due to randomization .
this is important to avoid a malignant increase in performance by a certain ordering of training and test data.
isolating a test set from the training set also conforms to hold out test design which is im portant to evaluate the classifier capability to predict new vulnerabilities .
result the results of the two classifiers learnt from hybrid attributes are shown in fig.
.
on average both models showed good performances with high vulnera bility detec tion rates and low false alarm rates .
but on some data sets such as phorum html and phorum sql mlp could not discriminate vulnerabilities whereas lr is able to.
therefore based on current results we advise to the use of lr to build vulnerabi lity prediction models.
the significantly low false alarm rates achieved by our new models indicate that the models precision has improve d from our initial work .
yet t o provide an exac t comparison baseline we also built lr models from static ana lysis attributes alone and evaluated them in the same way as the above models.
results are shown in fig.
.
on average our proposed lr models built from hybrid attributes achieved pd pf pr improvements over the lr models built from static analysis attributes only.
as suggested by dem ar we also used one tailed wilcoxon signed ranks tests to perform pairwise comparisons of the measures achieved by the two measure data classifier pd pf pr schmate html lr mlp faqforg e html lr mlp utopia html lr mlp phorum html lr mlp cutesite html lr mlp myadmin html lr mlp average results on xss prediction lr mlp schmate sql lr mlp faqforge sql lr mlp phorum sql lr mlp cutesite sql lr mlp average results on sqliv prediction lr mlp overall ave rage lr mlp fig.
.
classification results of xss and sqli vulnerability predictors built from hybrid attributes.
measure data classifier pd pf pr schmate html lr faqforg e html lr utopia html lr phorum html lr cutesite html lr myadmin html lr schmate sql lr faqforge sql lr phorum sql lr cutesite sql lr overall average lr fig.
.
classification results of xss and sqli vulnerab ility predictors built from static analysis attributes.
types of lr models over the data sets.
the tests show that the improvements of recall and precision were statistically significant at a level though only the increase in recall is interesting from a practical standpoint.
we can conclude that dynamic analysis attributes contribute to significantly improving the accuracy of vulnerabilit y predictors.
as these attributes are designed to store the information about potentially correct and incorrect input validation and sanitization procedures implemented in the program the se results support h1.
d. unsupervised vulnerability prediction regardi ng h2 we evaluated a clustering model learnt from our proposed hybrid attributes.
cluster analysis unlike classification methods cluster analysis works in the absence of class labels for training instances.
but its predictive capability would be expecte d to be inherently lower due to the absence of supervision.
like portnoy et al.
s unsuperv ised intrusion detection study the performance of our cluster analysis here should depend on the following two assumptions non vulnerable sinks are much more frequent than vulnerable sinks and vulnerable sinks have different characteristics from non vulnerable sinks .
if these two assumptions are met and h2 is true vulnerable sinks would b e clustered together as outliers in terms of hybrid attribute values which could then b e detected by cluster analysis.
because there is no need to label instances unsupervised learning such as cluster analysis is expected to be much less expensive tha n building classifiers for vulnerability prediction.
we evaluated k means clustering algorithm applied to our proposed hybrid attributes .
k means is a simple and often effective partitioning algorithm.
given an input k it partitions a set of instances in to k clusters in such a way that similarity among instances is maximized within the same clusters and minimized across the different clusters.
for similarity measurement standard distance functions can be used.
for our experiments we used the euclidean d istance function .
further details about the algorithm are provided in .
parameter estimation as clustering only groups instances based on their similarities some parameters must be defined to label the clusters as vulnerable or not vulnerable.
the problem here is given a set of clusters produced by a clustering algorithm what are the best rules parameters to single out clusters that contain a large proportion of vulnerab le sinks ?
.
in portnoy et al.
s clustering based intrusion detection study a parameter n was used as the percentage of the largest clusters that would be labeled as normal as it was found to optimize their results .
for our clustering based vulnerability prediction stu dy we used a parameter normal .
it defines the minimum size in terms of percentage of instances of clusters that would be labeled as not vulnerable.
for example if normal the clusters containing more than of data would be labeled as not vulner able.
as required by k means algorithm we also needed to determine a parameter k that indicates the number of clusters to be produced by k means .
we determined the two parameters by performing experiments that optimize results on the test subjects used in our initial work .
the resulting parameters k and normal were then consistently used throughout this evaluation.
result more than of sinks in schmate html faqforge html schmate sql faqforge sql and cutesite sql are v ulnerable sinks see vuln.
in table .
these data sets clearly violate the first assumption stated above as they contain many vulnerabilities .
we expect low predictive power fro m our clustering models for such data sets.
c onsequently we separated the data sets which meet our assumptions from the ones that violate the assumptions and performed separate evaluations.
the results on the former data sets are shown in fig.
and the results on the latter sets are shown in fig.
.
as shown in fig.
the k mean s detection rate is very good especially on utopia html and phorum sql data sets.
but its average precision is half that of the supervised models above.
this is directly caused by the inherent weakness of the unsupervised learning scheme.
it is also affected by different trade offs between detection rates and false alarm s. the trade offs mainly result from the parameter normal .
with a high value of normal we label more clusters as vulnerable and reduce precision .
tuning s uch a parameter must be done in context based on available resources for vulnerability detections.
as expected as shown in fig.
cluster analyses on data sets which violate our first assumption result in very low detection rates because many or all of the vulnerab le sinks did not appear as outliers in terms of hybrid attribute values to our clustering model .
pr was also undefined for some data sets as both pd and pf were null.
from the results in fig.
we c an conclude that if certain assumptions are met cluster analysis on unlabeled instances using hybrid attributes can help accurately predict vulnerabilities thus support ing h2.
mea sure data pd pf pr utopia html phorum html cutesite html myadmin html phorum sql average fig.
.
k means clustering analysis re sults on the data sets which meet the assumptions .
measure data pd pf pr schmate html faqforg e html schmate sql faqforge sql undef ined cutesite sql undefined average undefined fig.
.
k means clustering analysis re sults on the data sets which violate the assum ptions .
e. threats to validity our data only reflects the known vulnerabilities that are reported in vulnerability databases.
hence our vulnerability prediction s based on classifiers do not account for undiscovered vulnerabilities.
the application of clus ter analysis is limited by the two assumptions stated above .
in our experiments clustering based prediction models could accurately isolate vulnerabilities in the data sets which satisfy those assumptions .
however it is unclear how frequently these assum ptions hold in practice across systems and types of vulnerabilities .
further we estimated two parameters k and normal driving the accuracy of cluster analysis based on our experience with preliminary experiments.
we used the same two parameters for all the data sets.
the parameters worked well for our context but may not generalize well elsewhere.
but as most of our test subjects such as phpmyadmin are widely used real world applications we believe that the above threats do not significantly affect ou r results although tuning the parameters may be required for some applications.
the use of different or more data preprocessing activities may also alter our results.
for example during our preliminary experiments we tested the data sets with and withou t pca see section .
.
results without pca were significantly inferior to results with pca for the majority of data sets though no significant differences were observed for some .
different classification and clustering algorithms could result in differe nt results.
in our experiments we used two very different classification algorithms which are statistical based and network based respectively.
we also tried other classifiers like c4.
and na ve bayes but the average results were similar.
we have not tried another algorithm for clustering based prediction but we expect similar results if similar parameters i.e.
k and normal are used.
like all other empirical studies our results are limited to the applied data mining processes the test sub jects and the experimental setup used.
one good solution to refute prove or improve our results is to replicate the experiments with new test subjects and probably with further data mining strategies .
this can be easily done since we have clearly define d our methods and setup and we also provide the data used in the experiments and the data collection tool on our web site .
v. related work our work applies data min ing for the prediction of vulnerabilities in web applications.
hence its related work fal ls into three categories defect prediction vulnerability prediction and vulnerability detection.
defect prediction data mining models used by our approach are similar to those used in many defect prediction studies .
in these studie s defect predictors are generally built from static code attributes such as objec toriented design attributes loc counts and code complexity attributes because static attributes can be cheaply and consistently collected across many systems .
however it was quickly realized that such attributes can only provide limited accuracy .
arisholm et al.
and nagappan et al.
reported that process attributes e.g.
developer experience and fault history could significantly improve prediction models.
on the other hand as process attributes are difficult to measure and measurements are often inconsistent menzies et al.
showed that static code attributes can still be effective if predictors are tuned to user speci fic goals .
in contrast to defect prediction studies our study targets security vulnerabilities in web applications.
since the se studies show that there is no universal set of attributes we define specific attributes targeted at predicting vulnerabilitie s based on automated and scalable static and dynamic analysis .
vulnerability prediction shin et al.
used code complexity code churn and developer activity attributes to predict vulnerable program s. they achieved recall and false alarm rate .
their assumption was that the more complex the code the higher the chances of vulnerability .
but from our observations many of the vulnerabilities arise from simple code and if a program does not employ any input validation and sanitization routi nes it would be simpler but nevertheless contain many vulnerabilities.
walden et al.
investigated correlations between the security resource indicator sri and the numbers of vulnerabilities in php web applications.
sri is derived from publicly ava ilable security information such as past vulnerabilities secure development guidelines and security implications regarding system configurations.
neuhaus et al.
also predicted vulnerabilities in software components from the past vulnerability inform ation and the import s and function calls attributes.
their work is based on the concept that components which contain imports and function calls that are similar to known vulnerable components are likely to be vulnerable as well.
they achieved recall and precision .
these existing vulnerability prediction approaches generally target software components.
by contrast our method target s specific program statements for vulnerability prediction.
the other difference is that we use code attributes that characterize input validation and sanitization routine s. vulnerability detection jovanovic et al.
and xie and aiken showed that many xss and sqli vulnerabilities can be detected by static program analysis techniques.
they identify v arious input sources and sensitive sinks and determine whether any input data is used in those sinks without passing through sanity checks.
in general such static taint tracking approaches are effective but not efficient as t hey generate many false alarm s. to improve precision fu and li and wassermann and su approximated the string values that may appear at sensitive sinks by using symbolic execution and string analysis techniques.
more recent approaches incorporate dynamic analysis techniques such as concolic execution and model checking .
these approaches reason about various paths in the program that lead to sensitive sinks and attempt to generate test cases that are likely to be attack vectors.
all these approaches reduce false alarm rates.
but symbolic concolic and model checking techniques often lead to a path explosion problem.
it is difficult to reason about all the paths in the program when the program contains many branches and loops.
further the performance of these ap proaches also depends very much on the capabilities of their underlying model checkers or string constraint solvers in handling a myriad of string operations offered by programming languages.
by contrast although our approach also requires dynamic analysi s this is done at the function level.
it does not require string solving and reasoning of potentially infinite program paths like concolic execution and model checking techniques.
however symbolic concolic and model checking approaches could possibl y yield high precision rates which may never be matched by data mining methods.
thus our objective is not to provide a replacement for such techniques but rather to provide a complementary approach to use when they are not applicable or in combination wit h them .
one could for example first to gather vulnerability predictions on code sections using data mining and then focus on the code sections with predicted vulnerabilities using any of the more precise techniques mentioned above .
thereafter ideally t he confirmed vulnerabilities should be removed by manual auditing or by using automated vulnerability removal techniqu es such as the ones proposed in .
vi.
conclusion the goal of this paper is to aid security auditing and testing by providing probabili stic alerts about potentially vulnerable code statements .
we propose attributes based on hybrid static and dynamic code analysis which characterize input validation and sanitization code patterns for predicting vulnerabilities related to sql injectio n and cross site scripting .
given a security sensitive program statement we collect the hybrid attributes by classifying the nodes from its data dependency graph.
static analysis is used to classify nodes that have unambiguous security related purposes.
d ynamic analysis is used to classify nodes that invoke user defined or language built in string replacement matching functions since classification of such nodes by static analysis could be imprecise.
we evaluated if the se hybrid attributes can be used to build effective vulnerability predictors using both supervised and unsupervised learning methods .
the latter have in practice the advantage of not requiring labeled training data with known vulnerabilities but may be significantly less accurate .
in th e experiments on six php web applications we first showed that the hybrid attributes can accurately predict vulnerabilities recall and precision on average for logistic regression .
we also observed that dynamic analysis helped achieve much better accuracy than static analysis alone thus justifying its application .
last but not least when meeting certain assumptions cluster analysis showed to be a reasonably accurate unsupervised learning method when no labeled data is available for tra ining recall and precision on average .
but since it is not nearly as accurate as supervised learning it should be considered as a trade off between data collection cost and accuracy.
to generalize our current results we hope that researchers will replicate our experiment possibly using the data and tool we posted online .
we also intend to conduct more experiments with industrial applications.
while we believe that the proposed approach can be a useful and complementary solution to existing v ulnerability detection and removal approaches studies should be carried out first to determine the feasibility and usefulness of integrating multiple approaches i.e.
prediction detection removal .