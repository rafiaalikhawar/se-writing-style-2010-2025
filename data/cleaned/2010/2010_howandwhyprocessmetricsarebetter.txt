draft!how and why process metrics are better foyzur rahman university of california davis mfrahman ucdavis.edupremkumar devanbu university of california davis ptdevanbu ucdavis.edu abstract defect prediction techniques could potentially help us to focus quality assurance efforts on the most defect prone files.
modern statistical tools make it very easy to quickly build and deploy prediction models.
software metrics are at the heart of prediction models understanding how and especially why different types of metrics are effective is very important for successful model deployment.
in this paper we analyze the applicability and efficacy of process andcode metrics from several different perspectives.
we build many prediction models across releases of large open source projects to address the performance stability portability and stasis of different sets of metrics.
our results suggest that code metrics despite widespread use in the defect prediction literature are generally less useful than process metrics for prediction.
second we find that code metrics have high stasis they don t change very much from release to release.
this leads to stagnation in the prediction models leading to the same files being repeatedly predicted as defective unfortunately these recurringly defective files turn out to be comparatively less defect dense.
i. i ntroduction software based systems pervade and greatly enhance modern life.
as a result customers demand very high software quality.
finding and fixing defects is expensive defect prediction models promise greater efficiency by prioritizing quality assurance activities.
since defect distribution is highly skewed such models can usefully finger the most defective bits of code.
typically defect prediction models rely on supervised learners which use a labeled training dataset to learn the association between measured entity properties e.g.
metrics calculated on files or methods with the defect proneness of these entities.
careful choice of metrics can improve prediction performance.
researchers have mostly focused on two broad classes of metrics for defect prediction code metrics which measure properties of the code e.g.
size and complexity and process metrics e.g.
number of changes number of developers .
researchers have long been interested in which class of metrics process or code are better for defect prediction.
moser et al.
compared the power of code and process metrics on eclipse project and found that process metrics outperform code metrics.
however menzies et al.
report that code metrics are useful for defect prediction.
arisholm et al.
found that process and code metrics perform similarly in terms of auc but code metrics may not be cost effective .
our work deviates from existing approaches in two important ways.
firstly and most importantly we seek to understand how andwhy process metrics are better for defect prediction.
secondly our methodology is squarely based on a prediction setting.
existing studies mostly evaluate different types ofmetrics using cross validations on few projects.
however the most attractive use of models is in a prediction setting such models can be used to focus cost constrained quality control efforts.
in a release oriented development process this means training models on earlier releases to predict defects of latter release.
we build and compare prediction models across multiple projects and releases using different combinations of metrics and learning techniques using a broad range of settings.
our experiments lead to the following contributions we compare the performance of different models in terms of both traditional measures such as auc and f score and the newer cost effectiveness measures.
we compare the stability of prediction performance of the models across time and over multiple releases.
we compare the portability of prediction models how do they perform when trained and evaluated on completely different projects.
we study stasis viz.. the degree of change or lack thereof in the different metrics and the corresponding models over time.
we then relate these changes with their ability to predict defects.
we investigate whether prediction models tend to favor recurringly defective files we also examine whether such files are relatively more defect dense and thus good targets of inspection efforts.
ii.
b ackground and theory defect prediction models are mostly built using supervised learning techniques logistic regression svm decision trees etc.. during training the model systematically learns how to associate various properties of the considered software entities e.g.
methods files and packages with the defect proneness of these entities.
researchers have historically hypothesized that properties of the code measured using code metrics could usefully predict defect proneness.
code metrics could measure size larger files may be more defect prone or complexity more complicated files may be more defect prone .
the value of code metrics in defect prediction has been well explored .
however software development processes per se can be quite complex.
lately researchers have been interested in the impact of development process on software quality.
recent studies suggest that an entity s process properties e.g.
developer count code ownership developer experience change frequency may be important indicators of defect proneness.draft!clearly the relative efficacy of these metrics in defect prediction is of vital concern to the practitioner.
should she use all types of metrics?
should she mix different types of metrics?
moser et al.
compared the prediction performance of code and process metrics in three releases of eclipse and found that process metrics may outperform code metrics in defect prediction.
arisholm et al.
compared various metrics and prediction techniques on several releases of a legacy java middleware system named cos and found that code metrics may perform well in terms of traditional performance measures such as auc while it may not be cost effective.
menzies et al.
found code metrics very effective for defect prediction.
existing studies mostly limit themselves to a cross validation based model evaluation on a limited set of projects.
however many projects align their development and quality assurance activities with releases.
therefore a release based evaluation of model performance may be more appropriate for such settings.
research question inrelease based prediction settings how do the process and code metrics compare to predict defect locations?
release based prediction performance may however destabilize after a major shift of activities between releases.
for example a release comprising a burst of new features might be followed up by a series of incremental quality improving releases.
the process aspects that cause defects may also shift thus confounding prediction performance.
ekanayake et al.
found that defect prediction models destabilize viz.. perform poorly over time due to project phase changes.
we therefore evaluate how process and product metrics affect the prediction stability of models.
presumably a more stable model would also adapt better to project phase changes armed with this information a practioner can then more effectively select prediction metrics based on specific project dynamics.
research question are process metrics more less stable than code metrics?
another interesting way of comparing process and code metrics would be the portability of the learned prediction models between different projects.
portability would be useful for smaller software companies without large available portfolios of software for training prediction models.
even big companies with diverse product portfolios may find it useful to port their prediction models to a new product.
portability also called cross project prediction recently attracted quite a bit of attention but we have not found any that compare the portability of different types of metrics.
research question are process metrics more less portable than code metrics?besides exploring the practical utility of metrics in defect prediction models we also seek to understand why one class of metrics outperforms another by exploring the distribution of metrics values in more detail.
one important property of metrics is stasis .
one can reasonably expect that as a system evolves the distribution of defects does not remain unchanged.
therefore we might expect that the values of useful metrics ones with defect prediction power would also tend to vary with release.
metrics whose values remain unchanged would willy nilly tend to predict the same files as defective release after release.
research question are process metrics more less static than code metrics?
the above question asks if metrics change or fail to change as software evolves but even a very dynamic metric is a useful predictor only if it co evolves significantly with defect occurrence.
furthermore usually many metrics are used together as a group in a prediction model.
even if a single metric isn t very dynamic it might synergistically work together with other also static metrics to form a good prediction model.
it is therefore important to understand whether models stagnate viz.. they tend to repeatedly predict the same files as defective.
research question do models built from different sets of metrics stagnate across releases?
of course it is possible that even stagnant models work if the same files are recurringly defective across multiple releases then a stagnant model that selects these defect prone files will be reasonably successful.
we hypothesize that stagnant models predict defect distributions close to those they originally learned.
perhaps they flag mostly the defective entities which don t change much e.g.
complicated files which repeatedly become defective.
we consider such defective entities defective in both training release and test release as recurringly defective and the rest as incidentally defective .
we hypothesize that a stagnant model would predict mostly the recurringly defective entities.
research question do stagnant models based on stagnant metrics tend to predict recurringly defective entities?
arisholm et al.
report that code metrics perform well as well as process metrics when considering cost insensitive entity based measures such as auc but not as well when considering cost.
while arisholm et al.
don t explore the reasons for this result we believe this arises from prediction bias away from defect density .
larger files are more expensive to inspect if such investment does not pay off in terms of number of defects uncovered it s not cost effective.
wedraft!table i studied projects and release information project description releases avg files avg sloc cxf services framework .
.
.
.
.
.
.
.
.
.
camel enterprise integration framework .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
derby relational database .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
felix osgi r4 implementation .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
hbase distributed scalable data store .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
hadoopc common libraries for hadoop .
.
.
.
.
.
.
.
.
.
.
.
hive data warehouse system for hadoop .
.
.
.
.
.
.
.
.
.
.
.
.
.
lucene text search engine library .
.
.
.
.
.
.
.
.
.
.
.
.
.
openejb enterprise java beans .
.
.
.
.
.
.
.
.
.
.
.
openjpa java persistence framework .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
qpid enterprise messaging system .
.
.
.
.
.
.
wicket web application framework .
.
.
.
.
.
.
.
.
.0b2 table ii process metrics short name description comm commit count adev active dev count ddev distinct dev count add normalized lines added del normalized lines deleted own owner s contributed lines minor minor contributor count sctr changed code scattering nadev neighbor s active dev count nddev neighbor s distinct dev count ncomm neighbor s commit count nsctr neighbor s change scatttering oexp owner s experience exp all committer s experience examine whether code metrics have a prediction bias towards larger files and thus ones with lower defect density and whether such prediction bias emanates from the stagnation of the models as discussed earlier.
research question do stagnant models have a prediction bias towards larger less defect dense files?
iii.
e xperimental methodology a. projects studied our sample projects are listed in table i. all are javabased and maintained by apache software foundation asf however they come from a very diverse range of domains.
for each project we extracted the commit history from its git repository1.
we also used git blame on every file at each release to get the detailed contributor information.
our blame process uses copy and move detection and ignores whitespace changes to identify the correct provenance of each line.
all projects use jira2issue tracking system.
from jira we extracted the defect information and the fixing commits for fixed defects.
then for each fixing commit we extract changed lines author etc.from git.
any files modified in these defect fixing commits are considered as defective.
b. predicting defects our defect prediction studies are at file level.
we choose file level predictive code and process metrics that have been widely used in defect prediction literature.
we used a wide range of learning techniques logistic regression j48 svm and naive bayes all from weka3.
this reduces the risk of dependence on a particular learning technique.
note in the default setting j48 and svm do not provide a probability of defect proneness which is important for cost effectiveness comparison.
however weka provides settings for j48 and svm that do yield probabilities4.
as did arisholm et al.
we find that the prediction performance depends mostly on the types of metrics used and not on the learning technique.
therefore for brevity we typically just report the findings from logistic regression and mention deviations in other techniques if any.
all of our models are binary classifiers predicted defectivejclean .
however a file may have zero or more defects fixed during a release.
as in prior work we declare a file as defective if it had at least one defect fix or clean otherwise.
using both code and process metrics we build models in a release based prediction setting we train the model on a given release and evaluate its performance on the next release.
so e.g.a model trained on the k th release is only tested on thek th release.
however as discussed in section iv for stability we evaluate the model on all future releases following a training release.
for sensitivity analysis of other research questions we also evaluated all of our models using all future releases instead of just the immediately succeeding release and found consistent results.
forportability we evaluated models trained on one project on all releases of other projects ignoring time ordering.
4for j48 an option to use unpruned decision tree with laplace smoothing is available to find the estimate of defect proneness of each file.
similarly weka s svm implementation allowed us to fit a logistic models to svm output to find the probability of defect proneness.draft!table iii code metrics type metrics count file countdeclmethodprivate avglinecode countline maxcyclomatic countdeclmethoddefault avgessential countdeclclassvariable sumcyclomaticstrict avgcyclomatic avgline countdeclclassmethod avglinecomment avgcyclomaticmodified countdeclfunction countlinecomment countdeclclass countdeclmethod sumcyclomaticmodified countlinecodedecl countdeclmethodprotected countdeclinstancevariable maxcyclomaticstrict countdeclmethodpublic countlinecodeexe sumcyclomatic sumessential countstmtdecl countlinecode countstmtexe ratiocommenttocode countlineblank countstmt maxcyclomaticmodified countsemicolon avglineblank countdeclinstancemethod avgcyclomaticstrict37 class percentlackofcohesion maxinheritancetree countclassderived countclasscoupled countclassbase method countinput countoutput countpath maxnesting c. process metrics our file based process metrics are listed in table ii.
all process metrics are release duration.
comm measures the number of commits made to a file.
adev is the number of developers who changed the file.
ddev is the cumulative number of distinct developers contributed to this file up to this release.
add anddel are the normalized by the total number of added and deleted lines added and deleted lines in the file.
own measures the percentage of the lines authored by the highest contributor of a file.
minor measures the number of contributors who authored less than of the code in that file.oexp measures the experience of the highest contributor of that file using the percent of lines he authored in the project at a given point in time.
exp measures the geometric mean of the experiences of all the developers.
all these metrics are drawn from prior research .
we also used a simple line based change entropy metric derived from the location of the changes made sctr measures the scattering of changes to a file scattered changes could be more complex to manage and thus more likely to induce defects.
sctr is the standard position deviation of changes from the geographical centre thereof.
kim et al.
s celebrated bugcache is populated using a simple co commit history.
this work suggests a range of different process metrics based on co commit neighbors.
for a given file fand release r these metrics are based on the list of files co committed with f weighted by the frequency of co commits during r.nadev nddev ncomm andnsctr are just the co commit neighbor measures of adev ddvev comm and sctr .
we measured the usefulness of all the neighbor based metrics as well as sctr using single variable predictor models and found them to be highly significant defect predictors with a median auc of around .
.
all of our process metrics are cumulated and measured on a per release basis.
in this paper we focus on measuring prediction performance rather than testing hypotheses.
therefore issues such as vif goodness of fit variable significance etc.
were not such a concern and following the suggestion of menzies et al.
we simply use all the available variables.d.
code metrics we used understand from scitools5to compute code metrics.
all code metrics are listed in table iii.
our metrics set includes complexity metrics such as cyclomatic complexity essential complexity number of distinct paths fan in fan out etc.
v olume metrics such as lines of code executable code comment to code ratio declarative statement etc.
and object oriented metrics such as number of base classes number of children depth of inheritance tree etc.
space limitations inhibit a detailed description however these metrics are well documented at the u nderstand website6.
most metrics are file level some metrics e.g.oo metrics are class level.
since all projects are java based most files contain a single class so we aggregate class level metrics to file level using max.
similarly some metrics are available only at method level such as fan in fan out and we aggregate those at file level using min max andmean .
e. evaluation all of our prediction models output probabilities of defect proneness of files.
to classify a file as defective one can use varying minimum thresholds on the probability value.
most models are fallible thus different choices of threshold will give varying rates of false positives negatives fp fn and true classifications tp tn accuracy of the model is the proportion of correct predictions and is defined astp tn tp fp tn fn.
ma et al.
noted that in a highly class imbalanced data set with very few defective entities accuracy is not a useful performance measure even a poor model that declares all files as clean will have high accuracy.
precision measures the percentage of model declared defective entities that are actually defective.
precision is defined as tp tp fp.
low precision models would waste precious qualitycontrol resources.
recall identifies the proportion of actually defective entities that the model can successfully identify.
model with low recall 100755025100755025percent of locpercent of bugs foundopr 755025100755025percent of locpercent of bugs found10020 p1p2rfig.
cost effectiveness curve.
on the left o is the optimal r is random and p is a possible practical predictor model.
on the right we have two different models p1 and p2 with the same overall performance but p2 is better when inspecting of the lines or less.
figure quoted from our fse paper would be unable to find most of the defects.
recall is identified astp tp fn.
a good model yields both high precision and high recall .
but it is well known that increasing one often reduces the other hence the f measure .
f measure is the harmonic mean of precision andrecall .
all of these measures require the use of a minimum probability threshold to declare a file defective.
lessermann et al.
decry the use of performance measures that require an arbitrary threshold.
mende et al.
argue that threshold based performance measures make replication difficult.
arisholm et al.
argues that in the context of se considering defect proneness as continuous and prioritizing resources in the order of the predicted defect proneness is more suitable.
this brings us to measures that are threshold invariant.
roc receiver operating characteristic roc is a curve that plots the true positive rates tpr tp tp fn against false positive rates for all possible thresholds between and1.
this threshold invariant measure gives us a d curve which passes through and .
the best possible model would have the curve close to y with and area under the curve auc close to .auc always yields an area of under random guessing.
this enables comparing a given model against random prediction without worrying about arbitrary thresholds or the proportion of defective files.
cost effectiveness the above measures ignore costeffectiveness.
consider a model that accurately predicts defective files but orders those files in increasing order of defect density.
then we might allocate resources e.g.
code inspectors to the least defect dense files.
assuming cost proportional to the size of the file this would be undesirable.
in contrast a costeffective model would not only accurately predict defective files but also order those files in decreasing order of defect density.
analogous to roc we can have a cost effectiveness curve refer to left plot of figure plotting the proportion of defects against proportion of sloc coming from the ordered using predicted defect proneness set of files.
unlike roc however we only consider a small portion of area under the cost effectiveness curve aucec tailored for the resource constraints.
thus during deadlines we may only consider the cost effectiveness for at most sloc while other times we may consider sloc.
based on the choice of the maximum proportion of sloc that a manager may be interested to consider different models may become more or less competitive.
e.g.
in the right plot of figure p2is a better cost effective model up to around sloc after that p1performs better.
in this paper we use auc aucec at10 aucec and aucec sloc to compare the models performance in a threshold invariant manner.
however to give readers a way to compare with existing literature we initially report f measure at 5threshold f50 as used by the influential zimmermann et al.
paper .
to compare process and code metrics we evaluate combinations of metrics.
first we build the model with only process metrics.
next we build the model with just code metrics.
we then build the model with process metrics and size countlinecode in table iii .
size is a very important metric by itself and there is a considerable body of evidence that size is highly correlated with most product metrics including mccabe and halstead metrics .
this is to ensure that we can separate the influence of the combination of size and process metrics from the entire collection of process and code metrics.
finally we build the model with the entire collection of process and code metrics.
following menzies et al.
and others we log transformed all of our process and code metrics.
this transformation significantly improves prediction performance.
iv.
r esults we begin with comparing the performance of process and code metrics in release based prediction settings using auc aucec and f .
rq in release based prediction settings how do the process and code metrics compare to predict defect locations?
figure compares the auc of different types of metrics for four different classifiers.
the metrics are marked in x axis as p process c code s process size and a all process and code metrics combined .
we compared the auc performance for all types of metrics for a given learning technique using wilcoxon tests and corrected the p values using benjamini hochberg bh correction.
we also do the same to compare the performance of different learning techniques for a given set of metrics.
we find that process metrics always perform significantly better than code metrics across all learning techniques with very low p value p .
however process metrics and size together may not perform any better than process metrics alone all p values were insignificant .
combining process and code metrics together also doesn t yielddraft!
logistic j48 svmnaivebayes .
.
.
.
pcsapcsapcsapcsaauc process codeprocess size all a auc performance logistic j48 svmnaivebayes .
.
.
.
.
pcsapcsapcsapcsaf 50 process codeprocess size all b f50performance logistic j48 svmnaivebayes .
.
.
.
pcsapcsapcsapcsaaucec 20 process codeprocess size all c aucec 20performance fig.
performance of different classifiers and metrics any better performance than process metrics alone.
similar to menzies et al.
we find that for code metrics naivebayes works best while j48 and svm may not perform as well.
however logistic regression performs well for all types of metrics and is at a statistical dead heat with naivebayes for code metrics p .
we also present the f50and aucec 20in figure .
again we compare different classifiers and set of metrics and found that process metrics easily outperform code metrics in terms of both f50and aucec p 001in both cases .
particularly similar to arisholm et al.
s findings we found code metrics are less effective for cost effective prediction.
interestingly while naivebayes is the best classifier for code metrics when measured using auc but does worst for aucec p after bh correction against all other classifiers .
in general our results show models using code metrics provide reasonable auc albeit not as good as models using process metrics.
for aucec code metrics don t do much better than random different learning techniques don t help much either.
therefore for brevity we only report results only from logistic regression lr which does well for both process and code metrics indeed lr yields better aucec than naivebayes for code metrics based models.
next we report results on the stability of prediction models as discussed in models with stable prediction performance are more useful in practice specially so in rapidly evolving projects.
rq are process metrics more less stable than code metrics?
we evaluate stability by using all available older releases to predict newer ones.
development activity on a project evolves with time and older releases may be quite different from newer ones this approach is a good test of the prediction stability of a set of metrics.
therefore for this rq rather than predicting an immediately subsequent release we predict all releases in a project following a training release.
process code all .
.
.
.
release differenceauc a auc stability process code all .
.
.
.
release differenceaucec 20 b aucec 20stability fig.
stability of different metrics figure 3a shows the stability of auc when predicting thru releases in the future.
for brevity only results from lr are presented other learning models give similar results.
we see a perceptible downward trend in performance as we train on older releases to predict newer releases.
however sample wilcox tests on all pairs of boxplots viz.
for both consecutive and non consecutive releases for each types of metrics provide no statistical significance of this trend.
we observe similar result in terms of aucec 10and aucec figure 3b for either code or process metrics.
earlier findings of instability were based on a continuous time line .
releases may be capturing more wholesome encapsulations of similar activities than do equal time intervals although some releases may take longer than others all releases may group activities in the same way.
thus inherently release based models may be less susceptible to concept drift .
portability of prediction models across projects may be quite important for some organizations specially newer or rapidly changing organizations.
cross project defect prediction recently attracted quite a bit of attention however to our knowledge none compare the relative merits of different types of metrics in such settings.draft!
process code all .
.
.
.
.
withincrosswithincrosswithincross portabilityauc a auc portability process code all .
.
.
.
withincrosswithincrosswithincross portabilityaucec 20 b aucec 20portability fig.
portability of different metrics rq are process metrics more less portable than code metrics?
figure 4a and 4b compares the portability of models for different sets of metrics in terms of auc and aucec .
performance degrades in cross project settings for both process and code metrics the degradation is statistically significant with low p values less than .
it s also clear that code metrics show a larger decline of performance than process metrics and the notches of code metrics based boxplots are nearly nonoverlapping.
we do see a large number of outliers in process metrics suggesting less portability in some cases.
we observed similar pattern for aucec p value from wilcox test is less than 001for code metrics 024for process metrics and 009for all metrics and aucec p value from wilcox test is less than 001for code metrics 031for process metrics and0 006for all metrics with process metrics showing more portability than code metrics.
our findings so far indicate that code metrics are less stable andless portable than process metrics.
why do code metrics show such high resistance to change ?
we guessed that common code metrics are less responsive to development activities and thus less tied to factors that influence defectproneness.
for example a defect introduced by replacing astrncp call with a strcp wouldn t affect code metrics but would affect process attributes such as the modification time number of active developers ownership etc.. the joint distribution of metrics and defects are estimated and exploited by most learning algorithms relatively change resistant more static metrics might do poorly at tracking changes in defect occurrence.
rq are process metrics more less static than code metrics?
we use the spearman correlation of each metric of every file between two successive releases as a measure of stasis.
we then combine all the correlations in a violin plot for each group of metrics.
this would tell us how similar a group of metrics look in two successive releases.
figure 5a presents the comparison of our spearman correlation stasis measure for .
.
.
.
.
.
.
process codespearman correlations a stasis of metrics .
.
.
.
.
.
.
process code allspearman correlations b stasis of models fig.
comparing stasis of different metrics and models process and code metrics.
as we can see from the figure code metrics are highly correlated and therefore changes very little over releases.
process metrics on the other hand show quite a range of stasis.
while the median value of stasis in process metrics is under the top quartile is over !
to examine this disparity further we ranked the process metrics using the median value of our stasis measure for that metric.
the ranking of metrics in decreasing order of stasis was oexp exp own minor ddev sctr comm adev nsctr ncomm nddev nadev del and add.
interestingly the top oexp thru ddev have a very high median stasis score of over 93while the rest 9have a median stasis score of under .
this disparity between mean and topquartile prompted us to examine in more detail the effects of process metrics stasis on predictive power.
there are 5metrics with high stasis and 9that show low stasis.
we hypothesized that stasis is important for predictive power and that low stasis measures make better predictors.
for comparison purpose we chose all 5high stasis process metrics and chose different groups of 5from the low stasis process metrics.
we used a sliding window based approach to select 5groups of 5low stasis metrics the 5groups were the metrics ranked by stasis of low to high correlations and then those ranked all the way up to .
we then built prediction models from all groups of and additionally the high stasis metrics group of ranks .
this gave us a total of 6models 5based on low stasis metrics and based on high stasis metrics.
we found that the median auc of models built from metrics with low correlations are over while the auc of models built from highly correlated process metrics are barely around .
a paired wilcox test of each of the models of low stasis metrics against the model from high stasis metrics suggests that models of low stasis metrics are statistically significantly better predictors than models with high stasis metrics for all performance criteria auc aucec aucec and f with very low p values less than .
this case study above suggests that stasis plays an important role in predictive power.
we attempted to replicate this study for code metrics comparing high stasis and low stasis code metrics but unfortunately code metrics generally have very high stasis around .
this suggests that models baseddraft!
recurring training only test only .
.
.
.
.
pcapcapcaaverage normalized rank a ability to rank files according to the change of defect proneness .
.
.
.
.
.
.
.
.
.
log sloc defect density recurring test only b sloc and defect density for change of defect proneness fig.
ranking ability log sloc and defect density for change of defect pronenes on code metrics would also have high stasis these models would repeatedly predict the same files over and over again as defective.
prediction models learn the joint distribution of metrics and defect proneness from the training release and use this knowledge to predict defects in the test release.
the probability assigned by models to files in the test release reflect this learned defect distribution.
in an active project as different parts of the system become foci of attention and activity defects gets introduced into different parts.
there is thus every reason to expect that the defect distribution in subsequent releases should differ file fthat was buggy in release kisn t necessarily also buggy in k and vice versa.
we would therefore expect that a good model wouldn t stagnate viz.
it would generally indicate a change of defect probability in files from release to release.
rq do models built from different sets of metrics stagnate across releases?
the rank correlation between the predicted probabilities from test set and the learned probabilities from training set is one measure on the model s adaptation in an active project.
a high rank correlation suggests that the model is probably predicting the same set of files as defective.
figure 5b shows the value range of spearman correlations between probabilities of defect proneness across all pairs of training test releases.
the difference is stark the code metrics based models are essentially spitting out the original probabilities it learned from the training data.
this clearly indicates that the stasis of code metrics leads to stagnant prediction models that predict the same files as defective over and over again.
to be fair even stagnant models might be useful in a project where same set of files become recurringly defective identifying a critical set of recurringly defective files unambiguously might still help focus testing and inspection.rq do stagnant models based on stagnant metrics tend to predict recurringly defective entities?
to evaluate the effect of stagnation we partition the set of files in three sets based on how much their defect proneness changes.
included in set are files which are defective in both training and test these recurring files should be easy prey for stagnant models.
in contrast set files which are defective in the training set but notin the test set are bait which might trap a stagnant model into a false positive prediction.
finally files which are defective in the test set but not in the training set are decoys which might mislead a stagnant model into a false negative prediction.
for each type of metrics and every test release we rank the files in that release by their predicted defect proneness.
we then normalize these ranks using the maximum possible rank which is the number of files in the associated test release and partition the normalized ranks in three sets as discussed above.
we can then compare the averaged normalized ranks averaged by the size of the partition across different partitions for different test releases and metrics types.
normalization allows comparison between releases with varying numbers of files.
a better model produces a higher normalized ranks for defective files.
figure 6a compares average normalized ranks for different types of metrics for different defect occurrences.
as we can see from the figure even for just the recurringly defective files left plot process metrics outperform code metrics.
process metrics also middle plot avoid ranking training only defective files higher thus better at avoiding such false positives.
finally process metrics can sniff out the files with newly introduced defects test only better.
we compared the average normalized rank of the models from each pair of metrics using wilcoxon test and corrected the p values using bh correction.
the p values confirm statistical significance of the superiority of process metrics over code metrics.
process metrics were always better lower normalized rank for training only and higher normalized rank for other two cases than code metrics p .
process metrics also outperformed all metrics with lower training only ranking p .
this suggests that in all three defect occurrence scenarios process metrics are better suited for prediction models.
all our findings clearly indicate that process metrics are superior to code metrics for building prediction models.
still code metrics are easy to use multiple tools support easy gathering of code metrics.
their auc around performance though inferior to process metrics is not bad.
if process metrics are hard to use should one use code metrics instead?
prior work by arisholm et al.
in a cross validation evaluation suggests that in a cost constrained setting code metrics are not as useful and indeed when measured with aucec don t do much better than random.
we supplement arisholm et al.
with evaluation in a prediction setting to understand why code metrics perform so poorly in terms ofaucec while giving reasonable auc.
as both auc anddraft!aucec rely on the ordering of defective entities based on the predicted defect probabilities we conjecture that code metrics based models are clearly prioritizing less defect dense files.
furthermore our findings suggest that code metrics models are fairly good at predicting recurringly defective files.
these two pieces of evidence suggests that code metrics tend to predict recurringly defective but not very defect dense files .
question are recurringly defective files larger and less defect dense thereby rendering the models with prediction bias towards such files less cost effective?
figure 6b compares the log sloc and defect density of files that were recurringly defective defective in both training and test release with the files that only became defective in the test release.
as is evident from the figure larger files are less defect dense and more likely to stay defective in subsequent releases.
we also confirmed the statistical significance of this phenomenon using wilcoxon test with bh correction.
recurringly defective files are statistically significantly larger p than files that are only defective in the test set.
at the same time files that are defective only in the test set have statistically significantly higher defect density p .
given our observed prediction bias of code metrics based models towards recurringly defective files such models would be at a disadvantage to predict cost effectively.
moreover the inability of code metrics based prediction models to predict newly introduced defects which may be more defect dense would only worsen the cost effectiveness of such models.
v. t hreats to validity data quality we use a large selection of projects from different domains.
all projects use a high fidelity process to link bug fixing commits to issues in jira issue tracking system.
our projects have a median defect linking rate of over which is much higher than reported typically under in the literature .
completeness of code metrics we only used code metrics as available from scitool s popular understand tool.
understand does not generate all possible code metrics ever reported or used in literature.
however it does produce a large set of diverse code metrics.
furthermore the auc of our code metrics based models are similar or better than that reported by arisholm et al.
.
our comparisons of process and code metrics based models have large effect sizes thus our results appear fairly robust to the choice of code metrics.
completeness of process metrics our set of process metrics are easily obtained and based on a single release.
we used a diverse set of process metrics widely used in the literature ranging from code ownership and developer experience to file activities.
our neighborhood based metrics are motivated by highly cited research of kim et al.
.
our location of change metric sctr measures simple change entropy .
we therefore argue that our set of process metrics are comprehensive.
stability analysis we study stability in the context of releases instead of a continuously shifting time window likeekanayake et al.
.
we argue that each release is a self contained logically defined epoch more easily comparable to other releases furthermore many budgeting resource decisions are based on a release granularity.
thus we believe this is a suitable granuarity for evaluating prediction models.
generalizability we use large number of projects consisting of85releases.
we observed small variances in all of our findings.
to avoid ecological fallacy we also compared our findings in a per project setting and got very similar results.
therefore we believe our result should be generalizable in similar application domains and development dynamics in a release oriented prediction context.
however all of our projects are developed in java and are oss.
there is a definite threat to generalizability based on the fact that all are apache projects however we believe this threat is ameliorated by the diversity of the chosen projects.
also our findings may be less generalizable for commercial projects which have a completely different governance style and may demonstrate different influence of process metrics on defect proneness of files.
therefore we hope other researchers with access to commercial data would replicate our findings.
vi.
c onclusion we studied the efficacy of process and code metrics for defect prediction in a release oriented setting across a large number of releases from a diverse set of projects.
we compared models from different types of metrics using both costsensitive and auc based evaluation across different objectives of performance stability and portability to understand when a set of metrics may be suitable for an organization.
our results strongly suggest the use of process metrics instead of code metrics.
we also try to understand why a set of metrics may predict a type of defect occurrence by focusing on the stasis of metrics.
our findings surprisingly show that code metrics which is widely used in the literature may not evolve with the changing distribution of defects which leads code metric based prediction models stagnating and tending to focus on files which are recurringly defective.
finally we observed that such recurringly defective files are larger and less defect dense therefore these large files may compromise the cost effectiveness of the stagnant code metric based models with a prediction bias towards such files.