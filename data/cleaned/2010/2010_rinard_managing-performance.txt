mit open access articles managing performance vs. accuracy trade offs with loop perforation the mit faculty has made this article openly available.
please share how this access benefits you.
your story matters.
citation stelios sidiroglou douskos sasa misailovic henry hoffmann and martin rinard.
.
managing performance vs. accuracy trade offs with loop perforation.
in proceedings of the 19th acm sigsoft symposium and the 13th european conference on foundations of software engineering esec fse .
acm new york ny usa .
as published publisher association for computing machinery acm persistent url version author s final manuscript final author s manuscript post peer review without publisher s formatting or copy editing terms of use creative commons attribution noncommercial share alike .
managing performance vs. accuracy trade offs with loop perforation stelios sidiroglou sasa misailovic henry hoffmann martin rinard computer science and artificial intelligence laboratory massachusetts institute of technology stelios misailo hank rinard csail.mit.edu abstract many modern computations such as video and audio encoders monte carlo simulations and machine learning algorithms are designed to trade o ffaccuracy in return for increased performance.
to date such computations typically use ad hoc domain specific techniques developed specifically for the computation at hand.
loop perforation provides a general technique to trade accuracy for performance by transforming loops to execute a subset of their iterations.
a criticality testing phase filters out critical loops whose perforation produces unacceptable behavior to identifytunable loops whose perforation produces more e fficient and still acceptably accurate computations .
a perforation space exploration algorithm perforates combinations of tunable loops to find pareto optimal perforation policies.
our results indicate that for a range of applications this approach typically delivers performance increases of over a factor of two and up to a factor of seven while changing the result that the application produces by less than .
categories and subject descriptors c. reliability availability and serviceability general terms performance reliability experimentation keywords profiling loop perforation quality of service .
introduction many computations are designed to produce approximate results.
lossy video encoders for example are designed to give up perfect fidelity in return for faster encoding and smaller encoded videos .
machine learning algorithms are designed to produce probabilistic models that capture some but not all aspects of phenomena that are di fficult if not impossible to model with complete accuracy .
monte carlo computations use random simulation to deliver inherently approximate solutions to complex systems of equations that are in many cases computationally infeasible to solve exactly .
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
esec fse september szeged hungary.
copyright acm ... .
.this paper presents and evaluates a technique loop perforation for generating new variants of computations that produce approximate results.
these variants occupy di fferent points in the underlying performance vs. accuracy tradeo ffspace.
our results show that perforating appropriately selected loops can produce significant performance gains up to a factor of seven reduction in overall execution time in return for small less than accuracy losses.
our results also show that the generated variants occupy a broad region of points within the tradeo ffspace giving users and systems significant flexibility in choosing a variant that best satisfies their needs in the current usage context.
.
loop perforation loop perforation transforms loops to execute a subset of their iterations.
the goal is to reduce the amount of computational work and therefore the amount of time and or other resources such as power that the computation requires to produce its result.
of course perforation may and in our experience almost always does cause the computation to produce a di fferent result.
but approximate computations can often tolerate such changes as long as they do not unacceptably reduce the accuracy.
our implemented system uses the following techniques to find e ffective perforations criticality testing it perforates each loop in turn executing the perforated computation on representative inputs to filter outcritical loops whose perforation causes the computation to produce unacceptable results crash increase its execution time or execute with a memory error.
filtering out such critical loops enables the subsequent perforation space exploration algorithm to focus on the remaining tunable loops that respond well to loop perforation.
perforation space exploration it explores the space of variants generated by perforating combinations of tunable loops together.
our implemented system supports both exhaustive and greedy search algorithms.
the result of the exploration is a set of pareto optimal variants each of which maximizes performance for a specific accuracy loss bound when run on representative inputs.
.
experimental results we evaluate our implemented system on applications from the parsec benchmark suite .
our results show that the perforation space exploration algorithm can find perforations that deliver significant performance increases for five of the seven applications.
specifically our performance measurements show that the perforated applications can run as much as seven times faster than the original applications while producing outputs that di ffer by less than from the corresponding outputs of the original applications.
our results also show that the perforation space explo ration algorithm finds e ffective perforations our evaluation of the source code of the applications indicates that the final perforations are appropriate for a wide range of inputs and not just the training and production inputs used in the experimental evaluation .
.
scope applications that interact well with loop perforation have some flexibility to change the result that they produce subject of course to accuracy requirements .
they also contain some underlying source of redundancy that enables them to produce an acceptable result even after discarding parts of their computation.
applications that process sensory data such as video audio and images often have both the redundancy and the flexibility required for loop perforation to work well.
other classes of applications include monte carlo simulations information retrieval and machine learning computations and the wide variety of scientific and economics computations for which the important consideration is producing an output within an acceptable precision range.
our results show that loop perforation can often improve the performance of all of these kinds of applications while preserving acceptable accuracy.
we acknowledge that many computations for example compilers databases and operating systems may have hard logical correctness requirements that leave them with little or none of this kind of flexibility.
we therefore do not claim that loop perforation is a universally applicable technique.
our experimental results show however that when the computation is amenable to loop perforation and many of our benchmark applications are loop perforation can be surprisingly e ffective at improving the performance while maintaining acceptable accuracy.
.
why loop perforation works we have identified local computational patterns that interact well with loop perforation .
examples include the sum pattern which computes the sum of a set of numbers and the argmin pattern which computes the index and value of a minimum array element .
an analysis of these patterns delivers a probabilistic guarantee that under appropriate assumptions the perforated computation is highly likely to produce a result that is close to the result that the original computation would have produced .
while many of the perforated loops in our benchmark applications are instances of these patterns they are also embedded as components within a larger application.
because the probabilistic analysis does not address how the e ffect of the perforation propagates through the application as a whole which may either amplify or dampen it it provides at best only a partial explanation for why it is acceptable to perforate these loops.
in this paper we identify global computational patterns that interact well with loop perforation.
analyzing the interaction between these patterns and the local perforated computations we can understand why loop perforation works well for the application as a whole and not just for the local computations embedded within the application .
we identify the following global patterns see section7for more details search space enumeration the application iterates over a search space of items.
the perforated computation skips some of the items returning one of the items from the remaining part of the search space.
search metric the application uses a search metric to drive a search for the most desirable item from a set of items selection aselection metric quantifies the desirability of each item encountered during the search.
filtering afiltering metric determines if the search should remove the item from a set of active items.
termination atermination metric determines if the search should terminate either because an acceptable item has been found or because the likelihood of finding a more desirable item appears to be small.
perforating a search metric computation produces a new less accurate but more e fficiently computable metric.
the e ffect is that the perforated search may return a less desirable but still acceptable item.
we note that some applications use the same metric for more than one purpose.
in this case the metric is a combined selection filtering and or termination metric.
monte carlo simulation the application performs a montecarlo simulation.
the perforated computation evaluates fewer samples to produce a potentially less accurate result more efficiently.
iterative improvement the application repeatedly improves an approximate result to obtain a more accurate result.
the perforated computation performs fewer improvement steps to produce a potentially less accurate result more e fficiently.
data structure update the application traverses a data structure updating elements of the data structure with computed values.
the perforated computation skips some of the elements leaving the previous values in the skipped elements.
successful perforations exploit computations that are partially redundant at both the local loop and global application level.
this redundancy often appears when computations process multiple items to obtain a single result.
for example a computation that searches a set to find the most desirable item is partially redundant when the set contains similar items searching only a subset of the items is likely to produce an item that is close to the most desirable item in the set.
we note that many of our applications perform inherently approximate computations for example because they work with approximate models of reality or because exact solutions are too expensive to compute.
in such cases perforation may make an already approximate computation even more approximate.
.
uses of loop perforation potential uses of loop perforation include performance enhancement guided by the parameters of the tradeo ffspace a user can select a desired level of performance that provides acceptable accuracy.
or the user can select a desired accuracy then use loop perforation to maximize the delivered performance.
energy savings because loop perforation can reduce the computation required to produce an acceptable result and computation translates directly into energy consumption loop perforation can reduce the energy required to produce the result.
moreover for real time computations loop perforation may free up the time required to enable additional synergistic energy saving optimizations such as voltage scaling .
new platforms or contexts applications often come tuned for use in a specific context or computational platform for example desktop machines .
loop perforation can support effective redeployment onto a new platform for example mobile devices or into a new context with di fferent performance or accuracy requirements.
dynamic adaptation our implemented compiler can generate code that can dynamically move between di fferent variants as the computation executes .
this capability enables the construction of control systems that use loop perfo ration to dynamically adapt application behavior to satisfy real time deadlines in the face of changes such as clock speed changes load variations or processor loss in the underlying computational platform .
developer insight a developer can examine the perforation to gain insight into where it is possible to trade accuracy for increased performance .
the developer may then choose to accept the perforated computation as is use the perforated computation as a starting point for further optimization or use the perforation to identify computations that are suitable targets for manual optimization.
.
contributions this paper makes the following contributions loop perforation it presents and evaluates loop perforation as an automatic technique for generating multiple variants of approximate computations with the di fferent variants occupying di fferent points in the underlying performance vs. accuracy tradeo ffspace.
criticality testing it presents a criticality testing algorithm that identifies tunable loops that respond well to perforation.
perforation space exploration it presents and evaluates exhaustive and greedy algorithms for exploring the perforation space to find a set of pareto optimal perforations.
experimental results it presents experimental results for applications from the parsec benchmark suite.
these results show that loop perforation can deliver significant performance improvements the perforated applications run up to seven times faster than the original applications while maintaining accurate execution the results from the perforated applications di ffer by at most from the original results .
global patterns it identifies and analyzes global computational patterns for example loops that combine multiple partially redundant values and searches driven by perforatable metrics that interact well with loop perforation.
these patterns explain why loop perforation works well for our benchmark applications and can serve as a foundation for the broader application of loop perforation across a large range of applications.
we have previously proposed the use of loop perforation for quality of service profiling to help developers find suitable manual optimization targets .
this paper presents a new criticality testing methodology for identifying perforatable loops a new methodology for exploring a perforation space with multiple loop perforation rates experimental results that characterize the induced performance vs. accuracy tradeo ffspace for a broad range of perforation rates and inputs including results that characterize how well results from training input generalize to previously unseen production inputs and an identification of specific global computational patterns that work well with loop perforation including an explanation of why loop perforation is applicable to these patterns.
together these results support the use of loop perforation as an optimization in its own right rather than simply as a profiling technique.
.
perforation transformation we implement loop perforation within the llvm compiler framework .
the perforator works with any loop that the existing llvm loop canonicalization pass loop simplify can convert into the following form for i i b i ... in this form the loop has an unique induction variable in the code above i initialized to 0and incremented by 1on every iteration with the loop terminating when the induction variable i exceeds the bound in the code above b .
the class of loops that llvm can convert into this form includes for example forloops that initialize an induction variable to an arbitrary initial value increment the induction variable by an arbitrary constant value on each iteration and terminate when the induction variable exceeds an arbitrary bound.
the loop perforation transformation takes as a parameter a loop perforation rate r which represents the expected percentage of loop iterations to skip.
interleaving perforation transforms the loop to perform every n th iteration here the perforation rate is r n .
conceptually the perforated computation looks like see for a more detailed treatment for i i b i n ... all of the experimental results in this paper use interleaving perforation.
our implemented perforator can also apply truncation perforation which skips a contiguous sequence of iterations at either the beginning or the end of the loop or random perforation which randomly skips loop iterations .
.
accuracy metric the accuracy metric measures the di fference between an output from the original program and a corresponding output from the perforated program run on the same input.
we decompose the metric into two parts an output abstraction which maps an output to a number or set of numbers and an accuracy calculation which measures the di fference between the output abstractions from the original and perforated executions.
the output abstraction typically selects relevant numbers from an output file or files or computes a measure such as peak signal to noise ratio of the quality of the output.
many approximate computations come with such quality measures already defined and available see section .
the accuracy calculation computes the metric accas a weighted mean scaled di fference between the output abstraction components o1 .
.
.
omfrom the original program and the output abstraction components o1 .
.
.
omfrom the perforated program acc mm summationdisplay i 1wi vextendsingle vextendsingle vextendsingle vextendsingle vextendsingleoi oi oi vextendsingle vextendsingle vextendsingle vextendsingle vextendsingle here each weight wicaptures the relative importance of the ith component of the output abstraction.
note that the closer the accuracy metric accis to zero the more accurate the perforated program.
we often report accuracy metrics as percentages.
.
perforation exploration the loop perforation space exploration algorithm takes as input an application an accuracy metric for that application a set of training inputs an accuracy bound b a maximum acceptable accuracy metric for the application and a set of perforation rates in our experiments .
.
.
and iteration .
the algorithm produces a set sof loops to perforate at specified perforation rates.
.
criticality testing the criticalility testing algorithm algorithm starts with a set of candidate loops land perforation rates r.lconsists of the loops identified by profiling that account for at least of the executed instructions with a cuto ffafter the top loops .
in general perforating a candidate loop may cause the program to crash generate unacceptable output produce an infinite loop or decrease itsalgorithm criticality testing find the set of tunable loops pin agiven training inputs tand accuracy bound b inputs a an application t a set of representative inputs b an accuracy bound l a set of candidate loops for perforation r a set of perforation rates outputs p a set of tunable loops and perforation rates for a given bandt p for angbracketleftl r angbracketright l rdo leta angbracketleftl r angbracketrightbeawith lperforated at rate r fort tdo run a angbracketleftl r angbracketrightont record speedup sptand accuracy acct sp summationtext t tspt t acc summationtext t tacct t ifacc b sp 1then fort tdo run a angbracketleftl r angbracketrightusing valgrind to find err t memory errors if uniontext t tet then p p angbracketleftl r angbracketright return p performance.
algorithm 1is designed to find and remove such critical loops from the set of candidate loops.
the algorithm perforates each loop in turn at each of the specified perforation rates then runs the perforated program on the training inputs.
it filters out a loop if its perforation fails to improve the performance as measured by the speedup sp which is the execution time of the perforated application divided by the execution time of the original unperforated program running on the same input causes the application to exceed the accuracy bound bor introduces memory errors such as out of bounds reads or writes reads to uninitialized memory memory leaks double frees etc.
.
if the memory error causes the execution to crash on some input t its accuracy loss acc t is .
the result of criticality testing is the set of tunable loops p angbracketleftl0 r0 angbracketright .
.
.
angbracketleftln rn angbracketright where angbracketleftli ri angbracketrightspecifies the perforation of loop liat rate ri.
.
perforation space exploration algorithms we next present two algorithms exhaustive and greedy for exploring the performance vs. accuracy tradeo ffspace of perforated programs.
exhaustive exploration algorithm.
the exhaustive perforation space exploration algorithm starts with the set of tunable loops then exhaustively explores all combinations of tunable loops lat their specified perforation rates r. the algorithm executes all combinations on all training inputs and records the resulting speedup and accuracy.
it also runs each combination under valgrind discarding the combination if valgrind detects a memory error.
we use the results to compute the set of pareto optimal perforations in the induced performance vs. accuracy tradeo ffspace.
a perforation is pareto optimal if there is no other perforation that provides both better performance and better accuracy.
this approach is feasible for applications such as those in our set of benchmarks that spend most of their time in relatively few loops.
if exhaustive exploration is infeasible it is possible to either use the greedy algorithm or drop enough of the least timeconsuming tunable loops to make exhaustive exploration feasible.
hybrid approaches are also possible.
greedy exploration algorithm.
algorithm 2uses a greedy strategy to search the loop perforation space to produce for a given accuracy bound b a set of loops and corresponding perforation ratesalgorithm greedy exploration find a set sof loops to perforate in agiven training inputs tand accuracy bound b inputs a an application t a set of representative inputs b an accuracy bound p a set of tunable loops generated by algorithm outputs s a set of loops to perforate in agiven b p prime angbracketleftl r angbracketright angbracketleftl r angbracketright p p score angbracketleftl r angbracketright score angbracketleftl p angbracketright s for angbracketleftl r angbracketright p primein sorted order according to score angbracketleftl p angbracketrightdo c s angbracketleftl r angbracketright letacbeawith all loops in cperforated fort tdo run acont record speedup sptand accuracy acct sp summationtext t tspt t acc summationtext t tacct t ifacc b sp 1then fort tdo run acusing valgrind to find err t memory errors if uniontext t tet then s c return s s angbracketleftl0 r0 angbracketright .
.
.
angbracketleftln rn angbracketright that maximize performance subject to b. the algorithm uses a heuristic scoring metric to prioritize loop perforation rate pairs.
the scoring metric for a pair angbracketleftl r angbracketrightis based on the harmonic mean of terms that estimate the performance increase and accuracy loss of the perforated program score angbracketleftl r angbracketright sp angbracketleftl r angbracketright acc angbracketleftl r angbracketright b where sp angbracketleftl r angbracketrightandacc angbracketleftl r angbracketrightare the mean speedup and accuracy metric respectively for the angbracketleftl r angbracketrightperforation over all training inputs and bis the accuracy bound.
in comparison to other heuristic functions based on arithmetic mean or geometric mean this harmonic mean based metric requires a much higher performance increase to select the loop that causes a small increase in accuracy loss.
the algorithm first computes a set of pairs p prime.
for each tunable loop l p primecontains the pair angbracketleftl r angbracketright where rmaximizes score angbracketleftl r angbracketright.
it then sorts the pairs in p primebyscore angbracketleftl r angbracketright.
the algorithm maintains a setsof angbracketleftl r angbracketrightpairs that can be perforated together without violating the accuracy bound b. at each step the algorithm produces a new version of the application with the loops in s angbracketleftl r angbracketrightperforated.
if the additional perforation of angbracketleftl r angbracketrightkeeps the overall accuracy within b the algorithm adds the pair angbracketleftl r angbracketrightintos.
note that for each loop l this algorithm considers only its best perforation rate r according toscore angbracketleftl r angbracketright .
.
benchmarks and inputs we evaluate loop perforation using a set of benchmark applications from the parsec .
benchmark suite .
these applications were chosen to be representative of modern and emerging workloads for the next generation of processor architectures.
we collect all results using a cluster of intel x86 servers with dual .
ghz xeon x5460 quad core processors.
we use the following applications x264 bodytrack swaptions ferret canneal blackscholes and streamcluster.
together these benchmarks represent a broad range of application domains including financial analysis media processing computer vision engineering data mining and similarity search.
for each benchmark we acquire a set of evaluation inputs then pseudorandomly par benchmark training inputs production inputs source x264 hd videos of frames hd videos of frames parsec xiph.org bodytrack sequence of frames sequence of frames parsec additional input provided by benchmark authors swaptions swaptions swaptions parsec randomly generated swaptions ferret image queries image queries parsec canneal netlists of 2m elements netlists of 2m elements parsec additional inputs provided by benchmark authors blackscholes 64k options 10m options parsec streamcluster streams of 19k 100k data points streams of 100k data points uci machine learning repository summary of training and production inputs tition the inputs into training and production inputs.
we use the training inputs to drive the loop perforation space exploration algorithm section and the production inputs to evaluate how well the resulting perforations generalize to unseen inputs.
table 1summarizes the sources of the evaluation inputs.
for each application the parsec benchmark suite contains native inputs designed to represent the inputs that the application is likely to encounter in practical use.
with the exception of streamcluster we always include these inputs in the set of evaluation inputs.
for many of the benchmarks we also include other representative inputs typically to increase the coverage range of the evaluation set to promote an e ffective partition into reasonably sized training and production sets or to compensate for deficiencies in the native parsec inputs.
the parsec benchmark suite also contains the following benchmarks facesim dedup fluidanimate freqmine and vips.
we do not include freqmine and vips because these benchmarks do not successfully compile with the llvm compiler.
we do not include dedup and fluidanimate because these applications produce complex binary output files.
not having deciphered the meaning of these files we were unable to develop meaningful accuracy metrics.
we do not include facesim because it does not produce any output at all except timing information .
x264.
this media application performs h. encoding on raw video data.
it outputs a file containing the raw uncompressed input video encoded according to the h. standard.
the output abstraction for the accuracy metric extracts the peak signal to noise ratio psnr which measures the quality of the encoded video and the bitrate which measures the compression achieved by the encoder .
the accuracy calculation see section weights each component equally with a weight of one .
if the reference decoder fails to parse the encoded video during training we record an accuracy metric of and reject the perforation.
this accuracy metric captures the two most important attributes of a video encoder the quality of the encoded video and the compression achieved through the encoding.
the native parsec input contains only a single video.
we therefore augment the evaluation input set with additional inputs from xiph.org .
bodytrack.
this computer vision application uses an annealed particle filter to track the movement of a human body .
it produces two output files a text file containing a series of vectors representing the positions of the body over time and a series of images graphically depicting the information in the vectors overlaid on the video frames from the cameras.
the output abstraction extracts the vectors that represent the position of the body.
in the accuracy calculation the weight of each vector is proportional to its magnitude.
vectors which represent larger body parts such as the torso therefore have a larger influence on the accuracy metric than vectors that represent smaller body parts such as forearms .
the application requires data collected from carefully calibrated cameras.
the native parsec input contains a single sequence of frames.
we augment the evaluation input set with another sequence of frames.
we obtained this sequence from the maintainers of the parsec benchmark suite.swaptions.
this financial analysis application uses a monte carlo simulation to solve a partial di fferential equation that prices a portfolio of swaptions.
the output abstraction simply extracts the swaption prices.
the distortion calculation weights the corrected swaption prices equally with a weight of one .
the resulting accuracy metric directly captures the ability of the perforated application to produce accurate swaption prices.
each input to this application contains a set of parameters for each swaption.
the native parsec input simply repeats the same parameters multiple times causing the application to repeatedly calculate the same swaption price.
we therefore augment the evaluation input set with additional randomly generated parameters so that the application computes prices for a range of swaptions.
ferret.
this application performs a content based similarity search of an image database.
for each input query image ferret outputs a list of similar images found in its database.
the accuracy metric computes the intersection of the sets of images returned by the perforated and original versions divides the size of this set by the size of the set of images returned by the original version then subtracts this number from one.
so if both versions return images of which are the same the accuracy is .
.
we use the image queries from the parsec benchmark suite.
canneal.
this engineering application uses simulated annealing to minimize the routing cost of a microchip design.
canneal prints a number representing the total routing cost of a netlist representing the chip design we use this cost as the output abstraction.
the resulting accuracy metric directly captures the application s ability to reduce routing costs.
the parsec benchmark contains only one large netlist.
we obtained additional input netlists from the maintainers of the parsec benchmark suite.
blackscholes.
this financial analysis application solves a partial differential equation to compute the price of a portfolio of european options.
the parsec application produces no output.
we therefore modified the application to print the option prices to a file.
the output abstraction extracts these option prices.
the distortion calculation weights these prices equally with a weight of one .
the resulting accuracy metric directly captures the ability of the perforated application to compute accurate option prices.
the native input from the parsec benchmark suite contains million di fferent option parameters.
we do not augment this input set with additional inputs.
streamcluster.
this data mining application solves the online clustering problem.
the program outputs a file containing the cluster centers found for the input data set.
the output abstraction extracts the quality of the clustering as measured by the bcubed metric .
the resulting accuracy metric directly captures the ability of the application to solve the clustering problem.
the native parsec input consists of a randomly generated set of points drawn from a uniform distribution.
for this input all clusterings have equal quality and all clustering algorithms even the trivial algorithm that outputs no clustering at all have identical accuracy.
we therefore use the evaluation input set with more realistic inputs from the uci machine learning repository .
.
experimental results we next present experimental results for the loop perforation space exploration algorithms section for our benchmark applications section .
we use the training inputs to drive the exploration the result is a set of pareto optimal perforations in the loop perforation space.
we then run selected pareto optimal perforations on the production inputs to evaluate how well the training results generalize to previously unseen inputs.
.
criticality testing results table 2presents timing results for the criticality testing runs algorithm .
the table contains a row for each application.
the second column accuracy presents timing results for executions that measure the speedup and accuracy of di fferent perforations.
the third column valgrind presents timing results for executions that use valgrind to find memory errors.
each entry of the form x y indicates that algorithm considered xdifferent combinations of perforated loops and that the executions took a total of yminutes to complete.
the total execution times range from minutes for blackscholes to approximately hours for streamcluster with other applications requiring significantly less time.
it is of course possible to execute di fferent criticality testing runs in parallel on different machines.
we performed all of our runs on eight dual quad intel xenon .
ghz machines so the wall clock times required to perform the runs are approximately a factor of eight less than the total execution times reported in table .
application algorithm total accuracy valgrind time x264 108m 840m 949m bodytrack 35m 1316m 1351m swaptions 7m 108m 115m ferret 17m 53m 71m canneal 405m 540m 945m blackscholes .5m 5m .5m streamcluster 3083m 782m 3865m criticality testing statistics for benchmark applications table 3summarizes for each application the fate of each loop in the criticality testing algorithm from section .
.
each column presents results for a given perforation rate .
.
.
and iteration for an accuracy bound of .
the first row candidate presents the starting number of candidate loops.
this number is always unless the application has fewer than loops that account for of the executed instructions see section .
the second row crash presents the number of loops that algorithm 1filters out because perforating the loop caused the application to terminate with an error.
the third row accuracy presents the number of loops filtered by the algorithm because perforating the loop caused the application to violate the corresponding accuracy bound.
the fourth row speed presents the number of remaining loops that algorithm 1filters out because perforating the loop does not improve the overall performance this typically happens for tight loops at a .
perforation rate .
the fifth row valgrind presents the number of remaining loops that algorithm 1filters out because their perforation introduces a latent memory error detected by the valgrind memcheck tool .
.
perforation space exploration results figures 1through 6present the results of the exhaustive loop perforation space exploration algorithm.
the graphs plot a single point for each explored perforation.
the y coordinate of the point is the mean speedup of the perforation over all training inputs .x264 filter .
.
.
iter candidate crash accuracy speed valgrind remaining bodytrack filter .
.
.
iter candidate crash accuracy speed valgrind remaining swaptions filter .
.
.
iter candidate crash accuracy speed valgrind remaining ferret filter .
.
.
iter candidate crash accuracy speed valgrind remaining canneal filter .
.
.
iter candidate crash accuracy speed valgrind remaining blackscholes filter .
.
.
iter candidate accuracy speed valgrind remaining streamcluster filter .
.
.
iter candidate crash accuracy speed valgrind remaining criticality testing results for individual loops1 x264 exhaustive bodytrack exhaustive swaptions exhaustive ferret exhaustive canneal exhaustive streamcluster exhaustive the x coordinate is the corresponding percentage accuracy loss of the perforation.
green points have accuracy losses below red points have accuracy losses above .
the blue line in each graph connects the points from pareto optimal perforations a perforation is pareto optimal if there is no other perforation that provides both better performance and better accuracy .
the pink triangle identifies the perforation that the greedy algorithm produces.
the graphs show that for these applications loop perforation is usually able to increase performance on the training inputs by at least a factor of two up to a factor of seven for bodytrack while reducing the accuracy by less than ferret is the exception .
the graphs also illustrate the broad range of points in the performance vs. accuracy tradeo ffspace that the loop perforation space exploration algorithm is able to find.
the graphs show that the greedy algorithm is able to find points with good combinations of performance and accuracy but that these points are sometimes less than optimal.
we attribute this lack of optimality in part to the fact that the greedy algorithm explores only one perforation rate for each loop specifically the highest priority rate as identified by equation .
in some cases this equation conservatively ranks perforations with low speedup and high accuracy over perforations with higher speedup but lower accuracy even though the lower accuracy is still within the accuracy bound .
this is the reason for example that the greedy algorithm does not find a better point for canneal figure .
table 4presents the time required to run the exhaustive and greedy exploration algorithms.
there is a row for each application and a column for the exhaustive and greedy algorithms.
each entry of the form x y indicates that algorithm considered xdifferent perforated versions of the application and that the runs took a total of yminutes to complete.
as before the reported times represent the total computation time for execution on a dual quad intel xeon running at .
ghz.
the exhaustive search times range from minute for blackscholes to about hours for streamcluster .
the second column presents the time required to complete theapplication exhaustive greedy x264 665m 9m bodytrack 1968m 33m swaptions 9m 7m ferret 43m 2m canneal 12m 11m blackscholes 1m .5m streamcluster 3941m 31m exhaustive and greedy search times greedy exploration algorithm algorithm .
note that these times do not include the time required to complete the criticality testing algorithm algorithm table 2presents the time required to complete this criticality testing algorithm.
the greedy algorithm explores substantially fewer points than and executes in a fraction of the time of the exhaustive algorithm.
.
training and production results table 5presents accuracy and speedup results for selected paretooptimal perforations in the loop perforation space.
there is a row for each application and a group of columns for the training and production inputs.
each group of columns presents results for the pareto optimal perforation for four accuracy bounds b .
.
and .
each entry of the form x y presents the corresponding mean speedup xand mean accuracy yfor that combination of application bound and input set.
with the exception of ferret all applications show a reasonable correlation between training and production results indicating that the results from the training inputs generalize well to other inputs.
.
perforation evaluation we next discuss for each application our source code based evaluation of the tunable loops in pareto optimal perforations.
table6presents data for every loop which is perforated in at leastapplication training production .
.
.
.
x264 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
bodytrack .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
swaptions .
.
.
.
.
.
.
.
.
.
.
.
ferret .
.
.
.
.
.
.
.
.
.
.
.
.
canneal .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
blackscholes .
.
.
.
.
.
.
.
.
.
.
.
streamcluster .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
training and production results for pareto optimal perforations for varying accuracy bounds x264 function time type x264 mb analyse inter p16x16 .
sse argmin x264 pixel sad 16x16 outer .
sms t sum x264 pixel sad 16x16 inner .
sms t sum x264 me search ref .
sse argmin pixel satd wxh outer .
sms t sum pixel satd wxh inner .
sms t sum bodytrack function time type update .
ii imageerrorinside inner .
sme ratio imageerroredge inner .
sme ratio insideerror outer .
sme sum intersectingcylinders .
smf sse swaptions function time type hjm swaption blocking outer .
mc mean hjm simpath forward blocking outer .
dsu hjm simpath forward blocking inner .
dsu discount factors blocking .
dsu ferret function time type emd .
sms ii lsh query bootstrap outer .
sse lsh query bootstrap middle .
sse lsh query bootstrap inner .
sse canneal function time type reload .
dsu blackscholes function time type bs thread .
streamcluster function time type pfl inner .
ii pgain .
sme t dsu dist .
sme t sum pgain .
sme t patterns in pareto optimal perforations one pareto optimal variant with an accuracy loss under .
the first column presents the function in which the loop appears and when the loop appears in a loop nest whether the loop is an inner loop or outer loop of a loop nest.
the second column presents the percentage of time spent in each loop before perforation .
the third column presents both the global as discussed in section and local as presented in computation patterns for the loop.
for example the entry for the first loop in x264 is sse argmin which indicates that the global pattern for the loop is the search space enumeration pattern while the local pattern is the argmin pattern.
some loops are not an instance of any of the identified local patterns.
in this case we present the global pattern only.
x264.
the x264 encoder divides each frame into blocks then performs the encoding at the granularity of blocks.
motion estimation attempts to find a block from a previously encoded frame that issimilar to the block that x264 is currently attempting to encode.
if it finds such a block x264 encodes the delta over this previously encoded block.
motion estimation consumes the vast majority of the computation time in x264.
all of the tunable loops in x264 appear in the motion estimation computation.
two of these loops x264 mb analyze inter p16x16 andx264 search ref are instances of the search space enumeration global pattern and the argmin local pattern which computes the index and value of the minimum element in an array of elements .
the first x264 mb analyze inter p16x16 searches previously encoded reference frames to find a block that is a good match for the block that x264 is currently encoding.
perforating this loop causes x264 to search fewer reference frames.
the second x264 search ref is given a single reference frame and searches within that frame to find a good match.
perforating this loop causes x264 to consider fewer blocks within the frame.
all of the remaining loops are instances of the search metric for both selection and termination global pattern and the sum local pattern which computes the sum of a set of numbers .
these loops all compute a metric that measures how well the current block matches a previously encoded block.
perforating these loops causes x264 to skip pixels when it computes the metric producing a new less accurate but more e fficiently computable metric.
in addition to using this metric to select the previously encoded block that is the best match x264 also uses the metric to decide when it should terminate the search within a frame x264 terminates the search when it fails to find a new desirable block after a given number of probes .
because the perforated metric makes fewer distinctions between blocks perforation will typically cause the search to terminate with fewer probes.
all of these perforations may cause x264 to choose a less desirable previously encoded block as a starting point for encoding the current block.
but because there is significant redundancy in the set of previously encoded blocks typically many previously encoded blocks are a reasonably good match for the current block the results show that x264 is still usually able to find a good block even after perforation.
bodytrack.
bodytrack uses an annealed particle filter to track the movement of specific parts of a human body head torso arms and legs as a person moves through a scene .
at each input video frame bodytrack starts with a probabilistic model of the position of the body from the previous frame.
this model consists of a weighted set of particles .
each particle consists of a set of angles between body parts that together specify a body pose.
each particle is assigned a weight a likelihood that it accurately represents the current body pose .
the goal of the computation is to compute a new model of the body in the current frame as a weighted average of the particles.
bodytrack starts with the model from the previous frame then refines the model by iterating through multiple annealing layers.
at each annealing layer it processes each particle to create a representation of the body position and location as a set of cylinders each of which represents a specific body part.it then compares this representation to image processing data from the current frame then uses the comparison to update the weight of the corresponding particle.
the update loop performs the annealing steps.
because these steps are designed to improve the accuracy of the model we identify this loop as an instance of the iterative improvement global pattern even though it is possible for an individual step to produce a slightly less accurate model .
perforating this loop causes bodytrack to perform fewer annealing steps and produce a potentially less accurate model more e fficiently.
theimageerrorinside imageerroredge and insideerror loops compute a metric that characterizes how closely the body pose for a given particle matches the observed image data.
these loops select a number of sample points along the edges and interiors of the cylinders that represent the body position.
we identify these loops as instances of the search metric for selection pattern they compute values that measure how closely the particle matches the image data.
two of these loops are instances of the ratio local pattern which computes the ratio of two sums .
the third is an instance of the sum local pattern.
perforating these loops causes bodytrack to consider only a subset of the sample points when it computes the metric.
the result is a more e fficiently computable but potentially less accurate metric.
the intersectingcylinders loop iterates over all pairs of cylinders in a given particle to compute if any of the pairs intersect.
if so bodytrack removes the particle from the model and may potentially replace it with a new particle .
we identify this loop as an instance of the search metric for filtering pattern because it computes a simple metric either the particle is valid or invalid .
we also identify this loop as an instance of the search space enumeration pattern because it enumerates all pairs of cylinders.
perforating this loop causes bodytrack to consider only a subset of the pairs of cylinders.
the result is that bodytrack may consider a particle to be valid even though it represents an unrealistic body pose.
bodytrack will therefore keep the particle in the model instead of filtering it out.
because these particles represent unrealistic positions they will typically be given little or no weight in the model and will therefore have little or no e ffect on the accuracy.
note that this effect may actually decrease the performance although we observed little or no impact on the performance or accuracy in practice .
swaptions.
swaptions uses monte carlo simulation to price a portfolio of swaptions.
each iteration of the hjm swaption blocking loop computes a single monte carlo sample.
each complete execution of this loop computes the price of a single swaption.
perforating this loop causes swaptions to compute each swaption price with fewer monte carlo samples.
because of the way the computation is coded the perforation produces prices that are predictably biased by the perforation rate.
the system therefore uses extrapolation as described in to correct the bias.
thehjm simpath forward blocking loop updates the matrix representing the path of the forward interest rate.
this matrix is later used to calculate the swaption price.
perforating the computation leaves the skipped matrix elements at their initial value of .
the discount factors blocking loop updates a data structure containing discount factors used to compute the price of the swaptions.
perforating the computation leaves the skipped elements at their initial value of .
our results show that both of these perforations have minimal impact on the accuracy of the computation.
ferret.
given a query image ferret performs a content based similarity search to return the nimages in its image database most similar to the query image.
ferret first decomposes the query image into a set of segments extracts a feature vector from each segment indexes its image database to find candidate similar images ranksthe candidate images by measuring the distance between the query image and the candidate images then returns the highest ranked images.
the computation has two main phases.
the first phase uses an efficient hash based technique to retrieve a fixed length double the number of requested images list of likely candidate images from the database.
the second phase performs a more accurate comparison between the retrieved images and the query image.
thelsh query bootstrap loops execute as part of the first phase.
the first two loops iterate over lists of images indexed in selected hash buckets to extract the set of candidate images from the database.
perforating these loops causes ferret to skip some of these images so that they are not considered during the subsequent search these images will therefore not appear in the search result .
we identify these loops as instances of the search space enumeration pattern because they iterate over part of the search space.
the remaining lsh query bootstrap loop finds where to insert the current candidate image in the fixed length sorted list of search results.
perforating this loop may cause the candidate image to appear in the list out of order.
because the second phase further processes the images in this list the final set of retrieved images is presented to the user in sorted order.
the emdloop executes as part of the second phase.
this loop computes the earth mover distance metric between the query image and the current candidate image from the image database.
because ferret uses this metric to select the most desirable images to return we identify the loop as an instance of the search metric for selection pattern.
this metric is also used for the final sorting of the images according to desirability.
interestingly enough this search metric is implemented as an instance of the iterative improvement pattern it improves the distance estimate until it obtains an optimal distance measure or exceeds a maximum number of iterations.
perforating this loop creates a new more e fficient but less accurate metric.
as a result the program may return a di fferent set and or ordering of images to the user.
canneal.
canneal uses simulated annealing to place and route an input netlist with the goal of minimizing the total wire length.
the reload loop traverses the state vector for the mersenne twister random number generator to reinitialize the vector.
for our set of inputs the resulting change in the generated sequence of random numbers causes canneal to execute marginally more e fficiently.
blackscholes.
the experimental results show that it is possible to perforate the outer loop in bs thread without changing the output at all.
further investigation reveals that this loop simply repeats the same computation multiple times and was apparently added to the benchmark to increase the workload.
streamcluster.
streamcluster partitions sets of points into clusters with each cluster centered around one of the points.
each clustering consists of a set of points representing the cluster centers.
the goal is to find a set of cluster centers that minimizes the inter and intra cluster distances.
streamcluster uses a version of the facility location algorithm to find an approximately optimal clustering.
the algorithm contains a while loop that executes a sequence of clustering rounds each of which attempts to improve the clustering from the previous round.
the while loop terminates if a round fails to generate a clustering with significantly improved cost.
the pflloop executes once per round.
each iteration of this loop generates by adding a randomly chosen new candidate cluster center to the current set of cluster centers and evaluates a new candidate clustering.
if this candidate clustering improves on the current clustering the loop body updates the current clustering optionally merging some of the clusters .
we identify this loop as an instance of both the search space enumeration pattern because it iteratesover a part of the search space of candidate clusterings and the iterative improvement pattern because it uses the current clustering to generate improved clusterings .
perforating the pflloop therefore causes streamcluster to consider fewer candidate clusterings per round.
the result is that streamcluster performs fewer attempts to improve the clustering before the next round termination check which may in turn cause streamcluster to produce a less desirable clustering more e fficiently.
we note that the following comment appears in the source code above the definition of the constant iter that controls the number of iterations of the pfl loop higher iter more likely to get correct number of centers higher iter also scales the running time almost linearly this comment reflects the fact that the number of iterations of the pfl loop controls a performance vs. accuracy tradeo ff which is not exposed to the user of the application .
in e ffect the perforation space exploration algorithm rediscovers this tradeo ff.
the first pgain loop calculates the partial cost of a candidate clustering by computing sums of distances between data points and the new cluster center.
it also marks the data points that would be assigned to the new cluster center.
the second pgain loop uses the computed partial sums to compute the total cost of the candidate clustering.
we identify these loops as instances of the search metrics for selection because the computed costs are used to select desirable clusterings and termination because the facility location algorithm uses this cost as a measure of progress and stops if the progress is too small pattern.
perforating these loops produces a less accurate but more e fficiently computable cluster cost metric.
we also identify the first pgain loop as an instance of the data structure update pattern.
perforating this loop may leave some of the data points assigned to an old cluster even though these points should be assigned to the newly opened cluster.
the dist loop computes the distance between two points.
we identify this loop as an instance of the search metric for selection and termination pattern because it is used to compute candidate clustering costs.
it is also an instance of the sum local pattern.
perforating this loop causes streamcluster to compute the distance between points using a subset of the coordinates of the points.
the result is a more e fficiently computable but less accurate distance metric.
.
related work trading accuracy for other benefits is a well known technique.
it has been shown that one can trade accuracy for performance robustness energy consumption fault tolerance and e fficient parallel execution .
we note that developers have for years manually developed algorithms most prominently lossy compression algorithms that are designed to operate at a variety of points in a performance vs. accuracy tradeo ffspace.
in this section we focus on more general techniques that are designed for relatively broad classes of applications.
loop perforation and task skipping.
as we have earlier discussed loop perforation can be seen as a special case of task skipping .
the first publication on task skipping used linear regression to obtain empirical statistical models of the time and accuracy e ffects of skipping tasks and identified the use of these models in purposefully skipping tasks to reduce the amount of resources required to perform the computation while preserving acceptable accuracy .
the first publication on loop perforation presented a purely empirical justification of loop perforation with no formal statistical probabilistic or discrete logical reasoning used to justify the transformation .
the first statistical justification of loop perforation used monte carlo simulation to evaluate the e ffect of perforating local computational patterns .
the first probabilistic justification for loop perforation used static analysis of local computational patterns and also presented the use of profiling runs on representative inputs and developer specifications to obtain the required probability distribution information .
chaudhuri et al.
present a program analysis for automatically determining whether a function is continuous .
the reasoning is deterministic and worst case.
an extension of this research introduces a notion of function robustness and under an input locality condition presents an approximate memoization approach similar to loop perforation .
for a special case when the inputs form a gaussian random walk and the loop body is a robust function the paper derives a probabilistic bound to provide a justification for applying loop perforation.
these previous analyses with the exception of the empirical task skipping analysis all focus on local computations and do not address the e ffect of loop perforation on global end to end application performance and accuracy.
the present paper in contrast provides empirical results that characterize the global performance and accuracy e ffects of loop perforation for our set of benchmarks.
it also identifies a set of global computational patterns and explains why these patterns interact well with loop perforation.
we anticipate that others may be able to identify these or similar patterns in other applications and use these patterns to justify the application of loop perforation to these applications.
multiple implementations.
researchers have developed several systems that allow programmers to provide multiple implementations for a given piece of functionality with di fferent implementations potentially occupying di fferent points in the performance vs. accuracy tradeo ffspace.
such systems include petabricks green eon and powerdial .
petabricks is a parallel language and compiler that developers can use to provide alternate implementations of a given piece of functionality.
green also provides constructs that developers can use to specify alternate implementations.
the alternatives typically exhibit di fferent performance and accuracy characteristics.
petabricks and green both contain algorithms that explore the tradeo ffspace to find points with desirable performance and accuracy characteristics.
eon is a coordination language for power aware computing that enables developers to adapt their algorithms to di fferent energy contexts.
in a similar vein energy aware adaptation for mobile applications adapts to changing system demands by dynamically adjusting application input quality.
for example to save energy the system may switch to a lower quality video input to reduce the computation of the video decoder.
powerdial exploits multiple implementations without requiring the developer to explicitly modify the application .
instead powerdial converts existing command line parameters into data structures that a control system can manipulate to dynamically adapt to changes such as load and clock rate in the underlying computing platform.
in contrast to systems such as petabricks green eon and powerdial loop perforation can find accuracy vs. performance tradeoffs even when none are directly exposed in the application.
instead of requiring the developer to provide multiple implementations of the same functionality or find and annotate potential optimization opportunities our system generates and explores a performance vs. accuracy tradeo ffspace to find multiple potentially desirable points in the tradeo ffspace.
other systems have provided mechanisms that are designed to allow developers to identify loops that performiterative refinement .
loop perforation in contrast can automatically discover instances of a range of computational patterns that include but are not limited to iterative refinement.
autotuners.
autotuners explore a range of equally accurate implementation alternatives to find the alternative or combination of alternatives that deliver the best performance on the current computational platform .
researchers have also developed apis that an application can use to expose variables for external control by for example the operating system .
loop perforation in contrast operates on applications written in standard languages to find perforations which improve performance while maintaining acceptable but not necessarily identical accuracy.
.
conclusion standard approaches for obtaining applications that can trade accuracy in return for enhanced performance have focused on the manual development of new algorithms for specific applications.
our results show that loop perforation can e ffectively augment a range of applications with the ability to operate at various attractive points in the tradeo ffspace with perforated applications often able to deliver significant performance improvements typically around a factor of two reduction in running time at the cost of a small typically or less decrease in the accuracy.
we acknowledge that loop perforation is not appropriate for all applications.
but within its target class of applications results from our implemented loop perforation system show that it can dramatically increase the ability of applications to trade o ffaccuracy in return for other benefits such as increased performance and decreased energy consumption.
.