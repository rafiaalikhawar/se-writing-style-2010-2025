automated oracle creation support or how i learned to stop worrying about fault propagation and love mutation testing matt staats division of web science technology korea advanced institute of science technology staatsm gmail.comgregory gay mats p.e.
heimdahl department of computer science and engineering university of minnesota greg greggay.com heimdahl cs.umn.edu abstract in testing the test oracle is the artifact that determines whether an application under test executes correctly.
the choice of test oracle can significantly impact the effectiveness of the testing process.
however despite the prevalence of tools that support the selection of test inputs little work exists for supporting oracle creation.
in this work we propose a method of supporting test oracle creation.
this method automatically selects the oracle data the set of variables monitored during testing for expected value test oracles.
this approach is based on the use of mutation analysis to rank variables in terms of fault finding effectiveness thus automating the selection of the oracle data.
experiments over four industrial examples demonstrate that our method may be a cost effective approach for producing small effective oracle data with fault finding improvements over current industrial best practice of up to .
observed.
keywords testing test oracles oracle data oracle selection verification i. i ntroduction there are two key artifacts to consider when testing software the test data the inputs given to the application under test and the test oracle which determines if the application executes correctly.
substantial research has focused on supporting the creation of effective test inputs but relatively little attention has been paid to the creation of oracles.
we are interested in the development of automated tools that support the creation of part or all of a test oracle.
of particular interest in our work are expected value test oracles .
consider the following testing process for a software system the tester selects test inputs using some criterion structural coverage random testing engineering judgement etc.
potentially employing automated test generation techniques the tester then defines concrete expected values for these inputs for one or more variables internal state variables or output variables in the program thus creating an expected value test oracle.
the set of variables for which expected values are defined is termed theoracle data set .
our focus is on critical systems past experience with industrial practitioners indicates that expected value test oracles are commonly used in testing such systems.
program mutants test inputs generated externally variable effectiveness ranking oracle datageneratemeasure variable effectiveness run against expected value test oracletesterspecifies expected valuefigure .
supporting expected value test oracle creation we present an approach to automatically select the oracle data for use by expected value oracles.
our motivation is twofold.
first existing research indicates that the choice of oracle has a significant impact on the effectiveness of the testing process .
however current widespread practice is to define expected values for only the outputs a practice that can be suboptimal.
second manually defining expected values is a potentially time consuming and consequently expensive process.
thus in practice the naive solution of monitor everything is not feasible because of the high cost of creating the oracle.
even in situations where an executable software specification can be used as an oracle e.g.
in some model based development scenarios limited visibility into embedded systems or the high cost of logging often make it highly desirable to have the oracle observe only a small subset of all variables.
our goal is to support the creation of test oracles by selecting oracle data such that the fault finding potential of the testing process is maximized with respect to the cost.
we illustrate our proposed approach in figure .
first we generate a collection of mutants from the system under test.
second the test suite generated externally is run against the mutants using the original system as the oracle fully automated back to back testing .
third we measure how often each variable in the system reveals a fault in a mutant and based on this information we rank variable effectiveness in terms of fault finding.
finally we estimate based on this ranking which variables to include in the oracle data for an expected value oracle.
the underlying hypothesis is that as with mutation based test data selection oracle data that is likely to reveal faults in the mutants will also be likely to reveal faults in the actual system under test.
this oracle data selection process is completely automated and requires no manual intervention.
once thisoracle data is selected the tester defines expected values for each element of the oracle data.
testing then commences with a hopefully small and highly effective oracle.
we hypothesize that this oracle creation support process will produce an oracle data set that is more effective at fault finding than an oracle data set selected with a standard approach such as the monitoring of all output variables.
to evaluate our hypothesis we have evaluated our approach using four commercial sub systems from the civil avionics domain.
to our knowledge there are no alternative approaches to oracle data selection discussed in the literature.
to provide an evaluation in the absence of related work we perform a comparison against two common baseline approaches current practice favoring the outputs of the system under test as oracle data and simple random selection of the oracle data set.
in addition we also compare to an idealized scenario where the seeded faults in the mutants are identical to the faults in the actual system under test thus providing an estimate of the maximum fault finding effectiveness we could hope to achieve with any oracle data selection support method.
our results indicate that our approach is generally successful with respect to the baseline approaches performing as well or better in almost all scenarios with best case improvement of .
and consistent improvements in the range.
furthermore we have also found that our approach often performs almost as well as the estimated maximum.
we therefore conclude that our approach may be a cost effective method of supporting the creation of an oracle data set.
ii.
b ackground r elated work in software testing a test oracle is the artifact used to determine whether the software is working correctly .
we are interested in test oracles defined as expected values test oracles that for each test input specify concrete values the system is expected to produce for one or more variables internal state and or output .
during testing the oracle compares the actual values against the expected values1.
we term such oracles expected value oracles .
in our experience with industrial partners such test oracles are commonly used when testing critical software systems.
our goal is to support the selection of the oracle data ororacle data set2.
the oracle data is the subset of internal state variables and outputs for which expected values are specified.
for example an oracle may specify expected values for all of the outputs we term this an output only oracle.
this type of oracle appears to be the most common expected value oracle used in testing critical systems.
other types of test oracles include output base oracles whose 1for our case examples all variables are scalar and cannot be a heap object or pointers.
thus comparison is straightforward.
2oracle data roughly corresponds to richardson et al.
s concept of oracle information .oracle data contain all the outputs followed by some number of internal states and maximum oracles whose oracle data contain allof the outputs and internal state variables.
in the remainder of this paper we will refer to the size of an oracle where sizerefers to the number of variables used in the oracle data set.
larger oracle data sets are generally more powerful than smaller oracle data sets .
this phenomenon is due to fault propagation faults leading to incorrect states do not always propagate and manifest themselves as failures in a variable in the oracle data set.
by using larger oracle data we can improve the likelihood we will detect faults.
while the maximum oracle is always provably the most effective expected value test oracle it is often prohibitively expensive to use.
this is the case when expected values must be manually specified consulting requirements documents as needed a highly labor intensive process or when the cost of monitoring a large oracle data set is prohibitive e.g.
when testing embedded software on the target platform.
in current practice testers must manually select the oracle data set without the aid of automation this process is naturally dependent on the skill of the tester.
we therefore wish to automatically construct small but effective oracle data sets for expected value oracles.
work on test oracles often consists of methods of constructing test oracles from other software engineering artifacts .
in our work we are not constructing the entire test oracle rather we are identifying an effective oracle data set for an expected value oracle.
we are not aware of any work proposing or evaluating alternative methods of selecting the oracle data set.
v oas and miller suggest that the pie approach which like our work relies on a form of mutation analysis could be used to select internal variables for monitoring though evaluation of this idea is lacking.
more recent work has demonstrated how test oracle selection can impact the effectiveness of testing indicating a need for effective oracle selection techniques .
xie and memon explore methods of constructing test oracles specifically for gui systems yielding several recommendations .
briand et al.
demonstrate for object oriented systems that expected value oracles outperform state based invariants with the former detecting faults missed by the latter .
several tools exist for automatically generating invariant based test oracles for use in regression testing including eclat diffgen and work by evans and savoy .
note that these tools do not quantify the potential effectiveness of invariants and thus unlike our approach no prioritization can be performed.
fraser and zeller use mutation testing to generate both test inputs and test oracles for java programs.
the test inputs are generated first followed by generation of postconditions capable of distinguishing the mutants from the program with respect to the test inputs.
unlike our work the end result of their approach is a complete test case with inputs paired with expected results in this case assertions .
such tests being generated from the program under test are guaranteed to pass excepting program crashes .
accordingly the role of the user in their approach differs the user must decide for each input and assertion pair if the program is working correctly.
thus in some sense their approach is more akin to invariant generation than traditional software testing.
the most recent version of this work attempts to parameterize generalize the result of their approach to simplify the user s task .
however this creates the possibility of producing false positives where a resulting parameterized input assertion can indicate faults when none exist .
to .
during evaluation further changing the user s task.
with respect to evaluation no comparisons against baseline methods of automated oracle selection are performed developer tests assertions are compared against but the cost i.e.
number of developer tests assertions is not controlled and thus relative cost effectiveness cannot accurately be assessed.
our work chiefly differs in that we are trying to support creation of a test oracle rather than completely automate it.
the domain and type of oracles generated also differ as does the nature of the test inputs used in our evaluation.
mutation analysis was originally introduced as a method of evaluating the effectiveness of test input selection methods .
subsequent work also explored the use of mutation analysis as a means of generating tests.
in jia and harman summarize both the technical innovations and applications related to mutation testing.
to the best of our knowledge only fraser and zeller have leveraged this to address test oracles.
iii.
o racle data selection our approach for selecting the oracle data set is based on the use of mutation testing for selecting test inputs .
in mutation testing a large set of programs termed mutants are created by seeding various faults either automatically or by hand into a system.
a test input capable of distinguishing the mutant from the original program is said to killthe mutant.
in our work we adopt this approach for oracle creation support.
rather than generate test inputs that kill the mutants however we generate an oracle data set that when used in an expected value oracle and with a fixed set of test inputs kills the mutants.
to accomplish this we perform the following basic steps generate several mutants called the training set from our system under test.
run test inputs provided by the tester over the training set and the original system determining which variables distinguish each mutant from the original system.
process this information to create a list of variables ordered in terms of apparent fault finding effectiveness the the variable ranking .
examine this ranking along with the mutants and test inputs to estimate as x how large the oracle data set should be.
alternatively the tester can specify x based on the testing budget.
select the top xvariables in the ranking as the oracle data.
while conceptually simple there are several relevant parameters that can be varied for each step.
the following subsections will outline these parameters as well as the rationale for the decisions that we have made for each step.
a. mutant generation and test input source during mutation testing mutants are created from an implementation of a system by introducing a single fault into the program.
each fault is created by either inserting a new operator into the system or by replacing an operator or variable with a different operator or variable.
this mutation generation is designed such that all mutants produced are both syntactically and semantically valid no mutant will crash the system under test.
the mutation testing operators used in this experiment are similar to those used by other researchers for example arithmetic relational and boolean operator replacement boolean variable negation constant replacement and delay introduction that is use the stored value of the variable from the previous computational cycle rather than the newly computed value .
a detailed description is available in .
the type of faults used to create mutants may impact the effectiveness of the selected oracle data when used to test the actual system under test.
note that the type of mutants used in the evaluation in this report are similar to those used in where the authors found that generated mutants are a reasonable substitute for actual failures in testing experiments.
this offers evidence that our use of mutation testing will support the creation of oracles useful for real systems.
our approach can be used with any set of test inputs.
in this work we assume the tester is equipped with an existing set of test inputs and wishes to determine what oracle data is likely to be effective with said test inputs.
this assumption allows the numerous existing methods of test input selection to be paired with our approach for oracle data selection.
furthermore this scenario is the most likely within our domain of interest.
b. variable ranking once we have generated mutants we then run the test inputs over both the mutants and the original program.
during execution of these inputs we collect the values for every variable at every step of every test i.e.
the complete state at every point of the execution .
we term this resulting data as the trace data .
a variable is said to have detected a fault when the variable value in the original correct system differs from the variable value produced by a mutant for some test.
we track which mutants are killed by which variables.
note that duplicate detections in which a variables detects the same mutant in multiple tests are not counted.
once we have computed this information we can produce a set of variables ranked according to effectiveness.
one possible method of producing this ranking is simply to order variables by the number of mutants killed.
however the effectiveness of individual variables can be highly correlated.
for example when a variable vais computed using the value of a variable vb ifvbis incorrect for some test input it is highly probable that vais also incorrect.
thus while vaand vbmay be highly effective when used in the oracle data set the combination of both is likely to be only marginally more effective than the use of either alone.
to avoid selecting a set of variables that are individually effective but ineffective as a group we have elected to use a greedy algorithm for solving the set covering problem to produce a ranked set of variables.
in the set covering problem we are given several sets with some elements potentially shared between the sets.
our goal is to select the minimum set of elements such that one element from each set has been selected.
in this problem each set represents a mutant and each element of the set is a variable capable of detecting the mutant for at least one of the test inputs.
calculating the smallest possible set covering is an npcomplete problem .
consequently we employ a wellknown effective greedy algorithm to solve the problem select the element covering the largest number of sets remove from consideration all sets covered by said element and repeat until all sets are covered.
in our case each element removed corresponds to a variable.
these variables are placed in a ranking in the order they were removed.
the resulting ranking can then be used to produce an oracle data set of size n simply select the topnelements of the list.
c. estimating useful oracle data size once we have a calculated the ranked list of variables we can select an oracle data set of size etc.
up to the maximum number of variables in the system with the choice of size likely made according to some testing budget.
in some scenarios the tester may have little guidance as to the appropriate size of the oracle data.
in such a scenario we would like to offer a recommendation to the tester we would like to select an oracle data set such that the size of the set is justifiable i.e.
not so small that potentially useful variables are omitted and not so large that a significant number of variables not adding value are selected.
to accomplish this we determine the fault finding effectiveness of oracle data sets of size etc.
over our training set of mutants.
the fault finding effectiveness of these oracles will increase with the oracle s size but the increases will diminish as the oracle size increases.
consequently it ispossible to define a natural cutoff point for recommending an oracle size if the fault finding improvement between an oracle of size nand sizen 1is less than some threshold we recommend an oracle of size n. in practice establishing a threshold will depend on factors specific to the testing process.
therefore in our evaluation we explore two potential thresholds and .
iv.
e valuation we wish to evaluate if our oracle creation support approach yields effective oracle data sets.
preferably we would directly compare against existing algorithms for selecting oracle data however to the best of our knowledge no such methods exist.
we therefore compare our technique against two potential baseline approaches for oracle data set selection detailed later as well as an idealized version of our own approach.
we wish to explore the following research questions question is our approach more effective in practice than current baseline approaches to oracle data selection?
question what is the maximum potential effectiveness of the mutation based approach and how effective is the realistic application of our approach in comparison?
question how does the choice of test input data impact the effectiveness of our approach?
a. experimental setup overview in this research we have used four industrial systems developed by rockwell collins engineers.
all four systems were modelled using the simulink notation from mathworks inc. and were automatically translated into the lustre synchronous programming language in order to take advantage of existing automation3.
two systems dwm1 anddwm2 represent distinct portions of a display window manager dwm for a commercial cockpit display system.
two other systems vertmax batch andlatctl batch describe the vertical and lateral mode logic for a flight guidance system.
subsyst.s blocks outputs internals dwm1 dwm2 vertmax batch latctl batch table i case example information all four systems represent sizable operational systems.
information related to these systems is provided in table i. for each case example we performed the following steps generated structural test input suites detailed in section iv b below generated training sets section iv c generated evaluation sets section iv c ran test suite 3in practice lustre would then be automatically translated to c code but this is a syntactic transformation.
our usage of lustre is purely for convenience if applied to c the results here would be identical.on mutants section iv e generated oracle rankings section iv d and assessed fault finding ability of each oracle and test suite combination using evaluation sets section iv e .
b. test suite generation as noted previously we assume the tester has an existing set of test inputs.
consequently our approach can be used with any method of test input selection.
since we are studying the effectiveness using avionics systems two structural coverage criteria are likely to be employed branch coverage and mc dc coverage .
several variations of mc dc exist for this study we use masking mc dc as it is frequently used within the avionics community .
we use a counterexample based test generation approach to generate tests satisfying these coverage criteria .
this approach is guaranteed to generate a test suite that achieves the maximum possible coverage.
we have used the nusmv model checker in our experiments for its efficiency and we have found the tests produced to be both simple and short .
counterexample based test generation results in a separate test for each coverage obligation.
this results in a large amount of redundancy in the tests generated as each test likely covers several coverage obligations.
such an unnecessarily large test suite is unlikely to be used in practice.
we therefore reduce each generated test suite while maintaining coverage.
we use a simple randomized greedy algorithm.
we begin by determining the coverage obligations satisfied by each test generated and initialize an empty test set reduced .
we then randomly select a test input from the full set of tests if it satisfies obligations not satisfied by any test input inreduced we add it to reduced .
we continue until all tests have been removed from the full set of tests.
we produce different test suites for each case example coverage criterion to control for the impact of randomization.
c. mutant generation for each case example mutants are created by introducing a single fault into the correct implementation.
we then produce training sets by randomly selecting subsets of mutants.
for each training set the mutants notselected for the training set are used to construct an evaluation set.
we then remove functionally equivalent mutants from each evaluation set resulting in a reduction of mutants.
we remove functionally equivalent mutants using the nusmv model checker.
this is possible due to the nature of the systems in our study each system is finite and thus determining equivalence is decidable and in practice fast4.
the removal of equivalent mutants is done for the evaluation sets as it represents a potential threat to validity 4equivalence checking is fairly routine in the hardware domain a good introduction can be found in .in our evaluation our method is effective only if it detects faults which can impact the external behavior.
note that the removal of equivalent mutants for training sets is possible for the case examples examined and would likely improve the effectiveness of our approach but is notdone as it may not be practical for sufficiently large systems and is generally undecidable for systems that are not finite.
d. oracle data set generation for each set of mutants generated training sets and evaluation sets we generated an oracle ranking using the approach described in section iii.
the rankings produced from training sets reflect how our approach would be used in practice these sets are used in evaluating our research questions.
the rankings produced from evaluation sets represent an idealized testing scenario one in which we already know the faults we are attempting to detect.
rankings generated from the evaluations sets termed idealized rankings hint at the maximum potential effectiveness of our approach and the maximum potential effectiveness of oracle data selection in general and are used to address question .
we limited each ranking to contain mvariables where mis10or twice the number of output variables whichever was larger since test oracles significantly larger than outputonly oracles were deemed unlikely to be used in practice and using larger oracles makes visualizing the more relevant and thus interesting oracle sizes difficult.
to answer questions and we compare against two baseline rankings.
first the output base approach creates rankings by first randomly selecting output variables and then randomly selecting internal state variables.
thus the output base rankings always list the outputs first i.e.
more highly ranked followed by the randomly selected internal state variables.
this ranking was chosen to reflect what appears to be the current approach to oracle data selection select the outputs first and if resources permit select one or more of the internal state variables.
second to provide an unbiased method the random approach creates completely random oracle rankings.
the resulting rankings are simply a random ordering of the internal state and output variables.
e. data collection for each given case example we ran the test suites against each mutant and the original version of the program.
for each execution of the test suite we recorded the value of every internal variable and output at each step of every test using an in house lustre simulator.
note that the time required to produce a single oracle ranking generate mutants run tests and apply the greedy set cover algorithm is quite small less than one hour for every case example.
this raw trace data is then used by our algorithm and our evaluation.
the fault finding effectiveness of an oracle is computed as the percentage of mutants killed versus the number of mutants in the evaluation set used.
we perform this analysisfor each oracle and test suite for every case example and use the information produced by this analysis to evaluate our research questions.
v. r esults d iscussion in this section we discuss our results in the context of our three research questions q1 is our approach more effective than existing baseline approaches for oracle data selection?
q2 what is the maximum potential effectiveness of our mutation based technique and how does the real world effectiveness compare?
and q3 how does the choice of test input data impact the effectiveness of our approach?
.
in figure we plot the median fault finding effectiveness of expected value test oracles for increasing oracle sizes5.
four ranking methods are plotted both baseline rankings our mutation based approach and the idealized mutationbased approach.
median values were used for these plots as plotting all test suite training set combinations using boxplots scatter plots etc.
yields figures that are very difficult to interpret.
for each subfigure we plot the number of outputs as a dashed vertical blue line.
this line represents the size of an output only oracle this is the oracle size that would generally be used in practice.
we also plot the and .
thresholds for recommending oracle sizes as solid orange lines see section iii c .
note that the .
threshold is not always met for the oracle sizes explored.
in table iii we list the median relative improvement in fault finding effectiveness using our proposed oracle data creation approach versus the output base ranking.
in table iv we list the median relative improvement in fault finding effectiveness using the idealized mutation based approach an oracle data set built and evaluated on the same mutants versus our mutation based approach.
as shown in figure random oracle data performs poorly and detailed comparisons were thus deemed less interesting and are omitted.
a. statistical analysis before discussing the implications of our results we would like to first determine which differences observed are statistically significant.
that is we would like to determine with statistical significance at what oracle sizes and for which case examples the idealized performance of a mutation based approach outperforms the real world performance of the mutation based approach and the mutation based approach outperforms the baseline ranking approaches.
we evaluated the statistical significance of our results using a two tailed bootstrap permutation test.
we begin by formulating the following statistical hypotheses6 5for readability we do not state median relative improvement median fault finding etc.
in the text though this what we are referring to.
6as we evaluate each hypothesis for each case example and oracle size we are essentially evaluating a set of statistical hypotheses.h1 for a given oracle size m the idealized approach outperforms the standard mutation based approach.
h2 for a given oracle size m the standard mutationbased approach outperforms the output base approach.
h3 for a given oracle size m the standard mutationbased approach outperforms the random approach.
we then formulate the appropriate null hypotheses h01 for a given oracle size m the fault finding numbers for the idealized approach are drawn from the same distribution as the fault finding numbers for the standard mutation based approach.
h02 for a given oracle size m the fault finding numbers for the standard mutation base approach approach are drawn from the same distribution as the fault finding numbers for the output base approach.
h03 for a given oracle size m the fault finding numbers for the standard mutation base approach approach are drawn from the same distribution as the fault finding numbers for the random approach.
our observations are drawn from an unknown distribution therefore we cannot fit our data to a theoretical probability distribution.
to evaluate our null hypotheses without any assumptions on the distribution of our data we use the twotailed bootstrap paired permutation test a non parametric test with no distribution assumptions with samples with median as the test statistic.
per our experimental design each evaluation set has a paired training set and each training set has paired baseline rankings output base and random .
thus for each case example and coverage criteria combination we can pair each test suite t idealized ranking with t training set ranking for h01 and each test suite t training set ranking with t random or output base ranking h02 h03 .
we then apply the statistical test for each case example coverage criteria and oracle size with .
our results indicate that null hypotheses h01andh03 can be rejected for all combinations of case examples coverage criteria and oracle sizes with p .
we therefore accepth1andh3for all combinations of case examples coverage criteria and oracle sizes.
in the case ofh02 there exist combinations in which the differences are not statistically significant or marginally statistically significant near .
these combinations mostly correspond to points of little to no relative improvements over outputbase oracles.
we list these p values results in table ii8and 7note that we do not generalize across case examples or coverage criteria as the appropriate statistical assumption random selection from the population of case examples and coverage criteria is not met.
furthermore we do not generalize across oracle sizes as it is possible our approach is statistically significant for some sizes but not others.
the statistical tests are employed to where observed differences between oracle data selection methods are unlikely to be due to chance.
8also for latctl batch when using mc dc tests and an oracle of size one h02cannot be rejected as p .branch mc dc oracle size dwm dwm dwm .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
table ii h02p values highlight all results without statistical significance with a in table iii.
p values not specified are .
b. evaluation of practical effectiveness q1 when designing an oracle creation support method the obvious question to ask is is this better than current best practice?
as we can see from figure every oracle generated outperforms the random approach with statistical significance often by a wide margin.
we thus can immediately discard random oracle data selection as a useful method of oracle data selection as both our approach and the outputbase approach outperform it in all scenarios.
we do not discuss the random approach further.
we can also see that for both coverage criteria and every case example nearly every oracle generated for three of four systems outperforms the output base approaches with statistical significance.
the pattern goes as follows for oracles smaller than the output only oracle our approach tends to perform relatively well compared to the outputbase approach with improvements ranging from .
to .
.
this reflects the strength of prioritizing variables we generally select more effective variables for inclusion into the oracle data earlier than the output base approach.
as the test oracle size grows closer in size to the output only oracle the relative improvement decreases but often of case example coverage combinations still outperforms the output only oracle up to .
.
finally as the test oracle grows in size beyond the output only oracle our relative improvement when using our approach again grows with improvements of .
for largest oracles.
this observation is illustrated best using the dwm 1case example.
here we see that for both coverage criteria while the output base ranking method performs relatively well for small oracle sizes the set covering approach nevertheless ranks the most effective variable first locating roughly .
and .
more faults than the variable chosen by the output base technique for the branch and mc dc coverage criteria respectively.
the set covering oracle continues on to select a handful of variables that find additional faults but as additional outputs are added to the output base approachthe relative improvement decreases becoming statistically insignificant in the case of mc dc coverage.
this indicates the wisdom of the current approach but also demonstrates room for improvement for branch coverage the proposed approach is at worst .
better than the output oracle.
for the mc dc approach we show no improvement though we achieve similar fault finding to the output only oracle using smaller oracles.
finally as the oracle grows incorporating by necessity internal state variables our approach fares well as effective internal state variables continue to be added reaching up to .
relative improvement.
the only exception occurs for the dwm 2system.
for this case example although mutation based oracles tend to be roughly equivalent in effectiveness as output base oracles we generally cannot reject h02at only for small or large oracles approximately sizes from the outputonly oracle does our approach do relatively well.
examining the composition of these oracles indicates why the ranking generated using our approach for this case example begins mostly with output variables and thus oracles of generated using our approach are very similar to those generated using the output base approach.
the performance gain of mutation base oracles at small sizes suggest that certain output variables are far more important than others but what is crucially important at all levels up to the total number of outputs is to choose output variables for the oracle .
however in a few instances our approach in fact does worse with statistical significance than the output base approach.
the issue appears to be the greedy set coverage algorithm for dwm the approach tends to select a highly effective internal state variable first which prevents a computationally related output variable from being selected for larger oracle data.
given an optimal set cover algorithm this issue would likely be avoided.
however eventually for larger oracle sizes this issue is corrected with statistically significant improvements of .
and .
observed.
despite this inconsistency it seems clear that our approach can be effective in practice.
we can consistently generate oracle data sets that are effective over different faults generally with higher effectiveness than existing ranking approaches.
in cases where our approach does not do better the difference observed tends to be small if present.
furthermore we can provide the tester with recommendations of the cost effective oracle sizes thus avoiding the need for testers to manually estimate an effective oracle size.
c. potential effectiveness of oracle data selection approach q2 as noted previously there is limited empirical work on test oracle effectiveness.
consequently it is difficult to determine what constitutes effective oracle data selection clearly performing well relative to a baseline approach indicates our approach is effective but it is hard to argue the approach is effective in the absolute sense.
we therefore5 oracle size020406080fault finding effectiveness .
estimate estimateall outputs random output base mutation based idealized mutation based a dwm branch inputs oracle size2030405060708090100fault finding effectiveness .
estimate estimateall outputs random output base mutation based idealized mutation based b dwm mc dc inputs oracle size010203040fault finding effectiveness .
estimate estimate all outputsrandom output base mutation based idealized mutation based c dwm branch inputs oracle size01020304050fault finding effectiveness .
estimate estimate all outputsrandom output base mutation based idealized mutation based d dwm mc dc inputs oracle size010203040506070fault finding effectiveness .
estimate estimate all outputs random output base mutation based idealized mutation based e latctl batch branch inputs oracle size020406080100fault finding effectiveness .
estimate estimate all outputs random output base mutation based idealized mutation based f latctl batch mc dc inputs oracle size010203040506070fault finding effectiveness .
estimate estimate all outputs random output base mutation based idealized mutation based g vertmax batch branch inputs oracle size20406080100fault finding effectiveness .
estimate estimate all outputs random output base mutation based idealized mutation based h vertmax batch mc dc inputs figure .
median effectiveness of various approaches to oracle data selection posed q2 what is the maximum potential effectiveness of a mutation based approach?
to answer this question we applied our approach to the same mutants used to evaluate the oracles in q1 as opposed to generating oracles from a disjoint training set .
this represents an idealized testing scenario in which we already know what faults we are attempting to find and thus is used to estimate the maximum potential of our approach.the results can be seen in figure and table iv.
we can observe from these results that while the potential performance of a mutation based oracle is naturally almost always higher than the real world performance of our method the gap between a realistic implementation of our approach and the ideal scenario is often quite small.
indeed apart from the dwm 2system for most case examples and oracle sizes the difference between the idealized andbranch mc dc oracle size dwm dwm vertmax batch latctl batch dwm dwm vertmax batch latctl batch .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
table iii median relative improvement using mutation based selection over output base selection realistic scenarios is less than the gap between the outputbase approach and our approach.
thus we can conclude that while there is clearly room for improvement in oracle data selection methods our approach appears to often be quite effective in terms of absolute performance.
d. impact of coverage criteria q3 our final question concerns the impact of varying the coverage criteria and thus the test inputs on the relative effectiveness of oracle selection.
in this study we have used two coverage criteria of varying strength.
intuitively it seems likely that when using stronger test suites those satisfying mc dc in this study the potential for improving the testing process via oracle selection would be less as the test inputs should do a better job of exercising the code.
precisely quantifying likeness is difficult however as shown in figure for each case example the general relationship seems to our surprise to be roughly the same.
for example for the dwm 1system we can see that despite the overall higher levels of fault finding when using the mc dc test suites the general relationships between the output base baseline approach our approach and the idealized approach remain similar.
we see a rapid increase in effectiveness for small oracles followed by a decrease in the relative improvement of our approach versus the output base baseline as we approach oracles of size corresponding to an output only oracle followed by a gradual increase in said relative improvement.
in some cases relative improvements are higher for branch coverage latctl batch and in others they are higher for mc dc vertmax batch .
these results indicate that perhaps more that the test inputs used characteristics of the system under test are the primary determinant of the relative effectiveness of our approach.
unfortunately it is unclear exactly what these characteristics are.
testers can of course estimate the effectiveness of our applying approach by automating our study applying essentially the same approach used to provide theuser a suggested oracle size9.
however this is potentially expensive and provides no insight concerning why our approach is very good relative to baseline approaches and in an absolute sense for some systems and merely equivalent for others.
developing metrics that can allow us to a priori estimate the effectiveness of our approach is an area for potential future work.
vi.
t hreats to validity external validity our study is limited to four synchronous reactive critical systems.
nevertheless we believe these systems are representative of the class of systems in which we are interested and our results are therefore generalizable to other systems in the domain.
we have used lustre as our implementation language rather than a more common language such as c or c .
however as noted previously systems written in lustre are similar in style to traditional imperative code produced by code generators used in embedded systems development.
a simple syntactic transformation suffices to translate lustre code to c code that would be generally be used.
in our study we have used test cases generated to satisfy two structural coverage criteria.
these criteria were selected as they are particularly relevant to the domain of interest.
however there exist many methods generating test inputs and it is possible that tests generated according to some other methodology would yield different results.
for example requirements based black box tests may be effective at propagating errors to the output reducing the potential improvements for considering internal state variables.
we therefore avoid generalizing our results to other methods of test input selection and intend to study how our approach generalizes to these methods in future work.
9indeed automating our study to estimate the effectiveness of our approach is akin to applying mutation testing to estimate test input oracle data effectivenessbranch mc dc oracle size dwm dwm vertmax batch latctl batch dwm dwm vertmax batch latctl batch .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
table iv median relative improvement using idealized mutation based selection over realistic mutation based selection the tests used are effectively unit tests for a lustre module and the behavior of oracles may change when using tests designed for large system integration.
given that we are interested in testing systems at this level we believe our experiment represents a sensible testing approach and is applicable to practitioners.
we have generated approximately mutants for each case example with mutants used for training sets and up to mutants used for evaluation.
these values are chosen to yield a reasonable cost for the study.
it is possible the number of mutants is too low.
nevertheless based on past experience we have found results using less than mutants to be representative .
furthermore pilot studies showed that the results change little when using more than this mutants for training or evaluation sets.
construct validity in our study we measure the fault finding of oracles and test suites over seeded faults rather than real faults encountered during development of the software.
given that our approach to selecting oracle data is also based on the mutation testing it is possible that using real faults would lead to different results.
this is especially likely if the fault model used in mutation testing is significantly different than the faults we encounter in practice.
nevertheless as mentioned earlier andrews et al.
have shown that the use of seeded faults leads to conclusions similar to those obtained using real faults in similar fault finding experiments .
vii.
c onclusion and future work in this study we have explored a mutation based method for supporting oracle creation.
our approach automates the selection of oracle data a key component of expected value test oracles.
our results indicate that our approach is successful with respect to alternative approaches for selecting oracle data with improvements up to .
over outputbase oracle data selection with improvements in the range relatively common.
in cases where our approachis not more effective it appears to be comparable to the output base approach.
we have also found that our approach performs within an acceptable range from the theoretical maximum.
while our results are encouraging and demonstrate that our approach can be effective using real world avionics systems there are a number of questions that we would like to explore in the future including estimating training set size our evaluation indicates that for our case examples a reasonable number of mutants are required to produce effective oracle data sets.
however when applying our approach to very large systems or when using a much larger number of test inputs we may wish to estimate the needed training set size.
oracle data selection without test inputs in our approach we assume the tester has already generated a set of test inputs before generating oracle data.
we may wish to generate oracle data without knowing the test inputs that will ultimately be used.
is oracle data generated with one set of test inputs effective for other different test inputs?
viii.
a cknowledgements this work has been partially supported by nasa ames research center cooperative agreement nna06cb21a nsf grants ccf and cns cns1035715 an nsf graduate research fellowship and the wcu world class university program under the national research foundation of korea and funded by the ministry of education science and technology of korea project no r31 .