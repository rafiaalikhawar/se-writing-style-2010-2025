mitigatingtheconfoundingeffectsofprogram dependencesf oreffectivefaultlocalization george k. baah georgia institute of technology atlanta ga baah cc.gatech.eduandy podgurski case western reserve university cleveland oh podgurski case.edumary jean harrold georgia institute of technology atlanta ga harrold cc.gatech.edu abstract dynamic program dependences are recognized as important factors in software debugging because they contribute to triggering the effects of faults and propagating the effects to a program s output.
the effects of dynamic dependences also produce significant confounding bias when statistically estimating the causal effect of a statement on the occurrence of program failures which leads to poor fault localization results.
this paper presents a novel causal inference technique for fault localization that accounts for the effects of dynamic data and control dependences and thus significantly reduces confounding bias during fault localization.
the technique employs a new dependence based causal model together with matching of test executions based on their dynamic dependences.
the paper also presents empirical results indicating that the new technique performs significantly better than existing statistical fault localization techniques as well as our previous fault localization technique based on causalinference methodology.
categories and subject descriptors d. .
software engineering testing and debugging debugging aids diagnostics general terms algorithms experimentation keywords causal inference potential outcome model matching fault localization program analysis debugging .
introduction the pervasive impact of software systems and applications on society requires software developers to ensure that their products are of high quality.
however software development is a human process and developers unintentionally introduce faults into software that cause it to fail under certain conditions.
when software failures are observed developers engage in debugging to find and fix the fault s .
fault localization is the part of debugging that involves locating faults in a program.
because of the size and complexity of many software products fault localization is often a difficult and permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are notmadeordistributedforprofit orcommercialadvantageandthatcopies bearthisnoticeandthefullcitationonthefirstpage.
tocopyotherwise to republish topostonserversortoredistributetolists requirespriorspecific permissionand orafee.
esec fse september5 szeged hungary.
copyright2011acm978 ... .
.time consuming task which contributes significantly to the cost of software development and maintenance.
to reduce the cost of fault localization researchers have proposed automated statistical fault localization techniques e.g.
.
these techniques compute statistical measures of association between the execution of individual program statements or other entities and the occurrence of program failures using execution data from passing and failing executions e.g.
coverage information .
the measures are called suspiciousness scores .
the techniques rank program statements in decreasing order of their suspiciousness scores for examination by developers.
the intuition underlying this approach is that the program statements most strongly associated with failures are the most likely to be faulty.
however statistical association does not necessarily indicate causation .
thus statements with high suspiciousness scores may not be faulty and faulty statements may have low suspiciousness scores.
consequently existing statistical fault localization techniques can perform poorly with certain programs and faults.
in recent work we showed that statistical fault localization techniques based on measures of association between the occurrence of certain runtime events e.g.
coverage of a given statement and the occurrence of program failures are subject to confounding bias because of influences between statements in a program.
for example with a number of coverage based statistical fault localization techniques a correct statement scand a faulty statement sfthat happen to be covered by the same subset of test cases will necessarily receive the same suspiciousness score.
more generally any other statement that influences scandsfand also influences the occurrence of failures will confound the suspiciousness scores of scandsf.
to accurately assess the distinct effects of covering scandsfon the occurrence of failures it is necessary to adjust for such confounding factors.
to address the confounding influence of some program statements on the suspiciousness scores of other statements we presented a preliminary causal inference model for software fault localization see section .
we used this model to estimate the causal effect of covering a statement son the occurrence of program failures.
to control or adjust for confounding by other statements the model incorporates a coverage indicator variable for the forward control dependence predecessor of s in addition to a coverage indicator for sitself .
the model is grounded in the extensive body of research on making causal inferences from observational data which spans such diverse fields as econometrics the social sciences epidemiology statistics and computer science .
we contrasted the model analytically with sever al existing fault localization metrics that do not address confounding and presented empirical results indicating that the model is significantly more effective than some of those techniques and can be incorporated into other techniques to improve their performance.
these results suggest that existing methodology for making causal inferences from observational data is very relevant to statistical fault localization.
however it is easy to see that our previous model does not address all possible sources of confounding in fault localization such as runtime patterns of data dependences data flows the values taken on by inputs and program variables and non determinism because of concurrency .
making the best use of causal inference methodology in software fault localization will require determining the relative importance of such factors and devising effective and reasonably efficient ways of controlling for them.
in this paper we present a new technique that controls for local patterns of both dynamic data dependences and dynamic control dependences that can confound the estimated causal effect of covering a statement son the occurrence of program failures which for brevity we call the failurecausing effect ofs.
static and dynamic data and control dependences have long been recognized as important factors in software testing and debugging because they contribute both to triggering the effects of faults and to propagating those effects to a program s output .
our technique uses information about dynamic data and control dependences to reduce confounding bias more than is possible with our previous model and to thereby rank statements more effectively for fault localization.
when estimating the failure causing effect of a statement s confounding bias due to dynamic data and control dependences affecting scan be reduced by considering two groups of executions those that cover s referred to as the treatment group and those that do not cover s referred to as the control group .
the executions that cover sshould have the same pattern of dynamic data and control dependences as the executions that do not cover s in which case the groups are said to be balanced .
in practice the set of test cases or operational executions used in statistical fault localization is usually given and the subset of them that cover and do not cover a particular statement scannot be assumed to be balanced.
to achieve relative balance between the treatment and control groups and thus reduce confounding bias our technique employs a classical causal inference technique called matching .
for each program statement s the executions in the two groups are reorganized so that they are similar with respect to dynamic data and control dependences that directly affect s. each execution ethat covers sis matched with the execution not covering sthat is most similar to e as measured by applying the mahalanobis distance metric to dynamic dependence information collected from the executions.
after matching the failure causing effect of sis estimated based on the outcomes of the matched executions.
in this paper we also present empirical results indicating that our new technique yields fault localization rankings that are more effective than those obtained with our previous causal model and with several well known statistical fault localization techniques.
for example our new technique performs better than our old technique on .
of272 faulty program versions with an average fault localization improvement of .
.
our new technique also performs better than tarantula on .
of the versions with an average fault localization improvement of .
.
the main benefit of our work is that it provides a practical way to use both data and control dependence information to reduce confounding bias and thereby improve the effectiveness of statistical fault localization.
another benefit is that basing our technique on accepted causal inference methodology will help to put statistical fault localization on a sounder footing and stimulate further improvements.
the contributions of the paper are a new statistical fault localization technique that uses a new causal model based on dynamic data and control dependences together with covariate matching to more accurately estimate the causal effect of covering a particular program statement on program failures.
empirical results indicating that our new technique is more effective than our previous causal technique and other statistical fault localization techniques.
.
background in this section we first present background on the potentialoutcome model for causal inference and the dynamic programdependence graph.
we then describe our previous causalinference model for statistical fault localization which is based on the potential outcome model on pearl s structural causal model and on dynamic programdependence analysis .
.
potentialoutcomemodel thepotential outcome model provides a theoretical basis for estimating causal effects using data from either observational studies or randomized experiments.
suppose that a researcher wants to estimate the effect of a new or existing treatment and suppose further that a set of units or subjects are available to the researcher.
let tbe a binary variable that takes on the value ti for a unit uiifui receives the treatment and that takes on the value ti ifuidoes not receive the treatment.
let yrepresent the outcome for an arbitrary unit.
the potential outcome model associates with ytwopotential outcome random variables y1andy0.
variable y1represents the potential outcome for atreated unit and variable y0represents the potential outcome for an untreated control unit.
the individuallevel causal effect of treatment for a unit uiis defined as y1 i y0 i where y1 iandy0 iare the realizations of y1andy0 for unit i. we assume that for any given unit only one ofy1andy0can be observed the other variable s value iscounterfactual counter to the facts for that unit.
this implies it is impossible to estimate y1 i y0 idirectly.
because individual level treatment effects cannot be estimated average treatment effects are estimated instead.
for the scenario above the average treatment effect is defined to be the difference between the expected outcomes of the two groups e e where e is the expectation operator.
suppose that a randomized experiment is conducted in which a researcher ran1thisproblem has been called the fundamental problem of causal inference .
147domly assigns units to either the treatment group or the contr ol group and suppose an outcome yiis observed for each unit ui.
then the difference y1 y0between the sample means for the treatment group and the control group respectively is an unbiased estimator of .
the estimate is unbiased because random treatment assignment tends to ensure that the treatment group and the control group are balanced with respect to the distributions of confounding variables whether or not the variables are observed.
consequently the treatment variable tis independent of the potential outcomes y1andy0 y1 y0 t and so knowing whether a unit is assigned to treatment does not provide any information about y1andy0.
it is typically assumed that execution data to be used for statistical fault localization is generated from a set of existing test cases or is collected in the field from operational executions.
in both cases statistical fault localization is a kind of observational study rather than a randomized experiment because the executions were not randomly assigned to either the treatment group e.g.
executions covering a statement s or the control group e.g.
executions not covering s .
note that creating test cases specifically to achieve coverage of every conditional branch does not ensure that the test cases that cover a statement sare otherwise similar to the test cases that do not cover s. to estimate average treatment effects based on observational data statistical techniques that control for or condition on the observed values of potential confounding variables are used.
an unbiased estimate of the average treatment effects of treatment variable ton outcome variableycan be obtained by conditioning on a set xof covariates that render tconditionally independent of the potential outcome variables y1andy0 y1 y0 t x in this case the average treatment effect is given by the equation e e where e is the conditional expectation operator.
the outer unconditional expectation is evaluated with respect to the probability distribution of x. one way of controlling for observed confounders when estimating an average treatment effect is to include them as predictors in a statistical regression model such as the linear model y t x in this model yis the outcome variable tis the treatment variable xis a vector of covariates that includes the observed confounders and is a random error term that ideally does not depend on the values of tandx.
equation implies that e t x. the fitted value of the coefficient is an unbiased average treatment effect estimate assuming the model is correctly specified.
.
dynamicprogramdependences informally in a program p statement s2iscontrol dependent on a statement s1if as indicated by the control flow graph cfg for p 2s1determines whether s2is executed.
statement s1dominates statement s2in the cfg if all paths from p s entry to s2contains s1.
the control dependence graph cdg of pis a directed graph where nodes represent statements in pand edges represent control dependences.
statement s1is aforward control dependence predecessor ofs2in the cdg if s2is control dependent on s1ands2does not dominate s1in the cfg.
statement s2is data dependent on statement s1if there is a definition of a variable v i.e.
a write of v ats1 a use of v i.e.
access with no write at s2 and there is a least one path froms1tos2in the cfg on which vis not redefined.
the program dependence graph pdg of program pis a directed graph whose nodes represent statements in pand whose edges represent control and data dependences.
the graph is annotated with two special nodes start and exit which represent entry to and exit from p respectively.
informally the dynamic program dependence graph dynamic pdg is a directed graph constructed from a set of program executions in which nodes represent executed statements and edges represent executed control dependences and data dependences.
statement s1is adynamic forward control dependence predecessor ofs2ifs1is a forward controldependence predecessor of s2in the dynamic pdg.
the dynamic pdg which is a subgraph of the pdg is similar to the graph obtained with agrawal and horgan s technique approach for dynamic slicing.
however the latter is created using one execution whereas the dynamic pdg described here is created using a set of executions.
we define the dynamic forward control dependence subgraph as the subgraph of the dynamic pdg that contains only forward control dependence edges.
.
previousfaultlocalizationmodel to specify a causal model for statistical fault localization it is necessary to indicate the form of the model the treatment variable and the covariates.
our initial causal model includes a linear regression model for estimating the failure causing effect of covering a program statement s y s sts scs s this model is fit separately for each statement in a program using statement coverage profiles from a set of passing and failing executions.
in the model yis a binary variable that is for a given execution if and only if the program failed tsis a binary treatment variable which indicates whether s was covered executed at least once during the execution csis a binary covariate which indicates whether the dynamic forward control dependence predecessor dfcdp s of s was covered sis a constant intercept and sis a random error term that does not depend on the values of tsandcs.
the average treatment effect of tsupon yis estimated by the fitted value sof the coefficient s which is used as a suspiciousness value for s. a regression model such as equation describes only statistical relationships between variables.
to obtain a full causal model it must be augmented with additional information about causal relationships.
this information can be represented by a causal graph which is a directed acyclic 2inacontrol flow graph nodes represent program statements and edges represent the flow of control between the statements.
148t1t2t3t4t5 match void proc int x read .
na int y read .
na if x .
na if y .
.
y .
.
print out .
.
print y y .
.
figure procedure with test cases execution data and causal effect estimates.
error at statement correct computation should be y y. graph dag whose nodes represent random variables corresponding to causes and effects and whose edges represent causal relationships.
an edge a bin a causal graph indicates that a potentially causes b. equation is implicitly based on the causal graph obtained from a structured program s dynamic forward control dependence subgraph by associating with each node a coverage indicator variable for the corresponding statement and augmenting the dynamic forward control dependence subgraph with edges from each node to a new node for the failure indicator y. the role of the coverage indicator variable csfordfcdp s in equation is to control for confounding of the estimated failure causing effect of s due to coverage of other statements.
intuitively conditioning on csreduces confounding because dfcdp s is the most immediate cause of sbeing covered or not being covered on a particular run.
pearl s wellknown back door criterion for causal graphs provides some formal justification for equation because dfcdp s blocks all back door paths from ttoyin the augmented dynamic forward control dependence subgraph.
such a path is actually a semi path one in which edges may point in either direction whose first edge is of the form t zfor some node z. under the very strong assumption that the augmented dynamic forward control dependence subgraph is sufficient to model failure causation in a program the backdoor criterion implies that tis conditionally independent of the potential outcomes y1andy0given the value of cs as required for equation to hold with tsandcsin place oftandx respectively.
.
addressing both data and controldependences the causal model given in equation which was used in our previous technique for statistical fault localization relies on the dynamic forward control dependence subgraph as a model of failure causation.
however the dynamic forward control dependence subgraph and therefore equation does not adequately reflect failure causation in which data dependences play an important role.
in this section we present our new technique for estimating a statement s failure causing effect which addresses confounding due to both dynamic data dependences and dynamic control dependences by matching executions based on information about both.we begin with an example that illustrates the importance of incorporating data dependences into our causal model and then present the details of the model.
.
motivatingexample consider procedure procin figure which has a fault at line .procshould print the square of the value of yat line but instead prints two times the value of y. the first column showsprocwith line numbers associated with each of its statements.
columns through represent test cases t1 t5 respectively.
the top entry of each column shows the values ofxandythat are read at lines and respectively.
the numbers in the column for a test case indicate whether the corresponding program statement is covered by the test case for covered for not covered .
the bottom row shows the outcome of each test case execution with indicating a failing execution and indicating a passing execution.
suppose we want to compute the failure causing effect of statement using equation .
for statement as figure shows test cases t2 t3 and t5are in the treatment group because they cover statement and test cases t1andt4are in the control group because they do not cover statement .
the dynamic forward control dependence predecessor of statement is statement .
variable csin equation corresponds to a coverage indicator c3for statement which is covered by all tests.
because c3is constant it or the intercept sfrom equation can be dropped from the model for statement .
the column labeled in figure indicates the failure causing effect estimates obtained with equation for the statements in procedure proc.
the estimate for statement which is faulty is .
.
however the estimates for statements and are also .
even though they are not faulty.
statements and have the same estimate as statement because they are in the same control dependence region and hence are covered by the same tests.
this example illustrates that serious confounding may occur even after conditioning on coverage of each statement s dynamic forward control dependence predecessor.
.
newtechnique our new technique addresses the inadequacy of our previous technique for fault localization.
the technique consists of two main components a new causal model and a matching technique.
.
.
overview algorithm localizefault shown in figure takes as input the dynamic program dependence graph dynamicpdg of a procedure and the coverage information coverageinfo produced by executing the program containing the procedure on a set of test cases.
localizefault processes each statement sin the dynamic pdg it initializes the causal estimate of each statement s s to .
line .
localizefault computes the first major component of our technique the causal model of s model s line .
at line the algorithm computes the predecessors of s pred s from model s .
after computing pred s localizefault then computes the second major component of our technique by matching on the pred s in the coverage info to produce the matched data mdata s line .
if matching is successful the s is estimated using equation line if matching is not successful equation is used to estimate s line .
the algorithm computes s for every state149localizefault dynamic pdg coverage info fo reach s dynamic pdg do s .
model s compute causal model of s pred s compute predecessors of s from model s mdata s match on pred s in coverage info if mdata s ne ationslash compute s from mdata s using equation else compute s using equation done sorted sort in descending order return sorted figure algorithm for new technique ment in the dynamic pdg.
after the causal estimate for each statement has been computed localizefault sorts all the estimates in descending order at line .
it then returns the sorted estimates at line to the developer.
.
.
newcausalmodel our new causal model extends our previous causal model equation by addressing dynamic data dependences as well as dynamic control dependences.
dynamic data dependences are important because they carry the values that are used at a given statement.
whereas a statement s forward control dependence predecessor determines whether the statement is covered the statement s data dependences determine the computation it carries out.
conceptually the additional causal influences that we want to account for can be represented by including dynamic data dependences in the causal graph for a program in addition to dynamic forward control dependences.
a complication is that loopcarried data dependences give rise to directed cycles in dependence graphs whereas much of the theory of causal inference developed by pearl and other authors including the back door criterion is based on causal graphs that are acyclic.
however to control confounding when estimating the failure causing effect of a particular statement si it suffices to consider acyclic dependence chains terminating at s because if there is a causal path from sjtosithat contains one or more cycles there must also be a cycle free path from sjtosi.
for example consider a program loop of the form while ... if ... x f y else y g x each edge in the data dependence cycle may be relevant to localizing a distinct fault.
however in seeking to control confounding while estimating the failure causing effect of statement we can ignore the edge .
similarly when estimating the failure causing effect of statement we can ignore the edge .
the kind of causal graph that is required can be conceptualized in terms of the transitive reduction of a digraph .
in essence a transitive reduction of a digraph dis an acyclic spanning subdigraph hofdwith no redundant arcs such that the transitive closures of dandhare isomorphic .
3klamtand colleagues employ a form of transitive reduction to model causality in biological networks with cycles .
a dynamic pdg of proc.
b icg of statement .
figure a dynamic program dependence graph dynamic pdg of procedure proc.
b integrated causal graph of statement .
aho and colleagues present a transitive reduction algorithm that for a digraph with cycles replaces each equivalence class of vertices appearing in the same cycle with a new vertex.
consider a slight variant of this algorithm applied to a graph of dynamic data and control dependences between program statements.
for a given statement swhose failure causing effect we want to estimate this variant preserves all nodes in s s equivalence class if there is more than one rather than collapsing them to a single node.
however it breaks any cycles involving sby deleting s s outgoing edges.
therefore for each statement sin the dynamic program dependence graph an acyclic subgraph hscan be constructed that reflects all causal influences on s.hscan be transformed into a proper causal graph by augmenting it as described in section .
we call the augmented hsthe integrated causal graph forsand denote it icg s .
in the integrated causal graph icg s there will be multiple back doors to sif it is dependent on multiple statements.
observe however that the set of predecessors of s pred s blocks all back door paths from sto the failure node y. thus by conditioning on the coverage indicator variables associated with pred s we may be able to further reduce confounding when estimating the failure causing effect of s. for example figure a shows the dynamic program dependence graph of proc dotted edges represent data dependences and solid edges represent control dependences.
figure b shows the icg of statement where nodes and of statement are nodes in the dynamic program dependence graph of proc.
as the graph shows there are three back door paths in the graph y y and y. .
.
matching matching is a technique that brings some of the benefits of randomized controlled experiments in terms of reduced confounding bias to observational studies.
matching involves mapping if possible each treatment unit from an observational study to one or more control units that are similar to it in such a way that balance is achieved between the resulting treatment group and control group with respect to covariate values.
matching may involve removing 4one subtlety is that if a node ninicg s represents an equivalence class cof nodes from d a set zof coverage indicator variables should be associated with n. for each statement trepresented in c zshould contain a coverage indicator for t. 150unmatched units or in effect duplicating certain units.
we first describe a simple form of matching called exact matching before describing the more complex type of matching used with our new causal model.
inexact matching each treatment unit is matched with one or more control units that have exactly the same covariate values as the treatment unit if such control units exist.
all matched treatment and control units are retained and all unmatched units are discarded.
the difference in the group means of the resulting treatment group and control group is an estimate of the treatment effect.
we now illustrate exact matching with reference to the procedure and data in figure .
let the covariates for each statement sin procedure proc be the coverage indicator variables associated with the possible dynamic control and data dependence predecessors of s. for example for statement there are three predecessors statement which defines the value of yand may be used at statement statement which is statement s forward control dependence predecessor and statement which defines the value of y and may be used at statement .
figure shows the treatment units and the control units with their associated test cases for statement .
the treatment units which are test cases that cover statement are t2 t3 and t5 the control units which are test cases that do not cover statement aret1andt4.
the vector of covariate values generated by the test cases is shown a vector of the form means that for that test case statement was covered statement was covered and statement was not covered respectively.
there is an arrow from each treatment unit to a matching control unit.
note that there is no match for t3 because it is the only test case to cover each of statement statement and statement .
consequently t3 is not used to estimate the failure causing effect of statement .
using equation the estimate for statement is .
which is the difference between the average outcome values for the treatment group and the average outcome values for the control group.
the causal effect estimates for the other statements are shown in table in the column labeled match.
the na for statement s and reflects the fact that an estimate could not be computed for the statements because there were no control units for the statements.
the estimate for statement is .
and the estimate for each of statements and is .
.
it can be seen that the faulty statement statement has the highest estimate.
unfortunately as the number of covariates increases it becomes more difficult to find exact matches for treatment units.
this difficulty results in more discarded units which can increase bias5when computing causal estimates.
therefore other forms of matching are typically used.
matchingwithmahalanobisdistance to obtain more flexibility in matching treatment units with control units our technique uses the mahalanobis distance metric which is a measure of the similarity dm a b between two random vectors aandb.
let sbe the covariance matrix of aandband let the superscript tdenote 5thisform of bias is different from confounding bias.
the bias is the difference in an estimator s expected value and the true value of the parameter being estimated.
figure units in treatment group and control group with their covariate values.
matrix vector transpose.
then dm a b p a b ts a b the matrix sis the sample covariance matrix for the treatment and control units.
each treatment unit test case covering statement s with covariate vector ais matched with a control unit test case not covering s with covariate vector bfor which dm a b is minimal with respect to a threshold.
in matching without replacement the two units are then removed from further consideration.
in matching with replacement the control unit is retained to be matched with another treatment unit.
replacement is normally used if the number of treatment units exceeds the number of control units matching with replacement also reduces bias5 in the causal estimate.
a property of mahalanobis distance matching is that it regards all the components of the covariate vector as equally important .
this property is important when matching on the predecessors of sbecause in the absence of any prior information about the relative importance of the predecessors our technique treats all predecessors of sas equally important.
.
empiricalevaluation to evaluate the effectiveness of our technique for fault localization we implemented it and performed empirical studies on a set of subjects.
in this section we first describe the set up and then present the results of the studies.
.
empiricalstudysetup for our studies we used programs with faulty versions.
we used seven programs from the unix suite cal col comm spline tr and uniq all programs in the siemens suite print tokens print tokens2 replace schedule schedule2 tcas and tot info and programs sed and space.6table shows the characteristics of the subjects.
for each subject the first column gives the program name the second column provides the ratio of the number of versions used to the number of versions available the third column gives the number of lines of code for the subject the fourth column gives the number of test cases the fifth column shows the average number of vertices in the dynamic program dependence graph and the last column provides a description.
we omitted faulty versions because either there were no syntactic differences between the c files of the correct version and the faulty version of the program or none of the test cases failed when executed on the faulty version of the program.
6we obtainedthe unix programs from eric wong of university of texas at dallas and the space and sed programs from the software artifact infrastructure repository .
we obtained the siemens suite from the aristotle research lab.
151table subjects used for empirical studies.
number of versions used number of number of number of program number of versions lines of code test cases nodes description cal .
calendar printer col .
filter line reverser comm .
file comparer look .
word finder spline .
curve interpolator tr .
character translator uniq .
duplicate line remover print tokens .
lexical analyzer print tokens2 .
lexical analyzer replace .
pattern replacement schedule .
priority scheduler schedule2 .
priority scheduler tcas .
altitude separation tot info .
information measure sed 14k .
stream editing utility space 6k .
adl interpreter for our implementation we used the cil framework which supports the analysis of ansi c programs to analyze the subject programs.
we implemented the algorithms to instrument the programs and to compute dynamic controlflow graphs in the object caml language ocaml because ocaml is required to interface with cil.
recall that our technique uses dynamic control flow graphs to compute dynamic dependences because the former contain only statements that are actually executed by a set of test cases.
we implemented all fault localization algorithms in r which is a statistical computation system with its own language and runtime environment.
for matching on program dependences we used the rpackage matching .
we used matching with replacement and a default minimal distance of .
.
we also computed fault matrices that indicate for each faulty program which test cases pass and which test cases fail.
we performed our studies on mac os x version .
.
.
effectivenessstudies to study the effectiveness of our technique for fault localization we use a cost metric that has been used in previous studies .
the metric which we denote as cost measures the percentage of faulty statements a developer must examine before encountering the faulty statement.
we assume that the statements are examined in a non decreasing order of suspiciousness scores.
if there are ties we assume the developer must examine all the tied statements.
for example if there are nstatements in a program and all n statements have the same suspiciousness score we assume that the developer must examine all nstatements.
therefore thecostof finding the fault is .
to compare the effectiveness of fault localization techniques a and b with respect to a given program version pi we use b as a baseline and compute the difference between thecostof applying b to piand the costof applying a to pi.
a positive result implies that a performed better than b onpiand a negative result implies that b performed better than a. the difference between the costs corresponds to the magnitude of improvement.
for example if the costof a is and the costof b is developers will examine fewer statements if they use a for fault localization.
to reduce the uncertainty in the causal effect computed for each statement because of the matching technique wecomputed a statement s causal effect times and took the average of the causal estimates.
we found to be an acceptable threshold for providing stable causal effects.
each computed causal effect has an associated standard error.
we computed the average of the standard errors and used it to construct a bound one standard deviation on the causal effect for each statement.
for example if the average of the causal effects of a statement on program failure is .
and the average standard error is .
then is the range of the causal effect over the samples with .
being the lower bound and .
being the upper bound.
for brevity we denote our new fault localization technique which matches executions based on data and control dependences as pd we denote a variant of the technique that matches only on data dependences as dd and we denote our previous causal technique which considered only control dependences as cd.
we also use pdmin and pdmax to represent variants of pd computed using the lower bounds and upper bounds of the causal effect estimates of pd respectively.
for example if for a statement the average causal estimate obtained with pd is .
and the average standard error is .
then pdmin and pdmax yield causal estimates of .
and .
respectively.
table summarizes the results of comparing fault localization techniques.
the first column fault loc.
tech.
shows the two fault localization techniques i.e.
a vs b that are being compared the second technique in each pair i.e.
b is the baseline.
the second column positive shows the percentage of faulty versions for which a performed better than b the third column negative shows the percentage of faulty versions for which a performed worse than b and the fourth column neutral shows the percentage of faulty versions for which there was no improvement.
for example the first row which compares pd to cd shows that pd performed better than cd on .
of the faulty versions performed worse on .
of the faulty versions and performed identically on .
of the faulty versions.
table shows the minimum min median med maximum max and mean mean values of a s improvement over b. half the faulty versions with positive improvement values have improvements between the minimum and the median and the other half have improvements between the median and the maximum.
for example the first row which compares pd to cd shows that half of the .
versionimprovement figure comparison of new technique pd to old technique cd .
of faulty versions with positive improvement values had improvements between .
and .
while the other half had improvements between .
to .
.
the average positive improvement of pd over cd was .
.
.
.
study1 newtechniquevsoldtechnique the goal of this study is to compare the fault localization effectiveness of our new technique pd to that of our previous causal technique cd which considers control dependences but not data dependences.
to do this we compared the costvalues for the two techniques.
figure shows the bar graph that summarizes the comparison of pd to cd over all program versions.
the horizontal axis baseline represents the costof using our previous technique cd the vertical axis represents the magnitude of improvement of pd over cd.
the graph contains a vertical bar for each faulty version for which there was positive or negative improvement on the horizontal axis the vertical bar shows the difference in costs.
the bars above the horizontal axis represent faulty versions for which pd performed better than cd positive improvement and the bars below the horizontal axis represent faulty versions for which pd performed worse than cd negative improvement .
for example for faulty version on the graph pd performed better by about .
.
figure shows that pd performed better than cd overall.
as indicated in table pd performed better than cd on .
of the faulty versions worse on .
of the faulty versions and showed no improvement on .
of the faulty versions.
the first row of table characterizes the degree of positive improvement of pd over cd.
as the table indicates half the .
of the faulty versions with positive improvement values had improvements between .
and .
and the other half had improvements between .
and .
.
the average positive improvement of pd over cd was .
.
we also compared the costs of pdmin and pdmax to pd.
as table shows pdmin performed better than cd on .
of the faulty versions performed worse on .
of the faulty versions and performed identically on .
of the faulty versions.
the table also shows that pdmax performed better than cd on .
of the faulty versions performed worse on .
of the faulty versions and performed identically on of the faulty versions.
table shows that the performance of pd pdmin and pdmax aretable comparison of fault localization models.
fault loc.
tech.
positive negative neutral pd vs cd .
.
.
pdmin vs cd .
.
.
pdmax vs cd .
.
.
pd vs dd .
.
.
dd vs cd .
.
.
pd vs tarantula .
.
.
pd vs ochiai .
.
.
co vs ochiai .
.
.
table distribution of positive improvements.
fault loc.
tech.
min med max mean pd vs cd .
.
.
.
pdmin vs cd .
.
.
.
pdmax vs cd .
.
.
.
pd vs dd .
.
.
.
dd vs cd .
.
.
.
pd vs tarantula .
.
.
.
pd vs ochiai .
.
.
.
co vs ochiai .
.
.
.
similar.
the results of pdmin and pdmax provide evidence of the stabilit y of the causal estimates computed with pd.
overall the results indicate that pd improved the accuracy of fault localization over cd by further reducing confounding bias.
although pd performed better overall we wanted to see how much of the improvement resulted from data dependences.
to do this we compared dd to cd with respect tocost.
table indicates that dd had lower cost than cd on .
of the faulty versions and higher cost on only .
of the versions.
thus most of the improvement seen with pd is due to considering data dependences.
considering both control and data dependences increases the accuracy of our technique by about .
tables and indicate that causal models based on both data and control dependences are more effective than causal models based on either type of dependence alone.
overall the results show that by blocking back door paths created by program dependences in a faulty program confounding bias can be reduced when estimating the failure causing effect of a statement.
.
.
study2 newtechniquevsothertechniques the goal of this study is to compare pd to two well known statistical fault localization techniques tarantula and ochiai with respect to cost.
we used the costs of tarantula and ochiai as baselines and subtracted the costof pd.
figure shows that pd performed better than tarantula on most faulty versions.
table indicates that pd performed better than tarantula on .
of the faulty versions performed worse on .
of the faulty versions and performed identically on .
of the faulty versions.
table also shows that half the .
with positive improvement values had improvements between .
and .
and that the other half had improvements between .
and .
.
the average positive improvement of pd over tarantula was .
.
in our previous work ochiai performed better than our older technique cd .
however ochiai does not address confounding bias so we incorporated cd into ochiai calling it causal ochiai and showed that causal ochiai performed better than ochiai.
here we show that pd performs better than ochiai even when the techniques are not composed.
figure shows that pd performed better than ochiai versionimprovement figure comparison of our technique pd to tarantula.
versionimprovement figure comparison of our technique pd to ochiai.
on most of the faulty versions.
table indicates that pd performed better than ochiai on .
of the faulty versions performed worse on .
of the faulty versions and performed identically on .
of the faulty versions.
table also shows that half the .
of the faulty versions with positive improvement values had improvements between .
and .
and that the other half had improvements between .
and .
.
the average positive improvement of pd over ochiai was .
.
tables and show that the performance of ochiai was improved when pd was composed with it to produce causal ochiai co .
co performed better on .
of the faulty versions performed worse on .
of the faulty versions and performed identically on .
of the faulty versions.
also half the .
of faulty versions with positive improvement values had improvements between .
and .
and the other half had improvements between .
and .
.
the average positive improvement of co over ochiai was .
.
overall the results indicate that pd performs better than both tarantula and ochiai and improves ochiai.
.
.
computationtime in this section we present the average computation time required to compute all the fault localization results for oneversion of some of the subjects.
we found that the computation time was largely dependent on the size of the test suite of a program.
the matching package takes considerable time to invert the large covariance matrices.
for tcas which had test cases it took on average minutes.
for schedule which had test cases it took on average .
minutes.
for printtokens2 which had test cases it took on average .
hours.
.
threatstovalidity there are three main types of threats to validity that affect our studies internal external and construct.
threats to internal validity concern factors that might affect dependent variables without the researcher s knowledge.
the implementations of the algorithms we used in our studies could contain errors.
the matching package we used in our studies is open source and has been used by other researchers for experimentation which provides confidence that the algorithms in the package are stable.
to address potential errors when constructing the dynamic program dependence graph we compared manually generated dynamic programdependence graphs of smaller subjects to graphs generated automatically by our technique.
threats to external validity occur when the results of a study cannot be generalized.
in this work such threats are greatly alleviated because our work is based on established causal inference theory and methodology.
however more empirical studies on additional subjects are needed to fully address this threat.
threats to construct validity concern the appropriateness of the metrics used in our evaluation.
we used the costmetric to determine the effectiveness of our technique for fault localization.
the costmetric is a ranking metric that has been used to compare techniques in many fault localization studies though under a different name score .
however it is not established whether this metric is well suited for presenting fault localization information to developers.
.
relatedwork there are many techniques that attempt to locate the fault or faults in a failed program.
this section discusses those most closely related to ours and compares our new technique to them.
one category of fault localization techniques are statistical fault localization techniques e.g.
.
these techniques compute the association between program entities and program failure.
however as we discussed in our previous work and demonstrated with analytical and empirical studies statistical association does not imply causation.
in contrast to these existing statistical fault localization techniques our technique for finding the cause of program failure is grounded in the theory of causalinference methodology.
our empirical results in previous work and those presented in this paper demonstrate the effectiveness of our approach over the associative techniques.
another category of fault localization techniques are slicing techniques e.g.
.
slicing techniques compute the set of program entities e.g.
statements that potentially or actually affect the values of variables at a given program point e.g.
the program output .
one limitation of slicing techniques is that the techniques do not find the cause of the failure but instead compute the set of program entities associated with the failure.
a second limitation is 154that the slicing techniques do not provide guidance as to how a developer is to examine the statements in the slice.
in our work we have demonstrated with analytical and empirical studies that such association does not imply causation.
our technique which is grounded in causal theory is more effective at finding causes of failures and as such more effective than slicing techniques.
additionally our technique provides guidance to developers on how to systematically examine program entities to find the cause of the fault.
a third category of fault localization techniques are statealtering techniques e.g.
.
like our technique state altering techniques are causal techniques.
these techniques have a number of limitations that can make them difficult to apply.
first in performing experiments on programs the techniques must ensure the semantic consistency of memory changes which can be difficult and expensive.
second in performing experiments on programs the techniques require multiple re executions of the program which can also be expensive.
third the techniques require the presence of an oracle preferably automated to determine the outcome success or failure of the program after each such re execution which in practice can be difficult if not impossible to create.
the main difference between our technique and state altering techniques is that our causal technique is observational whereas they are experimental.
our technique uses observational data e.g.
coverage information that is often available by executing the program on test cases.
moreover our technique does not require an automated oracle because the test cases have pre determined outcomes and it does not require multiple re executions of the program.
finally our technique avoids the semantic consistency problem encountered by state altering techniques.
one other technique uses causal analysis and program slicing to generate graphs that explain unexpected behaviors produced by computational models .
this technique is similar to ours in that both are based on the theory of causal analysis.
however their technique does not find the location of faulty statements.
instead their technique creates a causal graph that explains an unexpected behavior.
yet another technique determines whether two versions of a program are the same given that one of the versions has been transformed e.g.
code has been obsfucated .
to do this their technique performs dynamic matching on the control flow graphs of the two versions.
their technique differs from ours in that we match on program dependences to reduce confounding bias for effective fault localization whereas they match on control flow to determine whether the programs are the same.
finally in previous work we presented a causal model that is based on only control dependences.
in this work we have augmented the causal model with data dependences and have shown through empirical studies the effectiveness of the new causal model for fault localization.
.
conclusionandfuturework in this paper we have presented a novel statistically based causal inference technique for software fault localization that matches executions based on information about dynamic data and control dependences to obtain more accurate less biased estimates of a given statement s effect on the occurrences of program failures.
the use of both covariate matching and data dependence information in our technique are innovations.
we also presented empirical results indicat ing that the new technique is more effective overall than other techniques including our previous causal inference technique which does not consider data dependences.
although the presented technique performed well overall it performed relatively poorly on some faulty program versions which indicates there is room for improvement.
we mention three possible reasons for the technique s performance in some cases and we suggest how it might be improved.
first the results obtained with the new technique are subject to limitations of the test suite used for fault localization.
even if the test suite has desirable properties overall it may be inadequate for accurately estimating the failure causing effects of certain statements.
this will be the case for any statement that is covered by very few test cases or for which it is not possible to extract two reasonably well matched comparison groups of test cases that respectively cover and do not cover the statement.
suspiciousness scores computed for such statements cannot be trusted because they lack adequate support.
to address this issue we plan to investigate the automatic generation of test cases with the appropriate coverage characteristics.
second although matching using mahalanobis distance was generally effective in our studies it sometimes failed to yield balanced comparison groups.
in the latter cases the rankings produced by our technique are not consistent because they are based on valid causal estimates mixed with biased estimates.
using mahalanobis distance for matching is also sometimes inefficient because it is expensive to compute mahalanobis distance when there are many program dependences.
to address these issues we plan to explore the use of other matching techniques such as those based on propensity scores .
finally some faults cannot be effectively localized without considering the variable values carried by data dependences.
for example considering variables values may be the only way to determine that a check for an unusual condition is missing in a certain program location.
although the proposed technique considers data dependences it does not currently consider variable values.
we are currently investigating how this can be done.