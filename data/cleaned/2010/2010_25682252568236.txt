care cache guided deterministic replay for concurrent java programs y anyan jiang tianxiao gu chang xu xiaoxing ma jian lu state key lab for novel software technology nanjing university nanjing china department of computer science and technology nanjing university nanjing china jiangyy outlook.com tianxiaogu gmail.com changxu xxm lj nju.edu.cn abstract deterministic replay tools help programmers debug concurrent programs.
however for long running programs a replay tool may generate huge log of shared memory access dependences.
in this paper we present care an application level deterministic record and replay technique to reduce the log size.
the key idea of care is logging read write dependences only at per thread value prediction cache misses.
this strategy records only a subset of all exact read write dependences and reduces synchronizations protecting memory reads in the instrumented code.
realizing that such record strategy provides only value deterministic replay care also adopts variable grouping and action prioritization heuristics to synthesize sequentially consistent executions at replay in linear time.
we implemented care in java and experimentally evaluated it with recognized benchmarks.
results showed that care successfully resolved all missing read write dependences producing sequentially consistent replay for all benchmarks.
care exhibited .
median .
smaller runtime overhead and .
median .
smaller log size against state of the art technique leap .
categories and subject descriptors d. .
testing and debugging debugging aids general terms algorithms performance reliability theory keywords replay debugging cache concurrency .
introduction concurrent programs are error prone but difficult to debug.
in order to alleviate such debug difficulties it would be better to have cyclic debugging techniques such as breakpoint and time traveling corresponding authors.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
icse may june hyderabad india copyright acm ... .
.debugging.
however this is a nontrivial task requiring cost effective execution record for deterministic replay .
by faithfully reproducing the progress of the recorded execution deterministic replay tools help programmers better pinpoint the root causes of such concurrency bugs.
the major challenge of replaying a concurrent program execution comes from its non determinism which is naturally associated with concurrency.
on a uniprocessor machine providing all external inputs and thread scheduling data of an execution necessarily eliminates all of its non determinism .
however such information is still insufficient for replaying a concurrent execution on a multiprocessor machine due to non deterministic outcomes of parallel shared memory accesses.
according to the methodology of tackling shared memory non determinism existing multiprocessor deterministic replay work can be roughly classified into two categories search based and order based .
search based techniques emphasize reduced record cost thereby their recorded information is typically incomplete for a faithful replay.
to construct a valid execution for debugging search based techniques usually use the best effort exhaustive state space search.
on the other hand order based techniques record dependences among all key events e.g.
variable read write and object lock unlock events at runtime.
such dependence information is usually sufficient for a faithful replay but may at the same time incur intolerably large record cost e.g.
huge log.
state of the art order based deterministic replay techniques have reported encouraging results in reproducing concurrent program executions.
however their record cost is still unsatisfactory.
for example stride generates over 30mb s .5tb day of log in its evaluation.
for programs that contain mass of shared memory accesses order based replay technique can easily generate hundreds of megabytes of log per second and nobody in practice can afford such performance degradation that may last unpredictably long.
in this paper we present an order based deterministic replay technique that is capable of reducing the record cost specifically reducing the log size.
this is achieved by a trade off in the replay guarantee our approach in theory gives only value deterministic replay.
fortunately we carefully design our algorithms such that they produce sequentially consistent replay executions for almost all programs in practice actually all evaluated benchmarks making it practically useful for debugging.
our key insight lies in the fact that a dominant percentage of memory accesses exhibit thread locality i.e.
for a specific variable its successive accesses can very likely be performed by a single thread1.
combining successive local memory accesses into a single log entry can greatly compact the log .
to identify 1in our evaluated benchmarks to of shared variable accesses exhibit such thread locality.permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
icse may june hyderabad india copyright acm ... .
local memory accesses we innovatively use light weight software value prediction cache to optimistically overestimate such locality property i.e.
identifying a superset of all such memory accesses with an ideal cache .
such overestimation reduces the log size but also makes replay difficult because some critical shared memory access dependences might be lost.
therefore we also propose two heuristics that can resolve such missing dependences.
such an idea leads to our cache guided deterministic replay approach care .
being a typical order based technique care logs a fraction of actual precedence orders between reads and their associated writes i.e.
read write dependences .
care assigns each thread with a value prediction cache that keeps buffered variable values.
each time a read action is executed the buffered value is compared with the one actually read.
only when a distinct comparison result occurs i.e.
cache miss the corresponding read write dependence is logged.
note that this is only a sufficient but not necessary condition indicating that a thread is reading a value written by another thread.
therefore care logs only a subset of all exact read write dependences.
another benefit of this treatment is that in case of cache hit care does not need to keep the read action synchronized further reducing the runtime overhead.
to understand how cache can reduce the log size consider an example that thread t1initializes a large data structure consisting of many variables.
thread t2then copies all these variables into t2 s thread local storage.
now all order based deterministic replay techniques including care keep read write dependence for each of these variables.
suppose that later t1once again writes to this data structure but overwrites most of the variables with their original values.
after that all these variables are read by t2a second time.
this time however in contrast to state of the art techniques like leap and stride that keep exact dependences for all of these variables care only keeps those dependences for variables being overwritten with different values reducing the log size.
however care s capability of reducing log size is at the cost of providing only value deterministic guarantee at replay.
though each thread s local behavior can be faithfully reproduced the global order of replayed actions might not be sequentially consistent causing trouble for the debugging procedure.
fortunately we propose two heuristics to mitigate such worst case possibility one is grouping variable sequence numbers into coarse granularity atomic counters at record and the other is prioritizing candidate actions at replay.
with these two heuristics we resolved all lost read write dependences in our evaluation.
we implemented care as a prototype tool in java2and evaluated it on a set of benchmark programs.
care realized cost effective record and replay for java programs by reducing runtime overhead and log size.
our experiments showed promising results that care reduced .
of runtime overhead and .
of log size compared with state of the art deterministic replay work leap on the dacapo benchmark suite and up to runtime overhead reduction and log size reduction on the scientific benchmarks.
for another state of the art replay work stride care also reduced record cost and realized log size reduction relatively on some scientific benchmarks.
in summary we make the following contributions in this paper .we propose using value prediction cache to reduce the record cost in deterministic replay of concurrent programs.
this strategy yields a new trade off between the record cost and the replay fidelity.
2the idea of care is general and extensible to other programming languages.
.we present two heuristics for synthesizing sequentially consistent execution at replay realizing that our approach ensures only value deterministic guarantee.
.we implemented our care approach and evaluated it experimentally with real world benchmarks.
the rest of this paper is organized as follows.
section overviews care .
section presents care algorithms for deterministic record and replay.
section elaborates on some important issues including care s cache organization replay guarantee and heuristics for resolving unordered actions.
section discusses care implementation and evaluation which are followed by related work in section and finally section concludes this paper.
.
care overview deterministic replay tools give developers the ability to timetraveling a past execution via logged information at runtime.
in this paper we focus on order based deterministic replay for failure diagnosis i.e.
to reproduce execution states by topologically sorting partially ordered actions observed at runtime.
to deterministically replay a concurrent java program on a multiprocessor machine we need happens before orders about shared memory accesses lock acquisitions releases and thread synchronizations.
as studied before the latter two kinds of orders are relatively fewer and can be easily captured.
however shared memory accesses can be many and a faithful replay requires preserving the relative order of any two conflicting variable accesses in the observed and replayed executions.
existing order based work keeps version numbers of shared variables to identify such causal relationships.
however simultaneous accesses to the same version number can occur on a multiprocessor machine.
thus these accesses have to be synchronized explicitly by additional locks or implicitly by atomic operations which can introduce large runtime overhead at record.
stride addresses this issue by recording inexact version numbers without synchronization and inferring equivalent read write dependences later at replay but value fingerprints of variable reads have to be logged.
on the contrary ourcare takes a fundamentally different approach.
the first innovation of care is using cache to automatically identify the dependences between shared memory accesses.
care assigns each thread with a software cache storing buffered variable values which is updated each time this thread accesses a shared variable either read or write .
the cache is queried each time immediately after a thread reads a shared variable.
cache miss occurs if the read value differs from the one buffered in the cache andcare logs a read write dependence corresponding to this variable access.
let us assume that the cache has an infinite capacity and never discards any buffered value.
if a program is sequential i.e.
single threaded no cache miss would ever occur and hence no log is kept.
otherwise cache miss always indicates that a read value is written by another thread cache misses successfully identify the read write dependences.
note that the converse does not necessarily hold as cache hit might occur at reading an inter thread value as illustrated in of figure .
when cache miss occurs one needs to log exact read write dependence of memory accesses.
however exact dependences can be hard to determine because other writes to the same variable may have already occurred in a short earlier period.
to ensure exact 3in this paper we use the visual convention that actions from the same thread are grouped vertically by program order.
we use r x w x acq x rel x to denote read write lock acquisition and release on x respectively.
solid lines are care s recorded happensbefore dependences while dashed lines are actual read write dependences omitted by care .458r x 1acq y rel y acq y rel y w x 2thread thread 2w x 1w x w x 1r x figure illustration of missing dependences r x 1acq x w x 1rel x missed dependenceacq x w x 2rel x acq x r x 2rel x logged dependencecache query miss cached read value discardthread thread 2r x first read attempt no synchronization second read with synchronizationmissed dependence write occurred before refetching figure illustration of refetching dependences to be logged care would perform the same read actions again i.e.
refetching with synchronization to identify exact writers corresponding to these read actions as illustrated in figure .
this trick favors programs that are dominated by the thread local accesses because cache hit queries incur only tiny overhead and require no synchronization.
the second innovation of care lies in its trade off between log size and replayed trace fidelity guarantee.
recall that cache miss is only a sufficient but not necessary indicator of an exact read write dependence.
as such not every linear extension of recorded partial orders is necessarily a valid execution.
figure gives such an example.
at record care omits the dashed dependences because of cache hit at hence possibly creating linear extension at replay.
such a linear extension is invalid because placing after violates the requisite of sequential consistency.
actually the care approach weakens the replay guarantee to be value deterministic as the trade off for reduced log size.
at replay care simulates the behavior of every thread scache to reproduce its exact local behavior.
these algorithms are formalized later in section and discussed in section .
realizing this drawback the third innovation of care is two powerful heuristics to increase the chance of producing valid execution at replay for debugging.
both heuristics are based on the observation that such limitation is caused by missed read write dependences.
the first heuristic tries to guide the replay scheduler to stop before an unordered read action as early as possible so that a thread can immediately continue once its target value is written by another thread.
the second heuristic is adding extra ordering information to reduce non determinism by grouping variables into bundles and recording ordering information at a granularity of bundle.
also intuitively the situation shown in figure rarely occurs in the execution of real world programs.
for all evaluated benchmarks care successfully resolved all missed dependences and obtained valid executions at replay.
besides care also achieved a significant reduction of log size up to as compared with the state of the art replay work leap .
.
care algorithms in this section we formalize the care instrumentation for collecting happens before dependences at runtime and the algorithm synthesizing a value deterministic execution.
we currently focus on the general description of algorithms explaining the principle ofcare .
more specific issues such as cache organization formal proof of replay guarantee and heuristics for execution synthesis are discussed in section .
.
notations our execution model is similar to the java memory model .
the basic building block of an execution is an action .
for action a of kind kperformed by thread taccessing variable or monitor object vwhose unique identifier is u we denote it by a tuple ht k v uiwith the following conventions we only consider kto be read write lock acquisition or lock release and assume that there is neither dynamic thread creation nor nested lock for brevity4.
a runtime variable vis uniquely identified by an object reference and a field name.
a monitor object vis uniquely identified by its reference.
we call both of them variables for short.
we use a t a k a vand a uto denote the thread identifier action type runtime variable and unique identifier of action a respectively.
the shared memory is denoted by heap which is a mapping from variables to their values.
the mapping can be altered by write actions at runtime.
we assume that any execution of a java program conforms the sequential consistency memory model5.
formally we denote a java program execution eby a tuplehp a v po!
sw!
sc!i.pis the program.
note that we assume all threads to be statically created and their identifiers can be uniquely determined by p.ais the set of all actions executed in e.vis the value function indicating the value read or written by a variable access action.
for any lock acquisition or release action a we define v a ?.po!denotes the program order of all actions in a.sw!is the partial order indicating the actual ordering of lock acquisitions and releases happened in e. every pair of lock actions performed on the same monitor variable is ordered insw!.sc!is a total order on ato define a sequentially consistent execution.
we assume that any actual execution eproduced by java runtime is valid i.e.
emeets the following constraints 4for thread fork join and wait notify actions in actual executions we technically treat them as special kinds of lock acquisitions and releases and create corresponding dependences.
5actually care instrumentation enforces the sequential consistency memory model.459algorithm instrumentation for d heap v generating r ht read v ui 1d heap v 2ifcache t v dthen synchronized v d heap v 5h h last v r 6g g f rg last v r cache t v d .sw!conforms the lock semantic i.e.
for any two paired lock acquisition and release actions on the same monitor variable v say acq rel1 and acq rel2 either rel1sw!acq 2or rel2sw!acq 1holds.
.sc!conforms the program order and lock order i.e.
sw!
sc!
andpo!
sc!hold.
.sc!is sequentially consistent i.e.
for every read action r2a there is a unique write action w2asuch that wsc!r w v r v v r v w and there is no wsc!w0sc!rfor any write action w0.
.
care record at record care instruments a given program s bytecode at class loading time to collect dependences of its actions.
during the program s execution log hg h piis collected for later deterministic replay6.g ais the set of all read actions that have ever encountered cache miss in the execution h a ais the inter thread causal dependences and p a ais the program order.
in order to collect such ordering information care maintains several data structures at runtime each thread tis associated with a value prediction cache cache tkeeping buffered values of variables.
formally cache tis a mapping from variables to their values.
initially cache t v ?for all v and cache t v can be reset to?before any cache update for arbitrary v. the cache resides in each thread s local storage and is invisible to other threads.
caches of different threads are allowed to have inconsistent values for a variable.
each variable vis assigned with last v denoting the unique identifier of the last thread that performs an action on v. for every java virtual machine instruction that can generate a non deterministic action care instruments it to collect dependences.
for instructions in thread tgenerating heap read action d heap v heap write action heap v dand lock acquisition action acquire v their instrumentations are shown in algorithms respectively.
in each algorithm the underlined action is the one being instrumented.
these algorithms are explained in detail below.
in algorithm for each instruction generating read action ht read v ui care first reads the value dfrom heap line .
then the buffered value cache t v in thread tis fetched from t s value prediction cache for a comparison with d. ifcache t v d care optimistically assumes that there is no write action on vsince the last update of cache t v int and hence no log other than the program order is kept.
otherwise we have cache t v d indicating that the read value dmust be written by another thread.
at this time 6the claim that care keeps log ofhg h piis for brevity of presenting our algorithms and proofs.
as shown in section .
we store such logs compactly in each thread s local storage in the implementation of care .algorithm instrumentation for heap v dgenerating w ht write v ui 1synchronized v heap v d iflast v t tthen 4h h last v w last v w 6cache t v d algorithm instrumentation for acquire v generating ht acquire v ui 1acquire v 2iflast v t tthen 3h h last v last v however it is already too late to exactly determine which write action is responsible for writing this value because other threads might write to vduring this short period.
instead of recording an inexact dependence and value fingerprint for replay time inference asstride did care logs exact read write dependence by re the read action again with the lock of vheld line .
since in algorithm all writes to variable vare synchronized on the monitor variable v the second read is serialized with any write to v and hence its depending write action can be correctly identified and logged inh line .
finally in line last v andcache t v are correspondingly updated.
in algorithm for each instruction generating write action ht write v ui care wraps it with a synchronization block on the monitor variable v. log is kept only if last v does not happen in thread t because successive writes in the same thread can be inferred by program orderp.last v andcache t v are subsequently updated as in the instrumentation of read actions.
in algorithm for each instruction generating lock acquisition actionht acquire v ui care maintains last v keeping the latest acquisition action that has successfully entered the monitor.
each time right after vis acquired care checks whether the last acquisition of vis previously performed by another thread.
if so this inter thread dependence is kept in h. in summary the care record algorithm logs the write total order synchronization order and read write dependences at the cache miss.
we discuss several remaining issues below.
first the record algorithm does not require a specific cache organization.
using an infinite cache that never discards any buffered value is both impractical and unnecessary.
note that using a finite cache does not affect the correctness of the replay algorithm reducing the cache size only makes cache miss more frequent incurring extra synchronizations and more detailed log.
we further discuss cache organization related issues later in section .
.
second the care instrumentation keeps actual orders that happen in a recorded execution.
assume that the actual execution is e hp a v po!
sw!
sc!i.
in the care implementation we use the thread identifier and the sequence number in each thread to uniquely identify an action thuspo!
pis confirmed.
also since every logged action is correctly synchronized if necessary hmust conform the actual execution.
particularly we havesw!
h and h sc!guaranteed.
letrec!be all the information of handp i.e.
460algorithm care replay 1s a t ?
2while s ?do finda2ssuch that a02sanda0rec!a execute thread a tfor one action obtaining action a0 5m m a a0 s snfag t t fag switch a.kdo case read d heap a0 v ifa gthen d cache t a0 v else cache t a0 v d case write heap a0 v d cache t a0 v d rec!
tr h p .
since bothp sc!andh sc!hold it is clear thatrec!
sc!is a valid partial order on a. .
care replay thecare replayer is a scheduler that controls a program s execution.
the replay algorithm suspends a thread when it is about to execute an instruction that can generate an action.
when all threads are suspended the scheduler decides which thread to resume according torec!.
the algorithm also generates a mapping m.m maps actions in a recorded execution to the ones in its corresponding replayed execution.
mis only used for proving properties of care and hence is not kept in the actual implementation.
we formalize the replay algorithm in algorithm .
to better illustrate the intuition behind algorithm we consider a special case that every read action rsatisfies r2g.
in this case all reads trigger cache miss and every read on twill be ordered with every write on tinrec!.
existing work has shown that any linear extension ofrec!is a valid failure reproductive and sequentially consistent execution.
then for this case line in the algorithm would be totally redundant.
for those r g no ordering information about rother than its predecessor and successor in thread r tis kept.
there can be interleaving read action r gand write action waccessing the same variable r v w vsuch that neither rrec!wnorwrec!rholds.
these unordered actions make obtaining a sequentially consistent linear extension ofrec!difficult.
nevertheless care is still able to faithfully restore such read values by simulating the cache behavior at replay.
note that line in algorithm is guarded by the path condition of v cache t cache t v d .
therefore r gis sufficient for conducting cache t v d. for a thread s every read action r makingv r cache t v at replay essentially reproduces that thread s execution path.
therefore care simulates the cache behavior and replaces the return value of every r gto be cache t v .
observing that v r forr gonly depends on r t s thread local state either an earlier read action in g or a write action.
ordering information for such an action is faithfully kept inrec!.
executing the program by the order ofrec!yields the identical cache state in the recorded and replayed executions and hence care is able to provide value deterministic replay.
however this replay technique alters the semantic of the program being executed at replay in a valid java implementation any sharedmemory read should always be fetched from the heap rather than from a thread local cache.
although the replayed execution s threadlocal behavior is identical with the one recorded the total order of these actions might not satisfy the sequential consistency criteria causing trouble in figuring out how failures happened and what is the root cause.
in practice however this issue is not that serious.
with an efficient heuristic of variable grouping at nearly zero cost the amount of unordered actions in the log of care can be largely reduced.
we also propose another heuristic to increase the probability of synthesizing a valid execution even if unordered actions exist.
these two heuristics are to be discussed in the next section.
actually this is exactly the key insight of the trade off in care to have an acceptable relaxation of the replay fidelity guarantee for much lessened read synchronization reduced log size linear replay time as well as synthesizing sequentially consistent executions for all evaluated benchmarks.
.
discussions last section gives the algorithms of care .
we still have some remaining issues with care algorithms.
first we have not discussed how the cache organization may impact the performance of logging.
second care keeps log for a read action only if cache miss is encountered.
this however is insufficient for easily restoring a sequentially consistent execution at replay.
in this section we discuss issues about care s cache organization value deterministic replay guarantee sequentially consistent execution synthesis and performance characteristics.
.
cache organization cache organization may impact care s runtime overhead because every shared memory access is associated with at least one cache query or update operation.
using a simple cache can reduce such overhead however on the other hand can incur a higher cache miss rate.
frequent cache misses would lead to redundant read synchronizations as well as redundant logs which are undesirable.
therefore cache organization is a trade off issue.
at one end cache can be of no use at all every read action triggers a cache miss.
in this case every shared variable keeps an individual log of all actions accessing it.
this becomes exactly the algorithm described in .
at another end we can also use an ideal cache of infinite capacity that never discards any buffered value.
read synchronizations and log size can be reduced to minimal in this setting.
however operations on such a cache is considerably slower than those on simple ones.
the most serious issue of such a cache organization strategy is that it virtually disables the garbage collection mechanism which can quickly drain all memory available for longrunning programs.
therefore we need to set up a cache organization that satisfies queries and updates are efficient memory consumption is moderate and its cache hit rate is comparable to that of an infinite cache.
at a first glance these requirements are conflicting.
fortunately for most real word programs memory accesses exhibit very strong locality.
a moderate sized cache readily brings satisfactory performance.
we study cache organization and its impact on care performance later in the evaluation.
.
value deterministic replay guarantee as mentioned in section .
there can be read action rand write action won the same variable that are not ordered inrec!.
resolving these unordered actions to obtain a sequentially consistent execution would be very costly.
instead of determining all exact read write dependences the care replay algorithm simulates the461cache behavior and overrides the read values from heap by values buffered in cache at replay.
to prove that such strategy provides value deterministic executions we first introduce several notations and definitions.
as defined in section .
an execution is defined by a program the actions performed a value function a program order a lock order and a total order of actions.
let a recorded execution be e hp a v po!
sw!
sc!iwhich is sequentially consistent.
care replay algorithm creates another execution e0 hp a0 v0 po0 !
sw0 !
sc0 !i but not necessarily sequentially consistent.
for two actions a2e anda02e0 we define a a0indicating their equivalence by a t a0 t a k a0 k a v a0 v v a v0 a0 for actions a b2e we define akbindicating that aandbare interleaving in the logged partial orderrec!by arec!b brec!a for read action r2a we define its interleaving write set w r by n w2a w k write w v r v rkw o value deterministic replay guarantee is reflected by reproducing each thread s local behavior in e0.
in algorithm we have already created mappingm which associates the actions in eto those in e0.
the replay guarantee is characterized by the following theorem theorem .
for all actions a2a a m a .
for all pairs of actions a b2a apo!bif and only ifm a po0 !m b .
sw0 !satisfies the lock semantics.
proof .
sketch prove by induction.
assume that each time at the beginning of an iteration in algorithm through holds concerning actions in t. specifically for all action a2t a m a program orderpo!ontis reflected inpo0 !
and all lock actions in t do not violate the lock semantics.
the inductive hypothesis trivially holds before the first iteration because t ?.
each time during an iteration an action a2sis chosen and the thread with identifier a tis executed for one action a0 m a .
hence a t a0 t. the one one correspondence between threads ensures that program orderpo!is reflected inpo0 !.
also note that the state of a thread i.e.
stack registers thread local storages etc.
is deterministic if values of read actions previously performed by the thread are known.
according to our inductive hypothesis that for all read actions r2t v r v m r we have a k a0 k anda v a0 v. we enumerate the action type a kto show that the inductive hypothesis still holds after this iteration.
case ais a lock acquisition or release action.
this would never violate the lock semantics becausesw!
rec!and line in algorithm ensures that for any actions pandqthatprec!q we havem p rec!m q .
case ais a write.
v a must be equal to v0 a0 according to the inductive hypothesis that each thread s local state is identical in the recorded and replayed executions.
case ais a read and a2g.
since all writes on a vare serialized by lock of a v the interleaving write set w a ?.
let the latest write to a vbefore ainrec!bew m w must also be the latest write to a vine0.
according to the inductive hypothesis v0 a0 v m w v w .
by the sequentially consistency assumption of e we have v w v a and hence v a v0 a0 .
case ais a read and a g. according to line of algorithm we know that cache t a v v a .
since a cached value dependsonly on the cache s associated thread state the inductive hypothesis tells that we can easily construct the value of cache t a v to be v a and this value is overridden as v0 a0 v a .
all possible configurations are enumerated and we thus conclude thata a0always holds at the end of each iteration.
the algorithm terminates when s ?andt a. at this time t aindicates that our theorem is proved.
the theorem above tells that care is able to correctly reproduce all read and write actions their values and their program order in e. observable failures such as wrong outputs assertion violations and uncaught exceptions are guaranteed to be triggered in e0.
still this is not sufficient for cyclic debugging.
value deterministic replay can be achieved by logging every read value of the failed thread and feeding them in the replay.
however this gives not enough information about how the failure has happened especially in the case that the failure is triggered by a sophisticated thread interleaving.
.
synthesizing sequentially consistent executions it is interesting that care records dependences of actions but only gives value deterministic replay while traditional order based approaches give provable replay guarantee.
intuitively care captures most of critical dependences for synthesizing a sequentially consistent execution.
an example is replaying datarace free programs.
as shown in for such programs any linear extension ofsw!is sequentially consistent.
sincesw!
h care restores sequentially consistent executions for such programs.
another example is programs that for every variable all write values to it are distinct.
cache miss now becomes a sufficient and necessary condition indicating that a value being read is certainly from another thread.
for such programs the log collected by care contains all exact read write dependences.
results in suggest that care would always restore sequentially consistent executions for such programs.
the intuition above suggests that care would mostly replay a sequentially consistent execution.
observing that read write dependences are hard to determine only when a specific value is back and forth written to the same variable together with writes writing other values in between at the same time being fetched by a racing memory read as shown in figure .
one can reasonably expect that such hard to determine memory accesses are very rare in practice because breaking any part of this condition can make care easily synthesize a sequentially consistent execution.
this motivates us to formulate an enhanced replay guarantee statement theorem .care can determine orders of interleaving read and write actions to obtain sequentially consistent executions in replay if for any read action r2a either of the following two conditions holds 8w2w r v w v r 8w1 w22w r v w1 v w2 v r .
proof .
sketch since write total order of each variable is contained inrec!
we can always find the latest write action w writing variable r vsuch that w rec!r.
in case we manually add rrec!wfor all w2w r .
this extra dependence conformssc!because for any w2w r wsc!r would never be the case otherwise there must be w02w r such thatv w0 v r according to sequential consistency ofsc!.
this contradicts our assumption that v w0 v r .
in case if v w v r rnever violates the sequential consistency assumption in any linear extension ofrec!.
in case that462v w v r there must exist a write action that writes v r between w andrinsc!and such an action must be in w r .
let the earliest write in w r bewe wesc!rmust be the case because w sc!r.
therefore we manually add werec!r.
since all writes in w r have the same value written the ordering issue is resolved.
the above analysis inspires us to use a heuristic rule giving those reads r ghigher priority so that they can be executed early at replay if a pair of interleaving read and write action could both be executed at a time one should try to schedule the read action first because executing the write action might prevent the read from returning the desired value.
therefore to synthesize a sequentially consistent execution care tries to schedule read actions interleaving with other write actions as early as possible.
only in case that the desired value of a read action is inconsistent with the one in the heap care suspends the reading thread and registers a listener on the desired value.
immediately after the desired value is written by another thread the reading thread is resumed.
another heuristic used in care is based on the observation that interleaving read and write actions can be reduced by recording more artificial dependences inrec!.
this leads to a straightforward strategy that randomly groups variables together into bundles and collects dependence log at a granularity of bundle.
in the actual implementation of care the precedence order is represented by sequence numbers and variable grouping is equivalent to binding several separate sequence numbers into a single atomic counter.
in this setting the heuristic neither increases the log size nor combines fine granularity locks into coarse ones.
we evaluate the usefulness of the two heuristics in section .
the results show that care s practical replay guarantee is quite satisfactory that it successfully produced sequentially consistent executions for all of our benchmarks including those who contain extensive benign data races.
.
performance care is efficient both at record and at replay.
at record a vast majority of read actions incur little overhead as they require care to perform only thread local cache queries without synchronization.
besides only a small portion of all inter thread dependences have to be logged.
now we present a theorem and a series of facts to explain the intuition why care is able to generate small log size with low runtime overhead.
theorem .suppose that the cache capacity is infinite.
if a variable is accessed by only one thread during a time period at most one memory access log entry for this variable is kept.
proof .
sketch suppose that variable vdoes not escape the thread scope during the time period.
the first access to vmight generate a log entry because last v may possibly not refer to the current thread and cache t v is immediately updated.
since cache t v and last v are not altered by other threads none of subsequent memory accesses needs any log.
there are also intuitive facts below explaining why care achieves both log size and runtime efficiency.
their impacts are presented in the following section of evaluation.
.memory accesses exhibit both temporal and spatial locality.
cache precisely identifies successive memory accesses happening in the same thread reducing both the log size and runtime overhead.
.a write action causes at most t 1cache misses assuming that there are tthreads in a program s execution.
however in practice such kind of broadcasting is quite rare mostdata are shared among only two threads.
together with the phenomenon of memory access locality cache would be very efficient in reducing synchronization needed for read actions.
.thecare algorithms are orthogonal to many of existing widely used optimization techniques such as static analysis e.g.
escape analysis and race free analysis lock grouping etc.
these techniques can be applied for further reducing the runtime overhead and log size.
.care records dependences of actions by logging sequence numbers.
these numbers typically follow specific patterns leading to a low entropy log.
if a log is needed to be archived one can further reduce the log size by compressing it with a standard tool e.g.
gzip .
finally note that finding a linear extension of a partial order can easily be done in linear time.
when cache size is fixed any operation performed on it takes only o time.
moreover the first heuristic requires only value listener registration which can also be implemented by hashing in o time.
it is thus straightforward that the time complexity of care replay algorithm is linear of the trace size.
.
evaluation in this section we present our care implementation and evaluate it with real world benchmark programs.
.
implementation we implemented care on the basis of our dpac tool which is an encapsulation of jvm tool interface and asm bytecode transformation libraries .
care instruments java bytecode at class loading to collect log at record and to insert breakpoints at replay.
according to section care virtually logshg h piby maintaining each variable s access sequence numbers.
an action aof sequence number smeans that it is the s th action performed on variable a v. we store such sequence numbers compactly in each thread s local storage each time when thread tis about to log a dependence on variable v v s sequence number is increased and logged in t s local log.
non conflicting actions are logged in parallel.
each thread s log is a list of tuples hc si.
each tuple denotes a logged action of sequence number s with cunlogged actions skipped before.
our variable grouping heuristic combines many separate sequence numbers into a single atomic counter.
since updates to sequence numbers are synchronized by care s instrumentation our atomic counters can guarantee to restore actual orders for these separate variables.
at replay the instrumented program generates a breakpoint each time before an action is actually executed.
breakpoint suspends the current thread.
according to algorithm when all threads are suspended our scheduler selects an eligible thread to execute for one action.
if multiple threads are eligible the heuristic of action prioritization favors such a thread whose next action ris read and cache r t r v is identical to the value of r vin the heap.
we also implemented state of the art deterministic replay work leap on the same base as our care for comparison.
we make sure that the low level implementation and optimization settings are both identical for comparison fairness.
for stride we did not implement it for comparison because it contains many optimizations yielding very impressive evaluation results but not available for us.
we tried an alternative way by making leap as the normalized line to connect the evaluation results of care andstride in the sense that leap andstride are from the same authors and they have made careful comparisons between them in .463table comparison of care andleap under benchmark programs benchmarkcare leap overhead log size s unordered resolved ?
overhead log size s avrora .
.18mb 23k y .
.3mb batik .
.51kb y .
.32kb h2 .
.2mb y .
.4mb lusearch .
.53mb y .
.0mb sunflow .
886mb y 6029mb tomcat .
.80mb y .
.5mb xalan .
.6mb y .
143mb tsp .
.84mb y 570mb moldyn .
.1mb y .
303mb .
evaluation results .
.
experimental settings our experiments were conducted on a dell poweredge server running linux kernel .
with dual core intel xeon e5645 processors and 48gb memory.
we disabled the turbo boost and the hyper threading technologies to alleviate unpredictable performance thrashing.
following existing replay work we chose a wide range of benchmarks from dacapo suite including concurrent programs on network simulation avrora vector imaging batik database transactions h2 text search lusearch graphics rendering sunflow web server tomcat and document processing xalan .
we also chose two scientific benchmarks used in tsp solving the travelling salesperson problem and moldyn simulating molecular dynamics.
we used available default settings for all benchmarks in our experiments.
in the following we first validate care s main claims that it can reduce the record time overhead and log size at the same time synthesized valid executions at replay.
we then study specific details ofcare .how does cache organization impact care s runtime overhead and log size?
.how does variable grouping heuristic impact care s runtime overhead?
.
.
runtime overhead and log size reduction our evaluation results of care andleap are presented in table .
columns and present the runtime overhead of care andleap and columns and present their log size.
all runtime overhead and log size data are normalized based on the running time of non instrumented executions i.e.
normalized log size per second is the total log size divided by the execution time of a noninstrumented execution .
columns and present evaluation results of our heuristics for synthesizing sequentially consistent executions.
the amount of unordered conflicting memory access pairs in the log is shown in column while results in column indicate whether care successfully resolved these ordering issues for synthesizing sequentially consistent executions y denotes successful resolve .
regarding the record cost for the dacapo suite benchmarks care exhibited on average .
and .
reduction on runtime overhead and log size respectively over leap .
for scientific benchmark tsp care was even faster and incurred smaller 7derby specjbb andice evaluated in were not included in our evaluation because they are neither available to us nor contain available default settings.table comparison of care andstride with normalized values benchmarkcare stride overhead log size overhead log size avrora .
.
.
.
batik .
.
.
.
h2 .
.
.
.
lusearch .
.
.
.
sunflow .
.
.
.
tomcat .
.
.
.
xalan .
.
.
.
tsp .
.
.
.
moldyn .
.
.
.
log.
this is because threads in tsp frequently read from shared memory without any lock protection.
leap had to serialize all these accesses while our care did not have to because cache hit is almost always the case.
similar situation occurred to moldyn and care also outperformed leap .
we also observed that care s heuristics worked well in synthesizing sequentially consistent executions because the evaluated benchmarks are almost correctly synchronized and there were not many actions left unordered.
as mentioned earlier we indirectly compare our care with stride based on leap by aligning two leap implementations data.
we had to do so as two leap implementations performed quite differently for different benchmarks for unknown reasons.
for benchmark avrora lusearch andxalan ourleap implementation incurred much smaller runtime overhead than reported in .
on the contrary for tomcat h2andsunflow leap s log size reported in was much smaller than ours.
we conjecture that this is due to the optimizations used in which can benefit some programs but may harm the others.
after leap data alignment leap data are all normalized to we obtain results in table .
according to absolute comparisons in table and in and relative comparisons in table we can safely conclude that the current care andstride implementations have comparable record costs.
for some benchmarks care outperformed stride but for some other benchmarks stride beat our care .
nevertheless we note that stride has done extensive engineering optimizations like escape analysis introduced by one whole page in to additionally reduce runtime overhead and log size.
therefore we believe that our care still has the potential to be further improved by additional engineering efforts.
number of cache setsruntime overhead way way way way a study of runtime overhead against cache organization32 number of cache setslog size mb s way way way way b study of log size against cache organization1 number of group s normalized runtime overhead avrora tomcat tsp moldyn c study of runtime overhead against variable grouping figure impacts of cache organization and variable grouping .
.
impact of cache organization to study the impact of cache organization we studied softwareimplemented k way set associative cache with nsets from sets to sets using the widely used lru strategy for replacement.
we conducted the study on the tsp benchmark because it contains frequent racing memory reads realizing that cache hit can largely affect the performance of such benchmarks.
figures a and b show the study results.
we observe that increasing cache size can greatly reduce the runtime overhead and log size when the cache size is small e.g.
less than sets .
when the cache is larger than a moderate size e.g.
sets the improvement becomes less significant and such a moderate size can be easily deployed on a commodity computer.
studies of other benchmarks also give similar results and we therefore omit their details.
from these results we conclude that using a moderate sized cache is already sufficient for reducing the record cost in care .
.
.
impact of variable grouping as mentioned earlier our variable grouping heuristic succeeded in synthesizing sequentially consistent executions but one may wonder its accompanied cost.
according to a global approach that serializes all write actions can outperform leap and even has comparable performance with stride in many evaluated benchmarks.
this result gives a clue in concluding that our heuristic could be efficient in practice because even the most aggressive variable grouping strategy that groups all variables into a single bundle still allows write actions to different variables to be executed in parallel which is much more efficient than global .
we study the impact of variable grouping in figure c and observe from it that the cost of variable grouping is almost negligible for our four benchmarks as compared to the setting of no variable grouping.
this is mainly because care generates log only at cache miss.
in a commodity processor executing millions of atomic operations only requires milliseconds of time and its out of order execution mechanism does not even suspend a thread if it just waits for an atomic operation to complete.
therefore we can safely use the most aggressive variable grouping strategy i.e grouping all sequence numbers into a single atomic counter to achieve a higher probability of reproducing sequentially consistent executions.
in fact we use such a strategy to produce data in table .
.
related work deterministic replay tools can either be implemented purely at an application level within a virtual machine monitor or evenwith assistance of customized hardware .
care is an application level and software only work but adopts experiences from approaches of other categories.
in a uni processor system deterministically replaying an execution is readily solved because logging thread preemptions and external inputs essentially eliminates non determinism for replay .
however in a multi processor setting racing memory accesses make deterministic replay much more challenging diverse execution paths emerge from interleaving data races.
according to the fundamental approach of taming non deterministic racy memory accesses existing pieces of work can be roughly categorized into search based ones and order based ones.
search based work takes a different fundamental approach than care does which logs only selective information at runtime and synthesizes a valid execution later at replay.
logging strategies for search based work can be very optimistic such that runtime overhead and log size can be largely reduced.
aggressive log reduction on the other hand makes finding a valid execution computationally intractable.
search based work typically only gives best effort replay guarantee or the success of replay depends on powerful state space exploration engine such as an smt solver.
in contrast order based work8 carefully keeps the actual happens before order of executed actions obtaining a partial order over these actions.
failure reproduction is generally achieved by executing a program in guidance of the topologically sorted order of the logged actions of this program.
to distinguish the precedence of variable accesses order based work has to instrument a program with extra synchronizations serializing racing memory accesses.
care also belongs to this category.
despite the fact that all order based work shares a similar idea of enforcing crew semantics and recording memory access orders there are still subtle trade offs for reducing runtime overhead log size and replay cost.
to the best of our knowledge care is the first order based deterministic replay technique striking the balance that both runtime overhead and log size are reduced replay is done at linear time and replay fidelity is only slightly weakened.
in order based deterministic replay logs for successive threadlocal memory accesses can be combined into a single entry thus reducing the log size.
such thread locality can be detected by a value prediction cache as used in idna and stride .
care further observed the cache s ability of finding inexact causal depen8stride considers itself search based but we classify it as order based because it records inexact read write dependences and perform replay by topologically sorting the recorded actions.465dences between memory accesses.
compared with leap its core algorithm can be considered as a special case of care that triggers cache miss at every memory read.
the technique of refetching read value at cache miss in care is similar with the one presented inreemu .reemu uses atomic counters and memory fences to guard shared memory reads which incur implicit synchronizations.
in contrast care s cache requires no extra inter processor synchronization at cache hit.
in reducing synchronizations one piece of ingenious work is stride which eliminates all read synchronizations by logging inexact version numbers and ensures strong replay guarantee.
stride tackles the problem of runtime overhead but not log size.
inference of exact read write dependences requires logging value fingerprint for every read and this substantially increases the log size and suffers from hash collisions.
care logs no such value and hence does not have such issues.
also note that extra synchronization of care is within a small constant factor than that instride in practice because care needs synchronized read only when cache miss is encountered.
all these order based deterministic replay techniques have attractive replay guarantee but with less satisfactory record time overhead.
to reduce this overhead static determinism analysis transitive reduction and domain specific optimizations are proposed .examining non racing memory accesses unnecessarily slows down the instrumented execution.
runtime overhead can be greatly reduced if these accesses on these variables are not instrumented and this is typically achieved by a static program analysis .
care is orthogonal to these deterministic analyses such as escape analysis and lock set analysis.
in extreme end of determinism analysis chimera statically converts racy programs to equivalent race free ones by adding extra synchronizations.
compared with chimera care tries to make racing reads as parallel as possible.
for programs with intensive intended racing reads of shared variables care would have less runtime overhead.
.the key idea of order based deterministic replay is logging causal dependences between read and write actions.
however logging all of them is of course not a minimal setting.
the problem of transitive reduction is addressed in .
actually the cache in care also does an approximate transitive reduction with very low runtime overhead removing some redundant logs.
we can adopt existing transitive reduction techniques for even smaller logs.
.for specific application domains performance of deterministic replay can be further tuned.
for example replaying android applications with gui interactions is studied in .
debugging non determinism caused by external inputs in libraries is discussed in .
they are out of our scope and thus not discussed further.
to replay concurrent executions one can also adopt deterministic execution that ensures a determinsitic execution environment.
however for general applications running on generic operating systems eliminating all non determinism is essentially too costly in the absence of customized hardware s assistance .
finally deterministic replay technology is the cornerstone of numerous testing and debugging tools for concurrent programs.
examples include cyclic debugging data race detection deadlock prediction to name but a few.
we believe that improvements in deterministic replay can considerably benefit these kinds of work.
.
conclusion in this paper we present care a cache guided deterministic replay technique.
by utilizing cache miss as the indicator of record ing necessary memory access orders care reduces a majority of additional synchronizations for read actions runtime overhead and log size compared with conventional techniques while still offering reasonable replay guarantee for debugging.
our experimental results show that care s generated log can be several orders of magnitude smaller than state of the art order based replay techniques.
to the best of our knowledge our work is the first one trying to trade slightly weakened replay guarantee for much reduced record cost in deterministic replay.
we also note that this trade off though might potentially create invalid traces at replay works fine in practice with the aids of our two replay heuristics.
hence it would be interesting to see whether it is possible to further extend this central trade off idea in care to more practical systems e.g.
systems with relaxed memory models .
.