micro interaction metrics for defect prediction taek leey jaechang namx donggyun hanx sunghun kimx hoh peter iny ydepartment of computer science and engineering korea university seoul korea comtaek hoh in korea.ac.kr xdepartment of computer science and engineering the hong kong university of science and technology hong kong jcnam handk hunkim cse.ust.hk abstract there is a common belief that developers behavioral interaction patterns may a ect software quality.
however widely used defect prediction metrics such as source code metrics change churns and the number of previous defects do not capture developers direct interactions.
we propose novel micro interaction metrics mims that leverage developers interaction information stored in the mylyn data.
mylyn is an eclipse plug in which captures developers interactions such as le editing and selection events with time spent.
to evaluate the performance of mims in defect prediction we build defect prediction classi cation and regression models using mims traditional metrics and their combinations.
our experimental results show that mims signi cantly improve defect classi cation and regression accuracy.
categories and subject descriptors d. .
distribution maintenance and enhancement restructuring reverse engineering and reengineering d. .
metrics product metrics k. .
management of computing and information systems software management software maintenance general terms algorithms measurement experimentation .
introduction defect prediction has been a very active research area in the eld of software engineering taek lee was a visiting phd student at the hong kong university of science and technology when this work was carried out.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
esec fse september szeged hungary.
copyright acm ... .
.
.
many e ective new defect prediction models including new metrics have been proposed.
among them source code metrics cms and change history metrics hms are widely used and yield reasonable defect prediction accuracy.
for example basili et al.
used chidamber and kemerer metrics and ohlsson et al.
used mccabe s cyclomatic complexity for defect prediction.
moser et al.
used the number of revisions authors and past xes and age of a le as defect predictors.
in addition to software cms and hms developers behavioral patterns are also believed to be an important factor a ecting software quality.
ko et al.
identi ed possible causes for programming errors using a breakdown model of cognitive chains.
deline et al.
surveyed developers work habits and found that work interruptions or task switching may a ect programmer productivity.
their studies imply a correlation between developers behaviors and software quality.
speci cally unexpected or abnormal behaviors may introduce bugs and cause software defects.
therefore it is desirable to use developers interaction information when building defect predictors.
however current defect metrics such as cms and hms do not directly capture developers interactions since current version control systems or bug report systems do not record developers interactions.
we use mylyn a context storing and recovering eclipse plug in to capture developers interaction events.
examples of such events include selecting and editing software source les.
additionally the order and time span of each event are also provided by mylyn.
since developers interaction patterns may a ect software quality and developer productivity metrics based on developers interactions could be important indicators to predict defects.
in this paper we propose micro interaction metrics mims based on developers interaction information stored in mylyn.
for example nummultitasks measures task complexity by observing the number of ongoing tasks.
repeatedlyselectedfilenum andrepeatedlyeditedfilenum measure repeated work activities by counting selecting and editing events for the same le.
the full list of mims and their descriptions are shown in appendix.
to evaluate the performance of mims in defect prediction we build defect prediction classi cation and regression models using mims cms hms and their combinations.
we evaluated various defect prediction models on eclipse subprojects which include the mylyn data.
our evaluation results showed that mims signi cantly improveprediction accuracy.
the defect classi cation f measure is .
with mims while it is only .
with the combination of cm and hm.
the mean square error of the defect regression model with mims is .
while it is .
with the combination of cm and hm.
these results concur with previous ndings .
since developers interactions a ect software quality and programmer productivity mims that capture a certain degree of developers interactions play an important role in defect prediction.
our paper makes the following contributions mims which capture developers interaction information from the mylyn data.
empirical evaluation of the role of mims in defect prediction.
in the remainder of the paper we start by presenting mims in section .
section describes our experimental setup including various defect prediction models metrics used and evaluation measures.
section presents results of various defect models with without mims.
section discusses threats to validity of our study.
we discuss related work in section and conclude the paper with future directions of our research in section .
.
micro interaction metrics this section proposes mims based on data collected by mylyn an eclipse plug in.
we brie y introduce mylyn and its data in section .
and describe the proposed mims in section .
.
.
mylyn mylyn is an eclipse plug in that records the context of developers task such as editing or selecting les .
the recorded context is restored when developers want to resume the task.
in this way even after a task switching developers can focus on the les they have previously worked on.
the stored context can be shared among developers and help other developers understand what les were browsed and edited for the task.
the mylyn data are stored as an attachment to the corresponding bug reports in the xml format.
the data include events and their attributes performed by developers.
currently mylyn records six types of events selection edit command propagation prediction and manipulation as shown in table .
when a developer selects a le a selection event occurs.
edit events are recorded when developers edit a le.
propagation events occur when a developer uses automatic refactoring features in eclipse.
for example les can be modi ed automatically via eclipse refactoring feature and this is recorded as a propagation event.
table event type in the mylyn data type description selection select a le in the explorer edit edit a le in the editor command invoke command by developer propagation propagated interaction prediction predict future interaction manipulation manipulate event is recorded with attributes including startdate end date structure handle and degree of interest the start date and end date attributes represent the starting and ending time of the event.
the structure handle attribute denotes corresponding les of the event.
for example for edit events the structure handle attribute indicates which le is edited.
the indicates developer s interest in the corresponding le.
the of a le increases when developers select or edit the le.
help developers identify more less important les for the task.
currently are automatically computed based on the frequency of developers interactions .
when developers double click a le and open it in an editor an edit event with no time spent zero time edit occurs.
the time spent can be easily computed from end date and start date attributes.
the zero time edit event means the start date and end date of the event are the same.
when developers change the content of the le such events are recorded as non zero time events.
we distinguish between these two types of events because it is important to separate real edits from simple double clicks.
more information about the mylyn data is available at the mylyn project home page .
.
design of mims the design principle of mims is quantifying the complexity and intensity of developers interaction activities such as browsing or editing of les.
based on this principle we designed two levels of mims le level and task level.
the le level mims capture speci c interactions for a le in a task.
for example numeditevent represents the speci c le edit events in one task.
the task level mims represent properties per task.
for example timespent shows the time spent on a given task.
we designed three categories of le level mims e ort since mylyn stores developers interactions this category measures developers e ort for a given le such as the number of events on a le.
interest by using we infer developers interest in a speci c le.
intervals this category measures time intervals between events.
task level mims include six categories e ort similar to the e ort category in the le level we measure the e ort made for a given task.
distraction this measures developers distraction for corresponding tasks such as low and non java le edit events.
work portion all events in one task are divided into three event periods before beginning the rst edit edit and after nishing the last edit sections.
this category measures how much time was spent for each section.
repetition there are some repeated events such as repeated selection or editing of the same le.
this category counts this kind of repeated events.
task load this category measures the task load by observing the number of simultaneously on going tasks nummultitasks .
event pattern since mylyn contains interactions we identify common patterns of sequential events.
this cat egory captures the number of identi ed sequential patterns.
the complete list of mims with descriptions is shown in appendix.
.
experimental setup this section describes our experimental setup including data collection prediction models and evaluation measures.
.
bug prediction process the commonly used le level bug prediction process is used for our experiments as shown in figure .
software archivesinstances labeling counting buggy clean metrics feature extraction training instances creating a training corpusmachine learner building a prediction modelinstance classification regression prediction evaluation figure overall steps of bug prediction process first we collect les as instances in the machine learning sense and count post defects for each le.
for the regression model we predict the defect numbers.
for classi cation we label a le as buggy if it has any post defect post defect number or clean otherwise.
the detailed post defect counting process is described in section .
.
then we extract mims cm and hms for each instance as explained in sections .
and .
.
finally we train prediction models using machine learning algorithms implemented in weka .
the trained prediction models classify instances asbuggy orclean classi cation or predict the post defect numbers regression .
detailed cross validation models and evaluation measures are explained in section .
.
.
data collection we extracted a total of mylyn tasks from eclipse bugzilla attachments between dec 20051and sep .
to explicitly separate metrics extraction and post defect counting periods we set arbitrary time split points pas shown in figure .
it is important not to collect any metrics from the post defect counting period.
time prepresents the present and our model predicts future defects after p using metrics from the past to p. thus we computed all metrics mims cms and hms of instances before p and counted post defects after p. in our experiments we used various time split points and to compare prediction results with di erent time split points.
for example table shows the number of instances and defect ratios for eclipse subprojects from the time split point.
the goal of various time splits is evaluating mim models by following the random split convention widely used in the literature .
to collect metrics and count defects instances must exist in both time periods i.e.
after and before time p .
if a le does not exist in the metrics collection period there is 1mylyn was released and widely used from dec .table collected le instances and post defects for the split.
subjects of instances les of defects mylyn .
team .
etc.
.
all .
no metrics to use for prediction.
on the other hand if a le does not exist in the post defect counting period the defect number for the le is always zero which is misleading.
to use only les which existed in both periods we checked their existence using eclipse cvs.
for this reason when we use di erent time split points and the number of instances changes.
f3.java f1.java f2.java f1.java f2.java f2.java f3.java time pdec sep post defect counting period f3.java f1.java f1.java f2.java f3.javatask1 task2 task3 task4 task5 task6 task7 metrics extraction period figure time split for metrics extraction and postdefect counting periods.
to count post defects we used edited le information in mylyn tasks.
since each task is directly attached to a bug report we checked if the corresponding bug report was a xed bug.
if it was not xed or not a bug such as featureenhancement or trivial we assumed the edited les for the particular bug report are not xes.
we marked the edited les only for xed bug reports as xed and increased the defect numbers for the les.
for example suppose tasks and in figure were attached to xed bug reports and task is attached to a feature enhancement bug report.
in this case the post defect number of f3.java is two since task is for feature enhancement.
in this way we could avoid false positives in marking defects.
the following sections describe metrics extraction techniques and evaluation measures for prediction models in detail.
.
extraction of mims all mims listed in appendix were extracted from the mylyn data.
since our models are le level defect predictors we need to compute mims for each le.
computation of le level mims is straightforward.
we rst compute le level mims for a given le.
if the le is edited multiple times in the metrics extraction period then it will have multiple metric values.
we just average and total the multiple values and use the totaled values as features in the machine learning sense of the le.
for task level mims such as timespent of a task rst we propagate the metric values to all edited les in the task.
then the propagated metric values are regarded as le level metrics.
if a le is edited multiple times and it has multiple metric values propagated from tasks we average the values and use them as features.
.
extraction of cms and hms we collected cms at time pas shown in figure since cms can be extracted from a snapshot.
we used the understand tool to extract cms.
the understand tool extracts le level and class level metrics such as chidamber and kemerer and object oriented metrics.
if a le has more than one class we derived le level metrics from multiple class level metrics.
the understand tool mostly provides two kinds of metrics avg and count .
to generate le level metrics from multiple classes in a le we averaged avg class level metrics.
however when we get le level metrics from count classs level metrics we added the values together.
we used all cms for our experiments.
selected cms are listed in table .
table list of selected source code metrics cms metrics description countlinecode lines of code countsemicolon of semicolons countstmtdecl of declarative statements sumessential sum of essential complexity of methods cntclasscoupled coupling between object classes cbo cntclassderived of child classes noc cntdeclmethod of local methods nom cntdeclmethodpublic of local public methods nopm mxinheritancetree depth of inheritance tree dit pcntlackofcohesion lack of cohesion lcom in addition we collected hms following moser et al.
s approach .
all hms were collected from the change history stored in eclipse cvs repository2during the metrics extraction period as shown in figure .
table lists hms used in our experiments.
the refactorings metrics indicates if a le change is refactoring .
this is determined by mining cvs commit logs if they contain the keyword refactor we assume it is a refactoring.
we counted the number of all refactored revisions of a le in the metrics extraction period.
the agemetric indicates the period of le existence .
the bugfixes metric represents the defect numbers in the metrics extraction period.
to compute this we mined commit logs to search explicit bug ids in the logs.
then we checked the bug reports and if they were xed bugs not feature enhancement we marked the change as bug x. in addition we searched for speci c keywords bugor x3 which indicate bug x changes .
if change logs had such keywords we marked the changes as bug xes .
.
performance measures in this section we explain evaluation methods used in our prediction models.
we compare the prediction performance of mims with other metrics.
thus we build a model by each metrics i.e.
mim cm hm and combination of them i.e.
cm hm .
to evaluate our prediction models we used fold cross validation which is widely used to evaluate prediction models .
we repeated fold cross validation times for each prediction model on each different time split to validate the prediction performance of mims by t test.
3the keywords post x and pre x are excluded .table list of history metrics hms metrics description revisions of revisions of a le refactorings of times a le has been refactored bugfixes of times a le was involved in xing bugs authors of distinct authors committing a le loc added sum of the lines of code added to a le max loc added maximum number of lines of code added aveloc added average lines of code added loc deleted sum of the lines of code deleted in a le max loc deleted maximum number of lines of code deleted avg loc deleted average lines of code deleted codechurn sum of added loc deleted loc max codechurn maximum codechurn for all revisions avecodechurn average codechurn per revision age age of a le in weeks weighted age age considering loc added .
.
classification to evaluate performance di erences between di erent prediction models we used f measure.
usually f measure represents harmonic mean of precision and recall.
we rst computed precision and recall values of buggy instances and then we obtained f measures.
the following outcomes were used to de ne precision recall and f measure predicting a buggy instance as buggy b !b predicting a buggy instance as clean b !c predicting a clean instance as buggy c!b .
we use the above outcomes to evaluate the prediction accuracy of our prediction models with the following measures precision the number of instances correctly classi ed as buggy nb!b over the number of all instances classied as buggy.
precision p b nb!b nb!b nc!b recall the number of instances correctly classi ed as buggy nb!b over the total number of buggy instances.
recall r b nb!b nb!b nb!c f measure a composite measure of precision p b and recall r b for buggy instances.
f measure f b p b r b p b r b .
.
regression by using linear regression models we predicted the number of post defects.
to compare the prediction performance of models of mims and other metrics we calculated correlation coe cient mean absolute error and root mean squared error .
correlation coe cient measures the correlation between predicted and real defect numbers.
if the correlation coe cient is closer to the metrics are more correlated to post defects.
both mean absolute error and root mean squared error represent the di erence between predicted and actual post defects numbers.
if both error values of a prediction model are less than others it means the model has higher prediction accuracy.
to compare the pre diction model of mims with those of cms hms and their combination we measured these three values.
we also repeated fold cross validation times to validate regression models for each metrics by t test.
.
.
t test to check statistical signi cance of prediction performance of mims and cms hms and cm hm the simple t test was used .
we checked if mean of f measure values of mims was not equal to the mean of f measures of cm and hm.
speci cally the null and alternative hypotheses for t test are h0f measure mean of cm hm is equal to the f measure mean of mim.
h1f measure mean of cm hm is not equal to the fmeasure mean of mim.
i.e.
mims have better performance if the mean value is higher we rejected the null hypothesis h0and accepted the alternative hypothesis h1if the p value was smaller than .
at the con dence level .
.
dummy classifier to evaluate the performance of classi ers using mims cm and hms we introduce a baseline dummy classi er guessing a change le as buggy or clean in a purely random manner.
since there are only two labels buggy and clean changes the dummy predictor could also achieve certain prediction accuracy.
for example if there are .
of changes in a project are buggy by predicting all changes as buggy the buggy recall would be and the precision would be .
.
it is also possible that the dummy predictor randomly predicts a change as buggy or clean with .
probability.
in this case the buggy recall would be .
but the precision will still be .
.
we used the f measure of the dummy predictor as a baseline when showing the classi cation results.
we computed the dummy f measure assuming the dummy predictor randomly predicts as buggy and as clean.
for example for a project with .
buggy changes as shown in table the dummy buggy f measure is .
.
.
results this section presents performance of bug prediction models using mim cm hm and their combinations.
.
result summary this section provides a quick summary of results from di erent experiments.
details of settings and results are explained in subsections.
mim outperforms existing metrics cm and hm and their combination cm hm for di erent subjects section .
.
.
mim outperforms cm hm and cm hm for di erent classi cation algorithms section .
.
.
mim outperforms cm hm for di erent split points section .
.
.
among the top important metrics for classi cation models are from mims.
all top important metrics are from mims section .
.
.
mim outperforms cm hm for regression models section .
.
.
predicting buggy files .
.
different subjects to evaluate the performance of mims for various subjects we built defect prediction models using j48 decision tree for three eclipse subprojects mylyn team and etc.
from the split point as shown in table .
to build the models mim cm hm cm hm and mim cm hm were used.
we use f measure to evaluate prediction performance as described in section .
.
.
the ten fold cross validation is used to train and test models.
since ten fold cross validation randomly samples instances and puts them in ten folds we run the ten fold cross validation times to avoid sampling bias.
figure shows f measure values for various eclipse subjects and various metrics.
f measure values vary but the trend is clear mim outperforms cm hm and their combination cm hm for all team and etc.. for mylyn the f measure of mim is slightly lower than that of cm hm but mim cm hm outperforms cm hm.
in addition the dummy f measure values are shown as a baseline the solid line in figure .
generally cm and hm outperform the baseline and mim signi cantly outperforms the baseline.
trialsf measure0.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.8mim cm hm 2060100mim 2060100cm hm 2060100cm 2060100hm 2060100all mylyn team etc.
figure performance comparison by di erent subjects classier j48 decision tree .
the dummy fmeasure is shown as a solid line.
table shows the mean of f measure values from ten fold cross validations and their statistical signi cance.
if f measure values of mim cm hm or mim are signi cant p value .
in comparison to cm hm the corresponding values are in bold.
for example the f measure .
of mims in team is better than that of cm hm .
and it is statistically signi cant.
for the mylyn project the f measure .
of mim is not better than that of cm hm .
.
however mim cm hm outperforms cm hm which indicates that mim complements cm hm to yield better prediction accuracy.
generally the results in table indicate that mim and or mim cm hm outperform traditional metrics cm hm .
in addition dummy f measures are shown in table .
table f measure mean values of each metrics in di erent subjects.
the f measures in bold indicate the value di erence in comparison to cm hm is statistically signi cant.
p value .
subjects mim cm hm mim cm hm cmhm dummy all .
.
.
.
.
.
mylyn .
.
.
.
.
.
team .
.
.
.
.
.
etc.
.
.
.
.
.
.
.
.
different machine learner this section compares the results of prediction models using three widely used classi cation algorithms bayesian network j48 decision tree and logistics in weka .
all instances with the time split in table are used for this experiment.
figure shows f measures from ten fold cross validations.
the f measures vary but they show a trend that mim outperforms cm hm and their combination cm hm in all algorithms.
trialsf measure0.
.
.
.
.
.
.
.
.
.
.
.
.
.
.5mim cm hm 2060100mim 2060100cm hm 2060100cm 2060100hm 2060100bayesnet decision tree j48 logistics figure performance comparison by di erent algorithms all instances .
the dummy f measure is shown as a solid line.
table shows mean f measure values from ten fold cross validations.
the f measure mean .
of mims in the bayesnet is better than that of cm hm .
.
if f measure mean values of mim cm hm or mim are statistically signi cant in comparison to cm hm we mark the values in bold.
for all algorithms mim outperforms cm hm with statistical signi cance.
table f measure mean values of each metrics in di erent algorithms.
the f measures in bold indicate the value di erence in comparison to cm hm is statistically signi cant.
p value .
measures mim cm hm mim cm hm cmhm dummy bayesnet .
.
.
.
.
.
decision tree .
.
.
.
.
.
logistics .
.
.
.
.
.
.
.
different split points to evaluate prediction performance of mims on di erent time splits we built prediction models using three di erent time split points and as explained in section .
.
table shows corpus information.
since we collected instances before and after time point p the number of instances varies for di erent time split points section .
.
the defect ratio varies since we have di erent periods for post defect counting as shown in figure .
table sample conditions per split period time split point p of instances of defects apr.
.
mar.
.
sep. .
figure shows f measure values from ten fold cross validations.
it indicates that mim outperforms cm hm for three di erent time split points.
trialsf measure0.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.7mim cm hm 2060100mim 2060100cm hm 2060100cm 2060100hm figure performance comparison by time split all instances classi er j48 decision tree .
the dummy f measure is shown as a solid line.table shows f measure mean values of mims and values are in bold if they are statistically signi cant in comparison to cm hm.
for example the f measure mean .
of mim in the time split point is better than that .
of cm hm and it is statistically signi cant.
as we observe in table mim outperforms cm hm for all time split points.
table f measure mean values of each metrics at di erent split points.
the f measures in bold indicate the value di erence in comparison to cm hm is statistically signi cant.
p value .
measures mim cm hm mim cm hm cmhm dummy .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
metrics effectiveness analysis to evaluate the e ectiveness of each metric for classi cation we measured the information gain ratio of mims cms and hms and ranked them accordingly.
all collected instances and post defects from the split point as shown in table are used for this analysis.
the information gain ratio indicates how well a metric distinguishes labels i.e.
buggy or clean of instances.
even though the metrics e ectiveness may di er based on machine learning algorithms generally metrics with a high information gain ratio is regarded as important .
top ranked metrics among metrics based on the gain ratio are shown in figure .
among the top important metrics for classi cation models metrics are from mims.
especially the top metrics are mims.
the best metric is numlow followed by numpatternexsx and timespentonedit .numlow represents the number of low editing events and editing low might a ect software quality.
numpatternexsx captures the event pattern of editing and selecting error prone les consecutively.
this pattern could be a defect prone interaction.
the average time spent on editing events is also an important metric to predict defects.
.
predicting defect numbers to evaluate the regression performance using mims we built prediction models using linear regression with the time split point.
mim cm hm and their combinations were used to build regression models.
we repeated fold cross validation times and computed mean values of correlation coe cient mean absolute error and root mean squared error .
figure shows correlation coe cients of various metrics.
the correlation coe cient of mim is better than cm hm which indicates that prediction results using mims are more correlated to real defect numbers.
in terms of error mims yield lower errors than cm hm as shown in figure .
table shows mean values of measures.
statistically signi cant values are in bold.
for example correlation coe cient .
of mims is better than that .
of cm hm and it is statistically signi cant.
in terms of error the mean absolute error .
and root mean squared error .
of mim are lower than the values of cm hm .
and .
normalized information gain ratiofeatures maxcyclomaticstrict cm sumcyclomatic cm sumcyclomaticmodified cm filehierarchydepth mim sumcyclomaticstrict cm bugfixes hm countlinecodedecl cm countstmtdecl cm countlinecode cm weightd age hm age hm numpatternsyex mim cntclasscoupled cm revisons hm numpatterneheh mim timespent mim numpatternsxsy mim numselbeforeedit mim numpatternslsl mim numpatternelel mim numpatternslsh mim numpatternehel mim numpatterneyey mim numpatterneysx mim numpatterneleh mim numpatterneysy mim numpatternsyey mim numpatternexey mim numpatternsxsx mim numpatternshsl mim numpatternsysy mim numpatternsleh mim repeatedlyselectedfilenum mim numpatternehsl mim numpatternexsy mim numpatternehsh mim numpatternelsl mim numpatternsxey mim timespentafteredit mim portionnonjavaedit mim numpatternshel mim timespentbeforeedit mim numuniqueselfiles mim numpatternelsh mim numpatternshsh mim numpatternsxex mim numpatternslel mim numlow mim repeatedlyeditedfilenum mim numpatternsheh mim numpatterneyex mim numpatternexex mim nummultitasks mim timespentonedits mim numpatternexsx mim numlow mim .
.
.
.
.
.0figure top ranked metrics among metrics based on the gain ratio.
respectively .
these results show that mim outperforms cm hm in regression models.
.
predicting cvs log based defects this section introduces prediction results using cvs based defect counting.
as explained in section .
we counted the number of post defects based on edited les recorded in mylyn tasks to avoid false positives in marking xed les.
however it is possible that some developers may not have used mylyn to x bugs i.e.
these xes are not recorded in the mylyn data.
this may lead to biased post defect numbers.
to address this issue we repeated experiments using the same instances but with a di erent defect counting method.
we used traditional heuristics to count post defects by searching for x or bug keywords and bug report ids in change logs.
the change logs containing the keywords or bug ids were marked as x changes.
after counting post defects in this approach we repeated ten fold cross validation times using three di erent classi cation algorithms used in section .
.
.
we used the time split point for this experiment.
the number of instances was the same number as in table .
however the defect rate increased to .
since we counted defects by mining cvs change logs.trialscorrelation .
.
.
.
.
.
.45mim cm hm 2060100mim 2060100cm hm 2060100cm 2060100hm 2060100figure correlation coe cient comparison all instances split point linear regression trialserror0.
.
.
.
.
.
.
.
.
.
.
.9mim cm hm 2060100mim 2060100cm hm 2060100cm 2060100hm 2060100meanabsoluteerr rootmeansquarederr figure error comparison all instances split point figure and table show f measure values for each metrics and classi cation algorithms.
for bayesnet andlogistics mim cm hm turns out to have the best overall performance.
for decision tree hm is slightly better than mim cm hm.
we would like to point out that in this experiment using mims alone does not yield better performance than cm or hm.
one possible explanation is that even though mylyn is widely used some of the tasks are still performed without it.
in this case we lost developers interactions for these tasks and the information loss may a ect the defect prediction performance.
however this issue will be automatically addressed when all les are edited using the mylyn plug in.
another reason could be the false positives in cvs based defect counting and they may a ect the defect prediction performance.
.
threats to v alidity we have identi ed the following threats to validity.
systems examined might not be representative.
since mims rely on the mylyn data we intentionally chose subprojects which include the mylyn data.
we might have a project selection bias.
in this sense our approach using mims is not generally applicable for projects which do not use mylyn.table correlation coe cient of mim was validated against cm hm metrics.
the values in bold indicate the value di erence in comparison to cm hm is statistically signi cant.
p value .
measures mim cm hm mim cm hm cmhm correlation coe cient .
.
.
.
.
mean absolute error .
.
.
.
.
root mean squared error .
.
.
.
.
trialsf measure0.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.55mim cm hm 2060100mim 2060100cm hm 2060100cm 2060100hm 2060100bayesnet decision tree j48 logistics figure performance comparison of prediction models by cvs based labeling all instances time split point .
the dummy f measure is shown as a solid line.
systems are all open source projects.
all subprojects examined in this paper are developed as open source projects.
hence they might not be representative of closed source projects.
commercial software developers may have di erent micro level interaction patterns.
defect information might be biased.
we collected defect information from changed les in cvs and edited les recorded in the mylyn data.
in addition we veri ed whether the corresponding bug reports were really xed.
however our defect information might be biased since some les are edited without using mylyn and developers may not leave explicit bug ids in cvs change logs.
table f measure mean values of each metrics in cvs based labeling.
the f measures in bold indicate the value di erence in comparison to cm hm is statistically signi cant.
p value .
measures mim cm hm mim cm hm cmhm dummy bayesnet .
.
.
.
.
.
decision tree .
.
.
.
.
.
logistics .
.
.
.
.
.
.
related work .
defect prediction software defect prediction is a very active research area in software engineering.
researchers have proposed new defect prediction algorithms and or new metrics to e ectively predict defects.
source code metrics such as complexity metrics are widely used for defect prediction since there is a common understanding that complicated software may yield more defects.
for example basili et al.
used chidamber and kemerer metrics and ohlsson et al.
used mccabe s cyclomatic complexity for defect prediction.
recently change history based metrics have been proposed and widely used for defect prediction.
nagappan et al.
proposed the code churn metric which is the amount of changed code and showed that code churn is very e ective for defect prediction.
moser et al.
used the number of revisions authors past xes and age of a le as defect predictors.
kim et al.
used previous defect information to predict future defects.
hassan adopted the concept of entropy for change metrics and found their approach is often better than the code churn approach and the approach based on previous bugs .
d ambros et al.
conducted an extensive comparison of existing bug prediction approaches using source code metrics change history metrics past defects and entropy of change metrics .
they also proposed two noble metrics churn and entropy of source code metrics.
defect metrics other than cms and hms have also been proposed.
zimmermann and nagappan predicted defects in windows server using network analysis among binaries .
bacchelli et al.
proposed popularity metrics based on e mail archives .
they assumed the most discussed les are more defect prone.
meneely et al.
proposed developer social network based metrics to predict defects .
these proposed metrics play an important role in defect prediction and yield reasonable prediction accuracy.
however they do not capture developers direct interactions.
proposed mims are the rst metrics using developer interaction data to predict defects.
we have also showed that developers interaction based metrics outperform traditional metrics such as cms and hms.
.
developer interaction history in recent years researchers used developer s interaction history for facilitating software development and maintenance.
zou et al.
proposed how to detect interaction coupling from task interaction histories .
their case study showed the information of interaction coupling is helpful to comprehend software maintenance activities.
robbes and lanza proposed a code completion tool based on programmer s code editing interaction history .
ying and robillard analyzed the in uence of program change tasks based on developers editing behavior and found editing patterns that are helpful for software tool designers .
kersten et al.
suggested task context model and implemented mylyn to store restore task context when developers switch their task context .
as mylyn is getting popular there are many available developers interaction history data captured by mylyn.
murphy et al.
analyzed statistics about ide usage using the mylyn data and showed the most used ui components and commands .
these approaches are similar to our work in that they areleveraging developer interactions to improve software quality.
however they do not address the software defect prediction issue using developers interaction history while we extract mims for defect prediction.
.
conclusions we proposed micro interaction metrics and showed that they signi cantly improve defect classi cation and regression accuracy.
our ndings concur with previous studies which indicates developers interaction patterns a ect software quality.
in our experimental evaluation mim based defect prediction models are applied to eclipse subprojects.
we plan to extend our experiments by adding more subjects including industrial projects.
current mims depend on the mylyn data which may not be available for some projects.
therefore we plan to extend mims leveraging other sources of developers interaction data.
overall we expect that future defect prediction models will use more information from developers direct and micro level interactions for e ective defect prediction.
mims are a rst step in this direction.
all data we used in our experiments are publicly available at .
.