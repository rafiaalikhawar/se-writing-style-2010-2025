docovery toward generic automatic document recovery tomasz kuchta cristian cadar imperial college london department of computing queen s gate london uk t.kuchta c.cadar imperial.ac.ukmiguel castro manuel costa microsoft research station road cambridge uk mcastro manuelc microsoft.com abstract application crashes and errors that occur while loading a document are one of the most visible defects of consumer software.
while documents become corrupted in various ways from storage media failures to incompatibility across applications to malicious modifications the underlying reason they fail to load in a certain application is that their contents cause the application logic to exercise an uncommon execution path which the software was not designed to handle or which was not properly tested.
we present d ocovery a novel document recovery technique based on symbolic execution that makes it possible to fix broken documents without any prior knowledge of the file format.
starting from the code path executed when opening a broken document d ocovery explores alternative paths that avoid the error and makes small changes to the document in order to force the application to follow one of these alternative paths.
we implemented our approach in a prototype tool based on the symbolic execution engine klee.
we present a preliminary case study which shows that d ocovery can successfully recover broken documents processed by several popular applications such as the e mail client pine the pagination tool prand the binary file utilities dwarfdump andreadelf.
categories and subject descriptors d. software engineering e. backup recovery general terms reliability keywords data recovery symbolic execution program analysis permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than the author s must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
ase september vasteras sweden.
copyright is held by the owner author s .
publication rights licensed to acm.
acm ... .
.
introduction the inability to load text documents e mails photos or music files in a desired application is one of the most user visible defects of consumer software.
this type of error can have diverse causes files can be inadvertently corrupted by storage media failures faulty network transfers or power outages documents created by one application cannot be opened in another purportedly compatible application code may have bugs that are triggered by specific file contents or documents can be intentionally altered by attackers to exploit security vulnerabilities.
one example from the last category is a known vulnerability in the pine e mail client in which an e mail with a maliciously crafted from field causes older versions of pine to abort execution while loading it preventing users from accessing e mails.
regardless of how the problematic document was generated the underlying reason it crashes or fails to load in a certain application is that its contents cause the application logic to exercise an uncommon execution path which the software was not designed to handle or which was not properly tested.
sometimes the file can be repaired by fixing its broken structure however this requires a special recovery component that contains a parser for the given file format.
while some popular document readers come with such customised recovery components this is the exception rather than the rule.
in this paper we introduce d ocovery a novel document recovery technique which does not require any prior knowledge of the underlying file format.
d ocovery takes as input a broken document and an application and uses symbolic execution to fix the document to achieve two key goals the recovered document does not cause a failure and it is similar to the broken document.
while creating a document that does not make the program crash is a prerequisite for a successful recovery d ocovery goes beyond this.
it attempts to create a document that avoids the crash while retaining as much of the content of the original document as possible.
we have implemented our approach in a prototype tool which we have successfully used to recover documents processed by several medium sized applications such as pine pr and readelf.
the rest of this paper is organised as follows.
in we define the document recovery problem and give a high level overview of our technique.
we present the various stages and challenges of the technique in discuss implementation details in and evaluate our prototype in .
we discuss the main limitations of our approach in present related work in and conclude in .
.
overview before describing our document recovery technique in detail we further define the problem and introduce some terminology.
for the purpose of this paper the term document is used to refer to an file contents 2file f fopen f.txt r 3char x fgetc f 4char y fgetc f 5if x 6if y crash!
else ok!
else ok!
figure a toy application.
input file processed by a certain program.
documents can be text or binary files.
we consider a document to be broken if it cannot be loaded successfully by an application.
note that our definition is relative to a given application that is a document might be broken in one application but not necessarily in another.
for example a file may crash an older version of some document processing library but be handled correctly by newer versions of the same library or by other parsers of the given document format.
in this paper we often refer to the broken document as the original document.
we consider that a document is successfully loaded if the application does not crash and does not terminate with an error message or an error exit code.
if available more sophisticated oracles can be used and in many cases these can be easily added by users or developers.
the aim of the document recovery process is to change the document so that it is successfully loaded by the application while retaining a high level of similarity between the original document and the recovered document.
the degree of similarity between two documents ultimately depends on the specific file type.
in the absence of a high level document specification we use a byte level similarity metric which we discuss in .
.
in our approach we attempt to recover a broken document without any prior knowledge of the file format i.e.
without any document specification.
instead we use the application code itself to understand how the program reads the document and what input bytes cause the loading error.
for instance if the application crashes after checking unsuccessfully that the sum of the first two bytes in the document is the first two bytes should be altered such that they do sum to .
docovery treats the document as an array d of nbytes which can be formalised as a sequence of length nover an alphabet a f0 255g.
our approach makes use of symbolic execution a popular program analysis technique that can precisely analyse and explore program execution paths.
on each explored path symbolic execution can gather exact mathematical constraints characterising all inputs which take the program along that path.
as an example consider the program in figure .
the program opens a text file reads the first two characters and stores them in variables xandy.
if both characters are greater than or equal to the program crashes.
when symbolic execution follows this path at each branch point it gathers the constraints on the input that take the program along this path x on line and y on line .
as a result the crashing path is precisely characterised by the conjunction of the constraints collected at each branch point x y .table paths explored in the toy application of figure with corresponding constraints and possible recovery candidates.
the first row represents the original broken document.
path lines constraints file ... x y ... x ... x y figure a high level overview of d ocovery .
in symbolic execution the program is executed on symbolic rather than concrete input and variables are represented as expressions over the symbolic input.
on each executed path symbolic execution maintains a path condition pc formula that characterises all the inputs that follow that path.
new constraints are added to the pc when the execution reaches a branch point for which both sides are feasible under the current pc at that point execution is forked following each side of the branch separately and adding the constraint that the branch condition is true to the pc of the then side and that it is false to the pc of the elseside.
in order to decide whether both sides of a branch are feasible aconstraint solver is used to solve the logical formula in the current pc for satisfiability.
a formula is satisfiable if there exists an assignment of concrete values to variables that makes the formula true.
these concrete values satisfying the pc are called a satisfying assignment and are generated by the constraint solver.
they represent an actual input that will drive execution along the same path on which that pc was collected.
in our example the formula x y is satisfiable because there exists a satisfying assignment e.g.
x y .
in d ocovery whose high level overview is illustrated in figure we start by executing the program on the broken document gathering constraints on the side as in the concolic variant of symbolic execution .
in other words we treat program input as symbolic and execute the program symbolically but at each branch point instead of forking and exploring both sides of the branch we use the concrete input values to decide which side of the branch to take.
by using the broken document as a program input that guides the execution we reach the point of program crash and collect a pc that corresponds to the execution path exercised by the broken document.
assuming the contents of the original document are then the collected pc will be x y as discussed above.
this condition encodes allinputs that follow that program path and cause the application to crash.
the relationship between concrete input values and the constraints in the pc is noteworthy the concrete values determine the execution path while the constraints describe all the input values that are valid on this path.
564figure main stages of the document recovery process.
in order to avoid the crash our approach explores alternative execution paths around the one executed by the broken document by systematically negating one of the constraints in the pc say the kthconstraint and dropping the remaining constraints from the k stto the last one.
this simulates an execution in which the program follows the path executed with the original document until it reaches the kthconditional statement where it takes the other side of the branch.
in our example there are two choices the first one is to negate the first constraint and drop the second obtaining the pc x corresponding to the path following the lines and the second one is to negate the second constraint obtaining the pc x y corresponding to the path following the lines .
on each path our technique takes the modified pc and solves it to generate a document similar to the original one.
for example the first pc could be solved to obtain document which differs only in the first character while the second pc could be solved to obtain document which differs only in the second character.
both recovered documents avoid the crash and thus represent valid recovery candidates which can be presented to the user.
table shows the pcs and possible recovery candidates associated with each explored path.
while our high level approach is conceptually simple there are several challenges that need to be addressed when it is scaled to real applications which we discuss in the next section.
.
docovery figure shows the main stages of our document recovery technique d ocovery .
the inputs to the recovery process are the broken document and the source code of the application that fails to load the document.1the process consists of two main stages broken document execution and alternative paths exploration.
the output of d ocovery is a set of recovery candidates.
during the initial broken document execution stage the application is executed twice.
in the first execution pass d ocovery identifies the bytes in the document that are potentially responsible for the failure .
.
on the second execution pass the bytes identified in the first pass are marked as symbolic and a limited number of alternative paths starting nbranch points before the failure are collected along with the associated pcs .
.
since real applications and documents can provide a huge number of possible alternative execution paths it is important to limit and prioritise the paths that will be processed.
in the second stage the alternative execution paths collected during the initial stage are explored in a search for a correct i.e.
not ending in a failure program execution.
the exploration process is iterative and consists of three steps.
first a path to be explored is selected path selection .
.
second a candidate document that follows the selected path is generated candidate creation .
.
1conceptually the technique can run directly on binaries but in our prototype we require source code.
an example of a system implementing similar techniques for binaries is presented in .table time needed to get the first recovery candidate when the whole document is symbolic whole and when only the potentially corrupt bytes are symbolic partial .
benchmark whole partial pr timeout 3600s .1s pine timeout 3600s .9s dwarfdump timeout 3600s .8s readelf .8s 1s third the candidate document is loaded in the original application in order to check whether it loads correctly candidate validation .
.
the candidates that are successfully verified in the third step are added to the pool of recovery candidates.
.
identifying potentially corrupt bytes in theory one could simply treat the whole broken document as symbolic and explore all feasible alternative paths.
however this could lead d ocovery to generate a large number of complex constraints and to explore many alternative paths.
docovery aims to reduce the amount of symbolic data by treating as symbolic only those bytes which when appropriately changed are likely to result in a recovery candidate.
in order to identify such potentially corrupt bytes d ocovery uses a form of dynamic taint tracking a technique which tracks the flow of information from a set of sources in our case the bytes in the document to a set of sinks in our case the computation where the failure manifests itself.
the tracking is performed by associating a unique token taint with each byte in the input file and then propagating these taints whenever an instruction is executed e.g.
the instruction x file file would propagate the taints of file and file to variable x. the final result of this analysis is a set of document bytes whose values are likely to be involved in the failure.
our analysis is both unsound i.e.
it may miss relevant bytes and imprecise i.e.
it may include irrelevant ones .
in general we have opted for higher precision that is we tried to minimise the number of irrelevant bytes included this limits the search space for recovery candidates because fewer bytes are made symbolic and thus fewer bytes can be changed to create recovery candidates.
in our evaluation we have never encountered a situation in which too few bytes were selected to allow recovery but if this happens it is possible to revert to a conservative taint analysis or simply include all the bytes in the document.
the design decisions that we have taken for our taint tracking implementation are as follows all motivated by the goal of minimising the number of bytes selected by the analysis byte level precision .
our taint tracking mechanism operates at the level of individual bytes i.e.
for every tracked byte we store the information about a set of document bytes that might have influenced the value of that byte.
565figure illustration of alternative execution paths.
the rightmost path is the one followed by the broken document and c1 c2andc3are the constraints collected on this path.
paths p1 p2 andp3are alternative execution paths p2andp3are feasible while p1is infeasible.
no control flow dependencies .
we only track data flow dependencies i.e.
we do not propagate taints that are associated with control flow branch points.
no address tainting.
when an address pointer is computed based on the data in the input document we do not propagate taints from the address to the target of a read write memory operation.
to illustrate the computational overhead associated with treating the entire document as symbolic we performed experiments using the smallest file in each of our benchmarks which are described in .
we measured the time taken to generate the first recovery candidate when the whole document is treated as symbolic and when only the bytes identified by our taint tracking analysis are treated as symbolic.
in both cases we set a timeout of one hour and collected at most alternative paths in the manner described in .
and .
.
table shows that taint analysis significantly reduces the time needed to generate the first recovery candidate.
in particular when treating the whole document as symbolic in pr pine anddwarfdump docovery cannot generate any recovery candidates within one hour whereas when treating only the identified bytes as symbolic d ocovery generates a recovery candidate in .1s .9s and .8s respectively.
.
lazily collecting alternative paths after the potentially corrupt bytes are identified the next step involves treating these bytes as symbolic and running the application on the broken document in order to collect alternative execution paths i.e.
those paths where execution could diverge from the one followed by the broken document if the identified bytes were changed.
the process is illustrated graphically in figure .
while each branch point involving symbolic data could potentially provide an alternative path not all such paths are feasible.
for example if the program first follows a branch on which x 10and afterwards encounters the branch point ifx then the alternative path x is not feasible at this point as xis already constrained to be and thus .
determining whether an alternative path is feasible requires a call to the constraint solver which is expensive.
with the large number of branch points encountered on real execution paths this would likely exhaust the entire time budget at this stage.
as a result our approach is to collect all paths diverging from the path followed figure overview of the recovery candidate creation process.
by the broken document both feasible and infeasible and lazily verify their feasibility only when they are selected for execution.
the drawback of this lazy approach is that we potentially need to store a large number of paths and memory consumption can become an issue.
in order to tackle this problem our strategy is to store only the last nalternative paths for further processing.
the intuition behind this strategy is that alternative paths which are closer to the fault are more likely to result in candidate documents that are similar to the original document because their pcs will share a larger prefix with the pc of the original execution which is already satisfied by the bytes in the original document .
to illustrate the need for lazily collecting paths let s consider two experiments on the readelf debug information display utility one of the benchmarks we explore in .
for the first experiment we selected a small 54kb file which we marked as symbolic in its entirety.
when the constraint solver was used to check alternative path feasibility at branch points the first candidate was generated after 267s compared with only .8s when lazy path collection was used.
to show the need to limit the number of collected paths we ran another experiment this time using a much larger .5mb file in which we only marked as symbolic the bytes identified by taint tracking and collected paths in a lazy manner.
when only the last alternative paths were collected the time needed to create the first recovery candidate was approximately 46s and the memory consumption was below 172mb.
however when all the alternative paths were collected it took approximately 242s to create the first candidate and the memory consumption reached .3gb.
this large memory footprint is due to the large number of alternative paths stored which was over in this case.
.
path selection our approach to path selection is to choose paths in decreasing depth we start from the deepest path and after that one is explored we take the next deepest and so on until all ncollected alternative paths are exhausted.
in the example from figure path p3will be selected first then path p2.
similar to the argument presented in .
the rationale behind this path selection strategy is to first choose paths that are closer to the bug i.e.
those which share a longer prefix of the pc with the original execution path.
intuitively since the bytes in the original document already satisfy the conditions on the shared prefix this strategy should reduce the number of bytes that need to be changed to make execution follow the alternative path.
pathp3shares constraints c1andc2with the original execution path whereas path p2has only constraint c1in common.
.
creating recovery candidates an essential part of d ocovery is the creation of recovery candidates that take the program along an alternative execution path and retain a high level of similarity to the broken document.
a diagram 566presenting d ocovery s candidate creation process is depicted in figure .
there are several inputs to the process the broken document.
all the bytes that do not need to be changed in the candidate file in order to follow the alternative path are left unchanged.
the set of potentially corrupt bytes as identified by the taint tracking algorithm presented in .
.
these are the bytes that are made symbolic in order to explore alternative paths.
the path condition pc associated with a given alternative path which is fed to a constraint solver to select new values for the symbolic bytes that make execution follow the alternative path.
the output of the recovery candidate creation process is a recovery candidate document that obeys the constraints in the pc of the alternative path and is similar to the broken document.
another way to see this is that we want to make the modified document satisfy the pc of the alternative path while changing as few bytes as possible.
there are two possible strategies that can be employed to create recovery candidates a satisfying assignment based approach which is simple but also potentially changes more bytes than necessary and a precise algorithm which changes fewer bytes but may require more calls to the constraint solver that can be expensive.
below we present each of these strategies.
.
.
satisfying assignment based approach as mentioned in a satisfying assignment is a sample set of values that satisfy a logical formula.
in the context of document recovery it is a sample set of input bytes which take the program along the execution path characterised by the given pc.
the simplest approach for creating document recovery candidates is to take the pc for the given alternative path and ask the constraint solver for a satisfying assignment to all the potentially corrupt bytes.
then the values returned by the solver need to be assigned to the corresponding bytes in the file while the rest of the bytes remain unchanged.
the problem with this simple approach is that we do not have any influence on the values returned by the solver.
the values in the satisfying assignment will obey the pc but they will not necessarily be similar to the original file.
however this issue is alleviated by our strategy of modifying only the bytes identified by taint tracking.
in our experiments the largest number of potentially corrupt bytes identified in the taint tracking phase was .
even if all these bytes are changed in the satisfying assignment returned by the constraint solver they represent only a small fraction of the total file size.
.
.
precise algorithm for more precision when creating recovery candidates we also used an algorithm that tries to minimise the number of changed bytes.
the algorithm already implemented by klee and zesti in another context iterates over each potentially corrupt byte biin the original file and asks the solver whether its value viobeys the new pc.
if that is the case the constraint bi viis added to the pc.
this process can be expensive if many bytes are identified as potentially corrupt because it involves one solver query for each such byte.
on the other hand if only a relatively small number of bytes are identified as potentially corrupt as is the case in our experiments the overhead of the algorithm is acceptable.
.
.
constraint independence optimisation to optimise the candidate creation algorithm we remark that after selecting one of the alternative execution paths it is only the last constraint that makes some of the byte values invalid i.e.
not feasible on the alternative execution path because the prefixes of the pcs for the original and alternative paths are the same for the two executions.
one optimisation that we use in both candidate creation algorithms is to eliminate all the potentially corrupt bytes that do not interact with the last constraint either directly or indirectly.
in other words we compute the transitive closure of the constraints dependent on the last constraint any bytes not involved in the transitive closure can be left unchanged in the file.
for example in our toy program in figure when we follow the alternative path with pc x y the transitive closure consists solely of the last constraint.
since the first byte in the file stored in variable x is not involved we know we can leave its value unchanged.
note that this optimisation makes use of the constraint independence pass introduced in .
.
candidate validation after a recovery candidate is created d ocovery still needs to check that execution does not hit another bug.
d ocovery guarantees that the candidate follows a different path but does not guarantee that it is correct.
we re execute the program natively as soon as the candidate document becomes available and check that it behaves correctly.
as discussed in the overview we currently only check that the program does not crash does not terminate with a non zero return code and does not output an error message.
to account for bugs that cause the program to hang we set a timeout.
if the timeout is exceeded we discard the candidate document.
.
analysing candidate similarity the output of d ocovery is a set of recovery candidates that do not crash the program and retain a high level of similarity to the broken document.
in order to quantify the similarity of the candidates to the broken document we employ a metric based on the levenshtein distance.
thelevenshtein distance also known as edit distance is a byte level similarity metric.
the two documents are treated as a sequence of bytes and their edit distance is the minimum number of byte insertions deletions or substitutions required to change one document into the other.
as discussed in the overview the degree of similarity between two documents ultimately depends on the specific file type but in the absence of a high level document specification the edit distance can be a useful proxy for many document types.
users are presented with all the recovery candidates generated within a certain amount of time ordered by their edit distance to the broken document.
it is up to them to choose the most appropriate candidate.
.
implementation we implemented our prototype system on top of the klee symbolic execution engine using the concolic execution functionality from zesti .
each analysed program first needs to be compiled into llvm bitcode the representation on which klee operates.
we currently use a modified version of the wholeprogram llvm2script for the compilation process.
llvm was configured to emit bitcode in which switch statements are represented as ifstatements.
567table benchmarks used to evaluate d ocovery .
application document typedocument size app specific kilobytes pr plain te xt .4k 1080k chars .
pine mbo x mailbox e mails dwarfdump ex ecutabletest ln tac dwarfdump dwarfdump22 readelf object filestrstrnocase.o 1615print types.o naming.o print abbre vs.o dwconf.o print frames.o since klee keeps track of all memory objects in the program it can detect memory problems at the point where they occur.
the ability to detect errors early makes it possible to avoid running the program after the error occurs and simplifies the recovery process.
docovery collects alternative execution paths in a fifo queue of a configurable size.
as the execution proceeds paths associated with more shallow branch points are removed from the queue and deeper paths are inserted.
.
ev aluation our experimental evaluation aims to provide information regarding the scalability of the technique the types of programs and documents to which it is applicable and its limitations.
we used an hp compaq elite cmt machine with an intel core i7 cpu at .4ghz 16gb of ram and a seagate st500dm002 1bd14 sata hdd running ubuntu .
lts.
.
benchmarks we evaluate our prototype system on four c applications processing various types of documents see table .
we have manually injected faults into documents to trigger previously reported bugs in each of the applications.
so the bugs are real but the broken documents are synthetic.
for each application we tested documents of various sizes.
below we provide a brief overview of each application and the corresponding bugs triggered by broken documents.
pr a paginating tool.
this tool is part of the gnu coreutils application suite available in virtually all linux distributions.
it has executable lines of code eloc and is linked against the libcoreutils library which has about eloc.3prtakes as an input a text file and paginates it according to a user specification.
to create broken documents we have used a buffer overflow bug inprreported in present in coreutils .
.
we trigger the error by inserting a buggy sequence of one hundred backspace characters followed by a tabulation.
the command line that we used ispr e300 file.txt.
we have tested recovery on synthesised text documents of sizes varying from .4kb to .1mb.
the documents were a sequence of single paragraphs of lorem ipsum4text characters wide.
we injected the buggy sequence at the end of each file.
pine an e mail client.
pine is a text mode e mail client which is orders of magnitude bigger than pr at around eloc we statically linked pine with the ncurses .
library which has about eloc.
we used a known bug5that causes older versions of pine to crash while displaying a message index containing 3all measurements are done using the cloc tool.
specially crafted e mail we used version .
in our experiments.
the bug is triggered by a from field in which the sender e mail address has multiple escaped double quote characters placed before the character.
the exact address that we used is ... host.fubar with the highlighted part repeated times.
this bug may prevent users from accessing their mailbox despite the fact that all the other e mail messages are valid.
in order to successfully recover pine mailboxes with our prototype we instrumented the program to indicate when it is done loading a mailbox.
annotating the program to detect that the document was loaded correctly would be needed in other similar interactive applications.
we used the following command line arguments to jump directly to a message index displaying the problematic e mail and used a custom .pinerc configuration file to set additional options pine i p .
.pinerc n the numbers correspond to the corrupt e mail number .
recovery was tested with mailboxes of sizes varying from to e mails.
we injected the buggy message at the end of each mailbox.
the injected message did not have a subject and a body.
the size of the corresponding mailbox file ranged from 13kb to .3mb.
dwarfdump a debug information display tool.
this utility is used to read and display debugging information stored in dwarf format.
in contrast to the first two applications dwarfdump operates on binary files.
we used version ofdwarfdump6and version .
.
of its libelf7dependency.
dwarfdump has about eloc and the linked libraries libdwarf andlibelf have about and eloc respectively.
the bug that we used for our benchmark was reported in and is triggered by setting a specific byte in the file to zero which causes a division by zero error during a sanity check done by the program.
we used two different command lines dwarfdump file anddwarfdump r file.
we tested recovery with five executable files listed in table with sizes ranging from approximately 60kb to 1mb in which we injected the fault described before.
readelf an elf file display tool.
the purpose of the readelf utility is to dump information about object files in the elf format.
we used readelf from binutils revision 6e09faca whose size is eloc the size of the linked libiberty library is eloc.
the bug that we used for our tests was reported in .
it is triggered when the user wants to print out the contents of the .debug ranges section of the buggy file and the relocation offset of the section is negative.
this results in a buffer overflow and a program crash.
we used the command line readelf wr file.
we have prepared six object files extracted from the dwarfdump2 utility of the libdwarf code package the same revision used for thedwarfdump tests with sizes ranging from approximately 54kb to .5mb.
we injected faults into these files by setting the relocation offset to 0xfd ff ff ff ff ff ff ff.
.
taint tracking results table summarises the results of running the taint tracking algorithm described in .
.
the third column shows the total number of potentially corrupt bytes identified by the algorithm while the last column lists those bytes using angle brackets to denote a comment.
pr.the taint tracking algorithm selected a single byte namely the tabulation character hexadecimal value 0x09 from the buggy sequence.
6git git.code.sf.net p libdwarf code 568table taint tracking results.
application documentnumber of potentiallypotentially corrupt bytescorrupt bytes pr .4k 1080k chars ...0x08 repeated times 0x09eof pine e mails ... host.fubar dwarfdumptest ln tac dwarfdump dwarfdump22 gcc ubuntu linaro .
.
1ubuntu5 .
.
bytes .. hex 0x00 readelfstrstrnocase.o 160x007b38 ...0x00d3b8 fdffffffffffffff print types.o 0x00c178 ...0x0180b8 fdffffffffffffff naming.o 0x0159c0 ...0x02ec90 fdffffffffffffff print abbre vs.o 0x02af58 c03e000000000000...0x068848 fdffffffffffffff dwconf.o 0x052278 f0b3000000000000...0x0c2230 fdffffffffffffff print frames.o 0x099bc8 a006010000000000...0x16e758 fdffffffffffffff pine.
in total characters were selected namely the first escaped quotation marks in the buggy e mail address the address is characters long .
dwarfdump.
two consecutive bytes were identified the first of which was the byte that we previously corrupted.
for readability purposes we provide in table an ascii representation of the relevant file fragment although the files are binary.
readelf.
the taint tracking algorithm selected two ranges of eight bytes each with the second range containing the buggy value.
in table we show the bytes using the format offset potentially corrupt bytes in hexadecimal.
it is interesting to note that for all benchmarks the same number of bytes were selected by the taint tracking algorithm regardless of the file size e.g.
forpine it selected bytes regardless of the mailbox size which varied from to e mails.
.
performance we have performed scalability tests for our benchmarks checking the recovery time for files of various sizes starting with a small document and roughly doubling its size in each experiment.
in this section we first comment on the results obtained using the simple satisfying assignment based candidate creation algorithm the performance of the precise algorithm is evaluated separately at the end of the section.
docovery was configured to collect up to alternative paths and to use a 5s timeout for the candidate validation step.
the timeout value was established by measuring the time needed to load the documents being considered which took typically less than .5s.
figures present the results.
the first bar of each measurement provides the interpretation time as a reference point.
it corresponds to running the application under d ocovery without marking the file as symbolic until the bug is hit.
this value represents the time spent by the symbolic execution engine interpreting llvm bitcode.
the second and the third bars represent taint tracking and alternative paths collection times.
the fourth bar is the cumulative value of bars two and three and represents the time needed to prepare for recovery.
the fifth bar represents the time needed to generate the first recovery candidate measured from the beginning of the recovery process.
finally the last bar shows the total recovery time i.e.
the time needed to exhaust all collected alternative paths.
overall the results show that the time spent on taint tracking and alternative paths collection dominates the time needed to create the first recovery candidate.
once these stages are completed generating recovery candidates is relatively quick.
pr.the performance results for prare presented in figure .
the time taken to generate the first recovery candidate was only a few minutes for document sizes of up to 271kb and more than half an hour for the largest document tested.
the time needed to create the .
.
time size interpreta3on a corrupt bytes iden3fica3on b alterna3ve paths collec3on a b first candidate all candidates figure prperformance measurements.
first recovery candidate once the alternative paths were collected was less than one second for all document sizes.
pine.
the results for pine are presented in figure .
as for pr the time taken to generate the first recovery candidate varied from only a few minutes to around half an hour.
the time needed to create a first suitable recovery candidate once all the necessary tainting and alternative paths information was collected was between .2s and .3s.
the higher than expected total recovery time for the e mail mailbox was the result of encountering multiple timeouts during candidate validation.
dwarfdump.
fordwarfdump we have tested two scenarios the first one using the roption which makes the execution reach the bug faster and the second without this option.
the results are presented in figures and .
the length of the execution until the bug is reached can have a significant impact on performance the overall time taken to create the first recovery candidate was under 40s in the first scenario and between a few minutes to almost an hour in the second scenario.
in both cases d ocovery needed under one second to create the first recovery candidate once the alternative paths were collected.
readelf.
the results for readelf are presented in figure .
as with the other benchmarks once the alternative paths were collected the overall time needed to create the first candidate was small at under one second for all files.
memory consumption.
we measured memory consumption of d ocovery itself excluding the memory needed for the constraint solver and native execution.
the memory footprint of d ocovery was not a problem the maximum memory consumption among all benchmarks never exceeded 880mb.
time of e mails interpreta1on a corrupt bytes iden1fica1on b alterna1ve paths collec1on a b first candidate all candidates figure pine performance measurements.
time file size in kb interpreta2on a corrupt bytes iden2fica2on b alterna2ve paths collec2on a b first candidate all candidates figure dwarfdump performance measurements with r .
performance of the precise candidate creation algorithm.
running times for the simple and the precise candidate creation algorithms were similar usually the difference between the two algorithms was below to generate the first recovery candidate and it took up to approximately longer for the precise algorithm in the worst case.
similar differences hold for the time to generate all recovery candidates.
many times the precise algorithm performed better than the simple one.
the performance differences are small because taint tracking identifies only a small number of bytes and thus the precise algorithm only needs to perform a limited number of additional calls to the constraint solver.
for dwarfdump the number of calls for the precise algorithm was the same as for the simple one.
.
recovery quality in this section we evaluate the quality of the created recovery candidates based on the edit distance to the original document and our manual analysis of the recovered documents.
tables and present the number of candidates created per document for each benchmark as well as the minimum and the maximum edit distance between a candidate and the broken document across all document sizes.
remember that by design d ocovery only changes the bytes identified as potentially corrupt in the taint tracking step so the maximum number is at most equal to the number of potentially corrupt bytes.
pr.docovery created three recovery candidates for each of the input documents regardless of the document size and the candidate creation algorithm used.
all candidates differed in one byte from the time file size in kb interpreta1on a corrupt bytes iden1fica1on b alterna1ve paths collec1on a b first candidate all candidates figure dwarfdump performance measurements without r .
time file size in kb interpreta2on a corrupt bytes iden2fica2on b alterna2ve paths collec2on a b first candidate all candidates figure readelf performance measurements.
broken file namely the last byte of the buggy sequence.
table shows some examples of changes to the last byte s value.
we performed a manual examination of the results by running pr natively on the created files.
all of the candidate documents seem to be printed out correctly including some text that we later added after the changed byte.
we did not notice any significant difference between the candidates.
the return code of the application was always .
interestingly the null character 0x00 present in the first candidate does not cause truncation of the output.
pine.
when the simple candidate creation algorithm was used docovery created between and candidates which had one or more of the escaped quotation marks changed.
some examples of recovery candidates are shown in table .
when multiple characters were changed the value for the first one was either the control character 0x0e candidate a or backslash candidate b and the rest of the new values were control characters 0x0e the edit distance for these candidates ranged between and .
when only one character was changed the new value was the null character 0x00 candidate c .
when the precise candidate creation algorithm was used candidates were created for each of the tested mailboxes.
all the candidates differed in one byte from the original file which was always changed to the null character in the recovered file candidate c .
the candidates created by using the precise algorithm were the 8note that in general the exact values chosen for the modified bytes can differ across runs due to non determinism in the constraint solver and klee.
570table simple algorithm an overview of recovery results.
benchmarkcandidates edit distance per document min max pr pine dwarfdump readelf table precise algorithm an overview of recovery results.
benchmarkcandidates edit distance per document min max pr pine dwarfdump readelf same as the candidates with edit distance created by the simple algorithm.
we verified all candidates manually by running pine natively with each of them and opening the recovered message after the mailbox was loaded this discarded between and candidates for each mailbox size which caused a crash when the message was opened.
the recovery candidates for pine illustrate the fact that the edit distance is not always the best similarity metric.
the mailboxes with edit distance were the ones which had one of the address bytes changed to a null character.
as a result the address in the buggy e mail was truncated in such candidates.
for candidates with a larger edit distance the new values were such that it was still possible to read the domain of the e mail address.
dwarfdump.
for each of the files two recovery candidates were created both differing in one byte from the original file.
the same files were produced by using the simple and the precise candidate creation algorithms.
table shows the byte changes performed.
we verified the files by running dwarfdump natively on them.
for all file sizes the execution of the first candidate finished with a zero return code producing the debug information dump while the execution of the second candidate ended with an error return code .
we also compared the output of the program for the first candidate with that produced with the initial valid file and there were differences.
finally since the recovered files are executables we manually confirmed that all the created candidates can still be executed by running them and checking their basic functionality.
readelf.
three candidates were created for strstrnocase.o and a single candidate for the other files.
table presents the bytes changed for the strstrnocase.o file using the simple candidates a b c with edit distances and and the precise candidate creation algorithms candidates d e f with edit distances .
we checked the created files by running readelf natively on them.
the application return code was always .
we compared the program output with the output produced when running the corresponding correct files.
all the candidates except for candidates b c e and f of strstrnocase.o produced a warning and had the beginning offset in the first printed row different than their corresponding correct counterparts.
the amount of output printed out was otherwise the same as for the correct files.
candidates b c e and f for strstrnocase.o produced almost no output for candidate btable examples of recovery candidates for pr.
document buggy sequence hex original ... candidate a ... candidate b ...0c candidate c ...0a table examples of recovery candidates for pine.
document buggy sequence original ... 30x host... candidate a... 11x 0x0e 15x 5x host... candidate b... 0x0e 23x 5x host... candidate c... 24x 0x00 5x host... and identical candidate e an error was printed and for the candidate c and identical candidate f the application did not find debug data in the object file.
since in readelf the tested documents are object files we tried to link each of the created candidates against the original program dwarfdump2 .
none of the files could be linked successfully.
this illustrates the fact that d ocovery is application specific that is in this case the recovery is done with respect to readelf and not with respect to the linking and executing the code in the object files.
forprandpine we injected the buggy sequences at the end of the file because we expect the two applications to process their input files sequentially.
we also verified how d ocovery performs for these applications when the buggy bytes are injected before the end of the file.
for pr we used the 541kb file and injected the sequence after approximately 269kb.
for pine we used the mailbox with emails and injected the buggy message after the 80th email.
the taint tracking algorithm selected the same number of bytes as in the previous experiments.
for pr the same number of recovery candidates were produced while for pine docovery produced candidates out of which two were rejected because they caused pine to crash.
the running times were about higher for pr and about higher for pine compared to the corresponding documents of about half the size 271kb and e mails respectively that had the fault injected in the end.
.
limitations in we demonstrated that our prototype can recover documents for medium sized applications.
we illustrated recovery on unstructured text data pr semi structured mbox file format pine and binary files dwarfdump readelf .
to show an example in which our approach does not work let us consider a bug present in version .
of the mupdf pdf processing library.9the bug is triggered in the parser s own recovery procedure when it tries to discover the locations of object definitions in the file.
this procedure starts after encountering an invalid offset to a cross reference table that normally stores offsets of objects.
if one of the object numbers is corrupted it can overflow to a negative value and pass a sanity check inside the parser which subsequently leads to a buffer overflow and a crash of the program.
we tried to recover from this bug but we were unable to create a recovery candidate with the presented approach.
the challenge associated with this corrupted pdf file and more generally with complex structured file formats are dependencies between bytes in the document.
in our example even if we are 571table examples of recovery candidates for dwarfdump.
document buggy sequence hex original ... ... candidate a ... ... candidate b ... ... table examples of recovery candidates for readelf strstrnocase.o file simple a c and precise d f candidate creation algorithm.
document buggy sequence hex original ... ...fd ff ff ff ff ff ff ff... candidate a... ... f001000000000080... candidate b...feffffffffffffff...fd ff ff ff ff ff ff ff... candidate c... ...fd ff ff ff ff ff ff ff... candidate d... ...fd ff ff ff ff ff ff ... candidate e...feffffffffffffff...fd ff ff ff ff ff ff ff... candidate f... ...fd ff ff ff ff ff ff ff... able to identify the incorrect object number as the source of the problem how to choose a new object number on the alternative path remains an open question.
in order to successfully recover this pdf document in the mentioned scenario we would need to change the corrupt bytes to a valid object number because this object can be referenced by other objects by its number.
by changing the number to an arbitrary value we may violate these inter object dependencies and thus create an invalid document.
at the place where the program crashes we do not have enough constraints to choose the right value.
one possible solution would be to implement an iterative strategy in which we first recover from the original bug and if the recovered document hits another bug we use that document as the new input to the recovery process.
furthermore our current approach and prototype have the following limitations our approach cannot add or remove bytes during recovery.
it is limited to mutating existing bytes.
this does not mean it is limited to recovering from corruptions that mutate bytes.
it can sometimes recover documents that are corrupted by adding bytes as shown in our prandpine benchmarks.
our approach cannot yet recover documents with several independent mutations.
we only handle bugs that result in generic errors such as program crashes buffer overflows and error return values.
the current prototype does not support file modifications during loading.
the current prototype requires c source code although our approach could in principle work directly on binaries.
.
related work the work most closely related to d ocovery is that of rinard et al.
who introduced the idea of input rectification.
at a high level their soap system starts with a learning stage in which the system learns a set of constraints characterising typical inputs for example it may learn that the height of an image is typically less than a certain value.
then in the rectification stage inputs not obeying the constraints are modified to do so e.g.
the image might be truncated to have its height within the limit inferred during the learning phase.
soap was successfully used to rectifyvarious media files like wa v or jpeg files that were crashing corresponding applications.
compared to d ocovery soap scales to much larger applications and document sizes because enforcing those constraints is much cheaper than using symbolic execution to explore alternative paths.
soap also does not require direct access to the application code or binary.
the key differences are that soap requires a specification of the input format and a training set to learn acceptable values for input fields.
d ocovery does not require a training set or any knowledge about the input format.
demsky and rinard propose an approach for repairing data structures starting from a manually written formal specification.
this approach has been successful in recovering errors e.g.
in a corrupted ext2file system and in a microsoft word file but the main downside is that it requires developers to write a formal specification for their data structures.
similarly to d ocovery prior research on vulnerability signature generation such as vigilante bouncer and shieldgen proposes to monitor the execution of malicious inputs to infer bytelevel constraints that characterise such inputs.
however unlike docovery the goal is not to modify but to discard such inputs.
an alternative approach to changing the document in order to avoid an error is to change the code of the application itself.
automatic generation of code patches can be accomplished using symbolic execution invariant enforcement genetic programming and various heuristics .
taint analysis and symbolic execution are well known program analysis techniques that have been used in a variety of contexts such as testing debugging and attack generation just to name a few.
d ocovery uses and adapts these techniques in the context of document recovery.
.
conclusion one of the most visible and frustrating defects of consumer software are application crashes and errors experienced while trying to load a document.
in this paper we presented d ocovery a novel document recovery technique based on symbolic execution.
unlike prior approaches d ocovery does not require any knowledge about the document format which makes it applicable to a wide range of applications.
the key idea behind d ocovery is to start from the code path executed by the broken document explore alternative paths that avoid the error and finally make small changes to the document in order to force the application to follow one of these alternative paths.
we applied d ocovery to popular applications such as the email client pine the pagination tool prand the binary file utilities dwarfdump andreadelf for which it has managed to recover documents of various sizes typically within minutes.
while these preliminary results are encouraging we still need to overcome several important challenges in order to make d ocovery practical dealing with complex structured formats supporting the addition and removal of bytes and scaling to larger documents.
.