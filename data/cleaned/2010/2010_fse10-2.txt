memory indexing canonicalizing addresses across executions william n. sumner xiangyu zhang department of computer science purdue university wsumner xyzhang cs.purdue.edu abstract fine grained program execution comparison examines different executions generated by different program versions different inputs or by perturbations.
it has a wide range of applications in debugging regression testing program com prehension and security.
meaningful comparison demands that executions are aligned before they are compared otherwise the resulting differences do not reflect semantic differences.
prior work has focused on aligning executions along the control flow dimension.
in this paper we observe that thememorydimensionisalsocriticalandproposeanovelso lution to align memory locations across different executions.
we introduce a canonical representation for memory locations and pointer values called memory indexing.
alignedmemory locations across runs share the same index.
we formally define the semantics of memory indexing and present a cost effective design.
we also show that memory indexing overcomes an important challenge in automated debuggingby enabling robust state replacement across runs.
categories and subject descriptors d. .
testing and debugging general terms algorithms measurement reliability .
introduction comparing executions is a fundamental challenge in dynamic program analysis with a wide range of applications.
for instance comparing executions of two versions of a program with the same input can be used to isolate regression faults and analyze the impact of code changes .
it helps identify implementation differences between the twoversions which can be exploited by attackers.
comparing program state at different points within executions can also be used to normalize and cluster execution traces simplify ing analyses that use those traces as input .
execution permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies arenot made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.fse november santa fe new mexico usa.
copyright acm ... .
.comparison also provides unique advantages in program deobfuscation anddebuggingcompileroptimizations where aggressivetransformationsmakestaticcomparisonlesseffective.
two executions from the same concurrent program canbe generated with schedule perturbations to confirm harmful data races and real deadlocks .
in computing cause transitions for failures a failing execution and a passing execution are compared to isolate instructions orprogram states relevant to the failure.
recently a technique called structural indexing was proposed to align the dynamic control flow of executions atthe granularity of instruction execution such that comparison can be carried out at aligned places.
this substantially improves accuracy in race detection deadlock detection and computing cause transitions for failures .
however structural indexing only solves one dimension of the problem control flow.
the other unsolved dimension orthogonal to control flow is memory.
in the presence of program differences input differences or non determinism corresponding heap memory regions are allocated in differ ent places across runs.
therefore although executions are aligned along control flow paths if memory regions are not properly aligned comparing values is hardly meaningful.
existing techniques rely on sub optimal solutions such as identifying memory using symbolic names.
in particular to compare memory states of two executions reference graphs are first constructedin which globaland lo cal variables are roots and memory regions especially heap regions are connected by reference edges.
roots align by their symbolic names other memory regions align by theirreference paths which consist of variable and field names.
we call it symbolic alignment .
however symbolic alignment is problematic in the presence of aliasing.
detailed discussion can be found in section .
in this paper we propose a technique called memory indexing mi .
the central idea is to canonicalize memory addresses such that each memory location is associated with acanonical value called its memory index .
memory locations across multiple executions align according to their indices.pointers are compared by comparing the indices of their val ues.
memoryindicesaremaintainedalonganexecutionsuch that they can be directly accessible or computable.
overall we make the following contributions.
we formally present the memory indexing problem.
we identify key properties of valid solutions.
we discuss two semantics for memory indexing.
thefirst one is an online semantics that computes indiceson the fly during execution and handles pointer arith217metic.
the second is a lazy semantics that computes indices on demand.
it has lower cost and is more suit able for languages without pointer arithmetic.
we introduce a practical design that uses a tree to allow multiple indices to share their common parts.optimizationsremoveredundanttreeconstructionand maintenance.
we illustratehowmemoryindexingfacilitatescomputing cause transitions for failures.
novel memory comparison and substitution primitives resolve limitations of existing solutions.
they allow robust mutation ofa passing run to a failing run by copying state across runs.
we evaluate the proposed mi scheme.
it causes a slow down and space overhead on average.
the results of two client studies show that mi is able to canonicalize address traces across runs and it scalescause transition computation to programs with com plex heap structures.
.
motiv ation execution comparison not only requires alignment on the control flow dimension but also on the memory dimension.
aligning and comparing memory snapshots across runs is thus a key challenge.
existing techniques do not provide satisfactory solutions to the following challenges.
figure pointer comparison.
linked lists represent the snapshots of different executions.
eachnode has two fields valandnext.
support for pointer comparison.
many applications require the ability to compare pointers across runs.
for example regression debugging and computing cause transitions rely on contrasting variable values in a passing run and a failing run to identify faulty values.
for pointer related failures it is critical to identify when a pointer contains a faulty value.
however due to semantic differencesor non determinism even pointers that point to the same heap data structure can have different values across runs and hence they are not directly comparable.
most existing techniques do not support pointer comparison.
instead non pointer field values such as p valandp next val in figure are compared following their symbolic referencepaths.
for the case in figure b such comparison yieldsthe right result.
that is only p valhas different values across executions.
whereas in case a the conclusionis thatp valandp next valhave different values implying the definitions to these fields are faulty in a debugging application which is not true.
a more appropriate conclusion is that onlythe pointer phas different values all other differences are manifestations of the pointer difference.figure destructive state mutation.
a snapshot in run one.
b in run two.
c mutating a to b .
destructive state mutation.
applications such as computing cause transitions compare memory snapshots from a passing run and a failing run.
a variable having dif ferent values in the two respective runs is called a difference .
in order to reason about the causal relevance of differences with the failure values of difference subsets are copied fromthe failing run to the passing run to see if the failure is eventually triggered in the mutated passing run.
however using symbolic alignment causes a destructive mutation problem in the presence of aliasing.
in particular a memory location may have multiple reference paths.
it may be classified as a difference when it is compared under one path but not along another path.
mutation along one path destroys the semantic constraints along other paths and often leads to undesirable effects.
consider the example in figure .
twosnapshots are shown in a and b with ppointing to different locations in each.
with symbolic alignment the root p is aligned first followed by nodes along paths from p.a sa result the first node in a is aligned with the second nodein b the second node in a with the third node in b and so on.
comparing the non pointer fields of the aligned nodes yields the following reference paths denoting differ ences p val p next valandp next next val.
they are fields in b having values different than those in a .
suppose we try to mutate a to b by copying val ues from b to a following the paths of differences.
the resulting state is shown in c .
observe that t s value is undesirably destroyed.
figure lost mutation.
lost mutation.
if multiple differences alias they may result inlost mutation when they are applied together.
specifically the differences applied earlier may be overwritten undesirably by differences applied later.
this may lead to in correct conclusions about the relevance of differences.
consider the example in figure .
assume the failure is that p val t val has the wrong value.
pointer tpoints to the wrong place and the node pointed to by phas the wrong value.
these together cause the failure.
symbolic alignment and non pointer value comparison identifies twodifferences denoted by their reference paths p valand t val.
however as pandtalias in a when the differences are applied to a in the order of pdifference first and then tdifference the rightmost leaf first has the value and then .
the mutated state does not lead to the expected failure.
hence we mistakenly conclude that the twodifferences are irrelevant to the failure.
.
problem statement and overview to overcome the aforementioned problems and provide robust support for memory comparison and mutation we propose a novel technique called memory indexing .
the basic challenge is to associate each memory location with a canonical value such that locations across runs are aligned by their canonical values pointers can be compared by their canonical values.
such values are also called memory indices because they essentially provide an indexing structure for memory.
figure overview of memory indexing.
the idea is illustrated by figure which revisits the example in figure .
focus on the parts inside the boxes for now.
each node is associated with a canonical value index circled at a corner.
nodes are aligned by their indices.
hence we can see the root nodes align as they have the same index .
the node with index on the right does not align with any node on the left.
besides its concrete value pointer palso has a canonical value in both runs.
pointer thas on the left but on the right.
with memory indexing the differences of the two states can be correctly identified pointerthas a different pointer value and the nodes pointed to byphave different field values.
when mutation occurs tis set to the location with index which is not present in the passing run and thus entails allocation.
p s field value is changed to .
such mutation properly induces the failure.
a valid memory indexing scheme should have the following property at any execution point each live memory location must have a unique index .
we call this the uniqueness property.
if this property is not satisfied multiple locations may share the same index or one location may have multiple indices which makes proper alignment across runs impossible.
symbolic alignment does not always satisfy thisproperty and is thus not a good indexing scheme.
a good indexing scheme should have the following additional feature locations across runs that semantically correspond to each other should share the same index .w ec a l lt h i s thealignment feature.
using the concrete address of a memory location is an indexing scheme satisfying the uniqueness property but it does not deliver good alignment.
figure graph matching may be undesirable.
inadequacy of graph matching.
finding the most appropriate memory alignment concerns program semanticsand is thus in general not a concretely knowable problem.
as pointed out in one possible approximate solution inthe general case is to represent the memory snapshots undercomparison as reference graphs and formulate the alignment problem as a graph matching problem the goal of which is to produce a match with the minimal number of graph dif ferences.
however this solution is too expensive np complexity to be practical .
more important we observe that it fails to deliver desirable alignment in many cases be cause it does not capture semantic differences.
consider the example in figure .
the failure in b occurs because the value field passed to the node constructor is incremented byone.
with a graph matching algorithm to minimize graph differences the second node in a aligns with the first node in b the third node in a aligns with the the second nodein b and so on.
as highlighted in the figure the graphdifferences namely the graph operations needed to mutate a to b are add b s tail to a add the edge to the added node remove the head in a .
however such differ ences imply that the shape of the linked list is faulty which is not true.
the most appropriate alignment matches the corresponding nodes in the lists resulting in four field valuedifferences that precisely reflect the semantic differences.
our indexing scheme.
we propose to use the execution point where the allocation of a memory region occurs as the index of the region.
we leverage the observation that semantic equivalence between executions often manifests itselfas control flow equivalence.
hence semantically equivalentmemory regions are often allocated at corresponding execu tion points.
figure presents an overview.
the two lines in the middle represent the control flows.
the memory indices are essentially canonical flow representations of the allocation points.
for instance the root nodes share the memory index indicating they are allocated at the same point .
in contrast index s presence in only the failing run means that the allocation does not occur in the passing run.
ourindexing scheme satisfies the uniqueness property and provides high quality alignment in practice.
.
semantics in this section we present two semantics for memory indexing.
the first is for low level languages such as c. it supports pointer arithmetic by updating indices on the fly.
this is called the online semantics .
the other semantics computes indices on demand and does not need interpretation of pointer arithmetic.
we call this the lazy semantics .
inour semantics each memory locationis canonicallyrepresented by a pair region offset withregionas the canonicalized representation of its containing allocated region and offsetas its offset inside the region.
the canonical representation of a region is generated when the region is allocatedand serves as a birthmark of the region during its lifetime.we provide a function mi that maps a concrete address to its index.
we also maintain a function pv that maps a pointer variable to the index of the value stored in the pointer.
in the lazy semantics pv p is lazily computed from mi p whereas in the online semantics pv p is updated on the fly through pointer manipulations on pointerp.h e n c e p v p may be different than mi p in the online semantics.
as we will later show separating pv from miallows us to precisely handle pointer arithmetic which isdesirable for certain applications.
219rule event instrumentation prog.
starts for each global variable g mi g nil global offset g enter proc.
x for each local variable lvofx mi lv cs local offset lv pc p malloc s for i to s mi p i i pv p mi p p v pv p mi v p q pv p pv q p q offset pv p pv q .first pv q .second offset figure online semantics.
a memory index mi a represents the memory index of an address a w h i c h is a pair comprising a region identifier and an offset.
cs represents the current call stack.
pcrepresents the program counter.
si represents the cur rent structural index.
pv p represents the memory index of the address value stored in p. .
online semantics in this subsection we discuss the online semantics.
indexing global memory.
we consider global memory locations as part of a global region .
hence the memory index of a global location is its offset in the global region rule5 of figure .
in our terminology gdenotes the concrete address of a variable g. if executions from different program versions are considered e.g.
in comparing regressing executions symbolic names of variables are used instead of theiroffsets.
it is easy to see the uniqueness property is satisfied.
indexing stack memory.
we consider stack memory to be allocated upon function entry.
the allocated region is the stack frame of the function.
hence we use a stack frame identifierandthe stackframeoffset of a locationto represent its index.
recursive calls allow multiple instances of the same function to exist in the call stack at an execution point so that we have to use the call path of a stack frame as its id.
such stack indices trivially satisfy the uniqueness propertyand providemeaningfulalignment.
this is presentedin rule 6offigure6.
someprogramsperformdynamicallocationon the stack which makes stack variables have varying offsets.we identify such variables through static analysis and useour own ids to replace the offsets.
.
.
indexing heap memory.
the essence of our technique is to create a birthmark of a memory location as its canonical representation.
the birthmarks of heap locations are more tricky.
using the program counter pc of the allocation point is not sufficient because multiple live heap regions may be allocated at the same pc.
thecallingcontextof theallocationpointis notsufficientei ther.
for example the code in figure.
a creates a linked list in the loop on lines and .
all allocations occur in the same context statement inside f .
adding an instance count does not help either because different executions may take different paths so that the same count does not imply correspondence.
inthispaper weutilize structural indexing .
background structural indexing is a technique that provides a canonical representation for execution points within control flow such that points across runs can be aligned.
readers familiar with structural indexing can skip to the ending 2symbol.
conceptually an execution is indexed by a tree representing its nesting execution structure.
a leaf node denotes anexecution point and internal nodes represent control structuressuchasbranches loopbodies andmethodinvocations.
consider the program in figure a .
the index trees for the two runs are shown in figure b and c .
the two control flow traces are displayed horizontally from left to right.
individual trace points are also the leaves.
considerthe b tree.
the root represents the whole execution which consists of six statement executions presented as the leaf children of the root namely statements and11.
since the forstatement has substructure an internal node t pronounced as the true branch of statement is created to represent the loop body.
the remainder is constructed similarly.
traces are aligned by the index trees in a top down fashion.
first the two roots align.
then the six leaf children align dictated by the aligned parents.
note that their alignment is independent of the branch outcomes of1a n d5.
internal nodes may or may not align depending on their labels i.e.
the branch outcomes .
if they do recursive alignment is performed.
the two trees in figure b and c align except for the subtree rooted at t. the structural index of an execution point is the path leading from the root to the leaf node representing the point .
for instance the index of the shaded in the left tree is .
deciding the presence of an execution point in other runs is equivalent to deciding the presence of its index in the corresponding trees.
an important property is that each dynamic point in an execution has a unique index .
rule event instrumentation invoke function fsi.push fc at call cite c exit proc.
f si.pop predicate ptakes si.push pb branch b statement s while sis the immediate postdominator of si.top si.pop si is the structural index represented as a stack.
figure semantics for structural indexing.
structural indices are computed using control dependence analysis .
figure defines the semantics of structural indexing.
each internal node in an index tree representsan execution region delimited by a function entry and its exit or by a branch point including loop predicates and its immediate postdominator.
regions are either disjoint ornesting analogous to function invocations.
hence a stack similar to a call stack named the structural indexing stack si is used to maintain the structural nesting relation.
an entry is pushed upon function entries rule or predicateexecutions rule .
the top entry is popped upon the exit of a function rule or when the immediate postdominator of the predicate on top is encountered rule .
the si stackalways contains the structural index of the current execution point.
more details and examples can be found in .
to index heap memory we use the structural index of the allocation point as the id of an allocated region to composethe memory index.
the uniqueness property of structural indexing ensures the uniqueness of heap indices.
the align ment feature of the memory indexing scheme also originates from the fact that structural indexing identifies equivalent allocation points across executions.
in particular heap in dices are set when a region is allocated rule in figure .
a heap index consists of the current si and the allocation sitepc.
besidessettingthememoryindices therulealsosets the canonical value of the pointer variable i.e.
pv p to the 220figure example for heap indexing.
the code constructs a linked list of three nodes with values of and .
initially the three pointers h p a n drall point to the head of list.
there is a regression bug at line in computing the predicate.
as a result the failing run takes the false branch making ppoint to its second node.
pointer pfurther advances to the third node at line .
in contrast the passing run takes the true branch eventually resulting in both pandrpointing to its third node.
the failure is observably wrong output.
the memory snapshots are before the failure at statement .
memory index of the head of the region.
such a canonical value will be used in pointer manipulation.
for example in the second iteration of the loop in figure after theallocation in statement pv h f t 1t .
memorylocationsacrossmultiplerunsarealignedbytheir indices.
by this criterion in figure the head of the list in b does not have alignment while the remaining three nodes align with the list in c .
a key feature of memory indexing is pointer value comparison across runs.
besides a concrete memory address a pointer variable is also associated with a canonical value.
canonical pointer values are updated on the fly in the onlinesemantics asspecifiedbyrules8 .
forbrevity weassume a simple syntax for pointer operations.
in particular if the address of a variable vis retrieved and assigned to a pointer the canonical value of the pointer is the memory index of v s address rule .
if a pointer variable is copied to another variable the canonical value gets copied too rule .
for pointer arithmetic expressions p q offset v a r i a b l e p s canonical value is computed by copying the region identifier ofqand adding rto the offset of q rule .
for brevity our semantics assumes type information has been processed so that offset variables are identified at the unit of bytes.
.
lazy semantics for high level languages in which pointer arithmetic is not permitted or when client applications do not require considering the effects of pointer arithmetic we can derive pv values on demand and hence allow a more efficient implementation.
the semantics is called the lazy semantics .t h e observation is that canonical values of pointers can be lazily inferred from their concrete values.
that is given a pointer p p v p mi p .
recall that in the online semantics pv is computed by interpreting pointer arithmetic rule and hence pv p is not necessarily equivalent to mi p .
rule event instrumentation pc p malloc s mi p query the index t a of heap address a while mi t nil t t return mi t .first a t figure lazy semantics.
the new rules are presented in figure .
on the fly computation is only needed upon heap allocation rule thecurrent si is assigned to the region base address but not to the other cells in the region.
when the mi value of aheap address is queried rule the algorithm scans back wards from the given address to find the first address with a non empty index.
the memory index of the given address consists of the region denoted by the non empty index andthe offset inside the region.
for large heap regions we set the mi for a number of addresses at set intervals besides the base address such that the linear scan can quickly encountera non empty mi.
no on the fly computation is needed for global and stack memory.
the mi values of global and stack addresses can similarly be computed on demand.
in languages with pointer arithmetic the lazy semantics does not instrument pointer operations or track the original regions of pointers.
the looser coupling with programsemantics may lead to undesirable imprecision in certain ap plications.
details of when this may occur can be found in our extended technical report .
.
design and optimizations rule event instrumentation pc p malloc s l new leaf pc p s tree insert si l mi p l free p tree remove mi p .first figure tree based indexing in lazy semantics.
thesemanticsintheprevioussectionareconceptual.
they model an index as a sequence of symbols the region and an integer the offset .
this is too expensive to operate with inpractice.
in our design we explicitly maintain an index tree for heap memory and represent a heap region as a reference to some leaf in the tree.
the full index of a heap location can be acquired by traversing bottom up from the leaf.
rules and show the tree based instrumentation for the lazy semantics.
upon heap allocation rule a leaf node rep resenting the allocation is created and inserted into the tree by calling tree insert .
the function first checks if the current si is part of the tree.
if not it adds the si to the tree before it inserts the leaf node.
at the end the instrumentation sets the mi of the region base address to the leaf.
upon deallocation rule treeremove is called with the leaf node corresponding to the to be freed region.
recursive tree 221elimination is performed meaning that removing a leaf node may lead to removing its ancestors if they have no children.shaded subtrees in figure are example heap index trees.
dotted edges link leaf nodes to memory regions.
we have the following optimizations to make our design practical.
opt removing redundant instrumentation.
we have two observations that help remove redundant instrumentation.
the first one is that we only need a partial structuraltreetoindexheapallocations.
hencewecan avoid instrumentation that maintains irrelevant structural indices.
if a function does not allocate heap memory it is not necessary to compute structural indices inside that functions.more formally a function or a predicate branch is relevant to heap allocation if and only if a heap allocation can directly or transitively occur in its body.
irrelevant functions and predicates are not instrumented with pushes and pops.
the second observation is that we do not need to instrument all relevant functions or predicate branches.
morespecifically given a relevant function other than main predicate n if all index paths from any of n sp a r e n t st oah e a p allocation inside n s body have to go through n w ed o n t need to instrument n.w ec a l l nadominant function predicate .
intuitively we don t need to instrument if we can infer the presence of non an index path given the allocation site and the parent node.
we have developed static analyses to identify relevant but not dominant functions and predicates.
they are analyses on call graphs and control dependence graphs.
details are elided.
note that such optimizations are not applicable to general structural indexingbecause they leverage heap allocation information.
opt handling loop nodes.
from the semantics loops require pushing multiple entries of the same predicate which can be optimized as follows.
as in we add a counter field to the stack entry then upon encountering a loop predicate it is first checked if the top entry is thesame predicate.
if so the counter is incremented instead of pushing.
when the current stack is materialized to the tree due to heap allocation a new node is inserted to thetree if there is not an existing node with the same counter value.
the optimization does not affect the uniqueness and alignment properties of indexing.
the space consumption is dominated by the tree whose size depends on its shape and the number of live heap regions.
a pessimistic bound is o maximum tree depth maximum live heap regions .
in theory the tree depth is unbounded because it is tied with loop counts and the depth ofrecursion.
in practice because we are only interested in the partial tree for allocations and we optimize loop predicatesusing counters tree depth is often well bounded such that the space overhead is feasible see section .
.
robust memory comparison and replacement cause transition computation produces a causal explanation for a software failure.
the technique takes two executions one failing and the other passing that closely resembles the failing.
the passing run can be generated by selecting an input similar to the failing input.
the overall ideais to compare memory snapshots of the two runs at selected executionpoints.
areferencegraph isconstructedtorepresent a snapshot.
memory comparison is reduced to graphcomparison driven by symbolic reference paths.
causalitytesting is conducted to isolate a minimal subset of graph differences relevant to the failure.
more specifically subsets ofgraph differences are enumerated through the delta debug ging algorithm.
a subset is considered relevant if replacing the program state specified by the subset in the passing runwith the corresponding values in the failing run produces thefailure.
the minimal subsets computed at the selected execution points are chained together to form an explanation.in the technique is improved by automatically aligningtwo execution traces using structural indexing before comparison.
however in both and memory comparison and replacement is driven by symbolic paths and hence hasthe issues mentioned in section .
consider the example in figure .
using symbolic alignment and comparing only non pointer values if only heapmemory is considered the set of differences failing pass ing p val r val r next val r next next val h next val h next next val .
none of the subsets including the set itself can inducethe same failure.
for instance applying subset p val r val does not work due to the lost mutation problem.
as a result the delta debugging algorithm terminateswithout finding the minimal failure inducing subset.
since aliasing is very common in general programs we need to perform robust memory comparison and replacement.
with mi we are able to develop two robust primitives comparison of memory snapshots mem comp and application of a memory difference diffapply i.e.
copy a value from one memory snapshot to the other across runs .
for the comparison primitive snapshots are first aligned bytheirindicesandthencomparisonisconductedonaligned locations.
memory locations with non pointer types arecompared by their concrete values.
locations with pointer types are compared by their canonical values.
differences are presented as a set of indices denoting that the corre sponding locations are different.
consider the two snapshots in figure .
global variables c h p a n drare aligned.
since cis of boolean type its values are compared and classified as differences.
in contrast canonical value comparison is conducted for pointer variables h pandr.
it is easy to see that they are different.
heap memory is compared by the index trees.
the regionpointedtoby hin b is identified as the only tree difference.
hence if we compute the difference set passing failing the result is nil offset c nil offset h nil offset p nil offset r f t .
the symbol in the last index signifies that the entire region is different.
it is much smaller than the symbolic results.
the second primitive is the application of a unit difference1represented as an index from which the corresponding concrete memory location in both snapshots can be identi fied.
the value is copied from the source snapshot to the target snapshot.
if the value is a pointer we cannot simply copy the concrete address.
instead we identify the properconcrete address in the target snapshot following the canon ical value of the pointer.
if the region is not present in the target snapshot it is first allocated.
function diff apply in algorithm describes how to apply a heap unit difference.
in the algorithm the source and target heaps are indexed by trees rooted at t primeandt respectively.
variable represents the unit difference.
lines 1a unitdifferenceis a differenceregardinga specificmemory location instead of a region.
222algorithm apply a heap difference.
description copy the value in location fromt primetot.
leaf node is of the type pc base size .
diffapply t t prime let be path offset let base prime be the leaf node in t primealongpath let base be the leaf node in talongpath a base offset a prime base prime offset if a prime t primeis not a pointer then a t a prime t prime else letpv a prime t primebe p prime f prime if tdoes not have path p prime then regioncopy t t prime p prime let b be the leaf node in tfollowing p prime a t b f prime description copy region path primeint primetot.
regioncopy t t prime path prime let base prime size prime be the leaf in t primealongpath prime r allocate size prime in the run denoted by t insertpath primetot set the leaf node following path primeintto r size prime for i tosize prime do diffapply t t prime path prime i and4 identifytheheapregiondenotedby int primeandt.i n lines and the concrete addresses are computed.
at line the algorithm tests if the computed address is a pointer the superscript specifies where the dereference occurs .
if not the algorithm copies the value line .
if so it testsif the region pointed to is present in t line .
if not it copies the region line .
finally at line the concrete address stored to the pointer is set to a location in the region int aligned with the source region in t prime .
function regioncopy copies a region denoted by the parameter path primefromt primetot.
it first locates the region in t prime line and allocates a region of the same size in t line .
the path primeis inserted to tand a leaf node is created to represent the allocated region lines .
this avoids allocating the same region again.
finally individual fields are copied from t primetotby calling diffapply lines .
note it may transitively copy more regions from t primetot.
applying stack and global differences is similarly defined.
example.
consider the example in figure .
assume we want to apply the differences of pandrto the passing run.
observe that ppoints to the third node in the failing run and pv p fail .
during the pdifference application following the path the concrete address of the fourth node in the passing run is identified and assigned top.
similarly after applying the rdifference rholds the concrete address of the second node in the passing run.
note that by applying these two differences the same failure can be produced.
applying other differences such as c a tt h i s point before statement has no impact on the failure.
the minimal failure inducing difference subset including p andris emitted as one cause transition.
the same memory comparison and difference minimization is further performed at aligned instructions and it stops at as no state difference is identified.
the chain ofcausetransitionsis chastheincorrectvaluefalseat5 thentable instrumentation and allocation.
program instmt.
instmt.
of alloc avg.
alloc tree func branch stat dyn.
size dep.
.gzip .
436k kb .vpr .
107k b .gcc .
.2m 5k b .mcf mb .crafty .
.
kb .parser .
.
mb .gap mb .vortex 258k b .bzip2 .
mb .twolf .
574k b pandrpoint to the wrong places at and and finally the failure.
these transitions compose a failure explanation.
next we define the composability property and show that it holds for the proposed primitive.
definition .a scheme for memory differencing and replacement is composable iff given a set of unit differences 1 2 ... n and the universal set uof all differences after applying the differences in fromt primetot t h e differences between t primeand the mutated tisu .
composability is very important for cause transition computation it ensures that the delta debugging algorithm isable to make progress because it mandates that the effectof applying a set of differences must subsume the effect ofapplying a subset of the differences .
if a replacement scheme is not composable applying the universal set of differences may even fail to convert ttot prime.
the symbolic path based scheme is not necessarily composable.
as shown in figure applying the two differences of p valand r valfrom the failing run to the passing run results in a state in which p valstill manifests itself as a difference.
property .the proposed mi based memory differencing and replacement primitive is composable.
the proof is elided due to space limits.
however from algorithm we observe that for non pointers the primitive faithfully copies values hence the property is trivially true.
for pointers the primitive either allocates a region when itis not present in the index tree or simply assigns the address if the region is present.
such behavior does not lead to additional differences that were not present in the original difference set or mask any other existing differences.
.
ev aluation the implementation consists of both semantics and two client studies.
it is based on the cil infrastructure and has lines ocaml lines c and lines python.
.
efficiency the first experiment focuses on cost.
the evaluation is on specint benchmarks.
we excluded .eon and .perlbmk because they were not compatible with cil.
all experiments were executed on an intel core .1ghz machine with gb ram and running ubuntu .
.
table shows the instrumentation needed and characteristics of allocations.
all executions are acquired on reference inputs.
the second column shows the number of instrumented functions after optimizations and their percentageover all functions.
the third column shows the same data 223for predicates.
the fourth column shows the numbers of static allocation sites and dynamic allocations.
the fifthcolumn shows the average size of each allocation.
the last column shows the maximum depth of the memory index tree.
we observe that some programs make a lot of allocations with various sizes gccandtwolf and some make very few but large allocations mcfandbzip2 .
they have different impacts on the performance.
programs parserand gap allocate a memory pool at the beginning and then rely on their own memory management systems.
our current systemdoesnottraceintomemorypoolmanagement.
weleave it for future work.
observe the maximum tree depth is nothigh with respect to the structural complexity of programs.
recall that we collapse consecutive instances of a loop predicate so the depths are largely decoupled from loop counts.
the overhead can be seen in figure in which fullrepresents implementation without removing redundant instrumentation partremoving redundant instrumentation flow t h eo n l i n es e m a n t i c s a n d lazythe lazy semantics.
the figure presents the performance overhead for a number ofcombinations.
in practice part lazy is desirable for most applications as illustrated by later client studies.
space represents the space overhead for part lazy .
all data is normalized against original runs without instrumentation.
we observe first that full lazy has substantially more runtime overhead than part lazy .part flow is slightly more expensive than part lazy due to instrumentation on pointeroperations.
theoverheadof part lazy islow .
next observe that in half the benchmarks there is little space overhead.
this is because the number of allocations and the tree depth are relatively low regarding the size of each allocation.
in contrast .twolf had the most overhead.
it performs a large number of very small allocations bytes on average so on average maintaining the index for each allocation is more costly .
nonetheless the average space overhead is without twolf .
the conclusion is that the cost of mi is feasible for many applications.
.
trace canonicalization trace canonicalization is the alignment of control flow and memory accesses across traces from two executions.
it plays a part in debugging and regression analyses among others.
with mi an important question can be answered given two address entries in two respective traces should they be considered differences?
note two accesses at the corresponding points in the two traces do not mean that they operate on the same data the accessed addresses being differentdoesnotmeantheydonotsemanticallycorrespond.
the study is on three common open source programs make gawk a n d dot.
we reported the number of address differences before and after mi canonicalization.
we turnedoff all memory layout randomization.
to avoid comparing trace entries that do not correspond we used structural indexing to establish which memory accesses occurred at thesame point in both traces and only compare those accesses.
the traces were generated from the programs provided test suites or in the case of dot the provided examples in the documentation.
each full trace was compared withtraces generated by a fixed percentage of the input i.e.
removing part of the inputs.
the results are shown in fig.
.
for each percentage of input similarity we present the per centage of matched stack and heap memory accesses before 2in our implementation we use bytes for each tree node.and after canonicalization.
for mi these are mi locals and mi allocs respectively while for the addresses without canonicalization they are addr allocs a n d addr locals .
we furthermore present the percentage of control flow correspondence control flow .
observe first that mi provides a substantially higher level of heap access correspondence more for makeand more for gawk and more for dot .
less benefit was observed in dotbecause dot s dynamic allocations are largely on fixed buffers that do not change according to inputs.
mi was able to find more corresponding addresses for local variables too.
observe that stack local allocation andvariable sized objects on the stack make it more difficult to find correspondences without mi e.g.
the makecase .
the control flow similarity increases as the input similarity increases.
note that the address correspondence without canonicalization stays roughly the same or even decreases as the control flow similarity increases like in dot .
the decrease happened because greater control flow similarity allows more different addresses to be compared.
in contrast the correspondence found by mi is roughly consistent.
.
cause transition computation this experiment evaluates the impact of mi on computing cause transitions.
the algorithm in was implemented as a platform on which we tested two versions of the memory comparison and replacement primitive one is symbolicpath based used in the other is the new mi based.
the study is on several real bugs in open source programs including gcc make a n d gawk that have non trivial heap behavior and aliasing.
the failing runs are generated according to the bug reports.
the passing runs are acquired from the correct inputs in the reports if provided previ ous non regressing versions or using an automated patch ing technique .
note that acquiring passing runs is an orthogonal problem out of our scope.
other patching techniques such as can also be used.
results are summarized in table .
the program column contains the buggy programs.
bug idpresents the bug id through which one can identify the report online or thepublication date on the mailing list.
bugdescribes each bug.
passing run shows the sources of the passing runs inputs provided in reports correct input non regressing versions non regressing and dynamic patching predicate switch .
the maximum number of differences present in failing and absent in passing found using symbolic differ encing is presented in sym diffs and the maximum when using mi is in mi diffs .i nsym diffs we report one memory cell only once although it may be a difference alongmultiple paths.
of further interest is the number of differ ences with aliases in column aliases or multiple symbolic paths.
they can cause issues as discussed in section .
column issuepresents the exhibited problems when using the symbolic path based primitive.
we also present the number of transitionsand the average number of differences included in each transition in trans diffs along with time required in seconds in timewhen using the mi version.
observe that in every case the number of symbolic differences is substantially times larger than the numberof differences when using mi because the proper memory correspondence cannot be found.
furthermore the aliases column shows that substantial aliasing is common creating a lot of difficulties for the symbolic method.
as seen in the 224figure normalized runtime and space overheads of memory indexing with and without optimizations.
figure percent of corresponding memory accesses w. and w o mi.figure heap accesses and control flow trace similarity after state mutation in the execution of make.
issuecolumn in most cases symbolic path based computation would not terminate within hours.
the main reason is that lost mutation and destructive mutation caused see section by aliasing prevent the relevant difference subset from being computed so the algorithm ends up searching subsets of the already significant difference sets.
for example in the first gcccase it may be possible that all the enumeratedsubsetsof the 8365differences need to be tested.
ingawk the algorithm simply terminated early unable to produce relevant transitions for the failure.
a case study on detailed comparison.
we performed a separate test focusing on makebug from table .
we selected sample points at intervals along the part of the passing run that is beyond the first divergence of the two runs.
at each sample point we compared the memorysnapshotsacrossthetworunsandthenmutatedthememory in the passing to that in the failing by applying the universal set of differences alternatively using the symbolic pathbased primitive and the mi based primitive.
then we collected the trace after the mutation and compared it to that of the failing run.
trace comparison is conducted usingcontrol flow canonicalization and mi based memory canon icalization.
according to the discussion in section the traces should be identical if the primitives are composable.
the results are shownin figure13.
heap mi and control flow mi represent the similarities of heap access and control flow traces using the mi based primitive and heap sym a n d control flow sym represent those using the symbolic primitive.
observe the access similarity when using mi is consistently near and the control flow similarity is consistently above until the end.
thismeans the mutation is mostly successful in turning the passing run to the failing run.
the similarity is not because we currently do not model external state such as fileids process ids etc.
thus such states are not eligiblefor meaningful comparison and replacement.
in contrast when the symbolic primitive is used the execution quicklydivergesfromtheexpectedcontrolflow havingnear0 similarity and it has near similarity for accessing the heap.in fact the mutated run often quickly crashes due to destructive mutation section .
this supports that the mi primitive is composable but the symbolic primitive is not.
a more in depth consideration of memory indexing and concrete examples of computing cause transitions can be found in our tech report .
in summary mi allows the strength of cause transition computation to be fully realized reflected by our success of scaling to programs like gccwith full automation.
note although a gcccase was presented in .
it was conducted with human intervention.
the later automated system works well for small programs without much aliasing.
.
related work trace normalization divides traces into segments.
segments with the same starting and ending state are considered equivalent.
client applications using such traces only need to look at a consistent representative segment fromeach equivalence class.
the outcome is reduced workload and increased precision.
memory indexing is complementary in that it provides a robust way of comparing program state across executions and hence helps identify equivalenttrace segments.
there has been recent work on comparing executions for debugging regression faults analyzing impact of codechanges and finding matching statements across program versions .
these techniques are able to construct a symbolic mapping of variables across program versions through profiling such as pointer xin version one being renamed to yin version two.
the constructed mapping is static.
in comparison we focus on comparing dynamic address valuesofcorrespondingvariables answeringquestions like does xpoint to the corresponding address in the two executions .
furthermore trace canonicalization facilitated by mi would improve the precision of these analyses.
225table cause transition computation for failures.
program bug id bug passing sym.
diffs mi diffs aliases issue trans diffs time s gcc .
.
wshadow warns on functions predicate switch 12h gcc .
.
776large array size causes abort predicate switch 12h gcc .
.
o1 breaks strength reduce provided input 12h make .
.phony targets are unrecognized non regressing 12h make .
parentheses break make targets provided input 12h make .
.
prevents self remake provided input 12h make .
112rules cannot handle colons provided input 12h gawk .
.
deallocate bad pointer provided input 509early term.
many debugging techniques compute fault candidates by looking at a large number of executions both passing and failing.
in these techniques execution profilesarecollectedandanalyzedstatistically.
somedebuggingtechniques compare a simple profile of a failure with a small number of correct runs usually one .
they use control flow paths and code coverage.
mi is complementary to these techniques by providing a way to canonicalize profiles before they are analyzed to achieve better precision especially for pointer related bugs.
we have demonstrated in this paperthat mi is able to drive cause transition computation thatis highly sensitive to memory alignment.
compared to the recent advances on generating causal explanations of failures the proposed robust finegrainedmemorydifferencingandsubstitutionprimitivesmake itfeasibletoextractsuccinctandin depthinformationabout failures.
for instance it is relatively easier for us to reasonabout whether a value at a given execution point is relevant to a failure.
furthermore mi improves cause transition computation by allowing alignment along the mem ory dimension which substantially improves robustness in the presence of aliasing.
abstractions for memory regions used in static analysis also have the notion that small bounded chains of the control dependence of a region s creation provide a notion of identity for that region .
in contrast we extend this into the dynamic domain efficiently capturing precise identitiesonline instead of just over approximations.
in execution indices are used to locate locks across executions of java programs but the approach is not as generalized or optimized as memory indexing.
.
conclusions we present a novel challenge in dynamic program analysis aligning memory locations across executions.
we proposeasolutioncalledmemoryindexing mi whichprovides a canonical representation for memory addresses such thatmemory locations across runs can be aligned by their in dices.
pointer values can be compared across runs by their indices.
the index of a memory region is derived from the canonical control flow representation of its dynamic alloca tion site such that control flow correspondence is projected to memory correspondence.
enabled by mi we also propose a novel memory substitution primitive that allows ro bustly copying states across runs.
we evaluate the efficiency of two memory indexing semantics.
our results show that the technique has runtime overhead and space overhead on average.
we evaluate effectiveness through two client studies one is trace canonicalization and the other is cause transition computation on failures.
the studies showthat mi reduces address trace differences by .
it alsoscales cause transition computation to programs with complex heap structures.
.