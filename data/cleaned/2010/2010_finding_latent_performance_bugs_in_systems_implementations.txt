finding latent performance bugs in systems implementations charles killian karthik nagaraj ryan braud james w. anderso n salman pervez ranjit jhala purdue university university of california san diego ckillian knagara spervez cs.purdue.edu rbraud jwa nderson jhala cs.ucsd.edu abstract robust distributed systems commonly employ high level rec overy mechanisms enabling the system to recover from a wide variety of problematic environmental conditions such as node failures packet drops and link disconnections.
unfortunately these recovery mechanisms also effectively mask additional seri ous design and implementation errors disguising them as latent performance bugs that severely degrade end to end system performance.
these bugs typically go unnoticed due to the challenge of dis tinguishing between a bug and an intermittent environmental co ndition that must be tolerated by the system.
we present techniq ues that can automatically pinpoint latent performance bugs in systems implementations in the spirit of recent advances in model c hecking by systematic state space exploration.
the techniques proc eed by automating the process of conducting random simulations i dentifying performance anomalies and analyzing anomalous exec utions to pinpoint the circumstances leading to performance degra dation.
by focusing our implementation on the m ace toolkit m acepc can be used to test our implementations directly without mo dification.
we have applied m acepc to five thoroughly tested and trusted distributed systems implementations.
m acepc was able to find significant previously unknown long standing perfor mance bugs in each of the systems and led to fixes that significantly improved the end to end performance of the systems.
categories and subject descriptors d. .
organization and design distributed systems d. .
testing and debugging testing tools general terms performance keywords mace macepc performance debugging distributed system s permission to make digital or hard copies of all or part of thi s work for personal or classroom use is granted without fee provided th at copies are not made or distributed for profit or commercial advantage an d that copies bear this notice and the full citation on the first page.
to cop y otherwise to republish to post on servers or to redistribute to lists re quires prior specific permission and or a fee.
fse november santa fe new mexico usa.
copyright acm ... .
.
.
introduction it is hard to build correct high performance distributed s ystems.
as with any concurrent setting the nastiest bugs are those t hat are caused by the unexpected temporal interleavings of events.
distributed settings compound this problem by exploding the nu mber of interleavings drastically through their node failures message reordering etc.thus in addition to designing communication protocols and data structures that work in the common case the dev eloper must account for the fact that nodes participating in th e system may join leave or fail at any moment and that the network sub strate may corrupt reorder or drop messages sent between n odes.
as a result the most pernicious problems arise not from alg orithmic issues which affect every execution and hence are amenable to profiling but from relatively rare corner case node inte ractions or unexpected packet delays or drops.
the resulting perform ance anomalies are difficult to reproduce and hence to find and to fix.
recently several authors have proposed techniques to find and debug corner case correctness problems in distributed systems.
while ensuring the correctness of distri buted systems is a necessity performance is crucial for these system s and finding and fixing performance bugs can be as important and cha llenging.
specific challenges in performance debugging incl ude first due to the importance of guaranteeing correctness an d reliability in the face of all possible event interleavings d evelopers typically build fail safes into the system worst case recovery mechanisms that periodically kick in and restore the system to a consistent and stable configuration.
unfortunately these recovery mechanisms sweep serious design and implementation flaws un der the rug by disguising them as latent performance anomalies that severely affect the responsiveness availability and end to end behavior of the system.
second the standard means of debuggin g distributed systems is to add code that logs the sequence of e vents along each execution.
unfortunately the amount of logging required to accurately correlate events across multiple node s can impact the performance characteristics of a given live or simu lated execution which makes it hard to use log analysis to find perform ance bugs.
third even if one could generate high fidelity logs wit h low overhead performance problems often only manifest in long executions not the short correctness violations returned by model checking or symbolic execution .
thus to isol ate the performance bug the programmer must undertake the daun ting task of wading through hundreds of megabytes of logs compris ing tens of thousands of events spread across multiple nodes ma nually tracing the complex communication and control flow.
in this paper we present the m ace performance checker a technique that automates the process of finding latent performance bugs in event based distributed systems implementations and a utomates the process of isolating the root cause of the error.
m acepc isbased upon the insight that a class of performance bugs manif est asanomalous executions i.e.
executions whose performance is much worse than the expected performance observed along oth er executions.
consequently we reduce finding performance bu gs to three tasks i determining expected performance of the sy stem ii finding executions whose performance is much worse than expected and iii sifting through anomalous executions to p inpoint circumstances causing the performance degradation.
macepc carries out these tasks by marrying state space exploration with time based event simulation.
first we show how by sampling the state space of the unmodified implementation we can construct event duration distributions edds probability distributions that describe how long each low level event take s to execute.
by combining edds with a programmer specified stopping condition that describes when the desired task is completed m acepc is able to develop a profile of the expected system performance.
second we show how to combine systematic testing wit h the edds to find anomalous executions that take much longer th an expected to reach the stopping condition i.e.
whose perfo rmance deviates significantly from expected performance.
third w e show how to analyze the executions explored to isolate the root ca use of the performance degradation.
we narrow the circumstances w ith an algorithm that characterizes the divergence point of an anomalous execution the point along the execution such that before the divergence some alternate execution achieves acceptable perfo rmance but immediately after the divergence executions of the system have bad performance.
we then demonstrate how collecting event p rofiles of executions can simplify debugging by correlating c ertain types of events with bad performance.
together these techn iques can save programmers a substantial amount of time devoted to diagnosing and fixing the performance bug.
while the techniques behind m acepc could be used with other event simulators to find performance problems in simulatio n we focus our implementation on the m ace toolkit .
m ace is a language and toolkit for building a broad class of event bas ed distributed systems.
we have implemented more than ten signific ant systems in m ace most of which were proposed by others.
this set includes distributed hash tables applicat ion layer multicast and file distribution network measure ment services and consensus protocols ready to run over the internet.
m ace has been in development for seven years is publicly available for download and has been used by researchers at p urdue ucsd epfl cornell ut austin ucla hp labs msr redmond silicon valley and asia and a handful of other unive rsities worldwide in support of their own research and development.
macepc leverages the m ace toolkit allowing us to test unmodified deployable m ace distributed systems implementations .
finally we present results from applying our techniques to five real complex distributed systems.
we discuss the applicat ion of our tool to the systems subtle performance bugs we found an d our solutions.
many of these bugs are instances of correctness b ugs masked by periodic corrective protocols.
importantly the implementations were already thoroughly tested and had been run a cross the internet for several years.
further while we had been aw are of intermittent performance issues and had attempted to dia gnose them on multiple occasions we were unable to do so without m acepc.
on using m acepc to distributed hash table dht systems we improved the worst case ring stabilization time for a pastry implementation by a factor of and in a highly tuned bamboo implementation we improved both the consitency and latency of key value lookups by .figure motivating example.
an example scenario seen in our random tree protocol.
nodes aandcinitially attempt to join the tree at node b. because node awas more fit to be the root of the tree than node b nodebtold node ato be the root and attempted to join under it.
meanwhile upon receiving a join from node c nodebchanges its state to joined since it accepts a child.
later node brejects the joinreply from node aas it is already joined.
this leaves the system in a state with two trees corrected only later during the recov ery protocol.
note that if either join completed before the othe r began this bug would not have occurred.
.
background our goal is to automatically isolate latent performance bug s that arise due to corner case conditions that escaped the develo per s attention during system design and implementation.
these bug s are particularly hard to find and fix because they cannot be easily detected in the presence of recovery code nor reproduced due t o their infrequency.
in particular we do not aim to automatically d iscover algorithmic bottlenecks arising from poor design these errors are easier to detect as they hinder every execution s performance.
as a motivating example consider the scenario illustrated in figure that arose deploying a file distribution application.
t he application used an overlay tree as the basic reliable multicast c ommunication channel.
under some circumstances the application stalled when the overlay tree took too long to stabilize.
upon painst akingly sifting through logs from system runs we found that network l atencies caused unexpected re ordering of events disconnecti ng certain nodes from the remainder.
later a recovery protocol restor ed the system triggered by its coarse grained timeout.
note that the recovery protocol eventually restores the sys tem to a consistent state and hence there is no correctness error per se .
moreover existing correctness checkers like ignore quantitative system aspects like time and hence cannot be u sed to find latent performance bugs such as the above.
in this section we first describe the basic system model of a d istributed system execution suitable for model checking.
we then describe the timed system model which integrates the timin g information of a time based simulator with the basic system mo del.
.
basic system model we consider distributed systems implemented as atomic even tdriven state machines.
the entire distributed system ther efore comprises processes running at the individual nodes together w ith the network layer that the nodes use to exchange events .
the execution of an individual node can be viewed as a state ma chine that moves from one state to the next by executing the transition triggered by reception of events.
events typically come fro m an application the network or the timer scheduler.
when a no de receives an event it executes a transition by executing the handler function registered to receive the callback.
the handler is an ar bitrary piece of non blocking code which executes atomical ly and may in turn trigger events asynchronously on other system no des.
the state of a node is the set of all variables of the state machine at that node.
the state of all nodes together combined with the network timer and application simulator state compri se the system state .
an execution of the distributed system is an initial system state and an ordered set of pairs angbracketleftnode event angbracketright.
intuitively the system evolves as follows at each step the system trigg ers event on node executing at node the corresponding transition and taking the entire system into a new state.
.
timed system model our goal is to isolate bugs that adversely affect the end to end performance of a given distributed system i.e.
bugs that ad versely affect the time taken to carry out the tasks for which the system was built.
thus we must extend our system model to track the pass age of time as other time based simulators do.
in an event driven system there are two ways in which time advances.
the first is the time that elapses between the sending and reception of an event.
this includes the passage of time due to network latency between the instants a network event is sen t and received and the passage of time between the instants a timer event is scheduled and fires.
the second is the time it takes to execute the transition corresponding to a given event i.e.
to e xecute the code of the event s handler.
while our system model requires event handlers be non blocking thus implying they are fast in p ractice they take a non negligible amount of time.
to be consist ent a node s time must be updated according to both of these factor s. to account for time we include a per node clock and extend t he notion of an execution as an initial system state and set of tuples angbracketleftnode event start duration angbracketright ordered by the element start .
intuitively the system evolves as follows.
at each step the system picks the next tuple and updat es the system clock for node to be the maximum of its current value and start .
next the system triggers event onnode thereby executing atnode the corresponding transition.
at the end of the transition thenode system clock is incremented by duration and thus the whole system moves to a new state.
for simplicity in the initial state assume all nodes share t he same start time .
we define the system time as the average node time across all the system s nodes.
we define the running time as the difference between the system time and the start time.
to for malize the notion of performance we require that the developer p rovide astopping condition a predicate over the system state that is true once the end to end task is accomplished.
in the motivating example above our stopping condition is that the system form a sp anning tree across all nodes.
thus the performance of a particular execution is formalized as the execution time the running time when the system satisfies the stopping condition.
.
algorithm we now present our algorithm that uses developer specified highlevel stopping conditions to find performance bugs.
figure shows the three phases of the algorithm training including both a event duration training where we determine the cpu time of each type of atomic event transition and b performance training where we use the event durations to determine the normal ex ecution time anomaly detection where we search the space of behaviors to find poor performing executions i.e.
whose exe cution time is significantly higher than normal and finally anomaly analysis including both a divergence detection where we analyze the anomalous execution to determine the divergence p oint unmodified system training search anomaly analysis event duration distributions ... ... ... .anomalous execution path anomalous execution log similar fast execution log manual debugging event frequencies average time figure system architecture.
first m acepc is trained on the unmodified system a synthetic set of event duration distributions edd are used to create realistic edd which are then used to sample the execution space to learn what an aver age execution is.
next the system and edd are input into the explorer algorithm and it continues until an anomalous execution is found.
finally our anomaly analysis algorithm loca tes the most similar average execution and updates the event frequencies reporting any correlations to execution time .
at this point any debugging tool or user process can be used to compare the two executions until the source of the bug is foun d. narrowing the search for the performance degradation cause and b frequency correlation where we determine if certain event behaviors are related to the performance degradation.
.
ingredients we start by presenting the key ingredients of our technique the notion of an event duration distribution and the two proced ures simulator andexplorer that are used across multiple phases.
event duration distributions.
for the purpose of simulation we represent the time taken to handle each event type e.g.
the t ime taken to execute a transition or deliver a message with a pro bability distribution generated from actual executions of the syste m. we call these the event duration distributions edd .
there are three main advantages edds offer over the naive approach of using r aw execution times observed during simulation.
repeatability after each event the simulator determines the event duration by randomly sampling the event s distribution.
by recording the sequence of random numbers used for sampling we ensure execution repeatability with exactly the same executi on time regardless of variability in events actual time.
conseque ntly even after subsequently modified code e.g.
to add logging per form time intensive testing etc.
the performance observed by the simulator remains the same as the original unmodified co de.
coverage edd sampling allows exploration of executions with event timing combinations never seen in a single run.
this al lows the simulator to explore more variations in timing beha vior including potentially hard to find corner cases.
malleability timing distributions for each event enable exploring what if scenarios with event times by changing the distri bution for events to see the effects on average execution duration.
this allows speculatively evaluating effects of different algo rithms buffering strategies etc.
without actual implementations.
we could construct similar distributions for network laten cy and bandwidth observed on a particular real world topology.
cu rrently we take a simpler approach of randomized latencies cf.
sect ion .
algorithm simulator input systems input stopping condition c input setedd of timing distributions events angbracketleftnode event start angbracketrightqueue timedevents angbracketleftnode event start duration angbracketrightqueue realtimes angbracketleftevent realtime angbracketrightqueue initialize sandevents whilecnot satisfied bysdo angbracketleftnode event start angbracketright events.pop node.time max node.time start starttime realtime simulate event onnode realtime realtime starttime duration edd node.time node.time duration timedevents .push angbracketleftnode event start duration angbracketright realtimes .push angbracketleftevent realtime angbracketright return angbracketlefttimedevents realtimes angbracketright algorithm simulator implements the core simulation mechanism.
this algorithm takes three input parameters a system to be simulated a stopping condition and a set of edds.
the algorithm constructs three queues i events which holds the currently pending events ii timedevents which records the sequence of already executed events and their sampled durations iii realtimes which records the sequence of already executed events and their realdurations.
events is initialized with a small set of events used to bootstrap the system typically an appl ication initializing event on each node and timedevents andrealtimes are initially empty.
next the simulator enters a loop in whi ch it keeps executing pending events until the stopping conditio n becomes true.
at each iteration the algorithm pops the first pe nding event i.e.
with the smallest start time off the events queue.
the node running the chosen event sets its clock to the maximu m of its current clock value and the scheduled start time of the event.
next the algorithm records the real time and executes the ev ent at the chosen node potentially causing other events to be enqu eued e.g.
in the case where a new timer or network event is scheduled .
to determine how long the event transition took the a lgorithm randomly samples the edd for the chosen event and assigns the result to duration .
further the chosen node advances its clock by duration .
next the algorithm records the event execution by adding the tuples angbracketleftnode event start duration angbracketrightand angbracketleftevent realtime angbracketrightto thetimedevents andrealtimes queues respectively.
when the stopping condition is satisfied the si mulation stops and returns timedevents andrealtimes .
note that we cantrivially ensure there is a pending event by adding a dummy ti mer that repeatedly fires at long periods.
similarly we ensure t he simulation terminates by encoding a timeout in the stopping cond ition.
algorithm explorer input systems input stopping condition c input setedd of timing distributions input integern execs angbracketleftexecution angbracketrightqueue eventtimes angbracketleftevent realtime angbracketrightqueue fori 1tondo reset system angbracketleftex times angbracketright simulator s c edd addextoexecs add each element of times toeventtimes return angbracketleftexecs eventtimes angbracketright algorithm explorer implements a search of the space of executions via repeated calls to simulator .
this algorithm takes four input parameters a system to be analyzed a stopping condit ion a set of edds and an integer ncorresponding to the number of executions to be explored.
the algorithm constructs two que ues i execs which holds the explored executions ii eventtimes which holds the recorded times for different events along th e explored executions.
both queues are initially empty.
the alg orithm iterates ntimes.
in each iteration it calls simulator and adds the returned execution to theexecs queue and each event real duration tuple to the eventtimes queue.
after the loop it terminates and returns the set of observed executions execs and eventduration tuples.
between each invocation of simulator the algorithm resets the system state which includes tasks such as c learing the simulated network of messages instantiating new nodes for the next execution removing scheduled timers and resetting t he random number generator state.
since simulator is randomized each invocation of simulator returns a possibly different execution.
event duration independence.
our approach to simulating the passage of time assumes that within a node event durations a re independent i.e.
the durations of different events are uncor related.
different edds can be provided for nodes modeling nodes at d ifferent speeds but our implementation does not support temp orally correlated durations e.g.
caused by resource competition from shortlived background processes .
nevertheless this simple ap proach suffices to explore naturally occurring variations in event orderings and timings unearthing many interesting performance bugs .
we leave modeling potential temporal correlations to future w ork.
deterministic replay.
a key property of our simulator is deterministic execution replay.
by recording each event tuple du ring the search phase the simulator has a path describing the comple te execution and can later replay this path by executing each eve nt on the appropriate node at the time indicated.
to provide consi stent executions when replaying a path the simulator must contro l all sources of non determinism.
we address non determinism in event orderings by using a simulated source of time as discussed a bove.
in addition to the event orderings real systems often make u se of non determinism within event handlers for randomized algorithms.
when executing in the simulator systems should use a determ inistic simulated random number generator.
.
finding performance bugs next we describe each of the phases of our algorithm.phase 1a event duration training.
first we build edds that describe how long each type of event transition takes to exe cute.
to compute these distributions we pick a value nsuch that n random executions cover all events i.e.
each event occur s in at least one of the nexecutions .
if we later determine coverage was incomplete we can either increase nand re train or substitute a similar event s distribution.
we define a seed edd for each ev ent where its duration is distributed uniformly over some fixed i nterval such as 10ms .
next we execute the explorer on the system the stopping condition the seed edds and n. when the search is complete we discard the returned set of executions and u se the angbracketleftevent realtime angbracketrighttuples returned in realtimes to compute edds for each event using the cumulative frequency distribu tion.
phase 1b performance training.
next we use the edds computed in phase 1a to quantify what should be deemed as anomalous performance.
to this end we determine the typical ti me it takes the system to carry out its high level task i.e.
ave rage time taken to reach the stopping condition.
concretely we i nvoke theexplorer algorithm on the system the stopping condition the edds computed in phase 1a and another n. we discard the returned set of event time tuples and use the returned set of ex ecutionsexecs to compute the values of the first and third quartiles of the execution time.
we use the definition of mild outliers q3 .
q3 q1 to flag anomalous executions.
phase anomaly detection.
in phase we explore the space of behaviors to find executions with poor performance i.e.
w hose execution time is anomalous as computed by phase 1b.
concret ely we runexplorer with the same parameters as before except we use a large n and terminate the search when we find an execution that falls outside the bounds determined in phase 1b.
th e use of different random number generator implementations allo w various search algorithms to be employed.
for example our prio r work on the m ace model checker m acemc used an iterative bounded depth first search generator for exhausti ve testing which is impractical when simulating microsecond granula rity timings.
in our experience a basic randomizing generator is su fficient to detect many performance bugs.
we leave to future work furt her exploration of other generators such as a best first generat or.
phase 3a divergence detection.
an anomalous execution can consist of tens of thousands of events.
the prospect of sifti ng through all these events to find the performance bug would dau nt the hardiest systems developer.
in this phase we analyze th e execution to pinpoint the divergence point the first event along the execution that leads to stable performance degradation.
by so the developer may skip past execution prefixes which may l ead to good performance focusing on the remainder.
divergence detection is an adaptation of the technique for fi nding a critical transition pioneered in m acemc.
the insight behind the algorithm is as follows.
many performance problems are caus ed by corner case race conditions which cause latent performanc e bugs.
prior to the race condition occurring in an execution branc hing the execution and following a different path avoiding the race c ondition leads to a good execution.
thus we identify prefixes of the exe cution which have notexperienced the race condition and narrow down where the race condition takes place.
our experience in dicates that knowing the divergence point can allow the progra mmer to ignore large portions of the execution trace in one case n early of the events could be ignored saving many hours of debugging time.
more precision is not provided by this techn ique because what it identifies in the execution is where the race c ondition finished being enqueued onto the pending events list no t when it actually occurs.figure we first perform an exponential search shown above the anomalous execution to determine bounds for the divergence point then a binary search shown below the anomalous execution to isolate the divergence point.
note that to avo id finding cases which are only slightly non anomalous any exe cution exceeding q3 will be considered a performance failur e. as illustrated in figure we begin by initializing the prefix length to one event replaying the portion of the path overla pping with the prefix and then performing up to krandom walks.
if any of the random walks satisfies the timing constraint the race c ondition has not occurred so we double the prefix length and repea t. eventually we reach a part of the path where none of the krandom executions satisfy the timing constraint and thus we have l ower and upper bounds on the divergence point.
note that for the ti ming constraint we use the third quartile rather than the outlier threshold.
otherwise the analysis will find branches that perform o nly slightly under the outlier threshold distracting rather t han enhancing the debugging.
though this can further decrease the prec ision of the analysis the goal is to confidently identify states wh ich are safe to ignore.
the algorithm s second phase isolates the di vergence point by conducting a binary search of the lower and upp er bounds.
the algorithm terminates producing an execution s atisfying the timing constraint with the longest common prefix to the anomalous execution.
phase 3b frequency correlation.
to further simplify the debugging process we also collect event frequencies and the exec ution time for executions simulated during phases and 3a.
comput ing this data is straightforward from the eventtimes queue returned with each execution.
we automatically compute the correlation and scatter plot o f each type of event with the execution time.
this information can quickly direct developers attention to the events whose pr esence or absence is related to the performance bug.
in the three sit uations we have applied this technique thus far after manually disc arding event types that are trivially correlated with path leng th such as periodic timer expiration remaining high correlations were directly or indirectly related to the bug drawing developer a ttention to important parts of the implementation.
an example of using event frequencies is shown in figure .
this plot is from the motivating example in figure .
it shows a correlation between the event described by receiving a joi nreply and sending a remove message and the execution time.
had thi s tool been available when we first diagnosed this bug we would have seen quickly that this event is correlated with longer p aths which would have helped us find the bug more quickly.
.
implementation details while the techniques behind m acepc can be applied to arbitrary event driven systems implementations and simulator s our implementation targets systems implemented using the m ace frame work .
focusing on m ace significantly simplified the effort in building m acepc particularly given m acemc as a starting point.
m ace structures each system component as a service implemented using c objects.
each service object is structu red as a state machine whose states correspond to valuations of the object s member fields.
each event s transition is implemente d as an atomically executed non blocking c method call whic h can asynchronously send events to itself through timers or non local nodes through network messaging.
finally m ace combines the high level service object specifications with scaffoldin g code that handles event dispatch serialization callbacks timers etc.to generate c code that is ready to run on live networks.
in the remainder of this section we describe the necessaril y implementation changes to enable the implementation of m acepc.
note that unlike many discrete event simulators the goal of macepc is not the accurate simulation of the distributed system but instead to be accurate enough to expose interesting perform ance bugs.
this allows us to keep our event simulator quite simpl e. scalable network simulation like other distributed systems simulators m acepc must simulate network delays.
m acepc seeks a middle ground between using a complete network simulator modeling congestion and u sing a simplistic model which only considers access link bandwidt h. we also considered using measured delay distributions as for e vents but found a simple model was sufficient to expose interesting bugs.
the time taken to send a message from a source to a receiver is computed as the sum of four factors i a propagation delay fixed based on configuration that models the delay to transfe r data from source to destination ii a transmission delay that models the bandwidth between the source and receiver and the number of simultaneous flows sharing the link and iii a random delay drawn from a pareto distribution to model router queues.
by default the propagation delay is set to 1ms and the bandwidth between peers at kbps.
however each of these is co nfigurable for different node pairs.
to simulate the common ca se where a node s first hop link is the bottleneck we count the n umber of active outgoing flows the node has to all other nodes abo ve a certain size and use this value to divide the available ban dwidth.
for example if the bandwidth is configured at kbps and t here are two simultaneous flows the transmission delay for a mess age corresponds to the time it takes to send the message over a kbps link.
we assume that all flows with a certain threshold nu mber of bytes in transmission are receiving their fair share from tcp.
in particular we exclude light flows from this equati on to avoid equally penalizing these flows.
in our current impleme ntation this threshold is set to bytes.
while the above does not perfectly mirror the behavior of net works it allows m acepc to simulate the network with enough efficiency and fidelity to unearth tricky performance bugs.
w e also validated that m acepc could find a known topology specific bug by configuring the latencies accordingly across system node s. time based simulation since we were adapted m acemc to build m acepc we had to add time to its execution model.
in so a pending event q ueue was added in place of querying simulators for possible event s at each step.
this modification had a dramatic effect on simulat ion complexity enabling m acepc to run simulations at much larger scale than is practical using m acemc.
random number generators all non determinism in the implementations is mapped to cal ls to the m ace random number generator library including selection of which event to execute.
as a result we can explore dif fer ent search techniques for the state space by implementing di fferent random number generators.
when we first developed m acepc it used the default m acemc random number generator that conducted a bounded depth first search.
however the degree of randomness in simulating microsecond level times rendered the search ineffective.
next we explored a search technique which emu lated the other random number generator by picking a fixed number of random candidates to explore in every step rather than an exh austive search.
this was more practical but added complexity w ithout providing any sort of guarantees about coverage.
curren tly we use a uniform random number generator which provides sta te space sampling rather than exhaustive search.
we leave as fu ture work using a best first random number generator that intenti onally pushes the system into poor performing states.
preparing a system for m acepc finally to use m acepc to find performance bugs the user must write i one or more test harnesses or driver applicatio ns that are used by m acepc to execute initialize and execute the application and ii a stopping condition that is used by m acepc to determine when the system has completed its task.
for exampl e to find performance anomalies in a file distribution system the user could write i a driver application in which one node publis hes the file and the other nodes request the file and ii a stopping co ndition that holds when each receiver node has finished downloadin g the file.
the compiled driver application is linked with the m ace system object files and simulator specific m ace libraries for message queuing and delivery system time timer scheduling and ran dom number generation and m acepc uses the resulting system to find performance bugs.
.
experiences we ran m acepc on m ace implementations of b ullet pastry b amboo c hord and a random tree protocol r and tree as described below and found significant previously unsolved performance issues with each.
the followi ng examples are intended to give an understanding of the types of b ugs we were able to find and the utility of the tool.
we try to descri be only enough of the system being executed to understand the bu gs described and the stopping conditions used.
all the systems we evaluated have been extensively tested in live runs and p astry bamboo chord and r and tree were tested for correctness using m acemc.
the b ullet implementation though not checked with m acemc was significantly performance tuned as a major differentiator relative to contempor aneous systems.
the experiments below were run in a variety of network configu rations.
m acepc ran on a single machine with an intel core2duo processor at 3ghz with 4gb of ram.
r and tree was tested at small scale and yielded fast executions.
b amboo was tested on nodes and the remaining systems were tested in configurat ions of nodes.
note that it is impractical to perform any ki nd of exhaustive search using standard model checking techniq ues on systems of this size.
executions run in m acepc took anywhere from seconds to minutes.
this execution time is highly var iable based on two primary factors the length of a path reachin g the stopping condition and the per event processing in m acepc.
for some systems such as p astry the stopping condition a perevent processing task has a complexity of o n2logn fornnodes so paths take much longer to explore as the size of the configur ation grows.
the time spent searching for anomalous executions varied de pending on the number of paths m acepc had to search beforefinding an anomaly but the bugs we found appeared relatively early.
the longest part of m acepc execution was the divergence detection phase which has to run o klogs steps where kis the number of random paths at each step a parameter impacting the er ror andsis the depth of the divergence point.
anomaly analysis range d from hour to hours to run.
though hours is on the long sid e the search was unattended and did allow us to ignore nearly of the events in the anomalous execution.
.
bullet bullet is a mesh based peer to peer file distribution protocol similar in functionality to b ittorrent .
each node contacts a source node receives a set of initial peers to join and then begins downloading a file in parallel from other nodes.
before discu ssing the bugs we found there are a few relevant implementation de tails to discuss.
first b ullet uses two transports one for data and a second for control messages.
the data transport is confi gured to only buffer one block s worth of data at a time while t he control transport is configured to buffer an unbounded numbe r of messages.
the tcp transport in m ace has the ability to buffer messages so that it can send them asynchronously without usi ng service resources.
limiting the buffer length allows b ullet to quickly react to changing network conditions and applicati on requests since buffered messages cannot be cancelled.
b ullet actively detects when message blocks will not fit in the queue a nd schedules a timer to send them later.
however diff messag es or those that inform a peer about blocks a node possesses are al so sent over the data transport and are not similarly protected aga inst a full buffer.
second b ullet is structured on top of r ansub a gossip protocol that periodically delivers a changing rand om subset of mesh participants to each node.
stopping condition.
the stopping condition for b ullet is simple all nodes should complete downloading the file.
anomalies.
in the first experiment with b ullet we used a setup of nodes downloading a 20mb file.
m acepc found the first anomaly after runs representing an second execution which exceeded the .
second upper bound as computed in section .
.
each execution took approximately seconds of real time a nd terminated in between simulator steps.
after examining the times that each individual node took in t his execution we determined that only one slow node limited ove rall system performance.
anomaly analysis showed that the discr epancy was based on the timing of one particular message the slo w node sent to one of its peers.
upon further investigation we realized that in the good execution the slow node s message ca used a diff to be sent to it successfully which happened to contai n information about two blocks that were never successfully sent by any other node.
the slow node was then able to request these block s from this peer and complete the file download.
in the anomalou s execution the message was timed such that the diff message s ent from the slow node s peer was dropped by a full transport.
as a result the slow node never learned about the two blocks lea ving it stuck since it did not know about any peers who had the missing blocks.
eventually r ansubdelivered a new set of candidate nodes and the slow node was able to join one of them and retrie ve the missing blocks.
however waiting on r ansubwas responsible for the delay causing the anomalous execution.
the second experiment with b ullet used only a 2mb file but macepc found the second anomaly in less than executions.
in this case the anomalous execution took around .
second s whereas a normal execution took no more than seconds of simu lated time.
to debug this case we once again examined indivi dual node completion times to learn that only a few nodes were slow .the first thing obvious from inspecting the slow nodes logs w as that no blocks were received until approximately seconds i nto the run then they quickly retrieved all the blocks.
we found tha t the node did not acquire any peers for the first seconds.
the slow node did receive a list of candidate peers from the source whe n it joined.
however its attempts to join each of them failed bec ause no candidate could accept another peer.
thus the node waite d seconds for r ansubto deliver it a set of new peers.
improvements.
to fix our first anomalous condition we simply changed diff messages to be sent over the control transport i nstead of the data transport.
this eliminated the problem of diffs b eing dropped by the transport and ensured that all nodes receive d all intended diff messages.
the second performance problem was based on the fact that when a node is rejected by potential peers it could have to wa it for r ansubto deliver it new ones.
to overcome this idle time we changed the joinreject message to contain the list of the rej ecting node s peers.
then when a node receives the joinreject mess age it has a set of other nodes it can try to join.
note that in both cases the overall system execution was correct .
however just as corner cases in execution can lead to errors similar corner cases can lead to unexpectedly slow performance .
automated state space exploration techniques appear to be wel l suited to automatically find both types of conditions.
.
pastry pastry is a well known distributed hash table dht protocol that enables nodes to self organize into a ring structure.
e ach node in the ring takes an address in a circular address space and b ecomes responsible for the address space in the immediate vicinity of its own address.
the p astry protocol organizes the ring to enable routing to any address using a path of no more than log n hops.
the primary functionality we are concerned with is the stabi lization of the p astry network.
p astry s performance can be measured by how long it takes nodes to finish their self organiza tion protocol.
this protocol includes two basic components.
the active join component allows a joining node to connect to an existing node and follows a protocol to find where in the network it shou ld insert itself.
a node njoins by routing a join message to its closest peer in the address space who tells nof its address space neighbors by sending nits leafset .
along the way it also gathers information about other nodes in the system.
this protoc ol will execute correctly as long as only one node is joining at a time and in the absence of departing nodes.
to handle multiple simultaneous node joins and departures the maintenance component periodically exchanges leafset information with peers to ensure that each node s state is accurate.
this protocol component can correct a wide variety of errors mis takes and dropped messages and therefore mask many performance b ugs.
stopping condition.
the p astry stopping condition is the time all nodes routing information has stabilized to a consiste nt state.
this involves checking both that each node knows its immedia te neighbors important for correctness and fault tolerance and that the routing distance between all pairs of nodes is logarithm ic in the number of nodes.
anomalies.
during the training of p astry anomaly conditions we stopped the training early.
in the first paths the averag e execution times for the pastry nodes in simulated seconds w ere .
.
.
.
.
.
variations of this magnitude were immediate indicators of a performance problem.
we re ran m acepc having it save any execution that took longer than simulated seconds.
of the fi rst40 paths took longer than seconds the longest taking .
seconds in simulator steps.
all executions were within a second of a multiple of seconds a clue that the execution time was being dominated by the behavior of a system timer firing every seconds.
anomalies such as these were not unexpected.
in earlier expe riments we had observed that the p astry implementation required either a long stabilization period after first being started or an extended stagger start period where nodes are slowly started over the first seconds of the experiment.
running m acemc over p astry did not flag these problems because p astry was still eventually reaching the stopping condition.
we also did not attempt to extensively debug this performance problem in live executi ons because of the difficulties of debugging a distributed system and the knowledge that we could just wait for it to stabilize before c onducting other experiments using p astry .
anomaly analysis on the longest path indicated that the perf ormance had not diverged before step .
in both fast and slow executions all nodes are trying to join at once.
since the bo otstrap node does not add a joining node until after each joining node has confirmed that it is finished joining multiple simultaneous joining nodes will initially believe they are in a node ring with ju st them and the bootstrap node.
each contacts the bootstrap node w ho initially knows no one.
it responds telling them about just its elf and does not add them to its leafset until after they confirm they h ave finished joining.
then all nodes nearly simultaneously ann ounce themselves as new members of this small ring largely oblivi ous to the other recently arrived nodes.
in the ensuing mayhem of the active join protocol the nodes closest in the address space to the bootstrap node are succes sful while the state of other nodes further away depends on the pre cise order in which nodes are added and removed from the leafset of the bootstrap node.
unlucky nodes take one or more executions of the periodic maintenance protocol to finally correct their stat e. each time the periodic protocol executes a node will move kneighbors nearer to their correct position in the ring.
if nnodes join simultaneously and n k this can take a very long time.
improvements.
the basic problem is that waiting seconds for each execution of the periodic protocol is a huge performanc e penalty during ring construction.
scheduling the protocol more fre quently would help at the added cost of higher overhead in the common case such fixes are only required during times of high node ch urn .
an adaptive timer could be used though its design would like ly involve difficult and network specific tuning parameters.
after exploring a range of options we settled on the followi ng solution.
the problems essentially occur for nodes who are r eplaced in the bootstrap node s leafset but know nothing abou t nodes near them in the address space.
thus we notify a node with the current leafset when removing it from the leafset.
when it receives the leafset it will be informed of several nodes which are close r to its correct place in the ring.
this same information would be rec eived at a later time when the maintenance protocol runs but at tha t point the information will be more out of date and less relevant.
it is important that the node get this particular version of the le afset because the later version will only contain nodes close to the b ootstrap node not to the evicted node.
after making these improvemen ts all execution times varied in length from .
simulated seconds.
.
bamboo bamboo is an enhancement to the original p astry protocol designed to better handle network churn and cause lower netw ork overheads than the very verbose p astry active join protocol.
assuch the performance metrics of interest and the stopping c ondition are the same.
bamboo accomplishes its lower overhead by shortening the active join protocol to only contain information about the lea fset rather than other information gathered along the way.
it als o reduces the amount of state in the maintenance protocol thus r educing overhead while still converging to good routing paths.
we used m acepc on our b amboo implementation and found three interesting performance bugs.
we describe two below bamboo keeps track of successors and predecessors but does not always know which one a node will be.
if there is a poor balance of node identifiers a node initially suspecte d to be a predecessor could later end up in the successor set.
to accommodate this there was code in b amboo to take a node pushed out of one set and check to see if it belongs in the other set.
however a bug caused the variables maintaining the boundary of the set to be updated before checking to see if the boundary belonged in the other set.
thus this caused such nodes to be forgotten and the problem to be later corrected by the maintenance protocol.
despite this active join change unusual orders of joining nodes sometimes cause peers to not learn of each other.
again this is because each peer first gathers information then announces its liveness.
this causes immediate neighbors to sometimes be wrong until the maintenance protocol executes .
to fix this problem we added an extra field to the inform message the b amboo implementation uses to confirm that it is alive.
this additional field indicates whether the info rmant believes it is adjacent to the informed.
if any informed node disagrees with this adjacency information it trigger s the maintenance protocol early to fill in gaps.
after implementing these fixes m acepc was unable to find any executions which take longer than second.
encouraged by th is improvement we conducted a real evaluation similar to the o ne described in the b amboo paper comparing the original m ace bamboo implementation with our fixed implementation.
in this test we ran a set of instances of b amboo using modelnet on a cluster of machines each with gb of ram dual bit quad core intel xeon cpus and gigabit ethernet connectio ns running gentoo linux .
.
.
the experiment measures the co nsistency and latency of b amboo routing lookup information as the median session time of a node ranges from seconds to seconds.
to measure consistency every set of nodes share a common lookup schedule and lookups are considered consist ent if a majority of responses indicate the same target node.
the results are shown in figures and .
the version produced by fixi ng the bugs found using m acepc allowed us to deliver better consistency eliminating roughly of inconsistent lookups while also reducing latency by up to under high degrees of churn.
these results demonstrate that the bugs we fix using m acepc translate to actual improvements in the protocols we test not just arcane nuances tickled by rare execution paths.
.
chord and rand tree we have also applied m acepc to the m ace implementations of c hord and r and tree and have found performance bugs in each.
these include the example bug illustrated in figure .
the chord bugs were similar in flavor to the p astry bugs with similar solutions.
the r and tree bugs were similar types of problems to the one in figure and caused the recovery protocol t o correct them.
the recovery protocol only executes every 220execution length sec joinreply remove frequency figure positive correlation between execution length and erroneous event.
700percent consistency median session time sec mace bamboo unmodified mace bamboo fixed figure percentage of lookups returning a consistent result.
700mean latency msec median session time sec mace bamboo unmodified mace bamboo fixed figure mean latency of consistent lookups.
seconds in a typical installation since it is only supposed to correct for network partitions and not protocol bugs.
thus th e bugs in r and treewere responsible for very slow tree formation when many nodes join at once and correcting them allowed these sc enarios to proceed in under simulated second rather than t ens of seconds.
.
limitations while this approach to finding performance bugs has been successful and seems quite promising it is not a panacea.
we des cribe some limitations of our approach.
first we run the real system under particular workloads.
th us our system can only find performance bugs manifested by those particular workloads though the use of randomized simulat ion allows us to exercise multiple behaviors for each workload.
second because our systematic exploration considers exec utions based on realistic distributions of event timings under a pa rticular environment it will not cover as many code paths as a traditi onal model checker .
this means it may have to be r un separately under different deployment environments e.g.
different network conditions etc.
.
the alternative approach of con sidering all possible performance conditions appears impractical.
thus our present implementation has not considered compli cated execution search strategies or temporal correlation s between individual event timings because they have not yet been nee ded.
the design supports these possibilities and we anticipate that additional performance bugs may be isolated with additional d etail.
.
related work macepc is related to several techniques for finding errors in software systems.
fault isolation.
our work is related to ideas in the fault isolation literature such as delta debugging which systematically searches a space of possible faults to isolate the one that tr iggers a particular bug.
a variant of this idea is predicate switching which flips branches along a path in order to explore alternat e executions thereby isolating the branch that exposes a partic ular bug.
these techniques focus on correctness problems and find bug s that cause well defined crashes in contrast our work attempts to isolate performance problems by finding executions that take an omalously long.
systematic execution exploration.
macepc is closely related to tools which find correctness bugs by systematically execu ting different paths through unmodified implementations.
v erisoft views the entire system as several processes communicating through message queues semaphores and other ipcs.
it schedules the seprocesses and traps calls that access shared resources.
by c hoosing the process to execute at each such trap point the scheduler can exhaustively explore all possible interleavings of the proce sses executions using a stateless search thereby finding a variety of errors.
cmc also directly executes the code and explores diffe rent executions by interposing at the os scheduler level.
m odist also directly executes the code and is focused on transpare nt execution and checking of fail stop and divergence errors.
chess improves the effectiveness of systematic exploration usin g iterative context bounding and fair stateless search .
j avapathfinder checks java programs by interposing at the jvm level in a manner analogous to cmc.
m acemc combines verisoft style stateless search with random walks to find liveness bugs.
furthermore m acemc uses a binary search over the erroneous execution to pinpoint the critical transition before which the system could have recovered to a live state but after whi ch the recovery becomes impossible.
our notion of a divergence poi nt is an adaption that differs in that it is easier to tell whether a state is dead i.e.
can never recover to a live state than whether per formance has degraded.
to be conservative our technique finds t he divergence point when paths begin to show signs of degradati on.
all the above ignore time and hence cannot be used to find error s related to performance anomalies.
performance tracking.
pip is a concurrent complementary technique for finding bugs in distributed systems.
p ipis an annotation language and an expectation checker which can be applie d to executions.
p ipprovides a way to visualize distributed path flow in a system and to write expectations to validate system pat hs.
by writing a set of execution validators the idea is that you ca n find performance bugs by looking at any non validated paths.
m acepc is easier to use as i it does not require a live deployment of the system ii it can automatically test a wide variety of executi ons and iii it does not require careful manual examination of ever y possible distributed path flow.
x t race allows developers to better understand the performance of their system by using exte nsions to the existing protocol stack to trace the flow of messages ac ross protocol layers networks and applications.
like p ip x t race is focused on debugging particular live live executions wher eas m acepc automatically finds executions with anomalous performan ce.
finally t rend prof allows users to measure the empirical computational complexity of implementations by plotting t he performance of the system across a range of input sizes.
diverge nces in expected behavior can pinpoint bottlenecks in the code e. g. functions whose run times grow faster than linearly with input s ize.
it is not clear if such techniques can be adapted to the uncertai n environment of distributed systems.
.
conclusions we have presented m acepc a technique that finds andisolates performance bugs in unmodified distributed systems code by searching the execution space for executions that perform f ar worse than is typical.
m acepc starts by training itself using the runtimes of actual events from real executions.
next m acepc uses the distributions to explore a large number of executions l ooking for executions that take abnormally long to complete.
fi nally upon finding an anomalous execution m acepc carries out systematic search for the most similar execution that does not e xhibit the performance bug.
the two executions along with an autom atically identified divergence point the step after which it b ecomes impossible for the execution to achieve acceptable perform ance serve dually to direct the developer to a portion of the execu tion believed to contain the bug and to attest that the bug does no t occur before the divergence point.
further event frequency d ata is available to the programmer and its correlation with perfo rmance can help guide the developer to focus on relevant types of eve nts.
we have applied m acepc to five mature systems finding longstanding performance bugs in each.
relative to running expe riments on nodes spread across the internet or even on a local area network emulator we have found that our performance checke r significantly simplifies and speeds the task of performance debu gging and does not require expensive manually inserted logging t hat often obfuscates the underlying bug.