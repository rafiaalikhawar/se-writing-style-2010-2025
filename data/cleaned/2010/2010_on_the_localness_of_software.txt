on the localness of software zhaopeng tu zhendong su premkumar devanbu department of computer science university of california at davis ca usa zptu su ptdevanbu ucdavis.edu abstract then gram language model which has its roots in statistical natural language processing has been shown to successfully capture the repetitive and predictable regularities naturalness of source code and help with tasks such as code suggestion porting and designing assistive coding devices.
however we show in this paper that this natural language based model fails to exploit a special property of source code localness .
we find that human written programs are localized they have useful local regularities that can be captured and exploited.
we introduce a novel cache language model that consists of both an n gram and an added cache component to exploit localness.
we show empirically that the additional cache component greatly improves the n gram approach by capturing the localness of software as measured by both cross entropy and suggestion accuracy.
our model s suggestion accuracy is actually comparable to a state of the art semantically augmented language model but it is simpler and easier to implement.
our cache language model requires nothing beyond lexicalization and thus is applicable to all programming languages.
categories and subject descriptors d. .
distribution maintenance and enhancement general terms algorithms documentation experimentation measurement keywords localness cache language model code suggestion .
introduction the spectacular advances in natural language nl processing in recent years in terms of speech recognition translation etc.
have in great measure been due to the ability of language models to capture the repetition and regularity in commonplace speech and writing.
language models assign a probability to a word sequence using an permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
fse november hong kong china copyright acm ... .
.estimated probability distribution.
high quality language models lie at the heart of most nl applications such as speech recognition machine translation spelling correction and handwriting recognition .
the most successful class of language models are n gram models introduced three decades ago .
recently hindle et al.
building on the uniqueness property of source code reported by gabel and su have shown that software corpora like nlcorpora are highly repetitive and thus predictable using language models.
good quality language models show great promise in software engineering applications.
hindle et al.
show that n gram language models perform quite well and leverage this fact for code suggestion.
in addition nguyen et al.
have adapted the n gram models for cross language porting and allamanis et al.
have applied them to the automatic recognition and checking of coding styles.
in this paper we follow this work to further improve language models.
we begin our work by remarking on a special property of software in addition to being even more repetitive and predictable than nl source code is also very localized .
due to module specialization and focus code tends to take special repetitive forms in local contexts.
the n gram approach rooted as it is in nl focuses on capturing the global regularities over the whole corpus and neglects local regularities thus ignoring the localness of software .
the localness of source code refers to the skewed distribution of repetitive n gram patterns in the locality namely the endemism andspecificity ofn gram patterns in the locality.
endemic n grams occur only in one locality such as a single file .
for example new programming units files modules usually introduce new identifiers variable names method names and such along with endemicn gram patterns of identifier use e.g.myinvitees.next .specificity is a less degenerate version of endemism signifying the tendency of some non endemic n grams to favor a specific locality while not being endemic to it.
for instance different programmers have idiosyncratic n gram frequencies e.g.some programmers favor for int i while others prefer for int size resulting in the frequency biased n gram patterns in the specific files that each programmer is involved in.
sincen gram models can only provide suggestions occurring in the training data n gram patterns specific or endemic to the locality can be overlooked.
for example let us predict the next token after a sequence for int .
suppose the token i followed these three tokens of the time in the training data while the token size followed them of the time.
the n gram model will assign i a probability of .
and size a probability of .
and thus i is chosen.
for an isolated line this would be the reasonable choice to make.
but now suppose that several previous lines in the same file contained the n gram for int size while none contained for int i .
arguably the token size shouldpermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
copyright is held by the author owner s .
publication rights licensed to acm.
fse november hong kong china acm 269then be assigned a much higher probability.
a token used in the immediate past is much more likely to be used again soon than its overall probability that n gram models would predict.
the key insight underlying our work concerns this critical limitation of traditional n gram models.
our central hypothesis is source code is locally repetitive viz.it has useful local regularities that can be captured in a locally estimated cache and leveraged for software engineering tasks.
we believe that this localness of software has it roots in the imperative to create modular designs where modules secrete localized knowledge .
to leverage this property we deploy an additional cache component to capture the localized regularities of both endemic andspecificn gram patterns in the locality.
however we need to overcome several challenges how to combine the global n gram model with the local cache model?
how to automatically pick the interpolation weights for different n grams?
what is the locality to use?
for example how much local data do we need?
which length of n grams should we use?
does the additional cache component impose high computational resource cost?
in section we solve these problems by introducing a mechanism that does not compromise the robust simplicity of n gram models.
our empirical evaluation on projects from five programming languages demonstrates that a cache language model consisting of bothn gram and cache components yields relative improvements of in suggestion accuracy over the state of the art n gram approach .
surprisingly using just the cache component built on 5k tokens by itself outperforms the n gram model trained on nearly 2m tokens in suggestion accuracy.
it is worth emphasizing that the cache model is quite simple and only requires lexicalization no parsing type checking etc.are required.
because of its simplicity and unique focus on localness it is complementary to most state ofthe art research on statistical modeling of source code.
contributions.
our key contributions are .we demonstrate section empirically over large software corpora that source code is localized in the sense that code regularities n gram patterns are endemic specific and proximally repetitive in the locality .
.we introduce section a novel cache language model to capture the localness of software that is simple requiring no additional information other than the tokens and easy touse automatic selection of dynamic interpolation weights for n gram and cache components .
.we show section that the cache model indeed captures the localness of source code and the strength of the cache model can be exploited for code suggestion which substantially improves the state of the art n gram approach .
4we provide section .
.
an option for code suggestion when there is not enough corpus to train a language model using only the cache component built on thousands of tokens which achieves comparable suggestion accuracy with the ngram model trained on millions of tokens.
.
background .
statistical language models language models are statistical models that assign a probability to every sequence of words which reflects the probability that this sequence is written by a human.
considering a code sequence s t1t2...tn it estimates the probability as p s p t1 n productdisplay i 2p ti t1 ... t i that is the probability of a code sequence is a product of a series of conditional probabilities.
each probability p ti t1 ... t i denotes the chance that the token tifollows the previous words the prefix h t1 ... t i .
however the probabilities p ti t1 ... t i are in practice impossible to estimate since there are astronomically large numbers of possible prefixes.
for a vocabulary of size and sentences with maximal length of there are approximately 1043different probabilities to be estimated which is impractical.
therefore we need a method of grouping prefixes into a more reasonable number of equivalence classes.
one possible approach to group them is by making a markov assumption that the conditional probability of a token is dependent only on the n 1most recent tokens.
the n gram model is such a model that places all prefixes that have the same n 1tokens in the same equivalence class p ti h p ti t1 ... t i p ti ti n ... t i the latter is estimated from the training corpus as the ratio of the times that the token tifollows the prefix sequence ti n ... t i p ti h count ti n ... t i ti count ti n ... t i thus if the fragment for int occurs times and for int i occurs times then p i for int .
.
.
code suggestion the code suggestion task refers to recommending the next token based on the current context.
there has been recent work on code suggestion.
for instance one thread of research concerns on the suggestion of method calls and class names.
nguyen et al.
and zhong et al.
aim to predict api calls and zhang et al.
focus on completing the parameter list of api calls.
eclipse and other development environments like intellij idea makes heavy use of compile time type information to predict which tokens may apply in the current context.
n gram models have been successfully applied to code suggestion .
hindle et al.
exploit a trigram model built from the lexed training corpus.
at every point in the code the model gives a list of possible tokens along with their probabilities that are estimated from the training corpus.
the probabilities can be used to rank the candidate tokens.
then the top krank ordered suggestions are presented to the user.
the advantages of the n gram approach are the possibility to suggest all types of tokens not just method calls permitting the model to incorporate valuable information indicating what most often does apply that is not described by the ide based approaches guess what may apply .
the n gram model is also very simple and unlike more recent work does not require anything beyond tokenizing the code.
on the other hand n gram models will not help much when dealing with tokens and n grams locally specific to a particular context and not present in the code used to train the n gram model.
270table java and python projects data and english corpora.
distinct tokens constitute the vocabulary.
thus anthas a total of tokens composed of distinct tokens.
tokens java project version lines total distinct ant batik cassandra log4j lucene maven2 maven3 xalan j xerces tokens python project version lines total distinct boto bup django cms django gateone play reddit sick beard tornado tokens english corpus version lines total distinct brown gutenberg table percentage of the endemic n grams only found in a single file .
freq.
denotes the frequency of the n grams in the file.
the denominator is the number of n grams in the corpus.
lang.
freq.
gram gram gram gram greaterorequalslant1 .
.
.
.
java greaterorequalslant2 .
.
.
.
greaterorequalslant1 .
.
.
.
python greaterorequalslant2 .
.
.
.
greaterorequalslant1 .
.
.
.
english greaterorequalslant2 .
.
.
.
.
localness of source code is source code localized?
to answer this question we analyzed both natural language and code corpora.
we found evidence supporting three aspects relating to our hypothesis the endemism specificity andrepetitiveness of code regularities in the locality.
data.
we collected the same data set of java projects and english corpora used by hindle et al.
.
the data set contains nine java projects with a total of more than 2m locs.
to investigate whether the commonality holds across different programming languages we also carried out experiments on another very different language python.
table shows the summary statistics on our data.
.
are n grams endemic to localities?
in this experiment we investigated whether some n grams are only found in a local context endemic to a locality .
table shows the percentage of the total n grams in the corpus that were only found in a single file.1for example suppose there are 1m grams extracted from one corpus 200k of which are only found in a single file then the percentage is .
we can see that .
of 31in english corpus each file is an article or a chapter of a book.
1024entropy log2 number of files log scale uniform gram gram gram gram figure entropies of the file distributions for non endemic ngrams grouped by the number of files for java.
uniform denotes that the n grams are distributed uniformly in the files.
grams from the java corpus .
from the python corpus are found only in one file .
the large proportion of n grams only seen in a local context denotes that source code is endemic to file localities.
among the endemic n grams in source code there are over of the total grams that occur more than one time in a single file and as the order i.e.
n of then grams increases a greater proportion repeat in single files.
these endemic but locally repeating n grams represent an opportunity to improve upon language models that do not take locality into account.
in contrast the percentage of the endemic but locally repeating n grams in english is much lower than java and python especially for long n grams e.g.
.
for grams .
this validates our hypothesis that the localness is a special feature of source code which cannot be found in nl.
.
is source code locally specific?
in this experiment we investigated whether some non endemic n grams favor a specific locality locally specific .
for each nonendemic n gram that occurs in multiple files there would be a discrete probability distribution pfor the set of files f. here we use locality entropy hlto measure the skewness of the distribution of locality of an n gram sequence hl summationdisplay f fp f log2p f where p f count n gram inf count n gram inproject generally speaking as f increases hlincreases.
however the more skewed the distribution the lower the entropy and the greater the opportunity for language models to exploit localization .
one extreme case is the endemic n grams that occur in only one file then the entropy will be the lowest viz.
.
another example is thatn grams are distributed uniformly in kfiles then the entropy will be the highest viz.
log2 k .
the lower the entropy the more then gram tends to favor a smaller subset of files in f. we place all non endemic n grams that occur in the same number of files in the same group and report their mean entropy as plotted in figure .
we can see that n grams with varying orders share the same trend the entropies of their file distributions are lower than that of a uniform distribution indicating that the non endemic n grams indeed favor a specific locality.
for example the locality entropy of the grams that occur in files is .
which is .
bits lower than that of uniform distribution viz .
.
the skewed file distribution of non endemic n grams reconfirms our hypothesis that n gram models can benefit from capturing the locality.
.
is source code proximally repetitive?
the degree of proximity of repeated n grams to each other is relevant to the design of locality sensitive language models exactly .
.
10distance number of tokens log10 order n of n gramsjava python figure distance between repetitive n grams.
what range of proximity should language models consider when estimating a local distribution?
we employ n gram distance to measure how far separated the repetitive n grams in the project are.
distance for a specific n gram is defined as the average span in term of the number of tokens between the proximate occurrences of thisn gram.2for example if for int size occurs times in the project and there are tokens between the first and last positions where it occurs we say the distance of for int size is tokens.
in other words we may expect the n gram to appear again on average tokens after the last location where it is previously found.
we plot in figure the variation of n gram distance log10 with different n gram order n. generally the n gram distance goes up with the increase of the order n. as seen the n gram distance of grams for java is .
and for python is .
suggesting that programming languages are locally repetitive.
the highly proximate repetitive property of the n grams suggests that locally sensitive language models have modest memory requirements we do not have to track too much local information.
source code is localized in the sense that code regularities in the locality are endemic specific and proximally repetitive .
.
cache language model .
illustration our primary goal is to address a critical limitation of the standard traditional n gram models their inability to capture local regularities .
we accomplish this by using a locally estimated cache component to capture the endemic andspecificn gram patterns.
capturing the structure of the surrounding locality in an evolving model that changes with the locality is a good approach to capture the local regularities.
first if we memorize the endemic n grams in the locality we can offer the correct suggestion when they occur again which can never be provided by the n gram model estimated from the training data.
this would be a good supplement to the globaln gram models which do not perform well e.g.
withngrams that are endemic and perhaps not seen before.
second a language model that captures short term shifts in ngram frequencies might perform significantly better than the pure n gram models described above.
if we memorize the n grams found in the locality or cache we can update the probability of the n grams based on their frequency in the cache.
let us revisit the last example.
from the n gram model we have p i for int .
andp size for int .
.
suppose on the existing lines in the file for int size occurs times and for int 2if then gram is found in different files we skip the other files that then gram does not occur in when we calculate the distance.i never appears.
then from the cache we have p i for int andp size for int .
.
if we use the average of the probabilities from the two components as the final probabilities we will assign i a probability of .
and size a probability of .
and thus size is chosen.
ourcache language model is based on these intuitions it includes both a standard n gram and an added cache component.
given a source file the cache will contain all the n grams found in the local code.
thus the combined model assigns each token candidate two probabilities the first based on its frequency in the training corpus then gram component and its frequency in the cache the cache component .
linear interpolation of the two probabilities produces an overall probability of each token candidate.
the main idea behind our work is to combine a large global static language model with a small local dynamic model estimated from the proximate local context.
the n gram and cache components capture different regularities the n gram component captures the corpus linguistic structure and offers a good estimate of the mean probability of a specific linguistic event in the corpus around this mean the local probability fluctuates as token patterns change in different localities.
the cache component models these local changes and provides variance around the corpus mean for different local contexts.
the strengths of the additional cache component are reflecting the code locality by capturing .endemic n gram patterns the cache component captures more different n grams by memorizing the endemic n grams in the locality which do not occur within the n gram set captured from the rest of the corpus.
when the endemic n grams repeat the cache component can offer a suggestion.
.specificn gram patterns if a token that followed the same prefix sequence has occurred often in the locality might span several local files it will be assigned a higher probability than when its local frequency is low.
in this way the inclusion of a cache component satisfies our goal to dynamically track the frequency biased patterns of n gram use in the locality.
in section .
we describe how the cache model automatically learns to interpolate between these two models.
then we analyze several local factors that affect the cache model s performance in section .
.
by setting the factors appropriately the cache component will capture the code locality with modest computational resources as shown in table .
.
mathematical treatment the cache model is now introduced mathematically p ti h cache pn gram ti h pcache ti h heretiis the token to be predicted his the prefix tokens that tifollows cache is the list of n grams that are stored in the cache and is the interpolation weight.
the combined model leaves the n gram component pn gram ti h of the language model unchanged.
note that the traditional n grams model is the special case of the cache model when .
then gram based probability pn gram ti h can be regarded as a good estimate of the mean around which the value pcache ti h fluctuates while the cache based probability pcache ti h is the variance around that mean.
this allows the estimate of pcache ti h to deviate from its average value to reflect temporary high or low values.
probability estimation in cache.
the cache based probability is calculated from the frequency of tifollowed the prefix hin the 272cache.
we estimate the cache probability pcache ti h by pcache ti h count h t i incache count hincache here we map the prefix hto them 1most recent tokens.
it is worth noting that the order mis not necessarily the same with the ordernfor then gram component as discussed in section .
.
automatic selection of dynamic interpolation weights.
in simple linear interpolation the weight is just a single number that may be set by hand.
but we can define a more general and powerful model where the weights are a function of the prefix.
we assume that the cache in which more records for the prefix are found is considered more reliable.
for example if more prefix sequences are found in the cache we want to bias ourselves more towards using the cache.
to accomplish that inspired by knight we replace h with h wherehis the number of times that the prefix h has been observed in the cache and is aconcentration parameter between and infinity p ti h cache h pn gram ti h h h pcache ti h we can see that if the prefix occurs few times his small then then gram model probability will be preferred.
but if the prefix occurs many times his large then the cache component will be preferred.
this setting avoids tricky hand tuned parameters and make the interpolation weight self adaptive for different n grams.
.
tuning the cache in this section we will discuss several factors of the locality that would affect the performance of cache language model cache context cache scope cache size and cache order .
cache context.
cache context by definition is the lexical context from which n grams in the cache are extracted .
the cache is initially designed to capture the local regularities from the preceding tokens in the current file the prolog in contrast with the succeeding tokens or epilog and is well suited for the initial development developing new files .
however for software development the maintenance and evolution are critical activities where both prolog and epilog contexts are available.
we believe our cache model will benefit from more context which means that our model would be more useful for software maintenance as shown in section .
.
.
we hypothesized that the performance of the cache component would depend on the prolog more than the epilog.
recall that given ann gram the probability from the cache component along with the weight depends on its frequency in the cache and the total number ofn grams that share the same prefix i.e.h eq.
.
hence we expected different performance for the cache component built on theprolog andepilog since there maybe different distributions of then grams that share the same prefix in the early and later parts of files.
this turned out to be false.
this aspect of results are discussed in section .
.
.
cache scope.
cache scope is the part of codebase used to estimate the cache component.
the intuitive way is to build the cache on the scope of the current file file cache .
however one problem with the file cache is that those initial sentences in a file may not benefit from the file cache.
we use a simple heuristic to alleviate the problem we build the cache on the previous ktokens which could span files in local grouping i.e.
same subdirectory in the codebase .3in other words the underlying assumption is that software is also localized 3the files in the same directory are ordered by the filename.in the codebase in the sense that the files in the same subdirectory are locally similar.
in this way our cache model incorporates a latent scope to capture the code patterns in local files.
cache size.
cache size is defined as the maximum number ofngrams stored in the cache.
intuitively it is more likely to observe a givenn gram if we use a larger cache.
however there has to be a trade off between locality andcoverage when we decide the size of cache.
the smaller the cache is the more local regularities the cache component can capture at the cost of missing more possibly usefuln grams.
in contrast a larger cache would cover more ngrams while neglecting the localness of source code.
an extreme case is to build the cache on the whole project which captures only the global regularities.
moreover maintaining a larger cache is computationally expensive.
as a consequence we need to decide an appropriate cache size to balance the locality and coverage of the cache model.
cache order.
cache order refers to the maximum order mofngrams stored in the cache.
generally the longer the prefix the more accurate the suggestion.
a longer prefix is more specific since it has more detailed information.
for example the long prefix for int i i i is more likely to predict the correct suggestion than the short prefix i .
given that a cache stores much fewer tokens than the n gram component we can employ a higher ordermwithout increasing much complexity.
data sparseness problems arise when we use higher order n grams in the cache.
we employ back off techniques when a long prefix is not matched.
.
experiments in this section we try to answer the following research questions r1.
can the cache model capture the code locality?
r2.
can the strength be leveraged for code suggestion?
r3.
why does the cache model help?
r4.
what factors affect the performance of the cache model?
in section .
we demonstrate using standard cross entropy over large software corpora that the cache model is indeed capturing the localness of source code for that the additional cache component estimated from the code locality decreases nearly one bit over the n gram model.
note that entropy is log scaled intuitively it means the cache model renders code nearly twice as predictable.
in section .
we evaluate our cache language model for code suggestion and show that the cache language model greatly improves suggestion accuracy for different programming languages thus retaining the portability and simplicity of the n gram approach.
furthermore our cache model is especially useful for cross project code suggestion and software maintenance.
in section .
we point out that identifiers benefit most from the cache model and contribute most to the accuracy improvement.
in addition the improvements are due to that the cache component captures the endemic andspecificn gram patterns in the locality.
in section .
we analyze four factors described in section .
that will influence the cache model.
our analyses show that there is no difference between prolog and epilog while combining them achieves a further improvement the localness of software is also reflected at the localized structure of codebase the suggestion accuracy increases rapidly with cache size and order saturating around 2k tokens and respectively.
setting.
we performed the fold cross validation in term of files on each project in table .
we tested the statistical significance 10cross entropy log2 order of n gramsn gram model cache language model figure cross entropies of java below and english above .
we use the same orders for both n gram and cache models.
the trend for python is similar to java.
using sign test .
for the cache model we set concentration parameter the cache size k and the cache order m .4our baseline is the lexical trigram model i.e.
n 3for then gram model which is the same with the n gram component in our cache language model.
.
can the code locality be captured?
we first studied whether the localness of software can be captured by the cache model.
we use cross entropy to measure how good a language model captures the regularities in a specific corpus.
given a corpus s ti...tn of length n with a probability pm s estimated by a language model m. the cross entropy is calculated as hm s nlog2pm s nn summationdisplay 1log2p ti h a good model is expected to predict with high confidence the test data drawn from the same population thus has a low entropy.
figure shows the averaged cross entropies of all projects for both java and english.
the two lines above are the averaged crossentropies for english corpora while the lines below are for java projects.
for english corpora the cache model only outperforms then gram model at unigram while not for higher order n grams.
this is because english is very flexible and n grams n are hardly found in a very local cache 5k words .
for java projects however the cache language model outperforms n gram model consistently showing that the frequency of the n gram in the recent past captured by the cache component is a good indicator to the probability estimation for programming language.
this reconfirms our hypothesis that programming language is quite localized and the localness can be captured by the cache model .
.
cache model for code suggestion we now show that the cache model can be applied to code suggestion although this is by no means the only application .
as mentioned earlier good language models will be useful for other applications code porting coding standards checking correcting syntax errors we emulate code suggestion in two scenarios coding in new files where only prolog is available section .
.
and .
.
and modifying the existing files where both prolog and epilog are available section .
.
.
metric.
we use the mean reciprocal rank mrr measure which is a standard measure of the accuracy of techniques that provide ranked lists of candidate answers.
for each token in the 4we will discuss the influence of the cache size and order in section5.
and in the future work.table accuracy with various settings on lucene .
here file cache denotes the cache built on the previous tokens in the test file while extended cache denotes the cache built on the previous 5k tokens.
although slamc has a slight higher accuracy our model is language independent while theirs is not.
model mrr top1 top5 .n gram .
.
.
.
file cache .
.
.
.
extended cache .
.
.
.n gram .
.
.
.n gram .
.
.
n gram model n a .
.
slamc n a .
.
test data the models produce a list of suggestion candidates ordered by the probability.
the reciprocal rank of a suggestion list is the multiplicative inverse of the rank rankiof the real token ti.5mrr averages the reciprocal ranks for all the tokens tin the test data mrr t t summationdisplay i ranki mrr is better at differentiating between the top few ranks than the top naccuracy measure how often the correct suggestion in the topnsuggestions .
mean reciprocal mrr .
means that one may expect the correct suggestion to appear on average at about the second suggestion and .
indicates correct answer in the top .
.
.
impact of components in this experiment we evaluated the impact of different components on code suggestion accuracy.
table shows accuracy of different configurations on lucene our largest java project.
the first row is the lexical n gram model .
the second and third rows show the accuracies of using only the cache and extended cache components which are built on the file and previous tokens respectively.
the fourth and fifth rows are the combinations of then gram and cache components.
surprisingly using only cache component built on the previous 5k tokens extended cache outperforms the n gram model built on about 2m tokens n gram .
the improvement is mainly due to the increase of top1 accuracy .
indicating that local context is quite specific and accurate.6using only cache component built on the current file produces worse performance confirming our hypothesis that the localness of software also reflects at the localized scope of the codebase section .
.
combining both n gram and cache components achieves the best performance.
the absolute improvements are .
.
and .
for mrr top1 and top5 accuracies respectively.
this suggests that n gram and cache components capture different regularities in the source code.
we also list the accuracies reported in nguyen et al.
.
we do not claim that those results are directly comparable to ours because of potential subtle differences in tokenization cross validation setting suggestion tool implementations etc but we present them here for reference.
even though we have a lower baseline our cache model achieves comparable performance with the semantic language model .
unlike the nguyen et al.
model our cache model is quite simple language independent and requires no extra information besides the tokens so the results are quite encouraging.
5if thetiis not in the suggestion list the reciprocal rank is .
6an increase of .
is still found for top accuracy when we set the same order i.e.
for both n gram and cache components.
274table accuracy mrr of code suggestion.
improv.
denotes the absolute improvement of cache language model cache lm over then gram model n gram .
sig.
denotes the statistical significance tests against n gram using sign test .
java proj.
n gram cache lm improv.
sig.
python proj.
n gram cache lm improv.
sig.
ant .
.
.
.
boto .
.
.
.
batik .
.
.
.
bup .
.
.
.
cassandra .
.
.
.
django cms .
.
.
.
log4j .
.
.
.
django .
.
.
.
lucene .
.
.
.
gateone .
.
.
.
maven2 .
.
.
.
play .
.
.
.
maven3 .
.
.
.
reddit .
.
.
.
xalan .
.
.
.
sick beard .
.
.
.
xerces .
.
.
.
tornado .
.
.
.
average .
.
.
average .
.
.
table accuracy of suggestion for other languages.
lang.
n gram cache lm improv.
c .
.
.
php .
.
.
javascript .
.
.
table speed and memory comparison.
s denotes microsecond 6second .
speed s token memory mb lang.
n gram cache lm n gram cache lm java .
.
.
.
python .
.
.
.
cache model portability.
in this experiment we compared our cache model with the lexical n gram model in two data sets of java and python.
table lists the comparison results.
for java projects the cache model achieves an averaged improvement of .
in mrr from .
to .
over the lexical n gram model.
for python projects the improvement is .
from .
to .
.
all absolute improvements are statistically significant atp .
using sign test .
table shows the result on three other languages c php and javascript.
as seen our cache model significantly p .
outperforms the n gram approach consistently indicating that the improvement of our cache model is language independent .
cache model resource usage.
we measured the computational resources used by the cache model.
see table the memory usage increases slightly and performance is 3x slower than the n gram model.
we contribute the increase to the additional maintenance of cache and the real time calculation of cache probabilities along with their weights.
note that the probabilities from the n gram model are calculated offline and only one inquiry is needed for each token.
in contrast the cache is dynamically updating during the whole suggestion procedure and thus the cache probabilities depend on the dynamic counts of n grams and are calculated online.
even so the code suggestion based on the cache model is fast enough to be imperceptible for interactive use in an ide.
it should be emphasized that the additional memory of the cache only depend on the size of cache not shown while is independent of the training corpus.
case study.
here are some interesting cases to show why our cache model improves the performance.
the variable heap is only found in the file cbzip2outputstream.java inant.
it occurs times following different prefixes of which occur more than time.
the n gram model does not suggest correctly whereas the cache model does when the n grams are found in the cache.
inlucene the identifier search works as various roles e.g.
method variable and occurs times.
the top prefixes thattable cross project code suggestion java .
proj.
n gram cache lm improv.
sig.
ant .
.
.
.
batik .
.
.
.
cassandra .
.
.
.
log4j .
.
.
.
lucene .
.
.
.
maven2 .
.
.
.
maven3 .
.
.
.
xalan .
.
.
.
xerces .
.
.
.
average .
.
.
it follows are lucene .
searcher .
solr .
.
given a specific file frenchstemmer.java it occurs times and works as a variable string search .
the top3 prefixes are i endswith .
the cache model recommends correctly when the n grams occur again while then gram model does not.
.
.
cross project code suggestion we performed another experiment to simulate a new greenfield project setting where training data can only be obtained from other projects.
most data driven approaches including language models are subject to the well known problem of lack of portability to new domains projects.
usually there is a substantial drop in performance when testing on data from a project different from the training data.
our data suggests that the cache model can alleviate the crossproject problem.
table shows the results of cross project code suggestion.
for each project we performed fold cross validation as in the previous experiment.
the difference is that we used the other eight java projects rather than the other nine folds for training.
as seen the lexical n gram model has a accuracy of .
which is .
lower than that of the in project setting reported in table which empirically reconfirms the cross project problem of language models.
comparing the accuracies in tables and we can see that the gap between the cross project and in project settings decreases from .
.
versus .
to .
.
versus .
.
we attribute this to that the cache component built on a fewn grams in a quite local context makes up for the loss of the project specific code regularities to some extent.
this also indicates that programming language is quite localized since project specific code patterns are grouped locally.
.
.
code suggestion in software maintenance in this experiment we emulate code suggestion in software maintenance where we change existing files rather than writing new files.
when predicting a token we build the cache on the rest tokens 275table maintaining code suggestion java .
the cache is built only on the current file rather than the previous 5k tokens.
proj.
n gram cache lm improv.
sig.
ant .
.
.
.
batik .
.
.
.
cassandra .
.
.
.
log4j .
.
.
.
lucene .
.
.
.
maven2 .
.
.
.
maven3 .
.
.
.
xalan .
.
.
.
xerces .
.
.
.
average .
.
.
table descriptions of token abstractions.
abstraction description examples id mcall method calls getmessage call id type types querynode scoremode id v ar variables i size node lit literals string a kw keywords int for while op operators sep separators in the current file.
it is different from previous settings at that we incorporate both prolog andepilog contexts here rather than only exploiting the prolog context in the above experiments.
comparing tables and we found that our cache model is even more useful for software maintenance by achieving significant improvement in suggestion accuracies .
versus .
with less tokens .9k versus 5k .
this result reconfirms our hypothesis that source code is localized at the file level .
.
why does the cache model help?
in this section we investigated the reasons why the cache model works.
we first checked which tokens benefit most from our cache model then investigated whether our cache model is indeed capturing the localized regularities of both endemic n gram patterns and frequency biased patterns of non endemic n grams .
.
.
token abstraction analysis in this experiment we investigated which tokens benefit most from our cache model.
we divided the tokens into classes of token abstractions method calls id mcall type identifiers id type variables id v ar literals lit keywords kw operators op and separators sep .
the first four classes belong to open vocabulary unlimited number of vocabulary while the latter three classes are close vocabulary limited number of vocabulary .
table lists the results for different token abstractions.
as one would expect the identifiers have a low suggestion accuracy ranging from .
to .
compared with the overall accuracy of .
.
a recent study of code cross entropy shows that identifiers contribute most to the uncertainty of the source code.
our results reconfirm these findings.
we have found that the use of local caches drastically improves the suggestion accuracy of identifiers .
to .
.
in contrast the programming specific abstractions i.e.
keywords operators and separators have relatively higher accuracy.
for example the separators which take a large proportion of the total tokens have a suggestion accuracy of .
.
consequently their improvements from the cache model are relatively small .
to .
.
this is because the regularities of these abstractions are more specific to the programming language syntax which can be captured by the lexical n gram models from the training data.table improvements from different patterns captured by the cache component.
endemic and nonendemic denote theendemic n gram patterns and the frequency biased patterns of non endemic n grams respectively.
java python abstraction endemic nonendemic endemic nonendemic id mcall .
.
.
.
id type .
.
.
.
id v ar .
.
.
.
lit .
.
.
.
kw .
.
.
.
op .
.
.
.
sep .
.
.
.
all .
.
.
.
case study.
inlucene the most frequent method length denotes the function of getting the length of a object and therefore it usually follows id v ar .
.
the method obtains improvements of .
.
and .
in mrr top1 and top10 accuracies respectively indicating that most of the improvement is contributed by reranking the candidates.
the keyword kw new occurs times in lucene and achieves an improvement of .
in mrr by using the cache model.
of the prefixes that it follows contain at least one identifier or literal.
for example nearly half of the prefixes are the instances of the pattern of constructing a new object and assigning it to a variable .
this suggests that the keywords also benefit from the locality of identifiers and literals.
.
.2n gram patterns analysis in this experiment we investigated whether our cache model is indeed capturing the localized regularities of endemicn gram patterns and of non endemic but specific n grams .
we use the terms endemic and nonendemic to denote the improvements from the two kinds of patterns above respectively.
we distinguished the accuracy improvement for a given token as follows.
if the correct suggestion only occurs in the cache component we attribute the improvement to the capture of endemicn gram patterns from the cache component otherwise it is from the capture of non endemic but specific n grams since the correct suggestion is found in both n gram and cache components however it is assigned a higher rank because the cache component correctly captures the more local usage patterns.
table lists the improvements endemic and non endemic for different token abstractions on java projects.
several observations can be made.
first the cache component indeed captures both the endemic n grams patterns and the non endemic but locally specific patterns of n grams.
they achieve the absolute accuracy improvements of .
and .
respectively on java and .
and .
on python.
second different abstractions benefit differently from the two kinds of patterns.
for identifiers id and literals lit the coverage of endemic n grams improves the most.
this jives with intuition because a large portion of new identifiers introduced by new functions and files can only be captured by the cache component.
in contrast the language specific tokens kw op and sep benefit similarly from both patterns.
.
what factors affect the cache model?
next we investigate several factors influence on the cache model.
.
.
cache context to investigate the influence of cache context on the cache model we build the cache on different kinds of contexts in the current file prolog epilog and both.
it came as a surprise to us that in general 276table improvements for different abstractions.
in project cross project abstraction percent n gram cache lm improv.
n gram cache lm improv.
java id mcall .
.
.
.
.
.
.
id type .
.
.
.
.
.
.
id v ar .
.
.
.
.
.
.
lit .
.
.
.
.
.
.
kw .
.
.
.
.
.
.
op .
.
.
.
.
.
.
sep .
.
.
.
.
.
.
all .
.
.
.
.
.
python id mcall .
.
.
.
.
.
.
id type .
.
.
.
.
.
.
id v ar .
.
.
.
.
.
.
lit .
.
.
.
.
.
.
kw .
.
.
.
.
.
.
op .
.
.
.
.
.
.
sep .
.
.
.
.
.
.
all .
.
.
.
.
.
table the influence of cache context on the code suggestion accuracy.
we build the cache on the current file.
cache context lang.
model prolog epilog both cache .
.
.
java n gram .
.
.
cache .
.
.
python n gram .
.
.
table the influence of cache scope on the code suggestion accuracy.
we build the cache on the previous 5k tokens.
file order lang.
model ordinal reverse random cache .
.
.
java n gram .
.
.
cache .
.
.
python n gram .
.
.
there is no difference between the prolog and epilog as shown in table .
we had naively assumed that the cache built on prolog would reflect the burstiness of the n grams and hence outperforms that on epilog.
it turned out to be wrong.
the results suggest that code locality holds its trend in the file.
intuitively the cache model benefits from more context reaffirming our finding that the cache model is especially useful for code suggestion.
.
.
cache scope it should be emphasized that the cache component is built on the previous 5ktokens which may across different files.
therefore the performance of our model can be influenced by the order of files.
table lists the suggestion results with different file orders ordinal the default setting in above experiments and reverse denote that all localized files in the same subdirectory are grouped together and are sorted in ascending anddescending order respectively while random denotes that all files are in randomized order and the localized files are not grouped together.
first using only the cache component built on the localized files either from ordinal or reverse order outperforms that on the random files consistently for both languages indicating that the localness of code is also reflected at the localized structure of codebase .
the improvement for python5055606570 10mrr score cache size k tokens k .80k .
.76k .89k .
cache orderm .65m .94m .78m .
figure the influence of cache size and order on the suggestion accuracy java .
the trend for python is similar to java.
is not so obvious as java.
one possible reasons is that in python projects a file consists of multiple related classes.
second working together with n gram component i.e.
n gram we achieve similar performances for different settings.
we conjecture that all settings exploit the previous tokens in the current file which captures most of the localized regularities.
this suggests that our cache model is robust by taking advantages of both n gram and cache components.
.
.
cache size and order the cache size and order have similar trends of the variation of mrr scores with the increase of the value as plotted in figure .
the mrr score increased rapidly with cache size saturating around 2k tokens.
it is worth mentioning that we have already achieved an absolute improvement of .
when the cache size k which confirms our hypothesis that the local knowledge of source code is assembled through a specific contextual piece.
when we employed an unlimited cache we stored in the cache all the previous tokens in the test data the suggestion accuracy is .
which is only .
higher than when k .
this suggests that localized regularities cannot be captured beyond the specific context.
similarly the accuracy went up sharply when the order mincreased from to while was improved slowly when the order continue increased.
one possible reason is that prefixes of tokens are precise enough for the suggestion task and longer n grams e.g.
grams that are locally repetitive are usually extended from these shortern grams e.g.
grams .
threats to validity and limitations.
the most likely threat to the validity of our results is the corpus we used.
although we chose 277many projects with large numbers of locs we cannot say for sure how representative our corpus is in practice.
nevertheless the commonality we have seen across different programming languages gives us confidence that our results hold generally.
the threat to internal validity includes the influence of the settings for the cache model e.g.
and cache scope .
for instance the cache model would be affected by the different orders of the the localized files under the same subdirectory section .
.
.
however the cache model built on the current file regardless of the localized files still achieves over improvement in mrr score which is only .
lower than that built on the localized files .
for code suggestion in software maintenance where both prolog and epilog contexts are available the cache model built on the current file achieves comparable performance with that built on the localized files section .
.
.
.
related work applying nlp to software engineering.
as more repositories of open source software have become publicly available e.g.on github and bitbucket software engineering researchers have turned to empirical methods to study the process of developing and maintaining software .
statistical language modeling has also emerged as an approach to exploit this abundance of data .
gabel and su reported on the non uniqueness of even large code fragments hindle et al.
showed that n gram models could capture the predictable properties of source code and thus support code suggestion.
along the same direction to alleviate the data sparseness problem that n gram models face allamanis and sutton learned language models over more data from different domains while nguyen et al.
exploited a more general unit semantic tokens.
our approach is complementary to theirs it captures different regularities of source code that is the localness of software .
the simplicity of our cache model makes it broadly applicable and easy to incorporate into other approaches just use an interpolation of the cache model estimated probability with that from an existing model.
notably allamanis and sutton showed that the identifiers contribute most to the uncertainty of source code even on a gigatoken corpus.
our approach works especially well for the identifiers.
analysis of identifiers.
identifiers take up the majority of source code tokens hence play an important role in software engineering in both code cognition and understanding .
it has been shown that identifiers are most difficult to predict .
our results confirm this finding the n gram approach only achieved suggestion accuracies of .
to .
on identifiers which is half of other types it is even worse in cross project code suggestion .
with the help of the cache component it greatly improves the suggestion accuracies of identifiers.
there has been a line of research on predicting the identifiers based on the contextual information .
for instance holmes et al.
and kersten and murphy incorporated contextual information to produce better recommendations of relevant api examples.
bruch et al.
and robbes and lanza concerned predicting the most likely method calls from prior knowledge in similar scenarios.
nguyen et al.
used graph algorithms to predict api which is similar to the current code.
our approach is generally complementary the prediction can be improved using local statistics of corpus.
local regularities have been exploited in the past to help splitting source code identifiers .
they focused on splitting compound identifiers based on the observation that terms composingidentifiers usually appear in a local context e.g.
method or file .
cache lm in nlcommunity.
it is worthy emphasizing that our cache language model is different from that is used in nlcommunity.
as noted in section .
natural language is not as locally repetitive as programming languages at the levels of n grams.
therefore the cache component in nlp is usually a unigram model and working with class language models .
a typical class language model first predicts the classes of the words e.g.grammatical part of speech then transforms the classes to the words on the basis of their frequency within the classes.
nguyen et al.
implemented a similar cache lm as in nlp where semantic tokens worked as word classes and the cache for variables stored all the variables that belong to the same or containing scope in the search path for each semantic token.
while their work does not explicitly use a cache language model to capture the general localness of software over a large corpus the differences lie in we directly build the cache model at the granularity of n grams for programming languages we cache then grams of all types of tokens not only variables and we don t require any additional information e.g.type information code path which is difficult and expensive to produce.
our work is based on the observation that programming languages are far more regular than natural language and code fragments of surprisingly large size tend to reoccur .
the main advantage of our cache language model is that the cache component exploits more contextual information and makes more precise suggestions.
therefore the probabilities of the cache component is directly interpolated with those from the n gram component.
our additional caching component achieves better improvement in top1 accuracy than that used in nguyen s work on the same data .
versus .
.
we contribute the improvement to the broader coverage of tokens and more expressive prefixes exploited in the cache component.
.
conclusion the cache language model introduced here captures the localness of software .
it augments n gram models with a cache component to capture the endemic andspecificn gram patterns in the locality.
experimental results show that the cache language model captures the localized regularities in the source code.
consequently the suggestion engine based on the cache model improves the suggestion accuracy over the n gram approach especially in the cross project setting and during software maintenance.
it should be emphasized that our cache language model is quite simple and requires no additional information other than the tokens thus is applicable to all programming languages.
furthermore our approach is complementary to most state of the art works since they capture different regularities of the source code.
besides the simple n gram models our method is also applicable to other paradigms such as the semantic language model .
another interesting direction is to further explore the potential of the cache such as building the cache on more semantic scopes e.g.
import the same header files or packages or files that are changed together with the current files or introducing a decaying factor to account for recency i.e.
n gram distance in the cache.
.