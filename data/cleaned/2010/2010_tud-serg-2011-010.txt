delft university of technology test confessions a study of testing practices for plug in systems greiler ms van deursen a storey ma publication date document version accepted author manuscript published in international conference on software engineering icse citation apa greiler ms. van deursen a. storey ma.
.
test confessions a study of testing practices for plug in systems.
in m. glinz g. murphy m. pezz eds.
international conference on software engineering icse pp.
.
ieee.
important note to cite this publication please use the final published version if applicable .
please check the document version above.
copyright other than for strictly personal use it is not permitted to download forward or distribute the text or part of it without the consent of the author s and or copyright holder s unless the work is under an open content license such as creative commons.
takedown policy please contact us and provide details if you believe this document breaches copyrights.
we will remove access to the work immediately and investigate your claim.
this work is downloaded from delft university of technology.
for technical reasons the number of authors shown on this cover page is limited to a maximum of .
delft university of technology software engineering research group technical report series test confessions a study of testing practices for plug in systems michaela greiler arie van deursen and margaret anne storey report tud serg sergtud serg published produced and distributed by software engineering research group department of software technology faculty of electrical engineering mathematics and computer science delft university of technology mekelweg cd delft the netherlands issn software engineering research group technical reports for more information about the software engineering research group c copyright by the authors of this report.
software engineering research group department of software technology faculty of electrical engineering mathematics and computer science delft university of technology.
all rights reserved.
no part of this series may be reproduced in any form or by any means without prior written permission of the authors.test confessions a study of testing practices for plug in systems michaela greiler arie van deursen delft university of technology m.s.greiler bardblarie.vandeursen tudelft.nlmargaret anne storey university of victoria bc canada mstorey uvic.ca abstract testing plug in based systems is challenging due to complex interactions among many different plug ins and variations in version and configuration.
the objective of this paper is to increase our understanding of what testers and developers think and do when it comes to testing plug inbased systems.
to that end we conduct a qualitative grounded theory study in which we interview senior practitioners about how they test plug in applications based on the eclipse plug in architecture.
the outcome is an overview of the testing practices currently used a set of identified barriers limiting test adoption and an explanation of how limited testing is compensated by self hosting of projects and by involving the community.
these results are supported by a structured survey of more than professionals.
the study reveals that unit testing plays a key role whereas plug in specific integration problems are identified and resolved by the community.
based on our findings we propose a series of recommendations and areas for future research.
keywords eclipse grounded theory plug in architectures open source software development i. i ntroduction plug in architectures permit the composition of a wide variety of tailored products by combining configuring and extending a set of plug ins .
many successful plugin architectures are emerging such as mozilla s add on infrastructure1used in the firefox browser apache s maven build manager 2the wordpress extension mechanism 3and the eclipse4plug in platform.
testing component based systems in general and plug in based products in particular is a daunting task the myriad of plug in combinations versions interactions and configurations gives rise to a combinatorial explosion of possibilities.
yet in practice the systems assembled from plug ins are widely used achieving levels of reliability that permit successful adoption.
so which test techniques are used to ensure plug in based products have adequate quality levels?
how is the combinatorial explosion tackled?
are plug in specific integration testing techniques adopted?
for what reasons are these approaches used?
questions like these calls for an in depth study of test practices in a community of people working on plugin based applications.
in this paper we present such a study revealing what eclipse community practitioners think and do when it comes to testing plug in based systems.
eclipse provides a plug in based architecture that is widely used to create a variety of extensible products.
it offers the rich client platform to build plug in based applications and a series of well known development environments .
eclipse is supported by a global community of thousands of commercial open and closed source software professionals.
besides that the eclipse case is interesting as it benefits from a rich testing culture .
we set up our investigation as an explorative study.
thus instead of starting out with preset hypotheses on how testing is or should be done we aimed to discover how testing is actually performed why testing is performed in a certain way and what test related problems the community is facing.
therefore we used grounded theory to conduct and analyze open interviews lasting hours with senior practitioners and thought leaders from the eclipse community regarding their test practices.
our results show a strong focus on unit testing while the plug in specific testing challenges and practices are tackled in an ad hoc and manual manner.
based on our results we identified barriers which hinder integration testing practices for plug in systems.
furthermore we analyzed how the lack of explicit testing beyond the unit scope is compensated for for example through self hosting of projects and involvement of the community.
we challenged our outcomes through a separate structured survey in which professionals expressed their dis agreement with specific outcomes of our study.
furthermore we used the findings to propose a series of recommendations at the technical as well as the organizational level to improve plug in testing community involvement and the transfer of research results in the area of integration testing.
the paper is structured as follows.
in section ii we sketch the challenges involved in plug in testing.
then in section iii we layout the experimental design and the steps we conducted as part of our study.
in sections iv vii we present the key findings of our study including the test sergtest confessions a study of testing practices for plug in systems tud serg 1practices used the barriers faced and the compensation strategies adopted.
in sections viii ix we reflect on our findings addressing implications as well as limitations of our research.
we conclude with a survey of related work section x and a summary of our key findings section xi .
ii.
p lug insystems capabilities and challenges plug in based systems rely on plug in components to extend a base system .
as argued by marqaurdt a base system can be delivered almost nakedly while most user value is added by plug ins that are developed separately extending the existing applications without the need for change.
in more sophisticated plug in architectures plug ins can build upon each other allowing new products to be assembled in many different ways.
in contrast to static libraries plug ins can be loaded at runtime.
further plugins make use of the inversion of control principle to allow customization of a larger software system.
this means that plug in systems can be complex compositions integrating multiple plug ins from different developers into one product and raising concerns about the compatibility of their components .
incompatibility be it because of combinations of plug ins or versions can be hard to strive against and may restrict the benefits plugin systems offer.
for example many users of the popular wordpress blog software suffer from compatibility issues and according to their own statement the number one reason people give us for not upgrading to the latest version of wordpress is fear that their plugins won t be compatible.
5there are many resources on the internet stating incompatible plug in combinations.6still incompatibility of plug in combinations is an open issue.
these same challenges also occur with eclipse where combinations of plug ins or versions can be incompatible.
for example while resolving a mylyn issue and tackling an integration problem with a specific bugzilla version a user states thanks but i think we have given up on eclipse and bugzilla integration.
9on project pages phrases such as however we can not guarantee compatibility with a particular plug in combination as we do not test with all possible connector combinations 10commonly appear.
such problems exist in many plug in systems which sparked our interest and led us conduct a thorough investigation.
iii.
e xperimental design testing plug in based systems raises a number of challenges related to the interactions between plug ins different 6for example plug ins incompatible with onswipe org support topic plugin onswipe list of incompatible plugins so far 8to mention only a few bugs on bugzilla 9bug identifier of the plug ins and different versions of the plug ins used.
the overall goal of this paper is to increase our understanding of what testers and developers think and do when it comes to testing plug in based systems.
a. the eclipse plug in architecture as the subject of our study we selected the eclipse plugin framework11along with its community of practitioners.
we selected eclipse for a number of reasons.
first eclipse provides a sophisticated plug in mechanism based on osgi12and to that is enhanced with the eclipsespecific extension mechanism.
it is used to build a large variety of different applications 13ranging from widely used collections of development environments to dedicated products built using the rich client platform rcp .
many of these plug in based products are large complex and industrial strength.
second there is a large community of professionals involved in the development of applications based on the eclipse plug in framework.
as an example approximately developers meet at the annual eclipsecon event alone.
third the eclipse community has a positive attitude towards testing as exemplified by the presence of substantial test suites see our analysis of the mylyn and egit test suites and books emphasizing the test driven development of plug ins .
moreover eclipse has explicit support for the testing of plug ins through dedicated plug in development environment pde tests.
finally the eclipse framework as well as the many projects built upon it are open source.
this makes it easy to inspect code or documentation as well as to share findings with other researchers.
since the eclipse platform is also used for closed source commercial development it is possible to compare open and closed source testing practices.
b. research questions our investigation of the testing culture for plug in based systems revolves around four research questions.
the first three we incorporated in the initial interview guidelines.
during our interviews many professionals explained how they compensate for limited testing which helped to refine the interview guidelines and led to the last research question.
rq1 which testing practices are prevalent in the testing of plug in based systems?
do these practices differ from non plug in based systems?
rq2 does the plug in architecture lead to specific test approaches?
how are plug in specific integration challenges such as versioning and configurations tested?
rq3 what are the main challenges experienced when testing plug in based systems?
ofeclipse based software 2test confessions a study of testing practices for plug in systems serg tud serg 010rq4 are there additional compensation strategies used to support the testing of plug ins?
c. research method this section outlines the main steps of our experimental design.
the full details of our setup can be found in the corresponding technical report .
we started with a survey of existing approaches to plugin testing.
we studied over resources about the testing of plug in systems in general and the eclipse plug in architecture in particular.
information was drawn both from developer forums and the scientific literature.
most of the articles found were concerned with technical problems such as the set up of the test environment.
they did not however provide an answer to our research questions.
next we conducted a series of interviews with eclipse experts each taking hours.
interviews were in german or english which we subsequently transcribed.
the questions were based on a guideline which was refined after each interview.
we followed a grounded theory gt approach an explorative research method originating from the social sciences but increasingly popular in software engineering research .
gt is an inductive approach in which interviews are analyzed in order to derive a theory.
it aims at discovering new perspectives and insights rather than confirming existing ones.
as part of gt each interview transcript was analyzed through a process of coding breaking up the interviews into smaller coherent units sentences or paragraphs and adding codes representing key characteristics to these units.
we organized codes into concepts which in turn were grouped into more abstract categories .
to develop codes we applied memoing the process of writing down narratives explaining the ideas of the evolving theory.
when interviewees progressively provided answers similar to earlier ones a state ofsaturation was reached and we adjusted the interview guidelines to elaborate other topics.
the final phase of our study aimed at evaluating our outcomes.
to that end we presented our findings at eclipsecon 14the annual eclipse developer conference.
we presented our findings to a broad audience of approximately practitioners during a minute extended talk where we also actively requested and discussed audience feedback.
furthermore we set up a survey to challenge our theory which was completed by practitioners and eclipsecon participants.
the survey followed the structure of the resulting theory the full questionnaire is available in the technical report .
d. participant selection for the interviews we carefully selected knowledgeable professionals who could provide relevant information on i domains projects and companies involved in the interviews domain project and or company ides eclipse distribution yoxos eclipsesource soa mangrove soa inria gui testing tool guidancer bredex version control systems mercurial inlandsoftware modeling xtext itemis modeling imp university of amsterdam persistence layer cdo domain specific language spoofax tu delft bpm solutions gmf bonitasoft gui testing tool q7 xored coverage analysis eclemma modeling emf itemis bpm solutions rcp product andrenaobjects scientific data acquisition opengda kichacoders runtime platform rap eclipsesource task management system mylyn tasktop embedded software microdoc rcp product eclipsesource testing practices.
we contacted them by participating in eclipse conferences and workshops through blogging and via twitter.
eventually this resulted in participants from different companies each working on a different project identified as p1 p25 in this paper whose detailed characteristics are provided in .
all have substantial experience in developing and or testing eclipse plug ins or rcp products.
participants are developers are project leads is a tester and is a test manager.
the respective projects are summarized in table i. in the survey phase we aimed to reach not only the experts but the full eclipse community.
to that end we set up an online survey and announced it via mailing lists twitter and our eclipsecon presentation.
this resulted in participants filling in the questionnaire.
the majority of the respondents were developers followed by project leads or managers.
only were testers or test managers.
e. presentation of our findings in the subsequent sections we present the results of our study organized in one section per research question.
for each question we provide relevant quotes and codes make general observations and list outcomes of the evaluative survey.
in the technical report we provide additional data supporting our analysis.
in particular we provide the coding system we developed comprising top level categories subordinate concepts and basic codes per concept giving a total of codes.
for each code we give the name as well as a short one sentence description.
furthermore the technical report provides pages of key quotes illustrating the codes.
last but not least we provide the full text of the survey as well as response counts and percentages.
15please note that for reasons of confidentiality not all companies and projects participating at the interviews are listed.
sergtest confessions a study of testing practices for plug in systems tud serg 3iv.
t esting practices our first research question seeks to understand which practices are used for testing plug in based systems and which software components i.e.
test scope these address.
a. open versus closed development setting approximately half of the participant projects are open source with the other half being closed source projects often for a single customer .
the participant companies that develop open source software typically also work on closed source projects.
the purpose of software development is purely commercial for all but two projects.
open source projects count for example on selling functional extensions for the open source product in supplementary products.
most of our participants are paid to develop open source software.
a few develop open source products in their free time but profit personally from the marketing effect e.g.
for their own consultancy company.
in the survey of the respondents indicated that they develop pure open source pure closed source and indicate that they work on both types of projects.
b. test responsibilities the interviews reveal that it is a common practice to have no dedicated test team but that testing is performed by the developers themselves p1 p2 p4 p5 p6 p7 p8 p9 p12 p13 p15 p16 p17 p18 p19 .
p5 explains tester and developer that s one person.
from our view it does not make sense to have a dedicated test team which has no idea about what the software does and can only write some tests.
only a few projects report to have dedicated testers either within the development team or in a separate quality assurance team p3 p10 p11 p14 p21 .
p21 explains automated tests are only developed by developers.
manual testing is done partly regression testing is done by someone from the customer.
both practices are used in open and closed source projects.
respondents to the survey indicate that closed source projects are more likely to have dedicated teams than open source or hybrid projects .
c. unit tests automated unit tests are very popular probably because in the majority of the projects developers are responsible for testing.
the teams of p1 p4 p7 p13 p16 p20 and p22 use unit testing as the only automated form of testing all other forms are manual.
p20 gives the strongest opinion we think that with a high test coverage through unit tests integration tests are not necessary.
and p18 says at our company testing is quite standard.
we have different stages.
we have unit testing and that s where we put the main effort at least of the total expenses.
also p15 reports the majority of the tests are written with junit and the main test suites comprise tests that do not depend on eclipse.
the majority of the participants share p14 s opinion try to get to a level that you write unit tests always whenever you can.
at max.
you use one integration or pde test to probe the code.
ultimately unit tests are our best friends and everything else is already difficult.
participants are aware that unit testing is not always applicable.
for projects that rely solely on unit testing this has visible implications.
as p20 confirms we try to encapsulate the logic as much as possible to be able to test with unit tests.
what cannot be encapsulated is not tested.
d. beyond unit testing there are many other testing practices used such as integration gui and system testing but many participants do not describe them as their focus or key practice.
the second most applied techniques are manual and automated integration testing p3 p5 p6 p8 p10 p11 p12 p14 p15 p17 p18 p19 p21 .
the pde test framework is most commonly used for automating integration testing.
participants indicate that they use integration tests for testing server side logic embedded systems and third party systems connected through the network.
integration tests also include tests indirectly invoking plug ins throughout the ecosystem.
in section v we will see that pde tests are often used in place of unit tests.
successful adoption and active use of automated gui testing is limited to four projects.
many participants see alternative solutions to the expensive p15 automated gui testing approaches by keeping the gui as small as possible and by decoupling the logic behind a gui from the gui code as much as possible p13 p16 p17 p20 p23 .
as p13 puts it we try to make a point of surfacing as little visible stuff in the ui as possible.
in summary the degree of adoption and especially automation decreases drastically for test practices with a broader scope.
the survey aimed at the broader eclipse community enquires about test effort and the level of automation used for unit integration gui and system testing.
the answers suggest a more or less balanced distribution of total effort per test form but a decrease in automation level.
thus as illustrated in figure automation drops from for unit to for integration to for gui and to only for system testing.
of the respondents indicate they rely solely on manual testing at the system scope.
what consequences does this have for integration testing?
do practitioners address plug in specific characteristics during integration?
the findings are described in the following section.
v. p lug inspecific integration testing our next question rq2 relates to the role that the plugin nature plays during testing and to what extent it leads to specific testing practices.
4test confessions a study of testing practices for plug in systems serg tud serg unit testing integration testing gui testing system testing fully automated main effort test automation main effort manual only manual testing i don t knowfigure .
test automation for each test practice a. the role of pde tests pde tests are designed to test plug in based eclipse applications.
they are junit tests using a special test runner that launches another eclipse instance in a separate virtual machine.
this facilitates calls to the eclipse platform api as well as launching plug ins during the test.
furthermore the headless execution mode allows tests to start without user interface components.
participants often use pde tests for unit testing purposes.
according to p1 the problem begins when a junit test grows into a pde test because of the dependencies on the workbench.
and p21 states our pde tests do not really look at the integration of two components.
there are often cases where you actually want to write a unit test but then it s hard to write because the class uses something from the workbench.
others also report that they use integration tests for testing legacy code and p14 reports to use integration tests to refactor a code passage or to fix a bug when you cannot write a unit test.
then at least you write an integration test that roughly covers the case to not destroy something big.
that we use a lot.
we next ask since eclipse is a plug in architecture are there plug in specific aspects to consider for integration testing?
b. plug in characteristics in response to the interview questions regarding the influence plug in architectures have on testing participants come up with a variety of answers.
most of the participants consider plug in testing as different from testing standalone java applications.
only p8 and p10 report to not see any influence and that testing of plug in systems is the same as testing monolithic java applications.
the most often recognized difference is the need to have integration tests p9 p14 p12 p15 p20 .
p14 thinks that integration testing becomes more important in a plug inbased set up because we have to test the integration of our code and the eclipse code and then you test in a way differently you have more test requirements there are more players in the game.
practices differ in the strategies participants use to test plug in systems and the extension mechanism.
p2 says i am not sure if there is a need to test if extensions correctly support the extension point because it is mostly a registration thing.
also p13 does not address the plugin aspect directly but says our test cases make use of extension points so we end up testing if extension point processing is working correctly.
p19 presents the most advanced technique to testing by stating in some cases we have extensions just for testing in the test plug ins.
either the extensions are just loaded or they implement some test behavior.
p19 s team also recommends that developers writing extensions should look at the relevant tests because those tests demonstrate how to use the api.
p12 p16 and p19 report that the extension mechanism makes the system less testable.
p16 says we tried a lot.
we test our functionality by covering the functionality of the extension point in a test case i.e.
testing against an api.
the small glue code where the registry gets the extension that s not tested because it is just hard to test that.
and for these untested glue code parts we had the most bugs.
and p19 says testing is more difficult especially because of the separate classloaders.
that makes it complicated to access the internals.
therefore some methods which should be protected are public to enable testing.
participants associate many different aspects such as improved modularization capabilities for production and test code with plug in architectures and testing.
surprisingly only a few participants mention the extension mechanisms and none of the participants mention osgi services runtime binding or combinatorial problems for plug in interactions.
this finding leads to our follow up questions for specific plug in testing techniques.
c. testing cross product integration to gain a better understanding of the participants integration testing practices we ask how they test the integration of their own plug ins with third party plug ins i.e.
crossproduct integration testing and how they deal with the corresponding combinatorial problem.
to our surprise none of the projects report to have automated tests to ensure product compatibility.
many participants report that products must play nicely with each other 16and that there are no explicit tests for different combinations.
does this mean that cross product integration problems do not occur?
the answers to this question split the participants in two opposing camps.
one group believes that these problems should not happen p4 p5 p8 p12 p13 p14 p17 but more than half of the participants report to have actually experienced such problems p2 p6 p7 p9 p10 p11 p15 p16 p18 p19 p20 p24 p25 .
some even pointed us directly to corresponding bug reports.
17bug identifier and sergtest confessions a study of testing practices for plug in systems tud serg yes we have automated tests for this.yes but in an ad hoc manner.yes we address that very thoroughly.yes this is done by the user community.figure .
cross product integration testing participants report that cross product integration testing is mainly performed manually or in a bug driven way p15 p16 p18 p19 .
p18 explains we handle problems between several plug ins in a bug driven way.
if there is a bug we write a test but we do not think ahead which problems could there be.
and p10 reports there are no specific types of tests for but it is covered by the end user tests and by the gui tests which communicate amongst plug ins but the internal coverage is more random.
in the open source domain participants report that the community reports and tests for problems with plug in combinations p6 p9 p13 p16 p19 p20 .
as p19 says we have no automated tests for cross product problems but we do manual testing.
then we install with or with other distributions like myeclipse to test for interoperability.
and then he adds the user community plays an important role in testing for interoperability.
user involvement emerged as an important strategy for dealing with combinatorial complexity as we will see in section vii.
in the survey of the participants indicate that they do not test the integration of different products at all.
out of the who stated that they test cross product integration claim to address this in an ad hoc manner and only claim to address this issue thoroughly see figure .
thus testing combinations with third party plug ins is not something participants emphasize.
this leads us to ask how are they ensuring compatibility of their plug ins with the many different versions of the eclipse platform?
d. testing platform and dependency versions only a few participants report testing for different versions of the eclipse platform typically the most currently supported version.
for most of the other participants p13 s assessment reflects what is done in practice a lot of people put version ranges in their bundle dependencies and they say we can run with .
up to version .
of the platform.
but i am willing to bet that of the people do not test that their stuff works they might assert it but i do not believe that they test.
however in addition to the platform plug ins have specific versions and stipulate the versions of dependencies they can work with.
how is compatibility for version ranges of plug in dependencies tested?
yes we have automated tests for this.yes but limited to e.g.
two versions.yes we address that very thoroughly.yes this is done by the user community.figure .
testing versions of plug in dependencies.
yes we have automated tests for this.yes but limited to e.g.
two versions.yes we address that very thoroughly.yes this is done by the user community.
figure .
testing eclipse platform versions.
in reality many participants report that they test with one fixed version for each dependency p8 p9 p11 p13 p14 p15 .
the minority of practitioners report that they have two streams of their systems.
one stream for the latest versions of dependencies and the other one for the dependency versions used in the stable release.
other projects report that they even ship the product with all dependencies and disable the update mechanisms.
updating dependencies to newer versions is often reported as a challenge.
many try to keep up to date though some report to update rarely p9 p11 p14 .
as p14 puts it we always have one specific version for platform and libraries that we use.
if we update that that s a major effort.
that we do only rarely.
and p9 says we use a very old version of the main plug in we depend on.
sometimes we update but there is always the risk that it will break something and then you have to do extensive testing.
testing version compatibility as well as combinations of systems is more often applied to third party systems i.e.
outside the eclipse ecosystem .
for example p10 p17 and p19 report to emphasize testing different versions of eclipseexternal third party systems during automated testing but not for eclipse plug ins they rely on or build upon.
also the majority of survey respondents indicate that they do not test version compatibility of either the platform or of plug in dependencies .
out of those testing different dependency versions only have automated tests indicate to limit it to a set number of versions and only test this thoroughly as illustrated in figure .
testing platform versions yields similar results out of the who indicate they test different versions have automated tests limit testing to a set number of versions and only indicate to address this thoroughly see figure .
vi.
b arriers for adopting plug inspecific integration testing practices in the preceding sections we looked at adopted testing practices.
in this section we outline barriers experienced by participants which limit adoption of plug in specific test 6test confessions a study of testing practices for plug in systems serg tud serg 010practices.
the set of barriers reflects what the interviewees considered most important.
to integrate the many different barriers and to identify relevant factors the constant comparison approach of gt proved particularly useful .
plug in systems are conglomerates of several different plug ins with different owners.
hence the responsibility for integration or system testing is less clear especially when system boundaries are crossed.
most projects restrict their official support for compatibility with third party plug ins and the eclipse platform itself.
as p8 puts it we only test the latest available versions of our dependencies those that are together in the release train.
in plug in systems end user requirements are often unclear or even unknown which makes testing a challenge as p7 explains is not an end user plug in.
other plug ins build on top of so integration testing would need to include some other components.
it is not the final the whole thing.
p7 also thinks that integration testing has to be done in strong collaboration with the developers of the end user plug in.
as an example he mentions syntax highlighting functionality only when i know about the language can i test it and see whether it was successful or not.
i need some third party component.
also unclear ownership of plug ins hinders testing as p7 explains you never know once you write a good test it will be obsolete with the next version of eclipse.
while there is a rich body of literature on unit testing literature on integration and system testing for plug inbased systems is scarce.
this unavailability of plug in testing knowledge makes it hard for beginners and less experienced developers and testers to test eclipse based systems.
p4 explains why is so difficult?
for web projects you find good templates.
for eclipse you don t. especially for testing plug ins we would need some best practices.
setting up a test environment for unit testing requires minimal effort as standard tooling e.g.
junit exists.
for integration system and gui testing the situation is different.
participants such as p4 report the difficulty of integration testing eclipse plug ins starts with the set up of the build that s difficult.
also long test execution time is often mentioned as a reason for the negative attitudes towards integration gui and system testing p1 p4 p5 p6 p10 p17 p21 .
p6 says the long execution time is really bad.
a big problem.
and p17 says it s a difference between seconds and minute with minute you switch to twitter or facebook.
as interviewees report the limited testability of eclipse can be challenging.
p6 outlines the problem is that the eclipse platform is very hard to test because components are highly coupled and interfaces are huge and all is based on a singleton state.
this is very hard to decouple.
the pde tooling and test infrastructure can also be a hurdle.
p21 says we use the pde junit framework towrite integration tests although we are not happy with it.
it s not really suited for that.
all of these technical hurdles have the effect that testing beyond unit scope is experienced as annoying p6 distracting p17 and painful p20 .
vii.
c ompensation strategies as we saw in the previous sections participants report that test automation for system and integration testing is modest.
they also mention that integration testing for plugin specific aspects like cross feature integration versioning and configurations of plug ins is often omitted or limited to a manual and ad hoc approach.
does this mean it is not necessary to test those aspects?
addressing this concern is the topic of research question rq4 in which we seek to understand how developers compensate for limited testing.
during the gt study we identified three main compensation strategies namely self hosting of projects user involvement developer involvement and a prerequisite for participation openness .
a. self hosting of projects self hosting refers to the process whereby the software developed in a project is used internally on a regular basis.
as p17 describes in our company we have different setups based on linux or windows.
this leads already to a high coverage because we use our own products on a daily basis.
then you are aware of problems and report that immediately.
in the survey a respondent writes we use self hosting as test technique.
that is we use our software regularly.
this provides a level of integration testing since common features are regularly exercised.
this practice is also applied at the code level which means that participants report to use the api and provided extension points in their own projects.
this principle referred to as eating your own dog food is well documented in the eclipse community and recognized for helping in managing and testing configurations of plug ins including combinations and different versions.
b. user involvement participants also report that they involve users to manually test their systems as p9 explains the tests that i perform are very simple manual tests the real tests are coming from the users who are all kind of different things with .
p9 is not alone with this practice.
participants openly state that they rely heavily on the community for test tasks such as gui testing testing of different eclipse platform versions and system testing and to cope with combinatorial testing and testing of plug in combinations.
as p12 says testing is done by the user community and they are rigorous about it.
we have more than installations per month.
if sergtest confessions a study of testing practices for plug in systems tud serg 7there is a bug it gets reported immediately.
i do not even have a chance to test .
there are too many operating systems there are too many eclipse versions.
c. developer involvement the eclipse plug in architecture enables developers to build plug ins on top of other plug ins.
because of this users of the software are often skilled developers whose projects also depend on and profit from the quality of the projects their work extends.
therefore projects dedicate part of their time to improve dependent projects.
as p11 states yes for the gef part we find and report bugs and we provide patches.
in fact perhaps it is not our own product but our product relies on this other product.
so it is normal to improve the other parts that we need.
projects also profit from the automated test suites of the projects they extend.
p13 explains that is one of the things i totally rely on e.g.
the web tools platform uses project heavily and they have extensive junit tests and so i am quite sure that when i break something that somebody downstream will rapidly notice and report the problem.
in eclipse the release train19is a powerful mechanism.
projects elected to be on the release train profit from the packaging phase in which different bundles of eclipse including specific combinations of products are created.
as p13 explains some testing is performed downstream when packages of multiple plug ins are produced.
some packages have plug ins like mylyn and a whole ton of other projects.
then there are people that test whether the packages behave reasonably.
.
and he reports that if there are problems people definitely report them so you do find out about problems.
d. openness a prerequisite for participation the question that remains is how to involve users and experts.
in this study we could identify one basic but effective principle applied consistently by the participants openness.
openness is implemented in communications release management and product extensibility.
open source projects select communication channels that allow the community to influence software development by giving feedback fostering discussions submitting feature requests and even by providing bug fixes.
in the closed source domain participants report that they open up their communication channels to allow community participation.
p19 reflects on the impact of user input i would say the majority of the bug reports come from the community.
we have accepted more than patches during the life span of this project.
of all bugs that have been resolved have been resolved through community contributions.
that s quite a high rate.
we take the community feedback definitely serious.
release plan giving feedback and foster ...providing bug reports or feature ...providing bug fixesmanual testing including ...automated testing including ... manual testing including gui combinatorial testing automated testing including community downstream project providing bug fixes providing bug reports or feature requests giving feedback and foster figure .
user involvement during testing an important prerequisite to user involvement is access to the software i.e.
open release management .
many open and closed source projects adopted a multi tier release strategy to benefit from the feedback of the alpha and beta testers that use unstable releases and pre releases.
in the survey of respondents report to have an open issue tracking system and report to have a publiclyaccessible software repository.
of respondents use mailing lists or newsgroups to inform users and only report to have a completely closed development process.
respondents also express that users are involved in giving feedback and fostering discussions in providing bug reports and feature requests and even providing bug fixes see figure .
of respondents indicate that users are involved in manual testing including gui testing and combinatorial testing e.g.
different operating systems eclipse versions or plug in combinations .
report that users are even involved in automated testing.
viii.
d iscussion this section discusses how the new insights on the testing of plug in based systems can be used to better support the testing process and outlines opportunities for future work.
a. improving plug in testing since the community turns out to be vital in the testing process a first recommendation is to make this role more explicit.
this can be achieved by organizing dedicated test days in line with mozilla or by rewarding community members who are the most active testers or issue reporters e.g.
at annual events .
additional possible improvements are a centralized place to collect compatibility information and clear instructions on how downstream testers can contribute to the testing process in an ecosystem.
as an example although downstream projects frequently execute upstream plug ins as part of their own testing at present it is hard to tell if these executions are correct.
distributing plug ins with a test modus e.g.
to allow plugins to enable assertions or to offer additional observability 20e.g.
wordpress introduced a crowd sourced compatibility checker plug in for their plug in directory plugin compatibility beta 8test confessions a study of testing practices for plug in systems serg tud serg 010or controllability interfaces would substantially leverage these executions.
the test modus could further report coverage information to a centralized server informing the upstream plug in provider about features combinations and configurations actually tested.
we believe that to leverage plug in specific testing and facilitate test automation plug in specific tool support is needed.
as an example by means of dynamic and static analysis test executions of plug in systems can be visualized in order to provide information to the developer about the degree of integration between several plug ins covered by a specific test suite.
in we propose such a technique and introduce etse the eclipse test suite exploration tool.
in general we see a need for the research community to revisit current test strategies and techniques with respect to plug in specific testing needs in line with memon et al.
for component based systems .
b. open versus closed source our study covers open and closed source development.
in the interviews this did not seem to be a differentiating factor both reporting similar practices and arguments.
in the survey we can combine data on the project nature with specific test practices.
one finding is that closed source projects have less test automation beyond unit scope.
a possible explanation is that closed source projects work more with dedicated test teams which rely on manual testing instead.
this is consistent with the fact that closed projects report more user involvement for manual gui testing for closed versus for open source projects .
another visible difference is that closed source projects adopt plug in specific integration testing approaches to address version or cross product integration less often.
a possible explanation is that closed source projects often aim to create full products rcp applications that are not intended for extension by others.
to discuss these differences in detail calls for additional research which we defer to future work.
ix.
c redibility and limitations assessing the validity of explorative qualitative research is a challenging task .
with that in mind we discuss the credibility and limitations of our research findings.
a. credibility one of the risks of grounded theory is that the resulting findings do not fitwith the data or the participants .
to mitigate this risk and to strengthen the credibility of the study we performed member checking and put the resulting theory to the test during a presentation to approximately developers and during a birds of a feather session at eclipsecon.
further we triangulated our findings in the interviews with an online survey filled in by professionals which helped us to confirm that the main concepts andcodes developed resonate with the majority of the eclipse community.
although there was a possibility of bias we believe we conducted an open minded study which led to findings we did not expect.
we closely followed grounded theory guidelines including careful coding and memoing and revisited both the codes and the analysis iteratively.
we provide rich descriptions to give insights into the research findings supported by a page technical report to increase transparency on the coding process.
threats to external validity i.e.
questioning whether the outcomes are valid beyond the specific eclipse setting are addressed in the following section.
b. beyond eclipse are our findings specific to open source ?
the compensation strategies identified certainly benefit from the open nature of eclipse.
however the strategies themselves are not restricted to open source and can be applied in other settings e.g.
with beta users .
furthermore more than half of the interviewees and the survey respondents are working on closed source projects.
our findings indicate a trade off between test effort and tolerance of the community for failures in the field.
in an open source setting the community may be more tolerant and willing to contribute.
in a closed source setting it may take more organizational effort to build up such a community such as with beta testing programs.
note that for some application domains there is zero tolerance for failure such as with business or safety critical systems.
therefore we do not expect our findings to generalize to such systems.
another concern might be the developer centric focus of eclipse.
for example the developer involvement discussed in section vii assumes the ability to report and possibly resolve issues found in the plug ins used.
note however that other findings such as the test barriers covered in section vi are independent of whether the applications built are intended for developers.
furthermore the eclipse platform is also used to create a large variety of products for non developers.
clearly the plug in based nature of eclipse plays an important role and is the center of our research.
we consider a plug in system as a specific form of a dynamic system with characteristics such as runtime binding versioning and combinability.
for systems sharing such characteristics we expect to find similar results.
further most of the outcomes are independent of the specific plug in architecture adopted.
an investigation to exactly differentiate between various groups of dynamic systems is still an open issue as well as an excellent route for future work.
c. beyond the people a limitation of the current study is that it is based on interviewing and surveying people only.
an alternative could have been to examine code design documents issue tracking system contents and other repositories .
note sergtest confessions a study of testing practices for plug in systems tud serg 9however that achieving our results with repository mining alone would be very hard as many test related activities do not leave traces in the repositories.
furthermore our emphasis is on understanding why certain activities are taking place.
however we see repository mining as an opportunity to further evaluate selected findings of our study which we defer to future work.
x. r elated work a few surveys have been conducted in order to reveal software testing practices .
our study is substantially different.
while these surveys focus on reporting testing practices our study had the additional aim of understanding why certain practices are used or are not used.
in a survey researchers can only address a previously defined hypotheses.
our preceding gt study allowed first to emerge a theory about the testing practices and to let the structure and the content of the survey follow from the theory.
as an implication while other surveys concentrate on preconceived barriers to testing such as costs time and lack of expertise we could address a much wider range of factors of an organizational and technical nature as expressed by the participants themselves.
further the gt findings drove the selection of test practices included in the survey.
this allowed us to concentrate on facts specially relevant for plugin systems reflected in a separate section of the survey and in turn to omit questions such as generation of test cases or defect prevention techniques used in previous studies.
there is substantial research on analyzing different aspects of open source software oss development.
mockus et al.
analyze the apache web server and the mozilla browser in order to quantify aspects of oss development e.g.
reported by raymond .
raja and tretter mine software defects and artifacts to understand several variables used to predict the maintenance model which also leads them to several hypotheses on the effect of users participation.
west et al.
report on the important role of openness for community participation and confirm that a modular software architecture decreases the barrier of getting started and joining an open source project .
krogh et al.
developed an inductive theory on how and why people join an existing open source software community .
whereas those studies address open source our findings apply to open and closed source software development.
furthermore the focus of our study lies on software testing a topic not covered in the earlier research.
whereby research on configuration aware software testing for highly configurable systems i.e.
product lines focuses on the combinatorial problems during interaction testing by detecting valid and invalid combinations of configuration parameters e.g.
by means of a greedy algorithm our work reveals broader testing practices and problems during plugin testing experienced in practice .xi.
c oncluding remarks the main findings of our study are unit testing plays a key role in the eclipse community with unit test suites comprising thousands of test cases.
system integration and acceptance testing on the other hand are adopted and automated less frequently.
the plug in nature has little impact on the testing approach.
the use of extension points plug in interactions plug in versions platform versions and the possibility of plug in interactions rarely lead to specific test approaches.
the main barriers to adopting integration testing practices include unclear accountability and ownership lack of infrastructure for setting up tests easily poor testability of integrated products and long execution time of integration tests.
to compensate for the lack of test suites beyond the unit scope the community at large is involved by means of downstream testing self hosting explicit test requests and open communication.
these findings have the following implications the integration testing approach implicitly assumes community involvement.
this involvement can be strengthened by making it more explicit for example through a reward system or dedicated testing days.
deferring integration testing to deployment calls for an extension of the plug in architecture with test infrastructure facilitating e.g.
a dedicated test modus selftesting upon installation runtime assertion checking and tracing to support upstream debugging.
innovations in integration testing typically coming from research will be ignored unless they address the barriers we identified.
while our findings and recommendations took place in the context of the eclipse platform we expect that many of them will generalize to other plug in architectures.
to facilitate replication of our study in contexts such as the mozilla android or jquery plug in architectures we have provided as much detail as possible on the design and results of our study in the corresponding technical report .
with this study we made a first step to understand the current practices and which barriers exist when testing plug inbased systems.
in addition this study should encourage the research community to facilitate technology andknowledge transfer from academia to industry and vice versa.
acknowledgment we would like to thank all participants of both the interviews and our surveys for their time and commitment.
10test confessions a study of testing practices for plug in systems serg tud serg 010references steve adolph wendy hall and philippe kruchten.
using grounded theory to study the experience of software development.
empirical software engineering pages .
antony bryant and kathy charmaz editors.
the sage handbook of grounded theory .
sage .
isis cabral myra b. cohen and gregg rothermel.
improving the testing and testability of software product lines.
in proceedings of the 14th international conference on software product lines going beyond splc pages berlin heidelberg .
springer verlag.
robert chatley susan eisenbach jeff kramer jeff magee and sebastian uchitel.
predictable dynamic plugin systems.
in7th international conference on fundamental approaches to software engineering fase pages .
springerverlag .
juliet m. corbin and anselm strauss.
grounded theory research procedures canons and evaluative criteria.
qualitative sociology .
erich gamma and kent beck.
contributing to eclipse principles patterns and plugins .
addison wesley longman publishing co. inc. redwood city ca usa .
vahid garousi and tan varma.
a replicated survey of software testing practices in the canadian province of alberta what has changed from to ?
j. syst.
softw.
november .
barney glaser and anselm strauss.
the discovery of grounded theory strategies for qualitative research .
aldine transaction .
nahid golafshani.
understanding reliability and validity in qualitative research.
the qualitative report .
michaela greiler hans gerhard gross and arie van deursen.
understanding plug in test suites from an extensibility perspective.
in proceedings 17th working conference on reverse engineering pages .
ieee .
michaela greiler arie van deursen and margaret anne storey.
what eclipsers think and do about testing a grounded theory.
technical report serg delft university of technology .
to appear.
warren harrison.
eating your own dog food.
ieee softw.
may .
abram hindle michael w. godfrey and richard c. holt.
software process recovery using recovered unified process views.
in proceedings 26th ieee international conference on software maintenance icsm pages .
ieee computer society .
klaus marquardt.
patterns for plug ins.
in proceedings 4th european conference on pattern languages of programs europlop page 37pp bad irsee germany .
atif memon adam porter and alan sussman.
communitybased collaborative testing and analysis.
in proceedings of the fse sdp workshop on future of software engineering research foser pages new york ny usa .
acm.
audris mockus roy t. fielding and james d. herbsleb.
two case studies of open source software development apache and mozilla.
acm trans.
softw.
eng.
methodol.
july .
s. p. ng t. murnane k. reed d. grant and t. y .
chen.
a preliminary survey on software testing practices in australia.
inproceedings of the australian software engineering conference aswec pages washington dc usa .
ieee computer society.
anthony j. onwuegbuzie and nancy l. leech.
validity and qualitative research an oxymoron?
quality quantity may .
klaus pohl and andreas metzger.
software product line testing.
commun.
acm december .
uzma raja and marietta j. tretter.
antecedents of open source software defects a data mining approach to model formulation validation and testing.
inf.
technol.
and management december .
eric s. raymond.
the cathedral and the bazaar musings on linux and open source by an accidental revolutionary .
o reilly associates inc. sebastopol ca usa .
j. rehmand f. jabeen a. bertolino and a. polini.
testing software components for integration a survey of issues and techniques.
software testing verification and reliability .
sherry shavor jim d anjou scott fairbrother dan kehn john kellerman and pat mccarthy.
the java developer s guide to eclipse .
addison wesley longman publishing co. inc. boston ma usa .
markus v oelter.
pluggable component a pattern for interactive system configuration.
in paul dyson and martine devos editors proceedings of the 4th european conference on pattern languages of programms europlop irsee germany july pages .
uvk universitaetsverlag konstanz .
georg von krogh sebastian spaeth and karim r. lakhani.
community joining and specialization in open source software innovation a case study.
research policy .
joel west and o mahony siobh an.
the role of participation architecture in growing sponsored open source communities.
industry innovation .
e. j. weyuker.
testing component based software a cautionary tale.
ieee software .
andy zaidman bart van rompaey arie van deursen and serge demeyer.
studying the co evolution of production and test code in open source and industrial developer test processes through repository mining.
empirical software engineering .
sergtest confessions a study of testing practices for plug in systems tud serg 11a.
experimental design a. motivation and goal plug in based systems rely on plug in components to extend a base system .
as argued by marqaurdt the base system can be delivered almost nakedly while most user value is added by plug ins that are developed separately which can extend the existing applications without need for change.
in more sophisticated plug in architectures plug ins can build upon each other allowing new products to be assembled in many different ways.
testing plug in based systems raises a number of challenges related to the interactions between plug ins different configurations of the plug ins and different versions of the plug ins used.
the overall goal of this paper is to increase our understanding of how systems assembled from such plug ins are to be tested.
a. the eclipse plug in architecture as the subject of our study we selected the eclipse plug in framework1along with community of practitioners.
we selected eclipse for a number of reasons.
firstly eclipse provides a sophisticated plug in mechanism based on osgi 2enhanced with the eclipse specific extension mechanism.
it is used to build a large variety of different applications3ranging from widely used collection of development environments to dedicated products built using the rich client platform rcp .
many of those plug in based products are large scale complex and industry strength.
secondly we choose eclipse because of the large community of eclipse professionals involved in development of applications based on the eclipse plug in framework.
as an example approximately developers meet at the annual eclipsecon event alone.
the third main reason is the positive attitude of the eclipse community towards testing as exemplified by the presence of substantial test suites see e.g.
our analysis of the mylyn and egit test suites and books emphasizing test driven development of plug ins .
moreover eclipse has explicit support for testing plug ins through its plug in development environment pde and the corresponding pde tests.
further the eclipse framework itself and many project building upon it are open source making it easy to inspect projects e.g.
access to bugzilla the code but also to share findings with other researchers.
besides open source development the eclipse platform is used for closed source commercial development making it possible to compare open and closed source testing approaches.
a. research questions our investigation of the testing culture for plug in based systems revolves around four research questions.
the first three we incorporated in the interview guidelines from the beginning on.
during our interviews we observed that many professionals explain how limited testing is compensated which led to the last research questions.
rq1 which testing practices are prevalent in the testing of plug in based systems?
do these practices differ from non plugin based systems?
rq2 does the plug in architecture lead to specific test approaches?
how are plug in specific integration challenges such as versioning and configurations tested?
rq3 what are the main challenges experienced when testing plug in based systems?
rq4 are there additional compensation strategies used to support the testing of plug ins?
a. research method the experimental design covers three phases as shown in figure comprising literature analysis actual interviews and an evaluation phase.
a. .
plug in specific literature analysis the literature analysis aims at identifying existing documentation that can help to answer our research questions.
1test confessions a study of testing practices for plug in systems serg tud serg 010emerging theory analysis plug in based literature pilot round interviews summit evaluation round interviews comparison research based literature evaluation eclipsecon survey interview guidelines start interviews and theory development evaluation figure .
phases of the study we studied over resources about testing of plug in systems in general and the eclipse plug in architecture in particular.
relevant sources include eclipsepedia 4eclipse resource articles 5stackoverflow6and accepted presentations at the two main eclipse conferences between .7to finalize our literature selection we blogged about our findings and asked for feedback and potential missing resources.
most of the articles are concerned with either the set up of the test environment including test execution systems or the use of test frameworks facilitating mocking gui testing or set up of infrastructure including provisioning of the osgi framework .
thus these articles offer little help in answering our research questions which is why we decided to engage in interviewing eclipse experts instead.
a. .
interviews and theory development in order to truly understand which test techniques eclipse developers adopt why they do this and which impediments they experience we adopt an explorative research method grounded theory gt .
this is a systematic inductive and comparative approach to grow a theory iteratively from data increasingly used in the field of software engineering .
in contrast to the hypothetico deductive method where the researcher has predefined hypothesis at the beginning of the investigation grounded theory is explorative aimed at discovering new perspectives and insights.
the process per interview is shown in figure .
it starts with an open interview of hours usually conducted over skype which we recorded and subsequently transcribed.
we preferred to use the mother tongue of the interviewees in cases it was either german or english.
to structure the interviews we compose a guideline which can be adjusted after each interview as our insight in the testing processes increases.
to analyze the transcripts we use coding breaking up the interviews into smaller coherent units sentences or paragraphs and adding codes to these identifiers representing a key characteristic such as mocking performance or singleton state .
codes can be organized hierarchically into concepts such as infrastructure or testing impediments which in turn can be grouped into categories .
the process of data analysis consists of a constant comparison of text units codes concepts and categories.
the resulting concepts and categories are connected via memos narratives explaining for example which factors play a role when a developer indicates his team does not have an explicit integration testing phase.
while analyzing each transcript the elements of the theory codes concepts memos etc.
and their connections are revisited possibly leading to theory refinement.
when we notice that interviewees progressively give answers that are similar to earlier ones a state of saturation is reached we consider the topic sufficiently clear and adjust the interview guidelines to focus on the testing practices that require further elaboration.
as shown in figure we group the interviews into two rounds.
the first round is broad in nature helping to get a first impression of the main testing processes.
a first evaluation of these results was conducted through a poster presentation at the 7eclipsecon usa and eclipse summit europe sergtest confessions a study of testing practices for plug in systems tud serg 13interview guidelines theory interviewing transcribing coding memoing theory refinement saturation analysis question refinement figure .
the interviewing and coding process followed for each interview eclipse summit europe.9the second round of interviews is more specialized focusing on specific testing issues such as the actual problems experienced for integration testing and the involvement of the user community.
a. .
evaluation only after all interviews and the data analysis has been finished we started a thorough literature study on studies of related research fields to compare our findings.
see section x for more details on the related work.
the final step in our study was a large scaled evaluation at the biggest eclipse event the eclipsecon 10with approximately participants.
at this event we firstly presented our findings to a broad audience during a minutes extended talk where we also actively requested and discussed audience feedback.
to engage in an even more in depth discussion with the community we also organized a dedicated discussion session11 called bof in which more than participants discussed with us their testing experiences and the usefulness and completeness of our theory.
in addition we prepared a survey to challenge and possibly refuse our theory and to get a broader perspective on our results which was completed by practitioners.
the structure of the survey followed directly from the resulting theory and addresses the testing practices plug in integration testing adoption factors and compensation strategies .
a. participant selection interview phase.
for the interviews we adopted theoretical sampling to identify suitable participants .
thus we carefully selected knowledgeable eclipse professionals called eclipsers sequentially who could provide relevant information on testing 12the survey is available at 3test confessions a study of testing practices for plug in systems serg tud serg 010domain project and or company ides eclipse distribution yoxos eclipsesource soa mangrove soa inria gui testing tool guidancer bredex version control systems mercurial inlandsoftware modeling xtext itemis modeling imp university of amsterdam persistence layer cdo domain specific language spoofax delft university of technology bpm solutions gmf bonitasoft gui testing tool q7 xored coverage analysis eclemma modeling emf itemis bpm solutions rcp product andrenaobjects scientific data acquisition opengda kichacoders runtime platform rap eclipsesource task management system mylyn tasktop embedded software microdoc rcp product eclipsesource table .
domains projects and or companies involved in the interviews p role cr ts technology kloc p1 developer c eclipse plug in closed p2 project lead o eclipse plug in p3 tester c eclipse plug in rcp product p4 developer o eclipse plug in p5 developer c osgi p6 project lead o eclipse plug in p7 project lead o eclipse plug ins p8 project lead o eclipse plug in p9 project lead o eclipse plug in p10 test c eclipse plug in closed manager rcp product p11 developer o eclipse plug in p12 project lead o eclipse plug in p13 project lead o eclipse plug in p14 developer c rcp product p15 project lead o rcp product p16 developer o eclipse plug in p17 developer c o eclipse plug in p18 project lead c rcp product p19 developer c o eclipse plug in p20 developer o rcp product p21 developer c rcp product p22 developer c eclipse distribution p23 project lead c rcp product closed p24 developer c rcp product p25 project lead c rcp product closed table .
participants involved p participants cr code repository closed or open ts team size sergtest confessions a study of testing practices for plug in systems tud serg 15practices.
we got into contact with them by participating in the eclipse testing day 13through a blog we maintained about the study 14via twitter 15and via a poster presentation at the eclipse summit europe.
eventually this resulted in participants identified as p1 p25 in this paper whose characteristics are provided in table .
they all have substantial experience in developing and or testing eclipse plug ins or rcp products.
a summary of the projects and application domains the participants worked on is given in table .
survey phase.
in the survey phase we aimed at reaching not only the experts but the full eclipse community.
to that end we setup an on line survey and announced via mailing lists twitter and most importantly during a presentation at eclipsecon attended by over people.