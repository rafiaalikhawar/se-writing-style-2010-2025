analysis of user comments an approach for software requirements evolution laura v .
galvis carre no and kristina winbladh department of electrical and computer engineering university of delaware newark de usa flgalvis winbladhg udel.edu abstract user feedback is imperative in improving software quality.
in this paper we explore the rich set of user feedback available for third party mobile applications as a way to extract new changed requirements for next versions.
a potential problem using this data is its volume and the time commitment involved in extracting new changed requirements.
our goal is to alleviate part of the process through automatic topic extraction.
we process user comments to extract the main topics mentioned as well as some sentences representative of those topics.
this information can be useful for requirements engineers to revise the requirements for next releases.
our approach relies on adapting information retrieval techniques including topic modeling and evaluating them on different publicly available data sets.
results show that the automatically extracted topics match the manually extracted ones while also significantly decreasing the manual effort.
index terms user feedback requirements topic modeling user comments software evolution information retrieval.
i. introduction user feedback is imperative in improving software quality.
many software companies collect data on user satisfaction through various means including focus groups surveys and error reports.
software users can also express their opinions on software applications in comments published on sites where an application is available for download.
this type of software distribution is particularly common on platforms that offer a wide range of third party developed applications such as the android marketplace.
the user feedback available on distribution sites thereby supply software companies with a rich source of information that can be used to improve future releases.
in fact user feedback can be and has been the driving force in software evolution of these kinds of applications.
it is not uncommon to see software releases on platforms such as apple s app store state that the newest release addresses many of the issues raised by users.
common challenges in user driven software evolution include a short time to market and obtaining the set of requirements that drive the changes.
in this paper we address these two challenges by offering an automated analysis technique of user feedback.
the technique adapts information retrieval ir techniques to extract common topics and present users opinions about those topics.
the goal of the work is to focus software developers on the most important requirements changes per user feedback at a lower effort level compared to manual processing of the feedback.fig.
provides a high level overview of our approach to automatically extract topics for changing and creating new requirements that better represent user needs.
the technique extracts topics prevalent in user comments and outputs results to be analyzed by the software team.
topic modeling user comments report to be analyzed for changing and creating new requirements input data text analysis requirements elicitation analysis fig.
.
high level overview of approach.
we applied this approach on a set of user comments for three publicly available applications on the android marketplace.
we evaluate the results by comparing the effort of manual and automated extraction of requirements from the user comments computing the accuracy of the results obtained with the automated technique compared to the manual results which are used as a true set and reasoning about the possible requirements changes.
as we are not the developers of the applications we use as input we cannot establish a true set that is undoubtedly true.
to avoid bias we establish the true set manually prior to running our approach.
the results show that the approach is successful in classifying different topics present in the data sets.
when compared to the manually generated data sets we have levels of precision around and recall values ranging from depending on the number of topics we specify in the algorithm.
with regard to requirements changes we see that many of the topics extracted lead to both new and changed requirements.
the remainder of the paper is organized as follows in section ii we introduce related work section iii describes our approach in detail section iv describes the experiments and section v summarizes the results of our evaluation and finally we conclude and outline future research in section vi.
ii.
related work current studies in requirements evolution mainly focus on two aspects requirements evolution during the life cycle of one version of a software and requirements evolution among different versions of a software product.
to support the second aspect researchers have analyzed the relationship between requirements requirements evolution and defects .
the978 .
c ieee icse san francisco ca usa582 outcome shows that there is no strong correlation between the number of requirements and the number of evolved requirements and more interestingly that the number of defects is more strongly correlated with the number of evolved requirements than the total number of requirements.
identifying and documenting requirements changes is thus very important with regard to defect estimation as it can be more accurately determined if based on the evolved and in particular added requirements rather than on the total number of requirements.
the analysis of user satisfaction is helpful in improving products in general and is already an integral part of requirements evolution.
however using user satisfaction models based on time consuming surveys becomes inadequate for software products that need to evolve rapidly not lose market share and user interest.
recent work in re addresses this shortcoming by suggesting to use a mobile feedback system that allows users to provide guided feedback spontaneously as they use a software application .
the technique suggests a way to focus and filter the feedback to make it usable by software developers while maintaining relevance and ease of submission for users.
key to this approach is a classification of topics and sentiments.
the obvious drawback of pre defining entities is mitigated by allowing users to specify new entities.
the results show that users came up with a significant number of entities that the software stakeholders had not considered.
other work includes more automated approaches particularly in data mining to quickly extract user feedback and avoid the shortcomings of surveys.
topic modeling including unsupervised analysis and automatic inference of semantic topics has previously been used to extract opinions from text corpora such as twitter youtube and blogs and the results have been used to suggest other relevant items movies videos people events and groups to the user .
in software engineering researchers have applied topic modeling to various aspects of software projects including traceability to follow the life of requirements source code to help support understanding and find reusable components and documentation to explore the evolving stream of topics over time .
ir techniques have also been used to locate concepts in source code.
the underlying idea of this technique is to treat source code as a text corpus use ir methods to index the corpus and then build a search engine.
the search engine allows developers to write textual queries based on their change request and the engine then retrieves relevant code documents and clusters of code documents .
another software engineering application focuses on comments posted in bug reports to locate and fix bugs here the goal is to measure the textual coherence and relevance of user comments in bug reports which should contain good textual descriptions of the problem and properly selected attributes .
an approach closely related to ours is the work by li et al.
that compares user satisfaction before and after software evolution with the goal of providing instructive information for the next step.
the work suggests that developers should pay attention to three kinds of indicators.
first why does the satisfaction in any indicator e.g.
operability significantly changebetween versions especially negatively?
.
here developers find the reasons for the changes to develop the next version.
second which indicators get high degrees of user concern?
such indicators are more important because they are what users are concerned about.
third which indicators that developers put much effort into present only small changes in user satisfaction?
this provides information about the effectiveness of the developers effort.
our work can complement these questions but focuses more on deriving topics of interest and correlating those with requirements changes.
liu et al.
presents an approach that helps developers judge the quality of a software based on searching and analyzing comments on the internet .
here the sentiment polarity positive or negative is identified and the quality aspects which are described in the comments are extracted.
this work has great applicability in the process of software resource selection and reuse.
the main difference with our work is the interest in a particular feature instead of all the possible features or topics.
an example of a feature of interest of their work is the quality of the resources and to study it they evaluate for example the organization flexibility and stability.
another interesting body of work includes automated sentiment analysis which has been applied in different domains mainly outside software engineering.
brody used sentiment analysis to determine the general attitude of restaurant patrons posting reviews online and pal to perceive customer sentiments toward products.
however using only sentiment analysis or topic modeling alone does not provide the association between topics and attitudes which is crucial for informing software developer about requirements changes.
to address the joint problem of discovering the topics in reviews and the sentiments toward those topics recent work has applied a combination of topic modeling and sentiment analysis to reviews of electronic devices and restaurants as well as movies .
these efforts extend latent dirichlet allocation lda one of the most popular topic models based on the assumption that documents are a mix of topics where a topic is a probability distribution of words.
our work adapts the model proposed by aspect and sentiment unification model asum which incorporates both topic modeling and sentiment analysis and we use the model on user reviews of software applications.
we chose this technique based on the possibility of associating topics with sentiments.
although in this work we describe just the first step towards this goal i.e.
topic extraction as an initial assessment of the validity of using topic modeling to get information relevant to requirements evolution.
we adapt the asum approach as described in the subsequent section to better fit in the domain of user feedback on software applications and to extract topics that are relevant to requirements engineers.
iii.
approach we use information extraction ie techniques including topic modeling to automatically get constructive feedback from user comments.
the goal is to find information that583can be used to change and or create new requirements for a future release of a software.
we take user comments under the premise that users would assess a software application through their opinions in user comments.
an underlying assumption of our approach is that the importance of a topic is proportional to the number of comments it receives.
the set of comments is processed to extract different topics that users have identified as worth commenting on.
these topics will be different depending on the software application.
finally a set of topics with a set of representative comments is produced.
this set should be used by software developers to identify new and changing requirements in a similar way that they would use survey data and other user feedback data.
fig.
presents an overview of this approach as three main steps.
step user comment analysis step text analysis step input data collect comments tokenize change to lowercase remove noise data preparation hidden structure of the comments user feedback report topic identification sentiment classification asum fig.
.
user comments analysis approach a. step input data the first step includes the acquisition and preparation of data.
in our experiments see section iv we use data from several mobile applications from the android marketplace.
the general approach is the same if data was obtained elsewhere.
the data then undergoes several pre processing steps.
tokenize each comment consists of a number of sentences or phrases.
tokenization converts a stream of characters into a sequence of tokens.
a token is generally a word but could also be a paragraph a sentence a syllable or a phoneme.
we separate the words by removing white spaces punctuations brackets and other common sentence delimiters.
change to lowercase we change each word into lowercase in order to compare them more easily.
our approach does not treat nouns and proper nouns differently and therefore the change does not affect the data.
remove noise the textual input contains noise e.g.
there are many words in the comments that are not helpful with regard to topic and sentiment extraction.
we first apply afilter to remove non words and non numerical characters.
we then remove words that match very common english words so called stop words .
the stop words typically include numbers individual characters articles and conjunctions.
we use the list provided and used in because it includes individual characters and standard words with the same root.
prior to removing the stop words we analyze sentences for negation.
for example to classify not good as a negative sentiment it is necessary to consider the two words collectively.
we therefore put the not prefix to a word when the word is located closely one or two words behind a not as in .
finally we also remove comments that have become empty as a result of the noise reduction.
the original textual data is thereby converted into data with no punctuation and stop words.
in addition words and sentences with less than three characters or words are removed.
these filters reduce the input data significantly to only contain the most relevant words to be classified.
b. step text analysis the pre processing steps conducted on the input data result in a vector p fs1 s2 s mgwheremis the number of sentences after noise reduction.
we then analyze the sentences to find topics associated with each sentence.
here we create two special data structures commonly used in ir.
we can then apply the aspect and sentiment unification model asum approach on these structures to receive the classification.
data preparation frompwe create two data structures a word list wcontaining all the unique words of the collection and a bag of sentences bwhere each sentence is composed of indices referencing the words in the word list.
hidden structure of the comments we use topic modeling with the hypothesis that the user comments in the collection share the same set of topics and each comment exhibits those topics with different proportion.
the original asum approach uses sentiment analysis to obtain knowledge about user opinions .
the goal is to use the implicit knowledge to extract an approximation of the hidden structure of the comments and sentiments .
the asum approach is applied to the entire collection of pre processed data.
asum is an extension of latent dirichelt allocation lda and constrains the words in a sentence to come from the same language model and includes sentiment classification.
the graphical representation or plate notation of asum is shown in fig.
.
the nodes are random variables the directed edges between variables are dependencies and plates are replications.
here only shaded nodes are observable the other variables are latent.
the variables and are the words topics and sentiment distributions.
the variables and are the parameters of the dirichlet prior for and .
the sentiment topic and word are represented by s z and w. the representation of the number of sentiments sentences topics and words are s m tandw.
the words are the only observable variables and the dirichlet prior parameters are assigned at the start of the process.
the latent variables are inferred by gibbs sampling.
the process is as follows for every pair of sentiment sand aspectz draw a word distribution sz dirichlet s for each comment d a draw the comment s sentiment distribution d dirichlet b for each sentiment s draw an aspect distribution ds dirichlet c for each sentence i choose a sentiment j multinomial d ii given a sentiment j choose an aspect k multinomial dj iii generate words w multinomial jk fig.
.
graphical representation of asum.
here the approximate probability of sentiment jin comment dis dj the approximate probability of aspect kfor sentiment jin comment dis djkand the approximate probability of wordwin senti aspectfk jgis jkwis dj cds dj jps j0 1cds dj0 j0 djk cdst djk jkpt k0 1cdst djk0 jk0 jkw cstw jkw jwpv w0 1cstw jkw0 jw0 asum incorporates the use of seed words into the generative process.
we use two sets of seed words one per sentiment containing positive and negative words that are not topicspecific e.g.
the sets of words can not be used to label a topic.
the seed words were manually selected by the authors using the public list .
these words were chosen for their lack of sensitivity to context.
additionally we include the words with the not prefix considered in the pre processing step to present a semantic orientation.
as further information we have the number of stars per comment as ratings to include.
the use of the rating information and the seed words are used to initialize the generative process.
c. step user comment report the goal of the last step is to provide meaningful feedback to developers.
we therefore generate a report using jasperreports and ireport1.
the report presents the classified information organized by topics and sentiments including or possible labels for each topic sentiment.
the labels are the words with the higher probability to belong to the topicsentiment group.
the seed words of each sentiment and a 1popular open source reporting engine and designer reporting software.
more information on of seed words with neutral meaning such as recently and seen are not used as possible labels.
these words are rejected as they are not related to the application domain and thereby less useful for developers when revising the srs.
we evaluate the usefulness of the report in a human subject study as described below.
the study helped answer important questions such as if the generated report saves developers time if it is possible to extract the same information in less time and with less effort and whether it is better to use the report compared to the original list of comments.
iv.
evaluation the evaluation consists of two parts.
first we evaluate the information retrieval technique using standard measurements and second we evaluate the quality of the generated report as a basis for new and changed requirements.
a. evaluation of approach to evaluate the approach we apply it on three different real world data sets of user comments for publicly available software applications and we compare the results with two other ways of classifying topics manual classification and similarity using the jaccard coefficient .
we choose to compare different classification schemes to experimentally analyze the strengths and weaknesses of our approach.
experiment setup the experiment setup has two important steps the preparation of the data and the classification of the data using different techniques for comparison.
data sets and preprocessing we extract user comments from the android market for the following applications calorie tracker mint.com personal finance and facebook for android2.
the selection of applications was based on the number of comments and the spread between positive and negative ratings.
also we chose two free applications and one paid application.
the thought behind this selection is that paid applications generate more negative comments than free applications.
we manually classified topics for both calorie tracker and mint.com as the number of comments in both cases was manageable.
we did not manually classify topics for the facebook app as it had many more comments.
we want to include facebook in our analysis as an example of using the approach on a large data set.
table i shows the datasets used to evaluate our approach including the number of comments sentences and words.
the calorie tracker application is classified within the health and fitness category on the android marketplace and it is a paid application.
after preprocessing the data set contains comments sentences and a wordlist of elements as shown in table i. the mint.com personal finance from the finance category and facebook for android from the social category are both free applications.
2more information about the applications on i data sets of user comments .
application comments sentences word list calorie tracker mint.com personal finance facebook for android classification we classify the data sets using three different techniques manual classification not facebook kmedian classification with the jaccard coefficient of similarity and asum.
the different classification techniques allow us to compare the results of different approaches to analyze the usefulness of each.
note that regardless of classification technique not all topics result in requirements changes.
some common topics could indicate bugs and some topics with very few comments might not be important enough to cause changes.
we exclude topics that only have one comment from our classifications.
a manual classification the manual classification of the comments for calorie tracker and mint.com was conducted by the second author using the following process i an user comment is analyzed and potential topics recorded on a separate sheet.
ii as a comment can contain multiple sentences and topics all topics present in a comment are recorded and each recorded topic is marked with the unique identifier of the comment from which it was derived.
iii as user comments are in natural language and users do not use the same exact wording to express common ideas the recorded topics are consolidated and separated based on semantics determined by the second author.
it took .
hrs to classify the topics of the comments of calorie tracker.
overall the process was tedious and errorprone.
in a second iteration of the classification procedure the second author reported changes to the initial classification indicating the difficulty of getting it right the first time around.
as we intend to use the manually classified data as truth sets we would like to point out that they do not describe absolute truth as the second author is not a domain expert of either application not involved in the development of either application and reported that the process is error prone.
in fact even within an organization no two people would come up with the exact same classification of topics.
this is an internal threat of validity.
to explore this potential weakness of the true set and minimize this threat we conducted a small experiment to measure the difference in how people classify topics manually.
we use this difference as a comparative measure to our automated technique.
that is if our automated technique is within the same range of recall precision and f measure relative to the selected true set the technique performs at least as well as manual classification assuming that the manual classification is one acceptable solution but with less effort.
recall is computed by the number of true positives divided by the sum of true positives and false negatives and precision is the number of true positives divided by the sum of truepositives and false positives.
the f measure is computed by the precision and recall evenly weighted using the general form f measure precision recall precision recall in this experiment a true positive is a topic that matches with the true set a false negative is a topic that is in the true set but not in the comparison set and a false positive is a topic that is present in the comparison set but not in the true set.
three graduate students were asked to classify a subset of the calorie tracker comments.
the precision and recall compared to the true set or classification are reported in table ii.
it is worth noting that precision is held fairly steady whereas the recall measure shows great variability.
table ii precision recall and f measure of manual classifications .
classification precision recall f measure classification .
classification .
classification .
b k median clustering classification we also want to compare asum to a simpler automated technique.
for this we chose k median clustering with the jaccard similarity coefficient between pairwise sentences.
the k median clustering with the jaccard similarity was chosen because it is a common clustering technique to classify similar objects.
the k median classification is a variation of k means clustering with the aim of trying to find the natural kcluster centers in a group of ntopics by minimizing the distance between each topic to the center of the cluster to which it belongs.
the difference is computing the median when the mean is not possible.
with respect to the distances we use the jaccard similarity coefficient defined as the ratio between the size of the intersection and the size of the union of the sets see equation .
here we compare the similarity of the 1gram and grams of each sentence.
that is we create a set for each sentence that contains the single words of the sentence gram and every consecutive two words grams .
j x y jx yj jx yj k median clustering requires us to specify the number of clusters a priori where kcorresponds to the number of expected topics in a data set.
our choices are somewhat subjective as there is no standard way to determine optimal values.
for our experiments we chose different values of k for each application.
for calorie tracker and mint.com the first value of kwas chosen to match the number of manually extracted topics the second value was chosen to double the number of manually extracted topics to account for the human factor of merging more topics based on semantics and the third value of kwas chosen to be much larger to see the effect of the algorithm on a large number of clusters.
for facebook where we did not have a true set of topics we chose two arbitrarykvalues to compare.586to minimize the human effort in inspecting topics we chose the sentence representing the center of each cluster to classify the topic.
the label of the cluster is thus the center sentence.
c asum classification we run the adapted asum algorithm as described in section iii.
asum provides us the approximate probabilities for each word within the extracted topics we use the probabilities to assign a label per topic to reduce the amount of manual work required.
to assign a label to each topic we use the word with the highest probability.
in this we found that there are some topics that include a variety of words with the same probability.
these groupings indicate a mix of actual topics and are excluded as misclassifications.
similarly to the other approaches we also exclude topics that contain only one comment because these could be seen as comments in the original raw presentation.
as the wording of the generated labels in the three approaches will differ we compare them on a semantic basis.
for example the manual classification could contain a topic named sync problem the k median could produce a topic labeled the website does not sync and asum could generate a topic labeled sync .
in our comparison we consider these three topics the same as they semantically mean the same thing.
in asum we also need to specify the number of topics prior to running the algorithm.
for fair comparison we use the same number of topics as in the k median experiments.
experiments our evaluation compares the results of the three classification approaches on three different data sets of user comments from different mobile applications.
for each experiment we evaluate the topics discovered.
there are three criteria for measuring the quality of the topics the topics should be coherent logical and consistent the topics should be specific enough to capture the details in the comments and the topics should be those that are discussed the most in the reviews.
we make the assumption that the manually extracted classification true set adheres to these criteria as was checked with two careful iterations and therefore compare the automatically derived topics to the true set.
another evaluation criterion which is domain specific to re is that the derived topics should result in requirements changes.
this aspect will be analyzed by inspection of the generated results.
b. evaluation of generated report to evaluate the quality of the generated report we organized a human subject study with the goal of determining the usefulness of the information generated through our approach.
in the study participants were divided into two groups a control group and a test group.
the control group was given user comments without any pre processing or classification and the test group was given the auto generated report.
in addition all participants were given a software requirements specification srs for the calorie tracker application.
as there are no srss distributed with the software on the android marketplace the authors created the srs based on their understanding of the application.
the srs contains a general description of the software application functional shallstatements non functional shall statements a data model and use cases.
the srs was created without bias that is without knowing the results of our approach.
furthermore the srs is not a utility in the study and remains the same for each group.
all the participants had hr to revise the srs.
the participants were asked to speak out loud and were interviewed after the experiments so that their thought process could be accurately captured and analyzed.
v. experiment results a. comparative analysis table iii shows the precision recall and f measure for the k median and asum classifications in comparison to the true set for the calorie tracker application.
for this experiment we usedk .
we can see that the level of precision when using the k median approach stays stable over the different values of k. that means that the approach is not finding many topics that were not also found by the manual classification.
the level of recall on the other hand increases significantly between k andk whereas it does not change much between k andk .
that means that the number of true positives increases a lot betweenk andk but only by a little as kgets bigger.
the asum approach shows increasing precision with an increasing number of topics.
as with k median asum shows a similar trend with regard to the recall and f measure.
we can compare these numbers to the precision recall and fmeasure numbers of different manual classifications presented in table ii.
the f measure shows how the performance in general increases and gives a general performance that reflects the levels of precision and recall for both classifications.
stable levels are achieved for bigger values of k. the manual results also show fairly stable precision levels ranging from .
whereas the recall levels between different manual classifications range between .
table iii calorie tracker .
performance of automated techniques compared to true set .
classification precision recall f measure k median classification k .
.
.
k .
.
.
k .
asum classification k .
k .
.
k .
.
table iv shows the precision recall and f measure of the k median and asum classifications compared to the true set for the mint.com comments.
for this experiment we used k .
as in the previous experiment the results show similar trends with regard to precision and recall when using the k median approach.
again the asum approach shows increasing numbers with regard to precision recall and fmeasure with an increasing number of topics.587table iv mint.com .
performance of automated techniques compared to true set .
classification precision recall f measure k median classification k .
.
k .
.
.
k .
.
.
asum classification k .
.
.
k .
.
.
k as there is no true set for the facebook data we discuss the results qualitatively.
we ran k median and asum with k andk .
figures and show word clouds of the results for k .
each sentence in the word clouds represents a topic .
in the word cloud for the k median approach the sentences are the centers of each cluster.
the visual size of the sentence is determined by the number of comments that belong to the cluster.
in a way that could represent the significance of the topic if we assume that the more a topic is mentioned by users the more important it is.
in the word cloud for the asum approach on the other hand the sentences are selected by looking at the probabilities of different sentences in combination with hand picking a sentence among the ones with probabilities between .
.
.
in both cases during this process we discovered groups that contain mixed topics as well as groups of duplicated topics.
the duplicates are visible in the word clouds.
for example the word cloud for k median shows a topic called keeps force close and another topic called force closes a lot .
the mixed topics cannot be discovered in this representation.
even with mixed topics and duplicates the word clouds do provide useful information.
it is not difficult to pick out five to ten topics in each that would inspire requirements changes.
comparing the two word clouds we can see that the distribution of comments per topic is more even for asum than for k median.
with a low kvalue relative to the number of comments asum does not produce very good results.
that is many of the groups contain mixed topics.
b. reasoning about requirements to provide an example of the kinds of topics present in the user comments table v shows ten topics per application.
for calorie tracker and mint.com the topics were extracted manually whereas for facebook the topics were generated using asum.
we can see that the topics are relevant in the sense that they cover important features of these applications.
however we still need the sentiment or a representative comment to understand what users think of each topic.
we can provide a representative comment for each topic to provide a requirements engineer with some context on the topic.
tables vi vii and viii show three topics for each of the three applications and provide sample of sentences for each of the topics.
a requirements engineer can look at a short list of topics with a short list of representative user comments and decide whether or not the topic is atable v sample topics per application .
calorie tracker mint.com facebook barcode scanner alerts updates calorie counter international use features database online version developers not enough features platform messages creating an account update force closes sign in log in usability photos bad interface transaction categories friends wrong visualization of food duplicates chat difficult to use budget notifications break down of nutrition cash transactions sony ericson candidate for a requirements change.
for example the topic features extracted from the facebook comments indicates that the requirements should include more of the features available on other platforms.
whereas the topic updates for the same application indicates a potential bug fix.
a requirements engineer could easily sort through this short list more efficiently than when having to consider the whole set of comments.
in addition to useful sentences under each topic we also found many nonsense sentences.
this could happen when a sentence is supposed to be considered together with previous ones or when a user writes poor sentences.
table vi calorie tracker application sentences topic barcode scanner works fine but if it had a barcode scanner it would be much better.
barcode scanner still not working.
topic database it doesn t have a large food database and there isn t a way to manually input.
very easy to use huge food database.
topic sign in log in took seven attempts to create account.
wont let me create an account.
table vii mint application sentences topic usability been having problems with it since installed which kind of makes it useless.
this is useless if your bank is not listed.
topic transaction categories however i would like to have a way to look at the most recent transaction.
only allows one category per transaction.
topic duplicates duplicate accounts are still an issue.
still no fix for duplicate entries.
c. generated report analysis the human study was conducted with participants in two groups a control and a test group.
threats to external validity include to what degree the participants of the study are representative of developer teams.
the participants are undergraduate and graduate students of the computer science and electrical computer engineering departments at the university of delaware and are familiar with application development and software requirements.
each group had seven participants revise the srs of the calorie tracker application588fig.
.
graphical representation of k median on facebook comments.
fig.
.
graphical representation of asum on facebook comments.
table viii facebook application sentences topic updates glitches constantly refreshes and goes back to top of page.
the last update messed up the newsfeed.
topic features the features on this app should be like the desktop its too many things you cant do.
still missing some features that the iphone has but pretty good overall.
topic developers i hope the devs can look into this and see about adding a rotate features.
you need to allow us to log out.
using either the raw user feedback control group or the generated report test group .
the selection of participants for each group was random and the graduate and undergraduate students were evenly split to avoid bias.
the background of all the participants is almost the same and this information was obtained with previous background questions.
to ensure that the results are applicable to development teams additional to the experience with software requirements some participants are students whose major is software engineering.
the software engineering majors were also split between the test and control groups.
a separate comparison showed that the behavior and results were similar compared with those obtained by the other participants.
threats to internal validity include the selection of conditions to generate the best results.
to minimize this threat the conditions for the experiment were the same for all the participants a quiet place environment the same material time instructions and procedure.
we recorded the number of comments analyzed and also the time spent on the task.
table ix presents the average of the results per group as well as some statistics including the average number of re table ix statistics from the study .
control group test group comments analyzed total time min information recorded new requirements req.
changes alterations reprioritization and deletions total changes quirements that were added altered deleted and reprioritized.
in the ideal case every srs revision should be evaluated for correctness and consistency.
in this study we assume every revision to be correct and counted toward the average.
the numbers were always higher for the test group than for the control group.
an obvious reason is that the test group had less comments to read through with them organized through the automatic classification.
in the interview the participants of the test group said that the classified presentation of the comments makes the process fast.
figure shows some of the topics in the srs and the average revisions made per topic for each of the groups.
the number of changes made by the test group using the generated report is superior for topics like views platform usability and accounts and is comparable to the control group in other topics like food log database reliability and security.
the comparison shows an advantage of using our approach to generate a report of topics to revise the srs.
it is also important to mention here that the results of the test group were obtained analyzing all the comments in the generated report instead of the on average comments analyzed by the control group.
to study if these results are in fact significant we perform the t test to evaluate whether or not the mean of revised material589fig.
.
comparison of changes in topics for each group of the study.
in the srs and the number of comments analyzed by the test group are significantly higher than for the control group.
the null hypothesis of this study is stated as h0 test control i.e the mean of the test group is lower than or equal to the mean of the control group.
the value of pis used to determine its likelihood.
if deemed unlikely then the alternative hypothesis ha test control is accepted i.e.
the mean of the test group is higher than the mean of the control group and that it is unlikely due to chance.
the null hypothesis is rejected if the value of pis lower than that determined by a level of significance.
the level of significance used in this analysis is .
for the case of the number of comments analyzed the pvalue is and for the number of revisions the p value is .
as both values are lower than the level of significance the null hypothesis is rejected and consequently the alternative hypothesis accepted.
this permits us to conclude the utility of the report generated by our approach.
we were also interested in studying the process participants use when analyzing the user comments to make decisions about requirements revisions.
as we did not provide guidelines in how to use the information provided each participant chose their own strategy for the task.
we asked the participants at the end of the revision about the strategies used and how useful they were to complete the process.
the most common strategy in the control group was to go through each comment and to select those with useful information.
the strategies used by the test group varied more.
some used the same strategy as the control group and others chose to read by topic sentiment cluster and then make the revisions.
some of the participants were comfortable using the labels of the groups of comments to assign or try to match the comments with requirements categories others did not use this additional information.
finally we asked the participants the following questions how easy was it to use the feedback to change the refig.
.
quality indicators.
quirements?
and how useful do you think this feedback is to software development?
.
figure summarizes the answers to these questions.
the effort was perceived higher for the control group highlighting the advantages of the automated classification of user comments.
on the other hand the utility of the information is higher for both groups than we expected which shows the importance of user feedback for developers and confirms the need for better techniques to accomplish this.
d. discussion of results the results by both k median jaccard and asum look promising.
compared to manual classification the results show that a good representation of topics can be extracted in a small amount of time compared to the manual process.
there are however some limitations of the k median approach that have not yet been discussed.
because we only consider the center sentence of each cluster to represent the group without exploring whether or not the rest of the sentences in the cluster also describe the same topic we do not catch mis classifications.
in asum we use the probabilities of the words to classify the group mis classifications are therefore more apparent as several words can have the same probability i.e.
there is no clear label for the topic.
the asum approach thereby imposes a stricter classification on whether or not to count a topic as a true positive.
the results of k median are thus over optimistic in comparison to asum.
another problem with the k median approach is that not only could there be mixed topics but there are many duplicated topics which is reflected in the recall levels.
even with larger numbers of k rather than finding the missing topics more duplicate clusters are formed.
a limitation of asum is that it requires a fairly large number of topics to avoid mis classification.
this is clearly shown in the word cloud of the facebook comments as the even distribution among the comments in the topics is visible in the font size.
a larger number of topics is obviously less ideal for the analyst that has to sift through the generated topics.
when using a larger kvalue asum performs better and the number of comments per topic is less evenly distributed.
determining the ideal value of topics or k using either of the automated approaches is challenging.
however our experiments showed an interesting trend which led to the hypothesis that the number of topics present in a collection of user comments is not proportional to the number of comments.
it seems that the number of actual topics levels off.590furthermore a company might be interested in only the top or topics for the next release and the kvalue could thus be chosen accordingly keeping in mind that the kneeds to be bigger at least twice the number of topics than the intended number of topics due to duplication .
choosing a largekfor the asum approach could yield good results as noticed in the facebook experiment.
however going through a large number of topics such as is still a tedious task for a requirements engineer although the number is reduced from the original comments.
we believe asum shows great promise because we can easily eliminate clusters that contain mixed topics leaving only classified information.
furthermore as a company could be interested in a smaller number of topics we can use the asum approach with a large k then rank the topics according to a quality criterion that considers word spread probabilities among other things.
finally asum s sentiment classification provides requirements engineers with the information of which topics are positive and which topics are negative.
although this part is not detailed in this paper it was used to present the topic sentiment clusters in the user comment report proving to be useful for the test group in order to suggest changes in requirements.
vi.
conclusion and future work this paper presents an initial assessment of the validity of using topic modeling to analyze a set of user comments for topics relevant to requirements evolution.
we adapted and applied a topic modeling technique on three different sets of user comments for mobile applications and could see that the approach clearly generates a good representation of topics that could be relevant with regard to requirements changes.
although the results are promising this work also serves as a motivation for future improvements in the technique.
we are exploring the sentiment analysis to automatically extract user opinions on the topics extracted as a way to provide requirements engineers additional information about each extracted topic.
we are currently working on enhancing asum s sentiment analysis scheme to also include user ratings associated with the comments in the calculation of the sentiment value and not just as initial information of the generative process.
one way we could address the mis classification or mix of topics that results from running asum with a low number of topics is to apply techniques to avoid groupings of irrelevant words.
we have already filtered out some of the irrelevant words using a stop words list but there are more domain specific words such as good or needs that occur in many comments but should not be used as classifiers.
one approach could be to use part of speech tagging post to identify nouns and verbs and give them a higher weight in the classification than for example adjectives.
another interesting improvement involves the design of quality criteria to evaluate the quality of each topic generated.
a high quality cluster would be one that has low levels of misclassification and is not a duplicate of a previously generatedtopic.
we are looking for ways to computationally express these qualities and use them to rank the topics generated.
by so we can for example easily extract the top topics or set a threshold based on the quality to avoid presenting irrelevant results to the requirements engineer.