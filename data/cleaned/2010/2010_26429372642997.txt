potential biases in bug localization do they matter?
pavneet singh kochhar yuan tian and david lo school of information systems singapore management university kochharps.
yuan.tian.
davidlo smu.edu.sg abstract issue tracking systems are valuable resources during software maintenance activities and contain information about the issues faced during the development of a project as well as after its release.
many projects receive many reports of bugs and it is challenging for developers to manually debug and x them.
to mitigate this problem past studies have proposed information retrieval ir based bug localization techniques which takes as input a textual description of a bug stored in an issue tracking system and returns a list of potentially buggy source code les.
these studies often evaluate their e ectiveness on issue reports marked as bugs in issue tracking systems using as ground truth the set of les that are modi ed in commits that x each bug.
however there are a number of potential biases that can impact the validity of the results reported in these studies.
first issue reports marked as bugs might not be reports of bugs due to error in the reporting and classi cation process.
many issue reports are about documentation update request for improvement refactoring code cleanups etc.
second bug reports might already explicitly specify the buggy program les and for these reports bug localization techniques are not needed.
third les that get modi ed in commits that x the bugs might not contain the bug.
this study investigates the extent these potential biases a ect the results of a bug localization technique and whether bug localization researchers need to consider these potential biases when evaluating their solutions.
in this paper we analyse issue reports from three di erent projects httpclient jackrabbit and lucene java to examine the impact of above three biases on bug localization.
our results show that one of these biases signi cantly and substantially impacts bug localization results while the other two biases have negligible or minor impact.
categories and subject descriptors d. .
software engineering distribution maintenance and enhancement permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
ase september vasteras sweden.
copyright acm ... .
.
terms experimentation keywords issue reports bug localization bias empirical study .
introduction issue tracking systems which contains information related to issues faced during the development as well as after the release of a software project is an integral part of software development activity.
issue tracking systems such as jira or bugzilla can help reporters report various kinds of issues such as bug reports documentation update refactoring request addition of new feature and so on.
well known projects often receive large number of issue reports which might be di cult for developers to handle.
a mozilla developer accepted that the project receives over bugs per day which needs triaging .
therefore it is important to have techniques which can help developers nd buggy les quickly which can help them resolve the bug faster.
to overcome the above issue researchers have proposed techniques which use information given in the bug report to identify source code les that contain the bug .
these techniques often use standard information retrieval ir techniques to compute the similarity between the textual description of bug report and textual description of source code.
based on the similarity scores these ir based bug localization techniques return a ranked list of source code les which are likely to be buggy for that bug report.
these techniques are evaluated using closed and xed issue reports marked as bugs collected from issue tracking systems.
the evaluation involves comparison of les returned by bug localization techniques with the actual les changed to x the bug.
past studies indicate that the performance of these techniques are promising up to of bug reports can be localized by just inspecting source code les .
despite the promising results of ir based bug localization approaches a number of potential biases can a ect the validity of results reported in prior studies.
if these biases signi cantly a ect the results of bug localization studies future researchers need to put more care in cleaning their evaluation datasets when evaluating the performances of their techniques.
in this work we focus on investigating three potential biases .wrongly classi ed reports.
herzig et al.
reported that many issue reports in issue tracking systems are wrongly classi ed .
about one third of all issue reports marked as bugs are not really bugs.
herzig et al.
have shown that this potential bias signi cantly affects bug prediction studies that predict whether a le is potentially buggy or not based on the history of prior bugs.
this potential bias might a ect bug localization studies too as the characteristics of bug reports and other issues e.g.
refactoring requests can be very different.
refactoring can touch a large number of les while bug xes are often more localized .
thus there is a need to investigate whether wrongly classied reports signi cantly skew e ectiveness results of bug localization approaches.
.already localized reports.
our manual investigation of a number of bug reports nd that the textual descriptions of many reports have already speci ed the les that contain the bug.
these localized reports do not require bug localization approaches.
the buggy les are already localized and only need to be xed.
evaluating bug localization approaches with these localized reports will unfairly in ate the e ectiveness results.
thus there is a need to investigate how common are localized reports and whether their presence in the evaluation data set can signi cantly skew the e ectiveness results of bug localization approaches.
.incorrect ground truth files.
kawrykow and robillard reported that many changes made to source code les are non essential changes .
these nonessential changes include cosmetic changes made to source code which do not a ect the behavior of systems.
past fault localization studies often use as ground truth source code les that are touched by commits that x the bugs .
however no manual investigation was done to check if these les are a ected by essential or non essential changes.
files that are a ected by non essential changes should be excluded from the ground truth les as they do not contain the bug.
including these non essential changes as ground truth les can unfairly in ate the e ectiveness results with more ground truth les there is a higher chance that one of them will be identi ed by a bug localization tool.
thus there is a need to investigate how common are the incorrect ground truth les and whether their presence in the evaluation data set can signi cantly skew the e ectiveness results of bug localization approaches.
to investigate the impact of the above mentioned possible biases in this study we analyse the following research questions rq1 what are the e ects of wrongly classi ed issue reports on bug localization?
rq2 what are the e ects of localized bug reports on bug localization?
rq3 what are the e ects of wrongly identi ed ground truth les on bug localization?
the rst research question has been partly answered in our short paper in msr .
we extend our short paper by performing a per project analysis instead of an allproject analysis we also employ an additional test to shedmore light on the research question.
the other two research questions have not been investigated in prior work.
to answer the above research questions we reuse the manually categorized issue reports dataset of herzig et al.
.
herzig et al.
s dataset consists of issue reports from bugzilla and jira issue tracking systems.
several studies in the past have shown that the bug reports in bugzilla are poorly linked whereas bug reports in jira are well linked as jira provides add ons to help connect issues to commits in the version control systems .
therefore we only use projects from herzig et al.
s dataset that use jira.
we investigate issue reports stored in issue tracking systems of three projects httpclient jackrabbit and lucene java.
httpclient is a java library for implementing the client side of the most recent http standards and recommendations .
jackrabbit is a content repository written in java .
lucene java is a high performance search engine library written in java .
table shows the dataset we use for this study.
table project details project issue tracker issue reports httpclient jira jackrabbit jira lucene java jira these reports have been manually classi ed by herzig et al.
into bug reports and other kinds of issues .
we perform suitable statistical tests mann whitney wilcoxon test and fisher exact test and compute e ect size measure cohen s d to investigate whether the skew introduced by the above potential biases matters.
we nd that some biases do not signi cantly or substantially a ect bug localization techniques performance.
however one of them signi cantly and substantially a ect bug localization techniques performance and researchers need to consider this bias by cleaning their evaluation dataset to correctly measure the e ectiveness of bug localization solutions.
the contributions of this paper are as follows .
we extend our preliminary study to analyze the e ect of wrongly classi ed issue reports on the e ectiveness of bug localization tools.
.
we analyze the e ect of localized bug reports on the e ectiveness of bug localization tools.
to do this we manually investigate bug reports and categorize them as localized partially localized or non localized.
we also build an automated technique that can categorize bug reports into these three categories with high accuracy.
.
we analyze the e ect of incorrect ground truth les on the e ectiveness of bug localization tools.
to do this we manually investigate source code les that are touched by commits to x various bugs and detect irrelevant les that do not contain bugs.
.
we release a clean dataset that researchers can use to evaluate future bug localization techniques.
the structure of this paper is as follows.
in section we give a brief summary of ir based bug localization techniques.
in sections and we investigate rq1 rq2 804and rq3 respectively.
we discuss other interesting ndings and threats to validity in section .
related work is discussed in section .
we nally conclude and mention future work in section .
.
bug localization a primer in recent years a number of studies have proposed information retrieval based approaches to automatically locate bugs in source code les.
they take bug reports as input queries and source code les as input documents and output a list of relevant source code les to a particular bug report.
the major goal is to nd a well designed metrics to capture the similarity between bug reports and source code les.
many techniques have been proposed to compute such similarity based on textual information stored in bug reports and source code e.g.
words appear in bug reports and source code les.
in section .
we describe a general bug localization framework and highlight one popular technique based on vector space model vsm .
in section .
we describe how bug localization studies often obtain ground truths to evaluate the e ectiveness of their techniques when applied on bug reports.
in section .
we describe a popular metric to measure the e ectiveness of bug localization techniques namely mean average precision map .
.
general framework the general framework contains three major steps code corpus pre processing and indexing bug report pre processing and retrieval and ranking.
we describe the details of each step as follows.
step code corpus pre processing and indexing.
we apply three preprocessing steps to process code corpus i.e.
source code les normalization stop word removal and stemming.
in the normalization step we rst extract comments identi ers and string literals from source code les.
tools like eclipse jdt1can be used to extract abstract syntax trees asts that can then be traversed to extract these textual contents from source code les.
next we remove punctuation marks special symbols and number literals from the extracted text.
we also convert all words into lower case.
in the stop word removal step we rst remove commonly occurring english words e.g.
i you we are etc.
.
we use the stop word list from mysql.com doc refman .
en fulltext stopwords.html.
we also remove programming language keywords e.g.
public class if for etc.
in the stemming step we apply the famous porter stemming algorithm2to reduce a word to its root form.
for example we reduce mapping mapped and maps to map .
at the end of the above steps each of the les in the corpus are represented by a bag of words.
we then index these bags of words so that one can locate les containing a particular word e ciently.
step bug report pre processing.
ir based bug localization approaches regard one bug report as a query.
textual information inside a bug report such as the content of the summary and description elds are extracted.
ta1 shows an example bug report with id jcr from jackrabbit project along with the contents of its summary and description elds.
after the text of the bug reports are extracted we perform several pre processing steps tokenization stop word removal and stemming.
in the tokenization step we convert the textual content of a bug report into a multi set bag of words that appear in it.
this bag of words are then input to the stop word removal and stemming steps which are are the same ones that are applied to pre process the code corpus.
table issue report jcr from jackrabbit summary incorrect results from joins on multivalued properties description it looks like join conditions on multivalued properties only use one of the multiple values for the comparison.
step retrieval and ranking.
in this step given a bug report we want to retrieve and rank relevant source code les.
retrieval and ranking of relevant source code les is based on the similarity between the bug report and each of the les in the code corpus.
in the ir domain various models have been proposed to measure the similarities of a query in our case bug report with a document in our case a source code le .
these include vector space model vsm smoothed unigram model sum latent dirichlet allocation lda etc.
in this work we focus on vsm which is the foundation of many state of the art ir based bug localization approaches e.g.
and has been shown to outperform many other models .
vsm takes the pre processed bug report as a query and pre processed source code les as documents.
in vsm each query or document is expressed as a vector of weights where each weight corresponds to a word that appears in the query or document.
the weight is usually computed using the tf idf weighting scheme.
the tf idf weight of word win document dgiven a set of documents d denoted astf id f w d d is computed as tf id f w d d log f w d logjdj jdi2d w2dij in the above equation f w d is the number of times word woccurs in document d and di2d w2direpresents documents that contain word w. after converting queries and documents into vectors of weights vsm computes the similarity between a query and a document as the cosine similarity between the two corresponding vectors.
equation shows the function to calculate the cosine similarity between a query qand a document d. similarity q d cos q d vq vd jvqjjvdj where vqandvdare vector of term weights for query q and document d respectively.
vq vdrepresents the inner product of the two vectors.
.
ground truth identification ir based bug localization approaches are often evaluated on bug reports submitted to various issue tracking systems.
805for each bug report there is a need to get the ground truth leswhich are the source code les that contain the bug.
manual identi cation of ground truth les would takes a lot of e ort therefore researchers have come up with an automated yet imperfect approach to identify these les.
the underlying idea is to map a bug report to commits in version control systems that x the bug.
often the identi er of the bug report appears in the logs of the corresponding commits.
after these commits are identi ed les that are touched by one or more of these commits are considered as ground truth les.
bug localization techniques are then evaluated based on their ability to recover these ground truth les from the bug report.
.
evaluation metric a bug localization technique outputs a ranked list of les for every bug report.
given a set of bug reports the technique will output a set of ranked lists.
a number of metrics can be used to evaluate the e ectiveness of the technique based on the position of the buggy les in the ranked lists.
one of the most popular metrics is mean average precision map which has been used to evaluate many recent studies .
to compute map for each ranked list we need to compute average precision ap which is de ned as follows ap mx i 1p i rel i all buggy les where mis the number of retrieved source code les rel i is a binary value that represents whether the ith retrieved source le is buggy or not.
p i is the precision at position iof the ranked list which is de ned as p i buggy les retrieved in top i positions i map is then the mean of the average precisions over all ranked lists produced for the bug reports.
the higher the map of a bug localization technique the more e ective is the technique.
.
bias report misclassification in this section we investigate the rst research question what are the e ects of wrongly classi ed issue reports on bug localization?
we describe the motivation of answering this question in section .
the methodology of our experiments in section .
and the results of our experiments which answer the question in section .
.
.
motivation issue tracking systems contain reports of several types of issues such as bugs requests for improvement documentation refactoring etc.
herzig et al.
report that a substantial number of issue reports marked as bugs are not bugs but other kinds of issues.
their results show that these misclassi cations have a signi cant impact on bug prediction.
in this question we want to analyse the consequences of misclassi cation on bug localization.
.
methodology step data acquisition.
we use herzig et al.
s dataset of manually analyzed issue reports softevo bugclassify .
we download the issue reports fromthe associated jira repositories and extract the textual contents of the summary and description of the reports.
after downloading we perform the preprocessing steps described previously.
in jira each issue report has a unique identier represented by the project name and a unique number.
for example httpclient represents issue number of project httpclient.
we use the gitversion control system of the projects to get the commit log les which are used to map issue reports to their corresponding commits.
commit logs contain unique identi er of the issue report as part of the commit message.
we use these mapped commits to check out the source code les prior to the commits that address the issue and the source code les when the issue is resolved.
for each source code le we perform a similar preprocessing step to represent a le as a bag of words.
step bug localization.
after the data acquisition we have the textual content of the issue reports the textual content of each source code le in the revision prior to the x and a set of ground truth les that are changed to x the issue report.
we give the textual content of the issue reports and the revision s source code les as input to the bug localization technique which outputs a ranked list of les sorted based on the similarity to the bug report.
step e ectiveness measurement statistical analysis.
after step for each issue report we have a ranked list of source code les and a list of supposed ground truth les.
we compare these two lists to compute the average precision score.
we divide the issue reports into two categories issue reports marked as bugs in the tracking system reported and issue reports that are actual bugs i.e.
manually labeled by herzig et al.
actual .
in herzig et al.
s dataset the set actual is a subset of reported.
we compute the map scores and use mann whitney u test to examine the di erence between these two categories at .
signi cance level.
we use cohen s d to measure the e ect size which is the standardised di erence between two means.
to interpret the e ect size we use the interpretation given by cohen i.e.
d .
means trivial .
d .
means small .
d .
means medium .
d .
means large and d .
means very large.
table mean average precision map scores for reported and actual project reported actual di erence d httpclient .
.
.
.
jackrabbit .
.
.
.
lucene java .
.
.
.
.
results e ect of misclassi cation on bug localization.
table shows the map scores for the two categories reports marked as bugs reported and manually classi ed bug reports actual .
we observe that there are di erences of .
.
and .
in the map scores for httpclient jackrabbit and lucene java respectively.
we perform the mann whitney wilcoxon test and compute cohen s d to examine the di erences between the two categories.
the results are also presented in table .
from the results we observe that for httpclient and lucene java 806the di erences are statistically insigni cant and the e ect sizes are trivial i.e.
less than .
.
for jackrabbit the e ect size is trivial however the di erence is statistically signi cant.
e ect of di erent misclassi cation types.
we now analyse the misclassi cation type that has the most impact on the di erence of map scores between reported and actual.
herzig et al.
classify issue reports into categories bug rfe improvement documentation refactoring backport cleanup spec task test build system design defect and others .
we omit issue reports that are misclassi ed one category at a time and recalculate the map score.
for example rfetobugrepresents issue reports which are rfe actual but are misclassi ed as bug reported .
table shows the map scores when we remove issue reports of particular misclassi cation types one at a time.
each row corresponds to a subset of reports where reports of a misclassi cation type is removed.
we observe that test tobughas the largest di erence in the map score followed by misclassi cation from improvement tobug.
table mean average precision map scores when issue reports of a particular misclassi cation type are omitted.
omit.
omitted misclass.
misclassi cation hc httpclient jb jackrabbit lj lucenejava.
the last column is the map of all three projects.
omi t. misclass.
type hc jb ljov erall ac tual to reported no ne .
.
.
.
rf etobug .
.
.
.
do cumentation tobug .
.
.
.
im provement tobug .
.
.
.
re factoring tobug .
.
.
.
ba ckport tobug .
.
.
.
cl eanup tobug .
.
.
.
sp ectobug .
.
.
.
ta sktobug .
.
.
.
te sttobug .
.
.
.
bu ild system tobug .
.
.
.
de sign defect tobug .
.
.
.
ot hers tobug .
.
.
.
bias which is wrongly classi ed issue reports signi cantly impacts bug localization result for one out of the three projects.
however the e ect of this bias is negligible the e ect sizes are less than .
.
.
bias localized bug reports in this section we investigate the second research question what are the e ects of localized bug reports on bug localization?
we describe the motivation of answering this question in section .
the methodology of our experiments in section .
and the results of our experiments which answer the question in section .
.
.
motivation localized bug reports are those whose buggy les have been identi ed in the report itself.
for these reports the remaining task to resolve the bug is simply to x the buggy les.
these bug reports do not bene t or require bug localization solutions.
past studies on bug localization do notseparate localized from non localized bug reports.
in this research question we want to investigate the number of localized bug reports and the impact of including localized bug reports in the evaluation of bug localization tools.
if bias exists then future bug localization solutions need to be careful to perform a data cleaning step to remove these localized bug reports from their evaluation dataset.
.
methodology to investigate this research question we rst need to identify localized bug reports.
we start by manual investigating of a smaller subset of bug reports and identify localized ones.
we then developed an automated means to nd localized bug reports so that our analysis can scale to a larger number of bug reports.
finally we input these reports to a number of ir based bug localization tools to investigate whether localized reports skew the results of bug localization tools.
table fully localized partially localized and not localized reports category description fully bug reports where all the les containing the bugs are explicitly speci ed in the report.
partially bug reports where some of the les containing the bugs are explicitly mentioned in the report.
not bug reports which do not explicitly specify any of the buggy les.
step manually identifying localized bug reports.
we manually analysed issue reports that herzig et al.
labeled as bug reports.
out of the issue reports from the three projects herzig et al.
labeled of them as bug reports.
we randomly selected these from the pool of bug reports from the three software projects.
for our manual analysis we read the summary and description elds of each bug report.
we also collected the corresponding les changed to x each bug.
we classi ed each bug report into one the three categories shown in table .
table and show example bug reports that are fully localized partially localized and not localized.
table fully localized report httpclient1078 summary decompressingentity not calling close on inputstream retrieved by getcontent description the method decompressingentity.writeto outputstream outstream does not close the inputstream retrieved by getcontent .
according to the documentation of httpentity.writeto important please note all entity implementations must ensure that all allocated resources are properly deallocated when this method returns.
imho this is not satis ed indecompressingentity .writeto buggy files decompressingentity.java 807table partially localized report jcr summary oracle bundle pm fails checking schema if users use the same database description when using the oraclebundlepersistencemanager there is an issue when two users use the same database for persistence.
in that case the checkschema method of the bundledbpersistencemanager does not work like it should.
more precisely the call metadata.gettables null null tablename null will also includes table names of other schemas users.
e ectively only the rst user of a database is able to create the schema.
probably same issue as here jcr buggy files bundledbpersistencemanager.java oraclepersistencemanager.java table not localized report lucene summary charfilters not being invoked in solr description on solr trunk all charfilters have been non functional since lucene3396 was committed in r1175297 on sept until yonik s x today in r1235810 solr .x was not a ected charfilters have been working there all along.
buggy files tokenizerchain.java step automatic identi cation of localized reports.
in this step we build an algorithm that takes in a set of les that are changed in bug xing commits and a bug report and outputs one of the three categories described in table .
our algorithm rst extracts the text that appear in the summary and description elds of bug reports.
next it tokenizes this text into a set of word tokens.
finally it checks whether the name of each buggy le ignoring its lename extension appears as a word token in the set.
if all names appear in the set our algorithm categorizes the report as fully localized .
if only some of the names appears in the set it categorizes the bug report as partially localized .
otherwise it categorizes the bug report as not localized .
we have evaluated our algorithm on the manually labeled bug reports and nd that its accuracy is close to .
step application of ir based bug localization techniques.
after localized partially localized and not localized reports are identi ed we create three groups of bug reports.
we feed each of them into the vsm based bug localization tool described in section .
we then evaluate the e ectiveness of these tools for each of the three groups of reports.
step statistical analysis.
we perform two statistical analyses.
first we compare the average precision scores achieved by vsm based bug localization tool for the set of fully localized partially localized and not localized reports using mann whitney wilcoxon test at signi cance level.
we also compute cohen s d on the average precision scores to see if the e ect size is small medium or large.second we compare a subset of bug reports where the vsm based bug localization technique performs the best and another subset where the vsm based bug localization techniques performs the worst.
we then compare the distribution of fully partially and not localized bugs in these two subsets.
we employ fisher exact test to see if the distribution for the rst subset signi cantly di ers with the distribution for the second subset.
.
results number of fully localized partially localized and not localized reports.
the numbers of bug reports that are identi ed as fully partially and not localized are shown in table .
we can observe that out of bug reports .
bug reports are fully localized i.e.
the bug reports contains the name of all the class les changed to x the bug.
over of the bug reports are either fully or partially localized.
this shows that a signi cant number of bug reports are already localized and do not bene t from a bug localization algorithm.
on the other hand bug reports .
are not localized at all.
table fully partially and not localized reports project category number proportion httpclientfully .
partially .
not .
jackrabbitfully .
partially .
not .
lucene javafully .
partially .
not .
average precision scores of fully vs. partially vs. not localized reports.
table shows the mean average precision map of the vsm based bug localization technique when applied to the set of fully partially and notlocalized reports.
we can note that the map score di erences between fully localized and not localized bug reports for httpclient jackrabbit and lucene java are .
.
and .
respectively.
also the map score differences between partially localized and not localized bug reports for httpclient jackrabbit and lucene java are .
.
and .
respectively.
table map scores fully vs. partially vs. not project fully partially not httpclient .
.
.
jackrabbit .
.
.
lucene java .
.
.
we also perform mann whitney wilcoxon test to examine the di erence between the following categories fully partially partially not and fully not.
table shows the p values between di erent categories.
the results show that there are signi cant di erences between average precision scores of fully localized and partially localized bug reports fully localized and partially localized bug reports 808table comparison fully vs. partially vs. not projectfully partially partially not fully not p value d e ect size p value d e ect size p value d e ect size httpclient .
.
large .
.
medium .
e .
large jackrabbit .
e .
medium .2e .
medium .2e .
large lucene java .
.
medium .
e .
small .
e .
large and partially localized and not localized bug reports i.e.
all the p values are less than .
.
we also compute cohen s d to measure an e ect size and nd that the e ect sizes are small to large.
the e ect sizes between average precision scores of fully localized and not localized bug reports are large for all three projects.
this shows that there is a large substantial di erence in the e ectiveness of a bug localization tool when applied to bug reports that are fully localized and those that are not localized.
best vs. worst bug reports.
we want to examine the di erence between the proportion of bug reports that are fully partially and not localized in the upper and lower quartile of the bug reports based on the ability of the vsmbased bug localization tool to localize them.
we simply sort the bug reports based on their average precision scores and identify the subset that appear in the top of the list upper quartile and another subset that appear in the bottom of the list lower quartile .
for jackrabbit and lucene java we randomly select bug reports from the upper quartile and another from the lower quartile.
for httpclient we randomly select bug reports from the upper quartile and another from the lower quartile since in our dataset httpclient has less than bug reports.
table shows the number of fully partially and not localized bugs for each of the projects.
we use fisher exact test to examine the di erence between the distribution of fully localized partially localized and not localized bug reports in the upper and lower quartiles.
the null hypothesis is that there is no di erence between the distribution of fully partially and not localized bug reports in the upper and lower quartiles.
the alternate hypothesis is that there is a significant di erence between the distribution of bug reports in the upper and lower quartiles.
we nd that the p values for all the projects are very small which shows that there is a signi cance di erence in the distribution of fully localized partially localized and not localized bug reports between the best and worst bug reports.
bias which is localized bug reports signi cantly and substantially impacts bug localization results.
more than of the bugs are already localized either fully or partially these reports explicitly mention some or all of the les that are buggy and thus do not require a bug localization algorithm.
the mean average precision scores for fully and partially localized bug reports are much higher i.e.
signi cantly and substantially higher than those for not localized bug reports.
the e ect sizes of average precision scores between fully and not localized bug reports are large for all three projects.
.
bias non buggy filestable fisher exact test best vs. worst reports project fully partially not p value httpclientupper .0041lower jackrabbitupper .
e lower lucene javaupper .
e lower in this section we investigate the third research question what are the e ects of wrongly identi ed ground truth les on bug localization?
we describe the motivation of answering this question in section .
the methodology of our experiments in section .
and the results of our experiments which answer the question in section .
.
.
motivation another issue which can bias the result is wrongly identi ed ground truth les.
in past studies wrongly identi ed ground truth les have not been removed since they require additional analysis.
these wrongly identi ed ground truth les can potentially skew the result of existing bug localization solutions.
in this research question we want to investigate to what extent do wrongly identi ed ground truth les a ect bug localization.
.
methodology step manually identifying wrong ground truth files.
we randomly select bug reports that are not already localized i.e.
these reports do not explicitly mention any of the buggy les and investigate the les that are modi ed in the bug xing commits.
we manually perform a di that gives us the di erences between the modi ed le and the original le.
based on these di erences we manually decide if a le contains a bug or not.
files that are only a ected by cosmetic changes refactorings etc.
are considered as non buggy les.
based on this manual analysis for each bug report we have the set of clean ground truth les and another set of dirty ground truth les.
thung et al.
have extended kawrykow and robillard work to automatically identify real ground truth les .
however the accuracy of their proposed technique is still relatively low i.e.
precision and recall scores of .
and .
.
hence we do not employ any automated tool to identify wrong ground truth les.
we also cannot extend the study to investigate a large number of bug reports since the identi cation of wrong ground truth les is time consuming.
step application of ir based bug localization techniques.
after the set of clean and dirty ground truth les are identi ed for each of the bug reports we input the bug 809reports to a vsm based bug localization tool described in section .
we evaluate the results of the tool on dirty and clean ground truth les.
step statistical analysis.
we compare the average precision scores achieved by the vsm based bug localization tool for the bug reports with clean and dirty ground truth les using mann whitney wilcoxon test at signi cance level.
we also compute cohen s d on the average precision scores to see if the e ect size is small medium or large.
.
results number of wrong ground truth files.
we found that out of les changed to x the bugs only les are really buggy.
the other les .
do not contain any of the bugs but are changed because of refactorings modi cations to program comments due to changes made to the buggy les etc.
figure shows the di of a le that is changed in a commit that x bug report lucene .
the content of the bug report with id lucene is shown in table .
figure example di of a file that is changed to fix a bug in lucene java project with id lucene2616.
note the name of the le segmentinfo.java an empty line and an import statement are deleted an empty line is deleted and another one is added.
table bug report lucene summary fastvectorhighlighter out of alignment when the rst value is empty in multivalued eld description non buggy file segmentinfo.java table map scores dirty vs. clean ground truths project dirty clean di erence d httpclient .
.
.
.
jackrabbit .
.
.
.
lucene java .
.
.
.
map scores dirty vs. clean.
we compare the mean average precision map scores of these bug reports when evaluated on dirty and clean ground truths.
table shows that the di erences in the map scores are between to .
.
we also ran mann whitney wilcoxon test andcompute cohen s d to check if each di erence is signi cant or substantial.
we nd that the di erence is not statistically signi cant and the e ect size is trivial .
.
bias which is incorrect ground truth les neither signi cantly nor substantially a ects bug localization results.
we notice that .
of the les present in the ground truth i.e.
they are changed in a commit that x a bug are non buggy.
also there is a di erence of .
between the map scores when a bug localization tool is evaluated on dirty and clean ground truth.
however this di erence is neither statistically signi cant nor substantial.
.
other findings and threats in this section we rst describe the e ects of the biases measured by several other popular evaluation metrics.
next we describe some threats to validity.
.
other evaluation metrics beside mean average precision map which we used in the previous sections hit n and mrr have also been used to evaluate bug localization studies .
hit n and mrr are presented below hit n this metric counts the percentage of bug reports with at least one buggy le found in the top n e.g.
ranked results.
mrr mean reciprocal rank the reciprocal rank of a bug report is the inverse of the rank of the rst buggy le in the ranked results.
the mean reciprocal rank takes the average of the reciprocal ranks of all bug reports.
for a set of bug reports q mrr is de ned as mrr jqjqx i rank i where rank iis the rank of the rst buggy le in the output ranked list.
figure before and after removing bias the e ect of bias bias and bias measured by hit and mrr are shown in figures to .
figure shows that for bias its e ect in terms of hit and mrr scores is 810figure before and after removing bias figure before and after removing bias minimal.
figure shows that for bias its e ect in terms of hit and mrr score is substantial.
figure shows that for bias for jackrabbit its e ect is minimal.
for httpclient and lucene java its e ect is more apparent albeit not as substantial as the e ect of bias .
for mrr since it is a mean of a distribution we also run mann whitney wilcoxon test and compute cohen s d values.
the results are shown in table .
we nd that for bias its e ect is not statistically signi cant for all projects.
for bias its e ect is both statistically signi cant and substantial when comparing the results of fully or partially localized bug reports with results of not localized bug reports.
for bias its e ect is not statistically signi cant for all projects.
to conclude the above results show that bias has substantial e ect on the performance of bug localization techniques.
the e ects of bias and are more minor or even negligible.
these results are in line with the ndings of sections and .
.
threats to validity threats to internal validity corresponds to errors in our experiments and our labeling.
in this work these threats are coming from human classi cation of bug reports.
for bias we consider the same issue reports that were manually categorized by herzig et al.
.
for bias we manually categorize bug reports as fully localized partially localized or not localized.
also we design an automated algorithm to identify localized reports and nd that it performs very well on the manually labeled bug reports.
however it is nottable results of mann whitney wilcoxon test and cohen d computation for mrr.
f p fully localized vs. partially localized.
p n partially localized vs. not localized.
f n fully localized vs. not localized.
bias type project p value d bias 1httpclient .
.
jackrabbit .
.
lucene java .
.
bias 2httpclient f p .
.
p n .
.
f n .
.
jackrabbit f p .
.
p n .2e .
f n .2e .
lucene java f p .
.
p n .201e .
f n .805e .
bias 3httpclient .
.
jackrabbit .
.
lucene java .
.
clear if performs as well on other bug reports in our dataset.
for bias we manually categorize les that are changed in commits that x bug reports.
to reduce bias two phd students majoring in software engineering analyze the bug reports and agree on the labels.
threats to external validity relates to the generalizability of our ndings.
in this work we consider three open source projects httpclient jackrabbit and lucene java.
we analyze bug reports for rq1 bias bug reports for rq2 bias and bug reports for rq3 bias .
we plan to include more projects and analyze more bug reports in a future work.
we have also only analyzed vsm based bug localization approach.
there are many other techniques proposed in the literature and we plan to analyze them in a future work.
many of these techniques are based on vsm e.g.
and they are likely to be a ected by the biases in a similar way as plain vsm.
threats to construct validity relates to the suitability of our evaluation metrics.
we make use of map hit n and mrr which are popular metrics that have been used in many past bug localization studies .
we also perform two widely used statistical tests i.e.
mannwhitney wilcoxon test and fisher exact test and compute one widely used e ect size measure i.e.
cohen s d .
.
related work in this section we describe studies that analyze bias in software engineering and ir based bug localization studies.
our survey here is by no means complete.
.
bias in software engineering many software engineering studies are highly dependant on data stored in software repositories.
however the dataset is not always clean which means it might contain bias.
a set of research work have shown that such bias in a dataset might impact software engineering studies e.g.
.
we highlight some of them especially closely related ones below.
811antoniol et al.
noted that bug tracking system not only maintains reports of bugs but also other issue reports such as reports that contain feature enhancement requests .
later herzig et al.
studied misclassi ed bug reports of multiple projects by manually checking around issue reports .
they found that around .
of their sampled bug reports are misclassi ed and such bias a ects bug prediction techniques.
bird et al.
investigated whether the bugs sampled based on commit logs are fair representation of the full set of xed bugs or not .
kawrykow and robillard observed that many changes to source code corpus contain non essential modi cations such as renaming variable and adding comments and thus might cause bias to techniques that analyze version control repositories .
thung et al.
extended kawrykow and robillard s approach to detect root causes of bugs from commits .
recently herzig and zeller investigated the impact of tangled code changes which are multiple changes for di erent tasks inside a single commit .
nguyen et al.
analyzed bug xing commits and found that more than of the changed les are non buggy les .
this work especially the rst research question is an extension of our previous short paper that analyzes how issue report misclassi cation a ects bug localization .
for our rst research question we extend the previous work by performing a per project analysis instead of all project analysis and by computing cohen s d to check if the impact of misclassi cation is substantial or not.
in the previous work we nd that bias signi cantly impacts bug localization results.
in this study we nd that bias only signi cantly impacts bug localization results for one out of the three projects.
the other two research questions are newly proposed in this work.
.
ir based bug localization approaches there are many ir based bug localization approaches that retrieve source code les that are relevant to an input bug report .
rao and kak conducted a comparative study on the performance of a number of general ir models on bug localization task .
they found that simple text models such as vector space model vsm and smoothed unigram model sum perform better than more sophisticated models such as latent dirichlet allocation lda .
since then a number of works have been done to improve the e ectiveness of standard ir models by considering more information applying advanced techniques and re ning queried bug reports.
zhou et al.
proposed an extended vector space model named rsvm to localize bug reports by leveraging information from similar bug reports .
saha et al.
made use of code structure information retrieved from source code e.g whether a word is used as a class name or a variable name and bug report structure e.g.
whether a word is appeared in the title or description led of a bug report to improve the e ectiveness of ir based bug localization .
based on the assumption that a bug report and its relevant code les share several latent technical aspects nguyen et al.
developed a customized topic model approach named bugscout to localize bug reports .
recently wang et al.
proposed an integrated bug localization approach by considering multiple resources i.e version history similar bug reports and structure information .
in this work we focus on potential biases that might im pact bug localization techniques.
our study highlights several steps that researchers need to take to clean up datasets used to evaluate the performance of bug localization techniques.
.
conclusion and future work many studies have proposed ir based bug localization techniques to aid developers in nding buggy les given a bug report.
these studies often evaluate their e ectiveness on issue reports marked as bugs in issue tracking systems using as ground truth the set of les that are modi ed in commits that x each bug.
however there are several potential biases that can impact the results of these bug localization studies.
firstly issue reports marked as bugs in issue tracking systems might not be bugs due to errors in the reporting and classi cation process.
secondly bug reports might already be localized i.e.
they might explicitly mention the buggy les which obviates the need to run localization on these bug reports.
thirdly les modi ed as part of a bug x commit might not be buggy i.e.
their modi cations only involve cosmetic changes such as declaring empty variable changing comments and so on.
our study analyzes the impact of these potential biases on bug localization results.
our empirical study highlights the following results .
wrongly classi ed issue reports do not statistically signi cantly impact bug localization results on two out of the three projects.
they also do not substantially impact bug localization results on all three projects e ect size .
.
.
already localized bug reports statistically signi cantly and substantially impact bug localization results pvalue .
and e ect size .
.
.
existence of non buggy les in the ground truth does not statistically signi cantly or substantially impact bug localization results e ect size .
.
our ndings suggest that future bug localization researchers need to at least remove already localized bug reports from their evaluation dataset since such reports have signi cant and substantial impact on the performance of bug localization techniques.
as a future work we plan to investigate more bug reports from additional systems to reduce the threats to external validity.
we also plan to investigate additional biases that might a ect bug localization studies.
acknowledgement we would like to thank kim herzig sascha just and andreas zeller for making the dataset used in their work publicly available.
dataset our dataset is made publicly available at com smusis buglocalizationbiases .
.