automatic recommendation of api methodsfrom feature requestsferdian thung1 shaowei wang1 david lo1 and julia lawall21singapore management university singapore2inria lip6 regal france ferdiant.
shaoweiwang.
davidlo smu.edu.sg julia.lawall lip6.frabstract developers often receive many feature requests.to implement these features developers can leverage variousmethods from third party libraries.
in this work we proposean automated approach that takes as input a textual descriptionof a feature request.
it then recommends methods in libraryapis that developers can use to implement the feature.
ourrecommendation approach learns from records of other changesmade to software systems and compares the textual descriptionof the requested feature with the textual descriptions of variousapi methods.
we have evaluated our approach on more than 500feature requests of axis2 java cxf hadoop common hbase and struts .
our experiments show that our approach is able torecommend the right methods from libraries with an averagerecall rate of .
and recall rate of .
respectively.we also show that the state of the art approach by chan et al.
that recommends api methods based onprecisetext phrases isunable to handle feature requests.i.
introductiondevelopers often receive requests for new features sub mitted via systems such as jira1.
given the requirementsexpressed in these feature requests developers need to locatecode units that should be changed and then implement therequired changes.
while a number of concern localizationtechniques have been proposed for locating code units ofinterest there is still littleautomated support to help developers implement the changesrequired to satisfy a feature request.many software systems rely on a variety of external librariesfor various functionalities.
accordingly developers often useexternal libraries to implement required changes.
however using these libraries effectively requires knowledge of therelevant methods and classes that they provide.
given the largenumber of libraries and the large number of methods andclasses that they provide it can be a challenge for developersto identify the methods and classes of interest given a targetrequirement document expressed as a feature request.considering the above issues and opportunities there is aneed for an automated approach that could help developers tobetter harness the power of libraries.
the automated approachshould be able to recommend library methods given a featurerequest.
we refer to our problem asmethod recommendationfrom feature requests.a number of techniques have been proposed to recommendcode units given a requirement.
mandelin et al.
and1 and xie propose a technique to generatecode snippets that can convert an object of a particular typeto another object of a different type.
while this technique isuseful for a number of situations it requires the informationabout the desired functionality to be expressed at code level.chan et al.
propose a code search technique that takes in textphrases and returns a graph of api methods that best matchthe phrases .
their approach requiresprecisetext phrasesthat match some words in the api methods.
these techniquesare not sufficient to automatically process feature requests which typically describe high level requirements written innatural language.
in this work we propose a complementaryapproach that recommends relevant library methods directlyfrom feature requests.our proposed approach learns from a training dataset ofchanges made to a software system recorded in repositories i.e.
issue management systems and version control systems .each change in the dataset has three parts the textual descrip tion describing the change text the code before the change pre change and the code after the change post change .
ourapproach takes as input a new textual description text andthen recommends methods from a set of libraries to be usedin the post change code.to recover methods that can be used to construct the post change code our approach performs a two pronged approachto rank relevant methods.
first it searches for similarclosedorresolvedfeature requests in the training data.
aclosedorresolvedfeature request is one that has been addressed bydevelopers and where appropriate changes have been made tothe software system.
it then looks into the api methods thatare used to implement these feature requests and measuresthe relevance of various methods based on the number ofsimilar closed requests which use them.
second our approachmeasures relevance by looking into the similarity between thetextual description of the feature request and the descriptionsof the api methods.
our approach then learns an integratedranking function that is used to recover a list of potentiallyrelevant library methods that are then recommended to thedevelopers.we have evaluated our solution on feature requests storedin the jira issue management systems of java applications axis2 java cxf hadoop common hbase and struts .each feature request in jira can be linked to the commitsin the corresponding version control system that implement978 c ieee ase palo alto usa accepted for publication by ieee.
c ieee.
personal use of this material is permitted.
permission from ieee must be obtained for all other uses in any current or future media including reprinting republishing this material for advertising or promotional purposes creating new collective works for resale or redistribution to servers or lists or reuse of any copyrighted component of this work in other works.290the requested feature.
we recommend methods from thirdparty libraries commons codec commons io commons lang commons logging junit servlet api easymock log4j slf4j api and slf4j log4j12.
these are the most popular librariesused by java applications developed under the apache founda tion.
these libraries provide various functionalities includingtesting logging i o etc.
the accuracy of our proposedapproach is measured using recall rate and recall rate these measures have also been used to evaluate past studieson bug report analysis .
our experimentsshow that we can achieve a recall rate and recall rate 10of .
and .
respectively.
on the other hand we showthat the state of the art code search approach by chan et al.
that recommends api methods from precise textual phrases isnot effective to directly process feature requests which oftencontain high level requirements.
indeed we find that theirapproach returns no relevant methods.our contributions are as follows we propose a new problem ofmethod recommendationfrom feature requests.
we propose a technique that leverages information frompast similar closed or resolved feature requests and com pares the textual description of a feature request withthose of library methods.
our technique learns an inte grated ranking function that is then used to recommendlibrary methods to be used in the post change code.
we evaluate our approach on change requests of 5applications and recommend methods from libraries.we show that our approach achieves a recall rate andrecall rate of .
and .
respectively.the structure of this paper is as follows.
in section ii we describe some preliminary concepts.
in section iii wepresent an overview of our proposed approach.
we elaboratethe three processing components of our approach in sec tions iv v vi.
we highlight our experimental methodologyand results in section vii and describe related studies insection viii.
finally we conclude and mention future workin section ix.ii.
preliminariesin this section we describe some preliminary materials thatare needed for latter sections.
we first describe the issue man agement system jira and show how it stores feature requests.we then describe some text pre processing techniques and thevector space model.a.
feature requests and jirajira is an issue management system developed by al tassian.2it is used in many software projects to captureand store issues that are reported by users and developers.among its users are the many projects developed by theapache software foundation.
figure shows a sample issuestored in the jira repository of an apache project.
an issuecontains a number of fields including summary description priority component etc.
for our work we are especiallyinterested in the fields listed in table i.table ifields inaj i r aissuename descriptionsummary the summary title of the issuedescription the detailed description of the issuecomponent the component affected by the issuereporter the name of the person who submitted the issue reportpriority the urgency of the issue to be addressedeach issue in jira can be categorized into one of thesetypes bug new feature task etc.
in this study weare interested in feature requests reported in jira.
a featurerequest in jira can be seen as an issue of type newfeature improvement or wish .
each issue also has apriority.
the priority indicates the urgency of the issue to beaddressed.
table ii lists the priorities available in jira alongwith their descriptions.table iipriority injiraname descriptionblocker blocks development and or testing produc tion could not runcritical crash loss of data severe memory leakmajor major loss of functionalityminor minor loss of functionality or other problemwhere an easy workaround is presenttrivial cosmetic problem like misspelled words ormisaligned textan issue can be assigned various status labels open inprogress resolved closed etc.
a new issue is typicallygiven the status open .
an issue that has been addressedto completion by developers is given the status resolved or closed .
each issue report in jira has a unique issueidentifier id used to identify the report.
the format of thisidentifier is typically a short name for the project followedby the issue number in the project e.g.
hbase .
jiracan be integrated with version control systems like svn git etc.
each issue in jira can then be linked to the commitsin the version control system that address the issue.
theissue identifier is added to the log messages of the commitsthat address the issue.
this provides an easy identification ofchanges made to address the issue.
we show an example ofthis link in figure .
we can see that it is easy to identifythe commits in the version control system that address thehbase 3850issue.b.
text pre processingtext pre processing is an important task in text mining .its purpose is to convert a piece of text into a commonrepresentation easily processed by a text mining algorithmand to remove certain noise.
widely used text pre processingstrategies include tokenization and stemming.291fig.
.
a sample jira issue fig.
.
sample link between a jira issue and a commit in a version control systemtokenization refers to the process that breaks a documentinto word tokens.
delimiters are used to demarcate one tokenfrom another.
typically space and punctuation are used asdelimiters.
after tokenization a document is converted to abag i.e.
a multiset of word tokens.
this is often referred toas thebag of wordsrepresentation.stemming is the process of converting a word to its baseform.
this base form is usually called the stem word.
forexample word argue argues argued and arguing have a common stem word argu .
even though word argu is not a dictionary word the conversion assures that we canidentify a word in its different forms and link these wordforms together.
without stemming the multiple forms wouldbe treated as different words altogether which is not desirablein many cases.
in our work we use the porter stemmer3tostem the words.
it employs several rule based heuristics toconvert a word to its stem word by stripping a suffix of theword.
the porter stemmer has been used in many past softwareengineering studies e.g.
.c.
vector space modelafter the text pre processing step the document is nowrepresented as a bag of words.
the vector space modelrepresents a bag of words as a vector of weights.
each word inthe bag becomes an element in the vector.
the weight of eachword indicates its importance.
term frequency and inversedocument frequency are often used to compute the weight ofa word and thus quantify its importance in a document.
frequency tf refers to the number of times a term i.e.
a word or a token appears in a document.
the moretimes a term appears in a document the more important thatterm is considered to be.
inverse document frequency idf isthe reciprocal of the document frequency df .
the documentfrequency of a term is the number of documents in the corpus i.e.
a set of documents or a document collection underconsideration e.g.
all feature requests all method descriptionsin the api documentation that contain the term.
the higherthe inverse document frequency is the more important is theterm as it can better differentiate one document from another.tf idf is often used to compute the weight of a termiin adocumentdconsidering a corpuscin the following way wi d c tfi d idfi cidfi c 1dfi c tfi drefers to the number of times a wordiappears in adocumentd.dfi crefers to the number of documents incthat contains the wordi.from the above given a bag of words representing a docu mentdin a corpusc we can convert it to its correspondingterm vector by computing the weight of each word incand putting them into a vector.
the weight of a word inc but not ind would be .
we denote the term vectorrepresentation of documentdconsidering a corpuscasvs mc d .
implementation wise a sparse vector represen tation is typically used i.e.
only the non zero entries of thevector are stored .292given two documents we can compute the similarity be tween them by comparing their representative vectors.
cosinesimilarity is often used to measure the similarity between twovectors.
letv1andv2denotes two vectors of weights of sizen then the cosine similarity of these two vectors is given bythe following equation cosine v1 v2 pni 1wi v1 wi v2qpni 1w2i v1qpni 1w2i v2 wi vrefers to theithweight in vectorv.iii.
overallframeworkthe overall framework of our approach is shown in figure .our framework consists of three important components his tory based recommender description based recommender andintegrator.history based recommendertakes as input the descriptionof the new feature request textual description and a his torical database containing old closed or resolved featurerequests historical feature request database hdb .
therecommender compares the new feature request with thosein the historical database and finds the closest ones.
it thenrecommends relevant methods based on the methods that wereused to implement those closest feature requests.description based recommendertakes as input the descrip tion of the new feature request textual description and thedocumentation of api libraries api documentations adoc .the recommender computes the similarity of the textualdescription of the new feature request with the descriptionof each method in the api documentations of the libraries.it recommends methods whose textual descriptions have thehighest similarity with the textual description of the newfeature request.integratorcombines the recommendations fromhistorybased recommenderanddescription based recommender.
ittakes as inputs recommendation scores from the two compo nents and outputs a final list of methods to be recommendedto the user.iv.
history basedrecommendationin the history based recommender component we first findthe nearest neighbors of a new feature request from thehistorical database of closed or resolved feature requeststhat we have.
we compare two feature requests based on thecontents of their summary description component reporter and priority fields see table i .
we compute a similarity scorefor each field and combine the scores into an aggregate scorethat specifies the similarity between two feature requests.
wedefine the similarity score for each field as follows.
summaryanddescription.
the contents of these fieldsare free form texts.
we pick only alphanumeric termsfrom these texts.
we employ standard text preprocessing tokenization and stemming and convert the terms intoa bag of word and its corresponding term vector seesection ii .
we have options we can take all terms inthe summary field we can take all terms in the descriptionfield and we can take all terms in both summary anddescription fields.
we compute the following similarityscores between the new feature requestf1and a historicalfeature requestf2in terms of their summary descriptionfields using cosine similarity simsum f1 f2 cosine vsmhdb fs1 vsmhdb fs2 simdesc f1 f2 cosine vsmhdb fd1 vsmhdb fd2 simsumdesc f1 f2 cosine vsmhdb fsd1 vsmhdb fsd2 in the above equations fs1denotes the content of thesummary field off1.fd1denotes the content of thedescription field off1.fsd1denotes the contents ofthe summary and description fields off1.hdbis thehistorical feature request database see figure .
component.
a feature request if implemented can affectmultiple components in the system.
thus the content ofthe component field of a feature request is a set of values.we compute the similarity scoresimcompbetween anew feature requestf1and a historical feature requestf2in terms of their components as follows simcomp f1 f2 ncf1 ncf2 p ncf1 p ncf2 ncfdenotes the set of components of feature requestf.
reporter.
the similarity scoresimreportbetween anew feature requestf1and a historical feature requestf2in terms of their reporters is if both of them havethe same reporter and otherwise.
priority.
each priority in jira can be assigned an ordinalvalue to quantify its level of urgency.
we assign value1 for blocker for critical for major for minor and for trivial .
a lower value means ahigher priority or level of urgency.
we compute thesimilarity scoresimpriobetween a new feature requestf1and a historical feature requestf2in terms of theirpriority based on these values.
the formula is as follows simprio f1 f2 priof1 priof2 priofdenotes the ordinal value corresponding to thepriority of feature requestf.example.consider the example feature request shown infigure as a historical feature request and a new featurerequest having values as shown in table iii.
we can computethe similarity between these two feature requests for each fieldas follows.
summaryanddescription.
since the computation stepsfor both summary and description are basically the same in this example we only compute the similarity scorefor the summary.
we convert a summary to a vectoroftf idfweights of its stemmed alphanumericalwords.
each word has a term frequencytfequal to .293historical feature request database hdb api documentation adoc textual description history based recommender description based recommender integrator ranked recommendation fig.
.
our recommendation frameworkassuming that theidfof each word is which meansthe word only appears in one document in the historicaldatabase of feature requests hdb the similarity scorefor the summaries of the historical and new featurerequest i.e.
simsum is1 p8 p6 .
.
component.
the historical feature request and the newfeature request do not share any component.
thus thesimcompscore is .
reporter.
the feature requests are reported by differentreporters so thesimreportscore is .
priority.
the historical feature request has critical priority which corresponds to the ordinal value whilethe new feature request has minor priority whichcorresponds to the ordinal value .
the denominator ofsimpriois equal to1 .
thus thesimprioscore is1 .
.table iiiexample ofanewfeaturerequestfield valuesid hbase 6372summary add scanner batching to export jobdescription when a single row is too large for the rs heap thenan oome can take out the entire rs.
setting scannerbatching in custom scans helps avoiding this scenario but for the supplied export job this is not set.
similar tohbase we can set the batching to a low number or if needed make it a command line option.components mapreducereporter lars georgepriority minorfinally to compute the final similarity score betweentwo feature requests we aggregate the similarity scoresof their constituent fields.
we compute the final similaritysimhistorybetween a new feature requestf1and ahistorical feature requestf2in the historical database usingthe following formula simhistory f1 f2 simsum f1 f2 simdesc f1 f2 simsumdesc f1 f2 simreport f1 f2 simcomp f1 f2 simprio f1 f2 simsum f1 f2 simdesc f1 f2 simsumdesc f1 f2 simreport f1 f2 simcomp f1 f2 andsimprio f1 f2 denote the similarity scores betweenf1 sandf2 s summary description combination of summary anddescription reporter components and priority respectively.
6denotes the weights of each field contributing to thesimhistoryscore.given a new feature request we rank the historical featurerequests in thehistorical feature request databasebased ontheirsimhistoryscores when compared to the new featurerequest.
the higher the score is the more similar a historicalfeature request is to the new feature request.
we then pick thetop kfeature requests with the highestsimhistoryscores.if there are feature requests with rank greater thankthat havethe same score as thek th feature request we group the featurerequests having this score.
we then randomly select featurerequests from this group until we haveknearest neighbors i.e.
ties are randomly broken .next we compute the recommendation scores for eachmethod based on these top knearest neighbors.
we collectthe methods that are used to implement the feature requestsin the top knearest neighbors and compute a score for eachmethod.
given a methodm thehistory based recommendationscoreof api methodmfor feature requestf denoted asrecscorehistory f m is computed as follows recscorehistory f m nncountmethod f m k nncountmethod m denotes the number of nearest neighborsof feature requestfthat use api methodm andkdenotesthe total number of nearest neighbors.
by default we set thenumber of nearest neighborskto be .
the api method withthe highestrecscorehistoryscore is the most suitable apimethod based on our history based recommender.example.consider a top nearest neighbor list containingn1andn2.
feature requestn1was implemented using methodm1andm2while feature requestn2was implemented usingmethodm2.
thus the value ofnncountmethodis form1and form2.
we can then computerecscorehistoryscoreofm1andm2which are .
and .
respectively.294v.
description basedrecommendationwhen adding a new feature to an application developersoften look at the api documentation to see which methodsthey can use to help them implement the feature.
the apidocumentation contains textual descriptions that explain eachmethod in the library.
by looking at the documentation devel opers can find out which api methods to use for implementinga feature.
our description based recommender componentmimics this process to find relevant methods given the textualdescription of a feature request.
given a new feature requestf it proceeds in the following steps feature request preprocessing.we extract the contentsof the summary and description fields of the input featurerequestf.
we again perform standard text preprocessingsteps to convert them into a bag of words.
this bagof word is then converted into its corresponding termvector representationvsmadoc f where each token i.e.
term in the bag is represented by its tf idf weight4andadocis api documentation see figure .
api method preprocessing.for each api methodmthat we consider we extract its method signature andits corresponding description in the api documentation.we extract the method descriptions from the javadoccomments in the code base of the apis.
we make useof eclipse java development tools jdt to extractthese javadoc comments.
javadoc has tags which serveas metadata.
examples include paramindicating thestart of the description of a method parameter returnindicating the start of the description of the return valueof a method etc.
since these tags only serve as metadataand are not specific to the api we remove them fromthe extracted javadoc comments.
additionally developerssometimes add html tags in the documentation toimprove its readability when it is viewed in e.g.
a webbrowser.
since these tags are only meant to improve thelook and feel of the api documentation and are again notspecific to the api we also remove all html tags.
next we perform standard text preprocessing i.e.
tokenizationand stemming to convert the cleaned method descriptionsin the javadoc comments into bags of words.
we thenconvert each bag of words into its corresponding termvector representation vsmadoc m .
similarity computation.next for each methodm we compute the similarity betweenvs madoc f andvs madoc m .
we use the cosine similarity to com pute the similarity between these two vectors seesection ii .
we refer to this similarity score as thedescription based recommendation scorebetween fea ture requestfand api methodmand denote it asrecscoredescription f m recscoredescription f m cosine vsmadoc f vsmadoc m 4the description of text preprocessing and vector space model is given insection ii.after the above steps we have therecscoredescriptionscores of various api methods.
the method with the high est score is the most relevant api method based on thedescription based recommender.vi.
unifyinghistory anddescription basedrecommendationthe last component in our framework is the in tegrator which combines the scores from the previ ous components.
we compute the final recommendationscorerecscorebetween feature requestfand apimethodmby combiningrecscorehistory f m andrecscoredescription f m as follows recscore f m recscorehistory f m recscoredescription f m and are the weights forrecscorehistoryandrecscoredescription respectively.to set the appropriate values for and ofrecscore see equation and the appropriate values for 6ofrecscorehistory see equation we heuristically find thebest set of weights that maximizes an evaluation metric basedon a training dataset we employ a greedy approach based ongibbs sampling that iteratively refines the set of weights.at each iteration each weight is optimized independently.
sev eral iterations are performed to further optimize the weights.the pseudocode of our approach to tune the set of weights isshown in figure .our algorithm takes the input historical feature requests aset of api documentations and the number of iterations toperform gibbs samplingnumiter.
it then outputs the set ofbest weights.
initially all weights are set to .
line .
we then iteratenumitertimes lines .
for each iteration we try to estimate the best weights independently lines .
we go through each of the eight weights and for eachweight we investigate settings i.e.
.
.
.
.
.
.
.
lines .
we pick the setting that give the best result lines .
methodevalevaluates how good a particularweight setting is with respect to an evaluation criteria line16 .
in this study we make use of recall rate k as the evaluation criteria see section vii .
atthe end of the above process we would have estimated thebest weights.in the end we want to get the top kmethods based onrecscore.
to do this we first get the set of methods withnon zerorecscorehistoryscores.
for all these methods we compute theirrecscorescores.
we then return the top kmethods based on therecscorescores.
if there are methodshaving the same score as thek th method we group themethods having this score and randomly select methods fromthis group until we havektop methods i.e.
ties are randomlybroken .
input freqs list of historical closed or resolved featurerequests3 docs the documentation of apis4 numiter number of iteration5 output estimated best weights method letweights .
.
.
.
.
.
.
.
letmaxevalscore letvalformax fori 0tonumiterdo12 forj 0toweights.lengthdo13 fork .0to0.0by .1do14 weights k15 evalscore eval freqs docs weights ifmaxevalscore evalscorethen17 maxevalscore evalscore18 valformax k19 end if20 end for21 weights valformax22 end for23 end for24 returnweightsfig.
.
pseudocode for our weight tuning algorithmvii.
experiments analysisin this section we first describe our dataset.
we then outlineour experimental methodology and research questions.
next we present the answers to the research questions and describesome threats to validity.a.
datasetwe first describe how we select libraries of interest andthe projects that we investigate.
next we describe the featurerequests that we use to evaluate our approach.
library selection we pick libraries that are frequentlyused by many projects of the apache foundation.
we chooseapache projects that use maven as their project managementtool.
maven includes a dependency management feature whichhelps us resolve the libraries used by the projects.
theselibraries have standard names in maven so it is easy to reliablymatch the libraries that are used across different projects.
wefirst download apache projects that use java as their mainprogramming language.
we then filter these projects basedon the existence of thepom.xmlfile in their root directory.thispom.xmlfile indicates the use of maven as the projectmanagement tool.
after this filtering process we have 207apache projects.
for each project we extract the librariesit depends on and count the number of projects using eachlibrary.
we then rank the libraries based on the number ofprojects using it.
we list the top libraries in table iv.
theseare the target libraries for our study we recommend methodsfrom these libraries.table ivtop mostpopularlibraries in207 apacheprojectsname descriptioncommons codec common encoder and decoder library for string url etccommons io common library for input output functionalitycommons lang common library providing extra methods for ma nipulating java core classescommons logging common library which encapsulates the loggingprocess for different logging implementationseasymock a library that provides easy way to use mock objectsin unit testingjunit unit testing frameworklog4j logging libraryservlet api library providing contracts between a servlet andthe runtime environmentslf4j api an abstraction library for various logging frame workslf4j log4j12 a binding library for slf4j and log4j2 project selection next we want to pick large projects lines of code from the apache founda tion projects whose feature requests we use to evaluate ourapproach.
we omit toy and small projects.
we filter outprojects that only use a few of the selected libraries.we choose these projects as we only recommend methodsfrom the libraries.
we also filter out projects that donot use jira issue management system.
we choose theseprojects as we need reliable links between bug reports andcorresponding commits in the version controls system.
theselinks are well maintained in jira issue management systems c.f.
.
table v lists the projects that we use in this studyand their descriptions.table vselectedapacheprojectsname descriptionaxis2 java server client web service enginecxf open source web service frameworkhadoop common common utilities used in other hadoop moduleshbase scalable distributed database based on big table struts enterprise ready web application framework3 feature request selection and gold standard extrac tion we pick feature requests from the jira issue manage ment system of the selected projects.
we pick only issues injira that are of relevant types.
as mentioned in section ii a there are issue types in jira that correspond to a featurerequest namely new feature improvement and wish .for these three issue types we pick issues that are either closed or resolved .jira contains explicit links between issue reports andrepository commits.
using these links we find the changedfiles for each commit that addresses an issue.
these files have apre change and a post change version.
we extract the methodsfrom the libraries shown in table iv that are invoked in thepost change version of the file as the evaluation benchmark296or gold standard c.f.
.
a good recommendation systemshould be able to recommend these methods.
there are threecases that we need to consider when extracting method callsfor gold standard file is added in the post change version.if the filedoes not exist in the pre change version we take all themethods from the libraries that are invoked in thepost change version as members of the gold standard.
file is changed in the post change version.if the fileexists in both pre change and post change versions wetake as the gold standard all the methods from the 10libraries that are invoked in the post change version butnot invoked in the pre change version.
file is deleted in the post change version.if the file isdeleted the file does not contribute any api method tothe gold standard.since we only recommend methods from the top li braries we only take feature requests whose gold standard setcontains at least one method from the libraries.
we ignorevery rare methods that are only used in one feature request.our history based approach requires a method to be used in atleast feature requests.
table vi shows the number of featurerequests for each of the projects that we use in this study.table vinumber offeaturerequests inourdatasetproject feature requestaxis2 java 108cxf 106hadoop common 79hbase 161struts 53total 507b.
methodology research questionsin order to measure the effectiveness of our approach weuse a commonly used evaluation measure namelyaveragerecall rate k .recall rate khas a value of either or 0wherekis the number of the returned items.
it has value if atleast one of thekreturned items i.e.
recommended method is a member of the gold standard and otherwise.
we userecall rate kas it has also been used in many past studiesthat also analyze entries in issue management systems .for each project we perform stratified ten fold cross valida tions to evaluate the effectiveness of our approach.
we dividethe feature requests of a project randomly into ten groups ofroughly equal sizes and then perform ten iterations.
foreach iteration one group is used as the test data i.e.
theyform the set of new feature requests and the remaining ninegroups are combined to be the training data i.e.
they form thehistorical feature request database hdb .
the test data isused to evaluate the effectiveness of our approach.
we reportthe average effectiveness over the ten iterations.we consider the following research questions rq1 what is the effectiveness of our proposed approach?rq2 what are the relative contributions of the variouscomponents of our approach?rq3 how effective is our approach compared to a state of the art code search based approach in recom mending relevant methods for a feature request?c.
experimental resultswe describe the answers to each of the research questionsbelow.
rq1 effectiveness of the proposed approach table viishows the effectiveness of our approach.
the average recall rate and recall rate are .
and .
respectively.we show that by only returning methods our approach cancorrectly recommend relevant methods for .
to .
of the feature requests in a project.
in other words ourapproach can put relevant methods in high ranking positions.if we increase the recommendation size to be methods our approach can correctly recommend at least one relevantmethod for .
to .
of the feature requests.table viieffectiveness ofourapproachproject recall rate recall rate 10axis2 java .
.908cxf .
.725hadoop common .
.709hbase .
.839struts .
.713overall .
.
rq2 relative contributions our proposed approachhas two main components the history based recommenderand the description based recommender.
our history basedrecommender computes the similarity between two featurerequests by aggregating similarity scores based on summary description summary description reporter component andpriority .
in this research question we want to investigatethe relative contributions of the various components and sub components of our approach.we employ gibbs sampling to tune weights to yielda semi optimal setting.
thus we can estimate the contri butions of the various components and sub components ofour approach from their corresponding weights.
the averagevalues of the weights across the ten fold cross validationperformed for computing recall rate and recall rate areshown in figure .
we find that all components and sub components of our approach are important as none of themis given a weight of zero.
for bothk 5andk 1has the lowest weight compared to the other parameters suggesting that it is less useful than the other informationleveraged by our approach.
in our approach each recom mended method needs to have a non zerorecscorehistoryscore see section vi .
however this does not mean thattherecscoredescriptionscore is not useful.
indeed wenote that the weight of our description based recommendation297score description is higher than that of our history basedrecommendation score historical .
this indicates that amongmethods with non zerorecscorehistoryscore we can userecscoredescriptionscores to rank them.
rq3 comparison with a code search based approach code search can also be used to recommend relevant apimethods.
chan et al.
propose a graph based approach thatcan search an api library for relevant methods given a setof text phrases .
to the best of our knowledge this isthe closest study to our work.
their approach processes thetext queries and returns a connected graph whose nodes aremethods.
they have evaluated their approach on a set ofprecisetext queries that contain keywords that match desiredmethods.
for example for input queries containing phrases store folder open and search they output several rele vant methods including javax.mail.store getdefaultfolder javax.mail.folder open int mode etc.
note that the inputqueries contain keywords that must appear in the signatures ofthe relevant methods.
we want to investigate if their approachcan also handle feature requests.to do this we preprocess a feature request into text phrases.we treat each word that appears in the summary and de scription fields of a feature request as a text phrase.
wethen run their tool on our processed data.
table ix show theaverage number of methods that are returned in the connectedgraphs outputted by their tool.
even though the tool returns anumber of methods unfortunately none of them are relevantfor each of the feature requests i.e.
their recall rate 5and recall rate are both .
this shows that approachesthat process precise text queries cannot handle feature requests.indeed feature requests often contain high level requirementswhile methods contain low level requirements.
our proposedapproach tackles this problem by leveraging historical featurerequests.table ixaverage ofreturnedmethods bychan et al.
sapproachproject average of returned methodsaxis2 java .2cxf .8hadoop common .7hbase .8struts .7average .84d.
threats to validitythreats to internal validity refers to experimental bias anderrors.
we have checked our code and data for errors.
stillthere could be errors that we have not noticed.
we alsoensure that we do not mix training and test data in our crossvalidation.threats to external validity refers to the generalizability ofour proposed approach.
in this study to address this threat we have considered a few hundred feature requests from 5software systems.
we have also recommended methods from10 libraries.
in the future we plan to reduce this threat furtherby analyzing more feature requests from additional softwaresystems and recommend methods from more libraries.
wehave also performed cross validation which is the standardapproach to assess how a proposed approach would performon an independent dataset.threats to construct validity refers to the suitability of ourevaluation metrics.
we make use of recall rate k which is acommonly used metric in many past studies .
thus we believe there is little threat to constructvalidity.viii.
relatedworkmandelin et al.
propose the tool prospector which recom mends objects and method calls referred to as jungloids toconvert an object of a particular type to an object of anothertype .
prospector takes as input a query consisting of apair of the input type and the output type.
it then analyzessignatures of api methods and constructs a signature graphto recommend jungloids based on the query.
a jungloid isranked based on the number of methods it contains and theoutput type.
thummalapenta and xie investigate the sameproblem .
however the make use of a code search engineto solve the problem.
the code search engine is used tocollect code examples which are then analyzed to recoverthe method sequences.
while the approach by mandelin etal.
analyzes library code the approach by thummalapentaet al.
analyzes client code retrieved by code search engines.our work differs in several respects we consider differentproblems method recommendation given a feature request vs.method recommendation given an input output type pair andwe leverage different resources historical feature requests api documentation vs. library code or client code returned bycode search engine .bruch et al.
propose a code completion system that recom mends method calls by looking for code snippets in existingcode repositories that share a similar context as the contextthat a developer is working on .
they propose three codecompletion systems based on frequency of method call usage an association rule mining algorithm and a k nearest neighboralgorithm.
the k nearest neighbor algorithm performs the best.our work differs in several respects we consider differentproblems method recommendation given a feature requestvs.
method recommendation given a code context and weleverage different resources historical feature requests apidocumentation vs. code repositories .robillard proposes a technique suade that recommendsmethods or other program elements of interest to help devel opers perform a software maintenance task .
suade takesas input a set of program elements and outputs other programelements that potentially interest developers.
it works byinvestigating the structural dependencies of program elementsand considers two criteria specificity and reinforcement.
aprogram element of interest needs to be specific enough tothe input set and its relationship to the input set is reinforcedby existing relationships among program elements in the input298table viiiaverageweights fork 5and10project kweight axis2 java .
.
.
.
.
.
.
.70cxf .
.
.
.
.
.
.
.81hadoop common .
.
.
.
.
.
.
.60hbase .
.
.
.
.
.
.
.70struts .
.
.
.
.
.
.
.71average .
.
.
.
.
.
.
.704axis2 java .
.
.
.
.
.
.
.80cxf .
.
.
.
.
.
.
.60hadoop common .
.
.
.
.
.
.
.51hbase .
.
.
.
.
.
.
.10struts .
.
.
.
.
.
.
.00average .
.
.
.
.
.
.
.602set.
saul et al.
addresses a similar problem .
their goalis to recommend a set of methods that are related to a targetmethod.
to achieve this goal structural information in a callgraph is analyzed.
they propose a new algorithm namedfran which performs random walk on the callgraph.
severalother approaches recommend methods related to a targetmethod using static analysis .
long et al.
proposealtair that recommends method based on variables that areshared among related methods .
zhang et al.
enhance acall graph with control flow information and use it for methodrecommendation .
in this work we consider a differentproblem method recommendation given a feature request vs.method recommendation given a set of methods of interest and leverage a different set of resources historical featurerequests api documentation vs. structural dependencies .the closest work to ours is that of chan et al.
whichrecommends api methods given textual phrases .
givena query expressed as a set of textual phrases their approachreturns a connected api subgraph.
for this they create anapi graph which is an undirected graph with nodes corre sponding to classes and methods and edges corresponding torelationships between them e.g.
inheritance input output and membership .
each node of the graph contains words thatappear in the corresponding method.
based on this api graphand an input query they mine a subgraph that maximizes aparticular objective function.
the approach is evaluated ona small number of short text phrases.
as we have found insection vii chan et al.
s approach performs poorly on featurerequests which are both longer than the phrases considered bychan et al.
and are less closely related to the actual code.
ourapproach is able to treat feature requests due to the inclusionof information about historical feature requests in the history based recommendation component.ix.
conclusion andfutureworkin this work we have proposed a new method recommen dation approach that takes as input a feature request andrecommends methods from a set of libraries.
in contrast toprevious approaches our approach does not require preciseinput information such as precise input or output types orprecise matching textual descriptions.
thus it is suitable fordirectly processing feature requests stored in bug repositories which often do not precisely specify relevant code elements.our approach is a hybrid approach combining history basedrecommendation and description based recommendation.
on10 libraries and feature requests from software systemswe achieve an average recall rate and recall rate of0.
and .
respectively.
we have also compared ourapproach to the latest method recommendation technique thatrequires precise textual descriptions from end users and showthat it is not useful for recommending methods from featurerequests.in the future we plan to improve our solution furtherto achieve even higher recall rate k scores.
some possibledirections include using state of the art natural language pro cessing taking the information stored in the codebase into account and specification mining e.g.
.
we also plan to experimentwith a larger number of feature requests from more softwaresystems and to perform a more thorough investigation of thefactors that affect the effectiveness of the different componentsof our approach in contributing towards the effectiveness of theproposed solution.
finally to improve the practical usefulnessof our approach we plan to integrate our proposed solutioninto an ide e.g.
eclipse and evaluate the resulting toolby means of a user study.
we also want to investigate theeffectiveness of our approach under different experimentalsettings e.g.
evaluating on projects with limited number offeature requests cross validating across projects etc.to extend our approach we would like to consider howto enable users to specify some constraints to be taken intoconsideration in the recommendation process.
we will alsoconsider whether our proposed approach can be effective forbug reports in addition to feature requests.acknowledgementwe would like to thank wing kwan chan and hong chengfor providing us with the source code of their tool forrecommending api methods from precise text phrases.
thisproject is partly supported by merlion ms12c0014 grant.299references t. f. bissyand e f. thung s. wang d. lo l. jiang and l. r eveill ere empirical evaluation of bug linking incsmr pp.
.
d. m. blei a. y. ng and m. i. jordan latent dirichlet allocation journal of machine learning research vol.
pp.
.
m. bruch m. monperrus and m. mezini learning from examples toimprove code completion systems inesec sigsoft fse pp.
.
g. casella and e. i. george explaining the gibbs sampler theamerican statistician no.
pp.
.
w. k. chan h. cheng and d. lo searching connected api subgraphvia text phrases insigsoft fse p. .
f. chang j. dean s. ghemawat w. c. hsieh d. a. wallach m. bur rows t. chandra a. fikes and r. e. gruber bigtable a distributedstorage system for structured data acm trans.
comput.
syst.
vol.
no.
.
v. dallmeier n. knopp c. mallon g. fraser s. hack and a. zeller automatically generating test cases for specification mining ieeetrans.
software eng.
vol.
no.
pp.
.
b. dit m. revelle and d. poshyvanyk integrating informationretrieval execution and link analysis algorithms to improve featurelocation in software empirical software engineering vol.
no.
pp.
.
g. gay s. haiduc a. marcus and t. menzies on the use of relevancefeedback in ir based concept location inicsm pp.
.
e. hill l. l. pollock and k. vijay shanker improving source codesearch with natural language phrasal representations of method signa tures inase pp.
.
n. jalbert and w. weimer automated duplicate detection for bugtracking systems indsn pp.
.
s. kumar s. c. khoo a. roychoudhury and d. lo mining messagesequence graphs inicse pp.
.
d. lo and s. c. khoo smartic towards building an accurate robustand scalable specification miner insigsoft fse pp.
.
d. lo s. c. khoo and c. liu mining past time temporal rules fromexecution traces inwoda pp.
.
d. lo and s. maoz specification mining of symbolic scenario basedmodels inpaste pp.
.
mining hierarchical scenario based specifications inase pp.
.
scenario based and value based specification mining bettertogether inase pp.
.
d. lo g. ramalingam v. p. ranganath and k. vaswani miningquantified temporal rules formalism algorithms and evaluation inwcre pp.
.
f. long x. wang and y. cai api hyperlinking via structural overlap inesec sigsoft fse pp.
.
d. mandelin l. xu r. bod k and d. kimelman jungloid mining helping to navigate the api jungle inpldi pp.
.
c. manning p. raghavan and h. schutze introduction to informationretrieval.
cambridge university press cambridge vol.
.
a. t. nguyen t. t. nguyen t. n. nguyen d. lo and c. sun duplicate bug report detection with a combination of informationretrieval and topic modeling inase pp.
.
d. poshyvanyk y. g. gueheneuc a. marcus and g. antoniol featurelocation using probabilistic ranking of methods based on executionscenarios and information retrieval intse .
s. rao and a. c. kak retrieval from software libraries for buglocalization a comparative study of generic and composite text models inmsr pp.
.
m. p. robillard automatic generation of suggestions for programinvestigation inesec sigsoft fse pp.
.
p. runeson m. alexandersson and o. nyholm detection of duplicatedefect reports using natural language processing inicse pp.
.
z. m. saul v. filkov p. t. devanbu and c. bird recommendingrandom walks inesec sigsoft fse pp.
.
s. shoham e. yahav s. j. fink and m. pistoia static specificationmining using automata based abstractions ieee trans.
software eng.
vol.
no.
pp.
.
c. sun d. lo x. wang j. jiang and s. c. khoo a discriminativemodel approach for accurate duplicate bug report retrieval inicse pp.
.
c. sun d. lo s. c. khoo and j. jiang towards more accurateretrieval of duplicate bug reports inase .
s. thummalapenta and t. xie parseweb a programmer assistant forreusing open source code on the web inase pp.
.
s. wang d. lo z. xing and l. jiang concern localization usinginformation retrieval an empirical study on linux kernel inwcre pp.
.
x. wang d. lo j. jiang l. zhang and h. mei extracting para phrases of technical terms from noisy parallel software corpora inacl ijcnlp pp.
.
x. wang l. zhang t. xie j. anvik and j. sun an approach todetecting duplicate bug reports using natural language and executioninformation inicse pp.
.
q. zhang w. zheng and m. r. lyu flow augmented call graph anew foundation for taming api complexity infase pp.
.
w. zhao l. zhang y. liu j. sun and f. yang sniafl towards astatic noninteractive approach to feature location intosem .
j. zhou h. zhang and d. lo where should the bugs be fixed?more accurate information retrieval based bug localization based on bugreports inicse pp.
.