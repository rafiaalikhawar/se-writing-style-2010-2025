uncertainty risk and information value in software requirements and architecture emmanuel letier david stefan earl t. barr department of computer science university college london london united kingdom e.letier d.stefan e.barr ucl.ac.uk abstract uncertainty complicates early requirements and architecture decisions and may expose a software project to significant risk.
yet software architects lack support for evaluating uncertainty its impact on risk and the value of reducing uncertainty before making critical decisions.
we propose to apply decision analysis and multi objective optimisation techniques to provide such support.
we present a systematic method allowing software architects to describe uncertainty about the impact of alternatives on stakeholders goals to calculate the consequences of uncertainty through monte carlo simulation to shortlist candidate architectures based on expected costs benefits and risks and to assess the value of obtaining additional information before deciding.
we demonstrate our method on the design of a system for coordinating emergency response teams.
our approach highlights the need for requirements engineering and software cost estimation methods to disclose uncertainty instead of hiding it.
categories and subject descriptors d2.
software architectures general terms design economics theory keywords software engineering decision analysis .
introduction uncertainty is inevitable in software engineering.
it is particularly present in the early stages of software development when an organisation needs to make strategic decisions about which it projects to fund or when software architects need to make decisions about the overall organisation of a software system.
in general these decisions aim at maximising the benefits that the software system will bring to its stakeholders subject to cost and time constraints.
uncertainty includes uncertainty about stakeholders goals and their priorities about the impact of alternatives on these goals about permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
icse may june hyderabad india copyright acm ... .
.the feasibility cost and duration of implementing the alternatives about future changes in stakeholders goals business context and technological environments and finally uncertainty about whether the right questions about decisions are even being asked and all their options identified.
in a decision problem uncertainty is a lack of complete knowledge about the actual consequences of alternatives.
for example software architects may be uncertain about the cost and performance impact of a proposed software architecture.
given their current knowledge they might estimate the cost to be between 1m to 3m and the achievable response time to be between and seconds.
a risk exists when the possible consequences of a decision include undesirable outcomes like loss or disaster .
continuing the example selecting the proposed architecture might carry the risks of the development costs exceeding 2m and the response time not achieving the minimally acceptable target of seconds.
in software architecture decisions the risks include selecting an architecture that is too expensive to develop operate and maintain that is delivered too late and most importantly that fails to deliver the expected benefits to its stakeholders.
numerous studies have shown that these risks are severely underestimated .
this is not surprising uncertainty and risks are rarely considered explicitly in software engineering decisions and the software engineering literature offers no principled approaches to deal with them.
in this paper we focus on early requirements and architecture decisions i.e.
decisions about the functionality the software should provide the quality requirements it should satisfy its organisation into components and connectors and its deployment topology.
we assume stakeholders goals and the alternatives have been identified using appropriate requirements engineering and software architecture methods .
our objective is to support reasoning about uncertainty concerning the impact of alternatives on stakeholders goals.
previous work dealing with uncertainty in early requirements and architecture decisions suffers important limitations they use unreliable methods for eliciting uncertainties some confuse group consensus with certainty they tend to evaluate alternatives against vague unfalsifiable criteria they provide no information about the risks that accompany uncertainty and they provide no support for assessing to what extent obtaining additional information before making a decision could reduce these risks.
we address these limitations by adapting concepts and techniques from statistical decision analysis to the problems of early requirements and architecture design decisions.
decision analysis is a discipline aiming at supporting complex decisions under uncertainty with systematic methods and mathematical tools for understanding formalising analysing and providing insights about the decision problem .
decision analysis is used notably in the health carepermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
copyright is held by the author owner s .
publication rights licensed to acm.
icse may june hyderabad india acm domain to inform decisions about the cost effectiveness of new medical treatments based on the results of clinical trials .
there are exceptional uses of these methods in the context of it investment decisions but despite their relevance to early requirements engineering and architecture decisions they have been largely ignored by the software engineering community.
our approach to early requirements and architecture decisions consists in formalising the decision problem in terms domain specific measurable goals to elicit and represent uncertainties as probability distributions to simulate the impact of alternatives on goals through monte carlo mc simulations and to shortlist a set of alternatives using pareto based multi objective optimisation techniques.
we introduce the software engineering community to the value of information a powerful notion from decision analysis that allows a decision maker faced with uncertainty to measure those uncertainties and determine which would be most profitably reduced.
the paper s main contribution is a systematic method for applying statistical decision analysis techniques to early requirements and architecture decision problems section .
by developing this method we were also led to make the following contributions .we define novel decision risk metrics tailored for requirements and architecture decision problems section .
.we extend the concept of value of information traditionally defined in terms of impact of additional information on expected outcomes only by considering how additional information reduces risk section .
.
.we introduce the concept of pareto optimal strip a generalisation of a pareto optimal front designed to resist modelling and measurement errors present in multi objective decisoin problem under uncertainty section .
we have developed a tool supporting our approach and have applied it to data from a real system from the literature .
our tool and all models discussed in this paper are available at ucl.ac.uk staff e.letier sdda .
.
cost benefit analysis under uncertainty before considering early requirements and architecture decision problems we first consider the simpler problem of selecting one alternative among a set of alternatives based on their costs and benefits.
such problem assume a model exists to calculate the costs and benefits of all alternatives in a common unit which is usually monetary e.g.pound euro dollar yen or rupee definition.
acost benefit decision model comprises a set aof alternatives a set of model parameters and two functions cost a andbenefit a that return the cost and benefit of alternative a given the parameter values .
the net benefit of an alternative is then nb a benefit a cost a .
to simplify the notation we sometimes leave the model parameters implicit and write nb a fornb a and similarly benefit a andcost a .
example.
an engineering firm is considering replacing an ageing computer aided design cad application with a new system.
the set of alternatives is a legacy new .
the cad application helps the firm design complicated engineering artefacts e.g.turbines aircraft engines etc.
that it sells to clients.
the benefits associated with each alternative a ais a function of several variables such as the market size the market share that each alternative might help achieving which itself is a function of features of each cad.
likewise the cost associated with each alternative is a function of several parameters such as the development maintenance and operational costs.
the cost and benefit functions would typically also include concerns related to incremental benefit delivery cashflow and discount factors .
the model parameters are the variables in these equations i.e.those that are not further defined in terms of other variables.
to keep our illustrative example simple we will hide the details of the cost and benefit functions and discuss decisions based on the results of these functions only.
.
computing expected net benefit and risk traditional cost benefit anlysis cba computes the net benefit of each alternative using point estimates exact numbers instead of ranges for each of a model s parameter.
such approaches therefore ignore the often large uncertainty about parameter values.
uncertainty about cost and benefit exists but is hidden.
in a statistical cba uncertainty about the model parameters is modelled explicitly as probability distributions and used to compute the probability distribution of net benefit for each alternative.
simple effective methods exist for eliciting the model parameters probability distributions from decision makers .
these methods have sound mathematical foundations and are based on significant empirical studies of how uncertainty can be reliably elicited from humans.
we will not be concerned with these methods in this paper beyond noting that they can and should be used to elicit reliable probability distributions from domain experts and decision makers.
once the model parameters probability distributions have been estimated one needs to compute the probability distributions for the cost benefit and net benefit of each alternative.
it is generally not possible to compute these probability distributions analytically because the model equations and parameters probability distributions can be arbitrarily complicated.
monte carlo mc simulations can however compute good approximations.
the underlying principle is to sample a large number of simulation scenarios generated by model parameter values drawn from their probability distributions and use them to compute the net benefit in that scenario.
the result of a mc simulation of a cost benefit decision model is a m n matrix hatwidernbwhere mis the number of simulated scenarios and nis the number of alternatives in a. the element hatwidernb denotes the net benefit for alternative jin the ithscenario.
from the result of a mc simulation one can for each alternative estimate measures of interest to decision makers such as the expected net benefit enb loss probability lp and probable loss magnitude plm defined as follows enb a e lp a p nb a plm a e where e denotes the expectation of a random variable x. example.
figure shows the results of a statistical cba for our illustrative example.
we assume cost and benefit have a normal distribution truncated at zero.
figure 1a shows the mean and confidence interval of these distributions.
a confidence interval means that decision makers believe there is a chance that the actual costs and benefits will fall within these ranges.
figure 1b shows the resulting expected net benefit loss probability and probable loss magnitude of each alternative.
it shows developing the new cad has a high expected net benefit but also high risks in terms of loss probability and probable loss magnitude.
in a traditional cba these risks would not have been quantified and would most likely have been underestimated if not entirely ignored.
.
the expected value of information if before making a decision decision makers could pay someone to obtain additional information that reduce uncertainty about the cost and benefits of alternatives how much would that information884mean ci benefit new 5m cost new 3m benefit legacy 1m cost legacy a mean and confidence intervals ci .new legacy enb 2m 1m lp plm .4m b expected net benefit enb loss probability lp and probable loss magnitude plm .evppi ppi lp bene f it new .54m cost new .14m bene f it legacy cost legacy c information value analysis.
the value of total perfect information ev t pi is .64m.
figure statistical cost benefit analysis for deciding whether to replace a legacy application by a new system.
be worth to them?
it is possible to answer this question by computing the expected value of information .
intuitively information that reduces uncertainty may lead decision makers to select an other alternative with highest expected net benefit than the alternative they would select without additional information.
the expected value of information is the expected gain in net benefit between the selected alternatives with and without the additional information.
the expected value of information for the different model variables tells decision makers to focus on reducing uncertainty about information with high expected value and to avoid wasting effort reducing uncertainty about information with low expected value or at least not pay more for information than its expected value .
computing the expected value of information can yield surprising results.
hubbard reports he has applied information value theory to it project business cases each having between to variables and observed the following pattern the majority of variables had an information value of zero the variables that had high information value were routinely those that the client never measured the variables that clients used to spend the most time measuring were usually those with a very low even zero information value .
the contrast between the second and third observations constitutes what hubbard has called the it measurement inversion paradox .
he cites the large effort spent by one of his clients on function point analysis a popular software development productivity and cost estimation method as an example of measurement with very low information value because its cost estimation were not more accurate or precise than the project managers initial estimates.
the expected value of information is defined with respect to an outcome to be maximised and assumes a default decision strategy of maximising expected outcome.
in this section the outcome to be maximised is the net benefit nbbut the definition applies to any other outcome e.g.maximising software reliability.
information is valued in the same units as the outcome that it measures.
this makes measuring information value with respect to net benefit particularly attractive as it assigns a financial value to information.
the expected value of total perfect information evtpi is the expected gain in net benefit from using perfect information about all model parameters evtpi e max a anb a max a ae .
in this definition the second term denotes the highest expected net benefit given the current uncertainty about the model parameters and the first term the expectation over all possible values for the parameters of the highest net benefit when the parameter values are in other words the expected net benefit from obtaining perfect information .
observe how the two terms invert the application of the expectation and maximisation operators.
it can be shown that evtpi is always positive or zero.
the evtpi can be estimated from the output hatwidernbof a mc simulation evtpi mean i ..nmax j ..m hatwidernb max j ..mmean i ..n hatwidernb .
as an illustration figure shows how evtpi is computed froma small mc simulation with scenarios the actual mc simulation used to produce the results in figure consists of 104scenarios .
information value theory also defines the expected value of partial perfect information i.e.perfect information about a subset of the model parameters and the expected value of partial or total imperfect information i.e.information that reduces uncertainty but without completely eliminating it .
in this paper we only use the expected value of perfect information either in total over all parameters or about a single model parameter.
the expected value of imperfect information and of perfect information about sets of parameters are harder to compute and they may not yield substantial practical benefits over simpler information value analysis.
the expected value of partial perfect information about a single model parameter noted evppi is the expected gain in net benefit from using perfect information about evppi e max a af a max a ae where f a e nb a is the expected nbof alternative a conditioned on the parameter fixed at and e denotes the expectation with respect to all model parameters in except .
the intuition of this definition is similar to that of evtpi.
as evtpi it can be shown that evppi is always positive or zero.
computing evppi is harder than computing evtpi .
in this paper we rely on a recent efficient algorithm that computes evppi by taking as input only the pair an bracketle t hatwide hatwidernb an bracketri htof simulations for the model parameter and the corresponding matrix of nbsimulations generated by the mc simulation .
this algorithm first finds a suitable segmentation of values in hatwide such that within each segment the differences in maximal expected nbremain small.
for each segment it computes the average gain in nbfrom knowing then averages these average gains weighted by the proportion of simulation that falls into each segment.
note that evtpi andevppi compute the expected value of information about some parameters before the value for these parameters are revealed .
once the actual values are revealed they may increase or decrease expected net benefit.
the evtpi andevppi merely compute how much the expected net benefit will change on average .
it is these averages that are always positive or zero.
the revelation of new information can also both increase or decrease uncertainty about the parameters true values.
when this happens an increase of uncertainty is most likely caused by a failure to mitigate overconfidence biases during the elicitation of probability distributions.
for example decision makers with overconfidence biases will express confidence intervals that are narrower than their true uncertainty.
overconfidence bias is a serious problem because the computations of expected net benefit risk and information value all assume the initial probability distributions are accurate.
this observation reinforces the importance of using appropriate uncertainty elicitation techniques designed to reduce the effects of overconfidence and other biases .
measuring expected information value is an alternative to sensitivity analysis.
there are several variants of sensitivity analysis.885scenarios hatwidernb new hatwidernb legacy max .33m .03m .33m .13m .96m .96m .05m .00m .05m .13m .06m .13m .39m .07m .07m mean .05m .02m .71m figure illustration of a mc simulation and computation of evtpi.
the second and third columns show the hatwidernbfor the new and legacy applications in random scenarios.
over these five scenarios the new application has the highest expected net benefit .05m .
the fourth column show the maximal possible net benefit in each scenario and its mean value over all five scenarios .71m .
we thus have evtpi .71m .05m .66m.
possibly the most common in software engineering consists in measuring for each model parameter taken individually the change of nb or other objective of interest for a selected alternative when the parameter varies between some low and high value .
a parameter is then said to have high sensitivity if the changes in nbare high.
the expected value of information differs from sensitivity analysis in that it takes into account the probability of changes in parameters values and possible changes of alternatives to optimise net benefit.
these differences have important implications a parameter with high sensitivity may have low information value if it has a low probability of change and vice versa a parameter with low sensitivity may have high information value if a highly probable change for this parameter leads to selecting a different alternative with much higher nb.
felli and hazen provide a more detailed analysis of the benefits of measuring expected value of perfect information over sensitivity analysis .
.
the impact of information on risk using additional information to maximise expected nbimpacts risk sometimes favourably as when selecting an alternative with highest expected nbalso reduces risk or unfavourably as when selecting an alternative with highest nbincreases risk.
measuring this impact gives decision makers additional information about the value of seeking additional information.
we have thus defined a new measure of the expected impact of perfect information on risk.
in our cost benefit analysis risk is measured by the loss probability and the probable loss magnitude.
to keep the exposition simple we define the impact of perfect information on risk with respect the a risk measure risk a p f a where f a is true when alternative afails when parameter values are .
for example for lp a f a isnb a .
our definition can easily be extended to risk measures such as plm defined over real valued rather than boolean ffunctions.
leta be an alternative that maximises expected nb.
if there is more than one alternative with equal highest expected nb a is one with minimal risk.
let a anda be alternatives that maximise nbwhen or respectively.
the expected impact of total respectively partial perfect information on risk is the expected difference between risk a respectively risk a andrisk a tpi risk e risk a ppi risk e risk a .
the tpi risk can be estimated from matrices hatwidernband hatwidefgener ated during the monte carlo simulation tpi risk mean i ..n mean i ..n .
where which.max j ..m hatwidernb denotes the column indices of the alternative with highest benefit in row i. to compute ppi risk we have extended the algorithm for computing evppi from the monte carlo simulation data an bracketle t hatwide hatwidernb hatwidef an bracketri ht.
our extension applies the same principle as the one used to compute tpi risk to compute the inrisk in each segment of values then returns the weighted average of those over all segments.
example figure c shows the expected value of information in our illustrative example.
the evtpi is .64m of expected net benefit.
measuring evppi shows that reducing uncertainty about the new application s benefits has high value and reduces most of the risks whereas reducing uncertainty about its cost has almost no value and little impact in reducing loss probability.
.
software design decisions under uncertainty software design decisions are usually more complex than the simple cost benefit decision problems of the previous section.
complexity arises in the solution space in the objective space and in the models that relate the two.
in the solution space instead of involving the selection of one alternative from a set they typically involve a multitude of interrelated design decisions concerning choices among alternative architectural styles design patterns technologies and responsibility assignments .
this leads to an exponential increase in the number of candidate solutions for example if the problem involves 10design decisions with options each the number of candidate architectures is around .
the solution space for software design decisions is therefore several orders of magnitudes larger than the solution spaces of other domains applying decision analysis techniques for example in healthcare economics the solution space rarely exceeds different treatment options .
in the objective space software design decisions typically involve multiple goals that are generally conflicting hard to define precisely and not easily comparable unlike cost and benefit they have different units of measure .
examples of goals include concerns related to security performance reliability usability and the improved business outcomes generated by the software.
clarifying these goals and understanding their trade offs is a significant part of supporting software design decisions.
the goals in healthcare decision problems are at least as complex as software design decision goals.
there has however been a much greater effort at defining these goals and their trade offs than for software engineering problems.
this has resulted in measures such as the quality adjusted life year used to compare alternative treatment options .
the models relating the design decision options to stakeholders goals are often hard to build validate and include a very large number of parameters.
they are typically composed of models of the software system to evaluate the impact of software design decisions on software qualities such as its performance and reliability and models of the application domain to evaluate the impact of software and system design decisions on stakeholders goals .
to deal with this complexity we propose the following process .
defining the architecture decision model .
defining a cost benefit decision model .
defining the decision risks8864.
eliciting parameters values .
shortlisting candidate architectures .
identifying closed and open design decisions .
computing expected information value steps and correspond to standard model elaboration activities performed notably in the atam and cbam approaches.
steps and are specific to architecture decisions under uncertainty.
step extends pareto based muliobjective optimisation techniques to decisions under uncertainty.
step identifies closed and open design decisions from this shortlist.
step computes expected information values.
at the end of these steps if some model parameters or variables have high expected information value software architects may choose to elicit further information and refine corresponding parts of their models to improve their decisions and reduce their risks.
in practice some of these steps may be intertwined.
for example the elaboration of the architecture decision model and the cost benefit model in steps and are likely to be interleaved rather than performed sequentially .
sas case study.
we apply our method on a case study of software architecture decisions presented at icse .
the software to be designed is a situational awarness system sas whose purpose is to support the deployment of personnel in emergency response scenarios such as natural disasters or large scale riots.
sas applications would run on android devices carried by emergency crews and would allow them to share and obtain an assessment of the situation in real time e.g.
interactive overlay on maps and to coordinate with one another e.g.
send reports chat and share video streams .
a team of academics and engineers from a government agency previously identified a set of design decisions options and goals to be achieved by this system see figure .
they also defined models for computing the impact of options on the goals and documented uncertainty about model parameters using three point estimates a method commonly used by engineers and project managers that consists in eliciting a pessimistic most likely and optimistic value for each model parameter.
they then applied a fuzzy logic based approach called guidearch to support design decisions under uncertainty .
to facilitate comparison between the approaches we will apply our method on the same model and data as those used by the guidearch method .
.
defining the architecture decision model the first step consists in identifying the decisions to be taken together with their options defining the goals against which to evaluate the decisions and developing a decision model relating alternative options to the goals .
the result is a multiobjective architecture decision model moadm .
definition.
amulti objective architecture decision model is a tuple d c g v where dis a set of design decisions where each decision d dhas several options od acandidate architecture is a function a d d dodthat maps each decision dto a single option inod the set of all candidate architectures is noted a1 cis a set predicates capturing dependency constraints between design decisions such as prerequisite mutual exclusion and mutual inclusion relations is a set of model parameters gis a set of optimisation goals partitioned into g and g1throughout the paper we use the term option to denote an alternative for a design decision and the term alternative to denote an alternative candidate architecture in the design space a.decisions options location findinggps radio triangulation file sharingopenintents in house report syncingexplicit implicit chat protocolxmpp open fire in house map accesson demand google cached on server preloaded esri hardware platformnexus i htc droid motorola connectivitywi fi 3g on nexus i 3g on droid bluetooth databasemysql sqlite architectural patternfacade peer to peer push based data exchange formatxml compressed xml unformatted datagoals battery usage response time reliability ramp up time cost development time deployment time figure overview of the sas case study .
denoting goals to be maximised and minimized respectively vis a goal evaluation function such that v g a is a real value denoting the level of attainment of goal gby candidate architecture awhen the model parameters have the concrete values .
optimisation goals include software quality attributes such as performance reliability etc.and stakeholders goals such as the number of lives saved and property damage avoided during an emergency response.
software quality evaluation models e.g.performance and reliability models and quantitative goal oriented requirements models are typical examples of goal evaluation functions.
these models have parameters such as the reliability of each component in a reliability block diagram or the likelihoods of different types of events requiring a coordinated emergency response in a quantitative goal model.
in the standard use of these models each parameter is assigned a point based estimate.
in step of our method the parameters are assigned probability distributions.
in goal oriented requirements models candidate architectures describe socio technical systems i.e.systems for which components include human agents and hardware devices as well as software components.
the design decisions include decisions about alternative goal refinements alternative assignments of goals to agents and alternative resolutions of conflicts and obstacles .
sas case study.
the sas design team identified the design decisions options and optimisation goals shown in figure .
following an approach similar to that used in many goal oriented decision models they defined the goal evaluation functions as the sum of the contributions of each option composing an architecture v g a d dcontrib g a d where contrib g o are model parameters denoting the contribution887of option oto goal g. for example contrib batteryusage gps denotes the contribution of gps to battery usage.
since the model has options and goals we have parameters.
like all models this model is imperfect.
for example evaluating the response time of an architecture by summing up the response time of its individual component is a basic performance model that will only give a rough approximation of an architecture response time.
evaluating the reliability of an architecture by summing the reliability of its components is most likely to be an inaccurate measure of the true reliability.
another significant problem with this model is that the goals have no clear definition.
for example what is meant by reliability and battery usage?.
similarly the levels of contribution of each option to each goal have no clear semantics for example what does the contribution of the gps to battery usage contrib batteryusage gps actually measure?
.
in order to separate issues concerning the validity of the sas decision model from discussions concerning the benefits of alternative decision support methods we temporarily assume this moadm to be valid.
we revisit this assumption after having compared the two decision methods on the same model.
.
defining the cost benefit model multi objective decision problems increase in difficulty as the number of objectives increases .
since a moadm could have a large number of optimisation goals one way to simplify the problem is to convert the moadm into a simpler cost benefit decision model .
the cost benefit model allows software architects to relate design decisions and levels of goal satisfaction to financial goals of direct interest to the project clients and stakeholders.
the set of alternatives of the cost benefit decision model is the set of candidate architectures in asatisfying the constraints in c. software architects in collaboration with project stakeholders define the cost and benefit functions.
the parameters of the cost benefit decision model include the parameters of the architecture decision model plus additional parameters involved in the definition of the cost and benefit functions.
the cost function would typically include software development deployment operation and maintenance costs but possibly also other costs incurred in the application domain such as salary material legal environmental and reputation costs.
the benefit function would model estimated financial values associated with achieved levels of goal attainment.
a problem with many cost benefit models is that they exclude from their equations costs and benefits that are perceived to be too hard to quantify and measure.
for example they omit the cost and benefit related to security usability company reputation etc.to be useful cost benefit models should include the hard to measure factors that are important to the decision so that their uncertainty can be assessed and analysed instead of being ignored.
systematic methods for transforming vague qualitative goals into meaningful measurable objectives exist and have been used successfully in many industrial projects .
many other projects however ignore these methods.
a popular alternative is to compute for each alternative a utility score defined as the weighted sum of the stakeholders preferences for each goal u a g gw g prefg v g a where the goal weights w g and preferences functions prefg x are elicited from stakeholders using appropriate techniques .
the goal preference values prefg x are real numbers in denoting the level of preference stakeholders associate with a value xfor goal g. a preference of denotes the highest possible stakeholders satisfaction a preference of denotes the worst.
for example if gis the response time of a web application a preference of may be given to an average response time of second or less and of to an average response time of seconds or above.
preference functions are often constructed as linear or s shape functions between the goal attainments corresponding to the lowest and highest preference .
this approach or a close variant is found in many requirements engineering methods .
an advantage of defining utility as a weighted sums of goal preferences is that it is extremely easy to apply.
its biggest inconvenience however is that the utility scores correspond to no physical characteristics in the application domain making them hard to interpret and impossible to validate empirically.
in other words the utility functions are not falsifiable .
in contrast in other domains e.g.in healthcare economics utility functions are not restricted to weighted sums and they denote domain specific measures such as the quality adjusted life year making it possible to refute and improve them based on empirical evidences .
when a utility function exists whether the utility is falsifiable or not it is possible to convert a utility score into financial units using awillingness to pay ratio ksuch that the benefit of an alternative is the product of its utility and k benefit a k u a omega this approach allows us to apply our statistical cost benefit analysis method on any requirements and architecture models developed using a utility based approach.
sas case study.
the guidearch method uses the equivalent of a weighted sum approach to define a utility score for each candidate architecture2.
the goal preferences are defined as linear functions where the preference and are associated to the lowest and highest possible values for that goal among all candidate architectures and all possible parameters values.
therefore instead of defining the goal preference functions in terms of stakeholder s preferences the guidearch model views these functions as normalisation functions expressing the percentage of goal attainment relative to the highest attainment achievable within the model.
the sas model utility score mixes both cost and benefit factors.
for our experiment we have thus assumed this utility score corresponds to the net benefit of our cost benefit model i.e.
nb a u a without distinguishing the cost and benefit parts of the utility function.
.
defining design decision risks software design decisions should take into consideration the risks associated to each candidate architecture.
in a cost benefit model these risks can be measured using the loss probability and probable loss magnitude introduced in section .
decision makers can introduce additional risk measures related to net benefits for example measuring the probability that the net benefit or return on investment i.e.the ratio between net benefit and cost are below some thresholds.
in addition to risk measures related to net benefits software architects may be interested in risks relative to the goals of the multi objective architecture decision model goal failure risks.
the risk for an architecture ato fail to satisfy a goal g noted grisk g a is the probability that afails to achieve some minimum level of goal attainment grisk g a p v g a must g 2the guidearch approach assigns to each architecture aa score s a to be minimized rather than maximised.
to facilitate exposition and relation to other work we convert the guidearch score to a utility score to be maximised.
we reproduced the guidearch method on the sas case study and verified our change did not affect the results our findings are available at e.letier sdda .888where must g is the level of goal attainment below which stakeholders would consider the goal to be unrealized.
this definition assumes gis to be maximised a symmetric definition can be given for goals to be minimized.
eliciting the must g values is part of many requirements engineering methods .
project failure risk .
the risk for an architecture ato fail the whole project noted prisk a is defined as the risk of failing to satisfy at least one of its goals.
if the goals are statistically independent we have prisk a g g grisk g a .
the project failure risk is defined with respect to goals are defined in the multi objective architecture decision model.
these goals may include concerns related to development costs and schedule.
sas case study.
the original sas model has no definition of risk and does not specify must values for any of its goals.
we thus decided to define the must g values relative to the goal level attainment of a baseline architecture whose goal attainments would be equal to those of the existing system.
the new system has to be at least as good as the current system on all goals otherwise the project would be a failure.
we selected as baseline the architecture with the lowest expected net benefit from among the top .
.
eliciting parameters values the following step consists in eliciting probability distributions or single value in case a parameter is known with certainty for all parameters in the architecture and cost benefit decision models.
as mentioned in section simple reliable methods exist for performing this elicitation .
sas case study.
the sas design team elicited uncertainty for all model parameters through a three point estimation method that consist in eliciting for each parameter its most likely lowest and highest values.
they interpreted these three points estimates as triangular fuzzy value functions which are equivalent to triangular probability distributions.
they also elicited point based values for each of the goal weights parameters unlike our approach guidearch does not allow these weights to be uncertain .
.
shortlisting candidate architectures the next step consists in shortlisting candidate architectures to be presented to software architects for the final decision and for computing expected information value.
for this step software architects have to decide what shortlisting criteria to use.
the default is to shortlist candidate architectures that maximise expected net benefit and minimise project failure risk.
software architects may however select other risk related criteria such as the probabilities that the project costs and schedule exceed some threshold or that the loss probability or probable loss magnitude do.
software architects may select any number of criteria.
however keeping the number of criteria below facilities the generation and visualisation of the shortlist.
architects may also specify for each criteria a resolution margin to resist specious differentiation when comparing alternatives.
for example setting the resolution margins for financial objectives such as expected net benefit to causes the shortlisting process to ignore differences of less then when comparing candidate architectures net benefit.
these margins make our shortlisting process robust against statistical error in the mc simulation and modelling error due to simplifications in the model equations.
shortlisting candidate architectures based on strict pareto optimality without our resolution margin can cause a priori rejection of a candidate architecture due to insignificant differences in objective attainment.our tool then computes the shortlist as the set of pareto optimal candidature architectures for the chosen criteria and resolution margins.
more precisely a candidate architecture ais shortlisted if there is no other candidate architecture a that outperforms aby the resolution margins on allcriteria.
if the moadm includes a non empty setcof dependency constraints between design decisions any architecture that violates these constraints is automatically excluded.
our shortlisting approach is an extension of the standard notion of pareto optimality used to deal with optimisation problems involving uncertainty.
in the objective space the outcomes of each candidate architecture for each criteria forms a pareto optimal strip or a pareto optimal front with margins.
our implementation identifies the pareto optimal alternatives through an exhaustive exploration of the design space.
it first computes the hatwidernbmatrix for the full design space using mc simulation then uses a classic algorithm for extracting pareto optimal sets that we have extended to deal with resolution margins.
our implementation is in r an interpreted programming language for statistical computing.
for the sas model on a standard laptop the mc simulations of all6912 alternatives takes around 5minutes for a mc simulation with 104scenarios and the identification of the pareto optimal strip less than a second.
other industrial architecture decision problems have a design space whose size is similar or smaller to that of the sas .
for example the application of cbam to nasa earth observation core system ecs involves binary decisions thus alternative architectures against for the sas .
our exhaustive search approach is thus likely to be applicable to most architecture decision problems.
the scalability bottleneck of our approach is more likely to be related to the elaboration of the decision models steps and and the number of parameters to be elicited from stakeholders step than to the automated shortlisting step.
if however a need to increase the performance and scalability of our shortlisting technique appears one could port our implementation to a faster complied programming language and use evolutionary algorithms commonly used in search based software engineering such as nsga2 to deal with much larger design spaces but at the cost of losing the guarantee of finding the true pareto optimal strip .
sas case study.
figure shows the pareto optimal strip for the sas candidate architectures evaluated with respect to expected net benefit and project failure risk.
the resolution margins for each criteria are set at .
and respectively.
the red crosses show the architectures shortlisted by our approach the blue squares the top architectures of the guidearch approach and the grey circles all other candidate architectures.
in our shortlist out of candidate architectures are in the pareto strip but not on the pareto front they would have groundlessly been excluded from the shortlist if we had followed the traditional approach of retaining solutions in the pareto optimal front only.
we observe important differences between our shortlist and top architectures guidearch identifies our shortlists identifies candidate architectures with slightly higher expected net benefit and much lower project risk than guidearch s top architectures.
we explain the difference between the two shortlists as follows.
guidearch did not consider project failure risk as we defined it in section .
or any other risk in their architecture evaluations.
it is therefore not surprising that its top architectures perform weakly with respect to this criterion.
instead of evaluating criteria against their expected net benefit or equivalently their utility score and some measure of risk guidearch ranks candidate architectures according to a single criterion corresponding to an uncertainty adjusted score defined as the weighted sum of an architecture s pessimistic most8890.
.
.
.
.
.
project failure riskexpected net benefit figure comparing our shortlisted architectures red crosses against guidearch top blue triangles .
the grey circles denote all other candidate architectures.
open decisions options file sharing openintents in house chat xmpp open file in house connectivity 3g on nexus 3g on droid architectural pattern facade psuh based closed decisions option location finding radio hardware platform nexus report syncing explicit map access preloaded esri database mysql data exchange format unformatted data figure open and closed decisions in our shortlisted architectures.
likely and optimistic net benefit in fuzzy logic.
the weights in the uncertainty adjusted score capture the importance decision makers give to pessimistic most likely and optimistic outcomes.
in other words guidearch scores architectures by taking into account the most likely net benefits in probabilistic terms the mode of the distribution and what its authors call the positive and negative consequences of uncertainty.
in our probabilistic approach both types of consequences are already taken into account by computing theexpected net benefit.
we argue our shortlisting approach has two advantages over the guidearch uncertainty adjusted scores it informs decision makers of both expected net benefit and risks and it does not require decision makers to specify uncertaintyadjusting weights whose impacts on the architectures ranking are difficult to interpret.
.
open and closed design decisions shortlisting a set of candidate architectures may conclude a set of design decisions.
a design decision is closed if all shortlisted architectures agree on the option to be selected for this decision a design decision is open if the shortlisted architecture contains alternative options for that option.
presenting the open and closed design decisions gives decision makers a useful view of the shortlisted architectures.
if the shortlist is large it can also be organised into clusters based on design decisions similarities .
sas case study.
figure shows the open and closed design decisions in our shortlisted candidate architectures.evppi evtpi ppip t pi risk ramp up time battery usage ramp up time battery usage ramp up time ramp up time battery usage development time development time development time development time figure expected value of partial perfect information.
.
computing information value the last step consists in computing the expected value of perfect information and its impact on risks.
the expected value of total perfect information and its impact on risk evtpi anderitpi give upper bounds on the value that additional information could bring to the decision.
if evtpi is small and the impact on risk low there is little value in reducing model parameters uncertainty.
the expected value of partial perfect information about a single model parameter and its impact on risk evppi anderippi help software architects to distinguish model parameters with high and low expected information value.
we also found it useful to measure the expected value of partial perfect information about the level of attainment of each goal and its impact on risk evppi v g a anderippi v g a .
this gives software architects a mean of separating high and low information value at the levels of goals instead of individual parameters which can be too numerous the sas model has parameters and fine grained.
to ease computations of expected information values we limit the alternatives to those in the shortlist.
in our case study this reduces the hatwidernbmatrix from which evtpi andevppi are computed from a size of by104 the number of alternatives by the number of simulation scenarios to a size of by .
one should be careful in interpreting evtpi andevppi values to remember that their accuracy is conditional on the validity of the decision model.
they only measure the value of reducing uncertainty about model parameters not about the model equations.
we come back to this issue below.
sas case study.
using the shortlisted architectures identified in section .
and hatwidernbmatrix for those architectures we compute that evtpi is0.05which represents only .
of the highest expected net benefit.
eirtpi is which is the full project failure risk of the highest benefit architecture in our shortlist.
this means that the impact of perfect information is to reduce project failure risk to zero.
figure shows all non zero evppi for all goals and architectures.
since these evppi are small the table shows the ratio of evppi to evtpi instead of absolute values.
the ramp up time and battery usage of of the shortlisted architectures are shown to have in relative terms much higher information value than other goals and architectures.
however in absolute terms these values remain low.
in order to experiment with the use of evtpi and evppi we have artificially extended uncertainty in the sas model and observed the effect on evtpi and evppi.
we have for example given uncertainty to the goal weights in the definition of the utility function.
we have assumed that the sas design team is likely to have overestimated the goal weights and have therefore replaced their constant value by a triangular distribution of parameters w g w g where w g is the initial goal weight estimated by the sas design890team.
this distribution resulting in a linearly decreasing probability distribution function from w g to0.
we observed that this uncertainty roughly doubled evtpi.
however in our all experiments evtpi remains small.
this is mostly due to the small differences in net benefit that exist among the shortlisted architectures even when most of the model parameters uncertainties are increased.
if we had confidence in the validity of the model utility function this result would mean that for this particular decision problem there is no value in reducing uncertainty before deciding among the shortlisted architectures.
however we have identified important limitations in the sas moadm and utility models that severely question their validity the most important problem being that these models are not falsifiable making it impossible to validate and improve them based on empirical evidence.
the project client should thus be sceptical of the choice of architecture risk assessment and information value generated using these models whatever decision support method is used.
in order to deal with such difficulties it would be desirable to be able to explicitly describe and reason not only about parameters uncertainty but also about model uncertainty also called structural uncertainty .
requirements and architecture decision problems would especially benefit from this capability.
it would enable an incremental approach where software architects could start from an inexpensive coarse grained decision model with large uncertainty then use expected information value about model uncertainty to decide whether and where to reduce uncertainty by refining parts of the model.
they could for example start with a coarse grained software performance model similar to the one used in the sas case study estimate their uncertainty about the model error the deviation between its predicted performance and the software s actual performance and compute the expected value of perfect information about this error to decide whether to refine this model into a more fine grained performance model.
we have started exploring how to extend our method to deal with model uncertainty by converting it to parameter uncertainty but the approach is still tentative and our method does not currently supports this.
.
ev aluation and future work evaluating decision support methods is hard.
often authors argue that their method is systematic liked by its users triggers useful discussions and generates insights into the decision problem .
none of these claims however consider whether a decision method is correct and produces better outcomes than another method or even than no method at all i.e.. decisions based on intuition alone .
the popular ahp method for example is criticised by decision experts for its mathematical flaws and lack of evidence that it leads to better decisions than intuition alone .
evaluation of software engineering decision methods should go beyond vague claims of usefulness.
in this section we propose to evaluate software engineering decision support methods according to their correctness performance and scalability applicability and cost effectiveness.
inspired by an adaptation of moslow s pyramid of human needs to software quality we visualize these criteria in a pyramid where the lower level criteria are necessary foundations for higher level ones.
we discuss the extent to which we can claim our method meets these criteria and outline a roadmap of future research to extend our evaluation and improve our method against those criteria.
.
correctness.
the first level is to establish what correctness properties can be claimed of the method.
one must distinguish correctness of the decision method from correctness of the decision models to which the method is applied.
our method is correct in the sense that its produces correct estimations of the candidatearchitectures expected net benefits risks and expected information value assuming validity of the decision models and accuracy of the parameters probability distributions3.
not all decision methods can make this correctness claim.
for example guidearch computes for each architecture a score that unlike our expected net benefit and risk makes no falsifiable predictions about the architecture and has therefore no notion of correctness.
the lack of validity and falsifiability of the decision model we used in the sas case study is an important weakness of our evaluation.
all models are wrong but some are useful .
unfortunately today no scientific method exists to help software architects evaluate how useful a model actually is to inform decisions.
as mentioned in the closing of the previous section we intend to address this shortcoming by extending our approach to deal with model uncertainty so as the be able to estimate modelling errors their impact on decisions and support an incremental modelling process guided by information value analysis.
our method assumes it is possible to elicit accurate probability distributions for all model parameters.
such elicitation can be hampered by important cognitive biases.
for example software estimations have been shown to be affected by anchoring .
significant research in uncertainty elicitation has show it is possible to counter the effects of such biases using appropriate methods .
however these methods have to our knowledge not yet been applied in a software engineering context and further evaluation is thus required in this area.
.
performance and scalability.
with the sas case study we have shown our method is fast enough to analyse a real software design decision problem whose size and complexity is similar to those of other published industrial architecture decision problems .
the manual steps of elaborating the decision models and eliciting all parameters probability distributions will most likely be the first scalability bottleneck of applying our method to more complex problems.
if our automated shortlisting step becomes a bottleneck its performance and scalability can be improved notably by using by using evolutionary search based algorithms to reduce the number of candidate architectures to evaluate.
in the near future we intend to conduct a systematic scalability analysis of the whole approach on real case studies before attempting to improve its performance.
.
applicability.
the next evaluation criteria is to show the method is applicable by its intended users not just the method designers in actual software engineering projects.
we distinguish technical applicability the extent to which the method is understandable and applicable by software architects in an ideal fictive project where actors do not intentionally or unintentionally game the decision making process from contextual applicability the extent to which the method is applicable in the context of real projects where the project governance incentives and political relations might affect the decision making process and reporting of uncertainty.
at the moment we see no critical threats to the technical applicability of our method.
our method takes as input decision models that correspond to those already produced by other requirements engineering and architecture methods .
the only other required inputs are probability distributions modelling the decision makers uncertainty about the model parameters.
as mentioned earlier simple reliable methods exist to elicit such probability distributions .
our analysis outputs need to be easily interpretable by 3our approach actually computes these quantities using mc simulation which introduces bounded and measurable simulations errors .
in our case study with simulations of 105scenarios these errors are negligible particularly when compared to the much wider modelling and parameter uncertainty.891decision makers.
although the concepts of risk pareto optimality and information value can be misunderstood we see no insurmountable obstacle here.
even if the method is technically applicable the political context and governance structure of a project may create obstacles to the accurate reporting of uncertainty and analysis of risks .
important research in this area will be needed to identify incentives and governance structures that are favourable to sound decision making under uncertainty.
.
cost effectiveness the next evaluation stage is to demonstrate the cost effectiveness of decision analysis methods in requirements and architecture decisions.
a method can be applicable without being cost effective.
showing cost effectiveness of a decision method dealing with uncertainty is hard.
one must distinguish a good decision from a good outcome .
a good decision may by the effect of chance lead to a bad outcome and vice versa a bad decision may also by the effect of chance lead to a good outcome.
however when analysed over many decisions a good decision support method should on average lead to better outcomes which for software engineering projects means higher business benefits from it projects and less costly project failures.
we believe that by setting expected benefits and risks as explicit decision criteria and by using falsifiable models that can be incrementally improved from empirical evidence our method has a better chance of achieving these goals than other methods relying on unfalsifiable models and utility scores not clearly related to benefits and risks.
.
related work most requirements and architecture decision methods ignore uncertainty and rely on point based estimates of their models parameters .
by simply replacing point based estimates by probability distributions our method can be directly applied to any previous decision model because the mc simulations at the heart of the method merely consist of evaluating the point based models on many different possible parameters values.
our method builds on previous methods for dealing the uncertainty in software architecture decisions notably cbam and guidearch .
the first two steps of our method are equivalent to cbam s model elaboration steps.
our method differs from cbam in that it relies on sound reliable techniques for eliciting probability distributions it includes explicit definition of risks against which alternatives are evaluated it shortlists candidate architectures based on multiple objectives e.g.enb and risk instead of assuming a single ranking criterion and it measures expected information value whereas cbam uses deterministic sensitivity analysis whose limitations were described in section .
.
elaborating on the first point cbam infers probability distributions from divergences between stakeholders single point estimates this confuses consensus about the most likely value with uncertainty about the possible ranges of values.
our method differs from guidearch in the following ways.
in step and our method allows decision makers to elaborate problemspecific decision models whereas guidearch relies on fixed equations for computing a score for each candidate architecture.
the guidearch equations are not falsifiable and therefore not amenable to empirical validation.
likewise unlike step of our method guidearch does not allow decision makers to define domain specific measures of risks.
in step we model uncertainty about parameters values as probability distributions for which sound uncertainty elicitation techniques exist whereas guidearch uses fuzzy logic values that cannot be empirically validated and calibrated.
in step we allow decision makers to shortlist candidate architecture based on expected net benefit and risks whereas guidearch ranks archi tecture using a single risk adjusted score whose interpretation is problematic.
finally guidearch has no support for assessing the value of information.
because guidearch does not require the elaboration of problem specific models it may be simpler to apply than cbam and our approach however the lack of validity of the fixed equations used to score alternatives should raise concerns regarding the validity of the rankings it produces.
our decision support method deals with design time knowledge uncertainty and should not be confused with the large body of software engineering research dealing with run time physical uncertainty e.g.
.
philosophers and statisticians use the terms epistemic and aleatory uncertainty respectively .
a probabilistic transition system may for example describe variations in the response time of a web service as an exponential distribution with a mean .
this models a run time physical uncertainty.
such probabilistic model could be part of a decision model where the mean is an uncertain model parameter.
the decision makers uncertainty about is a knowledge uncertainty.
other software engineering research streams are concerned with uncertainty during the elaboration of partial models and uncertainty in requirements definitions for adaptive systems .
these are different concerns and meanings of uncertainty than those studied in this paper.
graphical decision theoretic models and bayesian networks provide general tools for decision making under uncertainty.
they have supported software decisions regarding development resources costs and safety risks but not requirements and architecture decisions.
we did not used these tools to support our method because they deal with discrete variables only their use would have required transforming our continuous variables such as cost benefit and goal attainment levels into discrete variables.
boehm s seminal book on software engineering economics devotes a chapter to statistical decision theory and the value of information .
the chapter illustrates the expected information value on a simple example of deciding between two alternative development strategies.
to our knowledge this is the only reference to information value in the software engineering literature including in boehm s subsequent work.
this concept thus appears to have been forgotten by our community.
software cost estimation methods could be used to provide inputs to our decision method.
many already rely on statistical and bayesian methods to provide cost estimates they could easily generate cost estimates in the form of probability distributions instead of point based estimates.
.
conclusion requirements and architecture decisions are essentially decisions under uncertainty.
we have argued that modelling uncertainty and mathematically analysing its consequences leads to better decisions than either hiding uncertainty behind point based estimates or treating uncertainty qualitatively as an inherently uncontrollable aspect of software development.
we believe that statistical decision analysis provide the right set of tools to manage uncertainty in complex requirements and architecture decisions.
these tools may be useful to other areas of software engineering e.g.testing where critical decisions must be made by analysing risks arising out of incomplete knowledge.
in future work we intend to validate and refine our method on a series of industrial case studies and address the problem of reasoning about model uncertainty.
.