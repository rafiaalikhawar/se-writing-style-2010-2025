on the naturalness of software abram hindle earl t. barr zhendong su dept.
of computer science university of california at davis davis ca usa ajhindle barr su cs.ucdavis.edumark gabel dept.
of computer science the university of texas at dallas richardson tx usa mark.gabel utdallas.edupremkumar devanbu dept.
of computer science university of california at davis davis ca usa devanbu cs.ucdavis.edu abstract natural languages like english are rich complex and powerful.
the highly creative and graceful use of languages like english and tamil by masters like shakespeare and avvaiyar can certainly delight and inspire.
but in practice given cognitive constraints and the exigencies of daily life most human utterances are far simpler and much more repetitive and predictable.
in fact these utterances can be very usefully modeled using modern statistical methods.
this fact has led to the phenomenal success of statistical approaches to speech recognition natural language translation question answering and text mining and comprehension.
we begin with the conjecture that most software is also natural in the sense that it is created by humans at work with all the attendant constraints and limitations and thus like natural language it is also likely to be repetitive and predictable.
we then proceed to ask whether a code can be usefully modeled by statistical language models and b such models can be leveraged to support software engineers.
using the widely adopted n gram model we provide empirical evidence supportive of a positive answer to both these questions.
we show that code is also very repetitive and in fact even more so than natural languages.
as an example use of the model we have developed a simple code completion engine for java that despite its simplicity already improves eclipse s built in completion capability.
we conclude the paper by laying out a vision for future research in this area.
keywords language models n gram natural language processing code completion and code suggestion i. i ntroduction the word natural in the title of this paper refers to the fact that code despite being written in an artificial language like c or java is a natural product of human effort .
this use of the word natural derives from the field of natural language processing where the goal is to automatically process texts in natural languages such as english and tamil for tasks such as translation to other natural languages summarization understanding and speech recognition.
the field of natural language processing nlp see sparck jones for a brief history went through several decades of rather slow and painstaking progress beginning with early struggles with dictionary and grammar based abram hindle is now with university of alberta edmonton.
mark gabel was at uc davis when this work was done.efforts in the 1960s.
in the 70s and 80s the field was reanimated with ideas from logic and formal semantics which still proved too cumbersome to perform practical tasks at scale.
both these approaches essentially dealt with nlp from first principles addressing language in all its rich theoretical glory rather than examining corpora of actual utterances i.e.
what people actually write or say.
in the 1980s a fundamental shift to corpus based statistically rigorous methods occurred.
the availability of large on line corpora of natural language text including aligned text with translations in multiple languages 1along with the computational muscle cpu speed primary and secondary storage to estimate robust statistical models over very large data sets has led to stunning progress and widely available practical applications such as statistical translation used by translate.google.com .2we argue that an essential fact underlying this modern exciting phase of nlp is natural language may be complex and admit a great wealth of expression but what people write and say is largely regular and predictable .
our central hypothesis is that the same argument applies to software programming languages in theory are complex flexible and powerful but the programs that real people actually write are mostly simple and rather repetitive and thus they have usefully predictable statistical properties that can be captured in statistical language models and leveraged for software engineering tasks.
we believe that this is a general useful and practical notion that together with the very large publicly available corpora of open source code will enable a new rigorous statistical approach to a wide range of applications in program analysis error checking software mining program summarization and code searching.
this paper is the first step in what we hope 1this included the canadian hansard parliamentary proceedings and similar outputs from the european parliament.
2indeed a renowned pioneer of the statistical approach fred jelenik is reputed to have exclaimed every time a linguist leaves our group the performance of our speech recognition goes up!!!
see org wiki fred jelinek.
.
c ieee icse zurich switzerland will be a long and fruitful journey.
we make the following contributions we provide support for our central hypothesis by instantiating a simple widely used statistical language model using modern estimation techniques over large software corpora we demonstrate using standard cross entropy and perplexity measures that the model indeed captures the high level statistical regularity that exists in software at the n gram level probabilistic chains of tokens we illustrate the use of such a language model by developing a simple code suggestion tool that substantially improves upon the existing suggestion facility in the widely used eclipse ide and we lay out our vision for an ambitious research agenda that exploits large corpus statistical models of natural software to aid in a range of different software engineering tasks.
ii.
m otivation and background there are many ways one could exploit the statistics of natural programs.
we begin with a simple motivating example.
we present more ambitious possibilities later.
consider a speech recognizer receiving a noisy signal corresponding to in brussels today the european central radio phizz announced that interest rates remain unchanged.
.
.
.
a good speech recognizer might guess that the noisy word was bank rather than fish from context.
likewise consider an integrated development environment ide into which a programmer has typed in the partial statement for i i .
in this context it would be quite reasonable for the ide to suggest the completion i to the programmer.
why do these guesses seem so reasonable to us?
in the first case the reason lies in the highly predictable nature of newscasts.
news reports like many other forms of culturally contextualized and stylized natural language expression tend to be well structured and repetitive.
with a reasonable prior knowledge viz.. a good statistical model of this style it is possible to rank order likely utterances.
thus if we hear the words european central the next word is more likely to be bank rather than fish .
this fact is well known and exploited by speech recognizers natural language translation devices and even some ocr optical character recognition tools.
the second example relies on a lesser known fact natural programs are quite repetitive .
this fact was first observed and reported in a very large scale study of code by gabel and su which found that code fragments of surprisingly large size tend to reoccur.
thus if we see the fragment for i i we know what follows in most cases.
in general if we know the most likely sequences in a code body we can often help programmers complete code.
what this essentially amounts to is using a code corpus to estimatethe probability distribution of code sequences .
with the ability to calculate such a distribution andif this distribution has low entropy we will often be able to guess with high confidence what follows the prefix of a code sequence.
what should the form of a such a distribution be and how should we estimate its parameters?
in nlp these distributions are called language models .
a. language models a language model essentially assigns a probability to an utterance.
for us utterances are programs.
more formally consider a set of allowable program tokens3t and the overgenerous set of possible program sequences t we assume the set of possible implemented systems to be s t .
a language model is a probability distribution p .
over systems s s s s summationdisplay s sp s in practice given a corpus cof programs c s and a suitably chosen parametric distribution p .
we attempt to calculate a maximum likelihood estimate of the parameters of this distribution this gives us an estimated language model.
the choice of a language model is usually driven by practicalities how easy is it to estimate and use.
for these reasons the most ubiquitous is the n gram model which we now describe.
consider the sequence of tokens in a document in our case a system s a1a2.
.
.
a i. .
.
a n.n gram models statistically estimate how likely a token is to follow other tokens.
thus we can estimate the probability of a document based on the product of a series of conditional probabilities p s p a1 p a2 a1 p a3 a1a2 .
.
.
p an a1.
.
.
an n gram models assume a markov property i.e.
token occurrences are influenced only by the n 1tokens that precede the token under consideration thus for gram models we assume p ai a1.
.
.
a i similarequalp ai ai 3ai 2ai these models are estimated on a corpus using maximumlikelihood based frequency counting of token sequences.
thus if is a wildcard we can estimate the probability thata4follows the tokens a1 a2 a3with p a4 a1a2a3 count a1a2a3a4 count a1a2a3 in practice estimation of n gram models is quite a bit more complicated.
the main difficulties arise from data sparsity i.e.
the richness of the model in comparison to the available data.
for example with 104token vocabulary a trigram model must estimate 1012coefficients.
some trigrams may never occur in one corpus but may in fact 3here we use token to mean its lexeme.838occur elsewhere.
this leads to technical difficulties when we encounter a previously unseen n gram we are in principle infinitely surprised because an infinitely improbable event x which did not occur in the training corpus and was therefore estimated to have p x actually occurs.
this leads to infinite entropy values as will become evident below.
smoothing is a technique to handle such cases while still producing usable results with sufficient statistical rigour.
fortunately there exist a variety of techniques for smoothing the estimates of a very large number of coefficients some of which are larger than they should be and others smaller.
sometimes it is better to back off from a trigram model to a bigram model.
the technical details are beyond the scope of this paper but can be found in any advanced nlp textbook.
in practice we found that modified kneser ney smoothing e.g.
koehn gives good results for software corpora compared to plain kneser ney lidstone add one smoothing and maximum likelihood.
for instance maximum likelihood is often infinitely surprised and lidstone smoothing tends to overemphasize surprises.
however we note that these are very early efforts in this area and new modeling and estimation techniques tailored for software might improve on the results presented below.
how do we know when we have a good language model?
b. what makes a good model?
given a repetitive and highly predictable corpus of documents or programs a good model captures the regularities in the corpus.
thus a good model estimated carefully from a representative corpus will predict with high confidence the contents of a new document drawn from the same population.
such a model can guess the contents of the new document with very high probability.
in other words the model will not find a new document particularly surprising or perplexing .
in nlp this idea is captured by a measure called perplexity or its log transformed version cross entropy .4given a document s a1.
.
.
a n of length n and a language model m we assume that the probability of the document estimated by the model is pm s .
we can write down the cross entropy measure as hm s nlogpm a1.
.
.
a n and by the formulation presented in section ii a hm s nn summationdisplay i 1logpm ai a1.
.
.
a i this is a measure of how surprised a model is by the given document.
a good model has low entropy for target documents.
it gives higher probabilities closer to and thus lower absolute log values to more frequent words and see also .
page equation .
.lower probabilities to rare ones.
if one could manage to deploy a hypothetical truly superb model within an ide to help programmers complete code fragments it might be able to guess with high probability most of the program so that most of the programming work can be done by just hitting a tab key!
in practice of course we would probably be satisfied with a lot less.
but how good are the models that we can actually build for natural software?
is software is really as natural i.e.
unsurprising as natural language?
iii.
m ethodology and findings to shed light on this question we performed a series of experiments with both natural language and code corpora first comparing the naturalness using cross entropy of code with english texts and then comparing various code corpora to each other to further gain insight into the similarities and differences between code corpora.
our natural language studies were based on two very widely used corpora the brown corpus and the gutenberg corpus.5for code we used two corpora a collection of java projects and a collection of applications from ubuntu broken up into application domain.
all are listed in table i after removing comments the projects were lexically analyzed according to language syntax to produce token sequences that were used to estimate n gram language models.
most of our corpora are in c and java.
extending to other languages is trivial.
the java projects were our central focus we used them both for cross entropy studies and some experiments with an eclipse plug in for a language model based code suggestion task.
table i describes the java projects that we used.
the version indicates the date of the last commit to the master branch in the git repository when we cloned the project.
lines is calculated using unix wcon each file within each repository and tokens are extracted from each of these files.
tokens counts the total tokens extracted unique tokens counts the distinct tokens.
the ubuntu domain categories were quite large in some cases ranging up to million lines million tokens one million unique .
the number of unique tokens is interesting and relevant as they give a very rough indication on the potential surprisingness of the project corpus.
if these unique tokens were uniformly distributed throughout the project highly unlikely we could expect a cross entropy of log2 .15e6 or approximately bits.
a similar calculation for the java projects ranges from to bits.
a. cross entropy of code cross entropy is a measure of how surprising a test document is to a distribution model estimated from a corpus.
5we retrieved these corpora from i j ava projects c code from u buntu .
c ategories and english corpus used in our study .
english is the concatenation of brown and gutenberg .
ubuntu .
maverick was released on the number of projects in each category is in parentheses .
tokens java project version lines total unique ant batik cassandra eclipse e4 log4j lucene maven2 maven3 xalan j xerces tokens ubuntu domain version lines total unique admin .
doc .
graphics .
interp.
.
mail .
net .
sound .
tex .
text .
web .
tokens english corpus version lines total unique brown gutenberg thus if one tests a corpus against itself one has to set aside some portion of the corpus for testing and estimate train the model on the rest of the corpus.
in all our experiments we measured cross entropy by averaging over a fold cross validation we split the corpus in lines at random locations trained on the and tested on and measured the average cross entropy.
we used a open vocabulary model tokens unseen in the training text were smoothed to a small probability allocated to unknown tokens.
a further bit of notation when we say we measured the cross entropy of xtoy yis the training corpus used to estimate the parameters of the distribution model my used to calculate hmy x .
first we wanted to see if there was evidence to support the claim that software was natural in the same way that english is natural viz.
whether regularities in software could be captured by language models.
rq1 do n gram language models capture regularities in software?
to answer this question we estimated n gram models for several values of nover both the english corpus and the order of n gramslog perplexity or cross entropy fold cross validation english cross entropy java projects cross entropyfigure .
comparison of english cross entropy versus the code crossentropy of java projects.
java language project corpora using averages over fold cross validation each corpus to itself as described above.
the results are in figure .
the single line at the top is the average over the folds of the english corpus beginning at about bits for unigram models and trailing down to under bits for gram models.
when you build a model on one project we call the computation of cross entropy on test data from that same project self cross entropy .
the average self cross entropy for the java projects are shown below in boxplots one for each order from unigram models to gram models.
several observations can be made.
each project was concatenated and viewed as a single document.
first software unigram entropy is much lower than might be expected from a uniform distribution over unique tokens because token frequencies are obviously very skewed.
second cross entropy declines rapidly with n gram order saturating around tri or grams.
the similarity in the decline in english and the java projects is striking.
this decline suggests that the language model captures as much repetitive local context in java programs as it does in english corpora.
we take this to be highly encouraging the ability to model the regularity of the local context in natural languages has proven to be extremely valuable in statistical natural language processing we hope and provide some evidence to support the claim that this regularity can be exploited for software tools.
finally software is far more regular than english with entropies sinking down to under bits in some cases.840ant batik cassandra eclipse log4j lucene maven2 maven3 xalan j xerces22 corpus projectscross entropy self cross entropy project cross entropyfigure .
cross entropy versus self cross entropy of the java projects studied.
corpus based statistical language models capture a high level of local regularity in software even more so than in english.
this raises a worrying question is the increased regularity we are capturing in software merely a difference between the english and java languages themselves?
java is certainly a much simpler language than english with a far more structured syntax.
might not the lower entropy be simply an artifact of java s artificial simple syntax?
if the statistical regularity of the local context captured by the language model were simply arising from the simplicity of java then we should find this uniformly across all the projects in particular if we train a model on one java project and test on another we should successfully capture the local regularity in the language.
thus we sublimate this anxiety provoking question into the following rq2 is the local regularity that the statistical language model captures merely language specific or is it also projectspecific?
this is a simple experiment.
for each of the projects we train a trigram model and evaluate its cross entropy against each of the others then compare the result with the average fold self cross entropy.
we chose trigrams because they do not use much memory and use minimal context to produce low cross entropy.
this plot is shown in figure .
the x axis lists all the different java projects and for each the boxplot shows the range of cross entropies with the other nine projects.
the red line at the bottom shows the average self cross entropy of the project against itself.
in this figure each document was the concatenation of a project.
as can be seen the self entropy is always lower.
even for small projects like log4j and maven the self cross entropy is low because it is also obtained by fold cross validation thereis no risk of over fitting.
thus to evaluate self cross entropy with fold cross validation of the lines act as a test document and the corpus is the other of the lines.
this suggests that useful language models can be built even for small code corpora .
language models capture significant levels of local regularity that are notan artifact of the programming language syntax but rather arise from naturalness or repetitiveness specific to each project.
furthermore we have captured this regularity in projects with only about 62k lines of code.
this is noteworthy it appears each project has its own type of local non java specific regularity that the mode is capturing furthermore the local regularity of each project isspecial unto itself and different from that of the other projects.
clearly each project has its own vocabulary and specific local patterns of iteration field access method calls etc.language models are therefore capturing non javaspecific project regularity beyond the differences in unigram vocabularies.
in section iv we discuss the application of the multi token local regularity captured by the models to a completion task.
as we demonstrate in that section the models are able to successfully suggest non linguistic tokens tokens that are not java keywords about of the time this also provides evidence that the low entropy produced by the models are not just because of java language simplicity.
but projects do not exist in isolation the entire idea of product line engineering rests on the fact that products in similar domains are quite similar to each other.
this raises the interesting question rq3 do n gram models capture similarities within and differences between project domains ?
we approached this question by studying categories of applications within ubuntu listed in table i. for each category we calculated the self cross entropy within the category red box and the other cross entropy the crossentropy against all the other categories boxplot shown in figure .
here again as in figure we see that there appears to be a lot of local regularity repeated within application domains and much less so across application domains.
some domains e.g.the web appear to have a very high level of regularity and lower self entropy this is an interesting phenomenon requiring further study.
while larger projects i.e.more data is better these results suggest that even new projects can leverage corpora in the same or similar domains.
b. concluding discussion a high degree of local repetitiveness or regularity is present in code corpora and captured by n gram models.
admin doc graphics interpreters mail net sound tex text web2.
.
.
.
.
.
.
.
corpus categoriescross entropy self cross entropy cross entropy of alternative ubuntu categories116 31figure .
categories of ubuntu applications cross entropy categories total packages .
the data suggest that these local regularities are specific to both projects and to application domains.
the data also indicate that these regularities are not simply due to the more regular when compared to natural languages syntax of java but arise from other types of project and domain specific local regularities that exist in the code.
next we show that these project specific regularities are actually useful.
we exploit project specific models to extend the eclipse suggestion engine we also show that the n gram models quite often about of the time provide suggestions that are project specific rather than merely suggesting contextrelevant java keywords.
in natural language these local regularities have proven to be of profound value for tasks such as translation.
it is our belief that these simple local regularities can be used for code summarization and code searching.
we also believe that deeper semantic properties will also in general manifest themselves in these same local regularities.
these are discussed further in future work section vi.
iv.
s uggesting the next token the strikingly low entropy between and bits produced by the smoothed n gram model indicates that even at the local token sequence level a high degree of naturalness obtains.
with just tries we may very well guess the right next token!
a. eclipse suggestion plug in we built an eclipse plug in to test this idea.
most modern ides have a built in suggestion engine that suggests a next token whenever it can.
typically suggestions are based on type information available in context.
we conjectured thatalgorithm 1mse esugg nsugg maxrank minlen require esugg and nsugg are ordered sets of eclipse and n gram suggestions.
elong p esugg strlen p minlen ifelong negationslash then return esugg end if return esugg .. maxrank nsugg .. maxrank corpus based n gram models suggestion engine for brevity ngse could enhance eclipse s built in suggestion engine for brevity ecse by offering tokens that tend to naturally follow from preceding ones in the relevant corpus.
thengse uses a trigram model built from a project corpus.
after each token ngse uses the previous two tokens the test document already entered into the text buffer and attempts to guess the next token currently based on a static corpus of source code.
the language model estimates the probability of a specific choice of next token this probability can rank order the likely next tokens.
our implementation produces rank ordered suggestions in less than .2seconds on average.
both ngse andecse produce many suggestions too many to present so we use a heuristic to merge the lists from the two groups given an admissible number nof suggestions to be presented to the user choose ncandidates from both ngse s andecse s offers.
in general ngse was good at recommending shorter tokens whileecse was better at longer tokens we discuss842the reasons for this phenomenon later in this section .
this suggested the simple merge algorithm mse defined in algorithm .
in our experiments is the break even length after which eclipse outperforms our n gram model so we setminlen .
whenever eclipse offers long suggestions within the top n we greedily pick all the top noffers from eclipse otherwise we pick half from eclipse and half from n grams.
the relative performance of mse andecse in practice might depend on a great many factors and would require a well controlled human study to be done at scale.
a suggestion engine can present more or fewer choices it may offer all suggestions or only offer suggestions that are long.
suggestions could be selected with a touchscreen with a mouse or with a down arrow key.
since our goal here is to gauge the power of corpus based language models as opposed to building the most user friendly merged suggestion engine which remains future work we conducted an automated experiment rather than a human subject study.
we controlled for factors in our experiments the string length of suggestions l and the number of choices npresented to the user.
we repeated the experiment varying n forn 10andl forl .
.
.
.
we omitted suggestions less than characters as not useful.
also when merging two suggestion lists we chose to pick at least one from each and thus n .
we felt that more than choices would be overwhelming although our findings do not change very much at all even with and choices.
we choose projects for study ant maven log4j xalan and xerces .
each project was a mature apache foundation java project used adopted by many java open source projects.
in each project we set aside a test set of randomly chosen files set aside in all and built a trigram language model on the remaining files for each project.
trigrams are chosen because they use sufficient context tokens they use less memory than grams or grams to represent and trigrams represent an inflection point where increasing the order of nfor n grams results in a decreasing reduction of cross entropy see figure .
we then used the mse and ecse algorithms to predict every token in the set aside files and evaluated how many more times themse made a successful suggestion when compared to the basic ecse .
we do not report precision or recall since there is usually only one correct suggestion.
files were chosen to reduce run time while maintaining necessary statistical significance and power.
in each case we evaluated the advantage of mse overecse measured as the percent and absolute gain in number of correct suggestions at each combination of factors nandl.
b. how does the language model help?
figure shows the results.
note the two y scales the left side black circle points is percent additional correct g ggg gg g gg g gg g 020406080100120percent gain over eclipse raw gain count suggestion lengthgpercent gain raw gain count a gain using top suggestions.
g g g g gg g gg ggg g 020406080100percent gain over eclipse raw gain count suggestion lengthgpercent gain raw gain count b gain using top suggestions.
g g g g gg g gg ggg g 020406080percent gain over eclipse raw gain count suggestion lengthgpercent gain raw gain count c gain using top suggestions.
figure .
suggestion gains from merging n gram suggestions into those of eclipse.843suggestions and the right side red square points are the raw counts.
since the raw count of successful suggestions from the eclipse engine ecse also declines with length both measures are useful.
as can be seen mse provides measurable advantage over ecse in all settings of both factors though the advantage generally declines with l. the gains up through character tokens are quite substantial in the range of additional suggestions from the language model that are correct between and characters the gains range from .
the additional suggestions from ngse run the gamut including methods classes and fields predictable from frequent trigrams in the corpus e.g.
println iterator transform ioexception append tostring assertequals package names e.g.
apache tools util java as well as language keywords e.g.
import public return this .
an examination of the tokens reveals why the n grams approach adds most value with shorter tokens.
the language model we build is based on allthe files in the system and the most frequent n grams are those that occur frequently in all the files.
in the corpus we find that more frequently used tokens have shorter names naturally these give rise to stronger signals that are picked up by the n gram language model.
note that a significant portion viz.
of the successful suggestions are not java keywords guessed from language context they are project specific tokens.
statistical language models thus capture non language specific local regularity in each project.
in the table below we present another view of the benefit ofmse the total number of keystrokes saved by using the baseecse first row themse second row and the percent gain from using mse .
top top top ecse mse increase finally here we used one specific language model to enhance one specific software tool a suggestion engine.
with more sophisticated language models specifically ones that combines syntax scoping and type information we expect to achieve even lower entropy and thus better performance in this and other software tools.
v. r elated work a. code completion and suggestion bycompletion we mean the task of completing a partially typed in token by suggestion we mean suggesting a complete token.
the discussion above concerned suggestion engines.modern ides provide both code completion and code suggestion often with a unified interface.
two notable javabased examples are eclipse and intellij idea.
both draw possible completions from existing code but they operate quite differently from our completion prototype.
eclipse and idea respond to completion requests a keyboard shortcut such as ctrl space by conservatively deducing what tokens might apply in the current syntactic context.
eclipse and idea implement dozens of syntactic and semantic completion rules that are primarily guided by the java language specification.
for example both eclipse and idea first parse the surrounding code and infer the current context .
they then create a short list of expected token types .
if this list contains say a reference type the tools use the rules of the type system to add a list of applicable type names to the list of completions.
similarly if a variable is expected the tools list visible names from the symbol table.
as a final step both tools rank the completions with a collection of apparently hand coded heuristics.
we complement this approach.
rather than using language semantics and immediate context to guess what might apply our n gram model captures what most often does apply.
our approach is much more flexible since it is languageindependent and tolerates inchoate code.
it also has the potential to be much more precise the space of commonly used completions is naturally far smaller than the space of language allowed completions.
note that our approach complements the current ide approach language based guesses can be enhanced or ordered using corpus statistics.
it is noteworthy that perhaps some of the strongest evidence for the naturalness of software is how much our n grambased suggestion engine improves eclipse s language based engine section iv .
there are approaches arguably more advanced than those currently available in ides.
the bmn completion algorithm of bruch et al.
is focused on finding the most likely method calls that could complete an expression by using frequency of method calls and prior usage in similar circumstances.
we propose a broad vision for using language models of code corpora in software tools.
our specific illustrative completion application has a broader completion goal completing all types of tokens not just method calls.
later bruch et al.
lay out a vision for next generation ides that take advantage of collective wisdom embodied in code bodies and recorded human action.
we enthusiastically concur with this vision our specific approach is that natural software has statistical regularities that allow techniques from statistical nlp to be profitably applied in the endeavor to make this vision a reality.
robbes and lanza compare a set of different methodcall and class name completion strategies which start with a multi letter prefix.
they introduce an approach based on history and show that it improves performance.
our844approach is complementary to theirs it can provide full token completion of any token and is based a language model that exploits regularities in program corpora.
han et al.
uses hidden markov models hmm to infer likely tokens from short form tokens.
they make use of a dynamic programming trellis approach for backtracking and suggestion.
their hmm is in fact a language model but the paper does not describe how effective a model it is or how well it would perform for completion tasks without user provided abbreviations.
jacob and tairas used n gram language models for a different application to find matching code clones relevant to partial programming task.
language models were built over clone groups not entire corpora as we propose and used to retrieve and present candidate clones relevant to a partially completed coding task.
hou and pletcher propose and evaluate several strategies for improving eclipse s standard code completions.
they focus their effort on one specific class of eclipse s completions method calls and they find that ranking calls byfrequency of past use is effective.
both this work and our own completion prototype drive completions with usage data but our work is more general we use a much more general language model to predict and complete arbitrary code not solely method calls and we propose many other potential applications as well.
b. the naturalness of names in code this line of work aims to automatically evaluate if the names reflect the meanings of the artifacts and if not how could they be improved ?
work by h st and stvold also concerns method naming they combine static analysis with an entropy based measure over the distribution of simple semantic properties of methods in a corpus to determine which method names are most discriminatory then use it to detect names whose usage are inconsistent with the corpus.
this work does not use language models to capture repetition in code.
c. summarization and concern location this line of work aims to generate natural language descriptions summaries of code .
this work uses semantic properties of code derived by static analysis rather than using statistical models of the natural regularities of code.
it is complementary to ours properties derived by static analysis as long as they can be done efficiently and at scale could enrich statistical models of large software corpora.
another line of work seeks to locate parts of code relevant to a specified concern e.g.
place auction bid which could be local or cross cutting based on fragments of code names facts mined from code or cooccurrence of related words in code .d.
software mining work in this very active area aims to mine useful information from software repositories.
many papers can be found in msr conference series at icse and representative works include mining api usages patterns of errors topic extraction guiding changes and several others.
the approaches used vary.
we argue that the naturalness of software provides a conceptual perspective for this work and also offers some novel implementation approaches.
the conceptual perspective rests on the idea useful information is often manifest in software in uniform and uncomplicated ways the implementation approach indicates that the uniform and uncomplicated manifestation of useful facts can be determined from a large representative software corpus in which the required information is already known and annotated.
this corpus can be used to estimate the statistical relationship between the required information and readily observable facts this relationship can be used to reliably find similar information in new programs similar to the corpus.
we explain this further in future work sections vi c and vi d .
vi.
f uture directions we present now possible applications of corpus based statistical methods to aid software engineering tasks.
a. improved language models in this paper we exploited a common language model n grams that effectively captures local regularity.
there are several avenues for extension.
existing very large bodies of code can be readily parsed typed scoped and even subject to simple semantic analysis.
all this data can be modeled using enhanced models to capture regularities that exist at syntactic type scope and semantic levels.
there is a difficulty here the richer a model the more data is needed to provide good estimates for the model parameters thus the risk of data sparsity grows as we enrich our models.
ideas analogous to the smoothing techniques used in n gram models will have to be adapted and applied to build richer models of software corpora.
still if these models do capture regularities they may then be employed for software engineering tasks some of which we discuss below.
b. language models for accessibility some programmers have difficulty using keyboards because of rsi or visual impairment.
there has been quite a bit of work on aiding such programmers using speech recognition e.g.
.
however these approaches suffer from fairly high recognition error rates and are not widely used .
none of the published approaches make use of a statistical language model trained on specific code corpora.
we hypothesize that the use of a language model845can significantly reduce the error rates they certainly play a crucial role in conventional speech recognition engines.
because a large proportion of development work occurs in a maintenance or re engineering context language models derived from existing code should improve the performance of these speech recognition systems.
even when only a small amount of relevant code exists language model adaptation techniques could be applied using corpora of similar code.
c. summarizing and or retrieving code consider the task of summarizing code fragments or code changes in english.
consider also the approximate reverse task finding retrieving a relevant fragment of code e.g.
method call given an english description.
we draw an analogy between these two problems and statistical natural language translation snlt .
code and english are two languages and essentially both the above are translation tasks.
snlt relies on access to an aligned corpus which is a large set of sentences simultaneously presented in two or more languages e.g.. proceedings of parliaments in canada and europe .
consider the problem of translating a tamil sentence tto an english sentence e. the translation process is primed using an aligned english tamil corpus one estimates using the aligned corpus of e tpairs the conditional distribution using a bayesian formulation of english output sentences e given tamil sentences t. the translation process calculates the most likely sentence egiven a specific t. we propose to tackle the summarization retrieval task using statistical estimates derived from several corpora.
first we use an aligned english code corpora built from multiple sources one source arises from the version history of a program.
each commit in a typical project offers a matched pair of a log message english and some changes code .
another source of aligned examples are in line comments that are clearly matchable with nearby code .
second we can use any available english language text associated with a given project including code comments code design documents bug reports discussions on mailing lists to build a relevant english corpus.
finally models of the code and the associated english can be used select most likely translations.
d. software tools we hypothesize that the naturalness of software implies a naturalness of deeper properties of software such as those normally computed by powerful but expensive software tools as programmers tend towards repetitive use of code idioms we hypothesize that deeper more semantic properties of programs also manifest themselves in programs in superficially similar ways.
more specifically we hypothesize that semantic properties usually manifest themselves in superficial ways that are computationally cheap to detect particularly whencompared to the cost or even infeasibility of determining these properties by sound or complete static analysis.
for example the use of unprotected string functions like strcat as opposed to strncat is evidence for a potential buffer flow but not conclusive proof.
as another example suppose related methods wherein the relatedness has been detected using a recommender system open access close are called together in the same method with the methods occurring in that textual order in the code and access occurring within a loop.
this is evidence albeit not conclusive that the methods are to be used with the protocol open access close .
these are heuristics analogous to the probabilistic constraints used in merlin see livshits et al.
figure .
but where do they come from?
in merlin they are hard coded heuristics based on researchers intuitions we argue that they should be derived from corpuslevel distribution models that make use of prior knowledge about protocols already known to be used within those corpora.
this admittedly is a leap of faith however if it holds up and we have found anecdotal evidence that it does and some prior research implicitly makes a version of this assumption one can leverage this notion to build simple scalable and effective approximations in a wide variety of settings.
we contend that the annotation of code corpora for instance api usage rules can be automated using data mining techniques.
manually annotated code corpora may be well worth the investment by analogy with the penn tree bank and can be constructed using a volunteer community or perhaps via market mechanisms like the mechanical turk .
vii.
c onclusion although linguists sometimes revel in the theoretical complexities of natural languages most natural utterances in practice are quite regular and predictable and can in fact be modeled by rigorous statistical methods.
this fact has revolutionized computational linguistics.
we offer evidence supporting an analogous claim for software though software in theory can be very complex in practice it appears that even a fairly simple statistical model can capture a surprising amount of regularity in natural software .
this simple model is strong enough for us to quickly and easily implement a fairly powerful suggestion engine that already improves a state of the art ide.
we also lay out a vision for future work.
specifically we believe that natural language translation approaches can be used for code summarization and code search in a symmetric way we also hypothesize that the naturalness of software implies a sort of naturalness of deeper properties of software such as those normally computed by powerful traditional software analysis tools.
these are challenging tasks but with potentially high pay off and we hope others will join us in this work.846references k. sparck jones natural language processing a historical review current issues in computational linguistics in honour of don walker ed zampolli calzolari and palmer amsterdam kluwer .
m. gabel and z. su a study of the uniqueness of source code in proceedings acm sigsoft fse .
acm pp.
.
p. koehn statistical machine translation .
cambridge university press .
c. manning h. sch tze and mitcognet foundations of statistical natural language processing .
mit press vol.
.
m. bruch m. monperrus and m. mezini learning from examples to improve code completion systems in proceedings acm sigsoft esec fse .
m. bruch e. bodden m. monperrus and m. mezini ide .
collective intelligence in software development in proceedings of the fse sdp workshop on future of software engineering research .
acm pp.
.
r. robbes and m. lanza improving code completion with program history automated software engineering vol.
no.
pp.
.
s. han d. r. wallace and r. c. miller code completion from abbreviated input in proceedings ase .
ieee computer society pp.
.
f. jacob and r. tairas code template inference using language models in proceedings of the 48th annual southeast regional conference .
d. hou and d. pletcher an evaluation of the strategies of sorting filtering and grouping api methods for code completion in proceedings icsm .
d. lawrie c. morrell h. feild and d. binkley what s in a name?
a study of identifiers proceedings icpc .
d. binkley m. hearn and d. lawrie improving identifier informativeness using part of speech information in proceedings msr .
acm .
e. w. h st and b. m. stvold software language engineering d. ga evi c r. l mmel and e. wyk eds.
berlin heidelberg springer verlag ch.
the java programmer s phrase book.
e. h st and b. stvold debugging method names in proceedings ecoop .
springer pp.
.
g. sridhara e. hill d. muppaneni l. pollock and k. vijayshanker towards automatically generating summary comments for java methods in proceedings ase .
r. buse and w. weimer automatically documenting program changes in proceedings ase .
acm pp.
.
g. sridhara l. pollock and k. vijay shanker automatically detecting and describing high level actions within methods inproceedings icse .
d. shepherd z. fry e. hill l. pollock and k. vijayshanker using natural language program analysis to locate and understand action oriented concerns in proceedings aosd .
acm pp.
.
s. rastkar g. murphy and a. bradley generating natural language summaries for cross cutting source code concerns inproceedings icsm .
d. shepherd l. pollock and t. tourw using language clues to discover crosscutting concerns in acm sigsoftsoftware engineering notes vol.
no.
.
acm pp.
.
t. xie s. thummalapenta d. lo and c. liu data mining for software engineering ieee computer vol.
no.
pp.
.
m. gabel and z. su javert fully automatic mining of general temporal properties from dynamic traces in proceedings acm sigsoft fse .
acm pp.
.
d. mandelin l. xu r. bod k and d. kimelman jungloid mining helping to navigate the api jungle in acm sigplan notices vol.
no.
.
acm pp.
.
b. livshits and t. zimmermann dynamine finding common error patterns by mining software revision histories acm sigsoft software engineering notes vol.
no.
pp.
.
s. kim k. pan and e. whitehead jr memories of bug fixes in proceedings acm sigsoft fse .
acm pp.
.
e. linstead s. bajracharya t. ngo p. rigor c. lopes and p. baldi sourcerer mining and searching internet scale software repositories data mining and knowledge discovery vol.
no.
pp.
.
t. zimmermann p. weisgerber s. diehl and a. zeller mining version histories to guide software changes in proceedings icse .
ieee computer society pp.
.
s. arnold l. mark and j. goldthwaite programming by voice vocalprogramming in proceedings acm conf.
on assistive technologies .
acm pp.
.
a. begel spoken language support for software development in proceedings vl hcc .
ieee computer society pp.
.
t. hubbell d. langan and t. hain a voice activated syntax directed editor for manually disabled programmers inproceedings acm sigaccess .
acm .
s. mills s. saadat and d. whiting is voice recognition the solution to keyboard based rsi?
in automation congress .
wac .
world .
j. bellegarda statistical language model adaptation review and perspectives speech communication vol.
no.
pp.
.
g. antoniol g. canfora g. casazza a. d. lucia and e. merlo recovering traceability links between code and documentation ieee transactions on software engineering vol.
pp.
.
z. saul v .
filkov p. devanbu and c. bird recommending random walks in proceedings acm sigsoft esec fse .
acm pp.
.
m. robillard automatic generation of suggestions for program investigation in acm sigsoft software engineering notes vol.
no.
.
acm pp.
.
b. livshits a. nori s. rajamani and a. banerjee merlin specification inference for explicit information flow problems inacm sigplan notices vol.
no.
.
acm pp.
.
m. marcus m. marcinkiewicz and b. santorini building a large annotated corpus of english the penn treebank computational linguistics vol.
no.
pp.
.
a. kittur e. chi and b. suh crowdsourcing user studies with mechanical turk in proceedings chi .
acm .