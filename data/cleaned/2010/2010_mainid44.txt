see discussions st ats and author pr ofiles f or this public ation at .researchgate.ne t public ation learning to rank relevant files for bug reports using domain knowledge conf erence paper no vember .
.
citations 312reads author s including xin y e ohio univ ersity publica tions citations see profile chang liu ohio univ ersity publica tions citations see profile all c ontent f ollo wing this p age was uplo aded b y xin y e on sept ember .
the user has r equest ed enhanc ement of the do wnlo aded file.learning to rank relevant files for bug reports using domain knowledge xin y e razvan bunescu and chang liu school of electrical engineering and computer science ohio university athens ohio usa xy348709 bunescu liuc ohio.edu abstract when a new bug report is received developers usually need to reproduce the bug and perform code reviews to nd the cause a process that can be tedious and time consuming.
a tool for ranking all the source les of a project with respect to how likely they are to contain the cause of the bug would enable developers to narrow down their search and potentially could lead to a substantial increase in productivity.
this paper introduces an adaptive ranking approach that leverages domain knowledge through functional decompositions of source code les into methods api descriptions of library components used in the code the bug xing history and the code change history.
given a bug report the ranking score of each source le is computed as a weighted combination of an array of features encoding domain knowledge where the weights are trained automatically on previously solved bug reports using a learning to rank technique.
we evaluated our system on six large scale open source java projects using the before x version of the project for every bug report.
the experimental results show that the newly introduced learning to rank approach signi cantly outperforms two recent state of the art methods in recommending relevant les for bug reports.
in particular our method makes correct recommendations within the top ranked source les for over of the bug reports in the eclipse platform and tomcat projects.
categories and subject descriptors d. .
testing and debugging debugging aids h. .
information search and retrieval retrieval models i. .
learning parameter learning i. .
natural language processing text analysis general terms design experimentation languages permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than the author s must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
sigsoft fse november hong kong china copyright is held by the owner author s .
publication rights licensed to acm.
acm ... .
.keywords bug reports software maintenance learning to rank .
introduction and motivation a software bugordefect is a coding mistake that may cause unintended and unexpected behaviors of the software component .
upon discovering an abnormal behavior of the software project a developer or a user will report it in a document called a bug report orissue report .
a bug report provides information that could help in xing a bug with the overall aim of improving the software quality.
a large number of bug reports could be opened during the development life cycle of a software product.
for example there were bug reports created for the eclipse platform product in alone.
in a software team bug reports are extensively used by both managers and developers in their daily development process .
a developer who is assigned a bug report usually needs to reproduce the abnormal behavior and perform code reviews in order to nd the cause.
however the diversity and uneven quality of bug reports can make this process nontrivial.
essential information is often missing from a bug report .
lexical mismatches between natural language statements in bug reports and technical terms in software systems limit the accuracy of ranking methods that are based on simple lexical matching scores.
to locate the bug a developer needs to not only analyze the bug report using their domain knowledge but also collect information from peer developers and users.
employing such a manual process in order to nd and understand the cause of a bug can be tedious and time consuming .
therefore an automatic approach that ranked the source les with respect to their relevance for the bug report could speed up the bug nding process which in turn will lead to an overall improvement in the software team productivity.
if the bug report is construed as a query and the source code les in the software repository are viewed as a collection of documents then the problem of nding source les that are relevant for a given bug report can be modeled as a standard task in information retrieval ir .
as such we propose to approach it as a ranking problem in which the source les documents are ranked with respect to their relevance to a given bug report query .
in this context relevance is equated with the likelihood that a particular source le contains the cause of the bug described in the bug report.
the ranking function is de ned as a weighted combination of features where the features draw heavily on knowledge speci c to the software engineering domainin order to measure relevant relationships between the bug report and the source code le.
while a bug report may share textual tokens with its relevant source les in general there is a signi cant inherent mismatch between the natural language employed in the bug report and the programming language used in the code.
our system features bridge the corresponding lexical gap by using project speci c api documentation to connect natural language terms in the bug report with programming language constructs in the code.
furthermore source code les may contain a large number of methods of which only a small number may be causing the bug.
correspondingly the source code is syntactically parsed into methods and the features are designed to exploit method level measures of relevance for a bug report.
it has been previously observed that software process metrics e.g.
change history are more important than code metrics e.g.
size of codes in detecting defects .
consequently we use the change history of source code as a strong signal for linking fault prone les with bug reports.
another useful domain speci c observation is that a buggy source le may cause more than one abnormal behavior and therefore may be responsible for similar bug reports.
if we equate a bug report with a user and a source code le with an item that the user may like or not then we can draw an analogy with recommender systems and employ the concept of collaborative ltering .
thus if previously xed bug reports are textually similar with the current bug report then les that have been associated with the similar reports may also be relevant for the current report.
the resulting ranking function is a linear combination of features whose weights are automatically trained on previously solved bug reports using a learning to rank technique.
we have conducted extensive empirical evaluations on six large scale open source software projects with more than bug reports in total.
to avoid contaminating the training data with future bug xing information from previous reports we created strong benchmarks by checking out the before x version of the project for every bug report.
experimental results on the before x versions show that our system signi cantly outperforms a number of strong baselines as well as two recent state of the art approaches.
in particular when evaluated on the eclipse platform ui dataset containing over solved bug reports the learningto rank system is able to successfully locate the true buggy les within the top recommendations for over of the bug reports corresponding to a mean average precision of over .
overall we see our adaptive ranking approach as general enough to be applicable to a wide diversity of software projects for which domain knowledge in the form of api documentation and syntactically parsed code is readily available.
the main contributions of this paper include using api descriptions to bridge the lexical gap between bug reports and source code exploiting previously xed bug reports as training examples for the proposed ranking model in conjunction with a learning to rank technique and a strong benchmark dataset created by checking out a before x version of the source code package for each bug report.
the rest of the paper is structured as follows.
section outlines the system architecture.
this is followed in section by a detailed description of the features employed in the de nition of the ranking function.
the strong benchmark datasets are introduced in section followed by a de scription of the experimental evaluation setting and results in section .
after a discussion of related work in section the paper ends with future work and concluding remarks.
.
ranking model a ranking model is de ned to compute a matching score for any bug report rand source code le scombination.
the scoring function f r s is de ned as a weighted sum of kfeatures k where each feature i r s measures a speci c relationship between the source le sand the received bug report r f r s wt r s k i 1wi i r s given an arbitrary bug report ras input at test time the model computes the score f r s for each source le sin the software project and uses this value to rank all the les in descending order.
the user is then presented with a ranked list of les with the expectation that les appearing higher in the list are more likely to be relevant for the bug report i.e.
more likely to contain the cause of the bug.
the model parameters wiare trained on previously solved bug reports using a learning to rank technique.
in this learning framework the optimization procedure tries to nd a set of parameters for which the scoring function ranks the les that are known to be relevant for a bug report at the top of the list for that bug report.
.
feature engineering .
vector space representation if we regard the bug report as a query and the source code le as a text document then we can employ the classic vector space model vsm for ranking a standard model used in information retrieval.
in this model both the query and the document are represented as vectors of term weights.
given an arbitrary document d a bug report or a source code le we compute the term weights wt dfor each term tin the vocabulary based on the classical tf.idf weighting scheme in which the term frequency factors are normalized as follows wt d n ft d id ft n ft d t ft d max t2dt ft did ft logn d ft theterm frequency factor t ft drepresents the number of occurrences of term tin document d whereas the document frequency factor d ftrepresents the number of documents in the repository that contain term t.nis to the total number of documents in the repository while id ftrefers to the inverse document frequency which is computed using a logarithm in order to dampen the effect of the document frequency factor in the overall term weight.
.
.
surface lexical similarity for a bug report we use both its summary and description to create the vsm representation.
for a source le we use its whole content code and comments.
to tokenize an input document we rst split the text into a bag of words using white spaces.
we then remove punctuation numbers and standard ir stop words such as conjunctionsor determiners.
compound words such as workbench are split into their components based on capital letters although more sophisticated methods such as could have been used here too.
the bag of words representation of the document is then augmented with the resulting tokens work and bench in this example while also keeping the original word as a token.
finally all words are reduced to their stem using the porter stemmer as implemented in the nltk1package.
this process will reduce derivationally related words such as programming and programs to the same stem program which is known to have a positive impact on the recall performance of the nal system.
letvbe the vocabulary of all text tokens appearing in bug reports and source code les.
let r ands be the vsm vector representations of the bug report rand the source code le s where the term weights wt randwt sare computed using the tf.idf formula as shown in equation above.
once the vector space representations are computed the textual similarity between a source code le and a bug report can be computed using the standard cosine similarity between their corresponding vectors sim r s cos r s rts r s this is simply the inner product of the two vectors normalized by their euclidean norm.
the vsm cosine similarity could be used directly as a feature in the computation of the scoring function in equation .
however this would ignore the fact that bugs are often localized in a small portion of the code such as one method.
when the source le is large its corresponding norm will also be large which will result in a small cosine similarity with the bug report even though one method in the le may be actually very relevant for the same bug report.
therefore we use the ast parser from eclipse jdt2 and segment the source code into methods in order to compute per method similarities with the bug report.
we consider each method mas a separate document and calculate its lexical similarity with the bug report using the same cosine similarity formula.
we then compute a surface lexical similarity feature as follows 1 r s max fsim r s g fsim r m jm2sg i.e.
the maximum from all per method similarities and the whole le similarity.
.
.
api enriched lexical similarity in general most of the text in a bug report is expressed in natural language e.g.
english whereas most of the content of a source code le is expressed in a programming language e.g.
java .
since the inner product used in the cosine similarity function has non zero terms only for tokens that are in common between the bug report and the source le this implies that the surface lexical similarity feature described in the previous section will be helpful only when the source code has extensive comprehensive comments or the bug report includes snippets of code or programming language constructs such as names of classes or methods.
in practice it is often the case that the bug report and a relevant buggy le share very few tokens if any.
for example figure below shows a sample from a bug report3from the eclipse project.
this bug report describes a defect in which the toolbar is missing icons and showing wrong menus.
figure shows a snippet from a buggy le that is known to be relevant for this report.
at the surface level the two documents do not share any tokens consequently their cosine similarity will be thus unuseful for determining relevance.
bug id summary toolbars missing icons and show wrong menus .
description the toolbars for my stacked views were missing icons showing the wrong drop down menus from others in the stack showing multiple drop down menus missing the min max buttons ... figure eclipse bug report .
public class partrenderingengine implements ipresentationengine f private eventhandler trimhandler new eventhandler f public void handleevent event event f... mtrimmedwindow window mtrimmedwindow changedobj ...g...g...g figure code from partrenderingengine.java interface muilabel all known subinterfaces mtrimmedwindow ... description a representation of the model object ui label .
this is a mix in that will be used for ui elements that are capable of showing label information in the gui e.g.
parts menus toolbars perspectives ... .
the following features are supported label icon uri tooltip ... figure api speci cation for muilabel interface.
however we can bridge the lexical gap by using the api speci cation of the classes and interfaces used in the source code.
the buggy le partrenderingengine.java declares a variable window whose type is mtrimmedwindow .
as speci ed in the eclipse api muilabel is a superinterface of mtrimmedwindow .
as can be seen in figure the api documentation of the muilabel interface mentions tokens such as toolbar icon and menu that also appear in the bug report.
therefore for each method in a source le we extracts a set of class and interface names from the explicit type declarations of all local variables.
using the project api speci cation we obtain the textual descriptions of these classes and interfaces including the descriptions of all their direct or indirect superclasses or superinterfaces.
for each method bug.cgi?id 339286mwe create a document m api by concatenating the corresponding api descriptions.
finally we take the api specications of all methods in the source le sand concatenate them into an overall document s api m2sm api .
we bug id summary close all and close others menu options available when right clicking on tab in partstack when no part is closeable .
description if i create a partstack that contains multiple parts but none of the parts are closeable when i right click on any of the tabs i get menu options for close all and close others .
selection of either of the menu options doesn t cause any tabs to be closed since none of the tabs can be closed .
i don t think the menu options should be available if none of the tabs can be closed ... figure eclipse bug report .
bug id summary close all and close others may cause bundle activation.
description ... bug id summary close related context menu actions should show up for all stacks and apply to all items.
description ... bug id summary close parts under stacks with middle mouse click.
description ... figure bug reports that are similar with .
then compute an api enriched lexical similarity feature as follows 2 r s max fsim r s api g fsim r m api jm2sg i.e.
the maximum from all per method api similarities and the whole le api similarity.
.
collaborative filtering score it has been observed in that a le that has been xed before may be responsible for similar bugs.
for example figure displays an eclipse bug report about incorrect menu options for parts that are not closeable.
figure shows three other bug reports that were solved before bug was reported.
these three reports describe similar defects and therefore share many keywords with report shown underlined in the gures .
consequently is is not surprising that source le stackrenderer.java which had been previously found to be relevant for the three reports in figure was also found to be relevant for the textually similar bug report in figure .
this collaborative ltering effect has been used before to improve the accuracy of recommender systems consequently it is expected to be bene cial in our retrieval setting too.
given a bug report rand a source code le s let br r s be the set of bug reports for which le swas xedbefore rwas reported.
the collaborative ltering feature is then de ned as follows 3 r s sim r br r s the feature computes the textual similarity between the text of the current bug report rand the summaries of all the bug reports in br r s .
.
class name similarity a bug report summary may directly mention a class name in the summary which provides a useful signal that the corresponding source le implementing that class may be relevant for the bug report.
our hypothesis is that the signal becomes stronger when the class name is longer and thus more speci c. for example the summary of the eclipse bug report contains the class names workbenchwindow workbench and window after tokenization but only workbenchwindow.java is a relevant le.
lets class denote the name of the main class implemented in source le s and js class jthe name length.
based on the observation above we de ne a class name similarity feature as follows 4 r s js class j ifs class 2r otherwise this feature will be automatically normalized during the feature scaling step described in section .
.
.
bug fixing recency the change history of source codes provides information that can help predict fault prone les .
for example a source code le that was xed very recently is more likely to still contain bugs than a le that was last xed long time in the past or never xed.
as in section .
let br r s be the set of bug reports for which le swas xed before bug report rwas created.
let last r s 2br r s be the most recent previously xed bug.
also for any bug report r let r month denote the month when the bug report was created.
we then de ne the bugxing recency feature to be the inverse of the distance in months between randlast r s 5 r s r month last r s month thus if swas last xed in the same month that rwas created 5 r s is .
if swas last xed one month before rwas created 5 r s is .
.
bug fixing frequency a source le that has been frequently xed may be a faultprone le.
consequently we de ne a bug xing frequency feature as the number of times a source le has been xed before the current bug report 6 r s jbr r s j this feature will be automatically normalized during the feature scaling step described in section .
below.
.
feature scaling features with widely different ranges of values are detrimental in machine learning models.
feature scaling helps bring all features to the same scale so that they become comparable with each other.
for an arbitrary feature lettable benchmark datasets eclipse refers to eclipse platform ui.
of bug of xed les of java les in project time range reports per bug report different versions of the of api mapped project source package entries max median min max median min aspectj birt eclipse jdt swt tomcat min and max be the minimum and the maximum observed values in the training dataset.
a feature may have values in the testing dataset that are larger than max or smaller than min .
therefore examples in both the training and testing dataset will have their features scaled as follows if min min max minif min max if max .
benchmark datasets we created benchmark datasets for evaluation from six open source projects .
aspectj4 an aspect oriented programming extension for java.
.
birt5 an eclipse based business intelligence and reporting tool.
.
eclipse platform ui6 the user interface of an integrated development platform.
.
jdt7 a suite of java development tools for eclipse.
.
swt8 a widget toolkit for java.
.
tomcat9 a web application server and servlet container.
all these projects use bugzilla as their issue tracking system and git as a version control system earlier versions are transferred from cvs svn to git .
the bug reports source code repositories and api speci cations are all publicly accessible.
bug reports with status marked as resolved xed veried xed orclosed xed were collected for evaluation.
to map a bug report with its xed les we apply the heuristics proposed by dallmeier and zimmermann in .
thus we searched through the project change logs for special phrases such as bug or x for .
if a bug report links to multiple git commits or revisions or if it shares the same commit with others it will be ignored because it is clear which xed le is relevant.
bug reports without xed les are also ignored because they are considered not functional .
overall we collected more than bug reports from the six projects.
previous approaches to bug localization used just one code revision to evaluate the system performance on multiple bug reports.
however software bugs are often found in different revisions of the source code package.
consequently using just one revision of the source code package for evaluation may lead to performance assessments that do not match the actual performance of the system when used in practice.
for example the xed revision that is used for evaluation may contain future bug xing information for older bug reports.
furthermore a buggy le might not even exist in the xed revision if it were deleted after the bug was reported.
to avoid the problems associated with using a xed code revision we check out a before x version of the project for each bug report.
the exact versions of the software packages for which bugs were reported were not all available.
therefore for each bug report the version of the corresponding software package right before the x was committed was used in the experiment.
this may not be the exact same version based on which the bug was reported originally.
therefore the association may not capture exactly what took place in the real world.
however since the corresponding x had not been checked in and the bug still existed in that version it is reasonable to use this association in our evaluation.
for each project and the corresponding dataset table shows the time range for the bug reports and a number of basic statistics such as the number of bug reports that were mapped to xed les the number of xed les per bug report the project size and the number of api entries classes or interfaces from the project api speci cation that are used in our evaluation.
our dataset is publicly available10.
.
experimental evaluation as described in section our ranking model f r s is based on a weighted combination of features that capture domain dependent relationships between a bug report rand a source code le s. the model parameters wiare trained using the learning to rank approach as implemented in the svmrankpackage .
in this learning framework the optimization procedure tries to nd a set of parameters such that the scoring function ranks the les that are known to be relevant for a bug report at the top of the list for that bug report.
thus if s1is known to be relevant for bug rands2is known to be irrelevant for the same bug report then the objective of the optimization procedure is to nd parameters wisuch that f r s1 f r s2 .
for any given bug report the number of irrelevant source code les is very large which would make the training time infeasible.
therefore for each bug report rwe rst use the vsm cosine similarity feature 1 r s to rank all the les in the dataset and then select only the top irrelevant les for training.
in order to create disjoint training and test data the bug reports from each benchmark dataset are sorted chronologically by their report timestamp.
for all the projects but aspectj the sorted bug reports are then split into equally sized folds fold fold ... fold where fold 1contains the most recent bug reports while fold 10is the oldest.
the reports from aspectj are split only into folds due to the smaller size of the project.
furthermore the oldest fold is split into training and validation and a grid search is performed in order to tune the capacity parameter cof the ranking svm.
this is done by repeatedly training on the and testing on the for different values of cand selecting the one that maximizes mean average precision on the validation data.
since tuning on other folds results in similar values for c we use the c value that was tuned on the oldest fold for all training folds.
the ranking model is trained on fold k 1and tested on fold k for all k .
since the folds are arranged chronologically this means that we always train on the most recent bug reports which are supposed to better match the properties of the bugs in the current fold.
for each bug report from a test fold testing the model means computing the weighted scoring function f r s for each source code le using the learned weights and ranking all the les in descending order of their scores.
the system ranking is then compared with the ideal ranking in which the relevant les should be listed at the top.
at the end we pool the bug reports from all test folds and compute the overall system performance using the following evaluation metrics accuracy k measures the percentage of bug reports for which we make at least one correct recommendation in the top k ranked les.
mean average precision map is a standard metric widely used in information retrieval .
it is de ned as the mean of the average precision avgp values obtained for all the evaluation queries map jqj q 1avgp q jqj avgp k2kprec k jkj here qis the set of all queries i.e.
bug reports k is the set of the positions of the relevant documents in the ranked list as computed by the system.
prec k is the retrieval precision over the top kdocuments in the ranked list prec k of relevant docs in top k k mean reciprocal rank mrr is based on the positionfirst qof the rst relevant document in the ranked list for each query q mrr jqjjqj q first q .
results and comparisons we compared our learning to rank lr approach with the following baselines .
the standard vsm method that ranks source les based on their textual similarity with the bug report.
.
the usual suspects method that recommends only the top k most frequently xed les we also compared against recent state of the art systems .
buglocator ranks source les based on textual similarity the size of source les and information about previous bug xes.
.
bugscout classi es source les as relevant or not based on an extension to latent dirichlet allocation lda .
we implemented the two baselines as well as the buglocator method.
we tuned the parameter of buglocator on the training data for each project using a grid search from .
to .
with a step of .
.
we used the tuned value for testing because we observed it gives better results than the optimal value published in .
figures to present the accuracy k results for the implemented methods with k ranging from to .
the lr approach achieves better results than the other three methods on all six projects.
for example on the eclipse platform ui our approach achieved accuracy k of .
.
.
and .
for k and respectively.
that is to say if we recommend only one source le to users we can make correct recommendations for .
of collected bug reports.
if we recommend ten source les we can make correct recommendations for .
bug reports.
in comparison buglocator achieved accuracy of .
and accuracy of .
.
vsm and usual suspect achieved accuracy of .
and .
respectively.
an application of the mann whitney u test shows that the lr approach signi cantly p outperformed buglocator in terms of accuracy k for eclipse platform ui jdt and swt.
it also signi cantly outperformed vsm and usual suspects in terms of accuracy k for all six projects.
aspectj and tomcat are two projects where buglocator performs close to the lr approach.
for many bug reports in aspectj it is often the case that relevant les are les that have been frequently xed which also explains the relatively high performance obtained by usual suspects.
since the bug xing information is exploited by both the lr approach and buglocator it is expected that they obtain comparable performance on this dataset.
with respect to tomcat numerous bug reports contain rich descriptions that share many terms with the relevant les which explains the relatively high performance obtained by vsm.
consequently since both buglocator and our lr approach exploit textual similarity between bug reports and source les it is expected that they perform comparably on this dataset too.
figure and figure compare the same methods in terms of map and mrr.
here too the lr approach outperforms the two baselines and bugscout on all six projects.
for example on the eclipse platform ui project the map and mrr results for our lr approach are .
and .
which compare favorably with bug locator .
and .
vsm .
and .
and usual suspects .
and .
.
.
.
.
.
.
.
.
.
.
1234567891011121314151617181920accuracy k k learning to rank buglocator vsm usual suspectsfigure accuracy graphs on aspectj.
.
.
.
.
.
.
.
.
.
.
.
1234567891011121314151617181920accuracy k k learning to rank buglocator vsm usual suspects figure accuracy graphs on birt.
.
.
.
.
.
.
.
.
.
1234567891011121314151617181920accuracy k k learning to rank buglocator vsm usual suspects figure accuracy graphs on eclipse platform ui.
.
.
.
.
.
.
.
.
.
1234567891011121314151617181920accuracy k k learning to rank buglocator vsm usual suspects figure accuracy graphs on jdt.
.
.
.
.
.
.
.
.
.
1234567891011121314151617181920accuracy k k learning to rank buglocator vsm usual suspectsfigure accuracy graphs on swt.
.
.
.
.
.
.
.
.
.
.
1234567891011121314151617181920accuracy k k learning to rank buglocator vsm usual suspects figure accuracy graphs on tomcat.
aspectj birt eclipse jdt swt tomcat learning to rank .
.
.
.
.
.
buglocator .
.
.
.
.
.
vsm .
.
.
.
.
.
usual suspects .
.
.
.
.
.
.
.
.
.
.
.50map figure map comparison.
aspectj birt eclipse jdt swt tomcat learning to rank .
.
.
.
.
.
buglocator .
.
.
.
.
.
vsm .
.
.
.
.
.
usual suspects .
.
.
.
.
.
.
.
.
.
.
.50mrr figure mrr comparison.table comparison between bugscout bs and learning to rank lr on data replicated from .
project accuracy accuracy accuracy bs lr bs lr bs lr aspectj eclipse compared to buglocator bugscout is a more complex approach and thus more difficult to implement correctly.
since we were unable to get access to either the tool or the actual dataset used in we tried to recreate their dataset by following the description from .
thus we created a test dataset by collecting the speci ed number of xed bug reports from the eclipse platform and aspectj projects going backwards from the end of when bogscout was published .
to train our lr method we used bug reports that were solved before the bug reports in the testing dataset.
table shows the accuracy k results of the bugscout bs and the lr method for k and .
the bugscout results are copied from and are substantially lower than the lr results.
while it is possible that our replicated dataset is different from their dataset we believe there are two main reasons why the lr method performs better than bugscout .
lr uses features that capture domain knowledge relationships between bug reports and source les.
.
lr is trained directly to optimize ranking results.
but bugscout is trained for classi cation into multiple topics which may represent a mismatch during testing when the system is evaluated for ranking.
various ir approaches have been applied before on the task of identifying source les that are relevant for a bug report.
these include approaches based on lda latent semantic indexing lsi smoothed unigram model sum and svms .
since buglocator was reported to outperform the existing approaches using lda lsi and sum and since bugscout was reported to outperform the svm model proposed in we expect our lr system to compare favorably with all these previous approaches.
.
evaluation of feature utility in order to estimate the utility of the features used in our system for each feature iwe report in table the corresponding weight wi averaged over all training folds fold k where k .
based on the magnitude of the weights we can say for example that feature 1is the most important feature in all projects but birt followed by feature 3. looking in more detail at the eclipse platform ui project we performed a set of evaluations in which we used each feature separately for ranking.
the accuracy k results are shown in figure for each feature.
the accuracy based ranking of features shown in this gure is identical with the weight based ranking from table .
the best results are achieved when the system uses all the features a behavior that is consistent over all values of k from to .
the next best results are obtained when using only feature 1 followed by feature 3. that is to say on this project the most helpful information for ranking is provided by thetextual similarity between the bug report and the source le and its methods and by the textual similarity with the summaries of previously xed bug reports.
table the average model parameters.
project w1 w2 w3 w4 w5 w6 aspectj .
.
.
.
.
.
birt .
.
.
.
.
.
eclipse .
.
.
.
.
.
jdt .
.
.
.
.
.
swt .
.
.
.
.
.
tomcat .
.
.
.
.
.
1234567891011121314151617181920accuracy k k all feature1 feature2 feature3 feature4 feature5 feature6 figure single feature performance on eclipse.
9accuracy k of folds used for training k k k k k figure learning curves for eclipse platform ui.
.
impact of training data size in the evaluation of the lr method we always trained onfold k 1and tested on fold k for k .
to evaluate the impact of increasing the size of the training data we ran an experiment on the eclipse platform ui project in which we kept the test dataset unchanged as fold and trained on increasingly larger datsets.
thus we rst train on fold then we trained on fold fold until in the nal evaluation we train on all folds fold fold fold .
the learning curves in figure show the behavior of accuracy k as a function of the number of folds used during training.
the gure shows that increasing the size of the training data not improve the accuracy.
in terms of map training on fold 2table runtime performance results.
project index s training s ranking s max avg.
max avg.
max avg.
aspectj .
.
.
.
.
.
birt .
.
.
.
.
.
eclipse .
.
.
.
.
.
jdt .
.
.
.
.
.
swt .
.
.
.
.
.
tomcat .
.
.
.
.
.
achieved a value of .
while the best result is .
when training on fold 2tofold .
the mrr when training onfold 2is .
while the best mrr is .
obtained when training on fold 2tofold .
we applied the mannwhitney u test and found no signi cant difference between the result of training on fold 2and the result of training on a larger dataset.
given the reduced number of parameters in our model it is perhaps not surprising that we obtain at learning curves.
furthermore since fold 2is chronologically the closest to fold it is expected that bug reports from the other folds will not be as similar to bugs from fold and thus not as useful for ranking source les with respect to these bugs.
furthermore achieving optimal results using only a relatively small training dataset has a bene cial impact on the system s time and memory complexity.
.
runtime performance we performed time complexity evaluations on a computer with cpu intel r core tm i7 .67ghz cores 24g ram and linux .
.
table presents the runtime performance of our approach.
the indexing time refers to the time used to create a posting list and a term vocabulary for source les api descriptions and previously xed bug reports respectively.
the training time is the time needed to train the weight parameters of our ranking function.
the ranking time is the time used to calculate the le scores and to rank all source les for a bug report.
the average ranking time ranging from .07s to .69s makes the system suitable for practical use.
the maximum indexing time for every project is relatively high because we need to index all source les for the beforex version of the rst bug report.
when using vsm we need to index calculate tft dandid ftfor all source les and create a postings list and a term vocabulary .
to efficiently perform evaluation on over before x project versions we designed a method that indexes only the changed les.
taking the eclipse bug as an example we check out its before x version index the source les and perform evaluation.
when we perform evaluation for another bug we check out its before x version 602d549 and use the git diff command to obtain the list of changed added modi ed and deleted les.
based on this list we remove deleted and modied les from the postings list and the term vocabulary and index only added plus modi ed les instead of reindexing all source les present in version 602d549 .
therefore when we evaluate another bug report we only need to index the changed les.
this results in an average indexing time that is much lower than the maximum index time.
in practice because we just need to update the post ing lists and the term vocabularies for only the changed les in the new commit the average indexing time ranging from .99s to .71s is representative for most cases.
furthermore it is not necessary to perform indexing and training for every bug report because there may be multiple bugs found in the same version of the project.
.
related work recently researchers have developed methods that concentrate on ranking source les for given bug reports automatically.
saha et al.
syntactically parse the source code into four document elds class method variable and comment.
the summary and the description of a bug report are considered as two query elds.
textual similarities are computed for each of the eight document eld query eld pairs and then summed up into an overall ranking measure.
compared to our method the approach from assumes all features are equally important and ignores the lexical gap between bug reports and source code les.
furthermore the approach is evaluated on a xed version of the source code package of every project which is problematic due to potential contamination with future bug xing information.
kim et al.
propose both a one phase and a two phase prediction model to recommend les to x. in the one phase model they create features from textual information and metadata e.g.
version platform priority etc.
of bug reports apply na ve bayes to train the model using previously xed les as classi cation labels and then use the trained model to assign multiple source les to a bug report.
in the two phase model they rst apply their one phase model to classify a new bug report as either predictable or de cient and then make predictions only for predictable report.
however their one phase model uses only previously xed les as labels in the training process and therefore cannot be used to recommend les that have not been xed before when being presented with a new bug report.
furthermore while their two phase model aims at improving prediction accuracy by ignoring de cient reports our approach can be used on all bug reports.
zhou et al.
not only measure the lexical similarity between a new bug report and every source le but also give more weight to larger size les and les that have been xed before for similar bug reports.
their model depends only on one parameter even though it is based on three different features.
the parameter is tuned on the same data that is used for evaluation which means that the results reported in their paper correspond to training performance.
it is therefore unclear how well their model generalizes to unseen bug reports.
in comparison our approach introduces more project oriented features and applies an automatic learningto rank technique to learn the weight of every feature on a separated training dataset.
the generalization performance is computed by running the trained model on a separate test dataset.
nguyen et al.
apply lda to predict buggy les for given bug reports.
in their extended lda model the topic distribution of a bug report is in uenced by the topic distributions of its corresponding buggy les.
for ranking they use the trained lda model to estimate the topic distribution of a new bug report and compare it with the topic distributions of all the source les.
they also introduce a defect proneness factor that gives more weight to frequently xed les and les with large size.
in evaluations conducted by other researchers their approach performs comparably with the usual suspects method.
while they model the training task as a classi cation problem in which bug reports and les are assigned to multiple topics we directly train our model for ranking which we believe is a better match for the way the model is used in testing.
rao et al.
apply various ir models to measure the textual similarity between the bug report and a fragment of a source le.
through evaluations they reported that more sophisticated models such as lda and lsa did not outperform a unigram model or vsm.
although their approach was outperformed by other methods their comparative study showed that vsm is suitable for measuring the textual similarity between a bug report and a source le fragment.
lukins et al.
combine lda and vsm for ranking.
they index source les with topics estimated by the lda model and use vsm to measure the similarity between the description of a bug report and the topics of a source le.
our approach builds relationships between bug reports and source les by extracting information not only from bug reports and code but also from api documents and software repositories.
to support fault localization a number of approaches use runtime information that was generated for debugging.
other approaches analyze dynamic properties of programs such as code changes in order to infer causes of failures.
jin and orso burger and zeller and cleve and zeller use passing and failing execution information to locate buggy program entities.
liu et al.
locate faults by performing statistical analysis on program runtime behavior.
unlike these methods that require runtime executions our approach responds to bug reports without the need to run the program.
other researchers build models that associate bug reports to individual developers and functions in addition to source les.
ashok et al.
introduce debugadvisor a tool that allows search with free text queries that contain both structured and unstructured data describing a bug.
a graph with linked elements in the repository is used to recommend les and functions.
gay et al.
combine an ir based concept location method with explicit relevance feedback mechanisms to recommend artifacts for bug reports.
poshyvanyk et al.
introduce promesir which combines lsi and execution scenario based probabilistic ranking method to locate bugs for mozilla and eclipse systems.
another related area focuses on predicting software defects.
in order to support defect prediction lee et al.
analyze developer behaviors and build interaction patterns nagappan et al.
utilize the frequency of similar changes described as change burts hassan et al.
use code change complexity zimmermann et al.
build a dependency graph that implies an error proneness for les linked to buggy models.
moser et al.
and kim et al.
use machine learning techniques to train a prediction model based on code changes.
kim et al.
cache fault related code changes and predict fault prone entities based on the cached history.
menzies et al.
build a prediction model based on static code attributes.
nagappan et al.
apply principle component analysis pca while bell et al.
and ostrand et al.
use negative binomial regression to build models that predict fault prone les.
.
conclusion future work to locate a bug developers use not only the content of the bug report but also domain knowledge relevant to the software project.
we introduced a learning to rank approach that emulates the bug nding process employed by developers.
the ranking model characterizes useful relationships between a bug report and source code les by leveraging domain knowledge such as api speci cations the syntactic structure of code and issue tracking data.
experimental evaluations on six java projects show that our approach can locate the relevant les within the top recommendations for over of the bug reports in eclipse platform and tomcat.
our ranking model outperforms buglocator and bugscout two recent state of the art approaches.
in future work we will leverage additional types of domain knowledge such as the authorship of a source le or the pagerank score associated with each le in the dependency graph of the project.
we also plan to use the ranking svm with nonlinear kernels.
the model will be further evaluated on projects in other programming languages.
.