an analysis of the relationship between conditional entropy and failed error propagation in software testing kelly androutsopoulos middlesex university ukdavid clark university college london ukhaitao dan university college london uk robert m. hierons brunel university ukmark harman university college london uk abstract failed error propagation fep is known to hamper software testing yet it remains poorly understood.
we introduce an information theoretic formulation of fep that is based on measures of conditional entropy.
this formulation considers the situation in which we are interested in the potential for an incorrect program state at statement s to fail to propagate to incorrect output.
we de ne ve metrics that di er in two ways whether we only consider parts of the program that can be reached after executing sand whether we restrict attention to a single program path of interest.
we give the results of experiments in which it was found that on average one in tests su ered from fep earlier studies having shown that this gure can vary signi cantly between programs.
the experiments also showed that our metrics are well correlated with fep.
our empirical study involved programs for which we executed a total of test cases.
the results reveal that the metrics di er in their performance but the spearman rank correlation with failed error propagation is close to .
for two of the metrics.
these strong correlations in an experimental setting in which all information about both fep and conditional entropy is known open up the possibility in the longer term of devising inexpensive information theory based metrics that allow us to minimise the e ect of fep.
categories and subject descriptors d. .
testing and debugging testing tools and code inspections general terms information theory experimentation veri cation.
keywords program analysis information theory.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
icse may june hyderabad india copyright acm ... .
.
.
introduction coincidental correctness occurs when the program happens to produce the correct output for some input even though it has executed a fault the program is coincidentally correct rather than actually correct.
one of the causes of coincidental correctness is known as failed error propagation fep .
in this situation a faulty statement is executed and the resulting internal computational state becomes faulty but the di erences between the faulty and correct state fail to be observed at output.
we say that the error the faulty state has failed to propagate .
empirical studies have revealed that fep inhibits e ective software testing but it remains unclear how software testing could be better designed to ameliorate the problems it causes.
in order to improve software testing we need to reduce the probability that test cases will su er from fep.
however in order to do that we need metrics that can help us to identify parts of a program that are more likely to lead to fep.
failed error propagation can occur for a number of reasons.
for example it might be that the faulty state is simply never inspected by the test oracle.
in this case the failure to propagate the error is caused by an inadequate oracle rather than by any inherent property of the program under test.
such failures of error propagation could be addressed by oracle improvement .
a more interesting class of fep occurs when the program itself removes traces of an error before it has had a chance to propagate to a point at which it can be observed.
for this to occur faulty state changes must become lost along some paths through the program because state update functions along these paths squeeze out the faulty information .
one obvious way in which this could occur is when a path contains a killing assignment which overwrites the value of the variable with a constant.
a killing assignment is the most extreme example of a state update function squeezing out information.
in general anycomputation that reduces entropy of inputs will have the potential to squeeze out error information and thereby lead to failed error propagation.
this loss of error information suggests a connection between fep and information theory but this relationship has been little explored in the literature.
this paper introduces an information theoretic formulation of fep.
we introduce ve di erent metrics based on the computation of conditional entropy in a program with these di ering in the parts of the program considered.
when considering a statement s one natural approach is to exam permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
icse may june hyderabad india copyright acm ... .
ine a single path since a test case will lead to a path being followed.
however a test case might lead to di erent paths in some idealised correct program pand the program under testp0 by only considering a single path we do not recognise the potential for fep associated with this pair of paths to occur.
we therefore also consider approaches that look at the part of the program that follows s the statements that can be reached from s .
in total we introduce and evaluate ve di erent metrics experimentally evaluating each on di erent programs.
our results are based on the execution of test cases over original and fault seeded versions of the programs.
in these experiments one in test inputs su ered from fep these results are in line with those of masri et alia .
masri et alia also found that the potential for fep differs signi cantly between programs with over of tests being a ected by fep in of the programs studied .
for each of the ve metrics we performed an experimental evaluation of its relationship to fep.
we report spearman rank correlations between each of the metrics and fep.
the results of these experiments are promising indicating strong spearman rank correlations between several of our conditional entropy metrics and failed error propagation.
the primary contributions of this paper are as follows.
.
we introduce an information theoretic approach to fep de ning associated metrics.
.
we experimentally investigate the correlation between the metrics and fep using programs including open source programs and laboratory benchmarks.
the results reveal a spearman rank correlation of over .
for one metric and just under .
for another.
the main motivation for this work is that a better understanding of fep has the potential to lead to more e ective testing.
in particular the strong correlations in an experimental setting in which all information about both fep and conditional entropy is known opens up the possibility in the longer term of devising inexpensive information theory based metrics that allow us to minimise the e ect of fep when choosing test cases.
the rest of this paper is organised as follows.
in section we brie y describe related work and in section we provide background information regarding program semantics and information theory.
in section we outline how fep and conditional entropy relate conceptually with this feeding into research questions and associated hypotheses that are given in section .
section outlines the experiments and in section we give the results of these experiments.
finally in section conclusions are drawn and potential lines of future work described.
.
related work voas introduced the pie framework and so explicitly recognised the need for an incorrect program state to propagate to output in order for a failure to occur .
the notion of fep was also explored by laski et alia who called this error masking.
they explored the concept and proposed the use of a mutation approach to estimate the sensitivity of a given test suite and program component c this is the likelihood of an incorrect value produced by cbeing masked through fep.
the approach taken to mutation was to di rectly mutate the program state change it to some randomly generated state .
masri and podgurski used experiments to explore variable dependence within a program and how this relates to an estimate of information ow .
they found that many cases where there is a dependence through a mixture of control and data dependence there was negligible information ow.
this helps motivate our work since the lack of true information ow could be one source of fep although fep can occur even when there is a signi cant amount of information ow .
it also suggests that if we just use dependence when for example choosing test cases to exercise program elements then we are likely to encounter fep and so there is a need for alternatives.
masri et alia explored factors that adversely a ect coverage based fault localisation methods .
such methods assign a suspiciousness value to a program element s such as a statement in order to allow the developer to focus on those elements that are most likely to be responsible for observed failures.
they do this based on how many failing tests execute an element sand how many passing tests execute s. in their study which used versions of ten java programs seeded with faults they found that fep which they called coincidental correctness was relatively common but also the rates varied with program in of programs over of tests that led to a corrupted state did not produce a failure while in this e ect was not observed.
wang et alia also considered the e ect of fep on fault localisation.
however their approach was quite di erent and involved producing multiple versions of each program element by adding in program patterns .
each context pattern describes aspects of the control ow and data ow after this element.
the results of experiments suggested that this approach reduces the e ect of fep on fault localisation.
potentially there would be value in adopting a similar approach to de ne coverage metrics.
in mutation testing we judge the adequacy of test suite t for program pby executing ton mutants which are produced by making small changes to p. a mutant mofpis weakly killed by test case tif the executions of mandp produced di erent sequences of program states.
in contrast mis strongly killed by tif the execution of mandpwitht lead to di erent outputs.
thus fep corresponds to the difference between weak mutation testing and strong mutation testing experiments have shown that there are signi cant di erences between weak and strong mutation testing again indicating that fep is relatively common.
masri and assi considered how test suites can be cleansed of coincidental correctness fep .
their approach assumes that the test cases have already been applied and so are suitable for regression testing and identi es program elements that appear in all failing runs but also in a percentage of runs that do not fail the percentage must meet a threshold value these are considered to be a potential source of coincidental correctness.
then test cases are removed from consideration in fault localisation based on which identi ed elements they contain.
chen et alia de ned a measure that aims to approximate the probability of coincidental correctness using a syntax based metric.
the results of experiments with ve small programs for sorting arrays were positive and this suggests that such an approach is worth exploring further.
.
background .
control flow graphs cfg a cfg is an alternative representation of the syntax of a program which makes explicit the execution paths that may be travelled in a program.
we assume for simplicity that all nodes in the graph are of two types nodes corresponding to state updates which have a single output edge and nodes corresponding to control ow decision points which have two output edges one labelled tand the other labelled f. sometimes the convention is employed that state update nodes in cfgs are de ned in terms of blocks of straight line code but here we assume that each individual statement or call in the program has its own node and outgoing edge.
definition .
cfg .
for a given programming language a cfg is a pair hn ei wherenis a set of nodes that contains a unique start node and two virtual nodes nsa virtual start node which has a single edge esto the unique start node nea virtual exit node with a unique virtual exit edge ee with no successor node and eis a set of labelled directed edges so that nu fnjnis an assignment or a function call g nc fnjnis a control boolean expression g n nu nc fns neg l ft fg ea f n1 n2 jn12nu n22nand an empty label g ec f n1 n2 jn12nc n22nand a label in lg e ea eu fes eeg the virtual nodes fns neg are merely for convenience and allow the inclusion of edges esandeein the cfg that are associated with the initial and exit program points.
definition .
cfg paths .
a path in the cfg is a sequence of nodes such that any two successive nodes form an edge in the cfg.
an execution path in the cfg is a path whose rst node is the virtual start node and either the path is in nite or it is nite and the nal node is the exit node.
a pre x path is any nite path whose rst node is the virtual start node.
note that the set of pre x paths includes the set of nite execution paths.
we will use the notion of a well formed subgraph of a control ow graph i.e.
a subgraph of a cfg which is itself a cfg.
not every subgraph of a cfg is well formed.
for example the subgraph reachable from a control node is not well formed in general as it may lack a unique start node.
.
program semantics in what follows we informally set out some concepts from program semantics that occur in the course of our explanations of what we are measuring and why.
we assume a deterministic imperative language such as c c java et cetera.
we assume a small step structured operational semantics sos for the programming language.
an sos formally describes for a given input state the sequence of state updates and branching decisions that occur along the execution path for the input.
this sequence corresponds to a sequence of edges in the cfg that startswith the start node and is either in nite or is nite and terminates in the exit node i.e.
an execution path.
on this basis we can associate each state in the sos sequence with an edge in the cfg and each update instruction or branching decision with a node in the cfg.
the sos for a given program and a given input corresponds to an execution path in the cfg in which each edge is labelled with the states that occur after the execution or evaluation of a node in the cfg.
we will refer to this set of cfg paths as the execution semantics of the cfg.
to de ne some notation for this idea we put the emphasis on pre x paths rather than execution paths in the cfg and informally de ne a property that relates a program a path an input an edge on the path and the state reached at that edge using the input.
p t e x is the property that there exists a possibly incomplete sos sequence for input tto programpthat corresponds to the pre x path in the cfg forpandeis an edge in andxis a state that occurs at e. definition .
execution semantics for a path .
let be the set of pre x paths in the cfg of program pand let be the set of all possible states that could occur in the executions of p. the execution semantics for a path in has type ex !e!
!
and is de ned as ex e t fxj p t e x g the set of states is non empty in the case that eis an edge in and is a pre x of the execution path for t otherwise it is empty.
we de ne three abstractions of the execution semantics acollecting semantics forpre x paths forcontrol flow graphs and for programs .
these simply collect up sets of states arising from the execution semantics which occur at edges in the cfg.
the collecting semantics for a pre x path in a cfg is given by the set of states that can occur at each edge in the path over all runs of the program.
definition .
collecting semantics for a pre x path .
the collecting semantics for a path in and an edge in e has type pa !e!
and is de ned as pa e t2 ex e t the collecting semantics for a cfg is also de ned in terms of edges that occur in the cfg and collects the sets of states for all paths that pass through an edge.
definition .
collecting semantics for a cfg .
the collecting semantics for a cfg at an edge in e has type cfg e!
and is de ned as cfg pa e575to de ne the collecting semantics of a program we employ the concept of a program point .
in a formal semantics of a programming language these are taken to be points in the program syntax before or after the execution of a program construct or statement de ned in the grammar of the language.
in what follows we de ne them in terms of nodes in the cfg a program point is a node in the cfg and the set of states that can occur at that program point is the collection of the cfg semantics of the edges that exit that node that is the program point occurs immediately after any node in the cfg and collects up all states that the program may be in once control passes from this node.
this is consistent with cousot s reachability semantics an alternative way to present these semantic concepts but for a xed language.
definition .
collecting semantics for a program .
the collecting semantics for a program at a node in n has type pr n!
and is de ned as pr fe2eje n g cfg we de ne various semantic concepts in terms of these definitions.
the set of reachable states for a pre x path is pa !
where!
is the edge following the nal state update node of .
the set of input states for a program is the collecting semantics for the program at the unique virtual start node i.e.
prforcfg p .
the set of output states for a program pis the collecting semantics at the virtual edge eefollowing the virtual exit node i.e.
pr.
let be the set of all possible states for a program.
the weakest precondition for program pto terminate and satisfy is the largest so that restricting the set of inputs to produces a subset of as the set of output states.
we write .
this can be extended to paths and cfgs in the obvious way.
.
information theory we consider total onto functions with xed nite discrete domains.
that is a function fis equipped with a xed input domain i and an output range o so thatf i!o and overloading f o fiandi f 1o.
we take a probabilistic view of the behaviour of f. we overloadiandoto also represent random variables equipped with probability distributions iand orespectively.
shannon measured the information content or entropy of a random variable xwith probability distribution pas follows.
definition .entropy of a random variable.
h x x x2xp x log2p x since random variable ois completely determined by f s action oniall the information in ostems from i so h i h o is the amount of information destroyed by f. this is conditional entropy the entropy of iconditional on knowledge of o in the deterministic case.
to emphasise the role that this loss of information is playing we also call this quantity squeeziness .
if the function is one to one there areno collisions and the squeeziness of the function is since h i h o .
definition .squeeziness.
the squeeziness of total functionf i!o sq f is de ned as the loss of information after applying ftoi sq f i h i h o note that squeeziness considered as a function takes two arguments a domain actually a random variable in the values of the domain and a function applied to that domain.
the partition property of entropy allows us to reformulate squeeziness in a more useful way.
let f 1obe the random variable in the inverse image of o2o.
the inverse images of elements of opartitioni.
for each o2o o o p i2f 1o i i so ois the probability distribution for the random variable in the partitions induced by the inverse images.
these inverse images partition the input space.
by the partition property h i h o x o2op o h f 1o hence sq f i x o2op o h f 1o the rhs of the equation is a weighted sum of terms and h f 1o is the amount of information contained in the random variable in the set of elements mapped to a single output.
f. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
op o f 1oh f 1o friday september figure loss of information squeezing inverse images of outputs.
in what follows whenever the distribution on a set is not an induced one e.g.
it is a distribution on inputs we always use a uniform distribution.
this corresponds to the maximum entropy principle and can be viewed as producing metrics in which inputs have equal weight.
.
failed error propagation fep and conditional entropy we argue that a very useful way of looking at fep is to see it in terms of loss of information.
this has been hinted at in studies by masri woodward and others without any strong conclusions .
the nature of testing software i.e.
lack of knowledge of the error free program necessarily makes any analysis approximate rather than formal.
in this case576the proof of our analytical pudding will be experiment rather than proof based.
to overcome our ignorance of the ideal error free program from now on referred to as the ghost program we make the following two strong assumptions.
assumption .there is a single error in the program under test.
this is a fairly common simplifying assumption.
in order to state the second assumption we must rst compare execution of a test input using the ghost program with execution of the same input using the implementation under test iut .
suppose the program we intended to write the ghost program is program pbut the program we actually wrote the iut is another program which we will call p0.pis a perfect oracle for p0which exhibits not only the desired input output behaviour of p0but allows us to examine the desired internal states of p0.
the di erence between pand p0is thatp0may contain faults.
let us assume that there is only one fault in p0and that it occurs in a single structural component c0which corresponds to its fault free version c inp.
this is a scenario similar to mutation testing.
now consider a test input t and the execution of each program on t. this is the situation illustrated by figure .
in the cfg corresponding to each program we can break the execution path into three parts the upper path that precedes entry to the structural component the structural component and the lower path that succeeds the component.
since we assume that there is a single fault the respective upper paths will be the same i.e.
aanda0in figure are the same.
clearly candc0are not the same and in general the succeeding or lower paths bandb0 are not the same.
let us assume that cis the path through cand thatchas a nal node n which is a state update node.
in this case there is a single outgoing edge e and we expectc0 c0 n0ande0to play the corresponding roles in p0.
for a covering path a0 c0 b0inp0when describing theupper path we will mean u a0 c0and by the lower path we mean l b0.
the execution semantics in pis then ex e t and in p0is ex e0 t .
the path semantics in pis pa e and inp0is pa e0.
finally under the assumption that the nal node of c andc0 is a state update node the cfg collecting semantics at nandn0respectively and the program collecting semantics at eande0respectively are the same respectively .
that is cfg prand cfg pr.
in figure eis identi ed with program point pp ande0with program point pp0.
we will use these notations to frame hypotheses and describe the experiments in sections and .
to return to figure it may be possible that the states associated with each edge in the execution semantics are the same i.e.
ex e t ex e0 t but in general these will be di erent corresponding to the infection phase of the pie scenario.
however when fep occurs we have that the output o is the same in each case.
laski et alia observed that it is the behaviour of the subgraph whose input state corresponds to the edge e0in figure and which may be described as the subgraph reachable from the target node of e0which is somehow failing to propagate the infection to the output .
in pandp0these respective subgraphs are labelled qandq0.
it is the joint behaviour of these that causes fep.
laski and others fur ther noted that these subgraphs are exactly the same except in the case that the component c0is within a loop.
then c may occur many times in qcorresponding to c0occurring many times in q0but the cfg context in which they occur will be the same.
in the case that the fault occurs in a node n which is a control expression there is no unique outward edge as per our earlier assumption and the subgraph is not a well formed cfg.
this case did not occur in the paper by laski et alia as they assumed that the fault was within a well formed program construct.
suppose that the two subgraphs are the same q q0 then the collision where two di erent inputs produce the same output is an example of loss of information conditional entropy between inputs and outputs as discussed in a previous paper and above in section .
so even if they are not the same but their information ow behaviour is very similar we can use the information ow behaviour of q0to estimate how likely it is that fep occurs.
this is the key idea in this paper and our second assumption.
assumption .the sub programs qandq0following the error point in the ghost program and the iut are essentially the same from an information ow quantity perspective.
n t cfg p a a c c q o ob b q t cfg p pp pp n figure fep scenario.
.
the research question our long term aim beyond the scope of this paper is to produce a set of lightweight information ow based metrics which can support both coverage based testing and mutation based testing in generating test suites that minimally su er from fep.
as masri et alia have observed di erent programs su er from coincidental correctness to di erent extents .
however we can attempt to optimise for a particular program.
in what follows we consider the question of useful correlations from a coverage testing perspective.
there is only one research question research question .what are the useful correlations between the conditional entropies of di erent information ow channels in the iut and the probability of fep for a given erroneous program construct?577in what follows we propose six answers to this research question in the form of hypotheses.
in subsequent sections we will evaluate these answers experimentally.
it will be useful to refer to figure when interpreting the hypotheses.
the statements of the hypotheses refer to the following conventions .
hypothesis consider an incorrect program construct c0in an iut p0containing program point pp0corresponding to edge e0as above immediately following c0.
let ibe the set of inputs top0and letq0be the sub program of p0reachable from pp0and the collecting semantics at pp0be the set of states pp0 cfg.
let be the functional semantics of q0.
hypothesis .there is a correlation between the probability of fep for all input states whose execution path includese0and sq pp0 we unpack and motivate the metric.
laski et alia have observed that failed error propagation is due to the behaviour ofq0.
we propose that given our assumptions the more squeezy is on states that get mapped to the output of q0applied to pp0 i.e.
the higher the conditional entropy of on that domain the higher the probability that any internal state chosen at pp0su ers from fep.
but what should we estimate as the domain for the function ?
it has to be larger than pp0as it needs to contain states that could occur atppinp the ghost program i.e.
states in pp.
one way of estimating pp pp0is to consider the weakest precondition of the outputs of the iut for all execution paths throughpp0under .
this is the domain pp0 .
the consequence of a strong correlation here is that the squeeziness based metric would tell the tester whether or not it is necessary to use optimisation to minimise the probability of fep.
.
hypothesis consider the same setup as for the previous hypothesis.
similarly let r0be the sub program of p0which is backwardly reachable from pp0so that pp0is also the set of outputs from applying to i. hypothesis .there is a correlation between the probability of fep for all input states that reach pp0via the execution ofr0and sq pp0 sq pp0 except when sq pp0 .
this is the same as hypothesis but in addition we consider the sub program r0.
this may potentially squeeze multiple input states in ionto states in pp0that in turn su er from fep multiplying the e ect of q0.
to account for this we add the conditional entropy of on the input states that get mapped to pp0.
if has zero squeeziness on pp0 there should be no possibility of fep and no need to consider the multiplier e ect.
the consequences of a strong correlation are the same as for hypothesis .
in this case we are merely interested in which correlation is the stronger.
in the next two hypotheses we examine how we can provide an optimisation when the squeeziness metric is relatively high.
.
hypothesis consider an execution path inp0that covers the faulty component c0.
in general there may be many inputs to p0 that follow and correspond to states at both pp0and the exit point for p0.
let the outputs that reach the exit point forp0along be i. hypothesis .there is a correlation between the probability of fep for all states that reach pp0via execution along i.e.
pa e0 and sq i again we unpack and motivate the conditional entropy metric.
previously we considered all states that reach the program point pp0immediately after executing the potentially faulty program component c0.
here we consider the states that reach the program point only via a single covering path for c0 .
examining figure an input to both p andp0will reachppandpp0respectively via the same upper path modulo c c0 but may have di erent lower paths.
under assumption both of the lower paths are paths in q0so the degree of fep for inputs to p0that reach pa e0 depends on the degree to which q0is colliding states at pp andpp0to produce states in i. we estimate the states in pp iby considering the weakest precondition for q0but this time with a post condition of i. the consequence of a strong correlation in this case is that it gives us a means to rank covering paths for c0using squeeziness.
we can choose the least squeezy path and have con dence that by using that path to generate a test input that covers c0 we have maximised or at least improved the probability that our choice of test input will produce a failing test output in the case that c0has a bug.
.
hypothesis let a path covering a construct c0be expressed as the concatenation of two paths so that u las above in section where uis the upper path that terminates at pp0 e0 and lis the lower path that follows pp0 the part of beginning at the target node of e0 .
there is the possibility that usqueezes inputs onto states at pp0which magni es the degree of fep caused by the squeeziness of q0on pp0 pp.
in a way similar to hypothesis we add the conditional entropy of the upper path to improve the correlation.
hypothesis .there is a correlation between the probability of fep for all states that reach pp0via execution along i.e.
pa e0 and sq pa e0 sq i as in hypothesis our aim is to test whether the addition of this upper path conditional entropy improves the correlation.
.
hypothesis in a previous paper we speculated that the squeeziness of lon pp0would be correlated with the probability of fault masking for inputs that reach pp0 .
hypothesis .there is a correlation between the probability of fep for all input states that reach pp0via execution along and sq pa e0 578the consequences of this correlation are that we would only need to rank covering paths for a construct on the basis of the squeeziness of the lower path on the states that occur atpp0along path .
in comparison to metrics in earlier hypotheses this would be comparatively cheap to estimate.
.
hypothesis we know that for a function fand domain dwhere sq f d that fmust be one to one.
so we expect that if a squeeziness metric is zero then fmust propagate error states in dto the output of the function without any collisions i.e.
that the probability of fep is zero.
that is the theory.
however assumption may weaken this so we need to test the hypothesis experimentally.
hypothesis .let 0be a small number arbitrarily close to .
whenever sq i then p fep .
the consequence of this hypothesis being validated is that a squeeziness close to zero for a path means that we don t need to rank the covering paths but can simply use that path to generate a test input to cover the program construct.
.
experimental setup .
subjects and mutants three sources of subject programs are shown in table where we aggregate lines of code for each project with all programs being written in c. the toyproject contains small programs that we implemented based on designs that aimed to demonstrate squeeziness.
the other subject programs were taken from two real world projects the r project for statistical computing and graphics and the open source statistical package gretl gnu regression econometrics and time series library .
table projects under investigation project function total loc mutants toy r 221k gretl 286k the two real world projects contain many functions and so we chose entrance functions those directly called by a user or a program from outside of the project.
for r as shown in column of table we selected the functions with the most lines of code locs from the nmath library of r a c library of special mathematical functions.
however to simplify the experiments we did not use functions that contained array variables.
to add variety another three subject programs were chosen from the cephes library of gretl see table .
the functions from randgretl require other functions from their libraries.
we therefore formed subject programs by recursively including the required functions with column of table giving the numbers of functions involved.
columns and give the loc and physically executable lines of code sloc of the subject programs.
we generated di erent versions of programs by seeding a fault into each original subject program as done in mutation testing .
the faults were introduced by using thec mutation operator oaan that replaces an arithmetic operator with another for example with or with .
the mutation tool smt c was used to generate the mutants mutated programs and to run the mutation analysis .
some of the mutants could not be compiled and so were not used in the experiments.
to calculate fep both strong and weak mutation analysis were applied1.
finally several subject programs rhyper ptukey qgamma psiand qt led to too many mutants and in these cases we randomly selected mutants.
the numbers of mutants used in the experiment are given in table .
table real world statistical subject programs project function c files loc sloc rbratio rhyper gamma cody ptukey qgamma psi pnorm both pnchisq raw gammafn gretlqt i0 k0 unity .
experimental design in the experiments randomly chosen inputs were sampled from a uniform distribution.
this allowed us to estimate quantities.
for signi cance we have relied on a sample size which is relatively large over all programs and can be viewed as all inputs for a restricted input domain.
as all subject programs have numeric inputs the rngpack library was used to generate the random numbers .
initially we generated inputs from the complete ranges of the input parameters but we found that most inputs led to special numbers nan and infinite as output.
to address this problem for each program we limited the input domain from which inputs were chosen based on the rst level guards of that program.
this led to smaller input domains but useful test cases.
for example inputs for gamma cody were drawn from .
we then ran strong weak mutation analysis using smt c and the randomly generated inputs.
as the weak mutation analysis functionality of smt c is implemented based on the gnu debugger gdb it was possible to use gdb commands to extract the runtime program states and this allowed us to extend smt c to support information ow analysis.
we executed each subject program and each of its mutants with the same inputs and recorded the state after the mutation point at program point ppin the original programpandpp0in the mutant p0in figure the state at the end of the program oin the original program pando0 in the mutant p0in figure and the execution path taken by the mutant.
we ignored any inputs that led to invalid 1see section for a description of weak and strong mutation testing579outputs such as those that are not a number are in nite or that cause an exception.
the executions of the more than million test cases generated more than gigabytes of raw result les.
given mutant p0 the following shows how we calculated the probability of fault masking p fep on the inputs used.
p fep of tests that weakly kill p0 but do not strongly kill p0 of tests that weakly kill p0 note that the way we counted the total number of tests in the denominator varied between experiments.
in exp1 and exp2 we counted the tests that reach pp0via any execution path through c0.
in experiments exp3 exp4 and exp5 we counted the number of tests that reach pp0by following a single execution path to output.
for a test input to weakly kill a mutant it must lead to di erent program states for the mutant and the original program after the mutation point in figure the states atppinpandpp0inp0should be di erent .
a test input strongly kills a mutant if it leads to this di erence in program state propagating to the program s output in figure oin pan di erent .
therefore p fep computes the proportion of tests that cause a di erent program state after the mutation point at ppinpandpp0inp0but do not lead to a di erent output.
we classi ed tests according to whether they are able to propagate the seeded fault to the output.
the tests that successfully propagate faults have the property ep error propagation .
the rest of the tests su er from failed error propagation fep and other coincidental correctness cc1 .
tests classi ed as cc2 su ered from anomalies.
table lists the proportion of tests that have these properties.
the signi cant statistic is that approximately of tests across all program mutant pairs su ered from fep.
table the proportion of randomly generated tests for all subject programs that are weakly and strongly killed.
weakly killed strongly killed proportion ep yes yes .
fep yes no .
cc1 no no .
cc2 no yes .
given our complete knowledge in the mutation testing scenario of the experiments we can replace pp0 with the quantity it estimates namely pp pp0in exp1 and exp2.
similarly we can use the appropriate subset of pp pp0 calculated by direct examination of the data to replace its estimation i in exp3 exp4 and exp5.
this gives a maximal correlation useful when seeking to compare correlations and rate the strength of correlations.
we now outline the experiments performed.
.
.
experiment exp1 this experiment explored the strength of the correlation suggested in hypothesis by estimating the correlation between the probability of fep for inputs reaching pp0and sq pp pp pp0 can be calculated as follows.
first we set s j pp pp0j then we set s j pp pp0jslog2 s log2 s .
given outputo the probability for ogiven pp pp0is t o of states in pp pp0that lead to o j pp pp0j and consequently t p ot o log2 t o and the squeeziness forq0on pp pp pp0 s t .
.
experiment exp2 this experiment explored the strength of the correlation suggested in hypothesis .
the experiment is the same as exp1 with the di erence being that if sq pp pp0 we add to it sq pp0 i.e.
the squeeziness of r0 on the inputs that r0maps to states at pp0.
for each mutant the squeeziness of r0on the domain can be calculated as follows.
the probability assigned to the inputs isi log2 j ij recall that iis the set of inputs used .
the probability assigned to a state pp0is u of inputs that get mapped to byr0 j ij and we letu p u log2 u .
then the squeeziness ofr0is sq pp0 i u. if the squeeziness of q0is non zero then the squeeziness ofr0is added to it.
.
.
experiment exp3 for a given path which executes the component of interest p fep for states occurring on the path at pp0correlates withsq .
given path we will let pp0and ppbe the sets of states occurring on the path at pp0and pprespectively when following inp0and its equivalent in prespectively.
for each path in a mutant associated values of p fep and the squeeziness of q0 sq q0 can be calculated using an approach similar to that in the description of experiment exp1 except that the values used are speci c to .
thus we assign the probability s j pp pp0j then we set s ps log2 s .
the probability for the corresponding outputs for is given by t o of states in pp pp0that lead to o j pp pp0j and consequently t p ot o log2 t o .
the squeeziness forq0with respect to at pp pp0is then given by sq pp pp0 s t. .
.
experiment exp4 this experiment explored the strength of the correlation suggested in hypothesis .
the experiment is the same as exp3 except that we add to sq pp pp0 the squeeziness of the upper path u on the inputs to the program that follow execution path .
the latter can be expressed assq since pa e0 .580let .
the information content of the inputs that travel down path isi log2 j j .
the probability assigned at pp0is r of times that9s2 s j j and this leads to the term u p r log2 r .
then the squeeziness of uis given by sq i u. .
.
experiment exp5 this experiment assessed the correlation between the probability of fep on states in pa e0and the squeeziness of l on these states.
for each path in mutant p0 p fep for states that occur on atpp0 pa e0 can be calculated as in experiments exp3 and exp4.
to calculate squeeziness sq we have to calculate the probability distributions on pa e0 atpp0and iat the end of the program.
to calculate the latter let i .
it is su cient to determine for each output o how many inputs that follow lead too.
the probability assigned to oin ifor a given path is m o of inputs from that lead to o j j and we setm p om o log2 m o .
the probability assigned to state atpp0for is l of inputs from that lead to j j and we set l p l log2 l .
then the squeeziness is calculated as sq l m. .
.
experiment exp6 we examined various upper bounds near zero and found the maximum observed value for sq pp pp0 less than the bound and its corresponding value for p fep using the pairs calculated for exp1.
we looked at a small sample of three bounds and but examined a large number of pairs for each bound.
.
results the experiments inevitably have limitations.
we did not use a variety of mutation operators that for example delete statements replace boolean relations with others or replace boolean subexpressions with true or false.
however our approach is independent of types of faults as it is semantics based and considers program points and incorrect states.
we only generated mutants with a single fault while in practice programs may contain multiple faults but this is a common assumption in testing.
we aim to address this in future work.
we sampled inputs rather than considering all inputs.
this was the only way to make the experiments practical.
this necessity may serendipitously be the foundation of a sampling approach to estimating squeeziness metrics in the future.
although we did not consider formal statistical guarantees the sample size across all programs was large.
information from both ppfrompandpp0fromp0was used when in practice we would only have pp0fromp0.
when testing a program p0 p fep can never be known in practice as we don t have p. since our objective in thistable spearman s rank correlation coe cient for all programs.
experiment correlation exp1 .
exp2 .
exp3 .
exp4 .
exp5 .
table spearman s rank correlation coe cient for statistical programs.
experiment correlation exp1 .
exp2 .
exp3 .
exp4 .
exp5 .
paper was to use strength of correlation to nd the most suitable metrics using knowledge of ppwas not a drawback.
consider the correlations found in experiments .
the experiments computed the di erent metrics and table gives the spearman s rank correlation coe cient for all programs.
in order to understand the contributions from the small set of toy programs we wrote ourselves and the real world programs we looked at the correlations for these two groups separately.
table gives the results for the statistical programs and table gives the results for the small programs.
interestingly we have very strong correlations for experiments and these are particularly strong for the larger real world programs.
experiments and had lower correlation values than experiments and suggesting that the important correlations are with squeeziness of q0on di erent domains and that contributions from the upper program are not signi cant in fact retrograde.
in contrast experiment did not reveal a correlation rather invalidating the suggestion of this metric in our ipl paper .
these results give a very strong correlation for between information theoretic metrics and both fep and fep along a particular path.
we reproduce here the three plots of rank correlations corresponding to the table entries for experiment .
figure plots the ranks given to squeeziness and p fep when calculating spearman s rank correlation coe cient.
figure plots the ranks given to squeeziness and p fep for the statistical programs.
figure plots the ranks given to squeeziness and p fep for small programs.
table spearman s rank correlation coe cient for small programs.
experiment correlation exp1 .
exp2 .
exp3 .
exp4 .
exp5 .005787581figure the rank correlation of p fep and squeeziness for all programs exp2 .
figure the rank correlation of p fep and squeeziness for statistical programs exp2 .
figure the rank correlation of p fep and squeeziness for small programs exp2 .
table maximum p fep for all programs sq q0 range max sq q max p fep .
.
.
.
.
.
.
.
.
the plots show that the strong correlation for exp2 is in the most part derived from the larger real world statistical programs.
table gives the maximum values for squeeziness of q and p fep for a given small bound on sq q values over all programs using information from exp1.
the results validate theory and could be used to identify program constructs in an implementation under test that are highly unlikely to su er from fep.
.
conclusions and further work our analysis and the results of our experiments have shown that we can interpret failed error propagation during software testing using conditional entropy based metrics on the implementation under test.
this is a novel use of quantied information flow a concept whose applications have to date been in the area of secure information ow .
an enormous vista of possible future work now beckons.
having identi ed useful metrics the next task is to repeat the experiments using estimates of the weakest preconditions that appear in the metrics.
we expect that the rank correlations will be weaker but still highly signi cant.
the success of these experiments would put the utility of the approach beyond doubt.
beyond that lies the problem of estimating the metrics.
estimating entropy for the absolute values used in the nal experiment will be more di cult but not impossible .
an interesting challenge in the long run would be application in a concolic testing scenario such as that o ered by pex .
we believe this paper lays the foundation for signi cant future improvement in test suite e ectiveness.
.