n way model merging julia rubin ibm research at haifa israel and university of toronto canada mjulia il.ibm.commarsha chechik university of toronto canada chechik cs.toronto.edu abstract model merging is widely recognized as an essential step in a variety of software development activities.
during the process of combining a set of related products into a product line or consolidating model views of multiple stakeholders we need to merge multiple input models into one yet most of the existing approaches are applicable to merging only two models.
in this paper we define the n way merge problem.
we show that it can be reduced to the known and widely studied np hard problem of weighted set packing.
yet the approximation solutions for that problem do not scale for real sized software models.
we thus evaluate alternative approaches of merging models that incrementally process input models in small subsets and propose our own algorithm that considerably improves precision over such approaches without sacrificing performance.
categories and subject descriptors d. .
design tools and techniques computer aided software engineering case d. .
software engineering distribution maintenance and enhancement restructuring reverse engineering and reengineering general terms design management keywords model merging combining multiple models weighted set packing.
.
introduction model merging combining information from several models into a single one is widely recognized as an essential step in a variety of software development activities.
these include reconciling partial and potentially inconsistent views of different stakeholders composing changes made in distinct branches of a software configuration management scm system and combining variants of related products into a single copy software product line spl representation .
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
esec fse august saint petersburg russia copyright acm ... .
.merging is usually seen as a combination of three distinct operators compare match andcompose all of which have been extensively studied in the literature .
yet while many aforementioned activities require combining multiple sources of input together the existing work mostly focuses on merging just twoinput models .
a straightforward approach to merging multiple inputs is then to do so in the pairwise manner i.e.
merge n models using n 1pairwise operations.
a subset based approach further generalizes on that allowing to fix a certain number of input models that are processed in each iteration.
however the quality of the result produced using subset based approaches as well as the effect of the chosen subset size or the order in which the input models are picked remains unclear.
in this paper we take a closer look at the problem of n way merging.
we refine existing compare match and compose definitions to consider tuples rather than pairs of elements from multiple distinct input models.
we focus on the matching step the most challenging among the three and show that it can be reduced to the weighted set packing problem which is known to be np hard .
we study and compare the state of the art approximations to that problem both theoretically and empirically exploring their applicability to merging real life models of a reasonable size.
we discover that the scalability of these approximation algorithms is limited to a small number of small models and thus that they cannot be directly applied to real life cases of model merging.
further we investigate the quality of the subset based incremental approaches and propose our own polynomial time heuristic n way merging algorithm nwm that considers all input models simultaneously rather than processing them in any specific order.
we empirically evaluate the quality and the performance of nwm by comparing it to the subset based approaches using as subjects two real life and randomly generated models.
we show that nwm outperforms the alternative approaches in the majority of the cases without any significant performance penalty.
contributions.
this paper makes the following contributions .
we formally define the n way model merging problem.
.we provide a theoretical and an empirical evaluation of the state of the art approximation algorithms for n way matching via the weighted set packing problem.
.we describe a number of polynomial time subset based approaches for n way matching as well as contribute a novel heuristic algorithm nwm that simultaneously considers all ninput models together.
.we provide a theoretical and an empirical evaluation of the proposed algorithms showing cases where nwm significantly outperforms the alternative approaches.
the rest of the paper is structured as follows.
section describespermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse august saint petersburg russia copyright acm ... .
301m3 m2 m1 nurse physician name office nurse id ward care taker id office name ward a modelsm1 m2andm3.
m1 m2 m3 id office name care taker physician nurse id ward name b merged models m1 m2andm3.
m4 m6 m5 nurse name ward care taker name ward physician name office nurse name ward c modelsm4 m5andm6.
m4 m5 m6 name ward care taker nurse physician name office d merged models m4 m5andm6.
figure example uml models.
the running examples and illustrates n way merging.
section gives background on the classical merge problem.
we define n way merge in section and study the applicability of existing solutions to this problem via reduction to the np complete weighted set problem in section .
we describe polynomial time n way matching approaches including the novel nwm algorithm in section .
we evaluate these approaches theoretically and empirically in section .
we discuss related work in section and conclude in section .
.
example we illustrate the n way model merging problem using small uml model fragments inspired by our health care case study see figure a .
the first model fragment m1 contains a single uml class caretaker element which has two attributes idthat uniquely identifies the caretaker person and office that specifies where the person can be found.
the second fragment m2 also contains two classes physician element and nurse element .
both classes have two attributes that help to identify and locate the corresponding persons name andoffice forphysician and idandward fornurse .
when merging these two fragments element from m1can be combined either with element or from m2.
in fact we cannot tell at this point which combination is better in both cases the classes have one shared and one non shared attribute.
thus it is conceivable to combine elements and .
the third model fragment m3 contains a single class nurse element that has two attributes name andward .
as it was already decided to combine elements and in the previous step element can be combined either with the element that corresponds to their union or with element .
none of these results are desired though when considering all three fragments together it becomes apparent that nurses belong to wards while physicians have offices.
thus it seems more appropriate to combine element with and with as shown by bold lines connecting these elements in figure a .
figure b shows the result of the correspondingmerge where elements are annotated by models from which they originate elements and represent the merge of element with and with respectively.
this example illustrates the need and the value of considering multiple models simultaneously as a decision made in a certain iteration of a pairwise merging approach can impede reaching the desired result in later iterations.
it also illustrates the sensitivity of the pairwise approach to the order in which the input models are picked in this case considering inputs in a different order e.g.
mergingm1andm3first would produce the desired result.
ab g efc d h figure optimal matches across models.fixing a particular order cannot guarantee the desired result in the general case though as will be shown by our larger case studies.
intuitively optimal matches are spread across models as schematically shown by four model fragments in figure .
picking any two models without considering the global picture might result in binding elements of the two models with each other instead of keeping some elements available for later more desired combinations.
figure c shows another set of model fragments inspired by the health care case study.
models m4 m5andm6differ from the previous ones because the caretaker class ofm4 element now has attributes name andward making it more similar to the nurse class ofm5 element than to physician element .
also the nurse class ofm5now has the attribute name instead of id.
as the result elements and are combined together while element is just copied over to the merged result as shown in figure d .
this example illustrates the case where the desired result is obtained by combining elements from three distinct models instead of producing pairs like in figure b .
throughout the rest of the paper we use the above examples to showcase the merging approaches being discussed.
.
background model merging in this section we review definitions for merging of two input models following the presentation in which divided the process into three steps compare match andcompose .
compare m1 m2!
is a heuristic function that receives as input a pair of elements from the distinct input models m1and m2and calculates their similarity degree a number between and .
numerous specific implementations analyzing structural and behavioral properties of the compared elements exist.
most of these calculate the similarity degree between two elements by comparing their corresponding sub elements and weighing the results using empirically determined weights .
these weights represent the contribution of model sub elements to the overall similarity of their owning elements.
for example a similarity degree between two classes can be calculated as a weighted sum of the similarity degrees of their names attributes operations etc.
some works also utilize behavioral properties of the compared elements e.g.
dynamic behaviors of states in the compared state machines similar to checking bisimilarity.
match m1 m2 !m1 m2is a heuristic function that receives pairs of elements from the distinct input models m1 andm2 together with their similarity degrees and returns those pairs of model elements that are considered similar.
most implementations of match use empirically assigned similarity thresholds to decide such similarity.
more advanced approaches e.g.
rely on bipartite graph matching a.k.a.
the hungarian algorithm to determine corresponding elements.302finally compose m1 m2 m1 m2!mis a function that receives two input models m1andm2 together with pairs of their matched elements and returns a merged model mthat combines elements of the input in a prescribed way.
specific compose algorithms define how to treat the matched overlapping and unmatched non overlapping elements .
for example the union merge approach assumes that matched elements are complementary and should be unified in the produced result while unmatched elements are copied to the result without change.
it is also possible for the composition to include only the overlap.
another option is to annotate the union merge with origins of each element as we did in figures b and d by adding annotations like to indicate that the attribute idcame from model m1 a.k.a.
the annotative software product line approach .
.
n way model merging in this section we generalize the definition of merging given in section tonmodels.
we formally specify the n way counterparts of the compare match andcompose operators and discuss issues related to their implementation.
we assumeninput models mi i2 of sizekieach.
we usekto denote the size of the largest input model.
each model micontains uniquely identifiable elements e1 eki.
elements of distinct models form n tuples .
for an n tuple t we denote by t the ordered list of input models from which the elements of t originate.
definition .an n tuple a.k.a.
a tuple tis a set of elements satisfying the following properties a it contains at least one element jtj .
b no two elements belong to the same input model jtj j t j. for the example in figure a a possible tuple can consist of elements and from models m1 m2andm3 respectively and is denoted by h1 4i.
h1 4i fm1 m2 m3g.
other possible tuples areh1 2iandh1 4ibut noth1 3isince it contains two elements and from model m2.
in what follows let tdenote the set of all valid tuples for the input models mi i2 .
the size of tis nq i ki accounting for choosing an element from each model including none but disallowing an empty tuple.
.
definition of the operators compare assigns a similarity measure to a given tuple t2t.
we refer to the result of this function as a tuple s weight and denote it by w t .
thus compare is a function that receives a tuple t2tand returns the similarity measure w t for its elements.
the larger the value of w the more similar to each other the elements of tare considered to be.
match considers the compared tuples and selects those that are deemed similar.
a validity function vdecides whether a tuple is eligible to be selected.
it extends a simple threshold based comparison described in section to include more sophisticated validity criteria.
the weight of the set of tuples t tproduced by match is defined as a sum of weight of all tuples in tand is denoted by w t w t p t2 tw t .
the larger the value of w t the better is the produced match.
further match should produce a disjoint set of tuples in this work we assume that an element can only be matched with a single element of any given input model leading to the following definition definition .letvbe a boolean validity function.
then match is a function that returns a set of matches t tthat satisfy the following properties a all tuples are valid t2 t v t true .
b all tuples are disjoint t t02 t t t0 .
c the set tis maximal no additional tuples can be added to twithout violating constraints a and b .
for the example in figure a match can output the tuples h1 2iand h3 4i.
it can also output either h1 4iorh1 4i but not both since otherwise elements and would be matched to both elements and of m2.
tupleh1 3ionly is not a valid result as it can be augmented with the tuple h2 4i.
compose combines elements of each matched tuples in a particular way any of the approaches discussed in section can be applied.
figure b shows a possible result of merging the input models in figure a when compose assumes annotative software product line union merge semantics and match produces tuplesh1 2i andh3 4i.
.
towards implementation of the operators compare andcompose usually encapsulate domain specific information i.e.
which elements of a specific domain are considered similar and how such similar elements should be combined.
numerous works e.g.
proposed implementations of these operators for both models and code taking into account syntactical properties of the considered artifacts.
on the other hand match relies on the result of compare rather than domain specific knowledge and is the least explored operator especially for a collection ofninputs.
thus the main focus of this paper is on the implementation of the n way match step.
yet while a discussion of different ways for performing compare orcompose is out of scope of this paper we need to pick an operational definition of these operators which we describe below.
we assume that each model element contains a set of typed properties and we compare elements based on these.
if model elements are uml classes as in the example in figure a possible properties can be class names attributes methods associations to other classes generalization relationships etc.
for example element in figure a is defined by properties idandoffice of type uml class attribute and care taker of type uml class name.
for simplicity in our presentation we do not consider elements of types other than uml classes although it is easy to extend the element property approach to other types as well.
for an element e we denote by e the set of all of its properties.
similarly for a tuple t he1 emi we denote by t the set of distinct properties of all elements of t t s ei2t ei .
we assume without loss of generality that the goal of compare is to assign high weight to tuples whose elements share similar properties.
for each tuple tand a property p compare considers the number of elements in tthat have that property.
then for each tuplet it calculates the distribution of properties the number of properties that appear in jelements of t. we denote this number bynp j. for the example in figure a the property name appeared twice in tupleh1 4i in classes physician andnurse as well as the property office in classes caretaker andphysician .
the remaining properties namely attributes idandward as well as class names caretaker andphysician andnurse are unique to a single element in the tuple.
such properties differentiate tuple elements from each other and compare should penalize for that.
we thus define the compare function to assign a high weight to tuples with a large number of properties shared by a large number of elements w t p j mj2 np j n2 j t j.303the result is normalized by the number of input models n2 rather than the size of the tuple m2 so that tuples with elements from a small subset of input models receive lower scores.
for example the result of applying compare on the tupleh1 2i in figure a is22 only the office attribute appears in both elements of the tuple while there are five distinct properties in total the class names caretaker andphysician and the class attributes id office andname .
applying compare on tuplesh1 3i andh2 4iresults in22 45as well.
compare on tupleh3 4i yields22 applying it toh1 4ialso results in the same value i.e.
.
the weight of a tuple containing a single element e.g.
h1i as well as the weight of a tuple whose elements are completely disjoint e.g.
h1 4i is defined to be zero.
our validity criteria require that each element of a tuple share at least one property with other elements.
for example the tuple h1 4iin figure a is invalid as classes caretaker andnurse have no attributes in common.
indeed such classes should never be matched1.
all of the remaining tuples in this example are valid.
given the compare function and the validity criteria the match with the maximal weight is produced by a combination of two tuples h1 2iandh3 4i.
the weight of this solution is4 which is higher than picking the tuples h1 3iandh2 4iinstead or a single tuple that combines three elements from distinct models either h1 4iorh1 3i.
for the example in figure c the best match corresponds to the solution that has a single tuple h7 10iwith the weight32 .
in this case the value is higher than for any other combination of tuples.
forcompose we assume the annotative spl union merge semantics aligned with our broader research agenda.
its implementation is straightforward elements of each tuple in the solution found by match are merged by unifying their properties and are annotated by the corresponding source model for traceability purposes.
elements of the input models that are not part of any tuple produced by match are copied to the result as is.
for the example in figure a where match produced two tuples h1 2iandh3 4i the two elements in the merged model in figure b .
i.e.
elements and correspond to the merge of elements in the first and the second tuple respectively.
the merge of the models in figure c is shown in figure d .
here element corresponds to the single tuple h7 10ireturned bymatch .
element corresponds to element of the input modelm5which is not part of any tuple and thus is copied to the result as is.
.
n way matching via weighted set packing in this section we show that the n way matching problem is reducible to the well known np hard problem of weighted set packing .
we then consider the applicability of the existing approximations of this problem to merging software models.
.
weighted set packing and its approximation algorithms the reduction to the weighted set packing problem easily follows from its definition given a collection of weighted sets of cardinality at mostn in our case approximately kntuples consisting of elements fromninput models together with their weights as calculated bycompare the weighted set packing produces a collection of disjoint sets with the maximum total weight in our case the match .
1for tuples of size two the similarity measure of invalid tuples is but that does not hold for larger tuples.the problem is np hard and thus no solution that is polynomial in the size of the input in our case the set of tuples exists.
there are a number of theoretically bounded approximations to that problem polynomial in kn and thus exponential in n .
their main properties i.e.
approximation factor and time complexity are summarized in the first four rows of table .
the simplest approximation algorithm greedy picks tuples with the maximal weight out of the set of all possible tuples disregarding tuples that contain the elements already picked in the previous iterations.
this algorithm can clearly miss the optimal result picking a tuple with the maximal weight without looking ahead can block selection of combinations with a higher total weight.
since a selection of a tupletwith the weight wcan block the selection of up to nother tuples whose weight cannot exceed w the approximation factor ofgreedy isn i.e.
the weight that this algorithm computes is withinntimes of the optimal.
for the example in figure a greedy might pick the tuple h1 4iwith the weight2 preventing the generation of the optimal solution with tuples h1 2iandh3 4i and weight14 .
for the example in figure c greedy does find the optimal solution the tuple h7 10i.
a number of approaches improve on greedy by combining it with different types of local search .
they start from the solution found by greedy and iteratively attempt to improve it by selectingsdisjoint tuples that are not part of the current solution and trying to swap them with a minimal set of tuples in the solution so that tuples in the solution remain disjoint.
the algorithms vary by the selection criterion for swapping either the total weight of the solution increases or the square of the weight increases the selection of the swapping candidate either the first set that satisfies the swapping criterion or the best one and by the size sof the candidate set.
for the example in figure a if greedy produced the solution consisting of h1 4i such approaches can swap it with the pair of tuplesh1 2iandh3 4i producing the optimal result.
the approximation factors of these algorithms as given by their corresponding authors are summarized in table .
both greedy and local search based algorithms have high runtime and space complexity.
in fact even keeping all tuples in memory let alone the information about which tuples are disjoint is not possible for more than a few models with only around elements each.
in the last two columns of table we list time complexity for two version of each algorithm the ones that keep all data in memory and the ones that do not.
.
preliminary evaluation the time complexity estimations given in table might be too high for real life scenarios.
also our weight and validity functions reduce the space of tuples considered by the algorithms by assigning zero weight to a large set of tuples.
we thus experimented with applying the proposed approximation solutions to the n way matching problem on real life examples.
we implemented the algorithms in java introducing numerous optimizations including multi threading to improve execution times.
we executed these algorithms on an intel core2quad cpu .33ghz machine using jvm version .
as subjects we picked two sets of models reported in .
the first consists of eight different variants of a hospital system modeled as uml class diagrams.
the system handles services within the hospital and describes the role of the hospital staff its wards different types of patients and more.
a small snippet of the models is shown in figure a .
in total there are classes in all eight models with an average of .
classes per model.
the largest model has classes while the smallest has .
classes have .
attributes on average including name associations inheritance rela 304table matching algorithms.
approximation factor time complexity in memory memory efficient greedy gr n o k2 n o n3 k2n local search pick first n so kn s n s n o kn s n s n3 local search pick best n 3o k n n3 o k n n5 local search squared pick best n 2o k n n n4 o k n n n6 pairwise pw o n k3 greedy gr3 o n n3 k7 greedy gr4 o n n3 k9 nwm o n4 k4 table execution time for greedy on hospital and warehouse cases.
hospital warehouse k1 k2 k3 k4 k5 k6 k1 k2 k3 k4 k5 k6 n 3n 4n 5n 6n 3n 4n 5n 5s .86m .44m .9h 2s 46s 20m .9h tionships etc.
.
the largest class has attributes.
around of the attributes appear in more than models.
the second set consists of sixteen different variants of a warehouse system also modeled as uml class diagrams.
the system is designed to handle orders track their status provide computer support for warehousing workers keep track of inventory levels in the warehouse etc.
warehouse models have elements in total with an average of .
classes per model.
the largest and the smallest models have and classes respectively.
classes have .
attributes on average with attributes in the largest class.
around of the attributes appear in more than models.
a complete description of both case studies can be found in .
our experiment showed that none of the algorithms scaled to operate on the complete set of the input models i.e.
none achieved termination after hours.
instead we tried running the algorithms on the first three four five and six models from each of the case studies in the order of their appearance in .
algorithms based on local search failed to terminate after hours even on a threemodel subset.
execution times for greedy are shown in table .
kis capture the number of elements in each model.
even though it might be possible to come up with more efficient implementations of the algorithms than ours the results indicate that generally the algorithms do not scale well.
greedy seems to be the only feasible approach and only for merging up to five small models with elements .
this calls for a different solution for the n way merging problem polynomial in both kandn.
we explore it in the remainder of the paper.
.
polynomial time approach to n way matching in this section we discuss algorithms which are polynomial both in the number of input models and in their size.
first we define a set of solutions that incrementally combine all input models in small subsets and discuss their properties section .
.
we then present a novel algorithm nwm that considers all input models simultaneously section .
.
unlike the approximation algorithms for the weighted set packing problem the algorithms presentedin this section are not guaranteed to produce an answer within a particular factor of the optimal.
we empirically evaluate the algorithms in terms of their quality and scalability in section .
.
subset based approaches a straightforward solution for merging ninput models is to do so in smaller subsets e.g.
in pairs performing n 1pairwise combinations.
to do so we maintain a pool of models with all input models initially in it.
the algorithm iteratively selects and removes a subset of models from the pool merges them together and puts the result back into the pool for further merging with additional models.
subsets of size two can be merged using the bipartite graph matching algorithm which produces a disjoint set of matches with the maximal total weight.
the algorithm is based on combinatorial optimization techniques and solves the matching problem in time polynomial in size of the larger input model returning an optimal result .
larger subsets of three or four models can be merged using thegreedy algorithm described in section .
.
applying greedy on more than four models or applying additional more sophisticated algorithms does not scale well as shown in section .
.
we thus define three subset based algorithms pw pairwise matching gr3 greedy on subsets of size and gr4 greedy on subsets of size .
these are summarized in table .
the quality of the result produced by these algorithms in terms of the total weight of the solution largely depends on the order in which the models are picked.
for the example in figure a selecting models m1andm2first can result in merging elements and with each other as this combination is indistinguishable from the more desired combination of and since both pairs of elements have the same weight.
as the result of this selection it is impossible to generate the merged model shown in figure b where element is combined with while is combined with .
picking modelsm1andm3first could produce a better result if the highly dissimilar elements and are not merged.
then during the next iteration these elements could be combined with elements and fromm2 respectively producing the desired combination.
the305above discussion also makes it clear that subset based incremental algorithms have no theoretical approximation factor since any merge of two elements produced in a given iteration can prevent future more efficient combinations.
to consider different orderings of input modes we define and evaluate three variants of each subset based algorithm.
the first picks and processes the input models in the order of their appearance i.e.
at random .
we denote such algorithms by pw gr3andgr4.
the second variant arranges the input models by size in ascending order with the corresponding algorithms denoted by pw gr3 andgr4 .
the third variant arranges them in descending order with the corresponding algorithms denoted by pw gr3 andgr4 .
for the example in figure a algorithm pwpicks models m1and m2first pw picks models m1andm3 while pw picks either m2andm3 orm2andm1.gr3 gr3 andgr3 are equivalent in this case as there are only three input models in total so the ordering does not make any difference and gr4algorithms are not applicable at all .
we evaluate the relative effectiveness of these algorithms in section .
we also experimented with the idea of ordering input models by their cohesiveness i.e.
first merging those that are most similar but observed that there is a strong correlation between this approach and the size based ordering larger models produce more matches which increases the total weight of the overall result and thus the similarity of these models.
.
the algorithm nwm in this section we present a novel algorithm for n way merging nwm which considers all ninput models together and thus does not depend on any particular order of model selection see algorithm .
its main idea is based on picking optimal matches from distinct models and incrementally grouping them until a maximal set of tuples is produced.
the algorithm obtains as input a set of tuples t2tand outputs either a set of tuples s2tover the same elements with improved total weight w s w t or the input set t if such improvement is not possible.
brief overview of the algorithm.
in the first iteration elements of all input models m1 mnare represented by individual singleelement tuples.
for the example in figure a the tuples are h1i h2i h3iandh4i.
pairs of input tuples are assigned weights line and matched using the bipartite graph match algorithm line .
for each input tuple the algorithm selects either zero or one matching counterpart maximizing the total weight of the result.
for the example in figure a tuples h1iandh2i as well ash3i andh4i are matched with each other after the first iteration of the algorithm.
matched tuples are further processed and unified lines producing tuplesh1 2iandh3 4iin the above example.
the unified tuples are used as input to the next iteration of the algorithm line .
the algorithm terminates when no matches of the input tuples can be made and thus no improvements can be achieved lines .
detailed description of the algorithm.
in every iteration all pairs of input tuples are assigned weights.
these weights are further used by the bipartite graph matching algorithm to produce an optimal match a disjoint set of tuple pairs with the maximal total weight.
since matched tuples are further unified we assign a pair of tuples t1 t2 the weight of the tuple that corresponds to their union see the otherwise clause in line of algorithm w t1 t2 w t1 t2 .
for the example in figure a w h1i h2i w h1i h3i w h2i h4i 45andw h3i h4i .
some pairs correspond to tuples that should never be unified e.g.
algorithm nwm t t s t w s w t loop for all t1 t2 tdo w t1 t2 v t1 t2 w t1 t2 w t1 w t2 t1 t2 negationslash t2 o t1 w t1 t2 otherwise end for p match t t w c while p do pick first t t prime p p p t t prime if c c c cn t then if v ci c ci t prime then c end if else if c c c c1 t prime then if v ci c ci t then c end if else c c c c end if end while s optimize c if w s w t then return s else t s end if end loopassign weights to pairs of input tuples proceed with chaining for all matches can append t prime?
if valid can prepend t?
if valid start a new chain no further improvements?
reiteratewhen the unified tuple is invalid w.r.t.
the validity function v. for our example in figure a the tuples h1iandh4ishare no common properties and thus we treat their combination as invalid.
while it is possible to filter such bad combinations after the matching is done preventing their generation in the first place is preferred since the original tuples can then participate in more desired combinations.
we thus assign weight to combinations that are a priori illegal relying on the bipartite graph matching algorithm s ability to ignore pairs of elements with zero weight.
four types of such illegal combinations are described below and encoded in line of algorithm .
pairs whose unification results in a tuple which is invalid w.r.t.
the validity function v e.g.
the pair h1i h4i .
.
pairs for which the weight of the union is lower than the sum of input tuple weights unifying such pairs is clearly not worthwhile.
this situation cannot occur when unifying single element tuples as their weight is zero and the unification can only increase it.
.
pairs that contain elements from the same model that is t1 t2 .
for example h1 2i in figure a isfm1 m2g while h3 4i isfm2 m3g.
unifying these tuples would result in a tuple with two elements and from m2 which is not allowed.
.
pairs introducing circular dependencies between tuples i.e.
if a tuple t1is matched with t2 t2is matched with t3 and t1and t3contain elements from the same set of models the unified tuple would be illegal.
to limit circular dependencies we introduce a partial order o on the set of all tuples such that t1 o t2iff t1 is smaller than t2 in the lexicographical order.
for example h1i oh1 4i oh2i.
a pair of tuples t1 t2 for which t2 o t1 is assigned zero weight e.g.
the pair h2i h1i but not the symmetric pair h1i h2i .
for the example in figure a in the first iteration of the algorithm only four pairs of tuples get a non zero weight h1i h2i h1i h3i h2i h4i h3i h4i .306results of match are represented by the map pwhich relates matched tuples to each other line .
we say that t1is matched with t2if p t1 t2.
if a tuple tis not matched with any element p t isnull.
the map is ordered by the weight of the matches from the largest to the smallest so that the strongest matches are retrieved first.
for the example in figure a two matches are produced in the first iteration of algorithm p h3i h4iand p h1i h2i with the weights2 9and4 respectively.
the example in figure c also results in two matches p h9i h10iand p h7i h9i with the weights4 9and2 respectively.
pairs of matched tuples are subject to internal processing.
first the tuples are chained towards possible unification that incorporates more than two tuples lines if p t1 t2and p t2 t3 the tuples are grouped into an ordered set cconsisting of tuples t1 t2 t3.
more generally for the processed pair of tuples t t0 and the existing chain c if t cn the last tuple of the pair can be appended to the chain producing the chain lines .
otherwise if t0 c1 the first tuple of the pair can be prepended to the chain producing the chain lines .
for our example in figure c the pair h9i h10i is processed first then the tuple h7ifrom the pair h7i h9i is prepended to it producing the chain h7i h9i h10i.
the chain corresponds to the tuple h7 10iwhich contains all three elements.
during chaining the algorithm checks whether the appended prepended tuple causes the union of the chained elements to be invalid w.r.t.
the validity function v. similarly it checks whether the tuple intersects with at least one other tuple of the chain w.r.t.
their set of models while we introduced a partial order on the set of tuples for the pairs of matches p t1 t2and p t2 t3 t1 t3 might not be empty even though t1 t2 and t2 t3 .
in such cases the tuple should not be added to the chain lines and .
when both tuples of the matched pair do not belong to any chain a new chain is started lines .
that is the case for the first pair of tuples h9i h10i in the example in figure c as well as for both pairs of tuples in the example in figure a h1i h2i and h3i h4i .
every tuple added to a chain is removed from p line the chaining process continues until all tuples in pare processed line .
the chaining phase is followed by the optimization phase line in which we check whether the chaining was inefficient i.e.
that it chained too much and splitting a chain cinto smaller sub chains c1 cpimproves the weight of the result w c p i pw ci .
during the optimization step we only verify whether the chain can be broken into one or more parts without trying to reshuffle tuples of the chain and check their possible combinations.
this heuristic is reasonable as the chains are built while putting optimally matched tuples close to each other.
optimized chains form tuples that are used as input to the next iteration of the algorithm line and the process continues until no further improvements can be achieved.
for the example in figure a the algorithm stops after the first iteration producing tuples h1 2iandh3 4i.
no further unification is possible as the combination of these two tuples is invalid it contains two elements from the same model m2.
the result produced by the algorithm in this case corresponds to the desired solution in figure b .
likewise for the example in figure c the algorithm stops after producing the chained tupleh7 10i which also corresponds to the desired solution in figure d .validity of the algorithm.
by construction the algorithm ensures generation of valid matches per definition a all tuples are valid pairs of tuples that share elements from the same model are assigned zero weight and thus cannot be formed see line while the chaining step ensures that chains do not contain elements from the same model see lines and .
likewise pairs of tuples that are invalid w.r.t.
the validity function vare assigned zero weight and thus cannot be formed see line while both the chaining and the optimization steps ensure validity of tuples that correspond to the produced chains see lines and .
b all tuples are disjoint in the first iteration all input model elements belong to separate tuples.
in subsequent iterations a tuple can be added to one chain only see lines and thus becomes part of only one tuple.
by induction an element cannot belong to more than one output tuple.
c the set tis maximal the algorithm starts with the set of all input model elements thus all elements are part of at least one tuple.
it is not possible to expand the solution without violating the above disjointness constraint b .
clearly the algorithm is heuristic and does not produce optimal results in all cases.
yet it is reasonable to believe that it has a high chance of producing good matches because it simultaneously considers best combinations across different models rather than limiting itself to any particular ordering.
we evaluate the algorithm and compare it to subset based approaches in the next section.
.
ev aluation in this section we discuss runtime and space complexity of the polynomial algorithms described in section and then report on an empirical evaluation of the effectiveness of these approaches.
.
theoretical evaluation as can be seen in table the runtime complexity of our nwm algorithm is bounded by o n4 k4 making the algorithm polynomial in both n the number of input models and k the size of the largest input model there are a total of n kelements considered by the algorithm and each iteration reduces the number of input tuples by at least one.
bipartite graph matching of n kelements in our implementation is bounded by n k even though more efficient implementations are also possible the chaining phase is quadratic in the size of the input and the optimization phase is quadratic to the maximal length of the chain.
since all tuples in a chain contain elements from distinct models the length of the chain is bounded by n. the space complexity of the algorithm is bounded by the maximal number of matched pairs that is n2 k2.
the runtime complexity of all pwalgorithms is o n k3 they perform up to niterations each of which takes o k3 .
the space complexity of these algorithms is o k2 making them more efficient than nwm both w.r.t.
time and memory.
the runtime complexity of algorithms gr3andgr4iso n n3 k7 ando n n3 k9 respectively with the space complexity ofo for both cases we implemented the memory efficient version of the algorithms .
that is the gr3 algorithms have similar complexity to nwm while the gr4algorithms are more expensive.
as discussed earlier and indicated in table all of the above algorithms do not have a theoretical approximation factor any selection they make can result in matches that while reasonable in the current iteration block selected elements from participating in tuples with a higher weight.307table weights and execution times of nwm compared to subset based approaches.
for each case the results of nwm and the best subset based approach are boldfaced.
nwmsubset based approaches pw pw pw gr3 gr3 gr3 gr4 gr4 gr4 hospitalweight .
.
.
.
.
.
.
.
.
.
time .7s 1s 1s 1s .1s .1s .6s .6m .2m .0m warehouseweight .
.
.
.
.
.
.
.
.
.
time .9m .4s .2s 1s .4s .9s .8s .9m .9m .1m random modelsweight .
.
.
.
.
.
.
.
.
.
time .6m 1s 1s 1s .5s .6s .7s .3m .8m .1m loose scenarioweight .
.
.
.
.
.
.
.
.
.
time .5m 1s 1s 1s .3s .9s .2s .1m .6m .2m tight scenarioweight .
.
.
.
.
.
.
.
.
.
time .6m 1s 1s 1s .7s .2s .3s .4m .9m .6m .
empirical evaluation we now report on an empirical evaluation of the polynomial time n way merging approaches described in section .
specifically our aim was to answer the following research questions rq1.
how does the performance of nwm compare to the subsetbased approaches?
what are the conditions under which each algorithm is better?
rq2.
how does the size of the selected subset affect the quality of the results produced by the subset based approaches?
rq3.
how does the order in which input models are picked affect the quality of the results produced by these approaches?
subjects.
for our evaluation we used the hospital and warehouse examples from described in section .
.
before merging we performed a simple pre processing step on the models unifying syntactically similar properties e.g.
patient and patients since implementing compare algorithms is out of scope of this work.
we augmented the set of subjects by randomly generated models divided into sets of models each.
the first sets of models mimic the real life examples in terms of the number of models and their sizes the number of properties for each element the total number of properties in all models and their distribution into shared and unshared see section .
.
we refer to those as the random case.
based on our experience with the random case and to gain deeper insights in the qualities of the evaluated algorithms as well as the conditions that affect their performance we generated two additional random cases containing sets of models each.
the cases referred to as the loose scenario andthe tight scenario vary in the number of classes in each model the total number of properties in all models and the number of properties in each class as discussed below.
all models are available at toronto.edu mjulia nwm .
methodology and measures.
we implemented all algorithms in java and executed them on an intel core2quad cpu .33ghz machine using jvm version .
for fairness we used the same implementation of compare and the same validity criteria for all algorithms see section .
each element of a tuple in the solution must share at least one property with other elements of the tuple and the more properties are shared the higher is the tuple weight.for each algorithm we measured the execution times and the weights of the produced solution for each of the case studies.
we focused our evaluation on the matching stage rather than the perceived appeal of the result which is largely domain specific and depends on the definition of compare .
hence we considered an algorithm to be better if it produces a solution with a higher total weight.
we represented the result returned by each subset based approach as a set of tuples calculating the weight of each tuple and the weight of the overall solution as if they were produced by nwm .
the results are summarized in table .
the first two rows show the results for the hospital and warehouse cases while the other three show average results for the random the loose scenario and the tight scenario .
for those cases we merged each set of models individually and averaged the results for all sets within a case.
for comparison we mark in bold the results produced by nwm as well as the best result achieved by any of the subset based algorithms on the same case.
to compare nwm to each of the subset based approaches further we calculated the percentage by which it improved or degraded the weight of the solution compared to the other algorithms.
for example in the hospital case nwm found a match with the total weight of .
compared to .
found by pw.
this improves the matching by .
.
gr4 performed the best of the subset based algorithms achieving the result of .
.
nwm improves that by only .
.
gr4 performed the worst and nwm improves its result by .
for the warehouse case nwm finds a solution with the total weight of .
which improves the best result of .
found bypw by .
for the random cases we calculated the percentage of weight increase decrease of each run individually and summarized the distribution of results as boxplots in figure separately for each case.
on the horizontal axis we distinguish the nine algorithm variants used for comparison.
on the vertical we show the percentage of weight increase decrease produced of our algorithm compared to each variant.
for instance compared to pw the results achieved by nwm in the random case figure a range from .
to .
improvement indicated by the thin black vertical line half of the cases fall into a range between .
to .
improvement indicated by the grey box showing the upper and lower quartile .
the average improvement in this case is also .
indicated by a dia mean hospital warehouse pw pw pw g3 g3 g3 g4 g4 g4 boxplot a random case.
mean hospital warehouse pw pw pw g3 g3 g3 g4 g4 g4 boxplot b loose scenario case.
mean hospital warehouse pw pw pw g3 g3 g3 g4 g4 g4 boxplot c tight scenario case.
figure results of comparing nwm against subset based algorithms.
mond symbol and the median is .
indicated by a horizontal line within the grey box .
in addition we also plot the results of the hospital and the warehouse cases indicated by triangle and square symbols respectively .
the hospital and the warehouse results are also shown in figure b and c together with the respective results for the loose and the tight scenarios.
analysis and results.
the results show that for both the hospital and warehouse cases nwm outperforms the existing approaches w.r.t.
the weight of the found solution i.e.
the relative values are positive and appear above the line in figure .
this result is also seen in the random case see figure a nwm is able to achieve improvement on average compared to pw and at least .
on average compared to gr4.
we also noticed that in the warehouse case nwm was able to achieve a much more substantial improvement than in the hospital mean hospital warehouse pw pw pw g3 g3 g3 g4 g4 nwh boxplot figure relative performance of gr4 on the random case.
case with random being in the middle .
we analyzed the differences between these two cases in more detail.
it appears they mostly differ in the total number of properties in all input models properties in total in the hospital example compared to for the warehouse and the average number of properties in each class .
for the hospital case and .
for the warehouse .
also the range of model sizes differs from classes in each model in the hospital case to classes in the warehouse case.
thus the hospital case appears to be more tight the model size distribution is smaller there is a smaller number of properties in total with more properties that appear in each class.
the warehouse case on the other hand is more loose w.r.t.
these parameters.
we thus produced the loose and tight scenarios varying the number of the above discussed parameters.
indeed our algorithm was able to achieve much more substantial improvements in the loose case see figure b more than on average compared topw and at least .
on average compared to gr3 .
in the tight case see figure c the best improvement of .
on average was achieved compared to gr3 while gr4 outperforms our algorithm by .
.
it is not surprising that subset based approaches perform reasonably well in tight combinations with gr4 being the best in most cases the merged elements are more similar to each other thus allowing these algorithm to form valid matches in each step.
for example even if a subset based algorithm makes a wrong selection the remaining tuples still have a high enough weight as their elements are close to each other.
for such cases nwm performs similarly to the subset based algorithms.
with respect to the execution time of the approaches pwalgorithms are faster than the others terminating in less than one second see table as expected.
execution times for g3ranged between and seconds while for nwm they were seconds to .
minutes.
g4was significantly slower than the other algorithms taking from .
minutes on the hospital case to .
minutes in the warehouse case.
conclusion rq1 our experiments confirm that the nwm algorithm produces better results the subset based approaches in the majority of cases especially in the more loose combinations.
this includes the real life models that we used as input.
the weight increases achieved by the algorithm are substantial while the decreases are rare and minor.
furthermore the running time of nwm is feasible.
comparing the subset based approaches to each other it appears that the greedy approaches perform better than pairwise .
on average gr4 outperformed the other algorithms in the hospital and tight cases while gr4 outperformed the others in the random case.
gr3 was better than the alternative subset based approaches309in the loose case.
warehouse was the only case in which pw outperformed the rest of the subset based algorithms.
to investigate whether it is beneficial to take a more global view i.e.
by combinig input models into larger subsets we compared gr4 to the other approaches on the hospital warehouse and random cases presenting the results in the boxplot view in figure .
the results appear inconclusive most of the average improvements are only slightly higher than so there appears to be no benefit using this algorithm compared to the others.
the figure also shows again that nwm performs significantly better than gr4.
conclusion rq2 selecting larger subsets of input models e.g.
to merge does not have a significant effect on the results.
for each of the subset based approaches we also evaluated the correlation between the quality of the produced match and the order in which the subset of input models is picked.
as shown in table in all but two cases the strategy of combining models by size in the descending order i.e.
from the largest to the smallest produced better results than those using the ascending order.
intuitively we believe that handling larger models first produces a large model as a result providing more choices for subsequent iterations which leads to better matches.
while we saw some outliers in the majority of cases arranging models by size in the descending order was a more beneficial strategy.
conclusion rq3 for subset based approaches the quality of the produced result is sensitive to the order of input models with arranging them by size in the descending order being more beneficial.
.
threats to validity threats to external validity are most significant for our work.
these arise when the observed results cannot generalize to other case studies.
we attempted to mitigate these threats by using two reallife case studies and a considerable number of randomly generated models that mimic the properties of the real ones.
using automatically generated models as a basis for evaluation is by itself a yet another threat to validity.
we attempted to mitigate this by basing our generator on characteristics taken from the real life models and using a combination of generated and real life models for evaluation.
we experimented with a particular weight function described in section .
thus the results might not generalize to other possible weight calculations.
yet we believe that specific calculations of weights are orthogonal to our work as they have a similar effect on all of the compared approaches.
.
related work numerous approaches for model merging focus on merging two inputs with each other .
similarly model and code differencing approaches focus on identifying differences in two usually subsequent versions of the input artifacts.
some works e.g.
propose techniques for detecting many to many matches between the elements of two input models.
our work differs from those by addressing the problem of merging ninput models together for any n .
duszynski et al.
emphasize the need for simultaneous analysis of multiple source code variants identify commonality and variability of those variants and propose an approach for comprehensible visualization of the analysis results.
this work does not attempt to study and optimize the matching step though but rather greedily finds a match for each input element.
approximation algorithms for the weighted set packing problem applicable for the n way matching step are the most relevant to our work see also section .
.
arkin and hassin propose analgorithm based on local search.
the algorithm starts from any solution e.g.
an empty set or a solution found by greedy algorithm.
it then iteratively attempts to improve the solution by selecting s ndisjoint tuples that are not part of the it and trying the swap them with a subset of tuples in the solution if that increases the solution s weight while still keeping it disjoint.
chandra and halldorsson further improve that algorithm instead of any local improvement that increases the weight of the solution the authors propose to find and use the best possible improvement.
the algorithm also assumes that all improvements are of sizen which makes it more expensive computation wise.
berman proposes a yet another improvement to the above algorithm instead of performing the swap with the best possible improvement the algorithm performs a swap if the square of tuple weights is improved.
again this increases the computational complexity of the algorithms.
the exact approximation factors and time complexity of the above algorithms are shown in table .
as discussed in section .
these algorithms do not scale for more than a small number of small models while we aim to provide a practical solution that can be applied for merging real life software models.
.
conclusions and future work merging multiple inputs together is an important task in several software development activities.
these include combining overlapping views of different stakeholder or unifying multiple related products into a single copy software product line representation.
yet most of the existing works focus on merging two inputs together providing little guidance on how to handle multiple inputs.
in this paper we extended the model merging problem to consider ninputs.
we focused on the most challenging step of the merging process matching and showed that the problem is np hard.
we surveyed and evaluated state of the art approximations developed in the literature as well as current practices of incrementally merging ninput models together in smaller subsets.
based on this experience we proposed a novel polynomial time heuristic algorithm nwm that considers all ninput models simultaneously.
we evaluated our approach on a large set of cases including two real life examples and showed that it achieves substantial improvements over approaches that merge input models in subsets without a substantial degradation in performance.
however our approach as well as other available scalable approaches does not provide any theoretical approximation guarantees.
there are several directions for continuing this work.
first we are interested in exploring additional heuristics that could further improve the quality of the merge produced by nwm while keeping its execution time reasonable.
for example one could experiment with the idea of reshuffling chained tuples rather than only breaking the chain into pieces or even reshuffling elements from distinct tuples of the chain.
we also plan to augment the work by integrating it with domain specific compare andcompose approaches and by extending it beyond numerical definition of matching quality e.g.
providing means to differentiate between match solutions with identical total weight.
further analyzing subset based approaches e.g.
comparing all input models to each other and merging them in the order induced by an algorithm for finding a spanning tree is a yet another direction for possible future work.