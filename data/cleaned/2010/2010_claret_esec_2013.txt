bayesian inference using data flow analysis guillaume claret inria france guillaume claret.mesriram k. rajamani microsoft research india sriram microsoft.comaditya v. nori microsoft research india adityan microsoft.com andrew d. gordon microsoft research cambridge uk adg microsoft.comjohannes borgstr m uppsala university sweden johannes.borgstrom it.uu.se abstract we present a new algorithm for bayesian inference over probabilistic programs based on data ow analysis techniques from the program analysis community.
unlike existing techniques for bayesian inference on probabilistic programs our data ow analysis algorithm is able to perform inference directly on probabilistic programs with loops.
even for loop free programs we show that data ow analysis offers better precision and better performance bene ts over existing techniques.
we also describe heuristics that are crucial for our inference to scale and present an empirical evaluation of our algorithm over a range of benchmarks.
categories and subject descriptors d. .
software program veri cation statistical methods general terms algorithms veri cation keywords probabilistic programming algebraic decision diagrams data ow analysis .
introduction we present a data ow analysis for probabilistic programs which can be used to perform bayesian inference.
before delving into details of the analysis we rst give the reader some background on probabilistic programs and bayesian inference.
probabilistic programs are usual programs written in languages like c or java or lisp or ml with two added constructs the ability to draw values at random from distributions and the ability to condition values of variables in a program through observations.
a variety of probabilistic programming languages and systems have been proposed permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
esec fse august saint petersburg russia copyright acm ... .
.
.
however unlike usual programs which are written for the purpose of being executed the purpose of a probabilistic program is to implicitly specify a probability distribution.
bayesian inference is the problem of computing an explicit representation of the probability distribution implicitly speci ed by a probabilistic program.
probabilistic programs can be used to represent probabilistic graphical models which use graphs to denote conditional dependences between random variables.
probabilistic graphical models are widely used in statistics and machine learning with diverse application areas including information extraction speech recognition computer vision coding theory biology and reliability analysis.
they allow speci cation of dependences between random variables via generative models as well as conditioning of random variables using phenomena or data observed in the real world.
a variety of e cient inference algorithms have been developed to analyze and query probabilistic graphical models.
inference algorithms for probabilistic programs are broadly classi ed into dynamic methods such as the gibbs sampling algorithm the metropolis hastings mh algorithm which involve executing the program with random draws and computing statistics on the resulting data and static methods such as message passing and belief propagation.
current approaches to performing static inference on probabilistic programs involve compiling such programs to graphical models such as bayesian networks or factor graphs and using known inference techniques on such models.
for instance in a functional probabilistic program is rst translated into a factor graph and infer.net is used to analyze the resulting factor graph and perform inference.
we propose a new direction for e cient static inference of probabilistic programs based on techniques from data ow analysis.
our data ow facts are probability distributions and our analysis merges data ow facts at join points and computes xpoints in the presence of loops.
however we show that our data ow analysis does not lose precision and performs exact bayesian inference see theorem .
our approach is fundamentally di erent from sampling algorithms which use multiple concrete executions to represent distributions approximately using a set of samples and from message passing algorithms which maintain representations of approximate distributions.
performing probabilistic inference using data ow analysis o ers several advantages.
prior techniques for static inference are restricted to loop free programs .
we are able to statically analyze probabilistic programs with loops using the idea ofpermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse august saint petersburg russia copyright acm ... .
bool c1 c2 c1 bernoulli .
c2 bernoulli .
bool c1 c2 c1 bernoulli .
c2 bernoulli .
observe c1 c2 example .
example .
figure two probabilistic programs.
bool b c b true c bernoulli .
while c b !b c bernoulli .
bool c1 c2 c1 bernoulli .
c2 bernoulli .
while !
c1 c2 c1 bernoulli .
c2 bernoulli .
example .
example .
figure probabilistic programs with loops.
xpoints from the program analysis and veri cation communities.
even for loop free programs we show that data ow analysis o ers performance bene ts over existing techniques.
we are able to perform exact inference and hence compute an answer with better precision than current static techniques which use approximate distributions to scale.
.
probabilistic programs we motivate probabilistic programs and inference using a series of examples.
consider example in figure .
intuitively this program tosses two fair coins simulated by drawing from a bernoulli random variable with mean .
assigns the outcomes of these coin tosses to c1and c2respectively and returns the values of the two variables c1and c2.
the program represents a probability distribution over bernoulli variables c1and c2 where pr c1 false c2 false pr c1 false c2 true pr c1 true c2 false pr c1 true c2 true .
next consider example in figure .
in this program in addition to tossing the two coins and assigning the outcomes to c1and c2 we have the statement observe c1jjc2 .
the semantics of the observe statement classi es runs which satisfy the boolean expression c1jjc2 asvalid runs.
runs that do not satisfy c1jjc2are classi ed asinvalid runs.
the program speci es the generated distribution over the values of the variables c1 c2 conditioned over valid runs which is given by pr c1 false c2 false and pr c1 false c2 true pr c1 true c2 false pr c1 true c2 true .
next we consider probabilistic programs with loops.
consider example in figure .
this program initializes b totrue and cto the outcome of tossing a coin.
then it loops until cbecomes false toggling band assigning tocthe result from a fresh coin toss in every iteration of the loop.
the while loop terminates with probability since for the loop to not terminate cshould be always assigned true from the coin toss the probability of which decreases exponentially with the number of iterations.
the program speci es the generated distribution over the values of the variables b c given by pr b true c true andpr b false c true and pr b true c false and pr b false c false .
the probability pr b true c false is the probability that the program executes the while loop an even number of times and is given by the summation float skilla skillb skillc float perfa1 perfb1 perfb2 perfc2 perfa3 perfc3 skilla gaussian skillb gaussian skillc gaussian first game a vs b a won perfa1 gaussian skilla perfb1 gaussian skillb observe perfa1 perfb1 second game b vs c b won perfb2 gaussian skillb perfc2 gaussian skillc observe perfb2 perfc2 third game a vs c a won perfa3 gaussian skilla perfc3 gaussian skillc observe perfa3 perfc3 figure example trueskill skill rating.
which equals .
the probability pr b false c false is the probability that the program executes the while loop an odd number of times and is given by the summation which equals .
consider example in figure .
this program repeatedly assigns to c1and c2outcomes of fair coin tosses in a loop until the condition c1jjc2 becomes true.
thus this program speci es the generated distribution over the variables c1 c2 given by pr c1 false c2 false and pr c1 false c2 true pr c1 true c2 false pr c1 true c2 true .
the alert reader would notice that this distribution is identical to the distribution speci ed by example .
though observe statements can be equivalently represented using while loops using a simple program transformation illustrated in this example our inference algorithm handles observe statements more e ciently when compared to loops.
also there is no simple transformation that converts any while loop to an observe statement.
consequently we have both observe statements and while loops in our language.
the nal example we use in this introduction is a simplied version of the trueskill skill rating system used by microsoft s xbox live to rate the relative skills of players playing online games.
in example figure we have players a b and c whose skills are given by variables skilla skillb and skillc respectively which are initialized by drawing from a gaussian distribution with mean and variance .
based on the outcomes of some number of played games which is games in this example we condition the skills thus generated.
the rst game was played between a and b and a won the game.
this is modeled by assigning to the two random variables perfa1 and perfb1 denoting the performance of the two players in the rst game and constraining that perfa1 is greater than perfb1 using an observe statement.
note that the performance of a player such as player a is a function of her skill but with additional gaussian noise introduced in order to model the variation in performance we may see because of incompleteness in our model such as the amount of sleep the player got the previous night .
the second game was played between b and c and b won the game.
the third game was played between a and c and a won the game.
using this93model we want to calculate the joint probability distribution of these random variables and use this to estimate the relative skills of the players.
note that each observe statement constrains performances in a game and implicitly the skills of the players since performances depend on skills.
such a rating can be used to give points or match players having comparable skill for improved gaming experience.
in this example the skills skilla skillb and skillc inferred by our tool are skilla gaussian skillb gaussian skillc gaussian .
note that since a won against both b and c and b won against c the resulting distribution agrees with our intuitive assessment of their relative skills.
.
inference calculating the distribution speci ed by a probabilistic program is called inference .
the inferred probability distribution is called posterior probability distribution and the initial guess made by the program is called the prior probability distribution.
for instance in example the prior distribution for skilla isgaussian whereas the posterior distribution is gaussian .
.
.
one way to perform inference is runtime execution.
we can execute the program several times using sampling to execute probabilistic statements and observe the values of the desired variables in valid runs and compute statistics on the data to infer an approximation to the desired distribution.
alternatively a probabilistic program can be compiled to a graphical model over which inference is performed using message passing algorithms such as belief propagation and its variants .
data ow analysis invented by kildall uses a lattice of data ow facts merging at join points and xpoints for loops to compute solutions to several meet over all paths mop analysis problems.
the main contribution of this paper is a new technique to perform inference on probabilistic programs using data ow analysis.
we consider boolean probabilistic programs and in particular the programming language bernoulliprob see section where all variables are boolean and the only distribution allowed is the bernoulli distribution.
this language is similar to the pwhile language which in turn is based on the probabilistic while language in .
our main formal result theorem is the correctness of our inference algorithm with respect to the formal semantics of our probabilistic language.
probabilistic programs with discrete variables over a nite domain can be directly encoded with bernoulliprob without any approximation.
further probabilistic programs with continuous variables can be approximated to boolean programs by approximating continuous distributions with discrete distributions see section .
using such transformations any probabilistic program can be approximately represented in bernoulliprob and analyzed using our technique.
we have implemented our approach and we nd that in several examples we are able to perform exact inference.
if the probabilistic program has a large number of variables explicit representation of joint probability distributions is expensive.
our implementation uses algebraic decision diagram add a graphical data structure for compactly representing nite functions to represent probability distributions and perform data ow analysis symbolically.
forr2r x2vars t bool types uop not unary operators bop andjor binary operators d jtx1 x2 xn declaration e expressions x variable jc constant je1bope2 binary operation juope1 unary operation s statements x e deterministic assignment jx bernoulli r bernoulli assignment jobserve e observe jskip skip js1 s2 sequential composition jifethens1elses2conditional composition jwhileedos1 loop p ds programs figure syntax of bernoulliprob.
large examples where exact inference is infeasible we propose a batching technique where we periodically project the joint distribution to marginal distributions over individual variables to save space at the cost of some approximation.
in our experiments our approximate inference produces results with better precision than other state of the art inference approaches.
this work is related thematically to our other recent work in the intersection of program analysis and bayesian inference.
in particular related recent e orts include using weakest preconditions to perform e cient sampling and using a framework of model learners to do bayesian reasoning .
.
probabilistic programs we start by formally de ning boolean probabilistic programs.
figure shows the syntax of bernoulliprob .
the only type t allowed in the language is the boolean type with values true and false .
a program has a declaration of variables x1 x2 xnfollowed by statements.
we use v p to denote the variables of program p ands p to denote the statement body of program p. primitive statements include deterministic assignments bernoulli assignment observe and skip statements.
a deterministic assignment is of the form x e whereeis an expression.
expressions are formed from variables and constants using binary and unary operators.
a bernoulli assignment is of the form x bernoulli r whereris a real number.
an observe statement is of the form observe e whereeis an expression.
a skip statement is of the form skip.
compound statements are formed from primitive statements using sequential composition conditional composition and looping.
the operational semantics of bernoulliprob is given in figure as a probabilistic transition system.
a state of the program with variables x1 x2 xnis a valuation to all the variables.
the domain of all possible states is .
a con guration !is a pairh si where is a state andsis a statement.
intuitively a run of a program returns the nal state which is the rst component of the con guration on termination.
since programs are probabilistic each time we94h x ei!1h skipi h x bernoulli r i!rh skipi h x bernoulli r i!
rh skipi h observe e i!1h skipi if e true h skip si!1h si h s1 s2i!ph s0 s2i ifh s1i!ph s0i h ifethens1elses2i!1h s1i if e true h ifethens1elses2i!1h s2i if e false h whileedosi!1h skipi if e false h whileedosi!1h s whileedosi if e true figure semantics of bernoulliprob.
run a program pwe might get a di erent nal state.
the semantics of the program is the distribution over nal states returned by the program.
we formalize this below.
given a state we use the notation xi to denote the value of variable xiin and the notation e to denote the value of the expression ein .
the probabilistic transition system shown in figure has transition rules of the form form !
!p!
meaning con guration!takes a step to con guration !0with probability p inspired by the transition system for the functional language fun .
we use the notation for the state obtained by updating the value of xin withvand leaving the values of all other variables in unchanged.
the only transition in figure whose probability is not is the one for bernoulli assignment x bernoulli r .
this statement assigns true toxwith probability rand false tox with probability r. the con guration h observe e i transitions to the con guration h skipiwith probability if e is equal to true.
in this case we say that the observation succeeds .
otherwise if e is equal to false then the con guration h observe e igets stuck with no outgoing transitions.
thus implicitly the resulting distribution of a program is conditioned on all observations succeeding.
the sequential composition s1 s2transitions depending on the transition of the rst statement s1.
the conditional composition ifethens1elses2transitions according to how the current state evaluates the expression e. the while loop whileedosalso transitions according to how the current state evaluates the expression e. if the expression evaluates to false the loop exits otherwise it executes the bodysand loops.
arunof a statementsstarting from state and ending in state 0is a sequence !
!
!
!n for n where the following conditions are satis ed !
!p1!
!n!pn!n the initial con guration!
h si and the nal con guration !n h skipi where the statement part is equal to skip signifying termination of execution.
given such a run !
we say that the statement sevaluates to state 0starting at state with probability pr !
p1 pn.
the set of all runs of the statement sstarting from state and ending with state 0is denoted by s .
note that we are only concerned with runs of nite length in s that end in a con guration with a skip statement.
since the language bernoulliprob has loops the number of such runs is potentially in nite.
given a statement s the probability pr s of executing statement sstarting at state and ending at state 0is given by pr s !
s pr !
algorithm theinfer algorithm.
algorithm infer input abernoulliprob programp.
output a posterior distribution overp s output.
ite 8xi2v p xi false post s p n normalize return n note that even though the summation is potentially over an in nite number of runs we are interested in its limit which is always lesser than one.
given a distribution over program states we de ne pr s pr s consider a program pwith variables x1 x2 xnand statements.
let 0denote the state where all variables x1 x2 xnare assigned false and 0denote the dirichlet distribution over states ite where the expression ite e x y evaluates to xifeistrue andyife isfalse .
intuitively the semantics of the program pis the probability distribution obtained by starting the execution ofsfrom initial state o. formally the semantics of pis the distribution pr s .
we note that the semantics given here sums over all the runs of the program.
in section we present our main result of the paper which shows that this summation can be computed by a data ow analysis which merges data ow facts at join points.
we end this section by noting two facts about the semantics of bernoulliprob programs.
first if a program p has non trivial observe statements the resulting distribution pr s is not necessarily normalized i.e.
the sum of the probabilities over all states can be strictly less than .
if we wanted a distribution we can calculate the sum of probabilities sof all the states and appropriately normalize each of the probabilities by s. second we can write pathological programs containing statements such as observe false or non terminating while loops with no terminating executions.
the output distribution of such a program maps all states to probability which is equivalent to saying that the semantics of such a program is unde ned.
.
algorithm algorithm describes the inference algorithm infer for bernoulliprob programs which is based on data ow analysis.
infer takes a bernoulliprob programpas input and returns the joint distribution over the output states ofp see section .
line constructs an initial distribution which is a dirichlet distribution mapping the state with every variable set to false as having probability and all other states as having probability .
recall that the function ite e x y evaluates to xife true andyif e false .
the procedure post line returns the posterior sub distribution over the output of statements s p of programp starting with 0as the input distribution.
post is a recursive procedure shown in algorithm .
the function normalize takes the output of post which is a function from output values of pto and if range f0git returns a probability distribution nover output values given by v2 v recall that is the domain of all possible states .95algorithm thepost computation.
algorithm post s input an input distribution over the states of the program p and a statements output output distribution over the states of the program p switch s casex e return f 0j g casex bernoulli r return r f 0j g r f 0j g case observe e return ite e case skip return cases1 s2 post s1 return post s2 case ifethens1elses2 t ite e f ite e return post t s1 post f s2 case whileedos1 p ?
c while p6 cdo p c22 c post p ifethens1else gskip end while return ite e c end switch algorithm relies on notations introduced in section for valuations and values x e .
distributions c etc normalized or un normalized map valuations to .
let?denote the null map which maps every valuation to .
post operates recursively over the syntax of an input bernoulliprob statements as de ned in figure .
in lines post handles the case when the statement is a deterministic assignment.
in this case the output distribution maps a state to the sum over all the input densities of states 0that equal on executing the deterministic assignment.
lines handle bernoulli assignment.
the output distribution for this statement is a convex combination of the result of the deterministic assignment x true scaled byr and the deterministic assignment x false scaled by r. lines handle the observe statement.
the output distribution which is unnormalized maps a state to the density over the input distribution if the expression e evaluates to true and otherwise.
note that the output distribution here is unnormalized unless eistrue for all states.
lines handle the skip statement which is an identity for post .
lines handle sequential composition by rst computing post over the rst statement and using the resulting distribution to compute post over the second statement.
lines handle conditional statements.
the output distribution is the pointwise sum of the distribution obtained from the if part and the else part .
the if part and else part are recursively computed by applying post on their bodies after splitting the input distribution depending on the condition predicate eto tand f. the nal case lines handles a while loop by computing a xpoint.
it uses two scratch variables pand cto represent the previous distribution and current distribution respectively.
iteratively post is repeatedly applied on the input distribution pwith the statement ifethens1else skip until the output distribution cis the same as the input p. we note that this xpoint is potentially nonterminating even though pand cmay converge in the limit.
in our implementation we terminate the xpoint when the kl divergence between cand pgoes below a certain threshold.
readers familiar with data ow analysis will note that our data ow facts are probability distributions and our algorithm merges these distributions at join points lines and computes xpoints for loops lines .
next we prove that even with such merging of data ow facts at join points this algorithm computes the exact posterior distribution given by the sum over all paths de nition in section .
lemma .for any statement sand any distribution if the post algorithm terminates then pr s post s proof we show that for any statement sover variables x1 x2 xn any input distribution over the program states and any output state we have that pr s post s when the post algorithm terminates.
the proof is by induction over the structure of s and we carry it out by performing a case analysis of all types of statements supported in bernoulliprob .
case deterministic assignment ifs x e we have that pr s pr s f j g pr s f j g pr s f j g f j g from figure f j g post x e from line of algorithm case bernoulli assignment ifs x bernoulli r we have that pr s pr s f j g pr s f j g pr s f j g r f j g r from figure post x bernoulli r from lines of algorithm case observe statement ifs observe e we have that pr s pr s f je trueg pr s f je falseg pr s f je trueg ite f je falseg from figure ite e post observe e from line of algorithm 96case skip statement ifs skip we have that pr s pr s pr s f j g pr s f j g from figure post skip from line of algorithm case sequential composition ifs s1 s2 we have that pr s pr s pr s1 pr s2 from figure pr s1 pr s2 pr s1 pr s1 pr s1 pr s1 post s1 pr s1 by induction pr post s1 s2 post s1 s2 from lines of algorithm case conditional composition ifs ifethens1elses2we have that pr s pr s f je trueg pr s1 f je falseg pr s2 from figure ite e pr s1 ite e pr s2 post ite e s1 post ite e s2 by induction post ifethens1elses2 from lines of algorithm case while loop ifs whileedos1we have that pr s pr s f je falseg f je trueg pr s1 s from figure pr ifethens1 selse skip post whileedos1 from lines of algorithm our main theorem which states that the data ow analysis algorithm computes the exact posterior distribution as speci ed by the sum over all paths semantics is stated below and follows directly from the above lemma.
theorem .letpbe abernoulliprob program with nvariablesx1 x2 xnand a statement s. let obe a state which assigns all nvariables the value false and let obe the dirichlet distribution which maps such a state oto probability and all other states to the probability .
then if thepost algorithm terminates then the output distribution ofpis given by the infer algorithm.it is important to note that the post algorithm might not always terminate.
consider the following example program bool x true while true x !x return x in this program xis initialized to true and toggles in every iteration of the loop.
thus the probability distribution c computed by the post algorithm while analyzing the while loop is given by ite x at the end of even number of iterations of the loop and ite x at the end of odd number of iterations of the loop.
hence the post algorithm does not terminate on this example.
this example is reminiscent of markov chains with periodic cycles.
in section we show that in several real world examples where such oscillating behaviors do not occur the post algorithm is indeed able to terminate and produce useful results.
.
discretization we extend bernoulliprob to handle continuous distributions.
there are two phases.
first we extend bernoulliprob to support discrete distributions over nite sets.
next we show how we can approximate a continuous distribution as a discrete distribution over a nite set which e ectively shows that a probabilistic program de ned over continuous distributions can be approximated by a bernoulliprob program.
letdbe a distribution over elements of a nite set sof cardinalityjsj .
then we can encode elements of susing tuples of logjsjboolean variables in the standard way that is using boolean tuples in f0 1glogjsj.
thus using this encoding we are able to model das a distribution over tuples of boolean variables thus reducing a program de ned over arbitrary nite distributions to a bernoulliprob program.
now consider a continuous distribution n. for ease of exposition we assume that nis a gaussian distribution whose probability density function is de ned as follows f x 1p e x where and 2are the mean and variance parameters respectively.
for a suitable choice i2n de ne an interval such that a i andb i .
for some w de ne the following set.
s a kb a wj0 k w we de ne a discrete distribution dover the elements of swhich is a discrete probability mass function that approximates the gaussian probability density function f x as follows.
d x2s i w nzx b a w xf x dx whereiandware parameters that control the degree of approximation and n zb af x dx a normalization constant ensuring that dis a probability distribution.97it is easy to see that the degree of approximation is controlled by the parameters iandw.
in particular the approximation improves with i!
andw!
.
therefore with the two reductions described above we are able to reduce probabilistic programs de ned over continuous and nite distributions to bernoulliprob programs.
.
implementation algebraic decision diagrams.
recall that our algorithm in section maintains joint probability distributions at every program point.
a probability distribution is a real valued function over variable values.
for example after executing the fragment x bernoulli we obtain the function x representing the distribution which maps both values ofx true and false to .
.
as a second example after symbolically executing example figure we obtain the function c1 c2 ite c1jjc2 .
these are functions from tuples of boolean values to real numbers.
one way to represent such functions is using tables similar to truth tables but having probabilities as the range .
however with nboolean variables a table representation has 2nrows and this is infeasible for large n. algebraic decision diagrams adds can compactly represent such functions as directed acyclic graphs.
adds are generalizations of binary decision diagrams bdds invented by bryant .
an add is a directed acyclic graph.
each internal node of the graph is a decision node labeled with a variable name.
each leaf node is labeled with a real value.
each internal node nhas two outgoing edges labeled with and respectively and the target of these edges are called 0successor and successor of nrespectively.
each add xes a total order among its variables and the ordering of variables in every path respects this total order.
furthermore each variable occurs at most once on a path from the root to the leaf.
adds can be compactly constructed from decision trees by performing the following two reductions until saturation merging isomorphic nodes and eliminating nodes whose successor and successor are identical.
once we x a total ordering of variables and apply the above reductions in any order till convergence the resulting add is canonical regardless of the order in which the reductions were applied .
functions can be manipulated using graph algorithms on their add representations.
for example if f1andf2are two functions represented as adds with sizes jf1jandjf2j respectively the add for operations such as f1 f2orf1 f2 can be obtained using graph algorithms on the adds of f1 andf2with a worst case complexity of o jf1j jf2j .
given a functionfwith a free variable xrepresented as an add we can also obtain the add for 9x fby eliminating xfrom the add using graph operations and potentially doubling the size of the add in the worst case.
we refer the reader to for details of these algorithms.
for instance figure shows the add for the distribution of example from figure part a shows the distribution represented as a decision tree and part b shows the add obtained by applying the two reductions above until none applies.
our implementation of the inference algorithm uses adds for a compact representation over distributions.
each of the operations in the algorithm can be implemented using c1 c2 c1 c2 c2 a decision tree b add figure add representation of example .
add operations as described below.
consider the operation for processing the deterministic assignment statement in line .
let f be the add representation of and let fx0 ebe the add representing the relation x0 e where x0is a fresh variable.
we implement the summation in line as follows first we compute g f fx0 e. next we existentially quantify xfromgto geth 9x g. finally we rename x0toxinhto get the result of post ash and return the resulting add we note that existential quanti cation and renaming are implemented by add packages using graph operations .
the implementation for bernoulli assignment can be done by separately processing the two deterministic assignments x true and x false as above and scaling the two resulting adds by rand r respectively and adding them we note that scaling and adding operations provided by add packages .
the operations in lines and can be directly implemented on adds since iteoperation is supported by add packages.
the xpoint computation in lines can be implemented using the techniques described above and in addition we terminate the xpoint when the kl divergence between cand pgoes below a certain threshold.
each of these operations ite scaling summation equality check are directly supported by add packages such as cudd and takes time proportional to the product of the sizes of the arguments in the worst case.
the normalization operation in algorithm is not directly supported by add packages.
we implement this operation using a bottom up scan of the add in time proportional to the size of the add.
figure illustrates how the infer algorithm proceeds on example from figure .
in particular the post computation on the rst two statements c1 bernoulli and c2 bernoulli results in uniform distributions over c1and c1 c2 respectively.
both these distributions are compactly represented by adds with a single leaf node.
next post processes the statement observe c1jjc2 which results in a subdistribution represented by the add shown in the gure.
finally infer normalizes this subdistribution via the call to normalize in order to obtain the nal add representing the posterior distribution of c1 c2 .98c1 c2 c1 c2 c1 bernoulli .
observe c1 c2 c2 bernoulli .
normalize figure sequence of adds obtained by applying post and normalize to example .
our implementation uses the add library from the cudd package .
our implementation supports a much richer language than bernoulliprob including continuous distributions oating point variables multidimensional array with statically determined sizes for loops etc.
continuous distributions are automatically discretized using the technique in section and algorithm is easily extended to for loops and static sized arrays.
heuristics.
we have implemented the following heuristics for scaling and optimizing our implementation fast exponentiation to speed up the post computation over while loops we employ exponentiation.
we describe exponentiation by way of an example.
consider the program whileedos and an input distribution .
de ne f post ifethenselse skip .
then the post algorithm computes the posterior distribution by applyingfto 0until a xpoint is reached in practice a xed constant number of times .
if we can symbolically represent f then we can e ciently compute f2n by computing f2 f f f4 f2 f2 and nally evaluatef2n .
this can be done as follows f x t where t pr ifethenselse skip with this de nition of f it is easy to compute f2as follows f2 x t2 wheret2 p 00t t since the functiontmaps program states to real numbers it can be compactly represented as an add.
as a consequence t2can be e ciently computed and it follows thatf2nis also e ciently computable.
variable ordering the size of an add crucially depends on the ordering of variables used to construct it.
our compiler implements well known algorithms using the program s variable dependency graph to determine add variable ordering.
batch processing given a joint probability distributionp x1 x2 xn overnvariables the marginal distribution overxiis the projection of the joint distribution to that variable.
formally if each of the xiis a continuous variable ranging over values with a lowerbound and an upper bound u the marginal distribution over a particular variable xi denotedpi xi is de ned as zu zu p x1 x2 xn dx1 dxi 1dxi dxn computation of marginals for discrete distributions is done by replacing the integrals with discrete summation.
with adds such summation is easily implemented by existential quanti cation pi xi 9x1x2 xi 1xi xn p x1 x2 xn when the space required by the add for the joint distribution p x1 x2 xn becomes large we approximate the distribution using its projection to the tuple of marginal distributions as hp1 x1 p2 x2 pn xn i. the latter representation though more concise loses information about correlations between the variables and is therefore less precise.
however when exact inference runs out of memory this technique allows us to perform e cient and approximate inference with a very compact memory representation.
we use marginalization to implement a heuristic called batch processing as follows.
we periodically replace the add for the joint distribution by the component marginal distributions for batches of data for example every nnames in trueskill for some value of n and perform approximate inference.
.
evaluation benchmarks.
we present empirical results from running the inference algorithm on the following benchmarks1 students this example is adapted from the advisorstudent examples in markov logic networks .
we have an array of mstudents nteachers and kcourses and we have information about which teacher is teaching which course and which student attends which course.
the probabilistic program for this example models our belief that if a student sattends a course ctaught by teacher t then we can infer that slikest represented by a bernoulli variable likes s t is true with a certain probability p. the goal is to infer the posterior probability of every random variable in the two dimensional array likes.
friends this benchmark performs probabilistic transitive closure.
given nstudents and an input friendship matrix this is an n nsymmetric matrix f withf a b if student ais a friend of student b and ?
or unknown otherwise we wish to compute the set of all friends for each student.
the probabilistic program in this case encodes a probabilistic 1the source code for all benchmarks is available in .99unfaircoin p x p b true while b b random bernoulli .
if b x x if x .
x x else if x .
x else x return x uniform n g n while g n n g while n n n n if random bernoulli .
g g else g g return g a b figure the unfaircoin and uniform benchmarks.
transitive closure constraint that is the constraint f a b f b c f a c holds with a certain probability.
the objective is to complete the friends matrix fconditioned on the above constraint.
compare we have two n bit numbers where each bit is drawn from a bernoulli distribution.
we want to compute posterior probabilities of these distributions conditioned on the observation that the two numbers are unequal.
trueskill this is the trueskill model the simpli ed version of trueskill is shown in figure .
we have an array of nplayers each of whose skill is drawn from a gaussian distribution.
when player iplays with player j we observe that the performance of playeriin that game another gaussian with mean given by the skill of player i is greater than the performance of player jin that game.
using observations frommsuch games we desire to infer the posterior probability distributions for the skills of each player.
loopy programs the benchmarks onecoin example in section dice unfaircoin figure a and uniform figure b are benchmarks that contain loops.
the benchmark dice figure a and b are examples with complex loops that are beyond the scope of existing probabilistic inference solvers.
the program unfaircoin p simulates a biased coin with meanp.
its parameter is the number dof binary digits used in the discretization of real numbers.
for the experiments we took an arbitrary value of p .
the program uniform n simulates a uniform distribution over the interval .
in order to demonstrate the generality of our approach we also consider the benchmark mcthat is a program representing a markov chain de ned by the following transition matrix results.
all experiments were performed on an .00ghz intel i7 processor system with 4gb ram running microsoft windows .
the maximum memory consumed by add inference in any of the benchmarks is less than 200mb.table comparing runtimes of add inference with expectation propagation ep for loopy benchmarks.
benchmark parameters ep add seconds seconds onecoin .
.
dice .
unfaircoin d ?
.
d ?
uniform n ?
.
n ?
.
n ?
.
mc ?
.
table compares exact add inference with a number of probabilistic inference tools.
each benchmark is associated with a set of parameters that de ne the size of the problem.
the parameters for the student friends and compare benchmarks are students courses teachers and people width respectively.
we compared our tool with samiam an inference engine for discrete models implementing the algorithms shenoy shafer hugin zc hugin andrecursive conditioning and openbugs an inference engine that employs mcmc sampling.
samiam performs almost as well as the add algorithm for the discrete benchmarks.
exact add inference runs of out memory for the friends benchmark with p .
this motivates the need for a scalability heuristic like batch processing at the cost of approximation.
openbugs can quickly give approximate answers on small examples but is very slow for exact answers.
this is due to the fact that with a lot of observe statements it is hard to nd valid paths to compute a relevant answer this is a standard issue with all rejection sampling based techniques.
we give the computation times with a number of iterations set to get a posterior probability with a precision of .
gibbs sampling gs and expectation propagation ep are inference algorithms available with the infer.net toolkit.
as seen from the table add is signi cantly more performant than gs and ep even though gs and ep compute approximate answers .
table reports the results for add inference with batch processing and discretization.
for both the benchmark programs we use batch processing in order to get an approximate solution whose precision is comparable to the solution obtained by ep which is promising as ep is used by trueskill in xbox live .
for example in trueskill we marginalize the skill variables after every set of ngames.
for thetrueskill benchmark add also performs discretization in order to handle continuous distributions.
finally table shows the results of add inference over the loopy benchmarks.
we compare add inference with ep.
since ep does not support xpoints we unroll the loops in the benchmarks a xed number of times and then feed the resulting programs to ep.
except for the simple examples onecoin and dice the ep algorithm was not able to give the expected distributions.
on the other hand add inference converges in a small number of iterations for the loops generally or thanks to the fast exponentiation method.
for the mcexample we are essentially computing an add representation of the transition matrix which can be compact in presence of sparse data and then perform fast exponentiation until convergence we use kl divergence of distributions across iterations to detect convergence .100table comparing runtimes of exact add inference with other approximate inference algorithms.
the ?
entry represents a did not complete .
benchmark parameters shenoy shafer hugin zc hugin rec cond openbugs gs ep add seconds seconds seconds seconds seconds seconds seconds seconds students s c t .
.
.
.
?
.
.
.
friendsp .
.
.
.
.
.
.
p .
.
.
.
?
.
.
p ?
?
????
.
.
comparen .
.
.
.
?
.
.
n .
.
.
.
?
.
.
n .
.
.
.
?
.
.
table comparing runtimes of add inference with batch processing and discretization with expectation propagation ep .
benchmark parameters ep add seconds seconds friendsp .
.
p .
.
p .
.
trueskill matches n .
.
matches n .
.
matches n .
.
matches n .
.
matches n .
.
matches n .
.
matches n .
.
matches n .
matches n .
.
related work there are a variety of probabilistic programming languages and systems .
they perform either dynamic inference either by running the program and performing sampling or static inference by rst transforming the program to a probabilistic model such as a bayesian network and then using well known inference algorithms over the transformed model .
our technique is static and in contrast to previous work perform inference directly over the probabilistic program.
our technique merges data ow facts at join points and hence does not su er from explosion due to a large number of paths in the program.
data ow analysis for frequency counting has been explored before by ramalingam .
geldenhuys et al.
use symbolic execution to estimate the probability of executing parts of a program.
both and are frequentist in nature where the probability of a path is obtained via explicit counting.
on the other hand our work is bayesian in nature where we consider the richer class of probabilistic programs that include sample statements as well as observe statements and conditional distributions.
recent work has explored probabilistic abstract interpretation in the domain of numeric programs .
the idea of using adds for probabilistic inference has been explored before.
sannar and mcallester de ne a ne algebraic decision diagrams to perform inference over bayesian networks and markov decision processes.
kwiatkowska et al.
have used a variants of adds to perform probabilistic model checking in the prism project .
bolzga and maler have used adds to symbolically simulate markov chains .
all these papers study the problem of computing the steady state distribution of markov chains.
markov chains do not support observe statements and it is not clear how to encode posterior probability inference ef ciently in the framework of markov chains.
chavira and darwiche use adds to compactly represent factors in a bayesian network and thereby perform e cient inference via variable elimination.
in contrast we avoid factor graphs altogether and use adds to represent symbolic program states which are distributions at every program point much like a data ow analysis or an abstract interpreter .
furthermore in contrast to graphical models such as bayesian networks our technique can handle probabilistic programs with loops.
.
conclusion we proposed a technique to perform probabilistic inference using data ow analysis.
we have also implemented the technique using adds as a data structure.
we showed that our algorithm indeed computes the posterior probability of a probabilistic program.
we have also presented an implementation which shows promising results.
we believe this approach opens a door to applying other ideas from program analysis and veri cation such as slicing and abstract interpretation for probabilistic inference.
.