ecological inference in empirical software engi neering daryl posnett department of computer science university of california davis davis ca dpposnett ucdavis.eduvladimir filkov department of computer science university of california davis davis ca vfilkov ucdavis.edupremkumar devanbu department of computer science university of california davis davis ca devanbu ucdavis.edu abstract softwaresystemsare decomposedhierarchica lly for example into modules packages and files.
this hierarchical decomposition has a p rofound influence on e volvability maintainability and work as signment.
hierarchical decomposition is thusclearlyofcentralc oncernforempiricalsof twareengineering researchers but it also poses a quandary.
at w hat level do we studyphenomena such asquality distribution collaborationand productivity?
at the le vel of files?
packages?
or modules?
how does the level of study affect the truth meanin g and relevance of the findings?
in oth er fields it has been fo und that choosing the wrong level might lead to misleading or fallacious results.
choosingaproperleve l forstudy isthusvita llyimportantfor empirical software eng ineering research but this issue hasn t thusfarbeenexplicitly investigated.wedescrib etherelatedidea ofecologicalinference andecologicalfallacyf romsociologyand epidemiology and exp lore its relevance to e mpirical software engineering wealsopr esentsomecasestudies usingdefectand processdatafrom18o pensourceprojectsto illustratetherisks of modeling at an agg regation level in the c ontext of defect prediction aswellasin hypothesistesting.
i. introduction large systems are comp osed of multiple modul es.
modularization is critical to scaling up software e ngineering projects without proper modular decomposition and congruent work assignment large system developm ent would become hopelessly mire d in knowledge bottlene cks and coordination overheads.
ind eed as systems scale up to millions of lines of code and b eyond designers seek hierarchical modularization for exam ple the code in large sof tware products such as eclipse ex hibit at least hierarch ical levels of decomposition viz.
files packages and plu gins modules.
hierarchical decomposit ion in software produc ts naturally dovetails and is often b eneficially congruent with the hierarchical organizat ion of modern software d evelopment teams.
indeed quite of ten we find software p rocesses are themselvesalsohierarchi cal withstepscontained withinsteps.
thus we have hierarchic al systems developed by hierarchical teams using hierarchical processes.
while hierarchical deco mposition is largely an unalloyed blessing for large softwa re products teams and p rocesses we argue that it poses risks for empirical software e ngineering research ese .
ese is concerned with observab le outcomes such as quality and prod uctivity such outcomes are subjectto large sample studies so that a statistical me thods can be brought to bear for h ypothesis testing and b automated machine learning and m ining methods on past d ata can be built into tools that sup port programming tasks .
thus for example many studies fo cus on software quality b y choosing the number of defects in an element as a respons e variable and dependingon the hy pothesis measuresof suc h factorsas the complexity of the ele ment the number of cont ributors the developmentorganization ssocialandgeographics tructure as predictor variables.
like wise historical defect da ta is mined and used with machine learning models to aut omatically predict likely future loci for defects.
these ideas animate a large body of ese resea rch.
hierarchical decompositi on becomes important he re.
many papers1study phenomena at the level of files wh ile others study them at high er levels of aggregation e.g.
packages or modules .
but what is the right lev el of study?
the reason why this is an important question w orthy of being underlined and ita licized becomes clear w hen we ask two derivative questions a if we build a statistic al model to test a hypo thesis at an aggregated level e.g.
pa ckages do the findings i n the model hold at the dis aggregate d level e.g.
files ?
b if we build automated prediction models at vary ing aggregated levels e.g.
bug prediction at a m odule level or at a file level does the perfo rmance at these differen t levels give equally valid indications of the actual cost effect iveness of their predictions e.g.
when using these predic tion models for inspections ?
ecological inferenceis the conceit that an em pirical finding at an aggregated level e.g.
packages can apply at t he disaggregated level e.g.files .
when this inferen ce is mistaken we have theecological fallacy.
1there are too many pa pers to enumerate we merely present a few representatives978 .
c ieee ase lawrence ks usa362 contributions in this paper we make t he following contributions.
wepresentadetailed conceptualoverviewofe cological inference and ecological fallacy what are they?
why are they relevant in softw are engineering?
what a re the specific risks in software engineering?
we present a theore tical discussion of sever al factors known to give rise to ec ological inference risk sample size zonation andclass imbalance.
we empirically study the incidence of ecologic al inference in open source p rojects.
we find ecological inference risk in defect prediction mod els while it may appear from roc type measure s that aggregated package level predictions model s are similar or slightly b etter than disaggregated file level prediction mod els in fact when usin g cost effectiveness measu res file level models ar e decidedly better.
ecological inference risk in hypothesis testing us ing multiple regression we find quite a number of cases where a null hypot hesis is rejected p .
at an aggregated levelcannotbe rejected at the disaggregated level and vice versa.
the goal of this paper is to lay out a conceptual fr amework of ecological inference risk in software engine ering and empirically demonstrate the existence of this risk .
in future work we hope to study t he effects of the factors sample size zonation and class imba lance on this risk.
relevance to automated software engineering this paper is concerned with the cons truction of defect predic tion models which provide an autom ated way to focus qua lity control efforts.
ii.
background and concepts the risks of transferring statistical inferences from aggregated groups to smaller constituent groups were noticed as early as by robi nson .
he observed that at an aggregated level u.s. st ates immigrant status w as positively correlated .
with educational achievement but at the individual level it was negatively correlated .
.
this discrepancy has been attr ibuted to the tendency of immigrants to congregate in regions with higher levels of ed ucation.
in this case the congregat ional tendency of immi grants is a confounding phenomeno n at the aggregated region al level that jeopardized the internal validity of the study at that level.
this discrepancy betwee n the findings at the agg regated and disaggregated levels illus trates theecological fallacy.
detecting a phenomenon at an aggregated level a nd then inferring it to apply a t a disaggregated leve l is called ecological inference fallacy .for brevity we use the abbre viationeifor ecological inferenc e andeffor ecological fallacy .
while the above n owclassic exampleillustrate stheecologicalfallacy b ynomeans is it the case that all e cological inferences are subject to ecological fallacies.this issue has been expl ored in geographical and epidemiological research.
the te rm maup or modifiable areal unit problem introduced by o penshaw and taylor addresses the issues of scale and zonation in geographic data.scale refers to the size of the aggregated unit larger s cale means biggerandfeweraggrega tedunits.zonationreferstothemanner in which aggregation is performed.
a vividly p athological example of zonation is gerrymandering wherein geographic regions are aggregated in distorted artificial w ays to the deliberateelectoraladvan tageofapoliticalparty.m auprefers totheproblemofchoosin gtheproperscaleandzon ationwhen studying phenomena that are subject to aggregatio n and thus mitigating the risk of eco logical fallacy.
in empirical software e ngineering ese pheno mena are often studied at the aggr egated package or modu le levels .
certainly assumin g that hypotheses suppo rted at the package level hold at the file level is subject toeirisk.
this risk applies to predictio n models as well.
cons ider the use of prediction models th ey are intended to be us ed to focus quality control efforts suc h as inspection.
inspector s work line by line whereas predict ion models are trained o n aggregate metrics at the level of files or packages.
it s p ossible and indeedithasbeendocum ented thatpredictionmo delsthatare designed to work well a t the level of filesdo notwork well at the disaggregated leve l of lines which is argua bly the right leveltodetermineactual inspectioneffort!
.although theissueofeiarisesinempiricalsoftw areengineering toour knowledge there has bee n no explicit discussion o f this in the literature.
a. ese eiandef in the rest of this sectio n we qualitatively consi dereiin the context of ese illus trating the issues that can arise using several examples.
the d iscussion below has bee n informed by similar considerations presented in other fields and is in the spirit of the paper by briandet al.
on the application of me asurement theory in ese .
first we discuss the reasons why studies may have to be conducted at higher levels of aggr egation thus risking the ecological fallacy .
next we discus s construct validity issue s that arise directly from aggregation .
next we consider zona tion issues which can threaten inte rnal validity and were at the heart of robinson s original f ormulation of the ecolog ical fallacy.
finally we consider the basic fundamental issue of sample size that rears its head a t increasing levels of agg regation.
b. reasons for aggregat ion one might reasonably as k why not avoid ecologi cal fallacies altogether by always con ducting studies at the low est level of aggregation finest resolu tion ?
there are natural reasons for aggregated studies.
aggregatephenomena relevant phenomena app ly only at a higher aggregated level or differently at differen t levels.
as a simple example some object oriented phenome na such as inheritance and class co hesion apply only at the class level not at the lower l evel of methods.
fan in and fan out363 number of methods call ing or called by a meth od apply at the method level not at t he level of a line of code .
as a more subtle example certain ty pes of phenomena may o nly emerge at the team level some teams may be better m anaged or morecohesivethanother s andasaresult team re latedquality and productivity effects may apply equally to m embers of a team and differentially a cross different teams.
if teams as is common areassignedw orkbasedonmodulardec omposition thenmoduleswillreflect theseteameffects.insuc hsituations an aggregated module le vel study may be approp riate even so transferring findings f rom or prediction model s trained at the aggregated level to th e disaggregated level is r isky.
observationalresolutio ndata may be observable at only certain levels of aggrega tion.
field defect data m ay be only availableatthebinarylev el sincefailureinformati onavailable to users and customer support staff might not have filelevel information.
custo mer satisfaction ratings might only be available at the leve l of complete applicatio ns.
despite the unavailability of da ta at lower levels of a ggregation eimay be desirable as it may lead to actionable c oncepts.
for example if statistica l findings relating to fiel d defects at the binary level could be transferred to the file lev el it may suggest better ways of do ing work assignment at th e file level inordertoreduceinciden ceoffielddefects.ofcou rse ifsuch inferences are fallacious the results may not be ac tionable.
next we discuss possib le pitfalls of aggregated s tudies in empirical software engin eering.
c. difficulties of interpr eting aggregated results if we model data at an a ggregated level and obse rve some statisticallysignificantres ult whatisthemeaningo fthatresult at the disaggregated leve l?
given an aggregated l evel finding what action should we take?
a measure s inter pretation or content may not easily t ransfer from an aggregat ed level to a disaggregatedlevel thati s ameasureofaproperty atacoarser level of aggregation mig ht be difficult to interpre t and or act upon at a more fine grain ed level.
some epidemio logists see e.g.
schwartz have c onsidered this to be a form of construct validity.
in fac t a finding at a module level and a finding at a file level wi th the same measure mi ght indicate different remedies.
we n ow present twogedankenexamples of this organizationaleffects.
organizational effects have been foundtoaffectquality.fo rexample nagappaneta l report that metrics such as ow nership and organization al diameter are excellent predictors of defect occurrence.
th e study was conducted at the level of binaries in windows which are aggregations of files.
co nsider the simplest metri c presented the number of engineers noe working on a bin ary.
if noe is strongly correlated w ith defects at the binary level two interpretationsarepossib le.considerabinarywith highnoe and also high defects.
fi rst if the binary has a la rge number of source files each allo cated to a different deve loper thus having overall high noe then miscommunicatio n between developers might be lead ing to errors.
in this case the proper response might be to mor e carefully define inter fi le interfaces designrules .on theotherhand ifthebina ryhasonly afewfiles buteachisma intainedbymultipledeve lopers thus also having high noe the difficulty might be a rising from developers tripping over each other s work in wh ich case the response might be to allo cate fewer developers to each file or to actually split the files up into smaller work uni ts.
geographicalissues.
several studies have found that geographical distribution can affect so ftware quality.
the same argument s presented above apply here.
for example ramasubbu balan find that ge ographical distribution at the produ ct level negatively impa cts quality and productivity.
suppo se that products are co mposed of binaries andbinariesare composedoffiles.whati sthecorrect response to this finding?
should we ensure that al l developers of each product are in o ne location?
or that dev elopers of a single binary are not geo graphically split?
or that developers of single files are not split?
any of these typ es of splits might have given rise to the quality and productiv ity impact observed in the study.
int erestingly the other two studies were at binary and file levels and have yield ed different results.
spinellis co nsiders the geographical distribution of committers to files in freebsd birdet al.
consider distributionofdevelopers atthebinarylevel.these twostudies have found no effect of distribution on software quality.
the studies were conduct ed in different settings outsourced development open sourc e and globalized develo pment respectively in the order discussed above and do ubtless the results are influenced by the setting.
still it is rea sonable to speculate whether the dis crepancy is a result of th e choice of aggregation.
d.eirisk factor zonation another issue to conside r is the possibility of con founding factors inzonation the way in which sma ller units are aggregated into larger units .
these can threaten inter nal validity.
internal validity question s can arise when an obs erved relationship between indepen dent and dependent varia bles might be an artifact of confou nding variables.
aggrega tionper se can lead to confounding and thus threaten internal validity.
in robinson sexamplequot edabove thetendencyof immigrants tomovetoareaswherees tablishedresidentsaremo reeducated is confounding.
e.eirisk factor sample siz e another issue to conside r in aggregation is samp le size.
it may be possible to cond uct a study at the level of methods classes files packages modules or even produc ts.
suppose that in the study the ph enomena of interest are modeled by a set of independent var iables and a dependent v ariable and furthermore thatthisset ofvariablescanbereason ablyaggregated at multiple levels f or study.
as the level of aggregation increases there will nece ssarily be fewer and few er samples.
the lowest levels of agg regation with the large s ample sizes will yield the highest sta tistical power the higher levels will have lower power and g reater risk of over fit mo dels.
on the other hand it is possible that some of the variable s are only364 fig.
conceptual fram ework available at higher levels of aggregation for reason s discussed above.
f.eirisk factor class imba lance defectpredictiondataoft ensuffersfromtheclassi mbalance problem.
entities are lab eled as defective if they contain at least one defect and not defective if they contain no defects thisclass labelingis seldom balanced in that the majority of the entities contain n o defects and yet it is the few that do that we wish to ident ify.
the resulting class i mbalance is mediated both by zonatio n and sample size since the number of defects remains almo st constant across levels2.
even if only defective entities a ggregate only with othe r defective entities the number of ag gregated entities is often sufficiently smaller than the disaggre gated entities that class i mbalance is reduced.
consequently i nferences drawn from th e aggregated datamaynotholdatthe disaggregatedlevelifcla ssimbalance significantly affects mod eling efforts.
g. summary figure summarizes the discussion above and pre sents our preliminary conceptual framework of the way to approach eiin the context of empi rical software engineerin g work.
we consider two end go als prediction models w hich aim to focus human effort and h ypothesis testing which a ims to find statistical evidence in da ta in support or reject cl aims.
the figure illustrates ho w scaling and zonation arise out of aggregation.
at the s implest level scaling aff ects sample size ii e andcanthusaffec tthepowerofstatisticalm odels built with aggregated dat a. small sample sizes ca n also make the performance of pred iction models built with aggregated data unpredictable.
agg regation naturally also a ffects zonation.bothsamplesizean dzonationcanaffectclas simbalance.
for example considering the aforementioned defe ctive and non defective class lab els class imbalance at th e file level mightverywellbereduc edbyaggregationintopa ckages.furthermore the manner in which zonation groups n on defective files along with defective files can also affect imba lance.
all factors sample siz e zonation and class im balance affect the quality of the statistical models that a re built to do defect prediction or h ypothesis testing.
sample sizes have well known effects on t he validity and stability of models.
2in some cases a defect m ay be associated with mo re than one file within the same module causing a reduction in the numbe r of defects in the proces s of aggregation.zonationcancreateintern al validitythreatsasares ultoflatent choices made in the way samples are grouped into aggregates as shown by robinson.
f inally class imbalance a s discussed above also influences mo del quality.
iii.
relatedwork ese researchers have inv estigated a wide diversity of phenomena including variou s aspects of quality and p roductivity.
ecological inference and the attendant risks are li kely to be relevantinthisbroadcon text.ourstudyisfocused onresearch into software quality sp ecifically into technique s that build statistical models of def ect occurrence either fo r hypothesis testing or defect predictio n our survey of related w ork is thus largely confined to statis tical and predictive mode ls of defect occurrence.
a. modeling and predic ting defects the core idea in this very popular area of re search is to consider the number of defects as a respons e variable and choose predictors b ased on intuitions about factors that contribute to defects su ch as complexity desig n attributes personnel attributes prog ramming practices team structures and so on.
regression m odels and correlation st udies have been used to gauge the explanatory power of di fferent variables and thus test hyp otheses concerning the etiology of defects.
another line of research exemplified by the annual promise3meetings at icse attem pts to build accurate reliablepredictionmodel stopredictwheredefects mightoccur next usingpastdatafort raining.thisresearchiso pportunistic and eclectic leveraging a wide set of statistical and machinelearning techniques to im prove prediction perform ance.
this research area is to o rich and varied to su rvey and cite in detail.
our intere st here is in aggregated modeling of defects for prediction o r hypothesis testing so we present some examples of this ap proach.
aggregateddefectmod elsaggregationiscommoni ndefect modelingresearch acom monmotivationisobservationalresolution see ii b since field defect d ata is often reported at aggregatedlevels e.g.
atthelevelofbinaries oreven completeprojects .k oruet al.
recommendaggregat ing for a different reason i mproving model perform ance.
they summethod levelmeasur esintoclass levelmeasur es thenuse these aggregated measur es along with metrics o f class level properties to improve m odel performance.
they report that this approach overcomes problems arising from t he skewed distribution of defects in the nasa kc2 dataset and defend their approach arguing th at the source dataset is la rge enough to obtain statistically me aningful results.
ramleret al.
also address how to build a quality defect model and reiter ate the findings of kor uet al.
.
in a study aimed at pred icting the likelihood cou nt of post release failures at the mo dule level nagappanet al.aggregate per class and per functio n metrics by sum and m aximum and combine with module le vel metrics .
schr oteret al.use imports gathered at the file level to predict failu res for both 3predictor models in soft ware engineering365filesandpackages.impor tsareaggregatedtothep ackagelevel from the subordinate cl asses .
the authors a ssert intuitively predicting for a co arse granularity is easier while using fine grained input feature s yields better results.
z immerman et al.
report file and pac kage level correlations b etween defects and file level an d aggregate code metric s. in their study the package level correlations are consiste ntly higher than the file level corre lations.
similarly model s built from package level variables s howed a constantly highe rr2value.
in a later study zimmerm anet al.use function class and file level metrics aggregated to the binary level to pre dict failure prone binaries .
however aggregation h as not been without co ntroversy.
ambroset al.
criticize the use of aggregate level metrics fordefectmodelevaluatio n.theyclaimthat predic tionsatthe package level are less he lpful since packages are significantly larger andthat therevie wofadefect pronepacka gerequires more work than a class.
in addition they assert th at classes are the building blocks of object oriented system s and are self contained elements f rom the point of view of design and implementation and tha t package level informa tion can be derived from class level i nformation while the op posite is not true.
classimbalance as previously discussed defect data frequently suffers from clas s imbalance.
this proble m has been studied both generally and within the context of software defect data.
one simple technique to correct for class imbalance is to bias the sa mpling process.
undersa mpling and oversampling are opposi te procedures that bias t he sampling procedure to either favor the minority class oversa mpling or penalize the majority cla ss undersampling .
drum mond and holte showed that when using their cost sensitive evaluation technique viz.the cost of misclassifi cation is taken into account that undersamp ling outperforms oversam pling .
other more elaborate te chniques have also been proposed.
guet al.propose a technique t hat minimizes the impa ct of problem instances viz.
those instances whose predictors causeclassoverlap .
menzieset al.applymicro sampling which combines undersa mpling and data reductio n to defect data .
in conclusion previous efforts at statistical mod eling of defect occurrence have g iven some consideration to aggregationandclassimbalance issues.however ourspec ificconcern here is with the validity ofei whether models built at the aggregated level apply a t disaggregated levels.
w ith defect models the question co ncerns both the hypoth esis testing and prediction performan ce.
the former is addres sed in v before we address the la tter we first discuss how prediction performance is evaluated .
b. evaluating prediction models there are several appro aches to defect predicti on model evaluation we begin wi th the simplest precision recall and work up to the more sop histicated cost effectiveness curves.
precision recall defect prediction can be viewed as a binary classification problem.
e ntities are classified as d efect proneornotbasedonpredictor ssuchascomplexity size ownership and pre release defects.
a n entity predicted as defe ct prone is considered a true positiv e tp if it actually cont ains a postrelease defect and false positive fp if it does n ot entities predicted as not defect prone that contain defec ts are false negatives fn and the r est are true negatives tn .
well known prediction p erformance measures en sue from these counts.accuracy computed astp tn tp fn fp tnyields the chance that the total number of modules will be predicted correctly.precisionis the ratio of correct p redictions to the total predicted as defec tivetp tp fp.recallis the ratio of correct predictions to the actual number of defecti ve entities tp tp fn.
a good model should achieve both high precis ion andhighrecall butthere isawell knowntrade off betweenthe two.
the f measure take s the harmonic mean of t he two and hasbeenusedtomeasure overallpredictionperform ance.most classifiers do not produc e a hard yes no decision however.
the classifier output is t ypically a probability tha t must then be compared to a thresho ld to obtain a classificatio n decision.
significant work has add ressed how meaningful m odel evaluation criteria are with res pect to defect prediction models.
ma et al.
criticize accuracy a s it ignores the data distr ibution and cost information.
l essmannet al.
argue that the requirement of defining a threshold is reason en ough not to use such simple static me asures in a defect predict ion context.
rocan established method of evaluating classifiers independently of any particu lar threshold isreceiver operating characteristic roc analysis.
an ro c curve represents a family of precision reca ll pairs generated from varying the thresholdvaluebetween0and1andplottingthefalsepo sitive ratef p r fp fp tnon thex axis and the true posit ive ratet p r tp tp fnon they axis.
all such curves p ass throughthepoints and .thepoint represents perfect classification and points on the roc curv e close to represent high quality classifiers.
a common w ay to evaluate the overall qual ity of the classifier is to compute the area beneath its roc cu rve which we denote aucroc this has a value between an d .
a limitation of roc cu rves and by extension a ll of the aforementioned measure s is that they value all entities the same.classificationofan entityasdefectproneisn ottheonly oreventheprimary goal ofdefectprediction.idea lly thegoal ofsucheffortswouldbe toaidinefficientlyguidin gcorrective maintenance.
thus the cl assifier tells us where to find defects viz.
approximately where t o look to take correctiv e action.
we can view inspection e ffort as roughly proportio nal to lines of code and so it make s sense that the value o f inspecting a class depends on its bug density.
maet al.address this issue and recommend c areful evaluation of ro c curves at meaningful performance points arisholmet al.
address it somewhat dif ferently by defining a m etric cost effectiveness more appr opriate to defect predic tion models motivated by code inspec tion.
costeffectiveness suppose defects were uniformly distributed through the so urce code.
if an inspec tion budget allows inspection of10 of the source code then we might366table ii metrics gathe red and their description .
metric d escription loc s ource lines of code lines t otal lines in file package developers n umber of developers who have edited this file package active developers n umber of developers on this file pkg in current releas e churn n umber of added changed lines commits c ount of commits to file p kg features n umber of new features a s identified by issue tracke r improvements n umber of improvements as identified by issue tracke r choose10 of the lines at random and might reasonably expect to find about10 of the defects.
this sche me requires minimal work and no ex pertise in data gathering and defect modeling.
therefore it is reasonable to expect that any useful defect prediction method should be able to impro ve on this result.
this is the basis f or the cost effectiveness ce metric definedbyarisholmet al.
.thececurveplots percentage of identified faults found against the number of lin es of code accumulatedbyentitiesc onsidered.touseapredi ctionmodel weusethemodeltocom puteapredicteddefectpr obabilityfor each entity.
entities are ordered by decreasing o rder of fault probability and increasin g size.
a successful mo del is then one that predicts a greate r percentage of faults fou nd than the percentage of lines of co de inspected hence a cu rve that lies at least in part above t he liney x. the arisholmet al.
formulation computes on ly the area above the lin ey xas contributingtotheceme asure.wesimplytakethe areaunder the ce curve denoted aucce as our measure as we a re not evaluatingpracticalmode ls rather wearelooking attherange of cost effectiveness achi eved by models in differ ent settings.
arisholmet al.expand on their work o n ce in a systematic investigation of methods used in building defect prediction models considering issue s of roc vs cost effecti veness .
weusethe auccemeasuretoevaluatehow predictionmodels perform primarily with r espect to code inspection s. we argue that a independent of pre diction performance the model that facilitates the identificati on of the greatest numbe r of defects after inspecting the fewe st lines of code is of grea ter practical use.
iv.
experimental methods wenowpresentourfindi ngsillustratingtheriskso fecological inference in the two settings commonly used in empirical softwareengineering def ectpredictionand hypoth esistesting.
these findings support th e following claims prediction models are subject to ecological in ference risk.modelsbuiltusingaggre gateddata evenwhenthe y show reliable performan ce at the disaggregated level can have less reliable p erformance at the aggre gated level.
hypothesistestingissu bjecttoecologicalinferen cerisk.
strong practically signifi cant relationships observ ed atthe aggregated level may weaken at the disaggreg ated level but first some details of our experimental ap proach.
datagathering we extracted data from the jira defect tracking system and asso ciated git repositories for distinct versions of different asf apache software foun dation projects described in tab le i. for each release we extract the jirarssfeed anxml reportofjiraissues.we thencrawl theassociatedjirawebp ageforeachissuefound inthexml report and extract the co mmits related to that iss ue.
we then link this data to the git l og to determine which c ommits and consequently which files and packages are assoc iated with defects.
a file associated with a closed defect in a commit is considered to be a partia l repair for that defect in that commit andthefileislabeledasd efectivewithinthatreleas e.foreach fileinareleasewegather thesizeinloc linesofcode using thesloccounttoolandaggregatethese countstothepackage level .
the number o f developers associated w ith each file and package is identified from the unique author n ames in the git log and counted dire ctly for both files and p ackages.
we distinguish between the number of developers wh o have ever touched a file or package and active developers viz.
the count ofthoseactivelyworking onthepackageinthecur rentrelease.
similarly wecountuniqu edefectidsineachfilea ndpackage to identify the number of defects associated with e ach release.
we countimprovements the number of code im provements identified by the jira tra cking system as well asfeatures the number of new features.
we would certainly have liked to run our experim ents with existing data used by ot her researchers e.g.
such as can be found in the promise rep ository .
we note h owever the vast majority of such dat a is measured at a single aggregation level it is therefore i mpossible topost factoaccurately perform set unions to ag gregate defect and devel oper counts to a higher level.
a sing le defect as an example associated with each of three class es in a package contrib utes only a singledefecttothepacka ge.sincemostavailabled atacontains only counts and not iss ue identifiers it cannot be properly aggregated.
modelingdefects we use logistic regressi on to classify files and packages as defectiveusingthemetric sreferencedintable?
?aspredictors andtheexistence 0or1 ofapostreleasebuginth eentityasa response.
for each predic tor thez test statisticis computed by dividing the estimated va lue of the parameter by its standard error and used to asses t he significance of the va riable.
this statistic is a measure of the likelihood that the a ctual value of the parameter is not zero the larger the abs olute value of the statistic the less likely that the actual va lue of the parameter could be zero .
for each package file v ariable pair changes in thep valueassociated with the z tes t and analpha value of0.05to decide if a parameter can be judged to have a significant effect.
given the large number of projects and releases we used an automated model sel ection technique to iden tify models.
we enumerated all com binations of at most six predictors367table i apache projec ts and their description project releases description releases files packages abdera .
.
atom xml syndication implementation cassandra .
.
.
.
distributed databa se cayenne .
.
.
java object relational mapping framework cxf .
.
.
services framework httpcore .
.
.
http core library ivy .
.
.
.
agile dependency manager ivyde .
.
.
.
eclipse plugin for ivy james .
.
.
java apache mail enterprise server lucene .
.
.
.
text search engine library mahout .
machine learning frame work nutch .
.
web search software ode .
.
.
business process executo r openejb .
.
.
enterprise java be ans pluto .
.
.
.
java portlet referen ce implementation shindig .
.
.
.
opensocial applica tion solr .
.
.
.
lucene search serv er container wicket .
.
.
.
web application f ramework xercesj .
.
.
.
java xml parser from those presented in t able??.
it has been shown that size oftenfollowsalog norm aldistribution .cons equently we include the log transform ations of size variables in addition to their untransformed c ounterparts allowing the model selection process to choos e between them.
log tra nsform data can increase central tend ency and reduce heteros kedasticity however in some cases u ntransformed data may y ield a better fitting model .
we ra nk the models byakaike information criterion aic score and select the model with the lowe st aic value.
using aic alone however may yi eld models with high multicollinear ity amongst some of th e variables.
consequently we check for this by rejecting m odels with vifvariance inflation facto rhigher than5.
without specific reasons for including a high vif variable in a model the value5 isgenerallyconsidered tobethemaximumaccep table value .
by rejecting models with high vif an d sorting by how well the models fit the available data this pr ocess yields models that are similar t o what a researcher mig ht choose if she were manually iden tifying models.
in addit ion it does not constrain our process to choose the same mod el for each revision and project.
v. findings if we build a prediction model that performs we ll on an aggregated measure how does it perform on a dis aggregated measure?
to address th is question we build mo dels at two levels and explore the relationship between the m. for the aggregatedlevel weusefi lesandpackages apredic tionmodel works well at this level if it accurately predicts which files or packages have defe cts with as few false p ositives and false negatives as possib le.
as a measure of per formance at these aggregated levels we use a roc chart a s measured byaucroc.
for the disaggregated level we use a costeffectiveness lift chart a s measured by aucce.webuiltpredictionmode lsattwolevels atthebas elevelof files and then an aggreg ated level where files ar e aggregated into packages.
while m ost files contain a single java class some files may contain more than one each fil e however belongs to at most one p ackage.
of the availa ble versions of the projects disc ussed in section iv w e use only the versions that me et minimal sample size and events per variable requirements considered appropriate fo r use with maximum likelihood es timation approach used in logistic regression.
.
the min imum is required for both number of packages and number of files and is computed as follows.
let pbe the minimum class p ercentage e.g.
for100files with10 defective p .
.
letkbe the number of predic tor variables.
then the minimum numb er of samples isn 10k p. we thus reject datasets where the re are not only insufficie nt numbers of datapoints but also i nsufficient numbers of d atapoints in each class.
a. aggregation effects o n model quality in the first study we so ught to evaluate how th e level of modeling affects the qu ality of the models.
we used of the available revisions fr om projects based on the criteria that each dataset used n eeded a revision on whi ch to train and at least one subseq uent revision on which to evalute the predictive power of the trained model.
each of the prediction models was e valuated using two mea sures.
first at both levels we calcu lated the area under the roc curve aucroc for each of the models .
thus e.g.
at thepackage level aucrocmeasures how well the package level model predicts defectivepackages and so also respectively the filelevel model.
second for both levels we calculate theaucce measure for each of the models viz.at the package level we plotthecost effectiveness curve andcalculatethe aucce this is repeated for the file le vel.
.
.
.
.
.
.
.
.0area under roc ce c urve file aucrocpackage aucrocfile auccepackage auccefig.
comparing aucrocandauccefor packages and classes.
aucrocis not significantly differen t across aggregation leve ls by a two sided wilcox on rank sum test withp value .
.
aucceis significantly lower for package level results by wilcoxon rank sum test withp value .462e .
even though package and file level predictor model s are often indistinguishablebytheir aucrocmeasures filelevelmode ls show clearly better auccemeasures.
this can be ob served in figure showing that th eaucroc s are sometimes higher a t thepackagelevel.thus i fwebuildamodelatapackagelevel andevaluateonlyatthesam elevel packagelevelmodelsm ay appearto perform better.
.
.
.but file level models are actually better this phenomenon can be observed from fig ure showing that the aucce s are generally higher at the fi le level.
thus if we buil d a model at the level of files or pack ages but evaluate at the fine grained level note that aucceconsiders lines of code l evel rather than at the aggregated l evel we find that file le vel models perform better.
if an unwary investigato r built a prediction mod el at the package level and also at the file level and com pared them using same level aucroc he might falsely conc lude that the package level mode l is giving comparable or in some cases better performanc e whereas in fact it is performing worsein terms of the more de manding and realistic aucce measure.
from these ob servations we draw the following rather sobering conclusio n. aggregated prediction m odels when evaluated pu rely at the aggregated level can look better than they rea lly are.
b. ecological inference risk next we study the risks of ecological inference to ese.
conceptually the risk a rises in this setting on e builds a statistical model at an a ggregated level uses it to test hypothesis andsimply assumes that th e results also hold at the disaggregated settin g without testing the s ame model at the disaggregated set ting.
we show that modelin g defectoccurrence at two diffe rent aggregation levels can lead to substantially different fit s of model parameters to the data and consequently to very different statistical infere nces.
we used model selection based on aic and vif to select thebestmultipleregressi onmodelattheaggregate d package level and used the sam e set of variables from this best aggregated model to buil d a model at the disaggr egated file level.
this gives a match ed pair of aggregated dis aggregated modelsforallprojectrev isions 18projects 68ve rsionstotal .
in order to evaluate the hypothesis that the corre sponding model variable affects d efects it is standard pra ctice to use thep valueof a coefficient within e ach model.
if the p value is below a threshold s ay .
that model va riable has a significant effect on defe cts if not the null hypo thesis that it does not affect defects cannot be rejected.
we follow that practice here in selectin g the significant variabl es from our models.
furthermore fo r a particular variable w e check if the models at both leve ls give the same results i.e.
if it is significant at both levels or insignificant at both le vels .
thus for example we find th at thenumber of active deve lopers tends to be significant at both levels of aggregatio n in many of the models.
major changes in a var iable s significance can result in substantially different c onclusions from the inf erences.
to evaluateeirisk westudiedthechan geinthesignificanceofa variableinamodelbetwe entheaggregated disaggr egatedpair.
specifically for a given l evel of aggregation file o r package and a given project vers ion projects ver sions total we noted two propertie s of each model variab le its fitted parameter s sign positiv e or negative and its sig nificance to the model significant or insignificant .
we compared the corre sponding parameters be tween the two levels and focused on three outcomes ind icative of eirisk.thefirstisachang einsign from to orv iceversa between the two levels o f modeling we did not o bserve any such changes in our data .
the second isgain of significance or gs where the variable gain s significance when aggr egated from file to package.
th e last isloss of significance or ls where the variable is sig nificant at the package le vel but not at the file level4.
we discuss some observ ed examples of ls and g s below.
for example thenumber of commitsis an ls variable in the abdera project signi ficant at the aggregated p ackage level for the abdera project b ut insignificant at the file level.
thus if one were to conclude from the package level model that codechurnaffecteddefects andthen unwarilyusedecological inference to conclude th at files that were subjec ted to more commits are more defec tive that would be fallac ious in this case committing additio nal resources to inspect files would probably be unwise.
like wise in the case of ivy thenumber of active developersis a gs variable ins ignificant at the package level but gainin g significance at the file level.
thus an unwary researcher m ight conclude from a pa ckage level 4there can be two other possible observations w hen the coefficients are eitherboth significant or ssorboth insignificant ii but they carry no inference risk.369table iii counts of ga ins and loss of significan ce in inference models.
a s an example first row the predictorcommitsshowed loss of significa nce ls in different releases in different projects abderaandcxf the variable was actual ly significant p .
in releases in pr ojects in at least one level of aggregation.
type predictor releases significa nt releases projects significant projects projects ls commits abdera cxf ls activedevs abdera wicket ls improvements cxf openejb ls devs cxf openejb ls lines cxf wicket ls features cxf james nutch o de ls added cxf ls loc cxf openejb gs commits cassandra ivy nut ch openejb wicket gs activedevs cassandra cxf ivy wicket gs improvements cxf openejb wicke t gs devs ivy openejb gs lines cxf wicket abdera mahout gs features cxf gs added wicket gs loc lucene cxf ode xe rcesj model that this variable has no influence at the file level whereas in fact it does.
in summary we found t hat out of a total of variables used in the models we found instances of gs and instances of ls.
tabl e iii summarizes our find ings for the variables that showed ga in and loss of significan ce.
overall the table shows a worry ing number of cases of gs and ls.
certain variables such a snumber of commitsandnumber of improvements show either gs or ls in quite a number of cases and illustrate the potential for ecologically fallacious inferences when hypothe ses are tested at the aggr egated level and the results of these tests are inferred to ap ply at the disaggregated level.
espe cially notable is thenumber of new featuresvariable.
it shows both gain and loss of significa nce.
for it we observe gs in revisions on differen t projects and ls in revisions across different proje cts.
these numbers should be con sidered relative to the t otal number of revisions and project s where the variables w ere found to be significant at least at one level.
thus the number of new featureswas significant in rev isions in different proj ects.
thisgivesanindicationo fthechancesofrunningi ntoeirisk.
inferences drawn from m odels built at aggregated levels such as models package s or binaries may not t ransfer to disaggregated compon ents e.g.
files used to build the aggregations.
vi.
threats to validity perryet al.
identify three form s of validity that must be addressed in research studies.
we now examin e threats to each form of validity in our study and the metho ds used to mitigate these threats wh ere possible.
constructvalidityattemptstoreconcileme asuredproperties with the concepts they a re believed to represent.
files often containmorethanonecla ss hence theyarethemse lvesaggregations of classes.
it cou ld be argued that a better comparison is classes vs. package.
f iles however are the low est unit ofaggregation for which we had bug linking data so w e feel that the aggregation is justifie d in this case.
internalvalidityistheabilityofastudyto establishacausal link between independen t and dependent variable s regardless of what the variables ar e believed to represent.
we rely on linking data to identify d efects hence our models cannot capture defects notlinked by developers.
thisapproac h however is widely employed in th is area.
further this sam e limitation applies toimprovementandnew featureannotations of jira issue tickets.
we were careful to address mult icollinearity issues as part of our auto mated model selection p rocess.
external validityrefers to how these resul ts generalize.
the threats here are common to studies of this type.
w e use only open source data and the consider only source wri tten in the java language.
our case study is limited to a sing le source of projects the apache sof tware foundation which may affect towhatextentourresults cangeneralize.however westudya largenumberofrevisions collectedfrom18differ entprojects.
vii.
discussion conclusion we have discussed and illustrated the risks of e cological inference in software en gineering.
however it i s important to keep in mind that it is difficult indeed unwis e to always attempt to avoid this risk .
software is inherently h ierarchical.
certainly products are h ierarchical as discussed above with files packages and plug ins .
in addition teams are hierarchical with hierarchical management structures.
d evelopment processes can also be vie wed hierarchically with minor steps within major steps and iteration loops nested w ithin other loops.
becauseofaggregatephenomena andobservationalresolution as discussed above in s ection ii b it is often ne cessary to study phenomena and or gather data at aggreg ated levels of products teams or p rocesses.
it is also possi ble that the resulting findings are o nly actionable at the d isaggregated level viz.
at the level of files ind ividual people or steps of a process.
therefore it is unlikely that ecologic al inference risks notwithstanding ca n be completely avoided in empirical370software engineering.
w hen making the inferenc e however the risk of ecological f allacy needs to be con sidered and discussed.
as we have argued above there areconstruct validityissues it is not always c lear how to translate a fin ding relating to an aggregated metric into a concrete act ion that can be applied to a disaggreg ated product process or team.
there arealsointernalvalidityissues aswediscusseda bove factors that influence aggregation such as intentional or un intentional assortativity inourcase studies canconfoundthe results and threateninternalvalidity whenecologicalinferenc esaremade.
of course one must alw ays be mindful while ag gregating of the loss of statistical pow er due to reduction in sa mple size.
in conclusion we hope to have convinced the re ader that a ecological inference i s often unavoidable in s oftware engineering research and b that managing the re sulting risks of ecological fallacy is a ripe area for study in our field.
we hope others will join us to explore these issue s in other settings and other datase ts investigate specifically the effects ofsample size zonationandclass imbalanceoneirisk and also considereirisk in their future work .