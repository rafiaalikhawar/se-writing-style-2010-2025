sample size vs. bias in defect prediction foyzur rahman univ.
of california davis ca usa mfrahman ucdavis.edudaryl posnett univ.
of california davis ca usa dpposnett ucdavis.edu israel herraiz universidad polit cnica de madrid spain israel.herraiz upm.espremkumar devanbu univ.
of california davis ca usa ptdevanbu ucdavis.edu abstract most empirical disciplines promote the reuse and sharing of datasets as it leads to greater possibility of replication .
while this is increasingly the case in empirical software engineering some of the most popular bug fix datasets are now known to be biased.
this raises two significant concerns first that sample bias may lead to underperforming prediction models and second that the external validity of the studies based on biased datasets may be suspect.
this issue has raised considerable consternation in the ese literature in recent years.
however there is a confounding factor of these datasets that has not been examined carefully size.
biased datasets are sampling only someof the data that could be sampled and so in a biased fashion but biased samples could be smaller or larger.
smaller data sets in general provide less reliable bases for estimating models and thus could lead to inferior model performance.
in this setting we ask the question what affects performance more bias or size?we conduct a detailed large scale meta analysis using simulated datasets sampled with bias from a high quality dataset which is relatively free of bias.
our results suggest that size always matters just as much bias direction and in factmuch more than bias direction when considering information retrieval measures such as aucandf score .
this indicates that at least for prediction models even when dealing with sampling bias simply finding larger samples can sometimes be sufficient.
our analysis also exposes the complexity of the bias issue and raises further issues to be explored in the future.
categories and subject descriptors k. .
software maintenance general terms management permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
esec fse august saint petersburg russia copyright acm ... .
.keywords bias size defect prediction .
introduction detailed data on bugs are clearly crucial to empirical studies of software quality.
such data is generally collected inbug fix datasets .
several such datasets have been publicy released into repositories such as promise1.
these datasets include both detail on bugs reported by users and developers and the source code location where the bugs were fixed.
the fix location is provided by a linkto commits in the version control system.
these links identify the source code files involved in a bug report as well as other details such as the developer who committed the fix the date and time and the lines changed in the corresponding files.
this is a rich source of historical data for building software quality prediction models that may yield improved understanding of the factors that affect software quality.
these links between bugs and commits are recovered through a post facto examination of the commit logs in version control and or the comments and meta data associated to the bug report both of these are manually entered by code contributors .
thus the ability to recover these links depend on the tagging practices of the contributors to the software project under study.
these practices vary across each project this heterogeneity makes the consistent reliable recovery of all of the links between bugs and commits problematic.
indeed many datasets widely used within the research community are missing some links and have been found to be incomplete and biased .
training defect prediction models using biased and or incomplete datasets is problematic.
wu et al.
compared the accuracy of a prediction model trained using three different datasets a biased dataset obtained using heuristics a putatively unbiased dataset obtained manually and a putatively biased dataset obtained using an automated method.
the model trained with the biased dataset did not perform as well as the models trained with unbiased datasets.
this is a disappointing result suggesting that despite all the hard work invested in creating them one should abandon datasets that may be biased.
in this paper we therefore take a contrarian view on bias.
available datasets could possibly be biased but still be large in size.
can such datasets still be useful in some prediction settings?
if it should turn out obtaining larger training datasets of bugs with linked fixing commits is more important than the difficult task of obtaining entirely unbiased datasets then this would be good news large datasets could be used to train prediction models with some confidence that such prediction models would still perform usefully.
in this research our goal is to do a controlled study of the relative effects of bias and sample size on defect prediction .
in order to do this we begin with a very high quality dataset wherein linkage rates are very high median of as compared to the more typical for other datasets .
we then artificially sample sub datasets with varying levels of size and bias by selectively sampling linked defects based o n biases that are known or suspected to exist.
using these artificially biased sub datasets we examine the relative effec ts of bias and sample size on defect prediction models.
we use ameta modeling approach constructing meta models that explain the performance of prediction models .
our research makes the following contributions.
we examine the effects of bias on defect prediction by artificially sampling with bias from a dataset known to be very high quality.
we study the relative effects of different types of bias on defect prediction using meta models to analyze the prediction models.
we use the meta models to study the relative effects of bias and sample size on defect prediction.
finally we introduce and examine the effects of pollutionwhere data sets that fail to link defects that were actually fixed introduce false negatives e.g.
files that are really defective but are not marked as such.
.
related work and theory defect prediction models use supervised methods to learn the association of different predictors and the defect proneness of entities.
a labeled training dataset labels each enti ty as defective or not.
existing research identifies defective entitiespost facto based on the locations where defect repair occurs.
this requires clear identification of a defect fixing activity.
past research has used developers comments associated with the source code changes to determine whether a change is defect fixing.
ideally a responsible developer always identifies defect fixes in the change log.
keywords such as bug fixed etc.
in the change log message could be detected by a tool to identify defect fixing changes .
one can also identify numerical bug ids mentioned in the change log and match those ids with defect database such as bugzilla to locate defect fixing changes .
however developers do not always annotate a change with proper description this impairs the automated identification of defect fixing changes .
birdet al.
found that automated process only identified less than of the defect fixing changes in most cases.
bachmann et al.
noted that missing links the defectfixing changes that automated process fails to recover may impact the prediction performance of supervised learners.
.
biasmissing links can confuse and hinder a supervised learner.
links may be systematically missing based on the properties of a defect fixing change such as e.g.
severity of the defect or the experience of the fixer.
the resultingbiased dataset may trick the supervised learner.
for example suppose only experienced developers are annotating their changes.
automated tools will only identify defect fixing changes made by experienced developers we will have over representation of the entities fixed by experienced developers.
in earlier work we introduced the notion of bias and have provided formal probabilistic definitions of bias.
for our purposes here it is sufficient to informally define bias as the situation where the linked bug sample distributions of the co variates of interest are systematically different fr om the distributions of the same co variates among the entire population of fixed bugs .
consequently the linked sample is not truly representative of the population.
so for example the distribution of developer experience among the linked fixed bugs would differ from the distribution of developer experience among allof the fixed bugs.
different properties of defect fixing changes may introduce different types of bias.
thus as discussed above developer experience may introduce experience bias.
similarly severityofthedefectsmayintroduce severity bias.
bird et al.
found that defect fixing changes of less severe defects are more likely to be linked.
however given a dataset we may not know the source of bias e.g.
whether it is experience orseverity .
if different sources of biashave different levels of influence on prediction performance biaswould be more damaging due to the uncertainty over the source of bias.
we therefore study whether different sources of biashave different impacts on prediction performance.
we note here that kim et al.
have studied the influence of noiseon bug prediction.
they also propose an algorithm to find noisy instances in bug datasets so they can be removed to avoid potential problems.
our concern is more with systematic bias rather than noise.
research question do different sources of bias have varying impact on prediction performance?
pollution the second side effect of missing links which to our knowledge has not been addressed is false negatives .
an unlinked defect fixing change fails to identify the defective files associated with that change.
this also effectively labe ls files that are actually defective as defect free!
these falsenegatives constitute a form of pollution that might affect supervised learners.
we informally define pollution asan erroneous condition of bug fix datasets where files that are in fact defective are incorrectly labeled as defect free .
besides these two consequences missing links inherently reduce the number of defect fixing changes available.
we use the term sizeto represent the total number of links available.
increasing sizemay increase the performance of the learner and may also ameliorate the impact of biasandpollution .
sizeis also a more tractable problem larger datasets may be easier to obtain than data sets that are both unbiased and unpolluted.
while existing research studies biasand the presence of missing links so far none have compared the effects of size pollution andbias to determine the relative effects of each on the performance of prediction models this motivates our core research question.table studied projects and release information project description releases avg avg link files sloc rate cxf services framework july april releases .
.
.
camel enterprise integration framework january marc h releases .
.
.
derby relational database february october re leases .
.
.
felix osgi r4 implementation august november r eleases .
.
.
hbase distributed scalable data store jun may r eleases .
.
.
hadoopc common libraries for hadoop jun september releases .
.
.
hive data warehouse system for hadoop october may releases .
.
.
lucene text search engine library october november releases .
.
.
openejb enterprise java beans august april rel eases .
.
.
openjpa java persistence framework january february releases .
.
.
qpid enterprise messaging system november may releases .
.
.
wicket web application framework november may releases .
.
.
table process metrics short name description comm commit count adev active dev count ddev distinct dev count add normalized lines added del normalized lines deleted own owner s contributed lines minor minor contributor count nadev neighbor s active dev count nddev neighbor s distinct dev count ncomm neighbor s commit count oexp owner s experience exp all committer s experience research question considering bias pollution andsize which aspect of missing links affects prediction models the most?
false positives finally it is also possible that the bug links containfalse positives viz.. edits that are not really bug fixes but accidentally get included into bug fixing commit.
this issue is outside the scope of our current research and we hope to address this in the future.
however since developers may just accidentally include non bug fixing changes in a bugfixing commit our belief is that this type of data pollution i s more likely to be characteristically noisyrather than biased.
arguably the approach introduced by kim et al.
for dealing with noise should be effective in dealing with false positives.
.
experimental methodology projects studied table shows the open source projects studied in this paper.
in addition to release information and project size the table also lists the median percentage of defects for which we could identify the fixing commits link rates .
all are apache software foundations projects written in java however they come from a very diverse range of domains.
for each project we downloaded the gitrepository2and extracted the full commit history.
we also usedgit blame on every file at every release to identify the contributors information with more detail.
we carefully ignored whitespace changes and code movement during our blameprocess to identify the correct provenance of each line.
all the projects shown in table use jira3issue tracking system.
we mined jirato extract all the data associated to each bug report.
moreover thanks to the linking and tagging practices of the asf projects as well as to the features of jirabug tracking system we were able to retrieve the related commits which fixed each bug.
we only considered jira entries clearly identified as defects we ignored feature changes refactorings etc.
we then locate those fixing commits in gitto extract commit information such as changed lines and commit author.
any files modified in these bug fixing commits are considered as defective post facto .
jira data all of our projects have very high linking rates with a median rate of approximately .
even at these high link rates our data indicates a slight bias e.g.
severe defects are linked at a median rate of about and less severe defects at about .
nevertheless these linking rates are much higher than the rates reported in prior papers on bias .
from these very highly linked bug fix data sets we deliberately choose samples with higher and and lower bias to study the effects of bias.
predicting defects following common research practice we study prediction at the file level.
we use logistic regression from the weka toolkit to compute a probability that a file will be defective or not in a subsequent release4.
the models are trained in a prediction setting viz.
we train the model on k th release we test the model in k th release using process attributes with a binary response indicating whether a file is defective.
we describe our process metrics in detail below.
since we are interested in obtaining the best prediction model possible we want to use as many variables as we can to capture as much variation as possible.
multicollinearity viz.
strong correlation of two or more predictor variables can be an issue with models with many variables.
multicollinearity is typically mitigated through the use of either a manual or automated stepwise procedure where a discrete subset the variables are selected for the model.
because we are evaluating the use of biased samples we build a very large number of prediction models consequently it is impractical to individually determine a particular set of predictors to use in each prediction model manually and an automated stepwise procedure would dramatically increase our runtime.
as an alternative we use ridge regression .
ridge regression introduces coefficient bias reducing variance in the coefficients while improving the stability of the model.
the technical details are beyond the scope of this paper5.
in the best case the variance reduction is significantly larger in magnitude than the introduced bias.
because we are building prediction models however this is not a significant concern as we are not interpreting the prediction regression coefficients.
since we are introducing sampling bias into our training sets which may disturb the relationship between the training set and test set distributions ridge regression provides insurance that we can be reasonably certain that the coefficient bias introduced by multicollinearity is not impacting the quality of our prediction models in the face of sample bias introduced by our experimental setup.
our choice to use logistic regression is motivated by its popularity in empirical software engineering research.
moreover researchers have found that the choice of metrics rather than classification methods is the primary driver of prediction performance .
predictor metrics table shows the process metrics we use in the prediction models.
comm measures the number of commits made to a file during a release.
adevis the number of developers who made changes to that file during a release.
ddevis the number of distinct developers contributing to this file up to this release.
addanddelare the normalized by the total number of added and deleted lines added and deleted lines in the file during a release.
ownis the percentage of the lines authored by the highest contributor of a file.
minor is the number of contributors who authored less than of the code in that file.
oexpis the experience of the highest contributor of that file using the percent of lines he authored in the project at a given point in time.
expis the geometric mean of the experiences of all the developers.
all of these metrics have been widely used in prior research literature .
in addition we use some neighborhood metrics inspired by bugcache .
bugcache uses co commit history to identify files related to a buggy file which may also be buggy.
this suggests the use of co commit neighbor based process metrics.
for these metrics we first find the list of files co committed with a given file weighted by the frequency of co commit in a particular release we then average the above metrics over this list.
5see for details.
we use the weighted average of a metric of the commit neighbors of a file nadev nddev andncomm are just the derived measures of adev ddvev and comm.
all of these metrics are individually positively correlated with the existence of defects and are thus reasonable candidates for inclusion in a prediction model we rely on the aforementioned ridge estimator to handle any issues of multicollinearity.
software engineering data is highly skewed following menzies et al.
we log transform all variables to stabilize variance and improve prediction quality.
bias influence metrics our goal here as stated above is to try to understand the relative effects of size and different types of bias on prediction performance .
bias arises because programmers link only somedefect fixing commits to defect reports and fail to link others.
such bias can derive from the properties of the defects the defect fixer or the files fixed.
birdet al.
observed that developers are less likely to link severe defects than less severe defects and experienced defect fixers will more often link their fix than inexperienced developers.
experienceandseveritycanthusbeviewedas bias influencing properties .
we define bias influence metrics as measures of such bias influencing properties.
in this paper we consider different bias influencing properties each with an associated bias influence metric bi metric that have been discussed in prior literature .
experience defect fixer experience measured as the percent of all commits to date that were made by the developer who fixed that bug .
severity severity of the fixed defects an ordinal measure .
proximity proximity of the defect fixing commit to the release deadline days fromr defect resolution until the proximate future release .
latency the amount of time it took to fix a defect days from reporting date to resolution date .
cardinality size of commits total of number of files committed in the defect fixing commit .
creating biased sub datasets we use all of the above bi metrics to create biased samples.
for generality we consider both directions of bias for all sources of bias.
thus instead of assuming that experienced developers would be more likely to link their fixing commits or that the commits that fix severe defects are less likely to be linked we study both directions how is prediction performance affected when experienced developers are morelikely to link their commits as well as when they are lesslikely to link their commits likewise we also consider the cases of severe defects being morelikely to be linked andless likely to be linked.
for each bias influencing property we partition our set of fixing commits using the median of the corresponding bi metric .
this gives us two sets lowercontaining the fixing commits with bi metric medianbi metric and higher containing the fixing commits with bi metric medianbi metric .
we add the remaining fixing commits withbi metric exactly equal to the median bi metric to either lowerorhigher whichever is smaller.
this fairlycoarse split is done to keep the experimental combinatorics under control even with this simplification we already have millions of sub samples available for training.
based on this split we choose our biased subsamples.
our approach is to select subsamples with varying levels of linking probability from lowerandhigher.
so for each oflowerandhigher we vary the probability of linking p .
.
.
... .
.
thus when p .
and we are considering lower we assume that of the fixing commits from lowerare linked.
these discrete probabilities can be used to select different samples for each of lower andhigher designated as lower p and higher p for each value of p randomly chosen from lowerandhigher.
during the sampling all the unselected fixing commits are considered as missing links.
we then find the unions of all possible cartesian pairs of lower p higher p pairs to study different ranges of bias.
we discard the base set lower .
higher .
as we need at least one defect fixing commit to train a model.
this approach allows us to study the bi directional bias impact when loweris less likely to link than higher as well as when loweris more likely of each source of missing links.
in order to compare the relative effects of size and bias for each biased set of links bl lower p higher p we alsouniformly randomly sample an unbiased set of links ulfrom the entire pool of fixing commits of the samesize bl .
pollution effects besides possibly creating biased samples missing links may pollutethe data with false negatives we thereforealsoexaminetheimpactof pollution bysampling sub datasets both with and without pollution.
we create unpolluted sub datasets by discarding any false negatives from the sampled sub dataset i.e.
any files known to be defective but not selected based on the biased sampling procedure are discarded from the training set.
to study the impact of pollution we created polluted sub datasets by labeling defect fixing commits as non defective when they are not chosen by the biased sampling procedure.
we can then compare the impact of pollution against the performance of an unpolluted dataset.
finally after sampling the defect fixing commits with the four possible combinations biased and polluted biased and unpolluted unbiased and polluted unbiased and unpollute d we derive the set of defective files based on our sampled defect fixing commits.
any file that appears in one of the sampled fixing commits are considered as defective otherwise defect free.
we can then build models on these newly labeled coming from different combinations of bias and pollution dataset.
we replicate the entire process times to reduce the risk of random variation and average the performance measures over all runs.
summary of sampling procedure figure illustrates how we sample to create sub datasets.
the sampling process is adjustable it can be tuned to select sub datasets based on different settings described above in addition it can sample with or without pollution.
finally the procedure can also sample without any bias to create sub datasets of varying sizes.
given all of the combinations of bias pollution all of the product release pairs and our fold replication this sampling procedure eventually creates about million sub datasets.
these sub datasets are used to train prediction models.
we then evaluate the effect of bias pollution and size on prediction model performance.
figure pictorial summary of of the experimental procedure.
we begin with a highly linked dataset and selective ly sample under controls for bias pollution and size to create a cloud of sub datasets with and without polluti on and bias as well as an entirely unbiased sub datasets.
thes e sub datasets are used to estimate prediction models th e results of these prediction models are then subjected to mul tiple regression meta modeling to tease apart the effects o f pollution size and bias evaluation our evaluation is a two step process.
first we a build a model for each training release using the sub dataset and evaluate the model on the corresponding test release.
it should be noted that the test release data is used in full without sampling.
there are several different approaches to evaluating prediction models.
we discuss them in general terms details have been discussed in several prior publications.
precision reca ll and f score are the traditional performance measures.
precision and recall have a natural trade off and one can choose a high recall low precision combination or the reverse.
the precise value is based on a particular threshold of predicted probability of defects.
these measures are also dependent on the proportion of defects thus if most files are defective even random choice will give relatively high precision.
there are also several threshold independent non parametric methods.
first is the area under the roc curve auc which evaluates performance independent of threshold or defect occurrence rate.
however aucdoesn t directly consider the cost of methods such as inspection.
arisholm et al.
suggest that using threshold dependent measure such asprecision recall andf measure may not be suitable for software engineering data since inspection cost dependsondefect density.
theyrecommendtheuseof aucec area under the cost effectiveness curve and specifically aucecat aucec and aucec source lines of code sloc to compare model performance.
it should be noted however aucecis just one model of cost e.g.it does not consider the cost of false negatives.
if false negatives matter aucin fact might be a better measure.
forourpurposes weconsiderthreshold dependent threshold independent and aucecbased measures.
specifically we usef measure at .
cutoff f50 to give the reader some idea about the model performance when evaluated in traditional cutoff based settings.
following arisholm et al.
we stress that such a threshold based measure may give an erro neous impression about the true performance of the model due to the class imbalance only a small portion of our files are reported as defect prone that is common in bug fix datasets.
so we also measure performance using auc as well asaucec 10andaucec .
we replicate the entire process of random sampling and model building times and then average over these runs to obtain the average model performance for each lower p higher p andpollution combination pollution turned off and on .
meta modeling our goal is to use the models trained over the millions of samples of varying bias size is to determine the relative effects of size and bias on performance.
we do this by using a meta model to model the performance of the resulting millions of prediction models this meta model essentially gauges the degree to which bias pollution and sizeinfluence the performance of the millions of prediction models.
thus the meta response of this meta modeling step istheperformanceofthepredictionofpredictionmodels.
f50 auc aucec 10andaucec 20of the learned prediction models are all used as meta responses measures of performance in our meta analysis while bias pollution andsizeare the meta predictor variables.
we build one meta model for each training test release pair .
so for each training release we build an ensemble of samples of varying bias pollution size as described above.
we then train models for each sample testing on the next release to find auc f50etc.
we use the prediction performance of these models as a response to our meta model.
we validated our meta models using standard ols diagnostic techniques.
all of our models exhibited reasonably highr2.
forauc aucec 10andaucec 10we observed a medianr2of around .
while for the f measure models we observed an r2over .
.
we checked for influential points and excessive heteroscedasticy through visual examination of the regression diagnostic plots.
for influential points we looked for excessive separation or high cook s distance .
we also used a metric defined by lindeman merenda and gold lmg to measure the impact of the variables .
as described in detail in the next subsection lmg provides a robust way to measure the relative importance of predictor variables on a response.
lmg permutes the sequential sum of squares to yield an order insensitive degree of variance explained for each of the variables.
the use of lmg provides resilience against the impact of multi collinearity.
this pro cess is quite compute intensive building prediction model s for the million sub datasets over releases and the meta modeling phase consume about hours of time on xeon .
ghz core gb workstation.
we identify the percentage of variance explained by each of these variables using the r package relaimpo the relaimpo package computes the aforementioned lmg statistic which we use to measure the impact of the meta variables on the performance of the prediction models.
lmg measure of influence inanolsmultipleregression setting the relaimpo package computes a statistic called lmg for each predictor.
lmg is a measure of the influence that a predictor has on a response in a dataset.
using the lmg statistic we can compare the effects of size bias and pollution on the performance of prediction models.
we now present our reasons for using lmg.
ordinary least squares ols linear regression models can be used to evaluate the relationship between a set of predictors x1 ... xpand a response y. we write the modelform of the relationsip as y 0 1x1 ... pxp where each irepresents an estimated coefficient for the relationship between xiandy.
if iis positive and significant then we can infer that a iunit increase in xinduces a one unit increase in y. we were primarily interested in the magnitude of impact on the variance rather than the direction of the influence.
bias may improve or retard a particular model performance measure we would be interested in either equally.
consequently we are interested in the impact that each regressor has on the variance of the model response.
typically one measures the variance in the response of an ols model using the coefficient of determination denoted by r2.r2is interpreted as the percentage of variance explained in the response by the predictors and is computed as the proportion of model sum of squares over the total sum of squares ss r2 model ss total ss summationtextn i y y summationtextn i yi y whereyiis the given value of response in the ithsample yis the sample mean and yis the predicted response from the ols model from the values of predictors in the ithsample.
decomposing the sum of squares will allow us to compute the impact of each regressor on the variance of the response.
we can imagine a simple method for decomposing the sum of squares as follows we add each predictor of interest in turn to the model and compute the additional variance explained by each new predictor.
this will yield a particular decomposition of the sum of squares.
a decomposition determined this way is referred to as a sequential sums of squares .
no matter which order we choose to enter variables every sequential sum of squares of the same set of predictors will yield the same totalss.
practically the explained variance is typically computed by performing an anovaanalysis.
unfortunately this procedure is sensitive to the order in which the variables are added to the model although we will alwaysobtainthesametotalss thesequentialdecompositio n may not be consistent across each variable.
to see why consider two predictor variables x1andx2that are not fully independent viz.
each variable has both an indepenent component and a shared component.
then some part of x1can be explained by x2 and the intersection will impact the response ysimiarly for either variable.
if we initially compute the sum of squares for the model containing only x1 x1will account for all of the variance that is explained both by its independent component and the component shared with x2.
when we add x2to the model only its independent component adds additional variance explained as the variance owing to their collinearity is accounted for in x1.
reversing the order will attribute the joint variance now tox2instead of x1.
this order dependence makes this simple analysis unsuitable for decomposing the variance explained in our setting.
lmg is an order independent method that calculates the decomposition of variance for all possible variable orderings and then computes the mean impact of each variable on the response.
lmg is thus able to correct for order dependency and provides a uniform measure of the impact of each predictor on the explained response.
we calculate the raw lmg .
.
.
.
.
.
e c s l p sources of biasmedian auc a median auc0.
.
.
.
.
e c s l p sources of biasmedian f 50 b median f500.
.
.
.
.
e c s l p sources of biasmedian aucec 10 c median aucec .
.
.
e c s l p sources of biasmedian aucec 20 d median aucec figure median of performance for different bias sources.
e for experience c forcardinality s forseverity l for latency p forproximity which calculates each variable s influence as a proportion of the total variance in the response.
summary to recap by reference of figure the generated sub data sets in figure .
labeled controlled for size bias pollution etc are used to build prediction models.
the performance of these prediction models is evaluated.
we now have a collection of data one for each prediction model which capture the size bias andpollution of the data that went into that model and the measured performance.
thus for each model we have one element of a meta dataset.
we now use ordinary least squares meta modeling step6 in figure on this dataset and use lmg to tease apart the effects of size bias andpollution .
.
results we begin by investigating the impact of different sources of bias on prediction performance.
rq do different sources of bias have different impacts on prediction performance?
the raw data on the performance of the various models varies quite a bit due to different reasons bias type bias degree h l release cycle as well as sample size.
we deal with the relative effects size and bias subsequently.
for now to focus on the effects of bias type rather than bias degree or release cycle we focus on summary statistics median and variance per release while bias degree varies .
for each release and for each type of bias as we allow the bias degree to vary both the corresponding sample size and the degree of pollution will vary so we expect to see some variation in the performance of models trained on each sample.
so within each type of bias we calculate the median andvariance of performance per release as bias degree varies.
these two statistics give us a sense of the range of performance that can be observed for each bias type as its bias degree varies.
the bias types in our case could be one of experience cardinality severity latency andproximity .
since the goal here is to check whether different sources of bias have different effects and thus if any of these sources of bias are a greater threat than any other we consider only biased andpolluted sub datasets infected with bias of each type.
figure plots the median of prediction performance for different sources of bias.
the figure shows hardly any difference in the distribution of median performance acrossdifferent sources of bias.
our findings suggest that none of the observed variations are statistically significant.
a nonparametric kruskal wallis test also failed to reject the null hypothesis that the distributions are the same.
.
.
.
.
.
.
e c s l p sources of biasvariance auc a aucvariance0.
.
.
.
.
e c s l p sources of biasvariance f 50 b f50variance figure variance of performance for different bias sources.
e forexperience c forcardinality s forseverity l forlatency p forproximity in addition to analyzing the difference in median performance we examined the stability of prediction performance using the variance of the performance measures per release .
figure 3a and figure 3b presents the variance of aucandf50 for different sources of bias across releases.
for brevity we do not present aucec 10andaucec 20as they are closely resemble the aucplot figure 3a .
the figures suggest that the threshold dependent performance measure f50is more sensitive to different sources of bias than the threshold invariant measures such as aucandaucec.
a kruskal wallis test also failed to confirm any significant difference of variances between different sources of bias.
we observe similar findings foraucec 10andaucec .
it is clear that there is considerable variation in the median of the performance within each bias type.
we attribute this variation in training sample sizes across releases some releases have more defects and thus more training data than others.
we return to the subject of sample sizes again below.
bias effect these two findings together have an important implication.
in general across releases performance as measured by auc aucec 10andaucec 20varies fig these models however perform quite well the median of aucis about .
and the median of f50is around .
.
the median of aucec 10is around .
a figure that is times .
whichcorresponds to inspecting random lines6 and the aucec median figure is around .
or about .
times the random rate of .
.
however as we examine variation in performance within a release for models trained with sub datasets with varying biasandpollution arising from different sources of bias influence the variation in performance of prediction is very limited figure for the non parametric measures auc as well as for aucec 10andaucec not shown .
furthermore the effect on performance and the variance in performance across different sources of bias is also similar.
different sources of biashave very similar effects on performance furthermore the effect of varying rates ofbiasis also minimal on non parametric measures of performance for all sources of bias.
this is an unexpected result data that is selectively missing for different reasons might be expected to affect the performance differently and different rates of bias might also be expected to affect performance.
nevertheless we still can see that each source of missing links induce some within bias source variance in prediction performance.
this variance can be attributed to varying sizeandbiasand the pollution of the sample space with false negatives.
we therefore study the impact of size biasandpollution separately.
rq considering bias pollution andsize which aspect of missing links affects prediction models the most?
when studying the relative effects of size and bias we found a high degree of collinearity between size and bias in our meta models.
multi collinearity and the resulting difficulties in interpreting models with potentially high variance inflation factors casts doubt on the stability and validity of such models.
therefore we chose to focus on a simplified categorial measure of bias bias polarity .
bias polarity admits two categories of possible biaslandbiasm.biasl negative bias polarity represents all the samples where we havelower p higher p wherelower pandhigher prepresent the probability of linking from lowerandhigher respectively.
in case of experience bias this would indicate that inexperienced developers are more likely to link than experienced developers.
likewise biasm positive polarity represents all the samples where lower p higher p. the samples are coded so that biasl resp.
biasm take the value if the sample has negative resp.
positive bias polarity.
they both take the value if the sample has neutral bias viz.
lower p higher p. arguably for the bi metrics we consider polarity is a reasonable categorical abstraction.
concern about bias can be stated in categorical terms such as are more less experienced developers more less equally likely to link b ugs?
6if we inspect lines randomly over many samples we expect to discover bugs at the same rate we inspect viz.
of the defects for inspecting of the lines thus the area under the resulting diagonal line would be .
times .
times .
7the parametric measure f50does show a much more substantial effect but this is due to threshold parameter not always being ideal we discuss this laterordoes linking rate for fixes that include more changed files lower?
or higher?
or the same?
orare more less severe bugs less more equally likely to be linked than less severe bugs .
ourabstractionshedslightonwhethersuchcategoricalbiase s affect prediction performance.
our ensemble of samples includes both biased and unbiased samples.
our set of unbiased samples will include both uniformly randomly picked unbiased samples as well as samples where lower p higher p. we also add pollution as a treatment with two levels polluted and unpolluted as described above.
those are the predictors the continuous response variables are auc aucec aucec 20andf50as response.
to find the impact of different variables we use lmg as described in section .
figure shows the lmg impact of bias sizeandpollution variables on different measures of prediction performance.
biaslandbiasmare marked as l and m respectively on the x axis.
p and s represent pollutionandsizerespectively.
since there is one prediction model for each of the training releases we get a range of lmg values bias size and pollution these are shown as box plots labeled below with l m p and s as just explained.
the figure depicts a surprising trend forauc sizematters much more than both biaslandbiasm.
we ran two sample pairedwilcox test with alternative hypothesis set to size has a larger lmg impact than either biaslorbiasm on each pair and corrected the p values for multiple hypothesis test using benjamini hochberg correction.
for all sources of missing links we observed that the impact of sizeis statistically significantly more than the impact of either biaslor biasmwith very low p values p .
.
when considering auc sizeis much more important thanbiaspolarity.
focusing effort on collecting more samples may mitigate much of the impact of biaspolarity.
foraucec 10andaucec however the impact of size doesn t dominate that of bias.
a one sided wilcox test with alternative hypothesis sizehaslessimpact than eitherbiaslorbiasm failed to establish any statistical significance of biasagainstsizeafter benjamini hochberg correction all of the p values were greater than .
.
in fact testing the alternative hypothesis sizehaslargerimpact than either biaslorbiasm we observed that size has a larger impact than both biaslandbiasmforlatency p .
.
this result holds for both aucec 10and aucec .sizealso dominates both biasl p .
and biasm p .
for aucec 20when considering proximity andbiasmforaucec p .
when considering cardinality andbiasmfor both aucec p .
and aucec p .
when considering severity .
this suggests that sizeis as important as biaseven when evaluating aucec.
with respect to f50 we again observed a dominance of sizeoverbias.
in all of our comparisons we found a statistically significantly higher impact p .
of size compared to either biaslorbiasm.
in fact it is clear from the figure that for f50 size moreso than auc dominates bothbiaslandbiasm.experience cardinality latency proximity severity .
.
.
.
.
l m p s l m p s l m p s l m p s l m p s variableslmg a impact on aucexperience cardinality latency proximity severity .
.
.
.
.
l m p s l m p s l m p s l m p s l m p s variableslmg b impact on f50 experience cardinality latency proximity severity .
.
.
.
.
l m p s l m p s l m p s l m p s l m p s variableslmg c impact on aucec 10experience cardinality latency proximity severity .
.
.
.
.
l m p s l m p s l m p s l m p s l m p s variableslmg d impact on aucec figure impact of different aspects of missing links.
l stands for bia s polarity to low values biasl m bias polarity to higher values biasm p stands for pollution all of those are categorical varia bles.
s stands for size the sole numerical variable for the ir performance measures aucandf50 the effect ofsizestrongly dominates that of bias.
for the aucec performance measure sizehas as much of an influence asbiaspolarity.
we have discussed the impact of sizeandbias however missing links would inevitably introduce pollution .
as stated above we introduce pollution as an additional treatment in our meta model.
figure shows in addition tobiasandsize the impact of pollution on performance.
the figure shows that pollution matters much more for the threshold dependent measure f50 than the thresholdinvariant measures aucandaucec.
we compared the impact ofpollution withsize biaslandbiasmusing apaired wilcox test and corrected the p values using benjaminihochberg correction.
our findings suggest that for auc aucec 10andaucec the impact of pollution is typically neither significantly greater nor smaller than the impact of biaslorbiasm.
in fact pollution showed a significantly smaller impact p .
for onlyone case against biasl foraucandcardinality .
in all cases for aucandaucec pollution showed statistically significantly less impact than size forauc p .
and for aucecthe largest observed p .
.
however a similar comparison for the threshold dependent measure f50tells a different story.
in this case pollution was dominant over both biaslandbiasm and was only outperformed by size.
apairedwilcox test using a one sided alternative that pollution has greater impact than either biaslorbiasm shows a statistical difference between the variables with a p values p .
.
a pairedwilcox test for the alternative hypothesis that pollution has smallerimpact than size however confirms the dominant role of size p .
.
forf50performance pollution plays a more damaging role than biaspolarity.
sizestill has the most impact.
.
discussion while previous studies have considered bias and its implications ours is the first detailed comparison of the effects of size biasandpollution .
we now discuss the implications.
positive negative bias note in figure that the polarity of biasindicates varying impact on the prediction performance.
in case of experience top left of the figure withaucas performance measure linking by more experienced developers the box plot labeled biasm has a stronger effect on performance.
a statistical test confirms this .
but in the case of cardinality we see the opposite effect on auc.
we also examined the effect of combining positive and negative bias.
in general we found that combining bias into a directionless biased treatment tended to lower the effect o f bias p .
for all pairwise comparisons of directionless biaswith directional bias .
in practice however it is not always easy to determine whether a given dataset has positive or negative bias.
our study suggests that direction does matter and some effort perhaps based on sampling might provide useful information.
from figure we also notice that each biasmay have an impact on the prediction performance.
e.g.
in case of experience andaucas performance measure linking by more experienced developers may be more important.
a twosamplepairedwilcox test shows that the impact of biaslontheaucis significantly different than the impact of biasm forcardinality p .
and experience p .
.
foraucec the direction of biasmattered only for severity p .
foraucec 10andp .
foraucec .
forf50 direction mattered only for cardinality p .
and latency p .
.
pollution effect on f50from figure it is clear that the f50performance measure has a much greater variance than auc.f50also has a lower variance than the aucecmeasures.
it is also clear from figure that f50is quite strongly affected by pollution when compared to other prediction performance measurements.
f50assumes a threshold on the predicted probability from the underlying logistic regression prediction models.
we hypothesized that performance based on this fixed threshold is highly sensitive to the precise training set.
we expect that different sizes and biases in the training set will result in variable performance.
to investigate this further we measured thef50score on the full datasets for each release.
a twotailed two sample kolmogorov smirnov test showed that f50 over the unsampled data and f50over the sampled data are indeed different i.e.
sampling affects the f50performance.
to reduce the sample dependent effect on f50 we tried an alternative approach for each training data set we chose the threshold that maximized f measure performance on the training set.
this is a feasible approach since no test set information is used.
we call this f measure valueftrmax.
we calculated ftrmaxfor both sampled sub datasets and the full dataset.
a statistical test failed to reject the null hypothesis that the distribution of the ftrmaxscores in the sub datasets was different from the ftrmaxscores in the full dataset.
this provides a plausible explanation for the instability in f50.
implications overall our meta analysis shows that size influencespredictionperformanceatleastasmuchas biaspolarity and pollution .
when considering inspection oriented applications all factors matter.
the aucecresults suggest that one should not only try to maximize the size of the training data but also to the extent possible strive to obtai n unbiased and unpolluted samples.
aucec however is not the last word.
for example it doesn t account very well for the cost of false negatives.
when these matter aucmight be a more suitable measure.
in this case our data suggests that sizehas amuch stronger influence on performance therefore efforts to obtain larger training sets might be a more effective way to improve performance.
since in general it is difficult to determine the nature and extent of biaspolarity this finding is a reassuring and positive result if better aucis a goal and if there is concern about the polarity of bias a larger training sample is likely to help.
finally we note that our study focused on biaspolarity rather than the degree of bias.
our results indicate very strongly that bias polarity matters far less than size if large sample sizes are available our results suggest bias polarity per seis not a major concern that it is very likely that trained prediction models would perform reasonably well at least from from an aucorf50perspective.
indeed the effect of bias isso strongly confounded by the effect of size that we found it difficult to tease apart the effect by linear modeling and this remains for future work.
.
threats to v aliditychoice of metrics our prediction models use popular process metrics.
we did not use any code metrics generally speaking process metrics outperform code metrics .
our models performed well the median aucwas around .
and median f50was about .
for the full dataset on par to what is reported in the existing research .
using logistic regression we use lr to predict the defectproneness of files.
lr is very widely used moreover prior research indicates that the choice of proper metrics matters more than the learning techniques used and that lr has good performance .
data quality instead of relying on automated extraction of links from commit logs we rely on a high fidelity linking process available in jira.
all of our projects have very high linking rates with a median linking rate about .
this is well above the typical linking rate of under reported in prior research using traditional heuristic based linking technique .
the jira data has link bias for severity median link rate for more severe vs. for less severe andlatency vs it is unbiased for proximity .
nevertheless the high linking rate allows us to subsample and introduce both positive and negative bias and pollution to study the impact on prediction performance.
.
conclusion several publications in the last years have highlighted the presence of significant bias in bug fix datasets.
this has led to widespread concerns reported in several papers that biased datasets would lead to under performing even misleading prediction models of limited practical value.
we investigated this issue in depth using simulated sampling with bias from high quality dataset and found clear evidence that a the type of bias have limited impact in on prediction results and b the effect of bias is strongly confounded by size and c bias polarity has a relatively small effect effect when compared to size for the aucandf50measures and is comparable to size for the aucecmeasures.
indeed we found that the effect bias is difficult to tease apart from size and this remains a challenge for future work.
thus the bias effect results reported earlier may indeed be due to diminished sample size in the biased samples.
our work does strongly suggest however that even ifthere are concerns about bias polarity in a potential training sample of defect repairs such a sample could still be used to train a prediction model as long as it is large the resulting prediction performance is likely to be boosted more by the size of the sample than it is hindered by any bias polarity that may exist.
.