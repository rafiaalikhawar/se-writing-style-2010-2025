toddler detecting performance problems via similar memory access patterns adrian nistor1 linhai song2 darko marinov1 shan lu2 1university of illinois at urbana champaign usa 2university of wisconsin madison usa nistor1 marinov illinois.edu songlh shanlu cs.wisc.edu abstract performance bugs are programming errors that create significant performance degradation.
while developers often useautomated oracles for detecting functional bugs detecting performance bugs usually requires time consuming manual ana lysis of execution profiles.
the human effort for performance anal ysis limits the number of performance tests analyzed and enables performance bugs to easily escape to production.
unfortuna tely while profilers can successfully localize slow executing co de profilers cannot be effectively used as automated oracles.
this paper presents t oddler a novel automated oracle for performance bugs which enables testing for performanc e bugs to use the well established and automated process of testing for functional bugs.
t oddler reports code loops whose computation has repetitive and partially similar memory a ccess patterns across loop iterations.
such repetitive work is li kely unnecessary and can be done faster.
we implement t oddler for java and evaluate it on popular java codebases.
our experiments with previously known real world performa nce bugsshow that t oddler findsthese bugswitha higheraccuracy than the standard java profiler.
using t oddler we also found new bugs in six java projects ant google core libraries junit apache collections jdk and jfreechart.
based on ou r bug reports developers so far fixed bugs and confirmed more as real bugs.
i. introduction software performance is critical for how end users perceiv e the quality of the deployed software.
performance bugs1are programming errors that create significant performance deg radation .evenwhensoftwareismatureandwrittenbyexper t programmers performance bugs have been known to cause serious and highly publicized incidents .
the state ofthe art techniques for detecting and testing for performan ce bugs are still preliminary.
as a result performance bugs easily escape to productionruns hurt user experience deg rade system throughput and waste computationalresources .
becauseperformancebugsaredifficulttofind theyaffectev en well tested software such as windows s windows explorer which had several high impact performance bugs that escape d detection for long periods of time despite their severe eff ects on user experience .
a key reason why performance bugs escape so easily to production is that testing for performance bugs cannot use the well established process of testing for functional bugs performance bug is a well accepted term in some communitie s e.g.
mozilla bugzilla defines it as a bug that affects speed or res ponsiveness .
however others prefer performance problem or performa nce issue because these problems differ from functional bugs.
we take no position on this and use performance bug and performance problem interc hangeably.withautomated oracles .
an automated oracle detects if a test triggers a functional or performance bug in which case th e developer needs to inspect the test.
to test for functional b ugs developers usually follow three steps write as many and as diverse tests as allowed by the testing budget run the se tests and use automated oracles e.g.
crashes or assertion s to find which tests fail and inspect onlythe failing tests.
to test for performance bugs developers typically write a sma ll number of tests use a profiler to localize code regions that take a lot of time to execute and then reason whether these regions can be optimized and if the effort spent for optimizi ng time addedcodecomplexity isworththepotentialspeedg ain whichmay be difficult to ascertain beforeactuallyperform ing the optimization .
in contrast to functional bugs the lack of reliable automated oracles for performance bugs means that developers cannot easily find which tests fail as in step .
asaresult becausedevelopersneedtoinspectalltests pr ofiles in step they can use only a small number of performance tests in step .
in sum developers follow the current proc ess of testing for performance bugs not because it has advantage s but because developers have no reliable alternatives.
an automated oracle for performance bugs would enable developers to test for performance bugs using the well estab lished process of testing for functional bugs.
unfortunate ly profilers cannot be used as effective oracles for three reaso ns.
first profilers give a report for each test thus running many tests results in many reports not just a few failing tests as for a typical functional oracle.
second profilers may miss a performancebug even when it is executed if the buggycode is notslowcomparedtotherest ofthat execution it isnotrank ed high by the profiler and is likely ignored by the developer.
many performance bugs manifest by significantly degrading performance only for particular input conditions and the profiled inputs cannot cover all possible conditions.
th ird profilers report what takestime but not what wastestime i.e.
they do not distinguish truly necessary albeit expen sive work from the likely unnecessarycomputation.in other word s profilersarehighlyusefulwhenthedeveloperwantsto localize a slow code region but are not effective when the developer needs todecideif a test likely exposes a performance bug and thus needs further inspection.
this paper presents t oddler a novel oraclefor performance bugs.
in brief t oddler reports tests that execute loops whose computation is repetitive and very similar acro ss iterations.
the intuition is that such loops are likely perf or c ieee icse san francisco ca usa accepted for publication by ieee.
c ieee.
personal use of this material is permitted.
permission from ieee must be obtained for all other uses in any current or future media including reprinting republishing this material for advertising or promotional purposes creating new collective works for resale or redistribution to servers or lists or reuse of any copyrighted component of this work in other works.562mance bugs that waste time because the work is repetitive and similar it could be done faster.
we designed t oddler based on two observations about performance bugs.
first many severe performance bugs over in the study in section ii are contained by nested loops if an inefficient code region is executed outside of a nested loop then the inefficiency itself needs to be very severe e.g.
slow i o f or the code region to have a real impact on the overall program performance.second wasted computationis oftenreflected by repetitive and partially similar memory accesses across lo op iterations ifa groupofinstructionsrepeatedlyaccesses similar memory values then those instructions probably compute similar results.
we implemented a full blown t oddler tool for java and a simple prototype for c c .
our experiments with previously known real world performance bugs from java projectsshow that t oddler is able to find all these bugs.our c c prototype also finds previously known bugs in gcc mozilla and mysql.
moreover using t oddler helped us identify42 new real world bugs in six popular java projects ant google core libraries junit apache collections jdk and jfreechart.
based on our reports developers so far have fixed bugs and confirmed more as real bugs and the apache collections developers even invited the first paper author to become a project committer.
our bug reports are linked from .
ii.
study of performance bugs we study over performance bugs from open source projects to identify how these bugs depend on loops.
we study both java and c c projects to obtain more generality of our findings.
these bugs were collected independently of toddler in a recent study on performancebugs but their relationship to loops was not analyzed in detail.
our study shows that about of performance bugs involveloops andmorethan50 ofperformancebugsinvolve at least two levels of loops.
the bugs that involve nested loo ps can be categorized along two dimensions istheperformanceproblemin the inneror theouterloop?
is the performance problem caused by redundant computation or inefficient computation?
we define redundant computation asthesamecomputationbeingunnecessarily repeated on the same set of data with the same result.
we next describe the four types of real world performance bugs categorized along the above two dimensions and then discuss how this understanding of real world bugs can guide our bug detectiondesign.for space reasons we will giveco de examples only for two categories but covering both inner an d outer loops as well as redundant and inefficient computatio n .
a. categories of severe performance bugs category redundancy in outer loops redundant computation is conducted across iterations of an outer loop .
this redundant computation involves an expensive inner loo p which makes the performance problem severe.
problems of1 simplified from the xyplot class in jfreechart 2public void render ... 3for intitem item itemcount item outer loop 4renderer.drawitem ...item... calls drawverticalitem simplified from the candlestickrenderer class in jfreech art 8public void drawverticalitem ... 9intmaxvolume 10for inti i maxcount i inner loop 11intthisvolume highlowdata.getvolumevalue series i .intva lue 12if thisvolume maxvolume maxvolume thisvolume ... maxvolume fig.
.
a jfreechart bug with a redundancy in the outer loop this type are usually difficult for compilers to optimize be cause they involve nested loops and usually many functions.
they are usually fixed by storing and reusing results from previous loop iterations.
figure demonstrates such a bug from jfreechart a popular java framework for drawing charts.
this bug is particula rly severe because it causes the chart display to freeze.
the ou ter loop iterates over all the items in a data set line and for each item calls the method drawitem which in turns calls the method drawverticalitem .
the inner loop line in drawverticalitem computesthe maximumvolume line of all the items in the data set.
the repeated computation of maximum is redundant because the volumes of the items do not change between calls.
thus the inner loop can be performed only once not in every iteration of the outer loop .
indeed to fix this bug the developer changed the code to cache and reuse the maximum volume.
category redundancy in inner loops redundant computation is conducted across iterations of an inner loop .
this computation waste is amplified by outer loops that dynamically call the inner loop many times.
performance problems of this type are difficult for compilers to optimize when the redundant computation involves function calls.
th ey are usually fixed by hoisting computation out of the loop.
category inefficient outer loops the program has an expensivebut necessaryinner loop.unfortunately this loop is inefficiently used by an outer loop which leads to severe performance problems.
problems of this type cannot be optimized by compilers because they require deep understandin g of code.
they are usually fixed by changing outer loops so that the inner loop will execute less frequently.
category inefficient inner loops the inner loop conducts an inefficient but not redundant computation.
th is inefficiency is amplified by an outer loop that uses each iteration to execute the inner loop on a slightly different d ata set.
again problems of this type cannot be optimized by compilers because they require deep understanding of code .
their patches need to find a more efficient or incremental algorithm to replace the inner loop which often can be achieved with a more appropriate data structure for the data set under operation.
setdecorator class in google core libraries contained th is method call 2set.removeall arraylist simplified from the abstractset class in the standard java library 4public boolean removeall collection an bracketle t?
an bracketri htc 5if size c.size 6for iterator an bracketle t?
an bracketri hti c.iterator i.hasnext remove i.next else 9for iterator an bracketle t?
an bracketri hti iterator i.hasnext outer loop if c.contains i.next i.remove simplified from the arraylist class in the standard java li brary 17public boolean contains object o 18for inti i size i inner loop 19if o.equals elementdata return true 22return false fig.
.
a google core libraries bug with an inefficient inner l oop.
this was apreviously unknown bug found by t oddler.
figure demonstrates an example from google core libraries gcl .
this is a previously unknown bug found by toddler.
after we reported it gcl developers not only fixed this bug but also searched through their entire codebas e for similar code patterns and fixed other classes affected by similar bugs.
we count these instances as one bugnot bugs.
the gcl code called the removeall method on a setobject passing it an arraylist object as a parameter.
theremoveall method removes from the set thisall the elements contained in the specified collection c. the method has a performanceoptimizationthat chooseswhether to iter ate over the set thisor the collection cbased on their sizes line under the assumption that the cost of contains and removeoperations are similar for the set and the collection when they have similar sizes.
in the elsebranch the outer loop iterates over each element of thisand checks if c contains the element lines .
whencis anarraylist contains performs a linear search lines which is inefficient so it would have beenbettertoiterateover candcallremoveonthesetbecause it has a more efficient inner loop.
indeed the gcl developers changed their code replacing the call to removeall by conceptuallyinliningthebodyof removeall andkeepingonly thethenbranch from the body.
in general the solution for this category is to simplify the inner loop computation.
b. implications why do developers need automated support for performance testing?
the above examples demonstrate that many performance bugs are difficult to avoid because they involve library functions or apis whose performance features are opaque to developers.
in addition a lot of timeconsuming computation such as many inner loops in our examples is embeddedin codewritten by differentdevelope rs.
as shown in figure gcl developers did not initially consider that the performance of the java library method abstractset.removeall is sensitive to the data structuresused for parameters and this information is not even stated in the documentation for removeall .
tool support is needed to help developers detect these hard to avoid performance bu gs.
why do we focus on nested loop performance bugs?
bugs that involve nested loops usually have severe performance impact.
the reason is that the inner loop represents an expensive computation inside the outer loop and the oute r loop amplifies the performance penalty of the inner loop.
for example in the jfreechart bug from figure the inner loop is slow but if executed only once it cannot have a significan t effect on performance however if executed many times in th e outer loop it causes the chart display to freeze.
how can we detect nested loop performance bugs?
a common feature of above nested loop performance bugs is that theyofteninvolve repeatedmemory accesspatterns .bugs from category conduct redundant computation across outer loop iterations.
a big chunk of the computation in each outer loop iteration repeats the computation from an earlier iter ation with the same input and the same result.
hence outer loop iterations share long sequences of memory reads that return the same values.
for example the iterations of the outer loop in figure share a long sequence of reads inside the intvalue method line .
bugs from category conduct redundant computation during every iteration of an inner lo op which results in memory reads that repeatedly return the sam e value.
bugs from categories and have less regular pattern s than bugs from categories and but the memory access similarities are still strong.
the outer loop iterations i n bugs from categories and often work on similar data sets.
that is the reason why developers can effectively optimize these bugs.
that is also the reason why there are usually memory reads that return similar sequences of values across outerloop iterations.
in sum looking for repeated memory acces s patternsis an effective way to look for performancebugs fro m all four categories.
iii.
toddler design and implementations motivated by the study in section ii we have developed toddler an automated oracle that finds likely performance bugs by looking for loops that read similar sequences of values across iterations.
t oddler considers such similar sequences to be a strong indication of redundant or inefficie nt computation and reports such loops as performance bugs.
toddler is a dynamic technique.
it instruments the code under test runs each test from a given test suite and report s only the tests that contain loops with similar sequences.
we first describethe instrumentationthat t oddler adds.we then describe the data structures and algorithms that t oddler uses for storing information about reads and finding similar ity among sequences.
we finally discuss our two implementations of toddler in a full blown tool for java and a simple prototype for c c .
a. instrumentation to monitor loops and read instructions t oddler instrumentsthecode boththeapplicationundertest andthelibra ries5641startloop l1 2startiter read i v1 startloop l2 startiter read i v2 startiter read i v3 read i v4 startiter read i v5 startiter read i v6 read i v7 finishloop l2 9startiter startloop l2 startiter read i v8 startiter read i v9 read i v10 startiter read i v11 read i v12 startiter read i v13 finishloop l2 read i v14 read i v15 17finishloop l1 fig.
.
example events produced by running instrumented cod e it depends on because many performance bugs are caused by the misuse of libraries.
for loops the instrumentation i s straightforward t oddleranalyzesthecode assignsaunique id for each static loop and inserts in the code three types of method calls that inform the t oddler runtime whenever a loop starts a loop iteration starts or a loop finishes.
for read instructions the instrumentation itself is also simp le for each instruction that reads object fields or array elements from the heap e.g.
java bytecode instructions getfield or aaload toddler inserts a method call that informs the toddler runtime about the value read by the instruction and the call stack within which the instruction is executed.
not e that toddler identifies a read instruction by both the static occurrence of the instruction in the code andthe dynamic context i.e.
the call stack in which the instruction exec utes.
we use the term ipcs instructionpointer call stack to refer to a static instruction with its dynamic context.
b. collecting ipcs sequences we use the term ipcs sequence to refer to the sequence of values read by all dynamic instances of an ipcs iduring an iteration of a loop l. note that when iis inside an inner loop ofl theipcs sequencefortheouterloop lislikelytocontain more than one element.
also note that t oddler buildsone ipcs sequence per ipcs rather than one ipcs sequence per the entire loop iteration and thus a loop iteration has as ma ny ipcs sequences as it has ipcss.
to illustrate figure shows an example stream of events produced when some instrumented code is executed in represents an ipcs and vmrepresents a value read.
from these events t oddler createsipcs sequencesofvaluesread by the same ipcs during a loop iteration.
for example for theouterloopl1 toddler would create ipcs sequences i1 i2 andi3 for the first iteration and i2 i3 i4 and i5 for the second iteration.
note that the ipcs sequences for the innermost loops have length e.g.
for the first dynamic instance of the innerloop l2 the ipcs sequences would be just i2 i2 and i2 fori2and similar for i3.
also note that an ipcs need not occur in every iteration of a loop e.g.
i2does not occur in the third iteration of the first dynamic instance of l2 .
in1 instruction pointer and its dynamic context 2classipcs intip callstackhash cs value of a memory location 4classval long val ipcs sequence of values read by an ipcsin one iteration 6classseq list an bracketle tval an bracketri htlist dynamic loop record 8classdynloop 9intid static id of the loop 10callstackhash cs calling context 11intiterations number of iterations map each ipcsencountered during loop execution... ...to values read by the ipcsin the iterations 14map an bracketle tipcs list an bracketle tseq an bracketri ht an bracketri htmap fig.
.
data structures for storing and processing ipcs seq uences that case t oddler still creates an ipcs sequence for l1 of consecutive values read for the same ipcs even if these values are not read in consecutive loop iterations of l2 .
while this example illustrates t oddler only on the loop nesting depth of two t oddler handles larger nesting depths in the same manner by appending ipcs sequences for the same ipcs.
for example if one iteration of some loop l0 had the events shown in figure then for that iteration of l0 toddler would create i1 i2 i3 i4 andi5 .
c. data structures figure shows the data structures that t oddler uses to store information about loops.
ipcshas anipthat statically determines the instruction e.g.
its class method and by tecode offset within the method in java and the call stack that represents the dynamic context in which the instructio n executes.
call stacks can be efficiently computed using has hing .
valrepresents a value read by an instruction which is either a primitive value or an object id obtained with system.identityhashcode in java .
note that the id is oftheobjectbeing returnedbytheread notoftheobjectbeing dereferenced .forexample in e.next theidisof e.nextnot ofe.seqisanipcs sequenceofvaluesreadbythesame ipcs in one loop iteration.
dynloop records information about one dynamic loop instance the static loop id its class method and bytecode offset within the method in java the call stac k in which the loop executes the number of loop iterations an d the ipcs sequences across all iterations for each ipcs.
for example for i2 the two ipcs sequences of the outer l1 loop arei2 .
d. algorithm for finding performance bugs figure shows the pseudo code of the top level function.
toddler checks for potential performance bugs in each dynamic loop that had more than a few iterations by default miniter this threshold is a configurable parameter of our algorithm and section iv d discusses the impact of the parameters .foreach dynloop toddler findsallipcssthat have similar ipcs sequences across loop iterations.
if the re is any such ipcs toddler reports a performance bug.
given a test suite t oddler runs each test collects dynloop objects and reports a set of static loops that have5651 one parameter for loops 2intminiter absolute number of loop iterations input the record of a dynamic loop output whether this loop has performance bugs 6boolean hasperformancebug dynloop loop 7return!
computesimilaripcss loop .empty input the record of a dynamic loop output ipcss that read similar values across iterations 12set an bracketle tipcs an bracketri htcomputesimilaripcss dynloop loop 13set an bracketle tipcs an bracketri htsimilaripcss newset an bracketle tipcs an bracketri ht ignore very small loops 15if loop.iterations miniter returnsimilaripcss 16for curipcs loop.map.keyset compare ipcs sequences for iterations in which curipcs occurs 18if aresimilariterations loop.map.get curipcs loop.itera tions similaripcss.add curipcs 20returnsimilaripcss fig.
.
the top level function for t oddler similar ipcs sequences for at least one test.
for each stati c loop t oddler generates a set of records that help in understanding and debuggingthe problem.
each record contains th e test that executesthe loop the call stack for the loop the s tatic ip of the instruction that reads similar values the call sta ck for that instruction and statistics about similarity.
note that t oddler can find the same loop to be repetitive for multiple tests.
rather than printing a report for each te st and each loop t oddler clustersthese reports based on the staticouter loop.
clustering is commonly used for grouping failure reports in testing .
e. measuring similarity figure shows the pseudo code for finding similar ipcssequences across loop iterations.
t oddler compares consecutive ipcs sequences for the same ipcs.
as mentioned in section iii a an ipcs may not be executed in every iteration ofaloop.t oddlercomputestheratioofthenumberofipcssequences to the number of loop iterations and ignores ipcss that occur in a small ratio of iterations because even if the computation at these ipcss is similar and could be optimized they may not be an expensive part of the entire loop.
by default minseqratio .
to compare the ipcs sequences of an ipcs inside a loop l toddler determines whether these ipcs sequences are similarthroughout l based on the relative number of similar consecutive ipcs sequences.
the ipcs sequences are considered similar throughout loop lif and only if the ratio is larger than the threshold.
by default minsimratio .
redundant and inefficient computation can be reflected not only by ipcs sequences that are exactly the same across iterations such as the ipcs sequences from intvalue in figure but also by ipcs sequences that are slightly different across iterations such as the ipcs sequences fo r elementdata infigure2.thus weneedtojudgewhether two ipcs sequences are similarenough to represent potential performance problems.
figure shows the pseudo code of this algorithm.
t oddleruses thelongest common substring to measure the1 two parameters for loop iterations 2floatminseqratio relative number of ipcs sequences in the loop 3floatminsimratio relative number of similar iterations input ipcs sequences for all iterations of a loop output whether ipcsreads similar values across iterations 7boolean aresimilariterations list an bracketle tseq an bracketri htseqs intiterations ignore ipcsthat occurs in a small fraction of iterations 9if seqs.size iterations minseqratio return false 10intsimilar 11for inti i seqs.size i 12if aresimilarsequences seqs seqs similar 13return similar seqs.size minsimratio fig.
.
checking the similarity throughout a loop two parameters for ipcs sequences of values 2intminlcs absolute length of the longest common substring 3floatminlcsratio relative length of the longest common substring input twoipcs sequences output whether two ipcs sequences are similar 7boolean aresimilarsequences seq s1 seqs2 8lcs longestcommonsubstring s1 s2 .size 9lcsratio lcs min s1.size s2.size 10return lcs minlcs lcsratio minlcsratio fig.
.
checking the similarity of two ipcs sequences similarity between two ipcs sequences.
note that substring refers to the consecutive occurrences of values in the ipcssequences while subsequence would refer to the potentially non consecutive occurrences of values.
the longest common substring can be computed in o nm time where nandmare the lengths of the two ipcs sequences .
we define two ipcs sequences to be similar if both the absolute and relati ve length of their longest common substring are abovethreshol ds.
by default minlcs andminlcsratio .
f. filtering reads toddler can filter reads that have repetitive values but are unlikely to indicate performance bugs.
first t oddler ignores ipcs sequences that repeat only one value.
for example an inner loop of the form for int i i this.size i repeatedly reads the value for this.size butdoesnotcontainaperformancebug.notethatthisheuris tic may cause t oddler to lose some category bugs.
for example if this.size is returned by a synchronized getter method which is slower than just reading this.size one may want to pull the getter method call out of the loop.
toddler considers all operations to take an equal amount of time and therefore does not report the repeated getter meth od calls as a performance bug.
future implementations can add timing information to t oddler.
second t oddler for java ignores reads that happen in the class initializer methods because these are executed on ly once per class loading so even if the code contains a bug developers may not want to change it.
third t oddler allows the users to specify a set of fields and methods to be ignored when the users do not expect them to be indicative of performance bugs.
t oddler ignores ipcss that either read a specifiedfieldorexecuteinacontextwhereaspecifiedmethod566idapplication description locknown new bugsbugs 1ant build tool 2apache collections collections library 3groovy dynamic language 4google core libraries collections library 5jfreechart chart framework 6jmeter load testing tool 7lucene text search engine 8pdfbox pdf framework 9solr search server jdk standard library junit testing framework sum fig.
.
applications used in experiments previously known bugs and new bugs found with t oddler.
is on the call stack.
for example some fields are used as indexesand can appear in an inner loop as for ... ... this.cursor ... if the outer loop resets cursor the ipcs sequence would repeat but repeatedly reading the ind ex itself does not indicate inefficient or redundant computati on.
as another example appending strings in a loop often leads to repeated work and in fact it is an anti pattern in java to append many stringobjects.
however to simplify coding many times developers do append strings in loops and may not want to be bothered with reports of such coding patterns.
by default t oddler ignoresonly three fields and four tostring appendmethods from the standard jdk library java.util classes.
note that specifying these library fields and methods is done only once for all applications that use the library.
g. implementations we implemented the t oddler technique in a full blown tool for java which we also call t oddler and a simple prototype for c c .
our java implementation is based on static java bytecode instrumentation using soot .
.
.
toddler uses soot to instrument every instruction that reads an object field or an array element the start of each loop the start of each loop iteration and the exit of each loop.
the implementation closely follows the pseudo code algorithms presented earlier.
it performs similarity checks online i.e.
collects ipcs sequences of values read in a dynloop object and whenever the program exits a loop calls the hasperformancebug function from figure to process the dynloop object and decide if there is a performance bug.
section iv e discusses our c c prototype.
iv.
experimental results ourevaluationfocusesonthejavaversionof t oddlerand uses popularjava codebases.figure lists basic informat ion about these codebases.
we first evaluated t oddler on previously known real world performance bugs and on over existing functional tests from these codebases.
we thensettled on the values for the toddler parameters and evaluated it on newly written performance tests.
ourexperiments found real world performance bugs in these codebases in the application code and in the libraries they use .
the rest of this section first presents our experiments with the11previouslyknownbugs.itthenpresentsourexperimen ts with performance tests and the new bugs that we found.
it next presents the evaluation with the existing functiona l tests.
it finally presents a sensitivity analysis of the para meter values.
unless otherwise specified all the experiments use the following default values miniter minseqratio minsimratio minlcs minlcsratio .
we conduct all experiments where time is measured on an amd athlon machine with .1ghz cpu 3gb memory and oracle sun jvm version .
.
.
we also conduct experiments where time is not measured on a cluster of machines while toddler does not need a cluster for regular use we needed it for our extensive experiments.
a. experiments with previously known bugs to evaluate bug detection coverage accuracy and overhea d of toddler we first used known real world bugs from the codebases.
we searched the respective bug tracking databasestocollectthesebugs theywerereportedbytheus ers of these applications and the bug description clearly marks them as performance bugs.
we run t oddler on a performance test related to the bug report for each of the bugs.
because each test is supposed to reveal a bug we effectively evaluate if t oddler hasfalse negatives that miss some bugs.
we compare the results of t oddler with the results of a traditional profiler ran on the same tests.
as explained in section i profilers are not designed to detect performance bugs but are the only traditional tool that developers could use without t oddler.
specifically we use hprof the standard java profiler.
it outputs a ranked list of methods more precisely calling contexts that consume the most time.
we measure how highly hprof ranksthe buggymethod that containsthe buggycode region .
additionally for these tests we compare the ru ntime overheads of t oddler and hprof.
bug detection results figure summarizes the results for the bugs.
t oddler finds all the bugs no false negatives and produces only one false positive.
specifical ly for bug t oddler produces two reports one showing the realbugandonebeingafalsepositive.
sectioniv cdiscus ses false positives.
t oddler finds these bugs because they involve at least two levels of loops and have similar sequenc es ofvaluesreadacrossloopiterations.infact mostofthese bugs have so strongly similar sequences that t oddler can detect them under a wide range of threshold settings.
section iv d discusses sensitivity to threshold settings.
figure also shows the results for hprof.
we use it with thecpu times option as it gives more accurate results than cpu samples though at a higher overhead.
however even withcpu times the results of hprof for the same code and same input can vary from one run to another.
therefore 567known bug detected?
false p. rank slowdown bugtodd.
hprof todd.
hprof todd.hprof check .
.
.
check check .
.
.
check check .
.
.
.
check check .
.
.
.
check .
.
.
check .
.
.
check .
.
.
.
check .
.
.
.
check check .
.
.
check .
.
.
check .
.
.
sum .9x4.0x fig.
.
comparison of t oddler and hprof for bug triggering tests.
we ran each test under hprof times and show the mean ranking of the buggy method.
the developer is unlikely to inspect more than a handful of methods reported by a profiler.
if we consider that hprof correctly detects a bug when the buggy method ranks in top then hprof detects only out of cases that t oddler detects.
on the positive hprof ranks bug consistently as number one.
on the negative for out of bugs hprof does not rank the buggy method even in the top ten.
for example bug comes from a text search server solr.
the method with the performance bug constructs a set of strings thatrepresentfilterkeywords.undernormalserversetting this set is small and the method consumes only about .
of the total search querytime.
as a consequence it ranks only abo ut 178th in the profiling results.
a careful reader may wonder if an easier approach would suffice to find the bugs that t oddler finds could we simply report all nested loops as potentially buggy?we addedcode t o count nested loops during an execution more precisely stat ic outer loops that dynamically execute at least one inner loop .
for the tests the number of such outer loops ranges from to and the total number of such loops is .
thus a na ve technique that reports every nested loop as a performance bu g would have false positives for just these bugs .
in contrast t oddler can identify truly performance wasting nested loops by analyzing memory accesspatterns and repor ts only one false positive for these cases.
performance results the last two columns of figure show the slowdown that t oddler and hprof have over an execution with no tool for the bug triggering tests.
toddler causes on average a .9x slowdown that comes frommonitoringreadaccessesandcomparingipcs sequence s. our current implementation of t oddler is about times slower than hprof.
in the future we plan to further reduce the overhead of t oddler through sampling techniques and static analysis.
b. experiments with new bugs and performance tests we further evaluate bug detection coverage and accuracy of toddler by applying it on performance tests which is the intended usage scenario for t oddler.
to avoid the bias of usastoolauthorsmanuallywritingtests weusethreesetsoft ests not written by us automatically generated tests te sts manually written by an undergraduate student familiarwith performance testing expert and tests manually wri tten in a controlled experiment by graduate and undergraduate studentsunfamiliar with performance testing novices .
we use these different sets to assess how t oddler works for tests with various characteristics.
we focus our efforts on collection classes because they are widely used and make both automated generation and manual writing of tests easier than domain specific applica tions such as groovyor lucene.
ant apache collections and google core libraries gcl implement collection classes.
the performance tests for collections follow a simple pattern create some empty collection s insert several elem ents into the collection s and finally call a method under test.
note that performance tests need not necessarily check the functional results of the methods.
the collections for per formance tests should not be very small e.g.
when testing collection.removeall collection c boththisandc should have a reasonable number of elements say over each iftheyhadaverysmallnumber say 2each itisunlike ly the test would be useful for performance testing.
we wrote a simple library to automate generation of performancetestsforcollections.ourlibrarycangenerateindiv idual collections of various types sizes element types and ele ment values e.g.
generate an arraylist integer with elements .
moreover our library can generate multiple collections with various relationships in terms of types collections o f same or different types sizes collections of same small er larger sizes and intersection of elements collections t hat are disjoint equal or partially intersect e.g.
genera te a set with elements 50and a list with elements .
our library supports exhaustive and random selection of combinations o f these relationships.
the design goal for the library was not to extensively cover all the cases but to provide some reasonab le tests for t oddler.
we collected two types of manually written tests.
we asked the expert to writetestsforanymethodsingcl andapache collections.
we asked each novice to spend an hour writing tests for a given set of methods in a class from apache collections one of these methods contained a known performance bug and we wanted to check if the students would write tests that find this bug.
figure shows the number of tests generated written for eachcodebase the numberofdynamicloopsexecuted andthe number of reports that t oddler produces.
we examined all these reports to identify if they are real bugs or false posit ives.
we found new previously unknown performance bugs in ant apache collections gcl and even in a jdk class called from these projects based on our reports developer sso far have fixed of these bugs and confirmed more as real bugs.
t oddler was highly effective in finding performance bugs using both automatically generated and manually writt en tests.
both types of tests found bugs and sometimes foundth e samebugs.
ourstudyusedolderversionsofgclandapache568whoapp tests dyn.bugsbugs in falsesumloops test pos.
auto ex pert nov ice unique bugs found fps fig.
.
experiments with performance tests.
note that the s ame bug may be found by different automatically generated and manually written tests.
collections without the fixes for the bugs we reported.
surprisingly some novice written tests found two bugs i n a class that we expected to have only one bug.
we also found7 performancebugswherethe test codeitself is unnecessarily slow.
for example the novice written t ests hadassertionsthatcheckthemethodresults andthe assert ions themselves use rather slow code e.g.
nested loops that sea rch in lists but could have searched in sets.
if such loops appear ed in the code under test they would be definite bugs that should be changed.
c. experiments with functional unit tests the first two sets of experiments used tests written for performance which is the intended usage scenario for t oddler.
to further evaluate t oddler we run it on the functional junit tests that come with the codebases used in our evaluation.
note that this is not the intended usage scenario a developer would not use functional tests for performance testingand thus would not use t oddler on the functional junit tests.
we perform these experiments onlyto stressevaluate t oddler.
our experiments use tests shown in figure .
these tests execute dynamic loops and dynamic iterations per codebase a challengefortherun timemonitoringscalability.thetes tsalso cover unique static loops that contain nested loops p er codebase a challenge for the bug detection accuracy.
toddler successfully ran for this extensive evaluation and reported43staticloopsashavingsimilarmemoryaccessesa nd thuspotentialperformancebugs.weexaminedalltheserepo rts and found real bugs.
for jfreechart one bug is in the jfreechartcodeitselfandtheotherinthestandardjdklibr ary.
for apache collections one bug we reported is already fixed and the other three bugs are similar to three bugs we previously reported and developers resolved by changing th e javadoc documentation to clarify the performance problems .
forant allthreebugshavebeenalreadyfixedinthelate st release.
ourexperimentsuse olderversionsof the codebas es.
app tests dynamicbugsbugs in falsesumloops test pos.
unique bugs found fps fig.
.
experiments on junit functional tests.
note that this is not the intended usage scenario for toddler a developer would not use functional tests for performance testing .
for apache collections we also found performance bugs in tests where the test code is unnecessarily slow and would need to be fixed had it been in the application code.
the remaining reports are false positives due to three causes.
first in reports the test input itself contains a lot of repetition and similar values so t oddler detects similarity due to the specific input provided not because th e computationis repetitive in general.
such false positives could be eliminated by using less repetitive test inputs.
second in reports the code performs some computation on all possible pairs of values from two data sets.
such code is naturally repetitive but the repetitions are useful compu tation not performance bugs.
such false positives may be eliminate d by analyzing the data flow of computation results but such an analysis is beyond the scope of this paper.
third in repor ts the computation is truly repetitive but removing the repet ition would be too complex or would not provide clear speedup so a developer is unlikely to change the code.
d. parameter sensitivity the false positive and false negative rates of t oddler are affected by the values for the five parameters described in section iii.
all these parameters provide the minimum threshold that loops iterations sequences need to satisf y to be deemed indicative of performance bugs.
hence larger thresholds could lead to fewer false positives but more fals e negatives while smaller thresholds could lead to more fals e positives but fewer false negatives.
we experimented with various threshold values to understand their impact.
figure shows the results for several configurations.
for each configuration we change only one threshold value and keep the other four at the default setting.
to evaluate the impact on false negatives we apply t oddler on the bug triggering tests for previously known bugs section iv a and count the number of bugs found.
to evaluate the impact on false positives we cannot use t oddler in the intended scenarios from sections iv a and iv b because they have few false positives.
we thus use the functionaltests section i v c .
the default configuration finds all known bugs in the experiments from section iv a and reports false positive s in the experiments from section iv c. figure plots the569 fig.
.
parameter sensitivity experiments.
each configur ation changes only one threshold with its value shown on the x axis.
the defaul t values are boxed.
two sets of experiments are conducted for each configu ration the left dark bar shows false positives on junit tests and the r ight light bar shows bugs found on bug triggering inputs.
the y axis shows the numbers normalized to the results under default setting.
number of bugs found light yellow bars and false positive s dark blue bars normalized to the values for the default configuration.
for bugs found higher is better and for fals e positives lower is better.
impact on false negatives we increased the threshold value for each parameter and for only two of them such increase has caused false negatives.
the most sensitive is minlcs which measures the absolute length of the longest common substring between two consecutive ipcs sequences for an instruction.
when minlcsincreases from the default to the number of bugs found steadily decreases from to .
the longest common substring is usually shorter than the total number of inner loop iterations which is oft en determined by the input scale.
therefore when the input sca le is small a high minlcssetting could miss many bugs.
the other parameter whose increase caused false negatives isminseqratio which measures the ratio of loop iterations that haveexecutedthe particularmemory readinstruction .the inner loop of bug .
is buried inside an ifstatement that is executed by about half of the outer loop iterations.
as a result this bug is missed once minseqratio gets over .
we believe that this type of if then else situation is commo n enough to have the default value under .
note that except for this type of bugs minseqratio can be increased to and beyond without losing any bugs.
impact on false positives for the two parameters that caused false negatives above minlcsandminseqratio we both increased and decreased the threshold values.
for the other three parameters we only increased the values.
we can see that increasing miniter andminsimratio over the default values decreases the number of false positives by about without loosing any bugs .
in practice one may want to indeed increase these parameters but we chose conservative parameter values.
in contrast minlcsratio is the least sensitive increasing it from to changes neither false positives nor false negatives.
choosing the threshold values as seen from the discussion above t oddler can work well in a large rangeof threshold values.
note that we did not choose the default threshold values for toddler to obtain the best results for false positives and false negatives .
for example we could increaseminiter andminsimratio to get fewer false positives without missing any bug.
rather we chose the default values based on our intuition about the values that could giv e reasonableresults.
moreover we settled onthese values before running t oddler on performance tests section iv b .
e.toddler for c c code theperformancebugsthat t oddler findsdonotexistonly in java code as already mentioned in section ii such bugs alsoexistinc c code.tofurtherevaluateourtechnique we implemented a prototype t oddler tool for c c code.
our prototype uses pin to automatically instrument memory reads but currently does not automatically instrument loop s we manually added loop events for six real world bugs three from gcc two from mozilla and one from mysql .
the prototype logs values read and loop events and computes similarity offlineby processing these logs using python.
the results show that this prototype can find all these six bugs.
becausewedonotinstrumentalltheloops wecannotmeasure false positives for this prototype.
v. discussion loop nesting toddler misses bugsthat are notin nested loops.
we intentionally focused on nested loops because th ey create more severe performance hits.
however non nested loops can also be slow e.g.
loops that contain i o. t oddler canbeextendedtolookforbugsinsuchloopsby modeling the native i o methods in java to make their loops explicit .
other performance bugs toddler misses several categories of performance bugs including performance bugs specific to multi threaded code such as lock contention load imbalance or false sharing bugs related to idle time and object bloat .
t oddler finds performance bugs involving loops which the existing techniqu es miss so t oddler complements these techniques.
dynamic technique just like profilers t oddler requires a test input.
fortunately developersalready write some pe rformance benchmarks but typically measure only real time and look for regressions.
t oddler provides an oracle to identify performance bugs and encourages developers to write perfor mance tests.
as our evaluation shows performance tests are relatively easy to write manually even by developers who are not familiar with performance testing and one can sometime s evenuseautomatedtest generationtechniquesforperform ance tests.
future work can focus on developing specialized test generation techniques for performance bugs.
similarity measures because the longest common substring worked quite well for comparing similarity of ipcssequences we did not evaluate any other approach.
future work could for example use edit distance to compare ipcssequences or even further capture the memory accesses not asipcs sequencesofvaluesbut as executiontrees that enco de loop iterations and then measure tree similarity.570vi.
relatedwork profiling visualization and computational complexity profiling and performance visualization tools are critica l for developers to understand the performance features of diffe rent software components.
a lot of recent progress was made to provide more accurate and efficient profiling .
however as discussed in section i profilers hav e fundamental limitations in detecting performance bugs.
se veral tools estimate the worst time computational complexi ty of code but like profilers these tools report tha t some computation takes time not if it wastes time.
t oddler complements these techniques to find performance bugs.
performance bug detection several techniquesdetect the excessive use of temporary objects a common performance problem in object oriented software .
xu et al.
us e a run timeanalysistodetectlow utilitydatastructureswh erethe effort to construct member fields outweighs the usage of thes e fields .
jin et al.
study efficiency rules in performancebug patches and detect performance bugs that are similar with previously patched ones .
other techniques detect performance problems caused by idle time multi threa d false sharing orerrorrecoveryindistributedsystem s .
the success of these tools demonstrates the potential of performance bug detection but the existing work only cove rs a small portion of real world performance bugs.
t oddler focuses on performance bugs caused by inefficient or redundant computation across nested loops.
many of these bugs such as the real world example bugs discussed in the paper cannot b e detected by the existing performance bug detectors.
theref ore toddler well complements these techniques.
vii.
c onclusions performance testing would greatly benefit from automated oracles for performance bugs.
we presented t oddler a novel oracle that detects performance bugs by identifying repetitive memory read sequences across loop iterations.
t oddlerfound new bugs in popular codebases ant google core libraries junit apache collections jdk and jfreechart.
so far developers have already fixed of these bugs and confirmed more as real bugs.
we also evaluated toddler with previously known real world performance bugs and the experiments show t oddler can effectively detect performance bugs with a much higher accuracy than profiling tools.
t oddler can help expose more performance bugs before software release by discovering problems even before they manifest as slow computation found by profilers.
while these results are highly promising t oddler is just a starting point in addressing loop related performance bug s.