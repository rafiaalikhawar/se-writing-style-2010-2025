continuous test generation enhancing continuous integration with automated test generation jos campos1andrea arcuri2gordon fraser1rui abreu3 1department of2certus software v v center3faculty of engineering computer science simula research laboratory university of porto university of sheffield uk p .o.
box lysaker norway porto portugal abstract in object oriented software development automated unit test generation tools typically target one class at a time.
a class however is usually part of a software project consisting of more than one class and these are subject to changes over time.
this context of a class offers significant potential to improve test generation for individual classes.
in this paper we introduce continuous test generation ctg which includes automated unit test generation during continuous integration i.e.
infrastructure that regularly builds and tests software projects .
ctg offers several benefits first it answers the question of how much time to spend on each class in a project.
second it helps to decide in which order to test them.
finally it answers the question of which classes should be subjected to test generation in the first place.
we have implemented ctg using the e vosuite unit test generation tool and performed experiments using eight of the most popular open source projects available on github ten randomly selected projects from the sf100 corpus and five industrial projects.
our experiments demonstrate improvements of up to for branch coverage and up to for thrown undeclared exceptions while reducing the time spent on test generation by up to .
categories and subject descriptors.
d. .
testing and debugging testing tools general terms.
algorithms experimentation reliability keywords.
unit testing automated test generation continuous testing continuous integration .
introduction research in software testing has resulted in advanced unit test generation tools such as e vosuite or pex .
even though these tools make it feasible for developers to apply automated test generation on an individual class during development testing an entire project consisting of many classes in an interactive development environment is still problematic systematic unit test generation is usually too computationally expensive to be used by developers on entire projects.
thus most unit test generation tools are permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
ase september v ster s sweden copyright acm ... .
.
on the scenario that each class in a project is considered a unit and tested independently.
in practice unit test generation may not always be performed on an individual basis.
for instance in industry there are often requirements on the minimum level of code coverage that needs to be achieved in a software project meaning that test generation may need to be applied to allclasses.
as the software project evolves involving code changes in multiple sites test generation may be repeated to maintain and improve the degree of unit testing.
yet another scenario is that an automated test case generation tool might be applied to all classes when introduced for the first time in a legacy project.
if the tool does not work convincingly well in such a case then likely the tool will not be adopted.
by considering a software project and its evolution as a whole rather than each class independently there is the potential to use the context information for improving unit test generation when generating test cases for a set of classes it would be sub optimal to use the same amount of computational resources for all of them especially when there are at the same time both trivial classes e.g.
only having get and set methods and complex classes full of non linear predicates.
test suites generated for one class could be used to help the test data generation for other classes for example using different types of seeding strategies .
finally test suites generated for one revision of a project can be helpful in producing new test cases for a new revision.
an attractive way to exploit this potential lies in using continuous integration in continuous integration a software project is hosted on a controlled version repository e.g.
svn or git and at each commit the project is built and the regression test suites are run to verify that the new added code does not break anything in the application.
continuous integration is typically run on powerful servers and can often resort to build farms or cloud based infrastructure to speed up the build process for large projects.
this opens doors for automated test generation tools in order to enhance the typically manually generated regression test suites with automatically generated test cases and it allows the test generation tools to exploit the advantages offered when testing a project as a whole.
in this paper we introduce continuous test generation ctg which enhances continuous integration with automated test generation.
this integration raises many questions on how to test the classes in a software project for instance in which order should they be tested how much time to spend on each class and which information can be carried over from the tests of one class to another?
to provide first answers to some of these questions we have implemented ctg as an extension to the e vosuite test generation tool and performed experiments on a range of different software projects.
in detail the contributions of this paper are as follows we introduce the problem of generating unit tests for whole projects and discuss in details many of its aspects.
we describe different strategies of scheduling the order in which classes are tested to improve the performance.
we propose a technique to incrementally test the units in a software project leading to overall higher code coverage while reducing the time spent on test generation.
we present a rigorous empirical study on open source projects from the sf100 corpus eight of the most popular projects on github and five industrial projects supporting the viability and usefulness of our presented techniques.
all the presented techniques have been implemented as an extension of the e vosuite test generation tool which is freely available for researchers and practitioners at .
our experiments demonstrate that by intelligently using the information provided when viewing a software project as a whole the techniques presented in this paper can lead to improvements of up to for branch coverage and up to for thrown undeclared exceptions.
at the same time applying this test generation incrementally not only improves the test effectiveness but also saves time by up to .
however our experiments also point out important areas of future research on ctg there is potential to further improve budget allocation techniques and although we provide evidence that seeding can be beneficial we also observe cases where this is not the case thus calling for more intelligent techniques to apply seeding.
.
background this section provides a brief overview of the key concepts that are relevant to this paper.
.
continuous integration and testing the roots of continuous integration can be traced back to the extreme programming methodology.
one of the main objectives of continuous integration is to reduce the problems of integration hell i.e.
different engineers working on the same code base at the same time such that their changes have to be merged together.
one approach to deal with such problems is to use controlled version repositories e.g.
svn or git and to commit changes on a daily basis instead of waiting days or weeks.
at each new code commit a remote server system can build the application automatically to see if there are any code conflicts.
furthermore at each new build the available regression test suites can be run to see if any new features or bug fixes break existing functionality developers responsible for new failures can be automatically notified.
continuous integration is widely adopted in industry and several different systems are available for practitioners.
the most popular ones include the open source projects jenkins1 cruisecontrol2 apache continuum3 oracle s hudson4and bamboo from atlassian5.
the functionalities of those continuous integration systems can typically be extended with plugins.
for example at the time of writing this paper jenkins had more than plugins including plugins that measure and visualise code coverage of tests.
besides running regression test suites on dedicated continuous integration servers these suites could also be automatically run in the background on the development machines by the ide e.g.
accessed .
accessed .
accessed .
accessed .
accessed .eclipse .
the idea would be to provide feedback to the developers as soon as possible while they are still editing code.
some authors call this approach continuous testing .
in principle continuous testing does not need to be restricted to regression test suites.
if automated oracles are available e.g.
formal post conditions and class invariants then a test case generation tool can be run continuously and can report to the developers as soon as a specification condition is violated.
one such form of continuous testing is for example discussed in .
.
automated unit test generation unit test suites are commonly used by developers to check the correctness and completeness of the code they wrote and to guard it against future regression faults.
such test suites are commonly hand written often even before the classes they are testing are implemented the so called test driven development .
to support developers in this task researchers have devised methods to automatically generate unit tests.
some approaches assume the existence of a formal model of the class e.g.
uml and many other popular approaches require only source code.
to generate unit tests from source code the simplest approach is to do so randomly .
this approach can produce large numbers of tests in a short time and the main intended usage is to exercise generic object contracts or code contracts .
approaches based on symbolic execution have been popularized by dynamic symbolic execution dse which efficiently explores paths from a given entry function.
for example a popular variant to apply dse is to manually write a parameterized unit test as an entry function and then to explore the paths through a program by deriving values for the parameters of the test .
generating unit tests that resemble manually written tests i.e.
few short test cases with high coverage is commonly done using search based software testing sbst .
when applying sbst for unit test generation efficient meta heuristic search algorithms such as genetic algorithms are used to evolve sequences of method calls with respect to code coverage and other criteria .
automatically generated test cases could be added to existing regression suites and be run within continuous integration.
for this purpose there are also commercial tools like agitarone6that can be integrated into build environments e.g.
maven such that new test suites are generated with each new build of a system.
such tools will face the same problem we are addressing in this paper.
continuous test generation is closely related to test suite augmentation test suite augmentation is an approach to test generation that considers code changes and their effects on past test suites.
some test suite augmentation techniques aim to restore code coverage in test suites after changes by producing new tests for new behaviour e.g.
while other approaches explicitly try to exercise changed code to reveal differences induced by the changes e.g.
we are also working on extending e vosuite in this direction .
although test suite augmentation is an obvious application of ctg there are differences first ctg answers the question of how to implement test suite augmentation e.g.
how to allocate the computational budget to individual classes .
second while ctg can benefit from information about changes it can also be applied without any software changes.
third ctg is not tied to an individual coverage criterion for example one could apply ctg such that once coverage of one criterion is saturated test generation can target a different more rigorous criterion.
finally the implementation as part of continuous integration makes it possible to automatically notify developers of any faults found by automated agitaronejunitgeneratordatasheet.pdf accessed .
56oracles such as assertions or code contracts.
some of the potential benefits of performing test suite augmentation continuously have also been identified in the context of software product lines .
.
the evosuite unit test generation tool in this paper we use the e vosuite tool for automatic unit test suite generation for java programs.
e vosuite works at java bytecode level so it can also be used on third party systems with no available source code and it is fully automated it does not require manually written test drivers or parameterized unit tests.
for example when e vosuite is used from its eclipse plugin a user just needs to select a class and tests are generated with a mouse click.
evosuite implements a hybrid approach that combines the best of sbst and dse to generate unit test suites for individual java classes.
it uses a genetic algorithm in which it evolves whole test suites which has been shown to be more efficient at achieving code coverage than generating tests individually .
depending on the search properties dse is adaptively used to satisfy coverage goals that are difficult for sbst.
once unit tests with high code coverage are generated e vosuite applies various post processing steps to improve readability e.g.
minimizing and adds test assertions that capture the current behaviour of the tested classes.
to select the most effective assertions e vosuite uses mutation analysis .
e vosuite can generate test suites covering different kinds of coverage criteria like for example weak and strong mutation testing and it can also aim at triggering undeclared exceptions .
e vosuite can be integrated into a programmer s development environment with its eclipse plugin or it can be used on the command line.
the whole test suite generation approach implemented by e vosuite removes the need to select an order in which to target individual coverage goals and it avoids the need to distribute the search budget among the individual coverage goals.
the problem of choosing an order and distributing the test generation budget between coverage goals is similar to the problem of scheduling and distributing test generation budget between classes of a project.
however there are subtle differences that prevent an easy direct application of the whole test suite approach at project level.
on one hand when a test case is executed on a class under test cut we can easily collect information on all the branching predicates it executes.
on the other hand when generating tests for a specific cut a most of the other cuts will not be executed.
even if a class bis called bya test cases generated for awould not be directly useful as unit tests for b e.g.
all methods called would belong to a unless post processing is applied.
furthermore there might be scalability issues when keeping in memory whole test suites for all cuts in a project all at the same time.
.
testing whole projects test generation is a complex problem therefore the longer an automated test generation tool is allowed to run on an individual class the better the results.
for example given more time a search based approach will be able to run for more iterations and a tool based on dse can explore more paths.
however the available time budget is usually limited and needs to be distributed among all individual classes of a given software project.
the problem addressed in this section can thus be summarized at high level as follows given a project x consisting of nunits and a time budget b how to best use an automated unit test generation tool to maximize code coverage and failure detection on xwithin the time limitb?
the values for nandbwill be specific to the projects on whichtest generation is applied.
in our experiments the values for n range from to with an average of .
estimating what b will look like is more challenging and at the moment we can only rely on the feedback of how our industrial partners think they will use e vosuite on whole projects.
however it is clear that already on a project of a few hundred classes running e vosuite with a minimum of just a few minutes per cut might take hours.
therefore what constitutes a reasonable value for bwill depend on the particular industrial scenario.
if e vosuite is run on developer machines then running e vosuite on a whole project at each code commit might not be a feasible option.
however it could be run after the last code commit of the day until the day after.
for example on a week day assuming a work schedule from a.m. to p.m. it could mean running evosuite for hours and hours on weekends.
given a modern multicore pc e vosuite could even be run on a whole project during the day in a similar way as done with regression suites in continuous testing but that could have side effects of slowing down the pc during coding and possible noise issues that might be caused by the cpu working at .
an alternative scenario would be a remote continuous integration system serving several applications departments within a company.
here the available budgetbwould depend on the build schedule and on the number of projects for which the continuous integration server is used.
some companies also use larger build farms or cloud infrastructure for continuous integration which would allow for larger values of b or more frequent runs of e vosuite .
the simplest na ve approach to target a whole project is to divide the budget bequally among the nclasses and then apply a tool like e vosuite independently on each for b nminutes assuming no parallel runs on different cpus cores .
in this paper we call this simple strategy and it is the strategy we have used in past empirical studies of e vosuite e.g.
.
however this simple strategy may not yield optimal results.
in the rest of this section we describe different aspects of targeting whole projects that can be addressed to improve upon the simple strategy.
note that in principle test generation for a class can be finished before the allocated budget is used up i.e.
once coverage is achieved .
in this case the time saved on such a class could be distributed on the remaining classes that is the schedule could be adapted dynamically during runtime.
for our initial experiments we optimised for coverage and exceptions where no test generation run would end prematurely.
however we will consider such optimisations as future work.
.
budget allocation in the simple approach each ncut gets an equal share of the time budgetb.
if there are kcpus cores that can be used in parallel or a distributed network of computers then the actual amount of available computational resources is k b. for example assuming a four core pc and a minute budget then a tool like e vosuite could run on cuts for one minute per cut.
however such a resource allocation would not distinguish between trivial and complex classes requiring more resources to be fully covered.
this budget allocation can be modeled as an optimization problem.
assumencuts each taking a share of the total k bbudget withbexpressed as number of minutes.
assume a testing tool that when applied on a cut cforzminutes obtains performance responset c z y which could be calculated as the obtained code coverage and or number of triggered failures in the cut c. if the tool is randomized e.g.
a typical case in search based and dynamic symbolic execution tools like e vosuite thenyis a random variable.
letjzj nbe the vector of allocated budgets for each cut andjyzj nthe vector of performance responses t c z calcu57lated oncezis chosen and the automated testing tool is run on each of thencuts for the given time budgets in z. assume a performance measure fon the entire project that should be maximized or minimized .
for example if yrepresents code coverage one could be interested in the average f z py2yz nof all of the cuts.
under these conditions maximizing f z could be represented as a search problem in which the solution space is represented by the vectorz under two constraints first their total budget should not exceed the total i.e.
p zi2zzi k b and second it should be feasible to find a schedule in which those n jobs can be run on kcpus cores within bminutes.
a trivial consequence of this latter constraint is that no value in zcan be higher than b. given this optimization problem definition any optimization search algorithm e.g.
genetic algorithms could be used to address it.
however there are several open challenges with this approach like for example the optimization process has to be quick as any time spent on it would be taken from the budget k bfor test generation.
the budget allocation optimization has to be done before generating any test case for any cut but the values t c z yare only obtained after executing the testing tool and the test cases are run.
there is hence the need to obtain an estimate function t0 astcannot be used.
this t0could be for example obtained with machine learning algorithms trained and released as part of the testing tool.
a further approach could also be to execute some few test cases and use the gathered experience to predict the complexity of the cut for future test case generation efforts.
even if it is possible to obtain a near perfect estimate functiont0 t one major challenge is that its output should not represent a single concrete value y but rather the probability distribution of such a random variable.
for example if the response is measured as code coverage a possibility could be that the output of t0 c z is represented by a jrj vector where each element represents the probability pofy obtaining such a code coverage value with interval precision i.e.
r p y i wherepr2r .
based on how ris defined could even be a single value representing a statistics of the random variable like mean and median there can be different ways to define the performance measure f z on the entire project.
after having described the budget allocation problem in general in this paper we present a first attempt to address it.
we start our investigation of addressing whole projects with a simple to implement technique.
first each cut will have a minimum amount of the time budget e.g.
z i.e.
one minute .
then the remaining budget k b n can be distributed among the ncuts proportionally to their number of branches but still under the constraintz b .
in other words we can estimate the difficulty of a cut by counting its number of branches.
this is an easy way to distinguish a trivial from a complex cut.
although counting the number of branches is a coarse measure it can already provide good results as we will show in the empirical study in this paper .
it is conceivable that more sophisticated metrics such as for example cyclomatic complexity may lead to improved budget distribution and we will investigate this in future research.
having a minimum amount of time per cut e.g.
z is independent of whether a smart budget allocation is used.
for example if we only have one core and budget b minutes it would make no sense to run e vosuite on a project with thousands of cuts as only a few milliseconds would be available on average per cut.
in such cases it would be more practical to just run e vosuite on a subset of the classes e.g.
five such that thereis enough time e.g.
one minute for each of those cuts to get some usable result.
ensuring that all classes are tested would then require allocating the budget to different classes in successive runs of e vosuite in the following days section .1will present some more ideas on how to use historical data .
.
seeding strategies after allocating the time budget zfor each of the ncuts the test data generation e.g.
using e vosuite on each of those n cuts will be done in a certain order e.g.
alphabetically or randomly assuming n k i.e.
more cuts than possible parallel runs .
this means that when we start to generate test cases for a cutc we will usually have already finished generating test suites for some other cuts in that project and these test suites can be useful in generating tests for c. furthermore there might be information available from past e vosuite runs on the same project.
this information can be exploited for seeding.
in general with seeding we mean any technique that exploits previous knowledge to help solve a testing problem at hand.
for example in sbst existing test cases can be used when generating the initial population of a genetic algorithm or can be included when instantiating objects .
seeding is also useful in a dse context in particular to overcome the problem of creating complex objects and the use of seeding in test suite augmentation is established for sbst and dse based augmentation approaches .
in order to make it possible to exploit information from different cuts within a run of e vosuite on a whole project one needs to sortthe execution of the ncuts in a way that when a class ccan use test cases from another class c0 thenc0should be executed i.e.
generated test for before c and if test execution for c0is currently running then postpone the one of ctillc0is finished but only if meanwhile another class c00can be generated tests for .
for example if a cut atakes as input an object of type b then to cover a we might need bset in a specific way.
for example public class a public void foo b b if b.isproperlyconfigured ... target using the test cases generated for bcan give us a pool of interesting instances of b. to cover the target branch in a foo one could just rely on traditional sbst approaches to generate an appropriate instance of b. but if in ctg we first generate test suites forb then we can exploit those tests for seeding in a. for example each time we need to generate an input for a foo with a certain probability e.g.
we can rather use a randomly selected instance from the seeded pool which could speed up the search.
.
continuous test generation ctg so far we have discussed generating unit tests for all classes in a project.
however projects evolve over time classes are added deleted and changed and automated test generation can be invoked regularly during continuous integration by extending it to ctg.
ctg can exploit all the historical data from the previous runs to improve the effectiveness of the test generation.
there are two main ways in which ctg can exploit such historical data first we can improve the budget allocation as newly introduced classes should be prioritized over old classes that have been extensively tested by ctg in previous runs.
second the test cases generated in the previous runs can be directly used for seedinginstead of regenerating tests for each class from scratch at every ctg run on a new software version.
.
budget allocation with historical data the budget allocation described in section .1only takes into account the complexity of a cut.
however there are several factors that influence the need to do automated test generation when it is invoked repeatedly.
usually a commit of a set of changes only adds modifies a few classes of a project.
if a class has been changed more time should be spent on testing it.
first modified source code is more prone to be faulty than unchanged source code .
second the modifications are likely to invalidate old tests that need to be replaced or add new behaviour for which new tests are required .
if a class has notbeen changed invoking automated test generation can still be useful if it can help to augment the existing test suite.
however once the test generator has reached a maximum level of coverage and cannot further improve it for a given class invoking it again will simply waste resources.
for example suppose a project xhas two classes a simple ones and a difficult one d. assume that by applying the budgetallocation section .
the budget allocated for dis twice as much than for s i.e.zd zs.
now further suppose that only shas been changed since the last commit in this case we would like to increase the time spent on testing s even though it is a simple one.
for this we first use an underlying basic budget assignment e.g.
budget orbudget seeding and then multiple by a factorh such that the budget for sbecomeszs h zd.
thus ifh which is the value we use in the experiments reported in this paper then the modified simple class swill receive the same amount of time as the unchanged difficult class d. given an overall maximum budget see section .
the total budget should not exceed this maximum even in the face of changed classes.
that is p zi2zzi k b however it will happen that adding a multiplication factor hfor new modified classes results in the total budget exceeding this maximum.
as test generation will be invoked regularly in this scenario it is not imperative that all classes are tested especially the ones that have not been modified.
so one can apply a strategy to skip the testing of some unchanged classes in the current ctg execution.
to do that we rank classes according to their complexity and the fact of whether they were modified and then select the maximum number of classes such that the total budget k bis not exceeded.
for classes that have not been changed at some point we may decide to stop invoking the test generator on them.
a possible way to do this is to monitor the progress achieved by the test generator if coverage has been achieved then generating more tests for the same criterion will not be possible.
if less than coverage has been achieved then we can invoke test generation again.
however if after several invokations the test generator does not succeed in increasing the coverage we can assume that all coverage goals that the test generator can feasibly cover have been reached.
in the context of this paper we look at the last three runs of the test generator and if there has been no improvement for the last three runs then we stop testing a class until it is changed again.
.
seeding previous test suites when repeatedly applying test generation to the same classes the results of the previous test generation run can be used as a starting point for the new run.
this is another instance of seeding as described in section .
.
there are different ways how a previous result can be integrated into a new run of a genetic algorithm.
for example in previous work where the goal was to improve upon manually written tests we re used the existing test cases by modifying the search operators of e vosuite such that whenever a new test case was generated it was based on an existing test case with a certain probability.
xu et al.
considered the reuse of test casesduring test suite augmentation for dse or search based and hybrid techniques by using the old tests as starting population of the next test generation run in this approach the success of the augmentation depends strongly on the previous tests.
the approach we took in the context of ctg is to first check which of the previous tests still compile against the new version of a cut.
for example if from version pntopn a signature i.e.
name or parameters of a method or a class name is modified test cases may no longer compile and therefore are not candidates to be included in the next test suite.
one the other hand tests that still compile on the new version of the cut can be used for seeding.
we insert such a suite as one individual into the initial population of the new genetic algorithm thus essentially applying a form of elitism between different invocations of the genetic algorithm.
.
empirical study we have implemented the techniques described in this paper as an extension of the e vosuite unit test generation tool.
this section contains an empirical evaluation of the different strategies.
in particular we aim at answering the following research questions rq1 what are the effects of smart budget allocation?
rq2 what are the effects of seeding strategies?
rq3 how does combining seeding strategies with smart budgetallocation fare?
rq4 what are the effects of using ctg for test generation?
rq5 what are the effects of history based selection and budgetallocation on the total time of test generation?
.
experimental setup to answer the research questions we performed two different types of experiments the first one aims to identify the effects of optimizations based on testing whole projects the second experiment considers the scenario of testing projects over time.
.
.
subject selection we used three different sources for case study projects first as an unbiased sample we randomly selected ten projects from the sf100 corpus of classes which consists of projects randomly selected from sourceforge this results in a total of classes.
second we used five industrial software projects classes in total provided by one of our industrial collaborators.
due to confidentiality restrictions we can only provide limited information on the industrial software.
to simulate evolution with ctg over several versions we required projects with version history.
because it is complicated to obtain a full version history of compiled software versions for each project in the two previous sets due to different repository systems and compilation methods we additionally considered the top most popular projects on github.
we had to discard some of these projects for some e.g.
junit jna there were problems with e vosuite e.g.
e vosuite usesjunit and thus cannot be applied to the junit source code without modifications .
some projects jedis mongodb java driver require a server to run which is not supported by e vosuite yet.
we were unable to compile the version of rxjava last cloned march .
class files of the rootbeer gpu compiler project belong to the org.trifort.rootbeer package however they are incorrectly compiled as edu.syr.pcpratts package.
finally we removed twitter4j the largest project of the most popular projects as our experimental setup would not have allowed to finish the experiments in time.
this leaves eight projects classes in total with version history for experimentation.
.
.
experiment procedure for each open source project of the sf100 corpus and industrial project we ran e vosuite with four different strategies simple smart budget allocation section .
seeding strategy section .
and a combination of the latter two i.e smart budget and seeding strategy at the same time budget seeding .
for the open source projects from github we ran e vosuite with the same four strategies but also with another strategy a history strategy section .
which used seeding of previous test suites section .
.
when running e vosuite on a whole project there is the question of how long to run it.
this depends on the available computational resources and how e vosuite will be used in practice e.g.
during the day while coding or over the weekend .
in this paper due to the high cost of running the experiments we could not consider all these different scenarios.
so we decided for one setting per case study that could resemble a reasonable scenario.
in particular for all the case studies we allowed an amount of time proportional to the number of classes in each project i.e.
three minutes per cut.
for the smart budget allocation we allowed a minimum amount of time z 1minute see section .
.
unlike the other strategies the history strategy requires different versions of the same project.
as considering the full history would not be feasible we limited the experiments to the last commits of each project i.e.
we considered the latest consecutive commits of each project.
note one of the eight projects only has commits in its entire history.
for the experiments we configured history to use the budget allocation as baseline because the average branch coverage on the first set of experiments projects randomly selected from sf100 corpus achieved an highest relative improvement on that approach.
the maximum time for test generation was calculated for history for each commit in the same way as for other strategies proportional to the number of cuts in the project three minutes per cut .
on the open source projects from sf100 each experiment was repeated times with different random seeds to take into account the randomness of the algorithms.
as we applied history with a time window of commits to the github projects we only ran evosuite five times on these eight projects.
on the industrial systems we were only able to do a single run.
running experiments on real industrial case studies presents many challenges and that is one of the reasons why they are less common in the software engineering literature.
even if it was not possible to run those experiments as rigorously as in case of the open source software they do provide extra valuable information to support the validity of our results.
.
.
measurements as primary measurement of success of test generation we use branch coverage.
however branch coverage is only one possible measure to quantify the usefulness of an automatically generated test suite .
in the presence of automated oracles e.g.
formal specifications like pre post conditions one would also want to see if any fault has been found.
unfortunately automated oracles are usually unavailable.
one could look at program crashes but that is usually not feasible for unit testing.
however at unit level it is possible to see if any exception has been thrown in a method of the cut and then check whether that exception is declared as part of the method signature i.e.
using the java keyword throws .
as a second measurement we used undeclared exceptions.
if an exception is declared as part of a method signature then throwing such an exception during execution would be part of normal expected behaviour.
on the other hand finding an undeclared exception would point to a unit level bug.
such a bug might not be crit ical e.g.
impossible to throw by the user through the application interfaces like a gui and could even simply point to implicit preconditions.
for example some exceptions might be considered as normal if a method gets the wrong inputs e.g.
a null object but then the developers might simply fail to write a proper method signature.
this is the case when an exception is explicitly thrown with the keyword throw but then it is missing from the signature.
whether a thrown exception represents a real fault is an import question for automated unit testing.
in particular it is important to develop techniques to filter out uninteresting exceptions that likely are just due to violated implicit preconditions.
however regardless of how many of these exceptions are caused by real bugs a technique that finds more of these exceptions would be better.
for this reason tools like e vosuite not only try to maximize code coverage but also the number of unique undeclared thrown exceptions for each method in the cuts and experiments have shown that this can reveal reals faults .
for the first set of experiments the overall time per project was fixed.
in the second set of experiments on ctg we also look at the time spent on test generation.
.
.
analysis procedure the experiments carried out in this paper are very different than previous uses of e vosuite .
in previous empirical studies each cut was targeted independently from the other cuts in the same project.
that was to represent scenarios in which e vosuite is used by practitioners on the classes they are currently developing.
on the other hand here when targeting whole projects there are dependencies e.g.
in the smart budget allocation the amount of time given to each cut depends also on the number of branches of the other cuts.
when there are dependencies analyzing the results of each cut separately might be misleading.
for example how to define what is the branch coverage on a whole project?
assume a project pcomposed ofjpj ncuts where the project can be represented as a vector p fc1 c2 cng.
assume that each cut chas a number of testing targets c of whichk c are actually covered by applying the analyzed testing tool.
because tools like e vosuite are randomized the scalar k c value will be represented by a statistics e.g.
the mean on a sample of runs e.g.
with different random seeds.
for example if the tool was run rtimes in which each time we obtained a number of covered targets ki c thenk c pi r i 1ki c r. if we want to know what is the coverage for a cut c then we can use cov c k c c i.e.
number of covered targets divided by the number of targets which is what usually done in the literature.
but what would be the coverage on p?
a typical approach is to calculate the average of those coverage values averaged over all the rruns avg p nx c2pk c c however in this case all the cuts have the same weight.
the coverage on a trivially small cut would be as important as the coverage of a large complex cut.
an alternative approach would be to consider the absolute coverage on the project per run pi p c2pki c p c2p c and with that consider the average on all the rruns p ri rx i pi 60table branch coverage results for the open source projects randomly selected from sf100 corpus.
for each project we report the branch coverage of the simple strategy.
for each of the other strategies we report their branch coverage and the effect sizes a12and relative average improvement compared to the simple strategy.
effect sizes a12that are statistically significant are reported in bold.
results on the open source case study are based on runs per configuration.
project classes simple budget seeding budget seeding coverage coverage a12 rel.
impr.
coverage a12 rel.
impr.
coverage a12 rel.
impr.
tullibee .
.
.
.
.
.
.
.
.
.
a4j .
.
.
.
.
.
.
.
.
.
gaj .
.
.
.
.
.
.
.
.
.
rif .
.
.
.
.
.
.
.
.
.
templateit .
.
.
.
.
.
.
.
.
.
jnfe .
.
.
.
.
.
.
.
.
.
sfmis .
.
.
.
.
.
.
.
.
.
gfarcegestionfa .
.
.
.
.
.
.
.
.
.
falselight .
.
.
.
.
.
.
.
.
.
water simulator .
.
.
.
.
.
.
.
.
.
table thrown exception results for the open source projects randomly selected from sf100 corpus.
for each project we report the total number i.e.
sum of the averages over runs for each cut of undeclared thrown exceptions of the simple strategy.
for each of the other strategies we report their undeclared thrown exceptions and the effect sizes a12and relative ratio difference compared to the simple strategy.
effect sizes a12that are statistically significant are reported in bold.
results on the open source case study are based on runs per configuration.
project classes simple budget seeding budget seeding exceptions exceptions a12 rel.
impr.
exceptions a12 rel.
impr.
exceptions a12 rel.
impr.
tullibee .
.
.
.
.
.
.
.
.
.
a4j .
.
.
.
.
.
.
.
.
.
gaj .
.
.
.
.
.
.
.
.
.
rif .
.
.
.
.
.
.
.
.
.
templateit .
.
.
.
.
.
.
.
.
.
jnfe .
.
.
.
.
.
.
.
.
.
sfmis .
.
.
.
.
.
.
.
.
.
gfarcegestionfa .
.
.
.
.
.
.
.
.
.
falselight .
.
.
.
.
.
.
.
.
.
water simulator .
.
.
.
.
.
.
.
.
.
with p we are actually calculating the average ratio of how many targets in total have been covered over the number of all possible targets.
the statistics avg p and p can lead to pretty different results.
considering the type of problem addressed in this paper we argue that p is a more appropriate measure to analyze the data of our empirical analyses.
all the data from these empirical experiments have been statistically analysed following the guidelines in .
in particular we used the wilcoxon mann whitney u test and the vargha delaney a12effect size.
the wilcoxon mann whitney u test is used when algorithms e.g.
result data sets xandy are compared in rthis is done with wilcox test x y .
in our case what is compared is the distribution of the values pi for each project p. for the statistical tests we consider a confidence level.
given a performance measure w e.g.
branch coverage axy measures the probability that running algorithm xyields higher w values than running algorithm y. if the two algorithms are equivalent then axy .
this effect size is independent of the raw values ofw and it becomes a necessity when analyzing the data of large case studies involving artifacts with different difficulty and different orders of magnitude for w. e.g.
axy entails one would obtain better results of the time with x. beside the standarized vargha delaney a12statistics to provide more information we also considered the relative improvement .
given two data sets xandy the relative average improvement will be defined as x y mean x mean y mean y .
testing whole projects the first set of experiments considers the effects of generating unit tests for whole projects.
table 1shows the results of the ex periments on the open source projects randomly selected from sf100 corpus.
the results in table 1are based on branch coverage.
the simple strategy is used as point of reference the results on the other strategies smart budget seeding and their combination budget seeding are presented relatively to the simple strategy on a per project basis.
for each strategy compared to simple we report the a12effect size and also the relative improvement .
table 2presents the results on the number of unique pairs exception method for each cut grouped by project.
for each run we calculated the sum of all unique pairs on all cuts in a project and averaged these results over the runs.
in other words table 2is structured in the same way as table with the only difference that the results are for found exceptions instead of branch coverage.
the results on the industrial experiments were analysed in the same way as the open source software results.
table 3shows the results for branch coverage.
however due to confidentiality restrictions no results on the thrown exceptions are reported.
the results in table 1clearly show that a smart budget allocation significantly improves branch coverage.
for example for the project sfmis the branch coverage goes from .
to .
a relative improvement of .
.
the a12 means that in all the runs with smart budget allocation the coverage was higher than in allthe runs with simple strategy.
however there are two projects in which it seems it provides slightly worse results in those cases however the results are not statistically significant.
the results in table 2are slightly different.
although the smart budget allocation still provides significantly better results on a higher number of projects statistically better in five out of and equivalent results in two subjects there are three cases in which results are statistically worse.
in two of those latter cases the branch coverage was statistically higher and our conjecture is that the way exceptions are included in e vosuite s fit61table branch coverage results for the industrial case study.
for each project we report the branch coverage of the simple strategy.
for each of the other strategies we report their branch coverage and the effect sizes a12and relative average improvement compared to the simple strategy.
results on the industrial case study are based on one single run.
project classes simple budget seeding budget seeding coverage coverage a12 rel.
impr.
coverage a12 rel.
impr.
coverage a12 rel.
impr.
projecta .
.
.
.
.
.
.
.
.
.
projectb .
.
.
.
.
.
.
.
.
.
projectc .
.
.
.
.
.
.
.
.
.
projectd .
.
.
.
.
.
.
.
.
.
projecte .
.
.
.
.
.
.
.
.
.
ness function cf.
means that test suites with higher coverage as achieved by the budget allocation would be preferred over test suites with more exceptions.
in this case improving e vosuite s optimization strategy e.g.
by using multi objective optimization may lead to better results with respect to both measurements.
for therif project a framework for remote method invocation the decrease in exceptions is not significant but also the coverage is decreased insignificantly.
in this case it seems that the use of the number of branches is not a good proxy measurement of the test complexity.
this suggests that further research on measurements other than branches as proxy for complexity would be important.
on the other hand we would like to highlight that for the templateit project the relative improvement was .
.
rq1 smart budget allocation improves performance significantly in most of the cases.
regarding input seeding in table 1there is one case in which it gives statistically better results but also one in which it gives statistically worse results.
regarding the number of thrown exceptions there are two projects in which it gives statistically better results .
unlike the budget allocation the usefulness of seeding will be highly dependent on the specific project under test.
if there are many dependencies between classes and many branches depend on specific states of parameter objects then seedingis likely to achieve better results.
if this is not the case then the use of seeding may adversely affect the search e.g.
by reducing the diversity thus exhibiting lower overall coverage in some of the projects.
however note that the actual seeding implemented in evosuite for these experiments is simplistic.
thus a main conclusion from this result is that further research is necessary on how to best exploit this additional information during the search.
rq2 input seeding may improve performance but there is a need for better seeding strategies to avoid negative effects.
finally we analyse what happens when input seeding is used together with the smart budget allocation.
for most projects either performance improves by a little compared to just using smart budget allocation or decreases by a little.
overall when combined together results are slightly worse than when just using the budget allocation.
this is in line with the conjecture that seedingused naively can adversely affect results suppose that seeding on a particular class is bad for example as is the case in the a4j project then assigning significantly more time to such a class means that compared to budget significantly more time will be wasted on misguided seeding attempts and thus the relative performance will be worse.
note also that the overall result is strongly influenced by one particular project that is problematic for input seeding i.e.
a4j with a12 0in table .
this further supports the need for smarter seeding strategies.
rq3 seeding with budget allocation improves performance but seeding strategies may negatively affect improvements achieved by budget allocation.
.
continuous test generation the second set of experiments considers the effects of ctg over time.
figure plots the overall branch coverage achieved over the course of commits.
we denote the strategy that uses seeding from previous test suites and allocation based on history.
in most of the projects the higher coverage of the history strategy achieves clearly higher coverage and this coverage gradually increases with each commit.
the coverage increase is also confirmed when looking at the results throughout history to this extent table summarizes the results similarly to the previous experiment and compares against the baseline strategies simple budget and budget seeding.
in all projects the coverage was higher than using the simplestrategy only on springside is this result not significant .
compared to budget there is an increase in all projects significant for four but for scribe coverage is essentially the same.
compared to budget seeding there is a significant increase in five projects and an insignificant increase in two projects.
interestingly for the scribe project history leads to significantly lower coverage than budget seeding.
this shows that seeding of input values is very beneficial on scribe where of the classes have dependencies on other classes in the project and indeed on average the benefit of input seeding is higher than the benefit of the history strategy.
however in principle history can also be combined with budget seeding.
rq4 ctg achieves higher coverage than testing each project version individually and coverage increases further over time.
figure shows the time spent on test generation.
note that the strategies simple seeding budget budget seeding were always configured to run with the same fixed amount of time.
during the first call of ctg the same amount of time was consumed for thehistory strategy but during successive commits this time reduces gradually as fewer classes need further testing.
rq5 ctg reduces the time needed to maximise the code coverage of unit test suites for entire projects.
let us now look at some of the examples in more detail.
async http client exhibits two interesting events during its history see figure 1a from the first commit until commit number all strategies have a constant coverage value.
at commit classes were changed and three new classes were added.
although this affected the coverage of history and also other strategies history only increase its time for test generation briefly from minutes at commit to minutes on commit on average compared to minutes for a full test generation run .
figure 2a further shows a large increase of the test generation time at commit although the coverage does not visibly change.
in this commit several classes were changed at that time but only cosmetic changes happen to the source code commit message format with chars lines .
as e vosuite apparently had already reached its maximum possible coverage on these classes no further increase was achieved.
we can observe 62commitscoverage .
.
.
.
.
.
.
.
.
.
.
budget budget and seeding history seeding simple a async http client.
commitscoverage .
.
.
.
.
.
.
.
.
.
.
budget budget and seeding history seeding simple b http request.
commitscoverage .
.
.
.
.
.
.
.
.
.
.
budget budget and seeding history seeding simple c joda time.
commitscoverage .
.
.
.
.
.
.
.
.
.
.
budget budget and seeding history seeding simple d json.
commitscoverage .
.
.
.
.
.
.
.
.
.
.
budget budget and seeding history seeding simple e jsoup.
commitscoverage .
.
.
.
.
.
.
.
.
.
.
budget budget and seeding history seeding simple f scribe.
commitscoverage .
.
.
.
.
.
.
.
.
.
.
budget budget and seeding history seeding simple g spark.
commitscoverage .
.
.
.
.
.
.
.
.
.
.
budget budget and seeding history seeding simple h springside.
figure branch coverage results over the course of commits for the github open source case study.
table coverage over time.
for each project we report the time coverage the average branch coverage over all classes in a project version averaged over all commits.
these time coverages are averaged out of the five repeated experiments.
we compare the history strategy with the simple budget and budget seeding ones and report the effect sizes a12over the five runs and relative average improvement .
effect sizes a12that are statistically significant are reported in bold.
the number of classes is not a constant as it can change at each revision.
we hence report the total number of unique classes throughout the commits and in brackets the number of classes at the first and last commits.
project classes simple budget budget seeding history coverage coverage coverage coverage as rel.
impr.
ab rel.
impr.
ab s rel.
impr.
http request .
.
.
.
.
.
.
.
.
.
jodatime .
.
.
.
.
.
.
.
.
.
json .
.
.
.
.
.
.
.
.
.
jsoup .
.
.
.
.
.
.
.
.
.
scribe .
.
.
.
.
.
.
.
.
.
spark .
.
.
.
.
.
.
.
.
.
async http client .
.
.
.
.
.
.
.
.
.
springside .
.
.
.
.
.
.
.
.
.
similar behaviour in the plots of jsoup figure 1e where a major change occurred at commit with the introduction of a new class org.jsoup.parser.tokeniserstate which adds new branches to the previously existing branches.
thehttp request subject reveals a nice increase over time although the time plot figure 2b shows only small improvement less time in total .
this is because this project consists only of a single class.
consequently most commits will change that particular class leading to it being tested more.
in the commits where the class was not tested no source code changes were performed e.g.
only test classes or other project files were changed not source code .
thus http request is a good example to illustrate how using previous test suites for seeding gradually improves test suites over time independently of the time spent on the class.
because this project has only one class the seeding strategy has similar results on average to the simple strategy.
a similar behaviour can also be observed for json see figure 1d where history leads to a good increase in coverage over time.
there is a slight bump in the coverage plot at commit figure 2d where new classes were added to the project.
jodatime scribe and springside are examples of projects with only a small increase in coverage figures 1c 1fand1h respectively .
although these projects differ in size it seems that their classes are all relatively easy for e vosuite such that additional time or the seeding has no further beneficial effect.
for example of the classes in springside have less than branches.
however in all three cases the reduction in test generation time is very large figures 2c 2fand2hrespectively .
finally spark shows interesting behaviour where allapproaches lead to increased coverage over the course of time figure1g .
this is because during the observed time window of commits the project was heavily refactored.
for example some complex classes were converted into several simpler classes increasing the time spent for non history based strategies figure2g up to a maximum of minutes on the last commit.
this project also illustrates nicely why applying seeding blindly does not automatically lead to better results for example at commit there are only out of classes that actually have dependencies and many of the dependencies are on the class responsewrapper which e vosuite struggles to cover.
as a consequence there is no improvement when using seeding.
this suggests that there is not a single optimal seeding strategy but that seeding needs to take external factors such as dependencies and achieved coverage into account.
63commitstime min history others a async http client.
commitstime min .
.
.
.
.
.
.
history others b http request.
commitstime min history others c joda time.
commitstime min history others d json.
commitstime min history others e jsoup.
commitstime min history others f scribe.
commitstime min history others g spark.
commitstime min history others h springside.
figure time spent on test generation for the github open source case study over the course of commits.
.
threats to validity threats to internal validity might come from how the empirical study was carried out.
to reduce the probability of having faults in our testing framework it has been carefully tested.
furthermore randomized algorithms are affected by chance.
to cope with this problem we repeated each experiment times for the sf100 experiments and five times for the github experiments and followed rigorous statistical procedures to evaluate their results.
to cope with possible threats to external validity the sf100 corpus was employed as case study which is a collection of java projects randomly selected from sourceforge .
from sf100 projects were randomly chosen.
although the use of sf100 provides high confidence in the possibility to generalize our results to other open source software as well we also included on our experiments some of the most popular java projects from github.
because open source software represents only one face of software development in this paper we also used five industrial systems.
however the selection of those systems was constrained by the industrial partners we collaborate with.
results on these systems might not generalize to other industrial systems.
the techniques presented in this paper have been implemented in a prototype that is based on the e vosuite tool but any other tool that can automatically handle the subjects of our empirical study could be used.
we chose e vosuite because it is a fully automated tool and recent competitions for junit generation tools suggest that it represents the state of the art.
to allow reproducibility of the results apart from the industrial case study all subjects and e vosuite are freely available from our webpage at .
.
conclusions in this paper the scope of unit test generation tools like e vosuite is extended rather than testing classes in isolation we consider whole projects in the context of continuous integration.
this permits many possible optimizations and our e vosuite based prototype provides ctg strategies targeted at exploiting complex ity and or dependencies among the classes in the same project.
to validate these strategies we carried out a rigorous evaluation on a range of different open source and industrial projects totalling classes.
the experiments overall confirm significant improvements on the test data generation up to for branch coverage and up to for thrown undeclared exceptions while reducing the time spent on test generation by up to .
our prototype at this point is only a proof of concept and there remains much potential for further improvements seeding could be improved by making it more adaptive to the problem at hand for example by using the abundant information made available through ctg.
historical data offers potential for optimizations for example by using fault prediction models.
different coverage criteria could be used for example starting with simpler criteria such as statement coverage and slowly building up to more thorough criteria such as mutation testing or entropy .
differential testing generation could lead to better regression tests than using standard code coverage criteria.
different testing scenarios such as integration testing could benefit from ctg as well.
although our immediate objective in our current experiments lies in improving the quality of generated test suites we believe that the use of ctg could also have more far reaching implications.
for example regular runs of ctg will reveal testability problems in code and may thus lead to improved code and design.
the use of ctg offers great incentive to include assertions or code contracts which would be automatically and regularly exercised.