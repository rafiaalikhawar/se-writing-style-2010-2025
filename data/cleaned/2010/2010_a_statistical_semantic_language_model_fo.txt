a statistical semantic language model for source code tung thanh nguyen tung iastate.eduanh tuan nguyen anhnt iastate.eduhoan anh nguyen hoan iastate.edutien n. nguyen tien iastate.edu electrical and computer engineering department iowa state university ames ia usa abstract recent research has successfully applied the statistical ngram language model to show that source code exhibits a good level of repetition.
the n gram model is shown to have good predictability in supporting code suggestion and completion.
however the state of the art n gram approach to capture source code regularities patterns is based only on the lexical information in a local context of the code units.
toimprovepredictability weintroduceslamc anovelstatistical semantic language model for source code.
it incorporates semantic information into code tokens and models the regularities patternsofsuchsemanticannotations called sememes rather thantheir lexemes.
it combines the local context in semantic n grams with the global technical concerns functionalityintoan n gramtopicmodel togetherwithpairwise associations of program elements.
based on slamc wedevelopedanew codesuggestionmethod whichisempiricallyevaluatedonseveralprojectstohaverelatively18 higher accuracy than the state of the art approach.
categories and subject descriptors d. .
distribution maintenance and enhancement general terms algorithms documentation experimentation measurement keywords statistical semantic language model code completion .
introduction previous research has shown that source code in programming languages exhibits a good level of repetition .
studying million locs in software projects in sourceforge gabel et al.
reported syntactic redundancy at the levels of granularity from tokens.
for example a permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
esec fse august saint petersburg russia copyright acm ... .
.forloop such as for int i i n i or a printing statement system.out.println ... occur frequently in many source files.
hindle et al.
found that such code regularities patterns can be captured by the n gram statistical language model via training on existing codebases.
the model is then leveraged to support code suggestion and completion1.
the state of the art statistical n gram language model for capturing such code repetitions patterns and code suggestion relies on the lexicalinformation and local context of code tokens .
lexical analysis is performed on source code to break it into tokens.
the sequences of the tokens called n grams are collected with different sizes.
for a token only its textual representation called lexeme is extracted.
the n grams with high occurrence counts correspond to highly frequent code called code regularities patterns .
using only lexical information the n gram model focuses on capturing code patterns at the lexical level.
however source code written in programming languages has welldefined semantics.
programming patterns at the higher levelsofabstractionwouldbeusefulforcodesuggestion completionaswell.
forexample letusconsidertwosimplestatem ents int len str.length and int l s.length whenlenand lare of the same type int andstrandsare of the same type string.
both of them are the instances of the same pattern ofgetting the length of a stringobject and assigning it to an intvariable.
it could not be captured at the lexical level due to the differences of lexemes e.g.
strversuss lenversusl .
furthermore suchlexical n gramscanprovideonlythelocalcontext.
however severalprogrammingregularities patterns might involve program elements that scatter apart and cannot be captured within n grams with reasonable sizes.
the first kind of such patterns includes the pairs of program tokens that are required to occur together duetothe syntactic rulesof a programming language e.g.
the pair of try catch in java or due to the usage specification of a software library e.g.
lockandunlockin the mutual exclusion library .
let us call it pairwise association among tokens.
the second kind of such patterns involves multiple cooccurring tokens thatoftencometogethertorealizethe same technical functionality concerns .
the api elements such as methods and data types that are used to implement certain functionality concerns will appear together more frequently in the files related to those concerns.
for example in a source file relevant to file i ofunctionality the related apis such as file fopen fread etc would be more likely to occur 1code completion refers to completing a partially typed in t oken.
code suggestion means the suggestion of a complete code toke n following a code portion .permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse august saint petersburg russia copyright acm ... .
532than the apis for other concerns.
moreover with software modularity a source file often involves a few technical functionality.
thus knowing the technical concerns of a source file could benefit for the prediction of the next token.
in this paper we introduce slamc a novel statistical semantic la nguage m odel for source c ode that incorporates semantic information into code tokens and models the regularities on their semantic annotations called sememes rather than their lexical values lexemes .
a token is annotated with its data type and semantic role if available.
for example the token stris semantically annotated as the sememevar denoting it to have the role of a variable andstringdata type.
scopes and dependencies among tokens are also used.
in addition to the local context of code tokens we also consider the global technical concerns of the source files and the pairwise associations of code tokens.
we combine n gram modeling and topic modeling into a novel n gram topic model to capture the influence of both local context and global concerns on the next token s occurrence.
based on slamc s ability to suggest next tokens we developed a new code suggestion engine that is configurable for java orc .
unlike traditional code completion with template code our engine suggests a ranked list of sequences of tokens that would complete the current code to form a meaningful code unit andmost likely appear next .
meaningfulcodeunitsaredefinedbasedonthelanguage andappearance likelihood is computed based on the generating probabilities of the sequences.
the top ranked sequences are unparsedintolexemesandsuggested.
ourempiricalevaluation on several subject systems shows that our code suggestion engineimprovesfrom18 accuracyoverthestate of theart lexical n gram approach.
our key contributions are .slamc a novel statistical semantic language model for source code with the integration of semantic n grams global concerns and pairwise association section .a code suggestion engine based on slamc section .an empirical evaluation on its accuracy and comparison to the state of the art lexical n gram model section .
.
background statistical language models are used to capture the regularities patterns in natural languages by assigning occurrence probabilities to linguistic units such as words phrases sentences and documents .
since a linguistic unit is represented as a sequence of one or more basic symbols language modeling is performed via computing the probability of such sequences.
to do that a modeling approach assumes that a sequence is generated by an imaginary often stochastic process of the corresponding language model.
formally definition language model .
a language model lis a statistical generative model defined via three components a vocabulary vof basic units a generative process g and a likelihood function p .
l .p s l is the probability that a sequence sof the elements in vis generated by the language model lfollowing the process g. when the context of discussion is clear regarding the language model l we use p s to denote p s l and call it the generating probability of sequence s. thus a language model could be simply considered to have a probability distribution of every possible sequence.
it could be estimated i.e.
trained from a given collection of sequences called a training corpus .table lexical code tokens from len str.length lexeme token type len identifier equal symbol str identifier .
period symbol length identifier left parenthesis symbol right parenthesis symbol semicolon symbol .
lexical code tokens and sequences statistical language models have been applied to software engineering such as in code suggestion completion .
to apply such a model to source code one first needs to define the vocabulary i.e.
the collection of basic units also call ed termsorwords that are used to make up a sequence.
a vocabulary can be constructed via performing lexical analysis on source code as a sequence of characters i.e.
breaking it intocode tokens based on the specification of the programming language.
the lexemes lexical values of the tokens are then collected as the basic units in the vocabulary.
the input source code is represented as a sequence of lexical code tokens which is called lexical code sequence .
formally definition lexical code token .
alexicalcode tokenis a unit in the textual representation of source code and associated with a lexical token type including identifier keyword or symbol specified by the programming language.
definition lexeme .
thelexemeof a token is a sequence of characters representing its lexical value.
definition lexical code sequence .
alexical code sequence is a sequence of consecutive code tokens representing a portion of source code.
for example after lexical analysis the piece of code len str.length is represented by a lexical code sequence of eight tokens with their token types and lexemes shown in table .
len str andlengthare three identifier tokens while the other tokens have different types of symbols.
in lexical analysis no semantic information e.g.
data type is available.
for example stris not recognized as a stringvariable and length is not recognized as the name of a method in the stringclass.
.
lexical n gram model for source code an gram model is a language model with two assumptions.
first it assumes that a sequence could be generated from left to right.
second the generating probability of a word in that sequence is dependent only on its local context i.e.
a window of previously generated words a special case of markov assumption .
such dependencies are modeled based on the occurrences of word sequences with limited lengths.
a sequence of nwords is called a n gram.
when nis fixed at or the model is called unigram bigram or trigram.
definition lexical n gram .
the lexeme of a sequence of nconsecutive code tokens is called a lexical n gram.
it is defined as the sequence of the lexemes of those tokens.
those assumptions are reasonable for source code.
that is the next code token could be predictable dependent on 533the previously written code tokens .
for example in a source file the code sequence for int i i n is considered as the local context of the next token.
this piece of code could be recognized as a forloop with ias the iterative variable and thus in many cases the next code token is i. with the assumption of generating tokens from left to right the generating probability of code sequence s s1s2... smis computed as p s p s1 .p s2 s1 .p s3 s1s2 ....p sm s1...sm that is the generating probability of a code sequence is computed via that of each of its tokens.
thus a language model needs to compute all possible conditional probabilitiesp c p wherecis a code token and pis a code sequence.
withmarkovassumption theconditionalprobability p c p is computed as p c p p c l in which lis a subsequence made of the last n code tokens of p. with this approximation a model only needs to compute and store the conditional probabilities involving at most nconsecutive code tokens.p c l is often estimated as p c l count lex l c count lex l v. lexis the function that builds the lexeme of a code sequence.
for instance the code sequence i nmight occur in several places e.g.
in a fororifstatement.
the same lexeme sequence i lparen n would be created for them and they are all counted as the occurrences of the same code sequence.
is a smoothing value for the cases of small counting values.
.
discussions and motivation the lexical n gram model has been shown to capture well coderegularities patternsatthelexicalleveltosupportcode completion suggestion .
code patterns at higher levels o f abstraction could be also useful for that task.
however such patterns with well defined semantics could not be captured well with the lexical n gram model.
moreover n grams provide only the local context for code suggestion while other influence factors such as the global context of the source files could be also useful to predict the next token.
in this work we aim to address those by adding semantic information and adding global influence factors in the model.
.
.
motivation on adding semantic information let us consider the following statement len str.length .
the lexical n gram model represents it as a lexical sequence .
however an editor with semantic analysis capability will recognize it as an assignment statement with the left hand side being a variable and the right hand side being anexpression which in turn contains a method call to a method named length.
if the code under editing is sufficiently complete further semantic analysis such as typing and scoping will help identify the data types e.g.
lenbeing of the type intandstrbeing of the type string.
semantic analysis will also help verify the applicability of the method call tolengthon the variable str andtype compatibility of the assignmentofthereturnedvaluefrom lengthtothevariable len.
a language model could benefit from such semantic information to detect the pattern getting the length of a string object and assigning it to an intvariable .
without semantic information it is challenging for a language model to detect that pattern if the variable names are different in different places.
moreover the pattern could then help in codesuggestion.
for example assume that the statement was incomplete as len str.
and code completion is requested.
if the above semantic information is available a model could determine that a method of a stringobject is sought and the returned value will be assigned to an intvariable.
thus the method lengthwould be a candidate for the next suggested token.
in brief using semantic information a language model would capture better the code patterns at higher abstraction levels thus produce better code suggestion.
.
.
motivation for adding other influence factors duringprogramming thenextcodetokencouldbechosen based on not only the local context but also the broader factors of source code.
the first factor is the system wise globaltechnical concerns functionality .
for example if the current source file involves the file i o functionality the api functions related to i o operations such as fopen fread fwrite andfclosewould be more likely to occur than the ones related to other concerns such as graphics ordatabase.
another factor is the pair wise association of program elements.
in source code some program elements often go together due to the syntax specification of the programming language or the usage specification of the apis in libraries.
for example the pairs of api functions such as lock unlock andfopen fcloseoften co occur.
thus the occurrence of one token would likely suggest that of the other.
pairwise association complements to the local context factor in n gram.
the rationale is that two associated tokens might locate so far apart that the local context in n grams cannot capture their association.
for example there are often many code tokens in between the pair fopenandfclose.
combining these factors could help detect local and global trends patterns and recommend better the next code tokens.
for example the local context could suggest that in the currently editing code there is likely a function call after aniftoken.
then if the current concern is about file i o the functions feoforfreadwould be the better candidates since they relate to file i o and appear frequently after an iftoken.
moreover if fopenappeared previously the pairwise association could suggest fcloseto be the next token.
.
semantic language model let us present slamc a statistical s emantic la nguage model designed for source c ode.
slamc encodes semantic information of code tokens into basic semantic units and captures their regularities patterns.
it also combines local context with global concern information as well as the pairwise association of tokens in the modeling process.
.
overview and design strategies let us first explain our design strategies in selecting the kinds of semantic information to be incorporated into our model.
the first semantic information is the roleof a token in a program with respect to the written programming language i.e.
whether it represents a variable data type operator function call field keyword etc.
with that slamc willbeabletolearnthesyntacticalregularities patternssuc h as after a variable there is often an assignment operator .
second it is useful to include the data types of the tokens especially the types of variables fields and literals.
data types would help us capture both syntactical patterns and the patterns at higher abstraction levels e.g.
the parameter of function system.out.println is often a stringorinte534table construction rules for sememes of semantic code tokens token role construction rule example data type t type string type variable x var str string var literalv lit java lit function decl mfunc indexof func integer function call mcall length call parameter xpara str string para fieldf field left field operator o op op .
op cast t cast integer cast keyword to corresponding reserved token if if class class block open close to corresponding reserved token of a for loop forend special literal to corresponding reserved token empty null null unknown to special lexical token lex abc lex gerliteral .
for a method the return type the declared parameter list and the number of passing parametersare important to identify and characterize the method.
thus for a method we incorporate its signature including its name class name return type and parameter list.
those kinds of information are encoded as the semantic valuesof the code tokens which we call sememes will be formally defined later .
the sememes are included in the vocabulary of our model and used to construct the n gram sequences and associated pairs in the modeling process.
to extract meaningful sequences and pairs slamc uses the scopesanddependencies of code tokens.
that is it considers only the associated pairs of the code tokens that have dependencies and in the proper scopes.
for example only the pairs of function calls having data dependencies are considered.
moreover itconsidersonlythesequencesthatarein appropriately structural scopes.
for example the sequences spanning across block function or class boundaries are excluded.
let us formally describe the concepts.
.
semantic code tokens and sequences .
.
semantic code tokens definition semantic code token .
a semantic code token is a lexical code token with associated semantic information including its id role data type sememe scope and structural and data dependencies.
definition role .
theroleof a semantic code token refers to the role of the token in a program with respect to a programming language.
the typical token roles include type variable literal operator keyword function call function declaration field and class.
for example in str.length after semantic analysis str is recognized as a semantic code token with its role of a variable while the role of lengthis a function call.
definition sememe .
thesememe of a semantic code token is a structured annotation representing its sema ntic value information including its token role and data ty pe.
definition vocabulary .
avocabulary is a collection of distinct sememes of all semantic code tokens.
table lists the construction rules to build the sememes for the popular types of semantic code tokens.
for example lengthinstr.length has the semantic role of a functioncall its sememe consists of the annotation call its class name string its name length the number of passing parameters the returned type integer and as shown in the fifth row.
that sememe represents the semantic value of that semantic code token i.e.
a method call to length.
the separator tokens e.g.
semicolons and parentheses are not associated with semantic information thus are excluded.
semantic information might be unavailable e.g.
when the current code is incomplete leading to no typing information or un deciding whether an identifier is a variable data type or a method name.
in such cases the lexical token is kept and annotated with the sememe of type lex the last row .
for a variable its sememe does not include its name e.g.
the variable stris encoded as var to denote it as a string variable.
this allows us to capture more general code patterns involving variables because variables names are often individuals choices and the naming convention might be even different across projects.
for example two statements len str.length and l s.length express the same code pattern when landlenare of type int andsandstrare of typestring although the variables names are different e.g.
lenversusl andstrversuss .
to capture that pattern and improve predictability slamc represents those statements by two code sequences with the same sememe sequence var op var op call string l ength integer similarly the concrete literals values could vary in concrete usages.
for example the pattern of printing a string could be instantiated with different string literals in different usages e.g.
system.out.println hello world!
orsystem.out.println file not found!
.
to capture code patterns with higher abstraction levels and enhance predictability for code suggestion slamc annotates the sememe of a literal with its data type rather than its lexeme.
thus those two printing statements will have the same sememe sequence.
inothercases programmingpatternscouldinvolvespecial literal values.
for example many functions use a zero as thereturnedvalueindicatingasuccessfulexecution.
objects are frequently checked for nullitybefore being processed for example if node !
null .
thus slamc has also special sememes representing such values including null zero and empty string .
for instance the expression if node !
null is captured as the sequence if var op null .
definition scope .
ascopeassociated with a semantic code token identifies the block containing that token .
535table associated information of semantic code tokens for len str.length id role sememe lexeme scope depend t1 variable var len c1.m2.b3 t2 operator op c1.m2.b3 na t3 variable var str c1.m2.b3 t4 operator op .
c1.m2.b3 na t5 method call length c1.m2.b3 for a program a scope is modeled by a sequence of blocks identifiers in its abstract syntax tree ast .
for example the scope c1.m2.b3 identifies the third block in the second method of the first class in the current source file.
definition dependency .
thedependency set of a semantic code token tis a set of ids of the other code tokens that have structural or data dependencies with t. structural dependencies are defined as child parent relations in an ast.
data dependencies are defined among program elements and currently computed via data analysis on variables.
table illustrates the semantic code tokens for the example len str.length .
the variable len with the id of t1 depends on the stringvariable str t3 and the method call length t5 thus its dependency set is .
notethatlexemesofsemanticcodetokensareusedincode suggestion section .
the patterns with sememes only suggest the token role and data type of the next token e.g.
a variable of type string .
slamc needs to find the most suitablesemantictokenandusethelexemetofillinthecode.
.
.
semantic code sequences definition semantic code sequence .
asemantic code sequence is a sequence of semantic code tokens.
definition semantic n gram .
the sememe of a semantic code sequence of size n called a semantic n gram is the sequence of the sememes of the corresponding tokens.
for example slamc represents the piece of code if node !
null as the semantic code sequence of four semantic tokens keyword if variable node operator !
and special literalnull .
the sememe of this sequence computed from those of its tokens is a semantic gram if var op null.
for brevity we will use the terms code token code sequence andn gramto refer to the semantic counterparts.
.
n gram topic model prior research shows that the latent topics recovered by topic modeling on source files correspond well to the technical concerns in a system.
thus if the topics of the code sequences are recovered they could provide a global view on the current concern functionality of the code thus could help in predicting the next token.
inspired by topic modeling by wallach we have developed an n gram topic model that integrates the information of both local contexts via n grams and global concerns via topics .
the key idea is that the probability that a token cappears at a position is estimated simultaneously based on the global in formation kand the local sequence of n previous tokens.
our model assumes a codebase to have ktopics corresponding to its concerns functionality .
since a source file1function train b k n nt 2for each source file fin training codebase b extract its semantic code sequence s collect available sememes into v randomly initiate its z 6loopnttimes for each available topic kandn graml for each token c v k l c count l c k count l k kv for each code sequence sinb estimate s 12return 14function estimate s 15repeat for each position iins sampleziwherep zi k k k sn i si for each topic k k count zi k length s k 20until is stable 21return z figure n gram topic model training predicting might involve several concerns slamc allows a code sequence to contain all ktopics with often different percentages some might potentially be zero as well .
it represents the topics of a code sequence pas a multinomial distribution sampled from the dirichlet distribution dir k .
is called topic proportion ofpand kis the proportion of topickinp.
the proportion of topic kcould be measured via the ratio of the number of tokens on topic kover the total number of tokens.
for example a code sequence representing a source file could have of its tokens about i o about string processing and on gui.
each token in a code sequence is assigned with a topic.
in slamc the generating probability of a code token cis dependent on its topic assignment kas well as on its local contextl.
this dependency is modeled by a multinomial distribution k l calledtoken distribution which is a sample of the dirichlet distribution dir v .
then to compute the probability p c p for any given code token cand code sequence p we need to do two tasks training i.e.
estimating the multinomial distribution k l for all possible topic kand local context lfrom a training codebase and predicting i.e.
estimating the multinomia l distribution forpto compute p c p .
we have developed two algorithms for those two tasks based on gibbs sampling.
.
.
training n gram topic model figure illustrates our training algorithm.
the input includes a codebase b containing a collection of source files and other pre defined parameters such as the number of topicsk hyper parameters of dirichlet distributions and the maximal size of n gramsn and the number of training iterations nt.
the output includes the vocabulary v containing all collected code tokens the token distributio ns k lfor every topic kand every possible n graml.
in addition for each source file represented as a code sequence s the output also includes its topic proportion and the topic assignment zifor every position iins.
the training algorithm first parses all source files in the codebase and builds a semantic code sequence for each of them.
it collects all code tokens into the vocabulary vand randomly initiates all latent variables e.g.
z lines .
then it performs two phase processing as follows.
536phase .
slamc uses the existing topic assignments of all sequences variables z or randomly initiates for the first iteration to estimate the token distributions i.e.
k lfor every possible topic kandn graml .
they are estimated as in line k l c count l c k count l k kv .
in this formula function count l c k counts every positioniin every sequence swheresi c sn i landzi k i.e.
the token at position iiscand is assigned to topic k andn previous tokens make up the sequence l. similarly count l k counts such positions but does not require si c. the positive parameter is added to all the counts for the smoothing purpose for the computation in later iterations.
phase .
slamc uses the estimated token distributions variables to estimate the topic proportion and topic assignment zforeverycodesequence s eachforasourcefile in the codebase lines .
first a topic is sampled and assigned for each position iins.
the probability that topickis assigned to position iis computed as in line p zi k s k k sn i si wheresiisthetokenof satposition iandsi nisthesequence insofn tokens before i. once topics are assigned for all positions i.e.
ziis sampled for every i the topic proportion is re estimated as line k count zi k length s k .
that means slamc counts the number of tokens assigned to topic k and approximately estimates the proportionoftopic kbytheratioofthenumberoftokenswithtopic kover the length of sequence s. the positive parameter is added to all the counts for the smoothing purpose.
this sampling and re estimating process is repeated on each sequence until the topic proportion is stable i.e.
converged line .
when every sequence in the codebase has a stable topic proportion the algorithm goes back to phase .
it stops when the latent variables and are stable or the number of iterations reach the maximum number nt.
representation and storage.
to save storage costs and improve running time slamc does not directly store the token distributions k l. it instead stores all n grams and theircountsinatree.
eachtreenodehasthefollowingfields a pointer to its parent a sememe in the vocabulary as its labelc a counting vector of sizekfor the counts and the total count .
the root node is an empty node.
the path from a node to the root corresponds to an n gram.
let us uselto denote the n gram from the parent node bof nodec to the root.
the value kis equal to count l c k .count l k is computed by summing over kin all children nodes of b. thistreeiscreatedwhenthetrainingalgorithmconstructs the semantic code sequences.
when a new semantic code tokencis built the algorithm extracts all possible n gramsl that end right before c nvaries from to n .
then it traverses the tree to find the path that corresponds to each n graml.
if the last node of that path does not have a child with the label c such a child is created and its total count is assigned with the value of .
otherwise its total count is increased by .
then the tree is updated at the beginning of every phase in the training process.
the algorithm processes each code token in a sequence in the training set similarly to when it creates the tree.
however if the topic assignment for that token is k it updates kinstead of .
.
.
predicting with n gram topic model the prediction algorithm has the input of a trained ngram topic model which contains the token distributionsfor all topics and n grams and a code sequence p. it first uses function estimate figure to estimate the topic proportion ofp.
then it estimates the generating probability p c p for any available token cand sequence pnof the last n code tokens of p using the following formula p c p max n summationtext k k. k pn c .
pairwise association we use a conditional probability to model this pairwise association factor.
p c b is the probability that cwill occur as the next code token if a code token bhas previously appeared.
this probability is estimated as p c b count c b count b .
to avoid the pairs that co occur by chance we consider a pair of tokens c b only if they have data dependencies.
for example if two function calls openandcloseare performed on the same file i.e.
having a data dependency they are counted.
if they are used on different files their co occurrences might not be semantically related.
to reduce the storage and computational cost we do not compute and store the probability p c b for any pair candb it would be a huge cost to compute store v2such probabilities for the entire vocabulary .
we insteadconsider only the tokens for control structures including branching loop and exception handling statements and api entities includin g classes methods functions and fields .
we also consider only the pairs of tokens within the boundary of a method.
there might be several code tokens bassociating with c in a code sequence p. we choose the one with the highest conditional probability p c b .
then we combine this conditional probability with the generating probability p c p computed via our n gram topic model section .
.
currently we choose the higher between two probabilities.
.
code suggestion based on slamc s ability of next token suggestion we have built a code suggestion engine.
let us detail it next.
.
semantics context sensitive suggestion overview.
insteadofsuggestingcodeinatemplate ourengine suggests a sequence of tokens that isbest fit to the contextofthecurrentcodeand most likely to appear next .itprovides aranked list of such sequences .
slamc is semanticbased thus we define a set of suggestion rules that are based on the current context and aim to complete a meaningful code sequence .
the idea is that such a suggested sequence would complete the code at the current position to form a meaningful code unit and likely appear next .
currently weimplementtherulestodefineameaningfulcodeunitin term of a member access a method call an infix expression or a condition expression.
for example if the code context is recognized as an incomplete binary expression such as in x thesuggestionswillbeanexpressionfortheremaining operand with a data type compatible with xin the addition.
if the context is an incomplete method call a suggestion will be an expression with a compatible type for the next argument.
if it does not match with any pre defined context the token with highest probability is suggested.
to illustrate our algorithm let us consider an example .
assume that a developer is writing a statement if node and requests a suggestion see .
our engine first converts the code into a semantic code sequence p see .
analyzing this sequence our engine recognizes that 537table rules of context sensitive suggestion context example suggestion member node.
a token for method or access field name e.g.
sizeorvalue method map.get or a type compatible expression call math.max x for the next argument e.g.
y infix x a type compatible expression expression for the other operand e.g.
y condition if orwhile a boolean expression expression e.g.
x !
yor!set.isempty other for int i a next token e.g.
i table semantic context sensitive completion current code suggestions lexical if node !
null tokens null .isroot semantic if var op null tokens op null op call node isroot boolean it matches the rule for an incomplete condition statement.
then it searches for potential code sequences qthat connect with the current code to form a boolean expression.
such sequences are ranked based on the score p q p .
assume that the search returns a ranked list of three semantic code sequences as in .
those sequences are transformed back to lexical forms and presented to the user as in .
our code suggestion algorithm has three main steps figure .
in the first step lines it analyzes the code in the current method and produces its semantic code sequence.
since the current code might not be complete or syntactically correct it uses partial program analysis ppa for code analysis and then recognizes the matched code context.
ppa parses the code into an ast which is then analyzed by slamc to produce the semantic code tokens with their sememes and other associated semantic information.
if ppa cannot parse some tokens it marks them as unknown nodes and slamc creates the semantic tokens of type lexfor them.
it also estimates the topics using n gram topic model line .
in step lines it predicts the next cod e sequences that connect with the current code to form a typecompatible code unit as described in the rule of the matched context.
all such sequences are ranked based on their scores using a search based method.
in step lines those sequences are transformed to lexical forms and presented to users for selection and filling up.
let us detail the steps .
.
predicting relevant code let us use sto denote the semantic code sequence for the entire source file under editing and for its estimated topic proportion.
since the current editing position edposmight not be at the end of s the engine starts the search from a sub sequence pofs containing all tokens prior to edpos.
it looks for sequence s q c1c2...ct. the relevant score of qis p q p p c1 p .p c2 pc1 ...p ct pc1c2...ct this suggests that we could expand the sequences token bytoken and compute the score of a newly explored sequence from the previously explored ones.
thus our engine generates relevant next sequences by searching on a lattice of1function recommend currentcode f ngramtopicmodel 2s buildsequence f sequence of semantic code tokens 3 estimatengramtopic s topic proportion of f 4p getcodeprioreditingpoint s edpos 5l search p 6foreachq l lex unparse q 8u userselect lex 9fillin u 11function search p 12l new sorted list of size topsize q new queue 13q.add empty sequence score 14repeat q q.next if length q maxdepth then continue c q expandabletokens p q for each c c q q.add qc if contextfit p q thenl.add q score q p 20untilqis empty 21iflis empty then add the top relevant tokens to l 22returnl figure code suggestion tokens of which each path is a potential suggestion using a depth limitedstrategy.
thatis itkeepsaqueue qofexploringpathsandchoosestoexpandapath qifithasnotreached the maximum depth maxdepth which is a pre defined maximum length for q lines .
if qsatisfies a context rule its score will be computed and it will be added to the ranked listlof suggested sequences line .
if no sequences satisfy the context the top relevant tokens are added line .
.
.
expanding relevant tokens theoretically at each search step every token should be considered.
however to reduce the search space we choose only the tokens expandable for the current search path q function expandabletokens at line .
to do that we use the trained n gram topic model to infer the possible sememesv q for the next token of q and then choose semantic tokens matching those sememes.
assume that the current search path is q c1c2...ci.
to find the set of possible sememes v q of the next token c we connect pandq andextractanypossible n gramslendingat ci lmighthave tokens in both pandq .
then we look for lon the prefix tree ofn grams see section .
.
.
if lexists all sememes of its children nodes are added to v q .
for each sememe v v q we create a corresponding semantic code token and put it into the set of expandable tokensc q .
we use the rules in table to infer necessary information e.g.
role or lexeme.
for instance if the sememe is call the semantic code token has the role offunction call and the lexeme of isroot.
it has the same scope as the previous token ciinqand no dependency.
thesememesofvariablesandliteralsin n gramtopicmodel do not have lexemes.
thus we infer the lexemes for sememes of variables using a caching technique.
if vis a sememeforavariable weselectallexistingsemanticcodetokens in the sequence sthat represent variables.
then all tokens for variables that belong to the same or containing scope of the last code token ciof the search path and have the same type as specified in the sememe vwill be added to c q .
for example if cihas the scope c1.m2.b3 andvis avar allstringvariables in the scopes c1.m2.b3 c1.m2 andc1are considered.
for a literal sememe we create a semantic token with the default value for its type e.g.
null .
.
.
checking of context fitness our engine uses the rules in table to check if a recommended sequence qproduced by the above process fits with the context of the current code sequence p function contextfit line .
for example from analyzing the current code via ppa to build semantic tokens our engine knows that the last method call in the current code phas less number of arguments than that of parameters specified in its sememe the context is then detected as an incomplete method call.
then based on the type of context of p our engine checks ifqfits with pas they are connected.
if an expression is expected our engine will check if qis a syntactically correct expression and has the expected type in the context p. if the context is a method call it will check if qcontains the expression that has the correct type of the next parameter for the method in p. if the context is an infix expression then the result statement of connecting pandqmust have the form ofx y wherexandyare two valid expressions and have data types compatible with operator .
similar treatment is used for a condition statement in which a boolean expression is expected to be formed.
if a context cannot be recognized due to incomplete code contextfit returns false.
.
.
computing relevance scores the relevance score of a new path qcis computed incrementally by as p qc p r c .p q p in which r c is the relevance score of the token cto the current search path.
initially r c is computed as p c pc1c2...ci using the ngram topic model see section .
.
.
since models only local context and global concern r c is adjusted for other factors.
first if cis a token for a control keyword or a method call the maximal pair wise association probability p c b for every b pc1c2...ciis selected for adjusting section .
.
otherwise if cis a token for a variable r c is adjusted based on the distance r in term of tokens from the positionofitsdeclarationtothecurrentposition.
inourcurrent implementation r c is multiplied by log r .
that is the more distant the declaration of a variable the lower its relevance to the current position.
.
transforming to lexical forms the transformation of a sequence qis done by creating the sequence of lexemes for the tokens in q. this task is straightforward since the lexeme is available in a token.
however ourenginealsoaddsthesyntacticsugarsforcorrectness line .
for instance in call the lexeme is length and the method call has no argument.
thus the lexical form length is created with added parentheses.
finally the lexical forms will be suggested in the original ranking.
.
empirical ev aluation weconductedseveralexperimentstostudyslamc scode suggestion accuracy with various configurations and to compare it with the lexical n gram model .
experiments were conducted on a computer with amd phenom ii x4 .
ghz 8gb ram and linux mint.
for comparison we collected the same data set of java projects with the same revisionsusedinhindle et al.
.
thedatasetconsists of nine systems with a total of more than 039klocs.
to evaluate on c code we also collected nine c projects.
procedure and setting.
we performed fold cross validation on each project.
we first divided the source files oftable subject systems java proj.
rel.time locs c proj.
rel.time locs ant banshee batik cruisecontrol cassandra db4o log4j lucene.net lucene mediaportal maven2 nservicebus maven3 openrastar xalan j pdf clown xerces rasp library table accuracy with various configurations model top 1top 2top 5top .
lexical n gram model .
.
.
.
.
seman.
.
.
.
.
.
seman.
cache .
.
.
.
.
seman.
cache depend.
.
.
.
.
.
seman.
cache depend.
.
.
.
.
pair.assoc .
seman.
cache depend.
lda .
.
.
.
.
seman.
cache depend.
.
.
.
.
n gram topic .
seman.
cache depend.
.
.
.
.
pair.assoc n gram topic a project into folds with similar sizes in term of locs .
each fold was chosen for testing while the remaining ones were used for training.
we performed training and testing forbothslamcandthelexical n grammodel .
toevaluate the impact of different factors in slamc we integrated various combinations of factors and performed training and testing for each newly combined model.
for comparison all models are configured to produce a single next lexical token.
suggestion accuracy is measured as follows.
for a source file in the test data our evaluation tool traverses its code sequence ssequentially.
at a position i it uses the language model under evaluation to compute the top kmost likely code tokens x1 x2 ... xkfor that position based on the previous code tokens.
since the previous tokens might not be complete we used ppa tool to perform partial parsing and semantic analysis for the code from the starting of the file to the current position to build semantic code tokens.
if the actual token siat position iis among ksuggested tokens we count this as a hit.
the top ksuggestion accuracy for a code sequence is the ratio of the total hits over the sequence s length.
for example if we have hits on a code sequence of tokens for a test file accuracy is .
total accuracy for a project is computed on all positions of its source files in the entire cross validation process.
.
sensitivity analysis impact of factors in our first experiment we evaluated the impact of different factors on code suggestion accuracy.
we chose lucene our largest java subject system.
table shows accuracy with different combinations of factors.
the first row corresponds to the lexical n gram model .
the second row shows accuracy of the model with the n grams of semantic tokens i.e.
only semantic tokens and n gram local context are considered.
the 3rdmodel is similar to the second one however the recently used variables names are cached section .
.
.
the 4throw is for the model similar to the third one however the data dependencies among tokens in an n539table accuracy of code suggestion java java proj.
rec.
lexical slamc abs.
rel.
size n gram improv improv.
ant top .
.
.
.
top .
.
.
.
batik top .
.
.
.
top .
.
.
.
cassandra top .
.
.
.
top .
.
.
.
log4j top .
.
.
.
top .
.
.
.
lucene top .
.
.
.
top .
.
.
.
maven top .
.
.
.
top .
.
.
.
maven top .
.
.
.
top .
.
.
.
xalan top .
.
.
.
top .
.
.
.
xerces top .
.
.
.
top .
.
.
.
gramareconsidered.
the n gramswithdependenciesamong their tokens are assigned twice as weights as the ones without dependencies.
the 5thmodel contains an addition of pairwise association factor to the 4thone.
to build the 6th and 7thmodels we replaced pairwise association in the 5th model with lda and n gram topic model respectively.
the last row corresponds to slamc model with all factors.
as seen the models based on semantic tokens achieved better accuracy than the lexical n gram model in .
with the addition of only semantic tokens the relative improvement in accuracy is from .
top to .
top comparing the first two rows .
adding the cache of recently used variables names also improves over the lexical n gram model especially at top .
and top accuracy .
.
this suggests that for the practical use of slamc in code completion considering the variables in the surrounding scopes helps much in filling in the next variable.
we also found that requiring dependencies within an n gram does not improve much accuracy rows and .
this could be due to short sequences as nis in this study.
interestingly adding pairwise association improves slightly better than adding lda topic model rows and .
we examined concrete cases and found that pairwise association requires the co occurrences of two tokens while lda captures the topic via the co occurrences of two or more tokens.
thus lda is too strict in those cases.
adding n gram topic model improves better than adding pairwise association rows and .
importantly slamc achieves even higher accuracy last row .
in comparing to the state of the art lexical ngram model slamc has a good relative improvement in accuracy .
top and .
top .
.
accuracy comparison our second experiment was to compare slamc with the lexicaln grammodelintwodatasetsofjavaand c projects.
tables and show the comparison results.
first for java projects accuracy with a single suggestion is .
.
for the lexical n gram model and .
.
for slamc.
forc projects top accuracy with slamc is table accuracy of code suggestion c project rec.
lexical slamc abs.
rel.
size n gram improv improv.
banshee top .
.
.
.
bs top .
.
.
.
cruise top .
.
.
.
control cc top .
.
.
.
db4o top .
.
.
.
db top .
.
.
.
lucene.
top .
.
.
.
net ln top .
.
.
.
media top .
.
.
.
portal mp top .
.
.
.
nservice top .
.
.
.
bus nb top .
.
.
.
open top .
.
.
.
rastar or top .
.
.
.
pdf top .
.
.
.
clown pc top .
.
.
.
rasp top .
.
.
.
library rl top .
.
.
.
table training time comparison in seconds model bs cc db ln mp nb or pc rl lexical n gram slamc while lexical n gram model achieves only .
.
.
with top suggestions slamc s accuracy could be as high as .
java and c .
second slamc is able to relatively improve over the lexical n gram model from .
.
java and .
c in different top ranked accuracy.
third the suggesting time in both models is about a few seconds not shown and training time for all folds in the entire cross validation process in slamc is much higher times .
however it is still within a couple hours for the largest system.
finally the result suggests different levels of code repetitiveness in different projects.
it could be due to their nature and developers coding style.
examples.
here are some interesting patterns detected by slamc.
the sememe sequence for type map.entry string object var var call map string object entryset set map.entry string object capturesapatternforaccessinga mapobject s entries.
one of its instances is for map.entry string object entry map.entryset .
lexical n gram model did not suggest correctly in an instance of this pattern with a different variable name for map.entry string object e values.entryset .
thepattern whilevar call stringtokenizer hasmoretokens boolean var op var call isfrequently used for accessing all tokens of a stringtokenizer .
one of its instances is while st.hasmoretokens t st.nexttoken ... .
whenthevariablenameschange asin while qtokens.hasmoretokens tok qtokens.nexttoken slamc still recommends correctly while the lexical n gram model does not.
.
cross project training and prediction we performed another experiment to study slamc s accuracy when it is trained and used for prediction with data 540table cross project prediction accuracy java rec.
lexical slamc abs.
rel.
project size n gram improv.
improv.
ant top .
.
.
.
top .
.
.
.
batik top .
.
.
.
top .
.
.
.
cassandra top .
.
.
.
top .
.
.
.
log4j top .
.
.
.
top .
.
.
.
lucene top .
.
.
.
top .
.
.
.
maven top .
.
.
.
top .
.
.
.
maven top .
.
.
.
top .
.
.
.
xalan top .
.
.
.
top .
.
.
.
xerces top .
.
.
.
top .
.
.
.
across projects.
for each java project in table we performed fold cross validation as in section .
.
however to predict for one fold we used not only the other nine folds but also the other eight java projects for training.
as seen when both models used the training data from other projects slamcrelativelyimprovesoverthelexical n gram model from .
.
for top and top accuracy.
this is consistent with the relative improvement of .
.
in table when training data was from only a single project.
comparingslamc saccuracyintables11and wecan see that prediction accuracy is not improved much as using cross projectdatafortraining .
.
.
thisisalsotrue for the lexical n gram model also reported by hindle et al.
.
similar accuracy implies that the degree of regularity across projects is similar to that in a single project.
threats to validity and limitations.
our selected projects are not representative.
however we chose a high number of projects with large numbers of locs.
our simulated code suggestion procedure is not true code editing.
we re implemented lexical n gram model rather than using their tool.
inaccuracy is caused by the fact that slamc does not consider class inheritance and cannot correctly resolve types roles sometimes due to incomplete code.
it also facesout of vocabularyissue codeun seenintrainingdata .
.
related work statistical language models have been successfully used in software engineering.
hindle et al.
usen gram model with lexical tokens to show that source code has high repetitiveness.
thus the n gram model has good predictability and is used to support code suggestion.
in comparison slamc has key advances.
first its basic units are semantic code tokens which are incorporated with semantic information thus providing better predictability.
second slamc s n grams are also complemented with pairwise association.
it allows the representation of co occurring pairs of tokens that cannot be efficiently captured with nconsecutive tokens in n grams.
finally a novel n gram topic model is developed in slamc to enhance its predictability via a global view on current technical functionality concerns.
code repetition is also observed by gabel et al.
.
they reported syntactic redundancy at levels of granularity from tokens.
they considered only syntactical tokens and re named ids in the code sequences while slamc operates at the semantic level.
han et al.
have used hidden markov model hmm toinferthenexttokenfromuser providedabbreviations.
abbreviated input is expanded into keywords by an hmm learned from a corpus.
in comparison their model has only local contextual information while slamc has also n gram topic modeling and pairwise association.
ngram model has been used to find code templates relevant to current task .
n grams are built over clone groups.
other line of approaches to support code completion relies on the programming patterns mined from existing code.
grapacc mines and stores api usage patterns as graphs and matches them against the current code.
the patterns most similar to the code are ranked higher.
bruch et al.
propose three algorithms to suggest the method call for a variable vbased on a codebase.
first freqccs suggests the most frequently used method in the codebase.
second arccs is based on mined associate rules where a method is oftencalledafteranother.
thethirdalgorithm best matching neighbors uses as features the set of method calls of vin the current code and the names of the methods that use v. the features of methods in examples are matched against those of the current code for suggestion.
precise completes the parameter list of a method call.
it mines a codebase to build a parameter usage database.
upon request it queries the database to find best matched parameter candidates and concretizes the instances.
omar et al.
introduce active code completion in which interactive and specialized code generation interfaces are integrated in the code completion menu to provide additional information on the apis in use.
otherstrategieshavebeenproposedtoimprove codecompletion.
hill and rideout use small cloned fragments for code completion.
it matches the fragment under editing with small similar structure code clones.
robbes and lanza introduced six strategies to improve code completion usingrecent histories of modified inserted code during an editing session and on the methods and class hierarchy related to the current variable.
hou and pletcher found that ranking method calls by frequency of past use is effective.
eclipse and intellij idea support template based completion for common constructs apis for while iterator .
mapo mines api patterns and suggests associated code examples .
strathcona extracts structural context of the current code and finds its relevant examples.
mylyn a code recommender learns from a developer s personal usage history and suggests related methods.
.
conclusions we introduce slamc a novel statistical semantic language model for source code.
it incorporates semantic information into code tokens and models the regularities patterns of such semantic annotations.
it combines the local context in semantic n grams with the global technical concerns into an n gram topic model.
it also incorporates pairwise associations of code elements.
based on slamc we have developed a new code suggestion technique which is empirically evaluated on open source projects to have relatively18 higheraccuracythanthelexical n grammodel.
.