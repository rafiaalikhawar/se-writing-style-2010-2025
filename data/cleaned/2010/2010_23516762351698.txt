using unfoldings in automated testing of multithreaded programs kari k hk nen olli saarikivi keijo heljanko department of information and computer science school of s cience aalto university po box fi aalto finland kari.kahkonen olli.saarikivi keijo.heljanko aalto .fi abstract in multithreaded programs both environment input data and the nondeterministic interleavings of concurrent even ts can affect the behavior of the program.
one approach to systematically explore the nondeterminism caused by input data is dynamic symbolic execution.
for testing multithreaded programs we present a new approach that combines dynamic symbolic execution with unfoldings a method originally developed for petri nets but also applied to many other models of concurrency.
we provide an experimental comparison of our new approach with existing algorithms combining dynamic symbolic execution and partial order re ductions and show that the new algorithm can explore the reachable control states of each thread with a significantly smaller number of test runs.
in some cases the reduction to the number of test runs can be even exponential allowing programs with long test executions or hard to solve con strains generated by symbolic execution to be tested more efficiently.
categories andsubject descriptors d. .
software program verification d. .
symbolic execution general terms verification algorithms reliability keywords dynamic symbolic execution unfoldings .
introduction designing correct multithreaded programs is a very challenging task mostly due to the large number of ways how different threads can interleave their execution.
for example if there are nindependent operations being executed permission to make digital or hard copies of all or part of thi s work for personal or classroom use is granted without fee provided th at copies are not made or distributed for profit or commercial advantage an d that copies bear this notice and the full citation on the first page.
to cop y otherwise to republish to post on servers or to redistribute to lists re quires prior specific permission and or a fee.
ase september essen germany copyright acm ... .
.concurrently there are n!
possible interleavings.
different interleavings can lead to possibly different states in the pr ogram and therefore a programmer needs to make sure that no interleaving leads to an erroneous state.
it is however easy for the programmer to erroneously miss some of the possible interleavings.
it is easy to generate an execution tree that represents all the possible interleavings of a program.
this execution tree can be finite or infinite depending whether there is a cycle in the state space of the program.
a large number of the interleavings can be irrelevant for checking propert ies like the reachability of a control state in the program.
this is because quite often some of the operations executed by a thread are independent with operations in other threads and therefore the set of possible interleavings can be partitio ned into equivalence classes that are often called mazurkiewic z traces .
a sequence of executed operations is one lineari zation of a trace and the rest of the linearizations belonging t o the same mazurkiewicz trace can be obtained by swapping adjacent independent operations in the sequence.
as the number of possible interleavings can grow very quickly ways to fight the state explosion caused by this are needed.
partial order reduction methods e.g.
persistent sets can be seen as reducing the execution tree representation containing all possible interleavings by i gnoring provably irrelevant parts of the tree such that at least one representative from each mazurkiewicz trace equivalen ce class gets explored.
an alternative way to fight state explosion is to use a compression approach by constructing a symbolic representation of the possible interleavings th at is more compact than the full execution tree.
unfoldings first introduced in the context of verification algorithms by mcmillan is an example of such a representation see for an extensive survey on the topic .
in this work we present an unfolding approach to construct a compact representation of the interleavings of mul tithreaded programs.
this allows our new testing algorithm to avoid irrelevant interleavings.
in particular we can so metimes cover all of the local control states of the system using less test runs than there are mazurkiewicz traces of the system.
we will also show how this approach can be combined with dynamic symbolic execution dse to test multithreaded programs that use input values as part of the execution.
symbolic execution of sequential programs can also be seen as a compression approach.
it is easy to consider all possible combinations of input values however this becom es quickly infeasible because just two bit integer input va l permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
ase september essen germany copyright acm ... .
ues would generate 2possible combinations.
symbolic execution constructs a symbolic representation that parti tions the input values to equivalence classes such that all the possible input values are still presented by the symboli c structure.
given this similarity we feel that the unfoldin g approach integrates naturally to dse in an intuitive way.
it is also possible to combine partial order reduction methods with dse.
persistent set based dynamic partial order reduction and race detection and flipping are examples of algorithms presented recently that are suitable for this.
in this work we compare our approach with these algorithms and discuss the advantages and disadvantages of using a compression approach over reduction methods in testing multithreaded programs.
especially we show that our algorithm allows better reduction in the number of needed test runs than the current dynamic partial order reduction based approaches.
the main contributions are the following i a new algorithm combining dynamic symbolic execution and unfolding methods that allows multithreaded programs containing input values to be tested ii an approach to unfold a multithreaded program using only the information col lected while executing the program and iii a comparison o f the new approach with dynamic partial order reduction and race detection and flipping that are similar to algorithms based on partial order reductions.
the rest of this paper is organized as follows.
section provides the necessary background definitions and concepts used in this work.
section presents the new approach to explore multithreaded programs.
a comparison of this approach with related approaches is given in section .
an experimental evaluation of the new algorithm is given in section and section concludes the paper.
.
background this section introduces background technical definitions and concepts needed for understanding the new algorithm.
.
basicdefinitions .
.
petrinets anetis a triple p t f where pandtare disjoint sets of places and transitions respectively and f p t t p is a flow relation.
places and transitions are called nodes and elements of fare called arcs.
the preset of a node x denoted by x is the set y p t f y x .
the postset of a node x denoted by x is the set y p t f x y .
a marking of a net is a mapping p mapsto n. a marking mis identified with the multiset which contains m p copies of p. graphically markings are represented by putting tokens on circles that represent the places of a net.
a petri net is a tuple p t f m where p t f is a net and m0is an initial marking of p t f .
.
.
causality conflictandconcurrency causality conflict and concurrency between two nodes x andyin a net are defined as follows nodes xandyare in causal relation denoted as x y if there is a non empty path of arcs from xtoy.
in this case we say that xcausally precedes y. node xis in conflict with y denoted as x y if there is a place zdifferent form xandysuch that z x unfoldnet a petri net unf empty net pe possibleextensions unf while pe e atio slash do add an event e peand its output conditions to unf pe possibleextensions unf end while figure basic unfolding algorithm z y and the paths from ztoxand from ztoytake different arcs out of z. node xis concurrent with y denoted as xcoy if nodes are neither causally related x y ory x nor in conflict.
any two nodes xandy such that x ne ationslash yare either causally related are in conflict or are concurrent.
a co set is a set of nodes where all nodes in the set are pairwise concurrent.
.
.
occurrencenets andbranchingprocesses a directed graph can be unwinded into a possibly infinite tree when starting from a root node such that each node in the tree is labeled by a corresponding node in the graph.
for nondeterministic sequential programs this unwinding would be the computation tree of the program where the computations maximally share their prefixes.
in a similar way petri nets can be unfolded into labeled occurrence nets .
these nets have a simple dag like structure.
a multithreaded program can be unwinded to a computation tree but it can also be represented as an unfolding that allows the prefixes of computations to be shared even more succinctly.
intuitively these unfoldings represent both t he causality of events as well as the conflicts between them.
formally an occurrence net ois net b e g such that o is acyclic for every binb b for every x b ethere is a finite number of nodes y b esuch that y x and no node is in conflict with itself.
to avoid confusion when talking about petri nets and their occurrence nets the node s bandeare called conditions andevents respectively.
labeled occurrence net also called branching process is a tuple o l b e g l where l b e mapsto p tis a labeling function such that adapted from l b pandl e p for all e e the restriction of lto eis a bijection between e andl e the restriction of ltomin o is a bijection between min o and m0 where min o denotes the set of minimal elements with respect to the causal relation for all e f e if e fandl e l f then e f. it is possible to obtain different branching processes by stopping the unfolding process at different times.
the maximal branching process possibly infinite is called the unfolding of a petri net.
.
constructing unfoldings here we first consider a simple algorithm for unfolding petri nets and later adopt it for testing multithreaded pro grams.
the approach we take is based on truncating an151global variables thread thread int x local int a x x 2thread thread 4r r w wthread thread 2x1 1x2 n0 thread thread 2x1 1x2 n1 x1 x2 thread thread 2x1 1x2 n2 x1 x2 n42 x1 x 1x x 2x2 x1 x1 3x1 x 5n3... omitted for brevity figure unfolding of the simple example infinite unfolding similar to dynamic symbolic execution approaches that truncate the infinite computation tree to guarantee termination.
using advanced techniques such as cut off events see is non trivial and left for further s tudy.
algorithm in fig.
shows the basic approach to generate unfoldings of petri nets.
the algorithm starts with an empty net and in each iteration computes a set of events that can be added to the current copy of the unfolding and extends the unfolding with an event from this set.
as explained in to improve the performance of the algorithm it is not necessary to compute the the whole set of possible extension in each iteration but instead update the set with events that are enabled by the last event added to the unfolding.
computing the possible extensions is computationally the most expensive part of unfolding algorithms and in fact a decisio n version of the problem can be shown to be np complete for a more detailed analysis see .
as a first example of unfolding multithreaded programs consider the simple program in fig.
with two threads where the first thread simply reads a shared variable and the second thread writes to the shared variable.
there are two possible ways to execute this program either the read operatio nis executed before the write or the other way around.
to unfold this program we start with an initial set of conditions that contains a condition for the initial program counter va lues for each thread and a copy for the shared variable xfor each thread.
this is shown in fig.
as the net n0.
from this initial state the net can be extended by either adding an event corresponding to the read or the write operation.
n1is the resulting net after adding the write event.
note that the write accesses a copy of the variable xof all threads.
for this new net there are again two possible ways to extend it.
either by a read event from the initial state or by a read event that reads the value written by the write event already added to the net.
in n2the latter read is added to the net.
finally the net n4is the resulting unfolding of the program after all events have been added.
the construction of such unfoldings follows the general idea of the algorithm in fig.
and is formalized in section .
also note that the only race in the program is the race in the initial state between write in event and read in event .
this can be seen in the final unfolding where the condition x1 1has two outgoing edges such that the following events belong to different threads.
.
dynamicsymbolicexecution dynamic symbolic execution also known as concolic testing is a method where a given program is executed both concretely and symbolically at the same time in order to explore the different behaviors of the program.
the main idea behind this approach is to at runtime collect symboli c constraints at each branch point that specify the input values causing the program to take a specific branch.
as an example a program x x if x would generate constraints input1 and input1 at the if statement given that the symbolic value input1is assigned initially to x. a path constraint is a conjunction of the symbolic constraints corresponding to each branch point in a given execution.
all input values that satisfy a path constraint will explore the same execution path for sequential programs.
for multithreaded programs the schedule affects the execution path as well.
the nondeterminism caused by the thread interleavings can be handled in dynamic symbolic execution by taking control of the scheduler and considering the sched ule as an input to the system.
for more details see e.g.
.
.
representing multi threaded programs to formalize our algorithm we use a simple language to describe the programs that can be tested.
to keep the presentation simple this language does not not contain dynamic thread creation or dynamically varying number of shared variables instead these are fixed at program start time.
handling these features in the context of java programs is discussed in section .
.
the syntax of the language for describing threads in the programs is shown in fig.
and can be seen as a subset of programming languages such as c c or java.
we assume that the only nondeterminism in threads is caused by concurrent access of shared variables or by input data from the environment.
we also assume that the memory model is sequentially consistent.
operations that acce ss shared variables are called visible operations as they can b e used to share information between the threads.
the state of a multithreaded program consists of the local state of each of the threads and the shared state consisting of the shared152t stmt thread stmt l s labeled stmt.
s lv e sv lv statement ifb goto l lock sv unlock sv e lv sv c lv op lv input expression b true false boolean expr.
lv lv lv e atio slash lv lv lv lv lv lv lv lv lv op ... lv is a local variable sv is a shared variable and c is a constant figure simplified language syntax ... ...read global variable write global variable acquire lock l release lock lsymbolic branching true falsex1 x1 2x1 x1 2xn xn lxlypckpc ipcipci pcipci pcjpcjpcjpcjpcj figure modeling execution events variables.
the visible operations considered in this work a re read and write of shared variables and acquire and release of locks.
we assume that a read operation reads a value from a shared variable and assigns it to a variable in the local state of the thread performing the operation.
write assigns either a constant or a value from a local variable to a shared variable.
non visible operations such as if statements are evaluated solely on the values in the local state of the executing thread and therefore cannot access shared variables directly.
in real programs the statements can be modified automatically to satisfy these assumptions.
.
combiningdynamicsymbolicexecution andunfoldings in this section we describe how a multithreaded program can be tested using dse and unfolding techniques without constructing an explicit model of the program e.g.
a very large high level petri net that would be unfolded.
instead the approach constructs an unfolding of the program executions based solely on the information collected dynamicall y during test runs.
to build an unfolding to capture the control and data flow of a multithreaded program we model the visible operations executed during testing with nets shown in fig.
.
these constructs are the ones employed in .
when testing a program with nthreads the unfolding is initially set to contain one condition for each thread and nconditions for each shared variable.
the conditions for shared variablesunfoldprogram p program unf initial unfolding pe set of events enabled in the initial state of p while pe e atio slash do choose target pe event sequence execute p target k for all e event sequence do ife unfthen addeand its output conditions to unf pe pe e updatepotext pe unf e end if end for end while figure main loop of the new testing algorithm can be seen as local copies of the shared variable for each thread labeled xn ifor thread nin fig.
.
having local copies of shared variables allows for example two reads of a same variable by different threads to be independent by construction.
each event added to the unfolding has one condition that corresponds to a control location of a thread at preset and another such condition in the postset.
similarly events corresponding to visible operations have co nditions for shared variables in their pre and postsets.
for example reading a shared variable accesses the local copy o f the variable of the reading thread whereas writing accesse s the copies of all threads.
the conditions in fig.
can be divided into three categories.
one set of conditions represent the control locatio n or a program counter value of a thread the second set represents shared variables and the third represents locks .
to simplify the discussion we refer to these types of conditions as thread conditions shared variable conditions a nd lock conditions respectively.
as any program has a unique next operation for each program counter value it is easy to see that for each thread condition all the events in the postset of the condition must be of the same type e.g.
if a thread condition has a read event in the post set all other events in the post set must be read events as well .
when an event is added to the unfolding we need to compute its possible extensions.
by considering the combination of each thread condition and its following event type as a result of unfolding a transition in a petri net we could use existing standard petri net possible extension algorit hms in the construction of the unfolding.
however these algorithms are designed for arbitrary petri nets in mind and are computationally expensive.
in our case the structure of the unfolding we want to generate is quite restricted and we will show in the following that these restrictions allow the possible extensions to be computed more efficiently in practice.
.
the algorithm the new algorithm shown in fig.
starts by adding events that are enabled in the initial state of the program to a set of possible extensions denoted as pein line of the algorithm .
the algorithm then uses the standard dse approach where an unvisited location in the execution struc ture is selected in this case an event from the set of possible extensions in line and the program is executed with a schedule and input values obtained by solving the path constraints collected during earlier executions to explore th e target location.
the information obtained from this exe 153cution is then converted into a sequence of events line and any errors during the execution such as uncaught exceptions are reported.
this sequence can be seen as a net containing events corresponding to the operations observe d during the execution.
a net containing only events and and their pre and postsets in fig.
would be an example of an event sequence.
the construction of event sequences is further illustrated in the example at the end of this section .
notice that the events will not be added to the unfolding when an event sequence is constructed.
instead the events in the sequence are processed one by one in their execution order the for loop in line .
in this loop an event will be added to the unfolding unless it was already added by an earlier execution.
when a new event is added the set of possible extensions is updated line with new events that could be enabled after firing the event just added.
to guarantee termination the execution depth of each thread is limited by bound kand the test run is terminated if the number of executed operations exceeds this limit.
it should be noted that even if the event sequence is computed in line by re executing the program the algorithm works also when the event sequence is obtained by backtracking the execution to the point where the target event is executed and continuing from there.
this could be done for example by forking the execution at each point where new possible extensions are discovered.
example .
to illustrate the algorithm further consider the program in fig.
.
the unfolding shown in the figure has been generated by executing first the thread fully followe d by thread in the first test run.
the event sequence for this execution corresponds to the events and .
notice that the if statements of the threads do not generate events as they do not depend on input values in this particular execution i.e.
they are not evaluated after the x input statement has been executed .
adding event to the net does not result in any new possible extensions.
however when event is added the possible extensions algorithm notices that the write event of xcan be performed either before or after the event .
therefore two possible extension events are generated that correspond to events and .
the possible extension corresponding to event will already be explored by the current execution and therefore is taken out of the set of possible extensions when the event is added to the unfolding.
for the second test run an event from the set of possible extensions is selected as a new test target.
in this case the set contains only the event .
this event can be reached by a schedule that forces the test run to first execute two visible operations of thread and then allows the test run to follow an arbitrary schedule .
with this schedule after adding the event the test run evaluates the if statement of thread which now depends on the input value assigned to xby thread .
assuming that the test run follows the false branch an event corresponding to the true branch along with a path constraint is added as a possible extension event .
for the final test run the path constraint input is solved and the resulting concrete value is used as the input.
this allows the error location of the program to be reached.
if the part of an unfolding that belongs to a single thread is considered in isolation it can be seen as a symbolic execution tree constructed by dse together with additionalglobal variables thread thread int x local int a x local int b x if a if b error x input error5 4thread thread x 1x2 7x x x x 5x2 x2 x2 figure complete unfolding of the example branching for scheduling related operations.
note also tha t if some operation in the program can be reached by multiple execution paths or non equivalent schedules the respecti ve events in the unfolding are distinct as the set of events that causally preceed a given event exactly describe an executio n path through the control flow graph of the program and a set of schedules belonging to the same mazurkiewicz trace.
we next describe the updatepotext subroutine in section .
andexecute in section .
.
.
computing possibleextensions as discussed before we could use the standard possible extension algorithms to implement updatepotext .
for example to compute possible extensions for a thread condition that has read as the next operation we coukd check every shared variable condition of the thread executing the read operation to see if it is concurrent with the thread condition.
if it is then there is a read event as a possible exten sion that has the shared variable condition and the thread condition in its preset.
the number of shared variable conditions that need to be considered using this approach can be large and it might be the case that only a small subset of them are actually concurrent with the thread condition in question.
in this section we describe properties of the type of unfoldings that are generated by our algorithm that allows us in many cases reduce the number of conditions that need to be considered when computing possible extensions.
the approach presented here is inspired by the dpor algorithm that looks for transitions that are in race and can be seen as being adjacent in some test execution.
to make the discussion precise we use the following definitions.
definition .
if a place pin a petri net is marked initially and every transition that has pin its preset has it also in its postset the place pispermanently marked .
definition .
letsbe a set of places.
a set cof conditions represents sif s c and for each x sthere exists y csuch that l y x. for single conditions we write that a condition crepresents a place pifl c p. definition .
letc1andc2be co sets of conditions that represent the same set of places s. the co sets c1and154c2areadjacent if there exists a sequence of events e0...en and a reachable marking mcontaining all the conditions in c1such that firing the event sequence from mleads to a marking where all conditions in c2are marked and there is no prefix of the event sequence e0...ei nleading to a marking that contains a co set representing s. example .
let us consider the co sets a x1 x2 b x1 x2 andc x1 x2 in fig.
.
the sets a andbare not adjacent as there are two events connecting them.
however bis adjacent with candcis adjacent with a. definition .
two co sets c1andc2that represent the same set of places are alternatives if there exists a co set c3 that is adjacent to both c1andc2and there exists conditions a c1 b c2andx c3such that x a and x b .
lemma .letcbe a condition in an unfolding and sbe a set of initially marked places.
let gbe a graph constructed as follows there is a vertex in gfor every co set representingssuch that all conditions in the co set are concurrent withcand there is an edge between two vertices if the respective co sets are either adjacent or alternatives.
the graph g is connected.
proof.
see appendix.
once we have found one possible extension for some event lemma shows that the graph ggives the search space from where the other possible extensions can be found.
we can then do a backtrack search in this space to compute the possible extensions.
by restricting the search to graph g instead of searching the whole unfolding the search space for possible extensions is in practice in many cases greatly reduced.
in worst case however the search spaces in both cases are the same.
it will be shown in the following that one possible extension that acts as the starting point for th e search can always be found efficiently.
in order to do a backtrack search in the graph g we need to be able to compute adjacent and alternative co sets efficiently.
for a co set ccontaining only shared variable conditions it is easy to determine the adjacent co sets as the shared variables are modeled with permanently marked places.
specifically the event sequences in definition con tain only a single read or write event as firing any such event transforms cto a new co set representing the same shared variable conditions.
all these events can therefore be foun d in the pre and postset of the conditions in c. checking if two conditions are concurrent can be done in linear time in the size of the unfolding.
adjacent lock conditions can be efficiently computed by by maintaining a mapping from each lock event to the set of following unlock events and a mapping from unlock event to the causally preceding lock event.
the only case where there are alternative co sets is when there are multiple execution paths from a lock event to different unlock events.
these can again efficiently found by maintaining the mapping described above.
.
.
possibleextensionsfromthreadconditions computing possible extensions for thread conditions that have either unlock or symbolic branching as their next operation is straightforward.
for unlocks there can be onlyone extension.
the handling of symbolic branching is identical to normal dynamic symbolic execution a branch with a symbolic constraint and its negation are added to the unfolding.
for the other cases we need to compute a set of shared variable conditions or lock conditions that are concurrent with the given thread condition such that the set forms the preset of the possible extension event together with the thread condition.
assuming that one such set of conditions is known we can use lemma to restrict the number of conditions that need to be considered in order to find the the rest of the sets.
if the next operation of the thread condition is either read or write then one such set can be found by considering the marking obtained by firing the event sequence that is currently being explored by the algorithm in fig.
up to the point of reaching the thread condition.
the shared variable conditions in this this marking are then concurrent with the thread condition and because shared variables are modeled with permanently marked places there is a marked condition for every shared variable copy in every marking.
therefore the initial set of conditions needed by lemma can be directly obtained from the computed marking.
for a thread condition tthat has lock as the next operation it is not necessarily the case that in the marking obtained as above there is a lock condition marked that is concurrent with the thread condition.
in this case there either is no suitable lock condition that is concurrent with the thread condition or one such lock condition can be found either in the preset of the last lock acquire event executed i n the event sequence currently being explored or in one of the unlock events that release the lock acquired by this event.
this is because the lock conditions in the postset of the unlock events are guaranteed to be concurrent with the thread condition t i.e.
the lock operation following tbecomes enabled when the the current lock is released .
if such unlock event exists in the unfolding we have found one suitable lock condition.
in the case that no such unlock events yet exist in the unfolding they will be added to it later when the lock in question is unlocked.
in this case the only conditio n adjacent or alternate to these future conditions is the lock condition in the preset of the last lock acquire event that has been executed and therefore by lemma it is the only possible place to find a suitable concurrent lock condition.
.
.
possible extensions from shared variable and lockconditions there can also be possible extensions from shared variable or lock conditions that are not part of the extensions computed from the thread conditions.
for this we need to find all thread conditions having a next operation that accesses the shared variable or lock condition and that are concurren t with the conditions cin the postset of the event for which we are computing the possible extensions.
for each thread condition tfound we can then use lemma to restrict the search space to find possible extensions that all have tand cas part of their presets.
in order to find the thread conditions there are two types of thread conditions that need to be considered thread conditions for which no possible extensions have yet been computed and thread conditions with known possible extensions.
by lemma we know that if a suitable thread condition has known possible extensions one of the extensions must have a co set that is alternative or adjacent to the condi 155tions in c. therefore such thread conditions can be found by searching the events connected to the adjacent or alternative co sets of c more specifically the thread conditions preceding these events and checking if they are concurrent with c. as not all possible extensions are necessarily yet added to the unfolding we need to do this search in a net that is the union of the current unfolding and the currently known possible extensions.
for thread conditions that have no known possible extensions yet a list of them need to be maintained and each thread condition in the list must be checked in turn to see if it is concurrent with the conditions in c. notice that the only thread conditions that can be in the list are thread conditions that have lock as their next operation.
for all other operation types one possible extension is always known.
therefore the size of the list that need to be checked is small in practice in most cases.
.
computingschedulesandinputvaluesto execute tests theexecute subroutine performs a single test run to generate an event sequence.
for this a schedule and input values need to be computed.
this can be done by collecting all the events that causally precede the target event.
for each event we maintain the information in which order they were added to the unfolding i.e.
the labeling of events with numbers in our examples and which thread executed the event.
this allows a schedule for a test run to be constructed simply by ordering the collected events in the orde r they were added and requiring the scheduler to schedule threads in this order.
we also store the symbolic constraint information obtained by symbolic execution to these events the path constraint describing the set of possible input val ues can be constructed and given to a constraint solver in the same way as in standard dynamic symbolic execution.
that is we take a conjunction of all the symbolic constraint s in the collected events.
if the path constrain is unsatisfiab le the target event is unreachable and it is removed from the set of possible extensions.
a satisfiable path constraint gives input values that together with the computed schedule forces a test run to reach the target event.
.
handlingdynamicnumberofsharedvariables and threads the algorithm described above assumes that all shared variables are explicitly defined in the beginning and the number of threads is fixed.
it is possible to extend the algorithm to support dynamically changing sets of shared variables.
in order to do this each such variable needs an unique identifier across all possible executions.
in the con text of java programs it is possible to obtain such identifier s by adding an event to the unfolding when a new object is created and then identifying a shared variable a field of thi s object by combination of the object creation event and the field name of the variable.
static variables can be identified by their class and field names.
notice also that the concrete execution in dynamic symbolic execution gives precis e aliasing information for the variables.
therefore it is al ways known if two