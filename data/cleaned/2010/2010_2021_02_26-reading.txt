a practical guide for using statistical tests to assess randomized algorithms in software engineering andrea arcuri simula research laboratory p .o.
box lysaker norway arcuri simula.nolionel briand simula research laboratory and university of oslo p .o.
box lysaker norway briand simula.no abstract randomized algorithms have been used to successfully addre ss many different types of software engineering problems.
this typ e of algorithms employ a degree of randomness as part of their logic .
randomized algorithms are useful for difficult problems whe re a precise solution cannot be derived in a deterministic way wi thin reasonable time.
however randomized algorithms produce d ifferent results on every run when applied to the same problem inst ance.
it is hence important to assess the effectiveness of randomi zed algorithms by collecting data from a large enough number of runs.
the use of rigorous statistical tests is then essential to provi de support to the conclusions derived by analyzing such data.
in this pa per we provide a systematic review of the use of randomized algorit hms in selected software engineering venues in .
its goal is no t to perform a complete survey but to get a representative snapshot o f current practice in software engineering research.
we show tha t randomized algorithms are used in a significant percentage of pa pers but that in most cases randomness is not properly accounte d for.
this casts doubts on the validity of most empirical results a ssessing randomized algorithms.
there are numerous statistical tests based on different assumptions and it is not always clear wh en and how to use these tests.
we hence provide practical guideline s to support empirical research on randomized algorithms in sof tware engineering.
categories and subject descriptors d. .
general i. .
problem solving control methods and search general terms algorithms experimentation reliability theory keywords statistical difference effect size parametric test non parametric test confidence interval bonferroni adjustment systema tic review survey.
permission to make digital or hard copies of all or part of thi s work for personal or classroom use is granted without fee provided th at copies are not made or distributed for profit or commercial advantage an d that copies bear this notice and the full citation on the first page.
to cop y otherwise to republish to post on servers or to redistribute to lists re quires prior specific permission and or a fee.
icse11 may waikiki honolulu hi usa copyright acm ... .
.
.
introduction many problems in software engineering can be alleviated thr ough automated support.
for example automated techniques exis t to generate test cases that satisfy some desired coverage crit eria on the system under test such as for example branch and pat h coverage .
because often these problems are undecidabl e deterministic algorithms that are able to provide optimal sol utions in reasonable time do not exist.
the use of randomized algorith ms is hence necessary to address this type of problems.
the most well known example of randomized algorithm in soft ware engineering is perhaps random testing .
techniques that use random testing are of course randomized as for exam ple dart which combines random testing with symbolic exec ution .
furthermore there is a large body of work on the appli cation ofsearch algorithms in software engineering as for example genetic algorithms.
since practically all search algorith ms are randomized and numerous software engineering problems can be a ddressed with search algorithms randomized algorithms the refore play an increasingly important role.
applications of searc h algorithms include software testing requirement enginee ring project planning and cost estimation bug fixing aut omated maintenance service oriented software engineering compiler optimisation and quality assessment .
a randomized algorithm may be strongly affected by chance.
i t may find an optimal solution in a very short time or may never converge towards an acceptable solution.
running a randomi zed algorithm twice on the same instance of a software engineeri ng problem usually produces different results.
hence resear chers in software engineering that develop novel techniques based o n randomized algorithms face the problem of how to properly evalu ate the effectiveness of these techniques.
to analyze the effectiveness of a randomized algorithm it i s important to study the probability distribution of its output or various performance metrics .
for example a practitioner migh t want to know what is the execution time of those algorithms on average .
but randomized algorithms can yield very complex and high va riance probability distributions and hence looking only at a verage values can be misleading as we will discuss in more details i n this paper.
the probability distribution of a randomized algorithm can be analyzed by running such an algorithm several times in an ind ependent way and then collecting appropriate data about its results and performance.
for example consider the case in which we w ant to find failures in software by using random testing assumin g that an automated oracle is provided .
as a way to assess its perfo rmance we can sample test cases at random until the first failu re is detected.
in the first experiment we might find a failure afte r sampling 24test cases for example .
we hence repeat this experimentpermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
icse may waikiki honolulu hi usa copyright acm ... .
1a second time if a pseudo random generator is employed we n eed to use a different seed for it and then for example trigger the first failure when executing the second random test case.
if in a th ird experiment we obtain the first failure after generating 274test cases themean value of these three experiments would be .
using such a mean to characterize the performance of random testin g on a set of programs would clearly be misleading given the exten t of its variation.
since such randomness might hinder the reliability of concl usions when performing the empirical analysis of randomized algorithms researchers hence face two problems how many ex periments should be run to obtain reliable results and h ow to assess in a rigorous way whether such results are indeed reli able.
the answer to these questions lies in the use of statistical tests .
there are many books on various aspects of statistics e.g.
and that research field is still growing .
no tice that though statistical testing is used in most if not all sci entific domains e.g.
medicine and behavioral science each field has its own set of constraints to work with.
even within a field like so ftware engineering the application context of statistical te sting can vary significantly.
when human resources and factors introd uce randomness e.g.
in the phenomena under study th e use of statistical tests is also required but the constraints we work with are quite different from those of randomized algorithms su ch as for example the size of data samples and the types of distribu tions.
because of the widely varying situations across domains and the overwhelming number of statistical tests each one with its own characteristics and assumptions many practical guidelin es have been provided targeting different scientific domains such as bi ology and medicine .
in this paper we intend to do the same for r andomized algorithms in software engineering as they entail specific properties and the application of statistical testing is fa r from easy as we will see.
to assess whether the results obtained with randomized algo rithms are properly analyzed in software engineering resea rch and therefore whether precise guidelines are required we carr ied out a small scale systematic review.
we limited our analyses to th e year as our goal was not to perform an exhaustive systematic review but to obtain a representative recent sample on which t o draw conclusions.
we focused on research venues that deal with al l the aspects of software engineering such as ieee transactions of software engineering tse ieee international conference on s oftware engineering icse and international symposium on sea rch based software engineering ssbse .
the review shows that s tatistical analyses are either missing inadequate or incom plete.
for example though journal guidelines in medicine require a ma ndatory use of standardized effect size measurements to quantify the effect of treatments we have not found a single case in wh ich this was used to measure the relative effectiveness of a rand omized algorithm.
furthermore in half of the surveyed empirical a nalyses randomized algorithms were evaluated based on the resu lts of only one run and all the empirical analyses in tse were based o n a maximum of five runs.
given our survey s results we hence found necessary to devi se practical guidelines for the use of statistical testing in assessing randomized algorithms in software engineering applicatio ns.
note that though guidelines have been provided for other scienti fic domains and for other types of empirical analyses in so ftware engineering they are not necessarily applicable i n the context of randomized algorithms.
our objective is therefore a ccount for the specific properties of randomized algorithms in soft ware engineering applications.
notice that ali et al.
have recently carried out a systematic re view of search based software testing which includes some l imited guidelines on the use of statistical testing.
this paper bui lds upon that work by analyzing software engineering as whole an d not just software testing considering all types of randomi zed algorithms and not just search algorithms and giving precis e practical and complete suggestions on many aspects that were ei ther not discussed or just briefly mentioned in .
the main contributions of this paper can be summarized as fol lows we provide a systematic review of the current state of practice of the use of statistical testing to analyze randomized algorithms in software engineering.
the review shows that randomness is not properly taken into account in the researc h literature.
we provide practical guidelines on the use of statistical te sting that are tailored to randomized algorithms in software engineering applications and the specific properties and co nstraints they entail.
the paper is organized as follows.
section presents the sys tematic review we carried out.
section presents the concept of statistical difference in the context of randomized algorithm s. section compares two kinds of statistical tests and discussed thei r implications in our context.
the problem of censored data and how i t applies to randomized algorithms is discussed in section .
how to measure effect sizes and therefore the practical impact of r andomized algorithms is presented in section .
section investi gates the question of how many times randomized algorithms should be r un.
the problems associated with multiple tests is discussed in section .
practical guidelines on how to use statistical tests in ou r context are summarized in section .
the threats to validity of our wo rk are discussed in section .
finally section concludes the p aper.
.
systematic review systematic reviews are used to gather in an unbiased and com prehensive way published research on a specific subject and analyze it .
systematic reviews are a useful tool to assess g eneral trends in published research and they are becoming increas ingly common in software engineering .
in our review we want to analyze rq1 how often randomized algorithms are used in software engineering rq2 how many runs were used to collect data and rq3 which types of stati stical analyses were used to analyze those data.
to answer rq1 we selected two of the main venues that deal with all aspects of software engineering ieee transaction s of software engineering tse and ieee international confere nce on software engineering icse .
we also considered the inte rnational symposium on search based software engineering ssbse which is a specialized venue devoted to search algorit hms.
because our goal is not to perform an exhaustive survey of exi sting works but simply to get an up to date snapshot of current pr actice regarding the application of randomized algorithms in soft ware engineering research we only considered publications.
we only retained full length research papers and as a result short papers at icse and eight at ssbse were excluded.
a total of107papers were considered 48in tse 50in icse and nine in ssbse.
these papers were manually checked to verify wheth er in their empirical analyses randomized algorithms were use d. this left a total of 16papers using randomized algorithms three in tse .
of the total four in icse of the total and all the nine papers in ssbse .2notice that we excluded papers in which it was not clear wheth er randomized algorithms were used.
for example the techniqu es described in use external sat solvers and those might be based on randomized algorithms though we cannot say for sur e. furthermore even if a paper focused on presenting a determi nistic novel technique we included it when randomized algorithms were used for comparison purposes e.g.
fuzz testing .
tab le summarizes the results of this systematic review for the final se lection of16papers.
the first thing that results clear is that randomized algorithms are widely used in software engineering rq1 we f ound them in of the articles in tse and icse.
to answer rq2 the data in table show the number of times a technique was run to collect data regarding its performance on each artifact in the case study.
most of the time data are collect ed from only one run of the randomized algorithms.
only six cases out of 16show at least 30runs.
regarding rq3 only 5out of 16articles include empirical analyses supported by some kind of statistical testing.
more spe cifically we can see t tests and u tests for when algorithms are compared and linear regressions when prediction models are bu ilt.
however no standardized effect size measures section are reported in any of these articles to quantify the relative effectiven ess of algorithms in an interpretable form.
results in table clearly show that when randomized algorithms are employed empirical analyses in software engine ering do not properly account for their random nature.
many of the n ovel proposed techniques may indeed be useful but the results in table cast serious doubts on the validity of most existing result s. notice that some of empirical analyses in table do not use st atistical tests since they do not perform any comparison of th e technique they propose with alternatives.
for example in the aw ard winning paper at icse a search algorithm i.e.
geneti c programming was used and was run 100times on each artifact in the case study .
however this algorithm was not compared aga inst simpler alternatives or even random search.
if we look more c losely at the reported results in order to assess the implications o f that lack of comparison we see that the total number of fitness evaluat ions was400 a population size of 40individuals that is evolved for generations .
this is an extremely low number for example for test data generation in branch coverage it is often the case o f using100 000fitness evaluations for each branch and we can conclude that there is very limited search taking place whi ch implies that a random search would have likely yielded similar results.
this is directly confirmed in the reported results in in which in half of the case study the average number of fitness evaluat ions per run is at most thus implying that on average appropriate patches are found in the random initialization of the first po pulation before the actual evolutionary search even starts.
this sho uld not be surprising as the search operators were tailored to the sp ecific small set of bugs of the case study which then led to an easy se arch problem.
as discussed in a search algorithm should alwa ys be compared against at least random search in order to check tha t the algorithm is not simply successful because the search probl em is easy.
since comparisons with simpler alternatives at a very mini mum random search is a necessity when one proposes a novel rando mized algorithm or addresses a new software engineering prob lem statistical testing should be part of all publications reporting such empirical studies.
in this paper we provide specific gui delines on how to use statistical tests to support comparisons among randomized algorithms.table results of systematic review.
reference venue repetitions statistical tests tse u test tse none tse none icse t test u test icse none icse none icse none ssbse linear regression ssbse none ssbse u test ssbse none ssbse linear regression ssbse none ssbse none ssbse none ssbse none .
statistical difference when a novel randomized algorithm ais developed to address a software engineering problem it is common practice to comp are it against existing techniques in particular simpler altern atives.
for simplicity let us consider just one alternative randomize d algorithm and let us call it b. for example bcan be random testing andacan be a search algorithm such as genetic algorithms or an hybrid technique that combines symbolic execution with ran dom testing e.g.
dart .
to compare aversus b we first need to decide which criteria are used in the comparisons.
many different measures m can be selected depending on the problem at hand and contextual ass umptions e.g.
source code coverage execution time.
dependi ng on our choice we may want to either minimize or maximize m for example maximize coverage and minimize execution time.
to enable statistical analysis we should run both aandba large enough number n of times in an independent way to collect information on the probability distribution of mfor each algorithm.
astatistical test should then be used to assess whether there is enough empirical evidence to claim a difference between the two algorithms e.g.
the novel technique ais better than the current state of the art b .
a null hypothesis h0is typically defined to state that there is no difference between aandb.
a statistical test is used to verify whether we should reject the null hypothesi sh0.
however what aspect of the probability distribution of mis being compared depends on the used statistical test.
for example at test compares the mean values of two distributions whereas other s tests focus on the median or proportions as discussed in section .
there are two possible types of error when performing statis tical testing i we reject the null hypothesis when it is true we are claiming that there is a difference between two algorithms w hen actually there is none and ii we accept h0when it is false there is a difference but we claim the two algorithms to be equivale nt .
the p value of a statistical test denotes the probability of a type i error.
the significant level of a test is the highest p value we accept for rejecting h0.
a typical value inherited from widespread practice in natural and social sciences is .
.
notice that the two types of error are conflicting minimizin g the probability of one of them necessarily tends to increase the probability of the other.
but traditionally there is more emphas is on not committing a type i error a practice inherited from natu ral sci 3ences where the goal is often to establish the existence of a n atural phenomenon in a conservative manner.
in our context we would only conclude that an algorithm ais better than bwhen the probability of a type i error is below .
the price to pay for a small value is that when the data sample is small the probability of a type ii error can be high .
the concept of statistical power refers to the probability of rejecting h0when it is false i.e.
the probability of claiming statistical difference when there is actually a difference .
getting back to our comparison of techniques aandb let us assume we obtain a p value equal to .
.
even if one technique seems significantly better than the other in terms of effect s ize section we would then conclude that there is no difference wh en using the traditional .05threshold.
in software engineering or in the context of decision making in general this type of reasoning can be counter productive.
the tradition of using .
discussed by cowles has been established in the early p art of the last century in the context of natural sciences and is s till applied by many across scientific fields.
it has however an inc reasing number of detractors who believe that such thresholds a re arbitrary and that researchers should simply report p values and let the reader decide in context.
when we need to make a choice between techniques aandb we would like to use the one that is more likely to outperform t he other.
whether we get a p value lower than bears little consequence from a practical standpoint as in the end we must select an alternative e.g.
we must select a testing technique to v erify the system.
however as we will show in section obtaining p va lues lower than .05should not be a problem when experimenting with randomized algorithms.
the focus of such experiments s hould rather be on whether a given technique brings any practicall y significant advantage usually measured in terms of an estimate d effect size and its confidence interval an important concept addre ssed in section .
in practice the selection of an algorithm would depend on th e pvalue of comparisons the cost difference among algorithms e.g.
in terms of inputs and the estimated effect size.
given a co ntextspecific decision model the reader using such information could then decide which technique is more likely to maximize benefi ts and minimizes risk.
in the simplest case where compared tech niques would have comparable costs we would simply select t he technique with the best performance regardless of the p val ues of comparisons even if as a result there is a non negligible pr obability that it will bring no particular advantage.
.
parametric vs. non parametric tests the two most used statistical tests are the t test and the mannwhitney u test.
these tests are used to compare two data samp les e.g.
the results of running ntimes algorithm acompared to b .
thet test is parametric whereas the u test is non parametric .
a parametric test makes assumptions on the underlining dist ribution of the data.
for example the t test assumes normality and equal variance of the two data samples.
a non parametric tes t makes no assumption on the distribution of the data.
why there is the need for two different types of statistical tests?
a si mple answer is that in general non parametric tests are less po werful than parametric ones.
when due to cost or time constraints only small data samples can be collected one would like to use the most powerful test available if its assumptions are satisfied.
there is a large body of work regarding which of the two tests should be used .
the assumptions of the t test are in generalnot met.
considering that the variance of the two data sample s is most of the time different a welch test should be used instea d of a t test.
but the problem of the normality assumption remains.
an approach would be to use a statistical test to assess wheth er the data is normal and if the test is successful then use a w elch test.
this approach increases the probability of type i erro r but is often not necessary.
in fact the central limit theorem tell s us that t test and welch test are robust even when there is strong depa rture from a normal distribution .
but in general we cannot know how many data points n we need to reach reliable results.
a rule of thumb is to have at least n for each data sample .
there are three main problems with such an approach if we need to have a large nfor handling departures from normality then it might be advisable to use a non parametric test since for a large n it might be powerful enough the rule of thumb n stems from analyses in behavioral science and to the best of our k nowledge there is no supporting evidence of its efficacy for rand omized algorithms in software engineering the central limit t heorem has its own set of assumptions which are too often ignored.
w e now discuss points and in more details by accounting f or the specific properties of the application of randomized algori thms in software engineering using software testing examples.
th is choice was motivated by the fact that half the publications in searc h based software engineering are on software testing .
random testing when used to find a test case for a specific test ing target e.g.
a test case that triggers a failure or cover s a particular branch path follows a geometric distribution.
when t here is more than one testing target e.g.
full structural coverag e it follows a coupon s collector problem distribution .
given the probability of sampling a test case that covers the desired testin g target then the expectation of random testing is and its variance is 2 2 see .
figure plots the density function of a geometric distribution with .01and a normal distribution with same and 2. in this context the density function represents the probability that for a given number of sampled tes t cases l we cover the target after sampling exactly ltest cases.
for random testing the most likely outcome is l whereas for a normal distribution it is l .
notice that the geometric distribution is discrete i.e.
it is defined only on integer values whereas a n ormal distribution is continuous.
furthermore the density func tion of the normal distribution is always positive for any value where as for the geometric distribution it is equal to 0for negative values where in this context the values are the number of sampled test cases.
therefore a testing technique can never follow a normal distribution in a strict way although it might be a reasonable approximatio n. as it is easily visible in figure the geometric distributi on has a very strong departure from normality!
comparisons of nove l techniques versus random testing and this is the practice w hen search algorithms are evaluated using t tests are hence very arguable.
furthermore in contrast to many physical and beh avioral phenomena the probability distributions of search a lgorithms are often strongly departing from normality.
a common examp le is when the search landscape of the addressed problem has trap like regions .
the central limit theorem states that the sumofnrandom variables converges to a normal distribution .
for example consider the result of throwing a dice.
there are only six possib le outcomes each one with probability .
if we consider the sum of two dice i.e.
n we have 11possible outcomes from value 2to12.
figure shows that with n in the case of dice we already obtain a distribution that resembles the normal one even though with n it is very far from normality.
in our context these random variables are the results of the nruns of the analyzed40 .
.
.
.
.
test casesprobabilityrandom testing normal distribution figure density functions of random testing and normal distribution given same mean and variance 2 2 where .
.
algorithm.
this theorem has three assumptions the nvariables should be independent and their mean and variance 2should exist i.e.
they should be different from infinite .
when us ing randomized algorithms having nindependent runs is usually trivial to achieve we just need to use different seeds for the pseudo r andom generators .
but the existence of the mean and variance requ ires more scrutiny.
as shown before those values and 2exist for random testing.
a well known paradox in statistics in whic h mean and variance do not exist is the petersburg game .
si milarly the existence of mean and variance in search algorit hms is not always guaranteed as discussed next.
if the performance of a randomized algorithm is bounded with in a predefined range then the mean and variance would always ex ist.
for example if an algorithm is run for a prefix amount of time to achieve structural coverage for software testing and th ere arek structural targets then the performance of the algorithm w ould be measured with a value between 0andk.
therefore we would have kand 2 k2 so using a t test would be valid.
the problems arise if no bound is given on how the performance is measured.
a randomized algorithm could be run until it find s an optimal solution to the addressed problem.
for example ran dom testing could be run until the first failure is triggered ass uming an automated oracle is provided .
in this case the performanc e of the algorithm would be measured in the number of test cases that a re sampled before triggering the failure and there would be no u pper limit for a run.
if we run a search algorithm on the same proble m ntimes and we have nvariablesxirepresenting the number of test cases sampled in each run before triggering the first fai lure we would estimate the mean with n summationtextn i 1xi and hence conclude that the mean exists.
as petersburg game shows thi s can be wrong because is only an estimation of which might not exist.
for most search algorithms convergence in finite time is prov en under some conditions e.g.
and hence mean and varia nce exist.
but in software engineering when new problems are ad dressed standard search algorithms with standard search o perators may not be usable.
for example when testing for object orie nted software using search algorithms complex non standard se arch operators are required.
without formal proofs it is not safe t o speak about the existence of the mean in those cases.
however the non existence of the mean is usually not a problem from a practical standpoint.
in practice there usually are up per limits to the amount of computational resources a random ized algorithm can use.
for example a search algorithm can be pre maturely stopped when reaching a time limit.
random testing co uld be stopped after 000sampled test cases for example if it has found no failure so far.
but in these cases we are actually de aling with censored data in particular right censorship and this requires proper care in terms of statistical testing and the interpretation of results as discussed in section .
even under proper conditions for using a parametric test on e aspect that is often ignored is that t test and u test are two different approaches to analyze two different properties.
let us use a random testing example in which we identify the first test case that t riggers a failure.
considering a failure rate the mean value of sampled test cases done by random testing is hence .
let us assume that a novel testing technique ayields a normal distribution of the required number of test cases to trigger a failure.
if we furt her consider the same variance as random testing and mean that is of the one of random testing which one is better?
random test ing with mean orawith mean .85 ?
assuming a large number of runs e.g.
nis equal to one million a t test would state that a is better whereas a mann whitney u test would state exactl y the opposite.
how come?
this is not an error but the two tests are m easuring different things the t test measures the difference in mean values whereas the mann whitney u test deals with their sto chastic ranking i.e.
whether observations in one data sample a re more likely to be larger than observations in the other sample.
no tice that this latter concept is technically different from detectin g difference in the median values which can be stated only if the two distributions have same shape .
in a normal distribution the median value is equal to the mean whereas in a geometric distribution the median is roughly of the mean .
on one hand half of the data points for random testing would be lower than .7 .
on the other hand for awe have half of the data points above .85 and a significant proportion between .7 and0.85 .
this explains the apparent contradiction in results.
from a practical point of view which statistical test shoul d be used?
based on the discussions in this section in contrast t o and in line with we suggest to use mann whitney u test r ather than t test and welch test.
however the full motivation will become clear only once we discuss censored data effect size a nd the choice ofnin the next sections.
in the discussion above we have assumed that both aandbare randomized.
if one of them is deterministic e.g.
b it is still important to use statistical testing.
consistent with the abo ve recommendation the one sample wilcoxon test should be used.
given mbthe performance measure of the deterministic algorithm a o nesample wilcoxon test would verify whether the performance o fa is symmetric about mb i.e.
whether by using aone is as likely to obtain a value lower than mbas otherwise.
.
censored data assume that the result of an experiment is dichotomous eith er we find a solution to solve the software engineering problem a t hand success or we do not failure .
for example in software testing if our goal is to cover a particular target e.g.
a s pecific branch we can run a randomized algorithm with a time limit l. we will stop the algorithm as soon as we find a solution otherw ise we stop it after time l. the choice of ldepends on the available computational resources.
another example is bug fixing where we find a patch within time l or we do not.
these types of experiments are dealing with right censored data and their properties are equivalent to survival failure ti me analysis51 dice valuesprobability .
.
.
.
.
sum of two dice valuesprobability .
.
.
.
.
figure density functions of the outputs of one dice and the sum of two dice.
.
letxbe the random variable representing the time a randomized algorithm takes to solve a software engineering pro blem and let us consider nexperiments in which we collect xivalues.
we are dealing with right censorship since assuming a time limit l we will not have observations xifor the cases x l .
there are several ways to deal with this problem and we will lim it our discussion to simple solutions.
one interesting special case is when we cannot say for sure wh ether we have achieved our target e.g.
generation of test cases t hat achieve code branch coverage.
even when using a time limit l in these cases we are not tackling censored data.
putting aside trivi al cases there are usually infeasible targets e.g.
unreachable co de and their number is unknown.
as a result such experiments are no t dichotomous because we cannot know whether we have covered all feasible targets.
however if in the experiments the com parisons are made reusing a case study from the literature and if we wa nt to know whether within a given time we can obtain better cover age than reported studies then such experiments can be cons idered dichotomous despite infeasible targets.
let us consider the case in which we need to compare two randomized algorithms aandbon a software engineering problem with dichotomous outcome.
let xbe the random variable representing the time atakes to find a valid solution and let ybe the same type of variable for b. let us assume that we run antimes collecting observations xi and we do the same for b. using a time limitl to evaluate which of the two algorithms is better we can consider their success rate i.e.
the number of times out of the n runs in which they find a valid solution.
to evaluate whether t here is statistical difference between the success rates of aandb a test for differences in proportions is then appropriate such as the fisher exact test .
if there is no statistically or practically significant diff erence between the two success rates from a practical standpoint th e practitioner would then be interested to know which technique yi elds a valid solution in lesstime.
this is particularly important if the success rates are high.
there can be different ways to analyz e such cases such as considering artificial censorships at differ ent times beforel.
for example we can consider censorship at l i.e.
the success rate with half the time.
note that such analysis d oes not require to run any further experiments.
another way is to apply a mann whitney u test recommended above using only th e times of successful runs for which xiandyiare lower than l. one more complex situation is when one algorithm shows a significantly higher success rate but takes more time to produc e valid solutions.
since these two variables are not necessarily co rrelated a careful decision must then be made in these situations.
.
effect size when comparing a randomized algorithm aagainst another b given a large enough number of runs n it is most of the time possible to obtain statistically significant results with a t test or u test.
indeed two different algorithms are extremely unlikely to have exactly the same probability distribution.
in other words wi th large enoughnwe can obtain statistically difference even if that difference is so small as to be of no practical value.
though it is important to assess whether an algorithm fares s tatistically better than another it is in addition crucial to assess the magnitude of the improvement.
to analyze such a property effect sizemeasures are needed .
in their systematic review of empirical analyses in software engineering kampenes et al.
found out that standardized effect sizes were reported in on ly29 of the cases.
in our review we found none.
effect sizes can be divided in two groups standardized and u nstandardized.
unstandardized effect sizes are dependent f rom the unit of measurement used in the experiments.
let us consider the difference in mean between two algorithms a b. this value has a measurement unit that of aandb.
for example in software testing can be the expected number of test executions to find the first failure.
on one testing artifact we might have a b whereas on another testing artifact we might have a b .
deciding based on 1and 2which algorithm is better is difficult to determine since the two scales of measurement are differe nt.
is very low compared to but in that case ais100times worse thanb whereas it is only twice as fast in the case .
empirical analyses of randomized algorithms if they are to be reliabl e and generalizable require the use of large numbers of artifact s e.g.
programs .
the complexity of these artifacts is likely to wi dely vary such as the number of test cases required to fulfill a cov erage criterion on various programs.
the use of standardized effe ct sizes that are independent from the evaluation criteria measurem ent unit is therefore necessary to be able to compare results across a rtifacts and experiments.
in this section we first describe which is the most known standardized effect size measure and why it should notbe used.
we then describe two other standardized effect sizes and how t o apply them in practice.
the most known effect size is the so call edd family which in the general form it is d a b .
in other words the difference in mean is scaled over the standard dev iation several corrections exists to this formula but for mo re details please see .
though we obtain a measure that has no measu rement unit the problem is that it assumes the normality of the data and strong departures can make it meaningless .
for exam ple in a normal distribution roughly of the points lie within i.e.
they are at most away from the mean .
but for distributions with high skewness as in the geometric distribu tion and as it is often the case for search algorithms the results of scaling the mean difference by the standard deviation would not be valid because standard deviations can be very sensi tive to a distribution s shape .
in this case a non parametri c effect size should be preferred.
existing guidelines in ju st briefly discuss the use of non parametric effect sizes.
the vargha and delaney s a12statistics is a non parametric effect size measure .
its use has been advocated in and one example of its use in software engineering in which rando mized algorithms are involved can be found in .
in our cont ext given a performance measure m the a12statistics measures the probability that running algorithm ayields higher mvalues than running another algorithm b. if the two algorithms are equivalent then a12 .
.
this effect size is easier to interpret compared6to thedfamily.
for example a12 .7entails we would obtain higher results of the time with a. though this type of nonparametric effect size is not common in statistical tools i t can be very easily computed .
the following formula is repo rted in a12 r1 m m n wherer1is the rank sum of the first data group we are comparing.
the rank sum is a basic component in the mann whitney u test and most statistical tools provide it.
in that formu la mis the number of observations in the first data sample whereas nis the number of observations in the second data sample.
in most exp eriments we would run two randomized algorithms the same numbe r of times m n. when dealing with dichotomous results as discussed in sect ion several types of effect size measures can be conside red.
the odds ratio is the most used and is a measure of how many times greater the odds are that a member of a certain populati on will fall into a certain category than the odds are that a memb er of another population will fall into that category .
give nathe number of times algorithm afinds an optimal solution and bfor algorithm b the odds ratio is calculated as a n a b n b where is any arbitrary positive constant e.g.
.
used to avoid problems with zero occurrences .
there is no diffe rence between the two algorithms when .
the cases in which 1implies that algorithm ahas higher chances of success.
both a12and are standardized effect size measures.
but because their calculation is based on a finite number of observa tions e.g.
nfor each algorithm so 2nwhen we compare two algorithms they are only estimates of the real a 12and .
ifnis low these estimations might be very inaccurate.
one way to d eal with this problem is to calculate confidence intervals ci for them .
a ci is a a set of values for which there is probability that the value of the effect size lies in that ran ge.
for example if we have a12 .54and a ci with range then with probability the real value a 12lies in where a12 .54is its most likely estimation .
such effect size confidence intervals lead intuitively to decisi on making as benefits which are directly related to effect size can be compared to the costs of using alternative algorithms while acc ounting for uncertainty.
to see how confidence intervals can be calcu lated please see and .
notice that a confidence interval can replace a test of statis tical difference e.g.
t test and u test .
if the null hypothesis h0lies within the confidence interval then there is no enough stati stical evidence to claim there is a statistically significant diffe rence.
in the previous example because .5is inside the ci then there is no statistical difference at the selected sign ificance level .
for a dichotomous result h0would be .
.
number of runs how many runs do we need when we analyze and compare randomized algorithms?
as many as necessary to show with high co nfidence that the obtained results are statistically signific ant and to obtain a small enough confidence interval for effect size est imates.
in many fields of science e.g.
medicine and behavioral scie nce a common rule of thumb is to use at least n observations.
in the many fields where experiments are very expensive and ti me consuming it is in general not feasible to work with high val ues forn.
several new statistical tests have been proposed and discussed to cope with the problem of lack of power and violation of assumptions e.g.
normality of data when smaller numbe rs ofobservations are available .
empirical studies of randomized algorithms do not involve h uman subjects and the number of runs i.e.
n is only limited by computational resources.
when there is access to clusters o f computers as this is the case for many research institutes and un iversities and when there is no need for expensive specialized ha rdware e.g.
in hardware in the loop testing then large numbe rs of runs can be carried out to properly analyze the behavior of random ized algorithms.
many software engineering problems are furthe rmore not highly computationally expensive as for example code c overage at the unit testing level and can therefore involve very large numbers of executions.
there are however exceptions such a s the system testing of embedded systems e.g.
where each te st case can be very expensive to run.
whenever possible in most cases it is therefore recommend ed to use a very high number of runs.
for most problems in softwar e engineering thousands of runs should not be a problem and wo uld solve most of the problems related to the power and accuracy o f statistical tests.
for example as illustrated in in ta ble even when 100runs are used the u test might be not powerful enough to confirm a statistical difference at a .05significance level even when the data seem to suggest such a difference.
most discussions in the literature about statistical tests focus on situations with small numbers of observations e.g.
as in .
however with thousands of runs one would detect statistic ally significant differences on practically any experiment see sec tion .
it is hence essential to complement such analyses with the st udy of the effect size as discussed in section .
even when having la rge numbers of runs may not be necessary for a set level e.g.
.
if differences of practical significance also show p values less than additional runs would help tighten the confidence interval s for effect size and would be of practical value.
in section we suggested to use u test instead of t test.
for very large samples such as n there would be no practical difference between them regarding power and accuracy .
the choice of a non parametric test would be driven by its effect size measure.
in section we argued that effect size measures bas ed on the mean i.e.
the dfamily were not appropriate for randomized algorithms in software engineering.
it would be pointless t o detect statistical difference of mean values with a t test if then we cannot use a reliable measure for its effect size.
in other words it is advisable to use size measures that are consistent with the dif ferences being tested by the selected statistical test.
.
multiple tests in most situations we need to compare several alternative a lgorithms.
furthermore if we are comparing different algor ithm settings e.g.
population sizes in a genetic algorithm t hen each setting technically defines a different algorithm.
this oft en leads to a large number of statistical comparisons.
it is possible to use statistical tests that deal with multiple techniques trea tments experiments at the same time e.g.
factorial anov a and eff ect size has been defined for those cases .
however in our app lication context we would like to know the performance of each algorithm compared against all other alternatives individua lly.
given a set of algorithms we would not be interested to simply dete rmine whether all of them have the same mean values.
rather given k algorithms we want to perform z k k 2pairwise tests and measure effect size in each case.
however using several statistical tests inflates the proba bility of type i error.
if we have only one comparison the probability of type i error is equal to the obtained p value.
if we have many comparisons even when all the p values are low there is usu ally7a high probability that at least in one of the comparisons the null hypothesis is true as all these probabilities somehow add up .
in other words if in all the comparisons the p values are lower than then we would normally reject all the null hypotheses.
but t he probability that at least one null hypothesis is true could b e as high as1 zforzcomparisons which converges to 1asz increases.
one way to address this problem is to use the so called bonferroni adjustment .
instead of applying each test assuming a significance level we would use an adjusted level z. for example if we want a .05probability of type i error and we have two comparisons we would need to use two statistical tests w ith a0.025 and then check whether both differences are significant i.e.
if both p values are lower than .
.
however the bonferroni adjustment has been seriously criticized in the litera ture and we largely agree with those critiques.
for example let us assume that for both those tests we obtain p values equal to .
.
if a bonferroni adjustment is used then both tests will not b e statistically significant.
a researcher could be tempted to pub lish the results of only one of them and claiming statistical signific ance because .
.
.
such a practice can therefore hinder scientific progress by reducing the number of published results .
this would be particularly true in our application context in whi ch many randomized algorithms can be compared to address the same so ftware engineering problem it would be very tempting to leave out the results of some of the poorly performing algorithms.
tho ugh we do not recommend the bonferroni adjustment it is importa nt to always report the obtained p values not just whether a diff erence is significant or not.
if for some reasons the readers want to e valuate the results using a bonferroni adjustment or any of its va riants then it is possible to do so.
for a full list of other problems r elated to the bonferroni adjustment please see .
notice th at there are other adjustment techniques that are equivalent to bonf erroni but that are less conservative .
however the statistic al significance of a single comparison would still depend on the number of performed and reported comparisons.
in section we stated that in software engineering in genera l and for randomized algorithms in particular we mostly deal with decision making problems.
for example if we must test soft ware then we must choose one alternative among kdifferent techniques.
in this case even if the p values are higher than we need to test the software anyhow and we must make a choice.
in this context bonferroni like adjustments make even less sense.
just cho osing one alternative at random because there is no statistically significant difference does not make much sense as it ignores availa ble information.
.
practical guidelines based on the above discussions we propose a set of practical guidelines for the use of statistical tests in experiments c omparing randomized algorithms.
though we expect exceptions given the current state of practice section and we believe that it is important to provide practical guidance that will be va lid in most cases and enable higher quality studies to be reported.
we recommend that practitioners follow these guidelines and j ustify any necessary deviation.
there are many statistical tools that are available.
in the f ollowing we will provide examples based on r because it is a powerful tool that is freely available and supported by many statisticians.
but any other professional tool would provide simi lar capabilities.
practical guidelines are summarized as follows.
notice tha t often for reasons of space it is not possible to report all the data ofthe statistical tests.
based on the circumstances authors need to make careful choices on what to report.
on each problem instance e.g.
program in the case study run each randomized algorithm at least n 000times.
if this is not possible explain the reasons and report the tota l amount of time it took to run the entire case study.
if for example 30runs were performed and the total execution time was just one hour then it is rather difficult to justify why a higher number of runs was not used to gain statistical power lower p values and narrow the confidence interval of effect size estimates.
for detecting statistical differences use the non parame tric mann whitney u test for interval scale results and the fis her exact test for dichotomous results i.e.
in the cases of cen sored data as discussed in section .
for the former case inryou can use the function w wilcox.test x y where xandyare the data sets with the observations of the two compared randomized algorithms.
if you are comparing a randomized algorithm against a deterministic one use w wilcox.test x mu d where dis the resulting measure of the deterministic algorithm.
when we have number of successes afor the first algorithm and bfor the second you can use f fisher.test m where mis a matrix derived in this way m matrix c a n a b n b .
a .
could be added to each cell of the matrix to handle the zero occurrence cases.
report all the obtained p values whether they are smaller than or not and not just whether differences are significant.
always report standardized effect size measures.
for dichotomous results the odds ratio and its confidence interval is automatically calculated with f fisher.test m .
for interval scale results and the a12effect size the rank sum r1used in equation can be calculated with r1 sum rank c x y .
it is also strong ly advised to report effect size confidence intervals but the s upport for a12is unfortunately limited .
this is in fact a much easier to use substitute to p values for decision making whe re potential benefits can be compared to costs while accounting for uncertainty.
to help the meta analyses of published results across studi es report means and standard deviations so that effect sizes i n thedfamily can be used .
for dichotomous experiments always report the values aandb so that other types of effect sizes can be computed .
if space permits provide full statistics for the collected data as for example mean median variance min max values skew ness median and absolute deviation.
box plots are also use ful to visualize them.
when analyzing more than two randomized algorithms use pairwise comparisons followed by pairwise statistical tes ts and effect size measures.
.
threats to validity the systematic review in section is based on only three sour ces from which only 16out of 135papers were selected.
a larger review might lead to different results although we can safely argue that tse and icse are representative of research trends in so ftware engineering.
furthermore that review is only used as a moti vation8for providing practical guidelines and its results are in l ine with other larger systematic reviews .
last papers somet imes lack precision and interpretation errors are always possib le.
as already discussed in section our practical guidelines may not be applicable to all contexts.
therefore in every speci fic context one should always carefully assess them.
for some spec ific cases other statistical procedures could be preferable e specially when only few runs are possible.
.
conclusion in this paper we report on a systematic review to evaluate how the results of randomized algorithms in software engineeri ng are analyzed.
this type of algorithms e.g.
genetic algorithm s are widely used to address many software engineering problems such as test case selection.
similar to previous systematic revi ews on related topics we conclude that the use of rigorous sta tistical methodologies are somehow lacking when investigating rand omized algorithms in software engineering.
to cope with this problem we provide practical guidelines targeting researchers in software engineering.
in contrast to other guidelines in the literature for other scientific fields e.g .
and the guidelines in this paper are tailored to the speci fic properties of randomized algorithms when applied to software engi neering problems.
the use of these guidelines is important in ord er to develop a reliable body of empirical results over time whic h enable comparisons across studies and which will converge towards generalizable results of practical importance.
otherwise as in many other aspects of software engineering unreliable results would prevent effective technology transfer and would limit the impa ct of research on practice.