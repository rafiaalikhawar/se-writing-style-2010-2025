a machine learning approach for tracing regulatory codes to product specific requirements jane cleland huang adam czauderna marek gibiec and john emenecker systems and requirements engineering center sarec depaul university chicago usa jhuang cs.depaul.edu aczauderna mgibiec jaemenecker gmail.com abstract regulatory standards designed to protect the safety security and privacy of the public govern numerous areas of software intensive systems.
project personnel must therefore demonstrate that an as built system meets all relevant regulatory codes.
current methods for demonstrating compliance rely either on after the fact audits which can lead to significant refactoring when regulations are not met or else require analysts to construct and use traceability matrices to demonstrate compliance.
manual tracing can be prohibitively time consuming however automated trace retrieval methods are not very effective due to the vocabulary mismatch es that often occur between regulatory codes and product level requirements.
this paper introduces and evaluates two machine learning methods designed to improve the quality of traces generated between regulatory codes and product level requirements .
the first approach uses manually created traceability matrices to train a trace classifier while the second approach uses web mining techniques to reconstruct the original trace query.
the techniques were evaluated against security regulations from the usa government s health insurance privacy and portability act hipaa traced against ten healthcare related requirements specifications.
results demonstrated improvements for the subset of hipaa regulations that exhibited high fan out behavior across the requirements datasets.
categories and subject descriptors d. .
requirements specifications h. .
information storage and retrieval information search and retrieval.
general terms documentation legal aspects keywords traceability requirements classification regulatory compliance.
.
introduction software systems designed to support safety security or financially critical applications must conform to an increasing number of regulatory codes.
for example financial software systems in the usa must comply with the sarbanes oxley act of sox which establishes wide ranging standards for all u.s. public company boards management and public accounting firms.
similarly all healthcare related products in the usa must comply with the health insurance portability and accountability act hipaa which requires covered entities to use administrative and technical safeguards to protect patient medical information and governs privacy related practices including the use and disclosure of patient medical information for patient care and research.
embedded software systems often have to satisfy a staggeringly large number of regulatory codes.
for example two authors of this paper were recently involved in the traceability of a large software intensive system that included over requirements and different sets of relevant regulatory codes .
traceability costs for this system alone were estimated by the industrial requirements specialist at over usd million!
current methods for demonstrating regulatory compliance rely on the standard software engineering practice of requirements traceability which refers to the ability to track a requirement from its origins back to its rationale and downstream to various work products that implement that requirement in software .
full life cycle compliance can be achieved by tracing regulatory codes to product level requirements and from there to design documents code and test cases.
for example the u.s. food and drug administration fda states that traceability analysis must be used to verify that a software design implements all of its specified software requirements that all aspects of the design are traceable to software requirements and that all code is linked to established specifications and established test procedures.
to trace regulatory codes software developers are often forced to manually pore over documentation manuals to identify relevant sections and then painstakingly trace them to the product level requirements or implemented code .
traces are typically documented in a trace matrix.
unfortunately numerous case studies have shown that organizations struggle to implement successful and cost effective traceability primarily because creating maintaining and using traces is a time consuming costly arduous and error prone activity .
these traceability problems have been partially addressed through automated tracing methods using the vector space model latent semantic indexing probabilistic networks and other similar approaches which return a set of candidate links in much the same way that google returns results in response to a user s query for information.
automated methods have generally been quite effective returning a candidate set of traces that contain of the targeted links at precision rates of .
unfortunately these methods have limited success for tracing regulatory codes due to the significant disparity in terminology that can exist between the codes and product level requirements.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
icse may cape town south africa copyright acm ... .
.
in this paper we introduce and empirically evaluate two machine learning techniques designed to improve trace results from regulatory codes to product level requirements.
both techniques resulted in improvements for the subset of hipaa regulations that were hard to trace using traditional trace retrieval methods .
the first approach uses a manually created traceability matrix to train a classifier to trace regulatory codes while the second approach utilizes a novel information retrieval method to mine terms and phrases from domain specific documents in order to replace or augment the original trace query.
both techniques are illustrated and evaluated against the technical safeguards of the usa s health insurance portability and accountability act hipaa of .
this act defines administrative physical and technical safeguards designed to reduce the unauthorized use and disclosure of private healthcare related data .
the security safeguards which include requirements for access control audit controls authentication and transmission security are depicted in figure .
the experiments described in this paper are based on the software requirements specifications srs for ten patient healthcare products.
these srs documents which are summarized in table were obtained from a variety of sources including open source products it healthcare standards requirements exemplars and feature descriptions for commercial products.
in cases where requirements were extracted from product level documentation or from online forums care was taken to retain the original language and text and to ensure that no additional terminology was added.
four members of the research team at depaul university manually constructed and validated traceability links between the hipaa regulations and requirements in the srs documents.
for illustrative purposes the traceability matrix for the automatic logout al regulation is depicted in table .
the next section of this paper describes a baseline experiment in which a standard trace retrieval method is used to generate traces between regulatory codes and the healthcare requirements.
sections and then describe the two proposed machine learning techniques and evaluate their effectiveness in comparison to the baseline.
sections to analyze these results address the generality of the approach and discuss threats to validity.
finally section discusses related work and section concludes with a discussion of future work.
.
basic trace retrieval as previously stated there are many different approaches for automatically generating traces including the vector space model probabilistic network pn models and latent semantic indexing lsi however various studies have shown only minor and inconsistent differences between results obtained from these methods.
the pn model is therefore used in our baseline experiment for pragmatic reasons as it was adopted in the tracing tool we developed for conducting industrial pilot studies .
.
the probabilistic tracing model to prepare text for tracing a pre parsing phase is conducted to remove common words known as stop words and to stem the remaining words to their root forms.
a similarity score is then computed based upon the frequency and distribution of terms in the regulations and requirements to be traced.
in the probabilistic network method the basic probability of a link between a query artifact q and a traceable requirement rj is defined as follows pr rj q pr rj ti pr q ti k i pr q the first two parts of the formula pr r j ti and pr q t i represent the dispersion of the term ti within the requirement rj and query q i.e.
a regulatory code respectively.
these are estimated by computing the normalized frequency of terms.
for example pr r j ti is calculated by considering the frequency with which tj occurs in the requirement normalized over the total number of words in the requirement.
this is represented as pr rj ti freq rj ti freq rj tk k .
similarly pr q t i is calculated by figure .
hipaa regulations to be implemented in software .
156considering the frequency at which term ti occurs in query q normalized over the total number of potential queries in which ti occurs.
in the third part of the formula pr q is computed as pr q pr q ti i using simple marginalization techniques and represents the relevance of the term ti to describe the query concept q. the resulting probability is inversely proportional to the number of requirements containing the index term reflecting the assumption that rarer index terms are more relevant than common ones in detecting potential links.
a more complete description of the probabilistic network model is provided in several related papers .
.
trace retrieval metrics experimental information retrieval results are typically evaluated using recall and precision metrics .
recall measures the fraction of relevant links that are retrieved and is computed as recall relevant links retrieved links relevant links while precision measures the fraction of retrieved links that are relevant and is computed as precision relevant links retrieved links retrieved links for experimental purposes a threshold score is established such that all links above or at the threshold are retrieved and all links below the threshold are rejected.
because it is not feasible to achieve identical recall values across every trace query it can be difficult to compare recall and precision results across experiments.
another metric known as the f measure which computes the harmonic mean of recall and precision is therefore often used for comparative purposes.
in this paper we adopt a variant of the f measure known as the f 2measure which weights recall values more highly than precision.
this weighting is appropriate in the traceability domain where it is essential to recall as many of the correct links as possible.
the version of f measure used in this paper is computed as follows f2measure precision recall precision recall table .
healthcare related datasets used in experiments .
i d description source all reqs hipaa related requirements tot ac aud al eap i pa sed ted ts ic uui care2x an open source hospital info.
system his .
care2x user manual and forum.
cchit reqs for ambulatory health info exchange ehr certification.
the certification commission for healthcare technology clearhealth open source her.
clearhealth web s ite health.com physician electronic exchange of info between clinicians.
use case documents ww.hmss.org content files ctc use case.pdf itrust open source her north carolina state university.
use case documents .
csc.ncsu.edu itrust wiki doku.php trial implementations priority info exchange.
national coord.
for health info .
technology hit .
use case documents patientos open source healthcare info.
system.
patientos web site practiceone a suite of healthcare info.
systems.
practice one website lauesen sample requirements specification for an her sample specs s requi rementssl .doc worldvista open source version of veteran administrations ehr visa manuals her application help files http worldvista.org totals table .
requirements traced to hipaa regulation for automat ic logout .
implement electronic procedures that terminate an electronic session after a predetermined time of inactivity.
dataset req id requirement clearhealth ch37f system has a timer allowing automatic logoff care2x x5 system will implement session time outs and use cookies to terminate an electronic session sl sl28 the system must time out if the user has been away from the system for some time.
itrust it1.
electronic session must terminate after a pre determined period of inactivity.
administrator must be able to specify this period world vista wv45 the system shall timeout after a period of inactivity.
world vista wv45 the system shall ask the user if the user wants to continue using the system before timing out.
patientos pos85 automatic timeout can be specified by location.
patientos pos84 automatic timeout can be specified by role.
patientos pos86 when the system timeouts the user is returned to the login page to sign in again.
cchit cchit the system upon detection of inactivity of an interactive session shall prevent further viewing and access to the system by t hat session by terminating the session or by initiating a session lock that remains in effect until the user reestablish es access using appropriate identification and authentication procedures.
the inactivity timeout shall be configurable.
157recall precision and f measure metrics are unable to differentiate between results in which relevant links are closer to the top of the ranked listing of results versus those in which relevant links are lower down in the list.
we therefore introduce one additional metric average precision which favors results that return relevant links nearer to the top of the list.
an additional benefit of this metric is that it does not depend on an arbitrary threshold value.
average precision is computed a s average precision p r relevant r n r number of relevant documents where r is the rank n is the number of retrieved documents relevant is a binary function assigned if the rank is relevant and otherwise and p r is the precision computed after truncating the list immediately below that ranked position.
these four metrics are used throughout the remainder of the paper to document and compare results from each of the experiments.
.
basic automated tracing experiment in the first experiment the probabilistic network model described in section .
was used to generate traces from the ten targeted hipaa regulations shown in figure to requirements in each of the patient healthcare systems.
trace queries were formulated directly from the text of each hipaa security regulation.
recall precision and f measure metrics were computed individually for each of the hipaa regulations by establishing threshold values that optimized the f measure.
the decision to optimize recall and precision across each regulatory type was used consistently for all experiments reported in this paper.
average precision which is not dependent upon a threshold value was also computed.
the results are reported in table .
for four of the hipaa regulations namely storage encryption emergency access procedures transmission encryption and transmission security it was possible to recall all of the relevant requirements.
for many of the hipaa regulations the precision values were unacceptably low in some cases returning precision values close to .
however even for regulations with lower levels of precision the trace performed the useful function of excluding a large number of unlikely links.
for example the transmission encryption regulation returned recall results of .
and precision of only .
but successfully excluded of the potential links.
average precision results were highest for automatic logoff emergency access and storage encryption meaning that for these hipaa regulations the trace retrieval algorithm returned results that were generally closer to the top of the list than for the other regulations.
unfortunately prior studies have demonstrated that users lose confidence in a traceability tool that returns imprecise results.
furthermore human error is introduced when human analysts are asked to evaluate a long list of candidate links .
the remainder of this paper therefore describes two techniques for improving the accuracy of traces for regulatory codes.
.
machine learning approach machine learning methods are particularly appealing for tracing regulatory codes because the upfront effort of training a classifier can be potentially recouped when those same codes are applied across future projects.
although there are many potential classification techniques we adopted an algorithm that was previously developed for classifying non functional requirements .
prior studies demonstrated that this algorithm matched or outperform ed standard classification techniques including the na ve bayes classifier standard decision tree algorithm j48 feature subset selection fss correlation based feature subset selection cfs and various combinations of the above for the specific task of classifying nfrs in the studied datasets .
.
the classification process the trace classifica tion process includes the three primary stages of preparation training and classifying.
during the preparatory stage a training set of regulatory codes product level requirements and their associated traces is constructed .
each requirement is then pre processed using the techniques described in section .
.
a probabilistic weight is assigned to each term found in the requirements with respect to each of the regulatory codes .
this weight reflects the degree to which a term represents a specific regulatory code .
for example the term timeout occurs frequently in requirements related to the hipaa rule for automat ic logoff and occurs much less frequently in other types of requirements.
it is therefore assigned a relatively strong weighting with respect to automat ic logoff .
in contrast a term such as session which is found not only in automat ic logoff requirements but also across a variety of other types of requirements is assigned a much lower indicator weighting with respect to the automat ic logoff regulation .
the formulas for weighting indicator terms are as follows.
let q be a specific regulation such as the hipaa rule for automat ic logoff.
indicator terms of type q are mined by considering the set sq of all requirements i n the training set that are related to regulation q. the cardinality of sq is defined as nq.
each term t is assigned a weight score prq t that measures how well the term identifies a requirement that matches regulation q. the weight score prq t corresponds to the probability that a particular term t identifies a requirement as being associated to regulation q based on the s tandard information retrieval assumption that terms indicating relevance for a certain targeted regulation must be present in the document to be classified.
the frequency freq rq t of occurrence of term t in requirement rq is computed for each requirement in sq.
prq t is then computed as prq t nq freq rq t rq rq sq nq t n t npq t npq the first factor n freq rq t rq rq sq represents the term frequency component that is standard in information retrieval and shows that the weight score prq t increases if term t occurs frequently in requirements related to regulation q. it is table .
results from basic trace retrieval hipaa regulation max recall precision f2measure avg precision access control ac .
.
.
.
audit control aud .
.
.
.
automatic logoff al .
.
.
.
transmission encrypt ted .
.
.
.
emergency access eap .
.
.
.
stora ge encryption sed .
.
.
.
integrity i .
.
.
.
personal authentication pa .
.
.
.
transmission security ts .
.
.
.
unique user id uui .
.
.
.
158computed as the average term frequency of term t in type q requirements rq rescaled by the documents size rq .
the remaining component of the expression in measures inverse document frequency and penalizes the weight score if the term occurs in requirements related to other regulations.
the second factor nq t n t is the percentage of q type documents in sq containing t with respect to all requirements in the training set containing t whose number is denoted by n t .
this factor decreases if the indicator term t is used broadly throughout the requirements specification.
if the term is only used in q type requirements it will evaluate to for that type.
the third factor npq t npq is the ratio between the number np q t of projects containing type q documents with term t and the number np q of all projects in the training set with type q nfrs.
the purpose of this rescaling factor with values ranging between zero and one is to decrease the weight prq t for terms that are project specific.
it is equal to one only if a term appears in all the projects containing type q documents.
a probability score prq t is computed for each term t with respect to q and terms are then ranked by decreasing order according to prq t .
results from prior experiments with the nfr detector and a series of initial experiments with the hipaa security regulations showed that selecting the top terms for each of the q types returned optimal classification results in comparison to other selection methods.
this approach was therefore adopted for all of the reported experiments.
to classify requirements a score prq r is computed that evaluates the probability that a certain requirement r is associated with the regulation q. this probability score depends on the lexical content of requirement r and is based on the assumption that type q requirements are more likely to contain indicator terms for that type.
let iq be the set of indicator terms for regulation q identified during the training phase following expression .
we assume that the weighted indicator terms in iq are identified and their weights computed from a training set that contains correctly precategorized requirements.
the indicator terms are mined using the expression in .
the classification score that a requirement r belongs to regulation q is then defined as follows prq r prq t t r iq prq t t iq where the numerator is computed as the sum of the term weights of all type q indicator terms that are contained in r and the denominator is the sum of the term weights for all type q indicator terms.
the probabilistic classifier for a given type q will assign higher score prq r to a requirement r that contains several strong indicator terms for q. .
experimental evaluation to evaluate the effectiveness of the regulatory classifier a leaveone out cross validation experimental design was used for tracing hipaa security rules against the patient healthcare systems.
the manually created trace matrices were used as both the training and testing sets for this experiment.
during each iteration of the experiment nine datasets were used to train the classifier while the remaining dataset was used for testing purposes.
as a result following ten iterations of the experiment each dataset was tested one time.
in each iteration indicator terms were extracted from the requirements in the training set and were weighted using formula .
the top ten scoring indicator terms for each regulatory code were selected and used to classify requirements in the test set using formul a .
a multiple classification scheme was used so that for any targeted hipaa regulation all requirements that scored higher than a certain threshold value were classified as retrieved for that regulation.
this meant that a single requirement could be assigned to more than one hipaa regulation.
as with the previous experiment threshold values were set individually for each query so as to maximize f 2measure values.
.
results the results are reported in the confusion matrix of table which provides a useful visual means for analyzing classification results and has the ability to depict true and false positives as well as true and false negatives.
the true positives are depicted on the diagonal and have been highlighted in the diagram.
the matrix shows for example that of the access control requirements were correctly classified but that of them were incorrectly classified as audit control and as emergency access protocol etc.
as with the previous experiment the f measure and average precision are also reported.
figure depicts the average precision scores obtained using both the basic trace retrieval method and the machine learning method.
four of the regulatory codes showed very marked improvements in average precision when using the machine learning technique.
for example the average precision for access control traces increased from .
to .
and for audit control from .
to .
.
major improvements were also observed in transmission encryption and decryption and persona l authentication .
an examination of the requirements associated with each of these successful cases indicated that they had relatively strong themes that wove across the ten datasets making the training process relatively straightforward.
furthermore three of these cases had significantly larger than average training sets.
access control had a total of requirements across the datasets audit control had and personal authentication had .
the only hipaa goal with a small training set which performed well under the machine learning approach was transmission encryption and decryption with a training set of only requirements.
for the remaining hipaa regulations the machine learning technique did not improve average precision results in compa rison to the basic trace retrieval results .
however in addition to having smaller training sets five of the se regulations namely automatic logoff emergency access protocols transmission security storage encryption and decryption and unique user id had already all returned relatively strong average precision values for the basic traceability approach ranging from .
to .
.
these observations are discussed further in section of this paper.
for the final regulatory code of integrity neither the basic trace method nor the machine learning method were very effective.
an analysis of the hipaa integrity regulation suggests that the code itself is rather broadly defined stating only that the product should implement polic ies and procedures to protect electronic health information from improper alteration or destruction.
a total of requirements from the ten datasets were traced to this integrity code however an analysis of these requirements showed that they covered a relatively broad gamut of topics such as a very high level requirement that the system must warn t he user if format consistency or validity are in doubt and another one that stated 159that the system shall... retain all original documentation.
the bread th of integrity requirements suggested that the hipaa regulation itself was perhaps at too high a level to be concretely traced to requirements.
in summary the machine learning approach did not improve traceability for the five hipaa goals that performe d best under the basic traceability approach however it did significantly improve results for four of the five hipaa goals that had not previously performed well.
the machine learning approach might therefore be used to improve traceability of regulatory codes that do not tend to trace easily into requirements documents.
however this approach is only feasible when a training set is available.
for purposes of these experiments we spent approximately person hours developing the training set for the hipaa regulations even though it involved only individual requirements.
the vast majority of this time was spent finding and processing the requirements for healthcare related products and constructing the associated trace matrices.
this effort could be minimized if the training set were built as a natural by product of developing and tracing software systems in compliance to a given set of regulatory codes.
it could also be distributed across multiple organizations if a specific user community worked collaboratively to build the training set.
furthermore one could envision regulatory bodies publishing regulations augmented with terms to support automated trace retrieval processes.
unfortunately without this type of support the sheer number and breadth of regulatory codes suggests that it will not always be possible or cost effective to construct training sets using the manual techniques described in this paper.
the following section of this paper therefore proposes a novel and alternate approach for automatically discovering indicator terms through mining domain specific documents.
.
web mining approach the proposed approach is based on the idea that when a training set is not available a relevant set of indicator terms can be learned from domain specific documents mined from the internet .
as the experiments reported in this section will demonstrate this approach generally improved traces for the same set of hipaa regulations that were improved by the machine learning approach.
the benefit of the web mining approach is that it bypasses the time consuming step of manually constructing a training set.
the approach involves three steps.
first a set of relevant domain specific documents are identified.
second the documents are analyzed to ext ract a set of domain specific terms.
finally these terms are composed into a new query which is used to execute the trace.
these steps are now described in more detail.
.
retrieving domain specific documents domain specific documents are identified for each regulatory code through extracting one or more representative terms or phrases and using them to issue a web based search query.
for the hipaa regulations the initial query was formulated using the title provided for each regulation.
in cases for which the search results were deemed unsatisfactory the user manually refined the query by adding or removing additional terms and phrases .
following an initial series of experiments we established the rule of only rejecting a retrieved document if it contained small amounts of text i.e.
less than words in a single block because short websites were found to frequently represent product advertisements containing potentially limited and biased terminology.
websites that were judged by the researcher to be table confusion matrix showing results from basic machine learning trace method .
total ac aud al ted eap sed i pa ts uui recall precision f2 meas.
avg.
prec access control ac .
.
.
.
audit control aud .
.
.
.
automatic logoff al .
.
.
.
encryption ted .
.
.
.
emerg.
access proc.
eap .
.
.
.
encryption decryp t sed .
.
.
.
integrity i .
.
.
.
personal authentication pa .
.
.
.
transmission security ts .
.
.
.
unique user id uui .
.
.
.
non hipaa total retrieved figure .
average precision scores obtained using machine learning method versus basic trace retrieval .
160entirely non relevant were also rejected.
in the future the process of document selection will be automated and standard disambiguation techniques will be used to help reject non relevant documents.
.
term mining the retrieved documents were processed using an algorithm for extracting domain concepts from natural text .
q tag a partof speech tagger was used to parse the documents and identify single nouns and two term noun phrases.
the following metrics were then computed for each of these terms and term phrases domain term frequency dtf computes term frequency information for term t across multiple documents as follows dtf t freq t d t d d where freq t d t d is the total number of occurrences of term t across the domain specific document collection d and d is the total number of documents.
domain specificity ds measures the extent to which a term or term phrase is specific to the domain doc ument as opposed to occurring frequently across a broad spectrum of topics.
for our experiments domain specificity ds t of term t was estimated by comparing the relative frequency of the term within a domain specific document versus its relative freq uency in a general corpus of documents .
it is calculated as follows ds t ln freq t d freq t d t dfreq t g freq t g t g where the first component freq t d freq t d t d is the normalized number of occurrences of term t in the domain specific document co llection d and the second component is the normalized number of occurrences of t in the general corpus of documents.
concept generality cg computes the fraction of domain specific documents in which a specific term occurs.
concept generality differentiates between terms that occur in multiple domain specific documents versus those that occur in only a few.
the concept generality of term t cg t is computed as the number of documents containing term t dt over the total number of documents cg t dt d following a series of informal exploratory experiments the following heuristics were established for evaluating the set of generated terms .
terms were automatically rejected if cg t .
or the maximum ds t value was less th an .
.
trace execution in the final stage of the algorithm the terms identified through domain concept mining are concatenated into a query which is issued using the standard traceability algorithm described in section .
.
experimental evaluatio n to evaluate the web mining technique an experiment was conducted for tracing the hipaa security regulations against requirements in the ten healthcare datasets.
initial queries for selecting domain specific documents were formulated by extracting the headings from each of the hipaa regulations.
a researcher then evaluated the top results returned by bing google and yahoo and as a result modified the search queries to produce the final queries shown in table .
ten documents were selected for each hipaa regulation.
for example the d ocuments selected for access control included a wikipedia article apache http server documentation principles surrounding access rights in the context of the seventh framework programme of the european community acce ss control standard by state of maryland department of information technology and access control standards by purdue university.
each document was analyzed to extract domain concepts using the method s described in section .
and the ten top terms were i dentified for each regulation.
for illustrative purposes the indicator terms mined for the access control hipaa regulation are shown in stemmed form in table .
several terms such as access and control also occurred in the original regulation however additional terms such as role right and privilege were discovered and several relatively non useful terms such as persons and software that occurred in the original regulation were omitted from the reconstituted query.
it is also interesting to note that several terms such as right and privilege were discovered by both the standard machine learning method and the term mining approach.
a new trace query was formulated by concatenating the identified terms and phrases.
for example the access control query was transformed from its original form shown in figure to access control role resource password secur standard right privilege authent and executed using the probabilistic formula described in section .
of this paper.
results from the experiment are reported in table and compared against the other two techniques in figure .
four hipaa regulations namely encryption emergency access procedure encryption and decryption and transmission security were traced with recall.
these were the same four that had been recalled at by the original basic algorithm.
table .
queries used to select domain documents .
hipaa regulation final query access control ac access control audit control aud audit controls automatic logoff al automatic logoff auto logout termina te electronic session emergency access eap emergency access procedure integrity i integrity data integrity personal authentication pa person entity authentication person entity verification storage encryption sed encryption decryption transmission encrypt ted encryption transmission security ts transmission security data unique user id uui unique user id unique user identification table .
indicator terms mined for access control .
term concept generality sum of normalized term frequencies average specificity access .
.
.
control .
.
.
role .
.
.
resourc .
.
.
password .
.
.
secur .
.
.
standard .
.
.
right .
.
.
privileg .
.
.
authent .
.
.
161additionally the three regulations of access control audit controls and person or entity authentication returned recall values of over .
.
although integrity still did not perform well with a recall of only .
this was a significant improvement over both of the previous methods where maximum recall was only .
for the basic approach and .
for the machine learning method.
the trace for unique user id at recall o f .
performed worse than for both the previous methods.
however this particular problem highlighted one of the risks of this non deterministic tracing approach which is its dependen ce upon finding a suitable and expressive set of documents.
in this case the selection of domain documents failed to produce the term unique which occurred quite frequently across uui requirements.
future experiments in which the process of selecting domain specific documents is fully automated and therefore can include a more extensive analysis may be able to overcome this problem more satisfactorily.
the web mining approach outperformed the basic approach for three of the same hipaa regulations that were improved by the machine learning method these were access co ntrol audit control and personal authentication .
although improvements in average precision were less than those achieved by the machine learning method the human effort was also significantly less.
the web mining approach also improve d results for integrity which the machine learning approach had been unable to do.
however the web mining approach performed worse on four of the hipaa regulations than either of the other methods.
these were transmission encryption and decryption emergency access proce dures storage encryption and unique u ser id .
there were no cases in which the web mining technique outperformed both of the other two methods.
.
analysis of the resu lts this paper has described three methods for tracing regulatory codes to product le vel requirements and has illustrated and evaluated them with respect to the hipaa security rule.
the results suggest that machine learning methods can be used to improve trace query results when sufficiently large sized training sets are available.
a pe arson correlation analysis of the number of requirements associated with each hipaa regulation see table and the improvement in average precision achieved by utilizing the machine learning method instead of the basic trace retrieval approach returned a coefficient of .
indicating a strong positive correlation between these two factors.
surprisingly a similar correlation was observed for the web mining approach.
further analysis suggests that in cases where there are a larger number of requireme nts fanning out from one regulatory code the terms in the raw regulatory code may provide insufficient query terms to adequately retrieve the diverse set of related requirements .
in this case query augmentation is essential.
our experiments show that i n these cases although the machine learning method was most effective the web mining technique also improved results but without the excessive effort required to manually construct a training set.
these results are summarized in figure .
generali ty of the approach establishing the training sets and the infrastructure to conduct these experiments was very time consuming and so it was not feasible to repeat the entire study for a different set of regulatory codes within the scope of this paper .
ne vertheless a small and informal study was conducted to evaluate the generality of the second technique in which indicator terms are mined from the web and used in the tracing process .
table .
confusion matrix showing results from query reconstitution trace method .
total ac aud al ted eap sed i pa ts uui recall precision f2 meas.
avg.
prec access control ac .
.
.
.
audit control aud .
.
.
.
automatic logoff al .
.
.
.
encryption ted .
.
.
.
emerg.
access pro c. eap .
.
.
.
encryption decrypt sed .
.
.
.
integrity i .
.
.
.
personal authentication pa .
.
.
.
transmission security ts .
.
.
.
unique user id uui .
.
.
.
non hipaa total retrieved figure .
average precision of three different traceability methods by query type .
162in the first informal experiment a single regulation was traced from the american railway engineering and maintenance of way arema codes to contractual requirements for a large systems engineering project.
the selected code stated that when used combustion burners should be provided with suitable safety controls including a combustion safety control e.g.
flame detector high temperature limit safety control and other safety controls that may be required to provide for safe operation of the device .
this particular code was selected for the experiment because it had the most individual traces in the provided answer set.
applying the search augmented trace method led to an improvement in precision from to at consistent recall levels of .
in a second informal experiment a single regulation the total volume of gas required shall a be determined as the total vol ume for all appliances supplied... was traced from the canadian natural gas and propane code cngpc to component level requirements however for this particular trace precision was quite poor with and without the search augmented technique.
the single target link which originally appeared in the 17th ranked position moved only to the 14th position with the enhancement.
however the searchenhanced method was able to learn the previously undetected phrase of gas supply i.e.
supply of gas which was found in the targeted requirement .
although not incorporated into our current tracing tool phrasing is known to improve trace results and could therefore potentially be used to improve future traces.
an analysis of the arema and cngpc codes suggests that future enhancements are feasible if the algorithm is fine tuned to support a more focused search.
whereas regulatory codes such as hipaa tend to include relatively distinct topics the cngpc and to some extent the arema codes are much less distinct and individual codes contain overlapping ideas.
future work is clearly needed to fully understand the constraints of this approach and to fine tune techniques for retrieving more focused domain relevant documents or sections of documents.
.
threats to valid ity there are three primary threats to validity in this work.
the first relates to the generality of the healthcare related datasets.
this threat was largely addressed through using different requirements specifications taken from entirely different sources.
in all cases the requirements text was extracted directly from source documents and only very common words were added during the specification process that would either be ignored as stopwords or assigned insignificant weightings by the tf idf algorithm.
the second threat relates to the fact that one of the datasets was much larger than the others which could have introduced terminology bias.
although not reported in detail in this paper an informal analysis showed that the traces worked effectively across all of the different projects.
future work will examine this in greater detail.
the third major threat to validity relates to biases that could have been introduced in selecting domain specific documents for experimenting with the search based approach to learning indicator terms.
this threat was partially addressed through defining a set of selection rules.
nevertheless these rules were based on our observations of security related documents and may be biased towards hipaa regulations or towards security related documents in general.
as a result of this issue we cannot claim generality of the document selection process for other types of regulations or for the tracing problem in general.
as explained in section this will be left primarily to future work.
.
related work as referenced in the introductory section of this paper numerous researchers have investigated the use of semi automatic and automatic approache s to dynamically generate traceability links based on information retrieval ir methods including latent semantic analysis lsa the vector space model and probabilistic approaches .
extensive empirical studies have shown that in order to recall at least of the correct links precision levels of to can be achieved in most datasets .
despite these precision problems ir methods have been shown to save significant time and effort when used in place of brute force tracing methods.
associated tools such as retro poirot and adams are currently being used on a variety of government and industrial projects.
other semi automated tracing techniques include rule based approaches scenario and test case based methods eventbased approaches and policy based methods .
however much of this related work has focused on generating traces between documentation and code or across artifacts such as requirements design code and test cases within a project .
furthermore techniques such as scenario or policybased approaches are more appropriate for tracing within a project than for tracing external documents such as regulatory codes.
with respect to automated learning of indicator terms hu et al used wikipedia to augment users web based queries while several other researchers have used more general web knowledge to augment queries however their work assumes a typical web based query of only words.
their intent is therefore to augment the query with additional and potentially disambiguating information.
in contrast regulatory codes are much longer in nature typically involving from terms and so our approach is designed to augment a standard regulatory code with alternate terms that might be found in the search space while removing non critical peripheral terms that cause precision problems.
.
conclusions and future work the techniques described in this paper offer a promising new approach for supporting the task of tracing from regulatory codes such as the hipaa security rule to contractual and product level requirements.
experimental results have shown significant improvements over existing trace retrieval methods suggesting that the reported techniques could significantly reduce the human effort and increase the accuracy of tracing regulatory codes.
we are currently engaged in an industry pilot study and plan to repeat the traces for this study using the term mining technique figure .
mean average precision for hipaa regulations categorized by number of related requirements in dataset of healthcare requirements specifications .
163described in this paper.
other areas for future work include tracing requirements to lower level refinements of the hipaa regulations .
in prior work breaux and anton identified six types of data access constraints to handle complex crossreferences resolve ambiguities and assign priorities between access rights and obligations.
as part of our future work we will therefore apply the data mining techniques described in this paper to a more fine grained model of the hipaa regulatory codes showing specific rights and obligations with respect to specific patient healthcare information.
this approach is expected to produce a more precise traceability report that highlights regulatory risks and failures with respect to specific health care information.
.