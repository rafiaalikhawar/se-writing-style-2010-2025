active code search incorporating user feedback to improve code search relevance shaowei wang david lo and lingxiao jiang school of information systems singapore management university shaoweiwang.
davidlo lxjiang smu.edu.sg abstract code search techniques return relevant code fragments given a user query.
they typically work in a passive mode given a user query a static list of code fragments sorted by the relevance scores decided by a code search technique is returned to the user.
a user will go through the sorted list of returned code fragments from top to bottom.
as the user checks each code fragment one by one he or she will naturally form an opinion about the true relevance of the code fragment.
in an active model those opinions will be taken as feedbacks to the search engine for re ning result lists.
in this work we incorporate users opinion on the results from a code search engine to re ne result lists as a user forms an opinion about one result our technique takes this opinion as feedback and leverages it to re order the results to make truly relevant results appear earlier in the list.
the renement results can also be cached to potentially improve future code search tasks.
we have built our active re nement technique on top of a state of the art code search engine portfolio.
our technique improves portfolio in terms of normalized discounted cumulative gain ndcg by more than .
from .
to .
.
categories and subject descriptors d. .
software engineering distribution maintenance and enhancement general terms algorithms experimentation keywords code search user feedback active learning .
introduction millions of open source and industrial software systems have been developed and deployed.
maintaining these systems requires constant searching through various code bases and documents and relate di erent parts of the code together.
also the development of new systems can bene t from reusable knowledge hidden in many existing systems if the developers can search through existing code and nd relevant code for reuse.
a number of studies in the literapermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
ase september vasteras sweden.
copyright acm ... .
.
have proposed various code search techniques that can return pieces of code considered to be relevant to a user s query .
one factor that potentially a ects the e ectiveness of code search is the di culty in formulating a precise query that matches the relevant code fragments needed by users.
often users could not exactly specify what they want in the form of a set of keywords or constraints but they are often able to decide whether a given code fragment satis es their needs.
given a list of code fragments returned by a code search engine some code fragments might be close to what a user wants while others might be very di erent.
as a user checks each code fragment one at a time he or she will naturally form an opinion about its relevance.
it will be bene cial to utilize user opinions on code fragments that he or she has checked to improve the results.
the key idea of this paper is to improve code search relevance by integrating users feedback.
our approach records user s opinion about each code fragment when they check through a list of results returned by a search engine and renes the list so that code fragments that are more relevant to user s need would appear earlier in the list which save user s time.
it can be built on top of any code search engine that takes textual descriptions as query inputs the rst family of code search techniques mentioned above .
the feedback required by our approach does not impose much additional e ort on users either as they would need to navigate the search results anyway to decide whether the results match their needs the only di erence is that they need to explicitly indicate their opinion about a search result when using our approach.
speci cally in this paper we build our approach on one of the state of the art code search engines portfolio to incorporate user feedback.
users of our approach rst receive a list of search results from portfolio for their query as usual.
then they can provide relevance feedback on each search result when they check through the list one by one.
the feedback is expressed as a label in a point likert scale where and indicate whether the result is completely irrelevant mostly irrelevant mostly relevant and highly relevant.
once the relevance feedback for a search result is recorded our approach incorporates this feedback to re sort the remaining search results.
this is performed based on the semantic and structural similarity between each of the remaining results and all known relevant or irrelevant results.
the goal is to have search results that are more likely to be relevant to be re arranged nearer to the top of the list.
our approach can also utilize the past feedbacks collected from previous queries to potentially improve the current and future code search tasks.
we refer to this as active code search users can more actively a ect the output of a code search engine by providing a series of relevance feedback.
in the information retrieval community rocchio is a popular way to incorporate relevance feedback by re ning a query based on a set of labeled results .
our approach is a customized reference feedback mechanism tailored for code search.
di erent from rocchio which only considers textual information we also consider structural information extracted from source code.
we have evaluated our approach with queries on a code base containing programs written in c and or c comprised of about million lines of code.
our evaluation shows that portfolio achieves an average ndcg score of .
for the queries on the code base while our active code search approach portfolioactive achieves an average ndcg score of .
.
we have also compared our approach against portfolio merged with rocchio portfoliorocchio and nd that portfoliorocchioachieves a lower average ndcg score of .
.
the structure of this paper is as follows.
we present our active code search approach in section and zoom in to the re nement engine component in section .
our empirical evaluation results are presented in section .
we discuss related studies in section .
we nally conclude and mention future work in section .
.
approach overview this section presents the overall framework of our active code search approach.
section elaborates the details about the core component of our approach the re nement engine that reorders the search results from a normal code search engine based on relevance feedback from users.
our active code search framework is shown in figure .
it consists of two major processing components shown as rectangular blocks in the gure code search engine and re nement engine.
the interaction between users and these two major components would result in a list of results in the rounded rectangular block containing potentially relevant code fragments.
as a user can provide feedback piece by piece incrementally the list of search results would be re ned in multiple iterations as illustrated by the circular arrows.
the sequence of the interactions in our active code search framework is described in the following paragraphs.
code search engine list of results query relevance feedback refinement engine update output display query figure active code search overall structure first a user posts a query to a passive code search engine.
the search engine takes the user query and returns a list of code fragments i.e.
the list of results block in figure sorted according to the scores of the code fragments calculated internally by the search engine.
the earlier a code fragment appears in the list the more similar the search engine thinks it is to the user query.
in this paper we use portfolio in the code search engine block.
second as the user navigates through the list of results one by one he or she can provide relevance feedback.
for example after the user investigates the result at the top of the list the user forms a judgement if the result is relevant.
the judgement can be expressed as a label for the result in a point likert scale where and indicate whether the result is completely irrelevant mostly irrelevant mostly relevant and highly relevant respectively.
each judgement for the results investigated by the user is used as input to the re nement engine block one at a time.
third as the re nement engine receives a piece of relevance feedback it re nes the ordering of the results in the list that have not been investigated by the user aiming to improve the overall relevance of the re ned list.
for example an uninvestigated code fragment that is originally at the 5th position could be shifted to the 48th position as it is similar to some investigated code fragments that are given low relevance scores on the contrary an uninvestigated code fragment at the 48th position could be shifted to the 4th position as it is similar to some investigated code fragments receiving high relevance scores.
the process that involves re ning a list of results displaying a re ned list and providing relevance feedback repeats until the user decides to stop or when the list is exhausted.
when a user stops it could correspond to cases where the user has found what he or she wants or where the user decides to accept something marginally relevant and use the returned code from there or where the user gives up and decides to complete his or her task without reference code etc.
in the re nement engine the feedbacks of previous queries and their corresponding re ned results are cached.
when a new query is posted if a similar query is identi ed in the cache the re ned results of the similar query are returned.
we describe how we identify the relevant re ned results in section .
our active code search approach does not need to query the passive code search engine any more during the re nement and thus does not need changes to the normal code search engine and can be easily integrated with any engine that takes user queries in the form of textual descriptions.
.
refinement engine this section describes the core component of our approach in detail.
figure illustrates the structure of our re nement engine.
the engine takes in the original user query a list of results the relevance feedback from users expressed in 4point likert scores and outputs a re ned list of results that shows potentially more relevant results nearer to the top.
our re nement engine has several data blocks in rounded rectangles results labeled results unlabeled results re ned query representation and reordered results .
results are the list of results displayed to the user so far.
labeled results are results that have received relevance feedback from the user.
unlabeled results are those that have not received relevance feedback from the user.
re ned query representation is used internally in our re nement engine to represent the combined e ect of the original query and the results that have been investigated and labeled by the user.
reordered results are the re ned list outputted by 678res ults labeled results unlabeled results update query rep resen tation refined query rep resen reorder results reordered results relevance feedback parameter tuning update original query feedback list of results cache processor figure re nement engine component the re nement engine after taking user feedback into consideration and will be presented to the user for additional feedback.
our re nement engine has also several processing blocks in rectangles update query representation parameter tuning reorder results and cache processor .
these processing blocks work together to re ne the list of results for the original user query based on the relevance feedback.
the update query representation block produces the rened query representation from labeled results and the original query.
this re ned query representation is used by the reorder results block to produce the re ned results.
the update query representation block accepts parameters that are to be tuned periodically as more relevance feedback is received.
this is done by the parameter tuning block.
the cache processor block stores the original queries that the re nement engine have processed before and their corresponding labeled results re ned query representations and reordered results.
this block also checks if a new input query closely matches a past query if it does it will bootstrap the re nement engine with the cached data.
if the same query is processed by the re nement engine the cached data will be updated.
we elaborate these processing blocks in sections .
.
.
and .
.
.
update query representation in this block we incorporate information from search results that have received relevance feedback i.e.
labeled results into the original query.
we convert the original query into its representative vectors.
we also convert search results that have received relevance feedback into their representative vectors.
we then update the representative vectors of the original query to a re ned query representation.
to elaborate update query representation we present some de nitions rst and then the algorithm.
.
.
query and result representations we rst introduce the representations for a query and a result that can be used to transform a textual query or a code fragment into vectors of numerical scores.
we consider two representations semantic and structural.definition semantic representation .
in the semantic representation a query and a code fragment are viewed as a bag of words.
we use standard tokenization stop word removal identi er splitting and stemming to convert a query or a code fragment into a bag of words .
the semantic score of a word is given by the product of its term frequency andinverse document frequency tf idf .
the term frequency tf of a word is the number of times the word appears in the query or code fragment normalized by the total number of words in the query or code fragment.
the inverse document frequency idf of a word is the logarithm of the total number of documents i.e the number of methods in the code base in our code search setting divided by the number of documents that contain the word.
given a query or code fragment q we denote the vector of semantic scores representation of qasvscore q sem.
example.
let a query qbelock unlock le then the term frequency of words lock unlock and leare .
.
.
respectively.
also let the words lock unlock and leappear in and methods and there are totally methods in the code base so the inverse document frequencies for the words calculated by the equation idf w d logjdj jfd2d w2dgjare .
.
and .
respectively where wis a word in the query and dis a method in the code base d. finally the semantic scores are .
.
and .
for lock unlock and le respectively.
definition structural representation .
in the structural representation a query and a code fragment are viewed as a bag of function calls.
the structural score of a function is given by the product of its term frequency and inverse document frequency.
given a query or a code fragment rin the search results the term frequency of a function in ris the frequency of the function being called in rnormalized by the total number of function calls in r the inverse document frequency of a function is the logarithm of the total number of code fragments in the search results divided by the number of code fragments in the search results that call the function.
we denote the vector of structural scores of rasvscore r str.
note that the original query entered by the user does not contain method calls thus it is represented by a vector of zeroes.
however as we incorporate results that have received relevance feedback from users to the original query the query s vector of structural scores would be updated.
example.
consider a code fragment rwhich is a function sclockedfile .
there is only one function unlock called in sclockedfile .
so the term frequency of unlock is .
there are totally search results returned and the function unlock is called by code fragments in the results so its inverse document frequency value is log50 .
.
finally the vscore r strforunlock is .
.
.
.
vector operations we also de ne operations that are applied to vectors and help to make it easier to describe the algorithm in the following subsections of this paper.
definition vector summation and division .
letv be the score in a vector vcorresponding to a word or a functioni.
in the case that the word or the function idoes not have a corresponding entry in v letv returns .
let us 679also denote ws v to be the set of words and functions that have corresponding entries in vector v. given two vectors v1andv2 the summation of these two vectors would result in a new vector vr and8i vr v1 v2 .
consider a vectorvand a constant c the division of vector vbycwould result in a new vector vr and8i vr v c. we also need to de ne the similarity metric among vectors so that we can gauge the re nement of queries and search results.
in this paper we measure vector similarity by using the well known cosine similarity .
.
.
algorithm the procedure for update query representation is shown in algorithm .
the procedure takes in a set of search results labeled so far lbl the new feedback fback the set of unlabeled results ulbl the original user query origquery and a set of weights that determine the contributions of labeled results with the likert scores and respectively.
weight 4are set to be .
.
.
and .
initially.
the procedure updates the set of labeled results lbl and unlabeled results ulbl and creates a re ned query representation refquery which consists of two vectors refquery semandrefquery str which are the semantic and structural representations.
algorithm update query algorithm procedure updatequery input lbl labeled search results fback new feedback i.e.
a new likert score to an unlabeled search result ulbl unlabeled search results origquery original user query the weights of contributions of labeled results with the likert score and respectively output updatedlbl ulbl and a re ned query representationrefquery method addfback intolbl and remove the result labeled by fback fromulbl letlbl1 lbl2 lbl3 andlbl4be the sets of results in lbl with likert scores and respectively compute the semantic centers of lbl1 lbl2 lbl3 and lbl4and denote them as c1sem c2sem c3sem andc4sem13 compute the structural centers of lbl1 lbl2 lbl3 and lbl4and denote them as c1str c2str c3str andc4str14 letrefquery sem vscore sem origquery p i i cisem letrefquery str vscore str origquery p i i cistr returnlbl ulbl and refquery sem refquery str to transform the inputs to the outputs the procedure works in the following steps.
we split the results in lbl into four sets based on the relevance scores line .
after this step we compute the semantic center of each set line by the following equation cisem p r2lbl ivscore sem r jlbl ij similarly we compute the structural center of each set line by the following equation cistr p r2lbl ivscore str r jlbl ij the semantic or structural center of each set is a vector that is the summation of the semantic or structural vectors of all results appearing in the set normalized by the size of the set i.e.
the number of results in the set .
we then compute the semantic re ned query representation refquery sem by combining the centers of the corresponding four sets cisem with the semantic score vector of the original query origquery sem line .
the structural rened query representation refquery str is computed in a similar way line .
this re ned query representation refquery sem refquery str is then used to help reorder the search results in section .
.
.
reorder results block in this block we sort the unlabeled results based on their similarity with the re ned query representation.
the pseudocode is shown in algorithm .
it takes in the re ned query representation refquery generated by algorithm and a list of unlabeled search results ulbl.
it outputs a reordered ulbl by the following steps.
first it iterates through the list of unlabeled results to compute the structural and semantic similarity scores between the score vectors of each result and those of refquery i.e.
refquery sem andrefquery str lines .
it then takes the average of these two scores to compute the overall similarity score line .
second it reorders the unlabeled results according to their overall similarity scores in the descending order line .
algorithm reorder result algorithm procedure reorderresults input refquery re ned query representation ulbl unlabeled results output reorderedulbl method for allrinulbl do letsimsem cos vscore r sem refquery sem letsimstr cos vscore r str refquery str letsimoverall simsem sim str end for sort all results in ulbl in the descending order according to their overall similarities to refquery as computed at line returnulbl .
parameter tuning our re nement engine takes in weight parameters 3and .
these parameters are initially set to take the following values .
.
.
and .
respectively1.
we re tune these weight parameters whenever we receive a new relevance feedback rating a result with likert score or i.e.
the result is marginally or highly relevant .
we tune the algorithm by trying many possible parameter settings one at a time.
for each parameter setting we consider its e ectiveness on the set of labeled data known so far.
we pick the parameter setting which is the most e ective i.e.
it achieves the highest ndcg on the set of labeled data known so far .
the pseudocode of our tuning process is shown in algorithm .
let us de ne a notation ap a b s to represent an arithmetic progression ap between aandbwith a steps.
we initialize two ranges 2and 4to be ap andap respectively line .
the rst is the range of possible parameter values for 1these values are determined empirically.
680and .
the second is the range of possible parameter values for 3and .
then we try to adjust the parameters by trying di erent combinations of values picked from the sets 2and line .
after a combination of parameters are picked we evaluate its e ectiveness on the set of labeled results known so far lbl line .
we simply reorder lbl using the parameter combination setting and compute ndcg.
the larger the resultant ndcg score is we assume the better a parameter combination is.
we repeat the above two steps i.e.
line and until we nd a local optimum line .
we detect a locally optimal setting for 4by trying all combinations of values picked from the sets 2and 4in order and looking for a combination whose resultant ndcg score is or no smaller than the ndcg scores of its neighboring con gurations in the combinations of parameter values in the arithmetic progressions.
we nally output the local optimal setting of the parameters line .
algorithm parameter tuning procedure procedure parametertuning input lbl labeled results output parameters method initialize two sets 2and 4to beap and ap repectively repeat adjust the value of the parameters evaluate e ectiveness of the adjusted parameters on lbl until a local optimum is reached output the local optimum .
cache processor once a user posts a new query the cache processor block checks whether there exists a highly similar query in the cache.
if a highly similar query is identi ed its corresponding cached results will be used to bootstrap the renement engine.
to identify whether a highly similar query exists this block computes the similarity of the new query with each of the old queries.
the similarity of a new query qnewand an old query qoldis computed by taking the cosine similarity of their corresponding semantic vectors similarity sem cos vscore q new sem vscore q old sem we rank the old queries based on their similarity scores.
the top one old query whose similarity is larger than a thresholdtis identi ed as the highly similar query ties are randomly broken .
if no old query has similarity above t then no highly similar query is identi ed.
in this study by default we set tto which means only the exactly same query will be identi ed.
.
experiments experimental settings the code base we use in this work is from freebsd2.
we download program versions from this code base but use only the latest version for programs that have multiple versions.
finally we use programs written in c and or c in our study.
the total size of these programs is around 36gb and they contain 2ftp ftp.freebsd.org pub freebsd distfiles .
.
.
.
improvement .
.
.
improvement query idfigure improvement of portfolioactiveover portfoliooriginalin terms of ndcg about million lines of code million functions and million les.
we have used queries created by portfolio s author .
all these queries are formulated as set of keywords to address some programming tasks reported in portfolio s user study.
we involve users for evaluation and use a simple web application written in php to display search results and collect user feedback.
in the user study we have participants of them are phd students who have at least of two years of java and c programming experience and the other is a professional software engineer who has three years of java and c programming experience.
each participant is assigned a number of queries3and asked to examine the top fty search results for each query and provide a relevance score in a point likert scale for every result.
evaluation results we build our approach on top of portfolio and thus we need to compare the two approaches together.
we compare the ndcg of portfolioactivewith that of the original portfolio portfoliooriginal .
our evaluation shows that the original portfolio on average achieves an ndcg score of .
while portfolioactiveon average achieves an ndcg score of .
a .
improvement.
figure presents a detailed comparison of the ndcg score for every query.
portfolioactivewins in cases while marginally performing worse for other cases query number and .
we also perform a wilcoxon signed rank test on the ndcgs and nd that the improvement achieved by portfolioactiveis statistically signi cant p value .
.
in the default setting for each code search task users give a feedback to each of the rst results and after each feedback we apply our re nement engine.
we would like to test the e ectiveness of our active code search approach with di erent numbers of feedbacks from users denoted as kf .
in this experiment users only give feedback to the rst kf results.
we vary kfin the setf1 g. table compares the e ectiveness of portfoliooriginalwith our portfolioactivewith di erent number of feedbacks kf to re ne the results.
when only one feedback is given by user for re ning the results portfolioactiveachieves a .
improvement over portfoliooriginal.
as thekfvalue increases portfolioactiveachieves more and more improvement until kfreaches .
the e ectiveness of portfolioactiveremains constant when kfis increased from to .
we also want to compare the e ectiveness of our approach against portfolio with rocchio portfoliorocchio .
di erent from our proposed approach standard rocchio does not consider structural scores and use a static set of weights to incorporate labeled results to re ne a query.
in our experi3each query is assigned to only one participant.
681table comparison of portfoliooriginaland portfolioactivein terms of ndcg for di erent kf kfportfoliooriginalportfolioactiveimprovement .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
ments we set the weights of rocchio to their recommended values a b andc where a b andcare the weights of the original query results labeled as relevant and results labeled as irrelevant respectively.
we compare portfolioactivewith portfoliorocchioin terms of ndcg.
portfoliorocchioachieves an average ndcg score of .
.
porfolioactiveachieves an average ndcg score of .
which is a .
improvement over portfoliorocchio s result.
we also perform a wilcoxon signed rank test and nd that the improvement achieved by portfolioactiveis statistically signi cant p value .
threats to validity threats to internal validity include experimenter bias.
there might be subjectivity in the relevance scores that a participant assigns to returned code fragments.
threats to external validity relate to the generalizability of our ndings.
we have only investigated queries and considered a code base consisting of projects and million lines of code.
we have also only considered the case where the value of threshold tof the cache processor block is set to .
threats to construct validity refer to the suitability of our evaluation metrics.
we use ndcg a common metric used to investigate the quality of web search engines .
.
related work there are many code search approaches proposed to help users nd relevant code.
some of them take textual information as input to search code.
mcmillan et al.
propose portfolio that takes natural language descriptions as input and outputs a list of functions or code fragments along with corresponding call graphs .
chan et al.
propose an approach to help developers nd usages of api methods given simple text phrases .
they use an e cient graph search algorithm to return an optimum connected subgraph that matches user s query.
haiduc et al.
propose a code search tool named refoqus that is able to predict the quality of a textual query for further query reformulation .
di erent from haiduc et al.
s work we integrate the structural information of source code itself with text information to re ne code search results.
there are also some code search engines that allow users to query by specifying structural constraints .
gay et al.
use the rocchio algorithm to incorporate user feedback to improve the performance of concern localization.
they are interested in nding methods that need to be xed given a defect report.
in their approach each method and defect report is treated as a simple textual document.
di erent from their work we address a di erent problem we recommend code fragments from a short textual query and propose a new and specialized query re nement algorithm instead of using rocchio which incorporates structural information and employs parameter tuning.several existing studies in software engineering also employ active learning.
lucia et al.
propose an approach that adds an active learning layer on top of existing clone based bug detection tools to increase true positive rate .
hayes et al.
use the rocchio algorithm to improve the quality of requirement tracing techniques which infer links between two textual documents e.g.
high level to low level requirements .
.
conclusion and future work in this paper we propose active code search where a user can provide feedback to code search engines and guide the engines to improve the relevance of search results.
we propose a re nement engine that can take into consideration user relevance feedback based on a set of results whose relevance feedback have been received our engine enhances the user original query and use it to update the search results by reordering the potentially more relevant search results to the top of the list for users to see.
the re nement process can be repeated for a number of times until the list is exhausted or the user decides to stop searching further down the list.
our active code search technique imposes little additional overhead on users and can improve the relevance of search results from any passive code search engine that takes textual descriptions as user queries.
we have evaluated our approach on queries and nd that our active code search approach on average improves the e ectiveness of code search by .
in terms of ndcg.
we have also compared our approach against rocchio and nds that we can improve it by .
.
in this work we only apply our approach to portfolio.
we plan to apply our approach to other passive code search tools and show that the active code search paradigm can bene t those code search engines too.
.