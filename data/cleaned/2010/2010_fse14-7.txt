on the efficiency of automated testing marcel b hme saarland university germany boehme cs.uni saarland.desoumya paul national university of singapore pauls comp.nus.edu.sg abstract the aim of automated program testing is to gain con dence about a program s correctness by sampling its input space.
the sampling process can be either systematic or random.
for every systematic testing technique the sampling is informed by the analysis of some program artefacts like the speci cation the source code e.g.
to achieve coverage or even faulty versions of the program e.g.
mutation testing .
this analysis incurs some cost.
in contrast random testing is unsystematic and does not sustain any analysis cost.
in this paper we investigate the theoretical e ciency of systematic versus random testing.
first we mathematically model the most e ective systematic testing technique s0in which every sampled test input strictly increases the degree of con dence and is subject to the analysis cost c. note that thee ciency ofs0depends on c. speci cally if we increase c we also increase the time it takes s0to establish the same degree of con dence.
so there exists a maximum analysis cost beyond which ris generally more e cient than s0.
given that we require the con dence that the program works correctly for x of its input we prove an upper bound oncofs0 beyond whichris more e cient on the average.
we also show that this bound depends asymptotically only onx.
for instance let rtake 10mstime to sample one test input to establish that the program works correctly for of its input s0must take less than msto sample one test input.
otherwise ris expected to establish the degree of con dence earlier.
we prove similar bounds on the cost if the software tester is interested in revealing as many errors as possible in a given time span.
categories and subject descriptors d. .
software engineering testing and debugging general terms theory keywords partition testing random testing error based partitioning e cient testing testing theory the rst author conducted this work during his phd at the national university of singapore permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
fse november hong kong china copyright acm ... .
.
.
introduction we can never be sure!
complex software errors exist even in critical widely distributed programs for many years .
so developers are looking for an e cient and automated technique to gain con dence in their programs correctness.
inspiring con dence is the main goal of software testing .
by analyzing the program s speci cation tools can automatically generate test inputs that cover corner cases .
by analyzing the program s source code tools can generate inputs that stress potentially faulty statements branches or paths by increasing the coverage of the code .
by generating and analyzing deliberately faulty versions tools can generate even more e ective test input.
generally themore comprehensive such analysis the more e ective can the testing technique be.
but with increasing analysis time what about the associated reduction of e ciency ?
we model the testing problem as an exploration of errorbased input partitions .
suppose for a program there exists a partitioning of its input space into homogeneous subdomains .
for each subdomain either all inputs reveal an error or none of the inputs reveal an error.
the number and size of such error based partitions can be arbitrary.
assuming that it is unknown a priori1whether or not a partition reveals an error the problem of software testing is to sample each partition in a systematic fashion to gain con dence in the correctness of the program.
weyuker and jeng observe that a testing technique that samples from error based partitions is most e ective .
however realistic techniques can only approximate the errorbased partitions depending on the extent of the analysis .
for instance branch coverage requires that at least one input is sampled from each branch based subdomain where a subdomain may cover many error based partitions.
so some error based partitions may not be sampled at all.
we model the most e ective systematic technique s0that samples exactly one input from each error based partition and investigate its e ciency depending on the analysis cost.
every sampled input becomes a witness of the error revealing property of the sampled partition and strictly increases the established degree of con dence.
for each sampling we assign a constant analysis cost and observe with an increased cost it takes more time to establish the same degree of condence and discover the same number of errors.
in other words e ciency decreases when the analysis cost increases .
we ask for which analysis cost does systematic testing s0 become less e cient than unsystematic random testing r?
1if it was known whether or not a partition reveals an error there would be no need for testing.in this paper we study the maximum analysis cost cfor the systematic testing technique s0to remain more e cient than random testing r. not sustaining any analysis cost we say thatrtakes one unit of time to sample one test input.
thus we can give the analysis cost cofs0as a factor of the time it takesrto sample one test input.
we say that s0 takes cunits of time to sample one test input.
note that giving cost as a factor allows us to account for the time spent on the concrete sampling related tasks that are common to both techniques s0andr.
for instance if rtakes on average 5msto generate execute and check against an oracle the outcome of a test input then by de nition s0takes c ms which includes the same time spent on test generation execution and oracle checking and the time spent on analysis .
now with increasing analysis cost s0becomes less e cient whilerremains just as e cient.
so in order for s0to maintain its e ciency over r the analysis cost ccannot exceed a certain value and is thus bounded above!
we explore two notions of testing e ciency that may be considered as the main goals of automated software testing i to achieve a given degree of con dence in minimal time and ii to expose a maximal number of errors in a given time.
furthermore we take the analysis cost cas a constant for all programs.
however the analysis cost is likely to depend on the program size and if analysis cost is bounded above then program size is as well.
that is for such systematic testing techniques there exists a maximum program size beyond whichris generally more e cient .
the following are the three most important contributions of the paper.
analytical framework .
we provide a mathematical system to assess the e ciency of any automated testing techniquesrelative to that of random testing r. it accounts for the cost cofsdepending on which there exists a unique point in time where sandr break even towards reaching the testing goal.
so the relative e ciency of sis always bounded above and for a concrete instance can be computed similarly as discussed fors0in this paper where s0generates one test input for each error based partition that is chosen uniformly at random.
testing to achieve con dence .
given a degree of con dence x we show that the time it takes s0to sample an input cannot exceed ex ex2 1times the time it takes forrto sample an input.
otherwise r is more e cient than s0on the average.
for instance letrtake 10msto sample one test input randomly to establish the con dence that any program works correctly for of its input s0must take less than 41msto sample one test input systematically.
testing to discover errors .
given a time bound n we show that the time taken by s0to sample an input cannot exceed n k qmin n 1times the time taken byrto sample an input in order for s0to remain more e cient than r wherekis the number of partitions and qminthe fractional size of the smallest error revealing partition in the program s input space.
these are fundamental insights that hold for all programs and every systematic testing technique under the realistic assumptions stated in the following section.
.
preliminaries .
background in this work we focus on automated testing techniques that seek to establish a certain degree of con dence in the correctness of the program or reveal a maximal number of errors.
interestingly this eliminates inexhaustive automated techniques that seek to generate just one failing test input as evidence of the incorrectness of the program.
first the search for a failing test input may never terminate due to the undecidability of the infeasible path problem .
secondly the absence of a failing test input throughout the search does not inspire any degree of con dence in the absence of errors.
instead we shall focus on partition testing techniques such as coverage mutation and speci cation based testing.
partition testing comprises of testing techniques that divide the program s input domain into classes whose points share the same property in some respect and then test the program for at least one input from each class.
thus the problem of systematic testing is reduced to nding a good partition strategy.
for example a spec ciationbased partition strategy might divide the input domain into subdomains each of which invokes one of several program features or satis es the pre condition of some predicate .
mutation based partition strategies may yield subdomains each of which strongly kills a certain mutant of the program .
a di erential partition strategy yields subdomains each of which either homogeneously exposes a semantic difference or homogeneously shows semantic equivalence .
symbolic execution is a path based partition strategy .
one may also consider strategies that partition the input space such that classes of input do and others do not violate an assertion in the program.
however questioning its e ectiveness hamlet and taylor nd that partition testing does not inspire con dence .
varying several parameters the authors repeated the experiments of duran and ntafos who presented a surprising result the number of errors found by random and partition testing is very similar.
hamlet and taylor came to much the same conclusion.
the results universally favoured partition testing but not by much.
weyuker and jeng found that the e ectiveness of partition testing varies depending on the fault rate for each subdomain that is systematically sampled and concluded that a partitioning strategy that yields errorbased revealing subdomains is the most e ective.
subsequently several authors discussed conditions under which partition testing is generally more e ective than random testing e.g.
.
only recently arcuri et al.
pointed out that random testing is more e ective and predictable than commonly thought and that analytical and empirical analyses have not shown so far a clear inferiority of random testing compared with other more sophisticated techniques .
the authors provide a non trivial optimal lower bound on the number of test inputs that need to be generated to cover a given set of targets.
arcuri et al.
study the scalability of random testing.
in this work scalability refers to the ability of exercising many targets in the program as the number of targets increases.
speci cally the authors show that random testing scales better than a directed testing technique that focuses on one target until it is covered before proceeding to the next.
intuitively parallel search here random testing scales better than sequential search here directed systematic testing .we are the rst to introduce a theory of testing e ciency assuming the goal is to achieve a certain degree of condence in minimal time or to expose a maximal number of errors in a certain time.
thereby we assume error based partitioning and model a systematic testing technique s0 that samples exactly one test input from each error based partition.
hence s0is among the most e ective and disregarding the analysis cost one of the most e cient testing techniques.
note that realistic techniques with a similar partition sampling scheme are both less e ective and less e cient since some error based partitions are sampled several times and others not at all due to the approximation.
leaving the scope of our analysis are several practical concerns that are common to all automated testing techniques.
i firstly there is the oracle problem which states that a mechanism deciding for every input whether the program computes the correct output is pragmatically unattainable and only approximate.
partial solutions include the automated encoding of common and the manual encoding of custom error conditions as assertions .
ii secondly there is the typicality problem which states that automatically generated test cases may not represent the typical input a user would provide or valid input that satis es some pre condition for the program to execute normally.
technically both techniques could sample according to the operational distribution or using symbolic grammars .
then both techniques receive the same ability to sample typical valid inputs.
we make no such assumptions.
iii finally we want to stress explicitly that for the purpose of this paper the achieved code coverage is only secondary .
for instance suppose a branch somewhere in the program is exercised only if for some variable iwe have i .
then this branch may or may not have a very low probability to be exercised randomly.
instead the technique shall achieve con dence and expose errors.
in our investigations we also account for partitions that are relatively small possibly containing only one input.
.
definitions and notations given any program p the number of input variables to the program determine the dimensionality of the program s input space.
the values for an input variable determines the values of the corresponding dimension in the program s input space.
for instance a program with two input variables of type integer has a two dimensional input space that can take any integer values.
regarding the input space we make the following assumptions bounded dimensionality .
given any program p the space of inputs to phas a bounded dimension.
this assumption is realistic since the length of pis bounded it can only manipulate a bounded number of variables.
bounded input space .
given any program p every input variablepcan take only a bounded number of values from a nite domain.
this assumption is also realistic since in practice the size of the registers where the variables are stored is bounded.
given these assumptions we see that given a program p its input space can be taken to be a nite measurable metric spaced qd i 1aiwheredis the dimension of the input space ofpandaiis a nite set for every i d. in what follows we x a program pwhich in turn xes the dimensiondand the input space d.de nition error based partitioning the input spacedof a programpcan be partitioned into kdisjoint non empty subdomains diwhere i kwith the following property either every input t2direveals an error or every input t2didoes not reveal an error.
if every input of a partition direveals an error then we calldianerror revealing partition.
we notice that def.
requires determinism all executions of the same test input yield the same output.
this is satis ed also if a model that renders an execution deterministic like a speci c thread schedule is constituent of the test input.
sincedis nite kwill be nite too.
note that jdij for all i kwherej jdenotes the size cardinality of a set and jdj kx i 1jdij if we draw an input tuniformly at random from d for every partitiondithere is a probability that t2di.
we denote this probability by pi.
note that pi jdij jdj for alliand kx i 1pi if all partitions are of equal size jd1j jdkj then pi kfor all i k. for everyi i k let ibe the indicator random variable which is if partition direveals an error and otherwise.
the failure rate of programp is given as kx i 1pi i atesting technique samples the input space of the programunder test and discovers error based partitions.
we assume that the information whether a partition does or does not reveal an error is unknown a priori.
this is a fair assumption because otherwise there was no need for testing.
hence each sampled test case becomes a witness of whether or not the corresponding partition is error revealing.
de nition discovered partitions given a testing technique fthat samples the input space d we say thatfdiscovers partitiondiin iteration j if no test case has been sampled from diin any previous iterationj0 j. while the goal of software veri cation is to show the correctness of the program for allinputs the goal of software testing is to show the correctness of the program at least forsomex of the input.
arguably this more modest goal may also be more practical and economical.
de nition achieving con dence for a testing technique fthat samples the input space d and injiterations discovers partitions d fd1 dmg we say thatfachieves the degree of con dence xinj iterations if the following holds pm i 1jdij jdj xnow we de ne two particular testing techniques random testingrand the systematic testing technique s0.
for each technique we assign a sampling cost that corresponds to the time that is required for sampling a test input.
the sampling of a test input comprises of concrete tasks such as generating and executing the corresponding test case and checking the correctness of its outcome.
the sampling cost is computed as the sum of the time it takes each sampling related task.
de nition random testing r given a programp random testing rtestspby sampling at each iteration its input space duniformly at random.
the cost for each sampling is oneunit of time.
note that random testing rsamples with replacement .
the cost for each sampling of one unit of time includes the time to generate and execute the corresponding test case and verify the correctness of its output.
de nition systematic testing technique s0 given a program p the systematic testing technique s0 testspby sampling at each iteration exactly one undiscovered error based partition uniformly at random.
the sampled partition itself is also chosen uniformly at random from the remaining undiscovered error based partitions.
the cost for each sampling is cunits of time.
note thats0samples exactly one input from each errorbased partition.
eventually s0will have discovered all partitions and is thus most e ective .
the cost for each sampling ofcunit of time includes the time to generate and execute the corresponding test case and verify the correctness of its output and the time it takes for the additional analysis.
hence we call ctheanalysis cost ofs0.
sinces0samples without replacement it discovers all of kpartitions in ck units of time.
we note both techniques can sample from a reduced input subdomain that contains only e.g.
valid readable or typical test cases instead of sampling the program s complete input space if such are concerns.
we make no such assumptions.
we now delve into the technical details.
in the following we shall formalise relevant concepts of approximation and exponential decay.
de nition asymptotics letf r!randg r!rbe real functions.
we say .f giff n g n !1asn!
.
thus for every there exists n02r such that for every n n jf n g n j .
.f.gif there exist constants c n02r such that jf n j cjg n jfor alln n .
.f gif there exist constants c n02r such that jf n j cjg n jfor alln n .
note iff.gtheng fand conversely.
de nition exponential decay a function f r!rhas exponential decay if it is differentiable at every x2randdf x dx f x for some constant .
in particular note that the function ae x whereais a constant has exponential decay.
.
testing to achieve confidence while the goal of software veri cation is to show correctness of a program for allinputs one goal of software testing is to show correctness at least for somex of the input that is to say to establish a certain degree of con dence x. given a degree of con dence x we compare the expected time it takes to achieve xby random testing rand by the systematic testing technique s0.
after introducing the concepts and insights with an example we investigate the e ciency ofs0andr.
fors0 the expected degree of con dence established grows linearly with time.
in contrast for rit is subject to exponential decay.
given a degree of con dence x we nd that the analysis cost ofs0must be below ex ex2 1units of time in order to remain more e cient than r. for example to establish that the program works correctly for of its input sampling one test systematically must take much less than ve times the time it takes to sample one test randomly.
.
efficiency of s0andr confidence in this work we de ne the con dence that is achieved wrt.
the input space that is discovered def.
.
so we give the expected input space that is discovered by s0afternunits of time.
lemma con dence systematic s0 for the systematic testing technique s0 the expected input space discovered after ntime units is fs n jdj ck n where cis the number of units of time taken for sampling one test input.
proof by de nition s0discoversn cpartitions in nunits of time.
since the total number of partitions is kands0picks a partition uniformly at random from the set of undiscovered partitions the expected contribution of some partition diin any given trial is1 kjdij.
hence the expected contribution of diinntime units isn ckjdij.
by the linearity of expectation we have the expected input space discovered in ntime units isn ckpk i 1jdij njdj ck.
thus the expected size of the input space discovered grows linearly with the number of iterations.
as the cost increases the slope with the time axis jdj ck offs n decreases.
now we look at the case for random testing.
lemma con dence random r for random testing r the expected size of the input space discovered after nunits of time is fr n jdj kx i 1pi pi n jdj kx i 1pie npi proof by de nition rsamplesntests innunits of time.
letxibe the indicator random variable denoting the event that partitiondihas been discovered within these ntrials.
the probability to discover diin any given trial is pi.
the probability thatdiis not discovered after ntrials is pi n. thus the probability that it will be discovered in ntrialsis pi n. let the expected size of the input space discovered after nunits of time be given by the function fr n!r.
we have fr n e nx i 1xijdij kx i 1jdije kx i 1jdij jdjkx i 1pi jdj kx i 1pi pi n to approximate the above quantity we cast the problem of achieving con dence into the problem of nding the bonus sum in the generalized coupon collectors problem .
given jdjcoupons with kdi erent colours there are jdijcoupons of a colour iwhere i kand each coupon has a bonus value ofjdij.
note that the probability to collect a coupon of colouriispi jdij jdj.
then the above quantity is nothing but the bonus sum of the coupons collected after a person collectedncoupons when counting the bonus value of each colour only once.
from the result of r osen we have fr n jdj kx i 1pie npi .
example for equal sized partitions we illustrate the main insights for the simpli ed case where the size of each partition is equal jd1j jdkj and hencepi kfor alli i k. in this setting we demonstrate that the con dence achieved per unit of time decays exponentially for random testing rwhile it grows linearly for the systematic testing technique s0.
later this result is generalized for partitions of arbitrary size.
first we show a simple corollary of lemma .
corollary for random testing rwherepi kfor alli i k the expected size of input space discovered after ntime units is fr n jdj k n jdj jdje n where ln k k .
proof the proof follows directly from lemma when setting pi kfor every i kinfr n .
figure shows the expected size of input space discovered per unit of time for rands0whenk and c .
so it takess0twice as long to sample a test input compared tor.
on the average after units of time s0discovered partitions in of the input space while rdiscovered partitions in of the program s input space.
on the average after units of time both techniques break even having discovered partitions in of the input space.
n0 timeinput space coverage in fr n fs n x figure on the average s0andrbreak even after approximately of the input space was covered and random test inputs were sampled when c k pi k .
there exists a time n0where fr n0 fs n0 ands0has achieved more con dence than rfor anyn n on the average.
to assess the relative e ciency ofs0we pose the following question given a degree of con dence x what is the maximum cost c0fors0such thats0achievesxin time n n0?
we give the answer by the following lemma lemma given a degree of con dence x letnsandnrbe the time at whichs0andrare expected to achieve x respectively.
whenpi kfor everyi i k the maximum cost c0ofs0 such that ns nr is given as c0 c xln x for a constant c. proof settingfs n jdjxgives n xkc0 setting fr n jdjxyields x k n k xkc0 solving for c0gives c0 ln x ln k k xk c xln x where c kln k k figure shows for the segment from x x the exact cost c0fors0such that both techniques are expected to break even at a given degree of con dence.
giving the degree of con dence x the maximum cost is c0 and both techniques are expected to break even at xas shown in figure .
for x we see c0 in fig.
x c0 con dence bound x in cost c0 figure if the average analysis cost of s0exceeds c0 for a given degree of con dence x thenris generally more e cient than s0 here forpi k .
.
bounds on the expected size of the input space discovered for random testing under the simpli ed conditions of the example where each partition has the same size jd1j jdkj we have shown that the con dence achieved per unit of time decays exponentially for random testing.
in the following we prove that this is the case for partitions of arbitrary sizes.
towards that we de ne two quantities pminandpmax.
pmax maxk i 1fpigandpmin mink i 1fpig where the functions max andmin compute the maximum and minimum number in a given set respectively.
note that pmax kandpmin k. we claim lemma approximate bounds fr n is bounded above and below approximately as jdj .fr n .jdj proof let us denote the quantitypk i 1pi pi nbyq n .
letimax f1 kgbe the set of indices such that pmax pi i i2imax.
then for all i2imax we haveln pmax ln pi pmax pi .
let ni ln pmax ln p pmax pi note pmax6 pifori2imax.
this implies e nipi e nipmax pmax pi whence we get pmaxe nipmax pie nipi letnmax maxi2imaxfnigthus for all n nmaxwe have kx i 1pie npi x i2imaxpie npi x i 2imaxpie npi x i2imaxpie npi x i 2imaxpmaxe npmax x i2imaxpmaxe npmax x i 2imaxpmaxe npmax kpmaxe npmaxsimilarly let imin f1 kgbe the set of indices of the error based partitions such that pi pmin i i2imin let nmin maxi2imin ln pi ln pmin pi pmin we can show for all n nminthat kx i 1pie npi kpmine npmin so for alln maxfnmin nmaxg we have kpmaxe npmax kx i 1pie npi kpmine npmin kpmaxe npmax.q n .kpmine npmin hence jdj .fr n .jdj thusfr n being bounded above and below by exponential functions also behaves like one.
.
relative efficiency of s0 confidence we evaluate the e ciency of the systematic testing techniques0relative to that of random testing r. because of the additional analysis cost sampling a test input using s0 takes ctimes longer than sampling a test input using r. since in general the achieved con dence per unit of time decays exponentially for rwhile it grows linearly for s0 there is a point where s0andrare expected to break even.
its coordinates depend on the value of c. given a degree of con dence x we compute the maximum cost c0such that the expected time it takes for s0to achieve xis at most the same as the expected time it takes rto achievexands0remains more e cient than r. proposition given a degree of con dence x e x letns andnrbe the time at which s0andrare expected to achievex respectively.
for all programs p the maximum cost c0ofs0 such that ns nr is bounded above as c0.
ex ex2 proof fix a programpwhich in turn xes the number of partitionskand also the probabilities pifor alli i k. let cp 0be the cost ofs0 such that ns nrforp.
now settingfs n jdjxyields n xkcp settingfr n jdjxgives x kx i 1pie npi kpmine npmin kpmine xkcp 0pmin solving for cp 0gives cp .ln kpmin x kxpmin let us denoteln kpmin x kxpminash k pmin .
from c0 maxpfcp 0g.maxpfh k pmin g where maxpdenotes the maximum of the quantity h k pmin over all programs.
to nd the value max pfh k pmin g we rst relax the requirement that ktakes integral values and allow kto range over the reals r. by so we notice that h k pmin is a continuous function over r which is di erentiable everywhere.
this allows us to use techniques from di erential calculus to maximize h k pmin wrtpminandk.
as we shall see below h k pmin will have exactly one global extremum at some non boundary point.
hence the value of maxpfh k pmin g with the original requirement that k ranges over the discrete integral domain will be attained at one of the two nearest integers.
we rst set the partial derivative of h k pmin wrtpmin to .
pminln kpmin x kxpmin this yields a critical point for h k pmin when pmin e ex k the second partial derivative of h k pmin wrtpminis given by p2 minln kpmin x kxpmin ln kpmin x kxp3 min hence forh k pmin to be maximal wrt pminit must hold that ln kpmin x kxp3 min which yields pmin epe x k since satis es we have that h k pmin attains a maximum wrt pminatpmin e ex k. by a similar analysis we can demonstrate that h k pmin attains a maximum wrt katk e ex pminwhich is the same as eqn.
pmin e ex k. plugging pmin e ex kinto h k pmin we get c0.
ex ex2 finally to derive the bounds on the degree of con dence x for which the above inequality holds note that it must also hold that pmin kwhence from equation we have e ex k k which gives e x corollary given the degree of con dence x2f0 99g the maximum cost cx 0ofs0 such thats0is expected to achievexin at most the same time as ris given as c0 .
c0 .
c0 .
c0 .
c0 .
proof the proof follows directly from proposition .
figure the maximum for the cost c0in terms of pminandk for the degree of con dence x .
figure provides a graphical interpretation of the equation for the x degree of con dence.
the gure shows that the maximum cost c0for all programs with the total number of partitions k k and the probability for the smallest partition pmin p min reaches a maximum for some programs at below a c0of .
by proposition this holds in general for all programs.
.
testing to discover errors besides achieving con dence in the program s correctness another de nition of e ective software testing is to discover errors cf.
e measure .
so given the same time we compare the expected number of errors found by random testingrwith the expected number of errors found by the systematic testing technique s0.
after illustrating our main insights by an example we investigate the e ciency of s0 andrw.r.t.
the expected number of errors discovered.
the expected number of errors discovered per unit of time grows linearly fors0while it decays exponentially for r. first we slightly strengthen de nition stating that failing inputs revealing the same error are grouped into one partition.
this is reasonable because in practice several failing inputs may witness the same error.
de nition error based partitioning the input spacedof a programpcan be partitioned into kdisjoint non empty subdomains diwhere i kwith the following property either every input t2direveals the same error or no input t2direveals an error.
thus the number of error revealing partitions discovered corresponds to the number of errors found.
given a time bound n we nd that the expected number of errors discovered by rwithin ntime units is less than or equals that ofs0only if the analysis cost cincurred bys0 is less than n k qmin n wherekis the number of error based partitions and qminis the fractional size of the smallest error revealing partition in the program s input space.
duran and ntafos de ne a quantity ifor every partitiondiwhich gives the probability of that partition to reveal an error.
in our setting iis either or and can be de ned as i ifdiis error revealing otherwise then the total number of errors in pis given byz pk i i. .
efficiency of s0andr errors found first we give the expected number of errors found per unit of time i.e.
the e ciency for the systematic testing techniques0.
lemma errors found systematic s0 for the systematic testing technique s0 the expected number of errors discovered after ntime units is gs n z ck n forn n k where every trial costs cunits of time.
proof by de nition s0performsn cdraws innunits of time.
in this classical urn problem of sampling without replacement we shall call the discovery of an error revealing partition a success .
the expected number of successes in n cdraws without replacement from a nite population k containingzsuccesses is given byz ck n. the expected number of errors discovered w.r.t the number of iterations grows linearly.
as the cost cincreases the slope with the time axis z ck of the line gs n decreases.
now we look at the case for random testing.
lemma errors found random for random testing r the expected number of errors discovered after ntime units is gr n k kx i pi i n the proof is due to duran and ntafos .
by de nition every iteration occurs in one unit of time.
.
example for equal sized partitions we illustrate the main insights for the simpli ed case where the size of each partition is equal jd1j jdkj and hence pi kfor all i k. in this setting we demonstrate that the e ciency decays exponentially for r while it grows linearly for s0.
later this result is generalized for partitions of arbitrary size.
first we derive the corollary of lemma .
corollary for random testing rwherepi kfor all i k the expected number of errors found after ntime units is gr n z z k n z ze n where ln k .
proof there are a total of znumber of error revealing partitions.
after setting pi kin the formula of lemma we have forznumber of partitions that i and for k znumber of partitions that i .
thus gr n k kx i i k n k k z z k n z z k n z ze ln k n n0 timenerrors discoveredg0 r n gs n figure on the average s0andrbreak even after of errors were discovered and random test inputs were sampled when c z k pi k .
figure depicts the expected number of discovered errors per unit of time for random testing and s0in our example con guration.
as the cost cis it takess0twice as long to sample a test input compared to r. after units of time s0discovered of z errors on the average while r discovered .
errors more on the average.
after units of time both techniques discovered of z errors on the average.
this the point of time where both testing schemes s0andr are expected to break even.
there exists a time n0where gr n0 gs n0 meet and s0has discovered more errors than rfor anyn n on the average.
to assess the relative e ciency ofs0we pose the following question given a time bound n what is the maximum cost c0fors0such thatn0 n?
lemma in the case where pi kfor every i k the maximum cost c0of the systematic testing technique s0 such that the expected number of errors discovered by s0is at least the same as the expected number of errors discovered by random testing in nunits of time is given as c0 n k k n n kas n!
proof the proof follows directly from lemma and corr.
when xing nto nand setting gr n gs n .
figure depicts the exact cost c0fors0such that both techniques are expected to break even at a given time n. time bound ncost c0 c0 n k figure the maximum cost c0increases approximately linearly as the given time bound nincreases.
if the average analysis cost of s0exceeds c0for a given time bound n thenris generally more e cient thans0 here forpi kandk .
giving a time bound of n the maximum cost is c0 and both techniques are expected to break even at n as shown in figure .
increasing the time bound n increases the maximum cost c0approximately proportionally.
.
tight bounds on the expected number of errors discovered for random testing under the simpli ed conditions of the example where each partition has the same size jd1j jdkj we see that the e ciency of random testing decays exponentially .
in the following we show that this is the case for partitions of arbitrary sizes.
intuitively random testing discovers many error revealing partitions in the beginning and much less as the number of iterations increases.
towards that let q fp1 pkgbe a set of probabilities such thatpi2qi i for all indices i k. thus qis the set of pi s corresponding to all the error revealing partitionsdi.
letq fq1 qzg we de ne two quantities qmaxand qminas qmax maxfqjq2qgandqmin minfqjq2qg where the functions max andmin give the maximum and minimum elements in a given set respectively.
we have lemma tight bounds given a programp letkbe the total number of partitions of the input space out of which zare error revealing.
let min ln qmin and max ln qmax then we have z ze minn gr n z ze maxnproof gr n k kx i ipi n k 4x qi2q qi n x qi 2q13 k x qi2q qi n k z z zx i qmax n z ze maxn by similar analysis we can show that gr n z ze minn the function gr n being bounded above and below by exponentially decaying functions also behaves like one.
hence gr n also has the nature of an exponential function.
.
relative efficiency of s0 errors found we evaluate the e ciency of the systematic testing techniques0relative to that of random testing r. because of the additional analysis cost sampling a test input using s0 takes ctimes longer than sampling a test input using r. since in general the e ciency ofr here w.r.t.
discovering errors decays exponentially while that of s0grows linearly there is a point in time where s0andrare expected to break even.
the coordinates of this point depend on the value of c. given nunits of time we compute the maximum cost c0 such thats0remains more e cient than r. speci cally we compute c0such that the expected number of errors discovered bys0is at least the same as the expected number of errors discovered by r. proposition given a program p letkbe the total number of partitions of the input space out of which zare error revealing.
given nunits of time let dranddsbe the expected number of error revealing partitions discovered by the systematic testing technique s0and random testing r respectively.
then the maximum cost c0ofs0 such that dr ds is given as c0 k n qmin n whereqminis de ned as in eqn.
.
proof settinggs n gr n yields z n kc0 k kx i pi i n z n kc0 z z qmin n solving for c0having n k andz gives c0 k n qmin n .
practical implications in this paper we present strong elementary theoretical results about the e ciency of automated software testing.
for thirty years we have struggled to understand how automated random testing and systematic testing seem to be almost on par .
it seems yesterday when arcuri et al.
argued that analytical and empirical analyses have not shown so far a clear inferiority of random testing compared with other more sophisticated techniques .
today we have formally proven limits on the e ciency of automated systematic testing beyond which random testing is certainly superior .
we rst model an ideal systematic testing scheme which we call s0.
by sampling one test input from each error based partition s0is not only the most e ective but also a very e cient testing scheme.
next we assume thats0incurs a constant analysis cost cfor each of its trials while random testing does not.
then we argue that there must be a maximum value for cbeyond whichs0is less e cient than random testing.
now practical testing schemes are much less than ideal .
in reality our testing techniques end up sampling some error based partitions several times and others not at all.
this is because complete certainty about the true errorbased partitioning is unattainable .
in fact the quality of the approximation depends directly on the analysis cost.
the more comprehensive the analysis the more e ective the testing technique.
it follows that in practice to approach the e ectiveness of s0 we need to increase the analysis cost which in turn decreases the e ciency of the testing technique!
moreover practical testing schemes may be less e cient for bigger programs .
as opposed tos0 the e ciency of realistic schemes may not remain constant across all programs.
to maintain e ectiveness the analysis must be more comprehensive as the number of program artifacts increases that are analyzed.
since there is an upper bound on the analysis cost which itself is a function of program size it follows that in practice there exists a maximum program size beyond whichris generally more e cient!
testing schemes may become less e cient during testing .
as opposed tos0 the analysis cost may not remain constant but increase during testing.
take coverage based testing for example.
it requires almost no analysis to sample an initial set of inputs that cover much of the source code.
however it becomes increasingly di cult to cover the remaining few uncovered code elements .
besides the order in which the error based partitions are sampled may not be random def.
.
if so the expected con dence achieved and errors discovered may reduce over time rather than grow linearly.
a practical result of proposition the class of nines for a given degree of con dence xis directly proportional to the magnitude of the maximum analysis cost.
the class of nines for degree of con dence xis computed asb log10 x c whereb cis the oor function con dence xclass of nines bound on c nine c nines c .
nines c .
nines c 105a generalization of proposition it is trivial to show how the proposition holds for disjoint input subdomains that are homogeneous w.r.t.
other properties.
as xed in def.
we investigate the e ciency w.r.t.
error based partitioning.
however there is no reason why the partitioning should not be target based path based or di erential for example.
target based partitioning yields subdomains for which all inputs either do or do not reach a certain target in the source.
di erential partitions are di erence and equivalencerevealing subdomains in the context of regression testing.
path based partitioning groups inputs that exercise a certain path.
to illustrate this generalization of prop.
question we have a program with k z 106paths where the path with the least probability to be exercised is of fractional size qmin .
we have two testing tools a symbolic execution tool s0that exercises each path one at a time chosen uniformly at random from paths not exercised and a random testing tool rthat takes 10msto generate and execute a test case.
finally we only have one hour n 1h to exercise as many paths as possible.
which technique should we choose rors0?
answer we chooses0only if generating and executing one test case takes on the average less than about 1s!
to determine qmin we note that geldenhuys et al.
introduced a tool that can measure the probability of a path to be exercised using model counting on the path condition.
.
conclusion in this paper we explore two notions of testing e ciency that may be the main goals of automated software testing to show in minimal time the correctness of a program for a given percentage of the program s input domain sec.
and to discover a maximal number of errors within a given time bound sec.
.
we de ne a systematic testing technique s0that is most e ective in terms of both the above notions.
subsequently we explore the e ciency of s0again in terms of both the above notions.
however we believe that our work can also provide the formal framework to explore the e ciency of systematic testing techniques other than s0.
if the goal is to discover a maximal number of errors within a given time bound we prove an upper bound on the cost ofs0and show that it depends on the number of error based partitions and the fractional size of the smallest error revealing partition.
we discuss how this result generalizes to other homogeneous partitionings.
if the goal is to show in minimal time the correctness of a program for a given percentage of the program s input space we prove an upper bound on the cost of s0that depends asymptotically only on the given degree of con dence and holds for all programs under test.
the existence of an upper bound has great implications on the scalability of systematic testing if we consider the analysis cost not as a constant but rather a function on the program size.
.