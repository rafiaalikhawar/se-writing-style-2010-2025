an empirical evaluation and comparison of manual and automated test selection milos gligoric stas negara owolabi legunsen and darko marinov university of illinois at urbana champaign gliga snegara2 legunse2 marinov illinois.edu abstract regression test selection speeds up regression testing by rerunning only the tests that can be affected by the most recent code changes.
much progress has been made on researchinautomatedtestselectionoverthelastthreedecades but it has not translated into practical tools that are widely adopted.
therefore developers either re run all tests after eachchangeorperform manual test selection .
re runningall tests is expensive while manual test selection is tedious and error prone.
despite such a big trade off no study assessed how developers perform manual test selection and compared it to automated test selection.
this paper reports on our study of manual test selection in practice and our comparison of manual and automated test selection.
we are the first to conduct a study that analyzes data from manual test selection collected in real time from developers during a three month study and compares manual test selection with an automated state of the research test selection tool for test sessions.
almost all developers in our study performed manual test selection andtheydidso inmostlyad hocways.
comparing manual and automated test selection we found the two approaches to select different tests in each and every one of the test sessions investigated.
manual selection chose more tests than automated selection of the time potentially wasting time and chose fewer tests of the time potentially missing bugs .
these results show the need for better automated test selection techniques that integrate well with developers programming environments.
.
introduction regression testing is an important activity in so ftware development.
it checks that software changes do not break existing tests.
to quickly detect changes that break tests it is desirable to run tests frequently.
however regression testing is also expensive.
because software projects can have very many tests re running all the tests after every code change is often time consuming .
pe rmission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
ase september vasteras sweden.
copyright acm ... .
.
test selection rts aims to improve the efficiency of regression testing by selecting to re run only a subset of the test suite namely those tests that can be affected by the changes.
an rts technique is sa fe if it guarantees to select all affected tests i.e.
the be havior of unselected tests after the code changes remains the same as before the code changes.
one rts technique is moreprecisethan another rts technique if it selects fewer non affected tests.
while researchers have proposed many automated testselection techniques over the last three decades thereisstillnotoolthatimplementsthesetechni quesinawaythatispractical enoughfor widespreadadoption.
the few successful systems for automated regression testing e.g.
thetapsystematgoogle andthescout sy stem formerly known as echelon at microsoft us e techniques that are either imprecise e.g.
tap selects for each project either all tests or no tests based on compiletime project dependencies or unsafe e.g.
scout does not guarantee that the behavior of unselected tests will remain the same so scout only prioritizes but does not select te sts .
these systems can work well for very large codebases but most developers do not write code at the scale of google or microsoft.
rather most developers work on smaller projects for which running a coarse grained technique like tap would often be equivalent to re running all tests.
such smaller projects would require a finer grained technique for more precise rts e.g.
finding affected tests based on control flow edges or methods .
th e lack of practical rts tools leaves two options for developers either automatically re run all the tests or manually perform test selection.
re running all the tests is safe by definition but it can be quite imprecise and therefore inefficient.
in contrast manual test selection which we will refer to as manual rts can be both unsafe and imprecise developers can select too few tests and thus miss to run some tests whose behavior differs due to code changes or developers can select too many tests and thus waste time.
despite the importance of rts we are not aware of any research that studies ifandhowdevelopers perform manual rts and how manual and automated rts compare.
ouranecdotalexperienceshows thatdevelopersselect torun only some of their tests but we do not know how many developers do so how many tests they select why they select those tests what automated support they use for manual rts in their integrated development environment ide etc.
also it is unknown how developers manual rts practices compare with any automated rts technique proposed in the literature how does developers reasoning about affe cted tests compare to the analysis of a safe and precise automated rts technique?
the need for adoptable automated rts tools makes it critical to study current manual rts practice and its effects.
this paper presents theresults of thefirst studyof manual rts and a first comparison of manual and automated rts.
specifically we address the following research questions rq1.
how often do developers perform manual rts?
rq2.
what is the relationship between manual rts and size of test suites or amount of code changes?
rq3.
what are some common scenarios in which developers perform manual rts?
rq4.
how do developers commonly perform manual rts?
rq5.
how good is current ide support in terms of common scenarios for manual rts?
rq6.
howdoes manual rtscompare with automatedrts in terms of precision safety and performance?
to address the first set of questions about manual rts rq1 rq5 we extensively analyzed logs of ide interactions recorded from a diverse group of developers working on projects i.e.
some developers worked on multiple projects during our study including several experts from industry .
these logs cover a total of hours of deve lopment with test sessions and a total of executed tests.
a test session refers to a run of at least one test between two sets of code changes.
we refer to test sessions withasingletestas single test sessions andtestsessions with more than one test as multiple test sessions .
to address rq6 we compared the safety precision and performance of manual and automated rts for test sessions of one representative project using the best available automated rts research prototype .
se veral of our findings are surprising.
regardless of the project properties small vs. large few tests vs. many tests etc.
almost all developers performed manual rts.
of all test sessions executed a single test and of multiple test sessions on average had some test selection.
the pervasivenessofmanualrtsestablishes theneedto studymanual rtsin more depthandcompare it with automatedrts.
moreover our comparison of manual and automated rts revealed that manual rts can be imprecise in of the test sessions manualrtsselects more tests thanautomated rts and unsafe in of the test sessions manual rts selects fewer tests than automated rts .
finally our experiments show that current automated rts may provide little time savings the time taken by an automated rts tool1 pe r session to select tests was .
.
sec mean sd and the estimated time saved by not executing unselected tests was .
.
sec.
these results show a strong need for better automated rts tools.
this paper makes the following contributions evidence.
we conducted the first study to show how developers perform manual rts.
1thismeasuresonlytheanalysistimetoidentifytheaffected tests but notthe time to collect coverage.
examination.
we closely examined manual rts and its associated factors in practice.
comparison.
weperformedthefirstcomparisonofmanual and automated rts.
.
evaluating manualrts we present our methodology for analyzing manual rts data to answer rq1 rq5 and summarize our findings.
.
methodology we analyzed the data collected during our previous field study in which we unobtrusively monitored developers id es and recorded their programming activities over three months.
wehadusedthecollected datainourpriorresearch studies on refactoring and version control the wo rk presented in this paper is the first to focus on the regression testing aspects.
to collect data we asked our study participants to install our record and replay tool codingtracker in their ec lipse indigo ides.
throughout the study codingtracker recorded detailed code evolution data ranging from individual code edits start of each test and test outcome e.g.
pass fail up to high level events like automated refactoring invocations and test session executions.
codingtracker uploaded the collected data to our centralized repository using existing infrastructure .
in this study we only consider data from participants who had more than ten test sessions.
overall the data encompasses hours of code development activities by developers of whom five are professional programmers and nine are students.
the professional programmers worked in different software companies on projects spanning various domains suchas marketing banking business process management and database management.
the students were computer science graduate students and senior undergraduate interns whoworkedonavarietyofresearchprojects fromsix research labs at the university of illinois.
the programming experience of our study participants varied one developer had less than years eight developers had between years and five developers had more than years.
none of the study participants knew how we would analyze the collected data in fact we ourselves did not know all the analyses we would do at the time we collected the data.
in the rest of this section we discuss the tool used the projects analyzed the challenges faced and the answers we found to rq1 rq5.
.
.
codingtracker codingtracker integrates well with one of the most popular ides eclipse .
developers do not explicitly interac t withcodingtracker during their workflow and thus the data recorded by codingtracker is as close as possible to what developers normally do.
codingtracker collects information about all test sessions.
because test selection data is available at every test session we were able to capture developers manual rts decisions.
each test session includes a list of executed tests their execution time and their status on completion pass or fail .
further codingtracker collects information about code changes between test sessions.
whilecodingtracker logs provide a treasure trove of data they have limitations.
first codingtracker logs cannot fully confirm that developers performed manual rts.
in theory 362projecttest sessions available tests selected tests selective se ssions total single test debug minmax mean minmax mean sum timemin p1 .
.
.
p2 .
.
.
p3 .
.
.
p4 .
.
.
p5 .
.
.
p6 .
.
.
p7 .
.
.
p8 .
.
.
p9 .
.
.
p10 .
.
.
p11 .
.
.
p12 .
.
.
p13 .
.
.
p14 .
.
.
p15 .
.
.
p16 .
.
.
p17 .
.
.
summationtext5 ari mean .
.
.
.
.
.
.
figure statistics for projects used in the study selectiv e sessions is of multiple test sessions we exclude single test sessions as they may not be true selective sessions developer knows that not all affected tests are selected developers could have installed some eclipse plugin that would perform automated rts for them.
however we are not aware of any automated rts tool that works in eclipse.
moreover we have noticed significant time delays between code changes and the start of test sessions which likely correspond to developers selection times i.e.
time that developers spend reasoning about which tests to run and not automated tool runs.
therefore we assume that developers manually selected the tests in each test session.
second codingtracker collects information about code changes but not entire project states .
the original motivation for codingtracker was a study of refactorings which needed on ly code changes so a design decision was made for codingtracker tonotcollect the entire project states to save space time for storing logs on disk and transferring them to the centralized repository .
however the lack of entire states creates challenges to exactly reconstruct the project as the developer had it for each test session e.g.
to precisely count the number of tests or to compile and run tests for automated rts .
sections .
.3and3.
.3discuss how we address these challenges.
.
.
projectsunderanalysis as mentioned earlier we analyzed the data from developers working on research and industrial projects e.g.
a struts web application a library for natural language processing a library for object relational mapping and a research prototype for refactoring.
note that some developers worked on several projects in their eclipse ide during our three month study codingtracker recorded separate data for each project more precisely codingtracker tracks each eclipse workspace that was imported into eclipse.
figure1is a summary of test related data that we colle cted2.
for each project we first show the number of test se ssions.
our analysis showed that a large number of these sessions execute only one test.
we refer to such test sessions assingle test sessions .
further we found that many of these single test sessions execute only one test that had failed in the immediately preceding session.
we refer to such ses2due to the conditions of institutional review board approval we cannot disclose the true names of these projects.sions as debug test sessions .
next we show the number of available tests i.e.
the total number of tests in the project at the time of a test session discussed in more detail in section2.
.
.
then we show the number of se lected tests i.e.
a subset of available tests that the developer selected to execute including the total number of selected tests that the developer executed throughout the study and the total execution time for all test sessions3.
finally we show th e percentage of selective sessions i.e.
multiple test sessionswhere the number of selected tests is smaller than the number of available tests in other words the developer performed manual rts in each such test session by selecting to execute only a subset of the tests available in that session.
the total test execution time with manual rts is substantially lower than it would have been without manual rts.
the sum of the timemin column in figure 1shows th at when manual rts is performed the total test execution time for all developers in our study was minutes.
in contrast had the developers always executed all available tests we estimate4that it would have resulted in a total test ex ecution time of minutes.
in other words had the developers not performed manual rts their test executions would have taken about an order of magnitude more time.
we point out some interesting observations about singletestsessions.
first theprojectsusedinourstudyspanmany domains and vary in the number of available and selected tests but they all have some single test sessions and some multiple test sessions.
second single test sessions include both debug and non debug sessions.
non debug single test sessions usually happen when introducing a new class feature because the developer focuses on the new code.
by default in the rest of the paper we exclude all single test sessions from our analyses and only mention them explicitly 3the reported execution time is extracted from the timestamps recorded on developers computers.
it is likely that developers used machines with different configurations but we do not have such information.
4note that codingtracker does can not record the execution time for the unselected tests that were not executed we estimate the time from the averages of the sessions in which the tests were executed.
inputs session info extracted from codingtracker logs 2l ist a bracketle ttestsession a bracketri htsessions 3map a bracketle ttestsession set a bracketle tpair a bracketle tclassname methodname a bracketri ht a bracketri ht a bracketri htexecuted output available tests for each test session 6map a bracketle ttestsession set a bracketle tpair a bracketle tclassname methodname a bracketri ht a bracketri ht a bracketri htavailable compute available tests for each test session 9computeavailable 10set a bracketle tpair a bracketle tclassname methodname a bracketri ht a bracketri htt current available tests 11available 13foreachs sessions set a bracketle tpair a bracketle tclassname methodname a bracketri ht a bracketri hte executed s if e t t c m t c m e t t e available s t figure algorithm for computing a set of available test methods at each test session when some of the subsequent plots or other numbers that we report include single test sessions.
.
.
challenges codingtracker was initially designed to study how code evolvesovertime andthusitrecordedonlycodechanges an d various file activities but not the entire state of the developers projects.
as a consequence we could not easily extract the number of available tests for each test session whilecodingtracker did record the information about tests that are executed selected it had no explicit information about tests that were notexecuted.
therefore we developed an algorithm to estimate the number of available tests reported in figure .
we designed our algorithm to be co nservative and likely under estimate the number of available tests.
in other words developers likely performed even more manual rts than we report.
figure2shows the algorithm.
the input to the algorithm is alistoftestsessions extractedfromthe codingtracker logs each session is mapped to a set of executed tests and each test is represented as a pair of a test class and test method name.
the output is a mapping from test sessions to the set of available tests.
although we extract more information for each test session e.g.
execution time that information is not relevant for this algorithm.
the algorithm keeps track of the current set of available tests t initialized to the empty set line .
for each te st session the algorithm adds to tthe tests executed in that session line those tests are definitely available.
th e algorithm also attempts to find which tests may have been removed and are not available any more.
for each multiple test session the algorithm removes from tall the tests whose class matches one of the tests executed in the current session s line16 .
the assumption is that executin g one test from some class c in a session that has more than one test likely means that alltests from that class are executed in the session.
thus any test from the same class that was executed previously but not in the current session was likely removed from the project.
this assumption is supported by the fact that eclipse provides rather limited support for selection of multiple tests from the same class as discussed in section .
.
for single test sessions the algo rithm only adds the executed test to t the assumption is that the same tests remain available as in the previous ses sion but the developer decided to run only one of the tests.
finally tbecomes the available set of tests for the current session line .
note that our algorithm does not account fo r removed test classes but these are very rare in our data set.
for example we inspected in detail project p14 one of the largest projects and no test class was deleted.
.
investigating manual rts in summary the results showed that almost all developers in our study performed some manual rts.
they did so regardless of the size of their test suites and projects showing that manual rts is widely practiced.
next we provide details of our findings regarding research questions rq1 rq5.
rq1 how oftendodevelopersperformmanualrts?
developers performed manual rts in .
.
mean sd of the test sessions we studied column selective sessions in figure .
note that we first compute selecti ve session ratio for each developer and then an unweighted arithmetic mean of those ratios rather than weighting by the number of test sessions because we do not want developers with the most test sessions to bias the results.
020406080100selected available figure distribution of test selection ratio with left and without right singletest sessionsacross all multipletest sessions in our study the average ratio of selected tests tests that the developer executed to available tests tests that could have been executed i.e.
averagetest selection ratio was only .
.
note that this number is calculated from all test sessions as if they were obtained from a single developer.
we show the distribution of test selection ratios for all test sessions for all the developers using violin plots in figure .
av iolin plot is similar to a boxplot but additionally shows probability density of the data at different values.
the left part of figure 3shows the distribution of test selecti on ratios when single test sessions are included while the right part shows the distribution when single test sessions are excluded.
we show only one half of each violin plot due to space constraint themissing halves are symmetric.
it can be observed from the violin plots that manual rts happens very frequently and most of the time the test selection ratio is less than .
we note here that our finding constitutes the first empirical evidence concerning manual rts in practice.
more importantly we think that this fact should result in a callto arms by the automated rts community because poor manual rtscouldbe hamperingdeveloper productivityand impacting negatively on software quality.
rq2 does manual rts depend on size of test suites oramountofcodechanges?
developers performed manual rts regardless of the size of their test suites.
we draw this conclusion because almost all developers in our study performed manual rts 364and they had a wide range of test suite sizes.
the averag e test suite size in all projects we studied was .
tests column available tests in figure the minimum wa s tests and the maximum was tests.
considering that these projects are of small to medium size and because they exhibit manual rts we expect that developers of larger projects would perform even more manual rts.
we also consider the relationship between the size of recent code changes and the number of tests that developers select in each test session.
one may expect that developers run more tests after large code changes.
we correlate the test selection ratio with the code change ratio for all test sessions.
the code change ratio is calculated as the percentage of ast node changes since the previous test se ssion over the total ast node changes during the entire studyfor aparticular project.
to assess correlation we measure the spearman s and pearson s correlation coefficients5.
th e spearman s and pearson s coefficients are .
.
when single test sessions are included and .
.
when single test sessions are included respectively.
in all cases the p value was below .
which confirms that some corre lation exists.
however the low values of coefficients imply a low correlation between the amount of code changes immediately before a test session and the number of manually selected tests in that session.
this low correlation was a surprising finding as we had expected a higher correlation between code changes and the number of selected tests.
rq3 whatarecommonscenariosfor manualrts?
the most common scenario in which developers performed manual rts was while debugging a single test that failed in the previous session.
recall that we refer to such test sessions as debug test sessions .
as seen in figure colum n single test debug debug test sessions account for out of the total test sessions considered.
one common pattern that we found in the data was that after one or more tests fail developers usually start making code changes to fix those failing tests and keep re running only those failing tests until they pass.
after all the failing tests pass the developers then run most or all of the available tests to check for regressions.
another pattern is when a developer fixes tests one after another re running only a single failing test until it passes.
therefore even if the developers had a perfect automated rts tool to run after each change such a tool could prove distracting when running many debug test sessions in sequence.
specifically even if some code changes affect a larger number of tests developers may prefer to run only the single test that they are currently debugging.
the existence of other reasons for rts besides efficiency improvements shows a need for a different class of tools and techniques that can meet these actual developer needs we discuss this further in section .
it is also interesting that the sequences of single test sessions i.e.
single test sessions without other test sessions in between weremuchlongerthanweexpected.
themean sd of the length ofsingle test session sequences was .
.
.
the longest single test session sequence contains test ses5although the data is not normally distributed and the relationship is not linear we report the pearson s coefficient for completeness.
6a low p value indicates that spearman s or pearson s coefficient is unlikely .
un selected execution time sec 0100200300400500600selection time sec figure relationship between selection time and estimated time to execute unselected tests the plot also shows the identity line sions which may indicate that developers avoid running all tests when focusing on new features and debugging.
rq4 how do developers commonly perform manual rts?
we found that developers use a number of ad hoc ways for manual rts.
these include commenting out tests that should not be run selecting individual nodes of hierarchy by which we refer to the way tests are hierarchically organized from test methods to test classes to test packages to entire projects and creating test scripts which specify runs of several nodes of hierarchy.
manual rts by commenting one approach used by the developers was to comment out unit tests they did not want to run.
we observed that developers performed this type of selection at different levels of granularity.
some developers commented out individual test methods within a test class while others commented out entire test classes from junit annotations that specify test suites.
in both cases the time overhead incurred by the developer in deciding which tests to run and in commenting out the tests i.e.
selection time is likely to be non negligible.
in other words selection time is an estimate of the time spent by developers to manually analyze and select which tests may be affected.
using the available codingtracker data we estimate selection time to be the time elapsed from the last code change that immediately preceded a test session and the start of the test session.
we exclude selection time values greater than minutes as developers may re run tests after taking a break from work.
our experiments with break times of minutes and minutes did not significantly change any of the outcomes of our study.
in figure we show the correlation between se lection time and estimated time to execute unselected tests which is the time saved by not executing unselected tests .
while the overall time savings due to manual rts is significant we found that in of the cases points above the identity line in figure developers could have saved mo re time by simply running all the tests.
manual rts by selecting various nodes of hierarchy developers also perform test selection by selecting a node of hierarchy in their ide e.g.
they could select to run only a single test or all the tests from a single class or package.
this is a critical rts limitation in eclipse it restricts the developer to select to run only one node of hierarchy in the limit this node represents the entire project such that the 365entire test suite for that project is run .
in other words the de veloper is not able to select to run an arbitrary set of tests or test suites.
related but different in several projects by browsing through the changes collected by codingtracker we noticed that developers were writing scripts .launch files in eclipse to group tests.
using a script has the same limitation as manually selecting a node of hierarchy.
these limitations of eclipse are shared by several popular ides as shown in figure .
rq how goodis ide supportformanualrts?
ides provide varying levels of support for performing manual rts.
the ides we investigated are eclipse7 intellij id ea8 netbeans9 and visualstudio .
su pport for arbitrary manual rts recall from the answer to rq4 that in several cases the developers selected among tests by commenting out the tests within test classes or commenting out test classes within test suites.
this likely means that developers would like to arbitrarily select tests within nodes of hierarchy.
also ourexperience with running the automated rts tool as discussed in section shows th at all affected tests may not reside in the same node of hierarchy.
thus it is also important to be able to arbitrarily select tests across these nodes.
figure5is a summary of available ide support for selectin g tests at different levels of granularity within and across nodes of hierarchy.
all the ides allow developers to select a single test.
moreover several ides offer support for arbitrary selection.
intellij allows to arbitrarily select tests by marking in the gui each test to be run subsequently.
this may be tedious for selecting among very many tests and is only available for arbitrarily selecting test classes across test packages or test methods within the same class.
visualstudio allows arbitrary selection by specifying regular expressions for test names which may match across multiple nodes of hierarchy.
however not all developers are familiar with regular expressions and knowledge of all test names in the project is required to write them effectively.
still based on our study having this type of support seems very valuable given that it is needed by the developers.
more importantly eclipse lacks support for such arbitrary test selection.
support for rts across multiple test sessions we showed in theanswer torq3thatthemost commonpatternofmanual rts occurred during debug test sessions.
it is likely that the changes made between debug test sessions affect more tests than the test being fixed.
indeed we found this to be the case for project p14.
it is possible that the developers do not select other tests affected by the changes due to additional reasoning required to identify such tests.
thus their test selections during debug test sessions are likely to be unsafe and may lead to extra debug steps at a latter stage.
although visualstudio provides some level of rts automation it has some shortcomings that we discuss in section .
on e observation from our comparison of ides is that they differ in their level of support for the different patterns of 7kepler service release build id .
8version .
.
build id ic .
.
9version .
build id .
10we selected visualstudio rather than the latest version because visualstudio was the only ide that has ever supported automated rts interestingly enough this automated rts support has been removed from the ide in subsequent releases!rts capability eclipse netbeans intellij vs select single test run all available tests arbitrary selection in a node of hierarchy arbitrary selection across nodes of hierarchy re run only previously failing tests select one from many failing tests arbitrary selection among failing tests figure rts capabilities of popular ides.
intellij only pa rtially supports arbitrary selection manual rts but even if we combined the best rts features from all ides investigated it would still not be sufficient for safe and precise rts that developers need.
.
manual vs. automated rts we next discuss the results of our comparison of manual andautomatedrts bywhichweaddressquestionrq6.
we compare both approaches in terms of safety precision and performance usingone ofthelargest projects from ourstudy.
as noindustry strengthtool for automated rtsis available we used faulttracer a recently developed state of there search rts prototype.
.
methodology we investigated in detail the data collected from one of our study participants with the goal of comparing manual and automated rts.
we chose p14from figure for reaso ns described in section .
.
.
first we reconstructed the st ate ofp14at every test session.
recall that codingtracker doesnotcapture the entire state of the project for any test session.
we had to perform a substantial amount of work to find a code version that likely matched the point where the developer used codingtracker .
we acknowledge the help of thep14developer who helped with this information especially that the code moved from an internal repository to an external repository.
it took several email exchanges to identify the potential version on top of which we could replay the codingtracker changes while still being able to compile the entire project and execute the tests.
second for each test session we ran faulttracer on the project an d compared the tests selected by the tool with the tests selected by the developer.
because faulttracer is a research prototype it did not support projects in the general sense of the term software projects that are distributed across multipleeclipse projects inthespecificterminology ofwhat eclipse calls projects even in the same eclipse workspace.
we worked around this limitation of faulttracer by automatically merging all eclipse projects from p14into one project that faulttracer could analyze.
upon replaying the codingtracker logs and analyzing the data we discovered that the developer often ran multiple test sessions which had no code changes between them.
the developer had organized the tests in separate test suites and always selected torunthesetestsuites oneata time thereby potentially running multiple test sessions in parallel.
to compare manual and automated rts fairly and consistently we accounted for the occurrence of multiple test sessions without intervening changes.
this is because fault366tracerwould only select to run tests after detecting code c hanges between consecutive versions of the software.
our solution was to merge consecutive test sessions which had no intervening changes.
consider two consecutive test sessions xandy with no intervening changes.
suppose that the tests and their outcomes for xare andforyare .our merge would produce a union of the tests in xandy and if a test happens to have different outcome the merge would keep the result from x however because the test runs happenedwithoutinterveningchanges itis reasonable toexpect that if some tests are re run their outcomes should be the same.
we checked that in our entire study the test runs are largely deterministic and found a tiny percentage of nondeterministic tests .
.
the effect of non deterministic tests on rts is a worthwhile research topic on its own .
fo r the sessions xandyshown above the merged session would contain the tests test1 ok test2 failure test3 ok test4 ok .
having merged the manual test sessions the number of test sessions for comparing manual and automated rts we obtained was .
we further limited our comparison to the first of these test sessions due to difficulties in automating the setup of p14to usefaulttracer to perform rts between successive versions.
as we studied a very large project which evolved very quickly and had dependencies on environmentandmanythird partylibraries we couldnot easily automate thesetupacross all mergedtestsessions.
the test sessions used constitute the largest consecutive sequence of test sessions which had the same setup.
we discuss other challenges in section .
.
.
across all te st sessions considered p14has on average lines of code and .
available tests.
.
.
faulttracer the inputs to faulttracer are two program versions old versionpand new version p and the execution coverage of tests at version p i.e.
a mapping from test to nodes of extended control flow graph covered by the test .
let tbe the set of tests in p.faulttracer produces as output a set of tests t tthat are affected by the code changes between pandp .
the unselected tests in t t cannot change their behavior.
note that one also has to run new tests that are added in p but do not exist in p. we chose faulttracer because it represents the state ofthe research in rts and implements a safe rts technique.
also faulttracer works at a fine granularity level which improves its precision because it tracks coverage at the level of an extended control flow graph .
to identify co de changes faulttracer implements an enhanced changeimpact analysis.
in addition faulttracer targets projects written in java the same programming language used in p14 so there was a natural fit.
however note that we chose faulttracer from a very limited pool.
to the best of our knowledge there exists no other publicly available tool that performs test selection at such fine granularity level e.g.
statement control flow edge basic block etc.
.
systems such as google s tap system and microsoft s scout system are propr ietary.
moreover tap implements a coarse grained analysis based on dependencies between modules which would be overly imprecise for p14that has only few modules.
.
.
projectunderanalysis we chose p14for the following major reasons.
first it was one of the projects with the largest recorded data in terms of the number of test sessions of all .
hence there was a higher chance of observing a greater variety of test selection patterns.
this also means that we had more data points over which to compare manual and automated rts for the same developer.
second the developer worked on creating a large and industrially used library presenting the opportunity to study test selection in a realistic setting.
finally with the help of the original developer of the project we were able to gain access to the exact vcs commits of the projectwhichmatchedtherecordeddata.
atthetimeofthis writing developers of other projects have either been unable to provide us access to their repositories or we are unable to reconstruct the revisions of their projects that matched the exact period in the codingtracker recording.
.
.
challenges because codingtracker didnotcaptureentireprojectstate we had to reconstruct the p14 s developer s workspace to be able to build and run tests for our analysis.
using timestamps from the codingtracker logs we looked for a commit in the developer s vcs which satisfied the following conditions the time of the commit matches the vcs commit timestamp recorded in the codingtracker logs and the code compiles after checking it out of the vcs and adding required dependencies.
finally this checked out version was imported into eclipse and used as a basis for replaying the codingtracker logs.
by replaying the changes captured by codingtracker on top of this initial state we obtained the state of the entire project in every succeeding test session.
note that codingtracker captures changes to both the project under test and the testing code and thus the reconstructed developer s workspace contained all the tests available at any given test session.
we assume that the ability to replay the codingtracker logs from the initial vcs commit till the end of the logs without any error means that it was a likely valid starting point.
thus the reconstructed workspace is as close to the developer s workspace as it existed while codingtracker monitored the developer s programming activity.
to mitigate these challenges in future studies focusing on rts codingtracker would need to be modified to capture the complete initial state of the project as well as any dependencies on external libraries.
.
comparing manual and automated rts the number of selected tests we plot in figure the nu mber of tests selected by manual rts against the number of tests selected by automated rts i.e.
faulttracer for each test session.
a quick look may reveal that there is a substantial difference between manual and automated rts which we further analyze.
figure7shows the distribution across test sessions of the nu mber of tests selected by manual and automated rts.
we show the distribution for two cases with w and without w o single test sessions.
it can be seen that the median is much lower for the automated tool in both cases.
this implies that the developer is imprecise i.e.
selects more than necessary .
further if single test sessions are included we can observe that the arithmetic mean shown as a star is lower for manual than automated rts.
how3670 ma nual0100200300400500600700automated figure relationship of the number of tests selected in each test session by manual and automated rts for p14 w ma nualw automatedw o manualw o automated0100200300400500600700800tests selected figure distribution of selected tests for p14with w and without w o single test sessions ever when single test sessions are excluded we can see the opposite.
this indicates as expected that developer focuses on very few tests while debugging and ignores the other affected tests.
finally when single test sessions are excluded from the manually selected tests we found that many test sessions contain the number of tests equal to the median.
our closer inspection shows this to be due to the lack of support for arbitrary selection in eclipse which forced the developer to run all tests from one class.
safety and precision one major consideration in comparing manual and automated rts is the safety of these approaches.
in other words if we assume that the automated tool always selects all the tests affected by a code change does the developer always select a superset of these?
if the answer is in the affirmative then the developer is practicing safe rts.
on the contrary if the set of tests selected by the developer does not include all the tests selected by the tool it means that manual rts is unsafe or the tool is imprecise .
to compare safety between manual and automated rts for every test session we compare both the number of tests selected and the relationship between the sets of tests selected using both approaches.
figure6shows the relationship between the numbers of te sts selected by both approaches.
the spearman s and pearson s correlationcoefficientsare0.
p valuebelow0.
and .
p value is .
respectively.
these values indicate a rather low almost non existent correlation.
we compared the relation between the sets of tests selected using manual and automated rts.
in of the test sessions the developer missed to select at least one of the tests selected by faulttracer .
assuming that faulttracer is0 te sts selected0.
.
.
.
.
.0code changes ratiomanual automated figure relationship of manual and automated rts with re lative size of code changes for p14 safe we consider these cases to be unsafe.
in the remaining of the test sessions the developer selected a superset of tests selected by faulttracer .
moreover in of the test sessions the developer selected more tests than faulttracer.
assuming that faulttracer is precise we consider these cases to be imprecise.
note that a developer can be both unsafe and imprecise in the same test session if the developer selects some non affected tests and does not select at least one of the affected tests.
thus the sum of the percentages reported here is greater than .
correlation with code changes insection .
we found th at forallprojects in our study there is low correlation between code change ratio and manual rts.
we revisit that correlation in more detail for the p14project.
to further compare manual and automated rts we evaluate whether either of these selection approaches correlates better with code changes.
effectively we re check our intuition that the developer is more likely to select fewer tests after smaller code changes.
we measured the pearson s and spearman s correlation coefficients for bothmanualandautomatedrts.
the values for spearman s coefficients are .
p value below .
and .
p value is .
for manual and automated rts respectively.
the values for pearson s coefficients are .
p value is .
and .
p value is .
for manual and automated rts respectively.
while the correlation is low in all cases the slightly higher values of correlation coefficients for manual rts may indicate that compared to automated rts the developer indeed selects fewer tests after smaller changes and more tests after larger changes as it becomes harder to reason which tests are affected by larger changes.
the plot in figure 8visualizes th e relationship for each test session between code change ratio and the number of selected tests for both manual and automated rts.
we can observe that manual rts is less likely to select many tests for small changes e.g.
fewer red dots than blue dots are close to the x axis around the mark .
in the end the size of semantic effect of a change as measured by the number of affected tests is not easy to predict from the size of the syntactic change as measured by the number of ast nodes changed .
performance wefinallycomparemanualandautomated rts based on the time taken to select the tests.
figure shows the distribution of selection time first boxplot as de fined in section .
and analysis time second boxplot in curred by faulttracer .
we can observe that the developer is faster than the automated rts tool in selecting which tests to run the p value for the mann whitney u test is 368selection ma nualanalysis automatedunselected automatedselected automated050100150200250300time sec figure distribution for p14of estimated manual selection time first boxplot automated analysis time second boxplot execution time of the tests unselected by faulttracer third boxplot and execution time of the selected tests by faulttracer fourth boxplot below .
.
for comparison we also show the distribution of estimated execution time for tests that are unselected byfaulttracer third boxplot and actual execution time for tests selected by faulttracer fourth boxplot .
we ran all our experiments on a .
ghz intel xeon e3 v2 machine with 16gb of ram running ubuntu linux .
.
lts and oracle java bit server version .
.
.
on e can observe that faulttracer analysis took substantial time.
although the analysis time .
.
seconds is on average less than the time saved by not running unselected tests .
.
seconds it is important to note that one may also want to take into accounttime to collect necessary coverage information to enable change impact analysis if time taken for analysis plus overhead for collecting coverage plus running selected tests is longer than time taken for running all the tests then test selection provides no benefit.
this raises the question whether a fine grained technique such as the one implemented in faulttracer ca n be optimized to bring benefits to smaller projects.
one of our planned future directions is to explore which granularity level of automated rts techniques is appropriate for most projects.
further we believe that research studies on automated rts should provide more information about their complexity e.g.
time to implement the technique and efficiency e.g.
analysis time time to collect coverage etc.
.
previous research focused mostly on the number of selected tests i.e.
safety and precision which is not sufficient for proper comparison anddiscovering a test selection technique that works in practice.
.
discussion we briefly discuss test selection granularity our experience with an ide integrated automated rts tool and propose a potential improvement to automated rts in ides.
test selection granularity we mentioned earlier that systems such as tap at google and scout at micr osoft are successfully used for test selection prior itization.
however these systems are used as part of the gated check in infrastructure i.e.
all affected regression te sts are executed before a commit is accepted into the central repository .
in other words they are not used and are not applicable on developers machines where developers commonly work on few modules at a time and run testslocally .
even developers at either of these companies let alone many developers who do not develop code at the scale of google or microsoft would benefit from an improved finegrained test selection.
this provides motivation for research on finding the best balance between analysis time implementation complexity and benefits obtained from test selection.
improved fine grained test selection would be more widely applicable and could be used in addition to coarsegrained test selection systems.
experience with ide integrated automated rts we experimented with visual studio the only tool to the best of our knowledge that integrates automated rts with an ide.
we did this to see if such a tool would perform better than manual rts in terms of safety and precision.
specifically the test impact analysis tia tool in visual studio was designed to help reduce testing effort by focusing on tests that are likely affected by code changes made since the previous run of the tests.
we think this is an excellent step towards improved rts in developer environments and that similar tools should be developed for other ides.
we successfully installed tia and ran it on several simple examples we wrote and on an actual open source project.
however we found a number of shortcomings with tia.
most importantly the tool is unsafe any change not related to a method body is ignored e.g.
field values annotations etc.
.
also changes like adding a method removing a method or overriding a method remain undetected .
fu rthermore tia does not address any of the issues commonly faced byselection techniques su ch as library updates reflection external resources etc.
our opinion is that a safe but imprecise tool would be more appreciated by developers.
potential improvement of ides across all projects we observed that developers commonly select tests during debugging.
thus one common way by which an ide might help is to offer two separate modes of running tests a regular mode and atest selection mode .
in the regular mode the developer may choose to re run after a series of code changes one or more previously failing tests while ignoring other affected tests .
once the test passes the developer may run in the test selection mode to check for regressions.
notice that the test selection runs would be separated by a series of regular runs.
consider two test selection runs a andb figure10 .
ina some tests were selected to be run a nd failed.
developer then performs regular runs a1 a2 ... an untilthe previouslyfailing test passes.
the test selection runbis then executed to ensure that there are no regressions due to code changes since a. note that the analysis performed before running bshould consider the difference sinceaand not just the difference between anandb otherwise tests affected by the changes between aandanwould not be accounted for.
as a simple optimization step the tool could exclude the tests affected between aandbthat were already run after the change that was affecting them.
.
threats to validity external developers projects and tools the results of our study may not generalize to projects outside of the scope of our study.
to mitigate this threat we used projects that cover various domains and developers with different levels of programming experience.
further these projects vary significantly in size number of developers and number of tests.
regarding the comparison of manual and 369a ... ...b a a nregular runs selection runs figure anexample of a common patternwhen developer al ternates selection and regular runs automated rts we used the largest project for which we could reconstruct the entire state for many test sessions.
we used faulttracer a research prototype to perform automated rts.
other tools that im plement different test selection techniques could have led to different results.
we chose faulttracer because it implements a safe and precise test selection technique.
to the best of our knowledge no other publicly available tool for test selection exists except the proprietary tools that work at coarse granularity level which would not be applicable to any of the projects used in our study .
our experience with visualstudio demonstrated that the implemented approach is unsafe thus inappropriate for our study.
finally the patterns of test selection could differ in other languages but java.
we leave the investigation of how manual rts is performed in other languages for future work.
internal implementation correctness we extracted data relevant to manual rts from the study participants recordings.
to extract the data we wrote analyzers on top of the infrastructure that we used in our prior research studies on refactorings .
further new analyzers and sc ripts were tested and reviewed by at least two authors.
construct ides and metrics because codingtracker is implemented as an eclipse plugin all developers in our study used eclipse ide.
therefore our study results may not hold for other ides.
however because eclipse is the most popular ide for java our results hold for a signi ficant portion of java developers.
we leave the replication of our study using other popular ides both for java and other languages for future work.
.
related work wecomplement existingwork on automatedrts by investigating actual rts practices of deve lopers.
over the last three decades many automated rts techniques were proposed which have mostly focused on improving the precision and safety of test selection .
one ke ydifferenceamongthesetechniquesisthegranularitylevel at which they detect affected tests e.g.
statement controlflow edge method etc.
.
recent work has al so reported some success with automated rts in large software projects.
unfortunately after decades of research nopractical rtstool is available for widespread adoptionby developers.
the need for a practical tool is becoming even more urgent with the widespread adoption of agile development which heavily relies on regression testing .
in particular although we did not account for test driven development tdd in our study the practical impact of such testing oriented software development methodologies on rts is still largely unknown.
therefore our study aims to shed light on common manual rts practices.
our study is different in scope emphasis and approach from that of greiler et al.
who recently conducted ast udy of testing practices among developers.
we did not limit our scope to a specific class of software they focus on testing component based software .
their emphasis is on answering important questions about the testing practices that are not adopted by organizations and why.
on the other hand we focus on howdevelopers perform rts.
finally their approach utilizes interviews and surveys but we analyzed data collected from developers in real time.
concerning the empirical study of rts techniques the closest work to ours is the use of field study data by orso et al.
.
they collected usage profile data from us ers of deployed software for tuning their gamma approach for rts and impact analysis.
we study data collected from developers to gain insight on improving manual rts.
other empirical studies have been conducted mainly to evaluate proposed automated rts techniques .
our st udy is not technique specific rather we investigate common manual rts practices.
although our work is based on data that has been used previously this is the first use of the data for studying how developers perform testing.
the data was previously used to show that vcs commit data is imprecise and incomplete for studying software evolution for comparing manual an d automated refactorings and for mining fine grained co de change patterns .
.
conclusionsand futurework in this paper we provided evidence on the pervasiveness of manual rts and compared manual and automated rts.
the results show that while developers commonly perform manual rts they need better support to perform manual rts while working on their typically sized projects.
in particular all but two of the developers in our three month user study performed ad hoc manual rts in .
of test sessions.
the practice of manual rts occurred regardless of project properties like project size test execution time and the size of code changes made before test runs.
further we found that manual rts is most commonly performed during debugging.
by comparing manual and automated rts we also showed how they differ in terms of safety precision and performance.
we found that manual rts compared to an automated rts tool selected to run more tests than necessary in of the test sessions that we evaluated and in of the test sessions selected fewer tests than were affected by the most recent code changes.
future work there are three main thrusts in our planned future work.
first we want to work on finding a better balance between automated rts granularity level and implementation complexity so that we can deliver a widely used tool for automated rts.
second we plan to create rts techniques that incorporate the knowledge of common debugging scenarios to improve developers debugging experience.
finally we plan to investigate how developers manually perform other activities related to regression testing such as test suite reduction and test prioritization.