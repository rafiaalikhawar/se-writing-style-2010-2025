coalescingexecutions forfastuncertaintyanalysis williamn.
sumner taobao xiangyuzhang sunilprabhakar department of computer science purdue university wsumer tbao xyzhang sunil cs.purdue.edu abstract uncertain data processing is critical in a wide range of applications such as scientific computation handling data with inevitableerrorsandfinancialdecisionmakingrelyingonhuman provided parameters.
while increasingly studied in the area of databases uncertain data processing is often carried out by software and thus software based solutions are attractive.
in particular monte carlo mc methods execute software with many samples from the uncertain inputs and observe the statistical behavior of the output.
in this paper we propose a technique to improve the cost effectiveness of mc methods.
assuming only part of the input is uncertain the certain part of the input always leads to the same execution across multiple sample runs.
we remove such redundancy by coalescing multiple sample runs in a single run.
in the coalesced run the program operates on a vector of values if uncertainty is present and a single value otherwise.
we handle cases where control flow and pointers are uncertain.
our results show that we can speed up the execution time of sample runs by an average factor of .
without precision lost or by up to .
with negligible precision lost.
categories andsubjectdescriptors d. .
automatic programming d. .
testing and debugging d. .
language constructs and features generalterms languages experimentation performance keywords uncertainty sensitivity monte carlo coalescing .
introduction uncertain data processing is becoming more and more important.
in scientific computation data are collected permission to make digital or hard copies of all or part of this w ork for personal or classroom use is granted without fee provided th at copies are not made or distributed for profit or commercial advantage and th at copies bear this notice and the full citation on the first page.
to cop y otherwise to republish to post on servers or to redistribute to lists re quires prior specific permission and or a fee.
icse may waikiki honolulu hi usa copyright acm ... .
.throughinstrumentsorsensorsthatmaybeexposedtorough environmental conditions leading to errors.
computational processing of these data may hence draw faulty conclusions.
for example it was shown that a protein was mistakenly classified as a cancer indicator by slightly altering a parameter of the program used to process experimental data .
these parameters are uncertain because they are provided by biologists based on their experience.
such mistakes could be highly costly because expensive follow up wet bench experiments could be further conducted based on the faulty protein.
one of the most widely used sources for protein data is uniprot in which proteins are annotated with their functions.
the annotations may come from real experiments accurate or computation based on protein similarity uncertain .
software operating on these data must be aware of the uncertainty .
in modern combat soldiers rely on data collected by sensors deployed on harsh battlefields.
processing unreliable data and making proper decisions is key to their survival.
software facilitating financial decision making is often required to model uncertainty .
traditionally uncertaintyanalysisis conductedon the underlying mathematical models .
however modern data processing uses more complex models and relies on computers and programs rendering mathematical analysis difficult.
realizing the importance of uncertain data processing recently researchers have proposed database techniques to store maintain and query uncertain data .
however more sophisticated data processing is often performed outside a database.
addressing uncertainty from the software engineering and program analysis perspective becomes natural.
continuity analysis is a static technique analyzing if a given program output varies in a continuous way as input changes.
while the analysis has been shown to work on simple programs like sorting algorithms applying it to programswithcomplexcomputationremainsachallenge.
taint analysis tracks the set of inputs used to compute individualoutputsthroughprogramdependencetracking.
users can thus focus their attention on the relevant inputs when analyzing uncertainty.
however it does not provide direct help as it cannot identify whether one input in the lineage set is more important than another.
monte carlo mc methods provide a simple and effective means of studying uncertainty .
they randomly select input values from predefined distributions and aggregatethecomputedoutputstoyieldstatisticalinsightsonthe output space.
they are increasingly studied by researchers in software engineering.
for instance mc methods are used to classify input values critical to the output and test !
!
!
!
!
!
!
!
!
!
!
!
.
.
figure program specialization.
assume a anda are uncertain other a i. stability of numerical implementations .
in this paper we improve the efficiency of mc methods.
mc methods require computing independent solutions or trials for many samples.
in the context of uncertain data processing often only part of the data are uncertain.
hence the sample runs have a lot redundancy dictated by the certain part of the input i.e.
the part that remains the same across sample runs.
eliminating redundant execution caused by a certain part of the input is not a new challenge.
classic solutions include partial evaluation program specialization and memoization.
however they fall short for our purpose.
in particular partial evaluation generates specialized versions of a program that can run faster.
for example suppose a function has multiple input parameters and some of the parameters often take the same values.
specialized versions of the function can be generated by replacing those parameters and computation related to them with concrete values at compile or binding time.
consider the program in fig.
a .
suppose variable xoften takes the value .
.
specialization creates the version in b .
specialization is harder where only part of an array is uncertain.
recent work specializes execution traces working at the array element level but such techniques unroll loops and generate linear code .
supposing a anda are uncertain in our example figure c shows the program after trace based specialization .
observe that statements and part of are concretized as their outcomes are certain.
trace based specialization is intended for use on functions whereas we need to specialize whole programs.
section shows that when only one input element is uncertain up to .
of the executed instructions for some programs operate on uncertain data and are hence not concretizable.
this implies the specialized programs will have a huge static size.
memoization is a technique that caches function level output for frequently occurring function inputs so that redundant computation can be pruned.
it works well when a function is called frequently in a single run and possible inputs to the function are few.
however our scenario is different in one sample run a program may call some functions a large number of times but likely with different inputs each time the same function inputs only occur across runs.
to reduce such cross run redundancy we may have to cache for all the possible function inputs in a single run.
we propose a technique called execution coalescing that packs multiple mc trials into one execution.
the approach allows users to mark program inputs that should receive samples from a random distribution.
using this information our approach automatically finds work that is common across multiple trials and coalesces it so that the common work is only done once but the resultsof that work can still be used independently in each trial.
in particular a variable !
!
figure coalesced execution.
assume three samples are taken for a and they are .
.
.
.
the state shows the left hand side values after each statement execution.
is associated with a vector if it is directly or indirectly computed from uncertain input.
assume its size is n. theith element in the vector corresponds to the value of the variable in the ith sample run.
operations on uncertain values are carried out on individual elements in the vectors.
one step forward in the packed execution is equivalent to stepping forward in the nsample runs simultaneously.
variables that are certain have only one value.
in this case stepping forward in the nsample runs is done by executing only one operationinsteadofexecuting ninstructionssimultaneously which allows eliminating redundancy.
consider the program in fig.
a .
assume the random values .
.0and1.0are chosen for a in three separate trials.
the coalesced execution of these trials together is shown in fig.
.
these different values are shown as a vector at the first instance of line .
furthermore the instances of line also have vector values as they rely on the uncertain input and may yield different results for each trial.
in contrast because the values generated at other places are shared across all trials the state of the variables is represented by individual values.
note that if the three trials are executed independently the shared evaluations must be repeated three times.
for example line needs to execute three times without coalescing but only once with it.
recent advances in hardware and software enable efficiency in execution coalescing.
our technique tracks uncertain values that need to be associated with a vector through program dependence tracking which is often the dominant overhead factor.
recent work shows that such analysis can be implemented efficiently.
moreover the intel streaming simd extensions sse instruction set for x86 architectures allowsoperatingon a vectorof valuesin onecycle.
our technique is mostly transparent to users.
they only need to indicate which input values are uncertain and their distributions.
it has no specific requirements for the subject programs and hence can be adapted to other applications such as combinatorial testing.
our current implementation supports both c and fortran programs.our contributions are summarized as follows.
we present vector based program evaluation rules for packing multiple mc trials into one execution.
it coalesces the common work between trials while producing unique and correct results for each individual trial.
the semantics handles conditional statements on uncertain values.
if necessary it executes both branches one after the other.
it also handles uncertain pointers.
we conduct formal analysis on the possible savings of our technique that allows the user to estimate benefits in different use cases.
we devise optimizations to remove unnecessary uncertainty on the fly to further improve cost effectiveness.
the optimization allows the user to exchange precision for efficiency.
our evaluation shows that high performance can be achieved with little loss of precision.
we have implemented the approach and evaluated it to examine the impact and utility of different parameters that characterize efficacy in reducing the runtime costs of monte carlo techniques.
.
coalescing executions in this section we formally define execution coalescing and discuss some important properties.
p l s s stmt s1 s2 skip x e if x thens1elses2endif while e sendwhile e expr x c x1binopx2 x1binopc input ... x var x y ... c const true false ... input c normal c c uniform c c ... figure simple kernel language lwith monte carlo sampling.
our system is built on top of gccand hence supports multiple programminglanguages.
for generality our formal discussion is facilitated with a simple kernel language presented in fig.
.
the kernel language includes no ops assignments conditional execution via ifstatements looping with while statements binary operators and both constant and variable values.
it explicitly models program input through the input function.
the function returns a certain value or a distribution.
the simplest form is a uniform distribution over a range.
for the moment the kernel language does not model functions arrays and pointers.
we will discuss how to support these features in section .
trial ... n store var const samplestore var trial const samplemask p trial figure definitions for evaluation.
.
with certain control flow we first introduce how the technique works assuming control flow is certain.
in other words we assume predicates do not operate on uncertain variables and hence differenttrials follow the same control flow.
for instance in a matrix multiplication program if only the matrix elements are uncertain execution always follows the same path.
fig.
presents the definitions relevant to program state.
symbolnrepresents the number of sample runs we want to coalesce.
we assign a unique id for each sample run.
trial represents the set of ids.
symbol represents the regular store which is a mapping from variables to constants.
to allow coalescing we introduce an extra store in the sample space denoted as .
it maps a variable to a mapping from a sample run id to a value.
intuitively for each variable it stores the values of the variable for each sample run if the variable is uncertain.
we call the mapping the vector value of the variable whereas the value in the regular store is the regular value .
symbol is used in the presence of uncertain control flow which will be discussed in the next section.
the evaluation rules are presented in table .
they specify the actions for evaluating a statement when the conditions are satisfied.
the rule names are provided in the last column.
we allow two types of input through the explicit input method.
in rule input certain if the statement reads a certain input value to variable x xis associated with the value in the regular store and with in the sample store meaning that xis undefined in the sample store.
note thatxmight have had an uncertain value before the assignment.
rule input uncertain specifies that if the input is uncertain i.e.
the input method returns a distribution n samples will be taken and stored to the sample space.
the remaining three rules specify evaluation of the assignment of a binary operation.
rule binop both uncertain applies when both source operands x1andx2have uncertain values.
according to the rule the left hand side variable x maps to a vector storing the results of the binary operation on the corresponding elements in the two source vectors.
if x1is uncertain and x2certain rule binop 1st uncertain applies and xhas a vector value.
each vector element is computed from the corresponding vector element in x1and the value of x2in the regular store.
if both source operands have certain values rule binop both certain the resulting value is computed from the regular values and updated to the regular store and the sample store of xis reset to undefined.
other rules are similarly derived and hence elided.
fig.
shows an example of such evaluation.
.
with uncertaincontrol flow in real programs there are often predicates operating on uncertain values.
in such cases it is uncertain which branch will be taken.
to handle these cases upon encountering an uncertain predicate we split the coalescing into two subcoalescings one with the sample runs following the true branch called the true coalescing and the other with those following the false branch called the false coalescing .
we evaluate them one after the other.
at the merge point of the two branches the two subcoalescings conjoin to the original coalescing.
if uncertain predicates nest a subcoalescing may further split into smaller subcoalescings1.
we have to make sure the evaluations of the split subcoalescings of the same predicate are isolated otherwise a definition in the true branch evaluation may reach a use in the following false branch evaluation of the same predicate leading to errors.
table presents an important subset of evaluation rules.
1such splittings are bounded because subcoalescings with only one trial cannot be split.table evaluation rules with certain control flow.
statement condition action name x input input returns constant c x c x input certain input returns distribution d i trial x i random d input uncertain x x1binopx2 x1 ne ationslash x2 ne ationslash i trial x i x1 i binop x2 i binop both uncertain x1 ne ationslash x2 i trial x i x1 i binop x2 binop 1st uncertain x1 x2 x x1 binop x2 x binop both certain ... ... ... table evaluation rules with uncertain control flow.
we use llbrackets rrbracketto denote the action of evaluating a statement s. we use x as the shorthand for i x i .
statement condition action name if x thens1elses2 x true llbrackets1 rrbracket if uc true i x i true j x j false t i x i true f t push t llbrackets1 rrbracket f llbrackets2 rrbracket pop if uc both ... ... ... x input input returns constant c trial x c x input c input returns constant c trial x c input c mask input returns distribution d x random d input uc x x1binopx2 x1 x2 trial x x1 binop x2 x binop both c x1 x2 trial x x1 binop x2 binop both c mask x1 ne ationslash x2 ne ationslash i x i x1 i binop x2 i binop both uc ... ... ... x x1binopc x1 trial x x1 binopc binop c const mask ... ... ... while x sendwhile llbracketif x thens while x sendwhile else skip rrbracketwhile we enhance program state with a sample mask whose definition is in fig.
.
it identifies which trials are being evaluated along the current path.
we only need to compute the values for these trials in the current path.
other trials require separate computation along different paths.
the first two rules describe the evaluation of an ifstatement predicating on uncertain values.
the first rule if uctruespecifies that even though the predicate operates on a vector value if the vector elements are universally true we only evaluate the true branch.
rule if uc both specifies that if predicate xhas both trueandfalsevalues we divide the current mask into two submasks tand f each identifying those trials that follow the true and false branches respectively.
the true branch statement s1evaluates with tands2evaluates with f. note that although the stores are not explicitly specified in the rule they update in evaluation order.
in other words the evaluation of s2operates on stores that have been updated in s1 s evaluation.
the two submasks facilitate isolation i.e.
preventing the evaluation of s2from seeing values produced in s1.
thenextthreerulesdescribeinputbehavior.
rule inputcspecifies that we save the certain input to the regular store only when is the universal set trial meaning the current coalescing has not been split i.e.
the current path is certain.
rule input c mask specifies that although the input is certain the assignment is performed on the sample store if is a subset.
more intuitively the rule dictates that if the evaluation is for a split coalescing we cannot save the value to the regular store even though it is certain.
because it is certain only regarding the sample runs indicated by it might have a different value in other sample runs.the next three rules are for the assignment of a binary operation.
rule binop both c specifies that if both operands are certain and the sample mask is the universal set we updatetheregularstoreof xandresetthesamplestore.
incontrast from binop both c mask if the sample mask is only a subset we update the sample store although both operands are certain.
note we test if an operand is certain regarding the current mask i.e.
x instead of x .
the reason is that x trial might have been defined in the evaluation along the other split branch which has no implications on whether xis certain along this branch.
rulebinop both uc specifies that if both source operands are uncertain we update the sample store constrained by .
rulebinop c const mask is similar to binop both c mask .
observe that any assignment along a split path trial only updates the sample store but never the regular store which ensures that uses of the regular store in rule binopboth c mask in a split branch never see values defined in the other branch but rather those before the split.
rulewhileevaluates a whilestatement to an ifstatement so that the rules for the ifstatement can be used.
example.
consider the program in fig.
.
it computes a person s salary based on the rate and the hours she he works.
table presents a sample evaluation.
the first column shows the control flow the second and third columns show the regular store and the sample store and the last column shows the rules applied.
here we coalesce five sample runs.
at the beginning the sample mask is the universal set.
the input at line is uncertain so we take samples.
at line we apply rule if uc both and the mask is divided.
the 1st 3rd and 5th sample runs evaluate in the1 r input h input 3if p h 4thenr r s r h if q s thens 8elser r s r h 10endif output s figure example for uncertain control flow.
the program computes salary sfrom rate rand work hoursh.
the rate is higher line if a person works for more than hours.
the salary has a cut off value lines and .
if a person works for less than hours the rate is reduced line .
true branch and the 2nd and 4th runs in the false branch.
at line even though rholds a certain value the sample store updates for the 1st 3rd and 5th runs.
at line the mask further divides into two submasks.
hence the assignment at line only updates the value for the 5th run to as highlighted.
after the true branch evaluates the false branch evaluates.
at line rule binop c const mask applies so that the 2nd and 4th elements of r s vector update.
note that the values are computed from r s value in the regular store and hence the definition to rat line has no effect on line .
at line the submasks join.
observe at the end the values in the sample store are identical to those acquired in the corresponding independent sample runs.
safety.
in the following we present the safety claim of our technique.
it is critical to show that the coalesced execution is equivalent to the nindependent mc trials.
note that an independent mc trial is a regular program execution.
assume random sampling is deterministic for each uncertain input i.e.
given the same random seed and a distribution thesame nsamplesaregenerated.
we furtherdefinethe ith independent trial as the execution with each uncertain input taking the ith sample in the sequence of n. for instance the first mc trial in table corresponds to the execution taking the inputs r and h .
property .execution coalescing is safe.
in particular at the end of program evaluation if a variable xhas x it must have the same value across the nindependent mc trials and the value is identical to x if x ne ationslash x i must be identical to the value of x in theith mc trial.
the property holds for our kernel language.
the proof is elided.
for real programs it holds when we do not consider exceptions and interrupts.
we leave it to our future work to extend the technique to handle such cases.
.
costbenefitmodel the computational benefits of execution coalescing depend on a combination factors.
in this section we present a model for estimating the benefits.
without coalescing all mc trials are executed independently.
let s first consider the case that all trials execute along the same control flow.
if there are ntrials with each trial executing iinstructions the total number of instruc tionss n is given as follows.
s n ni in comparison the cost of execution coalescing in terms of executed instructions is decided by the following factors.
the number of trials that are coalesced.
let it be n. foreachinstruction ourtechniquemustcheckwhether it operates on uncertain variables.
let the slow down factor for such checking be k. we represent the percentage of instructions operating on uncertain values as t. hence in the coalesced execution i t instructions need to execute just once for thentrials.
for each of the remaining itinstructions the operation is performed on vectors which is equivalent to executing the instruction ntimes.
there is a constant bookkeeping overhead factor m when executing an instruction on a vector.
combining all these components yields the expected cost presented in equation .
c n k t m ki nmti k nmt i we further compute the benefit factor bin equation .
b s n c n k t m ni k nmt i k n mt execution coalescing is beneficial when b .
a few observations can be made from equation .
nneeds to be larger than kandmtneeds to be smaller than .
in our implementation kis often a constant in the range .
hence we need to coalesce enough executions to make the technique beneficial.
ifmt bincreases as nincreases reflecting that if more trials are coalesced each executed instruction that is shared across trials has a greater reduction in overall work.
it is bounded by1 mt.
the benefit increases as the uncertain percentage t decreases.
this reflects that as fewer instructions operate on uncertain values more of the execution can be coalesced across trials.
it is bounded byn k. the above analysis allows us to decide if execution coalescing is appropriate and tune the configuration of n and tif possible .
for instance assume m .
andt .
.
sincemt .
it is guaranteed that coalescing provides no benefit.
moreover assume m .
t .
andk .
.
if we want to achieve a speed up b we should use n .
when uncertain control flow is considered the benefit factor is computed as follows.
b u k n mt kmu uis the ratio between the instructions that are unique in a trial over those common in all trials describing the percentage of uncertain control flow.
we assume uis a constant over all trials.
the derivation of the inequality is omitted for space.
from the equation to satisfy b the following condition must hold.
u k n mt km table sample execution for program in fig.
.
h mapsto ... meanshhas value in the 5th sample run.
control flow rule r input r mapsto input c h input h mapsto input uc 3if p h p mapsto t1 f2 t3 f4 t5 if uc both 4thenr r r mapsto binop c const mask s r h s mapsto binop both uc if q s q mapsto f1 f3 t5 if uc both thens s mapsto assgn const mask 8elser r r mapsto binop c const mask s r h s mapsto binop both uc output s table example for uncertain array indices.
assume the sample mask is a mapsto a mapsto i mapsto j mapsto .
instruction wrong correct 1a a mapsto ... a mapsto a mapsto a mapsto 2a 7 a mapsto ... a mapsto a mapsto a mapsto for each i z libfoo x y z i libfoo x i y i figure calling a library function.
while we can configure nto reduce the effect of k nin the condition the technique must not be beneficial if u mt km .
intuitively our technique cannot be beneficial when theratiobetweenthedivergentcontrolflowandthecommon control flow is higher than a threshold.
.
handling practicalissues in this section we discuss how to support more complex features that are not modeled by our language.
uncertain array indices.
extra effort is needed when uncertain values are used as array indices.
we cannot simply perform the array operation on the regular store with the addresses specified by the vector value of the index even though that seems straightforward.
instead the operation must be performed on the sample store.
specifically a vectorvneeds to be created for each address specified by the uncertain index.
only the ith element of the vector created for theith address gets updated.
the reason is that the ith address should only be used in the evaluation of the ith trial.
consider the example in table .
indices iandjare uncertain.
both can take values and but take different values within the same trial.
the second column shows that if we follow the na ve approach after evaluating line a anda map to in .
after line both map to .
the state after line however is wrong and does not correspond to the coalescing of the two trials.
proper evaluation should yield that at line both a anda map to vector values and only the 1st trial in a and the 2nd trial in a are set to .
similarly after line only the 2nd trial in a and the 1st trial in a are set to .
the state correctly represents the coalescing of the two independent trials.
table presents some of the array access rules.
we currently do not support uncertain base addresses ahas to be certain .
uncertain pointers are supported in the same way because they are equivalent to uncertain array indices.
functions.
handling user defined functions is direct.
for library functions taking uncertain arguments since we do not have their source code we cannot transform the func tions to operate on uncertain values.
our solution is to wrap the function call in a loop that iterates through each trial in the sample mask.
each iteration calls the function with the regular values in a trial extracted from the sample store .
the results are written to the corresponding elements in the result vector.
fig.
shows an example.
.
optimizationbyremovingunnecessaryuncertainty we observe that during coalesced execution there are variables that are considered uncertain having vector values but the values in the vector are identical or have only negligible differences.
we can reduce the vector to a single value so that the subsequent computation with these variables can be re coalesced.
such cases can be caused by an uncertain value going through an operator representing a many to one mapping.
multiple inputs to the operator can lead to the same output.
sample scenarios include multiply by e.g.
y x modulo operation e.g.
y x bit operations e.g.
y x 0xfff and comparisons e.g.
p x .
note that our evaluation rules are suboptimal for these cases because yis uncertain as long as xis uncertain disregarding the values in the vector of y. floating point round off overflow and underflow can also lead to identical values in vectors.
for instance consider the following statement.
y x .0e9 and assume x .
.
.
.
since the floating point representation can only hold a fixed number of the most significant digits suppose it can hold digits the contribution from xis then rounded off leading to y .0e91 .0e92 .0e93 .
recall that when we encounter an uncertain predicate wefirstevaluateasubsetoftrialsalongthetruebranch and then the remaining trials along the false branch.
any assignments inside these branches are performed on the sample store.
it is possible that a variable is defined with the same value in the two branches.
there are also cases where floating point values in a vector are highly similar although they are not completely identical.
we hence develop a general solution to leverage the above observations.
given a significance threshold k if the differences between the first value in a vector x and any other values in the vector are less than k we re coalesce the vector to a single value.
the threshold based recoalescingtable evaluation rules for uncertain array indices.
statement condition action name a c a x ne ationslash i a i c arr const wr uc y a a x ne ationslash i a i ne ationslash i y i a i arr rd uc provides a means to trade a configurable degree of precision for increased efficiency by decreasing tin equation the uncertain ratio .
note that when k we only merge identical values no precision is lost.
.
empiricalevaluation our system is built in gcc based on the gimple ir.
since our technique operates on vectors we leverage simd single instruction multiple data instructions for better performance.
in particular we use packed floating point instructions suchasaddition subtractionorcomputingsquare roots from the sse2 extension of simd instructions.
note that the benefit of sse2 instructions is limited by the width of the sse registers which is bit for our machine.
in other words we can only pack two double precision floating point values at a time.
our experiments are performed on an intel quad core xeon .86ghz machine with 4gb ram.
we use specfp2000 as the benchmark set which includes both c and fortran programs.
we excluded programs.
in particular .lucas is a program that identifies prime numbers and hence uncertainty analysis is not applicable.
.mesa is omitted because we have not supported direct assignments of aggregate types such as assignments of struct.
we have total programs c and fortran .
our first experiment evaluates the benefits of execution coalescing together with its space overhead but without the optimization removing unnecessary uncertainty.
for each program we vary the number of coalesced trials factor n in eqn in section among and .
we also vary the percentage of input marked as uncertain which will affect the percentage of instructions operating on uncertain values tin eqn among input and of the input.
for each uncertain input we select samples from of the original value following a uniform distribution.
the reason that we select samples smaller than the original is that larger samples may fall out of the legal range.
detailed results are presented in table .
details for samples are omitted for space.
the nativecolumns show the originaltime.
the t columnsshowthepercentageofstatements operating on uncertain values.
the kcolumns show the normalized overhead when only one sample is taken which corresponds to running the program on the original input w o uncertainty with the overhead of checking and propagating uncertainty the kfactor in eqn .
the b columns show the benefit factor i.e.
the speedup with the subscripts representing the number of samples.
for .mgrid and301.apsi the columns under uncertain inputpresent data for marking all inputs as uncertain due to their small number of inputs.
we also summarize the results in fig.
showing the variation in the benefit over the number of samples taking the average over the percentage of uncertain input.
note that we put in the data for samples.
fig.
further shows the variation in benefit over uncertain input taking the average over the number of samples.
fig.
summarizes the normalized space overhead details omitted for space .
from the table and the figures we make the following observations.
.
.
.
sample sample sample figure change in speedup over the number of samples.
.
.
.
.
.
.
uncertain input uncertain input uncertain input uncertain input uncertain input uncertain input figure change in speedup over uncertain input.
sample sample sample sample figure the normalized space overhead.
coalescingcan speed up mc simulationsbyan average factor ranging from .
to .
with an average of .
when samples are coalesced.
we expect the number to be high if memory is large enough to fit more samples.
the average overhead of checking and propagating uncertainty k ranges from .
to .
with an average .
.
since the nature of the uncertainty propagation is similar to dynamic information flow tracking and the state of the art reports an average times speedup if their aggressive optimizations are applied we speculate our overhead can be similarly reduced in the future.
the large kimplies that we have to coalesce sufficient trials to make it beneficial see eqn .
the speedup increases with the number of coalesced samples.
it decreases as more inputs are marked uncertain.
the decrease is not that substantial for some programs.
when tis large we hardly benefit with samples.
the space overhead ties closely with the number of coalesced samples.
it could be high when coalescing a large number of samples.
we use the standard shadowtable performance with different configurations.
program native of uncertain input uncertain input uncertain input input t kb10b20t kb10b20t kb10b20 .wupwise .
.6e6 .
.
.
.
.
.
.
.
.
.
.
.
.swim .
.6e5 .
.
.
.
.
.
.
.
.
.
.
.
.mgrid .
.
.
.
.
.
.
.
.
n a .applu .
.
.
.
.
.
.
.
.
.
.
.
.
.galgel .
.2e3 .
.
.
.
.
.
.
.
.
.
.
.
.art c .
.0e4 .
.
.
.
.
.
.
.
.
.
.
.
.equake c .
.3e3 .
.
.
.
.
.
.
.
.
.
.
.
.facerec .
.6e4 .
.
.
.
.
.
.
.
.
.
.
.
.ammp c .
.6e3 .
.
.
.
.
.
.
.
.
.
.
.
.fma3d .
.
.
.
.
.
.
.
.
.
.
.
.
.sixtrack .
.9e4 .
.
.
.
.
.
.
.
.
.
.
.
.apsi .
.
.
.
.
.
.
.
.
n a program10 uncertain input uncertain input uncertain input t kb10b20t kb10b20t kb10b20 .wupwise .
.
.
.
.
.
.06n a .
.
.04n a .swim .
.
.
.
.
.
.
.
.
.
.
.
.mgrid n a n a n a .applu .
.
.
.
.
.
.
.
.
.
.
.
.galgel .
.
.
.
.
.
.
.
.
.
.
.
.art .
.
.
.
.
.
.
.
.
.
.
.
.equake .
.
.
.
.
.
.
.
.
.
.
.
.facerec .
.
.
.
.
.
.
.
.
.
.
.
.ammp .
.
.
.
.
.
.
.
.
.
.
.
.fma3d .
.
.
.
.
.
.
.
.
.
.
.
.sixtrack .
.
.
.
.
.
.
.
.
.
.
.
.apsi n a n a n a memory allocation strategy to allocate shadow space for a page when any address in that page is used by the original program.
we expect a more sophisticated strategy would reduce the overhead.
thesecondexperimentshowstheeffectofoptimizingaway the unnecessary uncertainty.
here we randomly select one configuration samples and single uncertain input.
we vary the re coalescing threshold k i.e.
if the variations of all elements in a vector are smaller than k we re coalesce the vector back to one value.
fig.
presents the speedup results.
table presents the resulting output errors due to re coalescing.
we do not collect output errors for the programs that do not benefit from the optimization.
observe the optimization is not always applicable depending on programsemantics seesection5 .
insuchcases wehavetopay the overhead for testing variations which explains the slight degradation for .equake in fig.
.
for some programs it substantially improves the speedup factor e.g.
from .
to .
for .wupwise with little lost in precision.
note that a larger kleads to a better speedup.
the precision lost also slightly increases.
our experience with the few other randomly selected configurations also shows similar results.
the third experiment observes the effectiveness of mc simulation.
here we vary the uncertain inputs and observe the output variations.
we randomly select .equake .
the program simulates the propagation of elastic waves in large basins.
an unstructured mesh consists of nodes and linear tetrahedral elements is used to model the earthquake area.
the program computes the displacements for each individual node.
we vary the altitude of a node from to of its original value 2and observe the outputs.
fig.
presents the outputs with the most substantial displacement 2weusedtheprovidedtestinputandvariednode977 which is randomly selected from those in the source of the quake.
.
.
.
.
.
.
.
.
no merging threshold 1e threshold 1e threshold 1e threshold 1e threshold 1e figure speedup by re coalescing with different thresholds.
values and their variations over the input changes.
each curve represents one output.
we can observe that many of these outputs change irregularly and substantially.
traditional uncertainty analysis based on monotonicity linearity and continuity would fail in this case.
furthermore we observe that a few percent change of the inputs may lead to substantial output changes.
the variations could be a few times larger than the origianl values.
case study.
in the following we further study the case and explain the substantial and irregular output variations by connecting them to uncertain control flow and uncertain array indices.
for each node i its position is defined by a tuple of latitude longitude altitude stored in c c c wherec is the altitude.
the altitude of node is .
.
we look at the samples from to of the original altitude value to explain the impact.
initially we have a vector for the altitude of node c mapsto .
c mapsto .
.
... .
.table the average and maximum output error after merging for samples and uncertain input.
programthreshold 1e threshold 1e threshold 1e threshold 1e threshold 1e avg max avg max avg max avg max avg max .wupwise .6e .6e .7e .5e .7e .6e .7e .1e .1e .6e .swim .0e .8e .0e .8e .0e .8e .8e .3e .8e .3e .art .4e .1e .4e .1e .4e .1e .4e .1e .4e .1e .facerec .4e .6e .3e .3e .3e .3e .3e .3e .3e .3e .ammp .3e .3e .1e .6e .1e .6e .1e .6e .7e .5e .fma3d .0e .9e .0e .9e .0e .9e .4e .0e .4e .0e .sixtrack .6e .6e .2e .3e .4e .2e .4e .2e .4e .2e .apsi .3e .0e .7e .7e .0e .0e .8e .9e .8e .0e 4e 3e 2e 1e 040e 001e 042e figure output variations for .equake .
search for the node closest to the point source bigdist1 .
for i i archnodes i c0 c c0 c c0 c d1 distance c0 src.xyz if d1 bigdist1 bigdist1 d1 src.sourcenode i one step of the computation selects the closest node to the given earthquake source point in src.xyz in the above code.
all the nodes are traversed with a loop between line and shown above .
in the 979th iteration at line c0 receives the vector .
.
... .
.
returning from the function distance d1at line gets the vector of values .
.
.
.
... .
.
.
the variable bigdist1 holds the value .
.
therefore we encounter an uncertain predicate d1 bigdist1 at line .
the sample mask is divided into two submasks t ... and f .
according to the evaluation rules in table we compute the values of bigdist1 andsrc.sourcenode in both branches with tand frespectively.
after line the values in the sample stores are joined together i.e.bigdist1 mapsto .
.
.
.
... .
.
.
src.sourcenode mapsto ... .
after the loop the node closest to the source point is selected and stored in variable src.sourcenode .
now we havesrc.sourcenode mapsto ... .
thus for the first trials the computed source node is node while for the last trials it is node .
another code snippet shows that uncertain array indices add to the irregularity.
variable corholds a certain value at line .
when it equals or the coalescing splits after evaluating the branch at line .
for instance whencorequals at line we get a submask of so only the 8th 9th and 10th elements of vertices s vector get updated according to the evaluation rules in table .
similarly the 1st to 7th elements of the vector get updated whencorequals .
for i i num elems i if cor src.sourcenode ... vertices c in later phases the sub coalescings are further divided because of other predicates.
this implies that we observe discontinuity or different trends between outputs belonging to different sub coalescings because they go through differentcomputation.
incontrast outputsbelongingtothesame sub coalescing often have continuity or even monotonicity.
these explain the curves in fig.
where monotonic segments are observed but irregularity exists across segments.
.
relatedwork parallelized monte carlo.
parallelization has improved the efficiency of mc techniques in the context of specific applications .
incontrast ourapproachisfullyautomated and works on already developed programs.
additionally we don t rely on parallelization to improve running time instead we reduce the common redundant work across monte carlo trials.
hence it is orthogonal to parallelization.
partial evaluation memoization.
partial evaluation generates a new version of a program where additional assumptionsaboutruntimebehaviorallowmoreaggressiveoptimization .
memoization caches the results of a function for given input such that the results can be reused instead of reevaluating the function.
as noted in section these are useful in program optimization but they cannot realistically handle the combining of multiple disparate executions.
deltaexecution.
deltaexecution eliminatesredundancy across state explorations in model checking.
each type in the program is wrapped in a container type that maintains a vector of the original type.
at runtime operations translate to operations on these container types.
the technique closely ties into model checking.
the savings largely come from places specific to model checking such as state comparison to avoid repeated state exploration.
they also handle control flow splitting differently.
leveraging state management in the model checker they allow split branches to work on isolated stores.
branches only merge at the end of a method.
in contrast we cannot afford checkpointing and restoration thus our split branches operate on the samestores and achieve isolation through careful evaluation rules.
we also aggressively merge split branches at the join point.
a variant of delta execution also relates .
while our approach enables execution of one program on multiple inputs at once the variant enables execution of multiple programs on one input at once efficiently finding differences.
uncertainty analysis.
uncertainty and sensitivity analyses compose one client field of monte carlo techniques.
other efforts provide a means of partially automating these analyses efficiently.
these approaches include model checkingandtheoremproving automateddifferentiation and controlled perturbation .
they have difficulty handling data structures with heterogeneous data certain and uncertain or multiple uncertain variables which is noted as one of the reasons to use sampling based approaches .
.
conclusions we propose a technique that can coalesce multiple monte carlo sample executions into one.
we leverage the observation that these sample runs often share a lot of common executionandhencecoalescingavoidsrepeatingthecommon execution.
coalescing is achieved by allowing the program to operate on a single value if it is the same across all the coalesced runs and on a vector otherwise.
our technique executes both the true and false branches of a predicate in an isolated fashion if its vector values can be both true and false.
pointers array indices and function calls on vector values are also handled safely such that the coalesced run produces the same results as independent runs.
our evaluation shows that we can speed up the runtime of sample runs by an average factor of .
without precision lost or by up to .
with negligible precision lost.
.