learning natural coding conventions miltiadis allamanis earl t. barr christian bird charles sutton school of informatics dept.
of computer science microsoft research university of edinburgh university college london microsoft edinburgh eh8 9ab uk london uk redmond wa usa m.allamanis csutton ed.ac.uk e.barr ucl.ac.uk christian.bird microsoft.com abstract every programmer has a characteristic style ranging from preferences about identifier naming to preferences about object relationships and design patterns.
coding conventions define a consistent syntactic style fostering readability and hence maintainability.
when collaborating programmers strive to obey a project s coding conventions.
however one third of reviews of changes contain feedback about coding conventions indicating that programmers do not always follow them and that project members care deeply about adherence.
unfortunately programmers are often unaware of coding conventions because inferring them requires a global view one that aggregates the many local decisions programmers make and identifies emergent consensus on style.
we present natural ize a framework that learns the style of a codebase and suggests revisions to improve stylistic consistency.
naturalize builds on recent work in applying statistical natural language processing to source code.
we apply naturalize to suggest natural identifier names and formatting conventions.
we present four tools focused on ensuring natural code during development and release management including code review.
naturalize achieves accuracy in its top suggestions for identifier names and can even transfer knowledge about conventions across projects leveraging a corpus of open source projects.
we used naturalize to generate 18patches for 5open source projects 14were accepted.
.
introduction to program is to make a series of choices ranging from design decisions like how to decompose a problem into functions to the choice of identifier names and how to format the code.
while local and syntactic the latter are important names connect program source to its problem domain formatting decisions usually capture control flow .
together naming and formatting decisions determine the readability of a program s source code increasing a codebase s portability its accessibility to newcomers its reliability and its maintainability .
apple s recent infamous bug in its handling of ssl certificates exemplifies the impact that formatting can have on reliability.
maintainability is especially important since developers spend the majority of their time maintaining code .
a convention is an equilibrium that everyone expects in interactions that have more than one equilibrium .
for us coding conventions arise out of the collision of the stylistic choices of programmers.
a coding convention is a syntactic restriction not imposed by a programming language s grammar.
nonetheless these choices are important enough that they are enforced by software teams.
indeed our investigations indicate that developers enforce such coding conventions rigorously with roughly one third of code reviews containing feedback about following them section .
.like the rules of society at large coding conventions fall into two broad categories laws explicitly stated and enforced rules andmores unspoken common practice that emerges spontaneously.
mores pose a particular challenge because they arise spontaneously from emergent consensus they are inherently difficult to codify into a fixed set of rules so rule based formatters cannot enforce them and even programmers themselves have difficulty adhering to all of the implicit mores of a codebase.
furthermore popular code changes constantly and these changes necessarily embody stylistic decisions sometimes generating new conventions and sometimes changing existing ones.
to address this we introduce the coding convention inference problem the problem of automatically learning the coding conventions consistently used in a body of source code.
conventions are pervasive in software ranging from preferences about identifier names to preferences about class layout object relationships and design patterns.
in this paper we focus as a first step on local syntactic conventions namely identifier naming and formatting.
these are particularly active topics of concern among developers for example almost one quarter of the code reviews that we examined contained suggestions about naming.
we introduce naturalize a framework that solves the coding convention inference problem for local conventions offering suggestions to increase the stylistic consistency of a codebase.
naturalize can also be applied to infer rules for existing rule based formatters.
naturalize is descriptive not prescriptive1 it learns what programmers actually do.
when a codebase does not reflect consensus on a convention naturalize recommends nothing because it has not learned anything with sufficient confidence to make recommendations.
the naturalness insight of hindle et al.
building on gabel and su is that most short code utterances like natural language utterances are simple and repetitive.
large corpus statistical inference can discover and exploit this naturalness to improve developer productivity and code robustness.
we show that coding conventions are natural in this sense.
learning from local context allows naturalize to learn syntactic restrictions or sub grammars on identifier names like camelcase or underscore and to unify names used in similar contexts which rule based code formatters simply cannot do.
intuitively naturalize works by identifying identifier names or formatting choices that are surprising according to a probability distribution over code text.
when surprised naturalize determines if it is sufficiently confident to suggest a renaming or reformatting that is less surprising it unifies the surprising choice with one that is preferred in similar contexts elsewhere in its training set.
naturalize isnot 1prescriptivism is the attempt to specify rules for correct style in language e.g.
strunk and white .
modern linguists studiously avoid prescriptivist accounts observing that many such rules are routinely violated by noted writers.
1arxiv .4182v3 apr 2014automatic it assists a developer since its suggestions both renaming and even formatting as in python or apple s aforementioned ssl bug are potentially semantically disruptive and must be considered and approved.
naturalize s suggestions enable a range of new tools to improve developer productivity and code quality a pre commit script that rejects commits that excessively disrupt a codebase s conventions a tool that converts the inferred conventions into rules for use by a code formatter an eclipse plugin that a developer can use to check whether her changes are unconventional and a style profiler that highlights the stylistic inconsistencies of a code snippet for a code reviewer.
naturalize draws upon a rich body of tools from statistical natural language processing nlp but applies these techniques in a different.
nlp focuses on understanding andgenerating language but does not ordinarily consider the problem of improving existing text.
the closest analog is spelling correction but that problem is easier because we have strong prior knowledge about common types of spelling mistakes.
an important conceptual dimension of our suggestion problems also sets our work apart from mainstream nlp.
in code rare names often usefully signify unusual functionality and need to be preserved.
we call this the sympathetic uniqueness principle sup unusual names should be preserved when they appear in unusual contexts.
we achieve this by exploiting a special token unkthat is often used to represent rare words that do not appear in the training set.
our method incorporates sup through a clean straightforward modification to the handling of unk.
because of the zipfian nature of language unkappears in unusual contexts and identifies unusual tokens that should be preserved.
section demonstrates the effectiveness of this method at preserving such names.
additionally handling formatting requires a simple but novel method of encoding formatting.
asnaturalize detects identifiers that violate code conventions and assists in renaming the most common refactoring it is the first tool we are aware of that uses nlp techniques to aid refactoring.
the techniques that underlie naturalize are language independent and require only identifying identifiers keywords and operators a much easier task than specifying grammatical structure.
thus naturalize is well positioned to be useful for domain specific or esoteric languages for which no convention enforcing tools exist or the increasing number of multi language software projects such as web applications that intermix java css html and javascript.
to the best of the authors knowledge this work is the first to address the coding convention inference problem to suggest names and formatting to increase the stylistic coherence of code and to provide tooling to that end.
our contributions are we built naturalize the first framework that presents a solution to the coding convention inference problem for local conventions including identifier naming and formatting and suggests changes to increase a codebase s adherence to its own conventions we offer four tools built on naturalize all focused on release management an under tooled phase of the development process.
naturalize achieves accuracy in its top suggestions for identifier names and never drops below a mean accuracy of when making formatting suggestions and we demonstrate that coding conventions are important to software teams by showing that empirically programmers enforce conventions heavily through code review feedback and corrective commits and patches that were based on natural izesuggestions have been incorporated into 5of the most popular open source java projects on github of the 18patches that we submitted 14were accepted.tools are available at groups.inf.ed.ac.uk naturalize .
.
motiv ating example both industrial and open source developers often submit their code for review prior to check in .
consider the example of the class shown in figure which is part of a change submitted for review by a microsoft developer on february 17th .
while there is nothing functionally wrong with the class it violates the coding conventions of the team.
a second developer reviewed the change and suggested that res andstr do not convey parameter meaning well enough the constructor line is much too long and should be wrapped.
in the checked in change all of these were addressed with the parameter names changed to queryresults andquerystrings .
consider a scenario in which the author had access to naturalize .
the author might highlight the parameter names and ask naturalize to evaluate them.
at that point it would have not only have identified res andstr as names that are inconsistent with the naming conventions of parameters in the codebase but would also have suggested better names.
the author may have also thought to himself is the constructor on line too long?
or should the empty constructor body be on it s own line and should it have a space inside?
here again naturalize would have provided immediate valuable answers based on the the conventions of the team.
naturalize would indicate that the call to the base constructor should be moved to the next line and indented to be consonant with team conventions and that in this codebase empty method bodies do not need their own lines.
furthermore it would indicate that some empty methods contain one space between the braces while others do not so there is no implicit convention to follow.
after querying naturalize about his stylistic choices the author can then be confident that his change is consistent with the norms of the team and is more likely to be approved during review.
furthermore by leveraging naturalize fellow project members wouldn t need to be bothered by questions about conventions nor would they need to provide feedback about conventions during review.
we have observed that such scenarios occur in open source projects as well.
.
use cases and tools coding conventions are critical during release management which comprises committing reviewing and promoting including releases changes either patches or branches.
this is when a coder s idiosyncratic style isolated in her editor during code composition comes into contact with the styles of others.
the outcome of this interaction strongly impacts the readability and therefore the maintainability of a codebase.
compared to other phases of the development cycle like editing debugging project management and issue tracking release management is under tooled.
code conventions are particularly pertinent here and lead us to target three use cases a developer preparing an individual commit or branch for review or promotion a release engineer trying to filter out needless stylistic diversity from the flood of changes and a reviewer wishing to consider how well a patch or branch obeys community norms.
any code modification has a possibility of introducing bugs .
this is certainly true of a system like naturalize that is based on statistical inference even when as we always assume all ofnaturalize s suggestions are approved by a human.
because of this risk the gain from making a change must be worth its cost.
for this reason our use cases focus on times when the code is already being changed.
to support our use cases we have built four tools devstyle aplugin for eclipse ide that gives suggestions for identifier renaming and formatting both for a single identifier 21public class executionqueryresponse executionqueryresponsebasic queryresults public executionqueryresponse queryresults res ireadonlycollection string str executionstepmetrics metrics base res str metrics figure a c class added by a microsoft developer that was modified due to requests by a reviewer before it was checked in.
figure a screenshot of the devstyle eclipse plugin.
the user has requested suggestion for alternate names of the each argument.
or format point and for the identifiers and formatting in a selection of code.
styleprofile acode review assistant that produces a profile that summarizes the adherence of a code snippet to the coding conventions of a codebase and suggests renaming and formatting changes to make that snippet more stylistically consistent with a project.
genrule arule generator for eclipse s code formatter that generates rules for those conventions that naturalize has inferred from a codebase.
stylish?
a high precision pre commit script forgit that rejects commits that have highly inconsistent and unnatural naming or formatting within a project.
thedevstyle plugin offers two types of suggestions single point suggestion under the mouse pointer and multiple point suggestion via right clicking a selection.
a screenshot from devstyle is shown in figure .
for single point suggestions devstyle displays a ranked list of alternatives to the selected name or format.
ifdevstyle has no suggestions it simply flashes the current name or selection.
if the user wishes she selects one of the suggestions.
if it is an identifier renaming devstyle renames alluses within scope of that identifier under its previous name.
this scope traversal is possible because our use cases assume an existing and compiled codebase.
formatting changes occur at the suggestion point.
multiple point suggestion returns a style profile a ranked list of the topkmost stylistically surprising naming or formatting choices in the current selection that could benefit from reconsideration.
by default k 5based on hci considerations .
to accept a suggestion here the user must first select a location to modify then select from among its top alternatives.
the styleprofile tool outputs a style profile.
genrule section .
generates settings for the eclipse code formatter.
finally stylish?
is a filter that uses eclipse code formatter with the settings from genrule to accept or reject a commit based on its style profile.
naturalize uses an existing codebase called a training corpus as a reference from which to learn conventions.
commonly the training corpus will be the current codebase so that naturalize learns domain specific conventions related to the projects.
alternatively naturalize comes with a pre packaged suggestion model that has been trained on a corpus of popular vibrant projects that presumably embody good coding conventions.
developers can use this engine if they wish to increase their codebase s adherence to a larger community s consensus on best practice.
here again we avoid normative comparison of coding conventions and do not force the user to specify their desired conventions explicitly.
instead the user specifies a training corpus and this is used as an implicit source of desired conventions.
the naturalize framework and tools are available at groups.inf.ed.ac.uk naturalize .
.
the n aturalize framework in this section we introduce the generic architecture of naturalize which can be applied to a wide variety of different types of conventions.
figure illustrates its architecture.
the input is a code snippet to be naturalized.
this snippet is selected based on the user input in a way that depends on the particular tool in question.
for example in devstyle if a user selects a local variable for renaming the input snippet would contain all ast nodes that reference that variable section .
.
the output of naturalize is a short list of suggestions which can be filtered then presented to the programmer.
in general a suggestion is a set of snippets that may replace the input snippet.
the list is ranked by a naturalness score that is defined below.
alternately the system can return a binary value indicating whether the code is natural so as to support applications such as stylish?
.
the system makes no suggestion if it deems the input snippet to be sufficiently natural or is unable to find good alternatives.
this reduce the clippy effect where users ignore a system that makes too many bad suggestions2.
in the next section we describe each element in the architecture in more detail.
terminology a language model lm is a probability distribution over strings.
given any string x x0 x1...x m where each xiis a token a lm assigns a probability p x .letgbe the grammar of a programming language.
we use xto denote a snippet that is a stringxsuch that x l g for some strings .
we primarily consider snippets that that are dominated by a single node in the ast.
we use xto denote the input snippet to the framework and y zto denote arbitrary snippets3.
.
the core of n aturalize the architecture contains two main elements proposers and the scoring function.
the proposers modify the input code snippet to produce a list of suggestion candidates that can replace the input snippet.
in the example from figure each candidate replaces all occurrences of res with a different name used in similar contexts elsewhere in the project such as results orqueryresults .
in principle many implausible suggestions could ensue so in practice proposers contain filtering logic.
ascoring function sorts these candidates according to a measure of naturalness.
its input is a candidate snippet and it returns a real number measuring naturalness.
naturalness is measured with respect to a training corpus that is provided to naturalize thus allowing us to follow our guiding principle that naturalness must be measured with respect to a particular codebase.
for example the training corpus might be the set of source files afrom the current application.
a powerful way to measure the naturalness of a snippet is provided by statistical language modeling.
we use pa y to indicate the probability that the language model p which has been trained on the corpus a assigns to the string y. the key intuition is that an lm pais trained so that it assigns high probability to strings in the training corpus i.e.
snippets with higher log probability are more like the training corpus and presumably more natural.
there are several key reasons why statistical language models are a powerful approach for modeling coding conventions.
2in extreme cases such systems can be so widely mocked that they are publicly disabled by the company s ceo in front of a cheering audience .
3the application of naturalize to academic papers in software engineering is left to future work.
3training corpus other code from project code for reviewcandidatestop suggestionsscoring function ngram language model svm proposers rename identifiers add formatting public void testrunreturnsresult printstream oldout system.out system.setout new printstream new outputstream override public void write int arg0 throws ioexception try testresult result junit.textui.testrunner.run new testsuite asserttrue result.wassuccessful finally system.setout oldout public void testrunreturnsresult printstream oldout system.out system.setout new printstream new outputstream override public void write int arg0 throws ioexception try testresult result junit.textui.testrunner.run new testsuite asserttrue result.wassuccessful finally system.setout oldout public void testrunreturnsresult printstream oldout system.out system.setout new printstream new outputstream override public void write int arg0 throws ioexception try testresult result junit.textui.testrunner.run new testsuite asserttrue result.wassuccessful finally system.setout oldout public void testrunreturnsresult printstream oldout system.out system.setout new printstream new outputstream override public void write int arg0 throws ioexception try testresult result junit.textui.testrunner.run new testsuite asserttrue result.wassuccessful finally system.setout oldout public void testrunreturnsresult printstream oldout system.out system.setout new printstream new outputstream override public void write int arg0 throws ioexception try testresult result junit.textui.testrunner.run new testsuite asserttrue result.wassuccessful finally system.setout oldout public void testrunreturnsresult printstream oldout system.out system.setout new printstream new outputstream override public void write int arg0 throws ioexception try testresult result junit.textui.testrunner.run new testsuite asserttrue result.wassuccessful finally system.setout oldout public void testrunreturnsresult printstream oldout system.out system.setout new printstream new outputstream override public void write int arg0 throws ioexception try testresult result junit.textui.testrunner.run new testsuite asserttrue result.wassuccessful finally system.setout oldout figure the architecture of naturalize a framework for learning coding conventions.
a contiguous snippet of code is selected for review through the user interface.
a set of proposers returns a set of candidates which are modified versions of the snippet e.g.
with one local variable renamed.
the candidates are ranked by a scoring function such as an n gram language model which returns a small list of top suggestions to the interface sorted by naturalness.
first probability distributions provide an easy way to represent soft constraints about conventions.
this allows us to avoid many of the pitfalls of inflexible rule based approaches.
second because they are based on a learning approach lms can flexibly adapt to the conventions in a new project.
intuitively because paassigns high probability to strings t athat occur in the training corpus it also assigns high probability to strings that are similar to those in the corpus.
so the scoring function stends to favor snippets that are stylistically consistent with the training corpus.
we score the naturalness of a snippet y y1 nas s y pa nlogpa y that is we deem snippets that are more probable under the lm as more natural in the application a. equation is cross entropy multiplied by 1to makesa score where s x s y impliesxis more natural thany.
where it creates no confusion we write s y eliding the second argument.
when choosing between competing candidate snippetsyandz we need to know not only which candidate the lm prefers but how confident it is.
we measure this by a gap function g which is the difference in scores g y z p s y p s z p .
becausesis essentially a log probability gis the log ratio of probabilities between yandz.
for example when g y z 0the snippet yis more natural i.e.
less surprising according to the lm and thus is a better suggestion candidate than z. ifg y z then both snippets are equally natural.
now we define the function suggest x c k t that returns the top candidates according to the scoring function.
this function returns a list of top candidates or the empty list if no candidates are sufficiently natural.
the function takes four parameters the input snippetx the listc c1 c2 ...c r of candidate snippets and two thresholds k n the maximum number of suggestions to return andt r a minimum confidence value.
the parameter kcontrols the size of the ranked list that is returned to the user whiletcontrols the suggestion frequency that is how confident naturalize needs to be before it presents any suggestions to the user.
appropriately setting tallows naturalize to avoid the clippy effect by making no suggestion rather than a low quality one.
below we present an automated method for selecting t. thesuggest function first sorts c c1 c2 ...c r the candidate list according to s sos c1 s c2 ... s cr .
then it trims the list to avoid overburdening the user it truncates cto include only the top kelements so that length c min k r .
and removes candidates ci cthat are not sufficiently more natural than the original snippet formally it removes all cifromcwhere g ci x t. finally if the original input snippet xis the highest ranked inc i.e.
ifc1 x suggest ignores the other suggestions setsc to decline to make a suggestion and returns c. binary decision if an accept reject decision on the input xis required e.g.
as in stylish?
naturalize must collectively consider all of the locations in xat which it could make suggestions.
we propose a score function for this binary decision that measures how good is the best possible improvement that naturalize is able to make.
formally let lbe the set of locations in xat which naturalize is able to make suggestions and for each lscript l let c lscriptbe the system s set of suggestions at lscript.
in general c lscriptcontains name or formatting suggestions.
recall that pis the language model.
we define the score g x p max lscript lmax c c lscriptg c x .
ifg x p t then naturalize rejects the snippet as being excessively unnatural.
the threshold tcontrols the sensitivity of naturalize to unnatural names and formatting.
as tincreases fewer input snippets will be rejected so some unnatural snippets will slip through but as compensation the test is less likely to reject snippets that are in fact well written.
setting the confidence threshold the thresholds in the suggest function and the binary decision function are on log probabilities of strings which can be difficult for users to interpret.
fortunately these can be set automatically using the false positive rate fpr the proportion of snippets xthat in fact follow convention but that the system erroneously rejects.
we would like the fpr to be as small as possible but unless we wish the system to make no suggestions at all we must accept some false positives.
so instead we set a maximum acceptable fpr and we search for a threshold tthat ensures that the fpr of naturalize is at most .
this is a similar logic to statistical hypothesis testing.
to make this work we need to estimate the fpr for a given t. to do so we select a random set of snippets from the training corpus e.g.
random method bodies and compute the proportion of the random snippets that are rejected usingt.
again leveraging our assumption that our training corpus contains natural code this proportion estimates the fpr.
we use a grid search to find the greatest value of t the userspecified acceptable fpr bound.
.
choices of scoring function the generic framework described in section .
can in principle employ a wide variety of machine learning or nlp methods for its scoring function.
indeed a large portion of the statistical nlp literature focuses on probability distributions over text including language models probabilistic grammars and topic models.
very few of these models have been applied to code exceptions include .
we choose to build on statistical language models because previous work of hindle et al.
.
has shown that they are particularly able to capture the naturalness of code.
the intuition behind language modeling is that since there is an infinite number of possible strings obviously we cannot store a probability value for every one.
different lms make different simplifying assumptions to make the modeling tractable and will determine the types of coding conventions that we are able to infer.
one of the most effective practical lms is the n gram language model.n gram models make the assumption that the next token can be predicted using only the previous n 1tokens.
formally the probability of a token ym conditioned on all of the previous tokens y1...y m is a function only of the previous n 1tokens.
under this assumption we can write p y1...y m m productdisplay m 1p ym ym ...y m n .
to use this equation we need to know the conditional probabilities p ym ym ...y m n for each possible n gram.
this is a table ofvnnumbers where vis the number of possible lexemes.
these are the parameters of the model that we learn from the training corpus.
the simplest way to estimate the model parameters is to setp ym ym ...y m n to the proportion of times that ymfollowsym ...y m n .
however in practice this simple estimator does not work well because it assumes that n grams that do not occur in the training corpus have zero probability.
instead n gram models are trained using smoothing methods .
in our work we use katz smoothing.
cross project language models as we show shortly an ngram model is a good scoring function for making suggestions but we can do better.
lms are flexible and can exploit other sources of information beyond local context when scoring a lexeme.
for example if we have a global set of projects gchosen for their good coding style we would like to transfer information about the style of gto the current application.
we do this by using a cross project lm .
we train two separate n gram models a global model pgtrained ong and a local model trained only on the current application a. then we create a new lm pcpby averaging pcp y pg y pa y where is a parameter that can be set using a development set.
the cross project model transfers information from g because for ann gramzto have high probability according to pcp it must have high probability according to both pgandpa.
incorporating syntactic context our approach can also flexibly incorporate higher level information such as syntactic context.
in java for example type information is useful for predicting identifier names.
a simple way to exploit this is to learn a probability model over tokens based on context.
given an ast tand a location iin the snippet y let t i be a function that returns a feature vector describing the current syntactic context without including the identity of the token yi.
for example if yinames a variable t i might include the variable s type determined statically via compilation.
more generally could include information about the current nesting level whether the token is within an anonymous inner class and so on.
we learn a probabilistic classifier psyn yi ti on the same training set as for the lm.
then we can define a new score function s y lv logp y nn summationdisplay i 1log psyn yi t i which combines the n gram model pwith the classifier psynthat incorporates syntactic context.
implementation when ann gram model is used we can compute the gap function g y z very efficiently.
this is because when gis used within suggest ordinarily the strings yandzwill be similar i.e.
the input snippet and a candidate revision.
the key insight is that in an n gram model the probability p y of a snippet y y1y2...y n depends only on the multiset of n grams that occur iny that is ng y yiyi ...y i n i n n .
an equivalent way to write a n gram model is p y productdisplay a1a2...an ng y p an a1 a2 ...a n .
since the gap function is g y z log anyn grams that are members both of ng y andng z cancel out in the quotient and to compute g we only need to consider those n grams that are not in ng y ng z .
intuitively this means that to compute the gap function g y z we need to examine the n grams around the locations where the snippets yandzdiffer.
this is a very useful optimization if yandzare long snippets that differ in only a few locations.
when training an lm we take measures to deal with rarelexemes since by definition we do not have much data about them.
we use a preprocessing step a common strategy in language modeling that builds a vocabulary with all the identifiers that appear more than once in the training corpus.
let count v b return the number of appearances of token vin the codebase b. then if a token has count v b 1we convert it to a special token which we denote unk.
then we train the n gram model as usual.
the effect is that theunktoken becomes a catchall that means the model expects to see a rare token even though it cannot be sure which one.
.
suggesting natural names in this section we instantiate the core naturalize framework for the task of suggesting natural identifier names.
we start by describing the single suggestion setting.
for concreteness imagine a user of the devstyle plugin who selects an identifier and asks devstyle for its top suggestions.
it should be easy to see how this discussion can be generalized to the other use cases described in section .
.
let vbe the lexeme selected by the programmer.
this lexeme could denote a variable a method call or a type.
when a programmer binds a name to an identifier and then uses it she implicitly links together all the locations in which that name appears.
let ldenote this set of locations that is the set of locations in the current scope in which the lexeme vis used.
for example ifvdenotes a local variable then lvwould be the set of locations in which that local is used.
now the input snippet is constructed by finding a snippet that subsumes all of the locations inlv.
specifically the input snippet is constructed by taking the lowest common ancestor in ast of the nodes in lv.
the proposers for this task retrieve a set of alternative names to v which we denote av by retrieving other names that have occurred in the same contexts in the training set.
to do this for every location lscript lvin the snippet x we take a moving window of length n around lscriptand copy all the n gramswithat contain that token.
call this setcvthe context set i.e.
the set ofn gramswiofxthat contain the token v. now we find all n grams in the training set that are similar to an n gram incvbut that have some other lexeme substituted for v. formally we set avas the set of all lexemes v primefor which v cvand v prime occurs in the training set.
this 5guarantees that if we have seen a lexeme in at least one similar context we place it in the alternatives list.
additionally we add toavthe special unktoken the reason for this is explained in a moment.
once we have constructed the set of alternative names the candidates are a list svof snippets one for each v prime av in which all occurrences of vinxare replaced with v prime.
the scoring function can use any model pa such as the n gram model equation .
n gram models work well because intuitively they favors names that are common in the context of the input snippet.
as we demonstrate in section this does notreduce to simply suggesting the most common names such as iandj.
for example suppose that the system is asked to propose a name for res in line of figure .
the n gram model is highly unlikely to suggest i because even though the name iis common the trigram queryresults i is rare.
an interesting subtlety involves names that actually should be unique.
identifier names have a long tail meaning that most names are individually uncommon.
it would be undesirable to replace every rare name with common ones as this would violate the sympathetic uniqueness principle.
fortunately we can handle this issue in a subtle way recall from section .
that during training of the n gram language model we convert rare names into the special unktoken.
when we do this unkexists as a token in the lm just like any other name.
so we simply allow naturalize to return unkas a suggestion exactly the same as any other name.
returning unkas a suggestion means that the model expects that it would be natural to use a rare name in the current context.
the reason that this preserves rare identifiers is that the unktoken occurs in the training corpus specifically in unusual contexts where more common names were not used.
therefore if the input lexeme voccurs in an unusual context this context is more likely to match that of unkthan of any of the more common tokens.
multiple point suggestion it is easy to adapt the system above to the multiple point suggestion task.
recall section .
that this task is to consider the set of identifiers that occur in a region xof code selected by the user and highlight the lexemes that are least natural in context.
for single point suggestion the problem is to rank different alternatives e.g.
different variable names for the same code location whereas for multiple point suggestion the problem is to rank different code locations against each other according to how much they would benefit from improvement.
in principle a score function could be good at the single source problem but bad at the multiple source problem e.g.
if the score values have a different dynamic range when applied at different locations.
we adapt naturalize slightly to address the multiple point setting.
for all identifier names vthat occur in x we first compute the candidate suggestions svas in the single suggestion case.
then the full candidate list for the multiple point suggestion is s v xsv each candidate arises from proposing a change to one name inx.
for the scoring function we need to address the fact that some names occur more commonly in xthan others and we do not want to penalize names solely because they occur more often.
so we normalize the score according to how many times a name occurs.
formally a candidate c sthat has been generated by changing a namev we use the score function s prime c cv 1s c .
.
suggesting natural formatting we apply naturalize to build a language agnostic code formatting suggester that automatically and adaptively learns formatting conventions and generates rule for use by code formatters like the eclipse formatter.
n gram models work over token streams for then gram instantiation of naturalize to provide formatting suggestions we must convert whitespace into tokens.
we change the5indent3s 1n space0id space1spublic space1svoid 6indent0 1nid space0 space0id space1sid space0 space1sthrows space1sid space1s 7indent0 1n 8indent 3s 1n figure the formatting tokenization of lines from a snippet of code of textrunnertest.java injunit .
tokenizer to encode contiguous whitespace into tokens using the grammar s t w s epsilon1 w spacespace tab indentspace tabs lines t id lit .
keywords .
figure shows a sample tokenization of a code snippet drawn from junit .
we collapse all identifiers to a single idtoken and all literals to a single lit token because we presume that the actual identifier and literal lexemes do not convey information about formatting.
whitespace at the start of a line determines the indentation level which usually signifies nesting.
we replace it with a special tokens indent along with metadata encoding the increase of whitespace that may be negative or zero relative to the previous line.
we also annotate indent with the number of new lines before any proceeding non whitespace token.
this captures code that is visually separated with at least one empty line.
in figure line indents by spaces in the directly next line.
whitespace within a line controls the appearance of operators and punctuation if vs.if and x y vs.x y .
we encode this whitespace into the special token space along the number of spaces tabs that it contains.
if between two non whitespace tokens there is no space we add a space0.
finally we annotate all tokens of type twith their size number of characters and the current column of that token.
this helps us capture the increasing probability of an intent when we reach a large line length.
to reduce sparsity we quantize these two annotations into buckets of size q. although hard to see an empty exception handler is straddling lines in figure .
if a programmer asks if this is convention naturalize considers alternatives for the underlined token.
we train the lm over the whitespace modified tokenizer then since the vocabulary of whitespace tokens assuming bounded numbers in the metadata is small we rank all whitespace token alternatives according to the scoring function section .
.
.
converting conventions into rules naturalize can convert the conventions it infers from a codebase into rules for a code formatter.
we formalize a code formatter s rule as the set of settings s s1 s2 ... s n andc a set of constraints over the elements in s. for example sicould be a boolean that denotes must be on the same line as its function signature andsjmight be the number of spaces between the closing and the .
thencmight contain sj si sj si.
to extract rules we handcraft a set of minimal code snippets that exhibit each different setting of si si s. after training naturalize on a codebase we apply it to these snippets.
whenever naturalize is confident enough to prefer one to the other we infer the appropriate setting otherwise we leave the default untouched which may be to ignore that setting when applying the formatter.
.
ev aluation we now present an evaluation of the value and effectiveness of naturalize .
this evaluation first presents two empirical studies that show naturalize solves a real world problem that programmers care about section .
.
these studies demonstrate that 6name forks watchers commit pull request description elasticsearch af17ae55 5075mergedrest search engine libgdx a42779e9 1400mergedgame development framework netty 48eb73f9did not submitnetwork application framework platform frameworks base a0b320a6did not submitandroid base framework junit d919bb6d 834mergedtesting framework wildfly 9d184cd0did not submitjboss application server hudson be1f8f91did not submitcontinuous integration server android bootstrap e2cde337did not submitandroid application template k d8030eaa 454mergedandroid email client android menudrawer 96cdcdcc 216openandroid menu implementation used as a validation project for tuning parameters.
figure open source java projects used for evaluation.
ordered by popularity.
programmers do not always adhere to coding conventions and yet that project members care enough about them to correct such violations.
then we move on to evaluating the suggestions produced bynaturalize .
we perform an extensive automatic evaluation section .
which verifies that naturalize produces natural suggestions that matches real code.
automatic evaluation is a standard methodology in statistical nlp and is a vital step when introducing a new research problem because it allows future researchers to test new ideas rapidly.
this evaluation relies on perturbation given code text we perturb its identifiers or formatting then check if naturalize suggests the original name or formatting that was used.
furthermore we also employ the automatic evaluation to show that naturalize is robust to low quality corpora section .
.
finally to complement the automatic evaluation we perform two qualitative evaluations of the effectiveness of naturalize suggestions.
first we manually examine the output of natural ize showing that even high quality projects contain many entities for which other names can be reasonably considered section .
.
finally we submitted patches based on naturalize suggestions section .
to 5of the most popular open source projects on github of the 18patches that we submitted 12were accepted.
methodology our corpus is a set of well known open source java projects.
from github4 we obtained the list of all java projects that are not forks and scored them based on their number of watchers and forks.
the mean number of watchers and forks differ so to combine them we assumed these numbers follow the normal distribution and summed their z scores.
for these evaluations reported here we picked the top scoring projects that are notin the training set of the github java corpus .
our original intention was to also demonstrate cross project learning but have no space to report these finding.
figure shows the selected projects.
like any experimental evaluation our results are not immune to the standard threat to external validity that if poorly chosen the evaluation corpus may not be representative of java programs let alone programs in other languages.
interestingly this threat does not extend to the training corpus because the whole point is to bias naturalize toward the conventions that govern the training set.
rather our interest is to ensure that naturalize s performance on our evaluation corpus matches that of projects overall which is why we took such care in constructing our corpus.
our evaluations use leave one out cross validation.
we test on each file in the project training our models on the remaining files.
this reflects the usage scenario that we recommend in practice.
we report the average performance over all test files.
for an lm we have used a gram model chosen via calibration on the validation project junit .
we picked junit as the validation project because on august .of its medium size.
.
the importance of coding conventions to assess whether obeying coding conventions specifically following formatting and naming conventions is important to software teams today we conducted two empirical studies that we present in this section.
but first we posit that coding style is both an important and a contentious topic.
the fact that many languages and projects have style guides is a testament to this assertion.
for example we found that the ruby style guide has at least 167un merged forks and the java github corpus has 349different .xml configurations for the eclipse formatter.
commit messages we manually examined 000commit messages drawn randomly from the commits of eight popular open source projects looking for mentions of renaming changing formatting and following other code conventions.
we found that of changes contained formatting improvements contained renamings and contained any changes to follow code conventions which include formatting and renaming .
we observed that not all commits that contain changes to adhere to conventions mention such conventions in the commit messages.
thus our percentages likely represent lower bounds on the frequency of commits that change code to adhere to conventions.
code review discussions we also examined discussions that occurred in reviews of source code changes.
code review is practiced heavily at microsoft in an effort to ensure that changes are free of defects and adhere to team standards.
once an author has completed a change he creates a code review and sends it to other developers for review.
they then inspect the change offer feedback and either sign off or wait for the author to address their feedback in a subsequent change.
as part of this process the reviewers can highlight portions of the code and begin a discussion thread regarding parts of the change for more details regarding the process and tools used see bacchelli et al.
.
we examined 169code reviews selected randomly across microsoft product groups during .
our goal was to include enough reviews to examine at least 000discussion threads.
in total these 169reviews contained threads.
we examined each thread to determine if it contained feedback related to a code conventions in general b identifier naming and c code formatting.
of the threads examined provided feedback regarding coding conventions of some kind.
suggested improvements in naming and suggested changes related to code formatting subsets of the .
in terms of the reviews that contained feedback of each kind the proportions are and .
during february just over reviews were completed at microsoft.
thus based on confidence intervals of these proportions between 560and18 900reviews received feedback 7type reviews ci commits ci p val conventions p lessmuch0.
naming p lessmuch0.
formatting p lessmuch0.
figure percent commits with log messages and reviews that contained feedback regarding code conventions identifier naming and formatting with confidence intervals in parentheses.
regarding formatting changes that were needed prior to check in and between 420and39 060reviews resulted in name changes in just one month.
figure summarizes our findings from examining commit messages and code reviews.
we also present confidence intervals based on the sampled results .
these results demonstrate that changes to a nontrivial degree violate coding conventions even after the authors consider them complete and also that team members expect that these violations be fixed.
we posit that like defects many convention violations die off during the lifecycle of a change so that few survive to review and fewer still escape into the repository.
this is because like defects programmers notice and fix many violations themselves during development prior to review so reviewers must hunt for violations in a smaller set and committed changes contain still fewer although this number is nontrivial as we show in section .
.
corrections during development are unobservable.
however we can compare convention corrections in review to corrections after commit.
we used a one sided proportional test to evaluate if more coding conventions are corrected during review than after commit.
the last column in figure contains the p values for our tests indicating that the null hypothesis can be rejected with statistically significant support.
.
suggestion in this section we present an automatic evaluation of natural ize s suggestion accuracy.
first we evaluate naming suggestions section .
.
we focus on suggesting new names for locals arguments fields method calls and types class names primitive types and enums these are the five distinct types of identifiers the eclipse compiler recognizes .
recall from section .
that when naturalize suggests a renaming it renames alllocations where that identifier is used at once.
furthermore as described earlier we always use leave one out cross validation so we never train the language model on the files for which we are making suggestions.
therefore naturalize cannot pick up the correct name for an identifier from other occurrences in the same file instead it must generalize by learning conventions from other files in the project.
single point suggestion first we evaluate naturalize on the single point suggestion task that is when the user has asked for naming suggestions for a single identifier.
to do this for each test file for each unique identifier we collect all of the locations where the identifier occurs and names the same entity and ask natural izeto suggest a new name renaming all occurrences at once.
we measure accuracy that is the percentage of the time that natural izecorrectly suggests the original name.
this is designed to reflect the typical usage scenario described in section in which a developer has made a good faith effort to follow a project s conventions but may have made a few mistakes.
figure reports on the quality of the suggestions.
each point on these curves corresponds to a different value of the confidence thresholdt.
thex axis shows the suggestion frequency i.e.
at what proportion of code locations where naturalize is capable of making a suggestion does it choose to do so.
the y axis showssuggestion accuracy that is the frequency at which the true name is found in the top ksuggestions for k andk .
ast increases naturalize makes fewer suggestions of higher quality so frequency decreases as accuracy increases.
these plots are similar in spirit to precision recall curves in that curves nearer the top right corner of the graph are better.
figure 7a figure 7b and figure 7c show that naturalize performance varies with both project and the type of identifiers.
figure 7a combines locals fields and arguments because their performance is similar.
naturalize s performance varies across these three categories of identifiers because of the data hungriness of n grams and because local context is an imperfect proxy for type constraints or function semantics.
the results show that naturalize effectively avoids the clippy effect because by allowing the system to decline to suggest in a relatively small proportion of cases it is possible to obtain good suggestion accuracy.
indeed naturalize can achieve suggestion accuracy across identifier types even when forced to make suggestions at half of the possible opportunities.
augmented models cross project lms raise the opportunity to exploit information across projects to improve naturalize s performance.
indeed across our corpus we find that on average .
of variable names are shared across projects.
this is an indication that we can transfer useful information about identifier naming across projects.
the cross project model section .
is an approach to achieving that.
figure shows the percent improvement in suggestion accuracy at k 5when using the cross project scoring model versus the base model.
our evaluation suggest that the cross project scoring model yields improvements of about for variables and types and an improvements of about for method calls at the suggestion frequency of indicating that transferring information is feasible.
we note that for smaller suggestion frequencies this improvement diminishes.
we also experimented with incorporating syntactic context into naturalize as shown in equation .
our initial results find this strategy yields minor performance improvements in single variable renaming suggestions with the best improvement being in the suggestion accuracy of field names.
we believe the problem is one of finding proper weighting for the priors which we leave to future work.
multiple point selection to evaluate naturalize s accuracy at multiple point suggestion e.g.
in devstyle orstyleprofile we mimic code snippets in which one name violates the project s conventions.
for each test snippet we randomly choose one identifier and perturb it to a name that does not occur in the project compute the style profile and measure where the perturbed name appears in the list of suggestions.
naturalize s recall at rank k chosen because humans can take in 7items at a glance is .
.
the mean reciprocal rank is .
meaning that on average we return the bad name at position .
single point suggestion for formatting to evaluate natural ize s performance at making formatting suggestions we follow the same procedure as the single point naming experiment to check if naturalize correctly recovers the original formatting from the context.
we train a gram language model using the modified token stream q discussed in section .
.
we allow the system to make onlyk 1suggestions to simplify the ui.
we find that the system is extremely effective figure at formatting suggestions achieving suggestion accuracy even when it is required to reformat half of the whitespace in the test file.
this is remarkable for a system that is not provided with any hand designed rules about what formatting is desired.
obviously if the goal is to reformat all the whitespace an entire file a rule based formatter is called for.
but this performance is more than high enough to support our use cases .
.
.
.
.
.
.
.
suggestion frequency0.
.
.
.
.0suggestion accuracy k k a variables .
.
.
.
.
.
.
.
suggestion frequency0.
.
.
.
.
.0suggestion accuracy k k b method calls .
.
.
.
.
.
suggestion frequency0.
.
.
.
.00suggestion accuracy k k c typenames figure evaluation of single point suggestions of naturalize when it is allowed to suggest k 1andk 5alternatives.
the shaded area around each curve shows the interquartile range of the suggestion accuracy across the evaluation projects.
.
.
.
.
.
.
.
.
suggestion frequency0.
.
.
.
.
.
.12suggestion accuracy improvement variables methods types figure improvement of suggestion accuracy using cross project n gram model at k such as providing single point formatting suggestions on demand rejecting snippets with unnatural formatting and extracting high confidence rules for rule based formatters.
binary snippet decisions finally we evaluate the ability of stylish?
to discriminate between code selections that follow conventions well from those that do not by mimicking commits that contain unconventional names or formatting.
uniformly at random we selected a set of methods from each project 500in total then with probability uniform probability we either made no changes or perturbed one identifier or whitespace token to a token in the n gram s vocabulary v. we argue that this method for mimicking commits is probably a worst case for our method because the perturbed methods will be very similar to existing methods which we presume to follow conventions most of the time.
we run stylish?
and record whether the perturbed snippet is rejected based on either its names or its formatting.
stylish?
is not made aware what kind of perturbation if any identifier or whitespace was made to the snippet.
figure reports natural ize s rejection performance as roc curves.
in each curve each point corresponds to a different choice of threshold t and thex axis shows fpr estimated as in equation .
and the y axis shows true positive rate the proportion of the perturbed snippets that we correctly rejected.
naturalize achieves high precision thus making it suitable for use for a filtering pre commit script.
for example if the fpr is allowed to be at most .
then we are able to correctly reject of the snippets.
the system is somewhat worse at rejecting snippets whose variable names have been perturbed in part this is because predicting identifier names is more difficult than predicting formatting.
new advances in language models for code are likely to improve these results further.
nonetheless these results are promising stylish?
still rejects enough perturbed snippets that if deployed at fpr it would enhance convention adherence with minimal disruption to developers.
.
.
.
.
.
.
fpr0.
.
.
.
.
.0tpr stylish formatting identifiersfigure evaluation of stylish?
tool for rejecting unnatural changes.
to generate unnatural code we perturb one identifier or formatting point or make no changes and evaluate whether naturalize correctly rejects or accepts the snippet.
the graph shows the receiver operating characteristic roc of this process for stylish?
when using only identifiers only formatting or both.
percent of junk identifiers introduced in corpus10 1percent of identifiers renamed to junk names figure is naturalize robust to low quality corpora?
the x axis shows percentage of identifiers perturbed to junk names to simulate low quality corpus.
the y axis is percentage of resulting low quality suggestions.
note log log scale.
the dotted line shows y x. the boxplots are across the evaluation projects.
.
robustness of suggestions we show that naturalize avoids two potential pitfalls in its identifier suggestions first that it does not simply rename all tokens common junk names that appear in many contexts and second that it retains unusual names that signify unusual functionality adhering to the sup.
junk names a junk name is a semantically uninformative name used in disparate contexts.
it is difficult to formalize this concept for instance in almost all cases foo andbar are junk names while iandj when used as loop counters are semantically informative and therefore not junk.
despite this most developers know it when they see it.
one might at first be concerned that naturalize would often suggest junk names because junk names appear in threshold t0.
.
.
.
.
surprising names preservedvariables methods typesfigure naturalize does not cause the heat death of a codebase we evaluate the percent of single suggestions made on unkidentifiers that preserve the surprising name.
the x axis shows the value of the threshold t which controls the suggestion frequency ofsuggest lowertmeans that suggest has less freedom to decline to make low quality suggestions.
many different n grams in the training set.
we argue however that in fact the opposite is the case naturalize actually resists suggesting junk names.
this is because if a name appears in too many contexts it will be impossible to predict a unsurprising followup and so code containing junk names will have lower probability and therefore worse score.
to evaluate this claim we randomly rename variables to junk names in each project to simulate a low quality project.
notice that we are simulating a low quality training set which should be the worst case for naturalize .
we measure how our suggestions are affected by the proportion of junk names in the training set.
to generate junk variables we use a discrete zipf s law with slope s .
the slope empirically measured for all identifiers in our evaluation corpus.
we verified the zipfian assumption in previous work .
figure shows the effect on our suggestions as the evaluation projects are gradually infected with more junk names.
the framework successfully avoids suggesting junk names proposing them at a lower frequency than they exist in the perturbed codebase.
sympathetic uniqueness surprise can be good in identifiers where it signifies unusual functionality.
here we show that naturalize preserves this sort of surprise.
we find all identifiers in the test file that are unknown to the lm i.e.are represented by an unk.
we then plot the percentage of those identifiers for which suggest does notpropose an alternative name as a function of the threshold t. as described in equation .
tis selected automatically but it is useful to explore how adherence to the sup varies as a function of t. figure shows that for reasonable threshold values naturalize suggests non unkidentifiers for only a small proportion of the unkidentifiers about .
this confirms that naturalize does not cause the heat death of a codebase by renaming semantically rich surprising names into frequent low content names.
.
manual examination of suggestions as a qualitative evaluation of naturalize s suggestions three human evaluators three of the authors independently evaluated the quality of its suggestions.
first we selected two projects from our corpus uniformly at random then for each we ran styleprofile on30methods selected uniformly at random to produce suggestions for all the identifiers present in that method.
we assigned profiles to each evaluator such that each profile had two evaluators whose task was to independently determine whether any of the suggested renamings in a profile were reasonable.
the evaluators attempted to take an evidence based approach that is not to simply choose names that they liked but to choose names that were consistent with existing practice in the project.
we provided access to the full .
.
.
.
.
.
.
.
.
.
.
suggestion frequency0.
.
.
.
.
.
.00suggestion accuracyfigure evaluation of single point evaluation for formatting.
onlyk 1suggestions are allowed in the ranked list.
the boxplots show the variance in performance across the evaluation projects.
source code of each project to the evaluators.
one special issue arose when suggesting changes to method names.
adapting linguistic terminology to our context homonyms are two semantically equivalent functions with distinct names.
we deem naturalize s suggested renaming of a method usage to be reasonable if the suggestion list contains a partial not necessarily perfect homonym i.e.
if it draws the developer s attention to another method that is used in similar contexts.
each evaluator had15minutes to consider each profile and 30minutes to explore each project before starting on a new project.
in fact the evaluators required much less time than allocated averaging about 5minutes per example.
the human evaluations can be found on our project s webpage.
surprisingly given the quality of our evaluation codebases of the suggestions were determined to be useful by both evaluators.
further no suggestion works well for everyone when we consider naturalize s performance in terms of whether at least one evaluator found a suggestion useful of the suggestions are useful with an inter rater agreement cohen s kappa of .
.
the quality of suggestions is strikingly high given that these projects are mature vibrant high profile projects.
this provides evidence that the naming suggestions provided by naturalize are qualitatively reasonable.
of course an obvious threat to validity is that the evaluators are not developers of the test projects and the developers themselves may well have had different opinions about naming.
for this reason we also provided thenaturalize naming suggestions to the project developers themselves as described in the next section.
.
suggestions accepted by projects we used naturalize sstyleprofile tool to identify highconfidence renamings and submitted of them as patches to the 5evaluation projects that actively use github.
figure shows the pull request ids and their current status.
four projects merged our pull requests of commits while one other ignored them without comment.
developers in the projects that accepted naturalize s patches found the naturalize useful one said wow that s a pretty cool tool!
.
junitdid not accept two of the suggested renamings as is.
instead the patches sparked a discussion.
its developers concluded that another name was more meaningful in one case and that the suggested renaming of another violated the project s explicit naming convention renaming etotis no improvement because we should consistently use e. .
we then pointed them to the code locations that supported naturalize s original suggestion.
this triggered them to change all the names that had caused the suggestion in the first place which shows that naturalize pointed out an inconsistency previously unnoticed that improved the naming in the project.
links to all the discussions can be found on our project webpage.
.
related work coding conventions readability and identifiers have been extensively studied in the literature.
despite this naturalize is to our knowledge the first to infer coding conventions from a codebase spanning naming and formatting.
coding conventions are standard practice .
they facilitate consistent measurement and reduce systematic error and generate more meaningful commits by eliminating trivial convention enforcing commits .
some programming languages like java and python suggest specific coding styles while consortia publish guidelines for others like c .
high quality identifier names lie at the heart of software engineering they drive code readability and comprehension .
according to dei enb ck and pizka identifiers represent the majority of source code tokens.
eshkevari et al.
explored how identifiers change in code while lawrie et al.
studied the consistency of identifier namings.
abebe et al.
discuss the importance of naming to concept location .
caprile and tonella propose a framework for restructuring and renaming identifiers based on custom rules and dictionaries.
gupta et al.
present part of speech tagging on split multi word identifiers to improve software engineering tools .
h st and stvold stem and tag method names then learn a mapping from them to a fixed set of predicates over bytecode.
naming bugs are mismatches between the map and stemmed and tagged method names and their predicates in a test set .
in contrast our work considers coding conventions more generally and takes a flexible data driven approach.
several styles exist for engineering consistent identifiers .
because longer names are more informative these styles share an agglutination mechanism for creating multi word names .
many rule based code formatters exist but to our knowledge are limited to constraining identifier names to obey constraints like camelcase or underscore and cannot handle convention like the use of ias a loop control variable.
pylint checks if names match a simple set of regular expressions e.g.
variable names must be lowercase astyle aspell and gnu indent only format whitespace tokens.
gofmt formats the code aiming to eliminating an entire class of argument among developers slide but provide no guidance for naming.
wang et al.
have developed a heuristic based method to automatically insert blank lines into methods vertical spacing to improve readability .
naturalize is unique in that it does not require upfront agreement on hard rules but learns soft rules that are implicit in a codebase.
the soft rules about which naturalize is highly confident can be extracted for by a formatter.
api recommenders code suggestion and completion systems aim to help during editing when a user may not know the name of an api she needs or that the api call she is making needs to preceded by another .
code suggestion and completion tools suggest the next token during editing often from a few initial characters.
essentially these methods address a search problem helping the developer find an existing entity in code.
our focus is instead on release management and improving code s adherence to convention.
for example code completion engines will not suggest renaming parameters like str in figure .
language models are extensively used in natural language processing especially in speech recognition and machine translation .
despite this extensive work lms have been under explored for non ambiguous e.g.programming languages with only a few recent exceptions .
the probabilistic nature of language models allows us to tackle the suggestion problem in a principled way.
there is very little work on using nlp tools to suggestrevisions to improve existing text.
the main exception is spelling correction to which lms have been applied .
however spelling correction methods often rely on strong assumptions about what errors are most common a powerful source of information which has no analog in our domain.
.
conclusion we have presented naturalize the first tool that learns local style from a codebase and provides suggestions to improve stylistic consistency.
we have taken the view that conventions are a matter ofmores rather than laws we suggest changes only when there is sufficient evidence of emerging consensus in the codebase.
we showed that naturalize is effective at making natural suggestions achieving accuracy in its top suggestions for identifier names and even suggesting useful revisions to mature high quality open source projects.