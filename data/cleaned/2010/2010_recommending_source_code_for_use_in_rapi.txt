recommending source code for use in rapid software prototypes collin mcmillan college of william and mary williamsburg va cmc cs.wm.edunegar hariri depaul university chicago il nhariri cs.depaul.edudenys poshyvanyk college of william and mary williamsburg va denys cs.wm.edujane cleland huang and bamshad mobasher depaul university chicago il jhuang mobasher cs.depaul.edu abstract rapid prototypes are often developed early in the software development process in order to help project stakeholders explore ideas for possible features and to discover analyze and specify requirements for the project.
as prototypes are typically thrown away following the initial analysis phase it is imperative for them to be created quickly with little cost and effort.
tool support for finding and reusing components from open source repositories offers a major opportunity to reduce this manual effort.
in this paper we present a system for rapid prototyping that facilitates software reuse by mining feature descriptions and source code from open source repositories.
our system identifies and recommends features and associated source code modules that are relevant to the software product under development.
the modules are selected such that they implement as many of the desired features as possible while exhibiting the lowest possible levels of external coupling.
we conducted a user study to evaluate our approach and the results indicated that our proposed system returned packages that implemented more features and were considered more relevant than the state of the art approach.
keywords software prototyping domain analysis recommender systems i. i ntroduction rapid prototyping is a software development activity in which programmers build a prototype of a software product by iteratively proposing reviewing and demonstrating the features of that product .
it is designed to help project stakeholders explore the features they would like to include in a product and to interact with the prototype in order to discover and specify requirements.
as prototypes are generally thrown away they must be built quickly and inexpensively and must provide the flexibility to easily add or remove features.
other factors such as efficiency or portability are less important as the prototype may not even share the same programming language or hardware platform as the final product .
therefore it is essential to minimize the manual effort involved in building prototypes and to maximize automation and source code reuse.
as such tool support for automatically locating and reusing features from open source repositories offers a tremendous opportunity for reducing this manual effort .
rapid prototyping is often divided into a horizontal and a vertical phase .
in the horizontal phase domain analysts identify an initial set of candidate features for implementation in the product.
these features which are often cursorilydefined are presented to the stakeholders for discussion feedback and refinement.
this activity is often supported by domain analysis tools and techniques which identify features that are common across similar or competitive software systems .
however such approaches provide only limited information about the implementation of those features.
in contrast during the vertical phase of rapid prototyping developers build full functionality for a selection of features identified during the horizontal phase.
this provides a much richer user experience in which project stakeholders can run the software and interact with the features in order to decide on specific use cases and to identify potential problems.
to reduce programming effort and shorten time to market programmers can find and reuse existing solutions for their prototypes.
source code search engines have been developed to locate implementations that are highly relevant to a feature specified by a programmer e.g.
via a natural language query .
however although these engines are effective for locating single features they are not designed for the more complex yet common case in which a prototype will incorporate a set of interacting features.
as a result existing search engines often return packages that match only a small subset of the desired features and developers have to invest considerable effort to integrate features from several different packages and projects.
under these circumstances the cost and effort required for a programmer to comprehend and integrate the returned source code can significantly reduce the benefits of reuse .
in this paper we present a novel recommender system for supporting rapid prototyping.
our system directly addresses several shortcomings of existing techniques and tools by integrating the horizontal and vertical phases of rapid prototyping.
our approach first recommends features and then locates and recommends relevant source code.
we utilize a hybrid set of algorithms based on pagerank set coverage and coupling between objects cbo in order to maximize the coverage of features while proposing a set of packages that minimize the integration effort involved in building a prototype.
we implemented the recommender system and have conducted a cross validation user study with participants to compare the effectiveness of our approach against that ofsoftware repositories module extractor feature description extractor feature recommender horizontal prototyping feature recommendations vertical prototyping module recommendations module recommender modules feature descriptions module dependencies module feature matrix product description source code documentation dependency extractor matched features confirmed features module recommendations source code search engine used to identify feature module relationships feature recommendations and feedback.
module recommendations figure .
overview of the architecture of our approach.
a state of the art search engine portfolio .
during the study users entered product descriptions and selected features recommended by our system.
the users then evaluated the packages recommended by each of the approaches.
the results from the study showed that our approach returned more of the desired features per recommendation than portfolio that a greater proportion of the source code was relevant to the product description and that users spent less time evaluating the results from our approach.
our recommender and user study data are online for public use1.
ii.
o verview before describing the specific details of the underlying algorithms we provide an architectural overview of our approach.
as depicted in in figure there are ten primary steps.
steps and focus on extracting features and modules from one or more software repositories.
first the module extractor retrieves software modules from one or more repositories .
these modules are collections of source code related to a particular application or functionality such as c namespaces or java packages in this paper we focus on java packages.
next the feature extractor discovers the set of features implemented in the repositories .
each feature describes a common function of the software such as email spam detection .
these features are discovered by analyzing the written specifications of applications in the repositories.
further details are provided in section iii.
in order to formulate package recommendations it is necessary to understand the relationships between features and modules and also the dependencies between modules.
steps and therefore focus on discovering these relationships.
in step a source code search engine is used to identify modules that contain specific features and to a module feature matrix that is used as input to thefeature and module recommenders .
in step module dependencies are extracted through examining the source code.
further details are provided in section iii.
a user then initiates a request for a recommendation by describing the required functionality of the product they intend to prototype .
this description is parsed and then elements of the description are matched to features known by the recommender system .
if matching features are found they are presented to the user who is asked to confirm or reject their relevance .
the feature recommender then generates additional feature recommendations and these are also presented to the user for feedback.
these recommendations support the horizontal phase of rapid prototyping.
a more complete description is provided in section iv.
our approach also supports the vertical phase of rapid prototyping.
in this phase the selected features are sent to the module recommender and a series of computations are performed in order to generate a set of module recommendations designed to provide high feature coverage and low external coupling .
a detailed explanation of this process is provided in section v. the recommended modules are then presented to the user .
we illustrate this process from the users perspective with a simple scenario showing both feature and module recommendations for the rapid prototyping of a midi music player.
as depicted in figure a the product description was initially matched to features labeled music plays in the background and sound supported and once these features were accepted by the user the feature recommender suggested three additional features.
all recommendations were accepted by the user.
the module recommender then proposed the three packages shown on the right hand side of figure b .
the projects from which the packages originate are displayed on the left.
the gui allows the user to see a a feature recommendation and selection b module recommendation figure .
example of rapid prototyping in which the user entered the product description midi music player.
description of the project as well as browse the java classes and source code inside the package.
iii.
m ining product and feature data in order to construct the recommender environment two different types of data are extracted from the software repositories.
first the feature recommender requires rich textual descriptions of features to provide meaningful and descriptive information to software developers and second the module recommender requires high quality source code for effective rapid prototyping.
although both of these artifact types could in theory be extracted from a single repository in practice different repositories contain different types of artifacts.
this difference created the additional requirement that there be significant overlap between the features contained in each repository.
a. feature descriptions feature descriptions were extracted from applications in softpedia2.
although softpedia is not a source code repository it does provide a repository of product descriptions that include marketing like summaries and bullet point lists of features.
in the remainder of the paper we therefore refer to it as a repository.
in general feature descriptions are mined from product documentation.
in the case of softpedia we extracted individual sentences from the product summary information and bulleted lists describing features products categorized under of softpedia s predefined categories and sub categories.
together these formed a set of feature descriptors .
many feature descriptors describe similar functionality.
for example a product that monitors cpu usage in realtime likely provides similar functionality to one that claims to show information about cpu usage.
our approach therefore clusters feature descriptors in order to discover a set of meaningful features.
we utilized the incremental diffusive clustering algorithm idc and feature naming approach described in our prior work .
idc takes an iterative approach.
in each iteration the spk means clustering algorithm is used to cluster the feature descriptors and then to identify and retain the best cluster based on the cohesiveness and size of the cluster.
this cluster s dominant terms are then identified and removed from all feature descriptors in order to allow latent topics to emerge in subsequent clustering iterations.
the clustering is repeated until no further meaningful terms remain.
all identified clusters represent a single feature and the feature is named by identifying the most representative descriptor for the cluster.
using this approach the softpedia data produced a set of features.
b. source code modules source code modules were extracted from java applications downloaded from sourceforge3.
the modules java packages and million lines of code.
the large size and public accessibility of both sourceforge and softpedia repositories suggests a large overlap in their domains meaning that many of the features discovered through analyzing the softpedia documentation are implemented in sourceforge applications.
c. relating features to modules a module is considered related to a feature if that module implements the feature.
in order to discover these relations we used the portfolio search engine .
portfolio takes a natural language query as input and locates chains of function invocations relevant to that query.
for this paper we modified portfolio to locate java packages and instantiated it over the source code modules we mined from sourceforge.
then we used the features identified by our idc algorithm as queries for portfolio.
the module feature matrix is a matrix where the rows are the modules the columns are the features and the cells indicate whether portfolio detected that feature as implemented by the package.
iv.
f eature recommendation when the user provides a description of the product to be prototyped the feature recommendation algorithm constructs an initial profile of the product by using the cosine similarity metric to match parts of the description to relevant features in our model.
we established a threshold score of .
in order for the product to be matched to a feature in keeping with previous practice .
as previously explained these features are presented to the user in order to confirm that the matching has been performed correctly.
given the feature set of the new product our feature recommender module identifies similar products and uses their feature profiles to make predictions about the existence of other relevant features in the new product.
in our prior work we used a product feature matrix based on features found in the softpedia products in order to generate recommendations .
the objective of the recommender system was to suggest features to include in a product.
in contrast the recommender system described in this paper is designed to recommend actual source code packages.
therefore although we utilize the algorithm defined in our previous work to recommend features we use a product feature matrix mined from the open source repositories.
one benefit of this approach is that recommendations are based on the actual co occurrence of features in implemented source code as opposed to the more abstract and incomplete descriptions of features provided by the softpedia product descriptions.
given the module feature matrix generated by the source code search engine the feature recommender module merges the rows representing modules originating from a single product to form a binary product feature matrix m mi j p f where prepresents the number of products mined from sourceforge fis the numberof features mined from softpedia and mi jis if and only if the feature jis implemented in product i. a. recommending additional features next our feature recommender module generates an additional set of feature recommendations which are presented to the user.
this is accomplished using the k nearest neighbor knn algorithm.
this method has been shown to be efficient for recommending features and requirements .
for the purpose of feature recommendation the similarity of the new product and each of the existing products in the product feature matrix m is computed and the top k most similar products are selected as neighbors of the new product.
the binary equivalent of cosine similarity is used to compute the similarity of the new product pwith each existing product nas follows similarity p n fp fn radicalbig fp fn where fpdenotes the set of features of product p .
after forming the neighborhoods features are recommended to the new product using an approach based on schafer s technique to predict the likelihood of feature fbeing relevant to product pas follows pred p f n nbr p similarity p n mn f n nbr p similarity p n where n nbr p represents a neighbor of p and mn fis an entry in the binary matrix mindicating whether product ncontains feature f. in general prediction scores will be computed for each candidate feature and the features with highest predictions will be recommended.
b. evaluating feature recommender to statistically evaluate the performance of the feature recommender based on the integration of softpedia and sourceforge data we performed a standard leave one out cross validation experiment.
given the product feature matrix m at each run of the experiment a random feature is removed from one of the products and the recommendation algorithm is executed.
the results are then analyzed to see if the recommender was able to recommend back the removed feature.
the hit ratio measures the likelihood that the removed feature is recommended as part of the top n recommendations.
in order to calculate the hit ratio for each test product p a feature fis randomly removed from the product profile and nrecommendations are generated using the remaining features.
if feature fis contained in the recommendation list then the hit ratio for pis .
otherwise it is .
.
the hit ratio of the recommendation algorithm is calculated by averaging over the hit ratio values of all the test products.
figure compares the hit ratio values of our feature recommender and a random recommender for different values of n. the results show that there was a sharp0 .
.
.
.
.
hit ratio number of recommendations knn random figure .
hit ratio comparison for knn and random recommender improvement over the random case for the early recommendations meaning that targeted features were recommended towards the top of the list of recommendations and that the feature recommender was effective.
v. m odule recommendation the module recommender takes as input the list of features agreed upon by the user as a result of the feature recommendation process and produces a list of recommended packages for use in creating the desired rapid prototype.
a. recommender goals our recommendation algorithm is designed to optimize the following goals in order to minimize the cost and effort of reusing existing packages in a rapid prototype.
coverage the recommended packages should provide coverage of as many targeted features as possible.
minimize number of recommended projects the overhead involved in downloading installing and integrating packages from many different projects makes it preferable to construct a rapid prototype using packages drawn from as few projects as possible.
our algorithm therefore attempts to minimize the number of projects from which the recommended packages are drawn.
minimize the external coupling of recommended packages high external coupling decreases developer comprehension of the package increases the effort needed to execute code in the package and makes it difficult and costly to reuse the packages.
b. package coupling costs before describing our module recommender algorithm we present our technique for computing package coupling costs.
these costs are measured using the coupling between objects cbo metric in which a coupling cost is defined for each package.
the total coupling cost for thepackage depends upon both direct and indirect couplings of that package to other packages however the dependency chain of coupling costs between packages makes the cost calculation problem nontrivial.
in the following section we explain how to calculate individual coupling costs and then to extend this metric to account for the common case in which multiple packages are selected from a single project.
the dependency information between packages can be modeled as a weighted directed graph g v e with each vertex v vrepresenting a package and each directed edge e erepresenting the dependency of one package to another.
an edge ei jfrom node vito node vjexists if and only if one or more classes in package viuse one or more classes in package vj.
the weight wi j on the edge ei j represents the cbo between the two endpoints and is defined as the fraction of classes in vithat use at least one class in vj.
calculating the coupling cost for packages can be seen as assigning real weights to vertices in the graph such that the weight of each vertex is a function of the weights on the outgoing edges as well as the weights assigned to all of its outgoing neighbors.
in this paper a variation of the pagerank algorithm is used to compute the vertices weights.
the pagerank algorithm was first developed to support the hyperlink analysis of web pages such that each page in the web graph is assigned a numerical weight between and known as its pagerank which represents the relative importance of the page.
the pagerank is then used by the search engine to sort and rank the results for a given query.
the pagerank algorithm is commonly referred to as the random surfer model .
when a random surfer reaches a page with noutgoing links he or she will take any of the outgoing links or will jump to a random page in the graph.
the pagerank score for each page depends on the number of times it has been visited.
more formally in a directed weighted graph the pagerank score of an arbitrary vertex viis iteratively computed as in equation until the algorithm converges pagerank vi d n d vj in vi wji pagerank vj vk out vj wjk where dis a damping factor that ranges between .
and .
.
if the damping factor is set to one then pages that have no outgoing external links will act as rank sinks and absorb all of the rank in the system.
for this reason the formula is adjusted so that with some probability the surfer jumps to a random node in the graph.
in the original pagerank algorithm the score of each node in the graph depends on all its incoming edges.
our problem is different in the sense that the coupling cost of a package depends on the cost of all the packages that it is using and hence depends on all the outgoing edges.
therefore in order to apply the pagerank method all thefigure .
partial reversed package graph for an example project edges in the package graph gwere first reversed and then the pagerank scores were computed for the reversed graph.
the calculated pagerank scores are an indication of the relative connectivity level of each package to other packages and so are used as the coupling cost values.
c. project coupling costs given a set of features a project can contain useful packages that implement desired features plus some additional utility packages that provide essential services to the useful packages but which do not directly implement any of the desired features.
the coupling cost associated with each project depends on the combined external coupling of the set of useful packages to their utility packages.
in order to accurately compute this cost all of the useful packages are merged together in the graph through removing internal edges that connect the useful packages and then replacing external edges i.e.
edges between utility packages and useful packages with edges to or from the merged package.
in the case that a utility package is connected to more than one useful package through outgoing edges all these edges are merged into a single edge and the weight of this newly formed edge is computed as the sum of all outgoing edges to the useful packages.
similarly all the incoming edges from the set of useful packages to the utility package are replaced with a single edge connecting the merged package to the utility package.
after merging all useful packages the pagerank scores are recalculated and the project coupling cost for the set of given features is computed as the pagerank score of the merged package.
figure provides an illustrative example.
on the left hand side of the diagram the reversed package graph depicts a set of features f f1 f2 f3 f4 for which the package set up p1 p2 p3 are useful and they are connected to other packages that do not implement any of the desired features.
on the right hand side of figure the graph is shown after the useful packages are merged.
unfortunately this approach can be computationally expensive as pagerank scores need to be recalculated each time a user issues a new recommendation request.
thereforewe considered two computationally inexpensive cost estimation techniques.
the first approach sums the individual costs of all useful packages in the project and has a tendency for overestimation while the second approach underestimates costs by using the cost from the package that exhibits the highest coupling values.
an initial analysis showed that the second approach produced better results and so it was adopted for all the remaining experiments described in this paper.
d. package recommendations package recommendations are therefore made as follows.
given a set of features f f1 ... fn our code search engine finds the set of all relevant packages pk pk1 ... pkn and relevant projects pr pr1 ... prm where each pkiis part of a project in pr.
as a single feature can be implemented in different packages across various projects the challenge is to find the optimal set of packages with respect to the objectives and constraints mentioned in section v a. by simplifying the problem to find the minimum number of projects that cover all the features our problem can be seen as equivalent to the set cover optimization problem which has been shown to be np complete .
furthermore if the problem were to find the minimum coupling cost combination of projects that cover all the features then it would be another variation of set cover optimization and np complete.
the greedy algorithm has previously been used to provide a good approximation of a near optimal solution .
we therefore adopted this approach.
our method as described in algorithm iteratively selects the best project at each step and then selects all of the packages in this project which implement a targeted feature.
this process continues until all the targeted features are covered or there are no more candidate projects to choose from.
our criterion for selecting the best project is based on the average cost per feature computed by determining the project coupling cost as described in section v c divided by the number of targeted features implemented by the project.
algorithm greedy set cover algorithm selectedpackages selectedpro jects while f ne ationslash 0do best getbestpro ject pr selectedpro jects selectedpro jects best selectedpackages selectedpackages use f ulpackages best f f f coveredfeatures best f end while vi.
e valuation in addition to the quantitative study reported in section iv b of this paper we also conducted a qualitative assess ment designed to compare the efficacy of our approach against the current state of the art approach.
this kind of assessment relies on expert human judgement and is an accepted practice for evaluating recommendations .
a. state of the art comparison the current state of the art technique for locating source code that is relevant to a given feature utilizes a source code search engine.
for purposes of this study we therefore compared our approach against the portfolio search engine which has been shown to outperform google code search and koders in studies where developers search for source code relevant to features they need to implement .
we replaced the package recommender from our approach with portfolio by concatenating the text descriptions of the features selected by the user into a single query.
this concatenation simulates the case where programmers search for code relevant to multiple features by entering those features into a search engine as a single query.
the java packages recommended by portfolio were then presented to the user using the same interface we designed for our approach.
in this way the user interface was identical across the user study regardless of whether the underlying recommendations were made by our approach or by the search engine.
b. research questions the ultimate goal of our rapid prototyping system is to support vertical prototyping through recommending relevant source code packages.
our approach is designed to maximize the number of features covered by the returned source code while minimizing the amount of source code returned that does not directly implement features.
therefore our study was designed to address the following research questions rqs rq1are the recommendations from our approach more relevant to the original product description than the recommendations from the state of the art approach?
rq2does our approach recommend fewer false positives than the state of the art approach?
rq3does our approach provide better feature coverage than the state of the art approach?
rq4do users require less time to understand the recommendations from our approach than from the state of the art approach?
rq1is designed to evaluate the recommendations from our approach in terms of overall relevance to the original product description given by the user.
this addresses the possibility that the recommended source code is relevant to the features selected but not relevant to the query entered by the user.
rq2is designed to evaluate whether the recommended source code implements the selected features.
each source code package that is returned should implement one or more of the previously specified features and ourtable ii the cross validation design of our user study .
different participants used different tasks with different approachs .
experiment group approach task set 1a our approach t1 b state of the art t2 2a state of the art t3 b our approach t4 approach attempts to maximize the number of selected features implemented per package.
rq3is designed to evaluate feature coverage.
finally a stated goal of our approach is to reduce manual prototyping effort by minimizing the external coupling of the recommended source code as well as the amount of that source code.
we designed rq4to evaluate the effort in terms of time required to understand the recommendations.
c. cross validation design of the user study a cross validation design was used in which experts compared the results from our approach to the results from a state of the art approach.
a cross validation design is important because it limits potential threats to validity such as fatigue bias towards tasks and bias due to unrelated factors e.g.
user interfaces .
table ii shows an outline of the experimental design.
the study was split into two experiments each lasting one hour.
the participants were randomly placed into two equally sized groups aandb.
the approaches and tasks were rotated among the groups such that different participants used different tasks on different approaches.
also the participants were prevented from knowing whether they were evaluating our approach or the state of the art approach to avoid introducting bias.
during the study our approach was denoted as the green approach and the state of the art approach as orange .
the approaches shared the same interface and participants saw only the color denotations.
participants computer science students were recruited from the college of william mary to participate in our user study.
twenty eight were graduate students while three were undergraduates.
the participants had an average of .
years programming experience and .
years experience with java.
fourteen reported professional programming experience in various industries.
tasks the experiments were designed around a set of different tasks.
these tasks were roughly equal in difficulty as determined by a vote of the authors and represented a range of potential prototyping tasks.
the following is an example task from the user study.
a complete listing of the tasks and other case study materials may be downloaded from our online appendix.
build a video player with adjustable bitrate and other video and audio paramters.
your programtable i summary of results from the user study showing relevance r precision p coverage c and time required in minutes t .
the column samples is the number of recommended packages for rand c the number of queries for p and the number of queries that users recorded their times for t. anova results are f fcritical and p1.
student s t test results are t tcritical and p2.
h var approach samples min max median f fcritical p1 t tcritical p2 decision h1 rour approach .
.
.
5e .
.
1e rejectstate of the art .
h2 pour approach .
.
.
.
1e .
.
1e rejectstate of the art .
.
h3 cour approach .
.
.
.
2e .
.
1e rejectstate of the art .
h4 tour approach .
.
.
2e .
.
1e rejectstate of the art .
should support multiple video formats and display the video inside a resizable gui window.
in each experiment a participant was assigned one of the two approaches and a set of tasks.
the participant had to formulate a query by defining a set of keywords that represented at least some of the features needed for the task at hand.
the participant then entered the query into the gui and selected features relevant to the query.
the system then returned a set of recommended packages.
the participants were asked to evaluate the results according to the relevance of the recommended packages and through specifying which packages implemented each of the targeted features.
d. metrics and statistical tests the following metrics were collected during the study.
relevance the relevance of a recommended package was evaluated by the participants on a four point likert score rated as an integer from one to four where four is highly relevant three is relevant two is largely irrelevant and one means completely irrelevant.
the relevance metric was used to answer rq1.
precision precision is the percent recommendations which implement at least one of the targeted features.
precision will be high when the number of false positives packages that implement no features is low precision is intended to help us answer rq2.
coverage coverage measures the number of features implemented by a recommended package and is used to answer rq3.
coverage is defined as fi fs where fiis the set of features implemented by a given package and fsis the set of features selected by the user.
coverage is high when the recommended packages implement a large portion of the features selected by the user.
anova one way anov a and the student s t test were used to evaluate the statistical significance of differences in relevance precision and coverage.
anov a is a parametric test that assumes a normally distributed sample.
according to the law of large numbers the central limit theorem applies when the sample size is greater than .
the study included participants indicating that the results are statically significant.
e. threats to validity there are two main threats to internal validity in our study.
first the partipants manually judged the recommendations and their ratings could be influenced by external factors such as fatigue prior knowledge of the approaches being evaluated programming proficiency or lack of motivation.
we addressed threats due to fatigue and prior knowledge in the design of our user study by rotating the tools among different groups of participants and denoting the different tools with only a color rather than a name.
the programming proficiency participants could also affect results because users with different proficiency levels could take different factors into consideration.
this threat was minimized by randomly distributing participants to the various groups.
finally the potential motivation problem was at least partially addressed by providing a small stipend to participants who completed the study.
the second main source of threats to internal validity are the tasks.
we selected tasks which were easily understood by the authors and which are in the scope of the projects in the repositories we used.
still tasks that are out of scope or which are too complex to be understood could cause our recommendation engine to produce low quality results.
therefore we rotated the sets of tasks that participants used so that in each experiment each group used different tasks on different tools.
also we ensured that our approach and the state of the art approach both recommended packages from the same repository.
sources of threats to external validity include the repositories we used and a potential mismatch of the features from one repository and the source code in another.
our approach relies on a search engine to determine which features are implemented in which packages see section iii .
the search engine we used has shown to perform well in controlled experiments however an external threat to validity remains in that the performance may vary on different repositories.
a relevance b precision c coverage d time figure .
boxplots showing the relevance precision coverage and time per query in minutes reported during the user study for the two different approaches.
the thick white line is the median.
the lower dark box is the lower quartile while the light box is the upper quartile.
vii.
e mpirical results confidence precision and coverage were measured for both our approach and a state of the art approach in a crossvalidated user study.
the statistical differences were then tested for these metrics.
in this section we present the results of these tests in order to answer our research questions.
a. hypotheses the following null hypotheses are meant to evaluate the directionality of the difference of means for relevance precision and coverage.
these hypotheses are used in the case when anov a indicates a statistically significant difference in the values of the metrics.
h1 the mean values of relevance are greater for the state of the art approach than for our approach.
h2 the mean values of precision are greater for the state of the art approach than for our approach.
h3 the mean values of coverage are greater for the state of the art approach than for our approach.
h4 the mean time per query in minutes is lower for the state of the art approach than for our approach.
table i is a summary of the results from the user study.
we reject the three null hypotheses.
for anov a the value offis greater than fcritical and p .
in all cases.
moreover for the student s t test of directionality texceeds tcritical .
therefore the mean values of relevance precision and coverage are all greater for our approach than the stateof the art approach.
b. rq overall relevance in section vii a we found that the mean values of relevance were greater for our approach than for the stateof the art approach.
this result indicates that our approach recommends packages which are more relevant to queries than the state of the art approach.
a key difference in the relevance values is that our approach returns a larger number of packages rated as that is highly relevant as shown in figure a .
seventeen percent of the packages from our approach were rated highly relevant while only from the state of the art approach were and these results wereconsidered outliers.
the tasks required multiple features to be implemented and it is likely that the users only rated packages as highly relevant if those packages implemented many of the necessary features.
however the state of the art approach a source code search engine focuses on locating packages that are relevant to single features.
thus our approach outperforms the state of the art approach in terms of relevance to the queries.
c. rq recommendations implementing features precision is a measure of the number of recommended packages which implemented at least one feature that the user selected see section vi d2 .
we found that the levels of precision for our approach were greater than for the stateof the art which suggests that our approach outperforms the state of the art in terms of the number of recommendations containing useful features.
note that both approaches recommended a large number of packages which did not include any of the selected features as shown in figure b .
this figure .
a histogram showing the number of features implemented per package as a percentage of the total number of packages recommended in the user study.
our approach recommends more packages that implement multiple features compared to the state of the art and fewer that implement no features.result can be expected when recommending source code because of the difficulty in matching features to source code and has been widely documented .
on the other hand for many queries our approach recommended a large number of packages which included relevant features.
for half of the queries at least of the packages included desired features.
the state of the art approach performed as well for only of the queries.
d. rq features covered by recommendations the packages recommended by our approach should implement as many features as possible.
we measured the amount of selected features in each package with the coverage metric and we found that our approach has greater levels of coverage that the state of the art showing that our approach outperforms the state of the art techniques in terms of features covered by each package recommendation.
figure c shows the levels of coverage from the user study.
while both approaches returned packages that did not implement the selected features our approach made recommendations that covered a larger percentage of the features.
for example of the packages from our approach implemented at least half of the features selected by the user compared to of the state of the art s recommendations.
a histogram of our results figure illustrates that our approach returns packages that implement multiple features.
roughly of recommendations from both approaches implemented one feature.
for packages with more than one feature our approach outperforms the state of the art.
e. rq time per query we found that the participants in the user study were able to complete their evaluations of the recommendations in less time when using our approach than when using the state of the art approach.
a stated goal of our approach is to reduce the effort programmers must expend in reusing code for prototypes and this result indicates that users of our approach are able to understand the source code more quickly than with a state of the art approach.
viii.
r elated work our technique for rapid prototyping combines domain analysis for horizontal prototyping with source code recommendation for vertical prototyping.
this section gives a brief summary of these areas.
domain analysis is the process of analyzing a set of relevant software systems to identify organize and represent features common to systems within a domain .
most approaches involve either the manual or automated extraction of domain vocabulary from requirements specifications and then use clustering to identify associations and common domain entities .
some authors have taken more structural approaches for example chen et.
al.
constructed requirements relationship graphs rrg fromseveral different requirements specifications which they then merged into a single domain tree .
other researchers such as niu et.
al.
have applied similar techniques to analyze functional requirements in a product line .
in contrast to our approach these techniques are generally applied to a set of requirements specifications with associated design documents code and test cases stored in a project repository making it relatively simple to retrieve code alongside a list of desired features.
however such approaches are constrained by the scope of an organization s project repository while our approach incorporates hundreds of thousands of project descriptions and source code packages to identify and recommend a far broader set of features.
building prototypes from existing source code has long been a goal of rapid prototyping tool support .
studies of rapid prototyping have shown that programmers often build prototypes through an iterative process of adding features by using source code examples .
this iterative process is known as opportunistic programming .
our approach builds on opportunistic programming by allowing programmers to locate source code relevant to several features.
in addition we recommend features that frequently occur in software alongside the features that the programmer needs to implement.
other techniques have been proposed for locating relevant source code including source code search engines.
these engines commonly match keywords in user queries to keywords from source code or documentation .
recent efforts have focused on improving search results using contextual information either from the programmer s development environment the dependencies of the source code being searched or test cases and use cases .
ix.
c onclusion the continuing growth of open source software creates ongoing opportunities for mining useful domain knowledge and for reusing code across projects.
in this paper we have explored the idea of using these repositories to support rapid prototyping.
our work has demonstrated that different types of repositories can be used synergistically to create an effective recommender system which can be used to help developers identify relevant source code packages.
it has advanced the current state of practice in which source code search engines consider only individual features.
in contrast our approach recommends sets of packages which are designed to facilitate the prototyping and development tasks and has demonstrated that source code recommendation can be substantially improved with algorithms that consider multiple features as selected by the developer.