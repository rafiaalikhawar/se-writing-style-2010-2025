dictionary learning based software defect prediction xiao yuan jing1 shi ying1 zhi wu zhang1 shan shan wu1 jin liu1 1state key laboratory of software engineering school of computer wuhan university wuhan china 2college of automation nanji ng university of posts and telecommunications nanjing china corresponding author jingxy 2000 .com abstract in order to improve the quality of a software system s oftware defect prediction aims to automatically identify defective software module s for efficient software test.
to predict software defect those classification methods with static code attributes have attracted a great deal of attention.
in recent years machine learning technique s have been applied to defect prediction.
due to the fa ct that there exist s the similarit y among different software modules one software module can be approximately represented by a small proportion of other modules.
and the representation coefficients over the pre defined dictionary which consists of histor ical software module data are generally sparse.
in this paper we propose to use the dictionary learning technique to predict software defect .
by using the characteristics of the metrics mined from the open source software we learn multiple dictionaries including defect ive module and defect ive free module sub dictionar ies and the total dictionary and sparse representation coefficients .
moreover we take th e misclassification cost issue into account because the misclassification of defect ive modules gene rally incurs much higher risk cost than that of defect ive free ones.
we thus propose a cost sensitive discriminative dictionary learning cddl approach for software defect classification and prediction .
the widely used datasets from nasa projects are empl oyed as test data to evaluate the performance of all compared methods .
experimental results show that cddl outperforms several representative state of the art defect prediction methods .
categories and subject descriptors d. .
software qualit y assurance sqa g. .
sparse structured and very large systems direct and iterative methods i. .
design methodology classifier design and evaluation .
general terms algorith ms keywords software defect prediction dictionary learning sparse representation cost sensitive discriminative dictionary learning cddl .
.
introductio n software defect prediction is one of the most important research topics in software engineering which is an efficient means to relieve the burden on software code inspection or testing.
to achieve the goal of detect ing and correct ing the greatest number of defects in software software defect prediction enables the organization s limited re source to be reasonably allocated.
it can be generally categorized into two types static and dynamic defect prediction technology .
static defect prediction technology mainly refers to defect number prediction or defect distribution prediction based on the defect related metrics .
dynamic defect prediction technology predict s the distribution of the system defects over time by using the defect generated time .
static prediction technique has been widely used because it can predict the defect proneness of new software modules with the historical defect data so as to improve the quality of software .
the key of static defect prediction technique is how to fully analyze and utilize the existing historical data and then build more precise and effective bina ry classifiers of software modules.
in recent years many popular classification methods such as support vector machine svm decision tree neural networks na ve bayes and c ost sensitive learning methods have been employed to achieve this goal.
however in the field of software defect prediction these classification methods often encounter some difficulties for example the class imbalance problem and the misclassification cost issue .
class imbalance problem indicates that a software system contains much fewer defective modules than defect ive free modules which leads to negative influence on decision of classifiers .
classifying a software module as defect ive prone implies that more testers should be invested in the verification activities thus adding to the development cost.
misclassifying a module as defect ive free carries the risk of system failure which is also associated with cost implications .
sparse representation a recently dev eloped technique arouses much interest from researchers due to its effectiveness and robustness.
the idea of sparse representation is that information of a signal can be efficiently represented or coded by a linear permission to make digital or hard copies of all or part of this w ork for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
icse may june hyderabad india copyright acm ... .
.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
icse may june hyderabad india copyright acm ... .
combination of a few elementary signals called atoms .
the atoms are usually chosen from a so called over complete dictionary i.e.
a collection of atoms that the number of atoms exceeds the dimension of the signal space.
sparse representation usually has favorable discriminative power a nd owns robustness to noise from signals both of which are the advantages over other conventional machine learning algorithms.
sparse representation based classification src scheme used training samples as the over complete dictionary while this dictio nary may not be the optimal one .
the d ictionary learning technique aims to produce an over complete dictionary that can code the input signal better .
recently lots of efforts have been made on learning a more compact and robust dictionary for representing the input signal well such that the reconstruction error is minimized and classification accuracy can be improved.
.
motivation in software defect prediction a test module could be similar to part of historical modules which almost belong to the same class.
in other words one new module will be compactly represented or coded by a liner combination of a small portion of the entire historical data .
this representation is thus naturally sparse .
therefore sparse representation can well descr ibe the natural characteristic s of historical data because it not only fully utilizes the information of samples mined from the software module but also is not influence d by the sample distribution.
thus it can achieve favorable classification effect.
i n this paper we introduce the sparse coding based dictionary learning to offer a solution to software defect prediction.
usually the dictionary can be constructed by directly using the original training samples whereas the original samples have much re dundancy and noise which are adverse to prediction.
for the purpose of further improving the classification ability we hope to learn a dictionary that is capable of representing test module well.
although the success of some works implies that learning a dictionary shared by all classes can bring high classification accuracy it loses the correspond ing relation between the dictionary atoms and the class labels.
to acquire a better solution we employ the supervised dictionary learning technique whose sub dictionary is separately learned from the specific data sets namely defective data and defective free data.
the reconstruction error associated with each class can be used for classification.
during the software defect prediction process two ty pes of misclassification errors are encountered.
the type i misclassification happens when a defective free module is predicted as defective one while a type ii misclassification is that a defective module is classified as defective free.
type i misclassif ication leads to increasing the development cost and type ii misclassification associate s with risk cost.
naturally we take the misclassification costs of type i and ii errors into consideration by incorporating the cost factors into process of dictionar y learning.
.
contribution in this paper we propose a novel approach that employs dictionary learning for software defect prediction.
and the contributions of our study are summarized as following three points although dictionary learning technique has been effective ly applied to other domains to the best of our knowledge we are the first attempt to introduc e the dictionary learning technique into the field of software defect prediction .
by using the characteristics of the metrics mined from the open source software we learn multiple dictionaries including defect ive module sub dictionary defect ive free module sub dictionary and the total dictionary and sparse representation coefficients .
in order to enhance the classification ability we desig n a cost sensitive discriminative dictionary learning cddl approach .
it can fully exploit the class information of historical data to improve the discriminative power by using the supervised dictionary learning technique .
meanwhile by increasing punishm ent on type ii misclassification a defective module is misclassified as defective free one in the procedure of dictionary learning cddl takes the various misclassification cost into consideration which is beneficial to the prediction performance .
cddl employ s the p rimary component analysis pca technique a widely used dimension reduction technique to initialize all t he atoms of each sub dictionary and simultaneously solve the class imbalance problem.
m oreover e ven if defect data are not enough cddl can easily build the over complete dictionary sub dictionar y for each class is complete due to the characteristic of low dimension in software module for example the dimension of soft ware module ranges from to which is measured by the co mmon metric elements mccabe or halstead metric .
in this paper we conduct the experiment s on ten nasa datasets which are public and widely used for software defect prediction .
the experimental results demonstrate that the proposed approach outpe rforms several representative methods .
the remainder part of this paper is organized as follows.
section introduc es the related work.
section describes the proposed approach .
section introduce s the experimental setup.
section shows the experimental results and analysis .
the conclusion is drawn in section .
.
related work .
defect prediction software archives instances metrics training instancesinstance learnerclassifier labeling defective defective free feature extraction creating a training corpus building a prediction model prediction evaluation fig.
defect prediction process fig.
shows the typical defect prediction process commonly used in the literature .
the fir st step is to collect and label module s. a software module can be labeled as defective or defective free according to whether the module contains defects or not.
then defect prediction metrics such as complexity metrics are used as module features.
the module features and labels are used to train prediction models by using learner .
finally the classifier predicts the new modules .
many traditional classification methods have been adopted for software defect prediction including svm decision tree neur al networks bayes methods and etc.
how to improve these methods has recently drawn great attention.
gray et al.
argued that it is important to explicitly carry out cleansing stages for all of the data in nasa datasets and then svm can be successful ly considere d as a classification method for defect prediction which can acquire preferable predictive power than directly employ ing svm without data pre processing .
wang et al.
proposed a new defect prediction model based on c4.
model which introdu ces the spearman s rank correlation coefficient into the process of choosing root node of the decision tree .
tuthan et al.
showed that the prediction performance can gain an improvement by using the weighted n a ve bayes classifiers that assign weights to static code attributes .
in the process of defect prediction type i misclassification cost and type ii misclassification cost are different.
cost sensitive learning methods can address this issue by generating a classification module with minimum misc lassification cost .
zheng presented cost sensitive boosting algorithm to improve neural ne twork classifiers for defect prediction which incorporate s the misclassification costs into the weight update rule of boosting such that the classification per formance on those samples with higher misclassification cost s can be improved .
jiang pointed out that t he effectiveness of fault prediction models varies with the different misclassification cost .
different from the above mentioned methods altering th e data distribution ensemble learning aims at preserv ing original data distribution .
sun et al.
presented a c oding based ensemble learning method which first converts imbalanced binary class data into balanced multiclass data and then bui lds a defect predictor on the multiclass data with a specific coding scheme .
thus it can avoid the loss of important information and class imbalance problem.
the difference between our proposed cddl approach and other defect prediction methods is that we incorporate the stateof the art dictionary learning technique into the field of software defect prediction .
since d ictionary learning has shown powerful classification capability in many application fields we present a dictionary learning approach to im prove the defect prediction performance.
.
dictionary learning both sparse representation and dictionary learning have been successfully applied to many application fields including image clustering compressed sensing as well as image classifica tion tasks .
in sparse representation based classification the dictionary for sparse coding could be predefined.
for example wright et al.
directly used the training samples of all classes as the dictionary to code the query face image and c lassified the query face image by evaluating which class leads to the minimal reconstruction error.
but t he dictionary in his method may not be effective enough to represent the query images due to the uncertain and noisy information in the original traini ng images.
in addition t he number of atoms of dictionary that made up of image samples can also be very large which increases the coding complexity.
dictionary learning dl aims to learn from the training samples space w here the given signal could be w ell represented or coded for processing.
most dl methods attempt to learn a common dictionary shared by all classes as well as a classifier of coefficients for classification .
mairal et al.
proposed a discriminative dl method by training a classifier of the coding coefficients and verified their method for digit recognition and texture classification.
pham et al.
proposed a joint learning and dictionary construction method with consideration of the linear classifier performance and applied their method to object categorization and face recognition .
based on zhang et al.
proposed an algorithm called discriminative ksvd for face recognition.
however the shared dictionary loses the correspond ing relation between the dictionary atoms and t he class labels .
then the classification results based on reconstruction error cannot be gained since this process is closely associated with the class labels.
yang et al.
learned a dictionary for each class and obtained better face r ecognition resul ts than src.
then yang et al further employ ed the fisher discrimination criterion to learn a structured dictionary.
the difference between cddl and traditional dl methods is that we use the supervised dictionary learning technique to solve the binary class classification problem for software defect prediction application .
in addition we design a cost sensitive supervised dictionary learning solution which takes into account the problem that misclassification cost of binary class modules is distinctly different .
.
our approach .
brief introduction of src wright et al.
presented the sparse representation based classification src method that regards a test ing sample as a linear combination of training samples.
suppose that we have c classes of training samples ... mn c a a a a r denotes the set of training samples n ... i imn i i i ia s s s r denotes the subset of training samples from class i m ijsr denotes the thj sample of class i and y denotes a testing sample .
the procedure of src is as follows .
sparsely code y over a via 1l norm minimization 21 arg min ya .
.
do classification by using identity arg miniiye where 2iiie y a ... t ic and i is the coefficient vector associated with class i .
finally the test sample y is assigned to the thi class corresponding to the smallest reconstruction error ie.
.
cost sensitive discriminative d ictionary learning based defect prediction to fully exploit the discriminative information of training samples for improv ing the performance of classification we design a supervised dictionary learning approach which le arns a dictionary that can represent the given software module more effectively .
moreover the supervised dictionary learnin g can also reduce both the number of dictionary atoms and the sparse coding complexity .
instead of learning a shared dictionary for all classes we learn a structured dictionary ... ... ic d d d d where id is the class specified sub dictionary associated with class i and c is the total number of cla sses.
we use the reconstruction error to do classification with such a dictionary d as the src method does.
suppose that ... ... ic a a a a is the set of training samples labeled software modules ia is the subset of the training samples from class i ... ... ic x x x x is the coding coefficient matrix of a over d that is a dx where ix is the sub matrix containing the coding coefficients of ia over d .
we requir e that d should have not only powerful reconstruction capability of a but also powerful discriminative capability of classe s in a .
thus we propose the cost sensitive discriminative dictionary learning cddl model as follow s arg min dx dxj r a d x x where r a d x is the discriminative fidelity term 1x is the sparsity constraint and is a balance factor.
let ... c i i i ix x x x where j ix is the coding coefficient matrix of ia over the sub dictionary jd.
denote the representation of kd to ia as k k k ir d x .
first of all the dictionary d should be able to well represent ia and therefore ... ...ic i i i i i c ia dx d x d x d x .
secondly since id is associated with the thi class it is expected t hat ia should be well represented by id not by jd ji which means both 2i i i ifa d x and 2j jifdx should be minimized.
thus the discriminative fidelity term is c ii i cc ij i i i i i j i f ffij jir a d x a dx a d x d xr a d x an intuitive explanation of three terms in iir a d x is shown in fig.
.
in software defect prediction there are two kinds of modules the defective modules and the defect ive free modules.
fig.
a shows that if we only minimize the ii fa dx on the total dictionary d ir may deviate much from ia so that subdictionary id could not well represent ia .
in order to achieve better powerful reconstruction capability and powerful discriminative capability we add another two parts that 2i i i ifa d x which minimizes the reconstruction error on sub dictionary of its own class and 2j jifdx which minimizes the reconstruction term on sub dictionary of the other class both of them should also be minimized.
fig.
b shows that the proposed discriminative fidelity term could overcome the problem in fig.
a .
ia ir jr ia ir jr a b fig.
illustration of the discriminative fidelity term.
as previously stated misclassify ing defect ive free module s leads to increasing the development cost and misclassif ying defect ive ones is related with risk cost.
cost sensitive learning can incorporat e the different misclassification costs into the classification process.
in this paper we emphasize the risk cost such that we add the penalty factor cost i j to increase the punishment when a defective software module is predict ed as a defect ive free software module.
as a result cost sensitive dictionary learning makes the p rediction incline to classify a module as a defective one and generates a dictionary for classification with minimum misclassification cost.
the discriminative fidelity term with penalty factor s is c ii i cc ij i i i i i j i f ffijr a d x a dx a d x cost i j dra xdx since there are only t wo classes in software defect prediction the defective class and the defect ive free class that is 2c the model of cost sensitive discriminative dictionary learning is 1arg min i d x i i i i i f fdx i j jifjj a dx a d x cost i j d x x where the cost matrix is shown in table .
table .
cost matrix for cddl predicts defect ive one predicts defect ive free one actually defect ive cost actually defect ive free cost .
optimization of cddl a lgorithm the cddl objective function in formula can be divided into two sub problems updating x by fixing d and updating d by fixing x .
the optimizati on proc edure is iteratively implemented for the desired discriminative dictionary d and corresponding coefficient matri x x. at first suppose that d is fixed the objective funct ion in formula is reduced to a sparse coding problem to compute x x x .
here 1x and 2x are calculated one by one.
we c alculate 1x with fixed 2x and then compute 2x with fixed 1x .
thus formula is rewritten as 1arg min i ii x i i i i i f fx j j i ifjj a dx a d x cost i j d x x .
formula can be solved by using the ipm algorithm in .
when x is fixed we in turn update 1d and 2d .
when we calculate 1d 2d is fixed then we compute 2d 1d is fixed .
thus formula is rewritten as 1arg min i iij d i j d j jif ij i i i j iffjj a d x d x a d x cost i j d x where ix is the coding coefficient matrix of a over id .
formula is a quadratic programming problem and we can solve it by using the algorithm in .
table shows the realization algorithm of cddl.
we initialize the sub dictionary of each class by using the pca technique .
since the data dimension of software defect prediction is low pca would construct a complete initialization subdictionar y for each class that is each sub dictionar y has the same number of atoms generally the number of sub dictionary s atoms is equal to the data dimension .
the algorithm of cddl converges since its two alternative optimizations are both con vex.
fig.
illustrates the convergence of the algorithm.
a cm1 dataset .
b kc1 dataset .
c mw1 dataset .
d pc1 dataset .
fig.
convergence of the realization algorithm of cddl on four nasa benchmark datasets.
table .
algorithm of cddl step .
initialize d .
initialize each atom of sub dictionary id by using the pca method.
step .
update the sparse codin g coefficient matrix x. fix d and solve 1x and 2x one by one by using formula .
step .
update dictionary d .
fix x and update 1d and 2d by using formula .
step .
output d .
return to step until the values of dxj in adjacent iterations are close enough or the maximum number of iterations is reached.
step .
classification prediction with src.
.
experimental setup in this section we describe the experimental setup in detail including benchmark database evaluation measures and experiment design.
.
benc hmark datasets in the experiment ten datasets from nasa metrics data program mdp are taken as the test data.
nasa benchmark datasets are publicly available and have been widely used for software defect prediction.
each dataset represents a nasa softwar e system or sub system which contains the corresponding defect marking data and various static code metrics.
the repository record s the number of defects for each module by using a bug tracking system.
static code metrics of nasa datasets include size re adability complexity and etc.
which are closely related to software quality.
here three mentioned metrics above are measured by lines of code loc counts operand and operator counts halstead attributes and mccabe complexity measures respectively.
for brevity we list the common basic metrics selected from metrics of ten nasa datasets such as loc locode locomment loblank and etc.
and their descriptions are given in table .
more detailed description of code metrics or information about the nasa datasets can be obtained from .
brief properties of ten nasa datasets are shown in table .
among the ten datasets the size of each one the number of modules of software ranges from to while the number of attributes range s from to .
.
evaluation measures there are four measures to evaluat e the performance of defect prediction model recall rate i.e.
probability of detection false positive rate precision and accuracy which can be defined by using a b c and d in table .
here a b c and d are the number of defective modules that are predicted as defective the number of defective modules that are predicted as defective free the number of defective free modules that are predicted as defective and the number of defective free modules that are predicted as defective free respectively.
table .
metrics selected from nasa datasets metrics description loc mccabe s line count of code for each module locode halstead s line count of code for each module locomment halstead s count of lines of co mments for each module loblank halstead s count of blank lines for each module locodeand comment halstead s count of code and comments for each module v g cyclomatic complexity for each module ev g essential complexity for each module iv g design co mplexity for each module total op total number of operators for each module total opnd total number of operands for each module uniq op number of u nique operators for each module uniq opnd number of u nique operands for each module n total operators and operands for each module v volume for each module l program length for each module d difficulty for each module i intell igent content for each module b error estimate for each module e programming effort for each module t programming time for e ach module table .
nasa benchmark datasets dataset s number of defective module number of total module s number of attribute s percentage of defective modules cm1 .
jm1 .
kc1 .
kc3 .
mc2 .
mw1 .
pc1 .
pc3 .
pc4 .
pc5 .
table .
defect prediction metric predict as defect ive predict as defect ivefree defect ive modules a b defect ive free modules c d recall rate the ratio is the number of defective modules correctly classified as defect ive to the number of defect ive modules which is defined as a a b .
this ratio is very important for software defect prediction because prediction model intend s to find out defective modules as much as possible.
false positive rate the ratio is the number of defective free modules wrongly classified as defe ctive to the number of defective free modules which is defined as c c d .
precision the ratio is the number of defective modules correctly classified as defect to the number of modules that are classified as defect ive which is defined as a a c .
this ratio evaluates the correct degree of prediction model.
accuracy the ratio is the number of modules that are correctly classified to the number of total modules which is defined as a d a b c d .
the ratio is widely used in all data mining classification applications.
a good prediction model desires to achieve high value of recall rate and precision.
however there exists trade off between precision and recall.
then a comprehensive measure of precision and recall rate is necessary .
f measure is the harmonic mean of precision and recall rate which can be defined as f measure recall precision recall precision all the above evaluation measures range from to .
obviously t he performance is better with higher values of recall precision f measure and accuracy and lower f alse positive rate is desired.
in the experiment we evaluate the performance of all the methods in terms of false positive pf recall pd and f measure.
.
experimental design to evaluate our cddl a pproach we conduct some experiments.
for all selected datasets we use the random division to obtain the training and testing sets for all compared methods.
the random division treatment may affect the prediction performance.
therefore we use the ran dom division and perform prediction times and report the average prediction results in section .
in our approach in order to emphasize the risk cost the parameters cost and cost are set as cost cost .
for different project s user can select different ratio .
and the parameter is determined by searching a wide range of values and choos ing the one that yields the best f measure value.
.
experimental results we compare the proposed cddl approach with several representative methods particularly presented in the latest five years including support vector machine svm compressed c4.
decision tree cc4.
weighted na ve bayes nb coding based ense mble learning cel and cost sensitive boosting neural network cbnn .
in this section we present the detailed experimental results of our cddl approach and other compared methods.
table .
experimental results pd and pf comparisons on nasa s ten datasets data set m svm cc4.
nb cel cbnn cddl cm1 pd .
.
.
.
.
.
pf .
.
.
.
.
.
jm1 pd .
.
.
.
.
.
pf .
.
.
.
.
.
kc1 pd .
.
.
.
.
.
pf .
.
.
.
.
.
kc3 pd .
.
.
.
.
.
pf .
.
.
.
.
.
mc2 pd .
.
.
.
.
.
pf .
.
.
.
.
.
mw1 pd .
.
.
.
.
.
pf .
.
.
.
.
.
pc1 pd .
.
.
.
.
.
pf .
.
.
.
.
.
pc3 pd .
.
.
.
.
.
pf .
.
.
.
.
.
pc4 pd .
.
.
.
.
.
pf .
.
.
.
.
.
pc5 pd .
.
.
.
.
.
pf .
.
.
.
.
.
table shows the pd and p f values of our approach and other compared methods on nasa datasets .
for each dataset pd and pf value s of all methods are the mean value s calculated from the results of runs.
the results of pf value s sugges t that in spite of not acquiring the best pf values on most datasets cddl can achieve comparatively better results in contrast with other methods.
we can also observe that the pd value s of cddl which are presented with boldface are higher than the corre sponding values of all other methods.
cddl achieves the high est pd values on all datasets .
the results indicate that the proposed cddl approach takes the misclassification cost s into consideration which makes the prediction tend to classify the defective free modules as the defective ones in order to obtain higher pd value s. we calculate the average pd va lues of nasa datasets in table .
as compared with other methods the average pd va lue of our approach is higher in contrast with other related method s and cddl improves the average pd value at least by .
.
.
.
table .
average pd value of nasa datasets svm cc4.
nb cel cbnn cddl average .
.
.
.
.
.
table shows the f measure values of our approach and the compared methods on nasa datasets .
in table f measure values of cddl are better than other methods on all datasets which means that our proposed approach outperforms other methods and achieves the ideal prediction effects .
according to the average f measure values shown in table cddl improves the average f measure value at least by .
.
.
.
to sum up table and show that o ur approach has the best achievement in the p d and f measure values.
table .
f measure values on ten nasa datasets datasets svm cc4.
nb cel cbnn cddl cm1 .
.
.
.
.
.
jm1 .
.
.
.
.
.
kc1 .
.
.
.
.
.
kc3 .
.
.
.
.
.
mc2 .
.
.
.
.
.
mw1 .
.
.
.
.
.
pc1 .
.
.
.
.
.
pc3 .
.
.
.
.
.
pc4 .
.
.
.
.
.
pc5 .
.
.
.
.
.
average .
.
.
.
.
.
to statistically analyze the f measure results given in table we conduct a statistical t est i.e.
mcnemar s test .
this test can provide statistical significance between cddl and other methods.
here the mcnemar s test uses a significance level of .
.
if the p value is below .
the performance difference between two compared methods is considered to be statistically significant.
table shows the p values between cddl and other compared methods on nasa datasets where only one value is slightly above .
.
according to table the proposed approach indeed makes a significant diff erence in comparison with other methods for software defect prediction.
table .
p values between cddl and other compared methods on ten nasa datasets dataset s cddl svm cc4.
nb cel cbnn cm1 .
.
.
.
.
jm1 .
.
.
.
.
kc1 .
.
.
.
.
kc3 .
.
.
.
.
mc2 .
.
.
.
.
mw1 .
.
.
.
.
pc1 .
.
.
.
.
pc3 .
.
.
.
.
pc4 .
.
.
.
.
pc5 .
.
.
.
.
.
conclusion although d ictionary learning has been shown effective in other domains to the best of our knowledge we are the first attempt towards improv ing software prediction performance by introducing the dictionary learning technique.
aiming at the characteristic s of defect data we specifically design a supervised cost sensitive dictionary learning approach to predict defect ive module s i.e.
cost sensitive discriminative dictionary learning cddl .
it can fully exploit the class information of historical data to improve th e discriminative power and provides effective solutions for the major problems in the field of software defect prediction which are misclassification cost and class imbalance problem s. as compared with several state of the art representative software def ect prediction methods the experiments on ten nasa dataset s show that the proposed cddl approach performs better.
it significantly improves both the average pd value s and f measure value s on all dataset s. the mcnemar s test experiment shows that the diffe rences between cddl and the compared methods are statistically significant .
experimental results demonstrate the effectiveness of our approach for the software defect prediction task.
software defect prediction techniques are mainly based on a sufficient a mount of historical project data.
however labeling software modules is time consuming .
in practical scenario software project exits many unlabeled software modules .
in order to fully exploit the both labeled and unlabeled information of software modules we plan to expand discriminative dictionary learning to semi supervised dictionary learning .
.