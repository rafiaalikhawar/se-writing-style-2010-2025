performance debugging in the large via mining millions of stack traces shi han yingnong dang song ge dongmei zhang microsoft research asia shihan yidang songge dongmeiz microsoft.com tao xie north carolina state university xie csc.ncsu.edu abstract given limited resource and time before software release development site testing and debugging become more and more insufficient to ensure satisfactory software performance .
as a counterpart for debugging in the large pioneered by the microsoft wind ows error reporting wer system focusing on crashing hanging bugs performance debugging in the large has emerged thanks to available infrastructure support to collect execution traces with performance issues from a huge number of users at the deployment sites.
however performance debugging against these numerous and complex traces remains a significant challenge for performance analysts.
in this paper to enable performance debugging in the large in practice we propose a novel approach called stackmine that mines callstack traces to help performance analysts effectively discover highly impactful performance bugs e.g.
bugs impacting many users with long response delay .
as a successful technology transfer effort since december stackmine has bee n applied in performance debugging activities at a microsoft team for performance analysis especially for a large number of execution traces .
based on real adoption experiences of stackmine in practice we conducted an evaluation of stackmine on performan ce debugging in the large for microsoft windows .
we also conducted another evaluation on a third party application .
the results highlight substantial benefits offered by stackmine in performance debugging in the large for large scale software systems.
i. introduction a modern software system tends to include an increasingly large number of components and lines of code loc and depend on an increasingly large number of system components .
for example among commercial software systems more than microsoft products could amount to more than million loc .
to assure high quality of such modern software systems testing and debugging remain the most commonly used techniques.
however g iven limited resource and time before software release development site testing and debugging become more and more insufficient to ensure high software quality.
development site testing typically covers only a limited percentage of various usage scenarios and vast multitude of execution environ ments that could occur upon the deployed software systems.
development site debugging typically relies on a limited number of failing runs which could be insufficient for effective debugging.
furthermore during development site debugging with limited res ource and time developers have relatively little knowledge on which bugs should be given higher priority to fix based on their impact on users including their impact scope e.g.
which bugs impact a large number of users at deployment sites and their impact severity e.g.
which bugs impact an individual user at a high severity level.
to address these issues recent industrial solutions are developed and deployed for collecting and leveraging a high volume of deployment site usage data to improve debuggin g with postmortem analysis.
there emerges a new frontier of debugging practice debugging in the large with a prominent example as the microsoft windows error reporting wer system a.k.a.
dr. watson .
using a long time p eriod in the post release stage and a huge number of information sources from real world users wer allows developers to obtain distribution information of crashing hanging bugs to guide their debugging prioritization.
since its operation in wer has accomplished huge success within microsoft.
however in spite of high importance performance bugs are not handled by wer which heavily focuses on crash hang debugging.
performance is one of the key properties of software primarily concerning responsiven ess throughput and resource utilization.
for example to conduct performance debugging along a call path associated with a thread blocked on a wait performance analysts typically start with looking for a subsequence of function calls that account for a non trivial portion of waiting time and then try to derive performance signatures from the subsequence.
a performance signature is one or more functions in which a potential performance bug resides often manifested as the location where the fix to the bu g is applied.
optimizing code implementation within the performance signature could reduce time consumption on wait.
for example a subsequence of function calls initcomponents gethashcode getshortpathname mmaccessfault comes from a trace associated w ith slow startup of a third party application accounting for approximately .
second waiting time.
there the function gethashcode is a performance signature fixing which could resolve the performance issue of slow startup this example is explained in more details in section .
in contrast to performance debugging against one or a few execution traces performance debugging in the large deals with execution traces with performance issues from a huge number of users from the deployment sites.
similar to wer microsoft has also provided infrastructure support for such purpose with mechanisms such as perftrack based on the event tracing for windows etw platfo rm.
in particular perftrack resides inside windows for measuring system responsiveness to user actions on operating systems os .
for example when a user click s on .
c ieee icse zurich switzerland 176a folder name perftrack measures how long it take s for the user to receive an expected reaction from the system and if the response time exceeds a pre defined threshold perftrack sends execution traces based on etw such as callstack information collected in the preceding period back to microsoft for performance debugging.
note that other o s platforms also provide similar mechanisms such as dtrace for solaris and several other unix like systems.
such execution traces collected from real users provide substantial advantages as discussed earlier including mor e traces with performance issues and more information on a performance issue s impact on users.
for example performance analysts now can start with looking for a subsequence of function calls that account for a non trivial portion of time consumption across all the collected trace s from real users to hunt for highly impactful performance bugs e.g.
bugs impacting many users with long response delay .
for example the earlier described subsequence of function calls account s for a total amount of .
secon ds waiting time from startups of the application .
however there exists no effective support for performance debugging in the large similar to wer to deal with the high volume of complex traces for performance debugging.
for example perftrack collected execution traces including more than billion of callstacks for response delay of the windows explorer user interface ui far beyond affordable investigation effort of performance analysts.
to fill the gap of existing performance debugging practices a nd the vision of performance debugging in the large similar to wer in this paper we propose stackmine a novel approach to enable performance debugging in the large and its supporting scalable system for postmortem performance debugging as the counterpa rt of wer in performance debugging.
our approach includes a novel costly pattern clustering mechanism consisting of two phases to reduce investigation effort of performance analysts.
in the first phase we apply a costly pattern mining algorithm such as subsequence mining on trace streams1 that include callstack traces and then apply clustering on the mined costly patterns based on a set of novel similarity metrics .
we propose these metrics to reflect domain specific characteristics of program executio n traces in contrast to other types of data.
in particular our costly pattern clustering mechanism addresses three significant challenges in performance debugging in the large.
first in modern multi tasking systems the collected trace streams recorded not only performance issue exhibiting executions of an application but also simultaneous normal executions of the same application as well as executions of other applications.
for a subsequence of function calls under investigation its measured performa nce metric values such as waiting time would include contributions from both performance issue exhibiting executions such as lock contention and normal executions such as waiting time on user inputs with the latter as noise for compromising a trace stream records a stream of system level events to capture the program execution behaviors during a time period of bad performance .
effectiven ess of performance analysis.
to address this challenge we propose the wait graph model which extracts from raw trace streams relevant traces with respect to performance issues before we apply mining techniques on the traces.
second it is infeasible to enumerate all subsequences of function calls and then rank them based on their time consumption across trace streams due to the combinatorial explosion.
to address this challenge we propose to conduct subsequence mining on traces to extract highly impac tful subsequences of function calls.
based on overall performance metric values of the traces performance analysts can set a meaningful performance metric threshold for the subsequence mining algorithm to efficiently output only costly subsequence pattern s of function calls with performance metric values beyond the threshold.
third partly due to trace collection from various deployment sites with different execution environments a unique nature of performance debugging in the large multiple mined cost ly patterns of function calls with slight differences could be related to the same performance bug compromising effectiveness of performance analysis for two main reasons.
first analysts would waste time to investigate multiple variant patterns correspon ding to the same performance bug.
second the impact computation used to hunt for performance bugs w ould be inaccurate since the contributions of the same performance bug are dispersed among multiple patterns.
for example a series of function calls for w indows bit on windows bit wow64 simulation would appear in trace streams on a bit system but would not appear in the corresponding ones on a bit system.
as another example a performance issue exhibiting subsequence of function calls in the kernel mode might be invoked by some user mode functions in some trace streams and by different user mode functions in some other trace streams.
to address this challenge we propose a clustering mechanism to group m ultiple mined costly patterns of functi on calls with slight differences .
stackmine is the outcome of two year effort of continuous development and improvement at the software analytics group of microsoft research asia in collaboration with microsoft product teams.
as a successful technology transfer effort since december stackmine has been applied in performance debugging activities at a microsoft team for performance analysis especially for a large number of execution traces.
during this period performance analysts in the team have app lied stackmine to analyze hundreds of millions of callstacks.
stackmine has been shown to reduce more than of the human investigation effort for identifying highly impactful bugs.
the substantial benefits of stackmine have been reflected by feedback gi ven by the performance analysis team we believe that the stackmine tool is highly valuable and much more efficient for mass trace streams trace streams analysis.
for trace streams we believe the tool saves us weeks of time to create new performance signatures which is quite a significant productivity boost.
this paper makes the following main contributions the first formulation and real world deployment of performance debugging in the large as a data mining problem on callstacks the w ait graph representation abstracted from callstacks to capture program behaviors essential to performance analysis a clustering mechanism for reducing costly pattern mining results based on domain specific characteristics of program execution traces industrial experiences on using stackmine in performance debugging in the large for microsoft windows .
for example a microsoft performance analyst applied stackmine on real world trace streams for response delays in the windows explorer ui.
the analyst s reviewed the top pattern clusters produced by stackmine created performance signatures and identified highly impactful hidden performance bugs of windows explorer.
these bugs had been hidden for at least more than one year since the release of windows and some of them can even be traced back to earlier versions of windows.
third party application experiences on using stackmine in performance debugging in the large for a third party application.
with the assistance of stackmine we discover ed highly impactful performance bugs of this application.
among these bugs of them have been confirmed with third party sources and of them is yet to be confirmed.
ii.
problem formulation in this section we first use an example to informally explain the intuitions behind formulating performance debugging in the large as a data mining problem.
we then provide a more formal description of the problem.
a. performance bug hunting from callstacks figure shows an il lustrative scenario within a small time window of a program s execution such as startup of program foo.
along the time axis thread t1 was running until it was blocked at time t1 because it requested a lock held by another thread t2.
then a context switch from t1 to t2 occurred on the cpu and then t2 entered its running state while t1 entered its waiting state.
after t2 released the lock requested by t1 another context switch occurred to enable t1 to resume running at time t6.
after thread t1 finished its execution the corresponding startup time of foo exceeded a pre defined threshold satisfying the triggering condition for trace collection.
then a low overhead mechanism for trace collection such as etw can collect two types of events with callstacks shown in the figure.
w1 is a waiting event of t1 with callstack snapshot w1.s with an upper function invoking a lower function on which t1 was blocked and switched out of the cpu.
we name this type of callstack as a waiting stack .
r1 r2 r3 and r4 are r unning events of t2 which are sampled with callstack snapshots r1.s r2.s r3.s and r4.s respectively.
typically the sampling is at a constant rate for every cpu e.g.
once per millisecond.
we name this type of callstack as a running stack .
from a thre ad s point of view its running stacks capture how it behaves when using the cpu resource while its waiting stacks capture why it is temporarily blocked and switched out of the cpu.
the waiting time associated with a waiting stack shows for how long the t hread is blocked while the sampling rate and sample size determine how much cpu time is associated with a running stack such as ms period of cpu consu mption for t2 in figure .
based on the experience of perfor mance analysts from microsoft a majority of existing software performance bugs fall into two categories cpu consumption and wait.
cpu consumption bug.
as shown in figure the cpu usage pattern of thread t2 can be reflected by running stacks r1.s r2.s r3.s and r4.s.
we notice that a subsequence pattern from these running stacks func ra func rb func rd func re func rf is executed throughout the ms period of cpu consumption.
we name such non consecutiv e subsequence pattern from callstacks as callstack pattern .
if a large amount of cpu consumption from many trace streams is observed on this callstack pattern it is suspicious to be a highly impactful performance bug.
the aggregated performance metric val ues on this callstack pattern can be used to reflect its impact and to rank the bug along with other bugs.
wait bug.
as shown in figure the wait reason of thread t1 at time t1 can be reflected by waiting stack w .s.
if a large number of waiting events from many trace streams are observed on the same waiting stack or on waiting stacks containing a common callstack pattern reflecting the same waiting reason e.g.
func wa func wc func wd func wf it is quite suspicious to be a highly impactful performance bug.
the content of the callstack pattern the full waiting stack w1.s can also be a callstack pattern can be used to both represent the bug and help analysts narrow figure .
a time window of program execution time axis func wa func wb func wc func wd func we func wf func wg func ra func rb func ry func rd func re func rf func rg w g func ra func rb func rx func rd func re func rf func rg w g func ra func rb func ry func rd func re func rf func ra func rb func rx func rd func re func rf running waiting w1.s stack of waiting event w1 of thread t1 at time t1 thread t1 thread t2 w1.s r1.s r2.s r3.s r4.s stacks of running events r1 r2 r3 and r4 of thread t2 at a sampling rate of once per millisecond t1 t2 t3 t4 t5 t6 178down the investigation scope.
the aggrega ted performance metric values on the callstack pattern i.e.
the number of occurrences and total waiting time can be used to reflect its impact and to rank the bug along with other bugs.
b. problem definition before we formalize the problem we define some concepts used in the problem definition.
definition .
a callstack is a sequence of function calls where is a function call for .
definition .
an event is of running event type or waiting event type with fields defined as in table table .
definitions of the fi elds in an event field type description callst ack running stack if waiting stack if int64 timestamp of event int64 cost as cpu consumption in ms if cost as waiting time in ms if int32 thread id definition .
a trace stream is a sequence of events where is an event for .
definition .
a callstack pattern for a set of trace streams is a non consecutive subsequence of a callstack of i.e.
.
definition .
the cost of a callstack pattern for a set of trace streams is defined as basic problem definition inputs a set of trace streams .
a number as the threshold of waiting time or cpu consumption time.
outputs all callstack patterns for where sorted by their costs in the descending order.
to make the problem manageable we reduce the basic problem to a costly subsequence pattern mining problem where the outputs are costly callstack pa tterns whose costs are equal to or higher than .
furthermore because m ultiple mined costly callstack patterns with slight differences could be related to the same performance bug we apply clustering on the mined costly subsequences to produce the final output as a set of clusters of costly callstack patterns where a cluster of callstack patterns is a set of callstack patterns denoted as .
we next define four typical performance metrics for a callstack pattern cluster as below total cost number of trace streams number of events average event cost iii.
approach overview for the described problem we propose our stackmine approach that includes the costly pattern clustering mechanism to address significant challenges in performance debugging in the large.
in particula r given a large set of trace streams stackmine includes three steps to reduce the investigation scope as callstack pattern clusters for performance analysts to investigate.
aoi extraction for different subjects under analysis the area of interests aois within the trace streams might be different.
for example when analyzing response delay of the windows explorer ui a large proportion of the given trace streams can be irrelevant to the delay in the ui thread.
the aoi extraction step is responsible for extracting relevant events and callstacks based on dependencies among thread executions due to resource sharing.
costly maximal pattern mining from callstacks in the extracted aois this step mines callstack patterns out of waiting stacks and runni ng stacks respectively with maximal subsequence pattern mining.
callstack pattern clustering to group variations of one performance bug that are in the form of a set of similar callstack patterns we propose a similarity model for callstack patterns a nd conduct hierarchical clustering on callstack patterns with this similarity model.
recall that each callstack pattern cluster is associated with a set of performance metrics as presented earlier.
analysts can inspect the pattern clusters ranked based on one of these metrics.
we next illustrate the technical details of these three steps.
iv.
aoi extraction the step of aoi extraction addresses two major issues related to effectiveness and efficiency respectively faced when applying performance analysis direct ly on all events from the trace streams.
first a trace collection platform typically collects trace streams that record not only the problematic time period with respect to performance but also some normal time periods around the problematic period.
in m odern multi tasking system environments even within the problematic period including performance issue exhibiting executions such as 179lock contention there would be normal executions such as waiting time on user inputs recorded together.
performance metric values of such normal executions need to be excluded since they would be noise for compromising effectiveness of performance analysis.
second all events within collected trace streams are typically too many and too complicated for even well designed and engineered mining algorithms to handle.
to address these issues based on more than year s interactions and collaborations with microsoft performance analysts from performance analysis industrial practices we identified two effective aoi extractio n techniques scope based extraction and content based extraction and built tool support in stackmine to enable such effective and efficient aoi extraction.
note that in practice performance analysts often use a mixture of these two techniques in this st ep.
scope based extraction.
recording a trace stream is typically triggered by some performance symptom e.g.
delayed finish of a feature or delayed handling of a message.
based on the recorded information of the trace stream such symptom can be automatically identified in the form of a triple denoting the execution of a thread within a time period of bad performance.
with respect to the identified symptom our technique identifies a scope to include the symptom s relevant events and callstacks within the trace stream.
in particular we propose the w ait graph model to extract such scope for a symptom.
a wait graph denoted as consists of a vertex set and an edge set .
each vertex represents a running or waiting event of a thread.
each directed edge always starts from a vertex for a waiting event and ends at a vertex for a running or waiting event denoting that the time span of has overlapping with the time span of and s thread makes s thread ready where could be acquired by calling .
figure shows an algorithm for constructing a wait g raph.
from the wait graph for a symptom we identify the relevant scope as all the waiting events and running events from the vertices in the graph.
despite the similar shape t he wait graph substantially differs from the wait for graph .
the wait graph uses finer grained entities running waiting events instead of processes threads as nodes uses directed edges to express their timing dependencies instead of resource dependencies and is used to reduce the inve stigation scope instead of detecting deadlocks.
note that the key idea of our technique can be applied in a more aggressive way to further reduce the investigation scope by identifying the relevant scope as the waiting events and running events from only the ui thread s or the critical paths of the wait graph as often done by microsoft performance analysts .
content based extraction.
performance analysts may often initiate performance analyses with certain hypotheses or focuses in mind .
for example microsoft p erformance analysts initiated a real world analysis when they suspected that some locks in the win32k.sys module caused poor performance and initiated another real world analysis when the analysts wanted to discover hard fault related bugs for causing slow windows logon.
in th ese situation s our technique allows analysts to conveniently extract required events e.g.
including at least one function for win32k.sys lock access or including one function for hard fault handling in its callstack for the preceding two examples respectively .
v. callstack pattern min ing after the step of aoi extraction our proposed mining algorithm mines costly callstack patterns from t wo types of callstacks within the extracted aoi i.e.
waiting stacks and ru nning stacks separately .
our algorithm is a novel adaptation of a classic algorithm for mining frequent maximal2 subsequences where the support of a pattern is the number of supporting entities in the input database such as transactions and sequences.
both a costly callstack pattern targeted by our algorithm and a frequent subsequence pattern targeted by the classic algorithm obey the apriori property whereas a nonempty subsequence of a frequent pattern must also be frequent.
in particular we adapt the classic bide algorithm with two major modifications .
first we modify the support of a subsequence pattern from its occurrences to .
second f or maximal pattern checking we modify the condition in the forward backward extension check of a super sequence pattern from originally used to mine closed patterns to .
for the condit ion an analyst can follow some guideline to determine an appropriate threshold value for the analysis task at hand.
for example assume that an analyst intends to conduct performance analysis on one when a frequent costly pattern is maximal none of its super patterns can be frequent costly .
for our problem we choose maximal patterns since so helps produce desirable results for insp ection reducing the pattern set while preserving call path information.
inputs trace stream and symptom denoting execution of thread within with bad performance output wait graph .
.
foreach where .
if .
.addnode .
.
foreach .
.addnode .
if .
.getreadierthreadid .
foreach where .
if .
.addnode .
.addedge .
if .
goto .
return figure .
pseudo algorithm for wait graph construction 180performance bug triggered in over of the collected trace streams with an impact of .
second response delay on average .
then the analyst can set seconds since seconds .
second trace stream trace streams .
vi.
callstack pattern clu stering given the mined cost ly callstack patterns our step of clustering groups similar patterns into clusters with hierarchical clustering a popular clustering technique offering two main benefits .
first when explor ing together callstack patterns of different variations analys ts can more easily recognize the common part and the variant part of the callstack patterns .
so not only helps better determine whether and where there is a hidden performance bug but also helps find a high covering fix likely falling into the comm on part that could fix performance issues across various similar patterns.
second using performance metrics of a cluster can help produce better prioritization of results for investigation since w ithout clustering performance metric values of a high impact bug can be spread across multiple callstack patterns of different variations causing the bug not to emerge with a high rank .
a key component of any clustering technique is the underlying similarity model used to measure how similar two entities are.
b ased on years of performance analysis experiences of microsoft performance analysts we construct a novel similarity model sharing the key concept with the edit distance model that takes into account characteristics of p rogram s and their behaviors .
in particular we calculate similarity of two callstack pattern s with two major steps align the two patterns section a and then calculate the similarity of the two patterns section b based on the weighted similarity between the aligned segments of the two patterns section c .
a. alignment similar to the edit distance model we use dynamic programming search to derive an optimal alignment with respect to minimizing the total cost of three kinds of editing operations match insertion deletion substitution.
for example g iven a pair of callstack patterns and we get the optimal alignment in the form of a series of operation segments as shown in figure .
when computing operation cost w e set the cost of a match operation as the cost of an insertion delet ion operation as .
and the cost of a substitution operation between function and as where is a cost function defined based on their function names we split a name into words by treating upper case characters as word boundaries intuitively and are more similar when is small er.
we define the preceding cost function based on substantial observation s from practice that two different callstack patterns of the same performance bug tend to share the similar name structure and words across their corresponding functions in a substitution segment with an example as segment s1 in figure .
more formally we define a segment of consecutive match operations as such as and shown in figure .
each represents a function that appears in both callstack patterns at th e corresponding position .
we assign and with the value for the sake of simplicity denoted as .
we define a segment of consecutive insertion deletion operation s as such as shown in figure .
each represents a function that appears in one callstack pattern at the corresponding position .
.
we defin e a segment of consecutive substitution operations from the left callstack pattern to the right one as such as shown in figure .
each pair of and represent s a pair of functions from the two patterns that are different at the corresponding position .
.
given a pair of callstack patterns we first derive an alignment of the callstack patterns as a set of operation segments as .
then based on this alignment we calculate the similarity of the two callstack patterns denoted a s with details illustrated next.
b. similarity calculation given an alignment of a pair of callstack patterns we use the edit distance model to calculate their similarity as the ratio of the length of the match operation segments over the length of all the segments.
however using only the lengths i.e.
the number of function calls of aligned segments to calculate similarity often cannot reflect desirable similarity in terms of program behaviors.
to address the issue we define a set of we ight functions on segments and then define the similarity as app main initialize initcomponents hashtableoperate gethashcode getshortpathname mmaccessfault miissuehardfault iopageread wow64serv ice wow64system wow64queryattr swapkernelstack expandkernelstack id1 id1 id1 insertion deletion substitution match m2 m2 m2 sl1 mmaccessfault miissuehardfault iopageread match app main initialize initcomponents hashtableoperate gethashcode getshortpathname m1 m1 m1 m1 m1 m1 m1 id1 s1 m2 sr1 callstack pattern l callstack pattern r figure .
an illustration of callstack similarity model 181we denote the function in a segment as where .
once we define a weight function for each function we can define the weight functions for the three types of segments as we define function weight in the next sub section.
c. function weight calculation we define the weight of function as in which denoting unigram information represents the weight for reflecting how unlikely is a common purpose function e.g.
the app main function.
note that a n editing operation on a common purpose function has less contribution to the overall pattern similarity than a non common purpose function .
and denoting forward bigram information and ba ckward bigram information respectively represent the weights for reflecting how unlikely is in a constant dominant call path given its caller and callee functions as context.
note that a callstack segment has less contribution to the overall pattern similarity when the segment is in a constant dominant call path.
for functions and we calculate and based on statistics derived from our trace stream database beyond just the trace streams under investigation as specially we define .
intuitively tends to be a common purpose function when is small.
tends to be a part of a constant dominant call path when or is small.
vii.
evaluations we conducted two evaluations on stackmine with two of the most popular real world software products microsoft windows and a third party application respectively.
in the evaluation with windows we intended to answer three research questions with results produced when microsoft performance analysts used stackmine in real world industrial settings.
q1.
how much does stackmine improve practices of the performance debugging in the large?
q2.
how well do the performance signatures derived with the assistance of stackmine capture perfor mance bottlenecks caused by performance bugs?
q3.
how does stackmine perform compared to alternative techniques?
stackmine was motivated by the needs of microsoft windows teams.
our evaluation serves as the first reported experience of performance debugg ing in the large for such large scale software products as windows.
after research and incubation for more than one year with close collaboration between microsoft researchers and performance analysts stackmine has been adopted by one team of windows per formance analysts and is becoming part of the standard windows performance analysis workflow.
our evaluation focuses on real world experiences of microsoft performance analysts when applying stackmine on a large performance analysis task response delay a nalysis of the windows explorer in short as windows explorer user interface ui .
due to confidentiality we are not able to disclose some low level details of the first evaluation s results such as detailed descriptions of the investigated bugs.
to al low the community to build upon our research and results with the help of researchers from north carolina state university our approach was applied on one popular third party application.
the details of the evaluation results can be found on our project website .
note that in our setting we take the role of third party performance analysts other than the developers of the application lacking deep knowledge of the application s code base while achieving substantial success with the assistance of stackmine.
a. windows study in december as a continued effort to improve the performance of windows performance analysts from one performance analysis team for microsoft windows planned to conduct an investigation to hunt for the hidden performance bugs that caused common impact on windows explorer ui response.
the investigation was against a large set of etw event trace streams collected through the perftrack mechanism.
this initial set included over such trace strea ms collected by satisfying a triggering condition of windows explorer ui response delay.
the performance analysts focused on trace streams by first randomly sampling trace streams and then excluding irrelevant ones e.g.
those not including a ny key function calls related to the windows explorer ui .
among the trace streams there were million callstacks in total among which million were waiting stacks and million were running stacks.
each typical trace stream file can include h undreds of thousands of events and callstacks with hundreds of megabytes in binary format.
q1.
overall improvement of practices given the trace streams stackmine took about hours of automatic analysis to output a ranked list of pattern clus ters.
in particular the aoi extraction phase reduced the million callstacks to thousand callstacks using both scope based in particular the critical path model and content based 182techniques.
the maximal callstack pattern mining produces cos tly patterns.
the callstack pattern clustering produces the final ranked list of the pattern clusters.
one analyst took day hours to go through the top clusters in the descending order of the metric values as defi ned in section b and derived performance signatures from these clusters.
these signatures covered .
of the response delay time of the windows explorer ui captured in the trace streams.
with additional deep in vestigation on both trace streams and source code based on the signatures the analyst successfully diagnosed and filed highly impactful hidden performance bugs of windows explorer.
these bugs had been hidden for at least more than one year since the release of windows and some of them can even be traced back to earlier versions of windows.
in terms of their performance impact one of the bugs caused significant response delay observed in of the trace streams and another one caused signifi cant response delay observed in of the trace streams with on average .
seconds of ui response delay once triggered.
ideally we would like to measure the effort of an analyst without the assistance of stackmine to derive performance signatures for discovering these highly impactful performance bugs in order to measure effort reduction achieved by stackmine.
however we could not attain such statistics because the performance analysis team would not afford to invest analysts to manually investig ate these trace streams before we introduced stackmine into their practices.
therefore we make a rough estimation as below based on past experiences of the performance analysis team.
on average it takes about minutes for an experienced performa nce analyst to derive signatures from a single trace stream.
therefore it would take days to derive signatures if each and every single trace were analyzed manually.
in contrast with stackmine deriving signatures can be finished in day .
the subs tantial benefits of stackmine have been reflected by feedback given by the performance analysis team we believe that the stackmine tool is highly valuable and much more efficient for mass trace streams trace streams analysis.
for trace stream s we believe the tool saves us weeks of time to create new signatures which is quite a significant productivity boost.
q2.
performance bottleneck coverage the study results against q1 show that stackmine substantially reduces the effort of the perf ormance analyst on deriving performance signatures.
however it is an open question on whether the effort reduction compromises the effectiveness i.e.
missing other highly impactful hidden performance bugs beyond these bugs that could be found by mo re expensive investigation without the assistance of stackmine.
to address this evaluation issue we propose and measure the performance bottleneck coverage of a set of performance signatures as below the coverage denotes the proportion of the bad performance time period that analysts can explain and take action on with the performance signatures.
the higher coverage the performance signatures achieve the lower possibility highly impactful performance bugs remain not captured by the performance signatures.
the reason is that shorter not covered bad performance time period causes the remaining performance bugs if any to be less highly impactful i.e.
causing less significant delay .
recall that the analyst identified performance signatures from the pattern clusters.
these signatures achie ved .
performance bottleneck coverage nearly of the total response delay captured in the trace streams can be explained and improved with these performance signatures.
the performance analysis team indicated to us that the achieved coverag e was quite satisfactory.
among these signatures two signatures led the analyst to discover two highly impactful performance bugs with high impact as described in section respectively.
q3.
comparison with alternative tech niques we identified three alternative techniques along with stackmine that rank trace streams for the performance analyst to investigate.
random this technique ranks trace streams in a random order serving as the baseline technique greedy total t his technique ranks trace streams in the descending order of total ui response delay time within a trace stream greedy max this technique ranks trace streams in the descending order of the maximum ui response delay time within a trace stream stackmine this technique first defines a set of performance signatures signatures in from the top pattern clusters clusters in .
after the signatures are sorted by their performance bottlene ck coverage the technique then selects one trace stream containing the top signature to investigate.
then it selects a new trace stream containing the next not yetinvestigated signature and so on.
figure shows the number of trace streams y axis required to achieve a certain performance bottleneck coverage x axis for the random greedy total greedy max and stackmine techniques respectively.
for example number of required trace streams to investigate performance bottleneck coverage baseline random greedy total greedy max stackmine figure .
comparison with alternative techniques 183these four techniques require the analyst to inves tigate and trace streams respectively to achieve .
coverage.
on average stackmine requires only .
.
and .
of trace streams required by the random greedy total and greedy max techniques respectively to achieve a cert ain performance bottleneck coverage.
the results highlight the substantial advantages of stackmine over other alternative techniques.
b. study results on a third party application to allow the community to build upon our research and results with the help o f researchers from north carolina state university our approach was applied on one popular third party application3.
such case reflects situations where users of our approach take the role of third party performance analysts other than the developers of the application lacking deep knowledge of the code base of the application.
in particular stackmine was applied to analyze the startup performance of the third party application based on trace streams with the following characteristics.
running a s pecific same version of the application containing the start point of a startup process indicated by the process start event of the application containing the end point of the startup process indicated by the first occurrence of function call user32.d ll!peekmessage after the start point of the startup process.
this first attempt for handling a user s input tends to reflect the end point of the startup process.
spending second for the identified startup process.
these trace streams included occurrences of slow startups each second .
the average time consumption of a startup was .
seconds and the slowest one took .
seconds.
these trace streams included million callstacks while million were waiting stacks and million were running stacks.
in this study we focused on wait related bugs.
discovering cpu consumption related bugs can be conducted similarly.
results of applying stackmine we next illustrate the results from each phase in details.
in the phase of aoi extraction wai ting stacks were extracted from the ui thread using the wait graph model.
then aoi extraction produced thousand callstacks with average length of function calls for each callstack.
the phase of maximal callstack pattern mining produced costly pa tterns.
the phase of pattern clustering produced pattern clusters.
the entire automatic processing took less than minutes.
finally these pattern clusters were ranked according to the four different metrics as defined in section b and the top pattern clusters in each prioritized list were examined .
based on the top pattern clusters it took about hour to manually derive performance signatures which achieved .
performance bottleneck cover age of the collected trace streams.
in contrast without the assistance of readers who would like to learn specifics of the third party application could contact the last author.
stackmine it would have to take to hours to manually derive these performance signatures based on empirical estimation.
among the signatures of them with .
perfor mance bottleneck coverage in total corresponded to three major factors that were widely discussed on the internet for slow startup of the application on demand scanning conducted by anti virus software before the application loads some external compon ents loading of a non trivial number of plugins loading of more thannecessary contents by the prefetching functionality.
the three main factors can be considered as known common issues and developers of the application can make improvements acc ordingly.
based on the remaining signatures performance bugs were discovered with .
performance bottleneck coverage.
among these bugs of them have been confirmed with third party sources and of them is yet to be confirmed.
in summary sta ckmine achieved substantial effort reduction with respect to research question q1.
for q2 these signatures achieved satisfying performance bottleneck coverage of .
.
due to space limit in this evaluation we could not include detailed comparison w ith alternative techniques for q3 which would have similar conclusions to what have been shown in the windows study.
representative performance bug we next describe one representative performance bug that has been identified with stackmine.
when ranked b ased on average waiting time this bug emerged in the 24th cluster with an average waiting time of milliseconds over trace streams.
note that among the higher ranked clusters of them were related to our newly discovered bugs while the other were related to known issues due to common factors.
figure shows the common part of the patterns from this 24th cluster.
it is divided into three segments and between the segments there are variant function call paths corresponding to different situations.
without clustering the performance metric value of the same logic but along different call paths would be split into different patterns and thus such splitting would prohibit the bug from emerging from a p attern with a high performance metric value.
reading the common part of the pattern cluster could help learn the following logic.
in segment the application would load a set of components during the initialization stage.
a hash table was used and the getshortpathname windows api function was invoked in gethashcode .
in segment the windows kernel would perform some operations to support getshortpathname .
in segment hard faults happened and disk i o was conducted causing additional time consump tion.
based on the pattern cluster further investigations exposed two major findings.
on one hand segment provided us with sufficient information to quickly locate the problem in the source code e.g.
the invocation of the getshortpathname api function.
the application used a hash table to manage the components that it loaded and the 184path name of a file was used as the key in the hash table.
however windows has a historical issue about short path names.
to ensure that both the long path na me and short path name of a single component would map to the same entry in the hash table the application adopted the short name and therefore the getshortpathname api function was always invoked at the first time of loading a component.
on the other ha nd the investigation on the trace streams exposed that the disk i o in segment was for reading mft of the file system and the readings on mft would introduce severe lock contention with many other applications and os components thus the readings co uld be blocked for a long period of time.
it was observed that all the key functions involved in this performance bug had been captured and connected by a call context in the pattern cluster.
based on the call context it was natural and efficient for an alysts to figure out the performance bug.
beyond that analysts can focus on this call context and figure out the corresponding optimization solution accordingly.
it has been shown that the usage of long path names was dominant on recent windows versions.
therefore an optimization solution is to adopt the long path name as the key of the hash table.
there are simple and reliable ways to detect whether a path name is a short name and the getlongpathname api function needs to be invoked only when a path nam e is detected as a short name.
this optimization solution is expected to significantly reduce the chance to read mft of the file system thus improving the performance.
viii.
related work previous work on performance debugging typically focused on one or a few full or sampled trace streams of a software system in contrast to a large number of trace streams focused by stackmine for performance debugging in the large .
for example ammons et al.
proposed an approach that includes a search tool built upon a simple profile interface to help analysts explore summaries of profile measurements to search for performance bottlenecks within a few trace streams .
their approach heavily relies on manual effort to navigate through traces .
to an alyze a few sampled trace streams collected from deployment sites of modern enterprise class multi tier server applications the ibm whole system analysis of idle time wait approach helps analysts diagnose idle time indicating a lack of forward motion which corresponds to wait bugs handled by stackmine which also handles cpu consumption bugs .
their approach heavily relies on an extensible set of manually specified declarative rules to abstract traces to states of observed idleness.
srinivas and srinivasan proposed to use thresholding and filtering to summarize performance problems on a component basis by identify ing a small set of interesting function calls in manually specified comp onents of interest.
the ibm jinsight tool allows analysts to explore traces at different dimensions with visualization support.
in contrast to these approaches stackmine does not require manual specifications of component s of interest or heavy manual effort to explore traces.
there were various previous approaches on applying frequent pattern mining or clustering on execution traces or source code .
however these approaches typically focus on mining api specifications rather than performance debugging calling for novel techniques in stackmine.
there were a number of previous debugging a pproaches based on real world usage data but none of them focused on performance debugging.
ix.
conclusion to enable performan ce debugging in the large we have proposed the stackmine approach that conducts mining and clustering on callstack traces from trace streams related to performance issues encountered by real world users.
stackmine helps performance analysts effectively di scover highly impactful performance bugs.
since december stackmine has been applied in performance debugging activities at a microsoft team for performance analysis especially for a large number of execution traces.
our evaluations on two large scale real world software products microsoft windows and a third party application demonstrated stackmine s substantial benefits in performance debugging in the large .
exemplified by wer and stackmine we envision and advocate a game changing paradigm for software quality assurance in the large based on usage data collected from the real world in order to cope with increasingly large and complex modern software systems such as ultra large scale systems .
acknowledgment the authors would like to thank the developers and analysts from the microsoft product teams for the collaboration throughout the stackmine project.
the authors would like to also thank the researchers from microsoft research for the discussions on the wait graph definition.
app main initialize initcomponents hashtableoperate gethashcode kernel32.dll!getshortpathname ...... nt!obopenobjectbyname nt!obplookupobjectname nt!opparsedevice ntfs.sys!ntfsfsdcr eate ntfs.sys!ntfscommoncreateonnewstack ...... nt!mmaccessfault nt!miissuehardfault nt!iopageread segment segment segment figure .
callstack pattern of a slow startup bug of the third party application 185references m. acharya t. xie j. pei and j. xu mining api patterns as partial orders from source code from usage scenarios to specifications proc.
esec fse pp.
.
r. agrawal and r. srikant fast algorithms for mining association rules proc .
vldb pp.
.
e. altman m .
arnold s .
fink and n. mitchell performance analysis of idle programs proc .
oopsla pp.
.
g. ammons r. bodik and j.r. larus mining specifications proc.
popl pp.
.
g. ammons j .
d. choi m. gu pta and n. swamy finding and removing performance bottlenecks in large systems proc.
ecoop pp.
.
r. y. chang a. podgurski and j. yang finding what s not there a new approach to revealing neglected conditions in software proc.
issta pp.
.
y. dang s. ge r. huang and d. zhang code clone detection experience at microsoft proc.
iwsc pp.
.
w. dickinson d .
leon and a. podgurski finding failures by cluster analysis of execution profiles proc.
icse pp.
.
k. glerum k .
kinshumann s .
greenberg g .
aul v .
orgovan g .
nichols d .
grant g .
loihle and g .
hunt debugging in the very large ten years o f implementation and experience proc .
sosp pp.
.
z. li and y. zhou pr miner automatically extracting implicit programming rules and detecting violations in large software codes proc.
esec fse pp.
.
b. liblit a .
aiken a .
x. zheng and m. i. jordan bug isolat ion via remote program sampling proc.
pldi pp.
.
e. linstead s .
k. bajracharya t. c. ngo p. rigor c. v. lopes and p. baldi sourcerer mining and searching inter net scale software repositories data min.
knowl.
discov.
vol.
no.
apr.
pp.
.
d. lorenzol i l. mariani and m .
pezz automatic generatio n of software behavioral models proc.
icse pp.
.
a. orso d .
liang m .
j. harrold and r. j. lipton gamma system continuous evolution of software after deployment proc.
issta pp.
.
a. a. porter c .
yilmaz a .
m. memon d .
c. schmidt and b. natarajan skoll a process and infrastructure for d istribut ed continuous quality assurance ieee trans.
software eng.
vol.
no.
aug. pp.
.
m.k.
ramanathan a. grama and s. jagannat han path sensitive inference of function precedence protocols proc.
icse pp.
.
e. s. ristad and p .
n. yianilos learning string edit distance ieee trans .
pami vol.
no.
may pp.
.
t. sch fer j .
jonas and m. mezini mining framework usage changes from instantiation code proc.
icse pp.
.
g. sevitsky w. de pauw and r. konuru an information exploration tool for performance analysis of java programs proc .
tools euro pp.
.
k. srinivas and h. srinivasan summarizing application performance from a components perspective proc .
esec fse pp.
.
s. thummalapenta and t. xie alattin mining alternative patterns for detecting neglected conditions proc.
ase pp.
.
s. thummalapenta and t. xie mining exception handling rules as sequence association rules proc.
icse pp.
.
j. wang and j. han bide efficient mining of frequent closed sequences proc.
icde pp.
.
j. yang d. evans d. bhardwaj t. bhat and m. das perracotta mining temporal api rules from imperfect traces proc.
icse pp.
.
windows 7reliability performance and perftrack .wikipedia.org wiki wait for graph us library windows desktop bb968803 v vs. .aspx