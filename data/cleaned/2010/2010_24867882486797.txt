learning revised models for planning in adaptive systems daniel sykes domenico corapi jeff magee jeff kramer alessandra russo imperial college london uk first.last imperial.ac.ukkatsumi inoue national institute of informatics tokyo japan inoue nii.ac.jp abstract environment domain models are a key part of the information used by adaptive systems to determine their behaviour.
these models can be incomplete or inaccurate.
in addition since adaptive systems generally operate in environments which are subject to change these models are often also out of date.
to update and correct these models the system should observe how the environment responds to its actions and compare these responses to those predicted by the model.
in this paper we use a probabilistic rule learning approach nomprol to update models using feedback from the running system in the form of execution traces.
nomprol is a technique for nonmonotonic probabilistic rule learning based on a transformation of an inductive logic programming task into an equivalent abductive one.
in essence it exploits consistent observations by finding general rules which explain observations in terms of the conditions under which they occur.
the updated models are then used to generate new behaviour with a greater chance of success in the actual environment encountered.
index terms adaptive systems software architecture runtime model machine learning feedback.
i. i ntroduction one of the chief challenges of and motivations for adaptive systems is to recognise that the information traditionally provided by an engineer at design time is to a greater or lesser degree incomplete and inaccurate.
indeed as the system runs its environment evolves such that incompleteness and inaccuracy grow with time and failures become ever more likely.
an adaptive system should adapt in line with the environment s evolution but it cannot reliably do so on the basis of outdated information.
this information typically comes in the form of architectural designs behavioural models requirements or plans which can be used to direct or verify the running system.
one solution to the problem is to keep the system designer in the loop by providing updated models or to minimise inaccuracy by providing abstract strictly declarative models .
a more dynamic and cheaper solution is to use techniques such as those used in machine learning which can automatically update models so that the predictions of the model correspond to the situations that are actually observed at runtime.
in our previous work we separated the highlevel behavioural model of the system from the architectural model and proposed mechanisms to update the architectural model according to observed events such as component failure.what was lacking was a comprehensive automated approach to change the behaviour of the system to respond meaningfully to changes in the execution environment.
in this paper we describe a solution which uses nonmonotonic probabilistic rule learning to revise a behavioural model following observations of real system execution in the environment.
the behavioural model in question comes from our previous work involving reactive planning .
the model domain model hereafter describes the expected behaviour of the environment and what effect actions available to the system have on the state of the environment.
from the domain model a reactive plan is extracted that for every state of the environment specifies a system action that will lead towards a goal .
reactive plans go some way towards handling discrepancies between the expected and the actual behaviour of the environment by checking the current state before performing every action.
if the environment has moved to an unexpected state the system simply performs the action associated with that state.
however if the unexpected state transition occurs consistently under certain conditions there is no way to make use of the fact by feeding the observation back into the domain model.
more importantly if some sequence of actions can no longer lead to the goal because some assumption has been broken the reactive plan will nevertheless continue to perform them.
in other words the reactive plan is generated under the assumption that the specified transitions are nearly certain and any unspecified transitions are very unlikely.
probabilistic rule learning allows us to exploit consistent observations by finding rules in a logic programming language which explain each observation in terms of the conditions under which it occurs.
this technique can discover previouslyunmodelled dependencies between states and actions even pertaining to historical conditions.
the technique also considers observations as inherently probabilistic which can capture the effect of unmodelled conditions and evolution of the environment.
the new rules are automatically combined with the original domain model so that they are considered when a new reactive plan is generated.
furthermore the rules remain human readable so that the system designer has the opportunity to review them whenever convenient.
in this way the adaptive system is able to overcome the incompleteness and978 .
c ieee icse san francisco ca usa63 inaccuracy in its original model sometimes called uncertainty and adapt to an evolving execution environment.
the paper is organised as follows.
in section ii we set out a motivating problem scenario in which the domain model differs substantially from the runtime environment.
sections iii and iv describe the technical details of our approach while section v demonstrates the approach on a case study and section vi provides a performance evaluation.
section vii compares the approach to existing work in the area and section viii concludes.
ii.
t heproblem our previous work has concentrated on elaborating aspects of the three layer conceptual model for adaptive systems shown in figure which aims to stratify adaptive processes into three layers of increasing abstraction .
in the lower component layer reactive domain specific behaviours that require little or no computation overhead are applied.
in the middle change management layer adaptations with a wider scope can be applied by manipulating the configuration of components running in the layer below.
in the upper goal management layer reactive plans are generated from a domain model describing the environment and the actions the system can perform therein.
having three layers means that the computational cost of adaptations involving the upper layers particularly planning is avoided whenever possible.
fig.
.
three layer conceptual model.
the work described in this paper elaborates one function of the goal management layer namely how to use feedback from the running system to improve the domain model.
from the improved model reactive plans are generated that by having a better understanding of the environment will be more likely to achieve the goal.
to illustrate our motivation we show how the approach in would work in the following scenario.
figure depicts the domain model for a robot operating in a factory floor setting.
the task of the robot is to carry fragile objects from one side of the factory state to the other side of the factory state via number of intermediate locations.
one possible run has the robot picking up the object state driving to an intermediate location state driving to the destination state and placing the object at the destination.an alternative run would have the robot driving via a different intermediate location state .
supposing the goal of the system was to reach state the reactive planning technique in could produce the plan shown in figure there is one alternative plan .
the reactive plan associates a single action with every state.
in order to execute the reactive plan the robot must check the current state which can be distinguished by checking where it is in the factory and whether an object has been picked up and perform the associated action.
if executing the action does not lead to the expected state the reactive plan can still continue from the new state.
provided that every performed action eventually has the expected outcome the goal will be reached.
fig.
.
robot domain model.
fig.
.
robot plan.
what if the environment never behaves as expected?
let us now consider some situations in which the plan will fail.
suppose that the sensors used to detect the location of the robot are correct only with some probability pwhen the robot attempts to move to state and otherwise report that the robot is in state .
this may cause the entire plan to fail with probability p or succeed albeit under the belief that it went via state .
let us further suppose that when the sensors fail in this way it disrupts the motion of the robot and the fragile object is smashed with a different probability q. when the object is smashed the act of putting it down fails and so the plan fails.
the probability of this plan succeeding is now p q. note that there is still an alternative plan in which the robot moves from state to that avoids these difficulties.
finally let us suppose that picking up the object fails with a small probability r trapping the robot in state because there is insufficient illumination in the factory perhaps there was a power cut .
then the probability of success is r p q 1the authors experienced this very problem in previous experiments with mobile robots.64under the current plan while the alternative plan has a greater success rate of r. unfortunately the designer of the above domain model did not foresee these failure scenarios.
even if it were possible to predict them it would be extremely difficult to provide exact values forr pandqwithout running tests in situ .
we propose to use trace executions of the running system to perform in effect such testing at runtime and to update the domain model appropriately.
what is needed is an approach that handles unmodelled conditions had the designer foreseen the effect of illumination on pickup it could have been added as an explicit pre condition for pickup and extra behaviour could have been added to deal with the case when there is no illumination such as turnonlights .
by observing that pickup sometimes fails the planner can choose a more reliable action where possible.
probabilistic factors the sensor failure in state and the subsequent chance of smashing the object represent probabilistic factors for which there is no action that could be applied to rectify the problem even if the problem could be reliably sensed .
the only solution is to avoid the situation occurring in the first place by not moving to state .
unmodelled transitions if attempting to move to state in fact reliably takes the robot to state meaning p then a new transition should be added to the model so that it can be taken account of when generating a new plan.
also when p then the probability of the transition from state to is and the transition can be ignored.
unmodelled historical dependencies the success of the putdown action is dependent on which path was taken to arrive at state .
in the case of the path through state success is dependent on the chance of sensor failure.
making the path taken a pre condition of performing putdown would enable the planner to produce plans avoiding the unreliable route.
evolution in the environment will over time exhibit a combination of these issues.
in this paper we propose an approach that takes advantage of existing work on probabilistic rule learning to handle environmental evolution and model inaccuracies through a process of model updating.
iii.
a pproach figure gives an overview of our approach for elaborating the goal management layer of the three layer model.
reactive plans are produced from the domain model provided by the system designer and passed to the lower layers for execution.
the change management layer keeps a record of the traces actually executed along with an indication of whether the goal was reached or not a time out is sometimes necessary to detect failure in practical cases .
since the environment may behave differently from the expectations encoded in the domain model such traces may exist in neither the plan nor the domain model.
once a certain number of runs or specifically failing runs have been executed the traces are passed back to the goal management layer in order to update the domain fig.
.
overview of the approach.
1c o n n e c t e d loc1 l o c .
2c o n n e c t e d loc1 l o c .
3c o n n e c t e d loc3 l o c .
4c o n n e c t e d loc4 l o c .
5c o n n e c t e d loc3 l o c .
6c o n n e c t e d loc4 l o c .
8p o s s i b l e pickup t n o t h o l d s a t h o l d i n g o b j e c t t h o l d s a t a t l o c t .
11p o s s i b l e putdown t h o l d s a t h o l d i n g o b j e c t t h o l d s a t a t l o c t .
14p o s s i b l e move l1 l2 t h o l d s a t a t l1 t c o n n e c t e d l1 l2 .
.
.
.
i n i t i a t e s pickup h o l d i n g o b j e c t t .
19t e r m i n a t e s putdown h o l d i n g o b j e c t t .
i n i t i a t e s move l1 l2 a t l2 t .
21t e r m i n a t e s move l1 l2 a t l1 t .
.
.
.
fig.
.
part of a domain model.
in each rule tindicates a time point.
model and then generate a refined plan.
the planner may also conclude that it is no longer possible to achieve the goal.
this feedback cycle continues for the lifetime of the adaptive system.
a. domain model in order to make use of probabilistic rule learning explained in the next section it is necessary to encode the domain model as a logic program in terms of conditions and actions occurring at time points t similar to event calculus as used in .
in previous work we encoded the domain model as ltl linear temporal logic constraints which are straightforward to transform into a logic program.
figure shows part of the domain model corresponding to the example in figure .
lines describe what actions are according to the designer possible in the domain and under what pre conditions they can take place.
for example theputdown action cannot occur at a time point t unless65holdingobject andat loc5 are true.
the actions such as putdown and conditions such as at loc5 have a domainspecific meaning evaluated at runtime during plan execution which takes place in the lower layers.
lines show the effect of executing actions that is their post conditions.
for instance putdown makes theholdingobject predicate false.
other lines such as express domain specific constraints in this case how factory locations are connected .
we have omitted general parts of the model which are not domainspecific and facilitate model update and planning.
planning in the absence of probabilities is exactly as previously described which when the domain model is given as a logic program corresponds to abductive reasoning as in .
planning with probabilities is described in section iv c. b. execution traces execution traces record what conditions held and what actions were performed at each time point.
in addition success or failure is recorded.
for example the trace 1h o l d s a t a t l o c .
2do pickup .
3h o l d s a t a t l o c .
4h o l d s a t h o l d i n g o b j e c t .
5do move loc1 l o c .
6h o l d s a t a t l o c .
7h o l d s a t h o l d i n g o b j e c t .
8do move loc3 l o c .
9h o l d s a t a t l o c .
10h o l d s a t h o l d i n g o b j e c t .
11do putdown .
represents a successful execution over time points to of the path via state of figure .
in contrast the trace 1h o l d s a t a t l o c .
2do pickup .
3h o l d s a t a t l o c .
4h o l d s a t h o l d i n g o b j e c t .
5do move loc1 l o c .
6h o l d s a t a t l o c .
7h o l d s a t h o l d i n g o b j e c t .
8do move loc4 l o c .
9h o l d s a t a t l o c .
10h o l d s a t h o l d i n g o b j e c t .
11do putdown .
12h o l d s a t a t l o c .
13h o l d s a t h o l d i n g o b j e c t .
14do putdown .
.
.
.
represents a failing trace caused by the location sensor giving an incorrect reading on line resulting in the object being broken and putdown failing at time points and .
traces acquired by executing different plans for a given goal or altogether different goals can be used for model updating.
the use of traces from different plans also reduces the risk of the system entering cycles of unsuccessful adaptations.
note that traces and consequently states and transitions within those traces have a probability by virtue of their frequency within the overall set of traces.iv.
p robabilistic rule learning here we give an overview of nomprol which we use for model updating omitting technical details that the interested reader can find in .
nomprol is a technique for nonmonotonic probabilistic rule learning based on a transformation of an inductive logic programming ilp task into an equivalent abductive one which is solved using efficient answer set programming asp tools.
more specifically given some background knowledge and a set of observed facts the task is to find a hypothesis a set of new or revised rules that explains the occurrence of the observations.
in general there are many such explanations and so they are ranked by the number of observations they are capable of explaining.
in this case the background knowledge also called the theory is provided by the domain model and the observations are provided by the set of execution traces.
the learned rules are added to the domain model so that it better represents the environment as observed thus leading to more successful plans.
a. rule learning the first stage consists of transforming the ilp task into one solvable using asp.
asp produces answer sets that is assignments of true or false to all atoms predicates that satisfy the background theory in conjunction with the observations.
in nomprol the answer sets contain atoms encoding hypotheses sets of rules that explain the observations with respect to the background knowledge.
the set of all answer sets encodes all possible hypotheses.
for example consider the observation of two factsfrain yesterday london yesterday g. the possible hypotheses include h1 frain x london x london x g h2 flondon x rain x rain x g which can be interpreted as if i am in london it is raining and i am always in london and if it is raining i am in london and it is always raining .
in any realistic ilp problem there are of course a huge number of possible hypotheses.
the space of hypotheses can be controlled by using mode declarations .
each mode declaration states that an atom is permitted to appear in the head the consequent or body the antecedent of a rule in the hypothesis.
mode declarations are specified as m mode t r m s wheremis the label the first argument specifies whether it is a head h or body b mode declaration the second argument specifies the maximum number of occurrences of m and the third argument specifies the schema .
a schema sis an atom possibly negated that contains one or more placemarkers for arguments.
a placemarker consists of one of the three symbols for input placemarkers for output placemarkers and for constant placemarkers and a constant called the type.
for example the mode declarations m1 mode h rain date m2 mode h london date m3 mode b rain date 66state that the london andrain atoms are permitted to appear in the head of rules and the rain atom is permitted to appear in the body.
each has an input of type date .
this restricts the space of hypotheses so that the only solution is now h2 flondon x rain x rain x g the rules we are interested in learning to update the domain model state the conditions under which actions can be performed.
therefore we restrict the hypothesis space ato rules containing an action in the head and environment conditions in the body by generating mode declarations mode h succeeds act time for all actions actand for all conditions cond mode b holdsat cond time mode b notholdsat cond time it is also sometimes necessary to have body mode declarations for related domain specific predicates such as connected from figure .
we also restrict the number of occurrences of a condition in a rule to two occurrences in this case since this results in more general rules avoiding over fitting and improves the performance of the learning process.
an added benefit is that fewer more general rules are more readable should the system designer wish to review them.
b. probability estimation once a set of hypotheses hhave been found the next step is to find the probabilities associated with each rule and thus to find the maximum likelihood hypothesis that is to say the set of rules from the space of hypotheses defined by the mode declarations which explains with the maximal probability the observations.
since the absolute value of the time points included in the execution traces is normally not relevant2 we divide the traces into a history and an observed statex for every observed state in the trace.
an observed state consists of a number of holdsat atoms at a particular time point and the associated history consists of all holdsat and doatoms up to that time point.
for example the succeeding trace from section iii b can be divided in four ways one for each time point one of which is x2 holdsat at loc3 holdsat holdingobject holdsat at loc1 do pickup holdsat at loc1 holdsat holdingobject do move loc1 loc3 following this procedure the maximum likelihood hypothesis is the set of rules which explains the most observations x given the preceding history .
2although it could become relevant with long running systems if for instance the robot s battery becomes progressively unreliable.each condition a2ain each rule of a hypothesis has an associated estimated probability .
from these probabilities the probability of a hypothesis can be calculated p h y a2h ay a2anh a is estimated by minimising the mean squared error function mse jxjx i p xij i wherep indicates the ratio of the summed probabilities of hypotheses that predict the observation to those that do not given the current values p xij i p fh2 h ij xigp h p fh2 gp h the ideal values of will ensure all the hypotheses that predict the observation will have higher probabilities compared to those that do not.
for example consider the two hypotheses from the previous section h1 frain x london x london x g h2 flondon x rain x rain x g suppose the probabilities associated with the heads and bodies are initially h1h h1b h2h h2b .
thenp h1 andp h2 .
this gives an mse of .
since hypothesis h2that predicts the observation oflondon yesterday has low values.
increasing h2hand h2btowards 1improves the mse.
once the minimisation of the mse has converged using gradient descent in our current implementation the maximum likelihood hypothesis h can be selected by choosing the one with the largest p .
the probabilities obtained for each hypothesis through minimising the mse indicate how well they predict the observations given the situations in which their conditions hold.
for example in a set of observations in which rain x is true in cases while london x is true in of those cases the rule london x rain x should have a probability close to .
provided there are no other rules in the hypothesis .
c. applying learned rules in order to use the learned rules for planning a probability for the rule as a whole which represents the probability that the performing the action will have the specified successful outcome is required.
however the estimated probabilities do not relate directly to rules but rather to the presence of conditions in the body of rules3.
in other words each rule in the hypothesis h represents a set of possible applicable rules where each applicable rule contains a subset of the conditions mentioned in the original rule.
the probability r of an applicable rule is calculated by multiplying the for 3this representation can be used directly for probabilistic inference.67each condition kept from the original rule by for each omitted condition analogous to equation .
we then select the applicable rule with the highest probability.
for example the rule below with estimated probabilities for each condition can give rise to several possible applicable rules s u c c e e d s move l1 l2 t h o l d s a t a t l1 t .
c o n n e c t e d l1 l2 .
l2 !
loc3 .
l2 !
l o c .
.
the applicable rule omitting the last condition l26 loc5has the greatest probability .
while the rule including the last condition has a lower probability .
.
the chosen applicable rules hereafter we simply call them rules are combined with the original domain model so that a new plan can be generated.
suppose the scenario in section ii resulted in the following learned rules r1 .
s u c c e e d s pickup t .
r2 .
s u c c e e d s move l1 l2 t h o l d s a t a t l1 t c o n n e c t e d l1 l2 l2 !
l o c .
r3 .
s u c c e e d s putdown t n o t happened move loc2 l o c t .
r4 .
s u c c e e d s putdown t happened move loc2 l o c t .
ruler1with probability .
reflects the fact that picking up the object fails some of the time with no other dependencies.
ruler2states that moving to a location succeeds only if the destination is not loc3 reflecting the unreliable sensing in that location.
rule r3states thatputdown will succeed if the robot has not attempted to move to loc3previously.
rule r4states thatputdown can succeed with a small probability if the robot did move via loc3.
the updated domain model can be visualised as the labelled transition system lts shown in figure where each state represents a particular conjunction of conditions.
a transition from a source state to a target state is shown where an applicable rule states that an action from the source state can succeed with an estimated probability and where the target state is specified in the domain model as the outcome of performing that action.
notice that two new states 5a and 5b have been introduced as a result of the happened condition distinguishing between the two possible paths into loc5.
the grey self loops indicate the correspondence between this model and a markov decision process when an action does not succeed the process is expected to remain in the same state with the residual probability4.
figure shows the plan generated from the updated domain model which in contrast to figure avoids the risky move toloc3.
in fact the domain model does not permit such a move by virtue of rule r2.
if however the learned hypothesis 4though in general there is a residual probability of moving to any other state.
fig.
.
updated domain model.
fig.
.
new plan.
stated that the move was possible with a low probability then the planner would have to choose between the paths vialoc3andloc4.
to handle this circumstance a small extension to the planning algorithm is required.
the planner constructs a reactive plan by starting at the goal states and backtracking along transitions adding states until no more states can be added.
when each state is added the planner must choose a transition to include in the plan for that state.
previously when faced with a choice of transitions from the same state the planner would make an arbitrary choice.
our extended planner uses the transition probabilities in order to choose the most reliable path.
to achieve this the planner backtracks from the goal state recording the probabilities of paths the product of transition probabilities leading from previous states.
when all transitions from a state have been considered the maximum probability path is recorded for that state and the planner can consider transitions leading into that state.
when no more transitions can be considered the plan is complete and prescribes for every state the transition that maximises the chance of success over the whole path5.
standard techniques for solving markov decision processes can also be applied given an appropriate reward function.
in figure the transition selected for state goes to state because this path has a higher probability of success .
than the path via state 5a probability .
.
v. f easibility case study we now apply the approach to a case study based on a variant of the production cell taken from .
the scenario comprises a tray of products either processed or unprocessed to which are applied various processing operations such as drilling pressing cooking and so on.
a robot arm moves the 5assuming that the environment behaves as the updated model predicts and the optimisation of mse did not get stuck in a local minimum.681p o s s i b l e p u t d r i l l p t n o t h o l d s a t i n d r i l l p t n o t h o l d s a t d r i l l e d p t h o l d s a t p i c k e d u p p t .
5p o s s i b l e d r i l l p t h o l d s a t i n d r i l l p t n o t h o l d s a t d r i l l e d p t .
8p o s s i b l e g e t d r i l l p t h o l d s a t i n d r i l l p t h o l d s a t d r i l l e d p t n o t h o l d s a t p i c k e d u p p t .
13p o s s i b l e p u t oven p t n o t h o l d s a t i n oven p t n o t h o l d s a t cooked p t h o l d s a t p i c k e d u p p t .
17p o s s i b l e cook p t h o l d s a t i n oven p t n o t h o l d s a t cooked p t .
20p o s s i b l e g e t oven p t h o l d s a t i n oven p t h o l d s a t cooked p t n o t h o l d s a t p i c k e d u p p t .
25p o s s i b l e g e t t r a y p t n o t h o l d s a t p i c k e d u p p t .
27p o s s i b l e p u t t r a y p t h o l d s a t p i c k e d u p p t .
d r i l l e d a n o t cooked a .
cooked b n o t d r i l l e d b .
fig.
.
production cell domain model.
products from the tray to each processing unit and back again.
following we restrict the scenario to two products of type aandband the processing units to a drill and an oven.
figure shows our initial domain model.
the rules have the intuitive meanings such as a product can be placed in the drill unit if it is not already there not already drilled and being held by the robot arm lines .
lines state constraints on the processing of products aandb amust be drilled before being cooked and bmust be cooked before being drilled.
the goal is to have both products aandbcooked drilled and placed on the tray.
unfortunately the lts representations of both the domain and the plan are too large to show here.
we now suppose that the oven develops a fault that with some probability breaks product aif the drill is in use at the same time.
the observable consequence of breaking product ais that the robot arm can no longer successfully pick up the product after a cook action.
after executing the production cell a number of times the domain model should be updated with a new rule to reflect the faulty oven with a body that captures the context in which faults occur.
the maximum likelihood hypothesis produced includes rules distinguishing the two cases when get oven p succeeds.
in the first case when the product is not a the action always succeeds.
the second case states that when the product isaandin drill b holds success is much less likely due to the fault .r1 .
s u c c e e d s g e t oven p t h o l d s a t i n oven p t h o l d s a t cooked p t n o t h o l d s a t p i c k e d u p p t p !
a .
r2 .
s u c c e e d s g e t oven p t h o l d s a t i n oven p t h o l d s a t cooked p t n o t h o l d s a t p i c k e d u p p t h o l d s a t i n d r i l l b t p a .
the updated model provides feedback to the designer and enables the planner to produce an alternative plan that avoids putting ainto the faulty oven when the drill is already in use making overall success far more likely.
while this case study has given us an indication of the feasibility and promise of the approach we recognise that further work is necessary to explore practical constraints on its utility.
the next section discusses the performance of the model updating approach and its impact on the adaptive system as a whole.
vi.
p erformance evaluation a key test of the approach is whether its computational cost is prohibitive for application inside the adaptive feedback loop.
the rule learning part of the approach has a time complexity proportional to the space of possible hypotheses a. this space is dependent on the number of mode declarations employed and is exponential in the permitted number of occurrences of each condition6.
however we found that the rule learning step has reasonable performance in comparison to the probability estimation step using gradient descent in our prototype implementation see figure .
to give an indication of the time taken finding the set of hypotheses for an example of similar size to those described herein took seconds while probability estimation took seconds to converge.
while such performance might ordinarily be a cause for concern inside a classical sense plan act loop the three layer conceptual model means that the costly deliberations of the goal management layer are avoided whenever possible.
in addition since the rule learning can make use of any number of traces and indeed incomplete traces model updates can take place concurrently with normal execution which in some domains may take place over hours or even days7.
when a failure eventually occurs the goal management layer is already prepared with an updated model and a new plan.
since plan execution in the change management layer can tolerate an inaccurate domain model such as the one provided by the system designer it may be acceptable to use suboptimal probabilities resulting from limiting the running time of gradient descent.
indeed the marginally 6hence why we limit the occurrences to two in the motivating example.
7for instance generating the execution traces for the factory floor example took several hours.69updated model may provide further execution traces that improve the performance and accuracy of the next round of learning.
fig.
.
graph showing probabilities of conditions as estimated during gradient descent.
vii.
r elated work the paramount importance of maintaining a model of the environment at runtime and updating it to match observed behaviour as the environment evolves has been recognised in a number of works notably in kami .
in that work a discrete time markov chain dtmc is used to analyse the non functional properties such as reliability of a system with respect to its behaviour.
the probabilities associated with transitions in the model are then updated through bayesian estimation.
while this ensures that the probabilities in the model are progressively more accurate it does not account for the possibility that the structure of the model is incorrect.
moreover the choice of formalism means that the updated probabilities are assumed to be independent the markov property whereas in section ii we showed the possibility of probabilities being dependent upon the path by which the state was reached.
finally the updated information is not explanatory as an inferred requirement to the designer in the way that a learned rule is in the approach described here.
another approach that uses learning as part of a feedback loop is described in .
in that work the model being updated is a mapping from states to architectural configurations where the selected configurations are those with the highest expected reward.
feedback from execution provides updated reward values.
a similar approach learns the model relating feature configurations to the expected values of nonfunctional properties.
our approach is appropriate for more expressive lts models and feedback and architectural issues are handled separately.
ilp has been used previously by alrajeh et al.
to refine system requirements at design time using user provided scenarios.
our approach moves the process to runtime using feedback from the running system as counter examples traces that violate the goal .
the work in likewise uses machinelearning to refine specifications in this case using neuralsymbolic networks that handle noise.
in addition feedback from the running system is used to learn specifications.
this work is likely applicable in an adaptive context.
d ippolito et al.
provide an alternative technique for generating controllers plans for domains in which the successful outcome of actions is not guaranteed in a single execution .
this could be employed in place of reactive planning in the three layer model although the approach only permits a single failing outcome while rule learning may provide several possible outcomes.
viii.
c onclusions and future work in this paper we have presented an approach for updating process models used for planning the high level behaviour of adaptive systems.
updating the model is performed at runtime on the basis of execution traces which since the reactive plan can often continue when an action has an unexpected unmodelled outcome need not have been described in the original domain model.
such updates enable the adaptive system to cope with incomplete knowledge given by the designer and a noisy ever evolving environment.
we use a non monotonic probabilistic rule learning technique nomprol that finds hypotheses consisting of new rules specifying the probability of an action achieving its specified outcome under particular conditions.
the same technique can be used to learn different action outcomes shown in the initiates andterminates clauses in figure which we intend to demonstrate in future work.
nomprol is sound and complete with respect to logic programs and consequently the finite domain models we employ herein.
however the fundamental limitations of this kind of learning for adaptive systems come from the set of conditions which can be sensed from the environment and the actions that can be performed within it.
in other words a rule cannot be learned for a condition that is not sensed or an action that is not performed in the execution traces.
traditionally the designer only models the aspects of the domain thought relevant to the achievement of the system s goals that being the essence of abstraction but in a dynamic environment the designer s assumptions about what is relevant in addition to how they are relevant are likely to be broken.
there is therefore an incentive to guard against future change by modelling irrelevant conditions and actions in case they ever become important for the achievement of goals.
this leads to a trade off between adaptability and cost as the extra modelling increases development time and reduces the efficiency of learning by enlarging the hypothesis space .
one solution to be examined would be to keep the system designer in the loop to manage the macro adaptations that require new actions while the system handles micro adaptations within the existing alphabet of actions autonomously.
as indicated in section vi it is desirable though not critical to have an efficient goal management layer so that new more successful plans are available when required.
in order to keep the examples tractable we currently enforce a strict70limit on the length of learned rules.
we aim in future work to make efficiency gains by exploiting the statistical nature of the environment so that the entire space of hypotheses need not be explored every time.
indeed several questions arise when considering the behaviour of the system over several rounds of learning rather than the single round on which we have focused in this paper.
for instance it is possible that in one round of learning the environment behaves in a highly atypical manner and that the rules learned are unsuccessful when subsequently applied in planning.
however we expect that using many execution traces from a long period many rounds of learning will both reduce its susceptibility to anomalous rounds and improve the learning by avoiding over fitting.
the length of period i.e.
the window from which traces are used is likely a domain specific property.
consider for example an environment that is stable and predictable for a long period and then changes to be highly dynamic or stable for only a few rounds.
in this case a large trace window would be inappropriate as it would make the system very resistant to change.
another potentially fruitful avenue for future investigation would be to allow the system to explore the environment by executing actions outside the strictures of the current plan.
this provides extra information that may become valuable for future adaptations.
indeed an exploratory approach would enable to system to start with an empty domain model and no plan and gradually construct a domain model alternating between exploration to refine the model and exploitation of it to achieve goals.
evidently this is not feasible in some domains e.g.
safety critical systems but where it can be applied safely it presents a relatively new software design process automated prototyping.
finally we make no pretence to have exhausted the field of machine learning.
there is a wide variety of alternative techniques that may be applicable including non symbolic methods.
however our chosen approach has the benefit of producing human readable logical rules that naturally fit our existing planning formalism while enabling us to restructure the model learn historical conditions and deal with a probabilistic environment.