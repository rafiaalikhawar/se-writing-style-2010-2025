program analysis for secure big data processing julian james stephen savvas savvides russell seidel patrick eugster department of computer science purdue university abstract the ubiquitous nature of computers is driving a massive increase in the amount of data generated by humans and machines.
two natural consequences of this are the increased e orts to a. derive meaningful information from accumulated data and b. ensure that data is not used for unintended purposes.
in the direction of analyzing massive amounts of data a. tools like mapreduce spark dryad and higherlevel scripting languages like pig latin and dryadlinq have signi cantly improved corresponding tasks for software developers.
the second but equally important aspect of ensuring con dentiality b. has seen little support emerge for programmers while advances in cryptographic techniques allow us to process directly on encrypted data programmerfriendly and e cient ways of programming such data analysis jobs are still missing.
this paper presents novel data ow analyses and program transformations for pig latin that automatically enable the execution of corresponding scripts on encrypted data.
we avoid fully homomorphic encryption because of its prohibitively high cost instead in some cases we rely on a minimal set of operations performed by the client.
we present the algorithms used for this translation and empirically demonstrate the practical performance of our approach as well as improvements for programmers in terms of the e ort required to preserve data con dentiality.
categories and subject descriptors d. .
software engineering design tools and techniques c. .
general security and protection keywords cloud computing big data privacy financially supported by darpa grant n11ap20014 northrop grumman information systems purdue research foundation grant and google award geodistributed big data processing .
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than the author s must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
ase september vasteras sweden.
copyright is held by the owner author s .
publication rights licensed to acm.
acm ... .
.
introduction the cloud computing model has evolved as a cost e cient data analysis platform for corporations governments and other institutions.
a rich set of tools like mapreduce dryad or spark just to name a few allow developers to execute data analysis jobs in the cloud easily.
however in order to take advantage of these possibilities developers are faced with the decision of moving sensitive data to the cloud and placing trust on infrastructure providers.
even with a trusted provider malicious users and programs and defective software can leak data.
.
computing on encrypted data several existing cryptographic systems cryptosystems allow meaningful operations to be performed directly on encrypted data.
advances occurring regularly in fully homomorphic encryption fhe schemes e.g.
further increase the scope and performance of computations on encrypted data.
these cryptosystems present an opportunity to maintain sensitive data only in an encrypted form in the cloud while still enabling meaningful data analysis.
even within an enterprise maintaining data in an encrypted format helps prevent insider attacks and accidental leaks.
unfortunately such advancements in cryptography have not translated into programmer friendly frameworks for big data analysis with acceptable performance.
currently for a big data programmer to e ciently put existing homomorphic schemes to work she will have to explicitly make use of corresponding cryptographic primitives in her data analysis jobs.
consider a simple program that nds the total marks obtained by students the programmer is burdened by the task of identifying an encryption scheme suitable to represent such an operation paillier in this example with addition and express the operation summing in cryptographic terms multiplication of encrypted operands followed by a modulo division by the square of the public key .
to make this task even more di cult for the programmer big data analysis jobs are typically expressed in domainspeci c security agnostic high level data ow languages like pig latin or flumejava which are commonly compiled to sequences of several mapreduce tasks .
this makes the explicit use of cryptographic primitives even harder.
recent approaches to automatically leveraging partially homomorphic encryption phe e.g.
addition of values encrypted with paillier cryptosystem via multiplication for secure cloud based computing target di erent application scenarios or work loads.
for instance the mysql based cryptdb supports sql queries on encrypted data yet does not enable parallelization through mapreduce mrcrypt only supports individual mapreduce tasks.
.
approach in this paper we present a program analysis and transformation scheme for the pig latin high level big data analysis language which enable the e cient execution of corresponding scripts by exploiting cloud resources but without exposing data in the clear.
we implemented this program analysis and transformation inside a secure pig latin runtime spr .
more speci cally we extend the scope of encryption enabled big data analysis based on the following insights extended program perspective by analyzing entire data ow programs spr can identify many opportunities for operating in encrypted mode.
for example spr can identify operations in pig latin scripts that are inter dependent with respect to encryption or inversely independent of each other.
more precisely when applying two or more operations to the same data item many times the second operation does not use any side e ect of the former but operates on the original eld value.
thus multiple encryptions of a same eld can support di erent operations by carefully handling relationships between encryptions.
extended system perspective by considering the possibility of performing subcomputations on the client side spr can still exploit cloud resources rather than giving up and forcing users to run entire data ow programs in their limited local infrastructure or defaulting to fhe and then aborting when phe does not su ce.
for example several programs in the pigmix i ii benchmarks end up averaging over the values of a given attribute for several records after performing some initial ltering and computations.
while the summation underlying averaging can be performed in the cloud via an additive homomorphic encryption ahe scheme the subsequent division can be performed on the client side.
considering that the amount of data continuously decreases as computation advances in most analysis jobs it makes sense to compute as much as possible in the cloud .
.
contributions the contributions of this paper are as follows.
after presenting background information on homomorphic encryption and pig latin we .
propose an execution model for executing pig latin scripts in the cloud without sacri cing con dentiality of data.
.
outline a novel data ow analysis and transformation technique for pig latin that distinguishes between operations with side e ects e.g.
whose results are used to create new intermediate data and without e.g.
lters .
the results are semantically equivalent programs executable by spr that maximize the amount of computations done on encrypted data in the cloud.
.
present initial evaluation results for an implementation of our solution based on the runtime of pig latin scripts obtained from the open source apache pig pigmix benchmarks.the remainder of this paper is organized as follows.
section presents background information on homomorphic encryption and pig latin.
section outlines the design of our solution.
section presents our program analysis and transformation.
section outlines the implementation of spr.
section presents empirical evaluation.
section discusses limitations.
section contrasts with related work.
section concludes with nal remarks.
a preliminary version of this report outlining our approach and infrastructure appeared at the hotcloud workshop .
.
background this section presents background information on homomorphic encryption and our target language pig latin.
.
homomorphic encryption a cryptosystem is said to be homomorphic with respect to certain operations if it allows computations consisting in such operations on encrypted data.
if e x andd x denote the encryption and decryption functions for input data xrespectively then a cryptosystem is said to be homomorphic with respect to addition if s.t.
d e x1 e x2 x1 x2 dually a cryptosystem is said to provide additive homomorphic encryption ahe .
similarly a cryptosystem is said to be homomorphic with respect to multiplication or support multiplicative homomorphic encryption mhe if9 s.t.
d e x1 e x2 x1 x2 other homomorphisms are with respect to operators such as and order preserving encryption ope or equality comparison deterministic encryption det .
randomized ran encryption does not support any operators and is intuitively the most desirable form of encryption because it does not allow an attacker to learn anything.
.
pig pig latin apache pig is a data analysis platform which includes the pig runtime system for the high level data ow language pig latin .
pig latin expresses data analysis jobs as sequences of data transformations and is compiled to mapreduce tasks by pig.
the mapreduce tasks are then executed by hadoop and output data is presented as a folder in hdfs .
pig allows data analysts to query big data without the complexity of writing mapreduce programs.
also pig does not require a xed schema to operate allowing seamless interoperability with other applications in the enterprise ecosystem.
these desirable properties of pig latin as well as its wide adoption1prompted us to select it as the data ow language for spr.
we give a short overview of pig latin here and refer the reader to for more details.
.
.
types pig latin includes simple types and complex types .
the former include signed bit and bit integers intand long respectively bit and bit oating point values 1according to ibm yahoo estimates that between and of its hadoop workloads are generated from pig scripts.
with cpus at yahoo and roughly running hadoop that s a lot .
278unencrypted data encryption service untrusted cloud service trusted client udfs encrypted data unmodified pig service program transformation execution handler .
.
.
.
.
.
.source script figure execution model of spr.
shading indicates components introduced by spr.
float double arrays of characters and bytes chararray bytearray and booleans.
pig latin also pre de nes certain values for these types e.g.
null .
complex types include the following bags ... are collections of tuples.
tuples ... are ordered sets of elds.
maps are sets of key value pairs key value.
furthermore a eld is a data item which can be a bag tuple or map.
pig latin statements work with relations a relation is simply a outermost bagoftuples.
relations are referred to by named variables called aliases .
pig latin supports assignments to variables.
.
.
operators and expressions relations are also created by applying operators to other relations.
the main relational operators include join this same operator is used with various parameters to distinguish between inner and outer joins.
the syntax closely adheres to the sql standard.
group elements of several relations can be grouped according to various criteria.
note that group creates a nested set of output tuples while join creates a at set of output tuples.
foreach...generate generates data transformations based on columns of data.
filter this operator is used with tuples or rows of data rather than with columns of data as foreach...generate.
operators also include arithmetic operators e.g.
comparisons casts and store andload operators.
pig latin is an expression oriented language.
expressions are written in conventional mathematical in x notation and can include operators and functions described next.
.
.
functions pig latin includes built in anduser de ned functions.
the former include several di erent categories eval functions operate on tuples.
examples include avg count concat sum tokenize.
math functions are self explanatory.
examples include absorcos.
string functions operate on character strings.
examples include substring ortrim.user de ned functions udfs in pig latin are classi ed as built in udfs or custom udfs.
the former are pre de ned functions that are delivered with the language toolchain.
examples include composed functions like sum.
in this paper we focus on the former kind of udfs due to the complexity involved in analyzing the latter.
.
.
example to understand the constructs of pig latin we present a simple word count example in listing .
in this example a relation input lines is rst generated with elements called line of type chararray read from an input input file.
next the script breaks these lines into tokens representing individual words.
then occurrences of a same word aregrouped and their respective number of occurrences counted.
finally the data in this table is stored in a le output file.
1input lines load input file as line chararray 2words foreach input lines generate flatten tokenize line asword 3word groups group words byword 4word count foreach word groups generate group count words 5store word count into output file listing source pig latin script s1 .
execution model spr is a runtime for pig latin that supports cloud based execution of scripts written in the original pig latin language in a con dentiality preserving manner.
the adversary in our model can have full control of the cloud infrastructure.
this means the adversary can see all data stored in the cloud and the scripts that operate on data.
in order to preserve con dentiality in the presence of such an adversary only encrypted data is maintained in the cloud and spr operates on this encrypted data.
however spr does not address integrity and availability issues.
corresponding solutions are described in our previous work which inversely however does not address con dentiality.
figure presents an overview of the execution model of spr.
script execution proceeds by the following steps .
program transformation.
script execution starts when a user submits a pig latin script operating on unencrypted data source script .
spr analyzes the script to identify the required encryption schemes under which the input data should be encrypted.
for the transformed script to operate on encrypted data operators in the source script 279are replaced with calls to secure udfs that perform the corresponding operations on encrypted data.
e.g.
an addition a bin the source script will be replaced by a bmodn2 withnthe public key used for encrypting aandb.
constants are also replaced with their encrypted values to generate a target script that executes entirely on encrypted data.
details of this transformation yielding an encryption enabled script are presented in section .
.
infer encryption schemes.
using the input le names used in the source script the encryption service checks what parts of the input data are already encrypted and stored in the cloud.
some parts of the input data might already be encrypted under multiple encryption schemes to support multiple operations and other parts might not be available in the cloud at all.
the encryption service maintains aninput data encryption schema which keeps track of this mapping between plain text input data and encrypted data available in the cloud.
based on the input data encryption schema and the required encryption schemes inferred in the previous step the encryption service identi es the encryption schemes missing from the cloud.
.
encrypt and send.
once the required encryption schemes that are missing from the cloud is identi ed the encryption service loads the unencrypted data from local storage encrypts it appropriately and sends it to the cloud storage.
the encryption service makes use of several encryption schemes each implemented using di erent cryptosystems.
implementation details of these encryption schemes are presented in .
each of these encryption schemes has its own characteristics.
the rst scheme is randomized ran encryption which does not support any operators and is intuitively the most secure encryption scheme.
the next scheme is the deterministic det encryption scheme which allows equality comparisons over encrypted data.
orderpreserving encryption ope scheme allows order comparisons using the order preserving symmetric encryption .
lastly additive homomorphic encryption ahe allows additions over encrypted data and multiplicative homomorphic encryption mhe allows us to perform multiplications over encrypted data.
.
execute transformed script.
when all required encrypted data are loaded in the cloud the execution handler issues a request to start executing the target script.
.
udfs.
spr de nes a set of pre de ned udfs that handle cryptographic operations.
such udfs are used to perform operations like additions and multiplications over encrypted data.
the target script calls these udfs using standard pig latin syntax as part of the script execution process.
.
re encryption.
during the target script execution intermediate data may be generated as operations are performed.
the encryption scheme of that data depends on the last operation performed on that data.
for example after an addition operation the resulting sum is already encrypted under ahe.
if that intermediate data is subsequently involved in an operation that requires an encryption scheme other than the one it is encrypted under for example multiplying the sum with another value requires mhe the operation cannot be performed.
the ability to apply additions and multiplications in sequence is viewed as de ning characteristic of fhe.
spr handles this situation by re encrypting script analysis encryption analysis script transformation dag maf met target scriptsource script output encryption input data encryption schema operand encryption generates data artifacts program transformation uses updates figure program transformation components and artifacts the intermediate data.
speci cally the intermediate data is sent to the client where it can be securely decrypted and then encrypted under the required encryption scheme for example the sum is re encrypted under mhe before sent back to the cloud.
once the re encryption is complete the execution of target script can proceed.
.
results.
once the job is complete the encrypted results are sent to the client where they can be decrypted.
.
program analysis and transformation in this section we give a high level overview of how pig latin scripts are analyzed by spr and transformed to enable execution over encrypted input data.
.
running example we use the pig latin script shown in listing as a running example to explain the analysis and subsequent transformation process.
this script is representative of the most commonly used relational operations in pig latin and allows us to explain key features about spr.
the script loads two input les input1 with two elds and input2 with a single eld.
the script then lters out all rows from input1 which are less than or equal to line .
lines and group input1 by the rst eld and nd the sum of the second eld for each group.
line joins the sum per group with the second input le input2 to produce the nal result which is stored into an output le out line .
1a load input1 as a0 a1 2b load input2 as x0 3c filter abya0 4d group cbya1 5e foreach dgenerate group as b0 sum c.a0 asb1 6f join ebyb0 b byx0 7store finto out listing source pig latin script s1 .
definitions in order to describe our program analysis and transformation we rst introduce the following de nitions.
.
.
map of expression trees met all the expressions that are part of the source script are represented as trees and added to the map of expression trees met as values.
the keys of the met are simple literals used to access these expression trees.
figure 3a shows the met for the pig latin script in listing .
keys of the met are shown in square brackets.
note that operands that are not part of any expression are surrounded by a nop no operation vertex and operands that are part of a group by or join operation are surrounded by an assert det assert deterministic vertex.
the assert det vertex can be seen as a simple operator that requires its operand to be present in det encryption.
a0!
!
!
!a1!assert det!
!group!nop!
!a0!sum!
!b0!
!x0!
!assert det!assert det!
a map of expression trees a load c filter a by !d group c by !e foreach d generate !f join e by !
b by !b load!
b data ow graph figure met and dfg for pig latin script in listing .
keys of met are shown in square brackets.
.
.
data flow graph we represent a pig latin script susing a data ow graph dfg .
more precisely s fv eg wherevis a set of vertices and eis a set of ordered pairs of vertices.
each v2vrepresents a relation in s. eache2erepresents a data ow dependency between two vertices in v. note that it follows from the nature of the pig latin language that dfg is acyclic.
for describing our analysis we consider a vertexv2vrepresenting the pig latin relation rto have two parts a the pig latin statement representing rwith all expressions replaced by their keys in the met and b the elds in the schema of r. programmatically a data ow graph gexposes the following interface iterator getiter returns an iterator representing the list of relations in gin topologically sorted order.
.
.
annotated field af we represent each eld in the schema of each relation by an annotated eld abstraction.
in other words there will be one af for every hrelation fieldipair.
the intution behind using afs instead of actual eld names is that in the transformed program one eld in the source script may be loaded as multiple elds each under a di erent encryption scheme.
each af consists of the following three annotations.
parent afs track their lineage using this parent eld.
the parent of an annotated eld could be another annotated eld or an expression tree in the met.
in cases where an af represents a eld which is newly generated as part of a pig latin relational operation group ..by foreach etc.
the parent will be the expression tree used to generate it.
otherwise the parent of an af will be the corresponding af in the vertex in the dag that comes before it.
avail this annotation represents the set of encryption schemes available for the eld being represented.
at the begining of the analysis this property will be empty.
this will be lled in as part of the program analysis.
req this represents the encryption scheme that is required for this eld by our execution engine for encryptionenabled execution.
.
.
map of annotated fields maf we capture the mapping between hrelation fieldipairs and corresponding afs using map of annotated elds maf .
the maf for the pig latin script in listing is shown in table .
note that annotated elds are unique within a pig latin script which allows us to implement the maf as a bidirectional map allowing lookups based on keys as well as values.
table map of annotated elds for pig latin script in listing .ykeys are pairs ofhrelation fieldiandzvalues are corresponding annotated elds.
nerepresents no encryption.
keyyva luezafter analysis a a0 f0 null ne ne f0 null all ope a a1 f1 null ne ne f1 null all ne b x0 f2 null ne ne f2 null all det c a0 f3 f0 ne ne f3 f0 all ne c a1 f4 f1 ne ne f4 a1 all det d gr f5 ne ne f5 det ne d a0 f6 f3 ne ne f6 f3 all ahe d a1 f7 f4 ne ne f7 f4 all ne e b0 f8 ne ne f8 det det e b1 f9 ne ne f9 ahe ne f b0 f10 f4 ne ne f10 f4 all ne f b1 f11 f3 ne ne f11 f3 ahe ne f x0 f12 f3 ne ne f12 f3 all ne .
analysis u sing the programming abstractions de ned above we describe the program analysis and transformation that we perform.
before the analysis as part of initialization all operators and udfs to be used in the script are pre registered with the encryption scheme required for operands and the encryption scheme of output generated.
some pig latin relational operators also require elds to be in speci c encryption schemes.
for example the eld or expression which is the grouped eld of group by require the eld or expression to be available in det encryption.
further the encryption scheme of the new group eld generated by the group by operator will also be available in det encryption.
for example the assert det function that we introduced will be registered with required encryption scheme as det and output encryption scheme also as det.
to keep the description 281algorithm determining available encryption scheme for elds in maf fsfor a dfgg met .
met for dfg g procedure availableenc g fs iter g getiter whiler iter next do anfields getafs fs r for eachaf2anfields do findavail af end for end while end procedure function findavail af .
af is annotated field ifaf parent isnull then af avail encservice met getkey af else ifaf parent2met keys then exprtree met get af parent af avail outenc exprtree root.operator else af avail af parent avail end if end function 1a load enc input1 as a0 ope a0 ah a1 det 2b load enc input2 as x0 det 3c filter abyope greater a0 ope 0xd0004d3d841327f2cce7133abe1efc14 4d group cbya1 det 5e foreach dgenerate group as b0 sum b.a0 ah asb1 6f join ebyb0 b byx0 det 7store finto out listing transformed pig latin script simple we encapsulate this static information as two function calls in our algorithm outenc oper which returns the output encryption scheme of the operator oper andinenc oper which returns the operand encryption scheme required foroper.
these functions are summarized in table .
the available encryption scheme for each af is identi ed by observing the lineage information available through the parent eld of afs and metadata about the encrypted input le.
details of available encryption scheme analysis is presented by algorithm .
the available encryption schemes for afs part of the load operation are set based on metadata about the encrypted le.
for other than load operators the available encryption schemes for afs depend on their lineage.
if the af is derived by an expression available schemes for the af are determined by the deriving expression.
for example the available encryption scheme for the af representing eld b1inb foreach agenerate a0 a1 asb1 is determined by the expression a0 a1 or more precisely by the operator .
if the af is not explicitly derived but is carried forward from a parent relation then the af simply inherits the available encryption schemes of the parent af.
for example the available encryption schemes for the af representing eld a0inb filter abya0 is same as the encryption schemes available for the parent af representing eld a0in relation a.table description of functions used in program analysis and transformation function name description getafs maf r returns list of annotated elds which contain the relation ras a part of their key in maf outenc op returns the encryption scheme in which the result will be after performing the operation op inenc op returns encryption schemes in which the operands of operator opmuch be speci ed addtolist list elem addselem to the list of values represented by list replaceaf af fld replaces all occurrences of the annotated eld afin expressions and relations with the eldfld insertvertex v bv inserts new vertex vbefore vertexbvinmet.
the parent of v is set to be the parent of bvand the parent of bvis set to be v next we describe how the required encryption eld is populated in each af.
as part of our initialization once the maf is generated we replace each eld in the met by the af representing that eld.
once this is done the required encryption for each af can be identi ed by iterating over all leaf vertices of all expression trees in the met.
the required encryption scheme for each leaf vertex is the same asinenc parentoperator whereparentoperator is the operator for which the leaf vertex is the operand.
this procedure is thus straightforward and we do not present this as a separate algorithm.
.
transformation next we describe how a pig latin script represented as s fv eg is transformed into the encryption enabled target script.
we describe the transformation process in multiple sections.
in each section we give an overview of why we perform a speci c transformation and then describe the transformation process itself.
.
.
multiple encryption fields a potential overhead for the client is having to re encrypt data in an encryption scheme appropriate for execution in the cloud.
we use multiple encryptions for the same column whenever possible to minimize such computations on the client side.
for example in listing column a0 is used for comparison line and for addition line .
the na ve approach in this case would be to load column a0 in ope to do the comparison and to re encrypt it to ahe between lines and to enable addition.
but this puts a computational burden on the client cluster and requires sending data back and forth between the client and the cluster thus increasing latency.
such a re encryption can be avoided if we transform the source pig latin script such that both encryption forms for a0 are loaded upfront as two columns.
we 282algorithm program transformation s .
pig latin script s met .
met file .
input le used in load procedure transformmain s .process plain text input les loaded in script s for eachinputfile2sdo transform inputfile end for end procedure function transform file initilize empty list for eachptcolid2filedo af affromcol ptcolid findreq list af ptcolid .list now contains all the encrypted columns .corresponding to pt colid that needs to be loaded encfileloader list le end for end function function findreq list parent af ptcolid for eachaf2mafjaf parent parentafdo ifaf req6 af avail then insertvertex re enc af else encfield encservice pt colid af req addtolist list encfield replaceaf af encfield findreq list af end if end for end function identify opportunities for such optimizations using the maf abstraction.
we describe next how the di erent encryption schemes to be loaded for a eld ai aibeing a eld in the source script and not an af can be identi ed.
once the req eld is populated in maf as part of analysis we query the maf for all afs where aiis part of the key.
these afs represent all usages of eld aiin the pig latin script.
the set of values of reqof these afs and their child afs represents the di erent encryption schemes required for eld ai.
the transformation then generates the list of elds to be loaded from the encrypted version of the input le using metadata returned by the encryption service and modi es the met to use the newly loaded elds.
this is described as function findreq in algorithm .
listing shows the transformed pig latin script for our example.
note that eld a0is loaded twice under two encryption schemes.
.
.
re encryption and constants afs where the reqencryption is not a subset of avail represent cases where a valid encryption scheme is not present to perform an operation.
in such cases we wrap the afs in a speci c reencrypt operation in the target script.
this operation contacts the encryption service on the trusted client to convert the eld to the required encryption scheme.
constants in met are also transformed into encrypted form as required for the operation in which they are used.
note that listing shows the constant 10encrypted using the ope scheme to perform the comparison.
.
implementation in this section we provide details about our prototype implementation of spr.
.
overview spr is implemented as lines of java code with two separate builds a a client component which is deployed in the trusted space and b a cloud component which is deployed in the untrusted cloud see figure .
the client component can generate and read both public and private keys used for encrypting and decrypting data as well as access the plain text input.
the cloud component can only read the public keys used to encrypt data and has no access to private keys or plain text input.
we use mq as the underlying messaging passing subsystem of spr and the gnu multiple precision arithmetic library gmp to perform fast arbitrary precision arithmetic operations.
both these libraries are implemented in native code and we use jni to invoke the corresponding functions from java.
our current version of spr works with apache pig .
.
running on top of hadoop .
.
.
.
program transformation program transformation is the component responsible for generating the transformed pig latin data ow graph that works on encrypted data by performing the transformation process described in section .
.
in particular if a transformed script contains re encryption operations the program transformation component assigns a unique operation id to each such operation.
this operationidis included as part of the re encryption function in the transformed pig latin script.
subsequently the details needed to perform the reencryption for example the encryption schemes involved are assigned to the operationidwhich is then registered with the encryption service.
when the script is executed the encryption service uses the operationidto retrieve the information needed to perform the re encryption.
.
encryption service the encryption service handles the initial data encryption as well as runtime re encryptions.
initial data encryption are performed using pig latin scripts on the client side where data is available as plain text.
encryption is done using udfs that take each data item and convert it into a ciphertext corresponding to the required encryption scheme.
reencryption requests are received by the trusted client over tcp sockets.
these requests are added into blocking queues of worker threads.
worker threads process each request and return a response.
encryption schemes.
we implement randomized encryption ran using blow sh to encrypt integer values taking advantage of its smaller bit block size and use aes which has a bit block size to encrypt everything else.
we use cbc mode in both of these cryptosystems with a random initialization vector.
we construct deterministic encryption det using blow sh and aes pseudo random permutation block ciphers for values of bits and bits respectively and pad smaller values appropriately to match the expected block size.
we perform order preserving encryption ope using the implementation from cryptdb.
we use the paillier cryptosystem to implement additive homomorphic encryption ahe and the elgamal 283cryptosystem to implement multiplicative homomorphic encryption mhe .
.
udfs the cloud component consists of custom udfs and communication channels.
re encryption operations are implemented as aggregation udfs that take an entire relation as argument.
this allows us to pipeline re encryption requests as opposed to re encrypting tuple by tuple.
we also de ned cryptographic equivalents of built in pig latin udfs like sum or .
.
ev aluation this section evaluates our approach in terms of runtime performance and programmer e ort.
we also share lessons learned.
.
synopsis we evaluate four aspects of our approach a practicality to compute over encrypted data in terms of latency b programmer e ort saved by our automatic transformations with respect to manual handling of partially homomorphic encryption c performance compared to existing solutions for querying on encrypted data and d scalability by running queries on large data sets tbs.
.
practicality pigmix we ran the apache pigmix benchmark to evaluate the practicality and performance of spr .
the evaluation was carried out using a cluster of c3 large nodes from amazon ec2 .
the nodes had two bit virtual cpus and .
gb of ram.
input data with rows 5gb was generated by the pigmix data generator script and encrypted at the client side.
we discuss these results here.
figure shows the results of the pigmix benchmark.
on average we observe overhead in terms of latency which is extremely low compared to fhe e.g.
for simple multiplications we measured around slowdown with helib which is currently still at least .
we also observed that this overhead is correlated more towards size of the input data and not the actual computation.
some of the challenges we faced during the tranformation deals with limitations of pig latin in dealing with constants bigger than bit integers or bit longs.
the range of numbers that appear in the cipher text space may exceed what can be represented using integers andlongs and we overcome this by representing numeric constants as strings and converting them to arbitary presicion objects like bigintegers within udfs.
more details are outlined in our workshop report in .
.
programmer effort pigmix we now compare the number of tokens in the pig latin script written to be computed on plain text data with the pig latin script generated by our transformation.
this gauges the additional work that a programmer would have to do in order to explicitly enable her script to compute on encrypted input data.
we use the pigmix pig latin benchmarks for this comparison.
table summarizes this result.
l1 l2 l3 l4 l5 l6 l7 l8 l9 l10 l11 l12 l13 l14 l15 l16 l17 time sec pigmix script pig spr figure latency of pig latin scripts in pigmix script original transformed increase l1 l2 l3 l4 l5 l6 l7 l8 l9 l10 l11 l12 l13 l14 l15 l16 l17 table number of tokens in original and transformed pigmix scripts as can be observed our transformation generates pig latin scripts with more keywords on average.
for the scripts used in the pigmix benchmark our transformation produces scripts which are between and more complex than the original scripts.
as can be understood from our analysis these additions are far from trivial.
.
performance comparison weather data in order to compare our solution against the state of the art cryptdb system we chose an application that analyses historical weather data set.
the data set comprises of station names dates and temperatures recorded on those dates on respective stations.
the application issues common queries against an encrypted version of this data set.
we compare the time taken by our solution to that of cryptdb.
cognizant of the fact that cryptdb was designed to address a di erent set of challenges than spr we perform this comparison to understand how well our system and cryptdb performs when size of data increases.
cryptdb is built on top of mysql a database that stores data in specialized data structures like b trees and optimized for data retrieval using indices and supports transactions including updates.
in contrast spr is built on top of the pig runtime 284which specializes on reading from at les.
furthermore the pig runtime depends on mapreduce which has an intermediate stage which involves writing data to disk before reducers read it remotely over tcp channels.
conversely by using mapreduce pig can leverage the processing power and memory of multiple machines while cryptdb is conned to a single node.
we perform three comparisons how our solution performs on a xed size cluster as size of input data set varies how our solution scales by increasing the number of nodes in the cluster and what is the monetary cost of performing the computation in amazon ec2 .
the results presented here compare the latency of running a query that nds the average temperature for each station using encrypted data.
we compare the time taken by cryptdb single node with the latency of running equivalent pig latin script on a hadoop cluster with nodes worker nodes and control node .
we used m3.medium nodes on amazaon ec2 for all our evaluations.
as can be observed from figure spris able to exploit the parallelism o ered by multiple nodes and scales well.
cryptdb was designed for high throughput in terms of queries per second and not for processing high volumes of data.
this can be observed by the increase in time taken for the queries over cryptdb as the number of records increases.
for million rows spr is .
faster than cryptdb and shows much slower increase in job completion latency as the number of rows increases.
figure shows how time taken by spr changes as the number of nodes in the cluster increases.
we show the time taken by cryptdb running on one node as a base line.
we can observe that running on encrypted data shows trends comparable to regular pig latin jobs and latency reduces as the number of nodes increases.
it may seem surprising that the time taken by the pig latin script running on one node was quite comparable to the time taken by cryptdb.
the reason for this is that for both cryptdb and spr the time taken is bound by the number of cryptographic operations that the cpu has to perform which is the same for both systems.
furthermore in order to nd the average temperature we nd the ahesum andcount of temperature readings for each group.
because both the udfs implements combiner andalgebraic interfaces and the number of distinct groups we have is not high most of the output generated by the map phase is combined before being pulled by the reducer.
also even with a single node we have two mappers and one reducer running simultaneously providing some level of parallelism.
these factors limit the overhead caused by data transfers and the latency is predominantly determined by the time taken by the cpu to perform a xed number of operations.
we also look at the monetary cost of running our computation and compare it with the cost of running cryptdb.
we compute the cost by summing up the total amount charged by amazon for all the nodes used for running our experiments.
we present the calculated cost in figure .
as can be observed we incur a surprisingly similar overall cost despite completing the job up to .
faster.
.
scalability word count in order to test the e cacy of our solution on big data we ran experiments using tb of data obtained from wikipedia dumps .
since the data was in xml format we rst ran a pig latin script to obtain the text section from the le.
next we tokenized and encrypted all the words then exe0 time s nodes spr cryptdb figure comparison of spr and cryptdb runtime latencies with varying nodes nodes nodes time sec plain text input encrypted input figure latency to run word count on encrypted and plain text input time sec records cryptdb spr figure comparison of spr and cryptdb runtime latencies with varying records cuted our word count script on the encrypted words.
lastly we decrypted and stored the top words.
in addition to an encrypted word count we also ran our word count script on the plain text.
the time for spr does not include the time needed to encrypt the data since it is assumed that the user will have the encrypted data on the untrusted cloud.
we measure the latency of running word count on the encrypted input.
we use hadoop clusters of and nodes and summarize the results in figure .
we observe that for both jobs reduction in latency as number of nodes increase follows the same trend.
.
threats to validity in evaluating our system we have considered several threats to validity.
multi tenancy and the virtualized nature of amazon ec2 can lead to variations in latency measurements.
we thus ran all our evaluations several times typically and took the average to counter variations.
the variations in latency we observed in di erent runs of the same pig latin scripts were relatively low .
for cryptdb we observed a variance of up to .
we have tried to use benchmarks that are representative meaningful and used by other publications.
pigmix for example is a standard benchmark used by apache on every new release of the pig runtime system.
while the data sets we used for it are only in the gb order such sizes are not atypical for big data in fact and used also by others .
one factor to con2850 cost records cryptdb spr figure comparison of spr and cryptdb execution cost sider with respect to big data analysis is that when volume of data becomes very high characteristics on input data may heavily determine the time taken for completion.
.
discussion two big drawbacks of computing on encrypted data is bloating of data size and additional cpu cycles required to do more complicated arithmetic.
for example if we are using paillier with bit length keys a bit long may get converted to a cipher text bits in length an increase of times and an addition in plain text is replaced by multiplication of cipher text followed by a modulo operation in paillier.
unfortunately we cannot avoid paying a higher price in terms of storage and computation based on state of art cryptosystems.
another aspect of security that we do not consider is that even though data is encrypted an attacker can still see data access patterns.
this includes frequency of data access groups of data accessed together operations done on data etc.
.
related work approaches to protect con dentiality of data vary based on adversary models.
di erential privacy aims to improve the accuracy of statistical queries without revealing information about individual records.
the server which performs the query in this case is trusted with access to plain text data.
this is in contrast to spr that assumes data and computation reside in an untrusted server.
stile keeps data con dential by distributing computations onto large numbers of nodes in the cloud requiring the adversary to control more than half of the nodes in order to reconstruct input data.
airavat combines mandatory access control and di erential privacy to enable mapreduce computations over sensitive data.
both stile and airavat require the cloud provider and cloud infrastructure to be trustworthy.
excalibur utilizes trusted platform modules to ensure cloud administrators cannot modify or examine data.
compared to spr excalibur adds extra cost because the trusted platform modules need to be installed and maintained.
however spr does not guarentee integrity.
we plan to combine it with our previous work clusterbft to ensure integrity as well as availability.
cryptdb is a seminal system leveraging phe for data management.
as mentioned earlier cryptdb is a database system focusing on sql queries.
cryptdb uses a trustedproxy that analyzes queries and decrypts columns to an encryption level suitable for query execution.
it does not consider the more expressive kind of data ow programs as in pig latin or mapreduce style parallelization of queries.
monomi improves performance of encrypted queries similar to those used in cryptdb and introduces a designer to automatically choose an e cient design suitable for each workload.
mrcrypt consists in a program analysis for mapreduce jobs that tracks operations and their requirements in terms of phe.
when sequences of operations are applied to a same eld the analysis defaults to fhe noting that the system does not currently execute such jobs at all due to lack of available fhe cryptosystems.
thus several pigmix benchmarks are incompletely covered for evaluation.
many static and runtime tools exist to assist programmers in discovering and xing vulnerabilities in their code.
however these systems require the programmer to create queries or assertions to state security properties they want their programs to exhibit.
this is not the case with spr.
a user simply submits her pig latin script and after the transformations it meets our security properties.
work has also been done on secure programming languages for distributed systems.
these include languages like dsl and smcl which include type systems that ensure that programs run securely.
however these languages focus on other issues such as interaction between di erent users and thus do not support automatic parallelization via mapreduce or a similar framework.
.
conclusions and future work in this paper we presented original data ow analysis and program transformation used in spr a system able to perform big data analysis jobs on encrypted data which allows us to leverage untrusted cloud resources while at the same time preserving con dentiality.
our program transformations translate pig latin scripts into semantically equivalent scripts that operate entirely on encrypted data.
by automatically transforming pig latin scripts developer e orts for achieving security are minimized.
we evaluated our approach through standard benchmarks and applications and demonstrated its applicability and performance.
our evaluations show that our approach scales well and can handle big volumes of encrypted data whilst retaining good performance.
furthermore we gauge the savings in terms of developer e ort of our automatic approach by comparing scripts that operate on unencrypted data with the transformed pig latin script which our system generates.
we are currently investigating a number of optimizations and further heuristics for selecting from di erent possible execution paths in the transformed pig latin script.
in particular we are investigating paths which perform re encryption of data at clients in contrast to client side termination or costly re encryption in the cloud as with fhe and minimizing these re encryptions themselves.
to this end we are also employing sampling to determine amounts of data at di erent points in analysis jobs in order to better select between di erent options.
.