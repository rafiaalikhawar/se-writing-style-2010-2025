scalable test data generation from multidimensional models emina torlak electrical engineering and computer sciences university of california at berkeley technical report no.
ucb eecs july 2012copyright by the author s .
all rights reserved.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission.
acknowledgement we thank rastislav bodik and the anonymous reviewers of this paper for their valuable comments and feedback.scalable test data generation from multidimensional models emina torlak u.c.
berkeley emina eecs.berkeley.edu abstract multidimensional data models form the core of modern decision support software.
the need for this kind of software is signi cant and it continues to grow with the size and variety of datasets being collected today.
yet real multidimensional instances are often unavailable for testing and benchmarking and existing data generators can only produce a limited class of such structures.
in this paper we present a new framework for scalable generation of test data from a rich class of multidimensional models.
the framework provides a small expressive language for specifying such models and a novel solver for generating sample data from them.
while the satis ability problem for the language is np hard we identify a polynomially solvable fragment that captures most practical modeling patterns.
given a model and optionally a statistical speci cation of the desired test dataset the solver detects and instantiates a maximal subset of the model within this fragment generating data that exhibits the desired statistical properties.
we use our framework to generate a variety of high quality test datasets from real industrial models which cannot be correctly instantiated by existing data generators or as e ectively solved by general purpose constraint solvers.
categories and subject descriptors d. .
testing and debugging testing tools d. .
software program veri cation formal methods keywords test data generation multidimensional models speci cation constraint solving .
introduction background and motivation.
olap online analytical processing technologies provide interactive decision support in many domains enabling users to obtain abstract yet intuitive views of very large datasets and to pose what if queries about the impact of hypothetical view changes on the this research was conducted at logicblox inc. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
sigsoft fse november cary nc usa.
copyright acm .
.underlying data.
as such they have become indispensable tools for experts in many elds e.g.
business medicine education research and government and the demand for them is growing.
with the rapid increase in the variety and volume of data being collected there is signi cant interest in extending decision support facilities to both non relational datasets such as streams sequences text and networks and extremely large relational datasets that exceed storage capacities of traditional data warehouses.
a key feature and attraction of olap is the use of multidimensional data models which enable both the interactivity and intuitiveness of olap tools.
these models structure data in a multidimensional analysis space called the data cube .
dimensions themselves are graphs and their structure de nes the levels of detail at which data can be summarized for viewing and manipulation.
for example fig.
1c shows a toy person dimension whose structure species that data about individuals such as the tness data in fig.
1a can be summarized by gender and location which is given at the level of cities states or provinces and countries.
queries on a multidimensional dataset compute projections or views of the underlying data cube.
olap style tools exploit this to achieve interactive response times by answering queries about less detailed views of the data e.g.
at the level of countries using more detailed precomputed views e.g.
at the level of cities .
but the application of this core technique is complicated by several hard problems such as ensuring that all dimensional structures are summarizable which is a pre condition for obtaining correct results from precomputed views and selecting an optimal set of views to precompute for a given analysis space.
solving the rst problem for all but simplest structures requires intricate dimension transformations .
the second problem is not only np complete but it is also inapproximable if p6 np .
it is usually addressed with sophisticated view selection heuristics e.g.
.
like most software olap style tools are validated through testing.
however olap systems need large data sets with special characteristics evaluating view selection heuristics at scale for example requires a variety of large data sets with summarizable dimensions that have di erent graph structures and statistical properties.
yet real structurally rich datasets are hard to come by due to privacy and proprietary concerns and existing database synthesizers are of limited help.
they can generate simple tree shaped dimensions but not valid dags such as person since as we will show the data generation problem for these structures is itself np hard.
1approach.
in this paper we propose a new model based approach to synthesizing a rich class of multidimensional structures suitable for systematic testing and evaluation of decision support software.
our approach is implemented in an industrial strength testing framework called testblox which provides a small declarative modeling language and a scalable solver for generating sample data from testblox speci cations.
the language is complete in the sense that it can express every multidimensional structure and it captures common features of existing multidimensional models .
the solver takes as input a multidimensional model and optionally a set of statistical constraints on the shape i.e.
distribution properties and scale i.e.
size of the desired instance of that model.
the model consists of a schema given as a dag over abstract categories e.g.
fig.
1c together with a set of integrity constraints.
its instances if any are dags that conform to the schema and that satisfy the integrity constraints.
while deciding the satis ability of an arbitrary testblox model is np hard most of the models observed in practice fall within a fragment of the modeling language that can be instantiated in polynomial time.
the testblox solver identi es and solves constraints in this fragment generating a sample instance that satis es a maximal polynomially solvable subset of the input model and that exhibits the desired statistical properties when possible .
instance generation is parallelized across all model components including those related by global invariants.
usage scenarios.
scalable model based synthesis of dimensional structures as provided by testblox is helpful in many validation and benchmarking scenarios testing and performance tuning of decision support applications .
in practice domain speci c decision support tools are usually built on top of a generic olap engine.
the facts to be analyzed are stored as tuples in a relational database.
each fact associates a point in a structured analysis space with a set of numerical values or measures .
for example fig.
illustrates a toy tness survey application where the analysis space consists of two dimensional structures person education and the measures specify the height weight and bmi body mass index for a set of people.
the user of the application can view and manipulate the given tness facts with respect to the analysis space by posing queries such as if the average bmi for people of all education levels in each country were to decrease by points what would be the corresponding decrease in the average weight of college educated males?
see fig.
1b for the answer.
as in the case of traditional database applications the olap developer uses integrity constraints on fact and dimension schemas to express preconditions on the inputs accepted by the application.
to test the application however he may need instances that are not only valid but that also have speci c statistical properties for example a small dimensional instance with uniformly distributed edges for functional testing and many large instances with various edge distributions for performance tuning.
testblox can help in either case by quickly synthesizing valid dimensional structures of various shapes and sizes given only the data model and relevant statistical constraints.1in fact even if 1testblox will also generate a valid fact database with desired statistical properties but relational data generation is not unique to our system see e.g.
.the developer had access to a real large end user dataset which is rare in practice he would still need synthetic data to expose corner cases since sampling a single real dataset would produce a biased test suite of similarly shaped inputs.
benchmarking of decision support systems .
many common modeling patterns lead to non summarizable dimensional structures which interfere with basic olap optimizations and have to be specially handled either at the engine or the application level.
the testblox modeling language can express these patterns sec.
and it can be used together with statistical constraints to precisely describe a wide variety of non summarizable dimensional structures of di erent shapes and sizes.
an application developer can use the data generated from such speci cations to benchmark competing olap engines in order to determine which one o ers the best trade o between performance and native support for the kind of structures that arise in the targeted application domain.
validation of new decision support facilities .
in addition to generating non summarizable structures testblox can also synthesize any desired summarizable structure.
we expect that such structures would be particularly useful for validation of new kinds of decision support facilities as they usually assume summarizable analysis spaces.
for example one could apply random testing to validate a new cloud based olap engine against a standard relational one using a dag generator to synthesize random dimensional schemas and testblox to synthesize random summarizable instances of those schemas.
contributions.
to the best of our knowledge testblox is the rst framework to tackle the problem of generating large scale realistic instances of rich multidimensional models.
its key enabling features are also the novel contributions of this paper a small but expressive modeling language with a practical fragment that can be detected and decided in polynomial time relative to model size .
a solver for the polynomial fragment with algorithms for computing data generation parameters that yield instances of desired shape and scale.
a data generation architecture that produces instances in polynomial time relative to instance size and in parallel even in the presence of global constraints.
the modeling language and statistical or soft constraints are introduced in sec.
followed by a discussion of hardness results and the de nition of the polynomially solvable fragment in sec.
.
the solver and the architecture are presented next sec.
as is an evaluation of the framework on three real industrial models sec.
.
a discussion of related work sec.
and future directions sec.
conclude the paper.
.
multidimensional modeling to illustrate the multidimensional analysis problem consider the toy application scenario in fig.
.
the application provides multidimensional analysis for facts collected via an online tness survey.
the goal of the survey is to investigate the relationship between individuals education level and tness as roughly measured by the body mass index.
the end user is a public health policy analyst.
she wants to use 2person education height m weight kg bmi h.s.d.
.
b.sc.
.
m.sc.
.
h.s.d.
.
b.a.
.
m.eng.
.
ph.d. .
a tness factsmale female high school .
.
college .
.
graduate .
.
b a what if view of tness facts decrease in average weight in kg after a point decrease in the average bmi persongender citystate provincecountry male female dcbos lanyc tomtlmd ma ca ny on qcusa canada c person schema left and instance right figure data for a toy tness survey application the application to view for example the e ect of a point decrease in the average bmi of the entire north american population on the average weight of males and females with di erent education levels .
based on these analysis requirements a developer may design the application to provide an analysis space with two dimensions person andeducation .
the education dimension not shown classi es academic degrees into high school college and graduate in the usual way.
the person dimension has a more interesting structure classifying individuals according to gender and location.
in the rest of this section we use the person dimension to introduce thetestblox modeling language as well as the statistical constraints that the developer may provide to our solver in order to obtain test instances of the person model.
.
dimension model like the standard relational data model a dimension model consists of a schema and a set of integrity constraints.
a dimension schema represents the upper bound on the structure of valid dimension instances the person schema for example admits many instances including the one shown in fig.
1c.
some instances that conform to the schema however violate domain speci c requirements such as gender having exactly two members.
dimension integrity constraints encode these additional requirements restricting the set of instances that are valid under the model as a whole.
testblox schema and constraints form a complete multidimensional modeling language that naturally captures common modeling patterns.
dimension schema and instances.
a dimension schema is traditionally represented as a dag directed acyclic graph over category names or simply categories.
each category represents an abstract analysis concept.
schema edges de ne abstraction relations between categories mapping more detailed categories such as city to less detailed i.e.
more abstract ones such as state .
definition dimension schema .
adimension schema is a directed acyclic graph d c .
schema vertices c1 jgenderj cardinality c2 jgenderj c3 jcountryj c4 person!gender city rollup c5 city!state province country exclusive rollup c6 person gender drilldown c7 state province country exclusive drilldown figure dimension constraints for the person schema called categories are drawn from an in nite set of category names c. the expression ci cjdenotes the edge hci cji in the schema graph.
we say that ciis aparent ofcj and thatcjis achild ofci.
the symbol stands for the re exive transitive closure of .
a dimension schema describes a family of dimension instances which are themselves dags over member names or members associated with each category.
each schema edge ci cjrepresents a set of instance edges between the members of categories ciandcj.
the re exive transitive closure of an instance graph called the rollup relation is constrained to be functional or strict between every pair of categories.
in other words a member of any category such as person may be abstracted by or equivalently may rollup to at most one member of another category such as country .
definition dimension instance .
adimension instance is a tripled d m whered c is a dimension schema mis a function from each category c2c to a non empty nite set of members and relates membersxiandxjonly ifxi2m ci xj2m cj andci cj.
the member set of each category is drawn from an in nite set of member names m and member sets of di erent categories are disjoint.
the rollup relation de ned as the re exive transitive closure of is functional between every pair of categories.
that is for all categories ci cj and for every memberxi2m ci there is at most one member xj2m cj such thatxi xj.
dimension constraints.
in addition to a schema a dimension model may also specify a set of integrity constraints that instances have to satisfy.
these include cardinality exclusive rollup and exclusive drilldown constraints.
cardinality constraints bound the size of a category s member set in every instance while exclusive rollup and drilldown constraints specify which edges must exist between adjacent categories and how they are structured.
figure shows examples of each kind of constraint applied to the person schema.
the cardinality constraints c1 c3 require each instance to have exactly two genders and no more than the current total number of countries in the world.
the rollup constraint c4 forces every person to rollup to a member of some speci ed child category expressing the expectation that the toy survey results may be incomplete but will always include some gender or location information about every person.
the exclusive rollup constraint c5 on the other hand ensures that each city rolls up via to a member of exactly one speci ed child category expressing the domain knowledge that north american cities belong to either a state or a province or a country.
these two rollup patterns specify non overlapping specialization .
3drilldown constraints such as c6 and c7 provide complementary functionality to rollup constraints and help express non overlapping generalization .
according to c6 and c7 each gender must have members of some speci ed parent category in this case only person rolling up to it and each country must have members of exactly one speci ed parent category rolling up to it via .
every person instance therefore contains at least one person of each gender and all countries contain either states or provinces but not both.
note that c7 does not prevent countries from having cities as children along with either states or provinces.
all sample constraints are satis ed by the instance in fig.
1c.
definition dimension constraint .
a constraint is adimension constraint on a schema d c i it takes one of the following forms jcj korjcj k wherec2candk c!w i 1ciorc!j i 1ci wherec cifor alli w i 1ci corj i 1ci c whereci cfor alli.
a dimension instance d d m satis es written dj i it ful lls the conditions given below or their natural extension to omitted constraints dj jcj ki jm c j k dj c!wc0i 8x2m c 9ci2c0 9y2m ci x y dj c!jc0i 8x2m c !ci2c0 9y2m ci x y .
dimension model.
the person schema and our sample dimension constraints collectively form a dimension model.
to simplify reasoning about dimension models we require that any included cardinality constraints be consistent and minimal.
in particular we disallow models with constraints that con ict with one another such as jcj kandjcj k or that are mutually redundant such as jcj kandjcj k .
we also require that no two constraints involve the same schema edge unless one is exclusive rollup and the other exclusive drilldown.
the person model for example may not include another rollup constraint on the city state edge but it could include a drilldown constraint on the same edge.
the testblox solver automatically rejects models with constraints that violate these requirements.
definition dimension model .
adimension modelis a tupled d whered c is a schema and is a set of dimension constraints on d. each schema edge ci cjparticipates in at most two constraints in one exclusive rollup and one exclusive drilldown and each categoryc2cis constrained by at most two non con icting cardinality constraints in one lower bound and one upper bound .
a dimension instance d d m satis es the modeld writtendj d i it satis es every constraint in .
that is dj for all .
expressiveness.
despite its compactness the testblox modeling language is complete in the sense that it can be used to fully describe any dimension instance by e.g.
introducing a category for each member constraining that category s cardinality to be exactly and introducing a rollup constraint on each edge between singleton categories with adjacent members .
our language is also practical in 2recall that !
stands for there exists exactly one.
s1 personfsize 1000g scale s2 person!cityf shape source .
target .
distribution normal g figure sample soft constraints for the toy retail application that it naturally captures important families of instances for example all summarizable instances of a given schema.
an instance of a schema is summarizable and thus directly amenable to e cient multidimensional analysis if its rollup relation is strict total and surjective between every pair of categories related by the schema.
we can describe all such instances of a schema using a model that constrains every schema edge ci cjwithci!cjandci cj.
.
soft constraints while an application or a technique needs to work on all valid instances of a multidimensional model this set is in most cases too large to be tested exhaustively.
as a result testing e orts generally focus on a subset of instances that are representative or interesting in some way.
to guide testblox toward generation of such instances we provide a set of soft constraints which describe the desired scale and distribution characteristics of generated data.
we also de ne two simple metrics to quantify the degree to which an instance satis es a given set of soft constraints.
the testblox solver optimizes the value of these metrics when computing data generation parameters.
figure shows a sample of supported soft constraints applied to the person schema.
a scale constraint speci es the preferred size of a category according to s1 for example person should contain roughly members.
a shape constraint characterizes the rollup relation between adjacent categories using three parameters the percentage of members in each category that participate in a rollup edge and the distribution of rollup edges between participating members.
for example s2 describes a person tocityrelation that maps of people to of the cities according to a normal distribution where each participating city has on average 30participating people rolling up to it with a standard deviation of .
for ease of exposition we de ne soft constraints on a dimension schema using functions over categories and schema edges as follows.
definition soft constraints .
soft constraints fon a dimension schema d c are expressed with four functions.
these are n c!n s t c c!
and p c c!p where pis the set of all probability distributions.
the meaning of f n s t p is de ned in terms of a desired instanced d m as follows.
for all categories c2c n c jm c j. for all schema edges e ci cj s e jm ci j j dom e jandt e jm cj j j ran e j where eis the rollup mapping from m ci tom cj and dom e andran e are its domain and range.
the number of members of cithat rollup to a member of cjvaries according to p. because soft constraints may not be precisely satis able especially in the presence of dimension constraints we de ne two metrics to quantify the conformance of an instance to the scaling and participation requirements encoded by n s and t.scale delity measures the squared distance between the 4desired and actual sizes of category member sets and shape delity measures the squared distance between the desired and actual participations of category members in the rollup relation.
measuring and optimizing the conformance of instances to desired distributions is an area for future work.
definition fidelity .
given a dimension instance d d m and soft constraints f n s t p ond we de ne the scale andshape delity ofdwith respect to fas f d p c2c n c jm c j 2and f d p e ci cj s e jm ci j jdom e j t e jm cj j jran e j .
we say thatdhashigher scale delity thand0with respect to a modeld d and soft constraints fi dj d d0j d and f d f d0 .
similarly dj dis said to have higher shape delity thand0j di f d f d0 .
.
hardness of data generation to instantiate a multidimensional model we rst need to decide whether its constituent dimension models are satis able i.e.
whether or not they each have an instance.
this decision problem turns out to be np hard and as a result we cannot e ciently generate arbitrary dimension instances.
most practical models however tend to exhibit regularity which testblox exploits.
such models characterized by tieredness which restricts the use of cardinality constraints and weak exclusivity which restricts the use of exclusive rollup and drilldown can be satis ed in polynomial time and space.
the hardness of the dimension satis ability problem stated below can be proved by reduction from 3sat.3we do note that perhaps surprisingly the proof does not use exclusivity constraints even a simpler version of the decision problem with no exclusive rollup or drilldown constraints is hard.
theorem .deciding the satis ability of dimension modelsd d is np hard even when is limited to containing only cardinality rollup and drilldown constraints.
the key to ensuring tractability of exclusivity free models is a simple and natural restriction on the use of cardinality constraints.
the restriction called tieredness requires that the upper bound on the cardinality of every category be no smaller than the lower bound on the cardinality of each of its descendants.
in other words the model must admit a hierarchical instance in which every category has at least as many members as its more abstract descendants in the dimension schema.
this is in fact an implicit assumption in nearly all dimensions observed in practice more detailed categories rollup to less detailed ones with fewer members.
definition tieredness .
a dimension model d d over a schema d c istiered i h ci l cj for allci cj whereh andl are de ned as follows h c k9k jcj k2 otherwise and l c k9k jcj k2 1otherwise definition hierarchy .
a dimension instance d d m ishierarchical i jm ci j jm cj jfor allci cj.
theorem .a tiered dimension model dwith no exclusive rollup or drilldown constraints is always satis able and it has hierarchical instances that can be encoded via closures in polynomial time and space with respect to the size of d. 3all proofs can be found in the appendix.while tiered exclusivity free models are easily satis ed the problem becomes intractable if we allow unrestricted use of either exclusive rollups or exclusive drilldowns.
to maintain tractability as well as practical expressiveness we limit the use of these constraints so that a schema edge involved in an exclusive constraint is otherwise unconstrained a condition called weak exclusivity.
weakly exclusive models can capture non overlapping generalization and specialization which is the main use case for exclusivity constraints but the generalized or specialized rollup relation cannot also be explicitly constrained to relate all members of involved categories.
for the purposes of data generation the latter can be ameliorated by the use of soft constraints.
theorem .deciding the satis ability of tiered dimension modelsd with either exclusive rollups or exclusive drilldowns is np hard.
definition weak exclusivity .
a dimension modeld d isweakly exclusive i for each exclusive rollup and drilldown constraint there is no other that involves a schema edge constrained by .
tiered weakly exclusive models are said to be regular.
many models exhibit regularity for example a model that captures all summarizable instances of a schema is regular.
another example of a regular model is the person model from our toy survey application which is not summarizable.
regular models always admit at least one polynomially encodable hierarchical instance.
as we will see in the next section these instance encodings can be executed in parallel to e ciently generate test data of desired scale and shape.
definition regularity .
a dimension model d d is regular i it is tiered and weakly exclusive.
theorem .a regular dimension model dis always satis able and it has hierarchical instances that can be encoded via closures in polynomial time and space with respect to the size ofd.
.
generating data for maximal regular fragments of models given a dimension model with soft constraints testblox rst solves the model with respect to the soft constraints producing a set of closures that encode the desired instance one closure for each edge in the model s schema.
this solution is then instantiated by invoking the closures to generate an explicit representation of the instance as a set of text les one for each schema edge .
despite having to collectively satisfy the strictness requirement on dimension instances def.
the closures are constructed in such a way that they can be called in any order.
as a result instance generation can be fully parallelized with each schema edge instantiated by a separate process independently of all other edges.
once a model is solved the remainder of the data generation process is straightforward.
this section therefore focuses on the four steps taken to solve a dimension model d d with respect to soft constraints f nd d t w a maximal regular fragment of d since we know that such a fragment is e ciently solvable solve the cardinality constraints t to obtain a cardinality card c for each category such that the solution is hierarchical with maximal scale delity f 5max regular fragment d 1t w fg 2for c2 such that iscardinality c do 3ifistiered d t f cg then 4t t f cg 5for e2 such that iscardinality e do 6ifisweaklyexclusive d w f eg then 7w w f eg 8return d t w figure algorithm for computing a maximal regular fragment of a dimension model d d solve the constraints w on schema edges using shape delity fas a guide to obtain rollup parameters dom e and ran e for each edge and use the rollup parameters to construct individual rollup closures for all schema edges.
.
finding a maximal regular fragment the rst step to e cient generation of dimension instances is to check whether the input model d d is regular and therefore solvable in polynomial time.
if so we proceed to the next step using d. otherwise we proceed using a locally maximal fragment of dthat is regular discarding the constraints outside of the fragment with a warning.
figure shows a greedy algorithm for detecting and enforcing regularity.
the algorithm simply builds a regular model d t w out of the schema d a maximal subset t of the cardinality constraints and a maximal subset w of the constraints on schema edges.
the set tis maximal lines in that it cannot be augmented with any omitted cardinality constraints without violating the tieredness condition i.e.
d t f cg is not tiered for any cardinality constraint c2 nt.
similarly wis maximal lines in that d w f eg violates weak exclusivity for any omitted edge constraint e2 nw.
as a result d t w is a maximal regular fragment of d which may beditself e.g.
the person model figs.
1c and .
the auxiliary function iscardinality tests whether its argument is a cardinality constraint.
the functions istiered and isweaklyexclusive return true only when applied to a tiered or a weakly exclusive model respectively.
both can be implemented directly from defs.
and with worst case running time of o jdj wherejdjis the number of nodes and edges in the schema graph.4the fragment computation algorithm as a whole is therefore quadratic in jdj.
.
solving cardinality constraints once we have identi ed a regular fragment dr d t w of our input model the next step is to compute the number of members that each category c2cwill have in the generated instance.
these values denoted by card c!n need to satisfy two sets of constraints the cardinality constraints t and the hierarchy constraints card ci card cj for each ci cjind def.
.
semantically card is required only to satisfyt.
we impose the hierarchy constraints for e ciency by thm.
the regularity of drensures the satis ability of both sets of constraints while also enabling e cient encoding of the rollup relation.
4recall from def.
that contains at most jdjconstraints.the cardinality and hierarchy constraints collectively form a system of linear inequalities over integer variables as shown in fig.
.
the variables represent category sizes each vi encodes the size of a category ci2cin the generated instance.
inequalities and bound the sizes of categories according to the cardinality constraints in t. in particular the functions ltandht def.
map each cito its lower and upper bound speci ed by t if any or to the implicit lower bound of def.
and upper bound of int1 e.g.
otherwise.
the remaining inequalities are induced by the hierarchy constraints.
figure shows the corresponding inequalities for the person model.
minimize x ci2c n ci vi subject to vi lt ci for allci2c vi ht ci for allci2c vi vj for allci cj2d figure quadratic program for a model d t w with d c and soft constraints f n s t p overd vperson 1vperson int1vperson vgender vgender 2vgender 2vperson vcity vcity 1vcity int1vcity vstate vstate 1vstate int1vcity vprovince vprovince 1vprovince int1vcity vcountry vcountry 1vcountry vstate vcountry vprovince vcountry figure system of inequalities for the customer model it is easy to see that the linear system fig.
is satis able in polynomial time.
for example we could assign vperson and vgender to and all other variables to .
more generally we can set each vitomaxci cjlt cj which is the maximum lower bound over all categories reachable from ci including ciitself in the schema graph.
due to the tieredness of t def.
a solution of this form would be valid for every system derived from a regular model dr. it is unlikely however that such a solution would have high scale delity def.
with respect to the provided soft constraints f. we therefore solve the system by forming an integer quadratic program iqp that minimizes the solution with respect to the scale metric f fig.
line .
in general solving iqps is intractable .
but our problem has a special structure we can show that the matrix corresponding to the system is totally unimodular and that the matrix corresponding to the quadratic term of the optimization function is principally unimodular .
this allows us to relax the iqp to a quadratic program qp over real variables and still obtain an integral solution .
moreover since the optimization matrix is also positive semide nite an optimal solution to the relaxed problem can be obtained in polynomial time .
we use cplex to solve the relaxed problem and populate card.
.
solving edge constraints given card we next compute the domain and range sizes of the rollup functions ijthat will be generated for the schema edges ci cj.
the computed sizes are stored in 6strengthen d w 1ds ws d fg 2for s2select w do 3ds s restrict ds s 4ws ws f sg 5return ds ws restrict ds cs s s 1es constrainededges s 2hci cji chooseany es 3ifisexclusive s 4theneomit esnfhci cjig 5elseeomit fg s constrain ci op s cj 7d0 s cs sneomit 8return d0 s s figure strengthening algorithm for d w c card c person gender city state province country 4e dom e ran e e0 person gender e1 person city e2 city state e3 city province e4 city country e5 state country e6 province country figure sample solutions for the person model dom c c!nand ran c c!n respectively and they satisfy a system of integer in equalities arising from two sources the exclusive rollup and drilldown constraints inw and the strictness constraints on the rollup relation as a whole def.
.
as before the in equalities solved in this step are stronger than those implied by wand strictness alone in order to enable e cient encoding of rollup closures in the nal step.
to simplify the presentation we show only how to strengthen a constraint in wwith respect to the schemad and how the strengthened problem translates into integer inequalities.
the algorithms for selecting which constraints to strengthen and for solving the inequalities are described only brie y in terms of their inputs and outputs.
to determine dom andran we begin by transforming the schema graph dand a subset of the constraints w with the help of the procedure strengthen .
its key step is the application of the function restrict which takes two inputs a schema ds and a constraint sover two or more edges inds.
given these inputs restrict outputs a schema d0 s dsand a constraint s s which admit fewer instances than ds s .
if sis a non exclusive constraint lines such as ci!cj ck the output is ds itself together with a single edge constraint s s such asci!cj.
for exclusive constraints line the output is sand a schema d0 s ds which omits the same edges fromdsas sdoes from s. the auxiliary functions are self explanatory and all run in constant time.
it follows easily from defs.
that the restrict transformation simply reduces the set of satisfying instances.
we can also prove from def.
and thm.
that applying strengthen todand all ofwwill not eliminate all instances ofdr.
in most cases however testblox selects only a small subset of wto strengthen.
it would not for example strengthen any constraints in the person model.
in general we strengthen all multi edge constraints except the following an exclusive rollup constraint over edges that form a cut set of d such as c5 and any constraint over cut edges found in dsafter the exclusively constrained cut sets are removed e.g.
c4 and c7 .
using a strengthened schema and constraints ds ws we obtain the system of in equalities in fig.
.
the integerdij 0rij for allci cj2ds dij card ci rij card cj for allci cj2ds dij rij sgn dij sgn rij for allci cj2ds x dij card ci for allci!
cj2ws x dij card ci for allci!k cj2ws x rij card cj for all ci cj2ws x rij card cj for allk ci cj2ws figure domain and range inequalities for ds ws variablesdijandrijrepresent the size of the domain and range of ij respectively.
these variables are non negative and bounded above by the corresponding category sizes .
due to the strictness requirement each ijmust be a function and consequently its domain must be at least as large as its range .
moreover the range must contain some element whenever the domain does i.e.
dijandrijmust both be or both be positive since we cannot generate a function from a non empty domain to an empty range.
the rest of the system re ects the meaning of exclusive rollup and drilldown constraints.
for example an exclusive rollup constraint ci!cj ckconstrains the domains of the functions ijand ikto partition the members of ci.
as a result the sizes of the domains are related by the equality dij dik card ci .
the system in fig.
is always satis able in polynomial time due to the regularity of ds t ws .testblox solves it with a simple greedy algorithm using the shape delity metric wsto guide the assignment of values to variables.
we are unaware of results that would suggest the structure of this problem admits an optimal solution in polynomial time.
after computing a greedy solution to the problem testblox invokes cplex to search for a better one within a user speci ed time bound.
the functions dom andranare populated using the best available solution.
.
constructing instance closures given the solutions from the previous steps as well as the strengthened model ds t ws we use the procedure generate shown in fig.
to construct a rollup closure for each schema edge e2ds.
for simplicity the version of generate presented in this section omits the handling of soft distribution constraints speci ed by p def.
.
testblox uses a more sophisticated version that takes pinto account guaranteeing conformance to distributions provided for edges that are exclusively constrained by ws and for all cut edges inds.
the implemented algorithm would faithfully handle distributions on all edges of the person model for example.
details aside both the shown and implemented algorithms follow the same basic approach.
they generate rollup mappings from citocjwith the aid of three lazy streams x which produces a permutation of the domain set line y which produces a permutation of the range set line andz which produces the number of domain members that map to each range member line .
the main loop lines prints zvalues drawn from xfor each value drawn fromy wherezitself is obtained from the stream z. the function permutation stream implements gray s permutation generator which deterministically maps a pos7generate e ci cj ds ws card dom ran 1x permutation stream card ci 2y permutation stream card cj 3z partition stream e ds card dom ran 4xo yo offsets e ws card dom ran 5advance streams x y z x o yo dom e ran e 6for r ran e r r r do 7y next y 8for z next z z z z do print next x y partition stream e ci cj ds card dom ran cbcc ebcc biconnected component ds e 2ifjebccj 1then 3return sum stream ran e dom e 4a sort 5m binary search a card cj 6n binary search a card ci 7z sum stream a a 8for k n k m k k do 9z compose sum stream a a z 10return z figure generating roll ups for ci cjin ds t ws itive number nto a permutation of the set f0 n 1g.
note that category members are referred to by their indices card c .
we use the offsets function line to indirectly coordinate instantiation of edges that are constrained by ws.
to illustrate consider the sample solution for the person model in fig.
.
according to c4 the functions e0and e1must collectively map all members of person .
line yields the same permutation xof the person set for both e0and e1.
theoffsets function returns xo 0as the domain o set of e0 andxo card person dom e1 as the domain o set of e1.
using these o sets line e0maps the rst dom e0 members of x which we denote by x while e1skips the rst members mapping x instead.
consequently all members of person x are mapped by at least one of the functions.
offsets can be straightforwardly implemented with o jdsj worst case time.
thepartition stream procedure ful lls the global strictness requirement on the rollup relation def.
.
it works by maintaining strictness within each biconnected component of the schema graph obtained by viewing the schema as undirected.
recall that a biconnected component of an undirected graph is a maximal subgraph with no articulation vertices.
an articulation vertex is a node that cannot be removed without disconnecting the rest of the graph.
for example the undirected version of the person schema has two articulation vertices person andcity and three biconnected components fe0g fe1g andfe2 e3 e4 e5 e6g.
it is not hard to see that any rollup relation for the person graph is strict if and only if it is strict within each component.
partition stream computeszstreams for all edges within a biconnected component line via a deterministic sequence lines of stream compositions lines based on the cardinalities of the categories within the component.
the function sum stream k n returns a stream of kpositive values that add up to n denoted by zk n.compose takestwo streams zk nandzn s and composes them via addition to produce a stream zk swithkvalues that sum up to s. to see how this construction process enforces strictness consider the component fe2 e3 e4 e5 e6gin fig.
1c.
line constructs the array a for all ve edges.
the arrayagoverns the construction of the resulting zstreams so that for example partition stream e2 z8 partition stream e5 z4 and partition stream e4 compose z4 z8 the above means that both the rollup function e4 and the composition of e2and e5 are contained in the same total surjective function from citytocountry .
consequently every city must map to the same country via all paths in a customer instance that go directly through e4 or indirectly through e2and e5.
a similar intuitive argument can be made for all pairs of paths in a biconnected graph.
.
evaluation to evaluate testblox we conducted a set of experiments on three multidimensional data models developed at a retail consulting company.
all three models are used in real decision support applications.
for con dentiality reasons we refer to the applications as a1 a2 and a3.
two of these a1 and a3 are deployed at major retail chains a2 is a demo application.
figure shows the anonymized dimension schemas for each application s analysis space.
every graph corresponds to one dimension model for a total of models.
the experiments were designed to evaluate the scalability oftestblox and the quality of the generated data.
the experimental setup is described in sec.
.
and the results in sec.
.
.
all experiments were conducted on an ordinary laptop with a two core .
ghz processor and gb of ram.
testblox is implemented in java using cplex for optimization and the apache commons mathematics library for distribution sampling.
.
experimental setup our applications data models were developed using a graphical modeling tool with support for specifying schema graphs but no notion of dimension constraints.
as a result any assumptions about the expected shape of the dimension data are hard coded in the application logic and di cult to recover.
we therefore evaluate testblox against each application s schema using a randomly generated dimension model and four sets of randomly generated soft constraints.
the soft constraint sets describe instances of varying sizes ranging from .
to gb of data .
the generated dimension models are intended to simulate a worst case usage scenario in which every element of its schema graph participates in as many constraints as allowed by def.
.
we place random lower and upper bounds on the cardinality of each category and ensure that each edge is involved in at least one exclusive rollup or drilldown constraint.
the cardinality constraints are tiered and the exclusivity constraints are placed only on edges leaving the sources or entering the sinks of biconnected components.
the placement of exclusivity constraints approximates the use of generalization and specialization patterns .
the generated soft constraints are similarly intended to simulate a realistic but challenging usage scenario.
we place a1 a2 a3 figure dimension schemas for three retail applications a1 a2 a3 categories edges cardinalities rollups exc.
rollups drilldowns exc.
drilldowns a number of categories edges and dimension constraintsa1 a2 a3 soft .
.
.
soft .
.
.
soft .
.
.
soft .
.
.
b instance size in gb for each soft constraint set table summary of experimental parameters a random scale constraint on each category ensuring only that the requested size is within the generated cardinality bounds.
every edge is also constrained using a shape constraint with random source and target participations as well as a random distribution function.
the participations are chosen from between .
and while the distribution is either uniform normal exponential or zip an.
we attempt to choose the de ning parameters of a distribution so that they are compatible with other constraints since testblox ignores distributions that are unlikely to produce values within solution parameters.
for example it would not try to create rollup mappings for members of a category cusing a uniform distribution over the interval card c .
.
results and discussion e ciency.
to evaluate the scalability and parallelism of data generation we executed testblox four times on the models and soft constraint sets for each application increasing the number of instance generation threads from to .
no special load balancing strategy was used the generator relies on the balancing provided by the standard java concurrency library.
the timing results are grouped by application and shown in fig.
.
as evident from the graphs the time taken to generate data increases roughly linearly with instance size and decreases linearly with the number of threads.
solving time is negligible in all cases with the entire process dominated by instance generation.
generating the largest instances with gb of data each takes seconds or less for all benchmarks.
data quality.
in addition to execution time we also tracked two sets of indicators of data quality for each generated instance i1 the ratio between the desired and actual size of each category and i2 the ratios between the desired and actual source participation target participation and distribution mean for each rollup function.the results are summarized in figure for the largest instance of every benchmark.
smaller instances exhibit similar patterns.
since the dimension solving algorithm nds an optimal solution to cardinality constraints the i1 values for all three applications remain very close to .
only two categories overall had ratios that were within percent of rather than exactly.
the i2 values are less consistent for two reasons.
first the solution to edge constraints is not guaranteed to be optimal and second we can only guarantee conformance to speci ed distributions for certain kinds of schema edges as described in sec.
.
.
for the remaining edges distribution constraints are sacri ced to enforce strictness.
consequently a3 which has the simplest structure also has indicators for of its edges within percent of .
but even the indicators for the more complex structures remain largely within percent of the ideal of all a1 edges have ratios between .
and .
while the same is true for percent of a2 edges.
state of the art.
these results are a signi cant improvement on the general constraint solving approach which is currently the only way to generate complex multidimensional structures.
as discussed in sec.
specialized database generators can be used to instantiate tree shaped schemas but not general schema dags such as those in fig.
.
we could not nd publicly available prototypes of these tools for comparison on tree shaped schemas.
for a rough comparison to general constraint solving we manually translated the person model to alloy and instantiated it using the alloy4 solver which is based on sat.
the largest instance we could obtain in seconds consists of .
gb of data corresponding to a dimension graph with about edges.
this instance satis es the person model but its shape is otherwise arbitrary.
like most constraint solvers alloy provides no support for statistical constraints.
testblox in contrast took seconds using threads to produce gb of data for a1 corresponding to three dimension graphs with a total of edges.
the generated dimensions additionally exhibit desired statistical properties.
.
related work database generation.
a variety of techniques have been developed for generating synthetic databases with speci c characteristics.
many of them focus on scalable parallel generation of large databases subject to user speci ed column distributions and intra table correlations.
a classic use case for such tools is the generation of realistic data for tpc benchmarks .
other approaches such as are designed to produce smaller volumes of data subject to richer constraints.
most of them address the following problem given a query a schema .
.
i2 indicators for a1number of schema edgesratio rangesourcetargetmean .
.
i2 indicators for a2 ratio range .
.
i2 indicators for a3 ratio range015304560 i1 indicators for a1 a2 and a3number of schema categoriesratio rangea1a2a3 1234time to generate a1 instancesgeneration time sec number of threads0.
gb0.
gb0.
gb1 gb 1234time to generate a2 instances number of threads1234time to generate a3 instances number of threadsfigure time to generate .
to gb of data for each benchmark using to threads .
.
i2 indicators for a1number of schema edgesratio rangesourcetargetmean .
.
i2 indicators for a2 ratio range .
.
i2 indicators for a3 ratio range015304560 i1 indicators for a1 a2 and a3number of schema categoriesratio rangea1a2a3 1234time to generate a1 instancesgeneration time sec number of threads0.
gb0.
gb0.
gb1 gb 1234time to generate a2 instances number of threads1234time to generate a3 instances number of threads figure quality of gb instances given as the number of categories and edges with i1 and i2 values in the speci ed ranges and a speci cation of the desired output produce an input database that conforms to the schema and that will elicit an output with the speci ed characteristics.
the speci cation may take the form of a table a set of relation cardinalities or a set of coverage rules .
while many of the existing tools are quite general with some being based on general purpose constraint solvers e.g.
none are designed to work on multidimensional models with rich constraints.
most could be used to generate instances of chain and tree like dimension schemas but not of general schema dags.
for practical use the latter requires enforcement of global strictness constraints as well as constraints implied by common modeling patterns.
testblox is to our knowledge the rst tool with this capability.
test data generation.
in addition to database generators there are also many tools for producing other kinds of test data from models e.g.
and from programs e.g.
.
of these the orm generation tool by smaragdakis et al.
and mcgill et al.
is most closely related to testblox .
given a speci cation in the object role modeling orm language which is np hard to satisfy the tool identi es a subset of the input that is within a commonly used polynomial fragment of the language.
it then checks whether the identi ed subset of the model is satis able and if it is generates an instance of that subset.
the orm fragment recognized by the tool cannot express global strictness constraints over relations making it unsuitable for specifying and instantiating multidimensional models.
the tool itself di ers from testblox in that it is designed to produce any satisfying instance it does not have a notion of a preferred instance described by soft constraints.
multidimensional modeling.
thetestblox modeling language was inspired by much of the prior work on multidimensional modeling .
schemas and instances as de ned in sec.
capture all dimension shapes expressible in existing multidimensional models with the exception of instances with non strict rollup relations .
we omit this shape since it is not supported in practice .
but it could be easily integrated into our framework by removing the strictness requirement from def.
and treating it as an explicit integrity constraint on a subset of schema edges.
like the model by hurtado et al.
testblox supports the notion of a dimension constraint.
the focus of the two languages is di erent however.
the testblox language is designed for easy encoding of common modeling patterns and for e cient generation of test data.
the language in is designed for reasoning about a generalized notion of summarizability.
it supports a much richer superset of our rollup constraints but it cannot express either cardinality or drilldown constraints.
it would be interesting to extend testblox to handle a larger subset of the rollup constraints proposed in .
.
conclusion testblox is a new framework for modeling and scalable generation of complex multidimensional structures.
the rst of its kind the framework provides a small but expressive modeling language and a solver for generating data from speci ed models.
while the problem of generating data from testblox models is np hard in general we have identi ed a practical fragment of the language that can be solved in polynomial time.
our solution approach is modular enabling parallel data generation in the presence of global constraints.
we have also shown how to solve constraints in this fragment so that the resulting instance exhibits preferred scale and shape.
plans for future work include enriching the language extending its polynomial fragment and investigating ways to incorporate reasoning about distributions into the solver.
acknowledgement.
we thank rastislav bodik and the anonymous reviewers of this paper for their valuable comments and feedback.
.