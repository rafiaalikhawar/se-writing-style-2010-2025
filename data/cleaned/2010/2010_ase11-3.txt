towards more accurate retrieval of duplicate bug reports chengnian sun david loy siau cheng khoo jing jiangy school of computing national university of singapore yschool of information systems singapore management university suncn comp.nus.edu.sg davidlo smu.edu.sg khoosc comp.nus.edu.sg jingjiang smu.edu.sg abstract in a bug tracking system different testers or users may submit multiple reports on the same bugs referred to as duplicates which may cost extra maintenance efforts in triaging and fixing bugs.
in order to identify such duplicates accurately in this paper we propose a retrieval function rep to measure the similarity between two bug reports.
it fully utilizes the information available in a bug report including not only the similarity of textual content in summary and description fields but also similarity of non textual fields such as product component version etc.
for more accurate measurement of textual similarity we extend bm25f an effective similarity formula in information retrieval community specially for duplicate report retrieval.
lastly we use a two round stochastic gradient descent to automatically optimize rep for specific bug repositories in a supervised learning manner.
we have validated our technique on three large software bug repositories from mozilla eclipse and openoffice.
the experiments show relative improvement in recall rate k and relative improvement in mean average precision over our previous model.
we also applied our technique to a very large dataset consisting of reports from eclipse resulting in a recall rate k of and mean average precision of .
i. introduction due to complexities of software systems software bugs are prevalent.
to manage and keep track of bugs and their associated fixes bug tracking system like bugzilla1has been proposed and is widely adopted.
with such a system end users and testers could report bugs that they encounter.
developers could triage track and comment on the various bugs that are reported.
bug reporting however is an uncoordinated distributed process.
end users and testers might report the same defects many times in the bug reporting system.
this causes an issue as different developers should not be assigned the same defect.
figuring out which bug reports are duplicate of others is typically done manually by a person called the triager .
the triager would detect if a bug report is a duplicate if it is the triager would mark this report as a duplicate report and the first report as the master report.
this process however is not scalable for systems with large user base as the process could take much time.
consider for example mozilla in it was reported that everyday almost bugs appear that need triaging.
this is far too much for only the mozilla programmers to handle .
address this issue there have been a number of studies that try to partially automate the triaging process.
there are two general approaches one is by filtering duplicate reports preventing them from reaching the triagers the other is by providing a list of top krelated bug reports for each new bug report under investigation .
in this study we focus on the second approach for the following reasons.
the first approach is more difficult and the state of the art approach proposed in is only able to remove of the duplicate reports.
the remaining of the duplicate bug reports would still require manual investigation.
furthermore duplicate bug reports are not necessarily bad.
bettenburg et al.
notice that one bug report might only provide a partial view of the defect while multiple bug reports can complement one another .
thus in this study we focus on providing a technique that could help in linking bug reports that are duplicate of one another.
unfortunately the accuracy of proposed techniques for duplicate bug report detection through ranking is still low.
in this work we show an alternative approach with the goal of improving the accuracy of existing techniques.
with a more accurate technique triagers could be presented with better candidate duplicate bug report lists which would make his her job easier.
our approach is built upon bm25f model which is studied not long ago in the information retrieval community .
bm25f is meant for facilitating the retrieval of documents relevant to a short query.
in duplicate bug report detection given a bug report which is a rather lengthy query we would like to retrieve related bug reports.
we extend bm25f model to better fit duplicate bug report detection problem by extending the original model to better fit longer queries.
studies in process only natural language text of the bug reports to produce a ranking of related bug reports.
in this study we consider not only text but also other features that are available in bugzilla e g the component where the bug resides the version of the product the priority of the report.
wang et al.
also include execution traces as information to predict duplicate bug reports .
we do not use execution traces in this study as they are hard to get and are often unavailable in typical bug reports.
we evaluate our approach on several large bug report datasets from large open source projects including mozilla a software platform hosting several sub projects such as firefoxbrowser and thunderbird email client eclipse a popular open source integrated development environment and openoffice a well known open source rich text editor.
for eclipse we also consider a larger dataset that includes bug reports.
in terms of the types of programs and the number of bug reports considered for evaluation to the best of our knowledge our study is larger than any existing duplicate bug report detection studies in the literature.
we show that our technique could result in relative improvement in recall rate k and in mean average precision over state of the art techniques .
we summarize our contributions as follows we propose a new duplicate bug report retrieval model by extending bm25f .
we engineer an extension of bm25f to handle longer queries.
we make use of more information that is easily available in bug tracking system bugzilla as compared to existing studies that rank bug reports.
we use not only textual content of bug reports but also other categorial information including priority product version etc.
we are the first to analyze the applicability of duplicate bug report detection techniques on a total of more than bug reports across bug repositories of various large open source programs including openoffice mozilla and eclipse.
we improve the accuracy of state of the art automated duplicate bug detection techniques by in recall rate k k and in mean average precision .
the paper is organized as follows.
section ii presents some background information on duplicate bug reports duplicate bug reports retrieval bm25f and the optimization of ranking functions.
section iii presents our approach to retrieve similar bug reports for duplicate bug report detection.
section iv describes our case study on more than bug reports to show the utility of our proposed approach in improving the accuracy of existing approaches.
section v discusses related work and finally section vi concludes and describes some potential future work.
ii.
b ackground this section covers necessary background information.
it first discusses duplicate bug reports then describes the general workflow to retrieve duplicate bug reports.
next it introduces bm25f an effective textual similarity measure in information retrieval area and lastly describes how to tune free parameters in a similarity function in order to achieve better performance.
a. duplicate bug reports a bug report serves multiple functions.
it is used to file defects propose features and record maintenance tasks.
a bug report consists of multiple fields.
the fields in different projects may vary to some extent but in general they are similar.
table i lists the fields of interest to us in openoffice bug reports.
fields summary anddescription are in naturallanguage text and we refer to them as textual features whereas the other fields try to characterize the report from other perspectives and we refer to them as non textual features or categorial features .
table i fields of interest in an openoffice bugreport field description summ summary concise description of the issue desc description detailed outline of the issue such as what is the issue and how it happens prod product which product the issue is about comp component which component the issue is about vers version the version of the product the issue is about prio priority the priority of the report i e p1 p2 p3 type type the type of the report i e defect task feature in a software project its bug tracking system is usually accessible to testers and even to all end users.
once a bug manifests people can submit a bug report depicting the detail of the bug.
however multiple reports from different submitters may correspond to the same bug which causes the problem of duplicate bug reports.
in this situation the triager needs to label the duplicate reports as duplicate and add a link to the first report about the bug.
we refer to the first report as master and the other duplicate ones as duplicate .
table ii shows three pairs of duplicate reports in issue tracker of openoffice with the fields in table i. the description of each report is not shown as it is long.
as we can see the two reports in each pair are similar in not only the summary field but also other fields.
for example the second pair has the same component priority andtype and adjacent version in chronological order.
this observation motivates us to consider these categorial features for duplicate bug report retrieval.
b. workflow for retrieving duplicate bug reports this section briefly describes the workflow to retrieve duplicate bug reports.
more detail has been discussed in our previous paper .
in a duplicate report retrieval system the fields summary and description of both existing and new bug reports are preprocessed by standard information retrieval techniques i e tokenization stemming andstop work removal .
figure depicts the overall flow.
the bug repository is organized as a list of buckets a hashmap like data structure.
the key of each bucket is a master report and its value is a list of duplicate reports on the same bug.
each bucket has a distinct bug as its master report is not contained in other buckets.
when a fresh bug report qis submitted the system computes the similarity between qand each bucket and returns k master reports whose buckets have the top ksimilarities.
the similarity of a report and a bucket is the maximum similarity between the report and each report in the bucket computed by the component similarity measure in figure .
in our previous work we used a support vector machine svm to train a model measuring the similarity between two reports .
the similarity is measured based on onlytable ii examples of duplicate bugreports from openoffice issue tracker pair id summary product component version priority type no scrolling of document content by use of the mouse wheel word code 680m240 p3 feature unable to scroll in a note with the mouse wheel word ui 680m240 p3 defect alt letter does not work in dialogs none ui ooh680m4 p3 defect alt key no longer works as expected framework ui ooh680m5 p3 defect connectivity evoab2 needs to be changed to build against changed api database none ooh680m4 p1 defect connectivity fails to build evoab2 in m4 database none ooh680m4 p2 defect fig.
.
overall workflow for retrieving duplicates textual features summary anddescription .
while this approach has been shown to be effective we believe it can be further improved.
thus in this paper we propose a new and comprehensive similarity function to substitute the component similarity measure .
the other parts of the retrieval process remain unchanged.
c. bm25f bm25f is an effective textual similarity function for structured document retrieval .
a structured document is composed of several fields e g summary anddescription in a bug report with possibly different degrees of importance.
given a document corpus dofndocuments each document dconsists of kfields and the bag of terms in the f th field can be denoted by d where f k. the definition of bm25f is based on two components.
the first is the inverse document frequency idf defined in which is a global term weighting scheme across documents idf t logn nd where ndis the number of documents containing the term t. the other component is the local importance measure tfd of a term tin a document ddefined in which is an aggregation of the local importance of tin each field of d. tfd d t k f 1wf occurrences d t bf bf lengthf average lengthf for each field f wfis its field weight occurrences d t is the number of occurrences of term tin field f length fis the size of the bag d average length fis the average size of the bag d across all documents in d and bfis a parameter bf that determines the scaling by field length bf 1corresponds to full length normalization while bf 0corresponds to term weight not being normalized by the length.
length normalization of term weights is to mitigate the advantage that long documents have in retrieval over short documents.
based on the two components above given a query qwhich can be a bag of words or a document the bm25f score of a document dandqis computed as follows bm25f d q t2d qidf t tfd d t k1 tfd d t in tis the shared term occurring in both dandq and k1 k1 is a tuning parameter to control the effect of tfd d t .
there is a set of free parameters to tune for bm25f to work most effectively for a certain document corpus i e wfandbffor each field f and k1.
with kfields there are k parameters in total.
the following section introduces a technique available in information retrieval area to tune these parameters.
d. optimizing similarity functions by gradient descent this section briefly introduces an optimization technique in based on stochastic gradient descent to tune parameters in a similarity function.
a function sim d q computes a similarity score between a document dand a query q and has a vector of nfree parameters x1 x2 xn .
in order for sim to perform most effectively for a document corpus we need a training set to tune the nparameters in sim .
in the training set each instance is a triple q rel irr where qis a query relis a document relevant to q and irris an irrelevant document.
ideally sim d q should give a higher score to rel q than to irr q namely sim rel q sim irr q .
in taylor et al.
use a simplified version of ranknet cost function rnc presented in to measure the cost of the application of sim d q on a training instance i q rel irr .
rnc i log ey where y sim irr q sim rel q lower rnc value means more accuracy of sim whereas higher value represents less accuracy of sim .
intuitively the rnc value for a training instance is large if the retrievalfunction sim fails to rank the two documents rel irrin the correct order that is sim irr q sim rel q .
the whole optimization of sim is a process of minimizing the cost function rnc for each training instance and the minimization is achieved by iteratively adjusting free parameters insim via stochastic gradient descent.
algorithm simplified parameter tuning algorithm ts a training set n the times to iterate through ts the tuning rate a small number such as .
forn 1tondo for each instance i tsin random order do for each free parameter xinsim do x x rnc x i end for end for end for algorithm displays a simplified version of the tuning algorithm.
generally the algorithm iterates through the training setntimes where nis provided by the user.
at each time for each training instance i the algorithm adjusts each free parameter xtox x rnc x i .
this adjustment is controlled by a small coefficient and rnc x the partial derivative of rnc with respect to the free parameter x. in principle the iterative adjustment of the value of x enables the latter to progress towards the minimum of rnc.
for a textbook treatment on gradient descent please refer to .
iii.
a pproach our approach consists of three aspects.
first we aim to improve the accuracy of textual similarity measures specially for bug reports.
thus we extend bm25f a successful measure in information retrieval area by considering the weight of terms in queries.
second in order to enhance the performance of duplicate bug report retrieval besides textual fields we try to take better advantage of the other kinds of information available in bug reports.
therefore we propose a new retrieval function which is a linear combination of similarities of textual and categorial features.
furthermore to enable the retrieval function to work most effectively on a certain bug repository we optimize it by performing gradient descent on a training set extracted from the bug repository.
the following sections describe these aspects respectively.
a. extending bm25f for structured long queries bm25f is designed for short queries which usually have no duplicate words.
for example the queries in search engines are usually of fewer than ten distinct words.
however in the context of duplicate bug report retrieval each query is a new bug report.
the query is structured as it is with a short summary and a long description and it can sometimes be as long as more than one hundred words.
we believe these two characteristics of bug report queries structural and long can further enhance the retrieval performance of bm25f sowe extend it into the following form by considering the term frequencies in queries.
bm25f ext d q t2d qidf t tfd d t k1 tfd d t wq where wq k3 tfq q t k3 tfq q t tfq q t k f 1wf occurrences q t in for each shared term between a document dand a query q its weight contains two components one is the product of idf value and term weight in dinherited from bm25f and the other is the term weight in query q wq.wqis derived from the query term weighting scheme ofunstructured retrieval function okapi bm25 and it involves weight from the query computed by tfqdefined in .
the free parameter k3 k3 is to control the contribution of the query term weighting for example if k3 then the query term contributes no weight as wq becomes always equal to and bm25f extis reduced to bm25f .wqis monotonically increasing and concave with k3 upper bounded by tfq.
in the weight of a term is the aggregation of the product ofwf the weight of field f and the number of occurrences oftinq .
different from tfddefined in there is no length normalization of query terms as retrieval is being done with respect to a single fixed query.
compared to bm25f bm25f exthas an additional free parameter k3 thus given documents with kfields bm25f ext has k free parameters.
b. retrieval function from the three pairs of duplicate reports shown in table ii we note that duplicate reports are similar not only textually insummary anddescription fields but also in the categorial fields such as product component priority etc to capture this observation given a bug report dand a query bug report q our retrieval function rep d q is a linear combination of seven features in the following form where wiis the weight for the i th feature feature i. rep d q i 1wi featurei each weight represents the degree of importance of its corresponding feature.
if a feature is good at distinguishing similar reports from dissimilar ones its weight should be larger than those features with weaker distinguishing power.
figure shows the definitions of the seven features which can be classified into two types textual features .
the first feature defined in is the textual similarity between two bug reports over the fields summary anddescription computed by bm25f ext.
the second featuredefined in is the same as the first one except that the fields summary anddescription are represented in bigrams.
a bigram consists of two consecutive words.
categorial features .
the rest five features are categorial features the features are based on the equality of the fields product component and type whereas the sixth and seventh features are the reciprocal of the distance between twopriorities orversions respectively.
feature1 d q bm25f ext d q of unigrams feature2 d q bm25f ext d q of bigrams feature3 d q ifd prod q prod otherwise feature4 d q ifd comp q comp otherwise feature5 d q ifd type q type otherwise feature6 d q d prio q prio feature7 d q d vers q vers fig.
.
features in the retrieval function the retrieval function rep in has free parameters in total.
for the first and second features as we compute similarities over twofields summary anddescription namely k 2inbm25f ext each feature has free parameters.
also we have a weight for each of the features in thus overall rep has free parameters.
table iii lists all these parameters.
the next section discusses how we tune these parameters for rep to gain good retrieval performance for a certain bug repository.
c. optimizing rep with gradient descent the parameter tuning for rep is based on gradient descent.
in this work we do not follow our previous approach using linear kernel svm to optimize rep although rep is a linear combination of features.
the reason is that svm is able to infer the feature weights w1 w7but cannot tune the free parameters inside the first and second features which are based on bm25f ext such as k1andk3.
however similar to svm the optimization technique used in this paper is also a discriminative approach as parameters are tuned by contrasting similar pairs of reports against dissimilar ones.
performing gradient descent also needs a training set.
in the following we first discuss how to construct a training set in a suitable format for our algorithm from a set of bug reports and then detail our training algorithm.
creating training set as mentioned in section ii d the training set is a set of training instances of the form q rel irr where qis a query bug report relis a duplicate report of q and irris a report on a different bug.table iii parameters in rep param description init example w1 weight of feature1 unigram .
.
w2 weight of feature2 bigram .
.
w3 weight of feature3 product .
w4 weight of feature4 component .
w5 weight of feature5 report type .
.
w6 weight of feature6 priority .
w7 weight of feature7 version .
wunigram summ weight of summary infeature1 .
wunigram desc weight of description infeature1 .
bunigram summ bofsummary infeature1 .
.
bunigram desc bofdescription infeature1 .
kunigram k1infeature1 .
kunigram k3infeature1 .
wbigram summ weight of summary infeature2 .
wbigram desc weight of description infeature2 .
bbigram summ bofsummary infeature2 .
.
bbigram desc bofdescription infeature2 .
kbigram k1infeature2 .
kbigram k3infeature2 .
algorithm constructing a training set from a repository ts resultant training set n parameter controlling the size of ts for each bucket bin the repository do r b master b duplicates for each report qinrdo for each report relinr q do fori 1tondo randomly choose a report irr s t irr r ts ts q rel irr end for end for end for end for return ts algorithm lists the procedure to construct a training set from a repository.
generally in each bucket of the repository it pairs two reports as a relevant query document pair q rel at line and .
next in line it randomly chooses nreports which are not duplicate of the current bucket as irrelevant documents and lastly pairs the relevant pair against each of thenreports to form ntraining instances.
in our case studies we set nto .
parameter tuning to apply gradient descent for each parameter xinrep we manually derive rnc x the partial derivativeof rnc with respect to x. next we initialize each parameter with a default value.
for parameters in bm25f ext we use recommended default values for parameters of bm25f from information retrieval area.
for example for a lengthy field its bshould be big to perform length normalization whereas for a short filed its bis supposed to be small.
thus we instantiate bsumm andbdesc .
based on our previous experience in the terms in summary are usually three times as important as those in description and hence we initially set wsumm andwdesc .
the column initin table iii shows these initial values used in our case studies.
given a training set we start the tuning by calling the procedure in algorithm .
the column example in table iii displays a set of example values we get after tuning for openoffice.
the tuned value for a parameter may vary with a different initial value but it does not affect the retrieval performance much as the tuning algorithm coordinates and adjusts all the free parameters towards the optimal.
algorithm lists the optimization process in detail.
algorithm tuning parameters in rep initialize free parameters in rep with default values fixk1and set k1 2infeature1andfeature2 fixk3and set k3 0infeature1andfeature2 tune unfixed parameters w1 w7 b wsumm andwdesc call algorithm with n and unfix k3 fixb wsumm andwdescinfeature1andfeature2 tune unfixed parameters w1 w7 k3.
call algorithm with n and 001again following the analysis in tuning the weights of summary and description fields is redundant to tuning k1.
thus we fix k1to an arbitrary value i e and let the tuning algorithm tune the remaining parameters for k1 .
different from bm25f bm25f exthas one more parameter k3that determines the scaling of term weights in queries.
tuning it is also redundant to tuning the weights of summary and description to some extent.
hence instead of tuning all these parameters together we tune the parameters in two rounds in the first round in line we fix k3infeature1and feature2to and tune the other parameters.
in the second round in line we unfix k3 fix all the other parameters in feature1andfeature2to their current values and start tuning the rest of unfixed parameters including w1 w7andk3.
in addition to avoiding redundant tuning we enjoy another benefit from the two round tuning.
as we fix k3to in the first round the bm25f extinfeature1andfeature2is reduced to bm25f .
all its partial derivatives except the one with respect tok3become equal to those of bm25f respectively.
therefore in our implementation we use the ones of bm25f instead ofbm25f extin the first round as they are faster and much simpler.
iv.
c asestudies we have built a prototype2to validate the effectiveness of the extension to bm25f and our new retrieval function 2both implementation and dataset are available online at nus.edu.sg suncn ase11 and have applied it to the bug repositories of three large open source projects openoffice mozilla and eclipse.
the experiments simulate the real world bug triaging process that is for each duplicate report we use the proposed techniques to retrieve a list of top similar master reports from the bug repository.
the evaluation of the retrieval performance is measured by two metrics recall rate k andmean average precision map .
by fixing the size of the top list to k recall rate k defined in measures the fraction of duplicate reports whose masters are successfully detected in the retrieved top kmasters ndetected among all the duplicate reports ntotal used in testing the retrieval process.
recall rate k ndetected ntotal map is a single figure measure of ranked retrieval results independent of the size of the top list.
it is designed for general ranked retrieval problem where a query can have multiple relevant documents.
however duplicate bug report retrieval is special as each query duplicate report has only one relevant document master report thus map in our case studies is reduced to the simplified form in .
for the complete form please refer to .
given a set qof duplicate reports for each duplicate the system continually retrieves masters in descendent order of similarity until the right master is retrieved and records its index in the ranked list.
then map can be computed as follows map q q jqj i index i where index iis the index where the right master is retrieved for the i th query.
higher map means that for each duplicate the retrieval system can return the right master at a higher place in the ranked result list saving triagers time on checking if a new report is a duplicate and on finding its associated master report.
our case studies serve as two purposes the first is to validate the effectiveness of bm25f extover bm25f the second is to compare the retrieval performance of the proposed retrieval function rep our previous retrieval model based on svm and the work by sureka and jalote in .
a. experimental setup we used the bug repositories of three large open source projects openoffice mozilla and eclipse.
openoffice is an open source counterpart of microsoft office.
mozilla is a community hosting multiple open source projects such as the famous web browser firefox email client thunderbird.
eclipse is an extensible multi language software development environment written in java.
these three projects are diverse in terms of purposes users and implementation languages thus help generalizing the conclusions of the experiments on them.
we extracted four report datasets from them by choosing reports submitted within a period of time t. in particular we created two datasets from eclipse one is for reports submitted in whereas the other is for reports submitted from thetable iv details of datasets dataset sizeperiod training reports testing reports from to duplicate all duplicate all openoffice mozilla eclipse large eclipse ..... .
.
.
.
.
.
.
k size of the retrieved top klist.recall rate .. ..bm25f .
..bm25f ext a openoffice..... .
.
.
.
.
.
.
k size of the retrieved top klist.recall rate .. ..bm25f .
..bm25f ext b mozilla ..... .
.
.
.
.
.
.
.
.
k size of the retrieved top klist.recall rate .. ..bm25f .
..bm25f ext c eclipse..... .
.
.
.
.
.
.
.
.
k size of the retrieved top klist.recall rate .. ..bm25f .
.. bm25f ext d large eclipse fig.
.
effectiveness of bm25f extcompared to bm25f inrecall rate beginning of eclipse project to .
the reason to create the latter larger one3is that sureka and jalote used this dataset in their work and we intend to compare with theirs using the same benchmark.
furthermore this dataset spans a long time and is extremely large hence it can further validate our technique across a long period of time.
these reports include all defect reports maintenance tasks and feature requests.
each duplicate report has been labeled as duplicate by triagers and linked to its master .
this information is used to test the retrieval and to measure the performance i.e.
extracting duplicates to construct a training set picking a duplicate to retrieve a top ksimilar masters and determining whether a master is the correct master of a duplicate.
since our retrieval function i e involves the order of versions we manually recovered the chronological order of all the versions for openoffice by searching the release date of each version on the internet.
we did not do this for the other datasets due to difficulty but we believe that it should be easy for project members to get and maintain such version order.
for the other datasets we just assume that all reports have the same version.
3we download this dataset from challenge table iv details the four datasets.
our case studies follow the approach used in past studies on duplicate bug report retrieval .
we selected the first mreports in the repository based on chronological order of which reports are duplicates as training set to tune the parameters in the retrieval function rep .
the column training reports displays the ratio of duplicates and all reports in the training set.
besides serving as a training set in our approach those m reports are also used to simulate the initial bug repository for all experimental runs.
the rest of the duplicates are used for testing the retrieval process shown in column testing reports .
at each experimental run we iterate through the testing reports in chronological order.
once reaching a duplicate report r we apply the corresponding technique to retrieve r s potential master reports until its right master is received.
after each retrieval is done we record the index of r s master in the top list for recall rate k and map calculation and then add rto the repository.
after the last iteration is done the recall rate for different sizes of the top list and map are calculated.
the prototype was implemented in c and all experiments were carried on a linux pc with intel core quad cpu .0ghz and 8gb memory.
since the training set is constructed randomly c f algorithm and the gradient descent alsoinvolves randomness c f algorithm we repeated all experiments five times and take the average values for our analysis and conclusion.
b. effectiveness of bm25f ext in this experiment we aim to validate the effectiveness ofbm25f extover bm25f .
to ensure fairness we only involve textual similarity on summary and description in our experiment and omit the use of any categorial features.
figure shows the curve of the recall rate k of the two similarity measures on the four datasets.
on each dataset for each different size of the top kresult list bm25f ext performs constantly better than bm25f and it gains and relative improvement over bm25f on openoffice mozilla eclipse and large eclipse datasets respectively.
moreover table v shows the mean average precision of the two metrics and the last row is the relative improvement by bm25f extover bm25f which is up to .
.
based on the large set of testing query duplicates in total we conclude that in the context of duplicate bug report retrieval bm25f extis more effective than bm25f .
table v map ofbm25f extandbm25f openoffice mozilla eclipse large eclipse bm25f ext .
.
.
.
bm25f .
.
.
.
relative impro.
.
.
.
.
c. effectiveness of retrieval function we evaluate the performance of our new retrieval function rep against previous techniques including our previous work based on support vector machine and a recent work using character n gram based features by sureka and jalote in .
we apply both our previous technique and rep on the four datasets and compare them directly.
however due to high overhead our previous technique on large eclipse is not able to complete despite running for two days thus there is no result for this experimental run.
for the work by sureka and jalote they do the evaluation on the large eclipse dataset.
we compare our result with the accuracy results reported in their paper.
figure shows the recall rate ofrep and our previous technique on the four datasets.
in the figure svm represents our previous technique rep v stands for the rep retrieval function with version information considered and rep nv is therep without version information considered.
as mentioned before we only recovered the chronological order of versions for openoffice therefore only applied rep v to openoffice.
from figure a we can see that rep v can improve the recall rate over rep nv by .
in figure d only the performance of rep nv is shown as running of svm experiences time out.
overall the new retrieval function outperforms svm in openoffice dataset ..... .
.
.
.
.
.
.
k size of the retrieved top klist.recall rate .. ..svm .
..rep v .
..rep nv a openoffice ..... .
.
.
.
.
.
.
k size of the retrieved top klist.recall rate .. ..svm .
..rep nv b mozilla ..... .
.
.
.
.
.
.
.
k size of the retrieved top klist.recall rate .. ..svm .
..rep nv c eclipse ..... .
.
.
.
.
.
.
.
.time out no curve for svm.. k size of the retrieved top klist.recall rate .
.
..rep nv d large eclipse fig.
.
comparison with our previous approach in mozilla dataset and in eclipse dataset.
table vi displays the map of each experimental run.
the last row is the relative improvement brought by rep over svm.
in sureka and jalote randomly selected duplicate reports to test their approach and reported that recall rate was was and was .
we experimented on the same dataset large eclipse and we tested all the duplicate reports to avoid randomness table vi map ofrep v rep nv and svm openoffice mozilla eclipse large eclipse rep v .
rep nv .
.
.
.
svm .
.
.
relative impro.
.
.
.
which we believe can produce more reliable result.
the recall rate is and is .
we can thus safely conclude that our technique improves upon past techniques in recall rate k aside from the performance improvement rep also significantly reduces the runtime compared to our past approach that uses svm .
table vii displays the time needed to finish an experiment run for each dataset.
table vii overhead of svm and rep in seconds openoffice mozilla eclipse large eclipse rep svm days v. r elated work in this section we describe the existing studies on duplicate bug report detection and other studies that analyze bug reports.
a. duplicate bug report detection there have been a number of past studies that detect duplicate bug reports.
we would present them in chronological order and then elaborate the difference between our work and theirs.
runeson et al.
perform one of the first studies on finding duplicate bug reports .
they take natural language text of bug reports and perform standard tokenization stemming and stop word removal.
each bug report is then characterized as a bag of word tokens and is modeled as a feature vector where each feature corresponds to a word in the bug report.
the feature value in the vector is computed based on the following formula log2 tf word while tf is the term frequency of the word.
each bug report is then compared to other bug reports by computing the similarity of their corresponding feature vectors.
three similarity measures are analyzed cosine dice and jaccard.
bug reports with high similarity scores are likely to be duplicates.
their study on bug reports of sony ericsson mobile communications show that cosine performs best and is able to detect of the duplicate bug reports.
wang et al.
use another feature vector construction approach where each feature is computed by considering both term frequency tf and inverse document frequency idf of the words in the bug reports following the formula tf word idf word .
furthermore they consider an additional source of information to measure the similarity between twobug reports namely program execution traces.
using cosine similarity of the composite feature vectors they detect topk similar reports as candidate duplicate bug reports.
their approach could detect of the duplicate bug reports in their case study on a small subset of mozilla firefox bug reports.
they only consider a relatively small case study as bug reports which contain execution traces are scarce.
jalbert and weimer propose another feature vector construction approach where each feature is computed by the following formula log2 tf word .
cosine similarity is also used to extract top k similar reports as candidate duplicate bug reports.
they also propose a classification technique to detect bug reports that are duplicated.
sun et al.
propose to use a discriminative approach to detect bug reports.
a support vector machine svm is used to train a model that would compute the probability of two reports being duplicate .
this probability is used to detect topkmost similar reports as candidate duplicate bug reports.
their case study on bug reports from openoffice firefox and eclipse show that they could outperform the approaches proposed in when only the natural language text of the bug reports is considered.
more recently sureka and jalote propose an approach that consider not word tokens but n grams as features in a feature vector that characterizes a bug report .
they show that their approach is able to detect duplicate bug reports with reasonable accuracy on a large bug report corpus from eclipse.
no comparative study however is performed to compare their approach with the other existing approaches.
similarities and differences.
we address the same problem of detecting duplicate bug reports or more accurately retrieving top krelated bug reports from a collection of bug reports.
we consider natural language text of the bug reports similar to the work in .
aside from natural language text we also consider other categorial features available in bugzilla.
the study by jalbert and weimer also considers categorial features .
different from the work by wang et al.
we ignore execution traces as these are often not available in bug reports.
indeed out of the many reports that we study only a small proportion of them contain execution trace information.
in this study we have extended one of the latest textual similarity measures in information retrieval for retrieving structured documents namely bm25f .
lastly we show that our proposed retrieval function involving textual and categorial similarities outperforms other state of the art measures proposed in the literature i e .
b. other bug report related studies categorization of bug reports is investigated by anvik et al.
cubranic and murphy pordguski et al and francis et al.
.
in this categorization is used to assign bug reports to the right developers.
a study on predicting the severity of bug reports is performed by menzies and marcus .
bettenburg et al.
extract stack traces codes and patches from textual bug reports .
ko and myersinvestigate the differences between defect reports and feature requests .
anvik et al.
perform an empirical study on the characteristics of bug repositories including the number of reports that a person submitted and the proportion of different resolutions .
sandusky et al.
study the statistics of duplicate bug reports .
hooimeijer and weimer develop a model to predict bug report quality .
bettenburg et al.
survey developers of eclipse mozilla and apache to study what developers care most in a bug report .
bettenburg et al.
later show that duplicate bug reports might be useful .
duplicate bug reports could potentially provide different perspectives to the same defect potentially enabling developers to better fix the defect in a faster amount of time.
still there is a need to detect bug reports that are duplicate of one another.
vi.
c onclusion f uture work in this work we improve the accuracy of duplicate bug retrieval in two ways.
first bm25f is an effective textual similarity measure which is originally designed for short unstructured queries and we extend it to bm25f extspecially for lengthy structured report queries by considering weight of terms in queries.
second we propose a new retrieval function rep fully utilizing not only text but also other information available in reports such as product component priority etc a two round gradient descent contrasting similar pairs of reports against dissimilar ones is adopted to optimize rep based on a training set.
we have investigated the utility of our technique on sizable bug datasets extracted from large open source projects i e openoffice firefox and eclipse and find that both bm25f ext andrep are indeed able to improve the retrieval performance.
particularly the experiments on the four datasets show thatbm25f extimproves recall rate k by and map by over bm25f .
for retrieval performance of rep compared to our previous work based on svm it increases recall rate k by and map by compared to the work by sureka and jalote rep performs with recall rate k of k and map of which are much higher than the results reported in their paper.
in the future we plan to build indexing structure of bug report repository to speed up the retrieval process.
last but not least we plan to integrate our technique into bugzilla tracking system.
acknowledgment we are grateful to the reviewers for their valuable comments.
this work is partially supported by a research grant r .