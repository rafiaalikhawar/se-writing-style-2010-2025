spotting working code examples iman keivanloo department of electrical and computer engineering queen s university kingston ontario canada iman.keivanloo queensu.ca juergen rilling department of computer science and software engineering concordia university montreal quebec canada juergen.rilling concordia.ca ying zou department of electrical and computer engineering queen s university kingston ontario canada ying.zou que ensu.ca abstract working code examples are useful resources for pragmatic reuse in software development .
a working code example provides a solution to a specific programming problem .
earlier studies have show n that existing code search engines are not successful in finding working code examples .
they fail in ranking high quality code examples at the top of the result set.
to address this shortcoming a variety of pattern based solutions are proposed in the literature .
however these solutions cannot be integrated seamlessly in internet scale source code engines due to their high time complexity or query language restrictions .
in this paper we propose a n approach for spotting working code examples which can be adopted b y internet scale source code search engines .
the time complexity of our approach is as low as the complexity of existing code search engines on the internet and considerably lower than the pattern based approaches supporting free form queries.
we study the performance of our approach using a representative corpus of open source java projects.
our findings support the feasibility of our approach for internet scale code search .
we also found that our approach outperforms ohloh code search engine previously known as koders in spotting working code examples .
categories and subject descriptors d. .
coding tools and techniques object oriented programming h. .
information storage and retrieval information search and re trieval search process general terms algorithms performance experimentation keywords source code search clone detection working code example .
introduction a source code example is a code snippet i.e.
a few lines of reusable source code used to illustrate how a programming problem can be solved.
source code examples play a n important role in programming and provide an intrinsic resource for learning and reusing .
code examples can accelerate the development process and increase the product quality .
a working code example is a code snippet that can be considered for both learning and pragmatic reuse.
such working code examples can spawn a wide range of application s varying from api usage e.g.
how to use jfreechart library to save a chart to basic algorithmic problems e.g.
bubble sort .
an ideal working code example should be concise complete selfcontained and easy to understand and reuse.
it is not common in software development to explicitly document code examples .
programmers have to search for code examples through previously wr itten projects and publicly available code repositories on the internet e.g.
sourceforge.net .
search for source code examples on the internet is realized using source code search engines .
these engines are known as internet scale code search engines such as ohloh code previously known as koders and google code search discontinued service as of march .
textual similarity between code snippets and the query is the dominant measure used by existing internet scale code search engines .
holmes et al.
study shows that such similarity is not sufficient for a successful code example search.
buse and wiemer discuss that the answers of existing code search engines are usually complicated even after slicing .
in summary existi ng internet scale code search engines do not support spotting working code examples well .
to improve internet scale code search engines without affecting the way users interact with the engine s any candidate solution must hold three properties.
first t he input and output format of the search algorithm must be identical to that of internet scale code search engines.
specifically it must support free form querying.
the output should be a ranked set of code snippets.
second the time complexity of the sea rch algorithm must be low.
otherwise the solution cannot be integrated with online engines for search on large scale data.
third it should not be limited to api usage queries.
although recent studies show that a considerable amount of code search querie s are related to api usage problems programmers still use code search engines for other problems .
several approaches known as pattern based code search have been proposed in the literature.
none of these approaches satisfies permission to make digital or hard copies of all or part of this work for personal or classroom use is gran ted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
a bstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org .
icse june hyderabad india copyright acm ... .
.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
icse may june hyderabad india copyright acm ... .
all of these propert ies.
for example the earlier approaches hold high time complexity.
the time complexity of sniff at run time is where n is the number of snippets .
such complexity decreases the ir applicability when there is a comprehensive corpus covering millions of code snippets.
moreover the earlier approaches mainly focused on api usage and their output is abstract programming solution s. an abstract programming solution is a sequence of api method call fingerprints e.g.
.
we propose an approach that improves the performance of internet scale code search engines in the context of spotting working code examples.
our research is different from earlier work e.g.
sniff and prime a s our approach satisfies the three mentioned conditions by exploiting clone detection fundamentals .
our approach combines baker s p strings with carter et al.
vector space model based approach for clone detection.
we extend this idea using frequent itemset mining to detect popular programming solutions .
this approach is not limited to api fingerprints it considers all source code tokens.
furthermore o ur approach for spotting working code examples is holding the same complexity as internet scale code search engines .
this approach answers a free form query within hundreds of milliseconds on a corpus covering millions of code snippets.
this paper makes the following main contributions we propose an approach that can be used seamlessly by internet scale code search engines e.g.
ohloh code .
our approach improves their performance in spotting working code examples.
we evaluate the scalability and feasibility of our approach with a corpus of java open source projects .
we identify a superior ranking schema for spotting working code examples on a noisy comprehensive corpus .
finally we observe that ou r approach outperforms ohloh code in spotting working code examples .
organization of the paper .
section outline s our research motivation .
section overviews the background clone detection techniques .
section provides the details of our approach in terms of fact extraction mining and search.
in section we present the evaluation results and the threats to validity.
r elated work conclusions and future work are presented in section s and .
.
motivating examples in this paper we propose a n approach that finds and rank s working code examples.
in this section we elaborate on our problem statement using two motivating example s. for each example we discuss the results one would obtain from existing internet scale code search engines previously proposed pattern based code search approaches e.g.
sniff and our approach .
to increase the readability we discuss our approach separately from the other pattern based code search approaches.
.
bubble sort example bubble sort is a classical programming problem .
however bubble sort is not a popular sorting algorithm due to its time complexity.
as a result there is no native or popular api that implements bubble sort in java.
nevertheless junior developers and students still search for bubble sort implementation on the internet .
a typical free form query is either or .
figure .
the bubble sort example internet scale code search engines.
the bubble sort query is one of the queries that are well supported by code search engines e.g.
ohloh code .
a relevance based search approach finds such snippet easily as it is usually embedded in a method or class that is called bubble sort.
ohloh code exploits such structural data for term matching.
as a result the first hit returned by ohloh code is a correct implementation of the algorithm that is embedded in a method called bubblesort .
however these engines heavily rely on term matching and relevance search that are adopted from web search engines .
therefore the second hit returned by ohloh code is an empty if condition .
this snippet is placed as the second hit since it belong s to a file called sortingalgorithms bubble sort .java .
this example highlights the challenges faced by any code search approach that depend s solely on term matching and textual similarity .
pattern based code search approaches .
approaches such as prime and sniff cannot answer the bubble sort query.
this is mainly due to the fact that they are limited to mining api related fingerprints .
our approach.
figure shows the first hit that our spotting approach returns for the bubble sort query within only millisecond s. the result is based on .
million indexed code fragments that each has at least lines of code extracted from open source projects .
the spotted snippet is one of the implemen tations of a bubble sort algorithm .
this motivating example also highlights one of the interesting features of our code search approach .
a matching answer might not necessarily contain the query terms.
we achieve this feature by exploiting ideas from clone detection .
in this example there is no occurrence of the query terms such as bubble sort or bubblesort within the spotted fragment while the code snippet actually implements a bubble sort.
it is worth mentioning that our search approach only uses the content of code snippet and does not consider other resources such as inline comments javadocs and the signature of the owner method class or file .
moreover our approach is able to search and mine over both regular and control flow statements with les s complexity compared to the graph based search approaches e.g.
prime .
in summary i t is essential to support mining and searching of control flow statements for cases such as the bubble sort problem since loops and conditions constitute the cornerstone of their implementations.
.
md5 example the second motivating example is a code search problem related to spotting working code examples for md5 hash value gener ation in java.
the goal is to acquire md5 representation of an object e.g.
password in a string format.
md5 hash code generation is not a trivial programming task using java native libraries.
first there is no method or class name within the java libraries called md5.
the actual class and methods responsible for the md5 generation are messagedigest getinstance update and digest .
second the conversion of the binary representation of hash values to string has special cases to be handled.
without a proper conversion method i f a generated hash value starts with this leading would be omitted during the conversion from the original format to string binary numeric string .
internet scale code search engines .
existing code search engines do not support such queries e.g.
well.
for example none of the top answers of ohloh code is a complete code example showing how to use messagedigest for md5 hash value generation.
in the best case the result only show s how to create an object of messagedigest using the getinstance method.
pattern based code search approaches .
this query is challenging for recently proposed pattern based code search approaches.
it is not possible to use most of the e arlier work for this running example as they do not support free form queries and they are limited to api names i.e.
cannot handle the md5 term within the query .
surprisingly sniff that supports free form querying also failed to answer this query.
our approach.
figure presents the top rank ed hit that our approach returns for the md5 query .
this is a complete answer based on buse and weimer discussion since it includes all implementation steps even the error handling cases.
.
summary in summary t he examples in this section highlight three major features of our approach spotting working code example s for api usage and algorithmic problems the ability to provide self contained examples and less dependency on term matching.
furthermore our proposed approach requires only the code snippets content1.
these features illustrate the potential of our approach for code search in the context of pragmatic reuse.
our approach takes advantage of clone detection fundamentals which allows us to eli minate limitations of earlier pattern based approaches and existing internet scale code search engines.
.
background originally similarity detection in source code has been explored by clone detection researchers in the context of software maintenance.
the underlying algorithms e.g.
koschke and inoue et al.
target detection of major clone types .
in general while type mandates content similarity i.e.
similarity in token names type focuses on pattern similarity.
further changes i.e.
addition al statements result in type similarity.
figure .
the md5 example comments javadoc and the method signatures are excluded.
although they are invaluable source of information for code search we excluded them from our study to evaluate the performance of our pattern mining approach independently.
the clone types are defined based on observable similarity forms in the source code.
at source code level clones share two forms of similarity pattern and content.
for example int temp and float f constitute a type clone pair since they are following the same pattern.
in this example the shared pattern is observable if we ignore the token names.
when substituting the major tokens by the out put for both statements will be which reveals the pattern similarity.
we found this approach useful for spotting working code examples.
for example by representing all code fragments in the corpus using techniques for type clone detection we can identify common lines of code implementing bubble sort algorithm e.g.
figure .
since it is also possible that all code fragments implementing bubble sort do not contain exact statements we also consider type clone detection techniques to find similar fragments with slightly different statements .
our approach is based on two early ideas proposed for clone detection in software maintenance by brenda baker and carter et al.
.
in the following we review each of these ideas separately.
finally we discuss the derived similarity search model based on these two adopted ide as.
type similarity detection via p strings.
in our research we adopted the idea of parameterized strings or p strings from brenda baker s parameterized pattern matching theory for type source code clone detection.
p strings provide us with the ability to match two strings in case of some s pecific forms of dissimilarity e.g.
using different variable names .
in this context the strings refer to sequences of source code tokens.
if two strings contain both ordinary and parameter symbols from alphabets and respectively they are called p strings .
by exploiting this idea we can det ect if two strings are similar i.e.
p match when a one to one transform function exists which translates one of the pstrings to the other one by manipulating the parameter symbols alphabet .
since dissimilarity in token names can negatively affect the recall of pattern mining in our research we use the idea of p strings to improve our recall .
for example t his approach helps us to find lines of code common to implement bubble sort e.g.
figure even if they are implemented with different variable names .
type similarity measurement via cosine similarity.
originally carter et al.
proposed cosine similarity as a measure that can be used for type clone detection.
they converted code snippets into small sized vectors.
the vectors represent occurrence and frequency of the underlying programming language keywords e.g.
while or return .
carter et al.
measured the similarity between two code snippets via the angle between corresponding vectors.
in our approach we extend the original idea of carter et al.
type similarity search by redefining the vector elements .
instead of using programming language keywords as the internal elements of vectors we use pstrings .
our generic similarity search model.
using the adopted clone detection techniques we are able to mine popular abstract solutions e.g.
programming patterns for bubble sort implementation even if the input data differs in statements and token names and derive a similarity search model that finds the closest patterns to a subject.
we refer to this derived function as the generic sim ilarity search model in this paper .
our search model uses v ector space model vsm which has been widely used in information retrieval e.g.
.
in our case a vector captures p strings of code snippets rather than terms.
ep p strings querysource code corpuscrawlersourceforge google codeanalysis tokenization term extraction transformation mining pas via p strings query analysisrelevance search for epsselected epskeywordspas similarity search for pas type search selected passimilarity search for code snippets type search selected code snippetsalignedsnippets pas p keywords k ep e snippets c aps a pas popular abstract solution ep encoded code pattern major processing step aligned datasets figure .
our approach towards spotting working code examples the space consists of vectors e.g.
with being the weight frequency of a p string .
similar to traditional information retrieval we also determine both local and global popularity of an entity by calculating its occurrence frequency.
our weighting function is derived from tf idf .
the loc al frequency denoted by captures the number of occurrences of a p string within a particular code snippet .
the global frequency represents the total number of code snippets with at least one occurrence of the p string where the total number of snippets is denoted by .
the similarity degree between two patterns is calculated using the cosine similarity function that measures the angle between participating vectors.
as a result we derive a similarity search function that supports type and pattern similarities.
note the complexity of our search function is similar to existing code search engines on the internet e.g.
ohloh code since both are using the same underlying search model that is vector space model.
the time complexity is where n is the number of matches.
the complexity of our approach is therefore lower than the other similar approaches e.g.
sniff .
with is the weight eq.
of p string .
approach figure summarizes the major online and offline processing steps of our proposed approach.
the key mining and search steps are marked in figure .
both key similarity search steps are covered by the generic simi larity search model section .
the offline processes are responsible to extract the code snippets generate the p strings and associated keywords.
finally the popular patterns are detected.
at run time for a given query first the most relevant p strings are identified.
then we use the generic similarity search model two times consecutively to first find the best candidate popular patterns and second locate the best code examples.
.
definitions in this section we define the major concepts in our approach.
definition .
a query is defined as an unordered set where each term can be a data type e.g.
primitive type class name or interface method name or general concept e.g.
download or bubblesort .
definition .
a code snippet is defined as an ordered list of lines of code that are extracted from a method body.
we use the code snippet only for final result preview.
denotes the collection of all snippets in the corpus.
definition .
an encoded code pattern is a set of all strings that are all mutually p match using the p strings idea .
each is identified by a unique identifier .
for each non empty line of code written in java we add a string .
by applying identifier splitting techniques on all strings belong to an we extract a set of associated keywor ds.
the set of associated keywords for the given are called ep s keywords .
and denote the collections of all s and s respectively where corresponding entities are aligned.
figure provides some examples for and sets.
definition .
an abstract programming solution is a unordered set of encoded code patterns .
in the major processing steps we use the abstract presentation i.e.
of code snippets .
is a unordered set of all encoded code patterns corresponding to lines of code within the snippet .
figure presents an example for of a snipp et related to md5 hash code generation.
finally we define a popular abstract solution as an abstract solution that all of its items occur in more than a certain number of es in the given corpus.
and denote the collections of all es and es.
while corresponding items within and are aligned remains a standalone collection.
.
data analysis steps in this section we describe the details of our approach for mining popular abstract solutions pas .
.
.
fact extraction the approach shown in figure requires at least two data families code snippets and popular abstract solutions.
while the code snippets can be extracted from extensive web crawling and data gathering identifying popular abstract solutions require different types of data processing.
the initial processing steps populate all parts of the corpus except for the popular abstract solutions.
the only input data is a source code dataset crawled from the internet.
the dataset only includes source code snippets.
there is no need for other information such as content of dependencies e.g.
binaries of libraries .
we split each file into code snippets.
for each method body we create one code snippet.
byte unencodedpassword password .getbytes messagedigest md new messagedigest .getinstance md5 md.reset md.update unencodedpassword byte encodedpassword md.digest .
new .
.
.
.
code snippet ci encoded code patterns ep unique identifiers of eps apsi abstract solution frequencies li associated keywords only for ep md5 sha rfc1321 crypt md digest getinstance messagedigest figure .
an example for abstract programming solution extraction to create the abstract programming solutions the original content code snippets have to be transformed at a higher level of abstraction.
our approach uses the idea of p strings to create this abstract representation which allows for removal of unnecessary code details e.g.
variable names .
we exploit a concept called encoded code pattern in order to map the p strings idea to the vector space model instead of the original parameterized suffix tree based search model .
in this a pproach we assume all pstrings that are p match shar e the same encoded code pattern .
we also assign a unique identifier to each encoded code pattern.
figure provides an example f or all internal i.e.
temporary and persistent extracted facts.
in this example t he input is a code snippet denoted by which has five lines of code.
first we create the corresponding for each line of code.
second we map all extracted s to the corresponding unique identifiers.
type clone detection via the p strings helps us to locate similar code patterns efficiently i.e.
such as lines and in figure .
for each we maintain a global set of all associated keywor ds known as .
that is every time that we meet an occurrence of an we extract all keywords from the given line of code and add them to the .
figure includes an example of associated keywords to .
as the example shows in addi tion to the current extracted keywords such as md5 the set also includes other related terms such as sha that have been added earlier by visiting the same pattern i.e.
in other usage scenarios and snippets.
finally t he gray area highlights the actual persistent output that constitutes three major sections updated associate keyword sets the abstract solution and the frequency information of snippet i. these three fact types constitute the baseline search space of our approach.
note is a set and it does not include the ordering information of the corresponding code snippet .
using such data presentation i.e.
and we can apply the vector space model and cosine similarity for type similarity search.
.
.
mining popular abstract solutions abstract programming solutions and associated keywords are not sufficient to support spotting working code example s. we need to identify the popular solutions in the corpus in order to impr ove the quality of our search approach.
to identify the popular abstract programming solutions a frequent itemset mining such as the fpgrowth algorithm can be employed.
since the input for the algorithm are encoded code patterns aps not the actual code the output will be popular abstract programming solutions .
frequent itemset mining algorithms are capable of extracting popular patterns within a provided record set with a record being one or more items.
in its most simplistic form the algorithm requires a dataset and a support value .
the support value determine s the minimum number of occurrences of a pattern before it can be considered a frequent item set.
we adapted a variation of the itemset mining concept referred to as maximal frequent itemset mining .
this variation has two specific properties it considers maximal itemsets and it has no ordering constraint.
the omission of the ordering constraint provides us with a robust mining feature where re ordering of code statem ents does not interfere with the pattern mining process.
moreover this approach help s us to successfully run the algorithm on large scale data successfully.
the maximal property overcomes some of the challenges of the other itemset mining approaches such as the possibility of producing an exponential number of frequent sub itemsets.
the occurrence of sub itemsets in the search space is a threat when answer completeness is required.
a maximal itemset is defined as given possible elements i.e.
encoded code pattern s in the code base and code fragments is the set of all possible reputable code patterns defined as where .
a freq uent itemset is maximal if .
is the minimum size and is the support i.e.
min popularity of the pattern .
as the final data processing step we run the mining algorithm to extract popular abstract solutions.
the popular abstract solutions along with original abstract solutions their occurrence frequency information and associated keywords constitute the complete search space of our approach.
.
searching and ranking working code examples since achieving an optimum result set in our research context is often impractical the alternative is to provide retrieval and ranking models capable of producing high quality ranked result sets.
popularity of a solution is a key criterion that cannot be ignored to avoid poor quality result set e.g.
.
there are other factors affecting the ranking process e.g.
textual similarity .
for a free form query similarity is continuous not binary so solutions other than simple filtering and matching are required.
the tradeoff between these factors makes spotting working code examples a challenging task.
in the following we describe how we use our generic similarity search model section to satisfy the above mentioned concerns.
for a given free form query the approach returns a ranked result set of working code examples by finding the most relevant encoded code patterns to the most complete popular abstract solution with regard to the output of the first step and the most similar code snippets to the output of the second s tep.
step relevanc e search.
the first querying process in figure selects the relevant encoded patterns by comparing their associated keywords to the query terms.
that is the data used in this search problem are query terms and s keywords k while the output consists of encoded patterns.
it should be noted that an encoded code pattern that shares a keyword with is not automatically included in the candidate list.
only hits are selected in order to maintain the relevancy between the query and the final spotted code fragments as query terms are no longer used explicitly in the search process after thi s step.
moreover the matching s are ranked based on their relevancy to the query.
this step reuses the well known relevance search approach as web search engines e.g.
.
step similarity search for pas selection .
in this phase the popular abstract solutions are identified using our generic type similarity search model where the query is made of the candidate encoded patterns i.e.
the output of step .
due to the clone search based approach the popular abstract solutions can now be ranked based on their similarity to this intermediate query.
in this greedy approach we look for the complete st popular solutions that are satisfying most of the candidate encoded code patterns.
step similarity search for working code examples .
in the last step figure the spotting of the best working code examples takes place.
the ranked result set of the previous step identifies the list of candidate popular abstract solutions sorted based on their completeness and relevancy to the free query.
in this phase we iterate over the candidate list and use each pas as a separate query to search over abstract solutions of indexed code snippets using our type similarity search model.
for each we find the most similar i.e.
concise and complet e code snippets .
the spotted snippets are ranked based on their abstract solution similarity to the target .
additionally this step is necessary since it ensures that the results are syntactically and semantically correct which is crucial as our mining and querying model ignores the ordering of the statements.
the internal result of this search approach is a two dimensional hit list for each free form query.
each row contains the ranked cod e snippets matching a corresponding poplar abstract solution.
therefore while the fragments in each row are highly similar they look different from solutions in the other rows as they satisfy different popular abstract solutions.
finally we report the spotted working code examples for a given free form query by select ing the first hit of each candidate popular abstract solution .
algorithm summarizes the three search steps of our approach.
filtering step.
some queries contain known java types i.e.
java classes and interfaces .
in this case we discard any code snippet that is not associated with the types mentioned in the query.
this is a heuristic that we derived empirically to improve the chance of returning relevant answers.
the heuristic is motivated to compensate issues associated with our type similarity search that ignores the token names during th e actual search process.
for each query we can have correct incorrect or no answer.
a no answer status occurs when the filtering step discarded all of the snippets and returns no answer found .
the value of is limited by the number of sn ippets that our approach can check for filtering at run time given the available processing resources.
.
case study in order to evaluate the feasibility scalability and performance of our approach we require a reasonably large corpu s. we also need a set of measures for assessing the quality of spotted working code examples .
in this section we summarize the details of our corpus .
we al so review the details of our query set and quality assessment approach.
finally we report our findings and observa tions.
.
setup corpus.
our corpus includes source code crawled in the first quarter of .
this dataset covers approximately projects.
the dataset is based on source code files that were downloaded from svn git and cvs repositories from sourceforg e and google code.
to remove high level duplications in the dataset only one java file is selected for each available class name identified by its fully qualified name.
during the filtering step we were biased toward latest revisions such as files appear ed in trunk directory.
the crawled data with duplicated files initially included million java files.
after the filtering step the data were reduced to million java files .7m regular java class files and 140k files with default package .
we also discarded small size method blocks e.g.
less than five lines of code.
finally we extracted .
million code snippets.
table summarizes the key statistics of our corpus.
deployment.
to deploy an instance of the search engine we use a linux based system with a .
ghz cpu intel i7 and gb of ram.
the deployed instance of the search engine uses a single process and thread schema except for the java virtual machine processes e.g.
garbage collection.
the data processing steps e.g.
mining and indexing finished within a single day.
query dataset.
as part of our performance evaluation we adopted mishne et al.
s query set since it is not limited to a single domain.
the original datas et includes queries from java libraries.
however we extended it by including additional queries.
the additional queries are taken from programming questions posted on stackoverflow.
to ensure that the dataset reflect s real word search scenarios we selected our candidate terms from koders query log dataset .
table summarizes the final query dataset consisting of queries.
in addition to the target library i.e.
domain and query terms i.e.
free form queries table also inclu des the description of the queries used to evaluate the relevancy of retrieved code examples.
quality assessment features of working code examples .
in general not every code fragment that meets query criteria e.g.
sharing terms can be considered as a working code example .
although there is no formal definition of what constitutes a good working code example several features are discussed in the literature .
first a working code example should be correct and rel evant with regard to the query .
the code example should include the mandatory steps required to implement the underlying programming problem .
second a working code example should be concise self contained complete and easy to und erstand and reuse .
algorithm .
searching and ranking algorithm input q k e p a s output h for all end loop return table .
the features of our corpus feature value raw data java projects total java files unique java files loc m selected fragments selected lines lloc processed data unique encoded lines observed frequent abstract solutions solutions size encoded lines unique items max.
observed support duplicated files are eliminated based on the common fully qualified name heuristic fragments with at least logical line of code lloc lloc after removing duplicated encoded lines within each fragment .
note the number of unique encoded lines can be smaller than the actual unique lines of code .
for example int y and int x are counted only once since their en coded patterns are identical.
maximal frequent itemsets min max support .
thresholds are selected empirically.
table .
the extended query set domain description query terms apache commons cli retrieve arguments from command line getoptionvalue commandline eclipse ui check user selection iselection isempty eclipse gef set up a scrollinggraphicalviewer scrollinggraphical viewer eclipse jdt create a project iproject monitor apache commons net successfully login and logout ftpclient webdriver click an element webelement jdbc commit and rollback a statement executeupdate roll back preparedstatem ent http send a http request via urlconnection response urlconn ection runtime redirect runtime exec output with system read runtime memory get os level information such as memory memory ssh ssh connection ssh download download and save a file from network download urlcon nection md5 generate a string based md5 hash value md5 httpresponse read the content of a httpresponse object line by line readline httpresp onse lucene search via lucene and manipulate the hits search indexsearch er table .
features of w orking code examples feature measure correctness an answer should address the query expectation sharing terms with the query is not sufficient .
conciseness irrelevant loc readability well chosen variable name completeness missing statements well typed variable initialization control flow and exception handling table provides a brief summary of the features and measures used for identifying high quality working code examples.
in this paper we evaluate the performance from correctness conciseness and completeness points of view.
we select these three features as they can be evaluated objectively.
.
case study results this section presents and discusses the results of our two research questions .
for each research question we present the motivation behind the question the analysis approach and our findings.
rq1 what is the best ranking schema for spotting working code examples ?
motivation.
in this paper we focus on the problem of spotting working code examples.
spotting emphasizes on the quality of the first returned answer.
therefore performance measures such as recall are not the major concerns when evaluating spotting approaches.
this r equirement distinguishe s spotting working code example research from other related code search problems.
thus we aim to find the best ranking schema for spotting working code examples.
an ideal ranking places its best answer at the top of the hit list.
previously the popularity of solutions has been considered for ranking in the context of code search e.g.
.
the intuition is that the higher the popularity of a solution the higher the chance of acceptance by end user.
in addition to popul arity we explore the impact of similarity and size factors for spotting working code examples .
approach.
for this research question we study the performance of a set of potential ranking schemas.
we can rank code examples based on their similarity to a given query popularity of the popular abstract solutions and the size of the popular abstract solutions .
in total we consider five ranking schemas as summarized in table .
the similarity is quantified based on the position of the hits within the ranked result set produced by our approach.
in other words is our intrinsic ranking schema discussed in section .
popularity of an abstract solution is derived by the mining step i.e.
support value of pas.
size is measured based on the length of the underlying pas.
as we discussed earlier in section .
and table there are several features that high quality working code example s should hold.
we evaluate the performance of each ranking schema in terms of correctness conciseness and completenes s. the measures are selected from table .
correctness.
we consider an answer to be correct only if it satisfies the description of the given query in table .
note that our spotting approach may answer a question incorrectly or return no answer due to o ur filtering step section .
in general we prefer not to answer a question instead of returning a wrong answer to avoid a negative effect on the end users trust.
however excessive occurrences of no answer cases in the result sets reduce perceived q uality of the search service.
in this study we distinguish between incorrect and no answer cases when we evaluate the schemas from the correctness point of view.
we calculate the percentage of correct answers with regard to the total number of queri es and answered queries.
the two related measures are query coverage and precision .
a good ranking s chema should be able to perform well from both points of view.
conciseness and completeness.
a correct answer might miss minor tasks such as variable initialization or error handling .
therefore it is important to measure not only the correctness but also conciseness and completeness.
figure .
summary of the completeness and conciseness measures table .
studied ranking schemas ranking schema coverage precision similarity based s .
the candidate popular abstract solutions are ranked based on their similarity to candidate s. popularity based p .
the candidate popular abstract solutions are ranked based on popularity.
combination of p and s schemas ps .
the top hits of s are re ranked based on popularity.
size based a .
the candidate popular abstract solutions are ranked based on their size.
combination of a and s schemas as .
the top hits of s are re ranked based on size.
figure .
the average size of our popular abstract solutions to be able to meas ure conciseness and correctness we require two lists covering necessary and complementary tasks to be accomplished for each programming problem a query .
these tasks are pre defined for our query set table .
the necessary tasks are derived based on the description of each query in table .
the complementary tasks are related to minor programming steps such as error handling or variable declaration.
we extracted these tasks for each query from the best answers provided by the original query set or top ranked answers on stackoverflow .
we manually measure the missing minor tasks via completeness e.g.
.
completeness is defined as the number of addressed ta sks divided by the total number of tasks.
finally for each correct answer we manually measure conciseness.
conciseness is defined as the number of irrelevant lines of code divided by the total lines .
a line of code which is not related to the identified tasks is irrelevant.
we use wilcoxon signed rank test a non parametric test to identify if the observations are significant ly different .
results.
table summarizes the result of our correctness study using the coverage and precision measures .
the results show that only the ranking scheme based on a combination of size and similarity as under performs in both measures.
popularity based ranking is able to answer more questions successfully while similarity based and size based schemas have bett er precision.
the best precision is achieved by the combination of popularity and similarity.
this combination also answers more questions than using only similarity i.e.
.
during our analysis w e observed that there exist s a relation between the popul arity and size.
figure summarizes the average size of popular abstract solutions in our corpus.
the abstract solutions are grouped by their popularity which is measured by the number of occurrences of the solution within the corpus i.e.
the support value .
the result shows that the size decreases as the popularity increases.
except for the first few groups t he average size remains in a narrow range between and .
this behavior partially describe s the lower performance of size based ranking schemas e.g.
as .
next we study the performance of the ranking schemas in terms of completeness and conciseness.
figure presents the summary of our results.
specifically we focus on the surviving schemas from the correctness evaluation phase i.e.
p and ps.
the results show that the combination of popularity with similarity outperforms the pure popularity ranking schema from both completeness and conciseness points of view.
the wilcoxon signed rank test also confirmed that the improvem ent is statistically significant at the significance level .
.
45678910solutions itemset size solutions popularity support average median rq1 .
the combination of popularity and similarity leads to a better ranking schema for spotting working code examples.
figure .
summary of the best hit rank and ndcg studies comparing our approach ps ranking schema with ohloh code rq2 can our approach outperform internet scale code search engines?
motivation.
the ultimate goal of our research is to propose a search approach that improves the performance of internet scale code search engines in terms of spotting working code examples.
in this research question we compare our approach with ohloh code previously known as koders which is a publicly available internet scale source code search engine.
approach.
we evaluate both approaches from the spotting problem point of view.
an ideal spotting approach places the best working code example at the top of the list.
we study two measures proper for evaluation in t he context of spotting working code examples.
we compare the ranks of the best answers.
specifically we find the rank of the best answer within the given top k hits.
this evaluation approach reveals how many hits i.e.
code snippets the end user should review browse until she finds the best answer within the ranked result set.
a superior approach places its best answer closer to the top of the list.
the best answer is selected amongst the top k hits of each approach independently.
the best answer is iden tified using the same measures used in rq1 for correctness completeness and conciseness.
this is necessary and also reasonable as each approach has its own corpus.
our initial value for k is i.e.
ohloh code default result set size.
however for the o hloh code study there were some queries with no correct answer within the first hits.
in this case we increase k by until we reach to a window with at least one correct answer.
we also study the performance using normalized discounted cumulative gai n ndcg .
ndcg is one of the state of the art measures in information retrieval.
normalized discounted cumulative gain evaluates the ranking capability of a ranking schema e.g.
whether highly relevant answers appear toward the top of the hit list.
t he major advantage of ndcg over the other popular measures e.g.
map is that its result is comparable when there is heterogeneity in the data under study.
it can compare two search engines or ranking schemas that are deployed with different datasets .
the environment issues such as data scarcity affects less ndcg comparing to the other measures such as precision at k. we calculate ndcg of each approach for queries in table .
we assign relevancy values to the top k hits of each approach by considering the same three measures studied in rq1 for correctness completeness and conciseness.
results.
figure provides a summary of our study on the rank of the best available answer within the top k hits.
the results show that our approach outperforms ohloh co de in using the available resources .
our approach place s its high quality answer s closer to the top of the ranked result set.
our approach ranks its best answer as the second hit in the worst case.
figure also summarizes our ndcg observation.
similar to the previous measure our approach outperforms ohloh code.
finally wilcoxon signed rank test confirmed that the observed improvement is statistically significant .
.
threats to validity there are some threats to validity related to our performance evaluat ion study.
threats to external validity concern the possibility to generalize our results.
we identified two major threats to external validity of our study.
the first concern is the differences in the input data.
ohloh code and our approach are using diff erent datasets.
however both are covering similar number of java open source projects crawled from the internet.
the second concern is about the query dataset.
we chose a recent query dataset that is not limited to a single application domain .
since the number of quer ies was smaller than earlier studies e.g.
to queries we randomly added queries from programming questions posted on stackoverflow.
the final query dataset consists of queries.
we also confirmed that the query terms are reflecting real word queries by choosing terms extracted from koders query log dataset .
although we tried to address both concerns still they remain threats to validity of our study.
in our study we considered c orrectness conciseness and completeness measures to evaluate the quality of spotted working code examples.
we selected the measures based on earlier studies on the quality of code examples.
nevertheless these measures do not replace other evaluation approaches suc h as user studies.
.
related work various forms of recommendation systems for software engineering exist in the literature .
in this section we review a subset of the research focusing on source code search .
we report the related work separately based on their output type s. .
abstract solution diverse mining and search approaches are proposed for recommendation on api usage scenarios .
the ir main output is a method call sequence.
in some cases the sequence includes further rq2.
our analysis shows that our approach outperforms ohloh code the studied internet scale code search engines for spotting working code examples by consistently placing the best available answer within the two top hits.
information such as object instantiation steps.
mapo xie and pei establishes the baseline of the frequent pattern mining based code search in the context of api usage.
acharya et al.
extended this approach by including api static traces and employing partial orders mining .
up miner is presented by wang et al.
as a successor of mapo .
it combines clustering and sequence mining to find re occurred sequences of api fingerprints i.e.
method call tokens .
in this area sniff chatterjee et al.
is one of the few models that directly addresses the free form querying problem by exploiting longest common subsequence mining and programming documents i.e.
javadoc .
however sniff requires the complete compilation unit e.g.
external libraries and documentations .
mishne et al.
approach prime follows a lazy method by delaying the pattern mining task to run time.
lazy mining has been investigated earlier in mapo sniff and parseweb .
prime extracts partial temporal specification from method call sequences to find possible solutions.
prime is extending parseweb s approach with extra semantic analysis.
abstract solutions help us to identify high level steps of programming problems but they can not replace source code examples .
although it is possible to report corresponding code snippets for each abstract solution using proactive book keeping the code snippets are not ranked.
this is problematic for large scale experiments when for an abstract solution numerous code snippets with different quality level s are available e.g.
up to snippets .
our research addresses this problem by directly spotting and ranking code examples.
moreover our approach is not limited to api fingerprints.
.
synthesized source code another approach is to synthesize sou rce code from abstract solutions .
parseweb thummalapenta and xie answers program ming questions formulated as source destination template.
the answer is a synthesized code snippet that satisfies the given question.
to decide about the best approa ch to answer the given question parseweb models method invocation sequences including the data flow information as directed acyclic graphs and exploits shortest path discovery algorithms.
the final synthesized answers are ranked based on their popularity and size.
parseweb improves prospector mandelin et al.
approach by considering additional source of information i.e.
code samples as prospector relies only on api definitions.
recently buse and wiemer apply mining on graph models created from the data flow and method call sequences.
buse and wiemer research provides promising results on synthesizing source code examples however still it cannot outperform human written code examples.
our research proposes a solution that can spot working code examples from source code written by developers.
.
code snippets strathcona holmes and murphy uses structural matching to find the most similar code examples to api usage problems.
the core of the approa ch is made of six heuristics for structural comparison via similarity measurement between inheritance links method calls and type uses.
the top code examples are ranked based on their success in surviving more heuristics.
xsnippet sahavechaphan and c laypool improves strathcona by using graph mining techniques.
the goal is to decrease the number of irrelevant answers.
however it answers only object instantiation problems.
to improve the ranking a combination of popularity size and context aspe cts are employed.
as discussed by chatterjee et al.
current approaches that are returning code examples are limited to api usage scenarios and expect the end user to know what classes and methods she has to use to accomplish the given develop ment task.
they rely on specific querying templates such as single api name e.g.
up miner source destination e.g.
parseweb or incomplete program e.g.
prime .
free form querying relaxes this condition by allowing end users to express a query using a variety of terms including class and method names.
sniff is the closes t approach to ours.
however its output is a ranked list of abstract solutions not code examples.
its time complexity is high at runtime i.e.
where is the number of hits which decreases its usability for real applications with large scale corpus.
finally it is limited to find code example for known apis and it requires a complete compilation unit including their documentations.
there are also som e studies that propose improvement to internet scale code search engines.
reiss proposes the idea of semantic based code search which exploits a variety of resources such as test cases to improve code search experience.
mcmillan et al.
approach portfolio supports programmers in finding relevant funct ions and their usage scenarios.
portfolio search is based on natural language processing and network analysis algorithms such as pagerank on call graphs.
a comparison with koders revealed that portfo lio is better in finding the relevant functions to a given programming task.
our research focuses on finding working code examples.
the closest study to our research is done by bajracharya and lopez .
they proposed a solution that improves the quality o f the retrieval approach in terms of recall by exploiting api fingerprints.
diversely in our study we improve the quality of the search approach in terms of finding better examples.
.
conclusion available internet scale code search engines do not support well queries about code examples .
in this paper we use code clone detection models to support spotting working code examples.
our approach makes it possible to mine programming patterns and search for similarities over all types of statement s e.g.
control flow data flow and api fingerprints without graph based models improving scalability and efficiency .
our approach supports free form querying.
this is different from most of the earlier work expecting partial code api names or data flow information as the query.
the time complexity of our approach is similar to internet scale code search engines.
we study the feasibility of our approach with a representative corpus of open source java projects.
we first identify the best ranking approach in our research context.
we find that the combination of popularity and similarity outperforms the dominant ranking approach in the literature .
second we study the performance of our approach by comparing it with ohloh code an internet scale code search engine .
our study show s that our approach significantly outperforms ohloh code in finding working code examples .
as a result our approach can be adapted internally by existing code search engines to improve their performance in spotting working code examples.
as the immediate future work we plan to study the feasibility of our approach for the other popular programming languages .
we also plan to release our research as a standalone search engine.
.