will you still compile me tomorrow?
static cross version compiler validation chris hawblitzel shuvendu k. lahiri kshama pawar hammad hashmi sedar gokbulut lakshan fernando dave detlefs and scott wadsworth microsoft microsoft way redmond wa abstract this paper describes a cross version compiler validator and measures its e ectiveness on the clr jit compiler.
the validator checks for semantically equivalent assembly language output from various versions of the compiler including versions across a seven month time period across two architectures x86 and arm across two compilation scenarios jit and mdil and across optimizations levels.
for month to month comparisons the validator achieves a false alarm rate of just .
.
to help understand reported semantic di erences the validator performs a root cause analysis on the counterexample traces generated by the underlying automated theorem proving tools.
this root cause analysis groups most of the counterexamples into a small number of buckets reducing the number of counterexamples analyzed by hand by anywhere from to .
the validator ran on over methods across a large suite of test programs nding previously unknown correctness and performance bugs in the clr compiler.
categories and subject descriptors d. .
compiler validation general terms veri cation reliability keywords compilers veri cation translation validation .
introduction compilers have grown enormously complicated in response to demands for new language features and better optimizations.
when adding new features and optimizations to an existing compiler there s always a danger of introducing new bugs into the compiler.
such bugs might cause existing programs to fail when recompiled with a new version of a permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
esec fse august saint petersburg russia copyright acm ... .
.compiler this danger is particularly worrisome for just intime jit compilers where installed programs might suddenly stop working due to a jit compiler upgrade.
worse a jit compiler bug might introduce a security vulnerability into a system that trusts the compiler for secure execution of downloaded code as in the case of javascript engines java virtual machines and .net silverlight virtual machines.
to prevent new bugs in existing compilers compiler developers typically run the compiler on large suites of regression tests.
even very large test suites will not catch all compiler bugs though .
therefore many researchers have turned their attention to static validation techniques for compilers using theorem proving technology to perform compiler verication or translation validation .
compiler veri cation uses a theorem prover or proof assistant to verify a compiler correct once and for all so that all compiler output is guaranteed correct for all valid source programs.
translation validation on the other hand runs a theorem prover after each compilation to check that the output of the compiler is semantically equivalent to the source program.
compiler veri cation requires verifying the compiler implementation which is di cult for large compilers.
so far compiler veri cation has scaled to moderately sized research compilers .
although larger compilers have been formalized to some extent they have not been proven correct and it remains a daunting task to do so.
by contrast translation validation requires only veri cation of the compiler output not the compiler implementation.
as a result translation validation has been used to test gcc and llvm both large widely used compilers .
nevertheless translation validation has long su ered from false alarms because program equivalence is undecidable the theorem prover often fails to prove equivalence between the compiler output and the source program even when equivalence actually holds.
necula reports false alarm rates of up to per function for individual compiler optimization passes while tristan et.
al.
and stepp et.
al.
report false alarm rates of for combined series of optimizations.
when running the theorem prover on tens of thousands of test functions it s currently very difcult to investigate all of these failures for all optimization passes by hand.
to make static compiler validation practical for day to day compiler development we must reduce the burden of false alarms on compiler developers.
this paper proposes two techniques for static compiler validation with a low false alarm burden we compare multiple versions of the compiler assembly language output rather than comparing the sourcecode to the assembly language code.
we test several incarnations of this idea comparing the output of multiple versions of a compiler over time speci cally across check ins to the compiler implementation over many months comparing the output of a compiler for two di erent architectures arm and x86 comparing the output of a compiler in jit mode to the compiler in mostly ahead of time mode based on the mdil machine dependent intermediate language and comparing the output of a compiler with di erent optimizations enabled.
when an alarm is raised our tool automatically produces counterexample traces showing values that cause divergent behavior in the two assembly language programs.
the tool then uses the traces to automatically infer the suspected root cause of the divergent behavior.
the root causes classify the counterexamples into a small number of buckets greatly reducing the human e ort needed to investigate equivalence failures.
comparing multiple versions of a compiler over time produces a particularly low false alarm rate because much compiler development consists of refactoring of the existing compiler implementation or adding new features to the compiler.
in these cases the goal is to notdisturb the compiler output for existing programs to minimize the risk of new bugs that break compilation of existing programs.
thus across checkins to the compiler we often nd that the compiler assembly language output remains syntactically identical.
even when the output is syntactically di erent the di erences are often easy for a theorem prover to check for semantic equivalence.
one concern with this cross version validation is a lack of ground truth the validation may miss a bug that appears in both the old version of a compiler and the new version.
thus one might worry that even if cross version validation is easier to use than traditional translation validation it might provide weaker correctness guarantees.
we believe though that cross version validation and translation validation have complementary strengths.
for example a developer might use translation validation once to provide ground truth for one version of the compiler but then use cross version validation to check subsequent versions of the compiler relative to the original translation validated version.
in addition existing implementations of translation validation don t check the complete translation from source code to assembly language code.
instead they usually assume that the source code has been parsed into an abstract syntax tree and thus miss bugs in the compiler s parser.
furthermore they typically check the intermediate compiler representation but not the generation of assembly language code .
by contrast cross version validation catches any changes to the compiler that cause semantically di ering output including changes in parsing and assembly language generation.
the rest of this paper describes our implementation of cross version validation and root cause analysis and describes their application to a real world optimizing compiler the .net common language runtime clr jit compiler section describes the implementation of our crossversion validator based on the symdi symbolic di erencing tool the boogie program veri er and the z3 automated theorem prover.
these tools make the implementation of the validator considerablyeasier allowing us to focus our attention on the semantics of assembly language instructions the model of memory and the model of the run time system.
section describes our root cause analysis based on the output from the symdi boogie and z3 tools.
section presents experimental results containing measurements of false alarms across months of compiler check ins for source control branches of the clr under active development including x86 to arm validation jit to mdil validation and optimized tounoptimized code validation.
the measurements show that the average false alarm rate for month to month di erences is .
an order of magnitude lower than the false alarm rate for recent translation validators and the false alarm rate for month to month syntactic identity checking.
the measurements also show the e ectiveness of root cause analysis which grouped anywhere from to of the false alarms into a small number of buckets with no false alarms ever placed in the wrong bucket .
one limitation of our current work is that the validation is not sound section discusses the reasons for unsoundness such as assumptions about aliasing assumptions about the run time system and unsound modeling of loops.
nevertheless even without an absolute guarantee of correctness the tool is still useful for catching bugs measurements in section .
show that the tool is very e ective on real code with arti cially injected bugs with just a false negative rate while section .
describes real bugs found with the tool during clr development and testing.
.
implementation our validator takes assembly language programs generated by two versions of the clr compiler and tries to prove the assembly language programs equivalent.
each assembly language program consists of a series of compiled method bodies functions each consisting of a sequence of assembly language instructions.
as in earlier translation validation work our validator works on one method body at a time.
in other words if compiled program p consists of method bodies m1..mn and compiled program p consists of method bodies m1 ..mn the validator tries to prove m1 equivalent to m1 m2 equivalent to m2 etc.
the validator converts each method body into a procedure in the boogie programming language.
boogie is a simple imperative language supporting assertions e.g.
preconditions postconditions loop invariants that can be statically checked for validity using the boogie veri cation generator.
boogie is typically not used to write programs directly but instead is used to encode programs from other languages such as c and assembly language.
boogie provides basic statements such as if else goto and assignment and basic expressions such as variables integers bit vectors and function applications.
these constructs can encode a variety of other programming language constructs.
this section describes the encoding of assembly language used by our validator including encodings of arithmetic memory and calls to other methods and run time system functions.
it then describes how the symdi tool boogie tool and z3 automated theorem prover process the encoded assembly language.
.
arithmetic as a simple example the validator encodes the x86 instruction add eax ebx as a single boogie statement eax add eax ebx .
here add is a boogie function and eax and ebx are boogie variables type val var eax val ebx val ecx val edx val var esi val edi val esp val ebp val function add x val y val val ... there are several possible de nitions of add.
first it could be uninterpreted declaring just that there exists an add that computes an output deterministically as a function of its two inputs function add x val y val val however an uninterpreted add would lack properties such as commutativity and associativity.
thus if method mk contained add eax ebx while method mk contained add ebx eax the validator would be unable to verify that both add instructions compute the same value.
it s possible to specify some of these properties as boogie axioms axiom forall x val y val add x y add y x rather than axiomatizing arithmetic operations from scratch though it s more e cient to use the arithmetic functions built into modern theorem provers like z3 either using z3 s big vectors or z3 s integers type val bv32 function add x val y val val add32 x y type val int function add x val y val val x y the rst de nition declares the val type to be a 32bit bit vector value which exactly matches the register sizes on bit architectures such as x86.
the add32 function is the theorem prover s underlying addition operation for bit numbers.
the second de nition declares the val type to be an arbitrarily sized mathematical integer.
clearly the second de nition is only an approximation of the underlying hardware register values which are not arbitrary sized.
however an automated theorem prover usually reasons more quickly about mathematical integers than bit vectors.
thus there is a trade o between a sound representation of the hardware and theorem proving performance.
after implementing both de nitions we found that bit vectors caused the theorem prover to time out on medium sized and large methods making the validator only useful for small methods.
therefore we chose to model machine arithmetic using mathematical integers even though this can lead to both extra false alarms and missed bugs.
one drawback of using mathematical integers is a lack of built in de nitions for bitwise operations such as bitwiseand and bitwise exclusive or.
compilers often make clever use of such operations in generated assembly code.
for example x86 compilers often exclusive or a register with itselfto produce the value since the exclusive or instruction happens to have an e cient x86 encoding.
compilers may also use bitwise and to align the stack pointer to byte or byte boundaries or to truncate bit values to bits or bits.
to support such idioms we declare a small set of axioms for bitwise operations e.g.
the exclusive or of an integer with itself equals .
the validator also encodes the x86 oating point stack and status ags we omit these details for brevity.
.
memory boogie provides an array type written t2 that maps values of type t1 to values of type t2 along with expressions to read array elements and update arrays with new elements.
the validator encodes memory as an array mapping addresses to values.
however reasoning e ectively about memory loads and stores requires some understanding of the memory s structure and potential aliasing between di erent pointer values.
for example consider the following boogie encoding of the x86 instruction sequence mov eax mov edx mov eax type val int var mem val ... mem eax mem edx eax mem if ebx points to a heap object e.g.
an array while esp points to a stack frame the compiler will assume that addresses esp and ebx ecx do not overlap so the nal value of eax in this code should equal the initial value.
to prove that the code does not modify eax the validator needs to track the compiler s non aliasing assumptions.
to represent such non aliasing information we model memory as a set of disjoint regions with one region per stack frame one region per heap object and one region for static elds.
we assume that stores to one region will not affect loads from another region.
we also assume that adding an o set to an address in one region produces an address in the same region so that esp resides in the same region as esp and ebx ecx resides in the same region as ebx.
these assumptions are unsound a big enough ecx will cause ebx ecx to overlap the stack frame.
for type safe code with array bounds checks the compiler enforces the soundness of these assumptions but for unsafe code like c and unsafe c the compiler makes these assumptions without enforcement.
to track the region associated with each address we de ne each register value to be a pair of a region identi er and an integer value type ref region identifier type word int type val pair of ref word function val r ref i word val function valref v val ref function valword v val word the val constructor creates a value from a region and an integer word while valref and valword extract the regionand integer word components from a value.
non address values use a special null region identi er as their region component.
we lift arithmetic operations to work on region word pairs.
for example we de ne add as function add x val y val val valwithregion valref x valref y valword x valword y where valwithregion builds a value for x y with a region chosen as follows x s region if y s region is null y s region if x s region is null and a dummy region if both x and y have non null regions.
this allows the validator to tell for example that add esp val nullregion keeps esp s region and when ebx is an address and ecx is a non address add ebx ecx keeps ebx s region.
consider the x86 instruction mov eax again.
after incorporating regions the following boogie statement expresses the assignment to memory mem eax this statement is still inaccurate in one way storing a bit value to memory should actually update separate memory locations esp esp esp and esp placing bits into each location rather than putting the whole bit word in the single location esp .
although we did implement this byte accurate model we found that the theorem prover performance degraded signi cantly relative to the less accurate word oriented model shown in the statement above leading to excessive theorem prover timeouts.
therefore this paper s experiments use the unsound word oriented model which may cause missed bugs or false alarms rather than the byte accurate model.
the validator can handle complex instructions that modify multiple words of memory such as the x86 rep movs and rep stos instructions often used by the clr compiler to copy or initialize large values .
for example the rep stosb instruction lls memory addresses edi ... edi ecx with a byte value from register al expressed in boogie as function rep stosb mem val ecx val edi val al val val axiom forall mem val ... al val i val ... valref edi valref i valword edi valword i valword i valword edi valword ecx rep stosb mem ecx edi al al ... .
control flow boogie supports goto statements and if else statements so it is straightforward to encode assembly language jump and conditional jump instructions.
for call instructions the validator uses uninterpreted functions to model the call s e ect on the registers and heap.
for example it encodes the instruction call eax as function callmem addr val heapsig int args list val function callout addr val heapsig int args list val ... heap callmem eax heapsig mem cons arg1 ...cons argm ... ret1 callout eax heapsig mem ... ... retn callout eax heapsig mem ... mem ...combine stack heap... ... assign ret1...retn to registers stack slots... here arg1...argm are the arguments to the call and ret1...retn are the return values from the call.
by reference parameters are treated as both arguments and return values.
to generate the encoding the validator must know how arguments and return values are laid out in registers and stack slots.
therefore the validator requires a type annotation on each call instruction generated by the clr in our experiments .
the validator uses this type and the clr s calling conventions for primitive types structs generics pass byreference parameters and so on to compute which registers and stack locations hold arguments and return values.
we assume that the call s output depends on the state of the heap but not on the state of the caller s stack frame except for by reference arguments from the stack which explicitly appear in the list arg1...argn .
therefore we do not make callmem a function of the whole memory state mem but rather just a function of the heap portion of mem.
the function heapsig strips away the stack portion of memory compressing the heap portion of memory into an integer signature heapsig mem1 equals heapsig mem2 if and only if mem1 and mem2 are identical at all non stack addresses.
heapsig considers mem1 and mem2 equal regardless of the order of stores to mem1 and mem2 allowing the compiler to reorder heap stores without upsetting the validator.
since callmem and callout are uninterpreted the validator has no information about the internal behavior of the called function.
thus if method mk calls method mj while method mk inlines a call to mj the validator will fail to prove mk and mk equivalent the validator sees the uninterpreted functions for mj as di erent from the concrete inlined statements in mj.
to avoid false alarms we currently keep inlining disabled when validating the clr compiler.
while keeping calls uninterpreted is generally sound leading to possible false alarms but not to false negatives in theory it may sometimes be unsound to represent calls to run time system functions as uninterpreted functions.
this is because the run time system may change behavior from version to version so it might not always be the case that calling the same run time function with the same arguments produces the same values.
to be completely sound would require an accurate model of every version of the run time system including the behavior of casts allocation memory barriers lazy static initialization lazy jit compilation etc.. in practice we have written models for a few run time system functions mainly write barriers and byte arithmetic but have otherwise left the run time system uninterpreted.
currently the validator has only limited support for exception handling.
the validator assumes that thrown exceptions exit the method it lacks the controlow edges into the method s exception handlers.
because if this the validation may fail to notice semantic di erences in exception handlers.
section measures the impact of this unsoundness.
.
running symdiff boogie and z3 after encoding each method mk and mk in boogie the validator invokes the symdi symbolic di erencing tool to compare the boogie encodings for semantic equivalence.
symdi combines the encodings of mk and mk into a single block of boogie code that executes mk on some memory mem and register state eax...esp executes mk on an independent memory mem and state eax ...esp and then asserts the nal state of mk and mk is the same ...encoding of mk... ...encoding of mk ... assert heapsig mem heapsig mem assert returnval returnval assert calleesaved calleesaved the assertions say that for mk and mk to be considered equivalent the nal heap state must be the same the return values must be the same and the callee save state must be the same.
the variable calleesaved is a boolean that is true if the method correctly restores callee save registers e.g.
ebx ebp etc.
and returns to the correct return address.
initially we created a separate assertion for each callee save register assert ebx ebx assert ebp ebp etc.
but found that this didn t quite work for our cross architecture experiments because the x86 method and the arm method have di erent sets of callee save registers.
symdi feeds the combined boogie code to the boogie veri cation tool which attempts to prove that the assertions hold.
the boogie tool converts the assertions into a veri cation condition a pure logical formula that encodes both the assertions and the meaning of the statements in mk and mk .
the z3 automated theorem prover attempts to prove that the veri cation conditions are valid.
if this proof succeeds then the validator deems mk and mk equivalent.
for code with loops the veri cation condition generation and proof is not entirely automatic boogie needs program annotations in the form of loop invariants to avoid generating an in nite veri cation condition.
generating loop invariants is undecidable but earlier work by necula describes how symbolic evaluation can be used to build simulation relations that serve as potential loop invariants.
for ease of implementation however the validator currently employs a more expedient solution in the spirit of automated unsound bug nding tools the validator simply eliminates loops by unrolling them n times ignoring any behaviors past the n th iteration n in the experiments in this paper .
while this is certainly unsound because it fails to capture semantic di erences that require more than n iterations to appear we still observe a fairly low rate of false negatives see section as most di erences are observable after only one or two iterations.
after this loop unrolling the encodings of mk and mk are loop free and thus require no loop invariants during veri cation condition generation.
.
root cause analysis if the proof of the veri cation condition fails z3 generates a model a counterexample showing values of variables that make the veri cation condition false.
the symdi tool parses this model and annotates the variables in mk and mk with values from the model.
this annotation provides a human readable trace through each method body helpingto see where the values in the variables of mk di er from the values in the variables of mk .
nevertheless for long methods the trace may contain hundreds of values from the model which may take or minutes for a human to make sense of when analyzing hundreds of false alarms this time adds up.
therefore it s useful to provide an automated mechanism for nding the location in the trace most responsible for the semantic di erence.
this section brie y describes our root cause analysis which attempts to nd the most relevant values highlight them in the trace and provide information useful for grouping related counterexamples into buckets.
our main root cause analysis technique works by following counterexample traces backwards through the data ow graphs of the two methods being compared.
for comparison with other known techniques we also implemented a second analysis based on max sat space constraints preclude a full description of this but section .
summarizes the results.
both analyses work on the boogie code generated from the assembly language code.
as a running example consider the following boogie programs with input i and outputs z1 and z2 x1 f i x2 f i if i if i y1 g1 x1 y2 x2 else else y1 x1 y2 g2 x2 z1 h x1 y1 z2 h x2 y2 this example contains only one meaningful di erence between the two programs the statement y1 g1 x1 differs from y2 g2 x2 because the functions g1 and g2 di er.
in this example we assume that the functions f g1 g2 and h are uninterpreted there are no axioms constraining their outputs so the theorem prover must assume that the outputs of the functions may di er if the function names di er or any inputs to the functions di er.
by contrast functions like and are interpreted the theorem prover knows that x x and that x is the negation of x .
during root cause analysis we treat functions such as load store callmem and callout as uninterpreted.
the key insight of our analysis is that while identifying root causes is di cult in general uninterpreted functions are relatively easy to reason about.
for example the statements z1 h x1 y1 and z2 h x2 y2 both call the same uninterpreted function h. given a counterexample showing that z1 and z2 di er we can safely conclude that di ering inputs to h cause the di ering outputs from h di ering x1 x2 or di ering y1 y2 values cause the di ering z1 and z2 values.
furthermore z3 s counterexample trace provides the values assigned to x1 x2 y1 and y2 for a particular counterexample allowing us to pinpoint the o ending inputs y1 y2 in this case .
thus we reduce the question of why z1 and z2 di er to the question of why y1 and y2 di er.
this insight leads to a straightforward algorithm for programs with only uninterpreted functions if an uninterpreted function output di ers use the counterexample trace to select the di ering inputs and recurse on the uninterpreted function that generates those di ering inputs.
of course real programs have a mixture of interpreted and uninterpreted functions.
for example various interpreted functions if then else produce y1 and y2 y1 i ?
g1 f i f i y2 i ?
f i g2 f i our solution is crude but e ective in practice we collapsemultiple interpreted functions into single uninterpreted functions ignoring the interpreted functions semantics y1 u1 i g1 f i f i y2 u2 i f i g2 f i since the inputs to the interpreted functions may appear in di erent orders e.g.
for commutative operators like we group all inputs together in unordered sets.
the question of why y1 and y2 di er then reduces to the question of why the two sets above di er.
to answer this our algorithm uses heuristics to match the members of the set as well as possible taking into account values from the trace and the uninterpreted function names and input names that appear in the data ow subgraphs for each member.
for example the two f i members are matched based on both their trace value and their common names f and i. after matching i and f i the members g1 f i and g2 f i remain and thus are considered the cause of the di ering y1 and y2 values.
the algorithm then recurses on g1 f i and g2 f i leading to the root cause that g1 di ers from g2.
in cases where all members of the sets match the analysis blames the interpreted functions that were erased when creating the sets although the analysis is unable to pinpoint exactly which interpreted function is to blame .
our algorithm employs additional heuristics such as using mem heap values in the trace to skip over some of the backwards search replacing stack memory accesses with variable accesses and taking into account the semantics of some interpreted functions e.g.
the x86 exclusive or idiom for setting a register to we omit details of these for brevity.
.
bucketing once a root cause is identi ed the algorithm emits a short summary of the root cause e.g.
g1 g2 are mismatched function names and a list of matched uninterpreted functions traversed by the algorithm on the way to the root cause just h in the example above .
a user of the root cause algorithm can then write simple classi ers to group related root causes together into buckets eliminating the need to manually review all the counterexamples in each bucket.
for example mismatched runtime system calls appear as mismatched uninterpreted function names we created a classi er for each such mismatch so that di erent run time system function mismatches are grouped together into different buckets.
as another example we created a classi er to detect when the algorithm traverses a load uninterpreted function whose inputs contain mismatched integer constants or mismatched symbolic constants indicating mismatched eld o sets.
each classi er relies only on the short summary generated by the root cause analysis so new classi ers do not require modifying the root cause analysis.
.
experimental results this section presents results from running the validator on the output of the clr common language runtime compiler including measurements of month to month version di erences optimized vs. unoptimized assembly language di erences di erences across architectures x86 and arm and di erent compilation scenarios jit vs. ahead of time mdil .
this section also presents results from running the validator on some of the same tests but with faults arti cially injected to check for false negatives missed bugs .
the clr just in time compiler compiles managedlanguage bytecode into assembly language for execution inthe clr virtual machine.
other higher level compilers generate bytecode from various managed languages such as c f and managed c .
all assembly language in the measurements was generated by the coreclr subset of the clr which is used to run mobile code silverlight applications and is hence a particularly security critical version of the clr worth extensive testing.
this subset uses the same just in time compiler as the full desktop clr but has a smaller set of libraries and an easier installation process allowing us to easily build and run many versions of the compiler on the same machine.
although the clr compiler runs in just in time mode by default it also supports ahead of time compilation the native image generation or ngen feature which we used to generate the assembly language les for the validator.
one aspect of the clr s jit oriented design made cross version validation slightly di cult though the generated code contains many embedded addresses and eld o sets rather than symbolic names.
these addresses and o sets vary from version to version and even from compilation to compilation leading to many false alarms that merely report mismatched addresses e.g 0x004fe208 !
0x003dd484 .
therefore we modi ed the clr to print symbolic information for addresses and elds.
these modi cations consisted of about lines of code scattered across les.
we also extended the clr to print more information about methods and their argument types so that the validator could determine register and stack usage for each call this modi cation consisted of lines of code added to one le.
these modi cations were shared across all the clr versions in our experiments.
as input to the clr compiler we used test programs from two sources the publically available silverlight libraries bytecode les and the largest bytecode les in the bartok compiler test suite.
.
month to month comparisons comparing just two versions of the compiler would provide only a limited glimpse of how version to version comparison works in practice across long periods of time so we used a diverse set of clr versions in several dimensions first we selected versions from the clr source control server across seven months.
speci cally we chose eight di erent dates each spaced exactly one month apart we refer to those dates as date ... date and the months that separate them as month ... month .
second we selected versions from two di erent source control branches.
clr compiler development takes place across many source control branches including one main branch and many feature branches where independent features are developed.
we chose the two branches with the most check ins to the compiler source les over the seven month time period the main branch and one feature branch.
third we con gured the compilers in x86 generation mode and arm generation mode.
in each con guration we left all optimizations enabled except for inlining as explained in section .
since these are internal versions under development rather than released compilers not all versions could compile all the test les.
some versions did not work at all in the coreclr ngen con guration we omitted these from the re month 1month 2month 3month 4month 5avg missing .
.
.
.
.
.
timeout .
.
.
.
.
.
different .
.
.
.
.
.
equivalent .
.
.
.
.
.
identical .
.
.
.
.
.
method bodiesmain branch x86figure month to month comparisons for feature branch x86 code month 2month 3month 5month 6month 7avg missing .
.
.
.
.
.
timeout .
.
.
.
.
.
different .
.
.
.
.
.
equivalent .
.
.
.
.
.
identical .
.
.
.
.
.
method bodiesmain branch arm figure month to month comparisons for feature branch arm code m m m m m m avg missing .
.
.
.
.
.
.
timeout .
.
.
.
.
.
.
different .
.
.
.
.
.
.
equivalent .
.
.
.
.
.
.
identical .
.
.
.
.
.
.
method bodiesfeature branch x86 figure month to month comparisons for main branch x86 code month 3month 5month 6month 7avg missing .
.
.
.
.
timeout .
.
.
.
.
different .
.
.
.
.
equivalent .
.
.
.
.
identical .
.
.
.
.
method bodiesfeature branch armfigure month to month comparisons for main branch arm code x86 opt vs. unoptarm opt vs. unoptx86 vs. armmdil vs. jit missing .
.
.
.
timeout .
.
.
.
different .
.
.
.
equivalent .
.
.
.
identical .
.
.
.
method bodiesfeature branch x86 arm mdil figure optimized vs. unoptimized comparisons and x86 vs. arm comparisons m month month month avg missing .
.
.
.
.
timeout .
.
.
.
.
different .
.
.
.
.
equiv correct .
.
.
.
.
equiv unsound .
.
.
.
.
method bodiesfeature branch arm w fault injection figure month to month comparisons for feature branch arm code with injected faultssults.
other versions compiled some test programs but not others one date in each branch compiled just over half the of test programs to x86 and another date in each branch compiled only of test programs to arm.
for these we kept the successful test programs and omitted the failing test programs from the results.
other versions compiled most but not all methods in each test program we kept these failed methods in the results but classi ed the methods as missing in the measurements.
in addition the validator itself sometimes failed to process a method usually due to missing address or call information from the clr these methods are also classi ed as missing .
we randomly sampled method bodies from each of the test programs for each of the months for each of the source control branches and for each of the architectures with optimizations enabled for a total of methodbody to method body comparisons less than due to the omissions described above .
figures and show the validation output for these method body pairs.
the di erent category shows the percentage of method bodies categorized by the validator as semantically di erent.
the measured di erence rate for x86 code is .
main branch and .
feature branch while the rate for arm code is .
main branch and .
feature branch for an overall average of .
.
the higher rate for arm may re ect the youth of the arm code generator relative to the x86 code generator.
nevertheless even if the arm code generator experiences more churn than the x86 code generator the x86 validation is still important to ensure that changes to the compiler for arm compilation do not introduce bugs into the x86 compilation.
the equivalent and identical categories show the method body pairs judged equivalent by the validator.
the equivalent method body pairs are methods judged equivalent by symdi .
the identical method body pairs are method bodies with exactly the same instructions i.e.
they are syntactically identical after replacing integer addresses and eld o sets with their corresponding symbols.
for these pairs symdi would always consider the method bodies equivalent so there s no reason to run symdi .
for the x86 code about of methods are syntactically identical.
this is an important reason for the low false alarm rate of version to version testing even a simple syntactic di tool could achieve a false alarm rate of less than for the x86 tests.
nevertheless reducing the false alarm rate to less than requires semantic checking.
furthermore fewer than of arm methods are syntactically identical for these a syntactic di tool would have too many false alarms and semantic checking is essential.
some method body pairs are too large for boogie z3 to handle in a reasonable amount of time.
we set a limit of seconds on the time z3 spends on any one method body pair.
we also set a limit of assembly instructions on any method we pass through boogie due to a boogie stack over ow issue although this instruction limit a ected less than .
of method body pairs.
the timeout category shows the method body pairs that exceeded either of these limits the time out rate ranged from .
we randomly sampled pairs reported as di erent from across figures and and then used the root cause analysis to examine the causes of the di erences.
the most common di erences were embedded addresses some addresses slipped through in spite of our e orts to con vert integer addresses to symbolic names inside the clr lack of aliasing information interprocedural optimizations and bit level arithmetic .
note that the false alarm rate depends on how often the validator is run we chose month to month comparisons but developers might run week to week validations or validations after each check in.
in this case we d expect a lower false alarm rate as fewer changes occur in a week than in a month.
.
optimization level architecture and compilation scenario comparisons figure shows measurements for varying optimization levels architectures and compilation scenarios.
each of these measurements uses the same version of the clr compiler in two di erent con gurations.
the two left measurements taken for the feature branch at date the most recent date common to both x86 and arm compare the clr output with default optimizations except inlining to the clr output with minimum optimizations.
this is effectively a validation of the optimization phases and the results show about a false alarm rate similar to other measurements of false alarms for compiler optimizations and much higher than our .
month to month validation rate.
this suggests that compiler generated code is more similar across time than across optimization levels.
figure also shows validation of x86 code against arm code both with optimizations enabled except for inlining .
the false alarm rate of makes this validation too unwieldy to perform very often but it might still be useful for bootstrapping the validation of code generation for a new architecture relative to an existing well tested code generator for another architecture.
after this bootstrapping monthto month validation over the new architecture can be used.
the fourth measurement in figure shows validation of arm code generated directly by the clr compiler in jit mode vs. arm code generated via machine dependent intermediate language mdil for a set of ten popular phone apps.
the mdil mode is used by windows phone to shift compilation work from the phone to a dedicated server the server s clr compiler compiles phone apps to mdil and the phone compiles the mdil to arm code.
the mdil code contains arm instructions but uses symbolic placeholders for eld accesses method calls and runtime system calls.
this leads to some di erences in the arm code for method calls and run time system calls which account for most of the false alarms in figure .
.
fault injection the month to month comparisons demonstrate a low false alarm rate but this by itself is not enough a tool that always said equivalent would have a low false alarm rate but would not be useful.
we also need to know that when two pieces of assembly language are code semantically different the tool is likely to report them as di erent.
in other words we want few false negatives semantically di erent method bodies reported as equivalent .
as an example we ran the tool on three known incorrect assembly language les generated by past clr versions and the tool reported all three as di erent.
while this was reassuring we also wanted to quantify the false negative rate on a broader set of bugs.
therefore we performed the following fault injection experiment.
we re ran the arm month tomonth tests for the feature branch but for each compari son of method body mk to method body mk we injected a random fault into mk before performing the comparison.
each fault was one of the following single instruction modi cations changed arithmetic opcode changed register changed integer constant randomly ipping one of lower bits changed symbolic o set address changed branch condition changed branch target.
we then hand inspected each method body pair reported as equivalent to see whether it was really equivalent.
to keep the number of hand inspections reasonable we ran the validator on random method bodies per test program rather than .
this resulted in method body comparisons of which were reported as equivalent .
of these hand inspection showed to actually be equivalent marked as equiv correct in figure .
nearly all of these involved modi cations to registers whose values were no longer needed in particular many small methods created a frame pointer but never used the frame pointer faults that corrupted the frame pointer did not change the meaning of the method body.
another interesting fault was changing a conditional jump if a register equaled to a conditional jump if the register was less than or equal to zero using unsigned comparison these conditions are equivalent.
the remaining were genuine false negatives marked as equiv unsound in figure were modi cations to load instructions whose results were ignored.
the compiler emitted these instructions not for their results but for the exception they cause in case of a null pointer access our encoding of loads does not capture this side e ect.
were due to unsound loop handling.
were faults in exception handling code considered unreachable in our encoding.
was due to a byte store to address which overlaps address .
because our memory model does not model word memory operations byteaccurately the validator does not detect that the store to corrupts the contents of .
was due to the validator omitting the implicit type dictionary argument to a generic method call.
.
performance figure shows the time taken to validate each method for the months from figure feature branch arm code .
we chose this particular comparison as a worst casemeasurement it contains the minimum number of syntactically identical method bodies from all the month to month comparisons just and syntactically identical methods take almost no time to validate.
figure shows the total bytecode size of each test le along with the number of assembly language instructions taken from date the most recent of the two compiler outputs in the month comparison and the number of method bodies generated by the clr compiler for the entire test le.
for the validator performance the average instructions per method is correlated with the average time taken to validate each method.
in particular the go test has far larger method bodies instructions per method body than the other tests takes longer to validate seconds on average and contains the majority of theorem prover bytecode size bytes assembly language instructions compiled method bodies instructions per method body time per method body seconds time outs percent bytemark .
crafty .
go .
ijpeg .
sat solver .
xlisp .
microsoft.visualbasic .
system.core .
system .
system.net .
system.runtime.serialization .
system.servicemodel .
system.servicemodel.web .
system.windows.browser .
system.windows .
system.xml .
average .
.
.
figure per test results arm feature branch months time outs from among the tests.
on average each method body pair takes .
seconds to validate on a single core of a .4ghz intel core2 q6600 processor with 6gb of memory.
while not as fast as more specialized approaches to translation validation this is fast enough to process tens of thousands of methods with few time outs.
.
root cause analysis and bucketing we evaluated the e ectiveness of root cause analysis and bucketing on three sets of results the monthly results and the two results from figure with the most false alarms x86 vs. arm and mdil vs. jit .
for each we ran section s data ow graph based root cause analysis on a random sample developed appropriate buckets for the output of the root cause analysis and manually checked the coverage and accuracy of the automated bucketing.
the mdil vs. jit results were best of methods classi ed as di erent were grouped into buckets leaving only of the methods unbucketed.
by contrast the monthly results out of grouped into buckets and x86 vs. arm out of grouped into buckets only bucketed of the di erences.
in all cases no di erences were ever placed incorrectly in a bucket i.e.
failures in the root cause analysis always caused an unbucketed result rather than a misbucketed result .
the di erence in success rates was due to the nature of the root causes the mdil vs. jit causes were dominated by di erences at function call sites which were relatively easy to diagnose and bucket while the other two sets contained more di erences further away from call sites.
even when the reported root cause wasn t precise enough for bucketing the root cause analysis correctly highlighted most of the relevant instructions in the majority of the cases still greatly reducing the human e ort required for diagnosis.
for comparison we evaluated the e ectiveness of maxsat root cause analysis on these tests using z3 s support for computing maximum satis ability.
the max sat analysis uses inputs from a particular counterexample transforms the programs into a system of constraints e.g.
x1 f i z1 h x1 y1 etc.
for the example from section and tries to satisfy as many of the constraints as possible while allowing the program to succeed on the inputs fromthe counterexample.
unsatis ed constraints correspond to possible root causes.
the resulting algorithm has the advantage over the data ow graph based analysis that it retains the semantics of interpreted functions during its search.
in contrast to the data ow graph analysis which tries to pinpoint a single cause max sat tends to return a larger set of possible causes.
this is useful for assisting manual analysis but doesn t lend itself directly to bucketing.
for example naive application of max sat highlights x1 f i as well as y1 g1 x1 in the example from section on the grounds that an unconstrained x1 can contain an arbitrary value that causes g1 x1 to equal g2 x2 for some possible g1 and g2.
to produce more bucketable results we applied max sat in a more constrained way disallowing equality between values unless both values were produced by the same uninterpreted function.
we also forced max sat to report pairs of statements one in each program rather than individual statements.
we then manually compared results from examples randomly sampled from the x86 vs. arm and mdil vs. jit tests.
to prevent max sat from always timing out we also limited the analysis to focus only on causes at function call sites.
overall the resulting algorithm bucketed fewer causes than the data ow graph approach and never bucketed a cause that the data ow graph approach failed to bucket.
however on some unbucketed examples max sat came closer to the real cause suggesting that combination the max sat and data ow graph approaches might lead to better bucketing than either alone.
.
bugs found the clr test team ran the validator on over methods from various test programs revealing bugs in the clr compiler that were previously unknown i.e.
had not been discovered even during extensive testing .
of these were correctness bugs including multiple cases of incorrect runtime system functions being called incorrect calling conventions for return bu ers in some corner cases compatibility issues with oating point rounding due to conversions between byte numbers and byte numbers incorrect treatment of static elds and exceptions.
the other bugs were performance bugs unintended performance regressions .
although we hadn t intended to look for performance issues some di erences that were false alarms for correctness were nevertheless real bugs for performance.
these included calls to slow versions of runtime system functions runtime system calls that were supposed to be inlined and unintentionally disabled common subexpression eliminations.
this suggests that false alarm di erences can yield useful insights so bucketing false alarms may be better than eliminating all false alarms.
.
related work although research on translation validation goes back over a decade few validators checked industrial scale compilers necula s validator targeted gcc peggy targeted llvm and tristan et.
al.
targeted llvm.
these focused on validating optimizations in the compiler s intermediate representation rather than end to end validation from source language to assembly language.
of these only necula s work reports false alarm rates of less than and this seems to be due to reporting rates for just a handful of individual optimizations which have false alarm rates as high as .
rather than a whole optimization pipeline forwhich it s unclear how well necula s approach would work.
peggy reports false alarm rates of for spec benchmarks under llvm while tristan et.
al.
report false alarm rates of .
ramos and engler report a semantic di erence rate of for optimized to unoptimized code comparisions although the semantic equivalence rate was only due to timeouts and tool issues.
when applied to the soot java optimizer rather than to llvm peggy produces false alarm rates of just indicating some promise for translation validation s practicality perhaps given more experience a false alarm rate is also within reach for compilers like gcc llvm and the clr.
on the other hand compared to relatively clean research projects like soot the many complexities of industrial scale compilers tricky optimizations complex language features intricate run time system interactions unexpected special cases may simply overwhelm translation validators.
if so version to version comparison may have an easier job since most compiler complexities become inextricably engrained in the compiler source code and are thus stable over time.
when discussing peggy the authors mention the desire for a ner grained heap model and smt theorem proving technology .
having implemented ner grained heap models for cross version validation and having used the z3 smt solver for cross version validation we can report that these techniques work and enable the handling of complex instructions like rep stosb but that it can take seconds or more per method acceptable for o ine testing via random sampling but not practical as an online compiler pass.
by contrast tristan et.
al.
move in the opposite direction towards more specialized better performing representations with less proof search.
it would be interesting to see the false alarm rate for these specialized representations for cross version validation.
an open question is whether such specialized representations are as easy for humans to diagnose as counterexample traces generated from z3 s models.
yang et.
al.
found a large number of compiler bugs by comparing the output of di erent compilers to each other including gcc and llvm.
rather than using semantic validation techniques they simply executed the assembly language generated by the di erent compilers and looked at the output.
while execution on concrete inputs may have limited coverage compared to semantic equivalence checking yang et.
al.
compensated by randomly generating a large set of test programs to cover a large fraction of the compilers behaviors.
current translation validators are limited to the compiler behaviors exercised by their test suites on the other hand translation validation and cross version validation gives greater assurance about the behavior of the chosen test programs which are often widely deployed programs like silverlight since they consider all possible inputs to the test programs some of the bugs that our tool found would have been very unlikely to occur on random inputs.
.
conclusions our results show that month to month cross version validation achieves a low false alarm rate in practice .
using our validator on the clr when compared to recent translation validation results false alarm rates and to the results of running our validator on unoptimized vs. optimized code false alarm rate .
this indicates that cross version validation can serve as a useful supplement to translation validation and traditional testing.
.