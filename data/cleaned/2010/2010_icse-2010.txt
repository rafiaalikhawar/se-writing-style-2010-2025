adaptive bug isolation piramanayagam arumuga nainar computer sciences department university of wisconsin madison arumuga cs.wisc.eduben liblit computer sciences department university of wisconsin madison liblit cs.wisc.edu abstract statistical debugging uses lightweight instrumentation and statistical models to identify program behaviors that are strongly predictive of failure.
however most software is mostly correct nearly all monitored behaviors are poor predictors of failure.
we propose an adaptive monitoring strategy that mitigates the overhead associated with monitoring poor failure predictors.
we begin by monitoring a small portion of the program then automatically refine instrumentation over time to zero in on bugs.
we formulate this approach as a search on the control dependence graph of the program.
we present and evaluate various heuristics that can be used for this search.
we also discuss the construction of a binary instrumentor for incorporating the feedback loop into post deployment monitoring.
performance measurements show that adaptive bug isolation yields an average performance overhead of for a class of large applications as opposed to for realistic sampling based instrumentation and for complete binary instrumentation.
categories and subject descriptors d. .
software program verification statistical methods d. .
testing and debugging debugging aids distributed debugging monitors tracing i. .
problem solving control methods and search graph and tree search strategies heuristic methods plan execution formation and generation general terms experimentation measurement performance reliability supported in part by afosr grant fa9550 doe contract de sc0002153 llnl contract b580360 and nsf grants ccf ccf and cns .
any opinions findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of nsf or other institutions.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
icse may cape town south africa copyright acm ... .
.keywords statistical debugging binary instrumentation dynamic feedback heuristic search control dependence graphs dyninst .
introduction in an imperfect world with imperfect software debugging does not end the day software is released.
statistical debugging techniques monitor run time behavior to identify causes of crashes in end user executions.
lightweight instrumentation allows non intrusive post deployment monitoring while statistical models identify profiled events that strongly predict crashes or other failures.
yet most programs mostly work nearly all code in any given application is not relevant for any given bug.
broad spectrum instrumentation of many program behaviors while seemingly necessary to catch a wide variety of bugs guarantees that almost all data collected is uninteresting.
in one study fewer than in instrumented behaviors were reported as failure predictive .
over .
of each execution profile was discarded but only after consuming resources cpu time network bandwidth storage space etc.
that could have been better used for other purposes.
the problem with current monitoring systems is that they begin with the worst case assumption that nearly anything could be a clue for a bug and then continue monitoring events even after statistical analyses show that most are not predictive of failure.
contrast this with the focused debugging activity of an expert programmer.
using feedback from a prior execution or even just an initial hunch the programmer uses breakpoints and other probes near points of failure to get more feedback about program behavior.
suspect code is examined more closely while irrelevant code is quickly identified and ignored.
each iteration enriches the programmer s understanding until the reasons for failure are revealed.
we propose to mimic and automate this process on a large scale.
instead of a single run we can collect feedback from thousands or millions of executions of the program by its users.
our technique starts by monitoring a small set of program behaviors.
based on analysis of feedback obtained during this stage our technique automatically chooses other behaviors that could be causing failures and monitors them during the next stage.
throughout this process statistical analysis results are available to the programmer who can fix failures if enough data is available or choose to wait for more data if the picture is unclear.
effectively we replace sampled measurement of all predicates with non sampled measurement of adaptively selected predicates.
non sampled instrumentation allows faster adaptation by quickly gathering sufficient data where it is most needed.
adaptive instrumentation improves upon existing approaches by prioritizing the monitoring of potentially useful behaviors over those that are less useful thereby conserving computational resources and bandwidth for both users and developers.application sizes and bandwidth limits preclude releasing new software each time the adaptive bug hunting system identifies new instrumentation targets.
therefore we use binary instrumentation as the chief mechanism for adaptivity.
adaptation decisions are distributed as a list of predicates that need to be enabled or disabled and a binary instrumentor at the user s machine re instruments the software using this list.
binary instrumentation in itself has several advantages over source level instrumentation.
we can instrument any program not just those written in languages supported by a source level instrumentor.
we can instrument and monitor a program even when its source code is unavailable.
we can also instrument system and third party libraries used by the application.
truly fixing bugs without source code is difficult but remediation may still be possible once the causes of failure are identified .
furthermore binary instrumentation adds only a constant overhead to the size of distributed software the size of one generic binary instrumentor usable for all monitored software in a machine.
this improves on static sampling schemes whose fast and slow path code variants roughly double the size of executables thereby increasing costs for packaging or network distribution.
the remainder of this paper is organized as follows.
section reviews the static non adaptive instrumentation model used in prior work as well as related work in the holmes project.
section describes our binary instrumentor and several optimizations developed to reduce the overhead of monitoring.
section describes our adaptive bug isolation technique and section presents the results of experimental evaluations of this technique.
section discusses related work and section concludes.
.
background before presenting our adaptive approach we review basic concepts and terminology used in previous work on statistical debugging including the recent holmes project that uses coarsegrained adaptivity.
we use a behavioral model based on that of the cooperative bug isolation project cbi of liblit .
.
terminology the decision of what to monitor is critical as later analysis can only find bug clues among the data it is given.
we follow the sitesand predicates approach commonly used in prior work .
an instrumentation site is a single program location at which the state of the running program will be inspected.
instrumentation sites are selected automatically based on syntactic features of the code.
for example one might associate one instrumentation site with each function call or each assignment or each conditional branch.
instrumentation sites may be both incomplete and mutually redundant with respect to possible program behavior they are not a perfect execution trace but rather are a wide net intended to catch useful clues for a broad variety of bugs.
each site is decomposed into a small collection of instrumentation predicates which partition the state space at that site.
at a branch instrumentation site we distinguish between executions that continue along the true versus the false branch.
thus a branch site decomposes into two predicates.
a function return instrumentation site resides in the caller just after the called function returns.
each function return site decomposes the state space into three subspaces corresponding to three predicates depending on whether the returned value is negative zero or positive.
this partition is especially well matched to c programs as the sign of a returned value often indicates success or failure of an operation.
for reasons of privacy and to limit the sizes of execution profiles prior work avoids reporting predicates as a linear stream ofevents.
instead predicates are counted one counter per predicate incremented when that predicate is observed to be true.
because each site s predicates partition the space of possibilities at that site each observation of one instrumentation site increments exactly one predicate counter.
thus the sum of all predicate counters at a site gives the overall coverage of the site.
liblit et al.
argue that one should not even collect complete predicate counts.
instead they offer a sampling scheme based on a static source to source transformation applied at compile time.
this technique derived from that of arnold and ryder creates instrumentation that yields a sparse but fair random subset of the complete counts.
sampling rates of 100to1 are typical.
this helps preserve privacy and also improves performance in some but not all cases .
in exchange static code size approximately doubles and data analysis becomes more difficult to cope with the fact that or more of requested data is missing.
our adaptive approach eschews sampling in favor of complete measurement of a more selective subset of all possible instrumentation.
a feedback report in the sites and predicates model consists of a vector of all predicate counts plus a single outcome label marking this run as good successful or bad failed .
in the simplest case failure can be defined as crashing and success as not crashing.
more refined labeling strategies are easily accommodated as subsequent analysis stages do not care how the success failure distinction was made.
in particular failure analysis does not use stack traces and therefore can be applied to non crashing bugs.
.
holmes theholmes project by chilimbi et al.
makes two orthogonal contributions to statistical debugging.
the first is a new predicate scheme that counts the number of times each path is taken in an acyclic region.
the second is a form of adaptive predicate selection at the granularity of functions.
based on partial feedback data weak predictors are selected and functions close to them in the program dependence graph are chosen.
predicates in these functions are instrumented during the next iteration.
here we propose and evaluate a heuristic search at a much finer granularity.
holmes also strengthens weak predictors by selecting path predicates in functions containing weak branch predicates.
this strengthening is orthogonal to the heuristic searches proposed here or in holmes .
we present a more detailed comparison with holmes in section .
.
binary instrumentation here we discuss the construction of a binary instrumentor that allows incorporating a feedback loop into post deployment monitoring.
we use the dyninst instrumentation framework that allows many optimizations that are difficult and in some cases impossible to achieve in other tools.
this section discusses those optimizations in detail.
unfortunately even with these optimizations overheads are too large for deployment to end users see section .
.
this motivates using an adaptive approach discussed in section to achieve truly lightweight monitoring.
.
static removal of instrumentation branch predicate counts are equivalent to the edge profiles of an execution.
we use static program structure to avoid redundant operations using approaches similar to those of ball and larus or tikir and hollingsworth .
however cbi collects more than just edge profiles.
coverage of any predicate such as a function return predicate implies coverage of the basic block in which that predicate is defined.
this in turn implies coverage of the edges leading to that block from all of its control dependence ancestors.
figure shows an example control flow graph cfg and the7654 0123b1t f 0123b2t f 0123b4 0123b5 0123b6 0123b3 a control flow graph7654 0123b1t f r1 aaaar2 0123b2t f 0123b37654 0123b4 r3 r4 0123b57654 0123b6 b control dependence graph figure example graphs for static removal of branch predicates.
true and false edges of branches are labeled tand f respectively.
corresponding control dependence graph cdg .
the additional ri nodes in the cdg are region nodes that group nodes with identical control conditions.
for example nodes b2andb3execute if and only if the condition at b1is true.
ordinarily a branch instrumentation site at b1would place one predicate counter along the edge from b1tob2and another along the edge from b1tob4.
if there is already any instrumentation site satb2orb3 then the branch predicate count along the b1!b2 must equal the sum of the counts of the predicates at s. thus theb1!b2edge need not be instrumented and its missing edge profile can be computed offline.
in general we define a branch predicate as redundant if its count can be inferred offline by a post mortem analysis of the execution profile.
consider a potential branch predicate palong the edge from some block uto a successor block v. predicate pis redundant if udominates vand at least one instrumentation site sis defined in vor any other basic block which is control equivalent tov.
the first condition ensures that the entire control dependence region corresponding to vis executed if and only if the edge from utovis traversed.
the second condition ensures that every execution of the control dependence region containing vwill be reflected in one of the predicates from sites.
under these conditions instrumentation for branch predicate pcan be omitted.
the count for pcan be derived offline by summing all predicates at site s. .
binarization and dynamic removal while some statistical debugging models use exact values of predicate counts others require only binarized data they consider only whether a predicate was true at least once but make no further distinctions among nonzero counts .
if a binarized model is to be used then predicate counters are merely flags.
instrumentation code can just store a which takes one memory operation instead of incrementing a counter which takes one arithmetic and two memory operations.
furthermore when using binarized data there is no benefit from additional observations of a predicate that has already been observed true once.
therefore a predicate s instrumentation code may be removed from the target once it has triggered .
dynamic instrumentation removal is especially well suited for branch instrumentation as each branch predicate adds code on a distinct edge and therefore each branch predicate can be removed independently.
.
performance impact instrumentation must have extremely low overhead if we are to collect feedback data from members of the general public.
exper iments with a small cpu intensive benchmark show that na ve binary instrumentation does not achieve this goal.
we use the spec .go benchmark compiled with gcc .
.
instrumented using a beta version of the dyninst .
release and run on an otherwise idle dual core .
ghz pentium cpu.
we use one small 2stone9 and two large 5stone21 and 9stone21 benchmark workloads.
all measurements reported are averages across five repeated trials.
execution time excludes instrumentor start up costs and reflects only time spent running the instrumented code.
start up costs can be amortized over several runs using the new binary rewriting feature in dyninst.
we instrument all branches and function returns in the main executable but not in shared libraries.
the unmodified goexecutable completes the small workload in .
seconds and the two large workloads in .
and .
seconds respectively.
na ve dyninst instrumentation slows execution by a factor of .
times for the small workload and .
times for the large workloads.
adding static branch instrumentation removal binarized counts and dynamic branch instrumentation removal increases this relative slowdown to .
for the small workload but shrinks it to .
for the large workloads the benchmark s small code footprint means that dynamic branch instrumentation removal is more beneficial for longer running tasks.
while .
is better than .
this is still too slow.
users will not accept a slowdown in daily use.
we also consider three non dyninst based approaches pin valgrind andsampler cc .
pin and valgrind are dynamic binary instrumentors that use just in time jit disassembly and recompilation as contrasted with dyninst s code patching approach.
our custom pin instrumentor built using pin version .
has slowdowns between .
and .
.
our custom valgrind instrumentor built using valgrind version .
.
performs similarly to lightly optimized dyninst with slowdowns between .
and .
.
however many of the more aggressive optimization strategies would not be practical to apply under a jit execution model.
moreover jit code patching imposes a baseline overhead to load and execute instructions even when no instrumentation is performed.
this limit cannot be improved with any static or dynamic optimization.
for pin this overhead comes to about .
which is higher than highly optimized dyninst instrumentation.
for this reason we evaluate our adaptive techniques in the next section using dyninst.
sampler cc is the cbi instrumenting compiler developed by liblit et al.
.
sampler cc instrumentation is highly optimized but is completely fixed at compile time.
we ran the instrumented benchmark using the sparsest possible sampling rate which should give the best performance and found relative slowdowns of .
across all workloads.
while sampler cc s static approach is the fastest considered here we are not willing to give up the benefits of dynamic instrumentation while simultaneously imposing a slowdown on end users.
the adaptive techniques described in the next section dramatically reduce instrumentation overheads while simultaneously avoiding the drawbacks of static instrumentation.
.
adaptive instrumentation abug predictor is any instrumented predicate which is predictive of failure.
numerous statistical debugging techniques have been proposed to find the tiny fraction of predicates that are good bug predictors .
all assume that instrumentation sites are selected once remain fixed thereafter.
the adaptive approach detailed in this section eliminates fixed monitoring plans.
instead sites are speculatively added to the instrumentation plan if it appears that they may be good bug predictors and are removed from the monitoring plan once their bug predictive ability or inability has been assessed.
our algorithm exploits the principle of locality if a predicate is highly predictive of failure then predicates in itsprocedure pseudo code for adaptive analysis monitored explored plan getinitialset while debugging do instrument and monitor sites in plan waitforsufficientdata monitored monitored plan best branch predicate with highest score in monitorednexplored explored explored fbestg plan vicinity best nmonitored end while vicinity are potentially good bug predictors as well.
the essence of our technique is to adaptively adjust the instrumentation plan by locating a predicate that is highly predictive of failure and extending the plan to include nearby sites.
procedure defines this iterative algorithm.
it is parameterized by four sub procedures getinitialset waitforsufficientdata score andvicinity that are described more fully later.
monitored is the set of sites for which feedback information is available from previous iterations.
explored is the set of branch predicates that have received the highest score in the previous iteration these are predicates whose nearby vicinity has already been explored.
plan is the set of sites that are being monitored during the current iteration.
bestis the branch predicate which receives the highest score in the current iteration.
at startup the analysis chooses the set of sites to be monitored by calling getinitialset .
the set of sites is monitored until the function waitforsufficientdata returns indicating that enough feedback has been collected for meaningful analysis to be applied.
using this feedback and feedback from earlier phases if any the best branch predicate which was not already explored is identified.
the plan for the next iteration is to monitor previously unmonitored sites in the vicinity of this best predicate as defined by the function vicinity .
the sets monitored andexplored are updated during each phase.
our choice of algorithms for the sub procedures getinitialset andvicinity determines whether the analysis in procedure is a forward section .
or backward section .
analysis.
the score function assigns numeric values to every predicate a higher value is assigned to a predicate that is a better bug predictor.
we describe the scoring functions that we explored in this paper in section .
and experimentally evaluate them in section .
some scoring heuristics developed for non adaptive instrumentation are designed to compute the inherent bug predictivity of a predicate and prevent the bugginess of nearby predicates from skewing the measure for this score.
such metrics are less suitable for our adaptive algorithm as they subvert the locality principle on which it relies.
waitforsufficientdata assesses whether sufficient data has been collected.
while not the main focus of this work we briefly discuss this issue in section .
.
section .
mentions some alternative design choices.
.
forward analysis of the program the general pattern in forward adaptive bug isolation is to start at the beginning of the program and iteratively work forward toward the root causes of bugs.
consider the control flow graph in figure 1a.
suppose the branch predicate associated with the true result of the condition at b1is found as the best predicate according to the score function.
this means that whenever the edge b1!b2is traversed the program is likely to fail.
this indicates that there might be some bug in the basic blocks b2 b3 b5andb6and predicates in these blocks may be even better at predicting the bug.
however we do nothave enough evidence to believe that b5andb6have good failure predictors.
it could be the case that the bug is in b5and hence none of the predictors in b6predict failure.
it is also possible that the bug is in b6and the predicates in b5are not relevant.
which of the two cases is true will be known when we have information about the branch site at b2.
we can defer monitoring sites in b5andb6until we have that information.
on the other hand we have enough reason to believe that good predictors will be found in b2andb3because the collected data shows that these blocks are executed in many failed runs.
in general the choice of b2andb3translates to choosing the children in the control dependence graph.
thus if the bestpredicate is associated with the branch at basic block bbeing true respectively false then we are interested in the basic blocks that are control dependent on bwith the true respectively false control condition.
therefore vicinity best returns the sites in these basic blocks.
function calls are handled automatically by using an interprocedural cdg.
to fit with the notion of searching forward in the cdg getinitialset returns the sites in basic blocks control dependent on the entry node of the cdg.
.
backward analysis of the program if a program fails by crashing then a stack trace of the program when it crashed may be available.
based on the folk wisdom that the bug is likely to be somewhere near the point of the crash exploring the predicates near the crash point may find good bug predictors faster than a forward analysis.
to illustrate backward analysis once again consider the cfg in figure 1a.
suppose the program crashes in block b3.
now consider the branch site at b1 which is b3 s control ancestor.
this site is the last point where execution of b3could have been skipped and hence the crash averted.
so b1is a good candidate bug predictor and we monitor it and measure its score.
suppose the programmer looks at the new feedback and has no idea why a predicate at b1could be causing the crash.
there are two possible reasons case i the bug may actually be in basic block b2 b5 orb6.
the predicate at b1may have a high score simply because it governs execution of these blocks.
case ii the branch predicate at b1may have a high score because the problem happens before the program reaches b1.
thus the program will fail irrespective of the outcome of this branch.
in case i predicates in b2 b5andb6are potential bug predictors.
using the same reasoning used during forward analysis we only monitor b2and delay monitoring of b5andb6until there is information about the branch predicates in b2.
in case ii we could explore further backwards in the cdg by considering b1 s control ancestor assuming that figure 1a shows just a fragment of a larger program .
since there is no way to decide whether the root cause is before the execution of b1or after it we take a conservative approach and include sites suggested by both cases i and ii in the monitoring plan for the next iteration.
to summarize for backward analysis getinitialset returns the branch site in the control ancestor of each basic block in which a crash occurs.
vicinity best returns sites in the control ancestors and control descendants having the appropriate control condition of the basic block in which the predicate bestis defined.
.
scoring heuristics in this section we consider possible definitions for score as used in procedure .
all possibilities considered are heuristics in thatone could contrive situations in which they perform badly.
our goal is to identify scoring heuristics that perform well on a variety of programs in realistic situations.
the data collection model of liblit et al.
aggregates the data collected for a predicate pinto four values s p andf p are respectively the number of successful and failed runs in which pwas observed to be true at least once.
s pobs andf pobs are respectively the number of successful and failed runs in which pwas observed at least once regardless of whether it was true or not.
the later two values correspond to the coverage of the site containing pand can be computed offline by examining the counts of all predicates including p at that site.
besides these four values there is another global value numf the total number of runs that were labeled as failures.
all heuristics described in this section are computed using these values.
failure counts the first heuristic scores a predicate paccording to the number of failing runs in which pwas observed to be true failcount p f p .
any region of code that is executed during many failed runs is potentially buggy.
failcount p may not be a good measure of bug predictability because it does not distinguish between two predicates which are true in different numbers of successes but same number of failures.
however a predicate seen in many successful and failing runs may capture some property of the program other than its outcome such as differing usage scenarios .
importance liblit et al.
argue that a good bug predictor should be true in few successful runs and should also have a significant effect on the outcome on the program.
this is captured by theincrease metric that measures how much more likely failure is specifically when pis true versus simply reaching the site containing pat all.
formally increase p f p s p f p f pobs s pobs f pobs the failure count at pis not completely discarded.
in information retrieval terms failcount p measures sensitivity orrecall while increase p measures specificity orprecision .
liblit et al.
balance the two by scoring candidate predicates by the harmonic mean of normalized f p andincrease p as follows importance p increase p log f p log numf liblit et al.
found importance p as a good measure of failure predictivity.
here we consider it as a candidate adaptive scoring heuristic.
maximum importance during initial experiments we found thatimportance as a scoring heuristic often leads to sub optimal adaptation decisions.
for example consider a branch condition that is always true and the associated branch predicate p. since the branch is always taken f pobs f p ands pobs s p .
thus increase p and consequently importance p are both .
even iff p is very large a branch predicate p0with marginally positive values for increase p0 but very low f p0 will be given preference over p. this is not a problem with the importance heuristic because as mentioned earlier there can be situations where the heuristics make choices that go against the principle of locality.
if the iterative ranking and elimination algorithm of liblit et al.
is used where the goal is to find predicates with high importance scores we can construct a heuristic that maximizes the importance score of predicates in vicinity p rather than the importance ofpitself.
predicates in vicinity p have not already been monitored so we cannot predict the exact maximum score among predicates in that set.
instead we compute an upper bound by considering a hypotheticalpredicate hthat has the best possible score.
such a predicate must be true in all the failing runs in which it is observed and false in all the successful runs in which it is observed i.e.
f h f hobs and s h .
when a forward analysis is used hwill appear in a basic block that is control dependent on the edge associated with pand hence hwill be observed only when pis true so f hobs f p ands hobs s p .
thus increase h f h s h f h f hobs s hobs f hobs f p s p f p we set maximportance p importance h to favor predicates that have the potential to reveal new predicates with high importance .
student s t test the next heuristic ttest p uses a statistical test called the student s t test .
given two samples this test uses the mean standard deviation and the size of the two samples to assign a numeric confidence in the range to the hypothesis that the means of the distributions underlying the two samples differ.
we can apply the t test on the two sample sets we have about a predicate p the observations of pin successful and failing runs.
letcbe the confidence assigned by the t test for the hypothesis that the truth value of a predicate pdiffers significantly between successful and failing runs.
during a forward analysis if pis seen in a larger percentage of failures than successes then the predicates invicinity p are also observed in a larger percentage of failures than successes and the heuristic should give preference to p. on the other hand if pis seen in a larger percentage of successes than failures then the predicates in vicinity p should have lower preference.
ttest p computes the t test confidence metric cand assigns a score of ctopin the former case and a score of ctop in the latter case.
since the information about pdoes not impose any useful restrictions on the coverage of the ancestor of the node associated with pin the cdg both maximportance andttest do not have a sensible interpretation for backward analysis.
student s t test is a parametric test and assumes that the samples are normally distributed.
we also evaluated a non parametric test the mann whitney u test which is less powerful but does not assume normal distribution.
the u test always performed worse or only as good as the t test indicating that the normality assumption is acceptable.
therefore we give the u test no further consideration.
other heuristics to evaluate the usefulness of our techniques we also define three other heuristics.
to assess whether search heuristics are useful at all we consider bfs a na ve breadth first search on the cdg.
random is a strawman heuristic that selects a best predicate at random on each iteration.
lastly an oracle heuristic helps to measure how well any search heuristic could possibly do we discuss oracle further in section .
.
.
waiting for sufficient data most statistical analyses have the ability to associate a confidence value with their output.
a typical confidence measure would be a probability between and that an observed trend is genuine rather than merely coincidental.
given such information we can wait until the analysis is able to compute its output with sufficiently high confidence for example with probability greater than .
.
in general this decision involves a compromise between the speed and accuracy of debugging.
collecting more reports improves accuracy but may take longer to produce interesting results while collecting fewer reports has opposite trade offs.
moreover user behavior might change across iterations and more reports will be needed to accomadate such instability.
as mentioned earlier this issue is not the main focus of this paper.
section explains a simple approachtable programs used for experimental evaluation program variants loc sites test cases bash bc ccrypt exif flex gcc grep gzip siemens to to to to space that we use for our experimental evaluation.
note that the definitions of getinitialset andvicinity as given in either section .
or section .
have an important completeness property.
with either approach starting with getinitialset and repeatedly expanding the search using vicinity will eventually instrument every site that is reachable from the program s entry point.
this property is important because the principle of locality is not a guarantee a good predicate may appear in code where other predicates have low scores.
the completeness property ensures that adaptive bug isolation can recover from wrong turns.
in the worst case it will still provide the same information as exhaustive non adaptive instrumentation it may just take longer to get there.
.
design alternatives as proposed here adaptive instrumentation is a heuristic search through the control dependence graph and the principle of locality is assumed to apply to predicates that are close in this graph.
however the effects of bad code can also propagate through data rather than through control flow.
thus it may be desirable to consider data dependence relations as well.
this can be done by using theprogram dependence graph pdg instead of the controldependence graph.
balakrishnan et al.
have demonstrated pdg construction for unannotated binaries but the infrastructure to do this is not yet generally available.
instead of constructing an interprocedural cdg one could initially treat calls as opaque and only instrument callee bodies if the results they return are highly predictive of failure.
however this incorrectly assumes that called functions are pure with no effects other than the values they return.
this clearly is not true for c. thus a completely modular function by function search is not appropriate in the general case.
procedure describes an automated search which can proceed without human intervention.
but the general framework is flexible and can be manually overridden if and when needed.
for example the programmer can override getinitialset to directly debug modules that are known to be buggy from prior experience or inhouse testing.
similarly an experienced programmer can look at already monitored sites and specify her own plan based on domain knowledge whether to test a hypothesis or simply chase down a hunch.
the set of monitored sites can be actively modified to balance aggressive exploration against user tolerable overhead.
.
ev aluation prior work has shown qualitatively and quantitatively that statistical debugging is effective.
in this section we evaluate the main contribution of this paper which is the use of adaptive binary instrumentation to further reduce performance overheads.
our evaluation uses the faulty programs of the siemens suite the bash flex grep gzip andspace bug benchmarks and gcc .
.
.
we also evaluated bc1.
ccrypt .
andexif .
.
each of which has known fatal bugs .
some test subjects have multiple variants each exhibiting a different bug.
table lists our test programs the number of variants size in lines of code and number of instrumentation sites and the size of the test suite used.
all test programs except siemens programs are realistic applications with several thousand lines of code kloc .
bash and gcc are the largest with greater than kloc.
bugs in some test subjects cause incorrect output rather than crashes.
a test case is labeled as a success or failure by comparing the output of the buggy program to that of a bug free reference version.
our statistical methods are applicable irrespective of the labeling strategy used.
for the purposes of backward analysis the failure point is defined as the statement in the program that prints the first incorrect byte in the output.
we find this location by tracing program output and associating each output byte with the code that printed it .
since this output tracer does not support bash we do not perform backward analysis for bash .
theimportance score is undefined if a variant fails in zero or one test case.
such variants are discarded and not included in table .
for the remainder we run the adaptive analysis given in procedure .
to mimic a real deployment in which no two runs are exactly alike we partition tests into random subsets of cases each and cycle through these for successive iterations.
when using the random heuristic all measures are averages across five trials.
.
comparison of heuristics the intent of each heuristic is to guide adaptive analysis toward high scoring predicates.
to evaluate effectiveness figure shows the score of the top scoring predicate found versus the total number of sites monitored so far.
note that vicinity best in procedure may return multiple sites.
when this occurs flat horizontal segments appear in the plots of figure reflecting a larger than unit jump in the number of sites monitored.
this is particularly common with bfs which can fan out quickly in each iteration.
in the plot for space in figure 2a all heuristics start by finding essentially the same top score before diverging after sites.
the divergence at sites may look small in the plot but it is significant considering the large range covered by the xaxis.
ttest performs best followed by importance .failcount lagsimportance in the early phases because it cannot distinguish predicates seen only in failing runs from those seen always.
the pathological cases for importance that motivated the design of maximportance appear to be uncommon in space as indicated by the fairly poor performance ofmaximportance .
wide fan out gives bfs a stepped profile and leads it to instrument many uninteresting sites on its way to a good predicate.
all of these outperform the random straw man.
the heuristics behave similarly on all other applications except gzip andflex bfs andrandom are significantly worse than the other heuristics ttest is the best or close to the best the others are close tottest for some cases and close to bfs in the rest.
for the large applications bash andgcc failcount is the best heuristic but only slightly head of ttest .
we omit the curves for these applications in interest of space.
however the plot for gzip in figure 2b deviates from this pattern with bfs and other heuristics significantly outperforming ttest .
manual inspection shows that the top predictors are near the top of the cdg usually two or three levels deep .
thus bfs andrandom are better at finding them early while ttest gets sidetracked by an initial wrong choice.
these are ultimately heuristics and therefore can occasionally perform sub optimally.
flex exhibits behavior similar to gzip .
figure plots the score of the top scoring predicate found versus the total number of sites monitored so far for space using the0 number of sites monitored00.
.
.
.
.
.
.7score of top predicatefailcount importance maximportance ttest bfs random a space number of sites monitored00.
.
.
.
.5score of top predicatefailcount importance maximportance ttest bfs random b gzip figure adaptation speed for various heuristics using forward analysis importance heuristic with backward analysis and using the ttest heuristic with forward analysis.
backward ttest is meaningless.
backward analysis begins finding good predictors much earlier than forward analysis.
after about sites exploration no longer significantly improves scores.
this affirms folk wisdom that many bugs are close to their points of failure.
the rise in the backwards curve near sites suggests that the principle of locality does not always apply high scoring predicates occasionally appear in the same locality as low scoring predicates.
backward analysis has the same behavior in other applications so we omit their plots in interest of space.
for bcandexif which have crashing bugs the top predictor is very close to the point of failure and is found rapidly by backward analysis after exploring fewer than sites.
.
instrumentation selectivity section noted that prior approaches discard over .
of instrumented predicates.
to gauge our improvement against this baseline we measure the total instrumentation effort required for the adaptive process to discover the same top ranked bug predictor as a traditional non adaptive method.
we choose the analysis of liblit et al.
as our non adaptive reference.
for each variant we note number of sites monitored00.
.
.
.
.
.
.7score of top predicate forward ttest backward importancefigure adaptation speed for space using backward analysis with importance and forward analysis with ttest bash bc ccrypt exif flex gcc grep gzip siemens space overall application020406080100percentage of sites exploredbfs random ttest oracle figure mean number of sites to find top ranked predictor.
the rightmost set of bars averages across all programs.
the top bug predictor as identified by liblit et al.
then count sites explored before an adaptive analysis identifies this same predicate.
we compare ttest with bfs andrandom to test whether a carefullyselected heuristic can help in this task.
we also consider an oracle heuristic that has perfect knowledge of program behavior.
it always selects the branch predicate that reaches the target predicate by monitoring the fewest instrumentation sites.
figure plots the percentage of sites explored for each program averaged across all variants.
bfs andrandom explore about of sites before finding the top predicate.
ttest explores just .
oracle suggests that there is room for improvement but also establishes a lower bound of about for any adaptive search that crosses cdg edges one at a time.
adaptive analysis performs very well in bash bc ccrypt exif and gcc finding the top predictor while instrumenting less than of sites on average.
table shows the mean number of sites instrumented during an iteration and the mean number of iterations required to find the top bug predictor.
bfs s wide fan out reveals the top predictor in fewer iterations but instruments many sites.
other forward heuristicstable mean number of sites instrumented per iteration and mean number of iterations mean number of iterations to find top ranked predictor mean number of sites instrumented per iteration program failcount importance maximp ttest random bfs bwimp failcount importance maximp ttest random bfs bwimp bash .
.
.
.
.
.
.
.
.
.
.
.
bc .
.
.
.
.
.
.
.
.
.
.
.
.
.
ccrypt .
.
.
.
.
.
.
.
.
.
.
.
.
.
exif .
.
.
.
.
.
.
.
.
.
.
.
.
.
flex .
.
.
.
.
.
.
.
.
.
.
.
.
.
gcc .
.
.
.
.
.
.
.
.
.
.
.
.
.
grep .
.
.
.
.
.
.
.
.
.
.
.
.
.
gzip .
.
.
.
.
.
.
.
.
.
.
.
.
.
siemens .
.
.
.
.
.
.
.
.
.
.
.
.
.
space .
.
.
.
.
.
.
.
.
.
.
.
.
.
table relative performance overheads sampling adaptive program binary ttest bfs bash .
.
.
.
.
.
bc .
.
.
.
.
.
gcc .
.
.
.
.
.
gzip .
.
.
.
.
.
exif .
.
.
.
.
.
overall .
.
.
.
.
.
instrument roughly the same number of sites per iteration but differ in the number of iterations required.
flex has a relatively flat cdg due to a large number of switch statements in its input scanner.
this causes significantly many sites to be instrumented per iteration.
the best predictor for bcis very close to the point of failure and hence backward analysis completes quickly.
the number of iterations required is in the order of tens for siemens programs and in the order of hundreds for large applications which is not large for wide deployments that generate many feedback reports.
.
performance impact table shows that very few sites are instrumented at any time.
we might then expect lower overheads.
we test this hypothesis by measuring the mean overhead for executing the monitoring plans suggested by the ttest andbfs heuristics.
table shows measured overheads relative to for non instrumented code.
we evaluate performance for bash bcandexif whose non instrumented programs run for approximately .
and .
seconds respectively.
the test suites for other programs being functionality tests rather than performance tests execute for extremely short periods order of a few milliseconds and their performance cannot be reliably measured.
for gcc andgzip we evaluate performance using inputs in the spec benchmark suite.
non instrumented programs take between .
and seconds on these inputs.
we compare our technique against complete binary instrumentation and the sampling scheme of liblit et al.
.
we experiment with sampling rates of the best case for sampling based instrumentation suggested by liblit et al.
for public deployments and .
adaptive instrumentation is at least an order of magnitude faster than complete binary instrumentation and significantly faster than all sampling variants.
bfs due to its wide fan out has a higher overhead than ttest .
the overhead is minuscule for bc gcc and gzip where long running times amortize instrumentation costs.
overheads for bash andexif are easily affected by measurement noise as even a .
second offset could change the overhead by at least .
if these short lived programs are excluded the average overhead for the remaining programs is for complete binaryinstrumentation for sampling at a rate of 100and for adaptive instrumentation.
an overhead of is effectively imperceptible to an end user.
selective instrumentation if applied by a static instrumentor can achieve order of magnitude improvements over sampling.
but enacting new plans would require distributing executable patches an impractically resource intensive proposition.
.
multiple bugs confronted with multiple bugs adaptive analysis might focus exclusively on the most frequent bug or it might split its attention between several different equally prevalent bugs.
neither is desirable.
infrequent bugs are ignored in the first case while it takes longer to find good predictors for any of the bugs in the second case.
we explore this issue using exif which contains three known bugs .
one bug was exhibited only once in our test suite of runs and hence is ignored.
the other two caused and runs to fail.
our technique finds the top bug predictor for the first bug in about iterations after instrumenting just of predicates.
the best predictor for the second bug is not found until iteration after instrumenting predicates.
manual inspection shows that many sites instrumented between iterations and relate to the first bug.
this affirms that adaptive instrumentation can stall in the presence of multiple bugs.
we propose a solution similar in spirit to that of liblit et al.
failures due to other bugs can be thrown away while pursuing a fix for a particular bug.
multiple instances of procedure each pursuing one bug can be active simultaneously.
deployed programs can be randomly split to collect feedback data for these multiple instances.
this requires grouping failing runs by cause.
for crashes one may use crash stacks or just the crashing program counter to label failures.
we label each failing run for exif based on the failing pc and run two separate instances of procedure .
the best bug predictor for the first bug is found in iterations and instrumentation sites.
the second bug is caught after iterations and sites.
thus while procedure by itself is not designed for multiple bugs it can be easily modified to handle them.
.
comparison with holmes as noted earlier holmes introduced two orthogonal concepts path based instrumentation and adaptive predicate selection.
we focus our comparison only on the later.
we do not consider path predicates in this section for two reasons.
first our binary and source instrumentors do not yet support the selective path profiling used to efficiently implement path predicates.
second holmes s use of path predicates is orthogonal to adaptivity and could be added to our system in the same manner.
in effect holmes defines vicinity at the granularity of entire functions.
holmes finds weak predictors instrumented in earlier iterations defined as predicates with importance scores between 5and0 .
it selects functions close to these predictors in the pdg and instruments all predicates in these neighboring functions in the next iteration.
weak predictors can be quite sparse.
because holmes explores only near weak predictors this creates a risk that it can get stuck with no new sites available to explore.
in our experiments this was the case in of the siemens experiments and of the larger experiments.
in of the experiments in which holmes finds the top predictor the sparsity of weak predictors is side stepped because the top predictor is so close to the point of failure that it is instrumented in the very first iteration.
while the definition of weak predictors seems to be the impediment here if we remove that restriction and define vicinity to explore near all predictors then holmes reduces to a breadth first search on the call graph which is a coarser version of the bfs heuristic evaluated earlier.
our approach cannot get stuck as noted in section .
.
exploring at the granularity of functions is even coarser than bfs.
hence more sites will be instrumented per iteration imposing more overhead but requiring fewer iterations.
our approach is more flexible allowing trade offs between overhead and the number of iterations.
we do not present a direct performance comparison because holmes performs only trivial single iteration explorations of all programs in table .
.
related work renieris and reiss present a model for debugging where the programmer does a breadth first search on the program dependence graph.
this model has been used in subsequent studies to quantify the effectiveness of bug predictors.
in this paper we adopt a similar model where we use a heuristic search instead of breadth first search.
it should be noted that in earlier papers the model was used as an independent metric to compare different analyses.
here we use it for a different goal to reduce the monitoring overhead.
also whereas prior search models operate on program dependence graphs we search across control dependence edges only.
this is due to the difficulty of extracting precise datadependence edges from binary code although work by balakrishnan et al.
may permit adaptive pdg exploration in the future.
in their execution classification tool haran et al.
select program behaviors to be monitored using weighted sampling.
like cbi sampling is used to reduce monitoring overhead.
in the presence of a large user community weighted sampling can be combined with our technique for a non uniform assignment of instrumentation sites to users using any of our heuristics as weights.
several other dynamic program analysis tools alter their behavior adaptively.
the dynamic leak detector of hauswirth and chilimbi profiles code segments at a rate inversely proportional to their execution frequencies.
yu et al.
use dynamic feedback to control the granularity of locksets and threadsets in their data race detection algorithm.
dwyer et al.
make adaptive online decisions to monitor just a subset of the program events in their dynamic finite state property verifier.
the ajaxscope platform for monitoring client side execution of web applications provides mechanisms for specifying adaptive policies the authors describe a performance profiling tool using this feature.
the paradyn project uses adaptive dynamic instrumentation for performance profiling of large parallel programs.
roth and miller emphasize automated on line diagnosis of performance bottlenecks.
unlike paradyn and the other online adaptive analyses our approach uses statistical bug detection with data being aggregated across many runs and analysis being performed offline.
paradyn s tools and techniques for managing and visualizing large data streams may be useful in our domain as well.thegamma project represents one of the first practical systems for run time monitoring of deployed software.
orso et al.
describe a data collection infrastructure termed software tomography that supports a variety of software evolution tasks and which allows post deployment changes to data collection.
however while gamma automates the distribution of data collection tasks among a user community selection of those tasks is assumed to be humandirected.
we propose an automatic heuristically guided system for bug hunting that changes data collection tasks in response to feedback.
as with paradyn visualization techniques developed by thegamma group may prove useful to help programmers understand and interpret data collected using our adaptive approach.
.
conclusions post deployment bug hunting is a search for a needle in a haystack.
monitoring strategies that cannot respond to feedback incur large overheads and waste considerable computational resources.
we use statistical analysis static program structure and binary instrumentation to develop an adaptive post deployment monitoring system.
of several search heuristics considered one ttest consistently performs well in the forward direction while another importance shows promise when working backward from known points of failure.
we find that this technique achieves the same results as an existing statistical method while monitoring on average just of potential instrumentation sites in the programs we considered.
performance measurements show that our technique imposes an average performance overhead of for a class of large applications as opposed to for realistic sampling based instrumentation.
monitoring overheads are so small as to be nearly immeasurable making our adaptive approach practical for wide deployment.
.