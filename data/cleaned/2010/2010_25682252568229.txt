characterizing and detecting performance bugs for smartphone applications yepang liu dept.
of comp.
sci.
and engr.
the hong kong univ.
of sci.
and tech.
hong kong china andrewust cse.ust.hk chang xu state key lab for novel soft.
tech.
dept.
of comp.
sci.
and tech.
nanjing university nanjing china changxu nju.edu.cn shing chi cheung dept.
of comp.
sci.
and engr.
the hong kong univ.
of s ci.
and tech.
hong kong china scc cse.ust.hk abstract smartphone applications performance has a vital impact on user experience .
however many smartphone applications suffer from bugs that cause significant performance degradation thereby lo sing their competitive edge .
unfortunately people have little u nderstanding of these performance bugs .
they also lack effective techniques to fight with such bugs .
to bridge this gap we co nducted a study of real world performance bugs collected from eight large scale and popular android applications.
we studied the characteristics e.g.
bug types and how they manifest ed of these bugs and identified their common patterns.
these findings can support follow up research on performance bug avoidance testing debugging and analysis for smartphone applications .
to demonstrate the usefulness of our findings we implement ed a static code analyzer perfchecker to detect our identified performance bug patterns .
we experimentally evaluated perfchecker by applying i t to popular android applications which comprise .
million lines of java code .
perfchecker successfully detected matching instances of our performance bug patterns.
among them were quickly confirmed by developers as previously unknown issues that affect application performance and were fixed soon afterwards by following our optimization suggestions .
categories and subject descriptors d. .
testing and debugging general terms experimentation measurement performanc e. keywords empirical study performance bug testing static analysis .
.
introduction the s martphone application market is expanding rapidly.
until july the one million andro id applications i n the google play store received billion downloads .
with more emer ging applications of similar functionalities e.g.
various web browsers perfo rmance and user experience has gradually become a dominant facto r that affects user loyalty in application selection .
however our inspection of android applications rando mly sampled from google play store using a web crawler re vealed an alarming fact of them have suffered or are suffering from performance bugs of varying severity as judged from their release logs or user reviews .
these bugs can significa ntly slow down application s or cause them to consume excessive resources e.g .
memory or battery power .
the pervasiveness of such performance bugs is attributable to two major reasons.
first smartphones are resourc e constrained as compared to pcs but their applications often have to conduct non trivial tasks like web browsing and graphics rendering .
thus poorly implemented a pplications can easily exhibit unsatisf actory performance.
second many smartphone applications are developed by small teams without dedicated quality assurance.
these developers lack viable techniques to help analyze the performance of their applications .
as such it is hard for them to exercis e due diligence in a ssuring application performance especially when they have to push application s to market in a short time .
existing studies have fo cused on performance bugs in pc or ser ver side applications and proposed several interesting testing and analysis techniques .
yet smartphone platforms are relatively new and we have limited understanding of their application s performance bugs .
whether existing techniques are applicable to smartphone applications is an open question.
ther efore in this paper we aim to bridge this gap by conducting an empirical study .
this study focuses on performance bugs from real world smartphone applications .
we restrict our scope to android applications due to their popularity and code availability .
our study covered real world performance bugs collected from eight large scale and popular android applications e.g.
firefox across five different categories.
the study aims to answer the following four research questions rq1 bug types and impact s what are common types of performance bugs in android applications?
what impacts do they have on user experience?
rq2 bug manifestation how do performance bugs man ifest themselves ?
does their manifestation need special inputs?
rq3 debugging and bug fixing effort are performance bugs more difficult to debug and fix than non performance bugs?
what information or tools can help with this?
corresponding author.
research questions rq1 .bug types and impacts .bug manifestation .debugging and fixing effort .common bug patternscombating performance bugs bug avoidance techniques rq2 rq4 effective testing methodologies rq1 rq2 debugging assistance tools rq3 bug detectors rq1 rq4 figure .
potential benefits of our empirical findings permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists r equires prior specific permission and or a fee.
icse may june hyderabad india copyright acm ... .
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
icse may june hyderabad india copyright acm ... .
rq4 common bug patterns are there common causes of performance bugs?
can we d istill common bug patterns to facilitate performance analysis and bug detection?
we studied relate d work and formulated the above research questions .
through answering them we aim to better u nderstand characteristics of performance bugs in smartphone appl ication s. for example we found that sma rtphone applications are more susceptible to performance bugs than pc or server side a pplications .
besides m any smartphone performance bugs do not need sophisticated data inputs e.g.
a database with hundreds of entries to manifest but instead the ir manifestation need s special user interactions e.g.
certain user interaction sequences .
some of our findings differ largely from those for perfor mance bugs in pc or server side applications as we will explain later.
besides facilitating bug understanding our findings can also support follow up research on performance bug avoidance testing debug ging and detection for smartphone applications as illustra ted in figure .
for instance with our identifie d common bug pa tterns one can propose guidelines for avoiding certain perfo rmance bugs in application development.
one can also design and implement bug detection t ools for identifying performance opt imization opportunities in application testing and maintenance.
to evaluate the usefulness of our empirical findings we further conducted a case study with large scale and popular android application s. in the study we implemented and tested a static code analyzer perfchecker which is bui lt on soot a widely used j ava program analysis framework .
perfchecker supports the d etection of two performance bug patterns identified in our empir ical study .
we applied it to analyze the latest version of the android application s which comprise .
million lines of java code .
perfchecker successfully detected matching instances of the two bug pattern s in of the se applications.
we report ed them to the corresponding devel opers and instances have been confirmed as real issues affecting application performance while others are pending .
among the confirmed instances developers have quickly fixed of them by following our bu g fixing su ggestions .
they also expressed great interest in our perfchecker .
to summarize we make two major contributions in this paper to the best of our knowledge we conduct ed the first empirical study of real world performance bugs in smartphone applic ations.
our findings can help understand characteristics of pe rformance bugs in smartp hone application s and provi de guidance to related research e.g.
performance testing and analysis .
we implemented a static code analyze r perfchecker to detect our identified performance bug pattern s in android applic ations .
perfchecker successfully identified performance optim ization opportunities in popular android applications.
this inspiringly validated the usefu lness of our findings.
the rest of this paper is organized as follows.
section brief s android application basic s. section presents our empiri cal study of real world performance bugs in android applications .
section present s our case study in which we leverage our empirica l findings to find performance optimization opportunities in android applications .
section discusses related work from recent years and finally section concludes this paper.
.
background android is a n open source linux based operating system.
it is now one of the most widely adopted smartphone platform s. many equipment and device manufacturers e.g.
samsung customize their own android variants by modifying the android software stack e.g.
kernel and libraries .
applications running on the an droid platform are mostly written in java language.
for perfo rmance considerations developers may write critical parts of their applications using native code languages such as c and c .
application component and l ifecycle .
conceptually an android application cons ists of four types of component s activity service broadcast receiver and content provider .
for example an applic ation s graphical user interfaces guis are defined by activities.
each application component is required to follow a prescribed lifecycle that defines how this component is created used and finally destroyed.
for example figure gives the lifecycle of an activity .
it starts with a call to oncreate handler and ends with a call to ondestroy handler.
an activity s foreground lifetime i.e.
the ru nning state starts after calling onresume handler and lasts until onpause handler is called when another activity comes to foreground.
an activity can interact w ith its user only when it is at foreground.
when it goes to background and b ecomes invi sible i.e.
the stopped state its onstop handler would be called.
when its user navigate s back to a paused or stopped activi ty the activity s onresume or onrestart handler would be called and the activity would come to foreground again.
in exceptional cases a paused or stopped activity may be killed for releasing memory to other a pplications with higher priorities .
single thread policy .
when an android applicati on starts android os creates a main thread also known as an ui thread to instantiate this application s components.
this thread dispatc hes system calls to responsible application component s and user events to appropriate ui widgets e.g.
buttons .
after dispatching the corresponding components lifecycle handlers and ui widgets gui event handlers will run in the main thread to handle the system calls or user events .
this is known as the single thread pol icy .
the policy requires developers to control workload s of their applications main thread s e.g.
not overwhelming a main thread with intensive work .
otherwise application s can easily exhibit poor responsiveness.
.
empirical study in this section we present our empirical study of real world pe rformance bugs from android applications.
the study aims to answer our earlier four research questions rq1 .
in the following we first describe our application subject s and their reported performance bugs and then discuss our empirical findings.
we selected ope n source android applications as our subjects for studying questions rq1 because the study require s application bug reports and corresponding code revisions.
candidate appl ications that satisfy the following three criteria were randomly selected from four popular open source software hosting pla tforms namely google code github sourceforge and mozilla repositories .
firs t a candidate should have achieved more than downloads popularity .
second it running stoppedlaunch activity paused destroyedonstop .
oncreate .
onstart .
onresume .
onrestart .
onstart .
onresume onpause onresume ondestroy kill kill figure .
lifecy cle of an activity 1014should own a public bug tracking syste m traceability .
third it should have at least hundred s of code revisions maintainability .
the three criteria provide a good indicator of popular and mature applications.
for these candidates we tried to identif y perfo rmance bugs in their bug tracki ng system s. due to different management practices some application developers explicitly label ed performance bugs using special tags e.g.
perf while other s did not maintain a clear categorization of reported bugs .
to ensure that we study real performance bugs we refine d our application selection by keeping only those contain ing clearly labeled perfo rmance bugs for our study .
as a result eight applications were finally selected as our subjects all applications were still used in our later case study for validating the usefulness of our empir ical findings .
from the m we obtained a total of performance bugs which were clearly labeled confirmed and later fixed .
our selection process could miss some performance bugs e.g.
those not performance labeled .
some related studies selected performance bugs by searching keywords like slow or latency in bug reports .
we found that such searches resulted in more than candidate performance bugs in all applic ations.
we randomly sampled and manually analyzed of these candidate bugs and found that most of them are inappropr iate for our study .
this is because more than of these cand idates are either not related to performance i.e.
their bug reports accidentally contain such keywords or are actually complex bugs that contain both performance and functional issue s e.g.
low performance as a side effect of wrongly implemented functional ity .
to avoid introducing such threats or uncontrollable issues to our empirical study we refrain ed from keyword search while focus ing on the explicitly labeled performance bugs .
table lists basic information of our eight selected android applications .
they are large scale up to .9k loc popularly downloaded up to million downloads and cover five diffe rent application categories.
in the following we analyze perfo rmance bugs collected from these applications and report our fin dings.
the whole empirical study took about person days involving three students two postgraduate and one final year undergraduate for data collection analysis and cross checking.
.
rq1 bug types and impact s we studied the bug repor ts and related discussions e.g.
comments and patch reviews of the performance bugs and assigned them to different categories according to their major consequence s. if a bug has multiple major consequences we assign ed it to multiple categories so accumulated percentages can exceed .
we observed three common types of performance bugs in android applications gui lagging .
most performance bugs .
are gui lagging .
they can significantly reduce responsiveness or smoot h ness of the concerned applications guis and prevent user events from being handled in a timely way.
for example in firefox browser tab switching could take up to ten seconds in certain scenarios firefox bug .
this may trigger the infamous application not responding anr error and caus e an applic ation to be no longer runnable because android os would force its user to close the application in such circumstances .
energy leak.
the second common type of performance bugs .
is energy leak .
with such bugs concerned applic ations could quickly consume excessive battery power with certain tasks which actually bring almost no benefit to users .
for exa mple the energy leak in zmanim bug made the application render invisible gui widgets in certa in scenarios and this use less computation simply wasted valuable battery power .
if an android application contains serious energy leak s its user s smart phone battery could be drain ed in just a few hours .
for instance my tracks has received such complaints bug i just inst alled my tracks on my galaxy note and it is a massive batt ery drain.
my battery lost i n standby just minutes after a full charge.
this app is destroying my battery.
i will have to uninstall it if there isn t a fix soon.
energy leak s in smartpho ne applications can cause great inco nvenience to users .
users definitely do not want their smartphones to power off due to low bat tery especially when they need to make important phone calls .
as shown in the above commen ts if an application drains battery quickly users may switch to other applications that offer similar functionalities but are more energy efficient .
such a switch can be common since nowadays users have m any choices in selecting smartphone application s. memo ry bloat.
the third common type of performance bugs .
is memory bloat which can incur unnecessarily high memory consumption e.g.
firefox bug and chrome bug .
such bugs can cause out of memory oom error s and application crash es.
even if a concerned application does not crash immediately i.e.
mild memory bloat its performa nce can become unstable as dalvik garbage collection would be frequently invoked leading to degraded application performance .
these three performance bug types have occupied a majority of our studied performance bugs .
some bugs belong to more than one type as aforementioned .
there are also o ther types of bugs e.g.
those causing high disk consumption or low network throughput but we observe d them only once for each type in our dataset.
thus we consider them not common.
all bugs can be retrieved in their applications bug tracking system s using our provided bug id s. we omit detailed urls due to page limit.
table .
subjects and selected bugs applicatio n name category size loc programming language downloads availability selected bugs firefox3 communication1 .9k2 java c c 10m2 50m mozilla repositories chrome3 communication .3k java c python 50m 100m google code ankidroid education .8k java 500k 1m google code k mail communication .2k java 1m 5m google code my tracks health fitness .1k java 10m 50m google code c geo entertainment .7k java 1m 5m github open gps tracker travel local .1k java 100k 500k google code zmanim books reference .0k java 10k 50k google code the application category information is obtained from google play store 1k 1m for firefox and chrome we count ed only their lines of code specific to android.
.
rq2 bug manifestation understanding how performance bug s manifest in android appl ications can provide useful implications on how to effectively test performance bugs.
our study reveals some observations which demonstrate unique challenges in such performance testing .
small scale inputs suffice to manifest performance bugs .
existing studies reported that two thirds of performance bugs in pc applications need large scale inp uts to manifest .
however in our study we observed only performance bugs out of that require large scale inputs to manifest .
here we consider a dat abase with data entries already large scale e.g.
firefox bug .
other bugs can easily manifest with small scale inputs.
for example firefox bugs and only need one user to open several browser tabs to manifest.
manifested bugs woul d significantly slow down firefox and make its gui less responsive.
we give some comments from their bug reports below i installed the nightly version and found tab switching is so slow that it makes using more than one tab very hard .
firefox should correctly use view holder patterns.
ot herwise it will just have pretty bad scrolling performance when you have more than a couple of tabs.
these comments suggest that android applications can be susce ptible to performance bugs.
if an application has issu es affecting its performance u sers can often have an uncomfortable experience when conducting simple daily operations like adding a browser tab firefox bug .
a few such operations can quickly cause performance degradation .
due to this reason c autious d evelopers should try their best to optimize the performance of their code.
for example c geo developers always try to avoid creating short term objects c geo bug because android document ation states that less object creation even an array of integers means less garbage collection .
special user interaction s needed to manifest performance bugs .
more than one third out of of performance bugs require spe cial user interactions to manifest.
for example zmanim s energy leak needs the following four steps to manifest switch ing on gps configuring zmanim to use cu rrent location start ing its main activity and hit ting the hom e button when gps is acquiring a location.
such bugs are common but can easily escape traditional testing.
they can only manifest after a cer tain sequence of user interactions happen to the concerned application but traditional code based testing adequacy criteria e.g.
statement or branch coverage do not really consider sequences of user interaction s. a recent stud y also show s that existing testing techniques often fail to reach certain part s of android application code .
hence our finding s suggest two challenge s and corresponding research directions in testing perfo rmance bugs for smartphone applications effectively t esting p erformance bugs require s coverage criteria that explicitly consider sequences of user interactions in a ssessing the testing adequacy .
since the validity of user intera ction sequences is essentially defined by an application s gui structure existing research on gui testing coverage criteria may help in address ing this challenge.
test input generation should construct effective user intera ction sequences to systematically explore a n application s state space.
since such sequences can be infinite research effort should focus on effective techniques that can identify equiv alence among constructed user interaction sequences avoiding redundant sequences and wasted test effort s. automated performance oracle needed .
performance bugs can gradually degrade an application s performance.
for example firefox becomes progressively slower when its database s size grows bug .
such bugs rarely cause fail stop cons equences like application crash es thus it is challenging to decide whether an application is suffering any performance bug.
yet our study found three co mmon judgment criteria that have been used in real world to detect performance bugs in android applications human oracle .
more than half of the judgments were made manually by developers or users in our investigated android applications .
people simply made judgment s according to their own experience s. product comparison.
many developers compar ed different products of similar functionalities to judge whether a partic ular product contains any performance bugs e.g.
checking whether conduc ting an operation in one product is remarkably slower than in other products .
we observed ten such cases in our stud y. for example upon receiving user complaints about performance k9 mail developers check ed whether their application s performance was comp arable to other email clients and then decide d what to do next k9 mail bugs and .
developers consensus.
developers also have some implicit consensus for judging performance bugs .
for instance google developers consider an application sluggish i.e.
gui lagging if a user event cannot be handled within ms .
mozilla developers assume that firefox s graphics rendering units should be able to produce frames per second to make smooth animations firefox b ugs and .
although these judgment criteria have been used in practice they either re quire non trivial manual effort thus not scalable or are not generally defined thus not widely used .
to facilitate perfo rmance testing and analysis automated oracles are thus desirable .
even if general oracles may not be possible application or bug specific oracles can still be helpful .
encoura gingly there have been initial attempts toward this end .
besides our prev ious work also proposed a cost benefit analysis to detect ene rgy leak s caused by improper or ineffective uses of smartphone sensors.
still more effort on general automated oracle s for pe rformance bugs is needed to further advance rela ted research .
performance b ugs can be p latform dependent.
we also observe d that a non negligible proportion out of of perfo rmance bugs requir e specific software or hardware platforms to manifest.
for example chrome s caching scheme would hurt performance on arm based devices but not on x86 based devi ces chrome bugs and .
firefox s animation works more smoothly on android .
than older systems firefox bug .
this suggests that developers should consider device varie ty during performance testing since android os can run on different hardware platforms and has so many customized var iants.
this feature differs largely f rom performance bugs in pc applications which are not so platform dependent .
.
rq3 debugging and bug fixing effort to understand the effort required for performance debugging and bug fixing for android applications we analyzed of our gui lagging energy leak and memory bloat are three dominant performance bug types in our studied android applications.
research effort can first be devoted into designing effective techniques to combat them.
effective performance testing need s new coverage criteria to assess testing adequacy effective techniques for genera ting user interaction sequences to manifest performance bugs and automated oracle s to judge performance degradation .
1016performance bugs .
we excluded remaining bugs because we failed to recover links between their bug reports and code rev isions3.
to quantify debugging and bug fixing effort for each of these bugs we measured three metrics that were also adopted in related studies bug open duration which is the amount of time from a bug re port is opened to the concerned bug is fixed number of bug comments which counts discussions among deve lopers and users for a bug during its debugging and bug fixing period patch size which is the lines of code changed for fi xing a bug .
intuitively i f a bug is difficult to debug and fix its report would be open for a long time d evelopers tend to d iscuss it more and its patch could cover more lines of code changes .
table reports our measurement results.
we observe that on a verage it takes develop ers about two month s to debug and fix a performance bug in an android application .
during this period they can ha ve tens of rounds of discussions resulting in many bug comments up to .
besides on average bug fixing patches can cover more than lines of code changes indicating non trivial bug fixing effort.
for comparison we also randomly selected non performance bugs bugs without performance labels from the bug database of firefox and chrome we selected bugs for each .
we did not select non performance bugs from other appl ication subjects for comparison because each of these subjects contai ns only a few performance bugs about two to four .
such small sample sizes may l ead to unreliable comparison results leaving a weak foundation for further research on related topics .
on the other hand the vast majority of our studied perfo rmance bugs come from firefox and chrome and therefore we select ed non performance bugs from these two subjects for co mparison.
the severity level s of our selected non performance bugs are comparable to those of performance bugs in firefox and our manual analysis of commit logs around bug fixing dates also failed to find corresponding code revisions.
chrome .
figure compares these tw o kinds of bugs by boxplot s. the results consistent ly show that performance debugging and bug fixing require more effort than their non performance cou nterparts .
for example in firefox the median bug open duration is workdays for performance bugs but only workdays for non performance bugs .
to understand the significance of the differences between these two kinds of bugs we conducted a mann whitney u test with the following three null hypotheses performance d ebugging and bug fixing do not take a signif icantly longer time tha n their non performance counterparts .
performance d ebugging and bug fixing do not need significan tly more discussion than their non performance counterparts .
patches for fixing performance bugs are not significantly larger than those for fixing non performance bugs.
table gives our mann whitney u test results p values .
the results reject ed the above three null hypotheses all with a conf idence level over .
i.e.
p values are all less than .
.
th us we conclude that debugging and fixing performance bug s indeed requires more effort than debugging and fixing non performance bugs.
this result can help developers better understand and prioritize bugs for fixing in a cost effective way as well as estimating possible manual effort required for fixing certain bugs.
we further looked into bug comment s and bug fixing patches to understand why it is difficult to debug and fix performance bugs in android applications and what support is expected in debugging and bug fixing .
we found that quite a few .
performance bugs involve multiple threads or processes which may have complicated the debugging and bug fixing tasks .
in addition these performance bugs rarely cause d fail stop cons equences such as application crash es.
due to this reason traditional debugging infor mation e.g.
stack trace can offer little help in performance debugging .
we analyzed all performance bugs and found that only four bugs have had their debugging and fixing tasks receiving some help from such traditional information as judged from their bug discussions e.g.
c geo bug and firefox bug .
on the o ther hand we found that debugging information from two kinds of tools has received more attention profiling tools.
profiling tools or profilers monitor an a pplic ation s execution record its runtime information e.g.
execution time of a code unit and trace details of its resource consumption e.g.
memory .
for example firefox and chrome developers often take three steps in performance debugging reproducing a performance bug with the information provided in its bug report if any running the application of concern for a long while to generate a profile using their own profi lers and perform ing offline profile analysis to identify performance bottle table .
performance bug debugging and fixing e ffort metric min.
median max.
mean bug open duration days .
number of bug comments .
patch size loc .
table .
p values of mann whitney u tests subject p value bug open duration bug comments patch size firefox .
.
.
chrome .
.
.
nperf a bug open duration days firefox b number of bug comments c patch size lines of code changes 125102050100200500chrome firefox chrome firefox chrome nperf perf nperf perf nperf perf nperf perf nperf perf perf125102050100200500 .
.
.
.
.
figure .
comparison of d ebugging and bug fixing effort perf perfo rmance bug nperf non performance bug 1017neck s bugs if possible .
however profile analysis can be very time consuming and painful because current tools e.g.
those from android sdk can record tons of runtime information but which runtime information can actually help performance debu gging is still an open question .
firefox developers have designed some visualization tool s e.g.
cleopatra to save manual effort in profile analysis but these tools are not accessible to other developers or applicable to other applications .
researcher s and practitioners are thus encouraged to design new general techniques and tools for analyzing aggregating simplifying and vis ualizing profiling data to facilitate performance debugging .
performance m easurement tools.
performance measurement tools can also ease performance debugging.
they can directly report performance for a selected code unit in an application .
for example firefox s frame rate meter measures the number of frames a graphic s rendering unit can produce per second e.g.
when debugging firefox bug .
this information can help developer s in two ways .
first it prioritiz es the code units that need performance optimization .
second it suggests whether a code unit has been adequately optimized .
for example firefox developers could stop further optimizing a graphics rendering unit if the frame rate meter reports a score of frames per second e.g.
when fixing firefox bug .
chrome developers also use similar tools e.g.
using smoothness measurement tools for debugging chrome bug .
such tools are useful and welcome d by android developers.
we show some comments about firefox s frame rate meter from the developers mail ing list i found it very useful for finding performance issues in firefox ui and web devs should find it useful too.
this is fantastic stuff.
it s a must have for people hac king on front end ui .
also for devs tracking animation perf.
besides understanding the challenges of performance debugging we also looked for reasons from bug fixing patches why fixing performance bug s is so difficult .
we found that such patches are often complex and have to conduct algorithmic changes e.g.
firefox bug design pattern reimplementation e.g .
firefox bug or data structure or caching scheme r edesign e.g .
chrome bug .
such bug fixing tasks are us ually complex.
this explains why fixing performance bugs took a longer time and incurred much large r patch sizes than fixing non performance bugs as illustrated in figure .
.
rq4 common bug patterns to learn the root causes of our performance bugs we studied their bug reports patches c ommit logs and patch reviews.
we managed to figure out root causes for of these bugs.
for the remaining bugs we failed due to the lack of info rmative mat erials e.g.
related bug discussions .
performance bugs in android applications can have complex or application specific root causes .
for example firefox s slow tab closing bug was caused by heavy message communication s between its native code and java code firefox bug while ankidroid suffered gui lagging because its database library was inefficient ankidroid bug .
despite such variety we still identified three common causes for out of the performance bugs .
.
we explain them with concrete examples below .
lengthy operations in main thread s. as mentioned earlier android application s should not block their main thread s with heavy tasks .
however when applications become increasingly more complex developers tend to leave lengthy operations in main thread s. we observed quite a few occurrences of such bugs .
.
figure gives a simplified version of firefox bug and its bug fixing patch .
this bug caused firefox to suffer gui lagging when its tab strip button was clicked.
the bug occurred because the button s click event handler transitively called a refreshthumbnails method which produced a thum bnail for each browser tab by iteratively call ing heavy weight bitmap compression apis lines .
later t o fix this bug deve lopers move d such heavy operations to a background thread lines which can asynchronously update firefox s gui.
wasted computation for invisible gui .
when an android a pplication switches to background it may still keep updating it s invisible gui.
this bring s almost no per ceptible benefit to its user and thus the performed computation e.g.
collecting data and updating gui simply waste s resources e.g.
battery power .
such bugs also form a common pattern which covers of the performance bugs .
.
for i nstance figure lists the concerned code and corresponding bug fixing patch for our aforementioned energy leak in zmanim bug .
when zman imactivity launches it registers a location listener to receive loc ation changes for updating its gui lines .
the location li stener is normally unregistered when the activity is destroyed line .
however if a user launches zmanim and then switches it to background android os will call onpause and onstop ha ndlers accordingly but not ondestroy the application will keep receiving location changes to update its gui which is however invisible .
the location sensing and gui refreshing are then useless but still drain battery power .
this can be common for many smartphone applications because users often perform multiple tasks at the same time e.g.
play ing facebook and twitter while listening to music and frequently switch between them .
to fix such bugs developers have to carefully monitor application state s and disable unnecessary tasks when an application go es to bac kground.
for example firefox developers suggest ed disabling timers animations dom events audio video flash plugins and sensors when firefox went to background firefox bug .
similarly as figure shows zmanim developers disable d location sensing by unregistering the location li stener in zmanima ctivity s onpause handler line and enabled it again in onresume handler when necessary lines .
frequently invoked heavy weight callbacks.
four out of the performance bugs .
concern frequently invoked callbacks.
these callbacks need to be light weight since they are frequently invoked by android os.
however many such callbacks in real world applications are ill implemented.
they are public voidrefreshthumbnails generate a thumbnail for each browser tab iterator tab iter tabs.values .iterator while iter.hasnext geckoapp.
mappcontext .genthumbnailfortab iter.next geckoappshell.gethandler .post newrunnable public voidrun iterator tab iter tabs.values .iterator while iter.hasnext geckoapp.
mappcontext .genthumbnailfortab iter.next note the method genthumbnailfortab compresses a bitmap to produce a thumbnail for a browser tab.
.
.
.
.
.
.
.
.
.
.
.
.
.
figure .
firefox bug simplified debugging and fixing performance bugs are generally more difficult than debugging and fixing non performance bugs.
i nformation provided by profilers and performance measurement tools are more helpful for debugging than traditional info rmation like sta ck trace.
existing profilers expect improvement for automatically analyzing aggregating simplifying and vis ualizing collected runtime profiles.
1018heavy weight and can significantly slow down concerned applic ations .
we illustrate with a list view callback example below .
a list view display s a list of scrollable items and is widely used in android applications.
figure gives one example where each listed item contains two elements i.e.
two inner views of the list item an icon and a text label .
when a user scroll s up a list view some existing items will go off the top of the screen while some new items will be added to the bottom .
to use a list view deve l opers need to write an adapter class and de fine its getview callback see figure for example .
at runtime when a new item needs to go onto the screen android os will invoke the getview callback to construct and show this item.
this callback conducts two operations parsing the new item s layout xml file and constructing a tree of its elements a.k.a.
list item layout inflation and traversing the tree to retrieve specific element s for updating a.k.a.
inner view retriev al and update .
however xml parsing and tree traversing can be time consuming when a list item s layout is complex e.g.
containing many elements or having hierarchical structures as android applications typically do .
screen scrolling can thus slow down if such operations are common ly perf ormed .
for perfor mance concern s android os recycles each item that goes off the screen while users scroll a list view .
the recycled items can be reuse d to construct new items that need to appear later.
such recycl e and reuse can be done as list items often have identical layout s. we give two version s of getview implementation in figure .
the first ineffic ient version conducts two aforementioned operations lines every time the callback is invoked.
the second version applies a view holder design pattern suggested by android documentation .
the basic idea is to reuse previously recycled list items.
it avoids list item layout inflation when there are recycled item s for reuse line s .
besides when a list item is constructed for the first time the