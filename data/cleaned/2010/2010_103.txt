it s not a bug it s a feature how misclassification impacts bug prediction kim herzig microsoft research cambridge united kingdom kimh microsoft.com sascha just andreas zeller saarland university saarbr cken germany just st.cs.uni saarland.de zeller cs.uni saarland.de abstract this submission presents work submitted and accepted at the international onference on software engineering in .
in empirical software engineering it has become common to mine his toric dat a to detect where bugs have occurred in the past or to predict where they will o ccur in the future.
the accuracy of such models depends on the quality of the data.
f o r e x a m p l e defect prediction models rely on the accuracy of hist oric data such as bug reports.
bug reports that refer to any other than corrective develop ment activities may cause code artefacts to be falsely marked as defective .
this ma y have severe consequences for the resulting models and its accurac y. earlier studies raised concerns about bug reports referring to error unrelated d evelop ment activities.
but how often does such misclassification occur?
further doe s it a ctually impact analysis and prediction models?
these are the questi ons we addr essed in this paper.
in a manual examination of more than issue reports from five opensource projects we found .
of all bug reports to be miscla ssified threatening bug prediction models confusing bugs and features o n average of files marked as defective actually never had a bug.
the pres entation will cover causes for issue report misclassification and the result of our study some newer results not in the paper .
talk summary empirical studies are threatened by the quality of data ana lyzed and interpreted.
a commong task in empirical software engineering is to separate d efective from defect free code artifacts e.g.
to build defect prediction models which relies on historic bug data.
the majority of issue reports are classified as bugs that is requests for corrective code maintenance and suggest that code changes resolving these issue s should be c onsidered as bug fixes and that the associtaed code artifacts shoul d be considered as defective.
however it remains unclear how reliable issue report clas sifications are.
in antoniol et al.
raised concerns about bug reports refe rring to error unrelated development activities.
if such mix ups which mostly st em from issue reporters and developers interpreting bug differently occurred frequently and systematically they would introduce bias in data mining models threatening the external validity of any 103study that builds on such data predicting the most e rror prone files for instance may actually yield files most prone to new features.
but how o ften does such misclassification occur?
and does it actually bias analysi s and prediction?
our study targeted the following research questions rq1 do bug databases contain data noise due to issue report misclas sification and how much?
rq2 which percentage of issue reports associated with a category wa s marked as misclassified?
which category do these misclassified reports a ctually belong to?
rq3 what is the impact of misclassified issue reports when m apping issue reports to source code changes?
rq4 how does bug mapping bias introduced by misclassified issue r eports impact the top of most defect prone source files?
to answer these research questio ns we manually inspected and r e classified more than issue reports from five ope n source java projects develop ed by the apache and mozilla foundations we will give more details about the cl assification process in the talk .
comparing the re classified issue categories with the orgina l issue report type as stated in the bug database showed that over of all issue report s in the analyzed bug databases were associate to inaccu rate issue report types rq1 .
concentrating on bug reports we showed that over of all bug reports are miscla ssified rq2 .
during the talk we show details of the analysis and discuss sources o f misclassification many of which refer to the fact that bug databases and bug reports provide communication platforms for different stakeholder e.g.
enginners and custome rs wich have a very different perception of issues and a very different level of te chnical understanding.
estimating the impact of these miscalssifactions on ma ppings be tween actual code fixes and their changed code artifacts we show that on average of all files originally marked as defective actually never had a bug rq3 .
this impac t on file mapping threatens bug count and bug prediction models.
in fact we show that when identifying the top most defect prone source files to of these files do not belong in this category because of issue report misclassification.
the original published paper this talk is covering can be found on the publisher s website