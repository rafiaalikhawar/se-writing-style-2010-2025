boosting concolic testing via interpolation joxan jaffar vijayaraghavan murali national university of singapore joxan m.vijay comp.nus.edu.sgjorge a. navas the university of melbourne jorge.navas unimelb.edu.au abstract .
concolic testing has been very successful in automatically generating test inputs for programs.
however one of its major limitations is path explosion that limits the generation of high coverage inputs.
since its inception several ideas have been proposed to attack this problem from various angles de ning search heuristics that increase coverage caching of function summaries pruning of paths using static dynamic information etc.
we propose a new and complementary method based on interpolation that greatly mitigates path explosion by subsuming paths that can be guaranteed to not hit a bug.
we discuss new challenges in using interpolation that arise speci cally in the context of concolic testing.
we experimentally evaluate our method with di erent search heuristics using crest a publicly available concolic tester.
categories and subject descriptors d. .
testing and debugging symbolic execution testing tools general terms algorithms reliability keywords concolic testing interpolation symbolic execution .
introduction testing is the most commonly used method to ensure software quality.
it executes a given program with some inputs and the objective is then to nd bugs or validate the program with respect to the given inputs.
traditionally testing was carried out using manually generated inputs which became cumbersome and ine ective.
random testing alleviates this problem by generating random test inputs but su ers from poor code coverage.
recently more intelligent methods based on concolic testing a variant of symbolic execution have emerged that generate inputs by systematically exploring program paths attempting to increase coverage.the main idea of concolic testing is to execute the program simultaneously with concrete values and symbolic values.
when the program is executed symbolic constraints along the executed path are collected in a formula called path condition .
then a branch is picked and negated from the path condition resulting in a new formula which is then fed to a constraint solver to check for satis ability.
if it is satis able concrete test inputs are generated to follow the new feasible path.
if it is unsatis able the new path is infeasible and another branch has to be picked to be negated.
this way concolic testing attempts to improve the poor code coverage of random testing.
a key characteristic of concolic testing is that path conditions can be simpli ed using concrete values whenever the decidability of their symbolic constraints goes beyond the capabilities of the underlying constraint solver.
one major problem with concolic testing is that there are in general an exponential number of paths in the program to explore resulting in the so called path explosion problem.
recently several methods have been proposed to attack this problem from various angles using heuristics focused on branch coverage function summaries using static dynamic program analysis and so on.
we propose a new method based on interpolation largely complementary to existing approaches that signi cantly mitigates path explosion by pruning a potentially exponential number of paths that can be guaranteed to not encounter a bug.
our method inspired by aims at assisting concolic testing by making use of the concept of interpolation interleaved with the concolic execution process.
the use of interpolation for pruning paths in the context of symbolic execution is well known see e.g.
.
the idea is as follows rst assume that the program is annotated with certain bug conditions of the form ifcthenbug where if the condition cevaluates to true along a path the path is buggy.
then whenever an unsatis able path condition is fed to the solver an interpolant is generated at each program point along the path.
the interpolant at a given program point can be seen as a formula that succinctly captures the reason of infeasibility of paths at the program point.
in other words it succinctly captures the reason why paths through the program point are not buggy.
as a result if the program point is encountered again through a di erent path such that the interpolant is implied the new path can be subsumed because it can be guaranteed to not be buggy.
the exponential savings are due to the fact that not only is the new path subsumed but also the paths that this newpermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse august saint petersburg russia copyright acm ... .
23path would spawn by negating its branches.
unfortunately methods such as cannot be used directly for concolic testing due to several challenges.
first the soundness of these methods relies on the assumption that an interpolant at a node has been computed after exploring the entire tree of paths that arise from the node.
in concolic testing this assumption is invalid as the tester can impose an arbitrary search order.
for example concolic testers such as crest and klee use often many heuristics that may follow a random walk through the search space thus making this method unsound.
to address this problem we need to keep track of nodes whose trees have been explored fully in which case we say the node is annotated with afull interpolant or partially similarly a half interpolant .
under this new setting only nodes with full interpolants are capable of subsumption in a sound manner.
as a result the amount of subsumption depends on how often nodes get annotated with full interpolants from the paths explored by the concolic tester.
unfortunately our benchmarks in section showed that the above method by itself results in very few nodes with full interpolants thereby providing poor benet to the concolic tester because the tester rarely explores the entire tree of paths arising from a node.
hence an important challenge now is to accelerate the formation of full interpolants in order to increase subsumption.
for this we introduce a novel technique called greedy con rmation that performs limited path exploration i.e.
execution of a few extra paths by itself guided by subsumption with an aim to produce a full interpolant at nodes currently annotated with a half interpolant.
it is worth mentioning that this execution of few paths is done without interfering with the search order of the concolic tester.
this technique ultimately resulted in a signi cant increase in subsumption for our benchmarks and is vital for the e ectiveness of our method.
we implemented our method and compared it with a publicly available concolic tester crest .
we found that for the price of a reasonable overhead to compute interpolants a large percentage of paths executed by those heuristics can be subsumed thereby increasing their coverage substantially.
.
related work the main innovation introduced by concolic testing originally presented in dart and cute was the fact that concrete inputs can be generated based on some intelligent decision by symbolically negating one of the executed branches.
since the seminal papers of many works have been published improving concolic testing in di erent ways.
we limit our discussion to related works that attempt at mitigating the scalability issues in concolic testing due to the exponential numbers of paths.
recent extensions e.g.
exe crest sage and klee have tried to address this challenge by using novel heuristics to guide the exploration of paths improving the naive depth rst search strategy originally used in dart.
they target branch coverage i.e.
number of branches evaluated to true and false rather than path coverage .
although branch coverage does not su er from path explosion it is understood that the ultimate goal of the concolic tester is pathcoverage.
branch coverage is just an inexpensive measure of the quality of a test suite and a good branch coverage is more of a necessary requirement for quality than su cient.
the main di erence with the above methods is that we focus on path coverage rather than branch coverage while respecting those search strategies.
another interesting line of research has addressed the caching offunction summaries as a way of reducing the exponential number of paths e.g.
smart and .
our method performs function inlining not intelligent interprocedurally while these methods execute the dart algorithm to generate reusable summaries not intelligent intraprocedurally .
this suggests that both approaches are orthogonal and could work together to bene t from each other.
the closest related tester to ours in the line of pruning paths is .
this method eliminates redundant paths by analysing the values read and written in memory by the program.
the main idea is to prune paths that only di er in program variables that are not subsequently read i.e.
dead variables .
we share with them the high level idea of removing irrelevant information in order to increase the chances of subsumption but the similarity ends there.
the most important di erence is that de nes irrelevant information using live dead variables so the subsumption test is simply a subset operation.
we use interpolation to prune paths so our subsumption test involves logical entailment checks which are more expensive.
however interpolants are much more powerful for subsumption which can result often in a greater amount of pruned paths.
we exemplify these di erences with through an example in section .
finally as mentioned before we have been clearly inspired by in the idea of interpolating infeasible paths which has been also applied in .
however assume full control of the search space by performing a dfs traversal to compute their interpolants which ensures an interpolant at a node always represents the entire tree of paths below a node.
this is the key to making sound subsumption.
this assumption is no longer valid in concolic testing since the tester controls the search space using some heuristic which may not be dfs.
interestingly crest compares di erent heuristics for concolic testing and concluded that dfs is actually the worst in terms of branch coverage.
thus are not readily suitable for concolic testing.
more importantly as we will see in section even if somehow the dfs restriction is lifted in these methods e.g.
by augmenting them with half and full interpolants it is not enough to provide a reasonable bene t to concolic testing as they scale poorly without greedy con rmation.
.
running example consider the program in fig.
where a read call signals the concolic tester to generate a concrete input.
in this case the inputs are used to decide the program s control ow.
assume initially the concolic tester generates a positive value for both inputs.
this drives the tester down the path which is shown by the left most path in the program s symbolic execution tree.
a symbolic execution tree represents the set of paths traversed by the program where each node corresponds to a program location and an edge between two nodes corresponds to the program24 1int s 2if read s s 4if read s s 6if s bug read 0read read read figure a program and its symbolic execution tree transition.
now the concolic tester provides us this path on which we perform symbolic execution and annotate it with interpolants in a backward manner.
at 8the path is not buggy and there is no infeasibility therefore the interpolant true is stored there denoted by ftrueg .
propagating this to 6we note that there is another branch to that has not been explored so we annotate the interpolant true at 6as ahalf interpolant to denote this fact.
once a node has been annotated with a half interpolant we stop and give control back to the concolic tester.
assume now the concolic tester attempts to turn around at 6into the path .
this path is infeasible as its path condition s s0 s s00 s0 s00 is unsatis able for simplicity we omitted the read constraints .
now we generate interpolants for this path by rst annotating ffalsegbecause it is an unreachable node.
propagating this back to 6through the label s we obtain the interpolant s .
we now note all paths from 6have been explored so we conjoin all interpolants at true s annotating it with the full interpolant s denoting that the entire tree of paths under 6has been explored.
now when the concolic tester generates a zero for the secondread and executes the path we check if the path condition at s s0 s implies the fullinterpolant at s0 after proper renaming .
it does so we can guarantee the entire tree of paths below 6to not be buggy and subsume it.
the exponential savings is because we saved the concolic tester from executing a potentially exponential number of paths in the tree under in this case two paths but in general exponential .
importantly note that only full interpolants are capable of subsumption and the amount of savings provided by our method is directly proportional to the number of full interpolants formed from exploring entire trees of paths.
unfortunately the method discussed so far has a catch.
we 1note that for this example we use interpolants based on weakest preconditions but our method is not limited to them and any interpolation method can be used.conveniently assumed the concolic tester would execute the paths in that speci c order which resembles a depth rst search dfs .
if after executing the rst path the tester did not attempt to turn around at thus leaving 6with a half interpolant 6would not have been able to subsume .
even worse is the fact that because of the half interpolant at 6all its ancestors along the path also become incapable of subsumption thereby losing a great amount of savings.
thus we need to accelerate the formation of full interpolants instead of simply relying on the concolic tester to explore the paths.
however a challenge in concolic testing is that whatever technique is employed for this purpose it must always stay proportional to the executed path that is not become intractable.
for this we introduce a technique called greedy con rmation that is both tractable proportional to the path length in the worst case and accelerates the formation of full interpolants resulting in a substantial increase in subsumption.
the basic idea is while backtracking along a path once we encounter a node with another branch that has not been explored by the concolic tester instead of simply annotating the node with a half interpolant and halting the process we explore the other branch ourselves while the concolic tester is waiting to generate the next path and attempt to con rm whether a full interpolant can be generated from it so that we can annotate the node as full and continue the backward propagation.
however the sub tree under the other branch could be exponentially large in which case we must avoid exploring it to remain tractable.
hence we introduce a restriction while exploring the sub tree each program point is allowed to be explored at most once.
if a program point is visited along more than one path we demand that it be subsumed.
if not we declare that the greedy con rmation process failed at the original branch node which will remain annotated with a half interpolant.
the reasoning behind the restriction is as follows.
during greedy con rmation we want to give each program point at least one chance to become subsumed so we must allow at least one visit to each program point.
however we do not want to resort to a full search within the sub tree which could become intractable.
thus this restriction ensures that the search is linearly bounded by the longest path in the program2.
the motivation behind this technique is that the di erence between the two branching sub trees might not a ect the bug condition and so one tree can quickly subsume the other.
let us see how greedy con rmation works on the example.
when backtracking along the rst path once we reach 6with the interpolant true we trigger greedy con rmation and attempt to explore the other tree under .
we immediately notice the other branch to 7is infeasible and annotate it with false .
propagating this back to 6we get the full interpolant s which can now be propagated back further.
hence at 5we get the full interpolant s .
propagating this to 4makes it a half interpolant since there is another branch from 4to .
triggering greedy 2we tried restricting to two three and other constant instances of each program point but we found no di erence in our benchmarks.25con rmation this time at we notice that 6is subsumed by 6with the full interpolant s .
propagating this to and conjoining it with the existing half interpolant we get s s or simply s at which is now a fullinterpolant that can be propagated back.
similar reasoning at 2subsumes a large tree of paths under 4resulting in the full interpolant s at .
now in this contrived example if the concolic tester executes any path in the program we are able to subsume it immediately at .
essentially greedy con rmation has pushed subsumption from happening at lower levels in the symbolic execution tree to upper levels.
a simple but e ective optimisation is if greedy con rmation failed at a node thereby leaving it with a half interpolant we can simply halt the annotation process because there is no use propagating a half interpolant to the node s parents.
finally note importantly that this example does not contain any dead variables so techniques like will not be able to prune any path whereas with interpolation we are able to.
remark.
the correctness of our method relies on the fact that whenever we subsume a path i.e.
we skip its execution we can ensure that the path will not be buggy see theorem .
in sec.
.
this guarantee is only feasible if the program is annotated with assertions.
without them we have no basis to make such a guarantee and subsume the path.
a key observation is that without assertions each path is unique because at least it will di er in one branch from the rest of paths and hence there is no hope for boosting the concolic execution.
however with an assertion many paths can be considered equivalent and this allows our method reporting savings.
in conclusion our method is only e ective if we consider programs annotated with assertions.
we believe that this step can be done automatically and it is not a big hassle in practice.
.
preliminaries syntax .
most of the formalism used here has been borrowed from .
we restrict our presentation to a simple imperative programming language where all basic operations are either assignments or assume operations and the domain of all variables are integers.
the set of all program variables is denoted by vars.
an assignment x e corresponds to assign the evaluation of the expression eto the variable x. in theassume operator assume c if the boolean expression cevaluates to true then the program continues otherwise it halts.
the set of operations is denoted by ops.
we then model a program by a transition system .
a transition system is a quadruple h i !
oiwhere is the set of states andi is the set of initial states.
!
ops is the transition relation that relates a state to its possible successors executing operations.
this transition relation models the operations that are executed when control ows from one program location to another.
we shall use op !
0to denote a transition relation from to executing the operation op2ops.
finally o is the set of nal states.
symbolic execution .
asymbolic state is a tripleh s i. the symbol corresponds to the current program loca tion.
for clarity of presentation in our algorithm we will use special symbols for initial location start2i nal location end2o and bug location bug2o if any .
w.l.o.g we assume that there is only one initial nal and bug location in the transition system.
the symbolic store sis a function from program variables to terms over input symbolic variables.
each program variable is initialised to a fresh input symbolic variable.
this is done by the procedure initstore .
the evaluation jck s of a constraint expression cin a storesis de ned recursively as usual jvk s s v ifc vis a variable jnk s n ifc nis an integer jeopre0k s jek s oprje0k s if c eopre0wheree e0are expressions and opris a relational operator !
and jeopae0k s jek s opaje0k s ifc eopae0wheree e0are expressions and opais an arithmetic operator .
finally is called path condition a rst order formula over the symbolic inputs that accumulates constraints which the inputs must satisfy in order for an execution to follow the particular corresponding path.
the set of rst order formulas and symbolic states are denoted by foand symstates respectively.
given a transition system h i !
oiand a state h s i2symstates the symbolic execution of op !
0returns another symbolic state 0de ned as h s jck s i ifop assume c and jck s is satis able h s iifop x e note that equation queries a constraint solver for satis ability checking on the path condition.
we assume the solver is sound but not necessarily complete.
that is the solver must say a formula is unsatis able only if it is indeed so.
abusing notation given a symbolic state h s iwe dene j k symstates!foas the formula v v2varsjvk s where vars is the set of program variables.
asymbolic path nis a sequence of symbolic states such that8i i nthe state iis a successor of i denoted as succ i i .
a path nisfeasible if n h s isuch that j k s is satis able.
if 2oand nis feasible then nis called terminal state.
otherwise if j k s is unsatis able the path is called infeasible and nis called an infeasible state.
if there exists a feasible path nthen we say k k n is reachable from 0ink steps .
we say 00is reachable from if it is reachable from in some number of steps.
a symbolic execution tree contains all the execution paths explored during the symbolic execution of a transition system by triggering equation .
the nodes represent symbolic states and the arcs represent transitions between states.
concolic testing .
we rst de ne a concolic path p as the path executed by the concolic tester represented as a sequence of program locations nand de ne the 3w.l.o.g we assume each state has at most two successors.26genericconcolic programp path p while termination conditions are not met do bi pick a branch from p p0 construct a path passing through the branches b0 b i bi if9satisfying assignment iforcingpthrough p0then q concretepath p i process qby either p qor genericconcolic p q endif endwhile figure a generic concolic tester transition relation op !
0as before.
in order to manipulate concolic paths we also de ne pre x p of a path pwrt a location as the pre x up to without including .
we also de ne constraints p that maps a concolic path into a formula representing the conjunction of the symbolic constraints along the path.
of course this formula is properly renamed i.e.
ssa form to take into account variables that are rede ned more than once.
we now show in fig.
a generic algorithm that performs concolic testing as described in .
the algorithm is generic in the sense that it can be parameterised with di erent search strategies by simply choosing di erent heuristics to pick a branch at line .
the algorithm is instantiated with the program pand an initial concolic path p usually chosen by running the program with random inputs.
in line a branchbiis chosen from the path pbased on the heuristic used.
in line a new path p0is built by keeping the pre x up tobiand negating the constraints at the branch bi.
in line the symbolic constraints associated with p0are then fed to a constraint solver to check for satis ability.
if they are unsatis able the algorithm returns to line and picks another branch to negate.
otherwise an assignment i concrete inputs is extracted from the solver which is used to guide the next concrete path along the negated branch.
this is done using the call to concretepath p i in line .
once the new path say q is executed depending on the heuristic it is processed by either replacing the old path pwithq or by making a recursive call to genericconcolic withq in line .
the entire process continues until some termination condition is met usually a xed number of iterations or recursive call depth shown in line .
.
concolic testing with interpolation we now present our symbolic execution based algorithm that runs synchronised with genericconcolic and helps the concolic testing strategy mitigate the path explosion problem.
first we give few de nitions critical to our algorithm definition craig interpolant .
given two formulasaandbsuch thata bis unsatis able a craig interpolant intp a b is another formula such that a aj b bis unsatis able and c all variables in are common to aandb.an interpolant allows us to remove irrelevant information inathat is not needed to maintain the unsatis ability of a b. that is the interpolant captures the essence of the reason of unsatis ability of the two formulas.
e cient interpolation algorithms exist for quanti er free fragments of theories such as linear real integer arithmetic uninterpreted functions pointers and arrays and bitvectors e.g.
see for details where interpolants can be extracted from the refutation proof in linear time on the size of the proof.
definition full and half interpolants .
an interpolant annotated at a symbolic state is afull interpolant if either a is a leaf node terminal infeasible or subsumed in the symbolic tree or b 0such that succ 0is annotated with a fullinterpolant.
an interpolant that is not full is called a half interpolant .
definition subsumption check .
given a current symbolic state h s iand an already explored symbolic state h iannotated with the interpolant we say issubsumed by subsume h i if is a fullinterpolant and j k s j .
to understand the intuition behind the subsumption check it helps to know what a full interpolant at a node actually represents.
a full interpolant at a node 0succinctly captures the reason of infeasibility of all infeasible paths in the symbolic tree rooted at let us call this tree t1 .
then if another state at implies it means the tree rooted at say t2 has exactly the same or more infeasible paths compared to t1.
in other words t2has exactly the same orless feasible paths compared to t1.
sincet1did not contain any feasible path that was buggy we can guarantee the same fort2as well thus subsuming it.
note that if was a half interpolant we cannot guarantee this because t1has not been fully explored.
.
symbolic execution of paths with interpolants we rst present the algorithm symexec figure that takes a path chosen by the concolic tester and executes it symbolically in order to annotate each program point in it with interpolants.
a collection of such annotated paths implicitly represents the symbolic execution tree.
then we will present genericconcolicwithpruning a modi ed version of genericconcolic that can prune paths using the annotated symbolic execution tree.
symexec takes as arguments a symbolic state and a path p. typically the state refers initially to the rst location inp.
it is also assumed that all procedures have access to the program s original transition system p which can considered a global variable to the algorithm.
the actual object of interest is the annotation done by the procedure which is persistent across multiple calls to it.
for the sake of simplicity ignore the gray box which we will come to later.27symexec h i p if terminal thenh fi htrue fulli else if infeasible thenh fi hfalse fulli else if9 h iannotated withh fullisuch that subsume h 0i then h fi h fulli elseh fi unwindpath p endif annotate withh fi if f6 full then halt unwindpath h s i p existing interpolant at h s jck s iif op !
02p op assume c h s iif op !
02p op x e andsxfresh variable symexec p and let the resulting annotation at 0beh i intp constraints pre x p constraints if9 h isuch that 00is not annotatedh fulliand succ then greedycon rmation and let the resulting annotation at 00beh f00i if f00 full then intp constraints pre x p constraints endif endif f full if8 000s.t.succ 000is annotated with h fulli half otherwise returnh fi figure symbolic execution with interpolation along a path first lines handle the three possible base cases for the symbolic execution of a path terminal infeasible and subsumed.
in line the function terminal checks if end.
if yes the path can be fully generalised returning the fullinterpolant true since it is feasible.
in line the function infeasible checks if the path condition of is unsatisable.
if yes again the path can be fully generalised but this time to false since it is infeasible.
finally line checks if there is another state 0that can subsume using the function subsume which implements de nition .
if yes we can simply annotate with the full interpolant of line .
if the three base cases described above are not applicable the algorithm executes symbolically the next location of the path by calling the procedure unwindpath .
this procedure at line obtains any interpolant that may be annotated at it can be assumed that initially all symbolic states are annotated with the default interpolant htrue halfi .
in line it executes one symbolic step along the path and calls the main procedure symexec with the new successor state line .
this mutually recursive call will in the end annotate 0with an interpolant say .
in line it computes the interpolant for the current state using 0and the constraints along the transition op !
0and then conjoining the result with any existing interpolant at .
finally in line it computes whether is a full or half interpolant by implementing de nition .
for example consider the case where constraints pre x p x0 x1 x0 constraints x2 x1 and x2 after proper renaming .
the call at line will generate an interpolant whose variables must be common to constraints pre x p and constraints together with .
thus it can only include x1.
an interpolant generated by mathsat would be x1 .
a weaker interpolant is x1 which can be computed by weakest preconditions.
returning back the tuple h fito the caller symexec at line it performs the actual annotation of .
finally in line symexec checks if it just annotated the state with a full interpolant.
if not it halts in a normal manner and returns control to the concolic tester this can be seen as a system wide halt and is also noti ed to unwindpath .
the reason for halting is that there is no use propagating the half interpolants to parent nodes because they will still remain as half interpolants and be of no use for subsumption.
in other words the algorithm only propagates full interpolants to parent nodes.
this is a simple but very e ective optimisation.
note however that the annotation of states done so far is still persistent.
more importantly a half interpolant at a node now could get converted to a full interpolant later when all successors of the node get full interpolants.
we now present a modi ed concolic tester genericconcolicwithpruning shown in fig.
with the gray boxes highlighting the changes made in order to prune paths using our method.
first after picking a branch bito negate 4this halting may make the algorithm seem impure but we believe it makes it much easier to understand.28genericconcolicwithpruning programp path p while termination conditions are not met do bi pick a branch from p p0 construct a path passing through the branches b0 b i bi if issubsumedpath p0 then continue endif if9satisfying assignment iforcingpthrough p0then q concretepath p i symexec h start initstore truei q process qby either p qor genericconcolicwithpruning p q endif endwhile figure a generic concolic tester with pruning line and constructing the corresponding path p0that goes through it line the concolic tester makes a call to issubsumedpath with p0at line .
this call queries the persistent annotated symbolic execution tree to check whether the path p0has been subsumed already.
note that this is just a look up and hence does not involve symbolic execution.
if yes the tester can simply skip p0from being executed and continue thus pruning a potentially exponential number of recursive calls at line and all those paths it would have generated.
the second change is that once a path has been concolically executed in line the tester has to invoke our method so that we can annotate it with interpolants.
thus line makes a call to symexec with the initial state of the path 0and the path qthat was executed.
.
greedy confirmation to accelerate formation of full interpolants the method discussed so far has laid out the framework to bring interpolation to concolic testing with the help of half and full interpolants.
the bene t provided by our technique relies heavily on the formation of full interpolants from paths explored by the concolic tester i.e.
the amount of times f is assigned fullinunwindpath line .
unfortunately the method explained so far does not perform well in practice as we will see in section because relying only on the tester to explore paths results in poor formation of full interpolants.
this is because the tester s arbitrary search strategy seldom explores the entire tree of paths arising from a node a requirement to annotate the node with a full interpolant and enable it to subsume other nodes.
hence an important challenge to deal with in concolic testing is to accelerate the formation of full interpolants.
for this we introduce a new concept called greedy con rmation .
the basic idea was described in section here we explain it in technical terms gray box in fig.
.
if the call to symexec line of unwindpath returned successfully it means the successor of was annotated with a full interpolant say .
now we check at line if the other successor of say is also annotated with a full interpolant.
if not then this half interpolant say is the one preventing from becoming a full interpolant.
now we greedily explore 00by ourselves in an attempt to con rm whether we can make 00a full interpolant so that we can immediately upgrade to full thus enabling it to perform subsumption.
the intuition behind this technique is that the di erence between the trees below the two siblings 0and 00might have no e ect on the bug condition and since the tree below 0has been fully explored thereby annotated with full interpolants it opens up opportunities to subsume nodes in the tree below .
thus we make the call to greedycon rmation in line .
greedycon rmation essentially performs symbolic execution of 00similar to symexec .
however a key impediment is that the symbolic execution of 00must not become expensive and it should have a proportional cost to the length of path otherwise we consider it intractable.
however the tree under 00might be arbitrarily large and so we impose an important restriction to maintain tractability each program point is allowed to be explored at most once during greedy con rmation of .
if a program point kis visited in two states k1and k2then we demand that one of them be subsumed not necessarily by the other .
if it is not possible then we declare that the invocation of greedy con rmation failed at which will not be annotated with a full interpolant.
this restriction ensures that in the worst case greedy con rmation at any symbolic state is bounded linearly by the length of the longest path in the program.
note that greedycon rmation does not need the help of the concolic tester to explore its paths.
in fact this is the whole point.
thus it does not take as argument the path p but instead explores paths on its own.
since greedycon rmationis almost identical to symexec for lack of space we do not show any pseudo code here.
the only modi cation to symexec is to keep track of counters for each visited program location that is not subsumed and stop if a counter becomes greater than one.
finally to perform the symbolic step line unwindpath greedycon rmation must pick op !
from the program s transition system pinstead of the path p. coming back to the description of unwindpath consider the call to greedycon rmation at line produced the annotationh f00iat .
if suppose 00was a full interpolant i.e.
greedy con rmation succeeded then at line we augment the interpolant at with this full interpolant.
more importantly at line fwill be assigned fullbecause both successors of 0and have been annotated with fullinterpolants 0and 00respectively .
this directly accelerates the formation of full interpolants resulting in more subsumption line of symexec succeeding more often .
in section we will see this greatly increases the savings provided for concolic testing while still maintaining tractability.
theorem .
.
when a symbolic state is subsumed by another state the error location bugis unreachable from .
the proof follows trivially from the correctness of the algorithms described in and the fact we only subsume if the interpolant is full.29corollary .
.
ifgenericconcolic will execute a path pthat leads to the bug location bug then genericconcolicwithpruning will also execute p. theorem .
states that we never subsume a node from which the error location bugwould be reachable.
thus it can be shown that we never prevent genericconcolicwithpruning from executing a path that will reach bug.
finally note that it is entirely possible that the concolic tester can detect infeasible paths after simplifying a complex constraint using concrete values that we cannot detect symbolically.
therefore if unsatis ability cannot be determined symbolically we cannot compute interpolants.
as a result the pruning of paths is limited only to those where unsatis ability does not require reasoning about complex constraints.
moreover it is possible for greedycon rmation to reach bugduring the exploration of a path in the presence of constraints whose unsatis ability cannot be determined.
in this case we also declare that greedy con rmation failed and return the control to the concolic tester since only the tester can decide the reachability of bugin order to ensure zero tolerance for false alarms.
.
experimental ev aluation we implemented our algorithm on the tracer framework for symbolic execution and modi ed the concolic tester crest to communicate with tracer during its testing process.
to achieve this the algorithm in fig.
was implemented in tracer while crest was slightly modi ed to implement the procedure in fig.
.
we conducted experiments on three strategies depth first search dfs uniform random search urs and control flow graph cfg directed search.
dfs explores the paths in a depth rst order naturally forming full interpolants in a bottom up manner even without the need for greedy conrmation and maximising the bene t of subsumption so it is the best case strategy for our method.
however according to it is the worst strategy in terms of branch coverage so we mainly provide it for completeness.
cfg is a heuristicbased strategy that rst computes the shortest distances between every basic block using the cfg of the program and decides on a target basic block to cover.
then when a path is executed it chooses the next path by turning around at a branch along the path with the least distance to the target block.
if it is unable to do so due to infeasibility of constraints it chooses the next closest branch to the target that is along the path and so on.
once the target is reached it randomly chooses a new target block to cover.
it is worth noting that cfg directed search was shown to be best strategy in terms of branch coverage by so we consider it an illustrative concolic testing strategy.
finally urs explores paths in a random order by choosing to turnaround at a random branch in a currently executed path.
we included urs just to provide another example of a typical random concolic testing strategy.
this random element in both cfg and urs hinders greatly with the formation of full interpolants and is the main challenge for our algorithm to overcome.
as benchmarks we used four programs from the ntdrivers simpli ed category of sv comp cdaudio diskperf oppy and kb ltr .
these programs range from to lines of code.
in spite of their relative small sizes these programs have a large number of paths due to very complex control ows.
thus we believe these programs can stress more the computation of interpolants and subsumption checks since symbolic paths contain a high number of constraints and it is often harder to reason about the infeasibility of these constraints.
one of the consequences is that interpolants are stronger in the logical sense since they must consider multiple reasons of infeasibility and hence they are less likely to be reusable.
we ran three main experiments with them on an intel .2ghz with 2gb memory.
a technical problem with the experiments is that if our algorithm prevented crest from exploring subsumed trees we would never know how many paths crest would have executed in those trees and subsequently the time taken for the same had we not prevented it.
also we do not want to meddle with the random number based sequence in cfg and urs by forbidding crest to execute certain paths.
hence for measurement purposes we do not forbid crest from exploring subsumed trees in our experiments but instead take note when crest is in subsumed trees and discard any measurements taken during that time.
first experiment performance at the outset we would like to measure the actual performance improvement provided by our method.
for this we measure the time taken by each strategy of naive crest i.e.
original version and crest aided by our method i.e.
algorithms in figs.
and to execute a target number of paths chosen based on the size of the benchmark 200k for cdaudio and oppy 500k fordiskperf and around 20k for kb ltr due to its smaller size .
the results are shown in fig.
where the number of executed paths is shown on the x axis and the time taken in seconds to execute those paths is shown on the y axis.
for naive crest we noticed that the time taken to execute a path is almost constant across all three strategies so for simplicity we took the average time of the three strategies shown as naive in fig.
.
when running with our method and with greedy con rmation we show the strategies separately labelled as cfg gc dfs gc and urs gc .
it can be seen that in each benchmark naive crest takes almost linear time to execute the target number of paths because the generation of each path involves simply one constraint solving which takes almost constant time and possibly a heuristic to choose the branch to negate which is negligible.
when aided by our method crest starts out a bit slower due to the overhead of interpolation however after a certain time the bene ts of interpolation i.e.
subsumption start to payo ultimately reaching the target number of paths much faster.
the magnitude of improvement depends on the strategy.
as expected our method works best with dfs providing more than times improvement followed by cfg and urs with about times improvement.
for example in diskperf naive crest takes on average 1700s to complete whereas with our aid cfg and urs take about 600s and dfs takes 50s.
note that every path subsumed in this experiment is a path executed by crest that need not have been executed.
more importantly the trend of the graphs of crest running with our algorithm indicates300200 0k 50k 100k 150k 200k na ve cfg gc dfs gc urs gc cfg gc 0k 100k 200k 300k 400k 500k na ve cfg gc dfs gc urs gc cfg gc 0k 50k 100k 150k 200k na ve cfg gc dfs gc urs gc cfg gc 0k 2k 4k 6k 8k 10k 12k 14k 16k na ve cfg gc dfs gc urs gc cfg gc a b c d figure timing for a cdaudio b diskperf c oppy d kb ltr .
x axis paths y axis time in seconds cfg dfs urs cfg dfs urs cfg dfs urs cfg dfs urs a b c d figure subsumption for a cdaudio b diskperf c oppy d kb ltr .
x axis paths y axis subsumption potentially exponential bene t in time over naive crest.
to measure the e ectiveness of greedy con rmation in our algorithm we measured the timing of crest with our algorithm but with greedy con rmation turned o i.e.
the gray box in fig.
removed .
hence the full interpolants would be generated only by crest exploring full sub trees in the symbolic execution tree.
we chose the canonical cfg strategy for this experiment and its result is shown as cfggc double dotted line in the graphs in fig.
.
it can be seen immediately that the timing without greedy con rmation is much worse than otherwise.
especially for diskperf in fig.
b the timing is even slower than running naive crest because the overhead incurred due to interpolation does not payo in subsuming other paths.
in other benchmarks it pays o resulting in a bene t to crest but the amount of bene t is small compared to running cfg with greedy con rmation cfg gc .
more importantly the trend ofcfg gc does not appear to provide exponential bene t to crest if at all.
this experiment shows that greedy conrmation is a indeed major contribution to the e ectiveness of our algorithm and interpolation based methods without it are not readily suitable for concolic testing.
second experiment subsumption now we would like to understand the basis on which the rst experiment provided bene t. a good measure for this is the amount of subsumption that our method provides i.e.
the percentage of crest paths that are subsumed.
a thing to take note is that cfg and urs may repeat execution of some paths due to their random element which we cannot control.
forthis and the subsequent experiment we do not include such paths in the calculations because a repeated path does not contribute to the percentage of subsumption.
we included them in the previous experiment because they indeed contribute to execution time.
the results are shown in fig.
.
in the graph of each benchmark the x axis represents the number of paths executed by crest and the y axis represents the percentage of those paths subsumed.
it can be clearly seen that dfs very quickly reaches almost subsumption whereas the other two strategies cfg and urs show an increasing trend towards it with dfs as their asymptote.
this is in line with the rst experiment where the magnitude of performance bene t was the greatest in dfs followed by the other two.
indiskperf we notice some noise as cfg and urs uctuate between subsumption.
the next experiment below suggests that this is because even though we subsume trees during this period cfg and urs are not interested in executing paths in those trees.
this is possibly why the magnitude of improvement was low only around for diskperf in fig.
b .
theoretically the rate of subsumption could decrease even be zero if crest constantly avoids executing paths in subsumed trees but we expect this to seldom happen in reality.
third experiment extra path coverage now we present a di erent view of the provided bene t. when we subsume a tree we not only provide the direct bene t of saving the paths that crest indeed executes in the tree the rst experiment but also the indirect bene t of covering31010000 cfg urs cfg urs cfg urs cfg urs a b c d figure extra coverage provided for a cdaudio b diskperf c oppy d kb ltr by our method.
x axis crest path coverage y axis additional path coverage from subsumption.
paths in the tree that crest cannot cover within its budget.
although such paths are of low priority to the strategy s heuristics they are provided free of charge by our method because the time we spend to subsume the whole tree is inclusive of these paths as well.
this can be considered extra path coverage because crest has no hope of executing them in its budget but if its budget were longer it may execute them in future.
note that this experiment does not make sense for dfs because when we subsume a tree dfs would immediately proceed to execute allpaths in the tree anyway.
in fig.
we measure the number of such free paths per path executed by crest.
note that we again do not include repeated paths in this experiment since they do not contribute to path coverage.
in each graph the x axis shows the actual path coverage of crest and the y axis shows the extra path coverage obtained due to subsumption.
the ratio of extra path coverage to crest s actual path coverage varies greatly depending on the benchmark from a magnitude of in cdaudio to about in diskperf note the logarithmic scale .
speci cally for the cfg strategy in diskperf we subsume a huge number of trees around paths but the previous experiment showed yet uctuating percentage of subsumption around that time indicating that cfg is not interested to execute paths in those subsumed trees.
in this experiment by taking credit for entire subsumed trees we measured the upper bound on the magnitude of bene t in path coverage that we can provide a mean of .
the lower bound around to is dictated by the rst experiment although we did not explicitly measure it there we can extract it from the timing where we took credit only for the paths that crest actually executed in the subsumed trees within its budget.
in general one can expect the bene t we provide to lie somewhere in between depending on the budget.
fourth experiment terminating testing finally we discuss a small but important experiment.
we wanted to make a pure comparison of concolic testing with and without our method notwithstanding the complications with measuring the number of subsumed paths random number sequences and repeated paths we encountered in the previous experiments.
in other words we wanted to actively forbid crest from exploring subsumed trees instead of let ting it run and discarding measurements like before.
the problem in this is that when terminating with a budget the sequence of paths executed by naive crest and our method would be di erent and hence incomparable.
however if the testing process terminates having explored the entire search space i.e.
veri ed the program the sequence of paths it took to do so does not matter.
we obtained a smaller non buggy version of kb ltr from the same benchmark suite and ran crest s cfg strategy on it with and without our method this time actively forbidding crest from exploring subsumed trees.
with our aid crest was able to completely verify the program in seconds whereas naive crest was able to complete only after seconds.
this experiment shows concrete evidence that our method indeed accelerates a typical concolic testing strategy such as cfg towards more path coverage in this case verifying the program much faster than otherwise.
unfortunately we could not run this experiment on other benchmarks as they contain a prohibitive number of paths and concolic testing could not explore all of them in a reasonable amount of time making it not possible to make such an interesting comparison.
remark.
although we focus on path coverage in this paper it is worthwhile to note that our method can be also used to improve branch coverage .
in fact in some of our preliminary experiments we targeted branch coverage and observed that we were achieving the same branch coverage but sometimes faster with our method than without.
however the problem of branch coverage is simpler than path coverage and hence pruning is not always vital.
in these cases the overhead of interpolation and subsumption may not pay o .
.
conclusion we attacked the path explosion problem of concolic testing by pruning redundant paths using interpolation.
the challenge for interpolation in concolic testing is the lack of control of search order.
to solve this we presented the concept of half and full interpolants that makes the use of interpolants sound and greedy con rmation that accelerates the formation of full interpolants thereby increasing the likelihood of subsuming paths.
we implemented our method and empirically presented its performance and path coverage gains.
.