automated reliability estimation over partialsystematic explorationsesteban pavese v ctor brabermanuniversidad de buenos aires epavese vbraber dc.uba.arsebasti n uchiteluniversidad de buenos aires andimperial college londonsuchitel dc.uba.arabstract model based reliability estimation of software sys tems can provide useful insights early in the development process.however computational complexity of estimating reliability met rics such as mean time to first failure mttf can be prohibitiveboth in time space and precision.
in this paper we present analternative to exhaustive model exploration as in probabilisticmodel checking and partial random exploration as in statisticalmodel checking.
our hypothesis is that a carefully crafted partial systematic exploration of a system model can providebetter bounds for reliability metrics at lower computation cost.we present a novel automated technique for reliability estimationthat combines simulation invariant inference and probabilisticmodel checking.
simulation produces a probabilistically relevantset of traces from which a state invariant is inferred.
theinvariant characterises a partial model which is then exhaustivelyexplored using probabilistic model checking.
we report onexperiments that suggest that reliability estimation using thistechnique can be more effective than full model probabilisticand statistical model checking for system models with rarefailures.i.
introductionmodel based automated verification for assessing reliabilityof software systems aims to provide insights early in the devel opment process that can reduce significantly both developmentcosts and costs associated with deploying unreliable software.however traditional reliability metrics such as mean time tofirst failure mttf require models that support describingprobabilistic non deterministic and timed behaviour modelsfor which estimating such metrics can be prohibitive in time space and or precision.model checking is emerging as an effective software ver ification method.
in particular in the context of reliability quantitative guarantees on mttf and other measures can becomputed for complex models using the techniques developedin the area known as probabilistic model checking .these techniques input probabilistic models such as markovchains and markov decision processes and can assess quan titative properties through exhaustive exploration of the modelstate space and subsequent numerical analysis.applicability of probabilistic model checking for reliabilityassessment of complex models is threatened by the size ofthe model.
although state space reduction techniques exist they may still fail to prevent state explosion on complexenough models.
further even if the entire state space can beexplored its size typically impedes exact numerical calculation e.g.
gaussian elimination of reliability metrics so iterativemethods such as jacobi or gauss seidel that approximatemetrics must be used.
however these methods do not haveconvergence guarantees and when they do converge theymay do so intractably slowly.
the latter can be a problemfor reliability metrics of models with unlikely failures e.g.probability of failure in a fixed period below10 and canlead to iterations being cut short far from the actual value ofthe metric being estimated.in summary although probabilistic model checking mayseem to promise exact calculation of quantitative reliabilityproperties state space explosion and numerical methods can becomputationally prohibitive or result in poor approximations.despite these limitations probabilistic model checking canprovide bounds with confidence for reliability metricseven though the distance of these bounds to the real valuecannot be known.numerical analysis and to some extent state explosion canbe avoided using statistical techniques.
these apply statisticalinference over a finite set of sample executions extracted fromthe model.
variations of these approaches are usually referredto as monte carlo estimations.
when using these techniquesto estimate metrics such as mttf the population meanxisapproximated through an estimator such as the sample meanx .
of course such estimation is subject to statistical errorand thus it is crucial to understand how far and how likely theestimator deviates from the actual mean.the deviations from the actual value that result from thespecific samples used is usually conveyed in terms of statisticalerrors and confidence intervals.
bounds for statistical error andconfidence intervals can be computed based on the number ofsamples being analysed.
although significant progress for fastgeneration of random walks over models has been made sam ple generation can be very costly timewise even for analyseswith modest guarantee requirements due to the sheer numberof samples required.
for monte carlo reliability estimations the length of samples can be particularly problematic sincesampled executions must reach a failure in order to allowcomputing an estimator.
this may make sample generationfor high reliability systems intractable.in summary statistical techniques can provide approxima tions with measurable confidence intervals and error bounds.however in the presence of large models with rare failures the number and length of samples may make such techniquesintractable or lose guarantees over results.
.
c ieee icse san francisco ca usa602 in this paper we present an alternative to exhaustive modelexploration as in probabilistic model checking and partialrandom exploration as in statistical model checking whichmay counter some of the limitations of existing model basedreliability verification techniques.
our hypothesis inspiredon the pareto principle is that a carefully crafted partialsystematic exploration of system models can provide goodbounds on reliability metrics with lower computation cost.more specifically probabilistic model checking of a submodelof the system can bound the value of reliability metrics for thecomplete model in a cost effective manner.
furthermore it canproduce better approximations given equal time and memorybudgets than probabilistic and statistical model checking.we hypothesise that there is a gain to be had by identi fying a small probabilistically significant portion of the statespace considering all other states as failures and performingprobabilistic model checking on the resulting submodel.
theintuition is that in contrast to full model probabilistic modelchecking performing a probabilistic check on a portion ofthe full model allows for faster iterations of the numericalanalysis methods.
consequently more iterations can be per formed within the same time budget and for slowly convergingmodels a better approximation may be achieved.more specifically in this paper we present a novel auto mated technique for mttf estimation that combines simula tion invariant inference and probabilistic model checking.
weuse simulation to produce a set of traces that represent likelybehaviour of the full model.
these traces are used to inferinvariants of the state space explored during the simulation.a submodel containing all states that satisfy the invariant isconstructed and its mttf is computed using a probabilisticmodel checker.the technique we propose obtains lower bounds to realmttfs with confidence as full model probabilisticmodel checking and in contrast to statistical model checking .preliminary evidence shows that the lower bounds achieved for a fixed budget of time and memory are higher thanthose obtained by full model probabilistic and stochasticmodel checking for models with rare failures.
furthermore automated invariant generation seems to perform reasonablywell against domain expert provided invariants.the remainder of the paper is organized as follows.
insection ii we provide background.
in section iii we describeour approach to the mttf estimation.
section iv providescase studies illustrating the approach and comparing resultsto existing techniques.
in section v we present a discussionof our results and of related work.
finally we offer ourconclusions and discuss future work in section vi.ii.
backgroundmean time to first failure is a widely accepted metric forsoftware reliability.
the metric represents the time a client canexpect to operate a software system until it experiences its firstfailure.
in order to calculate such a measure practitioners basetheir efforts on afailure model which describes conditionsunder which the component is known to fail.
most often theseconditions are probabilistic in nature.
this failure behaviouris usually modelled with a markov chain.definition .
discrete time markov chain adiscretetime markov chain dtmc is defined by a tuple angbracketlefts s0 a r angbracketrightwheres v cis a finite set of states defined by mapping a finite set of variablesvto values ona finite subset ofz c.s0 sis the initial state.ais afinite set of action labels.r s a d s is thetransition relation where the transition target is defined by adistribution on target states.ris such that si s !a a !
d s such that si a r. that is thechoice of transition distributions is deterministic.complex dtmcs can be built compositionally using paral lel composition to model components that run asynchro nously but synchronise on shared actions.in order to assert intended properties of dtmc modelsseveral modal logics have been proposed.
pctl a prob abilistic extension of ctl is widely used to describe modelproperties.
pctl formulae differ from ctl in that insteadof predicating about properties that may hold globally or forsome execution paths they aim at quantifying the probabilityof witnessing traces that satisfy a given property.definition .
traces and probabilities given a dtmcm angbracketlefts s0 a r angbracketright anexecution traceonmis a nonemptyand possibly infinite sequence s0p0 s1p1 s2... suchthat for alli si sand there exists a unique si a rsuch that si pi .
the probability induced bya finite path is given bypr m producttext0 i length pi wherelength is the number of transitions in .
we notethe existence of a finite execution trace froms0tosnbys0 sn.
we will denote the infinite set of all possible tracesthroughmas m .reward structures are used to convey some sense of valueto dtmc traces.
for example a transition reward structurethat assigns a value of to each transition is a standard wayof defining overall time steps cost for the traces of a dtmc.definition .
reward structures given a dtmcm angbracketlefts s0 a r angbracketright atransition reward structureis a function s a s r .given a trace of a dtmcm and a reward structure overm thepath rewardof is the sum of the reward ofeach of its transitions.
we will abuse notation and note to note the path reward of based on reward structure .we will note send m wheresendis a set of states torefer to the possibly infinite set of all execution traces ofm but where they have been pruned so that the last state of eachtrace is one of those insend and no other state insendexistsin the trace before the end.
note that send m may containtraces of infinite length i.e.
those that never reach a state insendand therefore have not been pruned .definition .
mean time to first failure letm angbracketlefts s0 a r angbracketrightbe a dtmc sevent sbe a set of states frommrepresenting failure states and a reward structure overmmodelling time passage on transitions.
let time to firstfailuretfbe a random variable onr such that theprobability of the time to failure being equal toxis defined as603pr tf x summationtext send m xpr m .
themeantime to first failure mttf for failuresseventinm notedt sevent m is given by themathematical expectationof therandom variabletf.it is generally accepted to employexecution timerather thancalendar timefor mttf estimations .
while calendar timemeasures real time in terms of hours weeks etc.
executiontime is the time actually spent executing the software.
thisdistinction is important for reactive systems which may havelong idle times.iii.
approachthis section formally defines an approach to computingbounds to mttf of software models.
the approach is basedon calculating mttf for a partial systematic exploration of themodel s state space.
we first define what is meant by a partialexploration and show that mttf computed over these partialexplorations are guaranteed bounds to the mttf of the entiresystem model.
we then show how some partial explorationscan be specified declaratively through invariant properties thatdrive the exploration.
finally we show how these invariant driven partial explorations can be obtained automatically fromany given model without need for human intervention.a.
partial explorationswe refer to a partial exploration of a system model as asubmodel.
intuitively a submodel of a dtmcmis a modelthat retains a subset of the states and transitions ofm including and reachable from the initial state and in whichall other states inmhave been abstracted away into a new trap state.
formally a dtmc submodel is defined as follows wheresupp denotes thesupport setof the distribution that is the set of valuesxifor which xi definition .
submodels given a dtmcm angbracketlefts s0 a r angbracketright asubmodelofmis a dtmcm prime angbracketlefts prime s0 a r prime angbracketrightsuch thats prime s s0 s prime andr prime s prime a d s prime is such that for alla a1 for each a r prime r prime it must be the case thatsupp r prime anda for alls s prime for alla a it must be that r prime d s prime such that s a r prime r prime r d s such that s a r r for alls1 s2 s primesuch thats1 negationslash and for alla a it must be that r prime d s prime such that s1 a r prime r prime s2 supp r prime r d s such that s1 a r r r s2 r prime s2 for alls1 s primesuch thats1 negationslash and for alla a it must be that r prime d s prime such that s1 a r prime r prime r d s such that s1 a r r r prime summationtexts2 supp r prime r s2 .submodels are key to our approach since they conserva tively approximate mttf.
that is the mttf of a modelmis always greater or equal to the mttf of its submodels.theorem .
submodels bound mttf letmandm primebetwo dtmcs with state spacessands primeand such thatm primeis!
!
!
!
!
!
!
!
!
!
!
!
!
!
fig.
.
example partial exploration of a state spacea submodel ofm.
ifsf sare all the failure states inmthent sf s prime m prime t sf m .proof note that for every trace in the complete model iteither exists completely in the submodel or the submodel con tains only a prefix that is extended by the state.
since rewardstructures are based on transitions every trace in the full modelaccruesat leastas much reward to the failure state possibly as the corresponding trace or prefix in the submodel.hence these prefixes contribute tot sf s prime m prime atmost what their extensions inmcontribute tot sf m the above result entails that if computing mttf of a systemmodel is intractable it can be conservatively approximated onany of its submodels.b.
automatic submodel generationalthough any submodel will provide a lower bound formttf the key to a tractable reliability estimation technique isto identify a submodel for which its mttf can be computedwithin a reasonable time budget and for which the resultingbound is a reasonable approximation to the mttf of the fullmodel.
of course and independently of the fact that the mttffor the full model is unknown this is a problem for whichcoming up with an exact solution i.e.
the best submodel is intractable .
in this section we discuss a heuristic forautomatically constructing submodels that can provide betterbounds for reliability at lower computation cost than fullmodel checking and monte carlo approaches.our approach adopts a heuristic based on the reasoningthat the submodel construction strategy should aim to identifya portion of the model that is probabilistically dense thatis a small submodel for which the probability of reachingthe trap state in a fixed time is low.
such models willcontain probabilistically likely loops that delay the tracesfrom reaching the submodel boundary hence contributing toa higher reliability bound.the problem of finding a probabilistically dense submodelis np hard.
our alternative approach attempts to approximatesuch a submodel through bounded simulation.
hence wesimulate several traces over the full model.
the resulting set offinite traces if sufficiently large and consisting of sufficientlylong traces is likely to cover a good part of a probabilisticallydense submodel.
these traces form the basis for building604our submodels.
the smallest submodel that includes the setof states and transitions covered by the simulated tracescan be constructed easily by simply adding any non visitedtransitions between any two visited states abstracting all non visited states into the trap state.
figure shows sucha construction where solid lines are transitions covered bythe simulated traces and dotted lines are transitions in themodel that were not covered.
states outside the boundaryhave not been covered and would be abstracted away inthe submodel.
however such submodels are likely to haverelatively short traces that escape the submodel see paths0 s2 s10 ...in the figure .
these short traces contributea relatively high probability of escaping the submodel theshorter the prefix the larger the probability of the set ofextended traces reducing the submodel s mttf.
note thats10falls back within the boundary tos6with high probabilityand including it would increase the submodel s mttf.
this isconsistent with our experimentation in where we observedthat submodels generated with a breadth first search strategytend to approximate reliability measures better.in our approach rather than adopting a syntactic notion ofbreadth first traversal for extending the submodel determinedby a simulation of the full model we take a more semanticapproach based on the attributes of states visited during thesimulation.
we compute state invariants based on the statesvisited during the simulation and then add to the submodelany states and transitions that satisfy the invariant.
in this way we expect to add behaviour that although different to whatwas simulated represents variations in terms of symmetries race conditions and independent events and contributessignificantly to the probabilistic weight of the submodel.we now formally define our submodel construction method.we start with the notion of invariant of a set of traces.definition .
invariant given a dtmcm angbracketlefts s0 a r angbracketright and a set of finite execution tracestobtained fromsaid model aninvariantofmthroughtis a state predicate on the variables ofmsuch that for every executiontracet s0p0 s1p1 s2... sn t it holds that i n si .an invariant then induces a unique submodel as follows definition .
invariant driven submodels letm angbracketlefts s0 a r angbracketrightbe a dtmc and an invariant aninvariant driven submodelinduced by is a submodelm prime angbracketlefts prime s0 a prime r prime angbracketrightofmsuch thata each states prime s primeis suchthats prime b for eachs prime1 s prime s prime1 negationslash s0 s0 s prime1 andfinallyc for all statess prime2 s s primesuch that there exists prime1 s s prime1 a r rwith r s prime2 it is the case thatm s prime2 negationslash .
in other words if a states prime2not in the submodel isdirectly reachable from a states prime1in the submodel it must bethe case thats prime2violates .
the submodel is thus maximallyconnected from the initial state through the invariant .our approach aims at automatically obtaining invariants.
tothis end we produce probabilistically driven walks over thefull system model recording the states i.e.
variable valua tions traversed.
we use daikon an invariant inferenceengine to obtain predicates that hold over all traversed states.iv.
validationin this section we set out to answer three questions in orderto validate our approach.q1 can our approach when compared to model checkingover full explorations produce higher bounds in less time formttf of system models with rare failures?q2 can our approach when compared to monte carloapproaches produce higher bounds in less time for mttfof system models with rare failures?q3 how do the mttf for submodels compare when thesesubmodels are generated from automatically inferred invariantsas in our approach against manually generated ones?q1andq2aim at comparing our proposal with establishedapproaches to reliability estimation to evaluate if our approachcan complement existing techniques.q3aims at assessing theadded value of automatic techniques for obtaining submodels.a.
methodologyfor each case study taken from the literature we builtdtmc system models to describe behaviour modelled failuresas state formulae and defined an appropriate reward structure.we used the same input for all mttf estimation techniques.we ran our approach for all case studies for several automat ically generated invariants varying the number and length oftraces used for invariant inference.
we used daikon v4.
.
configured to produce invariants that are conjunctions of termsof the formx y wherexandyare either variables inthe model or integer constants and .
theinvariants we obtained were used to automatically build anobservero a dtmc that monitors the validity of the invari ant.
this observer when composed with the system modelm synchronises with all actions and forces transitioning intothe trap state whenever the destination state of the intendedtransition would result in an invariant violation.forq1we used a modified version of prism v4.
.
toperform probabilistic model checking to estimate mttf bothfor the full state space and for its invariant driven submodels.modifications allow for batch trace generation used for in variant inference and time and memory use tracking used forgenerating intermediate mttf results and for timing out whentime budget is up .
intermediate mttf results were generatedfor visualising convergence rates.
prism was deployed on an8x core intel xeon cpu .
ghz with 8gb ram.prism provides different numerical methods for rewardcalculation.
we compared computation of mttf of the fulland partial explorations for the jacobi gauss seidel andpower methods as well as several optimisations over thejacobi and gauss seidel methods.
due to space limitations weonly report on results obtained with the backwards variation ofthe gauss seidel method bgs which proved to be the mosteffective method for full model probabilistic model checkingin terms of bounds obtained for time budget.prism runs were considered complete when any of the fol lowing criteria held eithera theabsolutedifference betweenresults of successive iterations of the numerical method wasless than .
relative differences are not adequate because of605slow convergence which causes iterative methods to cut tooearly .
alternatively b running time reached hours orc memory was exhausted.
note that the time measured includesonly the execution of the numerical methods.
this allows forconvergence analysis and favours full model exploration asthe time spent on construction of the model state space isnot considered we comment on execution time for submodelgeneration later in the experimental results subsection .forq2monte carlo simulations were generated usingthe same version of prism and the same hardware asq1.however note that while our approach produces lower boundsto mttf with confidence but for which precision percentual difference between the estimation and the actualvalue is unbounded monte carlo produces estimations withvarying degrees of confidence but for which precision can bebounded.
consequently we performed statistical model checksfor a range of confidence and precision values.for monte carlo approaches to be successful all randomlygenerated traces must eventually fail and enough traces mustbe generated in order to guarantee estimations with a fixedprecision and confidence.
setting a trace length horizon forthe simulator to ensure all traces fail is typically done basedon an estimation of the mttf.
we used the mttf estimationsobtained inq1to set this horizon for each case study.in addition to comparing probabilistic model checking ofsubmodels against monte carlo simulations of the completemodel we compared probabilistic model checking againstmonte carlo simulations over the same submodels.finally q3uses the same setup and mttf estimationapproach based on automatically inferred invariants as inq1.manually produced invariants for submodel generation wereput forth before any of the experiments were performed.hence the manually proposed invariants were not tainted byknowledge gained from the automatic approach.
the mainheuristic for coming up with the invariants was the use ofof necessary and more likely conditions for failures.b.
case studiestandem queueing network the first case study is a tandemqueueing network based on .
queueing systems have beenextensively studied in queueing theory and analytical solutionsfor some variants exist.
in the case of this model generalqueueing models do not apply.
generating an ad hoc analyticalformulation would require extensive expertise and time.the system consists of two process queuescandmofgiven capacities.
clients queue processes for execution in thefirst queue while it is not full.
the queue may either route aprocess to the second queue after a probabilistically chosentime elapses or it might choose to deal with the requestitself.
the behaviour of this first queue is governed by twodifferentphases the difference between the phases is givenby the probability with which it will choose to route to thesecond queue or deal with requests.
the second can serviceits requests after a probabilistically chosen time elapses.
afailure is observed when both queues are full.
the capacityof the queues is fixed at each.
clients are less inclined i.e.
they take more time in average to enqueue processes asthe free capacity of the queues decreases.the reliability metric to be estimated is the mean operationaltime until the first failure.
consequently the reward structure assigns the value1to every timing transition.
the state pred icate that captures failure isclic clim 1200andcomputing the metric amounts to calculating the expectationof the accumulated reward before reaching a state satisfyingthe state predicate.bounded retransmission protocol the second casestudy models a robust communication protocols thatattempts to ensure delivery of data the bounded retransmissionprotocol brp .brp is a variant of the alternating bit protocol which allowsfor a bounded number of retransmissions of a given chunk i.e.
a part of a file .
the protocol consists of a sender a receiver and two lossy channels used for data and acknowledgementsrespectively.
the sender transmits a file composed of a numberof chunks by way offrames.
each frame contains the chunkitself and three bits.
the first bit indicates whether the chunk isthe first one the second one if it is the last chunk and the thirdbit is the alternating one used for avoiding data duplication.the sender waits for acknowledgement of each frame sent.the sender may timeout if either the frame or the corre sponding acknowledgement are dropped.
when this happens the sender resends the frame and does so repeatedly up to aspecified retry limit.
if the limit is reached and the transmissionis terminated the sender may be able to establish that thefile was not sent if some chunks were left unsent or itmay not know the outcome if the last frame was sent butno acknowledgement was received .
in any case the sendermay send a new file resetting the retry count.
a maximum of256 retransmissions are attempted per file before the sendergives up and aborts transmission of the file.
once a file issent successfully or its transmission fails the system waits foranother file to be sent.protocol clients send files one at a time.
each of these files isof a different size in number of chunks .
the size is selectedprobabilistically for each file between just a few and 1500chunks.
exceedingly large or small files are modelled to beless likely to be sent than those of average size.we wish to estimate the mean time to the first failure wherefailure is defined as the sender failing to send a completefile incomplete or not being able to establish if a file wassent successfully unknown .
consequently the state predicatedescribing failures isincomplete unknown.
the definitionof time for this case study aims at establishing how many datapackets can be expected to be sent successfully before failure.c.
experimental resultswe now present some of the experimental results obtainedfor the three research questions presented above.
the modelsused and complete experimental results can be found at when comparing probabilistic model checkingof both full and partial models we are interested in considering606 1e 1e 1e 08estimated mttf operational time tandem queue mttf estimation submodel size states verification time seconds estimated mttf operational time fig.
.
results of analysis of tandem queue for different sized submodels backwards gauss seidel method.
35000submodel size states tandem queue submodel sizes sample size number of traces trace lengthsubmodel size states fig.
.
tandem queue submodels sizes for different sample size and tracelength parameters.table iselection oftandemqueue submodel sizes and inv ariants fordifferent parameter configurations.traces length statesinvariant10000114clic clim clic state clic state1000050112690clic clim state 14134clic clim state 16086clic clim state 23388clic clim state 22486clic clim state 20932clic clim state 25228clic clim state 24538clic clim state 24882clic clim state 26424clic clim state 23686clic clim state 26182clic clim state 31902clic clim state 29926clic clim state 30674clic clim state 23910clic clim state 29424clic clim state 29924clic clim state 29926clic clim state 27174clic clim state 27460clic clim state 9the impact between the inferred invariant the resulting sub model s size and the value of the mttf estimation obtainedfrom it.
we are also interested in gaining insight on combi nations of trace length and number that are likely to yield thebest overall result.for the tandem queue case study the estimated mttfcalculated in hours over the full model was4.
.the full model comprises .
107states.
regardingcomputations over submodels we report on mttf estimation figure submodel sizes figure and invariants obtained table i for various settings of numbers and lengths of traces.note that we report here on a subset of the values obtained however non reported data is in line with the trends shown.the first figure shows for different automatically generatedsized submodels the estimated mttf over a logarithmicscale along with how much time it took for the calculation tofinish.
executions that finished before the hour timeout areflattened on the mttf axis at the time the result was reached.it is noteworthy that none of the automatically obtainedsubmodels is larger than35000states comprising roughly0.
of the states of the complete model.
despite havingexplored only such a small percentage of the full model the obtained lower bound for mttf is quite large in somecases possibly sufficient to argue for high system reliability mttf isat leastin the order of1.
.
although verysmall submodels do not provide good bounds larger submodelmttf estimations increase dramatically quickly rising to the7 107maximum mttf witnessed which is a full two ordersof magnitude beyond the estimation for the full model.an important question is if good submodels can be ob tained in a consistent fashion regarding trace quantity andlength parameters of the simulation phase.
figure showsthat such submodels are easily attainable for this example.experiments with trace length below3000do not consistentlyproduce rich enough models that yield good mttf estimates.unsurprisingly small sample sets are also inconsistent in theirresults.
however once the sample set size parameter is set toat least6000samples the submodels produced consistentlyyield large mttf estimates.
in summary for this case studya minimum of6000samples of traces at least4000steps longare necessary for consistent results.
furthermore increasingthese parameters does not yield clear advantage in terms ofthe final mttf estimation.
both figures also show that resultsbecome more stable as these parameters are increased.state space size alone is not the only important factorwhen evaluating the effectiveness of the approach.
for agiven size many submodel of that size exist and not allof them may be effective.
preliminary work has shownthat submodels obtained through depth first search dfs explorations yield very poor results.
although breadth firstsearch bfs obtains higher mttf lower bounds than dfswhen used as a submodel generator it performs poorly againstour approach as the state space that it explores is not asrelevant.
for example our approach using10000traces10000states long one of the best performers obtains a27460state sized submodel which is characterised by the invariantclic clim state .
consider a similarlysized bfs generated submodel of28000states.
the tandemqueue model allows four different actions push fwd svc1 svc2 .
conservatively assuming at most two actions enabledat each state an equal sized bfs submodel would explore atmost log2 15levels deep.
such a submodel wouldonly allow for very limited behaviour.
if each transition levelgenerated a new state queues of no more than elementscould be generated by such a submodel.
of course it is notalways the case that a new state is generated.
in fact a bfsexploration that allows for elements per queue results ina32000state submodel.
the mttf obtained through such asubmodel is very far from the results we obtain.regarding potential overhead of trace generation and invari ant inference memory consumption is negligible with respect607 1e 1e 1e 08estimated mttf packages sent brp mttf estimation submodel size states verification time seconds estimated mttf packages sent fig.
.
results of analysis of brp for different sized submodels backwardsgauss seidel method.
600000submodel size states tandem queue submodel sizes sample size number of traces trace lengthsubmodel size states fig.
.
brp submodels sizes for different sample size and trace lengthparameters.to representing the state space of the full model as only onerelatively short trace needs to be kept in memory at a time.timewise analysis of10000traces of length10000took lessthan an hour.
accounting for this hour in the verification timebudget the submodel that yielded the highest mttf lowerbound would have achieved a result of 107in hours.although not intended to be shown to developers we reporton some of the automatically inferred invariants in table i.the discovered invariants deal with bounding the size of bothqueues while the variablestateencodes whether the queuesare full or not and the phase the system is in at the time.
it isnoteworthy that although it is intuitive that an invariant shouldbound the queue sizes it is unlikely that a human would comeup with the particular bounding values used.for the brp case study similar results were obtained andare shown in figures and and table ii.in contrast to the prior case study we were unable to obtainthe mttf for the full model due to state explosion thatexhausted available memory.
however observations prior torunning out of memory showed that the full model containsat least30million states which means that the submodelsanalysed represent at most2 of the size of the full model still a very low percentage.
furthermore the highest mttfbounds were obtained for submodels starting from400000states less than1.
of the full model yielding an mttf inthe order of2.
.
this result is most significant becauseof the impossibility of estimating mttf for the full model.note that around the400000and500000states mark thereare both estimations that provide very good bounds and thosethat yield not so useful ones.
interestingly those that do notperform well arise from submodels obtained through invariantsinferred from sets of traces shorter than7000states long while sets of longer traces perform very well.
this showsthat appropriate trace length is critical to the final mttfestimation.
as before a similarly sized submodel obtainedthrough bfs does not provide such higher mttf lowerbounds.
one of our best performers at10000traces10000states long produces a submodel392786states in size which with eight brp actions and conservatively assuming threeenabled at any time results in a bfs submodel of depth log3 which models very few frames beingsent.
in fact a bfs like submodel that allows only for5framesto be sent per file comprises 400000states and yields anmttf of only40.figure depicts information related to the possibility ofobtaining useful submodels.
it can be seen that it is quiteeasy to obtain such submodels without many restrictions onexperiment configuration.
in fact the configurations for thiscase study behave much more steadily than with that of thetandem queue.
sets of4000traces of at least7000states seemto be enough for obtaining good estimates.
further increasesof these parameters yield larger and slightly better performingmodels and this increase is much smoother hence predictable than is the case for the tandem queue submodels.as in the other case study trace generation and invariantinference incurs an overhead.
in this case since the model ismore complex this analysis can take up to additional hours.reducing the verification time by these hours the estimatedmttf would have been still large about2 .
recall thatthis overhead was not included in measured time to allowgraphs to show convergence speed of numerical analysis.as before we show for reference some of the inferredinvariants.
the variablesfilesize iandnrtrdescribe thesize of the file being sent how many frames have been sentfor that file and the number of retries attempted respectively.other variables such assab rab bsandfsencode the bitalternation in the protocol.
the invariants obtained establishrelationships between variables that seem unrelated makingthem quite unintuitive even for a domain expert.what both case studies and experiments indicate is that through careful partial exploration of the model we can obtainuseful bounds for mttf estimation with very low percentages .
of actual state space exploration.
further submodelsthat yield these results also converge very quickly muchbefore the hour timeout to good mttf results.
whilethe estimation does constantly improve during the rest ofthe hours it does so at a slower pace.
this is goodnews as even with the trace analysis overhead good mttfresults can still be attained under the same time budget.
fromthese results it follows that for these case studies effort intoestimating mttf through automatically obtained submodelsthrough model invariants of the full model pays off.it must be noted that it is possible that theactualmttfis much larger than any of those obtained.
of course weare always limited by the fact that the actual mttf cannotbe calculated neither with partial nor full models.
it can be608table iiselection ofbrpsubmodel sizes and invariants for different parameter configurations.traces length statesinvariant10000135srep nrtr srep filesize srep r srep rrep srep k srep l bs s ab bs fs bs ls bs fr bs lr bs br bs r ab bs recv s srep i s srep s i srep i1000050166282s srep nrtr filesize i r rrep k l s k s l srep filesize srep i srep r srep rrep nrtr filesize nrtr i filesize r filesize rrep filesize k filesize l r l 333099s srep nrtr filesize i r rrep k l s k s l srep filesize srep i srep r srep rrep nrtr filesize filesize r filesize rrep filesize k filesize l r l 392786s srep nrtr filesize i r rrep k l s k s l srep filesize srep i srep r srep rrep nrtr filesize nrtr i filesize r filesize rrep filesize k filesize l r largued though that it is often the case that the exact mttfvalue is not needed as such rather satisfying a minimumdegree of reliability is a sufficient guarantee.
hence methodswhich provide higher lower bounds faster are useful.question experimentation to answer this question is notstraightforward due to the problem of generating sufficientfailing simulations to ensure given precision and confidenceparameters.
we first aimed at performing a straightforwardstatistical analysis of the model.
a first experiment wasdesigned requiring a result precision of99 .
as is standardfor statistical analyses we also required a95 confidence.a straightforward calculation of the necessary sample sizebased on the chernoff bound determines that a totalof 60000samples are necessary which does not seemexcessive.
however recall that each sample must eventuallyfail.
for systems with rare failures this means that samplesmay be extremely long.
through trial and error and based onthe bounds obtained inq1 we tried to determine the minimumlength for samples to consistently reach failure states.
for thetandem queue full model even samples as long as4 108do not consistently fail.
considering that generating a sampleof such length takes minutes generation of the full60000traces required leads to a year period for sample generation.a similar situation is found upon analysis of the brp model.relaxing the precision requirement to95 reduces thesample generation cost to month.
further relaxation to90 still requires a week of execution.
in fact if we were to set a24 hour budget for sample generation the precision obtainedwould be70 .
that is the mttf estimate would be up to away from the true mttf value with a95 guarantee.note that this is a very conservative estimate as it is unlikelythat all traces of length4 108generated in the hourperiod will consistency reach failure states and possibly muchlengthier traces will be needed.to overcome this limitation of standard monte carlo verifi cation we tried a variation of wald s sequential testing .this procedure generates samples while at the same time itdetermines whether more samples are necessary or not.
as aresult of this online estimation it might require less samplesthan those mandated by the chernoff bound although itcannot be stated beforehand how many samples will be neededexactly.
this optimization does not eliminate the need forsamples to reach property determining states so sample lengthremains a problem.
we attempted to perform this analysistruncating generated samples at length4 108and treatingthem asfailingsamples once they reached this threshold.
thisis a similar strategy as the one used in our approach anythingbeyond the submodel is a failure .
however this procedureyielded no results after hours of execution indicating thatthe sequential testing still needed more evidence.furthering this strategy of over approximation of failuresin monte carlo verification we generated samples over thesubmodels with highest mttf obtained inq1rather thanthe full model.
however the problem of producing samplesthat consistently fail persisted failing to provide an mttfin the budgeted time.
these results suggest that monte carloapproaches may be unsuitable to answer reliability questionsin systems with high mttf i.e.
rare failures .question in this section we compare the results obtainedwhile answeringq1with the results a practitioner might obtainby specifying invariants herself based on her knowledge ofthe model.
prior to experimenting on automatically generatedinvariants we analysed the models and came up with at leastone invariant for each one.
these invariants were selectedbased on our understanding that their negation is a necessarycondition for failure.for the tandem queue case study we established theinvariant to be that the total number of enqueued processesglobally in both queues is less thanc and ran experiments fordifferent values ofcranging up to the total capacity of thequeueing system c .
a failure entails that the invariantdoes not hold forc c and that forc cthe resultinginvariant driven submodel is exactly the whole model.
in ourexperiments we found that there where multiplecvalues forwhich the invariant resulted in a significantly higher mttfthan the mttf estimated for the full model.
in table iii resultsare presented for various invariant driven submodel parametervalues together with estimated mttf and computation timeusing the bgs method.from the table it follows that the best mttf is obtainedfor the submodel which considers up to processes queued mttf .
.in the case of the bounded retransmission protocol casestudy a parametric invariant chosen was that the number ofretries performed while transmitting a single file was lessthanmaxretries.
we ran experiments for different values ofmaxretriesranging up to the true maximum number of retries .
a failure entails that the invariant does not hold formaxretries .
forretries maxretriesthe resultinginvariant driven submodel is the whole model.again we show a selection of submodels ranging from thevery small upwards to almost the complete model.
results forthese experiments are depicted in table iii.estimation results are even more significant than for theprevious case study considering that analysis of the full modelwith retries was not possible within the memory budget.609table iiiexperimental results for tandem queue 1200processes andbrp 256retries mean times to first failure times.csizebgsmttftime202398 st0.
.
s6560 tr408778 st1.
.
s24280 tr6019158 st1.
.
s53200 tr8033538 st1.
.
m93320 tr10051918 st1.
.
h144640 tr12074298 st5.
107to207160 tr140100678 st4.
107to280880 tr160131058 st3.
107to365800 tr180165438 st2.
107to461920 tr200203818 st1.
107to569240 tr9004067118 st8.
105to11381440 tr160011219198 st4.
105to31407194 tr240014362898 st4.
105to40213194 trretries sizebgsmttf time1366915 st1.
.
h489574 tr2480460 st1.
107to646758 tr5821095 st1.
107to1118310 tr101388820 st6.
106to1904230 tr505930620 st1.
106to8191590 tr15017285120 st4.
105to23909990 tr25028639620 st2.
105to39628390 tr256n a stn aoomn a trhowever the trend indicates that augmenting the number ofretries considered does not yield better mttf and in fact avery low number of retries gives a much higher mttf.it is interesting to note that for relatively small submod els e.g.c 80on the tandem queue case study andmaxretries 2for brp the estimated mttf is much higherthan the mttf computed over the complete model.still while the manual invariant approach did provide usefulbounds it turns out that the best mttf values generatedby the automatic approach obtains slightly higher boundsfor the same time budget.
for the tandem queue study thebest automatically estimated mttf is of 107against .
.
for the brp case study the best automaticestimation is .
107versus .
107when manualintervention is applied.v.
discussion andrelatedworkwe have presented a fully automated technique for mttfestimation of system models.
experimental results have shownthat this approach may provide more useful mttf estimationsthan both standard probabilistic model checking and montecarlo verification.
however two parameters exist that need tobe set for the approach to work the size of the simulationset and the length of the simulated traces.
good news isthat our experimentation has shown that at least for theexamples studied very good results can be obtained throughrelatively small set of short traces.
although the elaborationof guidelines on how to set the number and length of tracesis beyond the scope of this paper results show that theremay be a broad combination of parameter values for whichhigh mttf results are obtained in reasonable time.
further overshooting these parameters does not have an dramaticimpact in the resulting submodel size.
it is important to notethat exploration of an appropriate parameter space can be doneconcurrently taking as the final mttf estimation the highestof the bounds obtained.
full model probabilistic checkingcannot exploit concurrent computation in such a way.
montecarlo verification can be applied concurrently however webelieve that the significant time cost for sample generationwould not be outweighed by concurrent execution furtherexperimentation is needed to address this point.as was previously mentioned most probabilistic modelcheckers provide functionality that mayeither reduce the time required to obtain results or reduce thememory footprint required for verification such as symmetryreductions lumping and several numerical methods.all these optimizations are orthogonal to the model check ing procedure itself.
our work relies on probabilistic modelchecking and the experiments were run on prism whichimplements some of these optimizations.monte carlo verification of models where the requiredsize or verification time have determined exhaustive modelchecking to be intractable statistical simulation has proven tobe an effective technique.
as was mentioned in section iv an important issue with simulation approaches is that theytend to work well mostly in the case that the specifiedproperties are bounded in time i.e.
when these propertiescan be written in the form u t for a fixedt.
this isso because estimation of the random variablex by meansof a sample of traces irequires that the question of whetherm i or not be answered in a definite way for eachtrace iin the sample set.
if the formula is temporallybounded then termination is guaranteed when evaluating itstruth for the traces but for temporally unbounded formulaesuch termination is threatened.in such cases generating traces with acceptable lengthbounds that answer the property definitively can be veryunlikely.
to address this problembiased sampling has been studied.
however bias to sampling must bedone manually resulting in an impact on the analysis resultsthat cannot be quantified in general.
the result obtained byour approach is guaranteed to be a true bound to mttf.recent work by younes et.
al.
proposes two novelmonte carlo approaches that do not rely on biased sampling.however one of them may require an inordinate numberof samples to produce results while the other relies onreachability analysis which requires the full model to beconstructed relinquishing one of the key advantages of montecarlo model checking over probabilistic model checkingit should also be noted that monte carlo approaches aretypically inadequate for models that exhibit non determinism.a workaround is to transform non determinism into a proba bilistic choice introducing bias.
we believe that our proposedapproach can be straightforwardly adapted to be applied tonon deterministic dtmcs.
this remains future work.the analysis of system behaviour that exhibitsrareyet rele vant e.g.
failures events is the subject of focused study withinthe simulation community as well.
a technique that is usuallyused in conjunction with stochastic processes that have rareevents is that ofimportance sampling .
roughly speaking the idea of importance sampling is to replace the originalprocess s distribution for another more likely to generatethe originally rare event during the sample generation.
thedistribution replacement is chosen so that results from analyses610for the new distribution can be translated back to results validfor the original distribution.
although a promising approach finding suitable replacement distributions is a complex and ad hoc task for which further research and expertise is necessary.another promising simulation technique that also focuseson rare events is that ofsample splitting most notably therestart implementation which roughly rather thanstarting each simulation from the initial state it does so froma statesvisited in a previous simulation and from whichreaching a rare event is more likely.
the likelihood of reachingstatesfrom the initial state is taken into account for producingthe final analysis results.
key to the application of thesetechniques is making appropriate decisions on where to restartsimulations these decisions demand deep understanding ofboth the model and the underlying splitting technique.finally common to both the monte carlo approach andthe simulation techniques discussed is the fact that they areinherently statistical results.
as such there is always a nonzeroprobability that the results obtained are completely off themark.
further reducing this error probability may requireexcessive amount of additional traces to be sampled in orderto obtain the guarantee.
our technique though conservative inthe bounds it obtains is definitive in its answers.vi.
conclusions andfurtherworkin this paper we have proposed an approach to reliability es timation of system models.
the approach is a novel combina tion of simulation invariant inference and probabilistic modelchecking.
we report on experiments that suggest that reliabilityestimation using this technique can be more effective than fullmodel probabilistic and statistical model checking for systemmodels with rare failures.we believe the notion of reliability analysis over partialyet systematic explorations offers an alternative to and hencecomplements exhaustive model exploration as in probabilisticmodel checking and partial random exploration as in statisti cal model checking.we believe the experimental results presented in this paperare promising.
however further experimentation is due.
inaddition this work opens up two fronts that need to befurthered to develop more effective reliability estimation ap proaches.
one front is generalising the approach to supportestimation of other properties related to stochastic behaviour.in addition the generalisation of the kinds of models for whichthis technique can be applied is also of interest e.g.
markovdecision processes that extend discrete time markov chainsby introducing nondeterminism .additionally another area that calls for future work is betterunderstanding the relationship between the simulated set oftraces both its size as the trace length and the submodels thatresult from them.
this understanding should lead to heuristicsfor setting appropriate values to these parameters.acknowledgementsthis work was partially supported by grants pict pip ubacyt w0813 erc stg pbm fimbse andmeals .