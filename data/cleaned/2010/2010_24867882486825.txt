comparative causality explaining the di fferences between executions william n. sumner xiangyu zhang department of computer science purdue university usa wsumner xyzhang cs.purdue.edu abstract we propose a novel fine grained causal inference technique.
given two executions and some observed di fferences between them the technique reasons about the causes of such differences.
the technique does so by state replacement i.e.
replacing part of the program state at an earlier point to observe whether the target di fferences can be induced.
it makes a number of key advances it features a novel execution model that avoids undesirable entangling of the replaced state and the original state it properly handles di fferences of omission by symmetrically analyzing both executions it also leverages a recently developed slicing technique to limit the scope of causality testing while ensuring that no relevant state causes can be missed.
the application of the technique on automated debugging shows that it substantially improves the precision and e fficiency of causal inference compared to state of the art techniques.
i. introduction explaining why something happened is a subtle task philosophers have debated the notion of causation for centuries .
one common thread among the myriad approaches is that they involve comparing a world in which that something happened to others in which it did not.
many software engineering techniques take similar approaches i n explaining software behavior.
for example in probabilist ic fault localization a set of failing runs is contrasted with a set of passing runs to provide probabilistic insigh ts into the cause of the failures.
compared to techniques that d o not rely on comparison to explain software behavior such as program slicing these techniques are more precise as th ey use comparison to trim unnecessary information.
one classic fine grained comparative technique for identif ying causes when one execution e.g.
a buggy execution di ffers from another e.g.
a similar correct execution is zeller s delta debugging approach .
it is capable of reasoning about causality at the granularity of individual instructi ons and variables generating much more informative and precis e failure explanations compared to other techniques .
the technique involves replacing part of the state in the correc t execution with that from the buggy execution and determinin g whether such replacement induces the failure in the modified execution.
however due to the complexity of program state e.g.
inter connected data structures in the heap pointer s and external resources it faces many problems in practice.
in particular entangling the states from both executions all ows them affect each other in undesirable and unexpected ways leading to poor failure explanations.
more discussion of th e limitations of the technique can be found in section ii.in this paper we propose a novel fine grained causal inference technique.
given two executions and some observed differences between them the technique can precisely reason about the causes of such di fferences.
while the technique reasons about causality through state replacement it make s three key advances.
it features a novel execution model that avoids undesirable entangling of the replaced state and the original state such that the precision of causal inference c an be substantially improved.
it is capable of handling execut ion omission errors by analyzing both executions symmetricall y. it also leverages an existing slicing technique called dual slicing to limit the scope of causality testing while ens uring no relevant state di fferences can be missed.
as a result the efficiency is substantially improved.
our main contributions are highlighted as follows.
we first thoroughly discuss the limitations of the state of the art fine grained causal inference technique that has bee n used for many years.
we especially study the problems in state replacement.
we propose a novel causal inference model that is symmetric and comparative.
we declare the goals of the model which reflect the user s intention when reasoning about software behavior by comparison.
we propose a novel realization of the model.
it leverages dual slicing to ensure relevance of the causes and limit the scope of causality testing.
while it makes use of state replacement to determine causality a novel execution mode l and its approximation are developed to avoid the undesirabl e entangling of the state from both executions.
we implement and evaluate a prototype.
we apply the causal inference engine toward failure explanation for real world bugs including all the reported bugs for tar make andgrep in a one year period.
comparison against the causal inference engine from the most recent improved delta debugging and dual slicing techniques shows that our technique has substantially improved the e fficiency and effectiveness of failure explanation.
ii.
causal stateminimizationin deltadebugging delta debugging is a classic debugging technique that can minimize failure inducing inputs or the faulty interna l program state essential to reproducing a failure .
the original work first contrasts a buggy execution with a simila r correct execution to determine state di fferences .
it then performs causal state minimization csm to determine978 .
c ieee icse san francisco ca usa272 1x input 2y input 3z input 4ify z y 6else y y 7print y a x y z if false y print b x y z if false y print c braceleftbigg y mapsto z mapsto bracerightbigg d e fig.
a a program.
b c executions with di ffering input.
d csm.
e dual slice.
symbols diamondand diamondsoliddenote the cause point and e ffect point respectively.
the set in d represents the causal state set.
the minimal subset of state di fferences essential to reproducing the failure.
csm involves performing the correct execution up to a point of interest preceding the failure called the cause point replacing a subset of program state with state from the buggy execution and continuing this patched execution to determine whether the failure can be induced.
if so the subset is called a causal state set orcause set .
the technique makes use of a generalized binary search to enumerate and test different subsets until it identifies the minimal cause set.
sumner et al.
recently combined delta debugging with more precise execution alignment techniques to improv e its robustness precision and e fficiency.
by applying csm inductively a causal chain or summary of a failure can be computed comprising a sequence of the minimal causal state sets computed for a sequence of execution points leading fro m the root cause to the failure .
example.
consider the simple program presented in fig.
.
this program reads three integers re defines one of them an d then prints it.
in the execution of b the user inputs and the program prints .
in contrast in the execution c the user inputs and the program prints .
suppose that execution c is buggy.
given the buggy output on line called the effect point we apply csm to determine what state on line called the cause point actually caused the buggy output.
the cause and e ffect points are respectively marked in the figure as empty and filled diamonds in d .
note here the term buggy is a generalized notion as there is not a faulty statement per se.
any behavioral difference between the executions may be considered buggy and we are interested in what caused these di fferences.
the discussion and the technique are universally applicable for cases where tr ue faults cause the behavioral di fferences.
csm repeatedly replays execution b up to line .
each time it then replaces a subset of state with state from execu tion c to identify a subset su fficient to produce y mapsto within execution b .
for instance replacing on line the variable value mappings y mapsto and z mapsto in execution b with y mapsto and z mapsto from execution c yields y mapsto on line .
thus the process identifies that the values of yand zare buggy on line in execution c leading to the buggy output.
fig.
1d presents the causal state set on line along with relevant program dependences for comprehension.
the computation continues in order to determine whether a smaller causal set can be identified.
if not the identified minimal set will be reporte d. if we desire a summary of the failure the current causepoint becomes the new e ffect point and the identified causal state set becomes the new target buggy state.
the algorithm then continues to compute the causal state set for a precedin g new cause point until no such sets can be computed .
limitations.
delta debugging and its recent improvements all use csm.
while prior research demonstrated the e ffectiveness of these techniques we find that inherent limitations of csm often lead to low quality failure summaries.
next we discuss these limitations in de tail and motivate the need for a new causal inference engine.
confounding caused by partial state replacement the first problem with csm is that replacing only a subset of the state in an execution can induce new behavior that was not present in either of the original executions .
we call this problem the confounding of partial state replacement .
the introduced new behavior can a ffect the validity of a causality test.
particularly a causal chain may terminate premature ly because key buggy state is excluded due to confounding or it may contain additional state that does not pertain to the fai lure.
in the worst case the entire chain may not even be relevant for explaining the failure.
from our experiments of the real bugs suffered from this problem.
for example consider the program presented in fig.
.
previously we showed that csm can determine that y mapsto z mapsto is the causal state set on line .
suppose csm further conside rs a smaller subset y mapsto .
when replacing the value of yin execution b with that from c the condition of the if statement becomes true.
this redefines yon line rendering the target state y mapsto uninducible.
because of that condition csm finds replacing the values of both yand znecessary.
however zis unrelated to the original behavioral di fference.
the only contribution of zin both executions is its use on line which had the value false inboth executions.
ideally only the definition of y mapsto should be blamed for the failure.
1x input 2y input 3ifx 4y y 5ifx y 6x 7print x a x y iffalse iftrue x print b x y iftrue y iffalse print c x mapsto x mapsto d fig.
missing causes by execution omission.
a program.
b c exec utions with differing input.
d csm result.
execution omission the second problem is that csm may miss important causal state in the presence of execution omission errors where the buggy target state is produced because statements were not executed due to the bug.
in such cases the computed failure summaries are usually incomple te.
the root cause of the problem is that csm is asymmetric meaning the buggy and correct executions have asymmetric roles in the process csm reasoning is based on modifying state only in the correct execution its final results only in clude information from the buggy execution.
fig.
presents an example.
the correct execution in b follows the false branch of line then the true branch of line2735 and prints whereas the buggy execution in c follows thetrue branch of line then the false branch of line and prints .
suppose that initially the e ffect point is line and the cause point is line .
csm determines that replacing the value of xis sufficient to induce the buggy target state in b so it identifies x mapsto as the only buggy state at the cause point.
however the buggy output x mapsto on line in c is due to the undesirable omission of line which is partially determin ed by the buggy state of y mapsto .
missing y mapsto in the cause set leads to an incomplete summary of the failure.
suppose the computation continues backward with a new effect point on line and new cause point on line .
csm determines that replacing the value of xon line is sufficient to induce the buggy target state x mapsto on line .
fig.
2d shows the result of this analysis.
this implies x mapsto is the sole root cause of the bug.
however replacing the value of xon line in b cannot induce the final failure although it can induce x mapsto on line because line evaluates to true in the patched execution.
hence line produces y mapsto and leads to to x mapsto .
in our experiments of the real bugs face this problem.
efficiency csm may demand a large number of reexecutions.
the number of state di fferences can be as large as the size of the allocated memory .
the number of possible subsets that need to be tested for causality is potentially combinatorial in terms of the full set.
to combat this exist ing approaches use delta debugging to perform a generalized binary search over the subsets.
however the number of reexecutions can still be quadratic in the size of all used memory.
even the most recent implementation of csm may take a few hours to reason about a failure while the original execution time is just a few milliseconds.
iii.
comparative causality in this paper we propose a more e ffective and precise causal inference model called comparative causality cc .
this model focuses on symmetrically reasoning about two executions one buggy and one correct1 in order to explain why they both differ from eachother.
it also enables e fficient and practical implementation.
in the following we first defi ne a number of notations and concepts.
then we study the intended properties of the new model.
here we assume we can properly align the control flow and the variables memory regions of the two executions for fine grained comparison using existing work .
execution point we use a superscripted label leto denote a point in execution e. symbol l e1 e2 denotes a point that appears in both executions e1and e2 determined by the given control flow alignment .
it is also called an aligned point .
state difference we use x mapsto v1 v2 to denote that a variable x has value v1ine1and value v2ine2 with v1 nequalv2.
problem statement given a set of state di fferences at an aligned execution point l e1 e2 diamondsolidand a preceding aligned point l e1 e2 diamond we want to find a set of state di fferences at l e1 e2 diamond that isrelevant sufficient and minimal for inducing .
1how to acquire a correct execution given only the buggy execu tion can be found in a survey .the preceding execution point is the cause point and the latter one the effectpoint.
we demand aligned points because state comparison is not meaningful at non aligned points.
a n inducing state difference in the cause point is called a cause a state difference in is called an effect.
a. property one relevance the causes identified by cc must be relevant to the target effects.
intuitively a di fference dis relevant to a later difference dsifdsis transitively produced from dthrough a sequence of differences.
it represents the notion that buggy state must be derived from preceding buggy state except at the root cause .
consider the example presented in fig.
.
the state di fference z mapsto on line is not relevant to y mapsto on line even though there is a dynamic dependence path from line to line because the di fference of zis neutralized on line which yields false in both runs.
in contrast the di fference y mapsto on line is relevant to y mapsto on line .
the formal definition is as follows definition relevance a state difference diamondatl e1 e2 diamond isrelevant to a target state di fference diamondsolidat a later effect point l e1 e2 diamondsolid if either of the following conditions is satisfied.
there exists a dynamic program dependence path from diamondsolidto diamond ine1 e2 where all the statement computations along the path yield different results from the other execution e2 e1 .
there exists a state di fference xin an aligned point in between l e1 e2 diamond andl e1 e2 diamondsolid such that diamondis relevant to xand xis relevant to diamondsolid.
condition expresses the requirement that a di fference cannot be neutralized within an execution in order to be rele vant.
note that it is symmetric to both executions as relevan ce can be determined by a dependence path in either execution.
it allows us to precisely capture relevance in the presence of execution omission.
consider the example in fig.
state difference y mapsto on line is relevant to x mapsto on line due to the dependence path y true in .
observe that there is no dependence between y and x in the failing execution due to the omission of line .
the intuition is that omission is an asymmetric concept regarding one execution.
an omitted statement regarding one execution implies that it appears i n the opposing execution.
with our symmetric definition omissio ns are conceptually precluded.
condition expresses that relevance can be transitive even across the two executions.
b. property two su fficiency the identified set of causes must su fficiently induce the target effect of each of the two executions within its opposing execution.
this inducement acts as a new causality test and witnesses the causal relationship between the identified ca uses and the target state.
the property is symmetric as it requires the set of e ffects in either execution to be induced by the causes.
it means that if for all the variables in the cause set we copy their values from execution e1toe2 we can induce the target e ffect of e1 at the effect point in e2 and vice versa .274consider the example in fig.
.
state di fferences y mapsto x mapsto on line form a su fficient set regarding the e ffect x mapsto on line .
in contrast the di fference x mapsto itself is insufficient because although replacing x s value with in b can induce the e ffect x mapsto on line replacing x s value with in c cannot induce the e ffect x mapsto .
this symmetry ensures that we capture relevance due to execution omission .
more formally definition sufficiency a cause set diamondatl e1 e2 diamond issufficient for a given target effect set diamondsolidat a later effect point l e1 e2 diamondsolid if and only if in the absence of confounding copying the state of e2in diamondto e1at the cause point induces the e ffect of e2in diamondsolidin execution e1 at the effect point and vice versa.
one key condition is that reexecution should be confounding free.
unfortunately normal program executi on cannot guarantee this.
the remainder of this subsection foc uses on discussing confounding.
what is confounding?
determining sufficiency involves replacing part of the state in one execution with values from th e opposing execution.
however the continuation of the modifi ed execution has state from both original executions entangle d affecting each other and inducing undesirable and unexpected results in causal inference.
recall in fig.
we saw that partially changing the state of execution b with the single desired cause variable yyielded output different than in either execution b or c .
in addition we found that including zas a cause along with ywould yield the target state although zis not relevant to the output.
both of these are unexpected results that we call confounding from partial state replacement .
these confounding e ffects do not just have the ability to include arbitrary state within the set of identified causes they can exclude arbitrary state as well.
examples are omitted due to the space limitations.
at a high level these unexpected results occur because partial state replacement created new behaviors that did not exist in either of the original executions .
definition confounding given executions e1ande2as well as a patched execution epconstructed from them a causality test using episconfounded if either of the following conditions are satisfied an execution point in epis not present in e1ore2.
a data dependence in epis not exercised in e1ore2 condition corresponds to control flow confounding and to data flow confounding which means confounding can occur without exhibiting any new control flow.
1x 2y input 3z input 4x 5print x x ... y z x print x ... y z x print y mapsto z mapsto a b c d fig.
data flow confounding example.
a program.
b c executions w ith differing input.
d confounded explanation.
consider the example in fig.
.
this time the target state is x mapsto with cause and effect points at lines and respectively.
observe that in each execution the read from and written to elements of xare different.
thus the only identifiedcause for the different output should be the di ffering values of y which provides the index read from the list.
however when only the value of yis replaced on line in b the patched execution reads the new value written to the list on line .
thus the target state is not induced.
observe that in this ca se a new data dependence from line to line is exercised.
in later sections we will examine new execution models that can avoid mitigate confounding.
we argue that the two properties together with the minimali ty requirement are essential for understanding execution di fferences.
they precisely express the programmer s intentions .
iv .
realizing comparative causal inference in this section we discuss the realization of cc.
given a target effect set and a cause point we leverage a technique called dual slicing to compute a set of candidate causes and only apply causality testing on the candidate set.
dual slic ing is a symmetric slicing technique that works on two execution s. it first determines control flow and value di fferences in the two executions through trace comparison and then performs slicing on these di fferences in and across both executions .
the benefits of using dual slicing are twofold.
first it ensures relevance of the candidates.
second it is more e fficient because causality testing only needs to enumerate subsets o f the candidates instead of the full set of state di fferences as in csm .
after acquiring the dual slice we then symmetrically minimize the causes included in the slice to a minimal subset sufficient for inducing the target state within both executions.
during the minimization process one key step is to perform causality testing by state replacement.
in order to avoid confounding we devise an execution model that harnesses a patched execution in such a way that it respects the control flow and dependences in the two original executions while allowing flexibility for reasoning about the e ffects of state replacement.
a. background dual slicing dual slicing was first introduced to study concurrency bugs and software vulnerabilities .
algorithm dual slicing dualslice l e1 e2 diamondsolid input l e1 e2 diamondsolid the slicing criterion output d the dual slice a set of deps in either execution ife1 nequal then for each data dep dd l e1 e2 diamondsolidx e1l e1 e diamond do ife orxhas different values on l diamondthen d d dd dualslice l e1 e diamond control dep cd l e1 e2 diamondsolid e1l e1 e diamond ife orl diamondhas different branch outcomes then d d cd dualslice l e1 e diamond ife2 nequal then operations symmetric to when e1 nequal returnd275algorithm presents the basic dual slicing algorithm.
although it is not part of this paper s contributions we prese nt a simplified version of the algorithm for completeness.
given a slicing criterion an execution point that exhibits a state difference the algorithm returns its dual slice a set of dynamic dependences from both executions denoting the causality of the di fference.
lines describe the process of slicing in execution e1.
it first ensures that the current criterion l diamondsolidis present in e1 line .
here l e2 diamondsoliddenotes that l diamondsolidis not present in e1.
lines traverse each dynamic data dependence edge of the criterion in e1with x the variable involved denoted as l e1 e2 diamondsolidx e1l e1 e diamond .
we use variable e 2to show that l diamondmay or may not be in the second execution disregarding the value of e2.
on lines if l diamondis exclusively ine1 i.e e and thus is a control flow di fference or even if it is not exclusive but variable xhas different values in the two executions the data dependence is added to the slice .
the dual slice of l diamondis recursively computed and added to the slice too line .
thus when l diamondis present in both executions and produces the same value it is not added because it cannot induce the criterion.
in lines the algorithm traverses the control dependence edge in e1 denoted as e1 .
similarly if the guarding predicate is exclusive or has di fferent branch outcomes the edge gets added and the dual slice of the predicate is recursively computed.
lines are symmetric to lines describing the process of slicing in execution e2.
1t input 2x input 3y input 4z input 5ifx y z 6z 7ifx y z 8z 9ifz 0andy z t 11else print z a t x y z iftrue z iffalse if false print b t x y z iffalse iftrue z if false print c y input z input if1 y z z if1 y z z print z d e z mapsto z mapsto z mapsto f fig.
a program.
b c two runs.
d program from the dual slice.
e dual slice.
f cc explanation.
example.
consider the program in fig.
4a.
the dual slice of the two executions b and c is presented in fig.
4e including the crossed out dependences .
part of the compu tation is represented as follows.
we use ds as a shorthand fordualslice .
the superscripts of execution points are elided for brevity when explicit from the context.
the box in a step denotes that the next step is to execute the recursive call in side.ds b c 11z b6 ds b 11z c8 ds c 11z b6 b5 ds b c 11z c8 ... 11z b6 b5 5z b4 5z c4 ... ... at step the control dependence to line is not involved as it has the same branch outcome in the two runs.
also dual slicing line of execution b in step entails slicing li ne in both executions step .
line is not included even though it denotes a di fference as it is not reachable from the criterion.
the dual slice captures the behavioral di fferences of the two executions related to the criterion.
b. dual slices are relevant but not ideal dual slices are represented in terms of dependences whereas causal inference is conducted on program state.
hence we first introduce a projection from a dual slice to the corresponding set of state di fferences at a given execution point so that we can discuss the properties of dual slicing in our context.
these properties are unique to the proposed technique and have not been studied before.
given a dual slice and a cause point l e1 e2 which is an aligned point we define the cut of the dual slice with respect to the point as follows.
c d l e1 e2 x mapsto v1 v2 let diamondsolidx etlet diamond d with l diamond etl etl diamondsolidorl diamondsolid l andx mapsto v1 v2 on lwith v1 nequalv2 it denotes the set of state di fferences involved in the dual slice on the given cause point.
it essentially denotes the se t of variables when we cut the dual slice on the cause point.
symbol la etlbdenotes laprecedes lbin execution et.
consider the dual slice in fig.
4e.
the cut on line is the following.c d b c z mapsto y mapsto .
note that t mapsto is not in the cut.
theorem all the causes in a cut c d l e1 e2 are relevant to the slicing criterion.
all relevant causes on l e1 e2 are included in its cut.
the proof is omitted due to space limitations.
the property suggests that dual slices cover all the causes the programme r needs to inspect.
unfortunately a dual slice cut may not su fficiently induce the slicing criterion given the confounding prone regular execution model.
that is replacing the state of all causes in a cut may not induce the failure.
let us revisit the example in fig.
.
the dual slice is shown in fig.
1e.
its cut on line has only y. however from the discussion in section ii we know that replacing y mapsto with y mapsto in execution b does not lead to the target effect due to the confounding from z. a cut may also not be minimal.
it may contain causes that are not essential for inducing the target e ffect.
in fig.
4e the cut on line is z mapsto y mapsto but the minimal sufficient set is just z mapsto .
these limitations motivate us to realize the proposed cc by performing confounding free minimization on dual slices.276c.
the basic algorithm in this subsection we introduce the basic minimization algorithm assuming a confounding free execution model.
w e will discuss the execution model in the next subsection.
algorithm minimizing causes infercauses d l e1 e2 diamond l e1 e2 diamondsolid diamondsolid input d the dual slice l diamond the cause point l diamondsolid the effect point diamondsolid the target state output causes of target at l diamond c d l diamond min for each s by delta debugging do if s min ee1 l diamond l diamondsolid leadsto diamondsolid e2 ee2 l diamond l diamondsolid leadsto diamondsolid e1then min s return min algorithm presents the basic approach.
given a precomputed dual slice the cause and e ffect points and the target state the algorithm returns a minimal set of causes su fficient to induce the target state.
the algorithm starts by computin g a dual slice cut at the cause point which is essentially the set of relevant causes.
lines minimize the set to only those sufficient for inducing the observed target state of each execution in the other.
we leverage the delta debugging algorithm to enumerate subsets of the relevant causes and te st their causality.
symbol ee1 l diamond l diamondsolidmeans executing e1 up to the cause point l diamond replacing its variable value mappings inswith those from e2 and continuing the execution up to the effect point l diamondsolid.
symbol s e1denotes the projection of state differences son execution e1.
if the variables in the target state have the values from e2 we say that the target state of e2was induced written leadsto diamondsolid e2.
note in contrast to existing csm approaches our minimization algorithm performs two symmetric causality checks.
this is necessary to include causes via omission.
d. confounding free execution model recall that confounding occurs when new control flow or data dependences not in either original execution occur in a patched execution.
by theorem we know that all the relevant causes are included by the dual slice.
this suggest s we only need to perform causality testing within the dual slice.
conceptually the essence of our new execution model is to construct a program containing only the behavior of the dual slice and all reexecutions for causality testing occur on the constructed program.
statement executions not in the du al slice should be prevented in order to minimize confounding.
illustrative example.
consider the example in fig.
.
assume we start by using the target state z mapsto at line .
assume the cause point is line and we apply algorithm to minimize the causes at this point.
the cut of the dual slice involves variables yand z. when we consider variable zwith a regular execution model we reexecute c up to the cause point and replace the value of zwith .
it induces the false branch outcome on line but the true branch1 when lis not a conditional with l nelementd skips l. when lis a conditional and it was in both executions with branch te1 l branch te2 l unconditionally continue with the same branch as in the original executions.
when lis a conditional and it was in both executions with branch te1 l nequalbranch te2 l or lis in only one execution evaluate the statement according to rule and follow the computed branch.
when lis not a conditional with l d validate that all the operands involved in some data dependence in dhave the same data dependence as they did in the original executions otherwise terminate and report confounding for any operand not in any dependences ind denoted as x set its value to val tex l x and continue.
fig.
semantics ofe .
outcome on line which is di fferent than execution b .
hence z mapsto is not considered a valid cause set.
with our new execution model conceptually we construct a program representing the dual slice as in fig.
4d in which lines and are precluded as they are not in the slice.
also line is no longer guarded by any predicate.
operands that are in the slice and have identical values in both execut ions are concretized e.g.
xon lines and .
again let us determine the causality of variable zon line .
we reexecute c up to the cause point using the original program.
we replace the value of zwith then continue execution with the program in fig.
4d .
since lines and are not in the program we avoid confounding and can induce the desired target state.
hence z mapsto is the minimal inducing cause set.
observe that it allows us to prun e the relevant but not necessary cause y mapsto .
applying algorithm transitively we acquire a more concise failure explanation as shown in fig.
4f.
box semantics of the new execution model.
in the following we discuss the semantics that allows achieving the e ffect of executing exclusively within the dual slice without construct ing a new program.
during minimization we first reexecute the original program with normal semantics up to the cause point and then continue executing the program with the new semantics after state replacement until the e ffect point.
in the semantics we assume the runtime availability of the dual slicedand the traces of the original two executions denoted byte1 .
without losing generality we assume we are patching e1using information from e2.
the value of a variable xat a point le1in the original execution e1can be queried from the trace by val te1 l x .
if an execution point le1is a conditional statement branch te1 l queries its branch outcome in execution e1.
the semantics is presented in fig.
.
statement executions not in the dual slice are skipped when they are not conditional statements rule .
when executing conditional statements we cannot simply skip as we need to select a branch to proceed.
rules specify the cases for conditional statements.
in rule if a conditional had di fferent branch outcomes originally or was present in only one execution the semanti cs evaluates the predicate and follows the computed branch.
th e essence is to allow the flexibility to take either branch base d on the predicate evaluation in order to reason about the e ffect of2771 rule from fig.
.
when lis a conditional and it was in both executions with branch te1 l nequalbranch te2 l evaluate the statement normally and follow the computed branch.
when lis a conditional and it was in only one execution ex follow the branch that was taken in ex.
otherwise evaluate las in a regular execution model.
fig.
semantics of the approximate execution model.
state replacement when it is in the dual slice.
if the stateme nt is not in the dual slice it does not matter which branch is tak en because all non conditional statements inside the branche s must be skipped according to rule .
these statements must not be in the dual slice otherwise the conditional would ha ve been in the slice according to the dual slicing algorithm.
rule handles non conditional statement execution in the dual slice for all the operands not involved in any dependen ces in the slice implying that they must have identical values in the two executions we concretize them with values from the traces to achieve isolation.
for operands involved in so me dependence we ensure no data flow confounding.
this new model will not allow any confounded executions to go through as can be inferred from the semantic rules.
theorem a dual slice cut is su fficient within the new execution model.
this theorem ensures that algorithm must be able to find a minimal sufficient set of causes inducing the target state because in the worst case the cut is the minimal set.
informall y the theorem holds because reexecution is exclusively withi n the dual slice and hence replacing all the state in a cut leads to a reexecution equivalent to the part of the dual slice belong ing to the opposing execution and hence the target state.
a practical approximation.
unfortunately the semantics in fig.
demands a prohibitively expensive implementation.
i t requires collecting traces with dependences and values.
th e traces and the dual slice have to be accessed during each reexecution.
each statement has to be instrumented to decid e if it is in the dual slice rule or perform complex control rules .
the overhead could easily be many orders of magnitude not affordable for repeated reexecutions.
in practice we observe that control flow confounding is the dominant confounding factor and data flow confounding can only affect the execution by causing control flow confounding in most cases.
we hence propose a practical approximation th at can completely prevent control flow confounding and mitigat e data flow confounding.
the approximate model ensures a patched execution can only follow dynamic branches taken by at least one of the original executions.
consequently it enforces a control flow path composed of segments that occurred in either execution.
what we do here is essentially construct ing guard rails for the execution so that it can never deviate fro m the dual slice s control flow.
since data dependences heavil y depend on control flow the approximation can also mitigate data flow confounding.
the semantics is presented in fig.
.
observe that the semantics does not require the runtime of the dual slice or dependence value traces for runtime checking but rather just the control flow trace.
this can be very efficiently represented and accessed by using bit streamsthat simply record the sequence of boolean branch outcomes.
it does not skip statements.
it hence avoids instrumenting a ll statements to decide if one can be skipped at runtime.
theorem the approximate execution model is free of control flow confounding.
the theorem can be inferred from the semantic rules.
we implemented the approximate semantics and in practice it wa s able to suppress all confounding in our experience.
v .
evaluation we implemented our technique using llvm .
.
we have also implemented the csm and dual slicing approaches for comparison.
both implementations reflect the latest published designs .
the evaluation is in the context of automated debugging.
the techniques contrast buggy and correct executions using explanations for their different behavior as explanations of bugs.
first we compute explanations for a set of real world bugs by chaining togethe r the computed causes.
we contrast the explanations computed by the three different techniques.
second we examine in depth how the problems that csm faces a ffect its results in practice.
we used real world bugs taken from the repositories of open source programs.
they include all deterministic bugs fromtar grep andmake in a one year period that we were able to reproduce.
all the bugs in our study were noncrashing semantic bugs that produce incorrect outputs.
ta ble i presents the full set of programs and bugs.
the first three columns identify the buggy program bug id and the version of the program that actually contains the bug.
the ssloc column contains the static source lines of code computed withsloccount .
the alt.
column identifies how a second correct execution was selected.
we used a correct input when the bug report also provided it otherwise we used predicat e switching to automatically synthesize a correct execu tion from the failing one.
more information on acquiring a correc t execution from a given failing execution resides in sumner s survey paper .
we performed all experiments on a bit .4ghz cpu with 12gb ram using one core.
a. full explanation comparison our first experiment uses each of the three techniques to compute an explanation for each bug.
for each bug we first identify the last observable failure and use that as the init ial target state.
cc and csm select the last preceding definition of a target effect as the cause point to compute the causes.
they also proceed transitively using the computed causes a s the new target state and the current cause point as the new effect point until there are no more causes to identify e.g.
the two executions have no state di fferences .
we contrast the results of the di fferent techniques through their quality scale and e fficiency.
we measure quality through precision and recall with respect to a relevant su fficient and minimal explanation of why the correct and buggy executions differed.
this is manually checked at each step of the computation.
precision p is the proportion of the dynamic stateme nts in the computed explanation for a technique that coincide wi th278the statements in the correct explanation.
recall r is the proportion of the dynamic statements in the correct explanatio n that are also identified by the computed explanation.
we have to resort to manual inspection due to the lack of an automated oracle to tell us the ideal explanations for execution di fferences.
as we show later such ideal explanations are small enough for line by line human inspection.
we have done the following to mitigate threats to validity.
first we cross referenced the computed explanations with the root causes identified by the bug fixes or reports.
second we calibrated our system using the siemens suite before our experiments.
we computed the explanations for the over failing runs in siemens using the corresponding pass ing executions of the provided correct versions and validated that these explanations capture the injected faulty statem ents as the root causes.
the results are publicly available at .
third we also release the experimental results of the real world bugs at the same site for interested readers.
we measure the scale of a technique by the number of dynamic statements stmts in the computed explanation.
fina lly we measure efficiency in three ways the number of steps or rounds of causal inference the clock time required in secon ds and the number of reexecutions needed.
note that the clock time of cc includes dual slicing time.
table i shows the resul ts.
from these we make several observations.
cc consistently yields the highest quality explanations.
dual slicing generally has good recall but poor precision because it doesn t minimize.
csm is unpredictable because i t can arbitrarily include or exclude causes however it freq uently fails to identify causes for even a single step of an executio n. we shall explore the unpredictability of csm further in the next section.
in contrast cc yields high precision and high recall for every computed explanation.
for the bugs it capt ures of root causes whereas csm fails to do so in of cases.
where cc failed to identify root causes denoted by it still explained why the two executions differed thus the precision and recall.
in those cases the second execution w as too different to meaningfully explain the bugs as well.
the extra reexecutions for csm make it slower than cc even when it computes fewer steps.
on average csm takes .
minutes to compute an explanation even though it produces less of the correct explanation.
in contrast cc takes .
minutes on average because the extra dual slice inf ormation allows it to avoid considering all memory di fferences as potential causes.
this reduces the number of necessary reexecutions by up to two orders of magnitude.
cc produces more concise explanations than dual slicing.
the precision numbers show that cc is more precise than dual slicing .
vs .
.
on average cc produces explanations of dynamic statements while dual slicing produces statements.
this experiment illustrates that cc produces superior expl anations in terms of quality e fficiency and scale.b.
why and how csm fails a single incorrect cause at any point of the full chain computation can cascade through the rest of the computation causing more incorrect causes.
it is hence di fficult to determine the reasons behind the incorrectness by simply looking at th e full chains.
our second experiment examines why and how csm missed or erroneously included causes on a per step basis.
note that cc does not encounter these problems for the given benchmarks and dual slicing does not do minimization .
thus we focus only on csm for this experiment.
we first computed the causes for each step using csm as in the first experiment.
for each step we also supply the same csm target state and the same cause point to cc and compare the resulting causes from the two approaches.
this allows us to quickly observe any e ffects from confounding.
in this per step fashion we checked the results of csm for missing causes m extra causes e or failure to identify any causes f .
these are the ways that the technique can fail.
we also checked why these failures occurred including control flow confounding cfc data flow confounding dfc and execution omission o .
table ii contains these results.
csm suffers from all three problems.
it misses causes in almost all benchmarks out of has extra causes in out of and fails to produce any causes for a step in out of cases.
these failures resulted both from omission and from confounding although confounding was the more frequent cause.
control flow confounding causes errors in most of the csm explanations.
in out of cases the csm explanations are directly impacted by control flow confounding.
thi s shows that control flow confounding is a real world challenge that we must address.
data flow confounding does not directly impact csm.
while close inspection indicates that some data flow confound ing occurs it impacts the executions only through control fl ow confounding.
as cc prevents control flow confounding the impact of the corresponding data flow confounding is also suppressed.
for example data flow confounding may lead to an incorrect branch but cc forces the execution back to the correct branch through its execution model.
together these fine grained comparisons allow us to see that omission and confounding do indeed impact existing techniques.
furthermore taken with the results in table i t hey show that cc is resilient when faced with them.
c. example of resulting explanations next we demonstrate a failure explanation generated by cc and explain how csm fails to compute that explanation.
this chain is for bug .
version .
.
of tar has a bug when using the backup option.
when extracting files from an archive this option copies any already existing files int o a backup directory preventing these files from being overwri tten.
when extracting a directory that already exists however it appears to incorrectly prevent files from being extracted.
we used predicate switching to dynamically patch the buggy execution and derive a correctly behaving execution.
both t he buggy and the switched executions first extract some files279table i comparison of full explanations.
averages are arithmetic exc ept for p r which are geometric.
means that the root cause cou ld not be captured.
program idversion ssloc alt.cc csm dual slicing steps time tests stmts prroots steps time tests stmts p rroots stmts p r find .
.
73k switch .
.
x .
.
gnuplot .
.
144k switch .
.
x .
.
x .
.
gnuplot .
.
139k input .
.
x .
.
gnuplot .
.
134k input .
.
.
.
.
.
gnuplot .
.
134k switch .
.
.
.
.
.
grep .
.
12k switch .
.
.
.
.
.
grep .
.
12k switch .
.
.
.
.
.
grep .
.
12k switch .
.
x .
.
.
.
make .
.
30k switch .
.
x .
.
.
.
tar101.
.
20k switch .
.
x .
.
x .
.
tar111.
.
24k input .
.
x .
.
tar121.
.
20k input .
.
x .
.
tar131.
.
20k switch .
.
x .
.
.
.
tar14 .
21k input .
.
x .
.
x .
.
tar15 .
21k switch .
.
x .
.
x .
.
average .
.
.
.
.
.
.
.
.
.
.
.
.
.
table ii csm difficulties.
this includes symptoms m issing causes e xtra causes and complete f ailure.
it also lists reason s why control and data flow confounding cfc dfc or o mission.
id m e f cfc dfc o x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x before trying to extract a directory that already exists.
th e switched execution renames the extracted directory so that it does not conflict with the existing one and it correctly extr acts files to the new directory without error.
however the buggy execution appears to have not extracted any files at all even the previously extracted ones.
fig.
shows a simplified version of the relevant code as well as the explanation by cc which is slightly shortened for readability.
first predicate switching renames one of the extracted directories from dir2 to dir .
next a call to mkdir fails in the buggy execution returning 1because dir2 already exists.
in contrast the call succeeds in the switched execu tion and returns .
this difference 0vs.
gets propagated through the variable status back into extract archive where it makes the condition on line true only in the failing execution indicating an error when extracting dir2 .
so the buggy execution calls undo last backup .
this actually replaces all of the extracted files with the original backups.
as a result all of the files extracted before dir2 appear to never have been extracted even though they were.
in fact the original bug reports for this failure assumed the files had not beencode summary 1int read header primitive 2filename dir vs. dir2 4int extract dir file name 5tmp mkdir file name ... 7status tmp 8ifstatus iferrno eexist is dir file name pass elif!maybe recoverable filename mkdir error file name return status 15void extract archive ... status extract dir file name ifstatus backup option undo last backup explanation at file name is dir vs. dir2 .
at tmp is0vs.
.
at status is0vs.
.
at the return value is0vs.
.
at extract dir returns is0vs.
.
at status backup option isfalse vs.true .
so undo last backup iscalled overwriting the extracted files with the original ones.
fig.
example of a derived explanation using our technique extracted as well but our generated explanation clearly s hows that they were first extracted and then incorrectly overwrit ten.
the root cause is that extract dir should not fail even if mkdir fails due to the existence of the directory because extracting to an existing directory should not cause proble ms. atar developer can see this from the computed explanation on the bottom of fig.
and know how to construct a fix.
indeed the applied fix set status to0on line .
note csm cannot construct this explanation.
on line confounding prevents further analysis of the bug.
first th e condition on line only executes in the failing execution 280where it is true.
csm replaces the value of tmp at line to produce the failing status at line but the condition on line evaluates to false this time because it also requires a failing value for errno .
hence csm proceeds to line which reports an unrecoverable error and terminates.
this confounding prevents the identification of tmp alone as the cause.
additional confounding not shown here also prevents replacing both errno and tmpfrom inducing the failing status .
d. threats and limitations we have shown that cc is e ffective at explaining why two executions are different but there are limits to the technique our evaluation and what may be inferred from it.
we first note that explaining why a buggy and correct execution differ does not always provide a useful explanation of a bug as observed in of our generated explanations.
also manual examination of execution di fferences risks human error.
most of the explanations generated by cc are short enough that we can be confident of our inspection.
finally again comparative causality is presently limite d to examining deterministic bugs.
this inherently follows fro m exploiting reexecution within the technique.
vi.
related work the most relevant work is causal state minimization csm that was originally introduced by zeller and subsequen tly improved by others .
in contrast to csm our scc model avoids confounding handles execution omission by symmetric analysis and is much more e fficient.
recently r o ler et al.
also noted problems with zeller s original approach although they did not delve into what these problems were .
they also produce a technique for explaining bugs but it is based on test generation and requi res a strong oracle to evaluate each new test.
traditional dynamic slicing is a technique that captur es dynamic data and control dependences.
it has been extensive ly examined for its usefulness in debugging .
dynamic slic es are usually problematically large and su ffer from execution omission.
dual slicing is a kind of dynamic slicing techniqu e thatcompares two executions and extracts the differing dependencies between the two .
it forms the initial basis of our technique.
in contrast our computed explanations ar e much smaller due to state replacement and minimization.
several satisfiability based techniques also strive to prec isely explain failures either within a single program or when comparing correct and incorrect versions .
t he present limitations in constraint solving however have t hus far mostly limited these techniques to programs of a few thousan d lines of code.
in contrast our technique explains failures in programs with well over 100k lines.
our technique requires that the executions of interest be reproducible.
tools that aid failure reproduction for ins tance can make this more feasible in practice vii.
conclusions we presented a novel causal inference technique called comparative causality.
it allows precise and concise expla tions for the differences between two executions at a very fine granularity.
it advances the state of the art in three aspect s it improves robustness of underlying state replacement techniques by preventing confounding through novel execution models it handles execution omission errors by analyzing two executions symmetrically and it substantially improv es efficiency by leveraging dual slicing.
evaluation on a set of rea l world bugs shows that the proposed technique can generate high quality explanations at low cost.
viii.