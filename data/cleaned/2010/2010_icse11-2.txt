lime a framework for debugging load imbalance in multi threaded execution jungju oh georgia institute of technologychristopher j. hughes intel corporationguru venkataramani george washington university milos prvulovic georgia institute of technology abstract with the ubiquity of multi core processors software must make effective use of multiple cores to obtain good performance on modern hardware.
one of the biggest roadblocks to this is load imbalance or the uneven distribution of work across cores.
we propose lime a framework for analyzing parallel programs and reporting the cause of load imbalance in application source code.
this framework uses statistical techniques to pinpoint load imbalance problems stemming from both control flow issues e.g.
unequal iteration counts and interactions between the application and hardware e.g.
unequal cache miss counts .
we evaluate lime on applications from widely used parallel benchmark suites and show that lime accurately reports the causes of load imbalance their nature and origin in the code and their relative importance.
categories and subject descriptors d. .
testing and debugging debugging aids d. .
concurrent programming parallel programming d. .
metrics performance measures general terms performance measurement keywords load imbalance performance debugging parallel section .
introduction in recent years the number of cores available on a processor has increased rapidly while the performance of an individual core has increased much more slowly.
as a result permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
icse may waikiki honolulu hi usa copyright acm ... .
.achieving a large performance improvement for applications now requires programmers to leverage the increased core count.
this is often a very challenging problem and many parallel applications end up suffering from performance bugs caused by scalability limiters .
these prevent performance from improving as much as it should with more cores.
since we expect core counts to continue increasing for the foreseeable future addressing scalability limiters is important for developing software that will obtain better performance on future hardware.
load imbalance is one of the key scalability limiters in parallel applications.
ideally a parallel application assigns an equal amount of work to all cores keeping all of them busy for the entire application.
load imbalance occurs when some cores run out of work and must wait for the remaining cores to finish their work.
load imbalance is relatively easy to detect we can watch for threads waiting at the end of a parallel section i.e.
at a barrier or at a thread join point.
however it is much more difficult to diagnose the cause of load imbalance with sufficient precision to help programmers decide what changes to make to the application to reduce the imbalance.
the cause of load imbalance can be hard to diagnose because there are a variety of candidates.
for instance load imbalance can be caused by assigning an unfair proportion of tasks to a thread or by assigning too many long tasks to the same thread both of these manifest as control flow differences between threads.
diagnosing causes for imbalance becomes even harder when it occurs due to interactions between the application and the underlying hardware e.g.
threads having different numbers of cache misses .
such causes cannot be easily detected through code inspection or static analysis.
reducing load imbalance has long been an active research topic.
the most common approach to reducing load imbalance is to use dynamic task scheduling such as that provided with openmp and tbb .
dynamic task scheduling involves partitioning the parallel work into many more tasks than threads and using a run time system to assign tasks to threads on demand.
dynamic scheduling significantly reduces load imbalance but introduces significant runtime overheads both from executing scheduling code and from the loss of cache locality among tasks.
these overheads increase with the number of cores threads and can dominate performance .permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
icse may waikiki honolulu hi usa copyright acm ... .
201because load imbalance is a major issue in performance debugging of parallel programs and because existing solutions do not satisfactorily address this problem there is a need for tools to help programmers efficiently find and eliminate causes of load imbalance in their code.
this paper presents lime a framework that uses profiling statistical analysis and control flow graph analysis to automatically determine the nature of load imbalance problems and pinpoint the code where the problems are introduced.
unlike prior work that addresses load imbalance lime does not aim to automatically exploit or reduce load imbalance.
instead it provides highly accurate information to programmers about what is causing the imbalance and where in the code it is introduced with the goal of minimizing trial and error diagnosis and the programming effort needed to alleviate the problem.
we built and evaluated our lime framework on parallel sections from splash and parsec benchmark suites which are commonly used to evaluate performance of multi processor and multi core machines.
our results show that lime is highly accurate in pinpointing load imbalance problems caused by cache misses and control flow differences among threads.
we confirmed the accuracy of the tool s cache miss results by eliminating misses it reports and confirming that so dramatically reduces load imbalance.
we confirmed the accuracy of the tool s control flow results by verifying that when the tool reports control flow differences as the primary cause of load imbalance the reported line of code is the actual location where the load imbalance is introduced and leads the programmer to the code where it can be repaired.
.
overview of lime to help explain our framework and the problem it addresses we use the code example in figure .
this is an actual parallel section from splash s radix benchmark .
the splash benchmark suite was extensively optimized over a decade ago by experts in both parallel programming and multi processor hardware and has been used to evaluate parallel performance of multi processor and multi core machines ever since.
barrier ... if mynum !
... while of fset 0x1 !
... while of fset 0x1 !
... for i i radix i ... else while of fset 0x1 !
of fset ... for i i radix i ... while of fset !
if of fset 0x1 !
for i i radix i ... for i i radix i ... if mynum stats ... barrier ... figure code for sample parallel section from radix .
all non control flow statements are removed.
this parallel section begins and ends with barriers where each thread waits for all others to arrive before proceeding further.
any load imbalance will result in threads arriv ing at the end barrier line at different times forcing early arriving threads to wait i.e.
be idle until the longestrunning thread arrives.
within the parallel section each thread uses its private mynum andoffset values to decide which part of the parallel computation it should perform.
depending of these values the threads may have different execution times either by executing different code due to differences in control flow or by taking different amounts of time to execute the same code due to differences in how the executed code interacts with the hardware .
in figure there are several examples of possible control flow differences if then else blocks at lines and may cause only a subset of threads to execute the if path and for line other threads to execute the else path loops at lines and could all execute a different number of iterations for different threads.
nesting of loops and conditionals e.g.
line can compound these differences.
additionally threads may have differences in interacting with the system such as branch predictor performance some threads might have more predictable branch decision patterns than others or cache performance e.g.
threads that access data already in the cache may have more cache hits .
these differences are too numerous to point out even in our small example code because every branch jump load store etc.
instruction in the compiled code may be at least in theory a potential source of these performance differences.
it may seem that a trained programmer can inspect the source code to identify potential causes of imbalance and repair them.
however this is a very labor intensive and error prone endeavor because of the sheer number of potential causes and because of the complexity of understanding each cause and determining whether or not each is responsible for imbalance and then repairing those that are .
an actual example of the threads execution times for one dynamic instance of this parallel section is shown in figure .
the shaded part of each bar represents the useful execution time of each thread normalized to the overall execution time of the parallel section.
the white part of each bar represents the thread s waiting time at the end of the parallel section line .
.
.
.
.
.
normali zed event count norm alized exe cution time thread nu mber waitin g time load im balan ce thread execu tion time true decisions a t line true deci sions at line iteration count at line iteartion coun t at lin e iteration count at line iteration coun t at lin e figure execution time imbalance and thread behavior of key points in figure .
points that cause load imbalance are shown with thicker lines.
our discussion of possible causes of imbalance included two types of control flow causes iteration counts of loops and decision counts of if then else blocks.
these event counts are also shown in figure with each event s counts normalized to the maximum count for that event among all threads.202from visual inspection of the graph the decision at line appears to cause imbalance between the first thread and the others.
the differences in true decisions at line appears to account for the remaining imbalance note how well the useful execution time tracks this factor for threads through .
the loops at lines and have identical iteration counts in all threads and thus cannot be causing any imbalance code inspection reveals the same insight because the value of radix is constant and the same for all threads.
the loops at lines and do produce different iteration counts in different threads but this does not seem to correspond to actual imbalance.
finally loop iteration counts at lines and not shown have the same relationship with the imbalance that decision count at line has and loop iteration count at line has the same behavior as the decision count for line .
our lime framework performs this kind of analysis automatically and quantitatively.
for each thread in each parallel section lime measures execution time and various event counts .
the event counts are dynamic decision counts for all static control flow decision points control flow events as well as dynamic counts for each static code location that causes machine interaction events1 hardware events .
using this data lime s analysis framework determines how much imbalance exists which control flow decisions and machine interaction events are related to the imbalance and assigns scores that help programmers decide which cause of the imbalance to attack first.
our initial implementation of lime includes two different profiling environments.
the first implementation uses a cycle accurate hardware simulator called sesc that can be relatively easily extended to collect any desired machineinteraction event count however since it performs detailed simulation of a computer system it is very slow and can only be used for parallel sections that execute quickly e.g.
with carefully designed small input sets .
to overcome the speed limit we implemented lime with pin which is fast but can only accurately collect data for analysis of control flow causes of imbalance.
the simulator based implementation was designed to let us experiment with collecting different events and the pin based one to let us test lime on larger input sets.
for practical use a purpose built profiler could be employed to collect control flow events and key hardware performance counters more efficiently.
the analysis part of the framework processes profiling data from either profiling environment.
it first clusters together events whose counts are highly correlated to each other.
the purpose of this step is to group together events that seem to be related to the same potential cause of imbalance.
for example this step puts the decision count from line and the iteration counts from lines and in the same cluster because they are linearly related to each other they have zero counts in all threads but one so one of these event counts is equal to a constant times the execution count of another .
similarly the iteration count from line 1among machine interaction events we only experimented with cache misses because we expected them to be the only machine interaction event that plays a significant role in creating load imbalance.
as will be shown in section .
this turned out to not always be true.
however lime s analysis treats all hardware events in the same way and we expect it to readily extend to other events as long as they can be counted efficiently .
and the number of cache misses at lines are in the same cluster as the true decision count from line this would not be true if threads had differing cache miss rates for lines .
next lime finds the leader of each cluster.
the purpose of this step is to identify the event that corresponds to introducing a potential cause of imbalance.
in our two example clusters the true decision counts from lines and are found to be the leaders of their respective clusters.
the next step in lime uses multiple regression to find which cluster leaders are related to the imbalance in a statistically significant way and to find the strength of that relationship.
in our example the true decision counts from lines and are the only cluster leaders to have a statistically significant relation to the load imbalance.
finally lime ranks and reports the cluster leaders that are related to the imbalance.
the report includes the score the location in the code and the corresponding cause of imbalance.
in our example lime reports only two causes both with relatively high scores threads take different paths at line and threads have different biases true vs. false decision for the if then else at line .
the following section provides a detailed description of the analysis framework.
subsequent sections describe the two profiling implementations and an experimental evaluation of lime s accuracy with examples that provide more intuition about how lime works and how its results can be used by the programmer to reduce load imbalance.
.
lime analysis framework lime s analysis starts with data gathered by one of our profiling implementations see section .
the profiling data consists of per thread control flow event counts for each edge in the static control flow graph and per thread hardware event counts for each static instruction that can cause such an event.
.
causality analysis for hardware events before entering the main analysis routine lime conducts preprocessing on collected hardware events in order to establish causal relationships between related events.
for example an l1 cache miss can occur only when a memory access instruction is executed an l2 cache miss can occur only when an l1 cache miss happens etc.
if the dynamic count for a particular hardware event differs among threads this hierarchy among events allows us to split the blame for the difference between that event itself and the events that must precede it.
for instance when threads have different numbers of l1 misses at a particular static instruction this may due to some threads executing that instruction more times and thus having more l1 miss opportunities or due to how the application interacts with the l1 cache.
in the preprocessing step lime removes from subordinate hardware events e.g.
l1 misses the contribution from their superior events e.g.
instruction s execution count using the gramschmidt process leaving each event count only with the event s own contribution to variations among threads.
this adjusted hardware event count is used instead of the na ve one throughout the lime analysis.
.
hierarchical clustering the second step in the lime analysis is to cluster related events together.
there are two commonly used clustering al 203gorithms hierarchical clustering and k means clustering .
we use hierarchical clustering because it can automatically find an appropriate number of clusters for a given separation principle clustering threshold between clusters whereas kmeans yields a predetermined number of clusters.
for the same reason many projects on workload characterization rely on hierarchical clustering to find benchmarks that have similar behavior.
hierarchical clustering is performed in steps.
each step merges the two clusters that are closest according to a distance metric.
clustering ends when the distance between the two closest clusters is larger than a preset threshold.
in lime each event control flow event or hardware event is initially a cluster.
we then compute a proximity matrix in which an element i j represents the distance between clusteri and cluster j .
at each step we merge the two closest clusters and update the proximity matrix accordingly.
the distance metric lime uses is pearson s correlation between the event s per thread counts because it effectively captures similarity in how the event count behaves in different threads.
for linkage criteria we used average linkage upgma but other methods single completelinkage produced similar results.
lime stops clustering when the largest correlation is .
.
this threshold value provides the best results in most parallel sections we tested.
the exceptions are fmm and fluidanimate where we used threshold values of .
and .
respectively.
a poorly chosen threshold affects the usefulness of the report too high of a threshold prevents merging of correlated event clusters while too low of a threshold results in clusters that contain unrelated events.
clustering provides several benefits for further analysis it results in a major reduction in the number of subjects for further analysis.
it gathers highly co linear events into one cluster which improves accuracy of regression2in section .
.
it helps identify significant decision points in the program structure e.g.
branches where control flow differs between threads as we explain in section .
.
.
classification of clusters clusters are classified into two types those that contain control flow events control flow clusters and those with only hardware events hardware event clusters .
for hardware event clusters the absence of highly correlated control flow events indicates that different threads suffer the hardware events differently for the same code.
therefore if any load imbalance is eventually attributed to the cluster all the hardware events in the cluster are reported as contributing to that portion of the imbalance.
.
finding cluster leaders within a control flow cluster control flow events are typically interdependent.
to improve the usefulness of reported results for each cluster lime discovers a leader node a control flow instruction that steers program execution into the cluster.
intuitively if the cluster is related to the imbalance the leader node represents the code point where this imbalance is introduced.
2statistical regression works poorly with collinear vectors.leader nodes are important because they are the decision points that change thread execution characteristics.
they are the points in the program of most interest to the programmer by inspecting the code that affects the leader node s decision the programmer can typically find the high level reason for the imbalance.
to formally define a leader node assume a control flow graph of a program has vertices vand edgese.
the leader node of a cluster cis a vertex vin the control flow graph such that all incoming edges to vexcept backedges have source vertices outside the cluster i.e.
v csuch that s v e s negationslash c. c1a if then else cluster c loop cluster c1 with backedgecluster c with multiple leadersc2c2 c1 c2c1b e c f da b ca b e c df g h k i l jleader node basic block cluster figure example of clusters and leader nodes examples of typical clusters and their leader nodes are shown in figure .
the leftmost example shows a cluster whose leader is an if statement.
the middle example shows why a backedge restriction is needed in the leader definition it allows a loop cluster to have a leader the loop entry point .
the rightmost example shows a cluster with two leaders aandg this typically occurs when the same control flow decision is made in more than one code point.
after finding leader nodes for each cluster each leader node is assigned a score according to its significance in creating load imbalance.
first lime computes for each edge the pearson s correlation coefficient between that edge and the execution time.
the score of a leader node is the difference in this correlation between the node s incoming and outgoing edges again ignoring backedges .
for example if nthreads have execution times t t1 t2 tn and a leader node vhas incoming edge counts iev1 iev2 ievi and outgoing edge counts oev1 oev2 oevj the score sv of the leader node vis sv max x parenleftbig corr oevx t parenrightbig max y parenleftbig corr parenleftbig ievy t parenrightbig parenrightbig wherecorr a b is the pearson s correlation coefficient between vectors aandb.
intuitively score svmeasures the amount of correlation the leader node incurs in regard to the overall imbalance.
a high score means that the node converts events that are unrelated to load imbalance into events that are highly correlated to the imbalance.
.
multiple regression multiple regression analysis is a statistical technique that estimates the linear relationships between a dependent variable and one or more independent variables.
in lime the dependent variable is the vector of per thread execution times and the independent variables are the clusters.
if a cluster appears to be a good predictor of execution time we can infer with high confidence that the cluster is responsible for the differences in execution time between threads.
for the regression analysis we need to combine event counts of all events in the cluster so that the cluster behaves like a single event.
for this lime uses averaged zscores of events in a cluster.
a z score standardizes all events to have the same average and same variation so that all events carry equal weight in determining the cluster s overall event count regardless of the actual absolute event counts for each event.
for each event with a mean per thread dynamic count and a standard deviation for each per thread dynamic count x the z score is z x .
after this step regression proceeds by treating each cluster as a single event whose per thread event counts are the cluster s per thread z scores.
during regression analysis lime excludes clusters that are statistically redundant or insignificant in building a regression model to explain the execution time.
we use the forward selection method to choose which clusters to include in the model.
the method selects clusters based on their unique contribution to the variance in the dependent variable execution time .
lime iteratively adds the selected clusters to its regression model until none of the remaining clusters is statistically significant determined by f test .
multiple regression analysis computes a standardized coefficient c for each cluster c which represents how sensitive execution time is to that cluster.
that is the regression computes the values of cito best fitt ci ci where tis the vector of per thread execution times and ciis the vector of averaged z scores for cluster iacross all events in the cluster.
lime uses the cvalues as a measure of a cluster s importance for load imbalance as follows.
for each control flow leader node and hardware event lime computes a final score an estimate of how responsible that node event is for load imbalance.
the score is based on regression results and for control flow clusters on leader node importance scores.
for a leader node viin control flow cj the final score fsviis fsvi cj svi wheresviis the score from equation for leader node vi and cjis the standardized coefficient from regression for clustercj.
for a hardware event e.g.
cache miss hiin a hardware event cluster cj the final score fshiis equal to cj.
figure shows an example of computing final scores for two control flow clusters.
cluster c1has one leader node a while cluster c2has two leader nodes aandg.
note that nodeais a leader in both clusters.
in this example node gfrom cluster c2has the highest final score i.e.
a leader node score of .
and a c2of .
combine to give a score of .
.
therefore we conclude that cluster c2is important in explaining the imbalance and that node gis the prime suspect for introducing the imbalance.
c2 c1pearson s correlation to the execution time a .
a b .
a e .
d g .
g k .
multiple regression t .
c .
c c1 .
c2 .
cluster c leader node score a .
.
.
final score a .
.
.066cluster c leader node score a .
.
.
g .
.
.
final score a .
.
.
g .
.
.7462a b e c f d h i jk lgleader node basic block clusterfigure example of final score computation .
reporting to the programmer in general a program runs a static parallel section multiple times during its dynamic execution lime analysis operates on each dynamic instance independently because imbalance and other characteristics may vary across instances.
since programmers prefer feedback on static code lime then combines results from all dynamic instances of each parallel section using a weighted average with load imbalance of a dynamic instance serving as its weight.
for each leader node in a control flow cluster and for each hardware event e.g.
cache misses at static code location y in a hardware only cluster lime presents to the programmer its static code location type of event and weighted score.
.
profiling implementation the lime framework consists of two parts collection of profiling data and analysis of the data.
our contribution is primarily in the lime analyzer and our goal is to demonstrate the utility of our novel analysis techniques.
we built the analyzer in c using the armadillo linear algebra library .
there is a large body of prior work on profilers.
our framework can make use of any data profiler that can collect the required control flow and hardware event counts.
for our prototype tool we implemented two different versions of the profiler.
our first profiler is built on pin a binary instrumentation tool.
with this profiler we can collect control flow events but no hardware events.
we instrument synchronization points such as barriers to identify parallel sections in our program.
we also instrument all branch instructions and gather edge control flow event counts using a hashmap structure.
when branch instruction bis executed and the previous branch was p we increment the entry for edge p b .205benchmark suite parallel section type input sizeimbalance core core core core lu splash lu.c barrier matrix block .
.
.
.
volrend splash main.c barrier head .
.
.
.
fmm splash fmm.c barrier particles .
.
.
.
barnes splash code.c barrier particles .
.
.
.
canneal parsec annealer thread.cpp barrier swaps step steps .
.
.
.
fluidanimate parsec pthreads.cpp barrier 35k particles .
.
.
.
blackscholes parsec blackscholes.c t. join .
.
.
.
water sp splash interf.c barrier molecules .
.
.
.
radix small splash radix.c barrier integers small input .
.
.
.
swaptions parsec hjm securities.cpp t. join swaptions simulations .
.
.
.
radix splash radix.c barrier integers .
.
.
.
ocean splash slave2.c barrier grid .
.
.
.
fft splash fft.c barrier data points .
.
.
.
streamcluster parsec streamcluster.cpp barrier points .
.
.
.
radiosity splash radmain.c barrier room .
.
.
.
average .
.
.
.
table description of applications and parallel sections used in our experiments.
parallel section denotes the location of the pthread barrier wait orpthread join call that delimits the parallel section.
imbalance is the average percentage of idle time threads spend in the parallel section.
since dynamic instrumentation severely distorts execution time we use instruction count as an approximation to the execution time.
when a dynamic parallel section ends i.e.
a thread enters a barrier recorded data is exported to output files for later processing.
our second profiler is built on sesc a cycle accurate computer system simulator.
this simulator functionally emulates an application and uses the instruction stream to drive a detailed model of the caches memory and processor cores including the key microarchitectural structures e.g.
the instruction queue and reorder buffer which allow for out of order execution .
this simulator is routinely used by computer architects to evaluate architectural and microarchitectural proposals.
we use the simulator to collect cache miss event counts and control flow edge counts simultaneously.
event recording and data exporting is very similar to our pin based tool e.g.
control flow edges are counted with a hashmap .
the primary difference is that our sesc based tool can accurately measure execution cycles and hardware event counts but is much slower than the pin based version.
.
experiments and results in this section we present our experimental setup and the output of our lime analysis framework and then verify the output to show that lime reports imbalance related performance bugs accurately.
.
experimental setup we tested lime using data gathered from parallel sections in multithreaded applications from splash and parsec benchmark suites.
for runs with the sescbased profiler we simulate a typical multi core general purpose processor with cores at 1ghz 16kb each of data and instruction cache per core and 4mb of shared cache.
since the accuracy of lime depends on the number of threads i.e.
sample points we run our analysis on a varying number of cores and to verify that our scheme can find performance bugs accurately across a wide range of available cores.
in all configurations we run one application thread on each core.
table summarizes the location and type of each parallel section.
we sort the sections in order of decreasing imbalance on cores.
the tested parallel sections cover a widerange of imbalance from almost no imbalance radix for cores to over imbalance lu for cores .
.
simulator based profiling table summarizes the results of lime using the sescbased profiler.
the rpt columns show the number of important events those with a report score greater than .
for both control flow events and cache miss events.
the score column shows the highest score reported among control flow and among cache miss events.
the larger of the two highest scores is shown in bold font to emphasize the event type that is more important according to lime.
for all but two of the parallel sections lime is able to draw a consistent and clear conclusion on the most important event type.
for the top nine sections control flow events cause the load imbalance while for the next four cache miss events cause the imbalance.
for the bottom two parallel sections lime fails to find a consistent cause of imbalance this is because in our current profiling implementation we only collected control flow and cache miss event counts which do not cause the imbalance for these benchmarks.
we note that this is not a limitation of the lime analysis framework but rather a limitation of the even count profiling implementation.
section .
explains the true cause of imbalance for these two parallel sections and how it affects lime.
lime consistently reports a small number of important events this is important because it means the programmer should be able to inspect the spots in the source code that lime reports.
on average for benchmarks with imbalance from control flow it reports .
important control flow instructions per benchmark.
for benchmarks with imbalance from cache misses on average lime reports .
important code points per benchmark.
.
pin based profiling the limitations of the pin based implementation are that it introduces significant distortion in thread execution times and that it cannot accurately capture all the hardware interaction events that might be causing load imbalance.
however it can gather control flow edge information at speeds that allow real world problem sizes.
to circumvent execution time distortion in pin based pro 206benchmark8 cores cores cores cores ctrl flow cache miss ctrl flow cache miss ctrl flow cache miss ctrl flow cache miss rpt score rpt score rpt score rpt score rpt score rpt score rpt score rpt score lu .
.
.
.
.
.
.
.
volrend .
.
.
.
fmm .
.
.
.
.
.
.
.
barnes .
.
.
.
.
.
.
canneal .
.
.
.
.
.
.
.
fluidani.
.
.
.
.
.
.
.
.
water sp .
.
.
.
streamcls.
.
.
.
.
radiosity .
.
.
.
.
.
.
blackschls.
.
.
.
radix small .
.
.
.
radix .
.
.
.
ocean .
.
.
.
swaptions .
.
.
fft .
.
.
.
.
table results of lime analysis.
the rpt columns show the number of reported events with score greater than .
.
the number of all reported events is listed in parentheses.
the score columns give the reported score for the highest scored event of each type.
filing the instruction count is used as a proxy for execution time.
this eliminates the possibility of identifying hardwareinteraction events as causes of imbalance but this profiler does not collect those events anyway.
further the instruction count may not accurately represent the execution time.
we validate that this profiler is useful in quickly identifying control flow sources of load imbalance.
if hardware events trigger additional imbalance a more expensive simulatorbased approach can be used.
overall the control flow results of pin based profiling are very similar to those from the simulator.
for brevity table shows lime results for only five parallel sections for real inputs too large for simulation including one from a benchmark plsa from bioparallel benchmark that is infeasible to run in simulation.
also shown are simulationsize inputs for three benchmarks for comparison with scores from simulator based profiling shown in parentheses.
the profiling is done on an intel quad core xeon server using threads four cores each with support for two threads .
benchmark input size slowdownctrl flow rpt score lu1k 1k matrix .
.
.
16k 16k matrix .
.
barnes16 particles .
.
.
particles .
.
streamcluster4 points .
.
.
million points .
.
radiosity largeroom .
.
plsa 100k sequence .
.
table results with pin based profiling.
.
verifying reported cache misses while we have already shown that our framework consistently identifies a programmer manageable number of causes of load imbalance we now verify that our framework with the sesc profiler correctly identifies the causes of load imbalance starting with cache misses.
to verify that reported cache miss events are indeed causing load imbalance one possible approach would be to try to reorganize the data structures and algorithms in each application re evaluate parallel performance and check if load imbalance has been blacksc.
radix s radix ocean swaptions fftimbalance redu ction all high topfigure imbalance reduction of each parallel section when reported cache misses are eliminated.
reduced.
however we lack domain expertise and resources to make such extensive changes in so many applications.
further the results would highly depend on how well we understood each application data structure and algorithm.
instead we use cycle accurate simulation to artificially eliminate the reported cache misses while leaving all other aspects of the execution intact.
to erase misses from reported memory access instructions we override the simulated cache behavior for that instruction to make each dynamic instance of that instruction into a cache hit.
if the reported cache misses are indeed the source of the imbalance problem this modified execution should have a dramatically reduced load imbalance.
figure shows the results of this simulation for applications in which lime reports cache misses as the main cause of load imbalance.
the shaded portion of each bar represents the imbalance removed when only erasing the cache misses caused by the highest scoring instructions for each application.
the black portion represents additional imbalance removed when also erasing misses from all memory access instructions reported with final scores above .
.
the white portion is the additional imbalance removed when erasing all cache misses reported by lime as statistically significant causes of load imbalance.
for the first four applications in figure load imbalance is reduced dramatically when lime reported cache misses207668 if blockowner i j mynum parcel out blocks b a c a bmod a b c stri strj strk stri strk stri no.
address score code point func.
0x4018b4 .
lu.c lu report from our analysis reported source lines in lu.c long blockowner long i long j return i j p p number of threads figure lime report for lu.
are erased except for configurations that do not have significant load imbalance to begin with and core configurations for blackscholes .
it is important to note that in all these simulations we end up erasing misses from only .
of all dynamic memory accesses and the observed imbalance reduction is notcaused simply by a dramatic reduction in execution time in fact the speedup in these runs mostly comes from reducing imbalance making the slowest threads finish faster with little performance benefit for the fastest threads.
recall from section .
that lime did not find a consistent cause of imbalance for the last two applications because neither control flow nor cache misses are the true cause.
further investigation reveals that imbalance is actually caused by cores having different success in getting access to the l1 l2 on chip back side bus when servicing cache misses in these applications this bus has high utilization and the bus arbitrator seems to be favoring some cores at the expense of others.
we confirm this by simulating a higher bandwidth bus without erasing any misses and observing a imbalance reduction.
however unlike cache miss and many other events that can easily be attributed to particular instructions such attribution for bus contention events is an open problem that is beyond the scope of this paper.
.
verifying reported control flow causes to verify that lime correctly reports control flow events that cause imbalance we use a different methodology than for cache misses control flow events cannot be erased without affecting many other aspects including correctness of program execution.
thus in all parallel sections where lime reported control flow code locations as a significant cause of imbalance we manually confirm that the location and nature of the problem was correct.
for lack of space we only describe this analysis for three examples selected to both illustrate different types of imbalance causes and to only require brief code fragments for explanation.
.
.
lu among the parallel sections used in this paper luhas the most imbalance the last arriving thread takes over ten times longer than the first arriving thread.
for the core configuration our framework reports only one static instruction shown in figure as statistically significant cause of imbalance at line .
in the other three configurations lime also reports line as the topscoring by a large margin cause of imbalance.
the corresponding code point for the top scoring instruction is also shown in figure .
this is an if statement that no.
address score code point func.
0x4068ec .
render .c render report from our analysis reported source lines in render .c render int my node assumes direction is z if my node root observer t ransform light vector compute observer t ransformed highlight vector ray t race my node render my node if my node root writegrayscaletiff outfile image len ... writegrayscaletiff filename image len ... main.cfigure lime report for volrend .
parcels out blocks to threads using function blockowner which returns the summed block coordinates modulo number of threads p. intuitively this method of assigning blocks to threads should produce a balanced load.
however values of iand jvary in a range determined by the program s inputs they may be small and or not a multiple ofp causing uneven distribution of blocks to threads.
for example when iand jtake values through the blocks are distributed among threads and ideally each thread should get about blocks.
the actual assignment using i j p turns out to assign only one block to threads and blocks to threads and etc.
giving blocks to thread .
when line in the blockowner function is changed to return j ncols i nrows ncols where ncols and nrows are and respectively for a core configuration the assignment of blocks to threads becomes more balanced and results in eliminating of the original load imbalance.
this confirms that imbalance was indeed introduced at the code point reported by lime.
.
.
volrend our second verification example is from volrend which has up to .
imbalance on cores.
the lime report for cores summarized in figure suggests with high confidence that the return point of the function render is the source of imbalance.
at the first glance this looks like a false report but a closer inspection reveals that the problem is the mapping of the instruction to the line of source code because the compiler obfuscated the situation via inlining and other optimizations.
when we examine the code in a disassembler the reported control flow instruction is actually the if statement at line immediately after the callsite for render .
this if statement assigns extra work to the main thread.
when compiled without optimizations lime reports the if statement in line correctly.
.
.
barnes our third verification example is from barnes with .
imbalance on the core configuration.
the lime report for cores summarized in figure says the control flow edge from line in grav.c to line accounts for the imbalance.
this edge corresponds to the recursive function call towalksub barnes implements the barnes hut approach for the n body problem and walksub recursively traverses208105 void walksub nodeptr n real dsq long processid nodeptr nn if subdivp n dsq processid first branch in walksub if t ype n cell for nn subp n nn subp n nsub nn if nn !
null walksub nn dsq .
processid no.
address score code point func.
0x405470 .
grav.c grav .c 0x40548c .
grav.c walksub 0x4055cc .
grav.c grav.c 114report from our analysis reported source lines in grav .cfigure lime report for barnes .
the primary data structure a tree.
since lime reports the tree traversal is imbalanced this suggests that the tree itself is imbalanced.
further investigation shows that .
of the total imbalance comes from the first dynamic instance of the parallel section because the tree is skewed at the beginning of the run.
the first barnes hut iteration rebalances the tree and the load imbalance decreases.
.
scalability of lime while lime ran fast enough for this study two possible concerns are what happens to analysis time as the core count increases and what happens to analysis time as event count i.e.
program size increases?
a single threaded implementation of lime analysis took an average of .
.
.
and .
seconds to analyze the parallel sections for and cores respectively on a .67ghz intel quad core xeon processor with 12gb memory.
this time includes clustering leader selection and regression.
since the analysis time grows in sub linear proportion to the core count we expect that a parallel implementation of lime analysis will have a favorable scaling trend as the number of cores for both application execution and analysis increases.
the most time consuming part of lime is the clustering part of the analysis it accounts for over of the average analysis time when no optimization is applied.
a naive implementation of our hierarchical clustering method has asymptotic complexity of o n3 wherenis the number of event counts to create a near constant number of clusters lime does o n merges and for each merge it recomputes all o n2 entries in the new cluster proximity matrix.
this would be a major problem for applying lime to real applications.
to accelerate the clustering algorithm we can cache the proximity matrix and perform bulk clustering.
when merging two clusters only the proximity values involving those two clusters become useless.
therefore we re use cache the proximities and compute only o n new values.
the bulk clustering optimization merges multiple clusters per step as we compute the proximity matrix we track clusters that are very close to each other and merge all of them at once.
in this study we use a proximity threshold of .
for bulk clustering.
bulk clustering reduces the number of merges but runs the risk of producing less precise clustering.
we did not experience any imprecise clustering in our experiments.
figure compares the speed of the clustering implemen0.
.
.
2000time sec number of events core no opt.
core core no opt.
core core no opt.
core core no opt.
core linear quadraticfigure clustering time vs. number of events.
each point represents clustering time of one parallel section.
note the logarithmic scale markings on each of the axes.
tation with and without our acceleration technique using a log log plot.
while our optimized implementation is still o n2 bulk clustering does not reduce the asymptotic complexity in practice it shows near linear scaling with n. this demonstrates that lime is scalable with program size.
further optimizations are certainly possible but we leave them for future experimentation and fine tuning.
.
related work performance debugging has long been studied in distributed systems.
much work focuses on finding the causal trace or the trace with the longest path through a distributed system this is analogous to the slowest thread in our study.
the causal trace naturally tells programmers where to focus optimization efforts.
lime shares the same goal as this work but works in a different domain.
among the related literature the performance debugging method proposed by aguilera et al.
draws our attention they use a black box approach that finds the causal trace without any knowledge of the system and a signal processing technique called convolution to infer causal relationships.
lime collects profile data without a programmer s intervention and uses clustering and regression analysis to infer causal relations between events and load imbalance.
a number of tools exist to detect and measure parallel overheads and inefficiency i.e.
idleness .
one recent example is from tallent et al.
.
their goal is to pinpoint where parallel bottlenecks occur and classify bottlenecks as overhead or idleness.
our work is largely orthogonal to this once a programmer knows that a problem exists they can use our framework to help find the cause for the bottlenecks they have detected.
tallent et al.
also have work on analyzing lock contention .
this has a lot in common with our work since they try to detect the cause of lock contention and identify the lockholder to blame.
our work focuses on barriers rather than locks and provides more direct information about where in the code the problem arises.
there is also significant work on trying to optimize performance and energy in the presence of load imbalance.
thrifty barrier predicts how much slack i.e.
idle time each thread will have using a history based predictor and saves power by putting non critical threads into a sleep state.
meeting points uses a different predictor that counts thread deviation at checkpoints called meeting points.
it delays non critical threads using dynamic voltage and frequency scaling to save power.
it also attempts to accelerate209the critical thread by prioritizing it.
the thread criticality predictor similarly predicts thread criticality based on adjusted cache miss rates and prioritizes threads based on criticality.
while automatically detecting and prioritizing critical threads requires no programmer effort it can only reduce load imbalance by a limited amount.
instead we help programmers find and permanently fix imbalance problems in applications regardless of how severe the problems are this can and usually does have a dramatic performance impact.
.
conclusions this paper addresses a major problem in achieving good performance on multi core processors load imbalance.
the existence of load imbalance is relatively easy to detect but it is often very challenging to determine the cause of the problem and the point in the source code where the problem is introduced.
we propose lime a framework that automates this process.
lime first uses profiling to collect counts of control flow events and hardware events for the different threads.
it then uses clustering and regression to identify the small set of events that introduce significant amounts of imbalance.
we show that lime provides accurate and useful feedback to programmers on a set of popular parallel benchmarks.
we also show that our lime prototype runs fast enough to be used on large programs.
.