scaling exact multi objective combinatorial optimization by parallelization jianmei guo edward zulkoski rafael olaechea derek rayside krzysztof czarnecki university of waterloo canada sven apel university of passau germanyjoanne m. atlee university of waterloo canada abstract multi objectivecombinatorialoptimization moco isfundamental to the development and optimization of software systems.
we propose five novel parallel algorithms for solving moco problems exactly and efficiently.
our algorithmsrelyonoff the shelfsolverstosearchforexactpareto optimalsolutions and they parallelize the search via collaborativecommunication divide and conquer or both.
we demon strate the feasibility and performance of our algorithms by experiments on three case studies of software system designs.
a key finding is that one algorithm which we callfs gia achieves substantial even super linear speedupsthat scale well up to cores.
furthermore we analyzethe performance bottlenecks and opportunities of our par allel algorithms which facilitates further research on exact parallel moco.
categories and subject descriptors d. .
management software configuration management g. .
optimization constrained optimization keywords multi objective combinatorial optimization parallelization .
introduction multi objective combinatorial optimization moco explores a finite search space of feasible solutions and finds the optimal ones that balance multiple often conflicting objectives simultaneously.
moco is a fundamental challenge in many design and development problems in engineering andotherdomains.
forexample inmobile phonesystemdesign one often has to choose between different candidate designsthat tradeoff multiplecompeting objectives s u c ha sl o wc o s t and high performance.
each candidate design i.e.
a feasible solution involves a wide variety of design options which permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributedfor pro fit or commercial advantage and that copi es bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or re publish to post on servers or to redistribute to lists requires prior speci fic permission and or a fee.
request permissions from permissions acm.org.
ase september vasteras sweden.
copyright acm ... .
.
call features e.g.
enabling video calls with respect to a set of constraints e.g.
video calls requires camera a n d quality attributes e.g.
the cost of enabling video calls .
in the worst case the search space of candidate designs grows exponentially in the number of features.
exploring such a huge search space is often beyond human capabilities and makes optimal system design a very challenging task.
inthe same way many software engineering problems such asarchitecture design test data generation andproject planning involve the same form of moco.
moco problems are mostly np hard .
to address them approximateapproachesthatdependmainly on metaheuristics 1have been advocated for years.
in most cases they solve moco problems in an acceptable time but theyfind only near optimal solutions and often suffer from pa rameter sensitivity i.e.
the accuracy of the found solutions varies widely with the parameter settings of these approaches .
in contrast exact methods that scan allcandidate solutions one by one often take too long for large scale problems but they are accurate in finding all exact optimal solutions which is desirable for those stakeholderswho never want to miss any optimal opportunity.
parallel computingcarries outmultiplecalculations simultaneously on multiple processors .
it divides a large com putingproblemintomultiplesmaller onesandsolves theminparallel often with a significant performance improvement.in the past metaheuristics have been parallelized to addressmoco problems efficiently .
however there are only few parallel algorithms for exact moco .
we aim at exact parallel approaches that solve moco problems accurately and efficiently.
as a baseline we choose theguided improvement algorithm gia a generalpurpose sequential algorithm for solving moco problems exactly.
gia works with most off the shelf sat satisfiability smt satisfiability modulo theories and csp constraint satisfaction problem solvers.
accordingly thefirstparallel algorithm wepropose which wecall parallel gia pargia performs multiple gias simultaneously andcollaboratively.
in pargia each processor runs a gia with a different starting point in the search space once a processor finds an optimal solution it communicates the solutionto other processors so as to reduce duplicate searches.
1a metaheuristic is a strategy for exploring the search space of aproblemusingavarietyof methodsthattypicallyconsist of both a diversification i.e.
mechanisms to explore thesearch space and an intensification i.e.
mechanisms thatexploit previously found solutions procedure .
to further scale moco we propose two parallel divideand conquer algorithms objective split gia os gia and feature split gia fs gia .
os gia geometrically divides the search space of a moco problem into subspaces and then runs in parallel a gia for each subspace.
fs gia re cursively partitions a moco problem into subproblems byselecting anddeselecting certainfeatures thenitperforms in parallel a gia for each subproblem.
lastly we propose two hybrid parallel algorithms os pargia andfs pargia i n which the gia algorithm in os gia and fs gia respec tively is replaced with pargia.
we implemented our proposed algorithms and evaluated them in a series of experiments on three case studies of software system design.
our empirical results demonstrate the feasibility and performance of our parallel algorithms.in particular fs gia shows a desirable scalability with anincreasing number of available processors and it achieveseven super linear speedups.
in summary we make the following contributions algorithms.
we propose five novel parallel moco algorithms that search for exact optimal solutions us ing off the shelf solvers and that parallelize the searchvia collaborative communication divide and conquer or both.
implementation.
we implement our proposed algorithms and publish the source code and experimentaldata at .
evaluation.
we evaluate the performance and scalability of our parallel algorithms and analyze their per formance bottlenecks.
prospects.
we open a new direction in scaling exact moco algorithms and demonstrate the potential ofparallelization for exact moco.
.
preliminaries first we introduce moco by means of the example of mobile phonesystem design including a set of features con straints quality attributes and objectives.
next we de scribe how tospecify features and theirconstraints in propo sitional formulas and how to use solvers to acquire feasi ble solutions.
finally we describe the concepts of paretooptimal solutions and present how gia our baseline sequential algorithm finds all pareto optimal solutions and this way solves moco problems exactly.
.
a running example the design of a software system deployed on a mobile phone can be characterized by the features supported by the phone and the constraints defined between the features.
the features represent various design options.
for exam ple the phone s software may optionally include support forfeature video calls .
figure shows a design scenario of a mobile phone system adapted from benavides et al.
.we organize and visualize its features and their constraints using a feature model a tree structure in which each node except the root i.e.
mobilephone has one parent.
relations between a feature and its group of child features are 2speedup is defined as sp t1 tp w h e r et1is the sequential execution time of a problem and tpis the parallel execution time of the same problem using pprocessors.
linear speedup is obtained when sp pand super linear speedup is achieved when sp p.mobilephone os android ioscalls voice video cost lat.
0cost .
lat.
0cost .
lat.
.5cost lat.
0connectivity usb wifi camera bluetoothfeature quality attribute mandatory optional and group or group alternative group cross tree constraints video requires camera objectives minimizing cost minimizing latency figure a sample moco problem arising in mobile phone system design classified as and no arc or filled arc and alternativegroups arc .
the members of an and group can be eithermandatory filled circle or optional empty circle .
cross tree constraints comprise requires and excludes relations be tween features e.g.
videorequires camera .
stakeholders can customize the mobile phone system by selecting features thereby deriving different design variantsthat have different quality attributes e.g.
cost and latency .
each feature may have an influenceon the quality attributesof a feasible system design that can be implemented andmeasured .
for example selecting or deselecting feature bluetooth in figure increases or reduces the cost by and the latency by in the final mobile phone system.
the quality attributes of a feasible system design can be cal culated by aggregating the quality attributes of all selectedfeatures and feature interactions involved .
in this pa per we assume that we already have the quality attributes of features and potential feature interactions.
for research on measuring and inferring the quality attributes we referthe interested reader elsewhere .
among the many feasible design variants stakeholders often desire the optimal one that can simultaneously meetmultiple design objectives such as minimizing cost and minimizing latency.
this is a typical moco problem.
however the objectives are often conflicting.
for example a mobile phone system often achieves a lower latency only by raisingits cost e.g.
using a larger cache .
thus a moco problemusually has a set of optimal solutions not only a single one.finding all optimal solutions accurately and efficiently is a major challenge which we address in this paper.
.
speci fication and validation the solutions to a moco problem must be feasible i.e.
satisfying the constraints defined between features.
to this end we specify all features and their constraints in proposi tional formulas and then we use off the shelf sat smt or csp solvers to return feasible solutions.
note that combinatorialoptimization eithersingle objectiveormulti objective explores a finitesearch space and each feature ranges over a finite domain .
in general a finite domain featurecan be converted to a finite set of boolean domain features .
here we use only boolean domain features to describe moco problems.
in a nutshell we represent a feature as 410table feature selection constraints in propositional logic prepresents a parent feature and c1 ... c nare its child features m ... n denotes the mandatory features by their indices in an andgroup f1andf2denote arbitrary features type propositional formulas mandatory c1 p optional c1 p and group p logicalandtext i mci logicalortext i nci p or group p logicalortext i nci alternative group p logicalortext i nci logicalandtext i j ci cj requires f1 f2 excludes f1 f2 a boolean decision variable.
if a feature is supported by a system design then its corresponding variable is assigned true a n d falseotherwise.
furthermore we specify the constraints between features in propositional formulas.
for example the constraints defined in feature models can beformulated in propositional logic as summarized in table .
asolution is a selection of features i.e.
an assignment of value trueorfalseto the decision variable of each feature.
a feasible solution is a validselection of features that respects all constraints defined between features.
using off the shelf solvers we are able to check whether a potentialsolution satisfies all constraints and thus is a feasible solu tion .
for example by using a sat solver we determine that solution os android calls video connectivity camera in figure is feasible but solution os android calls video connectivity wifi is not because the latter violates the constraint videorequires camera.
.
pareto optimal solutions and gia as a baseline we use gia guided improvement algorithm todeterminewhetherafeasible solutionisparetooptimal and where to direct the search next.
figure illustrates the search space of the moco problem of figure in which each point indicates a solution.3given multiple potentially conflicting objectives a solution is pareto optimal if it is not dominated by any other solution.
a solution dominatesanother solution when it is better regarding at least one objective and not worse regarding all the other objec tives.
according to the definition of pareto dominance a solution partitions a search space into three areas inferior superior a n d equilibrium .
for example in figure the inferior area top right of solution s 5includes all solutions that are dominated by s .
the superior area bottom left contains all solutions that dominate solution s .a n y s o lution in the equilibrium areas top left and bottom right does not dominate solution s 5and at the same time is not dominated by s .
all pareto optimal solutions filled points constitute the pareto front .
gia uses a solver to return a solution and then augments the constraints to search for solutions that domi nate ones found already.
moreover gia incrementally findspareto optimal solutions duringcomputation and thusguar antees thatall solutions yielded by the algorithm are pareto3unless otherwise specified all solutions mentioned in the following sections are feasible.s1 s2s3 costminimizationlatency minimizations6 s5 s4s7 figure solution s 5partitions the search space into inferior top right superior bottom left and equilibrium top left and bottom right areas algorithm guided improvement algorithm gia input m a o output s 1s 2eqc true 3s solveone m 4whiles negationslash nulldo whiles negationslash nulldo s prime s supc gensupc s m a o s solveone m eqc supc 9s s s prime 10eqc eqc geneqc s prime m a o s solveone m eqc 12returns s6 s5 s4s7 s3 s2s1 a s6 s5 s4s7 s3 s2s1 b figure a gia returns solution s 4and calculates the constraints of the superior area of s b g i a finds a pareto optimal solution s 1and calculates the constraints of the equilibrium area of s optimal even if one terminates the execution halfway.
thisgains a competitive advantage over other exact methods .
algorithm lists thepseudo codeof gia.gia starts with am o d e l m a set of quality attributes a and multiple objectiveso.
the model mspecifies all features and their constraints as propositional formulas.
the output is the pareto fronts.
as demonstrated in figure 3a gia first inputs a 411model into a solver and computes a solution s line in algorithm .
gia calculates the constraints of the superior area of solution s grey area in figure 3a and addsthem to the solver for finding another solution dominating s lines .
a pareto optimal solution has been found when thereis no solution in its superior area.
as shown in figure 3b solution s 1is marked as pareto optimal filled .
next gia calculates the constraints of the equilibrium area of solutions grey areas in figure 3b and adds them to the solver for finding other pareto optima l solutions lines .
.
parallel algorithms in this section we present five novel parallel algorithms for exactly solving moco problems.
we use the exam ple of figures and to explain them.
the input of eachalgorithm includes a model mspecifying features and constraints a set of quality attributes a multiple objectives o and the number of available processors p. the output is the pareto front s. .
pargia pargia uses pprocessors to perform pgias simultaneously and collaboratively over the search space of a given moco problem.
each processor runs a gia instance to search for pareto optimal solutions and then communicatesthe constraints of the found solutions to all other processors.algorithm lists the pseudo code of pargia.
figure illus trates the search process of pargia using two processors.processor p 1inputs a model into a solver and computes as o l u t i o ns p 1keeps searching until it finds a paretooptimal solution s .
meanwhile processor p 2starts with as o l u t i o ns 4and then finds another pareto optimal solution s .
if processors p 1and p 2find s 1and s 2at exactly the same time they communicate to each other the con straints of the equilibrium areas of s 1and s lines in algorithm .
next both processors search the combinedequilibrium area grey areas in figure 4b looking for other pareto optimal solutions.
however in most cases the search processes of processors p 1and p 2are not synchronous.
for example processor p 1may find solution s before processor p 2reaches solution s .
in this case processor p 1communicates the constraints of the equilibrium area of s 1to processor p but it does not receive any constraintsfromp .
subsequently processorp 1keepssearching for other pareto optimal solutions in the equilibrium areaof s w h i c hm a yc a u s et h e overlapping search of solution s2that will be found by processor p 2later.
following the idea of optimistic parallelism pargia does not control or avoid potentially overlapping search of the same pareto optimal solutions.
but after all processors finish searching pargia checks all found pareto optimal solutions and re moves duplicates line .
.
os gia and os pargia os gia geometrically divides the search space of a given moco problem into psubspaces and then simultaneously conquersall subspacesusing pprocessors.
ineach subspace a gia instance is performed by one processor to search forpareto optimal solutions.
algorithm lists the pseudo codeof os gia.
we adopt the idea of cone separation to divide the search space.
first os gia converts all objectivesto be all minimizations or all maximizations line in algorithm3 .
notethatanobjectivetomaximize xisequivalent algorithm pargia input m a o p output s 1s 2foralli 1topin parallel do 3eqc i true s solveone m whiles negationslash nulldo whiles negationslash nulldo s prime s supc i gensupc s m a o s solveone m eqc i supc i s s s prime forj 1topdo eqc j eqc j geneqc s prime m a o s solveone m eqc i 14s postprocess s 15returns s6 s5 s4s7 s3 s2s1p1 p2 a s6 s5 s4s7 s3 s2s1p1 p2 b figure a pargia finds two solutions s 7and s simultaneously using two processors and calculates the constraints of the superior areas of the two so lutions b pargia finds two pareto optimal solu tions s 1and s 2and calculates the constraints of the combined equilibrium areas of the two solutions to the objective tominimize x. withoutloss of generality we assume that all objectives involve minimizations as the example of figures and .
then os gia determines areference point the crossed point in figure 5a whose value regarding each objective is sufficiently large such that all solutions are located in its superior area.
ifthegiven moco problemhasmore than two objectives os gia projects the search space onto a bi dimensionalplane formed by two chosen objectives line .
next os gia divides theprojective plane into conesby starting from the reference point dividing the angle encompassing the superior area of the reference point into pequal parts lines .
figure 5a shows the result of dividing the projectiveinto two cones each cone has a angle starting from the reference point.
finally os gia applies a gia to each cone to search for pareto optimal solutions line .
the overhead of geometric decomposition is very small.
however choosing the objectives that determine the projective plane and cones plays a critical role in the eventual 4if all objectives are maximizations the reference must be small enough regarding all the objectives.
412p1 p245 minimizationminimization a p1 p2 p3 p445 minimizationminimization b figure after geometrically dividing the search space of figure into two cones a os gia performs a gia instance using one processor in eachcone and b os pargia performs a pargia in stance using two processors in each cone algorithm os gia input m a o p output s 1s 2o preprocess o 3o2 choosetwo o 4forj 1topdo 5mj objectivesplit m a o p j 6forallj 1topin parallel do 7sj gia mj a o 8s s sj 9s postprocess s 10returns algorithm os pargia input m a o p output s 1s 2q p 3o preprocess o 4o2 choosetwo o 5forj 1toqdo 6mj objectivesplit m a o q j 7forallj 1toqin parallel do 8sj pargia mj a o 9s s sj 10s postprocess s 11returns workload of each processor for each cone.
in the worst case a poor selection of the two objectives may cause all solutionsto fall into one cone of theplane which results in one proces sor overloaded and the others idle.
to balance the workload of different processors we follow a straightforward intuition of choosing the two objectives that have the first and sec ond largest value ranges to form a large projective plane inwhich all solutions are distributed as evenly as possible.
note that the pareto optimal solutions found by each processor are localto that processor s cone.
moreover a solution that is locally pareto optimal in a cone may not beglobally pareto optimal in the entire search space.
therefore after all processors finish searching os gia collects all local paretofronts foundbyeachprocessor line8 removesduplicate solutions and calculates the globalpareto front in the search space line .
such calculation is straightfor ward and usually takes little time.
os pargia is a hybrid of os gia and pargia.
algorithm lists the pseudo code of os pargia.
as shown infigure 5b the key difference between os pargia and os gia is that os pargia searches each cone using a pargiainstance with two processors instead of using a gia instancewith one processor line in algorithm .
moreover theinputpofos pargiaisdoublethenumberofdividedcones line .
of course one can perform pargia using morethan two processors per cone but the performance gainsare not necessarily higher when using more processors aswe demonstrate in section .
.
.
fs gia and fs pargia fs gia divides a given moco problem into psubproblems and then simultaneously conquers all subproblems us ingpprocessors.
for each subproblem a gia instance is performed by one processor to search for pareto optimalsolutions.
the key idea of fs gia is to divide a moco problemintosubproblemsofrelatively equal size w h e r ee a c h subproblem is defined by a partial feature selection that represents a subset of solutions.
if a moco problem has n solutions in total fs gia partitions the set of nsolutions intopsubsets of roughly equal size ideally n psolutions .
subsequently fs gia uses the gia algorithm to search for pareto optimal solutions among each subset.
the approach has the potential to significantly reduce the number of solu tions that each processor has to explore by up to a factor ofp leading to a significant performance improvement.
algorithm lists the pseudo code of fs gia.
a major challenge of fs gia is to accurately and quickly calculate the number of all solutions selecting or deselect ing a certain feature.
this reduces to the well known sat problem where the goal is to count the number of satisfying variable assignments x for a given propositional formula x. although this is generally a hard problem existing sat solvers such as sharpsat which we use for fs gia 5are capable of quickly calculating the exact number of solutions for the propositional formulas of our casestudies as we demonstrate in section .
.
toacquiresubproblemsasequal sizedaspossible fs gia recursively chooses an appropriate feature and partitions each moco subproblemintoasubsubproblemthatincludesthe feature and a subsubproblem that does not lines 4in algorithm .
as shown in figure fs gia starts withthe input model mof a given moco problem.
fs gia exhaustively tests every feature fand computes the numberof solutions for submodels m fandm f i.e.
the number of valid assignments when the corresponding variable of fis trueorfalse .
then fs giachooses the best splitfeature e.g.
f 1in figure to divide model minto two submodels such that m f m f is minimal if multiple features satisfy the above equation one is ran domly chosen amongst them.
each submodel is divided recursively by further split features such as features f 2and .
413m m f1choose a feature fthat minimizes m f m f m f1 m f1 f2m f1 f2 m f1 f3m f1 f3f1 f1 f2 f2 f3 f3 figure fs gia recursively chooses features f1 f2 andf3to divide a given moco problem into four equal sized subproblems algorithm fs gia input m a o p output s 1s 2f parallelpreprocess m p 3forj 1topdo 4mj generate m f j 5forallj 1topin parallel do 6sj gia mj a o 7s s sj 8s postprocess s 9returns algorithm fs pargia input m a o p output s 1s 2q p 3f parallelpreprocess m q 4forj 1toqdo 5mj generate m f j 6forallj 1toqin parallel do 7sj pargia mj a o 8s s sj 9s postprocess s 10returns f3in figure .
the above recursive division process forms a binary decision tree as shown in figure .
note that the submodels generated at the same levelo ft h et r e ea r ee q u a l sized and non overlapping.
they represent the subproblemswe need for fs gia.
hence the number of generated sub problems has to be a power of two which equals to thenumber of assigned processors p since each subproblem is assigned one processor.
if we carry out the recursive division process of fs gia sequentially the dividing overhead increases linearly withp.
as shown in figure fs gia has to choose three split features and divide three times to acquire four subproblemsfor four processors.
to reduce the dividing overhead of fs gia we parallelize the preprocessing of fs gia line inalgorithm to choose split features simultaneously.
for example in figure the preprocessing tasks of choosingfeatures f 2andf3can be run simultaneously.
in general subproblems with the same number of split feature clauses can be further divided in parallel in step i 2i 1subproblems are split roughly in half producing 2ismaller subproblems.
thus the overheadof partitioning themodelincreaseslogarithmically withp.
fs pargia is a hybrid of fs gia and pargia.
algorithm lists the pseudo code of fs pargia.
the key dif ference between fs pargia and fs gia is that for eachsubproblem fs pargia performs a pargia using multipleprocessors instead of a gia using one processor.
for exam ple algorithm lines and addresses each subproblemusing two processors.
hence the input pof fs pargia is double the pof fs gia when addressing the same number of subproblems.
as with os gia and os pargia thepareto fronts found by fs gia and fs pargia are localto the corresponding subproblems.
thus a post processingstep algorithm5 line8 andalgorithm6 line9 identifieswhich locally pareto optimal solutions make up the global pareto front with respect to the entire search space.
this step takes proportionately little time.
.
implementation we implemented our parallel algorithms using python .
and its multiprocessing package.
the multiprocessingpackage effectively steps aside the issue of global interpreter lock by using subprocesses instead of threads.6it allows programmers to fully leverage multiple processors ona given machine in which each spawned subprocess is assigned a processor.
following the idea of optimistic parallelism and reducing the communication cost for pargia we avoid using any synchronization primitives such as locks but we usemessage passing mechanisms such as queues which provide a process safe communication channel.
thus each processor in pargia has its own queue to manage the constraints of the pareto optimal points found by itself and then commu nicates the constraints to the queues of other processors.
a recent study shows that smt might be the most efficient reasoning formalism in checking model properties compared to csp alloy and answer set programming asp .
smt combines standard sat with richer theo ries suchasequalityreasoning linear arithmetic bitvectors andarrays .
therefore inourimplementation wereasonabout moco problems in smt solvers.
in particular weimplemented gia and all five parallel algorithms with the efficient smt solver z3 developed at microsoft research.
.
evaluation we conducted a series of experiments to evaluate the parallel algorithms we propose.
we aim at investigating the performance and scalability of each algorithm and at iden tifying potential bottlenecks and opportunities.
.
subjects weevaluatedourparallel algorithms onthreemoco case studies from the domain of software system designs.
as moco problems reported by industry are not 6global interpreter lock is the mechanism used by python interpreter to assure that only one thread executes python bytecode at a time.
.
414table overview of subject systems features solutions objectives sas web portal e shop .02e available publicly we resorted to using three case studies from existing literature as subjects.
the first subject sas optimizesthearchitectureofareal worldsituationawareness system that deploys personnel in emergency re sponse scenarios .
the original authors collected seveninteger and float quality attributes for each feature.
ac cordingly sas has seven objectives minimizing cost maximizing reliability minimizing battery usage minimizing response time minimizing ramp up time minimizing develop ment time and minimizi ng deployment time.
web portal ande shop are two product line design models available at the splot website which is a popular repository of software system models used by many researchers.
sayyad et al.
extended the two models andformulated corresponding moco problems by adding inte ger and float quality attributes for each feature and defininga set of objectives.
they randomly generated quality at tributes resembling real world project characteristics .
we use the same data and target the same objectives they defined for both case studies minimizing cost minimizingthe number of defects maximizing the number of offeredfeatures and maximizing the number of features that wereused before.
as listed in table these case studies cover a reasonable spectrum of moco problems with different characteristics different number of features to different numberof solutions to orders of magnitude different num ber of objectives to different quality attributes e.g.
cost response time battery usage reliability and softwaredefects different value types of quality attributes integer and float and different optimization directions minimization and maximization .
furthermore finding the paretofront of these case studies takes different time ranges fromseconds to minutes and days .
.
experimental setup we conducted our experiments on sharcnet w h i c hi s a consortium of canadian academic institutions that share anetwork of high performance computers.
8to reduce fluctuations in measurements caused by different hardware envi ronments we performed all measurements on the same clus ter comprising amd opteron cores each at .2ghz and a total of 640gb ram.
however we were not able tooccupy all resources e.g.
cores and memory on the clus ter at all times because sharcnet uses a priority queue to schedule the jobs submitted to a cluster.
the priorityof a job is ranked according to the resources requested bythe job.
more resources requested result in lower priorityand more waiting time in the queue.
due to the limitationof resources and time our experiments use up to coressimultaneously for each job submitted to the cluster.
in our experiments the independent variables are thesubject systems the evaluated algorithms and the number of .assigned cores.
we measured the execution time of each sequential and parallel algorithm finding the pareto front of a certain subject system and we calculated the speedups ofeach parallel algorithm as the dependent variables.u n f o r tunately we were not able to determine all pareto optimalsolutions of e shop using any of the algorithms in six days which is the maximum execution time allocated to a job submitted to sharcnet.
hence we measured the number of pareto optimal solutions found by each algorithm in six days as the dependent variable for e shop.
to reduce fluctuations in the values of dependent variables caused by randomness e.g.
the random seeds usedby solvers to return a solution we evaluated each combination of the independent variables times.
that is for each subject system we executed each of the algorithms with thesame number of assigned cores times and we measuredthe resulting execution times.
we report only the means ofthe execution times for analysis.
as a baseline we performed sequential gia on the three subjects for performance comparison and speedup calcula tion.
on average gia takes .
seconds to find the paretofront of sasand .
minutes to find the pareto front of web portal .f o r e shop gia finds an average of only one pareto optimal solution in six days.
.
performance comparison in a first set of experiments we compare the performance of our parallel algorithms.
we aim at determining whichalgorithm has the best performance is the most scalable and is the most promising to solve large moco problemssuch as e shop.
to obtain results in reasonable time we use at most processors simultaneously and apply eachalgorithm only to sasandweb portal .
results.
figure presents the speedups of each parallel algorithm finding the pareto fronts of sasandweb portalusing up to processors.
fs giaachieves super linear speedups when using two processors for sasand when using to processors for web portal .
furthermore the speedup of fs gia increases steadily and rapidly when the number of available processors increases.
fs pargia acquires super linear speedups when using to processors for web portal .
using the same number of processors the speedup of fs pargia is less than that of fs gia.
also the speedup of fs pargia has a stableincreasing trend with the increasing number of processors but the speedup increasing rate of fs pargia is lower than that of fs gia.
for pargia os gia and os pargia we did not observe super linear or linear speedups in either case study.the speedup of pargia increases steadily and slowly as the number of processors increases and then reaches a plateau e.g.
when using or more processors for sas o ru s i n g5 or more processors for web portal .
the speedups of os gia and os pargia fluctuate considerably.
in some cases the speedups of os gia and os pargia are less than one which means that the execution time of os gia and os pargia is even longer than the execution time of the sequential algorithm gia.
discussion.
fs giareliesonasoundandefficient sat solver to control the overhead of dividing a given mocoproblemintosubproblemsandtoguaranteethatallsubprob lems are as equal sized as possible.
in the search space of each subproblem the number of solutions that fs gia ex415 .
.
.
.
.
.
.
.
.
.
.
.
processorsspeedup fs gia fs pargiaos giaos pargiapargia a sas processorsspeedup fs gia fs pargiaos giaos pargiapargia b web portal figure speedups of five parallel algorithms finding the pareto fronts of a sas and b web portal u s i n gu pt o1 6p r o c e s s o r s plores has been reduced substantially which results in lower workload of conqueringeach subproblem.
weareaware that equal sized subproblems may not be equally hard that is solving each equal sized subproblems may still need dif ferentworkload.
however ourempiricalresultsdemonstratethat fs gia effectively balances the overhead of the divideand conquer scheme and gains even super linear speedups.
in both case studies fs gia achieves the best performance.more importantly fs gia presents a desirable scalabilitywhen the number of available processors increases from to16 and the size of case studies augments from sastoweb portal.
fs pargia uses the same divide and conquer approach as fs gia.
for each divided subproblem fs pargia per forms a pargia using two processors while fs gia runs agia using one processor.
when using the same number ofprocessors fs gia works more efficiently than fs pargia.however when addressing the same number of subproblems fs pargia usually works more efficiently than fs gia.
for example working with the same subproblems ofweb portal fs pargia using processors gains a higher speedupthanfs giausing processors.
therefore thereis a trade off of using more processors for divide and conqueror for collaborative communicationin eachsubproblem.
ourempirical results show that it is worthwhile to spare moreprocessors to perform more equal sized subproblems instead of more gia instances in each subproblem especially when the number of available processors is limited.
pargia implements collaborative communication using message passing.
however due to the latency of messagepassing a processor may already start searching an overlap ping area before receiving the information about that area from other processors.
the problem of overlapping searches may get worse when using more processors.
according toour empirical results pargia suffers from a performancebottleneck when running a large number of gia instancessimultaneously and its speedup does not scale well with theincreasing number of processors.
os gia and os pargia geometrically divide the search space of a given moco problem into cones by projective plane selection and cone separation.
however it is hard toforesee how solutions are distributed in the global searchspace and in the projective plane.
hence the algorithmscannot guarantee that the workload of conquering each cone i.e.
finding pareto optimal solutions in each cone is effectively balanced among processors even though the cost ofdividing the original moco problem i.e.
obtaining cones is trivial.
unbalanced loads among processors gives riseto performance fluctuations when using os gia and ospargia as demonstrated by our experiments.
in the worst case finding the local pareto front in a cone may take moretime than findingthe global pareto front in the entire searchspace.
this can happen when the number of locally pareto optimal solutions in a cone is higher than the number ofglobally pareto optimal solutions in the entire search space.
.
scalability of fs gia in a second series of experiments we further explore the scalability of fs gia as it performed best in the first setof experiments.
we applied fs gia to all three case stud ies using up to processors simultaneously.
we measuredthe preprocessing time of fs gia i.e.
the time of dividinga given moco problem into roughly equal sized subproblems .
then we analyzed the impact of the preprocessing time on the scalability of fs gia.
results.
figure 8a shows the speedups of fs gia finding the pareto front of sasu s i n gu pt o6 4p r o c e s s o r s .f o r the normal execution time including the preprocessing solid line we observe that fs gia has a stable increasing trend of speedups when using to processors but the increas ing rate significantly slows down when using more than 32processors.
in contrast if we ignore the preprocessing time dashed line then the speedup of fs gia maintains a sta ble and rapid increasing trend with the increasing number of processors.
forweb portal as shown in figure 8b the preprocessing time has little impact on the speedup of fs gia.
regardless of whether the preprocessing time is included inthe execution time of fs gia f s gia reaches super linear speedups that scale well to any number of processors from 2t o6 .
processorsspeedup with preprocessing without preprocessing a sas processorsspeedup with preprocessing without preprocessing b web portal figure speedups of fs gia with and without preprocessing using up to processors to find the pareto front for sas and web portal fore shop fs gia finds more pareto optimal solutions when using more processors.
this is in contrast to the single pareto optimal solution found by the sequential algorithmgiaoverthecourseof six days.
ifwedeterminethespeedupin terms of the number of pareto optimal solutions found insix days as shown in figure fs gia still provides superlinearspeedupsinallcases rangingfrom 2to64processors .
furthermore the preprocessing time has almost no impacton the speedups of fs gia.
discussion.
the preprocessing time of fs gia reflects the overhead of dividing a given moco problem into sub problems.
as explained in section .
the preprocessing time of fs gia increases logarithmically with the number ofgeneratedsubproblems i.e.
equalto pwherepisapower of two .
our empirical results show that we can effectivelycontrol the dividing overhead of fs gia.
as listed in ta ble the preprocessing of fs gia takes .
.
and .8seconds to generate equal sized subproblems for sas web portal a n d e shop.
processors solutions with preprocessing without preprocessing figure number of pareto optimal solutions found by fs gia with and without preprocessing usingup to processors in six days for e shop for small moco problems such as sas e a c hd i v i d e d subproblem is small enough that it can be solved in sec onds.
in this case the impact of the preprocessing time onthe speedup of fs gia is non negligible.
as listed in table fs gia takes .
seconds to divide the moco problem of sasinto subproblems whereas all subproblems can be solved simultaneously in .
seconds.
thus .
ofthe execution time of fs gia is spent on the preprocessingstage.
however for large moco problems such as web portal ore shop the cost of conquering each subproblem is far more than the cost of dividing these moco problemsintosubproblems.
inthiscase thepreprocessing timeoffs gia has little impact on the speedups of fs gia and thespeedup of fs gia scales well with the increasing numberof processors.
.
threats to validity to increase internal validity we use either standard or straightforward techniquestoimplementouralgorithms.
wechose an exact general purpose algorithm gia as a base line to solve moco problems sequentially.
we rely on thestandard solver sharpsat to choose appropriate features for fs gia and fs pargia generating equal sized subprob lems.
in pargia we implement the collaborative commu nication between processors using simple message passing.in os gia and os pargia we use an intuitive heuristic tochoose two objectives with the first and second largest value ranges to form a large projective plane for cone separation.
however we cannot guarantee that the pareto optimal so lutions are distributed evenly among the cones and thustheparallelism in os gia and os pargia suffers from unbal anced workloads among the processors.
to avoid the misleading effects caused by random fluctuation in measurements we executed each algorithm timeson each case study for each hardware configuration from 2to processors and we used the means of the measuredexecution times in our analyse s of speedups and scalability.
to help ensure external validity we evaluated our proposed algorithms on three relatively large case studies from the literature.
the three case studies cover a reasonable 417table impact of the preprocessing time of fs gia tfs giaprep on the entire execution time of fs gia tfs gia and speedups p the number of processors s seconds m minutes p sas web portal e shop tfs gia s tfs giaprep s tfs giaprep tfs giaspeedups tfs gia m tfs giaprep m tfs giaprep tfs giaspeedups tfs giaprep s .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
spectrum of moco problems with different characteristics such as different sizes of problems different types of qualityattributes and different optimization directions.
one system has been used in a real world scenario and the other two systems are from a popular repository used by manyresearchers.
however we are aware that the results of ourexperiments may not transfer to other systems.
we expectthat especially large systems such as linux kernel with thousands of features would benefit from a combina tion of our parallel algorithms and other methods such asapproximation moco approaches.
.
related work in the field of search based software engineering sbse multi objective optimization has been identified as a ma jor challenge for many software engineering problems .metaheuristics such as multi objective evolutionary algo rithms have been used to provide approximate solutions for moco problems .
however it is non trivial for these metaheuristics to guarantee the accuracy of approximate so lutions.
on the one hand metaheuristics depend mainly ona number of heuristically chosen metrics such as hypervol ume indicator and maximum spread to evaluatethe accuracy of approximate solutions.
however every metric provides some specific but incomplete quantifications of accuracy and can only be used effectively under certainconditions .
on the other hand metaheuristics usuallysuffer from parameter sensitivity .
they often demand aconsiderable time to tune parameters for finding reasonablyapproximate solutions .
these problems motivated us to explore the feasibility of exact moco methods and improve their performance as far as possible.
as the size and complexity of a software system increases not only exact methods but also metaheuristics became tootime consuming to address large moco problems .
many parallel models for metaheuristics havebeen proposed to solve moco problems efficiently and they have beenevaluatedonawiderangeofacademicandreal worldmocoproblems in different domains .
furthermore paral lelization of exact combinatorial optimization methods suchas branch and bound and dynamic programming has been studied and implemented in multi core environments however it has been rarely addressed in the con text of multi objective optimization .
the only work weare aware is from dhaenens et al.
who parallelized theexact solving of moco problems by geometrically splittingthe search space into cubes and evaluated their algorithm on one case study but their parallelization is not able toscale well up to only processors.
thus we propose a different geometric decomposition to partition the searchspace into cones.
moreover we further scale the exact solving of moco problems by collaborative communication and equal sized partition using sat.
our parallel algorithmstarget all moco problems as long as each solution can beabstracted as a combination of features.
.
conclusion moco has been used to solve many problems in software engineering e.g.
architecture design test data genera tion and project planning and other domains e.g.
hybrid vehicle powertrain design electric vehicle battery design and civil infrastructure repair planning .
we proposed five novel parallel algorithms forexact and efficient solving of moco problems.
our algo rithms searchfor pareto optimal solutions usingoff the shelfsolvers and parallelize the search via collaborative communication and divide and conquer.
we conducted a series of experiments on three case studies of software system design coveringareasonable spectrumofmoco problemswith dif ferent characteristics.
ourempirical results demonstratethefeasibility and performance of our parallel algorithms.
the key findingfrom our experiments is that fs giaoutperforms all other proposed algorithms.
fs gia partitionsa given moco problem into subproblems of relatively equalsize which effectively balances the workload among the par allel processes.
the result is that fs giacan achievesuper linear speedups.
moreover the speedup of fs gia scaleswell up to processors and possibly beyond.
our work opens a new direction in scaling exact moco methods.
wehopethatourwork encouragesotherresearchersto reconsider the feasibility of exact moco methods andto try different ways to scale them.
appropriate paralleliza tion especially given the increasing availability of multi coresystems is definitely a promising approach.
in future we plan to further improve our parallel algorithms and evaluate them on larger case studies.
we expectthat a combination of our exact parallel algorithms withexisting approximate approaches would further improve theaccuracy and performance of solving moco problems.
.