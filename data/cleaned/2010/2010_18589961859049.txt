timesheet assistant mining and reporting developer effort renuka sindhgatta nanjangud c. narendra bikram sengupta karthik visweswariah ibm research india bangalore india renuka.sr narendra bsengupt vkarthik in.ibm.comarthur g. ryman ibm rational toronto canada ryman ca.ibm.com abstract timesheets are an important instrument used to track time spent by team members in a software project on the tasks as signed to them.
in a typical project developers fill timesheets manually on a periodic basis.
this is often tedious time consuming and error prone.
over or under reporting of time spent on tasks causes errors in billing developmentcosts to customers and wrong estimation baselines for future work which can have serious business consequences.
in order to assist developers in filling their timesheets ac curately we present a tool called timesheet assistant ta that non intrusively mines developer activities and uses statistical analysis on historical data to estimate the actualeffort the developer may have spent on individual assigned tasks.
ta further helps the developer or project manager by presenting the details of the activities along with effort data so that the effort may be seen in the context of the actual work performed.
we report on an empirical study of ta in a software maintenance project at ibm that providespreliminary validation of its feasibility and usefulness.
someof the limitations of the ta approach and possible ways to address those are also discussed.
categories and subject descriptors d. design methodologies d e sign tools and techniques computer aided software engineering general terms design experimentation human factors verification keywords timesheet development activity mining estimation thanks to saurabh sinha and subhajit datta for their feedback.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies arenot made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.ase september antwerp belgium copyright acm ... .
.
.
introduction one of the key features of a software development or maintenance contract is that usually customers provide payment based on the time spent by software developers in executing that contract.
this necessitates the use of timesheets which are records of effort expended.
these timesheets are manually filled in by developers and then verified by the softwarevendor and the customer.
appropriate payment is made to the software vendor after the verification.
in addition timesheets are also used within the software organization totrack effort and developer productivity.
while several products that facilitate manual timesheet reporting exist how to accurately estimate theactual effort expended on software development tasks continues to remain a thorny issue.
manually recording time in timesheets is a tedious and error prone process.
a number of reasons may lead to incorrect timesheet data beingrecorded for example developers often work on multiple development tasksin parallel or they interleave these tasks with other activities e.g.
code reviews learning activities etc .
later it becomes difficult to separate out time thatwas spent on these tasks and activities individually.
the developer may not have expended the full effortfor which he she is expected to be billed and to coverthat up the developer may over report the effort spent on certain tasks.
if the developer truthfully reports an effort that is significantly different from the one originally estimated often by the team lead then he she may have to explain why a certain task took more less time.
it hasbeen our experience that this is a situation many de velopers tend to avoid.
also it is difficult for project managers to thoroughlyanalyze and validate timesheet data given that a man ager would usually not know the full development context and manually retrieving and checking the necessary information from the project repositories for eachtimesheet entry is not practical.
errors in timesheets that arise due to such challenges can be damaging in many ways.
under reporting of effort results in lower revenue for the software vendor and also unrealistic expectations from the customer for future work which couldseriously impact the quality of software delivery.
on the other hand over reporting of effort if detected by the customer results in damaging the vendor s reputation leadingto potential loss of business.
timesheet data is also used to refine estimation baselines in organizations following a process improvement model such as capability maturity model integration cmmi .
a wrong baseline resulting from poor quality timesheet data can lead to severe estimation problems and delivery issues for the software organization in the long run.
to address these challenges we present timesheet assistant ta a tool that mines developer activities to i estimate the time i.e.
effort a developer may have actually spent on assigned tasks and ii generate an activity report that reviewers may drill down to different levels of detail to understand the characteristics of work carried out.unlike some existing approaches for timesheet automation e.g.
that involve intrusive and therefore unpopular methods such as recording keyboard mouse clicks and monitoring the applications that the developer has accessed ta is non intrusive and does not require any change to the environment a developer is working in.
one of the key requirements of ta is the availability of information on the files modified for a given task .
in most modern development environments such as ibm r circlecopyrtrational team concerttm rtc or visual studio team system vsts this information is available as a part of tasks or work items assigned to the developer.
in section we explain the role of such environments in our overall solution approach.
ta analyzes information in the following manner.
first it extracts all the tasks a developer had worked on in the given period of time.
second for each task ta mines the files that were changed and computes a set of metrics that help explainthe overall development effort and context for example the size of the change the expertise of the developer on the files that had to be changed etc.
third it uses some ofthe computed metrics and statistical techniques based on historical data in this paper we demonstrate usage of linear regression to estimate the time that may have been taken to complete the task.
finally it reports all the relevant information along with the tasks in the timesheet to help reviewers drill down and validate the data and periodicallyre calibrates the estimation model based on new effort valuessubmitted.
section presents the overall architecture of ta and explains the functioning of its different components.
our evaluation of ta in a maintenance project at ibm has provided preliminary validation of its usefulness.
for exam ple we have found that it is possible to develop reasonably accurate estimation models at the individual work item leveland use that to suggest timesheet entries based on analysis of development work undertaken.
we call this estimation in the small as opposed to the traditional project level es timation in the large exemplified by estimation methods such as cocomo or slim .
also user feedback from the case study suggests that summarizing and linking devel opment activities with timesheet reports significantly easethe task of reviewing the reports or justifying the entries to project managers or customers.
these results are encouraging and have helped spark a discussion with one ofour business partners on possible productization of ta con1 even as we carry out larger scale validation.
at the same time we have also identified some of the threats tothe validity of our current approach and possible ways toaddress those and realized that there are certain effort determinants that are inherently difficult to capture fully particularly using a non intrusive approach such as ours.
theresults and lessons learnt from our empirical study of ta are detailed in sections and while the rest of the paper discusses related work and presents our conclusions anddirections for future work.
we will conclude this section with a summary of the main contributions of our work .
we consider the challenges associated with timesheet reporting and validation that are of great practicalsignificance to the application services industry butwhich have been little studied by the software engineering research community.
.
we present a novel framework called timesheet assistant ta that provides automated and non intrusivesupport for addressing these challenges through a combination of software repository mining statistical anal ysis and in context reporting of development activities in timesheet reports.
.
our solution approach departs in two significant ways from traditional estimation literature we perform es timation in the small at the level of individual tasks and we estimate post facto based on actual develop ment work carried out.
.
we demonstrate the feasibility and usefulness of ta on a real life case study from a maintenance project atibm and discuss the lessons learnt.
.
towards timesheet automation while timesheets have been used in the software industry for decades we believe that a number of recent trends now make it possible to mine project repositories and improve and automate the way timesheets are reported.
tobegin with in the last few years integrated development environments ides have evolved from being developer focused tools providing features to compile debug and deploy software programs to collaborative environments supporting project planning work assignment discussions source code management build and test management project trackingand reporting.
in such development environments each task whether planning development testing or defect fix is modeled as a work item that is expected to deliver a de velopment plan design feature enhancement or a code fix as the case may be.
a work item carries a set of basic attributes that are useful for tracking it e.g.
name unique identifier description creator owner creation date closuredate estimated effort effort spent and so on.
custom attributes can also be defined as needed.
moreover links may be established between a work item and associated softwaredevelopment artifacts code test cases designs plans etc.
stored in a configuration management system through the definition of one or more change sets .
a change set is a collection of files grouped together by the developer in a manner that is meaningful for the project.
for example all gui related file changes can be grouped together into asingle change set and checked in against one or more work 266items.
this rich contextual information linked to work items makes it possible to track development activities undertakento implement the items and is core to the timesheet assistant approach.
with ides becoming increasingly popular and data rich and tool vendors focusing on end to end integration across the software development lifecycle sdlc tool stack we are also seeing the emergence of data warehouses to help archive large volumes of sdlc data efficiently and supportfast querying and retrieval of information.
data from sdlc tools can be extracted transformed and loaded into these warehouses and then business intelligence techniques canbe applied on the data to get deeper insight into the health of the project and obtain various kinds of reports for informed decision making.
along similar lines in the case of timesheet assistant our goal is to run statistical analysistechniques on data and metrics extracted from the development environment to estimate the actual effort that may have been spent on assigned tasks and then generate in sightful reports on the development activities undertaken to help reviewers validate timesheet entries .
the increasing adoption of data warehouses and bi techniques in mainstream software development along with modern ides described above thus provide the necessary ingredients to develop an automated solution for timesheets.
specifically timesheet assistant ta assumes rational team concert as the development environment rational insight 5as the data warehouse and reporting engine and gnu octave for statistical analysis and predictive modeling of effort.
.
ta timesheet assistant we begin this section by describing our approach towards estimation in the small i.e.
how we model factors that may determine or help explain effort spent on individual development tasks in timesheets.
following this we will present an overview of the timesheet assistant architecture and ex plain the functioning of its different components.
.
estimation in the small a number of parametric software estimation models have evolved over the last several decades to accurately predict the overall cost schedule and quality of a software product to be developed.
these models typically embody estimation in the large they apply across the software development lifecycle are governed by a set of gross effort indicators whose values themselves need to be estimated often subjectively at the start of a project and are then used for budgeting project planning and control tradeoff and riskanalyses etc.
given that these models have been built refined and calibrated through a large number of completed software projects over several years they provide an excel lent starting point for us to study well accepted factors that impact effort in the large and then consider which of those factors may also be relevant for estimation in the small and how they may need to be re interpreted.
for our study we used the well known cocomo ii post architecture model that estimates effort as a functionof the size of the software project in terms of thousands ofsource lines of code or function points adjusted by variables scale factors or effort multipliers values for which are by selecting a rating from a well defined scale ranging from very low to extra high .
the choice of cocomo iiwas motivated by its widespread acceptance in the softwarecommunity.
while size of source code produced or modified will naturally influence the effort needed even for individual development tasks tracked through timesheets weperformed an analysis of the identified variables to determine their applicability for task level estimation.
we kept two things in mind while this.
first we wanted to fixthe initial context of ta at the level of individual projects in other words we were interested in factors that can cause effort variations across tasks within the same project envi ronment rather than factors that are project wide and likely to impact all tasks more or less uniformly.
second since our goal is to automate timesheet reporting and validation tothe extent possible we wished to focus on factors whosevalues can potentially be mined from the development environment with relative ease accuracy without having to depend on subjective assessment by team members who fillup timesheets or burdening them with additional reporting overhead.
with these objectives in mind we first identified the set of cocomo effort factors that would normally character ize the project as a whole and whose effect on individual development tasks we will assume to be uniform.
these in clude for example development flexibility degree of conformance to requirements and interface standards analyst capability capability of personnel working on require ments team cohesion willingness of parties to work to gether personnel continuity annual personnel turnover process maturity e.g.
cmm level use of software tools degree of sophistication in tools used for the project plat form volatility e.g.
frequency of changes in compilers assemblers etc.
one may argue of course that even some of these factors may affect one work item more than anotherwithin a project for example willingness to work together may vary from one group of team members to another or some requirements analysts may be much more capable thanothers in the project.
but we have decided to treat these factors as having uniform influence for now given that it would anyway be difficult to objectively measure their influence onindividual development tasks carried out by team members.next we studied cocomo factors that may be relevant for individual work items once they have been re interpreted for estimation in the small.
we discuss these below.
.
.
reliability the cocomo factor required software reliability more effort would need to be expended to avoid software failures that carry higher risks may vary from one component and its work items to another within a project.
another factor data base size effort required to generate test data is considered as a measure of the testing effort required and is also related to reliability.
in our estimation model for timesheet tasks we have included a factor called required reliabilitythat will be measured in terms of the number of test cases that will have to be written for new features or executed for defect fixes before closing the work item.
we have as sumed that components that are deemed risky will be tested more comprehensively hence we have used number of test cases as an indicator of a component s required reliability.
.
.
complexity the product complexity factor in cocomo is factored into our model as task complexity in multiple ways.
first we recommend building separate models for different types of work items such as new feature implementations enhancements or defects since the dimensions of complexity varyacross these types.
for example for the same change interms of number of lines of code a defect could take longer time than an original feature development .
on the other hand the effort spent on developing a new feature may beinfluenced by the number of libraries that are being used as the functionality of each of them needs to be understood while in a defect fixing activity it may not be as impor tant since the developer may already be aware of the library functionalities.
this approach may be further extended to separate out server side work from client side work if needed since the former would typically involve more complex logic.
second for any work item the complexity of its associated development activity is measured in terms of the numberof distinct files changed for each such file the number ofmethods changed and for each such method i the wellknown cyclomatic complexity that measures the number of independent paths through a program unit and ii thefan out i.e.
number of external methods functions being called.
the number of files methods changed helps us track whether the change was localized or relatively distributed inthe latter case it may be considered more complex .
note that not all of these measures will necessarily impact the effort spent on each work item in a statistically significant way however we still track these since they also help in summarizing the overall development context to timesheet reviewers.
third a work item may manipulate files of different types.
the core functionality of the system may be implemented in some major programming language but there will also be accompanying miscellaneous minor files such as configu ration and build scripts properties files xml html files etc.
making a change to a minor file such as properties file will be in general far less complex than an equal sized change ina java file.
classifying changes by identifying the types of files changed is therefore an important factor in sizing work.
for example we classify files as major and minor for keydevelopment files and miscellaneous files respectively.
however for minor files due to their simpler structure we currently do not calculate any complexity measure.
.
.
expertise team capability related factors in cocomo such as programmer capability capability of programmers as a team applications platform experience level of applications or platform experience of the project team language and tool experience of team etc.
are re considered within our timesheet model in terms of the expertise of the devel oper performing the task.
this expertise has two dimensions.
the first one which corresponds roughly to experience may be used to differentiate how two developers withdifferent levels of application language platform familiarity may perform the same activity.
to handle this we can cluster developers by experience level which may be determinedby indicators such as the time spent on the project the overall development experience in years etc.
the intuition being that higher the experience less will be effort required for a given work item.
the second dimension of expertise whichdeals with task level familiarity may be mined from historical development data and will be used to adjust the effort adeveloper spends on a work item based on his her familiaritywith the files that need to be updated.
this low level notion of expertise is particularly important for estimation in the small since familiarity with files can significantly bringdown the effort that would be needed to update them to incorporate a new feature fix a defect.
we measure the work item level familiarity of a developer for a work item as the weighted average of the familiarityon each file changed by the developer where the weight assigned to a file is the proportion of the total developmentsize for the work item that corresponds to the file.
for example if a developer d has changed files f1 and f2 for a work item w with lines changed in f1 and lineschanged in f2 then d s familiarity index for w would bea weighted average of her familiarity on each file ff i.e.
.
ff for f .
ff for f .
developer expertise computed in this way lies between and with a highernumber indicating higher expertise.
the file familiarity ffof developer d with a file f is computed by first considering the ratio of the number of times f has been updated by developer d compared to the maximum number of times it has been updated by any developer throughout the evolution history of the file.
this providesa relative measure of expertise compared to an expert on that file.
the ratio is used to further classify developer file familiarity into buckets high medium low based onthreshold values that may be set by the user.
finally eachbucket is given normalized scores we used .
.
and .
for high medium and low familiarity respectively.
for a user updating a file for the first time familiarity is set closeto .
we realize that modeling expertise is in itself an interesting topic of research and hence we also experimentedwith a slightly different measure of file familiarity in terms of the proportion of total code in the file that has been updated by the developer compared to the updates made tothe file by other developers throughout the evolution history of the file.
however when we computed both the measures and cross checked with a team of developers who were part of the ta case study we realized that equating file exper tise with proportional code updated may result in grossly inaccurate familiarity levels since it ignores familiarity that a developer gains of code submitted by others in course ofmaking his her own updates.
on the other hand according to the first measure familiarity increases each time a developer updates a file since he she gets an opportunity toreview its source code and this model resulted in familiarity values that to a large extent mirrored the level of familiarity the developers themselves reported.
hence we have usedthe first approach in our model.
details of our interviews with developers on the topic of file familiarity could not be included here due to lack of space.
table summarizes the effort factors and associated work item attributes and metrics that we are tracking in our task level model for timesheets.
this information will be presented to reviewers of timesheets to help them quickly get anunderstanding of the development work carried out.
they will also be used in designing statistical effort models for development activities.
268figure timesheet assistant architecture .
timesheet assistant architecture we will now outline the architecture of ta comprising four main components as shown in figure activity tracker extracts all attributes and metrics of work items that provide context of the developmentactivity and are indicative of the effort spent on it.
steps and in figure indicate the extraction and storage of this data.
effort calculator uses statistical analysis techniques to build an effort model based on effort predictors ex tracted by activity tracker.
subsequently it uses theeffort model to compute the effort possibly expended by the developer on a work item steps and of figure .
re calibrator as more work item data is captured the regression model may need to be refined.
re calibrator re computes the regression coefficients whichare further used by effort calculator.
timesheet visualizer the data mined by the activitytracker and effort computed by effort calculator can be viewed using timesheet visualizer represented by step in figure .
the following sections describe the components in detail.
.
.
activity tracker the key components of activity tracker are work item data extractor code parser and metrics analyzer.
work item data extractor uses the rational team concert client api6to extract work item attributes such as type of work item creator of the work item owner status estimated effort and change sets associated with the work item.
for each file in the change set before and after versions of thefile are extracted the changes made are identified using the netbeans diff utility for windows.
code parser parses the file and changes for method declarations method invocations decision statements which are by metrics analyzer to compute the metrics listed in table .
these metrics are stored in the data store.
met rics analyzer also uses historical work item data availablein the data store to compute the expertise of the developer on changed files and further for the work item as a whole to allow projects the flexibility of defining their own metrics that can be configured and extracted by activity tracker we define an extension point for adding additional metrics by extending an abstractmetricprovider of metrics an alyzer component.
a metric can be defined for different levels of granularity work item file and changes.
activity tracker extracts metrics computed by all extension pointsand generates and stores them in the data store for each work item as name value pairs of metrics.
.
.
effort calculator the work item and metrics information mined and computed by the activity tracker from historical tasks and effort reported for these tasks are used by the effort calculator which applies statistical analysis techniques to predict effortfor subsequent tasks.
in our implementation so far we have used linear regression for effort prediction.
a user can create a model template for a project using ta.
a model template allows the user to define inputs for the linear regressionmodel such as the time period software size expertise complexity metrics etc.
to be considered and any other custom predictors defined in activity tracker.
the model templatecan be instantiated resulting in extraction of relevant predictors and using ordinary least squares ols regression to compute the model.
if the predictor distributions are not normal they are linearized by taking logarithms.
the regression coefficients and the resulting effort curve are stored in the data store.
the user can select discard or refine themodel template inputs based on a measure of goodness of fit described later in section .
once a model template is selected suitable for prediction for any new work item theeffort is computed based on the model contained in the tem plate i.e.
using its regression coefficients and predictors.
as stated earlier in this paper we have used gnu octave for linear regression modeling.
.
.
re calibrator as a project advances the influence of factors on the effort for a work item changes.
familiarity of technology stability of features through a development cycle etc.
may cause lesser effort to be expended for the same change as compared to the effort during the initial stages of the cycle.
on the other hand in long running projects code decaycan lead to increase in change effort over time .
either way it becomes necessary to adapt the model to changes in the project environment.
re calibrator essentially is thecreation of new effort calculator model templates.
hence during the project life cycle a new model template can be instantiated containing the relevant work item information metrics and predictors.
.
.
timesheet visualizer the information mined by activity tracker and effort predicted by effort calculator is used to visualize the timesheet via timesheet visualizer.
as shown in figure for a developer the work items and the original estimated effort for each work item are listed.
effort predicted by the effortcalculator is also presented.
the details view provides a 269table effort factors and related attributes metrics effort factor metric attribute description size lines of code loc number of non commented lines of code updated added changed required reliability number of test cases number of test cases that have to be written executed for a work item complexity task type type of task e.g.
feature development enhancement defect fix etc number of files methods changed number of distinct files methods that were updated file types change distribution across different file types e.g.
core logic properties xml etc cyclomatic complexity number of linearly independent paths through a program unit fan out number of other functions being called from a given program unit expertise experience level developer s overall experience e.g.
in years experience in project etc task familiarity developer s familiarity with the files modified by the task summary of the files and the changes made which can be further drilled down for details of changes made to each file.
size complexity expertise etc.
metrics are provided for each file that can help a developer or a project manageridentify why a specific development activity took more less time.
the visualizer can highlight metric values that cross a user defined threshold or range.
as timesheet visualizer runs on rational insight its reporting component can be used to create customized reports of the information extracted by activity tracker.
.
evaluating ta in this section we present the results of evaluating ta.
the case study was a software development project at ibm during its maintenance cycle hence we obtain only defect work items for our evaluation.
evaluation of ta on otherwork item types original feature development enhance ments etc.
is important and will be taken up in our next phase of experiments as when those types of work items become available to us.
nevertheless we consider this casestudy significant since maintenance activities involving defect fixes and minor enhancements constitute a significant share of the application services domain where timesheetsare routinely used.
.
goals and method the primary goals of our case study were to i determine how well the data extracted using ta could be used for estimating the actual effort at the work item level and ii obtain feedback from the developers and their project managers on our tool and approach in general and how it could be improved in the future.
the project team that we chose was responsible for maintaining a web based product and the team used rational team concert as their development environment.
each taskwas modeled as a work item estimated time was definedand the task was assigned to a team member.
the team member would work on the assigned task and check in the relevant development artifacts.
work item data collected for1.
months was used as a training set to predict effort for an additional month validation period.
we chose linear regression modeling as our statistical technique since it is one of the most widely used methods for effort estimation and is simple to use and experiment with.
for each work item we computed the metrics set shown intable with two exceptions experience level of the developer and number of test cases.
the former was not considered since all the developers who were part of themaintenance activity had been with the team for more thana year and had helped develop the product they were thenmaintaining.
there did not seem to be any significant differences between their platform application experience levels although we did not collect information on their overalldevelopment experience.
we could not collect data on the number of test cases that were executed for each defect item since the testing activity was not formally recorded againstthe defect items in the repository.
correlation among the factors was then determined using pearson coefficient and only relatively uncorrelated variables were selected asindependent variables for running the regression analysis.these variables were number of lines of code updated deltaloc average cyclomatic complexity of changed methods deltacc and exp where exp refers tothe task familiarity of the developer computed using the method outlined in section .
.
the variable exp measures the lackof expertise which should positively correlate with effort .
to compute exp values file change history was extracted for a period of months before the start of the case study.
the metrics calculated for determining the efficacy of the regression analysis are the well known r c o e ffi c i e n to f determination and magnitude of relative error mre .
r2is defined as actual effort predicted effort actual effort mean whereas mre is defined as the absolute value of predicted effort actual effort actual effort .
while the latter provides an indication of the typical fit error present in the model the former measures the goodness of fit of the regression model and may be interpreted as the proportion of responsevariation explained by the regressors in the model.
to ensure trustworthy learning data during the case study a specific process recommendation was implemented by theproject managers that ensured accurate reporting of effort data.
this was done by checking each work item in the training data set and interviewing the developer responsible for that workitem before the work item was closed in therational team concert tool.
.
estimation results using ta we were able to extract a total of work items as training data over the previously mentioned .
month period.
while fixing defects the project team primarily developed code in major programming languages such as java or javascript with insignificant number of minor files updated hence we did not consider the latter forour regression analysis.
in order to ensure accuracy of this historical data we verified the reported effort for each work item with the respective developer and his her project man ager.
270figure timesheet visualizer we ran the regression analysis on the input combinations deltaloc deltaloc exp and deltaloc deltacc exp .
table lists results of our regressionanalysis for these three combinations.
table also depicts regression analysis results with and without outliers which needs explanation.
when we analyzed the distribution of mre values we realized that a small share of work items contributed a very significant share of the estimation error.
discussions with the project team revealed that someoutliers will always exist for example a task may require a developer to learn a new framework library which takes significantly more time than the actual coding effort andthis will be included in the overall time spent.
for the input combination deltaloc exp a small subset of work items out of a total of contributed toabout of the overall fit error measured in terms ofsquared residuals where the squared residual is calculated as actual effort predicted effort .
project managers in the team told us that they tend to focus more on streamlining the development of the vast majority of work items hence we decided to rerun the regression analysis by removing such outliers as has also been recommended elsewheree.g.
after which we got much more predictable results as shown in table .
it is clear from the regression results that the size of the development effort deltaloc was a dominating variable in terms of its impact on effort with around of the effort variation attributable to it after outlier removal asshown by the r 2value.
while the mre value may appear high in terms it is mainly a result of the fact that many defect work items take only a few hours to fix.
when we included exp into the formulation the impact was smallin terms of its coefficient but significant in terms of improving the regression results.
since most of the developers were working on defect items associated with development work they had carried out earlier expertise was high in a largenumber of cases and consequently not much of a differentia tor in general.
however we discovered that for certain workitems where expertise was rated low by our model the highdevelopment effort required relative to the size of work was estimated much better leading to the improved results.
finally we included the complexity factor deltacc in our experiments.
interestingly this led to a slight deterio ration in the regression results for deltaloc exp although it was better than those obtained for only deltaloc.there could be a number of explanations for this including the possibility that the metric cyclomatic complexity of updated methods that we used is too generic and does notaccurately capture the actual dimensions of task complexity in a project.
we will experiment with other complexity metrics going forward.
however our hypothesis based oninterviews with developers on the topic of file familiarity isthat in software development code complexity and familiarity are closely related as developers become familiar with a file its perceived complexity comes down even thoughabsolute complexity measures remain the same .
when a developer is fixing defects in a file he she is familiar with its actual complexity measure is no longer an effort determinant of significance in fact including it along with expertise to estimate effort may even add noise in some data sets as seems to be the case in our experiments.
on the otherhand for new feature development that leads to creation of significant new code that a developer has no prior familiarity with metrics such as cyclomatic complexity number oflibraries used etc.
may become quite relevant and help inpost facto explanation of effort spent.
we will test this hypothesis in our next round of experiments with ta where we will have access to an ibm project where significant new features will be incorporated in a product.
to validate the model we collected data for an additional work items for the month validation period.
the average mre we obtained was with outliers.
since the 271table results of linear regression on training data effort equation with outliers without outliers effort .
deltaloc0.41r2 .
and average mre .
r2 .
and average mre .
effort .
deltaloc0.
exp .06r2 .
and average mre .
r2 .
and average mre .
effort .
deltacc0.01deltaloc0.
exp .06r2 .
and average mre .
r2 .
and average mre .
figure frequency distribution of squared residuals for training data using deltaloc exp figure deviation of actual vs. predicted effortfor validation data using deltaloc exp purpose of validation is to determine the estimation accuracy of the generated regression model we used mre values to remove the top of outliers.
after this removal the average mre value dropped to .
which is even better than average mre obtained in the training set.
the rele vant box plots of mre values with and without outliers are displayed in figure .
overall the regression analysis results are promising particularly in the absence of testing related information whichwas a clear gap since every work item went through a testing phase with some work items requiring many more test cases than others.
at the same time there are several ways in which our estimation model can be further improved some of which are discussed in section .
.
qualitative feedback we conducted semi structured interviews with the ten developers and the four project managers from the project team which participated in this case study.
each interview consisted of two parts specific objective type questions with pre specified answer categories and requests for qual itative feedback on ta.
figure summarizes the responses against pre specified answer categories details of the interviews are not presented here due to lack of space .
a few key findings can be summarized from the interviews figure developer feedback on timesheet assistant it is interesting to observe that the developers interviewed seldom update original estimated effort for tasks even though the actual effort differs frequently fig ure .
one of the managers also confirmed this and pointed out how it stands in the way of improved estimation for future tasks.
given this both developers and managers felt that bylinking development activities to timesheets ta will help justfy actual effort spent and promote greatertransparency.
managers also felt that a tool like ta can help improve their own productivity by cutting down on project tracking effort while the effort valueestimated will at least provide some benchmark againstwhich they can validate developer reported effort.
two developers who did not find ta useful said thatthey were not sure how accurate the task level effort prediction model would be in practice so whether their reporting burden would be actually reduced.
indeed there are limitations in our current approach that we discuss in section along with how some of them may be addressed.
some of the developers suggested that ta should be able to calculate pop up the estimated effort as soon as a developer closed the work item since thiswill act as a useful reference while recording effort.
currently this calculation is done offline and sent directly to the timesheet report.
interestingly while we internally used the metrics ofr 2and mre for measuring the goodness of estimation project managers suggested that we report these values through ta as they may be helpful in identifying projects that show low predictability so that appro priate process improvements can be initiated to drive 272down variation and make application services delivery more predictable.
finally it was also suggested that we integrate ta with requirements management and design tools so that the approach may be broadened to support timesheet assistance for these activities.
.
discussion in this section we discuss some of the limitations of the ta approach and possible ways to address the same.
one of the threats to the validity of our approach comes from the focus on development size lines of code as the keyeffort determinant and its possible impact on developer be havior.
expected size is central to all estimation approaches but since we are post facto analysis there is the danger that some redundant changes are deliberately made toincrease development size and claim or justify higher effort than should have been the case.
currently we do not perform any semantic analysis of code as part of our overallapproach and rely instead on process level monitoring e.g.
code reviews to discourage such practices.
since we link all changes made to the timesheet report it becomes easier for reviewers to check the changes made and detect if something is amiss.
also sometimes a trivial change made for valid reasons e.g.
renaming a widely used variable can lead to many lines of code change hence at least some lightweightsemantic analysis needs to be built into ta to ensure such changes do not lead to gross over estimates of effort.
a related issue is if such a tool will discourage developers from writing optimized code which may also take moreeffort .
in fact developers may have different coding styles with more experienced developers producing higher quality with less defects and more efficient code than their juniorcolleagues.
clustering developers by their experience levels and building models for each cluster may be one way to address this.
also cloning of code can significantly boost productivity.
this could be a significant factor for effort prediction of enhancements especially when there is user interface codeinvolved where reuse is very common.
there is a rich literature on code clone detection and we are currently reviewing how such techniques can be incorporated into ouroverall solution design.
there are at least two aspects of file familiarity which the expertise model discussed in section .
does not cover.first we need to factor in decay in file familiarity of a developer when no updates have been by him her over a sig nificant period of time this decay will be more pronouncedwhen other updates have been made in the meantime.
sec ond we only consider file familiarity obtained by making code updates.
what we found out during our case study is that developers also become familiar by simply reading codesubmitted by team members if they are pertinent to their own work or through formal code reviews.
in a nonintrusive approach like ours it is difficult to accurately track this as pect of familiarity possible solutions may include a process recommendation in ides such as rational team concert where we could associate reviewers to each work item orchange set.
this could be a factor in identifying familiarity of a developer with a given file.
apart from the actual act of writing code for an assigned task developers can spend a significant amount of time ondiscussions with team members to get a better understanding of the problem discuss solutions etc.
modern idesprovide collaborative features to hold such discussions in the context of work items so persisted discussions can provide at least some indication of whether a task required significant brainstorming or not.
however while distributedteams are often heavy users of these collaborative features collocated teams like the one in our case study often prefer face to face discussions which a tool like ta will be unableto track.
reusability is another aspect not considered within the current ta approach.
it is well understood that there will be a cost higher effort associated with writing a generic and reusable framework or library.
code complexity metrics indicating reusability such as fan in cannot be measuredwhen the code is written as these metrics evolve over a pe riod of time.
hence to identify a task that requires reusable components to be written we may need to trace back to its associated design space and analyze class or collaborationdiagrams to measure the reusability for the components and allocate additional time if needed.
.
related work time tracking solutions a number of commercial solutions e.g.
actitime baralga tasktop help office workers manually record their efforts for various activities and also link that information with other corpo rate tools such as erp crm and other project management tools.
however unlike ta none of these solutions provide a means to automatically extract the actual quantum of work performed by a software developer and estimate the effort thereof via a statistical technique.
in comparison odesk actually measures the effort taken per task by monitoringkeystrokes on a computer.
a similar solution is also pro vided by the eclipse environment which can keep track of the times when files are opened edited and closed.
however the intrusive nature of these approaches can limit their acceptance in practice.
mining change data in general our ta solution is inspired by the rapidly emerging area of development intelligence which is the application of business intelligence ideas to software development projects.
prominent among these ideas is how to effectively mine developer activitiesand change information from software repositories.
in the authors present a taxonomy of approaches for mining source code repositories for extracting developer activitiesand change data such as extracting via cvs annotations data mining heuristics and differencing.
in zimmermann describes a cvs plugin called apfel that collectsand stores fine grained changes from version archives in adatabase.
by searching for specific tokens such as method declarations variable usages exceptions method calls and throw catch statements apfel determines changes to filesin terms of these tokens.
on similar lines in the authors present a technique called annotation graph that identifies line changes in a file across several versions.
the ci tation presents an algorithm that compares the sets of lines added and deleted in a change set using a combination of information retrieval techniques.
we will investigate suchapproaches for code differencing in ta.
software effort estimation several parametric software estimation models have been proposed over the years e.g.
probe slim cocomo and empirically 273calibrated using actual project data with multiple regression approach being one of the commonly used techniques.these models are useful for estimation in the large while for estimating effort for timesheet tasks we had to design models at a much lower level granularity.
in addition our approach has the benefit of analyzing actual developmentwork to estimate factors such as complexity and developer expertise.
along similar lines presents a multivariate regression model for estimating effort for a modification re quest based on the following factors nature of work e.g.
defect or new feature development size of change the developer making the change and the date the change wasinitiated to account for a code decay factor .
while different developers were found to expend varying levels of effort for comparable work the reasons behind this were not investigated and in particular the impact of file familiarity infixing defects has not been considered.
moreover our focus is on providing an end to end automation framework that applies repository mining statistical analysis and data sum marization techniques to address some practical challenges in timesheet reporting and validation this also differentiates it from other research efforts where the primary motivationhas been offline analysis of historical data to build explanatory models.
on a different note presents an approach for calculating defect fixing effort by extracting the effortfor the nearest neighbors based on a similarity threshold using a text similarity engine of the defect in question.
it would be interesting to apply text similarity e.g.
defect work item descriptions to see if our ta estimation modelmay be improved.
.
conclusions and future work when we set out to develop timesheet assistant our aim was to determine the usefulness of our proposed estimation in the small approach in support of the filling and reviewing of timesheets.
we feel that the case study reported in our paper does point towards the general feasibility of thisapproach especially for maintenance projects.
the effort fit of the training set as well as the validation results are definitely encouraging and demonstrate the usefulness of ex pertise along with development size as key effort indicators.
the qualitative feedback received from our semi structured interviews is also quite positive with the idea of ta beingwelcomed not only by project managers but also by developers.
we believe that this is due to the non intrusive and overall helpful nature of ta.
at the same time we discussed some limitations of our current approach and ways in which the effort model may be enriched with additional parameters from the development environment.
our future work will be along the following directions.
first we are initiating a case study of ta in a project that involves significant development of new features it would be interesting to find out to what extent our current modelwould need to be tailored to account for effort in new development tasks.
second continuous re calibration of our prediction model has not yet been tested for which we willbe investigating techniques such as those reported in .
while outliers are currently eliminated in our model using them to create and maintain separate regression models foroutlier work items cf.
could also be useful.
finally so far we have used linear regression for effort prediction since we found that the simplest to use.
perhaps other techniquessuch as bayesian analysis or decision tree analysis would provide better results and we would like to try themout.
.