see discussions st ats and author pr ofiles f or this public ation at .researchgate.ne t public ation billions and billions of constraints whitebox fuzz testing in production article in proceedings int ernational conf erence on softw are engineering januar y .
ic se.
.
citations 131reads author s including ella bounimo va micr osoft publica tions citations see profile all c ontent f ollo wing this p age was uplo aded b y ella bounimo va on june .
the user has r equest ed enhanc ement of the do wnlo aded file.billions and billions of constraints whitebox fuzz testing in production ella bounimova patrice godefroid david molnar abstract we report experiences with constraint based whitebox fuzz testing in production across hundreds of large windows applications multiple microsoft product releases and over machine years of computations from to .
whitebox fuzzing leverages symbolic execution on binary traces and constraint solving to construct new inputs to a program.
these inputs execute previously uncovered paths or trigger security vulnerabilities.
whitebox fuzzing has found one third of all file fuzzing bugs during the development of windows saving millions of dollars in potential security vulnerabilities.
we present two new systems developed to manage our deployments sagan which collects data from every single fuzzing run for further analysis and jobcenter which controls deployment of our whitebox fuzzing infrastructure across commodity virtual machines.
since june sagan has recorded over .
billion constraints solved millions of symbolic executions and tens of millions of test cases generated.
our work represents the largest scale deployment of whitebox fuzzing including the largest computational usage ever for a satisfiability modulo theories smt solver to date.
we highlight specific improvements to whitebox fuzzing driven by our data collection and open problems that remain.
introduction fuzz testing is the process of repeatedly feeding modified inputs to a program in order to uncover security bugs such as buffer overflows.
first introduced by miller et al.
traditional blackbox fuzzing is a form of random testing where an initial seed input is randomly mutated to generate new fuzzed inputs.
while simple the technique has been shown to be startingly effective as thousands of security vulnerabilities have been found this way.
at microsoft fuzz testing is now required by the security development lifecycle for code that may handle untrusted input.
unfortunately random testing often misses bugs that depend on specific trigger values or special structure toan input.
more recently advances in symbolic execution constraint generation and solving have enabled whitebox fuzzing which builds upon recent advances in systematic dynamic test generation and extends its scope from unit testing to whole program security testing.
starting with a well formed input whitebox fuzzing consists of symbolically executing the program under test dynamically gathering constraints on inputs from conditional branches encountered along the execution.
each of those constraints are then negated and solved with a constraint solver whose solutions are mapped to new inputs that exercise different program execution paths.
this process is repeated using systematic state space search techniques inspired by model checking that attempt to sweep through as many as possible feasible execution paths of the program while checking simultaneously many properties using a runtime checker such as purify valgrind or appverifier .
in this paper we report on the usage of whitebox fuzzing on a very large scale for the first time.
earlier applications of dynamic test generation focused on unit testing of small programs typically consisting of a few thousand lines of code for which these techniques were able to achieve high code coverage and find new bugs for instance in unix utility programs or device drivers .
while promising this prior work did not report of any daily use of these techniques and tools.
in contrast we present here our experience running whitebox fuzzing on a much larger scale and in productionmode.
our work follows from one simple but key observation the killer app for dynamic test generation iswhitebox fuzzing of file parsers .
many security vulnerabilities are due to programming errors in code for parsing files and packets that are transmitted over the internet.
for instance the microsoft windows operating system includes parsers for hundreds of file formats.
any security vulnerability in any of those parsers may require the deployment of a costly visible security patch to more than a billion pcs worldwide i.e.
millions of dollars.
today our whitebox fuzzer sage is now running every day on an average of 200machines and has been 1running for over 400machine years since .
in the process it has found many previously unknown security vulnerabilities in hundreds of microsoft applications including image processors media players file decoders and document parsers.
notably sage found roughly one third of all the bugs discovered by file fuzzing during the development of microsoft s windows .
because sage is typically run last those bugs were missed by everything else including static program analysis and blackbox fuzzing.
achieving this milestone required facing key challenges in scalability across several dimensions symbolic execution how to efficiently perform symbolic execution on x86 execution traces with hundreds of millions of instructions and tens of thousands of symbolic variables for applications with millions of lines of code like microsoft excel .
constraint generation and solving how to generate solve and manage billions of constraints.
long running state space searches how to perform systematic state space explorations akin model checking effectively for weeks or months at a time.
diversity how to easily configure check and monitor whitebox fuzzing so that it is applicable to hundreds of diverse applications.
fault tolerance and always on usage how to manage hundreds of machines running whitebox fuzzing with as little down time as possible.
while the first challenge was previously discussed in we describe for the first time how to address the other four challenges.
in addition we faced challenges from running in production instead of in a research environment.
specifically large product groups at microsoft such as the windows and office divisions have large fuzzing labs of machines allocated to fuzz testing.
over the last few years our whitebox fuzzer was gradually deployed on a larger and larger scale in those labs which were not under our control.
indeed our internal customers were not mandated to use this new technology.
instead we had to progressively gain their trust and business by consistently finding new security critical bugs in many applications and missed by other competing fuzzers while growing our initial research prototype to a full fledged large scale operation over several years.
for this we had to gain a better understanding of how our whitebox fuzzer is being used and configured as well as monitor its progress while addressing the scalability challenges listed above and continually improving our tool and infrastructure.we have developed and deployed two new systems to help meet these challenges sagan and jobcenter .
sagan is a monitoring system which records information from every sage run and displays it via a web site making it easy to drill down into the progress of a run.
since june sagan has recorded over .4billion constraints solved millions of symbolic executions and tens of millions of test cases generated for hundreds of applications.
jobcenter is a control system which can auto assign sage jobs to machines as they become available and monitor progress of each run.
jobcenter also helps us manage the complexity of different configurations required for different programs under test.
our infrastructure enables data driven improvement where feedback from previous runs helps us focus limited resources on further research and improve future runs.
in section we demonstrate this with analyses enabled by sagan data that have led directly to changes in our whitebox fuzzing practice.
this paper is organized as follows.
in section we review whitebox fuzzing and sage .
in section we present our two new systems sagan and jobcenter and their main features.
then we present in section several original analyses on the performance of whitebox fuzz testing for runs of multiple weeks on many different programs.
each of those analyses were designed to lead to concrete actionable items that led to further improvements in our tool and infrastructure.
we discuss other related work in section before concluding in section .
background whitebox fuzzing .
blackbox fuzzing blackbox fuzzing is a form of blackbox random testing which randomly mutates well formed program inputs and then tests the program with those modified inputs with the hope of triggering a bug like a buffer overflow.
in some cases grammars are used to generate the wellformed inputs which also allows encoding applicationspecific knowledge and test generation heuristics.
blackbox fuzzing is a simple yet effective technique for finding security vulnerabilities in software.
thousands of security bugs have been found this way.
at microsoft fuzzing is mandatory for every untrusted interface of every product as prescribed in the security development lifecycle which documents recommendations on how to develop secure software.
although blackbox fuzzing can be remarkably effective its limitations are well known.
for instance the then branch of the conditional statement in 2int foo int x x is an input int y x if y abort error return has only in 232chances of being exercised if the input variable xhas a randomly chosen bit value.
this intuitively explains why blackbox fuzzing usually provides low code coverage and can miss security bugs.
.
whitebox fuzzing whitebox fuzzing is an alternative approach which builds upon recent advances in systematic dynamic test generation and extends its scope from unit testing to whole program security testing.
starting with a wellformed input whitebox fuzzing consists of symbolically executing the program under test dynamically gathering constraints on inputs from conditional branches encountered along the execution.
the collected constraints are then systematically negated and solved with a constraint solver whose solutions are mapped to new inputs that exercise different program execution paths.
this process is repeated using search techniques that attempt to sweep through all in practice many feasible execution paths of the program while checking simultaneously many properties using a runtime checker.
for example symbolic execution of the above program fragment with an initial value 0for the input variable x takes the else branch of the conditional statement and generates the path constraint x negationslash .
once this constraint is negated and solved it yields x which gives us a new input that causes the program to follow the then branch of the conditional statement.
this allows us to exercise and test additional code for security bugs even without specific knowledge of the input format.
furthermore this approach automatically discovers and tests corner cases where programmers may fail to properly allocate memory or manipulate buffers leading to security vulnerabilities.
in theory systematic dynamic test generation can lead to full program path coverage i.e.
program verification .
in practice however the search is typically incomplete both because the number of execution paths in the program under test is huge and because symbolic execution constraint generation and constraint solving can be imprecise due to complex program statements pointer manipulations floating point operations etc.
calls to external operating system and library functions and large numbers of constraints which cannot all be solved perfectly instructions executed instr.
executed after 1st read from file constraints generated full path constraint constraints dropped due to cache hits constraints dropped due to limit exceeded constraints satisfiable new tests constraints unsatisfiable constraint solver timeouts secs symbolic execution time secs constraint solving time secs figure statistics for a single symbolic execution of a large office application with a kilobyte input file.
in a reasonable amount of time.
therefore we are forced to explore practical tradeoffs .
.
sage the basis of our work is the whitebox fuzzer sage .
because we target large applications where a single execution may contain hundreds of millions of instructions symbolic execution is the slowest component.
therefore we use a generational search strategy to maximize the number of new input tests generated from each symbolic execution given a path constraint allthe constraints in that path are systematically negated one by one placed in a conjunction with the prefix of the path constraint leading to it and attempted to be solved by a constraint solver.
this way a single symbolic execution can generate thousands of new tests.
in contrast a standard depth first or breadth first search would negate only the last or first constraint in each path constraint and generate at most one new test per symbolic execution.
to give the reader an idea of the sizes of path constraints for large applications figure shows some statistics about a single sample symbolic execution of a large office application while parsing an input file of about kilobytes.
for file parser fuzzing each byte read off the untrusted input file corresponds to a symbolic variable.
our whitebox fuzzer uses several optimizations that are crucial for dealing with such huge execution traces.
these optimizations are discussed later in section .
our whitebox fuzzer performs dynamic symbolic execution at the x86 binary level.
it is implemented on top of the trace replay infrastructure truscan which consumes trace files generated by the idna framework and virtually re executes the recorded runs.
truscan offers several features that substantially simplify symbolic execution including instruction decoding providing an interface to program symbol information monitoring various input output system calls keeping track of heap and 3check for crashes appverifier code coverage nirvana generate constraints truscan solve constraints z3 input0 coverage data constraints input1 input2 ... inputn figure architecture of sage .
stack frame allocations and tracking the flow of data through the program structures.
thanks to off line tracing constraint generation in sage is completely deterministic because it works with an execution trace that captures the outcome of all nondeterministic events encountered during the recorded run.
working at the x86 binary level allows our tool to be used on any program regardless of its source language or build process.
it also ensures that what you fuzz is what you ship as compilers can perform source code changes which may impact security.
.
sage architecture the high level architecture of sage is depicted in figure .
given an initial input input0 sage starts by running the program under test with appverifier to see if this initial input triggers a bug.
if not sage then collects the list of unique program instructions executed during this run.
next sage symbolically executes the program with that input and generates a path constraint characterizing the current program execution with a conjunction of input constraints.
then implementing a generational search all the constraints in that path constraint are negated one by one placed in a conjunction with the prefix of the path constraint leading to it and attempted to be solved by a constraint solver we currently use the z3 smt solver .
all satisfiable constraints are then mapped to nnew inputs.
these nnew inputs are then tested and ranked according to incremental instruction coverage.
for instance if executing the program with new input1 discovers new instructions input1 gets a score of and so on.
next the new input with the highest score is selected to go through the expensive symbolic execution task and the cycle is repeated possibly forever.
note that all the sage tasks can be executed in parallelon a multi core machine or even on a set of machines we discuss this in the next section.
infrastructure .
sagan on a single machine a multi week whitebox fuzz testing run can consume hundreds of gigabytes of disk perform thousands of symbolic executions and create many test cases.
each task in the sage pipeline offers opportunities for something to go wrong potentially causing the fuzz run to stop in its tracks.
moreover each task also offers room for improvements by observing statistics starting with the basic time taken in each task.
when we first started running whitebox fuzzing at scale we had no way to capture this information short of logging in remotely to computers that had participated in fuzz runs.
even then to avoid running out of disk space we typically configured sage to delete intermediate results.
as a result we had limited ability to detect failures and learn from our test runs.
.
.
principles we designed a logging service called sagan to address those issues.
we developed the following key principles when designing our logging.
first every run of sage creates a unique log.
even if the run fails to start properly we assign a globally unique identifier to the run.
this allows us to unambiguously identify specific sage runs aiding our debugging and statistics gathering.
second every log contains enough information to reproduce the run including all configuration files and command line options.
this principle allows us to quickly reproduce failing runs and search for the cause of a problem.
in the case of failed tasks we also send back stdout and stderr files which helps us diagnose previously unseen errors.
third every log has a unique url that exposes the log information in a web browser.
we have found this simplifies collaborative troubleshooting and brainstorming over our next directions because we can simply e mail relevant links back and forth.
we have also created a summary web front end that shows recent runs and summary statistics such as number of crashing test cases found or number of runs that have a certain percentage of tasks failing.
using this front end we can quickly check ongoing lab runs and highlight those with failing tasks or those which are finding many crashing test cases.
in cases where users of sage need to change their 4configuration we can then send them the url for the log and highlight the problem.
fourth the logging infrastructure should be low impact for the client.
this starts with an infrastructure that takes a small amount of overhead for each client.
beyond this it means that even if the logging server fails we ensure that the sage run can stll continue.
finally the central logs contain enough information for all analyses.
prior to sagan we would perform analyses in support of our research by running sage on a target of interest then investigating files on disk to gather statistics and test hypotheses.
while this could be scripted it was necessarily time consuming because it could be done only on a machine by machine basis.
in addition because this operation required running sage in a way that kept all intermediate data it necessarily was not the same as the real test runs.
with sagan we can perform analyses including all the analyses reported in this paper without ever touching individual machines.
.
.
architecture we built the sagan logging service to support hundreds of simultaneously active machines.
a central server runs microsoft sql server and internet information server on windows server r2.
each of the client machines makes a direct connection to the sql server to insert log updates.
updates happen at the beginning of every sage run after every new crashing test case after every failed task and then at random intervals ranging between 30to 45minutes during the run.
we randomize the intervals for updates to avoid synchronization of periodic messages as recommended by floyd and jacobson .
while the overall disk usage of a sage run can total hundreds of gigabytes we applied our principles to reduce the amount of information that must be shipped to our central server.
first we limit the initial information sent to the configuration options and command line which are a few kilobytes each.
second each heartbeat contains counters representing the number of files created number of crashes found coverage and a small log file from our whitebox fuzzer which is also typically under 10kilobytes.this means that for a run with no failed tasks no constraint solver timeouts and no crashing test cases found our space and bandwidth requirements are modest.
for constraint solver timeouts while we do ship the entire constraint to sagan for later analysis we limit the number of such constraints shipped on each run.
finally while in the case of failed symbolic execution tasks we may need to ship instruction traces in the hundreds of megabytes we probabilistically decide whether or not to ship individual traces and ship at most 5such traces perrun.
.
.
data presentation we present the data in two main ways.
first we designed a web front end to expose information from the server.
every run has its own unique url that shows configuration information health and number of crashes found at a glance.
by clicking on a link we can drill down into statistics from any of the symbolic execution tasks that have completed during that fuzzing run.
for example we can see how many constraints were generated and how often optimizations in constraint generation were invoked.
second we run sql queries against the tables holding data from sage runs.
this gives us the flexibility to answer questions on the fly by looking at the data.
for example we can create a list of every sage run that has at least one symbolic execution task where more than 10queries to z3 timed out.
we use this capability to work with our partners and understand if there are specific features of programs that might cause long running constraint solver queries or other strange behavior.
all the analyses in section were performed using our infrastructure.
.
jobcenter the sagan system gives us insight but it is only one piece of the puzzle.
we need an active control system for the machines running sage .
in particular the programs on which we test sage are not static.
while sage is running developers are continually updating the code fixing bugs and adding new features.
periodically we must upgrade the programs sage tests to the most recent version to ensure that the bugs sage finds are most likely to reproduce on developers machines and have not been fixed already.
.
.
configuration management we run sage against multiple configurations.
a configuration consists of a set of initial test cases or seed files a target program for execution with its arguments and a set of parameters that define timeout values for each of the different sage tasks.
one program of interest may have many configurations.
for example a single large program may have parsers embedded for many different file formats.
together with our partners who run sage we have defined hundreds of distinct sage configurations in use today.
manually starting sage with correct configurations on hundreds of machines would be a nightmare.
we designed a control service called jobcenter that automates this work in conjunction with additional infrastructure created by different partners of sage .
our partners typ5ically have infrastructure that can automatically upgrade a machine to the latest build of the software under test and reset the disk to a clean state.
jobcenter then deploys the right version of sage and the right configuration to start the fuzz testing run.
while we still need manual effort to determine if a configuration has the right parameters and options jobcenter allows us to do this work once and reuse it across multiple machines automatically.
.
.
control and recovery we have made changes to sage that integrate it with jobcenter for runtime control.
a jobcenter web service allows changing configuration values on the fly which we use for fine tuning and experimentation.
for example we can change how long to wait while tracing the execution of the test program.
we can also pause a sage job allowing us to upgrade the version of sage used or perform in depth analysis.
we have implemented facilities for controlling multiple jobs at once as well.
jobcenter can detect when a sage run has terminated then trigger a request to jobcenter for a new configuration.
this means that even if a particular configuration is faulty and leads to a paused run we can try to recover and continue to use the machine we developed this after looking at the utilization data in section .
we currently have at least 90concurrent virtual machines reporting to jobcenter at any given time.
finally our test machines run windows and typically need periodic reboots for security patches.
power outages servicing or other events may also cause unexpected reboots.
we have modified sage to persist run information to disk in a way that ensures we can pick up after such an event when the machine comes back up.
the jobcenter remembers which configuration is associated with a machine and on machine boot can re start an interrupted whitebox fuzzing run.
prior to this we had difficulty achieving runs of more than a month in length.
figure shows the overall architecture with vms running sage talking to jobcenter and sagan .
.
task partitioning as we discussed in section a sage fuzzing run consists of four different types of tasks.
in principle these tasks could be run on separate machines.
for example we could perform all tracing on one machine then send the resulting instruction traces to a second machine for symbolic execution forward the resulting constraints to another machine.
finally we could run the newly created tests and measure coverage on a yet another machine.
in our runs we typically keep all tasks on the same machine.
we do this for two reasons.
first because the instruction level traces we create are often hundreds of figure architecture for jobcenter and sagan .
machines running sage communicate to sagan for logging and jobcenter for control.
crashing test cases are placed on a network share where they are picked up by the distributed file fuzzer test farm.
this test farm applies automatic triage to identify likely bugs which humans then review and file in a bug database.
megabytes in size which means we would incur delays in moving them between machines.
we do not have the ability to control the network infrastructure used by our partners so we cannot assume fast links between pairs of machines.
second as we will see in section in our experience most constraints are solved within a tenth of a second meaning that it is cheaper to solve them locally on the machine than to serialize send over the network deserialize and solve.
in office however our partners have developed an infrastructure for leveraging nights and weekends of machines deployed around the company to test files for crashes .
to leverage this infrastructure which can test hundreds of thousands of files in hours we have a mode where sage copies crashing test cases on a network share for later processing.
this infrastructure also performs an additional layer of triage identifying bugs that may have already been filed and prioritizing likely new bugs in front of users.
data driven whitebox fuzzing we now describe several specific analyses enabled by sagan data collected from runs of sage at scale.
for each analysis we describe improvements made to our fuzzing practice or key insights that suggest future directions for whitebox fuzzing.
the data we present spans 6several major sessions which each consist of hundreds of individual executions of the sage tool each sage run itself fuzzing a different application during typically two to four weeks on a dedicated multi core machine.
we note that the data analyzed here has been collected not from controlled experiments but from production test runs.
we did not choose which applications to test in each session the length of the run or the specific configuration settings.
this means that sometimes the data for different sessions might be more difficult to compare.
because the data that are being collected are so diverse however we were able to gain valuable feedback to track issues and evaluate improvements.
.
utilization and failure detection the first key analysis is monitoring utilization and detecting failures in sage deployments.
first we can determine failing runs by monitoring heartbeats.
before sagan we had no way to know how the runs progressed meaning that runs could die perhaps due to a wrong test setup or running out of disk space and it would not be apparent until we examined the results which could be weeks later.
using data from sagan we improved our labmachine utilization over time.
we show here data for three successive sessions.
in the first session with sagan we were able to detect that many of the machines died unexpectedly as shown on the leftmost chart of figure .
we then used the configuration files sent back by sagan to diagnose potential failures then develop a fix.
as a result we saw improved utilization in the second and third major sessions as shown on the middle and right charts of figure .
in another instance data not shown after several hundreds of runs had started and were expected to run for at least three weeks we were able to detect within hours that all of the symbolic execution tasks were failing due to a temporary networking problem that happened during the setup of the session.
we then corrected the problem and re started the session.
second besides obvious symptoms of a failing run we check data on sagan that indicate how the run is proceeding.
in particular we check whether the run generated any new tests whether the symbolic execution task detected symbolic inputs and how many bugs have been found.
this in turn helps detect configuration errors.
for example if the timeout for tracing the application is set too low then the trace will end before the application even reads from an input file.
we then see this show up in sagan as a large percentage of symbolic execution tasks that fail to detect any symbolic inputs.
for runs with problems the jobcenter infrastructure allows adjusting configuration parameters of the run.
in this example we can increase the maximum time allowed for creating the execution trace to solve the problem.
sagan data also showed us that in the first and second sessions and of all sage executions died due to the machines exhausting disk space.
we modified sage to remove non critical files automatically during low disk space conditions.
all subsequent runs have had failures due to low disk space.
we have also used sagan to detect which sage tasks have failed most and why.
for example in one session of the about 300sage runs had failures in symbolic execution which did not appear in the previous session.
when we analyzed sagan data about these failures we found a common problem and traced it to the changes in the compiler generating the binaries under test.
working with our partners we fixed the problem and in the next session we had only of all sage runs failing due to symbolic execution failures with none of these remaining failures due to this compiler change.
.
unique bugs found by day we also investigated when unique bugs are detected over the course of sessions.
our whitebox fuzzer sage uses appverifier configured to check for heap errors.
each memory corruption found such as buffer overflow is mapped to a crash.
because sage can generate many different test cases that exhibit the same bug we bucket crashing files by the stack hash of the crash which includes the address of the faulting instruction.
it is possible for the same bug to be reachable by program paths with different stack hashes for the same root cause.
our experiments always report the distinct stack hashes.
we collected earliest detection timestamps for each bucket found during a session.
figure presents this chart for a session over two hundred programs over three weeks.
we can see the first few days were the most productive due to high number of new executions traced covered by symbolic execution1.
the chart also shows two more peaks of new crash buckets on days and .
this shows that new crashes were found throughout the session.
.
incompleteness and divergences another key analysis we performed was tracking incompleteness in symbolic execution.
the x86 instruction set has over a thousand different instructions.
new extensions are added frequently for supporting simd operations such as intel s sse instruction set.
unfortunately sage does not understand how to symbolically execute every such instruction.
this is important because failures 1unfortunately we cannot reveal absolute numbers of crashes found.
7figure utilization graph for three sequential sessions with sagan .
the x axis shows time in days and the y axis shows the number of active sage runs.
by the third session our utilization has improved due to underlying errors identified thanks to sagan data and then fixed.
figure new unique crash buckets per day over days of running sage on about programs.
the data suggests that running longer would yield more unique crashes although the return becomes typically lower.
to properly symbolically execute can lead to an incomplete or wrong path constraint generation with the effect of missing bugs or divergences an input that is expected to drive the program along a new specific path actually follows a different path.
we added instrumentation to sage to detect whenever we found an instruction or an instruction sequence not properly handled by our symbolic execution engine.
we then configured sage to send back counts of how many of such cases and of which type were encountered during every symbolic execution.
moreover we prioritized the incompleteness cases into high and low severity categories.
the high severity case indicates instructions not handled at all by our symbolic execution.
low severity means that the handling has known shortcomings but still creates some kind of approximate constraint.
after a first session with this new instrumentation we analyzed the new resulting sagan data to determine which instructions and sequences of instructions had thehighest counts.
for example we found that over of the high severity instructions were shift instructions.
we implemented symbolic instruction handlers for these instructions.
as a result the data from the next session showed that in the high severity category the vast majority of the instructions had now been handled.
we repeated this process across subsequent session in order to address remaining holes in symbolic execution and prioritize the writing of new symbolic instruction handlers.
.
constraint generating and solving another large amount of data we have collected during our400machine years of running sage relates to symbolic execution and constraint solving.
we now present such data for a sample set of about 000symbolic executions performed on about 300different applications running on windows and their corresponding constraints.
to be exact this set consists of 190symbolic execution tasks.
the sum of all constraints generated and solved during those symbolic executions is constraints thus an average of 426constraints generated for each symbolic execution after all the constraint pruning done by the techniques and heuristics described later in this section .
.
.
constraint solving time figure presents the average solver time per constraint for each symbolic execution.
from this chart one can see that about of all constraints are solved by z3 in .
seconds or less and that about of all constraints are solved in second or less.
thus most solver queries are fast.
.
.
symbolic execution and solver time figure compares the total time in seconds spent symbolically executing programs versus solving constraints for each of the symbolic execution tasks considered.
even though most constraints are solved in less than 8figure average time for solving constraint queries for each symbolic execution task.
the blue bars show the number of tasks with an average solving time in the labeled bin.
the red line is a cumulative distribution function over all tasks.
.
of all tasks have an average solving time of .
seconds or less.
second we can see that solving time often dominates symbolic execution time because those runs solve many many constraints.
moreover we can also see extreme cases or outliers where symbolic execution time dominates strongly upper left or where solving time dominates exclusively lower right .
a more detailed analysis of the data depicted in figure reveals that almost of all the symbolic execution tasks fit in the lower left corner of the graph in a box bounded by 200seconds by 200seconds.
zooming in that part of graph reveals the picture shown in figure .
following this observation we implemented new features in sage to limit the time in symbolic execution and limit the number of constraints being generated.
these new features can cut off the outliers observed in figure which consume a lot of cpu time but produce few constraints and hence few tests upper left area or spend too much time solving constraints lower right area .
by zooming in on tasks in the lower right area we observed data not shown here that most of those constraints are actually unsatisfiable and therefore do not contribute to new tests.
indeed intuitively the more symbolic execution generates constraints the longer the path constraint by definition the more constrained the tail of the path constraint is since every negated constraint is put in a conjunction with all the constraints before it in the path constraint and the more unsatisfiable the constraints in the tail usually are.
therefore most constraints in long tails are usually unsatisfiable.
of course dropping figure each dot is a symbolic execution task.
on the y axis the time to create constraints secs .
on the xaxis the time to solve constraints secs .
notice extreme outliers in both directions.
unsatisfiable constraints is harmless for the purpose of test generation and finding bugs.
after enforcing such new limits on symbolic execution time and the number of constraints generated by each symbolic execution we saw in the next session increases in the number of symbolic execution tasks per sage run .
times as expected but also an increase in the average number of queries per symbolic execution task .
times and an increase in the total number of queries per sage run times .
.
.
unsatisfiable constraints figure plots the number of satisfiable constraints x axis versus the number of unsatisfiable constraints and timeouts y axis for all symbolic execution tasks in the previous data set that have a number of sat solver queries less than and a total number of queries less than .
this set contains tasks which represents .
of all tasks in the previous data set the remaining outliers are hard to visualize in this form and were therefore omitted .
this figure illustrates that most constraints generated by most symbolic executions are unsatisfiable most dots are above the red line where y x. why are most constraints generated by sage solved in a fraction of a second?
an important optimization we use is related constraint optimization which removes the constraints in the path constraint that do not share symbolic variables with the negated constraint a simple syntactic form of cone of influence reduction this optimization often eliminates more than of the constraints in the path constraint.
another critical and standard optimization is symbolic expression caching which 9figure symbolic execution tasks limited to a maximum of seconds for constraint generation and seconds for constraint solving.
this set accounts for over percent of all tasks shown in figure .
ensures that structurally equivalent symbolic terms are mapped to the same physical object and avoids an exponential blow up when the same sub terms appear in different sub expressions.
we also use local constraint caching which skips a constraint if it has already been added to the path constraint since its negation cannot possibly be satisfiable with the first constraint in the path constraint i.e.
p pis always unsatisfiable no matter what pis .
these three optimizations are sound that is they cannot themselves cause to miss bugs.
we also use other optimizations which are necessary in practice to avoid the generation of too long path constraints but arbitrarily prune the search space and are therefore unsound i.e.
can force the search to miss bugs.
specifically a flip count limit establishes the maximum number of times a constraint generated from a particular program branch can be flipped.
moreover using a cheap syntactic check constraint subsumption eliminates constraints logically implied by other constraints injected at the same program branch mostly likely due to successive iterations of an input dependent loop .
together these optimizations reduce the size and complexity of the queries sent to the constraint solver.
also most of these queries are by construction essentially conjunctive i.e.
large conjunctions of constraints and therefore known to be rather easily solvable most of the time.
in contrast large disjunctions are usually harder to solve.
our results show that for whitebox fuzzing the art of constraint generation is as important as the art of constraint solving .
figure each dot is a symbolic execution task.
on the xaxis the number of satisfiable constraints i.e.
the number of new test cases generated.
on the y axis the number of unsatisfiable or timedout constraints.
most tasks have a high proportion of unsatisfiable constraints.
figure on the x axis we place different branches observed during whitebox fuzzing.
on the y axis we have the frequency of occurrence.
the graph shows only the 100first most common branches out of 360total.
.
commonality between programs when running a whitebox fuzzer at our scale on hundreds of applications we are bound to have common subcomponents being re fuzzed over and over again especially in the windows operating system where a lot of the core system functions are packaged in dlls dynamically loaded libraries used by many of those applications.
figure shows statistics about all the program branches flipped during a whitebox fuzzing session for about applications running on windows.
each program branch is identified by a dll name and an offset in that dll identifying a conditional statement typically a jump at that location and where a symbolic inputdependent constraint was generated.
the data represent program branches flipped.
there are dis10tinct branches with a maximum frequency of extreme left and minimum frequency in the extreme right which is not shown here the tail is shown only up to distinct branch ranked as and there is a very long and flat tail up to distinct branch after this.
as one can clearly see from the figure a small percentage of unique program branches to the left are flipped over and over again and represent the part of the search space where most constraints were generated.
remember the data shown here was obtained after the pruning described in the previous subsection preventing the same branch to be flipped over a specific limit without this pruning the data would be even more tilted towards these few instructions.
this behavior is typical even in a single application.
for instance when whitebox fuzzing a structured non binary parser most of the input constraints generated are in the lexer of course.
re fuzzing over and over again the same subcomponents and dlls is wasteful.
in principle this can be avoided with compositional testing which createstest summaries from symbolic execution.
these summaries are not only re usable during a fuzzing session but also apply across applications that share common components such as dlls and over time from one fuzzing session to the next .
compositional testing can result in a search algorithm that is exponentially faster than a non compositional one.
every test run in a centralized infrastructure can create new test summaries to improve all future test runs through this component.
we are currently investigating how to best use and deploy such techniques in production mode including a centralized repository of test summaries for large parts of the windows operating system.
related work blackbox fuzz testing in clusters at a large scale is not new.
nagy described a custom built cluster dedicated to high volume testing of microsoft word that processes .
million test cases per day .
the office team at microsoft has built a distributed fuzzing framework that works across nights and weekends use of idle desktops as well as in clusters .
google s security team devoted 000cores over roughly four weeks to fuzz testing adobe flash .
we leverage previous work in this area on the classification of crashing test cases prioritization of bugs and automatic reporting of important bugs to developers.
what is new is the use of whitebox fuzzing at the scale described in this paper.
whitebox fuzzing combines and extends program analysis testing verification model checking and automated theorem proving techniques thathave been developed over many years.
one of the earliest proposals for using static analysis as a kind of symbolic program testing method was proposed by king almost years ago .
the idea is to symbolically explore the tree of all computations the program exhibits when all possible value assignments to input parameters are considered.
static test generation consists of analyzing a program statically by using symbolic execution techniques to attempt to compute inputs to drive the program along specific execution paths or branches without ever executing the program .
unfortunately this approach is ineffective whenever the program contains statements involving constraints outside the scope of reasoning of the theorem prover i.e.
statements that cannot be reasoned about symbolically.
this is frequent in practice due to complex program statements pointer manipulations floating point operations etc.
and calls to operating system and library functions.
a recent breakthrough was the emergence of a second approach dynamic test generation .
it consists of executing the program typically starting with some random inputs while performing symbolic execution dynamically collecting symbolic constraints on inputs gathered from predicates in branch statements along the execution and then using a constraint solver to infer variants of the previous inputs in order to steer the next execution of the program towards an alternative program branch.
a key advantage of dynamic test generation is that imprecision in symbolic execution can be alleviated using concrete values and randomization whenever symbolic execution does not know how to generate a constraint for a program statement or library depending on some inputs one can always simplify this constraint using the concrete values of those inputs .
over the last few years dynamic symbolic execution and test generation have been implemented in many tools such as apollo cute klee pex and s2e see for a recent survey .
for instance the klee tool built on llvm showed these techniques could yield high coverage on all programs in the coreutils andbusybox suites outperforming hand generated tests created over multiple years.
follow on tools have applied these techniques to testing drivers and finding trigger behavior in malware .
while promising this prior work did not report of any daily use of these techniques and tools.
another recent significant milestone is the emergence ofwhitebox fuzzing as the current main killer app for dynamic test generation and arguably for automatic code driven test generation in general.
this in turn has allowed the developmemt of the next step in this decadeslong journey the first productization of large scale 11symbolic execution and constraint solving.
this paper is the first to report on this development.
our work extends previous dynamic test generation tools with logging and control mechanisms.
these mechanisms allow us to run whitebox fuzz testing for weeks on end with low effort and cost and they enable data driven improvements to our fuzzing platform.
the closest related prior works are metafuzz and cloud9 .
metafuzz also performed logging of whitebox fuzzing on linux using the smartfuzz plugin for valgrind .
unlike our current work however metafuzz has no automatic control features.
furthermore the authors do not show how data from metafuzz directly inform future research or operations of whitebox fuzzing.
finally the metafuzz authors report experiments of 24hours in length while we report on multiple multi week runs of whitebox fuzzing on many more applications.
cloud9 is a system for scaling klee style symbolic execution across a cluster of commodity machines the authors use amazon s elastic compute cloud in their experiments.
like klee cloud9 focuses on creating high coverage test suites for commodity programs.
their techniques for scaling symbolic execution across multiple machines are complementary to ours and could be used to inform the task partitioning we described in section .
again however they do not report on multiple multiweek runs against hundreds of different test programs.
our work reports on usage and deployments which are orders of magnitude larger in all five scalability dimensions identified in section than any prior work in this space.
conclusion we have shown that whitebox fuzzing scales to production use.
our biggest pain points have been around the heterogeneity of different applications with respect to configuration and in the logistics of provisioning hundreds of machines.
the systems we described here were developed in direct response to these pain points.
the logging from sagan helps us keep track of hundreds of different programs and seed files.
the jobcenter service then lets us turn around and deploy new configurations to machines running with our infrastructure.
moving forward we know from our data that significant challenges remain in improving the precision of symbolic execution and combating path explosion.
as described in section and we currently use unsound optimizations to scale to long execution traces and programs such as the office suite but those may also miss important bugs.
our tool also supports many optional features such as reasoning with symbolic pointers which are sometimes expensive and therefore not always all turnedon in production runs.
by adding other monitoring features to sagan we hope to drill down and understand better these cost precision trade offs.
we also recognize that whitebox fuzzing is only one piece of the security puzzle and one niche application for automatic test generation.
the major longer term win from all this technology comes in a shared infrastructure for the entire picture of people processes and tools required to build secure reliable software.
our infrastructure reduces the cost to enroll a program in this infrastructure which is the gating factor to applying our techniques or any others.
while in our case the infrastructure is onpremise this could also involve the use of machines in the cloud.
no matter where it is located we see centralized data driven security testing and software engineering infrastructure as a key direction for future research.
acklowledgments this paper would not have been possible without the help and support of many people.
we thank michael levin for starting the development of the sage tool and participating in this project for several years.
we also thank chris marsh lei fang stuart de jong and others in the former center for software excellence for building the dynamic program analysis stack sage is built upon.
we thank the developers of the z3 smt solver including nikolaj bjorner and leonardo de moura.
we appreciate the support of our managers in pursuing this project including tom ball wolfram schulte and helen wang.
mehdi bouaziz made substantial implementation contributions to jobcenter while an intern at microsoft.
additional interns who have worked on sage include dennis jeffries adam kiezun bassem elkarablieh marius nita cindy rubio gonzalez johannes kinder daniel luchaup and louis jachiet.
the work reported in this paper was performed in collaboration with our partners in windows and in office.
in windows we thank specifically nick bartmon eric douglas dustin duran david grant elmar langholz vince orgovan isaac sheldon evan tice dave weston and russell west for running sage and supporting development of the binary analysis tools on which it depends.
in office we are particularly grateful to tom gallagher eric jarvi and octavian timofte for running sage and working with us to develop jobcenter.
from the microsoft security engineering center we thank matt miller lars opstad andy renk jason shirk and dazhi zhang.
we thank our msr colleagues rich draves jon howell stuart schecter and helen wang for feedback on drafts of this paper under short notice.
finally we thank all the users of sage across microsoft.
12references s. bhansali w. chen s. de jong a. edwards and m. drinic.
framework for instruction level tracing and analysis of programs.
in second international conference on virtual execution environments vee .
d. brumley c. hartwig z. liang j. newsome d. song and h. yin.
automatically identifying trigger based behavior in malware.
in botnet detection pages .
springer .
s. bucur v. ureche c. zamfir and g. candea.
parallel symbolic execution for automated real world software testing.
in proceedings of the sixth conference on computer systems eurosys new york ny usa .
acm.
c. cadar d. dunbar and d. engler.
klee unassisted and automatic generation of high coverage tests for complex systems programs.
in osdi .
c. cadar v. ganesh p. m. pawlowski d. l. dill and d. r. engler.
exe automatically generating inputs of death.
in acm ccs .
c. cadar p. godefroid s. khurshid c.s.
pasareanu k. sen n.tillmann and w. visser.
symbolic execution for software testing in practice preliminary assessment.
in icse honolulu may .
v. chipounov v. kuznetsov and g. candea.
s2e a platform for in vivo multi path analysis of software systems.
in asplos .
microsoft corporation.
security development lifecycle .
security sdl default.aspx .
l. de moura and n. bjorner.
z3 an efficient smt solver.
in proceedings of tacas 14th international conference on tools and algorithms for the construction and analysis of systems volume oflecture notes in computer science pages budapest april .
springer verlag.
b. elkarablieh p. godefroid and m. y. levin.
precise pointer reasoning for dynamic test generation.
inissta pages .
ch.
evans m. moore and t. ormandy.
fuzzing at scale .
http googleonlinesecurity.blogspot.
com fuzzing at scale.html .
s. floyd and v. jacobson.
the synchronization of periodic routing messages.
ieee acm trans.
netw.
april .
j. e. forrester and b. p. miller.
an empirical study of the robustness of windows nt applications using random testing.
in proceedings of the 4th usenix windows system symposium seattle august .
t. gallagher and d. conger.
under the kimono of office security engineering.
in cansecwest .
p. godefroid.
compositional dynamic test generation.
in proceedings of popl 34th acm symposium on principles of programming languages pages nice january .
p. godefroid n. klarlund and k. sen. dart directed automated random testing.
in proceedings of pldi acm sigplan conference on programming language design and implementation pages chicago june .
p. godefroid s. k. lahiri and c. rubio gonzalez.
statically validating must summaries for incremental compositional dynamic test generation.
inproceedings of sas 18th international static analysis symposium volume of lecture notes in computer science pages venice september .
springer verlag.
p. godefroid m.y.
levin and d. molnar.
automated whitebox fuzz testing.
in proceedings of ndss network and distributed systems security pages san diego february .
j. c. king.
symbolic execution and program testing.journal of the acm .
b.p.
miller l. fredriksen and b. so.
an empirical study of the reliability of unix utilities.
communications of the acm december .
d. molnar x. c. li and d. a. wagner.
dynamic test generation to find integer bugs in x86 binary linux programs.
in usenix security symposium .
b. nagy.
finding microsoft vulnerabilities by fuzzing binary files with ruby a new fuzzing framework.
in syscan .
com watch?v u j4yy 7cg.
s. narayanasamy z. wang j. tigani a. edwards and b. calder.
automatically classifying benign and harmful data races using replay analysis.
in programming languages design and implementation pldi .
n. tillmann and j. de halleux.
pex white box test generation for .net.
in proceedings of tap 2nd international conference on tests and proofs volume of lecture notes in computer science pages .
springer verlag april .
view publication stats