testing software in age of data privacy a balancing act kunal taneja north carolina state university raleigh nc ktaneja ncsu.edumark grechanik accenture technology labs chicago il mark.grechanik accenture.comrayid ghani accenture technology labs chicago il rayid.ghani accenture.comtao xie north carolina state university raleigh nc xie csc.ncsu.edu abstract database centric applications dcas are common in enterprise computing and they use nontrivial databases.
testing of dcas is increasingly outsourced to test centers in order to achieve lower cost and higher quality.
when proprietary dcas are released their databases should also be made available to test engineers.
however different data privacy laws prevent organizations from sharing this data with test centers because databases contain sensitive information.
currently testing is performed with anonymized data which often leads to worse test coverage such as code coverage and fewer uncovered faults thereby reducing the quality of dcas and obliterating benefits of test outsourcing.
to address this issue we offer a novel approach that combines program analysis with a new data privacy framework that we design to address constraints of software testing.
with our approach organizations can balance the level of privacy with needs of testing.
we have built a tool for our approach and applied it to nontrivial java dcas.
our results show that test coverage can be preserved at a higher level by anonymizing data based on their effect on corresponding dcas.
categories and subject descriptors d. .
testing tools d. .
information flow controls k. .
computers and society public policy issues privacy general terms security verification keywords data anonymity software testing privacy framework utility priest .
introduction large organizations today face many challenges when engineering software applications.
particularly challenging is the fact that permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
esec fse september szeged hungary.
copyright acm ... .
.many applications work with existing databases that contain confidential data.
a large organization such as a bank insurance company or government agency typically hires an external company to develop or test a new custom software application.
however recent data protection laws and regulations around the world prohibit data owners to easily share confidential data with external software service providers.
database centric applications dcas are common in enterprise computing and they use nontrivial databases .
when releasing these proprietary dcas to external test centers it is desirable for dca owners to make their databases available to test engineers so that they can perform testing using original data .
however since sensitive information cannot be disclosed to external organizations testing is often performed with synthetic input data .
for instance if values of the field nationality are replaced with the generic value human dcas may execute some paths that result in exceptions or miss certain paths .
as a result test centers report worse test coverage such as code coverage and fewer uncovered faults thereby reducing the quality of applications and obliterating benefits of test outsourcing .
automatic approaches for test data generation partially address this problem by generating synthetic input data that lead program execution toward untested statements.
however one of the main issues for these approaches is how to generate synthetic input data with which test engineers can achieve good code coverage.
using original data enables different approaches in testing and privacy to produce higher quality synthetic input data thus making original data important for test outsourcing.
a fundamental problem in test outsourcing is how to allow a dca owner to release its private data with guarantees that the entities in this data e.g.
people organizations are protected at a certain level while retaining testing efficacy.
ideally sanitized data sanitized data or anonymized data is the original data after anonymization.
we use the two terms interchangeably throughout this paper should induce execution paths that are similar to the ones that are induced by the original data.
in other words when data is sanitized information about how dcas use this data should be taken into consideration.
in practice this consideration rarely happens our previous work showed that a popular data anonymization algorithm called k anonymity seriously degrades test coverage of dcas.
naturally different dcas have different privacy goals and levels of data sensitivity privacy goals are more relaxed for a dca that manages a movie ticket database than for a dca that is used within banks or government security agencies.
applying more relaxed protection to databases is likely to result in greater test coverage since a small part of the databases is anonymized conversely 201stricter protection makes it more difficult to outsource testing.
the latter is the result of two conflicting goals making testing as realistic as possible and hiding the original data from testers who need this data to make testing effective.
balancing these goals i.e.
to anonymize data while preserving test coverage of dcas that use this data is emerging to be an important problem.
given the importance of this problem it may be surprising that there exists little prior research on this topic.
there may be two main reasons for the lack of prior research.
first elaborate data privacy laws are a new phenomenon and many of these laws have been introduced after the year .
second it is only in the past decade that applications are increasingly being tested by thirdparty specialized software service providers which are also called test centers.
numerous test centers have emerged and often offer lower cost and higher quality when compared to in house testing.
in the test outsourcing market was worth more than usd billion and growing at annually making test outsourcing the fastest growing segment of the application services market .
to address this issue we offer a novel approach privacy equalizer for software testing priest that combines a new data privacy framework with program analysis enabling business analysts to determine how anonymizing values of database attributes affects test coverage.
with priest organizations can balance the goals of preserving test coverage while releasing dcas to external test centers with a controlled disclosure of sensitive information.
the source code for priest as well as its illustration video are publicly available1.
our work is unique to the best of our knowledge there exists no prior approach that addresses the problem that we pose in this paper.
in summary we make the following main contributions in this paper we create a new privacy framework described in section that includes a novel combination of guessing anonymitybased privacy metrics and a technique of data swapping anonymization to enable organizations to keep original values in sanitized data keeping such values is important for improving the effectiveness of testing.
we design and implement a technique using program analysis for determining how values of database attributes affect test coverage of dcas that use this data see section .
.
we combine our privacy framework with this technique in priest to enable business analysts to balance data privacy with test coverage.
we evaluate priest using three open source java dcas and one large java dca that handles logistics of one of the largest supermarket chains in spain.
we show that with priest test coverage can be preserved at a higher level by pinpointing database attributes that should be anonymized based on their effect on corresponding dcas.
.
the problem in this section we provide the necessary background on how dcas interact with databases show how sanitizing data affects test coverage of dcas describe the state of the art and practice and formulate the problem statement.
.
background majority of enterprise level dcas use programs that are written in general purpose programming languages and relational databases e c o r d s e t db .
e x e c s q l select from t b l p a t i e n t n a t i o n a l i t y r e c o r d s e t .
g e t a t t r i b u t e i age r e c o r d s e t .
g e t a t t r i b u t e j d i s e a s e r e c o r d s e t .
g e t a t t r i b u t e d i s e a s e i f n a t i o n a l i t y p a l a u a n age f d i s e a s e figure an illustrative example that shows how replacing values of database attributes nationality andage of patients can make the function funreachable.
to maintain large amounts of data.
a primary way for these programs to communicate with databases is to use call level interfaces which allow dcas to access database engines through standardized application programming interfaces apis e.g.
java database connectivity jdbc .
using jdbc programs pass sql queries as strings to corresponding api calls to be sent to databases for execution.
once these queries are executed values of attributes of database tables are returned to dcas using jdbc recordset objects and these values are stored in variables of the dcas which in turn use these variables as part of their application logic.
in this way values from a database may be used in branch decisions and these values may therefore affect the subsequent execution of the dcas.
depending on returned values different paths can be taken in dcas and subsequently these values affect test coverage.
hence removing certain classes of values in database attributes may make some branches and statements in dcas unreachable.
to see how anonymization affects test coverage of dcas consider a fragment of code shown in figure .
after executing jdbc api calls that submit an sql query to a database and obtain the objectrecordset the values of the attributes age nationality anddisease are put in the corresponding variables of the dca nationality age anddisease respectively.
if the values of the attribute nationality are replaced with the generic value human the function call f becomes unreachable.
certain classes of values of database attributes that contain nonsensitive information should be anonymized.
these attributes also called quasi identifiers qi often contain information that can be linked with other data to infer sensitive information about entities i.e.
people objects .
for example given the values of the qis race sex height zipcode and the attribute that contains sensitive data about diseases it is possible to link a person to specific diseases provided that the values of these qis uniquely identify this person.
existing data anonymization approaches are centered on creating models for privacy goals and developing algorithms for achieving these goals using particular anonymization techniques .
one of the most popular privacy goals is k anonymity where each entity in the database must be indistinguishable from k others.
anonymization approaches use different anonymization techniques including suppression where information e.g.
nationality is removed from the data and generalization where information e.g.
age is coarsened into sets e.g.
into age ranges .
these and other techniques modify or suppress values of attributes and a common side effect of these modifications is non covered statements in dcas that are otherwise covered with the original data.
.
state of the art and practice after interviewing professionals at ibm accenture two large health insurance companies a biopharmaceutical company two large supermarket chains and three major banks we found that current test data anonymization is manual laborious and error202prone.
in a few cases client companies outsource testing using an especially expensive and cumbersome testing procedure called cleanroom testing where dcas and databases are kept on company premises in a physically secured environment .
typically business analysts and test managers from outsourcing companies come to the cleanrooms of their client companies to evaluate their clients dcas and to plan work.
however when better options to access data are not available testers from outsourcing companies are also allowed in these cleanrooms to test software on their clients premises.
actions of these test engineers are tightly monitored network connections phone calls cameras and usb keys are forbidden.
cleanroom testing requires significant resources and physical proximity of test outsourcing companies to their clients.
a more commonly used approach is to use tools that anonymize databases indiscriminately by generalizing or suppressing all data.
even though this procedure is computationally intensive it is appealing since it does not require sophisticated reasoning about privacy goals and protects all data.
but in many real world settings protecting all data blindly makes testing very difficult.
when large databases are repopulated with fake data it is likely that many implicit dependencies and patterns among data elements are missing thereby reducing testing efficacy.
moreover fake data is likely to trigger exceptions in dcas leading test engineers to flood bug tracking systems with false bug reports.
in addition testers often cannot use such anonymized data because dcas may throw exceptions that would not occur when the dcas are tested with original data or other real data in field.
a more sophisticated approach is selective anonymization where a team is assembled comprising of business analysts and database and security experts .
after the team sets privacy goals identifies sensitive data and marks database attributes that may help attackers to reveal this sensitive data i.e.
qis anonymization techniques are applied to these qis to protect sensitive data resulting in a sanitized database.
a goal of all anonymization approaches is to make it impossible to deduce certain facts about entities with high confidence from the anonymized data pages .
unfortunately this approach is subjective manual and laborious.
in addition it involves highly trained professionals and therefore this approach is very expensive.
currently there exists no solution that enables these professionals to accomplish this task efficiently and effectively with metrics that clearly explain the cost and benefits of selective anonymization decisions.
.
balancing utility and privacy utility of anonymized data is measured in terms of usefulness of this data for computational tasks when compared with how useful the original data is for the same tasks.
consider an example of the utility of calculating average salaries of employees.
adding random noise to protect salary information will most likely result in computing incorrect values of average salaries thereby destroying utility of this computation.
recent cases with u.s. census show that applying data privacy leads to incorrect results .
multiple studies demonstrate that even modest privacy gains require almost complete destruction of the data mining utility .
data swapping is an anonymization technique that is based on exchanging values of attributes among individual records while maintaining certain distribution properties .
data swapping is more effective in preserving utility than data suppression and generalization privacy algorithms since data swapping allows users to better preserve statistical information .
in this paper we develop a data swapping privacy algorithm that allows analysts to balance privacy and utility goals i.e.
to chooseappropriate levels of privacy that will guarantee certain basic test coverage.
.
the problem statement the problem statement that we address in this paper is how to enable stakeholders to balance a level of test coverage for dcas i.e.
utility with privacy for databases that these dcas use.
the problem space is restricted by three fundamental constraints of software development and privacy preserving data publishing as follows.
first a chosen anonymization approach should preserve original data as much as possible since it is important for preserving the testing utility of dcas.
anonymizing databases using data suppression techniques may result in a different behavior of the dcas.
suppose that no nis the set of all covered nodes in the control flow graph of a dca when the dca is tested with the original data andna nis the set of all covered nodes when the same dca is tested with the anonymized data.
in general testing with anonymized data makes dcas behave in ways that are different from specifications and as a result new execution paths in dcas with anonymized data may lead to exceptions or not covered branches being covered originally.
for example the dca logic handles suppressed values of nationality by not covering the body of theifstatement shown in figure .
therefore in the worst case nt no na where ntis the set of nodes of the preserved statement coverage.
a problem is how to keep a good extent of the original data in databases for testing dcas while guaranteeing certain levels of privacy.
our goal is to ensure that all statements i.e.
nodes in the control flow graph that are executed with original data will also be executed with anonymized data.
our goal is difficult to achieve since it is an undecidable problem to determine precisely how values of qis are used in dcas .
in addition anonymization algorithms present a dilemma suppressing attribute data with different values results in loss of test coverage and keeping original data in the database results in loss of privacy.
balancing these conflicting outcomes is the problem that we address in this paper.
second as a result of software evolution the code of the dca is modified.
in consecutive releases the dca may use different database attributes in different ways thereby requiring the dca owner to reanonymize data to ensure that the balance between privacy and utility is maintained.
however re anonymization introduces a problem if an attacker keeps the previous version of the anonymized data where values of some attributes are not sanitized then this attacker can link original values in different releases thus inferring sensitive information.
a solution should enable stakeholders to repeatedly release anonymized data in such a way that both privacy and utility are guaranteed at certain levels.
finally a privacy metric should be linked directly to test coverage and vice versa i.e.
guaranteeing a certain level of test coverage should allow stakeholders to calculate bounds of the privacy level.
for example if some path is controlled by branch conditions that use values of database attributes then it is possible to predict the effect of anonymization of these values on this path.
in other words the problem is to present business analysts with choices of predicted coverage levels for different anonymization goals.
.
our solution in this section we present core ideas behind our approach that we call privacy equalizer for software testing priest and we describe the priest architecture and its workflow.
in this section we concentrate on the overall architecture of priest that uses our privacy framework that we describe in section .
.
core ideas at the core of our work are three major ideas.
the first one is a privacy framework that enables organizations to keep original values in sanitized data.
the framework is based on a data swapping anonymization technique to preserve original values of database attributes.
as a result test coverage is not affected so negatively as it happens when data suppression and generalization techniques are used.
the technique swaps data based on a probability value provided by a user.
hence different levels of privacy can be achieved.
the second idea is our guessing anonymity privacy metric that allows stakeholders to quantify the level of privacy achieved in an anonymized database.
in particular the metric provides measurement of difficulty for an attacker to relate a sanitized record to the original record.
the third idea is an idea to statically determine how different database values affect the behavior of a dca.
this idea unifies dcas and their databases in a novel way database attributes are tied to the source code of the dcas and depending on how the dcas use values of these attributes business analysts and security experts can determine what anonymization strategy should be used to protect data without sacrificing much of test coverage.
a key point is that often not all of the database attributes have to be anonymized to achieve a given level of data protection.
therefore it is important to extend data protection strategies with information about how dcas use their databases to which these strategies are applied.
as it often happens control flow decisions in dcas are affected by a smaller subset of database attributes.
in an extreme case if some data in the database is not used in any control flow decision of any dcas then anonymizing this data will have no effect on these dcas.
a more subtle point is that values of some attribute may not affect most branch conditions in dcas and therefore test coverage will not be affected much if this attribute is anonymized.
thus it is beneficial to focus anonymization on those aspects of the data that have minimal influence on deeply nested control flow decisions.
.
priest architecture and process we propose a novel process for using priest that partially involves the cleanroom testing that we described in section .
.
recall that business analysts and test managers from outsourcing companies come to the cleanroom of their client company to evaluate their client s dcas and to plan work.
as part of their evaluation these analysts and managers determine different sets of attributes of the database i.e.
qis that can be used by attackers to re identify entities.
in general only a few subsets of these qis should be anonymized thus creating favorable conditions for preserving test coverage.
with priest this qi selection procedure can be improved by pointing out the qis that affect test coverage the least.
these analysts and managers can use priest as part of their evaluation and planning in order to determine how to maximize test coverage for dcas while achieving desired privacy goals for databases that these dcas use.
as the first step of the priest process programmers link program variables to database attributes using annotations so that these annotations can be traced statically using control and dataflow analyses.
tracing these attribute annotations is required to determine how the values of these attributes are used in conditional expressions to make branching decisions thereby influencing the execution flow of the dcas.
our goal is to quantify the effect of replacing values of database attributes on reachability of program statements.
at first glance it appears to be tedious and laborious work for programmers to annotate program variables with the names of data dbodca dbaanonymization algorithm schema extractorstatic analysis engineattribute rankings4 schema and relations5 3quasi identifiers qis privacy goals6 figure priest architecture and workflow.
solid arrows depict the flow of command and data between components numbers indicate the sequence of operations in the workflow.
base attributes from which these variables obtain values.
in reality it is a practical and modest exercise that takes little time.
programmers annotate selected program variables only once where these variables are first assigned values that are retrieved from recordset objects using specific database related api calls.
we observe that in many projects it is a small fraction of code that deals with obtaining values from databases and most code is written to implement application logic that processes these values.
this observation is confirmed by our previous study that shows that out of randomly chosen java programs in sourceforge there is approximately one jdbc related api call per loc on average that retrieves a value from a recordset object and assigns it to a program variable.
extrapolating this result means that programmers may have to annotate approximately variables for a project with million loc and such expense is acceptable.
in addition a variety of object relational mapper o r mapping tools and frameworks bridge the gap between an application s object model and the relational schema of the underlying database by generating classes that represent objects in relational databases .
since o r mapping is done automatically links between program variables and database attributes are recovered as a by product of using o r mapping tools.
for example one of our subject applications a logistics application for handling one of the largest supermarket chains in spain is written using ibatis2 an open source o r mapper.
figure shows the architecture of priest.
the inputs to priest are the dca bytecode and the original database dbothat this dca uses .
with priest business analysts test managers and security experts connect to dbo to obtain its schema using the schema extractor that uses jdbc metadata services to obtain database schema including all relations among different attributes.
next priest performs control and dataflow analyses using the soot toolkit3to establish how the dca uses values of different database attributes.
values of some attributes are used in expressions to compute other values which in turn are used in other expressions and statements.
in some cases these values are used in conditional statements and they affect control flows of dcas using control flow dependencies.
ideally attributes whose values affect many other expressions and statements in dcas in terms of statement coverage should not be picked as qis.
the output of this procedure is a list of attribute rankings that show how many statements are approximately encapsulated by branches whose conditions contain program variables that receive their values from database attributes.
at this point priest displays to the user the list of ranked 204attributes and their relations with other attributes in the database schema.
the user selects a subset of database attributes as qis whose values should be anonymized to protect sensitive data based on certain privacy goals that are required by the dca owner.
the selected qis are supplied to the anonymization algorithm along with the required privacy level.
in addition the algorithm takes dboas its input and outputs the anonymized database dba.
.
ranking attributes to understand which attributes affect dcas the most we rank these attributes by counting the numbers of statements that their values affect.
to find the preceding information our approach uses taint analysis to track the annotated variables as described in section .
corresponding to each attribute.
in particular for each attribute our approach uses control and data flow taint propagation to find out branch conditions that are tainted by an annotated variable corresponding to the attribute.
we then count the number of statements that are control dependent on these branch conditions.
we perform virtual call resolution using static class hierarchy analysis and we take a conservative approach by counting all the statements in all the target methods that can potentially be invoked.
our experiments on subject applications show that this approach can predict the effect of anonymization on coverage within less than .
error see section and table for further details .
improving the precision to compute how many statements are affected by these attributes is a subject of future work.
.
priest priv acy framework in this section we describe the priest privacy framework.
we discuss constraints show how to connect different aspects of data privacy with the testing utility and present a privacy metric and our anonymization algorithm.
.
constraints and goals after discussions with a number of experts from different organizations who are involved in testing dcas we identified two main constraints for a successful anonymization solution simplicity and consistency.
a simple anonymization solution should not impose significant laborious manual or intellectual effort on stakeholders who are involved in protecting privacy.
a case at hand is when using data suppression and generalization algorithms to achieve k anonymity a popular data privacy approach users must specify generalization hierarchies that guide corresponding anonymization algorithms to replace data values with generalized substitutes to protect sensitive information.
for example a branch of the generalization hierarchy for the attribute nationality could be canadian mexican usa northamerican american human .
in general identifying and properly using these hierarchies involves deep domain expertise and significant effort.
stakeholders want a high degree of automation where they are presented with choices for privacy levels and corresponding test coverages making it easy for them to balance privacy and utility.
consistency of data is the other constraint.
since data used in typical dcas can span multiple tables in databases any field that is used as a key to link entities across tables needs to be anonymized consistently so that the data can be linked correctly after anonymization.
thus anonymization techniques should take into consideration different constraints among attributes that are imposed by the database schema for describing this data.
preserving original data values is a goal of our anonymization framework no new values for a data field should be introducedand the unique set of values in a field should be preserved after anonymization.
this constraint is important because changing the values such as generalizing a five digit zip code to first three digits or replacing the city name with the state in the city field would result in new values for a field that the dca may not be able to handle properly.
having new data values often results in extra effort to modify the dca and may cause unintended consequences in production with real data for example exceptions that are thrown would not be thrown if the original data is used.
random data generation would typically preserve only univariate properties marginal distributions of each attribute.
our approach can preserve distributions over conjunctions of attributes gender female diagnosis ovarian cancer that are useful for preserving test coverage when conditional statements containing variables linked to multiple attributes are concerned and such cases are common in real world dcas.
finally in this paper we assume that the developer or data owner cannot be an attacker.
while other attack models are possible we assume that since developers have direct access to all information that is needed to create software it is reasonable to assume that developers enjoy a high level of trust.
.
guessing anonymity guessing anonymity is a privacy metric that enables stakeholders to quantify the level of privacy using a guessing strategy of the attacker as a sequence of questions of the form are these the original values of quasi identifiers that are used to generate a sanitized record?
definition the guessing anonymity of the sanitized record is the number of guesses that the optimal guessing strategy of the attacker requires in order to correctly guess the record used to generate the sanitized record.
to illustrate this definition consider an attacker with knowledge that alice and chris are in the database and knowledge of their true ages shown in table that contains original data.
sanitized data is shown in table where names are replaced with and the values of the attribute age are perturbed with random noise.
the guessing anonymity of each record is shown in the fourth column of table .
while the record that corresponds to alice has a guessing anonymity of two the sanitized record corresponding to chris has a guessing anonymity of one due to the fact that his age is significantly higher than the ages in the other records.
the distribution of guessing anonymity of the different records in a database can be used to define a variety of privacy metrics.
mean guessing anonymity minimum guessing anonymity and fraction of records with guessing anonymity greater than a certain value m are some metrics that we discuss later in this section.
for our toy database shown in table those values would be .
and .
form respectively.
to further illustrate our definition of guessing anonymity we link our definition to the definition of k anonymity.
recall that in kanonymity a database is considered private if every record has at least kother records in the original database with which the released record is indistinguishable.
k anonymity can be achieved by different anonymization operators but typically suppression and generalization operators are the most commonly used ones.
consider a released record that is anonymized such that there are exactly krecords in the original database with which the released record is consistent.
without any further information the optimal guessing strategy would choose among these krecords with uniform probability for its first guess.
the probability of the first 205table original database name age procedure or prescription alice antidepressants bob antibiotics chris chemotherapy diana abortion table sanitized database name age procedure guessing or prescription anonymity .1antidepressants .
antibiotics .
chemotherapy .
abortion algorithm thepriestprivacy algorithm.
priestprivacy qi p qi is the set of qis and pis the probability that the original data will remain unchanged in the anonymized set t bardblt bardbl null initialize values of the anonymized matrix.
forj to of attributes in bardblqi bardbldo distinctvalues bardblqi bardblj mapsto v j returns the set of distinct values for a given attribute.
fori to of rows in bardblqi bardbldo randomize qii j p mapsto v v js.t.ti j v end for end for return bardblt bardbl guess being correct is1 k. if the first guess is incorrect the second guess is chosen with uniform probability from among the remainingk records.
the probability of the second guess being correct is1 k k .
the expected number of guesses simplifies tok so the expected guessing anonymity of a k anonymized record is k .
.
the priest anonymization algorithm the algorithm priestprivacy is shown in algorithm .
we use a data swapping anonymization technique to preserve original values of database attributes.
the goal is to preserve test coverage better than when data suppression and generalization techniques are used while keeping the data usable by the dca.
to protect privacy each value for each row for each attribute is swapped with some probability with a different value for the same attribute.
for example for the attribute gender that contains two distinct values mandf the user may choose to replace the value of a given cell with the other value with probability .
i.e.
an unbiased coin flip.
excluding an attribute from anonymization means that the probability of value replacement for this attribute is zero.
this algorithm takes as its inputs the matrix of qis and their values bardblqi bardbl whose columns include qis and rows include different tuples for these qis from the original database dbo and the value of the probability p that the original data will remain unchanged in the anonymized matrix bardblt bardbl.
the matrix bardblt bardblhas the same dimensions and semantics as the matrix bardblqi bardbl and it contains anonymized values of qis.
bardblt bardbl s values are initialized to null in line2and this matrix is returned in line .
thefor loop in lines iterates through attributes that are in bardblqi bardbl and the procedure distinctvalues is called to compute the set of distinct values for each qi v .
next in lines the nestedfor loop iterates through all rows for the given qi and thealgorithm guessing anonymity metric calculation algorithm.
privacymetric bardblqi bardbl bardblt bardbl bardbld bardbl initialize values of the distance matrix d. fori to of rows in bardblt bardbldo fork to of rows in bardblqi bardbldo forj to of attributes in bardblqi bardbldo iftj j qii jthen di k di k end if end for di k di k o f attributesinqi end for end for compute privacy metrics pm1andpm2.
r di f f records d d bardbld bardbl dimensions of the similarity matrix.
fori toddo ifd i i 1then di f f records di f f records for metric pm2.
end if forj toddo ifi ne ationslash j d i j d i i then r r end if end for end for return pm1 r d pm2 di f f records d procedurerandomized is invoked to replace the original value in a cell with one of the original distinct values of the given qi and the replaced value is written in the corresponding location in the matrix bardblt bardbl.
once the algorithm iterates over all qis and all rows for each qi it terminates and returns bardblt bardblin line9.
an example of application of the algorithm priestprivacy is shown in table that contains for illustrative purposes three attributes age gender andrace .
the first column of this table holds the record number.
original values are shown in each cell with their sanitized replacements shown as a superscript for each value.
for example record 1contains original value 30for the attributeage that is anonymized and replaced with the value which is one of three distinct values for this attribute.
the superscripted values populate the matrix bardblt bardbl.
interestingly the sanitized record1matches the original record however since the attacker sees only the sanitized data it is not possible for the attacker to know with certainty that the sanitized record matches some original record.
.
privacy metrics we first calculate the probability distribution of guessing anonymity over all the database records.
specific privacy metrics are then defined as a function of that distribution.
in this paper we define three of them but the optimal metric may vary based on the task at hand.
a privacy metric measures how identifiable records in the sanitized table are with respect to the original table .
the privacy metrics that we propose in this paper are all motivated by the notion of guessing anonymity.
we calculate guessing anonymity using the algorithm shown in algorithm .
we begin by creating a similarity matrix that shows the similarity between sanitized and original records.
for each record roin the original table the similarity of roto a record rsin the sanitized table is 206table a table with original and anonymized superscript data.
for example the original value of the attribute age is and the sanitized value is .
record age gender race rec 3040fmwb rec 4040mmbh rec 4530mfhw rec 3040fmwh table a similarity matrix for the table shown in table .
each cell contains the value that shows the fraction of attributes values that are the same between original and sanitized records.
anonymizedoriginalrec rec rec rec rec .
rec .
.
rec rec .
.
measured by the fraction of attributes whose values are the same between roandrs.
this computation is performed in lines of the privacy metric algorithm.
this similarity matrix bardbld bardbl shows similarities between rows in the original matrix bardblqi bardbl and the anonymized matrix bardblt bardbl.
table shows an illustrative example for computing the similarity matrix bardbld bardblfor the data in table .
the similarity matrix bardbld bardblhas dimensions d d where dis the number of records in the matrices bardblqi bardbland bardblt bardbl.
rows correspond to anonymized records and columns correspond to the original records in bardbld bardbl.
the values of the attributes age gender andrace are30 f andwfor the original record respectively and the values for the sanitized record1are40 m andh respectively.
therefore the similarity score is zero for the original and sanitized record .
in contrast the similarity score is one for original record 2and sanitized record since all attribute values of the original record match the values of their corresponding attributes for the sanitized record .
we use the similarity matrix bardbld bardblto compute how difficult it is for attackers to guess original records given sanitized records.
consider an extreme case where bardbld bardblis a diagonal matrix i.e.
all entries outside the main diagonal are zero.
in this case each record is similar to itself only that is all records are unique and easily identifiable by attackers.
the privacy level for this sanitized table is zero.
the other extreme case is when all sanitized records are similar to all other records.
in this case it is very difficult for attackers to guess original data since all sanitized records are highly similar to one another.
correspondingly the privacy level for this sanitized table is close to one.
in practice the privacy level is between zero and one and our goal is to help analysts find the right balance between a privacy level and the utility of the sanitized database.
the distribution of guessing anonymity of the different records in a database can be used to define a variety of privacy metrics.
in this paper we propose three different privacy metrics that are derived from the guessing anonymity distribution but do not focus on providing the bestmetric.
we believe that different applications and risk tolerances of users would require the use of different derived metrics and leave the optimal metric determination as future work.
in this paper we propose three metrics based on guessing anonymity pm1 mean guessing anonymity pm2 fraction of records with guessing anonymity greater than a certain value m where we set m to one in this paper and unique records .
the computation ofthese privacy metrics pm1andpm2is shown in lines of algorithm and their values are returned in line .
pm1 the mean guessing anonymity of a database is the arithmetic mean of the individual guessing anonymities for each record in the database.
this metric gives us measurement of the overall privacy of the sanitized database and helps us compare different versions of sanitized databases.
naturally a database with higher mean guessing anonymity would be more difficult to attack and hence have higher privacy.
suppose that there is a record ro to where tois the table with original records.
after tois sanitized the table tswith sanitized records is obtained rs ts and we compute the similarity matrix bardbld bardbl for records in these tables.
then for each ro to we increase the counter r by one if we find a record in the table tsthat has the similarity score in bardbld bardblequal to or higher than the similarity score between roand its sanitized version rs not counting the similarity score between roandrs.
the formula for the privacy metric is pm1 r d where dis the number of records.
pm2 the fraction of records with guessing anonymity greater than mis our second metric.
we set mto one for the work in this paper such setting corresponds to measuring the fraction of original records that have been modified at all by the anonymization algorithm.
pm2is calculated as the ratio of records that were sanitized and that differ from their original record in at least one attribute value to the total number of records.
this metric is useful for a variety of reasons.
if the fraction of records that have guessing anonymity greater than mis too low and the users are unsatisfied they have the option to increase the level of privacy or remove the records that fall below the threshold from the sanitized database thus making the database more private.
uniquerecords is measured as part of pm2and it is defined as follows.
suppose that there is a record ro to where tois the table with original records.
after tois sanitized the table tswith sanitized records is obtained.
let rs tsbe the sanitized record forro to.rsis a unique record if there exists r o to such that all attributes of r ohave the same value as the attributes of rs.
a key idea of guessing anonymity is that although some records after applying data swapping may match some original records the attacker still cannot know with certainty which ones do and whether sensitive information that these records identify was not changed.
ideally the percentage of unique records in the anonymized database will be zero that is what we would like to achieve.
if the number of unique records is greater than zero and the desired goal is to have complete anonymity the users will have to delete these unique records before releasing the database.
.
experimental ev aluation in this section we describe the results of the experimental evaluation of priest on three open source java programs and one large commercial application that is used to manage logistics of one of the largest supermarket chains in spain.
.
research questions in this paper we make one meta claim our privacy framework for managing the tradeoff between data privacy and test coverage for dcas is better than other frameworks.
we define better in two ways coverage andflexibility .
achieving higher coverage with priest means that for a given level of privacy we can provide higher test coverage.
we measure it using the area under the privacy coverage curve.
flexibility means that we can give more choices to the business analyst to make informed decisions.
if more data points in that curve can be created using priest for 207a given range of privacy we have a better framework for managing the tradeoff.
priest does not compete with other anonymization techniques such as k anonymity since the latter is a metric not a framework.
our claim is that our framework that includes guessing anonymity is a better framework to achieve coverage and flexibility.
we seek to answer the following research questions.
rq1 how much test coverage does priest help achieve at given levels of privacy for subject applications?
rq2 how effective is priest in achieving different levels of data privacy while preserving original data?
rq3 how effective is priest in releasing different versions of anonymized databases for the same level of privacy and test coverage without enabling the attacker to link sensitive data?
with rq1 we address our claim that we designed and implemented a technique using program analysis for determining how values of database attributes affect test coverage of dcas that use this data.
our goal is to show that with priest the privacy metric is linked directly to test coverage and vice versa in other words guaranteeing a certain level of test coverage should allow stakeholders to calculate bounds of the privacy level.
with rq2 we address our claim that using different levels of privacy enables business analysts to make trade off decisions about privacy and utility.
with rq3 we address our claim that priest enables stakeholders to repeatedly release anonymized data in such a way that both privacy and utility are guaranteed at certain levels.
suppose that a technique of data suppression anonymization is applied to protect data and release the anonymized data with the dca to testers.
after some time programmers produce the next release of this dca that uses different attributes differently in its database.
suppose that the dca does not use the attribute nationality any more and it uses the attribute race instead.
at this point the database should be reanonymized since the original values of the attribute race should be left intact to preserve test coverage.
however in the previous anonymization the values of the attribute nationality are left unprotected.
it means that the testers the attackers know original values of the attribute nationality and now the testers will know the original values of the attribute race .
together these attributes enable the attackers to infer sensitive information from the released database.
.
subject programs we evaluate priest with three open source and one large commercial java programs that belong to different domains.
our selection of subject programs is influenced by several factors sizes of the databases size of the source code presence of unit system and integration tests and the presence of embedded sql queries that these programs use.
we selected four subject programs that come with test cases.
n2a is a program for handling logistics of one of the largest supermarket chains in spain.
n2a has a total of loc.
riskit is an insurance quote program4.durbodax enables customer support centers to manage customer data5.
finally unixusage is a program for obtaining statistics on how users interact with unix systems using their commands6.
as of march .
as of march .
as of march .dca app test db tbl att ic etc n2a .
.
durbodax .
.
unixusage .
.
riskit .
.
table characteristics of the subject dcas.
app application code test test cases db database tbl tables att attributes in all tables ic initial test coverage with the original database etc estimated worst test coverage with sanitized data using the approach described in section .
table contains characteristics of the subject programs their databases and test cases.
the first column shows the names of the subject programs followed by the number of lines of code loc for the program code and accompanying test cases.
the source code of the project ranges from .
to .
kloc.
the test cases range from .
to .
kloc.
other columns show the size of the database number of tables and attributes in the database test coverage statement coverage that is obtained with the original database and the estimated worst test coverage statement coverage with sanitized data using our approach described in section .
.
methodology to evaluate priest we carry out experiments to explore its effectiveness in enabling users to determine how to balance test coverage while achieving different levels of data privacy rq1 and rq2 and to show that it is possible to apply priest to get privacy guarantees while releasing different sanitized versions of the database for the same privacy levels thereby supporting software evolution rq3 .
.
.
variables the main independent variable is the value of p. two main response variables are the time that it takes to anonymize data in the database to achieve the desired level of kto answer rq2 and the test coverage in percentage of program statements to answer rq1.
.
.
the structure of the experiments for the experiments we select as qis all attributes whose values affect execution paths in the corresponding dcas.
it is physically not possible to carry out an experiment using all subsets of the powerset of attributes as qis where we measure test coverage for subject dcas while achieving anonymity since it would require us to consider the powerset of all attributes.
given that databases of the subject dcas contain between and attributes it is challenging to select a subset of them as qis to enable achieve the higher possible level of test coverage.
our goal is to run experiments for different values of the independent variable pand report the effect of varying the values on p on dependent variables.
to address rq3 we anonymize databases for the subject dcas for a given level of p and we report and analyze privacy metrics between different sanitized versions of the same original database.
.
threats to validity a threat to the validity of this experimental evaluation is that our subject programs are of small to moderate size because it is difficult to find a large number of open source programs that use nontrivial databases.
large dcas that have millions of lines of code and use databases whose sizes are measured in thousands of 208figure experimental results for the subject dcas.
all graphs show the dependence on the probability of replacing original data value that is assigned to the horizontal axis.
the vertical axis designates dependent variables that are given in the captions to these figures.
tables and attributes may have different characteristics compared to our small to medium size subject programs.
increasing the size of applications to millions of lines of code may lead to a nonlinear increase in the analysis time and space demand for priest.
future work could focus on making priest more scalable.
.
results the results of the experiments conducted to address rq1 and rq2 are shown in figure and table .
dependency of test coverage on the probability pof replacing original values is shown in figure a .
the maximum reduction in test coverage is close to from the initial test coverage with p for application durbodax.
in our previous work we showed that the maximum reduction in test coverage for the same applications reaches from the initial test coverage when using a popular algorithm datafly that is based on data suppression and generalization techniques .
we observed the biggest drop in test coverage with datafly when k while much smaller maximum drop is observed with priest for p which is the maximum value for the anonymization parameter.
the least drop in test coverage is observed for the application n2a while the biggest drop is attributed to the application durbodax.
our explanation is that n2a is least sensitive to values of the database attributes since it uses these values to compute results rather than in path conditions.
we explain the sharper drop for durbodax as a result of branch conditions that use conjunctions of multiple qis thereby making the application sensitive to joint distributions of values of different attributes that may be destroyed by anonymization.
we also observe that the shape of the test coverage curves show gradual decline in test coverage rather than abrupt changes.
the former makes it easier for stakeholders to balance privacy and coverage on a continuous scale.
dependencies of privacy metrics pm1andpm2on the probability pof replacing original values are shown in figure b and figure c .
combined with the test coverage curve shown in figure a these dependencies enable business analysts to select a level of protection that also ensures a certain level of test coverage.
while values of pm1are monotonically increasing for n2a riskit and durbodax with the increase in p the values of pm1 for unixusage slightly drop when the values of pare increasing from .
to .
.
this result can be explained as an effect of the variations of random value replacement that may result in small changes in the numbers of similar records.
the values for pm1 are relatively small for n2a since many of the selected qis were involved in some primary or unique key.
as a result the sanitizedtable results for privacy metrics to compare two sanitized databases for p .6for subject dcas subject pm1pm2 total unique ur dca records records n2a .
.
.
durbodax .
.
.
unixusage .
.
.
riskit .
.
.
records contain distinct values for these qis and fewer records in the original database are likely to be similar to a sanitized record.
opposite to the dependencies of privacy metrics pm1andpm2 the graph of the dependency of the percentage of unique records urshows corresponding decline in figure d .
it means that the percentage of unique records between the original and sanitized databases is monotonically declining as the values of the probabilitypare increasing thereby making it more difficult for attackers to use these unique records to guess the original data.
while values of urare monotonically decreasing for unixusage riskit and durbodax with the increase in p the values of urfor n2a slightly increase when the values of pare increasing from .
to .
.
this result can be explained as an effect of the variations of random value replacement that may result in small changes in the numbers of unique records.
to address rq3 we generated two sanitized databases for p .
and computed the privacy metrics pm1 pm2 and number of unique records between the two databases.
these metrics quantify the difficulty of an attacker to relate records from the two sanitized databases.
since the same database is anonymized independently and the probabilities of replacing cell values are independent from one another then resulting databases cannot be deterministically correlated since they are not used to produce each other.
thus the attacker has to guess original data independently meaning that the lowest guessing anonymity score can be used to quantify the difficulty of an attacker to guess records from both sanitized databases.
the values for pm1 are slightly lower to the values of pm1 for p .
in table while the values of pm2 are slightly higher to the values of pm1 for p .
in table .
this phenomenon is expected since the variation between the records of the two anonymized databases is expected to be more as compared to variation between the records of original and an anonymized database.
the number of unique records is similar to the number of unique records for p .
in table .
it is also important to note that the overall privacy of multiple sanitized databases is only as good as the privacy of the least private version.
if the first released version has pm2 and the second version has pm2 pm2for 209subject qis total initial worst max dependent the probability of value replacement p records cov exp cov err cov variable .
.
.
.
.
.
n2a .
pm1 .
.
.
.
.
.
.
pm2 .
.
.
.
.
.
.
tc .
.
.
.
.
.
.
ur .
.
.
.
.
.
.
durbodax .
pm1 .
.
.
.
.
.
.
pm2 .
.
.
.
.
.
.
tc .
.
.
.
.
.
.
ur .
.
.
.
.
.
.
unixusage .
pm1 .
.
.
.
.
.
.
pm2 .
.
.
.
.
.
.
tc .
.
.
.
.
.
.
ur .
.
.
.
.
.
.
riskit .
pm1 .
.
.
.
.
.
.
pm2 .
.
.
.
.
.
.
tc .
.
.
.
.
.
.
ur .
.
.
.
.
.
.
table results of experiments with subject dcas for different values of the independent variable pthat is shown in the last column header that spans seven subcolumns.
the second column qi shows the number of qi whose values affect control flow decisions in dcas the next two columns show the initial test coverage statement coverage with the original database and the estimated worst test coverage with sanitized data using the approach described in section .
the next column shows the error in the estimated test coverage that varies from .
to .
.
finally the next column lists four dependent variables privacy metrics pm 1and pm test coverage tc and the percentage of unique records ur.
finally the last seven subcolumns show values of these dependent variables for different values of p. the two databases combined will be .
in general for two released databases dianddj the overall pm2 min pm2 di pm2 dj .
result summary.
these results strongly suggest that priest helps achieve higher test coverage for given levels of privacy for subject applications when compared with datafly that is based on data suppression and generalization techniques thereby addressing rq1.
priest can also achieve different levels of data privacy while preserving original data as it is seen from table thereby addressing rq2.
finally the results shown in table strongly suggest that priest is effective in releasing different versions of anonymized databases for the same level of privacy and test coverage thereby addressing rq3.
.
related work our work is related to regression testing since priest is used to assess the impact of data anonymization on testing.
numerous techniques have been proposed to automate regression testing.
these techniques usually rely on information obtained from the modifications made to the source code.
these techniques are not directly applicable to preserving test coverage while achieving data anonymity for test outsourcing since regression information is derived from changes made to the source code and not to how this code uses databases.
closely related to priest is kb anonymity model that enables stakeholders to release private data for testing and debugging by combining the k anonymity with the concept of program behavior preservation .
unlike priest kb anonymity replaces some information in the original data to ensure privacy preservation so that the replaced data can be released to third party developers.
priest and kb anonymity are complementary in using different privacy mechanisms to preserve original data thereby improving its testing utility.
recently proposed is an anonymization technique for protecting private information in bug reports that are delivered to vendors when programs crash on computers of customers and the follow up work on this technique by clause and orso .
thistechnique provides software vendors with new input values that satisfy the conditions required to make the software follow the same execution path until it fails but are otherwise unrelated with the original inputs.
this technique uses symbolic execution to create new inputs that allow vendors to reproduce the bug while revealing less private information than existing techniques.
the technique requires test cases which are not present in our situation.
in contrast priest does not require any test case.
there has been a lot of recent work to achieve general purpose task independent data anonymization.
we choose the guessing anonymity approach in this paper because guessing anonymity can be used to provide privacy guarantees for data swapping algorithms and can also provide an optimal noise parameter when implementing data swapping algorithms for anonymization.
in contrast approaches that aim to achieve k anonymity do not allow the user to explicitly control how much each record is altered.
empirical results reported by rachlin et al.
show that guessing anonymity outperforms datafly a well known k anonymity algorithm on specific data mining tasks namely classification and regression while at the same time providing a higher degree of control over how much the data is distorted.
recent work on privacy introduced a similar definition of privacy for noise perturbation methods known as k randomization .
this work defines a record as k randomized if the number of records that are a more likely match to the original is at least k. although this notion is similar to the definition of guessing anonymity the definition differs by not providing a lower limit on the number of records that provide a more likely match and by explicitly establishing a connection between privacy and guessing functions.
.
conclusion we offer a novel and effective approach called priest that helps organizations to remove an obstacle to effective dca test outsourcing.
with priest dcas can be released to external testing organizations without disclosing sensitive information while retaining 210testing efficacy.
we built a tool and applied it to nontrivial dcas.
the results show that priest is effective in enabling users to determine how to balance test coverage while achieving different levels of data privacy and that with priest users can release different sanitized versions of the database for the same privacy levels thereby supporting software evolution.