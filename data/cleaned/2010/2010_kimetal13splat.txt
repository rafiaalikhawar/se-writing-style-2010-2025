splat lightweight dynamic analysis for reducing combinatorics in testing configurable systems chang hwan peter kim university of texas austin tx usadarko marinov1 1university of illinois and2groupon inc. usasarfraz khurshid don batory university of texas austin tx usa sabrina souto paulo barros marcelo d amorim federal university of pernambuco recife pe brazil abstract many programs can be con gured through dynamic and or static selection of con guration variables.
a software product line spl for example speci es a family of programs where each program is de ned by a unique combination of features.
systematically testing spl programs is expensive as it can require running each test against a combinatorial number of con gurations.
fortunately a test is often independent of many con guration variables and need not be run against every combination.
con gurations that are not required for a test can be pruned from execution.
this paper presents splat a new way to dynamically prune irrelevant con gurations the con gurations to run for a test can be determined during test execution by monitoring accesses to con guration variables.
splat achieves an optimal reduction in the number of con gurations and is lightweight compared to prior work that used static analysis and heavyweight dynamic execution.
experimental results on spls written in java show that splat substantially reduces the total test execution time in many cases.
moreover we demonstrate the scalability of splat by applying it to a large industrial code base written in ruby on rails.
categories and subject descriptors d. .
testing and debugging general terms languages veri cation keywords software product lines con gurable systems software testing e ciency .
introduction many programs can be con gured through selection of con guration variables.
a software product line spl for permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
esec fse august saint petersburg russia copyright acm ... .
.example speci es a family of programs where each program is de ned by a unique combination of features increments in program functionality .
many codebases that power modern websites are also highly con gurable.
for instance the code behind the groupon.com website has over boolean conguration variables and can in theory be deployed in over 2170di erent con gurations.
systematically testing con gurable systems and spls is challenging because running each test can in principle require many actual executions one execution for each possible con guration or feature combination.1thus one test does not simply encode one execution of a program the cost of running a test suite is proportional to the number of tests times the number of con gurations.
current techniques for handling this combinatorial problem can be divided into sampling and exhaustive exploration.
sampling uses a random or sophisticated selection of con gurations egpair wise coverage .
however such selections can run a test on several con gurations for which the test executions are provably equivalent thus only increasing the test time without increasing the chance to nd bugs or it can fail to examine con gurations that can expose bugs .
exhaustive exploration techniques consider all con gurations but use optimization techniques to prune redundant con gurations that need not be explored.
.
such works use either static analysis or heavyweight dynamic analysis based on model checking .
this paper presents splat a new lightweight technique to reduce the cost of systematic testing of spls and highly con gurable systems.
because a test typically exercises a subset of the code base it is likely that some of the features will never even be encountered during the test execution no matter how the test is executed.
combinations of such unreachable features yields many runs of a test that have the same trace or sequence of bytecode instructions executed by the test.
splat determines for each test the set of unique traces and hence the smallest set of con gurations to run.
speci cally let p1 pk fork be the con guration variables for a program.
each pitakes a value from a nite domain di in the case of spls each variable takes a boolean value that represents whether the feature is selected or not.
let candc0be two di erent con gurations to run on a testt and let and 0be their traces.
in both runs t 1for ease of exposition we use the terms con guration feature combination and program interchangeably.notepad toolbar base wordcount menubar menubar toolbar figure feature model xes the input values for non con guration variables.
congurationc0is unnecessary to execute if chas been executed and .
the set of all con gurations with unique traces forms a revealing subdomain fort .
our insight is that the con gurations to run against a test can be determined during test execution rather than using an up front static analysis.
splat achieves an optimal reduction in the number of con gurations iefor any test splat runs only con gurations that have a unique trace.
experimental results show that splat yields a reduction in testing time that is proportional to the reduction in the number of con gurations.
our insight into pruning con gurations was inspired by the korat algorithm for test input generation which introduced the idea of execution driven pruning for solving data structure invariants written as imperative code.
splat supports constraints among con guration variables which de nes the valid con gurations.
for a spl these constraints are expressed through a feature model that provides a hierarchical arrangement of features and de nes allowed con gurations.
splat uses sat to prune invalid con gurations and in tandem uses execution driven pruning to further remove the valid con gurations that are unnecessary for execution of each test.
splat is e ective because it monitors the accesses of conguration variables during test execution.
monitoring is lightweight both in terms of its execution overhead and in terms of its implementation e ort.
we developed two implementations one for java and one for ruby on rails.
the java implementation of splat leveraged the publicly available korat code .
the ruby on rails implementation of splat was developed from scratch and took only two days to implement while being robust enough to run against a large industrial codebase at groupon inc. this paper makes the following contributions lightweight analysis of con gurable programs we introduce the idea of lightweight monitoring for highly con gurable systems to speed up test execution.
splat instantiates this idea and can be easily implemented in di erent run time environments.
implementation we describe two implementations ofsplat one for java and one for ruby on rails.
evaluation we evaluate splat on java spls.
experimental results show that splat e ectively identi es relevant con gurations with a low overhead.
we also apply splat on a large con gurable system with over 171kloc in ruby on rails .
the system uses over con guration variables and contains over 19k tests with over 231kloc in ruby on rails .
to our class notepad extends jframe notepad getcontentpane .add newjtextarea void createtoolbar if toolbar jtoolbar toolbar new jtoolbar getcontentpane .add north toolbar if wordcount jbutton button new jbutton wordcount.gif toolbar.add button void createmenubar if menubar jmenubar menubar new jmenubar setjmenubar menubar if wordcount jmenu menu new jmenu word count menubar.add menu a code public void test notepad n new notepad n.createtoolbar automated gui testing framefixture f newfixture n f.show string text hello f.textbox .entertext text f.textbox .requiretext text f.cleanup b test figure notepad spl and example test knowledge this is the largest and most complex industrial codebase used in research on testing spls and splat scales to this size without much engineering e ort.
.
motiv ating example to illustrate the testing process we use a simple notepad product line and introduce spl concepts as needed.
figure shows the feature model of notepad.
this model has two mandatory features the root notepad andbase non root mandatory features are indicated by lled in dots and three optional features menubar toolbar andwordcount indicated by un lled dots .
a mandatory feature is always true it is present in every con guration.an optional feature may be set to true orfalse .
acon guration is an assignment of values to all feature variables.
a con guration is valid i it satis es all constraints the feature model expresses.
a feature model can also include additional propositional logic constraints.
in this example every notepad con guration must have a menubar or toolbar .
for example assigning false to both toolbar andmenubar would violate the disjunction constraint and therefore be invalid.
in contrast assigning false to one of these two features and true to the other feature is valid.
for the spls we consider in this paper a feature is a boolean variable within the code.
figure a shows the code for notepad.
base clear color represents the core functionality which in this case corresponds to constructing a notepad with a jtextarea that the user types into.
toolbar green color adds a jtoolbar to the frame.
menubar red color sets a jmenubar against the frame.
wordcount blue color adds its toolbar icon if the toolbar is present or its menubar item if the menubar is present.
figure b shows an example test that instantiates the notepad class and creates a toolbar for it.
note that test does notcall the createmenubar method.
to be able to execute a test each variable in the test except the feature variables must be given a value.
we use the automated gui testing framework fest to run the test.
the helper method newfixture is not shown for simplicity.
the test execution launches the frame simulates a user entering some text into the jtextarea of the frame checks that the text area contains exactly what was entered and closes the frame.
without analyzing the feature model or the code this test would need to be run on all combinations of the optional features to check all potential test outcomes.
however some con gurations need not be run.
analyzing the feature model we note that two con gurations are invalid mtw andmtw where m t andw stand for menubar toolbar and wordcount respectively.
hence no more than con gurations need to be run.
splat further reduces that number by dynamically analyzing the code that the test executes.
for example executing the test against the con guration c mtw executes the same trace as con guration c0 mtw .
the reason is that the test only calls createtoolbar which is empty in both con gurations candc0since toolbar is false in both con gurations.
although the code increatemenubar is di erent in candc0 the test never executes it.
therefore having executed c execution of c0is unnecessary.
we will show in section .
that splat runs this test for only three con gurations egmtw mtw mtw .
.
technique given a test for a con gurable system splat determines all relevant con gurations on which the test should be run.
each con guration run executes a unique trace of the test.
splat executes the test on one con guration observes the values of con guration variables and uses these values to determine which con gurations can be safely pruned.
splat repeats this process until it explores all relevant con gurations or until it reaches a speci ed bound on the number of con gurations.
we rst describe the feature model interface and then the core algorithm.class featurevar ... class varassign ... map featurevar boolean map ... interface featuremodel set assign getvalid assign a boolean issatisfiable assign a boolean ismandatory featurevar v boolean getmandatoryvalue featurevar v figure feature model interface .
feature model interface figure shows the code snippet that de nes the featuremodel interface.
the type featurevar denotes a feature variable.
a varassign object encodes an assignment of boolean values to feature variables.
an assignment can be complete assigning values to all the features or partial assigning values to a subset of the features.
a complete assignment is valid if it satis es the constraints of the feature model.
a partial assignment is satis able if it can be extended to a valid complete assignment.
the featuremodel interface provides queries for determining the validity of feature assignments obtaining valid con gurations and checking if particular informed features are mandatory.
given an assignment the method getvalid returns the set of all complete assignments that agree with on the values of feature variables in and assign the values of the remaining feature variables to make the complete assignment valid.
if the set is not empty for we say that issatis able the method issatisfiable checks this.
the method ismandatory checks if a feature is mandatory according to the feature model and the method getmandatoryvalue returns the mandatory value for the informed feature.
we build on a sat solver sat4j to implement these feature model operations.
.
main algorithm figure lists the splat algorithm.
it takes as input a testtfor a con gurable system and a feature model fm.
to enable exploration the algorithm maintains a state that stores the values of feature variables line and a stack of feature variables that are read during the latest test execution line .
splat performs a mostly stateless exploration of paths it does not store restore or compare program states as done in stateful model checking instead splat stores only the feature decisions made along one path and re executes the code to explore di erent program paths which corresponds to valid and dynamically reachable con gurations.
to that end splat needs to be able to set the values of feature variables to observe the accesses to feature variables during a test run and to reexecute the test from the beginning.
the algorithm rst initializes the values of feature variables lines using the feature model interface.
mandatory features are set to the only value they can have and optional features are initially set to false .
note that initial assignment may be invalid for the given feature model.
for example initially setting feature variables to false would violate the constraint in our notepad example.
we describe later how splat enforces satis ability during execution in line .
it adjusts the assignment of values to feature vari ables before test execution gets to exercise code based on an invalid con guration.
such scenario could potentially lead to a false alarm test failure as opposed to revealing an actual bug in the code under test.
note that the calls tostate.put both in the initialization block and elsewhere not only map a feature variable to a boolean value in the state maintained by splat but also set the value of the feature variable referred to by the code under test.
splat then instruments line the code under test to observe feature variable reads.
conceptually for each read of an optional feature variable egreading variable toolbar in the code if toolbar from figure splat replaces the read with a call to the notifyfeatureread method shown in figure .
the reads are statically instrumented so that they can be intercepted just before they happen during test execution.
mandatory feature variable reads need not be instrumented because the accessed values remain constant for all con gurations.
splat next runs the test line .
the test execution calls the method notifyfeatureread whenever it is about to read a feature variable.
when that happens splat pushes the feature variable being read on the stack if it is not already there e ectively recording the order of the rst reads of variables.
this stack enables backtracking over the values of read feature variables.
an important step occurs during the call to notifyfeatureread line .
the initial value assigned to the reached feature variable may make the con guration unsatis able.
more precisely at the beginning of the exploration splat sets an optional feature value to false .
when the code is about to read the optional feature splat checks whether the false value is consistent with the feature model iewhether the partial assignment of values to feature variables on the stack is satis able for the given feature model.
if it is splat leaves the feature as is.
if not splat changes the feature to true .
note that updating a feature variable to true guarantees that the new partial assignment is satis able.
the update occurs before execution could have observed the old value which would make the assignment unsatis able.
the reason why this change of value keeps the assignment satis able follows from the overall correctness of the splat algorithm it explores only satis able partial assignments line and it checks if the assignment is satis able in every variable read line thus if a partial assignment was satis able considering all features on the stack then it must be possible to extend that assignment with at least one value for the new feature that was not on the stack but is being added.
if the variable associated with the new feature stores false at the moment execution accesses that variable and if the partial assignment including that feature variable is notsatis able then we can change the value to true line .
recall that optional feature variables are initialized to false .
after nishing one test execution for one speci c con guration splat e ectively covers a set of con gurations.
this set can be determined by enumerating every complete assignment that has the same values as the partial assignment speci ed by variables state andstack lines and is valid according to the feature model line .
splat then determines the next con guration to execute by backtracking on the stack lines .
if the last read feature has value true then splat has explored both values of that feature and it is popped o the stack lines .
if the last read feature has value false then splat1stack featurevar stack 2map featurevar boolean state input shared with instrumented code 5featuremodel fm 7void splat test t initialize features state new map for featurevar f fm.getfeaturevariables state.put f fm.ismandatory f ?
fm.getmandatoryvalue f false instrument the code under test instrumentoptionalfeatureaccesses do repeatedly run the test stack new stack t.runinstrumentedtest varassign pa getpartialassignment state stack print configs covered print fm.getvalid pa while !stack.isempty featurevar f stack.top if state.get f state.put f false restore stack.pop else state.put f true pa getpartialassignment state stack if fm.issatisfiable pa break while !stack.isempty called back from test execution 45void notifyfeatureread featurevar f if !stack.contains f stack.push f varassign pa getpartialassignment state stack if !fm.issatisfiable pa state.put f true figure splat algorithm has explored only the false value and the feature should be set to true lines .
another important step occurs now line .
while the backtracking over the stack found a partial assignment to explore it can be the case that this assignment is not satis able for the feature model.
in that case splat keeps searching for the next satis able assignment to run.
if no such assignment is found the stack becomes empty and splat terminates.
.
example run we demonstrate splat on the example from figure .
according to the feature model figure notepad andbase are the only mandatory features and are set to true .
the other three feature variables are optional and therefore splat instruments their reads figure a lines and .
conceptually the exploration starts from the congurationmtw .
when the test begins execution notifyfeatureread is rst called when toolbar is read.
toolbar is pushed on the stack and because its assignment to false is satis able for the feature model its value remains unchanged ie stays false as initialized .
had the feature model required toolbar to be true the feature s value would have been set to true at this point.
with toolbar set to false no other feature variables are read before the test execution nishes.
in particular wordcount on line is not read because that line is not executed when toolbar isfalse .
therefore this one execution covers con gurations mtw where denotes a don t care value.
however con gurations mtw are invalid for the given feature model so this one execution covers two valid con gurations where toolbar is false andmenubar is true mtw .
note that even though the value of wordcount does not matter here it is given a value nonetheless for an execution because in an execution each variable must have a concrete value.
so let us say mtw here.
splat next re executes the test with toolbar set to true as it is satis able for the feature model.
wordcount is encountered this time but it can remain false and the execution completes covering mtw again for an execution all variables need to be set so let us say that mtw is what actually executes .
splat then sets wordcount totrue and the execution completes covering mtw let us say mtw was used .
splat nally pops o wordcount from the stack because both its values have been explored and pops o toolbar for the same reason so the exploration nishes because the stack is empty.
in summary the test s rst execution covers mtw mtw is executed second execution covers mtw mtw is executed and third execution covers mtw mtw is executed .
therefore the technique covers all valid con gurations by executing just three congurations.
.
reset function while a stateless exploration technique such as splat does not need to store and restore program state in the middle of execution like a stateful exploration technique does the stateless exploration does need to be able to restart a new execution from the initial program state una ected by the previous execution.
restarting an execution with a new runtime egspawning a new java virtual machine jvm in java is the simplest solution but it can be both inefcient and unsound.
it is ine cient because even without restarting the runtime the di erent executions may be able to share a runtime and still have identical initial program states egif the test does not update any static variables in the jvm state.
it can be unsound because a new runtime may not reset the program state changes made by the previous executions egprevious executions having sent messages to other computers or having performed i o operations such as database updates .
we address these issues by sharing the runtime between executions and requiring the user toprovide a reset function that can be called at the beginning of the test.
our sharing of the runtime between executions means that settings that would normally be reset automatically by creating a new runtime must now be manually reset.
for example java static initializers must now be called from the reset function because classes are loaded only once.
however we believe that the bene t of saving time by reusing the runtime outweighs the cost of this e ort which could be alleviated by a program analysis tool.
moreover for the groupon code used in our evaluation the testing infrastructure was already using the reset function developed independently and years before this research between any test execution the state of both memory and database is reset by rolling back the database transaction from the previous test and overwriting the state changes in the teardown and or setup blocks after before each test .
.
potential optimization the algorithm in figure is not optimized in how it interfaces with the feature model.
the feature model is treated as a blackbox read only artifact that is oblivious to the exploration state consisting of the state andstack .
consequently the issatisfiable andgetvalid methods are executed as if the exploration state was completely new every time even if it just incrementally di ers from the previous exploration state.
for example when running the test from figure splat asks the feature model if mtw is satis able line of the splat algorithm after the assignment mtw .
the feature model replies true as it can nd a con guration with the feature toolbar set to true.
then when wordcount is encountered while toolbar true splat asks the feature model if the assignment mtw toolbar true and wordcount false is satis able line of the splat algorithm .
note that the feature model is not aware of the similarity between the consecutive calls for mtw andmtw .
but if it were it would only have to check the satis ability of wordcount false .
the change to the algorithm to enable this synchronization between the exploration state and the feature model is simple every time a feature variable is pushed on the stack constrain the feature model with the feature s value and every time a feature variable is popped o the stack remove the corresponding feature assignment from the feature model.
a feature model that can be updated implies that it should support incremental solving iea feature model should not have to always be solved in its entirety.
our current splat tool for java does not exploit incremental solving meaning that the tool has not reached the limits of the underlying technique and can be made even faster.
.
implementation we implemented two versions of splat one for java and one for ruby on rails.
we selected these two languages motivated by the subject programs used in our experiments section .
for java we implemented splat on top of the publicly available korat solver for imperative predicates .
korat already provides code instrumentation based on the bcel library for java bytecode manipulation to monitor eld accesses and provides basic backtracking over the accessed elds.
the feature variables in our java subjects were al ready represented as elds.
the main extension for splat was to integrate korat with a sat solver for checking satis ability of partial assignments with respect to feature models.
as mentioned earlier we used sat4j .
for ruby on rails we have an even simpler implementation that only monitors accesses to feature variables.
we did not integrate a sat solver because the subject code did not have a formal feature model and thus we treated all combinations of feature variables as valid.
.
ev aluation our evaluation addresses the following research questions rq1 how does splat s e ciency compare with alternative techniques for analyzing spl tests?
rq2 what is the overhead of splat ?
rq3 does splat scale to real code?
in section .
we compare splat with related techniques using spls.
in section .
we report on the evaluation of splat using an industrial con gurable system implemented in ruby on rails.
.
software product lines we evaluate our approach with spls listed in table .
note that most of these subjects are con gurable programs that have been converted into spls.
a brief description for each is below 101companies is a human resource management system.
features include various forms to calculate salary and to give access to the users.
email is an email application.
features include message encryption automatic forwarding and use of message signatures.
elevator is an application to control an elevator.
features include prevention of the elevator from moving when it is empty and a priority service to the executive oor.
gpl is a product line of graph algorithms that can be applied to a graph.
jtopas is a text tokenizer.
features include support for particular languages such as java and the ability to encode additional information in a token.
minepump simulates an application to control water pumps used in a mining operation.
features include sensors for detecting varying levels of water.
notepad is a gui application based on java swing that provides di erent combinations of end user features such as windows for saving opening printing les menu and tool bars etc.
it was developed for a graduate level course on software product lines.
prevayler is a library for object persistence.
features include the ability to take snapshots of data to compress data and to replicate stored data.
sudoku is a traditional puzzle game.
features include a logical solver and a con guration generator.
2all subjects except 101companies have been used in previous studies on testing analyzing spls including gpl by elevator email minepump by jtopas by notepad by xstream by prevayler by and sudoku by .table subject spls spl features confs loc 101companies elevator email gpl jtopas minepump notepad .
prevayler sudoku xstream xstream is a library for de serializing objects to xml and from it .
features include the ability to omit selected elds and to produce concise xml.
table shows the number of optional features we do not count the mandatory features because they have constant values the number of valid con gurations and the code size for each subject spl.
more details of the subjects and results are available at our website .
.
.
tests we prepared three di erent tests for each subject spl.
the rst test referred as low represents an optimistic scenario where the test needs to be run only on a small number of con gurations.
the second test referred as med for medium represents the average scenario where the test needs to be run on some con gurations.
the third test referred as high represents a pessimistic scenario where the test needs to be run on most con gurations.
to prepare the low med and high tests we modi ed existing tests when available or wrote new tests because we could not easily nd tests that could be used without modi cation.
because some subjects were too simple tests would nish too quickly for meaningful time measurement if test code only had one sequence of method calls.
therefore we used loops to increase running times when necessary.
each test xes all inputs except the feature variables.
the tool test suites and subjects are available on the project website .
.
.
comparable techniques we compared splat with di erent approaches for test execution.
we considered two na ve approaches that run tests against allvalid con gurations newjvm and reusejvm.
the newjvm approach spawns a new jvm for each distinct test run.
each test run executes only onevalid conguration of the spl.
it is important to note that the cost of this approach includes the cost of spawning a new jvm.
the reusejvm approach uses the same jvm across several test runs thus avoiding the overhead of repeatedly spawning jvms for each di erent test and con guration.
this approach requires the tester to explicitly provide a reset function section .
.
because the tester likely has to write a reset function anyway we conjecture that this approach is a viable alternative to save runtime cost.
for example the tester may already need to restore parts of the state stored outside the jvm such as les or database.
we also compared splat with a simpli ed version of a previously proposed static analysis for pruning congurations.
whereas performs reachability analysis controlow and dataow analyses the simpli ed version which we call sra static reachability analysis only performs the reachability analysis to determine which con gurations are reachable from a given test and thus can be seen as the static analysis counterpart to splat .
sra builds a call graph using inter procedural context insensitive owinsensitive and path insensitive points to analysis and collects the features syntactically present in the methods of the call graph.
only the valid combinations of these reachable features from a test need to be run for that test.
finally we compared splat with an arti cial technique that has zero cost to compute the set of con gurations on which each test need to run.
more precisely we use a technique that gives the same results as splat but only counts the cost of executing tests for these con gurations not the cost of computing these con gurations.
we call this technique ideal .
the overhead of splat is the di erence between the overall cost of splat explorations and the cost of executing tests for ideal.
.
.
results table shows our results.
we performed the experiments on a machine with x86 architecture ubuntu operating system mips cores with each core having an intel xeon cpu e3 v2 at .50ghz processor and gb memory.
all times are listed in seconds.
our feature model implementation solves the feature model upfront to obtain all valid con gurations because this solving needs to be done for every feature model regardless of using splat or otherwise and because it takes a fraction of test execution time we do not include it.
here is a description of each column in table test refers to one of the three categories of tests described earlier.
all valid identi es the techniques that run the test against all valid con gurations namely newjvm andreusejvm .reusejvm shows time absolutely and as a percentage of newjvm duration.
columns under splat details information for splat confs shows the number of con gurations that splat runs for a particular test.
splattime shows the time it takes to run a test using splat .splat reuses the same jvm for di erent executions like reusejvm.
the time is shown absolutely and as a percentage of reusejvm notnewjvm .
idealtime shows the time in seconds for running splat without considering the cost to determine which con gurations to run for the test therefore this number excludes instrumentation monitoring and constraint solving.
overhead shows the overhead of splat calculated by subtracting idealtime from splattime and dividing it by idealtime .
columns under static reachability sra show results for our static analysis confs shows the number of con gurations reachable with such analysis overhead shows the time taken to perform the static reachability analysis and time shows the time taken to run the con gurations determined by this analysis.e ciency.
thereusejvm column shows that reusing jvm saves a considerable amount of time compared to spawning a new jvm for each test run.
for example for half of the tests reusing jvm saves over of the time because running these tests does not take much longer than starting up the jvm.
for tests that take considerably longer than starting up the jvm such saving is not possible.
splat further reduces the execution time over reusejvm by determining the reachable con gurations.
for example for the lowtest for notepad reusing the jvm takes of the time to run without reusing the jvm and with splat it takes just of the already reduced time.
in fact the table shows that in most cases as long as splat can reduce the number of con gurations to test ieconfs is lower than the total number of con gurations it runs faster than running each con guration ieless than of reusejvm .
comparison with static reachability analysis.
the static reachability analysis yields less precise results compared to splat the number of con gurations in the column confs is larger than the number of con gurations in the corresponding column for splat .
in fact for jtopas notepad andminepump the sra reports all features as being accessed from the call graph and therefore reports that all valid con gurations have to be tested.
for example for jtopas this is due to its tests invoking the main method of the spl from which all feature variable accesses may be reached using di erent input values which the analysis is insensitive to.
for notepad this is due to the the use of the fest automated gui testing framework which relies heavily on re ection.
because the method being invoked through re ection cannot necessarily be determined statically the analysis yields a conservative result.
for minepump each test happens to exercise a sequence of methods that together reach all feature variable accesses.
note that the sra approach rst statically determines the con gurations to run which takes the time in column sra overhead and afterwards dynamically runs them one by one which takes the time in column sra time .
comparing just the static analysis time sra overhead with the splat overhead splat overhead shows that sra has a considerably larger overhead in some cases two orders of magnitude larger.
although the static analysis overhead can be o set by re using the reachable con gurations it determines against tests that have the same code base but have di erent inputs in general it would require a very large number of such tests for the approach to have a smaller overhead than splat .
moreover comparing just the time to execute the con gurations computed by sra columnsra time with the time to execute the con gurations computed by splat column idealtime shows that sra again takes longer because sra computes a higher number of con gurations than splat due to the conservative nature of static analysis.
rq1.
based on the comparison with newjvm reusejvm and sra we conclude the following splat is more e cient than the techniques that run all valid con gurations for tests of spls or prune reachable con gurations using static analysis.
moreover compared with static analysis splat not only gives results faster but also gives more precise results.table experimental results for various techniques all valid splat static reachability sra test newjvm reusejvm confs splattime idealtime overhead confs overhead time 101companies con gs low .
.
.
.
.
.
.
med .
.
.
.
.
.
.
high .
.
.
.
.
.
.
elevator con gs low .
.
.
.
.
.
.
med .
.
.
.
.
.
.
high .
.
.
.
.
.
.
email con gs low .
.
.
.
.
.
.
med .
.
.
.
.
.
.
high .
.
.
.
.
.
.
gpl con gs low .
.
.
.
.
.
.
med .
.
.
.
.
.
.
high .
.
.
.
.
.
.
jtopas con gs low .
.
.
.
.
.
.
med .
.
.
.
.
.
.
high .
.
.
.
.
.
.
minepump con gs low .
.
.
.
.
.
.
med .
.
.
.
.
.
.
high .
.
.
.
.
.
.
notepad con gs low .
.
.
.
.
.
.
med .
.
.
.
.
.
.
high .
.
.
.
.
.
.
prevayler con gs low .
.
.
.
.
.
.
med .
.
.
.
.
.
.
high .
.
.
.
.
.
.
sudoku con gs low .
.
.
.
.
.
.
med .
.
.
.
.
.
.
high .
.
.
.
.
.
.
xstream con gs low .
.
.
.
.
.
.
med .
.
.
.
.
.
.
high .
.
.
.
.
.
.
overhead.
table also shows the overhead that splat has over the ideal technique column splat overhead .
the overhead is generally small except for the low tests and tests for several subjects egjtopas andmine .
the overhead is high for the lowtests because these tests nish quickly under seconds often under second meaning that instrumentation monitoring and feature model interaction take a larger fraction of time than they would for a longer executing test.
the overhead is high for jtopas because the feature variables are accessed many times because they are accessed within the tokenizing loop.
the overhead is high for minepump because feature accesses and their instrumentation take relatively longer to execute for this particular test as the subject is very small.
splat due to its cost in monitoring feature variables should not execute a test faster than knowing the reachable con gurations upfront and running the test only on those con gurations.
thus the occasional small negative overheads for email prevayler sudoku are due to theexperimental noise and or the occasionally observed e ect where an instrumented program runs faster than the noninstrumented program.
it is important to note that e ciency and overhead are orthogonal.
as long as the reduction in time due to the reduction in con gurations is larger than the overhead splat saves the overall time.
to illustrate the gpl slow test incurs over overhead but the reduction in con gurations outweighs the overhead and splat takes only of running all valid con gurations with the same jvm.
rq2.
based on the discussion about overhead we conclude the following splat can have a large relative overhead for shortrunning tests but the overhead is small for long running tests.
.
configurable systems groupon.
groupon is a company that features a daily deal on the best stu to do see eat and buy in countries .groupon pwa is name of the codebase that powers the main groupon.com website.
it has been developed for over .
years with contributions from over engineers.
the server side is written in ruby on rails and has over 171k lines of ruby code.
groupon pwa code is highly con gurable with over boolean feature variables.
in theory there are over di erent con gurations for the code.
in practice only a small number of these con gurations are ever used in production and there is one default con guration for the values of all feature variables.
groupon pwa has an extensive regression testing infrastructure with several frameworks including rspec cucumber selenium and jasmine.
the test code itself has over 231k lines of ruby code and additional code in other languages.
it is not uncommon for the test code to be larger than the code under test .
groupon pwa has over 19k rspec unit and integration tests.
a vast majority of these tests run the code only for the default con guration.
a few tests run the code for a non default con guration typically changing the value for only one feature variable from the default value.
running all the rspec tests on a cluster of computers with cores each takes under minutes.
splat application.
we implemented splat for ruby on rails to apply it to groupon pwa.
we did not have to implement the reset function because it was already implemented by groupon testers to make test execution feasible due to the high cost of re starting the system .
moreover no explicit feature model was present so feature model constraints did not need to be solved.
we set out to evaluate how many con gurations each test could cover if we allow varying the values of all feature variables encountered during the test run.
we expected that the number of con gurations could get extremely high for some tests to be able to enumerate all the con gurations.
therefore we set the limit on the number of con gurations to no more than so that the experiments can nish in a reasonable time.
this limit was reached by tests.
for the remaining tests table shows the breakdown of how many tests reached a given number of con gurations.
we can see that the most common cases are the number of con gurations being powers of two e ectively indicating that many features are encountered independently rather than nested as in figure where the read of wordcount is nested within the block controlled by the read of toolbar .
we also evaluated the number of features encountered.
it ranges from up to .
we found is a high number in the absolute sense indicating that a test may potentially cover 243di erent con gurations is also a relatively low number in the relative sense compared to the total of over features.
table shows the breakdown of how many tests reached a given number of feature variables.
note that the numbers of con gurations and feature variables may seem inconsistent at a glance egthe number of tests that have con guration is larger than the number of tests than have feature variables.
the reason is that some tests force certain values for feature variables such that setting the con guration gets overwritten by the forced value.table reachable con gurations con gs tests con gs tests table accessed features vars tests vars tests vars tests in summary these results show that the existing tests for groupon pwa can already achieve a high coverage of con gurations but running all the con gurations for all the tests can be prohibitively expensive.
we leave it as a future work to explore a good strategy to sample from these con gurations .
rq3.
moreover based on the fact that we could run splat on the codebase as large as groupon pwa we conclude the following splat scales to real large industrial code.
the implementation e ort for splat is relatively low and the number of con gurations covered by many real tests is relatively low.
.
threats to validity the main threat is to external validity we cannot generalize our timing results to all spls and con gurable systems because our case studies may not be representative of all programs and our tests may be covering an unusual number of con gurations.
to reduce this threat we used multiple java spls and one real large industrial codebase.
for spls we designed tests that cover a spectrum of cases from low tomed ium tohigh number of con gurations.
for the groupon codebase we nd that most real tests indeed cover a small number of con gurations.
our study has the usual internal and construct threats to validity.we believe that splat is a helpful technique that can be used in practice to improve spl testing.
an important threat to this conclusion is that our results do not take into account the cost of writing a reset function.
although other techniques that use stateless exploration also require reset functions egverisoft the cost of developing such functions could a ect practicality.
newjvm reusejvm and splat all require the state outside of the jvm to be explicitly reset but only newjvm automatically resets jvmspeci c state by spawning a new jvm for each test run.
.
related work .
dynamic analysis korat.
splat was inspired by korat a test input generation technique based on java predicates.
korat instruments accesses to object elds used in the predicate monitors the accesses to prune the input space of the predicate and enumerates those inputs for which the predicate returns true.
directly applying korat to the problem of reducing the combinatorics in testing con gurable systems is not feasible because the feature model encodes a precondition for running the con gurable system which must be accounted for.
in theory one could automatically translate a declarative feature model into an imperative constraint and then execute it before the code under test but it could lead korat to explore the entire space of feature combinations up to 2ncombinations for nfeatures before every test execution.
in contrast splat exploits feature models while retaining the e ectiveness of execution driven pruning by applying it with sat in tandem.
additionally splat can change the con guration being run during the test execution line in figure which korat did not do for data structures.
shared execution.
starting from the work of d amorim et al.
there has been considerable ongoing research on saving testing time by sharing computations across similar test executions .
the key observation is that repeated executions of a test have much computation in common.
for example shared execution runs a test simultaneously against several spl con gurations.
it uses a setof con gurations to support test execution and splits and merges this set according to the di erent decisions in controlow made along execution.
the execution sharing techniques for testing spls di er from splat in that they use stateful exploration they require a dedicated runtime for saving and restoring program state and only work on programs with such runtime support.
consequently they have high runtime overhead not because of engineering issues but because of fundamental challenges in splitting and merging state sets at proper locations.
in contrast splat uses stateless exploration and never merges controlow of di erent executions.
although splat cannot share computations between executions it requires minimal runtime support and can be implemented very easily and quickly against almost any runtime system that allows feature variables to be read and set during execution.
sampling.
sampling exploits domain knowledge to select con gurations to test.
a tester may choose features for which all combinations must be examined while for other features only t way most commonly way interactionsare tested .
our dynamic program analysis safely prunes feature combinations while sampling approaches can miss problematic con gurations .
spectrum of spl testing techniques.
k astner et al.
de ne a spectrum of spl testing techniques based on the amount of changes required to support testing.
on the one end are black box techniques that use a conventional runtime system to run the test for each con guration newjvm is such a technique.
on the other end are whitebox techniques that extensively change a runtime system to make it spl aware shared execution is such a technique.
splat which only requires runtime support for reading and writing to feature variables is a lightweight white box technique that still provides an optimal reduction in the number of con gurations to consider.
.
static analysis we previously developed a static analysis that performs reachability dataow and controlow checks to determine which features are relevant to the outcome of a test .
the analysis enables one to run a test only on all valid combinations of these relevant features that satisfy the feature model.
splat is only concerned with reachability so even if it encounters a feature whose code has no e ect it will still execute the test both with and without the feature.
but a large portion of the reduction in con gurations in running a test is simply due to the idea that many of the features are not even reachable.
indeed as section shows splat determines reachable con gurations with much greater precision and is likely to be considerably faster than the static analysis because splat discovers the reachable con gurations during execution.
static analysis may be faster if its cost can be o set against many tests because it needs only be run once for one test code that allows di erent inputs and if a test run takes a very long time to execute egrequiring user interaction .
but such situations do not seem to arise often especially for tests that exercise a small subset of the codebase.
.
conclusions splat is a new technique for reducing the combinatorics in testing con gurable systems.
splat dynamically prunes the space of con gurations that each test must be run against.
splat achieves an optimal reduction in the number of con gurations and does so in a lightweight way compared to previous approaches based on static analysis and heavyweight dynamic execution.
experimental results on software product lines written in java show that splat substantially reduces the total test execution time in most cases.
moreover our application of splat on a large industrial code written in ruby on rails shows its scalability.
.