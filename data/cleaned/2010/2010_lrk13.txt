scalable analysis of variable software j rg liebig alexander von rhein university of passau germanychristian k stner carnegie mellon university usasven apel jens d rre christian lengauer university of passau germany abstract the advent of variability management and generator technology enables users to derive individual variants from a variable code base based on a selection of desired configuration options.
this approach gives rise to the generation of possibly billions of variants that however cannot be efficiently analyzed for errors with classic analysis techniques.
to address this issue researchers and practitioners usually apply sampling heuristics.
while sampling reduces the analysis effort significantly the information obtained is necessarily incomplete and it is unknown whether sampling heuristics scale to billions of variants.
recently researchers have begun to develop variability aware analyses that analyze the variable code base directly exploiting the similarities among individ ual variants to reduce analysis effort.
however while being promising so far variability aware analyses have been applied mostly only to small academic systems.
to learn about the mutual strengths and weaknesses of variability aware and sampling based analyses of software systems we compared the two strategies by means of two concrete analysis implementations type checking and liveness analysis applied them to three subject systems busybox the x86 linux kernel and openssl.
our key finding is that variability aware analysis outperforms most sampling heuristics with respect to analysis time while preserving completeness.
categories and subject descriptors d. .
coding tools and techniques d. .
reusable software d. .
processors preprocessors general terms experimentation keywords software product lines c preprocessor type checking liveness analysis variability aware analysis permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
esec fse august saint petersburg russia copyright acm ... .
.
.
introduction generator based approaches have proved successful for the implementation of variable software systems .
for example the linux kernel can be configured by means of about 10000compile time configuration options giving rise to possibly billions of variants that can be generated and compiled on demand.
while advances in variability management and generator technology facilitate the development of variable software systems with myriads of variants this high degree of variability is not without cost.
how could we analyze all possible variants for errors?
unfortunately classic analyseslookatindividualvariantsanddonotscaleinthepresence of the exponential number of variants that can be typically generated from a variable system.
for systems such as the linux kernel it is not even possible to generate all variants to analyze them separately because they are so many more than the estimated number of atoms in the universe .
the idea of sampling is to select a reasonable set of variants to be analyzed using traditional analysis techniques.
many different sampling techniques have been proposed and theirapplicationhasprovedusefulinvariousscenarios .
although analysis time can be reduced significantly the information obtained is necessarily incomplete since only a subset of all variants is checked.
recently researchers have begun to develop a new class of analyses that are variability aware .
the key idea is to not generate and analyze individual variants separately but to directly analyze the variable code base before variant generation with the help of configuration knowledge.
in the case of the linux kernel variable code is implemented by means of conditional inclusion directives a.k.a.
ifdefs and variability aware analyses analyze the variable code directly instead of applying the generator the c preprocessor to generate the plain c code of individual kernel variants.
variability aware analysis requires more effort than traditional analysis of a single system because all local variati ons need to be considered however and this is the key success factor variability aware analysis takes advantage of the similarities among variants and avoids analyzing common code over and over again.
there are several proposals for variability aware analyses in the literature including parsing type checking data flow analysis model checking and deductive verification .
however while this work is promising variability aware analyses beyond parsing have not been applied to large scale real world systems so far previous work concentrated mostly either on formal foun dations or is limited with respect to practicality evaluated with academic case studies only as we discuss in section .
despite the foundational previous work it is unclear whether variability aware analysis scales to large systems as it co nsiders all code and all variations of a system simultaneously.
since sampling is still a de facto standard for analyzing variable software systems in practice we explore the feasibility and scalability of both sample based and variability aware analysis in practice empirically.
to this end we have developed two fully fledged variability aware analyses for c type checking and liveness analysis a data flow analysis .
we applied each of them to three real world large scale variable systems the busybox tool suite the linux kernel and the cryptographic library openssl.
in terms of scalability we compare the variability aware analyses to state of the art sampling strategies used in practice generating all variants i s not even possible in reasonable time in our case studies .
we found that variability aware analyses scale well even outperform some of the sampling strategies while still providing complete information on all system variants.
beside quantitative results we report on our experience with making variability aware analyses ready for the real world and we discuss insights into the development of variability aware analyses in general.
these insights subsume existing studies on variability aware analysis techniques a nd they can guide the development of further analyses.
overall we make the following contributions an introduction to the problem of analyzing variable software including possible solutions such as sampling and variability aware analysis.
an experience report of how to implement scalable variability aware analysis for preprocessor based systems based on an existing variability aware parsing framework .
a discussion of general patterns of variability aware analyses that can guide the development of further analyses.
a series of experiments that compare the performance of variability aware analysis with the performance of state of the art sampling strategies based on three realworld large scale subject systems.
a reflection of our experience with applying variabilityaware and sampling based analyses in practice and of challenges we encountered in our investigation.
thesubjectsystemsandallexperimentaldataareavailable on a supplementary website the analysis implementations are part of the typechef project .
.
preprocessor based v ariability before we get to sampled based and variability aware analyses and their comparison we introduce the development of variable software using the c preprocessor cpp.cppis a frequently applied tool for the development of variable software .
it provides several features to implement variable code fragments using conditional inclusion macros a.k.a.
ifdefs .
for instance our running example in figure contains three variable pieces of code an alternative macro expression line an optional function parameter line and an optional statement line .
the in exclusion of such annotated code is controlled by the values of configuration options here aandb that can be combined using logical operators.
ifdef a define expr a else define expr endif 5intr 6intfoo inta ifdef b intb endif 7if expr 8return b 10intc a 11if c 12c a ifdef bc b endif 15returnc figure running example in c with variability expressed in the form of preprocessor directives lines and for brevity we underlined and integrated ifdef directives inside single code lines.
in most cases not all combinations of configuration options of a system are valid so developers use variability models to express relations between configuration options and define which combinations of configurations options are valid.
one widely used tool in practice to express variability models is kconfig 1which is used for example in the development of linux and busybox.
variability models can be transformed into boolean formulas which enables efficient reasoning about them using current sat solver technology .
.
sample based analysis sample based analysis has its origin in early approaches of testing software .
due to the sheer size and complexity of real world systems the number of variants can grow exponentially with the number of configuration options a brute force approach of analyzing all variants in isolation is not feasible .
hence developers typically analyze only a subset of variants called the sample set using off the shelf analysis tools.
the idea is that even though we cannot analyze all variants individually we can still strive for analyzing a representative sample set to be able to draw informed conclusions about the entire set of variants e.g.
in terms of defect probability .
the sample set is selected by a sampling heuristic either by a domain expert or by an algorithm.
researchers and practitioners have proposed different sampling heuristics some of which require sophisticated upfront analyses.
we selected four that are common in practice single configuration random code coverage and pair wise coverage as described below.
for an overview of other sampling strategies see a recent survey .
single conf heuristic.
thesimplestsamplingheuristic single conf is to analyze only a single representative variant that enables most if not all of the configuration options of the variable system.
typically the variant is selected manually by a domain expert.
the strength of this heuristic is that one needs to analyze only a single variant hence it is fast.
by selecting many configuration options the heuristic tries to cover a large part of the system s code however it cannot cover mutually exclusive code pieces or intricate interactions specific to individual combinations of configuration options .
for the code snippet in figure we can create a configuration that enables all configuration options kconfig language.txtheader.h ifdef a 2intfoo inta ... else 4intfoo2 inta ... endif ifdef b 7intbar inti intj ... endifmain.c include header.h 12intmain ifdef a 14bar 15foo2 endif ifdef c 18print done endif figure c code with preprocessor directives the header file left contains one alternative and one optional definition the c file right uses the definitions of the header file.
a b c .
since code fragments in lines and are mutually exclusive a single configuration will cover only one of them leaving line uncovered in our case.
according to dietrich et al.
in the linux development community it is common to analyze only one predefined variant with most configuration options selected called allyesconfig .
similarly many software systems come with a default configuration that satisfies most users and that usually includes many configuration options.
random heuristic.
a simple approach to select samples is to generate them randomly.
for example in a project with nfeatures we could make nrandom independent decisions whether to enable the corresponding configuration option.
in projects with constraints between options we would discard variants with invalid configurations and would keep the remaining variants as our sample.
random sampling is simple and scales to an arbitrary sample size.
alternatively testing can continue until time or money runs out.
random sampling does not adhere to any specific coverage criterion though.
code coverage heuristic.
thecode coverage heuristic is inspired by the statement coverage criterion used in software testing .
in contrast to software testing the code coverag e heuristic aims at variant generation not code execution .
the goal of this heuristic is to select a minimal sample set of variants such that every lexical code fragment of the systems code base is included in at least one variant.
in contrast to the single conf heuristic the code coverage heuristic covers mutually exclusive code fragments.
however note that including each code fragment at least once does not guarantee that all possible combinations of individual code fragments are considered.
for the code snippet in figure two configurations a b c and the first selecting all options and the second deselecting them all would be sufficient to include every code fragment in at least one variant.
however it would not help to detect the compilation error i.e.
calling barwhenais selected but bis not.
although there is an algorithm to compute an optimal solution a minimal set of variants by reducing the problem to calculating the chromatic number of a graph this algorithm isnp complete and by far too slow for our case studies.2instead we resort to the conservatively approximated solution of tartler et al.
which speeds up the computation of the sample set significantly at the cost of producing a sample set that is possibly larger than necessary.
2for more details on the optimal algorithm see com ckaestne optimalcoverage .a subtle problem of this heuristic arises from the issue of how to treat header files.
when computing the sample set of two variants in our example we have implicitly assumed that we analyze coverage in the main file andthe included header file together.
due to the common practice of including files that themselves include other files a sing le include statement in the source code can bloat the code base of a single file easily by an order of magnitude something we frequently observed in linux in which on average 300header files are included in each c file .
in addition header files often exhibit their own variability not visible in the c file without expanding macros.
furthermore some header files may be included only conditionally depending on other ifdefdirectives such that for a precise analysis of all header code sophisticated analysis mechanisms beco me necessary e.g.
using symbolic execution of the preprocessor code .
this explosion and cost can make precise analyses that include header files unpractical or infeasible even with tartler s approximate solution.
therefore we distinguish two strategies for code coverage covering variability only in c files and covering variability in c files and their header files.
analyzing only the main file above a single configuration a c would be sufficient to cover all code fragments of the main file.
pair wise heuristic.
thepair wise heuristic is motivated by the hypothesis that many faults in software systems are caused by interactions of at most two configuration options .
using the pair wise heuristic the samp le set contains a minimal number of samples that cover all pairs of configuration options whereby each sample is likely to cover multiple pairs.
for the code in figure with three optional and independent features a pair wise sample set consist of configurations a b a c b c and .
the computation of pair wise sample sets is not trivial if constraints such as aimpliesborc exist in a variability model in fact it is np complete similar to the minimum set cover problem .
hence existing tools apply different con servative approximations to make the computation possible for large systems with many configuration options.
for our experiments we use splcatool3by johansen et al.
which computes pair wise samples using covering arrays .
the computed sample covers all pair wise interactions that occur in a given system but is not guaranteed to be minimal.
.
v ariability a ware analysis variability aware analysis also known as family based analysis takes advantage of the similarities between t he variants of a system in order to speed up the analysis process.
although individual variability aware analyses differ in man y details an idea that underlies all of them is to analyze code that is shared by multiple variants only once.
to this end variability aware analyses do not operate on generated variants but on the raw code artifacts that still contain variability and available configuration knowledge prior to the variant generation step.
in our context variability aware analyses work directly on c code that still contains preprocessor directives.
as code artifacts with preprocessor directives cannot be processed directlybystandardanalyses ananalysishastobeprepared it has to be made variability aware plain preprocessing does intrfunctiondef intfoo intachoice b intb stmt block if condition choice a a 00then block return b... bb b figure excerpt of the corresponding variable ast left and cfg right for our running example of figure .
not help as it removes variability .
technically one has to adapt existing analyses to empower them to work with variable code fragments.
this approach has been pursued for adapting existing type checking model checking and tes ting techniques to variable systems .
although variability aware analysis has been applied in academic projects showing promising performance improvements by orders of magnitude apart from parsing it has never been applied to real world software systems at the scale of linux.
since many industrial software systems are implemented in c and use define and ifdefdirectives and a build system to implement compile time variability we set the goal of implementing two variability aware analyses for cand of applying them to large scale projects.
for the purpose of presenting and discussing the results of our empirical study we explain in the remaining section our design decisions in implementing variability aware type checking and liveness analysis.
note that we implemented the liveness analysis for the purpose of our study.
it is the first variability aware data flow analysis for c. it scales to real world large scale systems such as linux.
variable abstract syntax trees.
many static analyses are performed on abstract syntax trees asts .
since we want to analyze an entire variable software system we have to construct an abstract syntax tree that covers all variants of a system and the corresponding configuration knowledge.
the desired variable ast is like a standard ast but it contains additional nodes to express compile time variation .
achoicenode expresses the choice between two or more alternative subtrees similar to ambiguity nodes in glr parse forests explored formally in the choice calculus .
for example choice a a figure left expresses the alternative of two expressions a 0and0that is controlled by configuration option a. the choice node is a direct representation of the variable expression in our running example figure ifdefs on line to and their usage on line .
one alternative of a choice may be empty see figure which makes the other in fact optional.
in principle we could use a singlechoicenode on top of the ast with one large branch per variant but a variable ast is more compact because it shares parts that are common across multiple variants e.g.
infigure3 westoreonlyasinglenodeforthedeclarationof r and a single node for the function name foo which are shared byallvariants .
itisthissharingandkeepingvariabilityloc al which makes variability aware analysis faster than a bruteforce approach see the discussion at the end of this section .
to reason about variability we need to represent configuration knowledge.
to this end we annotate subtrees withpresence conditions .
propositional formulas are suffi cient to describe presence conditions and can be efficiently processed by satsolvers and bdds .
as an example in figure parameter bis included only if bis selected whereas the condition of the ifstatement has two alternative subtrees depending on whether ais selected.
in our example presence conditions are atomic and refer only to a single configuration option but more complex presence conditions such asa b c are possible.
by storing presence conditions in choicenodes we can derive the code of every variant of the variable system given the configuration for that variant.
compact representations of variable asts in this or similar forms are commonly used in variability aware analyses .
the construction of a variable ast from a real world software system such as linux is not trivial.
whereas parsing preprocessed c code of an individual variant is well established parsing a variable system with ifdefs is challenging.
to make matters worse in the c preprocessor conditional compilation directives ifdef interact with the build system with macros define and with file inclusion facilities include across file boundaries in intricate ways.
in previous work we solved the parsing challenge and implemented a sound and complete parser as part of the typechef project incorporating prior work on variability model extraction and build system analysis .
variability aw are parsing always considers a c file with all its header files.
it is this recent breakthrough in parsing that now finally enables the analysis of real world c code with ifdefvariability.
for details on the parser see the corresponding publication .
in the remainder of this paper we use this parser framework as a black box and work on the resulting variable asts.
variability aware type checking.
a standard type checking algorithm for c traverses the ast collects declarations in a symbol table and attempts to assign proper types to all expressions gettype map expr type .
in principle a variability aware type checker works similar but covers all variants hence it must be aware of variability in each of the following three steps.
first a symbol variable function etc.
may only be declared in some variants or it may even have alternative types in different variants.
therefore we extend the symbol table similar to the proposal of aversano et al.
such that a symbol is no longer mapped to a single type but to a conditional type a choice of types or vst map .
we illustrate a possible encoding of a conditional symbol table for our example in table .
if a symbol is declared in all variants we do not need choice nodes however if a symbol is declared in a subtree of the ast that is only reachable given a certain presence condition we include the symbol and type in the symbol table only under that condition.
similarly we may declare a symbol with different types in different variants.
in our running example function foohas two alternative types depending on whether bis selected.
similarly we made the table for structures and enumerations in c variability aware.
second during expression typing we assign a variable type choices of types to each expression gettype vst expr choice wherealreadylookingupanameinasymbol tablemayreturnavariabletype.
forexample whenchecking that the condition of an ifstatement has a scalar type we need to check that all alternative choices of the variable type are scalar.
if the check fails only for some alternative results table conditional symbol table at line of our running example of figure .
symbol conditional type scope r int foo choice b int int int int int a int b choice b int we report a type error and pinpoint it to a subset of variants as characterized by a corresponding presence condition.
similarly an assignment is only valid if the expected variable type is compatible with the provided variable type in all variants.
therein an operation on two variable types can in the worst case result in the cartesian product of the types in either case of the choice resulting in a variable type with many alternatives.
all other type rules are essentially implemented along the same lines.
in our running example we would report a type error in line because symbol b cannot be resolved in variants without b see table .
third we can use the variability model of a system if available to filter all type errors that occur only in invalid variants.
to this end we simply check whether the presence condition of the type error is satisfiable when conjoined with the variability model checked with a standard satsolver .
variable control flow graphs.
for most data flow analyses we need to construct a control flow graph cfg which represents all possible execution paths of a program.
nodes of the cfg correspond to instructions in the ast such as assignments and function calls edges correspond to possible successor instructions according to the execution semantics of the programming language.
a cfg is a conservative static approximation of the actual behavior of the program.
as with type checking we need to make cfgs variable to cover all variants of systems.
to create a cfg for a single program we need to compute the successors of each node succ node list .
in the presence of variability the successors of a node may differ in different variants so we need a variability aware successor function that may return different successor sets for different variants succ node choice or equivalently but with more sharing succ node list .
using the result of this successor function we can determine for every possible successor a corresponding presence condition which we store as annotation of the edge in the variable cfg.
let us illustrate variable cfgs by means of the optional statement in line of our running example of figure .
in figure right we show an excerpt of the corresponding variability aware cfg node numbers refer to line numbers of figure .
the successor of the instruction c ain line depends on the configuration if bis selected statement c bin line is the direct successor if bis not selected return c in line is the only successor.
technically we add further nodes to the result set of the successor function until the conditions of the outgoing edges cover all possible variants in which the source node is present checked with a satsolver or bdds .
by evaluating the presence conditions on edges we can reproduce the cfg of each variant.
4alternatively we could have dropped the presence conditio ns on edges and express variations of the control flow with ifstatements.
on anifstatement a normal cfg does not evaluate the expression but conservatively approximates the control flow by re portingtable result of liveness computation of our running example of figure bbis shorthand for choice b b .
line uses defines in out a c a bb a bb c c a bb c a bb c a c c a bb c bb c bb cb cb bb cb cb c c variability aware liveness analysis.
liveness analysis is a classic data flow analysis for the computation of variables that are live that may be read before being written again for a given statement.
its result can be used for example to conservatively detect dead code.
in real world systems warnings about dead code that occurs only in specific variants are interesting for maintainers corresponding problems are regularly reported as bugs.5so again our goal is to make liveness analysis variability aware.
liveness analysis is based on two functions usescomputes all variables read and defines computes all variables written to.
while in traditional liveness analysis both functio ns return sets of variables in variability aware liveness analysi s both return sets that may vary depending on the configuration a choice of sets or a set with optional entries .
the computation of liveness is a fixpoint algorithm that uses two functions inandout which compute variables that are live at respectively after the current statement.
the result of in andoutis variable again and the signatures of both change fromnode set tonode set whereid represents the identifier of a live variable.
in table we show the results of variability aware liveness analysis for our running example.
we show the result of each equation as a set of variables together with their presence condition as subscript.
for example only cis live in thereturnstatement on line .
considering the control flow from line to b13 in the declaration statement on line variable ais live whereas bis only live if bis selected.
principle keeping variability local.
although we have introduced how variability aware analyses work we have not explained why we expect that they can be executed efficiently even for real world systems with myriads of possible variants.
the key is keeping variability local .
parsing already preserves sharing in the ast and keeps variability local code without ifdefdirectives is represented only once since it is common to all variants choices are introduced only locally where code differs between variants .
we preserve this sharing and locality throughout our analyses as far as possible.
specifically three patterns emerged that maximize sharing late splitting local variability representation andearly joining .
first late splitting means that we perform the analysis without variability until we encounter it.
for example type both alternative branches as possible successor statements e.g.
in figure both nodes and may follow node .
such sound b ut incomplete approximation is standard practice to make stati c analysis tractable or decidable.
however we do not want to lose pre cision for static variability.
furthermore we have only proposit ional formulas to decide between execution branches which makes comput ations decidable and comparably cheap so we decided in favor of prese nce conditions on edges which is in line with prior work on cfg in variable java programs .
5e.g.
.checking processes the declaration of symbol rin line only once and adds it to the symbol table only once whereas a brute force strategy or a sampling strategy would process this declaration multiple times.
also when we use symbol rlater it has only one type.
variability aware analyses only split and consider smaller parts of the variant space when they actually encounter variability for example in the declaration of parameter b. late splitting is similar to path splitting in on the fly model checking where splitting is als o performed only on demand .
second local variability representation aims at keeping variability local in intermediate results.
for example inste ad of copying the entire symbol table for a single variable entry we have only a single symbol table with conditional entries technically we use map instead of choice to achieve this locality .
therefore even after the conditional declaration of parameter b we only store a single type for aorr independently of b. third early joining attempts to joinintermediate results as early as possible.
for example if we have a choice of two identicaltypes choice a int int wecansimplyjointhem tointfor further processing.
so even if we need to compute the cartesian product on some operations with two variable types the result can often be joined again to a more compact representation.
this way variability from parts of the ast leaks into other parts if and only if variability actually makes a difference in the internal representations of types names or other structures.
also we need to consider only combinations of configuration options that occur in different parts of the ast if they actually produce different intermediate results when combined otherwise the results remain orthogonal.
note that the three patterns of late splitting local variability representation and early joining apply to any kind of variability aware analysis although not always made expli cit these patterns can also be observed in other variability aware analyses .
.
empirical study to evaluate feasibility and scalability of different analysis strategies we attempt to analyze three real world and large scale systems a fact that substantially increases exte rnal validity compared to previous work which concentrated mostly on formal foundations made limiting assumptions or relied on comparatively small and academic case studies see section .
we use both state of the art sampling heuristics single conf code coverage with and without headers and pair wise as introduced in section .
we apply both type checking and liveness analysis.
we report our experience and perform rigorous performance measurements.
.
hypotheses and research questions based on the goals and properties of variability aware and sampling based analyses we formulate two hypotheses and two research questions.
.variability aware vs. single conf analyzing all variants simultaneously using variability aware analysis is likely slower than analyzing a single variant that covers most configuration options.
the reason is that the variable program representation covering all variants is larger than the program representation of any single variant including the largest possible variant.h1theexecutiontimesofvariability awaretypechecking and liveness analysis are largerthan the corresponding times of analyzing the variants derived by single conf sampling.
.variability aware vs. pair wise while previous work has shown that pair wise sampling is a reasonable approximation of the analysis of all variants our previous experience is that it can still generate quite large sample sets.
hence we expect a variability aware analysis to outperform pair wise sampling h2the execution times of the variability aware type checking and liveness analysis are smaller than the corresponding times of analyzing the variants derived by pair wise sampling.
.variability aware vs. code coverage with respect to the comparison of variability aware analysis and codecoverage sampling we cannot make any informed guesses with respect to analysis time.
code coverage sampling generates sample sets depending on the usage of configuration options in the analyzed c files.
since we do not know details about the code we cannot predict how many variants will be generated and how large these will be.
therefore we pose a research question instead.
specifically the influence of variability that occurs in header files is unknown and therefore we look at two different variants of code coverage one including header files and one without.
rq1how do the execution times of variability aware type checking and liveness analysis compare to the times for analysis of the variants derived by codecoverage sampling with and without header files ?
.scalability finally we pose the general question of the scalability of variability aware analysis.
rq2does variability aware analysis scale to systems with thousands of configuration options?
the background for questioning scalability is that variability aware analysis reasons about variability by solvingsatproblems or by using bdds during analysis and so depends on the degree of sharing that is possible in practice see section .
generally satis np complete but previous work suggests that the problemsthatariseinvariability awareanalysisaretypically tractableforstate of the art satsolvers andbdds and that caching can be an effective optimization .
.
subject systems to test our hypotheses and to answer our research questions we selected three subject systems.
we looked for publicly available systems for replicability that are of subs tantial size actively maintained by a community of developers used in real world scenarios and that implement substantial compile time variability with the c preprocessor.
the systems must provide at least an informal variability model that describes configuration options and their valid combinations .
in this context we would like to acknowledge the pioneering work on variability model extraction and build system analysis which enabled us for the first time to conduct variability aware analysis on substantial real world systems that are an order of magnitude larger then previous work on java based subjects .
thebusybox tool suite reimplements most standard unix tools for resource constrained systems.
with configuration options it is highly configurable most of the options refer to independent and optional subsystems the variability model in conjunctive normal form has993clauses.
we use busybox version .
.
files and lines of source code .
thelinux kernel x86 architecture version .
.
.
is an operating system kernelwith millions ofinstallations worldwide from high end servers to mobile phones.
with6918configuration options it is highly configurable.
in a previous study we identified the linux kernel as one of the largest and most complex w.r.t.
variability publicly available variable software systems .
it has7691source code files with .
million lines of code.
note that already the variability model of linux is of substantial size the corresponding extracted formula in conjunctive normal form has over 60000variables and nearly 300000clauses a typical satisfiability check requires half a second on a standard computer.
the cryptographic library openssl implements different protocols for secure internet communication.
openssl can be tailored to many different platforms and it provides a rich set of configuration options.
we analyze openssl version .
.1c with 733files and 233450lines of code.
since openssl does not come with a formal variability model like busybox or linux we extracted a variability model based on manual analysis.
the resulting variability model has clauses.
.
experience with sampling beforewediscusstheperformancemeasurements wewould like to share our experience with sampling approaches which were surprising to us.
we expected that contemporary sampling tools can quickly compute representative sample sets.
however we found that deriving samples at this scale is far from trivial and that we even failed to compute sample sets for some sampling heuristics code coverage with headers for linux and that the computation time already takes up to several hours e.g.
pair wise 20h for linux .
the single conf heuristic worked well.
linux has a commonly used configuration allyesconfig which is maintained by the community and frequently used for analysis purposes.
for busybox and openssl we created large configurations by selecting as many configuration options as possible.
random sampling already proved problematic.
both busybox and linux have variability models with many constraints.
in1000000 random configurations there was not even a single configuration that fulfilled all variability model constraints.
randomsamplingwasonlyapossibilityforopenssl which has a comparably sparse variability model of randomly generated configurations were valid .
busybox developers actually uses a skewed form of random sampling in which one by one a random value is selected for every configuration option that is not yet decided by constraints of other options.
this approach depends strongly on variable ordering and violates the developer intuition about random selection .
duetothesesamplingproblemsinthepresenceofconstraints we did not consider random sampling any further.
for the coverage based and the pair wise heuristics we observed that the generation of samples took considerable time times for sample generation are not part of our reported experimental setup.
analysis times below and was not possible in all cases in particular for linux .
in contrast to all other heuristics heuristics based on code coverage need to investigate every file separately and optionallyalltheirheaderfiles .
wereimplementedtheconservativ e algorithm of tartler et al.
for this task in two variants one including header files code coverage and one without code coverage nh .
when headers are included and macros are considered the coverage analysis easily needs to process several megabytes of source code per c file .
surprisingly already the times needed for code coverage sample computation exceeded the times for performing variability aware type checking and liveness analysis.
to compute the pair wise sample set we only found one research tool splcatool which is able to compute a complete set of pair wise configurations for a given feature model see section .
splcatool did reasonably well for busybox and openssl but the larger configuration space of linux made the computation of the sample set very expensive.
also in this case the computation time exceeded the times for performing variability aware liveness analysis.
.
experimental setup we use typechef as underlying parsing framework.
as explained in section typechef generates a variable ast per file in which choicenodes represent optional and alternative code.
our implementations of variability aware type checking and liveness analysis are based on these variable asts and they are integrated into and deployed as part of the typechef project.
to avoid bias due to different analysis implementations we use our infrastructure for variability aware analysis also for the sample based analyses.
to this end we generate individual variants asts without variability based on the sampling heuristics.
we create an ast for a given configuration by pruning all irrelevant branches of the variable ast so that no choicenodes remain.
as there is no variability in the remaining ast the analysis never splits and there is no overhead due to satsolving because the only possible presence condition is true.
as liveness analysis is intraprocedural it would have been possible and more efficient to apply sampling to individual functions and not to files as done by brabrand et al.
for java product lines .
unfortunately preprocessor macros in c rule out this strategy as we cannot even parse functions individually without running the preprocessor first or without performing full variability aware parsing.
in our running example of figure we would not even have noticed that functionfoois affected by a because variability comes from variable macros defined outside the function.
variable macros defined in header files are very common in c code .furthermore we implemented only an imprecise liveness analysis since the analysis is performed without a real analysis question e.g.
which code is dead .
the strategy of abstraction is a common approach in model checking to handle the complexity of a system and to make the analysis feasible in the first place.
in particular during liveness computation our algorithms perform satchecks without taking the variability model of the analyzed system into account.
this way the computation is faster and still complete though false positives may occur.
false positives can be eliminated easily after a refinement step i.e.
using the variability model in satchecks so that only valid execution paths are taken into account .
in figure we illustrate the experimental setup.
depending on the sampling heuristics one or multiple configurations are checked.
for each file of the three subject systems we measured the time spent in type checking and liveness analysis each using the variability aware approach and the three sampling heuristics the latter consisting of multiple internal runs in total four analyses per subject system onevariability aware threerespectivelyfoursampling with and without header files for code coverage .
we ran all measurements on linux machines ubuntu .
with intel core i7 .4ghz and 32gb ram.
we configured the java jvm with upto 8gb ram for memory allocation.
to reduce measurement bias we minimized disk access by using a ramdisk and warmed up the jvm by running an example task before the actual measurement run.
however due to just in time compilation and automatic garbage collection inside the java jvm measurements of analysis times might slightly differ for similar inputs.
we could not mitigate this problem with repetitive analysis runs since the setup already takes weeks to finish but we believe that the large number of files produces still a reliable result.
analysis procedure and reporting.
wereportperformance measurements as total times for the entire analysis and additionally graphically as the distribution of analysis times for individual files in the project using notched boxplots on a logarithmic scale.
we highlight the median over all files of the variability aware analyses with a vertical line to simplify comparison with the medians of the sample based analyses.
furthermore we provide the number of analyzed configurations for each of the sample based analyses below the name of the analysis configs per file or short c.p.f.
.
single co nf requires the same number of variants for each file because they are based on global knowledge of the variability model only whereas code coverage and pair wise7require different numbers of variants in different files which we provide in terms of mean standard deviation.
we evaluate all research hypotheses with paired t tests at a confidence level of .
.
results in table we show the measurement results for each analysis and subject system.
we report sequential times though parallelization would be possible in all cases becau se all files are analyzed in isolation.
in figures and we 7in addition to the given variability model the build system of linux defines presence conditions for individual files.
so as for l inux each file has its own variability model.
nevertheless we use the g lobal variability model for the computation of pair wise sample s ets.
since the sample may contain configurations that are not valid for a file the overall number of analyzed configurations for a file decre ases.table total times for analyzing the subjects with each approach time in seconds with three significant digits .
type checking liveness analysisbusyboxsingle conf .
.
code coverage nh .
code coverage pair wise variability aware .3linuxsingle conf code coverage nh pair wise variability aware 13900opensslsingle conf .
.
code coverage nh .
.
code coverage pair wise variability aware .7liveness analysisvariability awarepair wise .
.
c. p. f. code coverage .
.
c. p. f. code coverage nh .
.
c. p. f. single conf config per file 2000type checkingvariability awarepair wise .
.
c. p. f. code coverage .
.
c. p. f. code coverage nh .
.
c. p. f. single conf config per file figure distribution of analysis times for busybox times in milliseconds logarithmic scale .
plot the distributions of analysis times for busybox linux and openssl as described in section .
.
in all subject systems and for both type checking and liveness analysis the variability aware approach is slower than single conf sampling h statistically significant and it is faster than pair wise sampling h statistically significant .
the results regarding code coverage sampling h are mixed variability aware analysis is faster for liveness analysis in linux slower for liveness analysis in busybox and openssl and for type checking of linux and openssl statistically significant .
we observe that code coverage without header files nh is often faster than with header files and sometimes it even outperforms single conf sampling.
the reason for this is that many ifdefs occur in header files something that is neglected in code coverage sampling nh.
single conf sampling considers variability in header files such that it may select a larger configuration with additional header code which is potentially unnecessary and is therefore slower than code coverage sampling nh.
table summarizes the actual speedups of all comparisons.
itisworthnotingthatwedidnotfindanyconfirmeddefects during our experiments.
for linux we found a defect already fixed in subsequent releases for busybox we found and reported several defects in earlier versions that have been fixed in the current version which we used for our experiments.
8bug reports .htmlliveness analysisvariability awarepair wise .
c. p. f. code coverage nh .
.
c. p. f. single conf config per file 10000type checkingvariability awarepair wise .
c. p. f. code coverage nh .
.
c. p. f. single conf config per file figure distribution of analysis times for linux times in milliseconds logarithmic scale .
liveness analysisvariability awarepair wise c. p. f. code coverage .
.
c. p. f. code coverage nh .
.
c. p. f. single conf config per file 1000type checkingvariability awarepair wise c. p. f. code coverage .
.
c. p. f. code coverage nh .
.
c. p. f. single conf config per file figure distribution of analysis times for openssl times in milliseconds logarithmic scale .
.
discussion our experiments confirm hypotheses h 1and h in all three subject systems variability aware analysis is faster than sampling based analysis using the pair wise heuristics but slower than using the single conf heuristics.
with respect to research question rq there is no clear picture.
the performance of code coverage sampling depends on the variability implementations in the respective files the number of sampled variants and performance results differ considerably between files inside each subject system see figure and .
so performance of the code coverage heuristic is hard to predict and depends strongly on implementation patterns.
a further observation is that the speedup of variabilityaware liveness analysis in relation to sampling is higher than the speedup of variability aware type checking.
this can be explainedbythefactthatlivenessanalysisisintra procedura l whereas type checking considers entire compilation units.
exploring the performance of variability aware inter procedural analyses of large scale systems is an interesting avenue of further work.
theexperimentalresultsforbusybox linux andopenssl demonstrate that variability aware analysis is in the range of the execution times of sampling with multiple samples code coverage and pair wise .
so with regard to question rq2 we conclude that variability aware analysis is practical for large scale systems.
an important finding is that the overhead induced by solving satproblems during analy table speedup of variability aware analysis a speedup .0means that sampling is faster and a speedup .0means that variability aware analysis is faster nonsignificant result in parentheses .
variability aware vs. type checking liveness analysisbusyboxsingle conf .
.
code coverage nh .
.
code coverage .
.
pair wise .
.59linuxsingle conf .
.
code coverage nh .
.
pair wise .
.24opensslsingle conf .
.
code coverage nh .
.
code coverage .
.
pair wise .
.
sis is not a bottleneck not even for large systems such as the linux kernel.
overall variability aware type checking compared to single conf in busybox takes as much time as checking variants variants in linux and variants in openssl .
for liveness analysis the break even point is after busybox linux and openssl variants.
that is if a sampling heuristic produces a sampling set larger than that or if continuing random sampling for more than this number of samples variability aware analysis is faster and also complete.
all values are very low compared with the number of the possible variants of the respective system showing that a complete analysis is already possible at the costs of an incomplete sampling heuristic.
threats to validity.
a threat to internal validity is that our implementations of variability aware type checking and liveness analysis support iso iec c but not all gnu c extensions used in the subject systems especially linux .
our analyses simply ignore corresponding code constructs.
also due to the textual and verbose nature of the c standard the implementation does not align entirely with the behavior of the gnu c compiler.
due to these technical problems we excluded files of busybox and 470files of linux from our study.
all numbers presented in this paper have been obtained after excluding the problematic files.
still the com paratively large numbers of 518files for busybox 7221files for linux and 733files for openssl deem the approach practical and our evaluation representative.
second the variants generated by the sampling heuristics represent only a small subset of possible variants which is the idea of sampling .
but for pair wise sampling it may happen that some variants of a file are very similar as the difference in the respective variant configurations affect the content of a file only to a minor or no extent.
however we argue that our conclusions are still valid as this lies in the nature of the sampling heuristics and all heuristics we have used are common in practice.
finally a standard threat to external validity is that we considered only three subject systems.
we argue that this threat is largely compensated by their size and the fact that many different developers and companies contributed to the development of these systems.
.
related work our implementations of variability aware type checking and liveness analysis are inspired by earlier work in two fields.first we and others have developed variability aware type systemsforacademiclanguagessuchasfeatherweightjava lightweight java the lambda calculus and ot her dialects of java .
first conceptual sketches even reac h back 10years .
although a prior version of our c type checker has been used to study a variability aware module systems and has been applied to busybox it has not been evaluated in an empirical assessment and comparison to sample based type checking .
second researchers proposed variability aware approaches for data flow analysis.
closest to our work brabrand et al.
compared three different algorithms for variability aware intra procedural data flow analysis for java against a bruteforce approach .
similarly bodden proposed an approach to extend an existing inter procedural data flow analysis framework to make it variability aware .
both approaches are limited to an academic environment in which the input java programs contain ifdef like variability annotations managed by a research tool there are no substantial variable real world systems that use this technique.
furthermore both variability aware analysis approaches make frequently limiting assumptions on the form of variability in particular type uniformity and annotation discipline wh ich do not hold in real world software systems .
.
conclusion inthispaper wereportedonourexperiencewiththeimplementation and performance of practical scalable variabilityaware and sampling based analyses for real world large scale systems written in c including preprocessor directives.
in a series of experiments on three real world large scale subject systems including the linux kernel we compared the performance of variability aware type checking and liveness analys is with the performance of corresponding state of the art sampling heuristics single conf pair wise and code coverage .
in our experiments we found that the performance of variability aware analysis scales to large software systems such as the linux kernel and even outperforms some of the sampling heuristics while still being complete.
in contrast to previous work on sampling we faced many problems and found several limiting factors that render state of the art sampling heuristics such as pair wise infeasible.
in future work we aim at the development of further analyses at experimenting with other sampling heuristics and with more case studies and at setting up an automated and incremental checking system for producing bug reports.
.