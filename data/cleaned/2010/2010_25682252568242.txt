interpolated n grams for model based testing paolo tonella roberto tiella fondazione bruno kessler trento italy tonella tiella fbk.eucu duy nguyen university of luxembourg luxembourg duy.nguyen uni.lu abstract models in particular nite state machine models provide an invaluable source of information for the derivation of e ective test cases.
however models usually approximate part of the program semantics and capture only some of the relevant dependencies and constraints.
as a consequence some of the test cases that are derived from models are infeasible.
in this paper we propose a method based on the computation of the n gram statistics to increase the likelihood of deriving feasible test cases from a model.
correspondingly the level of model coverage is also expected to increase because infeasible test cases do not contribute to coverage.
while n grams do improve existing test case derivation methods they show limitations when the n gram statistics is incomplete which is expected to necessarily occur as n increases.
interpolated n grams overcome such limitation and show the highest performance of all test case derivation methods compared in this work.
categories and subject descriptors d. .
testing and debugging general terms reliability experimentation keywords model based testing test case generation n gram statistics .
introduction in model based testing models of the system under test are used to identify the relevant system behaviours to be tested .
finite state machine fsm models or their variants have been widely used for test case derivation .
coverage criteria de ned upon the model are often adopted to decide how test cases are derived from the model permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
icse may june hyderabad india copyright acm ... .
.and among them state coverage and transition coverage are the most common ones.
models can be de ned upfront during the design phase of software development or they can be inferred from observations of actual executions of the system .
automated model inference has been an intensive area of research in the last few years and the proposed techniques can be roughly grouped into two categories state abstraction techniques which monitor the application s state and abstract it by means of properly de ned abstraction functions and event sequence abstraction which infer a regular grammar capable of recognising the observed event sequences while also generalising its recognition power beyond the actually observed sequences.
both manually de ned and automatically inferred models express the program semantics in an incomplete way as a consequence of the abstraction operated during model generation.
in fact a complete and precise speci cation of the system semantics is usually una ordable and would require a very complicated and large model which is against the initial goal of abstracting away the implementation details to get a concise system representation.
on the other hand the approximation necessarily introduced in the model makes some of the test cases derived from the model infeasible.
such test cases violate dependencies and constraints that are not expressed explicitly in the model.
test case infeasibility is one of the major open problems in model based testing .
the n gram statistics expresses the frequency of occurrence of n tuples of events.
we compute the n gram statistics by monitoring some program executions or by collecting some execution traces.
we propose a novel model based test case derivation method where the next event to be added to a test sequence is determined by its probability conditioned to the occurrence of the previous n events.
by deriving test cases that respect the n gram statistics we increase the likelihood of generating feasible test cases.
in fact a given event is added to a test sequence only if it is allowed in the context of the previous n events.
increasing n provides a longer context for test case derivation but the problem is that the number of n grams grows very quickly as n increases.
correspondingly the n gram statistics becomes largely incomplete at increasing n. to overcome this problem we interpolate the n gram statistics by taking advantage of a longer context when it exists and by falling back to a shorter context otherwise.
in practice the n gram statistics is computed as an exponential interpolation from short to long contexts where longer contexts are given anpermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
icse may june hyderabad india copyright acm ... .
exponentially higher weight.
experimental results indicate that interpolated n grams are superior to plain n grams and to traditional test case derivation methods both in terms of feasibility and coverage of the generated tests.
the paper is organised as follows section provides some basic background on model based test case derivation.
section presents our approach.
section describes the experimental study conducted to validate it.
related works are commented in section followed by conclusions and future work in section .
.
background in model based testing models determine the relevant subset of application behaviours to be considered for testing this step is called test base generation orderivation e.g.
with fsm models an output test case may consist of a sequence of events .
the adequacy criterion for modelbased test case derivation has usually the form of coverage de ned on the model.
by focusing on speci c equivalence classes of behaviours represented by the adequacy criterion of choice an e ective and thorough set of test cases can be obtained from the model.
a survey of the state of the art in model based testing has been conducted by dias neto et al.
and sha que .
one of the most frequently used kinds of model is the fsm model even though some alternatives do exist e.g.
briand et al.
use uml class and sequence diagrams .
a node in a fsm represents a state of the application and it can be determined by e.g.
the values of class attributes in case of object oriented applications or the values of graphical objects in case of gui based applications .
fsms are named concrete if each fsm state represents an actual application state or abstract if each fsm state represents a set i.e.
an equivalence class of concrete states e.g.
.
a transition in the fsm represents an application event action e.g.
an input event a method call or an event handler invocation that can change the application state when executed.
additionally guards and conditions can enrich the model to capture the context in which events and actions are executed.
by traversing the fsm sequences of application events are extracted for test case generation so as to satisfy some coverage criterion of choice .
state or transition coverage every fsm state or transition must be exercised by at least one test case is often adopted even if domain speci c e.g.
semantically interacting events criteria are sometimes preferred.
di erent test case derivation strategies can be used to produce a set of test cases that satis es the chosen adequacy criterion.
graph visit algorithms are widely employed to this aim and among them random graph visit rand depth rst visit dfv and breath rst visit bfv are the most preferred and used.
we will consider these three strategies as the baselines for comparison of the approaches proposed in this paper ngram and interp interpolated n grams .
figure shows the pseudocode of the random test sequence generation strategy.
the procedure randomvisit decides whether to add another event to the current test sequence or not in a stochastic way.
with probability recurse prob a randomly selected successor notation se indicates that sis a successor node if event eis triggered of the current node n is added p se to the current event sequence p and randomvisit is invoked recursively.proc randomvisit n node p path do if randprob recurse prob succ then se randchoose succ p randomvisit s p se if pincreases adequacy then addtotestsuite p od.
main while adequacy criterion not satis ed do p randomvisit startnode hi od figure test sequence derivation by random graph visit when recursion is not activated the generated test sequence is added to the test suite if it increases the test suite adequacy level.
multiple random visits are performed until the adequacy criterion e.g.
transition coverage is satis ed.
this algorithm is clearly non deterministic because of the random choice of the fsm transition to traverse invocation of randchoose with parameter succ .
the algorithm parameter recurse prob determines the length of the generated event sequences.
to produce event sequences with an average length equal to that observed in real executions recurse prob can be set to avg trc len avg trc len where avg trc len is the average length of the monitored traced event sequences.
depth rst and breadth rst model traversals share the overall recursive scheme of rand see figure .
they di er from rand in recursion condition and successor selection.
both dfv and bfv maintain a queue of not yet visited edges which plays the role of the selection of the successor node.
dfv makes use of a lifo queue to ensure in depth path exploration while bfv makes use of a fifo queue to ensure in breadth path exploration.
the recursion condition used by rand is replaced by a check for emptiness on the queues of nodes not yet visited when no path remains to be explored in the lifo fifo queue dfv bfv terminate their execution.
the pseudocode of dfv and bfv is provided in our workshop paper .
the dfv and bfv visit procedures are called multiple times from the start node until the chosen adequacy criterion is satis ed.
by construction when the adequacy criterion is transition coverage dfv and bfv are called just once.
an adequacy criterion di erent from transition coverage e.g.
maximum test budget may require multiple invocations of dfv bfv.
the result of the dfv and bfv procedures is non deterministic since it depends on the order in which successor nodes are added to the lifo fifo queues used by the two algorithms.
in fact when there are multiple successor nodes they can be added to the lifo fifo queues in any arbitrary order.
.
approach n gram language models are widely used in natural language processing nlp .
an n gram language model is a probabilistic model where the probability p eje1 e n that a word an event in our case eis preceded by a se 563quence of words events e1 e ndepends only on the last n words with n n p eje1 e n p eje1 e2 e n using probabilistic models knowledge about the n gram statistics supports word prediction .
in turn word prediction is a key component used to address several nlp tasks such as speech recognition handwriting recognition machine translation spell correction natural language generation etc .
in fact nlp algorithms admit usually multiple sentence derivations and n gram statistics can be used to select the most likely among the possible derivations.
the problem with model based test sequence generation is somewhat similar.
among all possible event sequences that satisfy some adequacy criterion e.g.
transition coverage only a subset represent feasible event sequences i.e.
event sequences that can be actually executed against the application under test.
infeasible sequences involve execution steps whose order is forbidden by the application under test or involve events whose valid inputs prevents the execution of a later subsequence.
avoiding the generation of infeasible event sequences is very similar to avoiding the derivation of unlikely sentences and n gram statistics can be used in a similar way as in nlp to achieve such purpose.
in fact by generating event sequences that contain n grams previously observed in real executions the likelihood that such sequences will in turn be executable is increased.
modeln gramstatisticstest casegenerationtest suitetracesmonitoringsimulationmodel definition refinement figure overall view of the n gram based test case generation method figure shows a high level view of the proposed approach.
while graph visit test case generation algorithms rand dfv and bfv require just one input i.e.
the model ngram based test case generation needs two inputs model and n gram statistics.
the model can be de ned manually by the user it can be inferred automatically from execution traces using state abstraction or event sequence abstraction or a mixed approach can be followed in which the model is rst inferred and then it is manually re ned by the user.
n gram statistics consist of the number of occurrences of the n tupleshe e e2 e n 1i to be read as epreceded by e1 preceded by e2 etc.
which can be turned by normalisation to into conditioned probabilities p eje1 e2 e n .
n gram statistics and the associated conditioned probabilities are the key components of n gram based test case derivation.
such statistics can be obtained by monitoring some executions of the application under test and collectingthe n tuple occurrence counts.
we can monitor the application under real usage or actively run it with a random test generator and keep only successful traces.
the statistics can be measured by analysing execution traces or logs.
a third alternative is to run a simulator of the user interacting with the application under test so as to collect a large amount of data to be used for n gram statistics computation.
depending on the application domain technology deployment etc.
the most appropriate among these three data collection methods may di er.
it should be noticed that monitoring requires to store just the n gram statistics while tracing involves storing of full execution traces.
simulation is the least invasive technique but its applicability might be limited if the creation of a faithful and realistic simulator is not doable for a given application.
independently of the statistics collection method a key question is when enough data has been collected for reliable n gram estimate.
to this aim the convergence metrics commonly used in statistics can be resorted to.
for instance the distance between probability estimates before and after adding a new trace can be compared against a convergence threshold to decide if more observations are needed to converge to a reliable n gram statistics.
.
n grams proc ngramvisit n node p path do if randprob recurse prob then se ngramchoose succ su x p n p ngramvisit s p se if pincreases adequacy then addtotestsuite p od.
main while adequacy criterion not satis ed do p ngramvisit startnode hi od figure test sequence derivation by n gram probability figure shows the pseudocode of the n gram test sequence generation strategy in the following by ngram2 ngram3 etc.
we indicate that n the size of the tuples considered in the n gram statistics is respectively etc.
.
the procedure is similar to the random visit shown in figure the key di erence being the way in which the next event to add to the event sequence is chosen.
instead of choosing which successor node to add randomly i.e.
according to a uniform probability distribution the successor to add is chosen in accordance with the conditioned probabilities of the next events given the last n events in the current execution path p e n e ngramchoose s he1 e n 1i se2s with prob p eje1 e n the conditioned probabilities p eje1 e n are estimated from the frequency of occurrence of the n tuples he e1 e n 1i .
.
.
he k e1 e n 1i.
speci cally the564choice among e e k of the next event to add to the event sequence has a probability which is proportional to the frequency of occurrence of the respective tuples he e1 en 1i .
.
.
he k e1 e n 1i an example is presented later in figure .
when non tuplehe e e n 1iappears in the available n gram statistics e.g.
none is present in the execution traces used to compute the n gram statistics the next eventeis selected randomly among the possible transitions outgoing from the current node n i.e.
p eje1 e n is zero for all successor events e hence providing no help for the selection operated by ngramchoose .
in such cases the ngram strategy degenerates to the random strategy.
this is expected to occur at increased frequency when ntakes higher and higher values since only a small fraction of the possiblen tuples will be represented in the observed traces.
we can thus predict that the performance of ngram will converge to the performance of rand as the value of the parameternincreases.
.
interpolated n grams the basic n gram test sequence derivation algorithm shown in figure is quite sensitive to the choice of the value of n. if a low value is chosen for n the n gram statistics will be quite reliable being supported by a relatively large number of occurrences of short n grams.
on the other hand the context captured by a low nmight be too limited to capture the feasibility constraints necessary to generate only feasible sequences.
when the dependencies between events span a longer interval than n they will not be captured in the n gram statistics with low n and infeasible sequences may be generated still with high probability.
choosing a high value for nensures that a long enough context is captured in the n gram statistics and that dependencies between events that have a relatively big temporal separation are taken into account hence increasing the chance of generating only feasible sequences.
however for a given set of traces the number of instances of n grams becomes exponentially smaller when compared to the total number of possible n grams as nincreases.
this means that the chance of nding at least one n tuplehe e e n 1i represented in the n gram statistics decreases exponentially withn.
when no such n tuple is found the ngramchoose procedure of the ngram algorithm performs a uniformly random selection becoming indistinguishable from the random visit algorithm rand.
interpolated n gram statistics are computed by taking advantage of long n tuples when these exist i.e.
they have non zero frequency of occurrence and by falling back to a smaller value of nwhen n gram statistics are not available for larger n. the idea is to interpolate the conditioned probabilities of the next event egiven the preceding k events with kvarying between and some big value n p eje1 e2 e k withk2 .
for some k probabilities may still be zero when no n gram statistics is recorded for the tuple he e e2 e ki but the interpolated value computed by varying kbetween and n is very unlikely to be zero.
interpolated probability values are computed as follows.
for eachkbetween and n conditioned probabilitiesp eje1 e2 e k are computed some may be zero and are then multiplied by a weight w k which increases with k. speci cally we have chosen an exponentially increasingfunction for the weight w k 2k so as to give substantially more importance to longer contexts when these exist.
the result is a weighted conditioned probability number formally no longer a probability wk wk eje1 e2 e k 2kp eje1 e2 e k weighted probability numbers are then summed up to interpolate their values and normalised to respect the constraint that they must sum up to as required by probability distributions resulting in the interpolated conditioned probabilities p of the next event egiven previous events he1 e n 1i s eje1 e2 e n n 1x k 1wk eje1 e2 e k p eje1 e2 e n s eje1 e2 e n where is a normalisation factor computed by summing the valuess associated with all possible successor events eof the current fsm node n given the previous execution path p e n e x e2succevents n s eje1 e2 e n by assigning an exponentially increasing weight to longer tuples of previous events n grams with big values of nare given a large weight in the computation of the probability of the next event in the test sequence.
however when no such long n gram exists i.e.
p eje1 e2 e k is zero for large k the n gram statistics available for smaller kis used instead of resorting to a uniform selection as done by the ngram algorithm .
in fact if wkis zero for a large k because the associated n gram probability is zero only the values of wk computed for lower kwill contribute to the computation of s see equation and of the interpolated probabilities p see equation .
the interpolated n gram test sequence derivation algorithm interp is the same as the ngram algorithm shown in figure the only di erence being the probabilities used by ngramchoose .
given the interpolated n gram statistics the probability of the next event to add to the test sequence will be p instead ofp so that the chance of resorting to a random selection of the next event becausepis zero for the current path su x of length n is drastically reduced.
in the following we indicate by interp3 interp4 .
.
.
interp10 the variants of the interpolated n gram algorithm with n interp2 is the same as ngram2 hence it will not be considered .
figure shows an example of computation of the interpolated n gram probabilities p .
given the n gram statistics at the top left with the occurrence counts provided in the second column let us consider two execution paths p1andp2 see top right in the gure both leading to the fsm state nwhich has two successor events e d. algorithm ngram2 will compute the probabilities of the next events e dbased on the occurrence counts of the bigrams hc eiandhc di respectively and since only the last event in the executions paths p1andp2is used by ngram2.
the resulting probabilities for both paths are .
and .
for eanddrespectively.
when applying ngram3 the existence of some565traces count a b c d a b c e h g c e x y w d 5p1 ha b ci p2 hx y ci succevents n fe dg ngram2 ngram3 interp3 e d e d e d a b c ?
.
.
.
.
.
.
x y c ?
.
.
.
.
figure example of conditioned probabilities computed by ngram and interp occurrences of the trigrams hb c eiandhb c diallows for the computation of the conditioned probabilities for the execution path p1 see figure columns under ngram3 while no trigram hy c eiandhy c diis present resulting in zero probabilities for p2 which means the algorithm will behave as rand and will select between dandewith equal probability .
.
when interp3 is applied a higher weight equal to is given to the probabilities computed by ngram3 for p1.
half of this weight i.e.
is given to the probabilities of ngram2.
the weighted sum of the probabilities is .
.
.
for eand .
.
.
for d which once normalised give the probabilities shown in figure under interp3.
on the other hand no probability is available under ngram3 for path p2and correspondingly the probabilities computed by interp3 are the same as those of ngram2.
this example shows some nice properties of interpolated n grams.
when n grams of variable length are available for a given execution path e.g.
p1 the longer n grams are given a higher weight correspondingly the resulting probabilities under interp3 are closer to ngram3 than to ngram2 .
when only shorter n grams are available instead of resorting to a uniform random selection of the next event interp3 makes use of the shorter n gram statistics which is available for p2 this is the ngram2 statistics .
.
experimental results we have conducted a set of experiments on ve subject applications to answer the following research questions rq1 feasibility how many feasible test sequences are generated by the interpolated n gram the noninterpolated n gram and by the baseline graph visit strategies?
rq2 coverage what level of transition coverage is achieved by the interpolated n gram the noninterpolated n gram and by the baseline graph visit strategies?
rq3 test suite size how many test sequences are generated by the interpolated n gram the noninterpolated n gram and by the baseline graph visit strategies?
rq4 test case length what is the length of the test sequences generated by the interpolated n gram the non interpolated n gram and by the baseline graph visit strategies?
the rst two research questions are key to validate the proposed approach.
we conjecture that interpolated ngram based test sequence generation will produce less infeasible test sequences hence higher coverage than noninterpolated n gram and graph visit model traversal.
with these two research questions we want to assess whether our conjecture is con rmed or not by the experimental data.
the last two research questions deal with some interesting properties of the automatically generated test suites namely the number of test sequences they contain and their length.
to make the comparison fair we adopt the same adequacy criterion with all alternative test sequence generation strategies transition coverage the most widely used coverage criterion in model based testing.
all test suites produced by the various strategies will be transition coverage adequate and will contain only test cases that contribute to increasing such adequacy level see algorithms in figures .
it is the presence of infeasible test sequences that might diminish the actual level of transition coverage from reached by design under the assumption of full feasibility to some lower level measured to answer rq2.
.
metrics to address the four research questions listed above we have collected the following metrics feas rq1 ratio between feasible test sequences and total number of test sequences generated by each test strategy.
cov rq2 ratio between covered transitions and total number of transitions in the model.
sz rq3 number of test sequences in the test suite.
len rq4 average number of events in the test sequences added to each test suite.
while metrics cov sz and len can be easily measured automatically using tools metrics feas requires human judgment since it is not possible to automatically decide if a test sequence is feasible.
we manually de ned a set of constraints for the subject applications to characterise the event sequences that can be legally submitted to and executed by the system under test.
such constraints are needed in this experiment to measure feas but are not necessary when applying the proposed technique in a real setting.
.
subjects the ve subject applications used in our empirical study have been selected by looking for open source web applications used in previous web testing research with a rich and stateful client which makes them appropriate for state based modelling.
the features of the ve subjects are listed in table .
their source code includes php java javascript actionscript jsp xml and css les whose approximate size varies between .
and kloc thousands lines of code as reported by the unix utility wc .
flexstore1is an online shopping application developed by adobe and made available from the company s web site to ex samples exstore v2.html566table features of the subject applications subject loc nodes trans traces flexstore .5k cyclos 75k theorganizer 4k taskfreak 35k hitlist .2k demonstrate the capabilities of their testing framework.
its client side is developed in flex actionscript and run by the flash plug in.
the application allows the user to browse a catalog of mobile phones and to focus on a subset of models by means of lters such as price range camera tri band and video availability.
the customer can select one or more models to perform comparisons among features.
eventually customers can put one or more phones in their shopping cart.
cyclos2is a popular open source java servlet web application supporting e commerce and online payment.
the client side runs ajax javascript code.
its main features include banking e.g.
payments loans brokering e commerce e.g.
advertising member payments and many others e.g.
access control management .
cyclos is a quite large system.
in our experiment we focused only on the payment functionality of cyclos.
theorganizer3is a web application that supports management and organisation of the activities in a personal agenda.
the server is written in java using servlets j2ee and spring jdbc .
the client is based on javascript ajax.
once authenticated the user can add remove and edit tasks appointments contacts and notes.
taskfreak4is a project management web application.
it is written in php and its client is based on the ajax technology.
the application supports the full life cycle of a project including its initial creation the de nition of its tasks including deadlines priority and context and their assignment to users.
hitlist5is an ajax rich php based contact manager.
it supports creation and editing of contact lists including advanced search and mark e.g.
star un star functionalities.
all subjects are being or have been used in previous empirical studies on web testing.
flexstore is being used as a case study of the european union fp7 project fittest6 cyclos has been used by nguyen et al.
in their work on the combination of combinatorial and model based testing theorganizer taskfreak and hitlist have been used by mesbah et al.
in their work on ajax testing.
we have obtained execution traces for the subject applications either by manually randomly navigating and exercising the various application features or by running a carefully crafted user simulator see figure .
speci cally manual navigation was used with flexstore and hitlist a simulator was run with theorganizer and taskfreak and nally random executions were used with cyclos among them only traces of successful non error runs were kept .
correspondingly a comparatively low number of traces was collected ttest for flexstore and hitlist with respect to the other three applications see table which allowed us to investigate the robustness of the proposed approach when the number of available traces varies a lot.
manual navigation was conducted according to a high level functional coverage criterion all functionalities provided in the top level application menus have been executed with input data that a user would be typically expected to type.
simulators have been written by one of the authors after thorough code inspection and dynamic analysis so as to properly include all validation checks and dependencies enforced by the original applications.
probabilities of user interaction along speci c navigation paths have been estimated from real navigations.
the ve applications have been instrumented so as to support trace collection.
we have obtained a fsm model of each application by applying state based abstraction to the execution traces and by manually re ning the resulting models see figure .
speci cally we have used the model inference component of the fittest integrated testing environment ite .
since this tool is based on dynamic analysis which is by definition incomplete a manual re nement phase conducted by one of the authors was included to make sure that the model is a faithful and complete abstraction of the modelled application.
examples of manual re nement actions include the addition of missing transitions or the split merge of different equivalent states.
re nement was based on thorough code inspection and on application execution under scenarios speci cally selected for model validation.
the size of the client side models of these ve applications ranges between and nodes and transitions see table .
in our experience these numbers are quite representative of typical client side models of web applications.
the number of paths in these models is unbounded because of loops and the derivation of e ective test sequences from the models is a non trivial task if done manually because of the combinatorial number of possible paths and of the feasibility constraints enforced by the running application both on the server and on the client side .
table portion of the state abstraction function used to infer the model of theorganizer state variable type abstract values dateselector string empty nonempty dayataglance string empty nonempty notes string empty nonempty tasks string empty nonempty contacts string empty nonempty appointments string empty nonempty formattribs number zero greaterthanzero formtdcounts number zero greaterthanzero table shows a portion of the state abstraction function used to automatically infer the model of theorganizer from its execution traces.
in this abstraction string variables dateselector dayataglance etc.
are assigned to two equivalence classes empty andnonempty .
this means that the exact content of the strings is not taken into account by this abstraction function and the only information being used is the string length being zero or greater than zero .
in a similar way numeric variables e.g.
formtdcounts are567grouped into two classes depending on the stored value being zero or greater than zero.
figure portion of the fsm model of theorganizer a portion of the model inferred from the execution traces of theorganizer is shown in figure .
from state s4 it is possible to either click on notes and navigate to a page that shows all notes stored so far in the application or it is possible to create a new note.
in state s9 the application displays all previously inserted notes.
from this state it is possible to edit an existing note or to add a new note.
both actions lead to state s11 where the subject and the text of the note can be edited inserted.
note submission leads to state s17 and then back to s9.
logo is possible in state s9.
the small portion of fsm model shown in figure contains some examples of infeasible paths.
if state s11 is entered after clicking on newnote the user interactions to type the new note textandsubject are mandatory before the new note can be successfully submitted to the application.
if not done the application does not leave state s11 and an error message is displayed to the user.
on the contrary if this state is entered after clicking on edit the form elds containing the note subject and textare already not empty which allows the user to perform a submission moving to state s17 even without typing anything into the subject or text eld of the form or possibly typing into just one of the two elds .
this constraint is not represented explicitly in the model hence traditional test sequence generators dfv bfv rand cannot take it into account.
the proposed approach ngram and interp will approximate such constraint through the n gram statistics and will be more likely to respect it during test sequence derivation.
.
procedure the test sequence generation algorithms dfv bfv rand ngram and interp have been applied to the models of the ve subject applications.
ngram and interp have been applied with increasing values of n until the algorithm performance started to decline or reached a plateau.
with ngram this happens typically at n with interp atn .
the n gram statistics was computed using the same traces used for model inference.
since all algorithmsare non deterministic each of them has been run times.
results are averaged over these runs.
in all runs each algorithm was executed with transition coverage set as the adequacy criterion to satisfy.
metrics sz and len are obtained by measuring the number of test cases and the number of events per test case in each test suite.
metrics cov has been obtained by means of a tool that visits the fsm models of the subject applications based on the feasible input sequences in each test suite keeping track of the covered transitions.
metrics feas has been measured based on a set of manually de ned constraints that are satis ed only by valid sequences of actions that can be successfully executed on the subject applications.
test cases which contain event sequences violating such constraints have been marked as infeasible and have been subtracted from the count of the feasible sequences.
they also do not contribute to metrics cov.
an example of constraint that holds for theorganizer has been provided in the previous section.
another example of constraint manually de ned for cyclos is amount immediate single multiple count clicksubmitbutton it is possible to submit a payment if the amount is greater than zero and the payment is immediate or single.
multiple payments require in addition that the recurrence count is greater than zero.
.
results all experimental data execution traces models test suites are available to researchers at to support replication of the study and comparison with alternative approaches.
the tool ngram mbt to derive the test suites from a model using dfv bfv rand ngram and interp is also available online at the values of the four metrics collected in the experiments are reported for the ve subject applications in tables .
the highest values of feas and cov are shown in boldface.
they have always been obtained by algorithm interp in one case cyclos ngram2 produced the same values .
such maximum feas and cov values have been compared for statistical signi cance against the highest values shown underlined in the tables produced by any of the graph visit algorithms dfv bfv or rand .
we used the non parametric wilcoxon statistical test to assess whether the di erence between the means is statistically signi cant setting the signi cance threshold to .
.
onflexstore interp stabilises around the highest feas performance when nis or more while a very high cov value is reached early even at the initial value of n .
rand and dfv have very poor performance.
bfv is better but still it is not comparable with interp.
ngram has also inferior performance.
moreover such performance is quite sensitive to the choice of n. ngram s performance is maximum at n and it starts to decline toward rand withn .
excluding bfv which produces test suites with many short test cases the other test suites contain between and test cases with a test case length between and events.
oncyclos ngram2 and interp for any value of n have the best performance both in terms of feas and568table results obtained for the subject applications flexstore cyclos theorganizer flexstore feas cov sz len dfv .
.
.
.
bfv .
.
.
.
rand .
.
.
.
ngram2 .
.
.
.
ngram3 .
.
.
.
ngram4 .
.
.
.
interp3 .
.
.
.
interp4 .
.
.
.
interp5 .
.
.
.
interp6 .
.
.
.
interp7 .
.
.
.
interp8 .
.
.
.
interp9 .
.
.
.
interp10 .
.
.
.
cyclos feas cov sz len dfv .
.
.
.
bfv .
.
.
.
rand .
.
.
.
ngram2 .
.
.
.
ngram3 .
.
.
.
ngram4 .
.
.
.
interp3 .
.
.
.
interp4 .
.
.
.
interp5 .
.
.
.
interp6 .
.
.
.
theorganizer feas cov sz len dfv .
.
.
.
bfv .
.
.
.
rand .
.
.
.
ngram2 .
.
.
.
ngram3 .
.
.
.
ngram4 .
.
.
.
interp3 .
.
.
.
interp4 .
.
.
.
interp5 .
.
.
.
interp6 .
.
.
.
interp7 .
.
.
.
interp8 .
.
.
.
interp9 .
.
.
.
interp10 .
.
.
.
cov.
ngram2 and interp achieve the maximum possible feas score .
the reason being that in this application bi grams subsume all feasibility constraints.
coverage reaches also the maximum possible value since two transitions in the model are infeasible which makes the maximum achievable coverage equal to .
.
with tri grams ngram3 several bi gram constraints are still met but there are cases where no tri gram is applicable because there is no such triple in the collected execution traces which results in a random selection of the next event.
this explains the lower performance of ngram3.
in contrast interp interpolates bi grams for all values of n resulting in a stable feas and cov performance which remains consistently the highest.
graph visit methods have substantially lower performance.
if we exclude bfv the test suite size and the test case length do not show high variability in this application being roughly between and both sz and len .table results obtained for the subject applications taskfreak hitlist taskfreak feas cov sz len dfv .
.
.
.
bfv .
.
.
.
rand .
.
.
.
ngram2 .
.
.
.
ngram3 .
.
.
.
ngram4 .
.
.
.
interp3 .
.
.
.
interp4 .
.
.
.
interp5 .
.
.
.
interp6 .
.
.
.
interp7 .
.
.
.
interp8 .
.
.
.
interp9 .
.
.
.
interp10 .
.
.
.
hitlist feas cov sz len dfv .
.
.
.
bfv .
.
.
.
rand .
.
.
.
ngram2 .
.
.
.
ngram3 .
.
.
.
ngram4 .
.
.
.
interp3 .
.
.
.
interp4 .
.
.
.
interp5 .
.
.
.
interp6 .
.
.
.
interp7 .
.
.
.
interp8 .
.
.
.
interp9 .
.
.
.
interp10 .
.
.
.
ontheorganizer there is a dramatic gap between graph visit strategies and n gram based algorithms with the former exhibiting extremely poor performance as compared to the latter.
the reason is that in this application a speci c authentication protocol must be followed initially to leave the start state and to exercise the application in depth.
at the same time such protocol is not enforced explicitly in the model.
as a result test sequences that do not take into account any interaction ordering constraint remain stuck in the initial application state.
thanks to the availability of the n gram statistics interp can easily escape such trap state and can exercise the subject application in depth achieving high feasibility and coverage.
for interp metrics cov is high even at n while feas tends to increase at a quite constant rate while nincreases.
ngram2 performs also pretty well but its performance degrades quickly at increasingn making it quite di cult to select the right n for a given application in fact n works well for flexstore while for cyclos and theorganizer n is much better .
test suite size and test case length bfv excluded are respectively between and and between and .
ontaskfreak interp achieves the highest feas and cov values but bfv is not far from such performance.
the absolute di erence between interp7 and bfv is small .
for feas and .
for cov .
however such small difference is statistically signi cant at level .
according to the wilcoxon test.
this means that such small di erence is not due to uctuations associated with the non deterministic nature of the algorithms.
on the contrary interp has consistently a slightly higher performance.
even though569bfv might be considered perfectly acceptable in this case it exhibits so much variability across applications that its use cannot be recommended in general.
on the contrary this does not happen with interp.
on this application the performance of ngram is quite high but still substantially lower than interp.
if we exclude bfv there is not much variability of the test suite size sz between and while the test case length has a minimum with dfv and then ranges between and .
onhitlist the highest performance is reached by interp7 with a large margin over the best graph visit performance dfv for feas and rand for cov .
ngram2 has also good performance in terms of coverage but as n varies the performance of ngram changes substantially while that of interp remains stable and predictable.
it is interesting to notice that despite the extremely low number of traces used to learn the n gram statistics the performance of ngram and interp are de nitely superior to those of the other algorithms showing that the n gram based approach is robust also with respect to the number of traces used to compute the n gram statistics.
moreover despite the low number of traces used to compute the ngram statistics most of the test cases generated by ngram .
and by interp .
di er from the event sequences in the initial ve traces ngram interp regenerate just one of the ve initial test sequences among the test sequences generated in total .
excluding bfv and dfv the test suite size is around test cases each with a length between and events.
all di erences between boldface and underlined values in tables have been found to be statistically signi cant al level according to the wilcoxon two sided nonparametric statistical test.
based on the results presented above we can positively answer rq1 andrq2 interpolated n gram based test sequence generation produces a higher proportion of feasible event sequences and achieves higher coverage than plain ngram and graph visit approaches .
in terms of test suite size and test case length rq3 rq4 we can notice that bfv produces generally test suites with a lot of very short test cases see tables .
the other methods are not very di erent from each other.
interp produced slightly longer and more numerous test cases than the other techniques on flexstore while on the other subjects there is no remarkable di erence between interp ngram and rand in terms of test suite size and test case length.
in summary the test sequences generated by means of interpolated n grams achieve the highest feasibility and coverage scores outperforming all alternative test case derivation techniques.
moreover the performance of interpolated ngrams is stable across applications and for various choices of the parameter n which is not true for the graph visit and plain n grams algorithms.
interpolated n grams have stable performance also when the number of traces available is reduced dramatically.
the interpolated n grams test suite size and test case length are in line with those of the other techniques and look generally acceptable for the tester.
the extra cost associated with the n gram method is limited to the collection of the data necessary to reliably compute the n gram statistics.
this cost is application dependent but it was always acceptable in the ve cases considered in this experiment.the interpolated n grams approach has stable performance across subject applications number of training traces and value of parameter n and achieves the highest feasibility and coverage scores.
.
qualitative analysis theorganizer let us consider the transition s11 s17 in the model of theorganizer see figure .
to cover this transition note textandsubject must be lled in before submitting the form if the note is a new one while there is no pre condition to be satis ed for edited notes.
if we look at the ngram2 statistics we can notice that event click newnote is never followed directly by click submit.
hence the probability of generating a test case that creates a new note and submits it without lling in textor subject is zero.
on the contrary graph visit algorithms may non deterministically choose such event sequence.
actually in the ngram2 statistics event type notecreateshow text follows click newnote with probability one so it will be always chosen as successor of click newnote .
in the ngram2 statistics event type notecreateshow text is followed by type notecreateshow subject with probability while it is followed by click submit with probability .
in fact the ngram2 statistics does not have enough context to distinguish the case where type notecreateshow text is preceded by click newnote from the case where it is preceded by click edit .
to capture such longer dependency the ngram3 statistics is needed.
in the ngram3 statistics the two events click newnote type notecreateshow text are never followed by click submit hence event click submit has zero probability in the ngram3 test sequences if preceded by new note creation and texttyping without subject typing .
in this case a context of length two i.e.
the tuples statistics is enough to choose only feasible successors of the current state.
in other cases e.g.
new account creation a longer context is needed.
let us consider the length context i.e.
the ngram4 statistics needed to handle correctly the feasible sequences involved in new account creation.
in the ngram4 statistics the pre x clickandwait image clickandwait image click newnote has zero occurrence count.
as a consequence when this path pre x occurs the next event being generated by ngram4 is chosen with uniform probability among the three possible successors see state s11 in figure .
this means that there is a .
chance of generating an infeasible sequence by choosing click submit as successor event .
on the contrary when interp4 is applied since the current length path pre x has zero probability it does not contribute to the selection of the successor events.
both the length path pre x clickandwait image click newnote and the length path pre x click newnote are followed by type notecreateshow text with probability .
correspondingly with this path pre x interp4 generates only feasible event sequences i.e.
interp4 chooses type notecreateshow text with probability while ngram4 has .
probability of generating an infeasible event sequence.
.
threats to validity the main threats that a ect the validity of the empirical study described above are the authors bias and the external validity threats.570authors bias the authors have been involved in a number of tasks carried out to complete the experiment and the way in which such tasks were actually conducted might have in uenced the results.
speci cally the authors have collected the traces used in the experiment have re ned the models inferred from the traces and have manually de ned the feasibility constraints.
some trace collection was carried out either manually or through a simulator.
when done manually it was executed with functional coverage in mind so as to have a clear goal to achieve in terms of what execution scenarios to exercise and when to stop tracing.
when done through simulation the simulator code was carefully developed so as to take into account all validity checks implemented in the real applications.
realistic probabilities obtained from observations of real executions are used in the simulator to decide which interaction the simulated user will activate in each application state.
manual model re nement was guided by the presence of mismatches between the modelled behaviours and the actual ones as apparent from code inspection and execution.
feasibility constraints have been obtained through code inspection and by executing the application to see what validation checks are performed at runtime.
since the connection between the manual activities conducted by the authors and the performance of ngram and interp is quite loose and di cult to predict a priori we think that our expectations on the performance of the proposed approach had no relevant in uence on the manual activities.
however we recognise that replication in a setting where the authors are not directly involved in any activity would be bene cial to con rm our ndings.
external validity we have validated our approach on ve web based applications for which the client side behaviour was abstracted into a model used for test sequence derivation.
moreover the model was rst inferred from execution traces and then re ned manually.
we expect similar results to hold for applications in the same domain web for which a similar modelling is carried out although we recognise that ve is a small number and additional subjects would be needed to corroborate our ndings even in this speci c domain.
generalisation beyond the domain of web applications and beyond the kind of models that we used should be done with extreme care.
for that additional experiments are de nitely required.
.
related works model based testing is a research area that amounts to a huge literature which was surveyed among others in the paper by dias neto et al.
.
it has been applied in a number of di erent domains and contexts .
it is possible to distinguish between design time models and inferred models.
in case of a design time model abstract test cases can be produced to check how the expected behaviours coming from the model have been implemented in the application .
hence the role of behaviour speci cations is to provide abstract test cases test oracles and a measure of the test adequacy .
instead the role of an inferred model is mainly to provide abstract test cases e.g.
to be instantiated in executable test cases by adding inputs and eventually test oracles e.g.
.
in both cases instantiation of abstract test cases derived from models is an expensive activity and the presence of infeasible test cases demands for substantial e ort and knowledge from the tester s side who is required to recogniseinfeasible sequences and to lter them out or replace them if the target level of adequacy is to be preserved .
while approaches exist which try to balance the degree of over under approximation of inferred models and metrics have been proposed to evaluate the generalisation capability of inferred models to the best of our knowledge this is the rst work which tries to address the problem of test case infeasibility by adopting a test case derivation strategy based on the n gram statistics that aims at increasing the likelihood of feasibility and correspondingly the level of coverage actually achieved.
this paper extends our previous workshop paper with a novel algorithm interp which has been shown to outperform the previous one ngram and to exhibit a more stable and predictable high performance level.
we also extended the empirical evaluation done for the workshop paper with three additional subjects.
.
conclusions and future work we have proposed a technique for model based test case derivation which takes advantage of the interpolated n gram statistics to address the infeasibility problem of model based testing.
by selecting the next events for a test sequence according to the n gram statistics we increase the likelihood of generating a feasible sequence.
correspondingly we also increase the level of model coverage achieved.
interpolation of the n gram statistics is required as n increases to compensate for missing n tuples at large values of n. we have implemented the proposed technique in the tool ngram mbt which is publicly available as open source code.
the experimental results including models traces and test suites obtained on ve public domain applications are also publicly available for future empirical research in the eld.
results indicate that test cases generated according to the interpolated n gram statistics outperform plain n grams and graph visit methods for model based test case derivation.
moreover the interpolated n grams method has consistently high performance across di erent applications and does not depend critically on the choice of the parameter n. this method is also robust with respect to the number of traces available.
our future work will be devoted to further experimentation of the proposed method on additional subjects and with alternative interpolation schemes.
in particular applicability to domains di erent from web applications and to models not necessarily inferred from execution traces using state abstraction would increase the generality of the obtained results and would corroborate our ndings.
.