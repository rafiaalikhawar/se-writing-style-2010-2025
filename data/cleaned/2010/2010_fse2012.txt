mining the execution history of a software system to infer the best time for its adaptation kyle r. canavera computer science dept.
george mason university kcanaver gmu.edunaeem esfahani computer science dept.
george mason university nesfaha2 gmu.edusam malek computer science dept.
george mason university smalek gmu.edu abstract an important challenge in dynamic adaptation of a software system is to prevent inconsistencies failures and dis ruptions in its operations during and after change.
several priortechniqueshavesolvedthisproblemwithvarioustradeoffs.
all of them however assume the availability of de tailed component dependency models.
this paper presentsa complementary technique that solves this problem in set tings where such models are either not available difficult to build or outdated due to the evolution of the software.
ourapproach first mines the execution history of a software system to infer a stochastic component dependency model r e p resenting the probabilistic sequence of interactions among the system s components.
we then demonstrate how this model could be used at runtime to infer the best time foradaptationofthesystem scomponents.
wehavethoroughlyevaluated this research on a multi user real world softwaresystem and under varying conditions.
categories and subject descriptors d. .
software architectures general terms algorithms design keywords datamining dynamicadaptation componentdependency .
introduction as engineers have developed new techniques to address the complexity associated with the construction of modernday software systems an equally pressing need has risen for mechanisms that automate and simplify the management ofthose systems after they are deployed i.e.
during runtime.this has called for the development of self adaptive soft ware systems .
however the construction of such systems has been shown to be significantly more challenging than traditional software systems .
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copiesbear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
sigsoft fse november cary north carolina usa.
copyright acm ... .
.one important challenge is the management of the runtime change to avoid inconsistencies during and after the adaptation.
informally an inconsistent application state is one from which the system progresses towards an error state .
in a component based software system application transactions change the state of the system.
an application transaction is defined as a set of related interactions among two or more software components.
the importantobservation is that while a transaction is in progress the internal state of the participating components may be mutually inconsistent .
to avoid inconsistencies replacementof components should be delayed until the transaction hasended and the participating components have a stable state.
in their seminal work kramer and magee developed a technique known as quiescence that from a static component dependency model of the system e.g.
uml component diagram calculates the components that have tobe halted passivated before a component can be safely adapted.
relianceonastaticcomponentdependencymodel however makes quiescence rather pessimistic in its analy sis which could lead to significant delays and disruptions.this is an issue that has been tackled in two recent approaches tranquility and version consistency which have showed that by leveraging the dynamic component dependency model of the system e.g.
uml sequence diagram it is possible to become more refined in the analysis and thus reduce the unnecessary overhead.
all of these approaches however assume the availability of an accurate component dependency model of the system.while this may be true in some cases often such modelsare either not available or provide an inaccurate representation of the system s evolving dependencies.
for instance consider that the majority of existing open source softwaresystems lack such models and when not the models arenot necessarily up to date with the system s implementa tion.
moreover in emerging software systems such as those comprised of externally provided services the dependencies among the system s components are constantly changing making it difficult to maintain such models.
inthispaper wepresentanovelapproachthatdetermines the best time for adapting a system s software componentsin settings where an accurate model of their dependenciesis not available.
we define best time to be the time atwhich the adaptation of a given component results in nei ther inconsistency failure nor significant disruption to the system.
the underlying insight is that by collecting a software system s execution history for a sufficiently long periodof time it is possible to mine a stochastic component de1pendency model of the system.
this model provides a new kind of probabilisticinformation that has been lacking in the models used for making adaptation decisions in the prior research .
we first leverage data mining techniques toinfer a set of probabilistic rules representing the dynamiccomponent dependencies among a system s software compo nents.
the rules are then used at runtime for determining the likelihood of a component being in an appropriate state for adaptation at a given point in time.
finally by checkingour predictions against the actual behavior of the system we are able to continuously refine the dependency models to the system s evolving interactions.
ourexperienceswiththoroughevaluationofthisapproach in the context of a large distributed software system have been very positive.
the results have shown the ability to in fer precise models that can be used to effectively manage the interruptions caused by adaptation.
we have also developed and evaluated a novel technique that prevents inconsisten cies even when our predictions are off.
the remainder of this paper is organized as follows.
section describes a software system used for illustration ofthe research and its evaluation.
section provides the nec essary background while section motivates the researchin the context of prior work.
section provides an overviewof our approach.
sections to delve into the details.
section presents the evaluation.
the paper concludes with an overview of prior research and avenues of future research.
.
illustrative example we illustrate the concepts using a software system called emergencydeploymentsystem eds andintendedforthe deployment and management of personnel in emergency response scenarios.
figure depicts a subset of eds s software architecture and in particular shows the dependencyrelationships among its components.
eds is used to accomplish four main tasks track the resourcesusingresourcemonitor distributeresourcesto the rescue teams using resource manager analyze different deployment strategies using strategy analyzer andfinally find the required steps toward a selected strat egy using deployment advisor.
interested reader may find a more detailed description of eds in .
it suffices to figure subset of the emergency deployment system s software architecture.say that eds is representative of a large component basedsoftware system where the components communicate by exchanging messages events .
in the largest deployment of eds to date it was deployed on nodes and used bymore than users .
systems such as eds are often deployed in highly unpredictable and dynamic settings.
therefore it is often desirable to be able to adapt such systems at runtime to deal with changes that may affect the system s functional or non functional properties.
however such changes should occurin a manner that do not lead to inconsistency or significant disruption in the services provisioned to the users.
.
research background kramer and magee showed that for a component to remain in a consistent state during after adaptation it should not be changed in the middle of a transaction.
theydefined transaction to be exchange of event between two componentsbywhichthestateofacomponentisaffected.
adependent transaction isinturnatransactionwhosecompletion depends on the completion of consequent transactions.
we first formally define and then illustrate these concepts using a subset of transactions comprising eds below.
fig ure shows the transactions corresponding to the strategy analysis capability which is only one of the use cases in eds.
anevent eisdefinedasatripletuple e s r c d s t t i m e where srcanddstare identifiers for the source and destination components and timeis the timestamp of its occurrence.
although an event is also likely to have a payload it is not relevant to this line of research and thus not mod eled.
in the eds example of figure events e e12 a r e depicted.
in this area of research it is assumed that events including their source and destination are observable.
a transaction tis defined as a triple tuple t s t a r t e n d r wherestartandendrespectively represent the events initiating and terminating the transaction t w h i l e ris a set of transactions that subsequently occur as a result of t.r negationslash when tis a dependent transaction e.g.
t t3 a n dt4in figure and r when tis an independent transaction e.g.
t2 t5 a n dt 6in figure .
atop level transaction tis a kind of transaction where there is no other transaction xin the system such that t x.r.
in other words a transaction is top level if its occurrence is not tied to other transactions in the system.
a figure transactions comprising strategy analysis scenario of eds.
2top level transaction corresponds to the system s use cases functional capabilities .
for instance t1in figure is a top level transaction initiated in response to e1 which represents the user requesting a service from the system.
replacing a component in the middle of a transaction could place the system in an inconsistent state.
considera situation in which strategy analyzer component of figure is replaced after sending request event e but before receiving the response event e8.
since the newly installed component does not have the same state as the old one itmay not be able to handle response e 8and subsequently initiate transaction t6via event e9 resulting in an inconsistency and potentially the system s failure.
even if the component is stateless inconsistency problems may arise.
consider a stateless compression component thatcompresses and decompresses data using two interfaces that are reverses of one another.
replacing this component with onethatusesadifferenttypeofcompressionalgorithminthemiddle of a transaction could break the system s function ality since the decompression cannot be performed on data that was compressed using the old component.
by the same reasoning state transfer in the case of stateful componentsis not sufficient to address inconsistency due to adaptation.
three general approaches to this problem have been proposed quiescence tranquility a n d version consistency.
quiescence is the established approach for safe adaptation of a system.
a component is in quiescence and can beadapted if it is not active meaning it is not participating in any transaction and all of the components that may initiate transactions requiring services of that component are passivated.
a component is passiveif it continues to receive and process transactions but does not initiate anynew ones.
at runtime the decision about which part of thesystem should be passivated is made using a static component dependency model such as that shown in figure .
for instance to change the mapcomponent on top of passivating itself weather analyzer strategy analysis kb hq ui simulation agent a n d resource manager components need to be passivated as well since those are the components thatmay initiate a transaction on map.
while quiescence provides consistency guarantees it is very pessimistic in its analysis and therefore sometimesvery disruptive.
consider that the static dependency model includes all possible dependencies among the system s components while at any point in the execution of a softwaresystem only some of those dependencies take effect.
to ad dress this issue tranquility proposes to use the dynamic component dependency model of a system in its analysis an example of which is shown in figure .
under tranquilitya component can be replaced within a transaction as long asit has not already participated in a transaction that it mayparticipate in again .
for instance under tranquility map c o u l db er e p l a c e de i t h e rb e f o r ei tr e c e i v e se v e n t e 2or after it sends event e7 but not in between.
a shortcoming of tranquility as realized in was lack of support for handling dependent transactions.
this issue was addressed in version consistency which guarantees ad e p e n d e n tt r a n s a c t i o ni ss e r v e db ye i t h e rt h eo l dv e r s i o nor new version of a component that is being changed.
.
motiv ation and objectives similar to the prior research we believe using static dependency models for achieving consistency to beoverly disruptive in most cases.
however unlike prior re search we do not assume the availability of dynamic com ponent dependency models e.g.
uml sequence diagram for the following reasons.
manually intensive dependency models are not always available and do not come for free.
to developthese models one has to understand the internal logicof components which is a manual cumbersome pro cess specially if the developer of those models is notpart of the team that implemented those components.
dynamism and evolution determining the dependencies prior to system s deployment in emerging and in creasinglydynamicparadigms suchasservice oriented and mobile domain is difficult.
as the system evolves the internal logic of its components changes makingthe manually constructed models inaccurate represen tations of the system which if used for making adap tation decisions may break the system s consistency.
therefore even when dependency models are available keeping them up to date is a challenge.
non determinism finally and perhaps most importantly component dependencies are often nondeterministic i.e.
a component depends on anothercomponent under some circumstances but not oth ers.
the model depicted in figure is deterministic since it assumes the transaction t 1always results in the same exact sequence of subsequent events and trans actions.
no prior research has developed mechanismsfor ensuring consistency and managing disruption in anon deterministic setting.
in this research we aim to infer the stochastic component dependency model of the system.
such a model not only infers the dynamic dependencies among the components i.e.
information equivalent to that captured in figure but italso provides a probabilistic measure of the certainty withwhich events and transactions may occur.
thus our approach does not compete with the prior research i.e.
tranquility and version consistency but rather paves the wayfor those techniques to be applicable in settings where dy namic dependency models are not available.
to keep our approach widely applicable we make minimal assumptions about the available information from theunderlyingsystem.
theseassumptionsarethesameasthosemade in the prior research .black box treatment we assume the software components implementation is not available.
this allowsour approach to be applicable to systems that utilize services or cots components whose source code is not available.
it also enables our approach to natu rally support the evolution of software components.
.observability of event w ea s s u m et h a te v e n t sm a r k ing the interactions among the system s componentsare observable.
an event could be either a messageexchange or a method call which could be monitoredvia the middleware facilities that host the components or instrumentation of the communication links.
.observability of transaction duration we assume eventsstartandend which as you may recall from section indicate beginning and termination of a transaction to be observable.
this is a reasonableassumption that has also been made by all prior re search .
for instance in the example of 3figure the hq uicomponent should be able to determine and record the occurrence of dependent transactiont1in terms of request e1 which corresponds to the user clicking on a button on the gui and its termination via response e12 which corresponds to the results to be displayed on the gui.
ourapproachmakesnofurtherpertinentassumptionsand requires no additional information from the system.
basedon this minimal information our objective is to infer thestochastic component dependency model of the system.
the crux of this is the ability to identify the causal relationship among the transactions.
in other words our objective is todetermine the set rfor every transaction occurring in the system recall the formal definition of transaction in sec tion .
this is a challenging problem to solve by simply monitoring the system given that there may be multiple concurrently running top level transactions at any point intime using the same set of components.
moreover compo nents in our approach could act non deterministically producing different behaviors under different conditions.
.
approach overview we present a novel approach for automatically deriving the stochastic component dependency model by mining the execution history of the software system.
the result of min ing is a set of rules expressing the probabilistic relationship among the occurrences of transactions in the system.
this set of rules represents our stochastic component dependencymodel.
given a set of active transactions in the system these rules can be used to predict the probability with whicha component canbe changed at a point in timewithout jeopardizing the system s functionality while minimizing the interruptions.
additionally by continuously monitoring thetransactions and the accuracy of predictions the approachprovides the means to adjust the rules as new patterns of interaction emerge.
figure provides an overview of our approach consisting of two complementary asynchronously running cycles mining rules andapplying rules.
the mining rules cycle starts by processing the event logof the system to construct a large number of itemsets.
an itemset indicates the events that occur close in time.itemsets are then passed through a data mining algorithmto derive transaction association rules tars relating the relationship between transactions that are occurring in thesystem and those that may happen in the future.
sincemining may generate a large number of rules some of whichmay be invalid and redundant we prunethe generated rules toarriveatasmallnumberofusefulrulesthatcanbeapplied efficiently at runtime.
the applying rules cycle starts with the track active transactions activity that monitors the currently running transactions in the system.
select relevant tars then uses the information about currently active transactions to pick a set of candidate tars from the rule base for estimating the usage probability of components.
update predictions uses candidate tars to update the u s a g ep r e d i c t i o nr e g istry which is a data structure that contains the up to date usage predictions for the components in the system.
the usage prediction for each component is the probability thatthe component will imminently be used as a result of thetransactions running in the system.
these predictions can be calculated either continuously or on an as needed basis.
figure approach overview.
finally as indicated by c h e c kp r e d i c t i o na c c u r a c y t h e predictions are scrutinized at runtime and if they go abovean unacceptable threshold a new round of mining based on the newly collected log of events is initiated.
this allowsthe approach to incorporate changes due to how the soft ware is used or its evolution into the mining process.
in thefollowing sections we describe the details of our approach.
.
mining rules this section describes the mining rules cycle recall figure .
this cycle runs asynchronously separate from the system s execution and potentially on a different platform.
it may repeat throughout the system s execution to adjustthe model to the evolving behavior of the software system.
.
event log mining operates on an event log of the system which represents an execution history of the system for a suffi ciently long period of time to be truly representative of howthe system is used.
clearly our approach is not applicableto systems where such a history cannot be collected or the system s past behavior is not indicative of its future but we believe most systems do not fall in this category.
since ourobjective is to infer the relationship among the transactions we would like mining to operate on a representation that isin terms of transactions as opposed to events.
as a result theevent log of the system is automatically processed to determine all of the transactions that have occurred by pairing thestartand theendevents for each transaction.
recall from section that consistent with the prior work weassumethesetypesofeventsareobservableandcould be used to identify the occurrence of transactions.
from thispoint forward we will mainly focus on transactions thoughthe reader should be aware of the relationship to the events.
.
constructing itemsets the first step to mining the relationship among the transactions is to construct itemsets see figure .
an itemset as in the data mining literature for association rule mining is a set of items that have occurred together.
in the context ofourresearch anitemset iisasetoftransactionsthathave occurred temporally close to one another at some particular point during the execution of the system i t t2 ... t n .
the transaction records for the execution history of the system are transformed into itemsets through a simple process.
a new itemset is formed for each top level transaction 4but not the transactions that those top level transactions initiate.
a top level transaction is automatically detected if its beginning end or both do not fall within the beginning and end of another transaction.
all other transactions areplaced in the itemsets for the transactions whose beginningand end times fully surround the beginning and end timesof the present transaction.
in reference to figure a new itemset would be created fort as its beginning and end determined by e1ande12 do not fall within any other transactions.
all the remainingtransactions t t3 t4 t5 a n dt6are added to it1itemset as follows it1 t1 t2 t3 t4 t5 t6 .
using this process an entire segment of a software system s execution history can be transformed into a set ofitemsetsrepresentingtheoccurrenceoftransactionstogetherin time.
given a sufficiently large usage history the approachcompensatesforconcurrentlyrunningtop leveltransactions.
consider a version of the scenario depicted in fig ure in which a second top level transaction t 7overlapping partially in time with t1starts and itself initiates a transactiont8that falls wholly within the beginning and end times of both t1andt7.
the approach will include t8in both it1andit7.
however since transactions t1andt7are truly independent the false placement of t8init1is a random event that is not likely to occur in a significantly large number of itemsets and thus safely ignored by the data mining algorithm using minimum frequency thresholds.
.
deriving rules several data mining approaches can be used to performlearningonthesetofitemsetsconstructedthisway.
wefound the association rule mining class of algorithms to bethe most suitable for our purposes.
the output of an algorithm of this type for our problem is a set of transaction association rules tars .
tars are probabilistic rules for predicting the occurrence of transactions as follows x y p. a tar states that the occurrence of set of transactions x implies the occurrence of a set of transactions ywith probabilityp.
as shown in figure tars derived in this way are eventually stored in the rule base for use during the system s adaptation at runtime.
for association rule mining algorithms an appropriate value for pis the confidence of the implication x y. confidence is defined as p parenleftbigg summationdisplay si braceleftbigg 1i fx si y si 0o t h e r w i s e .
parenrightbigg slashbigg parenleftbigg summationdisplay si braceleftbigg 1i fx si 0o t h e r w i s e .
parenrightbigg confidence is an appropriate metric for pin tars because it provides a measure of the strength of the implicationx y. tars with strong relations between xandy have a high confidence value while tars with weak rela tions between xandyhave a low confidence value.
another metric that is commonly generated by data mining algorithms during the learning phase is support s parenleftbigg summationdisplay si braceleftbigg 1i fx si y si 0o t h e r w i s e .
parenrightbigg slashbigg numberofitemsets while support is not appropriate for the value of pin tars it is useful in that it provides a measure of the frequencywith which xandyoccur together.
as such we use a minimum support value during the mining phase in order to filter out rare relationships that represent outliers in thegeneral usage of the system.
thus the errors introduced in itemsets due to concurrent execution of transactions inthe system recall section .
can be filtered out effectively using a minimum support and confidence threshold.
while the mining algorithm in the derive rules activity produces logically accurate tars it typically produces an excessively large number of tars some of which are notuseful.
as such the generated rules must be pruned to make them suitable for use at runtime.
as shown in figure thederive rules step terminates by passing the raw set of generated tars to prune rules.
.
pruning the rule base an excessively large number of tars is produced as a result of the derive rules activity because we set the minimum confidence for a tar to be very small i.e.
we donot filter out many tars based on the confidence level.
wetake this approach contrary to many other applications of associate rule mining because while a tar having a small pexpresses less confidence in the prediction than does a different tar having a larger p both predictions are accurate and can be used in unison as explained in section .
.
in addition many of the unnecessary tars are produced because the data mining algorithm and its input i.e.
item sets do not fully incorporate all of the knowledge that wehave about the system.
for instance itemsets are unorderedand thereby the resulting tars incorporate no ordering information.
as a result the mining algorithm produces an excessively large number of tars that are not useful.
since we would like to use the rules at runtime we need to prune them to a subset of highly predictive rules that can beapplied efficiently at runtime.
to that end and as depicted in figure the derive rules step terminates by passing the raw set of generated tars to prune rules .
there are three highly effective heuristics that we have developed for pruning the tars.
redundant tar pruning heuristic consider tars satisfying this pattern tar x1 y1 p1 tar x2 y2 p2 where x2 x1 y1 y2 p1 p2 in this scenario tar 1andtar 2predict the same set of transactions and at the same level of confidence.
however the conditions for satisfying tar 2is a subset of those for tar i.e.
x2is a subset of x1.
as will be explained in section .
a tar s conditions are considered to be satisfied when the transactions comprising its left hand side have been observed.
therefore tar 1andtar 2predict t h es a m ee x a c to u t c o m e e x c e p ttar 2requires fewer conditions to be satisfied.
we can safely prune tar s i n c ei t is redundant.
less specific tar pruning heuristic consider tars satisfying this pattern tar x1 y1 p1 tar x2 y2 p2 tar x3 y3 p3 where x1 x2 x3 y1 y2 y3 in this scenario tar 1makes a composite prediction of tar 2andtar .
all three tars are satisfied with the observation of the same set of transactions x1 x2 x3.
however because y1 y2 y3 tar 1is a composite prediction of the more specific predictions made by tar 2and tar .
given the definition of confidence and its use as the 5prediction value p the prediction value p1fortar 1will always be weaker lower than the prediction values of p2and p3fortar 2andtar respectively.
as a result tar 1is a less specific rule and can be pruned.
misordered tar pruning heuristic wecanalsoprune rules by incorporating our knowledge of what constitutes a valid behavior.
we can prune tar x y p where x x y y x.start.src y.end.dst .
in this kind of tar one of the predicted transactions in yhas as its destination the source of one of the observed transactions in x. therefore the tar is useless because it predicts the use of a component that must have already been used.
it is important to note that while this type of tar seems illogical andperhaps presumptively unlikely to be generated the associ ation rule mining algorithm and its input i.e.
itemsets donot recognize any transaction ordering.
furthermore these types of tars can be highly predictive and are very common.
essentially they predict that the transaction necessaryfor another transaction to occur will in fact occur with thattransaction.
therefore this pruning step removes many useless rules and has the largest impact in our approach.
at the completion of this activity a small subset of generated rules remains which is stored in the rule base and used for runtime prediction of component usage.
.
applying rules in this section we describe the activities comprising the applying rules cycle from figure .
.
tracking active transactions track active transactions step processes any observed eventto.startandto.end indicating the beginning and termination of transaction to respectively.
to that end we use a data structure called top level tracker and represented as settlt for each top level transaction active i.e.
currently running in the system.
the purpose of tlts is to keep account of the present transaction activity in the system.
upon observing to.start the state of tlts is updated as follows.
if tois a top level transaction a new tlt is created.
but iftois not a top level transaction its identifier is added to all open tlts i.e.
tois associated with every top level transaction that may have caused it.
this is done because there is no way of knowing which top level transaction has actually initiated this transaction.
upon observing to.end iftois not a top level transaction it is ignored.
on the other hand if tois a top level transaction then the tlt corresponding to tois closed.
changes to tlts impact the usage prediction registry.
in the following subsections we describe the process assum ingt o.starthas been observed but revisit the situation in whichto.endis observed before concluding.
.
selecting the relevant rules the updated tlts are used to determine what new predictions can be made about the probability with which com ponents will be used.
all predictions of the system activitya r em a d eb yu s i n gt h et a r ss t o r e di nt h erule base .w e must determine what new tars if any are implicated bythe observation of t o.start.
to that end we iterate over all tars in the rule base .a tar rulebase can only be implicated by the observation ofto.start i ftoi sam e m b e ro fs e tx of that tar.
that is to say we cannot make a new prediction based on the giventar unless tocontributes to the prediction.
if this criterion is met then we look to see if the taris satisfied by any open top leveltransactionastrackedbytlts.
fora tartobesatisfied all transactions in xmust have been observed during the processing of at least one tlt.
furthermore the tar s prediction i.e.
y should have new transactions other than the ones that have already occurred during the processing ofthe satisfying tlt.
stated differently the taris only considered to have a useful prediction if all of its prerequisiteshave been seen and at least some of its predictions areunseen.
if both of these conditions are met then the taris added to the set ctar which is a set of all new tars that are candidates for being applied at that given point in time.
the tlt that satisfies the conditions for presence of a tarin ctar is said to be a basisfor the application of that tar.
this basis information is tracked along with the tarand used in the next stages.
.
updating the usage prediction registry the next step is to apply the implicated tars to update the usage prediction registries represented as set up.
given a component c there are typically more than a single tar predicting its usage probability uc up.w h i l es o m e may be due to the new observation to.start o t h e r sm a yb e due to the prior observations.
therefore we must combinethe various pvalues from all of the satisfied tars into a single prediction value u c. before describing how uccan be calculated we need to define three sets ctar cis a set of candidate tars that are supposed to affect a given component ca n dd e fi n e da s ctar c tar tar ctar t tar.y t.start.dst c .
these are the new tars based on the observation to.start.
atar cis the set of active tars currently contributing toucdue to observations made prior to to.start.
f i n a l l y ptar c ctar c atar ci st h ec o m p l e t es e to ft a r s that determine the new value of uc.
we can now describe how ucis calculated in five steps removing duplicate tars we do not need to reconsider atar ctar c which is already actively predicting the usage of component c i.e.
tar atar c .
therefore we remove any such tarfromctar c i.e.
ctar c ctar c atar c .
removing superseded tars a superseding relationship occurs when we have tars satisfying this pattern tar x1 y1 p1 tar x2 y2 p2 where y1 y2 x1 x2 in this scenario tar 2predicts the same set of transactions astar h o w e v e r tar 2makes use of more information thantar 1and hence makes a more informed prediction.
therefore tar 1is removed from its set i.e.
either ctar c oratar c depending on which one it came from .
selecting the best candidate even after removing the redundant rules we may still have some partially overlap ping ones.
partially overlapping rules express the variousexecution paths that may eventually result in the use ofsame component.
consider the following two tars tar t1 to t3 tc p1 tar t2 to t4 tc p2 where tc.start.dst c sincetar .y negationslash tar .y thesupersedingrelationshipcannot be used to remove one of the tars.
however the obser vation of a single t o.startshould at most result in a single 6prediction for the component c. we use a heuristic and choose the tar with the highest pv a l u et ob et h eb e s tc a n didate.
this tar expresses the greatest risk that cwill be used.
after this step ctar cmust have a single member.
trimming ptar c analogous to the logic in the previous step it is reasonable to expect each top level transaction to make a single prediction for a component c. when there are more than one active top level transactions wec a n n o tk n o ww i t hc e r t a i n t yw h i c ht o p l e v e lt r a n s a c t i o na c tually initiated t o.start.
however based on the number of active tlts recall section .
we know how many toplevel transactions are active in the system when to.startis observed.
therefore we approximate by limiting the num ber of tars contributing to u cto the number of top level transactions active at that point in time.
as with the re duction of ctar cin the previous step we choose to be conservative by keeping the tars with the highest pvalues.
we remove the tars with the lowest pvalue from ptar c until the size of ptar cis equal to number of active tlts.
combining the predictions at this point we let the atar cto be equal to ptar c. we can now recalculate uc based on the updated atar c. because there are no duplicate overlapping or related tars in atar c w ec a l c u l a t e ucby combining the prediction values from individual tars inatar cas independent probabilities uc probability c is not used producttext at ar c i pi this follows from the fact that according to eachtar i atar c the probability of cnot being used as a result of the relationship modeled in tar iis pi.
so far we explained how the usage prediction registries are updated when to.startis observed.
however the observation of to.endcan also update the usage prediction registries.i f to.endis a top level transaction the tltocorresponding to to.endis removed.
as a result all the tars that have tltoas their only basis are removed from atar c. since in this case ctar c steps are skipped and step is performed to propagate the impact of these dele tions on all of the components predictions.
theusage prediction registry is either updated each time a transaction and its corresponding events are observed oron an as needed basis.
.
using registry for adaptation the ultimate goal in our research is to use the predictions for making adaptation decisions.
the probabilistic rules in ferred using our approach collectively represent the stochas tic dependency model of the system.
such a model couldbe used in the context of both tranquility and version consistency for adaptation.
in our current approach we employ a technique similar to that described in tranquility where we temporarily buffer store events intended for acomponent during the time it is being replaced.
alterna tively we could have employed a technique similar to that of version consistency where two instances of a component are leveraged and incrementally new top level transactionsare shifted to use the new version.
our approach could beused to both guarantee consistency andminimize disruption as described in detail below.
.
guarantying consistency as specifically noted in section inconsistency could result if a component is adapted at a time in which it has already participated in a transaction that it participates inagain.
that is to say to maintain consistency ac o m p o n e n t must not be adapted if it has been used in some top leveldependent transaction until that top level dependent transaction terminates.
our predictions would very nearly approximate that type of protection given that a componentthat is used typically ends up with a high usage predictionin its register and that value will not dissipate until the top level transaction that caused it terminates.
however there is a slight risk that our approach as described up to this point would not fully guarantee consistency because afterall one cannot guarantee the accuracy of mined rules.
in situations where such a risk is unacceptable we make a slight modification to the approach described in section .
that allows us to provide consistency guarantees.
when we observe a transaction t o whereto.start.dst c w el o c kt h e value of uc to prevent cfrom being adapted since we now know it has participated in a transaction and changes to it may result in inconsistencies.
however since we do not know in which top level transaction it has participated i.e.
we do not know the tlt we keep the prediction locked at1 until all of the tlts that are the basis of that predictionhave closed at which point we roll back to the mechanism described in section .
for updating its prediction value.
.
minimizing disruption whenuc we do not adapt c since the change is likely to leave the system in an inconsistent state.
however whenu c chas not yet been in a top level transaction but could still be used at anytime in the future.
if we adapt c we may disrupt the system as events sent to that component would be buffered until the adaptation has finished.
toeliminate disruption it is tempting to use u c as condition for adapting c. it may however take a long time for ucto become and this could create a reachability problem i.e.
a situation in which one has to wait a long time oreven forever before the condition for adaptation is met.
in practice it is often reasonable to accept the potential for a slight disruption and allow adaptation when u c epsilon1.
figure exemplifies how this approach works.
although the examples are hypothetical the registries indeed behavesimilarly in practice.
a typical registry goes through this motion many times over the execution of the system starting at when a top level transaction is initiated risingas new observations are made and tars are applied andfalling back to once the top level transaction has terminated.
the steps in these functions represent the times at which the registries are updated.
finally when the rules areaccurate we expect the step function to be skewed to theright when the component is eventually used and skewedto the left otherwise.
this is because typically when a component is eventually used additional observations are made that subsequently satisfy more tars which combine to in crease the component s usage probability.
as depicted in figure when a component has a u c epsilon1 at the time of adaptation decision and the component actu ally gets used before the end of that transaction active we say it is a true positive tp result.
when a component hasau c epsilon1at the time of adaptation decision and the component is eventually used active we say it is a false negative fn result.
similarly false positive fp and true negative tn results can be defined when a component is noteventually used inactive a sd e p i c t e di nf i g u r e4 b .
the remaining challenge is how to pick a value for epsilon1that 7timeuc epsilon1 component is used a tp c fn c timeuc epsilon1 component is not used b fp c tn c figure hypothetical behavior of ucasto.startis observed for some top level transactions over time a cis eventually used and b cis not used.
is meaningful.
we define epsilon1in terms of another parameterr which represents the tolerable rate of all adaptations that may result in disruption.
we believe ris a reasonable threshold that can be specified by the user e.g.
the userstating that on average no more than .
of adaptations should result in a disruption.
in essence ris used to make a trade off between reachability and disruption.
to be able to calculate epsilon1based on r w eh a v et or e l a t ea system wide threshold defined by rto a component specific threshold defined by epsilon1.
we do this from a probability distribution of prior predictions embodied in the recorded u c values.
let uarepresent the set of all recorded predictions for components that were eventually used active and ui represent the set of all recorded predictions for componentsthat were eventually not used inactive .
in essence u arepresents the set of all recorded values corresponding to thestep function of figure 4a for all components in the system whileu irepresents the same except for figure 4b.
as a result uaindicates the situations in which adaptations could have possibly disrupted the system in the past.
givenuaandui it is possible to build the corresponding frequency distributions puaandpuia ss h o w ni nf i g u r e s5 a and b respectively though epsilon1is not known when these are first built .
using conventional techniques we can derive the cumulative distribution function cdf fuaandfui frompuaandpui as depicted in figures 5c and d respectively.fua epsilon1 defines the fraction of all uasamples where ua epsilon1.
in other words r fua epsilon1 .
thus we can calculate epsilon1based on the rvalue specified by the user as the inverse epsilon1 f ua r .
in terms of probability theory this means that epsilon1is ther quantile of the probability distribution .
the cdf would need to be updated either periodically or asneeded based on the execution history of the system.
it should be apparent from figures 5a and b that the key to limiting error in the approach is to skew p uatowards high values of uandpuitowards low values of u. this will result infuaremaining at low values and then escalating quickly as it approaches .
while fuiescalates quickly and then grows gradually to .
.
this difference in fuaandfuican be seen in figures 5c and d based on the slight difference in skewing shown in puaandpuiin figures 5a and b. if the approach is able to skew the distributions for active and inactive components differently then it effectively achievestherealgoalofthisapproach itdistinguishesbetweenactiveand inactive components in advance.
.
ev aluation we have developed a prototype of the approach using apriori an association rule mining algorithm with an implementation provided in weka .
as explained in sec ua 1pua epsilon1r a ui 1pui epsilon1 b ua 1fua epsilon11 r c ui 1fui epsilon11 d figure a frequency distribution for ua b f r e quency distribution for ui c c d ff o r ua a n d d cdf for ui.
tion .
we intentionally use very low confidence and sup port thresholds p .
and s .
.
we performed experimentation on runtime adaptation of eds recall sec tion .
to evaluate the approach we used several versionsof eds as shown in table .
we used a baseline version of eds with a single user.
we then repeated the evaluations on higher concurrency systems to evaluate the sus ceptibility of the approach to concurrency errors.
the 80and experiments were simulated by using hyperactive dummy users as eds never naturally reached that level of concurrency error.
therefore the values for users aremerely projections and the precise values for concurrencyerror rate should receive primary focus.
table shows whatpercentage of all recorded transactions were actually erroneous duplicates caused by concurrency as well as the average number of these erroneously recorded transactions pertop level transaction.
each experiment had roughly truetransactions per top level transaction.
finally to assess the accuracy and performance of our approach under different conditions the user experiment was intentionally allowedt oe x e c u t ef o ral o n g e rp e r i o do ft i m e w h i c hr e s u l t e di nc o l lection of significantly more top level transactions.
.
effectiveness of tar reducing heuristics we first show the effectiveness of our rule pruning heuristics recall section .
significant reduction in tar volumeinthe prune rules stagetookplaceinalloftheexperiments.
the reduction number can be seen in table .
this reduction can only be truly appreciated when considered with two other facts the reduced rule base does not significantlydegrade the accuracy as evaluated next and becauseof this reduction the remaining rules can be applied veryefficiently at runtime evaluated in section .
.
table experimental systems used in evaluation and effects of tar pruning heuristics.
o f o ft l t concurrency errors o ft a r users observed rate per itemset initial remain .
.
.
.
.
.
.
.
.
.
.
.
8figure the results from the experiments a cdf of uaand b cdf of ui.
.
accuracy of component usage predictions a crucial evaluation dimension for our approach is the degree to which it correctly predicts the usage of a compo nent.
as discussed in section .
the accurate prediction ismanifested through skewing f uato a slow growth function that then escalates quickly at high values of ua while at the same time skewing fuito a quickly escalating function that then grows only gradually over high values of ui.
figures 6a and b show fuaandfuifor the various experimental systems that we used.
it is clear from comparison of these twocharts that our approach achieved significant differentiation between active and inactive components.
using these cdfs we can quantify the effectiveness of the approach in terms of fn tp tn and fp.
as discussed in section .
the approach uses epsilon1to fix the fn rate at r. therefore the effectiveness of the approach must be mea sured in its ability to minimize the fp rate based on thefixed value of fn.
because our approach achieved signifi cant differentiation between f uaandfui f o rr .
we were able to set epsilon1at relatively high values and achieve the very favourable error rates as shown in table .
as seen in all experiments except for that with the highest concur rency the unfixed error rate of fp was held to below well below the fixed fn error rate.
beyond demonstrating accuracy in the prediction of component activity these ratios also demonstrate that the approach was not noticeablyimpacted by an increase in concurrency in the system untilconcurrency reached extreme levels.
this quality of differentiation can be viewed with a receiver operating characteristic curve roc curve often used to evaluate a binary classifier as shown in fig ure .
in our case the roc curve depicts the change in ther a t i oo ft pt of pa sd i ff e r e n t epsilon1thresholds are chosen.
the extreme of epsilon1 .
exists at the origin of the roc plot while the extreme of epsilon1 .
exists at the point of the roc plot.
therefore it can be seen how the tp and fp ratesrespond by moving the epsilon1threshold to balance between table error and accuracy rates for the experimental systems.
o f false true true false epsilon1 users negative positive negative positive value .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.95rate of disruption and reachability of adaptation.
the roc curve shows that the approach does an incredible job ofachievingtruepositivesdespitechangesinthe epsilon1threshold.
the comparison of the different experiments also shows the effect of concurrency on the approach.
as seen in table higher values for epsilon1are needed to achieve r .
as concurrency increases.
this occurs because with many users in the system there are many more observations that allow the approach to predict usage of a component c when cis actually used.
therefore as concurrency increases the values for uaare more skewed towards .
until at a concurrency error rate of roughly for eds i.e.
case of 137users active components are constantly at u c .
until the transactions they participate in subside.
while thisis beneficial because it approaches perfect classification ofactive components as can be seen in figure higher concurrency systems actually escalate to the point more directly it results in two detriments to the approach.
first once the concurrency rate forces epsilon1to be set to .
given some rvalue epsilon1has reached its maximum value and as such cannot compensate for the increasing false positiverate by moving to a higher value.
therefore once concur rency forces epsilon1to be set to .
to achieve r the approach can no longer compensate for the higher fp rates caused byeven further increases in concurrency.
second as concurrency increases to greater levels components remain active for greater portions of time.
but since at that point allactive components are effectively always at u a .
problems of reachability may occur ifthe components never become inactive.
an implementation of our approach basedon version consistency would address this problem bybringing a new version of the component on line to ser vice the new top level transactions while the old componentgraduallytransitionstoaninactivestate.
thatsaid wehave neverbeenabletorecreatesuchanextremescenarioineds using real user loads or even the highly extreme simulatedcases.
.
accuracy of desired disruption rate the third point of evaluation is the degree to which the approach achieves the desired rrate of disruption during adaptation.
the evaluation results presented in the pre vious section and shown in table were prospective error rates due to setting epsilon1at the specified level based on historic prediction values.
in this section then we look to see howwell the false negative rate rwas tracked once epsilon1was set.
tafigure roc curve for the various experiments.
9table tracking of false negative fn threshold.
of users mean false neg.
rate conf.
interval .
.
.
.
.
.
ble3showsthemeanfalsenegativeratesand95 confidence intervals for those false negative rates for the different ex perimental systems.
these statistics were calculated basedon sample moving averages that were recalculated at sample intervals.
as shown the system very effectively tracks the chosen r .
and maintains a fairly tight confidence interval around its mean.
furthermore it should benoted that the rate of concurrency does not noticeably affectthe tracking of r. .
performance and timing the final evaluation criteria are the performance benchmarks of mining rules andapplying rules cycles.
we have collected these numbers on a macbook pro laptop with .
ghz intel core i5 processor and gb mhz ddr3 memory.
the mining of the event logs to generate the ruleshas been extremely fast.
although we set our support andconfidence values very low resulting in a large number of rules to be generated apriorih a sa l w a y sc o m p l e t e dt h a ti n less than seconds in all of the experiments described here.
the performance of updating the predictions at runtime consists of two primary elements retrieval of relevant tars recallctarfrom section .
and update of the usage prediction registry by applying the rules.
for the former mysql database version .
.
is used to store the rule base.however because retrieval of tars from mysql was ob served to take typically between .
seconds and .
seconds we implemented a simple caching of the rule base .
based on this caching the combined time of retrieving relevant tars and updating the usage prediction registry by applying the rules takes very little time.
the mean process ing times and confidence intervals for those processing times are given in table .
as seen the processing times are quite short tightly bound in the confidence intervals and not noticeably effected by the increase in concurrencyexcept for a few millisecond gain in mean processing time for larger rule bases.
.
related work in section we described the most related approaches namely quiescence version consistency and tranquility including their relationship to this work.
herewe focus on other related literature.
researchers have used log of event data collected from a system to construct a model of it for various purposes.
table performance of rule application.
o f mean time for rule confidence users application ms interval ms .
.
.
.
.
.
cook et al.
use the event data generated by a softwareprocess to discover the formal sequential model of that process.
in a subsequent work they have extended their work to use the event traces for a concurrent system tobuild a concurrency model of it.
gaaloul et al.
discoverthe implicit orchestration protocol behind a set of web ser vices through structural web service mining of the event logs and express them explicitly in terms of bpel.
motaharinezhad et al.
present an algorithmic approach for cor relating individual events which are scattered across sev eral systems and data sources semi automatically.
they use these correlations to find the events that belong to the same business process execution instance.
wen et al.
use the start and end of transactions from the event log tobuild petri nets corresponding to the processes of the sys tem.
none of these approaches aim to understand the behavior of the system for the purpose of adaptation.
software architecture has been shown to provide an appropriate level of abstraction and generality to deal with the complexity of dynamically adapting software systems .
gomaa and hussein developed the notion of reconfiguration pattern which is a repeatable sequence of stepsfor placing a software component in the quiescence state.
inrecent work we adopted this concept to provide safeadaptation support on top of a middleware platform.
finally mining software repositories e.g.
source control systems bug tracking systems etc.
is a thriving thrust ofresearch see .
although related our objective in thispaper is fundamentally different from that line of research.
to the best of our knowledge data mining has not been applied for determining the time at which changes shouldoccur in a running software system.
.
conclusion we provided an overview of a mining based approach that from the execution history of a software system infers a stochastic component dependency model of its components.
we have used this model for the purpose of determiningthe time at which a component can be replaced withoutleaving the system in an inconsistent state and creating a significant disruption.
our approach can be applied with minimal effort.
all that is needed is the ability to monitorthe interactions among the components of that system.
ourapproach can be used to learn the emerging component de pendencies as the software system evolves.
the evaluation of our approach using a real software system and numerous users have empirically shown the accuracy of the models in ferred in this way and the ability to leverage those modelsto make timely and effective adaptation decisions.
in the future we plan to experiment with other types of data mining algorithms that leverage frequency of itemoccurrence as well as temporal and ordering relationshipamong events.
with these sorts of approaches we hope toleverage information that is already available but not effectively utilized in our current approach to further improve its precision and performance.
.