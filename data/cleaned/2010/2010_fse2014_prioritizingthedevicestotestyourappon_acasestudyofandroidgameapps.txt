see discussions st ats and author pr ofiles f or this public ation at .researchgate.ne t public ation prioritizing the devices to test you r app on a case study of android game apps conf erence paper no vember .
.
citations 101reads author s including hammad khalid queen s univ ersity publica tions citations see profile emad shihab conc ordia univ ersity publica tions citations see profile ahmed e. hassan queen s univ ersity publica tions citations see profile all c ontent f ollo wing this p age was uplo aded b y ahmed e. hassan on july .
the user has r equest ed enhanc ement of the do wnlo aded file.prioritizing the devices to test your app on a case study of android game apps hammad khalid1 meiyappan nagappan2 emad shihab3 ahmed e. hassan1 1software analysis and intelligence lab sail queen s university kingston canada 2department of software engineering rochester institute of technology rochester usa 3department of computer science and software engineering concordia university montreal canada 1hammad cs.queensu.ca 2mei se.rit.edu 3eshihab cse.concordia.ca 1ahmed cs.queensu.ca abstract star ratings that are given by the users of mobile apps directly impact the revenue of its developers.
at the same time for popular platforms like android these apps must run on hundreds of devices increasing the chance for device specific problems.
devicespecific problems could impact the rating assigned to an app given the varying capabilities of devices e.g.
hardware and software .
to fix device specific problems developers must test their apps on a large number of android devices which is costly and inefficient.
therefore to help developers pick which devices to test their apps on we propose using the devices that are mentioned in user reviews.
we mine the user reviews of free game apps and find that apps receive user reviews from a large number of devices between to unique devices.
however most of the reviews originate from a small subset of devices on average .
furthermore we find that developers of new game apps with no reviews can use the review data of similar game apps to select the devices that they should focus on first.
finally among the set of devices that generate the most reviews for an app we find that some devices tend to generate worse ratings than others.
our findings indicate that focusing on the devices with the most reviews in particular the ones with negative ratings developers can effectively prioritize their limited quality assurance qa efforts since these devices have the greatest impact on ratings.
categories and subject descriptors d. .
management software quality assurance sqa general terms reliability management keywords mobile apps android fragmentation device prioritization .
introduction usage of android devices has grown at a tremendous rate over the past few years .
to capitalize on this growth both small permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
copyright 20xx acm x xxxxx xx x xx xx ... .
.and large companies are developing an enormous amount of applications called mobile apps designed to run on android devices.
however the top rated or the featured apps in the app markets are the apps with the most downloads and hence the most revenue .
also the app market is very competitive especially for game app developers who have to compete with almost game apps already in the google play store more than any other category of apps.
to compete in this environment developers need to get and maintain good ratings for their apps .
this can be difficult since users are easily annoyed by buggy apps and that annoyance could lead to bad ratings .
hence app developers need to test their apps thoroughly on different devices to avoid a poor rating.
to make matters worse there exists a large number of android devices each with its own nuances.
in fact dealing with device specific issues of the many android devices is considered one of the biggest challenges developers face when creating an android app .
a survey from appcelerator which has aggregated results from similar such surveys in the past three years shows that developer interest in android has fallen to in from a high of in .
a staggering of the developers that avoid working on android apps cited android fragmentation as the main reason .
android fragmentation refers to the concern that since there are many devices with different screen sizes different os versions and other hardware specifications an app that functions correctly on one device might not work on a different one .
joorabchi et al.
examined the challenges in mobile application development by interviewing mobile developers.
one of main the findings of their study is that dealing with device specific issues e.g.
testing these devices remains a major challenge for mobile app developers.
even google suggests that developers should test their apps on actual devices before they release the app .
there are now even business solutions based on providing devices remotely for testing .
however with costs ranging in approximately a dollar for every minutes of device time the total expense incurred to developers can get very high.
these concerns are especially worrisome for game app developers since they have to manually test their apps on device instead of just relying on automated tests due to the graphical and non deterministic nature of video games .
therefore android developers and game developers in particular need to carefully prioritize their testing and quality assurance qa efforts on the most important devices.
while there has been some previous research in automated testing for android apps to the best of our knowledge there has not been any work on prioritizing testing and qa efforts on the devices that have the most impact on the rating of an app.we coin the term review share which measures the percentage of reviews for an app from a specific device.
for example a review share of means that a specific device gave of all ratings for an app.
we use review share to demonstrate the importance of focusing qa efforts on a smaller subset of devices.
through a study of user reviews for free game apps from various android devices we explore the following research questions rq1.
what percentage of devices account for the majority of user reviews?
we find that on average of the devices account for of all reviews given to a free game app.
with this information app developers can prioritize their testing and qa efforts by focusing on a small set of devices that have the highest potential review share.
rq2.
how can new developers identify the devices that they should focus their testing efforts on?
we find that developers can use the list of devices with the most review share in all other gaming apps as a good indicator of which devices they should focus their testing and qa efforts on.
rq3.
do the star ratings from different android devices vary significantly?
by examining the reviews from different devices for the same app we find that some devices give significantly lower ratings.
developers can take corrective actions for such devices or remove support for them if they see fit.
takeaway developers can better prioritize their qa efforts by picking the devices that have the most impact on the ratings of apps.
some of these devices may give worse ratings than others developers can either allocate additional qa resources for these devices or remove support for them.
the remainder of this paper is organized as follows section surveys the related work.
section discusses in detail the data that we analyze for our study.
section presents the results of our study.
section performs further analysis on paid apps and apps in other categories to see if our findings generalize.
section discusses the implications of our findings for developers.
section discusses the potential threats to the validity for our study.
section concludes the paper.
.
related work in this section we present work related to android fragmentation work related to mobile app reviews and work related to the testing of android apps.
.
work related to android fragmentation a recent study by han et al.
manually labeled bugs reported for specific vendors of android devices .
they highlighted evidence for android fragmentation by showing that htc and motorola devices have their own vendor specific bugs.
they considered this as evidence for android fragmentation.
ham et al.
came up with their own compatibility test to prevent android fragmentation problems .
they focus on code analysis and api pre testing to identify possible issues.
our study differs from their work since we seek to help app developers determine which devices they need to test their apps on.
we determine this set of android devices from the user reviews of an app.
.
work related to app reviews and app quality recent studies have examined the importance of app reviews.
harman et al.
s pioneering work on mining the blackberry store found that there is a strong correlation between app rating and number of downloads .
kim et al.
also found ratings to be one of the key determinants in a user s purchase decision of an app .
linares vasquez et al.
show that the quality in terms of change and fault proneness of the apis used by android apps negatively impacts their success in terms of user ratings .
more recently there have also been studies on using the reviews for generating feature requests from the user comments.
pagano and maalej carried out an exploratory study on reviews from ios apps to determine their potential for requirements engineering processes.
there have also been studies on automatically extracting feature requests from the user reviews .
our work complements the aforementioned work since our goal is to use mobile app reviews like them but to assist developers in determining whether they can use a small subset of devices to test their game apps.
in a previous study we manually analyzed and tagged reviews of ios apps to identify the different issues that users of ios apps complain about .
we hope to help developers prioritize the issues that they should be testing for.
this study differs from our previous work as we are focusing on identifying the different devices that android app developers should be focusing their qa efforts on.
.
work related to testing android apps several recent studies have attempted to reduce the testing burden of android developers.
one of these studies is by kim et al.
who look at testing strategies to improve the quality of mobile apps.
agarwal et al.
study how to better diagnose unexpected app behavior .
hu et al.
suggested methods for automated test generation and analysis for android apps .
machiry et al.
presented a dynamic input generation system which can be used by developers for black box testing .
there has also been previous work which has aimed to automate testing in a virtual environment.
amalfitano et al.
presented a tool which automatically generates tests for android apps based on their graphical user interface .
droidmate is another such tool which uses genetic algorithms to generate input sequences i.e.
user interaction or simulated events .
while testing in a virtual environment is useful for identifying general issues developers also test their apps on actual android devices to identify device specific issues.
our work focuses on helping developers identify the devices that have the most impact on their rating and hence developer should focus their testing efforts on these devices.
previous research has confirmed the effect of android hardware and software fragmentation.
in addition recent studies have identified the importance of mobile app user reviews.
in this paper we seek to help developers understand how they can prioritize their qa efforts towards the devices that have the most impact on their ratings.
.
study design in this section we discuss the data used in our study the main sources of this data and the collection process for this data.
the main data used in our study are the user reviews of the top android game apps.
we collect these reviews from google play which is the main market for android apps.
after collecting these reviews we identify the devices that these reviews were produced from as well as the ratings associated with each review.
figure provides an overview of the process used in our study.
the following sections describe the data used in our study and our data collection method in further detail.selecting the top game apps android app market list of top apps collecting the reviews with a web scraper extracted reviews for analysis data selection collected data data collection figure overview of our process .
data selection for our study we selected the top free game apps as ranked by google play.
our proposed method is general i.e.
other apps can be examined see section for a discussion about our analysis across the different categories of the market .
nevertheless we picked these top game apps for two reasons a since game apps are the most popular apps in the android market and thus our results could have the greatest impact and b top game app developers like red robot labs pocket gems storm8 and animoca more than millions app download across their app portfolio in an article on techcrunch discussed the issues they are having with testing their apps on many devices .
currently they rely on their experience and app analytics data when available for choosing devices to test on.
.
data collection to collect the user reviews we build a web crawler using a web automation and testing tool called selenium .
our crawler extracts data such as the app name the review title the review description the device used to review the app and the numerical star rating of the review.
this crawler opens a new instance of the firefox browser visits the url of one of the selected apps and then clicks through the review pages of this app.
the required information is located on each page using xpath then stored into a database .
we used a similar crawler to collect the urls of the top apps.
this crawler is much more limited and slower than typical web crawling since google play puts heavy restrictions on crawlers.
we found this crawling approach to be the only reliable method for extracting the reviews.
all of the crawling is done using an anonymous user.
also google play limits the total number of reviews that a user can view to a maximum of for each of the star ratings i.e.
a user is restricted to viewing a maximum of reviews for each app as there are levels of ratings .
recent work by pagano and maalej shows that there exists a large amount of noise in reviews for example one word reviews .
hence even apps markets have moved to using more complex methods to calculate a global rating or ranking of an app instead of simply summing up all the reviews .
we use the most helpful reviews as they are considered by app stores and users as one of the most reliable sources of user feedback.
the helpfulness is determined by other users voting for reviews.
such crowd based filtering helps weed out spam reviews.
summary of collected data in total we collect reviews across the levels of star rating from the studied apps.
from this set of reviews we only consider the ones that have a device associated with them.
this reduces the set of reviews to each submitted by a unique user.
the implications of our review selection is discussed in more detail in section .
.
we limit our reviews to those given only after october to january a month window since the rate of churn in devices is very high.
.
preliminary analysis prior to delving into our research questions we perform some preliminary analysis on our dataset as a whole i.e.
using data from all game apps taken together.
we perform this analysis to determine whether our review dataset contains reviews from many different devices or a small set of devices.
in other words we would like to determine if android fragmentation does exist in our data or not.
for this we look at how the reviews are distributed across the devices for the apps taken as a whole.
using all of the reviews we identify the number of reviews that each device gave.
in total our dataset contains reviews from unique android devices.
out of these devices of these devices have provided more than user reviews.
these facts highlight the magnitude of the android fragmentation problem and the importance of identifying the devices that give the most reviews to apps for prioritizing qa efforts.
.
results now that we have determined that our dataset contains reviews from many different android devices in this section we answer our research questions.
rq1 what percentage of devices account for the majority of user reviews?
motivation as mentioned earlier android fragmentation is a major concern of developers who are seeking to develop high quality android apps .
there may be hundreds of devices that developers may need to test their apps on.
testing on such a large number of devices is not feasible for developers with limited time and budget.
therefore our goal is to determine what percentage of devices account for the majority of reviews.
approach to answer this question we use the reviews that we collected for the free game apps.
for each review we determine the device that the review was posted from.
then we calculate the review share for each device.
we define review share as the percentage of reviews from one device compared to the total number of reviews from all devices.
for example a review share of means that a device gave of all reviews.
initially we consider the reviews from all apps taken together.
we then determine what percentage of devices ordered by decreasing review share is required to cover x of the user reviews.
in our case x varies from to .
we also examine the results by breaking down the reviews by star ratings.
instead of looking at individual star ratings we combine and star reviews into a group which we call bad reviews and combine and star reviews into another group which we call good reviews .
we label star reviews as medium reviews .
we then calculate the review share for each device when we exclusively consider only bad medium or good reviews.
to ensure that our grouping makes sense we run a sentiment analysis tool over the text of the reviews in these groups to validate that these groupings are appropriate i.e.
good reviews actually have the most positive reviews and vice versa .
this tool assigns an integer to the sentiment expressed in the reviews where a negative integer represents a negative review .
in our case the negative reviews group was assigned a score of .
the medium reviews group was assigned a score of .
while the good reviews group was assigned a score of .
.
these scores support our grouping scheme.
findings figure shows the percentage of devices x axis vs. the cumulative percentage of reviews y axis in the different rating groups different coloured curves .
from this figure we find that of the devices account for approximately of the posted percent of devicescumulative review share all ratings bad ratings medium ratings good ratingsfigure percent of android devices used to give x x ranges from of the reviews for free game apps free game appsnumber of devicesnumber of devices needed for reviews number of devices that review an app figure number of android devices that account for of the reviews for each game app compared with the total unique devices that review the app user reviews .
this finding suggests that developers working on free game apps only need to focus their testing efforts on of the devices to cover the majority i.e.
approximately of the reviews.
observing figure we note that the bad medium and good ratings curves are very similar.
after comparing how different devices gave bad medium and good reviews we find that while the devices that gave the bad ratings were fairly identical to the devices that gave good ratings there were a few discrepancies.
the set of devices that had a cumulative sum of of the bad ratings had four devices that were not in the set of devices that gave most of the good reviews.
this finding suggests that there is additional variation in how specific devices rate these game apps.
discussion from rq1 we know that a few devices account for most of the reviews in an app.
we wanted to dig deeper and find if a similar trend exists at the app level as well i.e.
for each app if most of the reviews came from a small subset of devices.
the line chart in figure compares the number of total unique devices that rate an app with the number of devices that account for of the reviews for that app.
the top line in this chart shows thenumber of unique devices.
the minimum number of devices is the median is devices and while is the maximum number of unique devices that rate an app.
this finding clearly indicates that fragmentation exists even at the app level.
the bottom line in figure shows the number of devices that account for of the reviews.
as this figure shows this number is much lower than the total number of unique devices.
we find a minimum of a median of and a maximum of devices accounted for of the reviews per app.
while these numbers may seem large one would have to keep in context that android developers often have to think about testing their apps on hundreds of devices otherwise.
to get a better idea of the comparison above we calculate the percentage of devices that account for of the reviews in each of the free game apps.
we do this on a per app basis.
we observe that of the reviews can be addressed by considering a minimum of of the devices and at most of the unique devices.
the average percentage of devices required to cover of the review share is and the median is .
this finding indicates that app developers can cover the majority i.e.
of the reviews by focusing their qa efforts on of the devices on average.
a small set of devices are needed to cover of the reviews.
on average of all devices account for of reviews given to free game apps.
rq2 how can new developers identify the devices that they should focus their testing efforts on?
motivation thus far we have shown that a small percentage of devices make up the majority of user reviews.
an implication of this finding is that if developers carefully pick their set of focus devices then they can maximize the effectiveness of their qa efforts.
to illustrate consider a scenario where a team of developers is working on a new free game app but they can only afford to buy devices to test their app on.
identifying the optimal set of devices is even more important for such developers with limited resources who can only afford a few devices.
one method of picking these devices is to aggregate the review data of every top rated app in a category and select the devices which would lead to the most review share.
this method can provide developers a working set of devices to start from which they can later augment with other devices.
in this rq we examine the effectiveness of this method.
using our app review data we determine the percentage of reviews that devices with highest review share would cover.
we find that even a small number of devices if carefully chosen i.e.
by looking at which devices frequently post reviews for apps can account for a considerable number of reviews more than half of the reviews in most cases.
this leads to the question how can developers without any reviews for their app pick the best set of devices that they should focus their qa efforts on?
for example can a developer use the review share data from all the top rated apps that are currently present in the game app category?
should they just pick the most popular devices?
we explore this issue in rq2.
approach to answer this rq working with our dataset of free game apps we identify a set of devices with the most review share for each app and compare this set with the set of devices with the most review share in the remaining apps combined.
so allows us to simulate an app developer who picks the devices that provide the most reviews for other apps and using these devices to test his or her own app.
since we have the reviews for that appas well in our dataset we use these reviews to top apps based on market share top apps based on review share of other apps0123456number of devices missed a number of devices missed top apps based on market share top apps based on review share of other apps0 25percent of reviews missed b percent of review share missed figure number of devices and percent of review share missed if devices with the most reviews for all the remaining apps are chosen measure the review share that would be covered for an app based on the selected devices.
in other words this analysis allows us to identify the devices that were in the set of devices that gave the most reviews for an app but not in the set of devices that gave the most reviews for the rest of the group essentially a set difference .
if such devices are identified we sum the review share of these devices to highlight the review share that the developer would have lost missed if he or she had picked the devices with highest review share among all the other apps in that category.
these results are shown in figure .
findings using our proposed method to prioritize the devices works well overall.
in the majority of the cases the developer will identify the most review producing devices.
figure 4a right box plot shows the distribution of these missed devices for the free game apps.
we observe that the median number of missed devices is devices with a max of devices and a min of .
now we want to know what percentage of the review share would be impacted due to these missed devices.
figure 4b right box plot contains a box plot that shows the percentage of review share impacted due to the missed devices.
the median of missed review share is .
.
this is not a large loss in review share especially given that using this method i.e.
using devices that have a large review share in the app s category developers will have the benefit of picking an adequate set of devices even before releasing their app to the market.
discussion while identifying the devices with the most review share within an app s category is useful some developers may opt to pick the devices with the most overall market share which is posted on many mobile analytics websites e.g.
app brain to prioritize their qa effort.
market share is generally determined by the number of active android devices.
to compare this method of simply using market share to our proposed method which uses the reviews we compare the devices with the most market share to the devices that have the most review share for each of the game apps.
we obtain the list of devices with the most market share from app brain which gives us a list of the devices shown in table for our studied time period october to january .
figure 4a left box plot shows the distribution of missed de vices for each the free game apps when market share devices are compared with the devices that have the highest review share for the corresponding app.
we find that the median number of devices missed if we consider the market share devices is which is higher than if we use review share in the app s category.
moreover figure 4b left box plot shows the distribution of the percentage of review share impacted due to the missed devices.
once again we see that the median value is .
.
this rate is higher than the median if our method was used which has a median of .
.
the difference in the number of missed devices and missed review share between choosing our review share method and the market share method is statistically significant p value .
for a paired mann whitney u test .
our findings suggest that simply using the market share is not sufficient and using our method which uses reviews from apps in the same category can identify devices that have a greater chance to review the app.
thus by using our method a developer can improve the effectiveness of their device prioritization efforts since they will be able to identify devices that have a greater impact on the ratings of an app.
while examining the devices with the most review share in the game category we notice not all of the devices rate apps the same way.
this makes us wonder if some specific devices give worse ratings than others and thus need special attention from developers.
we explore this question next in rq3.
devices with the most review share across all existing top rated game apps are a good indicator of which devices are likely to have a large review share for each game app.
using this list of devices a developer can focus their qa efforts even before they release the first version of their app.
rq3 do the star ratings from different android devices vary significantly?
motivation since different devices have varying specifications it could be the case that users of the different android devices perceive and rate the same app differently.
understanding how different devices rate apps will allow developers to understand why their apps were given the ratings that they receive i.e.
is a device issue or an app issue .
if different devices give varying ratings this would imply that not all reviews of apps should be treated thetable the market share of the most popular android devices for our studied time period october to january top market share device market share samsung galaxy s3 .
samsung galaxy s2 .
samsung galaxy s .
samsung galaxy ace samsung galaxy note .
samsung galaxy y .
htc desire hd .
asus nexus .
samsung galaxy tab .
.
motorola droid razr .
table an example of the table used to compare bad ratings given by devices device angry birds temple run samsung galaxy s3 samsung galaxy s2 same way.
with this kind of information the app developer can do one of two things they can either allocate even more qa efforts e.g.
testing or focus group to devices that give a poor rating when the revenue from that device is critical or if there are not enough users of the app on the particular device then developers can manually exclude these devices from the list of supported devices on google play.
in either case knowing if certain devices give worse ratings than others will help developers prioritize their qa efforts even further.
approach we separately compare the bad and star reviews medium star reviews and good and star reviews reviews from the devices with the most review share of the free game apps.
for example to compare the ratings of the devices that rated game apps poorly we create a table where the columns are the game apps and the rows are a percentage of ratings from the top devices.
each cell in the table has a bad ratings all ratings percentage which is the number of and star reviews over the total number of reviews given to an app by a specific device.
table is an example of such a table.
for instance the ratio in the cell that corresponds to the row samsung galaxy s3 and the column angry birds means that out of every ratings that s3 devices gave to angry birds game were bad i.e.
and star reviews .
similarly out of every ratings from the samsung galaxy s2 device for the temple run game were bad.
to compare how the top review share devices give bad medium and good ratings we use the scott knott test with p value .
the scott knott test is a statistical multi comparison procedure based on cluster analysis.
the scott knott test sorts the percentage of bad reviews for the different devices.
then it groups the devices into two different groups that are separated based on their mean values i.e.
the mean value of the percentage of bad reviews to all reviews for each device .
if the two groups are statistically significantly different then the scott knott test runs recursively to further find new groups otherwise the devices are put in the same group.
in the end of this procedure the scott knott test comes up with groups of devices that are statistically significantly different in terms of their percentage of bad reviews to all reviews.
findings the devices that we examine in this rq are divided into statistically significantly different groups.
table shows the significantly different groups of devices as indicated by the scottknott test.
table also lists the devices that are in the group and the mean percentage of bad reviews for each of the devices.table scott knott test results when comparing the mean percentage of bad ratings given from each device to free game apps divided into distinct groups that have a statistically significant difference in the mean p value group device mean of bad ratings for the device per app g1 motorola droid x2 .
g2 droid bionic .
motorola droid x .
htc sensation 4g .
htc evo 4g .
htc desire hd .
samsung galaxy nexus .
htc evo 3d .
htc one s .
g3 motorola droid razr .
samsung galaxy s .
lg optimus one .
htc one x .
g4 samsung galaxy ace .
samsung galaxy note .
samsung galaxy s3 .
lg cayman .
samsung galaxy s2 .
asus nexus .
samsung galaxy y .
our findings show that indeed the users of some devices such as the motorola droid x2 give more bad ratings to apps than others.
we find that this device has a significantly higher ratio of bad ratings than the devices that give the least ratio of bad ratings to all ratings i.e.
samsung galaxy y .
the scott knott test also shows that this device has the lowest ratio for good ratings.
a reason behind these poor ratings could be manufacturer specific problems.
a recent study by han et al.
provided evidence of vendor specific problems when they compared bug reports of htc devices with motorola devices .
a report of this device from android police an android dedicated web blog describes its sluggish performance and poor screen resolution .
the poor screen resolution may be the main issue with this device since most game apps require a good screen resolution and performance.
on the other hand we notice that the users of some devices such as the samsung galaxy y asus nexus and samsung galaxy s2 give less bad ratings than other devices.
to better understand the results we further investigated the data to see whether the bad reviews were given to the same app or whether the bad reviews were given from different apps.
we discover that the bad reviews are different for all devices i.e.
it is not the same apps which are receiving the bad ratings across the different devices .
for example of all ratings given from the htc sensation 4g device to the tap tap revenge game app are bad ratings while the median percentage of bad ratings given by the same device is .
.
we also find that none of the other top devices gave this app such poor ratings.
thus we see that this particular app just does not work well on the htc sensation 4g device.
on further examination we find many complaints by users of this device on the forum of the developer of tap tap revenge .
examining the reviews from this device we see that this app crashes after most major events i.e.
a song ending in the app .
discussion our findings imply that developers should be aware that a few devices may give significantly worse reviews than others.
these bad ratings may be given because the app itself does not work well on a device as in the case of htc sensation 4g device and the tap tap revenge app .
alternatively the device itselfmay provide a poor user experience i.e.
motorola droid x2 .
in order to check this we analyzed the review comments from the reviews associated with motorola droid x2 .
in total there were reviews with a star rating less than stars that we classified as bad reviews.
we looked for keywords specific to certain types of complaints among these reviews like hang lag slow freeze or slug for identifying reviews where the users complained about the performance of the app.
we found the of the bad reviews from motorola droid x2 was related to performance issues.
we also found that of the reviews were complaints about the screen of the device.
to provide context we analyzed the reviews for the device with the lowest mean percentage of bad ratings namely samsung galaxy y .
there were a total bad reviews that we had in our dataset while this is more than that of motorola droid x2 the percentage of bad reviews to all reviews is small for samsung galaxy y .
among these bad reviews given to apps from samsung galaxy y we found that only of the complaints were about the performance of the app while of the reviews were about the screen of the device.
thus we can see that the performance of apps in samsung galaxy y is less likely to be an issue among users when compared to motorola droid x2 .
however the complaints about the screen of the device is consistent in both devices.
in any case developers aware of the above finding can specifically address the concerns expressed in the reviews from such devices e.g.
do detailed testing or remove the support for such devices.
we are not suggesting that developers only need to test on devices that give statistically different star ratings than others developers just need to devote specific attention towards problematic devices.
monitoring how different devices rate apps can reveal devices that are bringing down the overall rating of an app.
additionally our findings suggest that the user s perception of the quality of an app depends on the device on which it runs.
hence research on testing android apps should factor in the effect of the device.
another possible reason why some devices give worse ratings than others could be that those devices are simply older.
older devices also tend to have worse hardware specifications than new devices so the performance difference may be the main reason for these bad ratings.
to test this theory we identify the release dates of the devices with the most review share.
then we do a correlation test of their release dates and the median of the percentage of bad ratings to all ratings for the devices.
using the spearman correlation test we find a correlation of .
.
since the correlation is negative it means that newer devices have a lower percentage of bad ratings.
therefore this result implies that when it comes to the top devices the age of the device may be a factor for bad ratings.
it is important to note here that what we are observing is a correlation not causation.
for the devices that give the most reviews for game apps we find statistical evidence suggesting that some of these devices give worse ratings than others.
developers can take corrective actions for such devices by allocating more qa effort or removing support for these devices if they see fit.
.
generalizing the results we focus on free game apps since this lets us examine a very focused context thus avoiding other confounding factors such as cost functionality and end user profile.
we now wish to examine whether our findings generalize.
first we compare our results of free game apps with paid ones.
then we compare our results for different categories.
percent of devicescumulative review share all ratings bad ratings medium ratings good ratingsfigure percent of android devices which contribute x x ranges from of the reviews for all paid game apps .
comparison with paid game apps although it is likely that many devices review the paid game apps as well we believe that there will be a more even distribution of reviews among the devices.
the move even distribution is because users of paid apps may be more inclined to give reviews since they paid for the apps .
to conduct our comparison we collected all the reviews for the paid game apps.
in total we collected reviews of which were associated with one of devices.
similar to rq1 we identify the percentage of devices that are needed to account for the majority of the reviews given to the paid game apps.
we use the same methods that we used in rq2 to determine an ideal method to choose the top devices for focussed qa efforts based on the reviews from other apps or based on market share.
we use the same method that we used in rq3 to identify if ratings from different devices vary for paid game apps.
what percentage of devices account for the majority of user reviews?
for paid game apps we find that the median number of unique devices that review each paid game app is .
compared to free game apps which have a median of devices we find that paid game apps have much fewer unique devices that rate an app.
in terms of percentage of devices needed to account for of the reviews on a per app basis we find that while the median is the same as free game apps the range of these percentages is more for paid game apps in comparison to free game apps.
the minimum percentage is .
whereas the maximum percentage is .
.
figure shows the percentage of devices vs. the cumulative percentage of reviews in the different rating groups for all the paid game apps taken together.
again compared to free game apps we find that much less devices are needed to account for of the reviews.
we find that only .
of the devices are needed to cover of the reviews compared to the for the free game apps .
our finding suggests that developers working on paid apps should be even more attentive of their analytics since in some cases a select few devices have a huge impact on their ratings and hence their future revenue.
how can new developers identify the devices that they should focus their testing efforts on?
from figure 6a and figure 6b we can see that it is indeed more beneficial to target devices based on the review share of the other paid game apps instead of using the market share since we will be targeting devices that are used to review the apps more.
the difference in both the cases is statisticallytable scott knott test results when comparing the mean percentage of bad ratings given from each device to free game apps divided into distinct groups that have a statistically significant difference in the mean p value group device mean of bad ratings for the device per app g1 eeepad transformer tf101 .
motorola xoom .
htc evo 4g .
samsung nexus s .
galaxy tab .
.
g2 htc desire hd .
droid bionic .
eeepad transformer tf300 .
g3 samsung galaxy note .
htc sensation 4g .
htc evo 3d .
samsung galaxy s .
semc xperia play .
htc one s .
motorola droid razr .
htc one x .
samsung galaxy s2 .
samsung galaxy nexus .
g4 samsung galaxy s3 .
asus nexus .
significant p value .
for mann whitney u test .
this result is similar to the result for the free game apps.
however one noticeable difference is that the number of devices missed median of devices and the review share missed median of .
for paid apps when using the market share data is slightly higher than in the case of free game apps where the median values are devices and .
.
thus we can see that in the case of paid game apps the market share data is much less accurate in helping the developer identify the devices to test their app on first.
do the star ratings from different android devices vary significantly?
for the paid game apps we illustrate the differences in the percentage of bad ratings from each device in table .
our findings show that indeed even for paid game apps different devices provide different levels of bad ratings to apps.
the scott knott test groups the devices into statistically significantly different groups with p value .
for example the asus nexus and the samsung galaxy s3 devices give significantly better ratings to paid game apps than many of the other devices i.e.
motorola xoom eeepad tf101 htc evo 4g .
we also find that the set of devices that give a higher percentage of bad ratings in paid apps is different from the set of devices that give a higher percentage of bad ratings in free game apps.
for example the motorola xoom and eeepad tf101 are not even in the top devices that review free game apps.
thus developers need to be careful about using free games apps to prioritize their qa efforts for paid game apps as the devices that the users use for paid game apps do vary .
next we examine the devices that review apps in other categories not just games .
trends and results in paid game apps are similar to free game apps as well.
however the argument for prioritization is more pronounced in the case of paid game apps.
therefore paid game app developers can make optimal use of their qa budget by prioritizing their efforts based on the share of reviews from a device.table number of reviews collected and number of devices for the other categories of free apps category of reviews reviews linked to a device of devices business education sports entertainment .
analysis of apps in other categories while the findings of the study so far are most relevant for developers of game apps we want to see if our findings hold for apps in other categories as well.
more specifically we want to examine if the reviews of apps in the business education sports and entertainment categories have similar patterns to those we found in our study this can help other developers deal with android fragmentation as well.
statistics about the data used to perform this analysis is summarized in table .
what percentage of devices account for the majority of user reviews?
figure shows the percentage of devices vs. the cumulative percentage of reviews in the different rating groups for each of the categories.
we find that when considered together .
.
and .
of the devices account for of the reviews given to apps in the entertainment business and education categories respectively.
for apps in the sports category only .
of the devices account for of the reviews.
these numbers are similar to the in the free game apps category.
similar to game apps these findings imply that developers working on these categories only need to focus on a small subset of devices to cover the majority of the reviews given to their apps.
our finding suggests that developers working on apps in categories other than games can also greatly improve their efficiency by focusing on the few important devices since these devices make up the majority of the reviews given to an app.
how can new developers identify the devices that they should focus their testing efforts on?
similar to our results for free game apps we find that developers get more coverage of the reviews if they focus on the devices with the most review share instead of the devices with the most market share.
we find that by focusing on the devices with the most review share instead of the devices with the most market share developers can gain an extra .
.
.
.
review coverage on apps in the business education entertainment and sports categories respectively.
do the star ratings from different android devices vary significantly?
we now compare how different devices review free apps in entertainment business sports game and education categories.
using the scott knott test we observe that the variation of ratings from different devices is also present for the apps in these categories of apps.
in each category the scott knott test with p value divided the set of devices into four statistically significantly different groups based on the percentage of bad reviews to all reviews in an app.
we note that many of the devices that give the most ratings to apps in these categories vary in terms of their bad and good rating.
we also note that the asus nexus gave more bad ratings and less good ratings for the apps in the entertainment category than it did in the game category.
the fact that the asus nexus gave more bad ratings in the entertainment category may be because it is a tablet and not all apps scale well to larger screens indicating that some devices may rate apps in different categories with varying criteria.
our findings further suggest that developers paid apps based on market share paid apps based on review share of other apps1 7number of devices missed a number of devices missed paid apps based on market share paid apps based on review share of other apps0 40percent of reviews missed b percent of review share missed figure number of devices and percent of review share missed if devices with the most reviews for all the remaining paid game apps are chosen percent of devicescumulative review share games ratings education ratings sports ratings business ratings entertainment ratings figure percent of android devices used to give x x ranges from of the reviews of free apps in categories working on android must be attentive of their analytics as it could help them identify problematic devices.
after comparing the results of our rqs for the free game apps with paid game apps and apps in four other categories we find evidence that our results do generalize.
our finding suggests that developers working on apps in other categories can also use our methods to better prioritize their qa efforts.
when working on apps in the entertainment business sports and education categories developers only need to focus their testing efforts on a small set of devices to cover the majority of the reviews for their apps.
moreover we find that many of the devices give different ratings to apps in these categories.
.
implications for developers while we think that the results from this study will be useful for developers device usage may have changed by the time most developers view this study.
the change is because of the rapid growth and evolution of the mobile industry where newer devicesare constantly being released .
thus developers should focus on our general findings and method rather than the device specific results.
for example developers can take away the idea of prioritizing their qa efforts on a subset of impactful devices rather than all devices.
moreover they can use our method of analyzing reviews per device to potentially identify devices that consistently give poor ratings.
once developers identify these problematic devices they can remove support for them to avoid additional qa and their bad reviews all together.
if developers feel that the additional downloads which can generate high ad revenue from these devices are worth potentially lower average ratings they can allocate additional qa resources for these devices.
while it is ultimately up to the developers to decide how they are going to prioritize their qa efforts we think that focusing on the devices that have the most impact on the app s ratings is an effective method especially because ratings are directly correlated to the number of downloads and thus the revenue generated by apps .
.
threats to v alidity .
construct validity we compared the bad medium and good reviews given from different devices to identify if certain devices give different and worse ratings than other devices.
note that we are not raising a causal link here in the paper.
we are not claiming that an app gets a poor rating because of a device.
we are just saying that apps get rated frequently and sometimes poorly from a small set of devices.
this is similar to the vast literature on using software metrics for qa prioritization.
such literature do not claim a causal link between software metrics and software defects but just suggest that software metrics like churn can be used to prioritize qa efforts.
the underlying reason for poorer or more frequent reviews from a particular device could be the hardware specification of the devices the os running on the devices or just that the people using a particular device may have a specific profile.
more research has to be conducted to identify the underlying causes.
note that data on user profile or which os version is running on a device is currently notavailable openly to be mined by researchers.
hence a major data collection effort has to be launched to examine these underlying factors.
.
internal validity since we limited our reviews to only the reviews from a few months i.e.
october 1st till january our data may not accurately represent ratings for the entire year or the entire life of devices.
however to mitigate this threat we made sure to apply statistical tests where applicable to ensure that our findings are statistically significant.
also note that we extract the device information from user reviews.
this information as far as we can tell is very accurate and cannot be faked since a user cannot manually change this information when posting a review.
the device information is automatically taken from the device from which the review is posted.
since we require reviews to contain the device information we had to ignore reviews that were not linked to a device.
to determine the impact of this issue on our findings we measured the average rating from reviews that were linked to a device and reviews that were not linked to a device.
we found that for free game apps the average rating for reviews that were linked to a device is .
while it is .
for reviews that were not linked to a device.
for paid game apps the average rating for reviews linked to a device is .
while it is .
for reviews that are not linked to a device.
if we consider all of the apps not just game apps we found that the average rating for reviews that are linked to a device is .
and for reviews that are not linked to a device is .
and we found similar results when we took each category of apps separately .
we performed the wilcoxon statistical significance test and found that there is a statistically significant difference for free game apps and all apps however there is no statistically significant difference for paid game apps.
in all statistically significant cases we find the rating tend to be lower for reviews that are linked to a device.
this finding indicates that reviews that are linked to a device are more critical and hence are more important to developers who are trying to avoid negative reviews.
user reviews can contain some spam reviews that serve as noise in our dataset .
to mitigate this issue and ensure the quality of the reviews used in our study we selected the most helpful reviews since they provide us with the most reliable information.
all of our findings are derived from user reviews.
in certain cases user reviews may not directly correlate with other measures of quality such as defects.
however prior research showed that user reviews are directly correlated with app revenues even for free apps which make their money through ads .
therefore we believe that using user reviews is a good proxy of success of an app.
.
external validity since our study was performed on mobile apps our results may not generalize to all game apps.
to address this threat we pick apps which are labeled as top apps by google play.
we feel that these apps are an appropriate representation of the apps in the game category and a better choice than hand picking apps.
in addition we extended our study to paid game apps and free apps from four other app categories in google play.
we found that our results were often consistent and sometimes even more pronounced in these apps when compared to our results for free game apps.
given that the android os and app ecosystems are quickly evolving the device specific analysis in this paper may not be applicable in a few years or even months .
however we would like to emphasize that the main takeaways from this study are not about specific devices but are about our generalizable method for prioritizing qa efforts.
.
conclusion validity we assume that testing apps or conducting focus groups for certain devices will find problems e.g.
bugs which can be fixed by the developer of an app thereby improving the quality and hence the revenues of the app.
even though it may seem logical it still is an assumption.
for example we did not verify whether the test effort prioritization does actually improve quality or increase revenues.
however this assumption testing finds bugs that can be fixed to improve quality is the basis for most testing efforts.
nevertheless more indepth studies are needed to study such assumptions.
.
conclusion this study seeks to help game app developers deal with android fragmentation by picking the devices that have the most impact on their app ratings thus aiding developers in prioritizing their qa efforts.
by studying the reviews of game apps we find that a small percentage of devices account for most of the reviews given to apps.
thus developers can focus their qa efforts on a small set of devices.
new developers can use data from other apps in the same app category to prioritize their testing and other qa efforts if they do not already have reviews for their app.
we also find that some devices give statistically significantly worse ratings than others.
therefore developers should identify particularly problematic devices and prioritize their qa efforts even further towards such devices.
finally we find that the results from the free game category to generalized to paid game apps and free apps in the four other categories that were examined.
in conclusion developers can adopt our method of analyzing android app reviews in order to effectively alleviate the qa challenges brought forth by android fragmentation.
in future work we would like to investigate the problems reported from each device and why certain android devices have certain kinds of common problems.
such common problems might induce a grouping of the devices such that testing on a single representative from a device group might be enough to discover all device specific problems for the devices in such group.
.
replication package to enable replication of our work we provide the data and the r code that we used for analyzing the data in an online repository under fse2014 .