coverage guided systematic concurrency testing chao wang nec laboratories americamahmoud said western michigan universityaarti gupta nec laboratories america abstract shared memory multi threaded programs are notoriously di fficult to test and because of the often astronomically large number of thread schedules testing all possible interleav ings is practically infeasible.
in this paper we propose a coverage guided systematic testing framework where we us e dynamically learned ordering constraints over shared obje ct accesses to select only high risk interleavings for test execution.
an interleaving is of high risk if it has not been cover ed by the ordering constraints meaning that it has concurrenc y scenarios that have not been tested.
our method consists of two components.
first we utilize dynamic information collected from good test runs to learn ordering constraints over the memory accessing and synchronization statements .
these ordering constraints are treated as likely invariant s since they are respected by all the tested runs.
second during the process of systematic testing we use the learned ordering constraints to guide the selection of interleavings for future test execution.
our experiments on public domain multithreaded c c programs show that by focusing on only the high risk interleavings rather than enumerating a ll possible interleavings our method can increase the covera ge of important concurrency scenarios with a reasonable cost and detect most of the concurrency bugs in practice.
categories and subject descriptors d. .
testing and debugging general terms verification reliability keywords concurrency coverage partial order reduction .
introduction real world concurrent programs are notoriously difficult to test because they often have an astronomically large numpermission to make digital or hard copies of all or part of thi s work for personal or classroom use is granted without fee provided th at copies are not made or distributed for profit or commercial advantage an d that copies bear this notice and the full citation on the first page.
to cop y otherwise to republish to post on servers or to redistribute to lists re quires prior specific permission and or a fee.
icse may waikiki honolulu hi usa copyright acm ... .
.ber of thread interleavings.
furthermore many concurrenc y related bugs arise only in rare situations making it difficul t for programmers to anticipate and for testers to trigger these error manifesting thread interleavings.
in reality the common practice of load or stress testing is not effective since the outcome is highly dependent on the underlying operating system which controls the thread scheduling.
merel y running the same test again and again does not guarantee that the erroneous interleaving would eventually show up.
typically in each testing environment the same interleav ings sometimes with minor variations tend to be exercised since the scheduler performs context switches at roughly th e same program locations.
systematic concurrency testing techniques offer a more promising solution to bug detection than standard load or stress testing.
these techniques typically use astateless model checking framework to systematically explore all possible thread interleavings with respect to a gi ven test input.
the model checking is stateless in that it directly searches over the space of feasible thread schedules and in so avoids storing the concrete program states chara cterized as combinations of values of the program variables this is in sharp contrast to classic software model checkers e.g.
which search over the concrete state space a well known cause of memory blowup.
in systematic concurrency testing the model checker is often implemented by using a specialized scheduler process to monitor as well as control the execution order of statemen ts of the program under test.
a program state sis represented implicitly by the sequence of events that leads the program from the initial state to s. this is based on the assumption that in a program where interleaving is the only source of nondeterminism executing the same event sequence always leads to the same state.
the state space exploration is conducted implicitly by running the program in its real execution environment again and again but each time under a different thread schedule.
therefore systematic concurrency testing can handle programs written in full fledged programming languages such as c c and java.
although systematic concurrency testing has advantages over the common practice of load or stress testing where we are at the mercy of the os thread library in triggering theright interleaving it is based on a rather brute force exhaustive search.
although it has been shown to be very effective in unit level testing e.g.
because of the of ten large number of interleavings such brute force exhaustiv e search is practically infeasible for realistic applicatio ns at a larger scale.
more specifically its exhaustive search tend s to cover all possible interleavings w.r.t.
a given test input in apermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
icse may waikiki honolulu hi usa copyright acm ... .
pre determined order without favoring one interleaving o ver another or considering the characteristics of the programs or properties to be tested.
although there exist techniques to reduce the cost of exhaustive search in stateless model checking such as dynami c partial order reduction dpor and preemptive context bounding pcb they are not effective for large programs.
for example dpor groups interleavings into equivalence classes and tests one representative from each equiv alence class.
it is a sound reduction in that it will not miss an y bug.
however in practice many equivalence classes themselves are redundant since they correspond to essentially t he same concurrency scenarios.
therefore exhaustively testi ng them not only is expensive but also rarely pays off.
we propose a coverage guided selective search where we continuously learn the ordering constraints over shared ob ject accesses in the hope of capturing the already tested concurrency scenarios then we use the learned information to guide the selection of interleavings to cover the unteste d scenarios.
since in practice programmers often make but sometimes fail to enforce implicit assumptions regarding concurrency control e.g.
certain blocks are intended to be mutually exclusive certain blocks are intended to be atomi c and certain operations are intended to be executed in a specific order.
concurrency related program failures are often the result of such implicit assumptions being broken e.g.
data races atomicity violations order violations etc.
w e try to infer such assumptions dynamically from the already tested interleavings and use them to identify high risk interleavings i.e.
interleavings that can break some of the learned assumptions.
although the programmer s intent may come from many sources e.g.
formal design documents and source code annotation they are often difficult to get in practice.
for example asking programmers to annotate code or write documents in a certain manner is often perceived as too much of a burden.
the more viable approach seems to be to infer them automatically.
fortunately the very fact that stress tests are less effective in triggering bug manifesting inte rleavings also implies that it is viable to dynamically learn the ordering constraints.
the reason is that if no program failure occurs during stress tests one can assume that the tested interleavings are good they satisfy the programmer s implicit assumptions .
in addition if the program source code is available the assumptions may also be mined from the code e.g.
.
in our coverage guided selective search framework we use a metric called history aware predecessor set hapset to capture the ordering constraints over the frequently occur ring and non erroneous interleavings.
hapsets can capture common characteristics of a relatively large set of interleavings.
during systematic testing we use hapsets as guidance to reduce the testing cost.
assuming that it is not practical to cover all possible interleavings we choos e to execute only those interleavings that are not yet covered by hapsets.
during systematic testing we also update the hapsets by continuously learning from the good interleavings generated in this process until there are no more inter leavings to explore or the desired bug coverage is achieved.
we have implemented the proposed techniques in a systematic testing tool called fusion which is designed for te sting multithreaded c c programs using linux posix threads pthreads .
using some public domain concurrent applications as benchmarks we show that by using hapsetsas guidance in systematic concurrency testing we can significantly reduce the testing cost while still maintaining the capability of detecting most of the concurrency bugs in prac tice.
more specifically in our preliminary experiments th e new selective search algorithm found all the bugs and at the same time was often orders of magnitude faster than exhaustive search .
.
preliminaries .
concurrent programs we consider a concurrent program with a finite number of threads as a state transition system.
threads may access local variables in their own stacks as well as global variable s in a shared heap.
program statements that read and or write global variables are called shared memory accessing statements.
program statements that access synchronization primitives are called synchronization statements.
program statements that read and or write only local variables are called local statements.
for ease of presentation we assume that there is only one statement per source code line.
letstmt be the set of all statements in the program.
then each st stmt corresponds to a unique pair of source code file name and line number.
a statement stmay be executed multiple times e.g.
when it is inside a loop or a subroutine or when stis executed in more than one thread.
each execution instance of stis called an event .
let ebe an event and let stmt e denote the statement generating e. an event is represented as a tuple tid type var where tidis the thread index type is the event type and varis a shared variable or synchronization object.
an event is of one of the following forms.
.
tid read var is a read from shared variable var .
tid write var is a write to shared variable var .
tid fork var creates the child thread var .
tid join var joins back the child thread var .
tid lock var acquires the lock variable var .
tid unlock var releases the lock variable var .
tid wait var waits on condition variable var .
tid notify var wakes up an event waiting on var .
tid notifyall var wakes up all events waiting on var.
in addition we use the generic event tid access var to capture all other shared resource accesses that cannot be classified as any of the above types e.g.
accesses to a socket .
we do not monitor thread local statements.
.
the state space we usesto denote the set of program states.
a transition se s advances the program from one state to a successor state by executing an event e. an event is enabled in state s if it is allowed to execute according to the program semantics.
we use se s to denote that event eis enabled in s and state s is the next state.
two events e1 e2may be co enabled if there exists a state sin which both of them are enabled.
for programs using pthreads or java threads a thread may be disabled due to three reasons i executing222lock var when varis held by another thread ii executingwait var when varhas not been notified by another thread iii executing join var when thread varhas not terminated.
an execution interleaving is a sequence s0 .
.
.
s nof states such that for all i n there exists a transition si 1ei si.
during systematic concurrency testing is stored in a search stack s. we call s san abstract state because unlike a concrete program state sdoes not store the actual valuation of all program variables.
however scontains concrete memory addresses in order to identify events accessing shared memory locations.
instead each s is implicitly represented by the sequence of executed event s leading the program from the initial state s0tos.
this is based on the assumption that executing the same event sequence leads to the same state.
two concurrent transitions are conflict independent if and only if the two events can neither disable nor enable each other and swapping their order of execution does not change the combined effect.
for example two events are conflict dependent if they access the same the object and at least one is a write modification and a lock acquire is conflict dependent with another lock acquire over the same lock variable.
in the partial order reduction literature e.g.
two interleavings are considered as equivalent iff they can be transformed into each other by repeatedly swapping the adjacent and conflict independent transitions.
.
predecessor sets an execution s0.
.
.
s ndefines a total order over the set of memory accessing and synchronization events.
the predecessor set pset was designed to efficiently capture the event ordering constraints common to a potentially large set of executions.
to this end pset was defined over memory accessing statements i.e.
between read write operations that may be executed adjacent to each other.
synchronization statements were ignored this did not suit our purpose we extended it in our definition of hapset .
definition pset .
given a set 1 .
.
.
n of interleavings and a memory accessing statement st stmt .
the predecessor set denoted pset is a set st1 .
.
.
st k of statements such that for all i i k an event produced by statement stis immediately dependent upon an event produced by statement stiin some interleaving j where j n. event eisimmediately dependent upon eiif and only if ei is the last event in its thread accessing the same object as t and it is also dependent with e. both stand stiare statements rather than events which are distinct executio n instances of these statements.
this is meant to keep psets general enough so that the psets learned from one correct interleaving remain valid for another correct interleavin g possibly under a different program input.
psets are powerful enough to characterize the common bug patterns such as data races atomicity violations and order violation.
since each pset has at most stmt elements the space to store all psets is o stmt in the worst case.
on average however the psets learned from real world programs are usually very small.
the study in showed that of the program statements have an empty pset and for well designed test suites typically ittakes less than interleavings for pset learning to converge i.e.
few new updates are possible afterward.
.
ordering constraints although psets are efficient in capturing ordering constraints common to a large set of thread interleavings as a coverage metric it does not suit our purpose well.
this is because the applications are different.
in psets are co llected from good runs during testing and then treated as program invariants during production runs.
a special purpose microprocessor is designed to ensure that the psets are always obeyed with checkpoints and rollbacks upon pset violations .
the rationale is that if psets capture the concu rrency scenarios of the tested interleavings then by allowi ng only pset obeying interleavings in production runs one ca n steer away from program failures even if the programs are still buggy.
in this paper our goal is not runtime failure avoidance as in but to improve the coverage during testing.
the main difference is that for failure avoidance it is acceptable if some already tested interleavings are not captured by the psets as long as they are rare disallowing them in production runs will not hurt performance much .
however for testing it is crucial to capture what has already been tested since the purpose is to prevent the same concurrency scenario from being tested again.
.
history aware predecessor sets we extend the idea of pset to define a new coverage metric called hapset1.
there are two main differences between hapsets and psets.
first we consider both synchronization statements e.g.
lock acquires as well as memory accessin g statements in the definition of hapset.
second for each st stmt in addition to the fields fileandline we include thrandctx where thris the thread that executes standctx is the call stack at the time stis executed.
the reason is as follows with file line there remains some degree of ambiguity regarding the statement which produces an event at run time.
for example the same statement may be executed in multiple function method call contexts or from multipl e threads.
in many cases especially in object oriented programs such information is useful and should be included in order to capture any meaningful ordering constraint.
since at run time both the number of threads and the number of distinct calling contexts can be large to avoid memory blowup ctxonly stores the most recent k some small number in our experiments entries in the call stack andthronly takes two values means it is the local thread and means it is the remote thread.
let eand e be two events in an interleaving such that stmt e stand stmt e st we have st.thr and st .thr when tid e tid e and st.thr and st .thr when tid e tid e .
we do not consider tid e tid e since it never triggers the hapset update.
formally statement stis now defined as a tuple file line thr ctx where file is the file name lineis the line number thr is the thread and ctxis the truncated calling context.
definition hapset .
given a set 1 .
.
.
n of interleavings and a shared memory accessing or synchroniz ation statement st stmt .
the history aware predecessor set or hapset is a set st1 .
.
.
st k of statements such 1pronounced as happy set.
223thread t1 thread t2 alloc e1 p a use e2 if p ne ationslash e3 p free e4 p e4 w p e2 r p e1 w p e3 r p e3 w a figure serial execution of intended atomic e2e3 .
that for all i i k an event eproduced by stis immediately dependent upon an event eiproduced by stiin some interleaving j where j n. note that this metric includes both syntactic and semantic elements.
data conflicts are at the heart of most concurrency errors data races atomicity violations etc.
the se are tracked to make this metric relevant for the purpose of finding bugs.
however a generalization is achieved by associating it syntactically with statements rather than wi th events.
the thread index is again designed to distinguish between two threads for catching bugs but abstracts over specific thread ids thereby ensuring that it is scalable ove r many threads.
finally by including a bounded functional context we provide some measure of context sensitivity t his is especially useful for object oriented programs.
example.
consider figure which has two threads t1 t2 sharing the pointer p. assume that p initially.
in the given execution pis first initialized in e1 then used in e2 e3 and finally freed in e4.
we assume e1 e4are statements in the form file line thr ctx .
since e1is the last statement before e2and they have a data conflict we add e1 tohapset .
for e3we do not add any statement into hapset because e2is the last statement accessing pbut it is from the same thread hence no conflict .
we add e3to hapset since e3precedes e4in the given execution and they have a data conflict.
to sum up the hapsets learned from this execution are as follows hapset hapset e1 hapset hapset e3 .
.
why hapsets are useful?
we show that the seemingly simple hapsets are capable of capture subtle concurrency control patterns.
.
.
atomicity violation consider figure again where the block containing e2 e3 is meant to be executed atomically it first confirms that pointer pis not null and then stores to the pointed memory location.
therefore whether e2ande3are two consecutive reads of an interleaving is key to deciding whether the interleaving is buggy.
hapsets can capture this atomicity constraint in all good runs where atomicity is not violated hapset is always empty.
this is because although e1 e4 can be executed either before e2or after e3 event e3is always preceded by e2.
therefore neither e1nore4can appear inhapset .
second e2 ne ationslash hapset because e3 insteadthread t1 a do b lock a c tmp x d unlock a e while tmp thread t2 f lock a gx h unlock a execution c1 r x ... c2 r x ... g w x cn r x figure the busy waiting example.
without hapset there will be excessive backtracking.
ofe2 always precedes e4.
therefore the hapsets leaned from all the good runs are as follows hapset e2 hapset e1 e4 hapset hapset e3 .
during testing it is more fruitful to test interleavings th at have not been covered by the above hapsets.
one such interleaving is e1e2e4e3 which violates the atomicity and leads to the dereference of a null pointer.
note that corresponds to hapset e4 andhapset e2 .
.
.
busy waiting hapsets can be used to avoid the excessive testing of certain interleavings that do not offer any new concurrency scenario.
consider figure as an example.
there are two threads t1 t2communicating via variable x. assume that x initially.
in the given execution abcde kfghabcde the loop in t1is executed ktimes before gin thread t2is executed.
without using hapsets systematic testing would have to test a potentially large set of interleavings each with a di fferent number of loop iterations.
this is because strictly speaking none of these interleavings are equivalent to oth ers therefore based on the theory of partial order reducti on one needs to test all of them.
however such tests are often wasteful since they rarely lead to additional bugs.
the hapsets computed on these interleavings are hapset c hapset g hapset f hapset b .
this is because some instances of statement c orf are immediately dependent on instances of g orb and vice versa.
except for recursive locks we ignore unlock statements when computing hapsets.
when using hapsets as guidance we can avoid the aforementioned excessive backtracking because none of these interleavings can offer a concurrency scenario that has not been covered by the hapsets.
.
learning from good runs for our guided search to be effective we need to learn hapsets from a diversified set of interleavings.
the quality of the learned hapsets will be affected by both the test cases and the thread schedules.
randomized delay can be added to the scheduler to diversify the thread interleavings.
in t his testing environment the program is executed under the control of a scheduler process which is capable of controlling the order of operations from different threads.
these contro l points are inserted into the program source code automatically via an instrumentation phase before the source code is compiled into an executable.
for hapset learning we maintain the following data structures a set hapset for each statement st stmt and224a search stack sof abstract states s0.
.
.
s n where s0is the initial state and snis the final state of the interleaving.
recall that each s sis an abstract state because sdoes not store the actual valuations of program variables.
let si.sel be the event executed at siin the given interleaving in order to reach si .
the pseudo code of our hapset learning is presented in algorithm .
the procedure randctest takes the initial state s0as input and generates the first interleaving with a randomized thread schedule.
each state s sis associated with a set s.enabled of events.
recall for example that a lock acquire would be considered as disabled at s if the lock is held by another thread.
similarly a wait would be considered as disabled at s if the notification has not been sent.
at each execution step we randomly pick an event e s.enabled execute it from s which leads to state s .
note that the thread schedules ultimately are still determined by the underlying operating system.
this ensures that all the generated interleavings are real.
if any of them can trigger a program failure then it is a real bug.
otherwise all of them are assumed to be good runs in that they expose the desired program behavior.
algorithm learning from good test runs initially for all statements st hapset is empty sis an empty stack randctest s0 randctest s s.push s learnhapsets s learning hapsets while s.enabled is not empty let ebe a randomly chosen item from s.enabled delay thread tid e for a random period let s.sel e let s be the new state after executing se s randctest s s.pop s learnhapsets s if s ne ationslash s0 let sp sbe the state preceding s traverse stack s for each thread find the last state sd.
where sd.selandsp.selaccess the same object if sd.selandsp.selhave a data conflict let stp stmt sp.sel let std stmt sd.sel hapset hapset std during each run we invoke learnhapsets at every execution step.
the input to this procedure is the newly reached state s. let spbe the state prior to reaching the current state s and sp.selbe the event executed between spands.
for each thread we find the last executed event sd.selsuch that sd.seland sp.selaccess the same object they are executed by different threads and there is a data conflict read write write write lock lock or wait not ify .
if such an sd.selexists we add the statement stmt sd.sel into the hapset of stmt sp.sel .
.
systematic testing as we mentioned earlier systematically testing all possible interleavings can be achieved using stateless model checking.
it can be viewed as a natural extension of randctest in algorithm .
however the scheduler here has total control in deciding the thread schedule.
.
overall algorithm the overall algorithm is illustrated in algorithm by procedure sysctest .
it checks all possible thread schedules of the program for a given test input.
algorithm systematic concurrency testing framework initially sis an empty stack sysctest s0 sysctest s s.push s updatebacktrack s let t idsuch that t s.enabled tid t s.backtrack s.done while t tid t s.backtrack andt ne ationslash s.done s.done s.done t let s.sel t let s be the new state after executing st s sysctest s s.pop updatebacktrack s for each t s.enabled let sd sandsd.selbe the latest event such that sd.selis dependent and may be co enabled with t if such sdexists sd.backtrack sd.backtrack btset sd t in addition to s.enabled each shas an associated subsets.done s.enabled of events recording the scheduling choices made at sin some previous test runs.
furthermore each shas an associated set s.backtrack consisting of a subset of the enabled threads at s. each s.backtrack represents a future scheduling choice at s i.e.
thread will be executed at sin some future test run.
the procedure sysctest takes state sas input where s0 is used for the initial call.
at each step it first invokes sub routine updatebacktrack to update backtracking points at some previous state s s. backtracking will be explained in the next paragraph.
then from s.backtrack it picks an enabled thread to execute leading to a distinct thread interleaving.
the recursive call at line returns only after the interleaving ends and we have backtracked to state s. at this point s.backtrack must have been updated by some previous call to sysctest it may contain some threads other than meaning that executing them as opposed to from state smay lead to different interleavings.
the entire procedure terminates when we backtrack from state s0eventually.
since we do not store the concrete program states in s backtracking to a state s is implemented by re starting the test run and then applying the225same thread schedule till state s is reached again.
in the naive approach at every state s s s.backtrack consists of all the enabled threads.
the set of interleaving s generated by this naive algorithm is the same as the set of possible interleavings generated by the actual program execution.
however the naive approach may end up testing many redundant interleavings.
updatebacktrack s is designed to remove some of the redundant interleavings.
it takes the current state as input and iterates through all the enabled event t s.enabled to find the latest event sd.sel that is dependent and may be co enabled with t. if such an sdexists it means that if we flip the execution order from sd.sel .
.
.
t tot .
.
.
s d.sel the new interleaving will not be equivalent to the current one.
in practice the various systematic concurrency testing tools differ mainly in their way s of computing the backtrack set.
.
backtracking baseline and variations the baseline algorithm is only slightly different from the naive algorithm.
that is bt set tid q q sd.enabled it is still more efficient than the naive algorithm since it adds btset only at state sd as opposed to every state .
for example consider the case where sddoes not exist in line .
in this case tis independent with all the previously executed events sd.selfor all sd s and swapping the execution order of tand sd.selwould not lead to a new equivalence class.
the baseline algorithm would not add any backtrack point for such cases.
.
.
preemptive context bounding pcb traditionally a context switch is defined as the computing process of storing and restoring the cpu state context when executing a concurrent program such that multiple threads can share the same cpu resource.
the idea of using context bounding to reduce complexity of software verification was first introduced for static analysis and later f or testing .
it has since become an influential techniques since in practice many concurrency bugs can be exposed by interleavings with few context switches.
in this setting bt set tid q q sd.enabled andcb sd q mcb where cb sd q is the number of context switches after executing qatsd and mcb is the maximal number of context switches allowed.
from state sd one can execute qonly if the number of context switches will not exceed the bound.
although pcb can skip many interleavings for the ones with mcb context switches we still need exhaustive search.
for large programs even with small bound e.g.
or the number of interleavings is still extremely large.
.
.
dynamic partial order reduction dpor partial order reduction is based grouping interleavings into equivalence classes and then testing only one represen tative from each equivalence class.
it is a well studied topi c in model checking.
for concurrency testing the most advanced technique is the dpor algorithm by flanagan and godefroid .
btset is computed by algorithm .
first we search for an event q sd.enabled such that there exists a happens before relation between qand the currently enabled event t. intuitively qhappens before tin an interleaving if either a we cannot execute tbefore qdue to program semantics or b swapping the execution order of qandtwould lead to a different equivalence class.
obviously qhappens before tif they are from the same thread.
other examples include qandtare from different threads but have data conflict over a shared object and there exist events r sin the interleaving such that qhappens before r rhappens before s and shappens before t. the happensbefore relation is transitive cf.
.
algorithm computing the backtrack set in dpor.
let q sd.enabled such that either tid q tid t or there is a happens before relation between qandt if such qexists btset tid q else btset tid q q sd.enabled if such qexists then we have a reduction we only need to add tid q tosd.backtrack since executing thread tid q is necessary for the purpose of swapping tand sd.sel.
in por theory this backtrack set is called a persistent set .
otherwise we do not have reduction and have to resort to the baseline to add all enabled threads to sd.backtrack .
although partial order reduction is sound in that it never misses real bugs in practice the number of interleavings after dpor can still be very large.
.
guiding systematic testing our algorithm builds on algorithm .
in contrast to the exhaustive search in dpor and pcb we use hapsets learned from the already tested good runs to the selection of the next interleaving.
we will explain first how to use hapsets to select interleavings and then how to continuously update the hapsets.
.
guiding interleaving selection we achieve this by modifying the implementation of subroutine updatebacktrack .
recall that in algorithm line of updatebacktrack searches through the stack sto find the last event sd.selthat is dependent and may be co enabled with t. if such an sd.selexists it means that swapping the execution order from sd.sel .
.
.
t tot .
.
.
s d.sel would produce a different interleaving.
in the modified version we insist that in addition to the condition in line the following hapset related condition must hold stmt t ne ationslash hapset .
note that if stmt t is not in the hapset of stmt sd.sel it means that in all tested runs the statement that generates sd.selhas never been immediately dependent upon the statement that generates t. in this case the new execution order t .
.
.
s d.selrepresents a concurrency scenario that has never been covered by the previous test runs.
on the other hand if stmt t is already in the hapset of stmt sd.sel the new interleaving would have a lower risk because this concurrency scenario has been covered previously.
algorithm illustrates our new procedure updatebacktrack for hapset guided selective search.
one of the main advantages of our hapset guided search is that it fits naturally into the existing flow of systematic testing.
the addition of hapset guided search requires only small changes to the software architecture.
the guidance from hapsets affect only our selection of state sd line .
once sdis selected the backtrack set can be computed independently.
this means we can choose to use the various existing methods to226algorithm guiding the systematic testing with dpor updatebacktrack s for each t s.enabled let sd sandsd.selbe the latest event such that sd.selis dependent and may be co enabled with t stmt t ne ationslash hapset guiding if such sdexists sd.backtrack sd.backtrack btset sd t compute btset .
in practice we have found that both pcb and dpor work well under the guidance of hapsets although combining hapset with dpor often performs slightly better.
note that hapset guidance effectively prunes away large subspaces in the search.
unlike dpor this pruning is not safe i.e.
it may miss errors.
this is the basic tradeoff we make to gain scalability and performance.
at this point one may contemplate the possibility of using hapsets with both dpor and pcb.
we caution that there is a theoretical difficulty in soundly combining pcb with dpor or any persistent set based por in the first place.
pcb and dpor fundamentally are not compatible because if you use both and also set the context bound is k some equivalence classes may be missed completely even if they actually contain some interleavings with cb k. in musuvathi and qadeer designed a method to combine a sleep set based por with context bounding and their method is quite involved but to our knowledge there has been no method for soundly combining persistent set based por such as dpor with context bounding.
.
continuous learning in our guided search framework the quality of hapsets is very important.
although we can diversify thread schedules via randomization the training runs may still miss many concurrency scenarios.
the interleaving encountered duri ng the guided search may contain these missing concurrency scenarios and therefore are complementary to the initial learning.
therefore we update the initial hapsets during systematic testing by continuously learning from the tested good interleavings.
continuous learning is made possibl e by the fact that unless a bug is detected the interleaving checked by systematic testing is always a good run.
algorithm illustrates the overall selective search algorithm wherein the call to learnhapsets at line allows for continuous learning of hapsets.
the learning subroutin e is the same as the one used in algorithm .
the nice thing about continuous learning is that the good interleavings produced by systematic testing are freely av ailable since they are byproducts of the search.
the more concurrency scenarios we capture using the hapsets the less number of interleavings would need to be tested in the future.
this ensures progress with respect to the hapset coverage metric.
therefore on the fly updating hapsets al lows the guided search to become a self improving process making the whole process converge much faster.
example.
consider figure again.
assume that the first interleaving is 1 s0a s1f s2g .
.
.
s 5b s6c .
.
..algorithm continuous learning within systematic testing initially sis an empty stack guidedctest s0 guidedctest s s.push s learnhapsets s continuous learning updatebacktrack s let t idsuch that t s.enabled tid t s.backtrack s.done while t tid t s.backtrack andt ne ationslash s.done s.done s.done t let s be the new state after executing st s guidedctest s s.pop the hapsets computed from 1via continuous learning arehapset g hapset f .
furthermore the dpor backtrack sets will be s1.backtrack and s2.backtrack since thread is disabled at state s2.
according to our guided search algorithm the next interleaving to be executed is 2 s0a s1b .
.
.. the new hapsets computed from 2arehapset c hapset b .
after that however our guided search algorithm will allow no other interleavings.
a key point illustrated by the above example is that pruning actually happens at states like s1where locking statements are executed not when memory accessing statements c g are executed.
this is why we need to include synchronizations in the definition of hapset.
in fact if we use only memory accessing statements as in the definition of pset there will be no pruning possible for figure .
.
experiments we have implemented the proposed method in a tool called fusion .
the tool is capable of testing multithreaded c c programs in linux written using the posix thread library.
we use source code instrumentation to add the monitoring and control points to the program in order to control the memory accessing and synchronization statements at run time.
our implementation is based on the c c front end from edison design group.
the instrumentation consists of two steps before each shared memory access it also inserts a request to the scheduler asking for permission to execute before each pthreads library routine it inserts a request to the scheduler.
since identifying a priori the set of memory locations that may be shared by more than one thread is difficult due to pointers and heap allocated data structures we use a light weight intra procedural escap e analysis to conservatively decide whether a statement may access the shared memory.
this is a sound approximation because treating a local statement as if it is shared poses no threat to the correctness of our testing tool it merely increases the monitoring control overhead at runtime.
in addition system calls that may block the calling thread need to be monitored.
this includes for example socket communication routines e.g.
select send recv and system calls that use the real time information e.g.
usleep pthread cond timedwait .
such system calls are properly227table comparison of hapset dpor and pcb with various bou nds on the thrift lib cpp example.
test program hapset dpor pcb0 pcb1 pcb2 pcb3 name loc thrds bug type runs time s runs time s runs time s runs time s runs time s runs time s thrift lib w2 5t .5k deadlk .
.
no .
.
.
to thrift lib w3 5t .5k deadlk .
to to .
to to thrift lib w4 5t .5k deadlk .
to to to to to thrift lib w5 5t .5k deadlk .
to to to to to modeled by the scheduler of out testing tool.
we conducted experiments on some real world c c applications written for the linux pthreads platform.
all benchmarks are from the public domain accompanied by test cases to facilitate concrete execution.
our experimen ts were conducted on a workstation with .
ghz pentium d processor and 2gb memory.
we have compared the runtime performance as well as the bug detecting capability of the following methods hapset dpor and pcb.
here hapset is our guided search algorithm.
dpor is the original algorithm in .
for pcb we set the context bounds to ... for a fair comparison of these methods we skipped a priori hapset learning sessions while relying solely on continuous learning to infer the hapsets.
.
the thrift c library our first set of benchmarks come from the thrift c library part of a software framework used by facebook for cross language services development.
the library has .
k lines of c code.
we used the version from the current main development trunk.
the test program is also from the main trunk as part of the make check script.
there is a known deadlock error inside the concurrency package which itself is a thin c layer wrapping up pthreads mutex and condition variable routines to support thread pool and task management.
the original test program was written for stress tests.
it upfront creates hundreds of worker threads and tens of thousands of tasks to run in parallel.
this is a typical way of creating a heavy workload hoping to increase the odds of triggering some rare and bug manifesting interleavings.
with systematic testing we do not need that many threads tasks to expose bugs.
therefore we set the number of threads from to and with on average tasks per threads.
we compared the performance of the three methods.
the results are shown in table .
the first four columns show the name the lines of code the number of threads and the bug type.
here thrift lib w2 5t for example stands for the test case with worker threads and tasks per worker.
the remaining columns show the performance of each method including the number of interleavings tested and the run time in seconds.
we set the time bound to minutes per method i.e.
to in the table means timed out in seconds without finding a bug.
the results show that hapset found all the bugs and was also fast.
furthermore it scaled well as we increased the number of concurrent threads.
in comparison dpor found a bug for the worker case while timed out for the other cases.
for pcb0 with mcb it terminated in .
seconds and missed the bug.
our experience shows that in general pcb0 is not effective since it frequently misses real bugs.
with context bound set to pcb found the bugs for the worker and work cases.
however pcb did not scale as well when we increased the number of threads orthe context bound.
all three algorithms have significant runtime overhead in comparison to a native test execution.
depending on the types of target programs i.e.
cpu bound or communicationbound the slowdown ranges from 10x to 100x.
this overhead comes from two sources.
first in order to control the nondeterminism in executing concurrent programs the scheduler insists that at any time only one thread is allowed to execute.
this essentially serializes a concurrent execution.
second the monitoring and control of memoryaccessing events often have large overhead.
for thrift lib w2 5t although hapset checked runs it actually spent more time than what dpor spent on checking runs.
this is because not all these runs are included in the runs and each run may execute a different set of statements and therefore may take a different amount of time.
furthermore both hapset learning and guiding have some computational overhead.
.
the aget pbzip pfscan benchmarks our second set of benchmarks are medium size applications downloaded from the sourceforge.net .
they include aget .
a ftp client capability of concurrently downloading different segments of a large file pbzip2 .
.
a parallel implementation of bzip2 for file compression and decompression and pfscan .
a concurrent file scanner that combines the functionality of find xargs and fgrep .
first we compared the performance of the three methods.
the results are shown in table .
for these examples hapset found all the bugs and was also the fastest whereas both dpor and pcb2 timed out on pbz2 f and pfscan .
table comparison of hapset dpor and pcb2 on the aget pbzip2 pfscan examples test program hapset dpor pcb2 name loc bug thr runs time runs time runs time aget .2k race .
pbzip2 .9k order .
.
.
pbz2 f .9k race .
to to pfscan deadlk .
to to inaget there is a data race over a variable called bwritten which is shared by the multiple downloading worker threads and a separate thread updating the progress bar.
in pbzip2 there is an order violation between the main thread and the consumer threads sometimes causing a segmentation fault as a result of null pointer dereferencing.
after fixing this b ug pbz2 f our tool found a previously unreported data race over variables outputbuff .buf and outputbuff .bufsize between the consumer threads and the filewriter thread.
this is a real bug that may cause corrupted file output.228inpfscan there is an injected order violation where a variable called aworkers if initialized too late in time may cause the main thread to hang.
on aget we also compared the various settings of pcb with mcbfrom to to assess its scalability.
the results in table show that pcb0 timed out after minutes without finding the bug.
with all the other settings pcb found the bug.
although pcb1 has the best performance for this example we caution that in general one needs at least pcb2 since even the simplest atomicity violations need at least t wo context switches to trigger with pcb1 no program failure caused by atomicity violation can be detected.
table pcb with various context bounds on aget pcb0 pcb1 pcb2 pcb3 pcb4 runs time runs time runs time runs time runs time to .
on aget we also assessed the scalability of hapset by gradually increasing the number of worker threads from to .
the results in table show that the number of interleavings tested by hapset before the bug is detected grows only modestly.
this is mainly due to the abstraction over specific thread ids that we use in the definition of hapsets.
this is in contrast to both dpor and pcb where the number of interleavings typically grow exponentially a s we increase the number of threads.
table hapset on various threads of aget threads threads threads threads threads runs time runs time runs time runs time runs time .
.
.
.
.
threads threads threads threads runs time runs time runs time runs time .
.
.
.
.
extracted mozilla mysql bugs recall that dpor is a sound reduction whereas both pcb and hapset are unsound and in theory may miss bugs.
our results in previous subsections show that the reduction by hapset can be significant.
therefore a natural question is would it reduce too much to miss many bugs?
extracted bug samples.
each sample is a small program showcasing a bug extracted from the real code of mozilla and mysql.
these examples were kindly provided by the authors of .
for testing purposes we inserted some arbitrary shared memory accesses and locking statements to make them nontrivial.
since the programs are small the emphasis here is not on comparing the runtime performance since all three competing methods can finish quickly.
rather we would like to compare their bug finding capability.
table shows the experimental results.
the first two columns show the names and the bugs targeted by the test case.
here atom means an atomicity violation order means an order violation and deadlk means a deadlock.
eventually the program failures caused by these atomicity order violations are either segmentation faults or corrupted dat a. the next six columns compare the number of interleavings and the run time in seconds .
hapset not only is fast buttable comparison on extracted but real bugs test program hapset dpor pcb2 name bug runs time runs time runs time mysqllog atom .
.
.
nodestate order .
.
.
loadscript atom .
.
.
seektoitem atom .
.
.
updatetimer atom .
.
.
filetransport deadlk .
.
.
createthread order .
.
.
readwriteproc order .
.
.
openinputstr deadlk .
.
.
httpconnect order .
.
.
timerthread deadlk .
.
.
also finds all the bugs despite that it skips most of the interleavings explored by dpor and pcb2.
this provides strong evidence supporting our claim that in practice the drastic interleaving reduction achieved by hapset does not cause systematic testing to miss many real bugs.
.
related work the notion of predecessor set was first introduced by yu and narayanasamy .
their goal was runtime failure avoidance for which the psets learned during testing were encoded into the program s executable and designed a specia lpurpose microprocessor to ensure these pset constraints du ring the production runs.
in contrast our goal in testing is to try to trigger previously untested concurrency scenario s. to this end we have extended their idea to define the new metric called hapset.
as we have already explained in previous sections our work is related to systematic testing techniques based on stateless model checking but the interleaving selection in these methods are not guided by any coverage metric.
among classic model checkers spin and java pathfinder are closely related but they are based on manipulating concrete program states rather than stateless model checking.
ctrigger and calfuzzer are two testing tools that also use dynamically collected information.
ctrigger is based on the notion of access invariants i.e.
the atomicity of two consecutive memory accessing events and calfuzzer is based on detecting potential data races.
in comparison the hapsets used in our method are more general since they can characterize concurrency patterns that subsume data races and three access atomicity violations.
more recently shi et al.
proposed learning def use invariants over correct exe cutions of both sequential and multithreaded programs and then using such invariants to prune false positives and detect bugs.
however hapsets are significantly different from def use invariants in that they are defined also for two conflicting writes or for one write with a preceding read and the two involved accesses must come from different threads.
more importantly our interleaving selection is systemati c and each test run is guaranteed to exercise a not yet tested interleaving whereas in the other methods int erleaving selection is achieved by inserting sleep statements to certain program points to increase the odds of triggerin g certain interleavings.
therefore they typically do not hav e a guarantee of progress.
our method is also related to the various runtime error229detection algorithms e.g.
and .
these methods focus on analyzing a given interleaving to either de tect bugs or predict bugs in some other related interleavin gs.
these methods are orthogonal to ours since our method can systematically generate new interleavings to feed to these methods.
our method is also different from the various testing techniques based on randomization e.g.
ibm s contest and although randomization can be used to diversify the input to our hapset learning.
.
conclusions we have proposed a coverage guided systematic concurrency testing algorithm where ordering constraints learn ed from the good test runs are used to guide the selection of high risk interleavings for future test execution.
we propose hapsets to capture these ordering constraints and use them as a metric to cover important concurrency scenarios.
this selective search strategy in comparison to exhaustiv ely testing all possible interleavings can significantly incr ease the coverage of important concurrency scenarios with a reasonable cost while maintaining the capability of detectin g subtle bugs manifested only by rare interleavings.
.