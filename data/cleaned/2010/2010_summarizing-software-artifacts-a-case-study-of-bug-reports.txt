see discussions st ats and author pr ofiles f or this public ation at .researchgate.ne t public ation summarizing software artifacts a case study of bug reports conf erence paper may .
.
sour ce dblp citations 202reads author s including sarah r astk ar univ ersity of british columbia publica tions citations see profile gail murphy univ ersity of british columbia publica tions citations see profile all c ontent f ollo wing this p age was uplo aded b y gail murphy on june .
the user has r equest ed enhanc ement of the do wnlo aded file.summarizing software artifacts a case study of bug reports sarah rastkar gail c. murphy and gabriel murray department of computer science university of british columbia rastkar murphy gabrielm cs.ubc.ca abstract many software artifacts are created maintained and evolved as part of a software development project.
as software devel opers work on a project they interact with existing projectartifacts performing such activities as reading previously filed bug reports in search of duplicate reports.
these activities often require a developer to peruse a substantial amountof text.
in this paper we investigate whether it is possible tosummarize software artifacts automatically and effectivelyso that developers could consult smaller summaries instead of entire artifacts.
to provide focus to our investigation we consider the generation of summaries for bug reports.we found that existing conversation based generators canproduce better results than random generators and that agenerator trained specifically on bug reports can perform statistically better than existing conversation based generators.
we demonstrate that humans also find these generated summaries reasonable indicating that summaries might beused effectively for many tasks.
categories and subject descriptors d. .
general terms experimentation keywords machine learning human centric software engineering .
introduction individuals outside the profession of software development sometimes incorrectly believe that the profession is all aboutprogramming.
those involved in software development knowthat the profession has a strong component of informationmanagement.
any successful large and complex software permission to make digital or hard copies of all or part of this work for personal or classroom use is granted w ithout fee provided that copies are not made or distributed for profit or co mmercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise torepublish to post on servers or to redist ribute to lists requires prior specific permission and or a fee.icse may cape town south africa copyright acm ... .
.system requires the creation and management of many artifacts requirements designs bug reports and source code with embedded documentation to name just a few.
to perform work on the system a software developer must often read and understand artifacts associated with the sys tem development.
for example a developer attempting to fix a performance bug on a system may be told that a similar bug was solved six months ago.
finding the bug report thatcaptured the knowledge about what was fixed will likely re quire the developer to perform searches and read several bugreports in search of the report of interest.
each report readmay contain several sentences of description as well as tens of sentences representing discussion amongst team members.
for example bug from the mozilla system 1comprises sentences in the description and sentencesfrom comments.
if solving the bug requires an understanding of the java library weakhashmap class another sentences from documentation about that class will need to be read.
wading through text comprising system artifactsto determine which ones matter can be time consuming andfrustrating for the developer.
sometimes the amount of information may be overwhelming causing searches to be abandoned and duplicate or non optimized work to be performed all because the previous history of the project has been ignored.
one way to reduce the time a developer spends getting to the right artifacts to perform their work is to provide asummary of each artifact.
an accurate summary can enable a developer to reduce the time spent perusing artifacts that have been returned from search es found through browsing or recommended by team members or tools.
perhaps optimally the authors of system artifacts would write a suitableabstract to help other developers working on the system.given the evolving nature of artifacts and the limited timeavailable to developers this optimal path is not likely to occur.
alternatively it might be possible to generate summaries of project artifacts saving developers effort and enabling up to date summaries on demand.
in this paper we inves tigate the possibility of automatic summary generation focusing on one kind of project artifact bug reports to make the investigation tractable.
we chose to focus on these reports as there are a number of cases in which developers may make use of existing bug reports such as when triagingbugs e.g.
or when performing change tasks e.g.
and these reports can often be lengthy involving discussions amongst multiple team members.
1mozilla.org verified prepress proof file causal productions 1the problem of summarizing discussions amongst multiple people is similar to the problem of summarizing email and meeting discussions section .
we build on existingwork in this area investigating whether existing machine learning based approaches classifiers for generating extrac tive summaries can produce accu rate summaries for bug reports.
these approaches assign a zero or one value to each sentence in the bug report based on features of the sentence sentences assigned a one value appear in a generated sum mary.
to enable this investigation we had human annotators create summaries for bug reports creating a corpus of bug report data section .
these bug reports included both reports about defects and reports about enhancements.
we then applied existing classifiers trained on email and email and meeting data to produce summaries for reports in the bug report corpus.
we also trained a classifier spe cific to bug reports and applied it to the bug report corpus section .our bug report corpus serves as the training set for the bug report classifier and the testing set for all three classifiers.
we measured the effectiveness of these classifiers based on several computed measures section .
we found that all three classifiers perform well above the chance level arandom classifier .
we also found that the bug report classifier having a precision of more than out performs the other two classifiers in generating summaries of bug reports.
to evaluate whether a measure of produces summaries useful for developers we had human judges evaluate the goodness of a subset of the summaries produced by the bug report classifier.
on a scale of low to high the arithmetic mean quality ranking of the generated summaries by the human judges is .
.
.
this average rating suggests that the generated summaries can provide a sufficiently accurate summary for developers saving them the need inall cases to read lengthy bug reports.
this paper makes three contributions.
it demonstrates that it is possible to generate accurate summaries for one kind of project artifact bug reports.
it reports on the creation of a corpus of humanannotated bug report data from four different systems.
it demonstrates that while existing classifiers trained for other conversation based genres can work reasonably well a classifier trained specifically for bug re ports scores the highest on standard measures.
.
related work with the growing number of electronically available information there is substantial interest and a substantial bodyof work in the automated generation of summaries for thatinformation.
two basic approaches have been taken to gen erating summaries extractive and abstractive .
an extractive approach selects a subset of existing sentences to form the summary.
an abstractive approach builds an internal semantic representation of the text and then applies natural language processing techniques to create a summary.as the current state of the art in abstractive techniques hasnot yet supported meaningful application we focus in thispaper on extractive techniques.
extractive based summarization approaches have been applied to many kinds of conversations including meetings telephone conversations and emails .
some of these approaches have used domain specific characteristics such as email header information .
murray and carenini developed a summarizer for conversations in various modal ities that uses features inherent to all multi party conversa tions.
they applied this system to meetings and emails andfound that the general conversation system was competitive with state of the art domain specific systems in both cases.
in this work we investigate whether we can use thisgeneral conversation system to generate accurate or good summaries of bug reports.
while the format of bug reports vary depending upon the system being used to store the reports much of the information in a bug report resembles a conversation.
beyond the fixed fields with pre defined values such as the status fieldthat records whether the bug is open or closed or some otherstate a bug report usually involves free form text including a title or summary a description and a series of time stamped comments that capture a conversation between developers and sometimes users related to the bug.
in one common system the bugzilla bug reporting system the description and comments may be written but not edited fur ther adding to the conversation nature of a report.
this lack of editable text also means the descriptions do not serve as summaries of the current report contents.
figure displays part of a bug report from the kde bugzilla bug repository with comments from people the description and each comment can be considered a turn in a conversation.
as bug repositories contain substantial knowledge about a software development there has been substantial recent interest in improving the use and management of this information particularly as many repositories experience a highrate of change in the information stored .
for instance anvik and colleagues have shown how to provide recom menders for whom to assign a report whereas runeson andcolleagues and wang and colleagues have demonstrated how to detect duplicate reports.
while some of this work has applied classifiers to the problems they intend to solve none has attempted to extract a meaningful summaryfor developers.
other efforts have considered how to improve the content of bug reports.
ko and colleagues analyzed the titles of bug reports to inform the development of tools for both reporting and analyzing bugs .
bettenburg and colleagues surveyeda large number of open source developers to determine whatfactors constitute a good bug report and developed a toolto assess bug report quality .
developers in their studynoted that bug reports are often used to debate the relative importance of various issues it is this conversational nature of reports that makes summaries valuable and that we hope to exploit to produce the summaries.
.
bug report corpus to be able to train and judge the effectiveness of an extractive summarizer on bug reports we need a corpus of bug reports with good summaries.
optimally we would have available such a corpus in which the summaries were createdby those involved with the bug report as the knowledge ofthese individuals in the system and the bug should be thebest available.
unfortunately such a corpus is not available verified verified 2bug the applet panel should not overlap applets description from mangus version svn using devel os linux installed from compiled sources in amarok2 svn i like the the new contextview but i found the new bottom bar for managing applets annoying as it covers parts of other applets sometimes like lyrics one so that you miss a part of it.
could be handy to have it appear and desappear onmouseover.
thanks comment from dan the real solution is to make it not cover applets not make it appear disappear on mouse over.
comment from leo i dont understand your point dan... how do we make it not cover applets?
comment from dan thats your problem to solve the toolbar should be like the panel in kde it gets it s own area to draw in a strut in window manager terms .
the applets should not consider the space the toolbar takes up to be theirs to play in but rather end at the top of it.
product amarok component contextview status resolved resolution fixed target version unspecified priority nor severity wishlist votes figure an example of the conversational structure of a bug report the beginning part of bug from the kde bug repository .
as developers do not spend time writing summaries once a bug is complete despite the fact that the bug report maybe read and referred to in the future.
to provide a suitable corpus we recruited ten graduate students from the department of computer science at theuniversity of british columbia to annotate a collection of bug reports.
on average the annotators had seven years of programming experience.
half of the annotators had experience programming in industry and four had some experience working with bug reports.
.
annotation process we had each individual annotate a subset of bugs from four different open source software projects eclipse plat form g n o m e mozilla and kde.
we chose a diverse set of systems because our goal is to develop a summariza tion framework that can produce accurate results for a widerange of bug reposito ries not just bug reports specific to a single project.
although the annotators did not have experi ence with these specific systems we believe their experiencein programming allowed them to extract the gist of the dis cussions no annotator reported being unable to understandthe content of the bug reports.
the annotators were com pensated for their work.
the bugs reports nine from each project chosen for annotation have mostly conversational content.
we avoided selecting bug reports consisting mostly of long stack traces verified verified 25number of bug reports number of comments figure the distribution of bug reports in the corpus with regard to the number of comments.
and large chunks of code as this content may be used but is not typically read by developers.
the reports chosen varied in length reports had between five and fourteencomments the remaining eleven bugs had to comments each.
figure shows the distribution of bug reports based on the number of comments.
nine of the 36bug reports were enhancements to the target system the other were defects.
there are a total of sentences in these bug reports.
this corpus size is similarto that used in training an email classifier the email corpuscontains email threads and sentences .
each annotator was assigned a set of bug reports from those chosen from the four systems.
for each bug report we asked the annotator to write an abstractive summary ofthe report using their own sentences that was a maximum of words.
we limited the length of the abstractive summary to motivate the annotator to abstract the given report.the annotator was then asked to specify how each sentence in the abstractive summary maps links to one or more sentences from the original bug report by listing the numbers of mapped sentences from the original report.
to aid the annotators with this process the annotators used a version of bc3 web based annotation software 6that made it easier for them to manipulate the sentences of the bug report.
figure shows an example of part of an annotated bug report the summary at the top is an abstractivesummary written by an annotator with the mapping to thesentences from the original bug report marked.
the bug report corpus is publicly available.
.
annotated bugs on average the bug reports being summarized comprised sentences.
on average the abstractive summaries created by the annotators comprised just over five sentences with each sentence in the abstractive summaries linked on aver6 verified 7see html.
the corpus contains additional annotations including an extractive summary for each bug report and labeling ofthe sentences.
3summary kde the applet panel should not overlap applets summary description collapse all from mangus .
version svn using devel .
os linux .
installed from compiled sources .
in amarok2 svn i like the the new contextview but i found the new bottom bar for managing applets annoying as it covers parts of ot her applets sometimes like lyrics one so that you miss a part of it.
.
could be handy to have it app ear and desappear onmouseover.
.
thanks comment from dan .
the real solution is to make it not cover appl ets not make it apper disapeer on mouse over.
comment in the anarok2 svn contextview the bottom bar sometimes obscure applet content.
applets should not be larger than the viewable area and should be given an appropriate sizehint.
this bug was fixed in .
.
figure a screenshot of the annotation software.
the bug report has been broken down into labeled sentences.
the annotator enters the abstractive summary in the text box.
the numbers in the brackets aresentence labels and serve as links between the abstractive summary and the bug report.
for example thefirst sentence of the abstractive summary has links to sentences .
.
.
.
form the bug report.
table abstractive summaries generated by annotators.
mean stdv sentences in the summary .
.
words in the summary .
.
linked sentences from the bug report .
.
age to three sentences in the original bug report.
table 1provides some overall statistics on the summaries producedby the annotators.
a common problem of annotation is that annotators often do not agree on the same summary.
this reflects the fact that the summarization is a subjective process and there is no single best summary for a document a bug report in thispaper.
to mitigate this problem we assigned three annota tors to each bug report.
we use the kappa test to measure the level of agreement between the annotators .
the result of the kappa test k value is .
for our bug report anno tations showing a moderate level of agreement.
we askedeach annotator at the end of annotating each bug report to complete a questionnaire about properties of the report.
the annotators in the answers to the questionnaires rated with low and high the level of difficulty of summarizing the bug report tobe .
.
the amount of irrelevant and off topic discussion in thebug report to be .
.
and the level of project specific terminology used in the bug report to be .
.
.
.
summarizing bug reports the bug report corpus provides us a basis on which to experiment with producing bug report summaries automat ically.
we set out to investigate two questions .
can we produce good summaries with existing conversation based classifiers?
.
how much better can we do with a classifier specifically trained on bug reports?
the existing conversation based classifiers we chose to investigate are trained on conversational data other than bug reports.
the first classifier which we refer to as ec w a s trained on email threads .
we chose this classifier asbug report conversations share similarity with email threads such as being multi party and having thread items added at differing intervals of time.
this classifier was trained on a subset of the publicly available enron email corpus which consists of annotated email threads sentences in total .
the second classifier which we refer to as emc w a s trained on a combination of email threads and meetings .
we chose this classifier because some of the characteristicsof bug reports might be more similar to meetings such ashaving concluding comments at the end of the conversation.
the meetings part of the training set for emc is a sub4set of the publicly available ami meeting corpus which includes meetings.
the ecand emc classifiers are appealing to use because of their generality.
if these classifiers work well for bug reports it offers hope that general classifiers might beapplicable to software project artifacts without training oneach specific kind of software artifacts which can vary between projects or on project specific artifacts lowering the cost of producing summaries.
however unless these classifiers produce perfect summaries the question of how good of a summary can be producedfor bug reports remains open unless we consider a classifiertrained on bug reports.
thus we also chose to train a third classifier brc using the bug report corpus we created.
to form the training set for brc we combined the three human annotations for each bug report by scoring each sentence of a report based on the number of times it has beenlinked by annotators.
for each sentence the score is be tween zero when it has not been linked by any annotator and three when all three annotators have a link to the sentence in their abstractive summary.
a sentence is consideredto be part of the extractive summary if it has a score of twoor more.
for each bug report the set of sentences with ascore of two or more a positive sentence is called the gold standard summary .
for the bug report corpus gold standard summaries include sentences which is .
of allthe sentences in the corpus and .
of all words in the corpus.
as we have only the bug report corpus available for both training and testing the bug report classifier we use a crossvalidation technique when evaluating this classifier.
specifically we use a leave one out procedure so that the classi fier used to create a summary for a particular bug report istrained on the remainder of the bug report corpus.
all three classifiers investigated are logistic regression classifiers.
instead of generating an output of zero or one these classifiers generate the probability of each sentence being part of an extractive summary.
to form the summary we sort the sentences into a list based on their probability val ues in descending order.
starting from the beginning of thislist we select sentences until we reach of the bug re port word count.
the selected sentences form the generated extractive summary.
we chose to target summaries of of the bug report word count b ecause this value is close to the word count percentage of gold standard summaries .
.
all three classifiers were implemented using thelibliner toolkit.
.
conversation features the classifier framework used to implement em emc and brc can learn based on different features.
we hypothesize that the features useful for the emand emc classifiers are also relevant to the summarization of bug reports since these reports exhibit a conversational structure.
the features are based on representing a bug report as a conversation comprised of turns between multiple participants.
the features can be categorized into four major groups.
structural features are related to the conversational structure of the bug reports.
examples include theposition of the sentence in the comment and the posi8 verified 09tion of the sentence in the bug report.
participant features are directly related to the conversation participants.
for example if the sentence is made by the same person who filed the bug report.
length features include the length of the sentence normalized by the length of the longest sentence in the comment and also normalized by the length of thelongest sentence in the bug report.
lexical features are related to the occurrence of unique words in the sentence.
the detailed description of features can be found in .
.
ev aluation to compare the ec emc and brc classifiers we have used several measures that c ompare summaries generated by the classifiers to the gold standard summaries formed from the human annotation of the bug report corpus section .
these measures assess the quality of each classifier and enable the comparison of effectiveness of the differentclassifiers against each other.
however these measures do not tell us whether the intended end users of the summaries software developers consider the generated summaries as representative of the original bug report.
to check the performance of the classifiers from a human perspective we also report on an evalu ation in which we asked human judges to evaluate the goodness of a set of generated summaries against the original bug reports.
.
comparing base effectiveness the first comparison we consider is whether the ec emc and brc classifiers are producing summaries that are better than a random classifier in which a coin toss is used to decide which sentences to include in a summary.
we performthis comparison by plotting the receiver operator characteristic roc curve and then computing the area under the curve auroc .
for this comparison instead of using the word count to generate extractive summaries we investigate different probability thresholds.
as described in section the outputof the classifier for each sentence is a value between zero and one showing the probability of the sentence being part of the extractive summary.
to plot a point of roc curve we first choose a probability threshold.
then we form the extractivesummaries by selecting all the sentences with probability values greater than the probability threshold.
for summaries generated in this manner we compute the false positive rate fpr and true positive rate tpr w h i c ha r et h e np l o t t e da sap o i n ti nag r a p h .f o re a c hs u m mary tpr measures how many of the sentences present in gold standard summary gss are actually chosen by the classifier.
tpr sentences selected from the gss sentences in gss fpr computes the opposite.
fpr sentences selected that are not in the gss sentences in the bug report that are not in the gss .
.
.
.
.
.
.
.
1tprate fpratebrc random classifier figure roc plot for brc classifier.
the area under a roc curve auroc is used as a measure of the quality of a classifier.
a random classifier has an auroc value of .
while a perfect classifier has anauroc value of .
therefore to be considered effective a classifier s auroc value should be somewhere in between preferably close to .
figure shows the roc curve for the brc classifier.
the diagonal line is representative of a random classifier.
the area under the curve auroc for brc is equal to .
indicating that this classifier performs better than arandom classifier.
we also computed the auroc values for ecand emc .
the values are .
and .
respectively suggesting that each of the classifiers has the same level of efficiency compared to a random classifier.
.
comparing classifiers auroc is a measure of the general effectiveness of the classifiers.
when the summaries are generated with a predefined length in this paper we need other measures to compare the classifiers.
to investigate whether any of ec emc orbrc work better than the other two based on our desired word count summaries we compared them using the standard evaluation measures of precision recall and f score.
wealso used pyramid precision which is a normalized evalua tion measure taking into account the multiple annotations available for each bug report.
.
.
f score f score combines the values of two other evaluation measures precision and recall.
precision measures how often a classifier chooses a sentence from the gold standard sum maries gss and is computed as follows.
precision sentences selected from the gss selected sentences recall measures how many of the sentences present in a gold standard summary are actually chosen by the classifier.
for a bug report summary the recall is the same as the tpr used in plotting roc curves section .
.
as there is always a trade off between precision and recall the f score is used as an overall measure.
.
.
.
.
35pyramid precision bug brc ec emc figure pyramid precision for all classifiers.
fs c o r e precision recall precision recall figure shows the values of f score for the three classifiers across all the bug reports.
in this plot the bug reports have been sorted based on the values of the f score for thesummaries generated by brc.
this figure shows that the best f score typically occurs with the brc classifier.
.
.
pyramid precision the pyramid evaluation scheme by nenkova and passonneau was developed to provide a reliable assessment of content selection quality in summarization where there are multiple annotations available.
we used the pyramid precision scheme of carenini et.
al inspired by nenkova spyramid scheme.
for each generated summary of a given length we count the total number of times the sentences in the summary were linked by annotators.
pyramid precision is computed by dividing this number by the maximum possible total forthat summary length.
for example if an annotated bugreport has sentences with links and sentences with links the best possible summary of length has a total number of links equal to .
the pyramid precision of a generated summary of length with a total of8 links is therefore computed as pyramid precision .
figure shows the values of pyramid precision for the three classifiers across all the bug reports in the corpus.
thebug reports have been sorted based on the values of thepyramid precision for the summaries generated by brc.
the figure shows that brc has better precision values for most of the bug reports.
.
.
summary figure and figure show that brc out performs the other two classifiers for most of the bug reports.
table 2shows the values of precision recall f score and pyramidprecision for each classifier averaged over all the bug reports.
to investigate whether the bug report classifier brc i s .
.
.
.
.
.
.
35f score bug brc ec emc figure f score plot for all classifiers.
table evaluation measures.
classifier pyramid precision recall f score precision brc .
.
.
.
ec .
.
.
.
emc .
.
.
.
significantly better than the other two classifiers ecand emc we performed four paired t tests one to see if the pyramid precision of brc is significantly better that that ofec one to see if the pyramid precision of brc is significantly better that that of emc and so on.
the results confirm that the bug report classifier brc o u t p e r f o r m s the other two classifiers with statistical significance where significance occurs with p .
.
the classifier trained on emails ec and the classifier trained on emails and meetings emc have similar performance.
the paired t test confirms that there is no significant difference.
.
feature selection analysis all of the three classifiers used in our study ec emc and brc use a set of features to generate summaries of bug reports.
the values of these features for each sentence are used to compute the probability of the sentence being part of the summary.
to see which features are informative for generating summaries we perform a feature selectionanalysis.
for this analysis we compute the f score 9value for each of the features using the approach of .
this score is commonly used to compute the discriminability of features in supervised machine learning.
the score depends only onthe set of features and the training data and is independentof the classifier.
features with higher f score are the mostinformative in discriminating between important sentences which should be included in the summary and other sentences which need not be included in the summary.
9this score is computed for the feautures and is different from the f score computed for the summaries in section .
.
.table features key.
feature id description mxs max sprob score mns mean sprob score sms sum of sprob scores mxt max tprob score mnt mean tprob score smt sum of tprob scores tloc position in turn cloc position in conv.
slen word count globally normalized slen2 word count locally normalized tpos1 time from beg.
of conv.
to turn tpos2 time from turn to end of conv.
dom participant dominance in words cos1 cos. of conv.
splits w sprob cos2 cos. of conv.
splits w tprob pent entro.
of conv.
up to sentence sent entro.
of conv.
after the sentence thisent entropy of current sentence ppau time btwn.
current and prior turn spau time btwn.
current and next turn begauth is first participant cws rough cluewordscore cent1 cos. of sentence conv.
w sprob cent2 cos. of sentence conv.
w tprob table provides a short description of the features considered.
some descriptions in the table refer to sprob.i n f o r mally sprob provides the probability of a word being uttered by a particular participant based on the intuition that certain words will tend to be associated with one conversationparticipant due to interests and expertise.
other descrip tions refer to tprob which is the probability of a turn given a word reflecting the intuition that certain words will tendto cluster in a small number of turns because of shifting topics in a conversation.
full details on the features are provided in .
figure shows the values of f score computed for the features used by the classifiers to summarize bug reports.
the results show that the length features slen slen1 are among the most helpful.
several lexical features are also helpful cws cent1 cent2 sms smt.
10these results suggest that we may b ea b l et ot r a i nm o r ee ffi c i e n t classifiers by combining lexical and length features of the conversation.
.
human evaluation the generated summaries are intended for use by software developers.
does a classifier with pyramid precision of .
produce summaries that are useful for developers?
to investigate whether the summaries are of sufficient quality forhuman use we set up an evaluation of the generated sum maries of the brc classifier with a group of eight human judges.
we chose to focus on the brc classifier since it had performed the best based on the earlier measures.
eight of our ten annotators agreed to evaluate a number of machine generated summaries.
we asked the eightjudges to evaluate a set of eight summaries generated by the brc classifier.
each summary was evaluated by three different judges.
the human judges were instructed to read the original bug report and the summary before starting the evaluation process.
figure provides an example of a gener ated extractive summary the judges were asked to evaluate 10cws measures the cohesion of the conversation by comparing the sentence to other turns of the conversation.
.
.
.
.
.
.
.
.
.
slencwscent1cent2smsslen2smtcos1cos2tlocthisentdommxsmntmxtclocmnsppauspausentpentbegauttpose1tpose2f score featuresbug report data figure feature f scores for the bug report corpus.
a portion of the original bug appears earlier in figure .
we asked each judge to rank using a five point scale withfive the high value each bug report summary based on fourstatements mean and standard deviations are provided inparentheses following the statement .
the important points of the bug report are represented in the summary.
.
.
.
the summary avoids redundancy.
.
.
.
the summary does not contain unnecessary information.
.
.
.
the summary is coherent.
.
.
an evaluation of meeting data summarized by multiple approaches uses similar statements to evaluate the goodness of the generated summaries .
we ensured in this judging process that the bug reports were assigned to judges who had not annotated the same reports during the annotation process.
we also took care to choose summaries with different values of pyramid precisionand f score so as to not choose only the best examples of generated summaries for judging.
.
threats the primary threats to the evaluations we have conducted are the size of the bug report corpus and the annotation by non experts in the projects.
while the size of the corpus is sufficient for initial experimentation with these approaches we are limited in the size of the training set that can be used to train brc and we are unable to use a separate set of bug reports for training and testing.
optimally we would have had summaries created for the reports by experts in the projects.
summaries created byexperts might capture the meaning of the bug reports better than was possible by non experts.
on the other hand summaries created by experts might rely on knowledge that was not in the bug reports potentially creating a standard that would be difficult for a classifier to match.
this riskis mitigated by the experimental setup that requires a map ping from abstractive to extractive sentences.
by assigning three annotators to each bug report and by using agreementbetween two to form the gold standard summaries we haveattempted to mitigate the risk of non expert annotators.
for the human evaluation th e primary threat is the use of non experts who may not be the best judges of summaries of the reports.
another threat is not asking the human judges about the properties of interest through multiple questions which may have allowed a better determination of the judges opinions on those properties.
finally the judges may have wanted to please the experimenters.
in future studies we will consider interspersing classifier generated and human generated summaries to reduce this risk.
.
discussion with this work we have shown that it is possible to generate summaries for a diverse s et of bug reports with reasonable accuracy as judged both against gold summaries and by human judges.
the ability to produce summaries opens upthe possibility of using a summary generator to support the work of software developers.
we discuss these possibilities as well as consider ways to fu rther improve the summaries produced.
.
using a bug report summary automatically generated bug report summaries could aid a software developer in several ways.
during bug reporttriage activities a developer triager must often consultother reports.
for instance the triager must typically determine if a new incoming report is a duplicate of existing reports.
as this determination can be difficult for large fast changing repositories recommenders have been builtto suggest to the triager a set of existing bug reports to consider as duplicates e.g.
.
assessing these recommendations to determine which are true duplicates can be time consuming as the determination typically involves reading much if not all of the report.
an automatically produced summary of each existing report could ease this task by greatly reducing the amount of text the triager mustread understand and consider.
as another example previous work in our research group developed a recommender to suggest to a developer perform ing an evolution task on a system change tasks describedby bug reports that have been previously completed which might guide the developer .
as with the duplicate bug case the developer must again wade through a substantial amount of text to determine which bug report might be relevant.
summaries could also ease this process.
overall summaries can make it possible for a developer to use the body of knowledge about the project that is cap tured in the repository.
the ability to produce summaries automatically opens up new possibilities in making such recommenders effective and practic al to use.
in future work we plan to investigate whether the provision of such summaries can provide these benefits.
.
summarizing other project artifacts bug reports are not the only artifacts with a substantial amount of text in a software development project.
requirements and design documents can contain text describing thedomain discussions of alternative approaches and rationale for why decisions have been made.
even source code which is typically not considered to contain much text can includemany sentences about decisions or how a piece of code works.
given our success in producing summaries of bug reports 8summary the applet panel should not overlap applets in amarok2 svn i like the the new contextview but i found the new bottom bar for managing applets annoying as it covers parts of other applets sometimes likelyrics one so that you miss a part of it.
the real solution is to make it not cover applets not make it appear disappear on mouse over.
i dont understand your point dan... how do we make it not cover applets?applets should not be larger than the viewable area if there s an applet above it then the lower applet should get a smaller sizehint and resize if necessary when it s the active applet and therefore the only one on the screen the bug that is being shown here is the fact that you cannot yet resize your applets and as such we also don t set default sizes sanely.
of course just thought i should point out that the feature is not yet completed the polish that s gone into it lately could seem like an indication of feature completion and as such it would seem the prudent course to inform you that that isnot the case applets should not be larger than the viewable area if there s an applet above it then the lower applet should get a smaller sizehint and resize if necessary when it s the active applet and therefore the only one on the screen figure an extractive summary generated by brc for bug from the kde bug repository.
we plan to investigate the generation of summaries for other text that appears in software development projects.
in par ticular we are interested in investigating whether textual summarization techniques can be used to help explain features implemented in source code.
.
improving a bug report summarizer the bug report classifier we trained produces reasonable summaries.
although we have trained and evaluated the classifier across reports from four systems we do not know whether the classifier will produce good summaries for re ports from systems on which it was not trained.
we planto investigate the generality of the summarizer to other bugreporting systems.
another possibility for improving the accuracy of a bug report summarizer is to augment the set of features used.
for instance comments made by people who are more active in the project might be more important and thus should bemore likely included in the summary.
as part of the annotation process we also gathered information about the intent of sentences such as whethera sentence indicated a problem suggestion fix agree ment or disagreement .
these labels might also be used to differentiate the importance of a sentence for a summary.
we leave the investigation of these intentions to future research as we do not yet have the ability to automatically label the sentences for the test data.
it is also still an open question whether extractive summarization is the most appropriate choice for bug reportsor whether better results could be obtained through an ab stractive summarizer.
we leav e this determination to future work.
.
summary researchers rely on good summaries to be available for papers in the literature.
these summaries are used for several purposes including providing a quick review of a topic within the literature and selecting the most relevant papers for a topic to peruse in greater depth.
software developers must preform similar activities such as understandingwhat bugs have been filed against a particular componentof a system and determining why particular design decisions have been made as recorded in design documents and elsewhere.
however developers must perform these activities without the benefit of summaries leading them to either expend substantial effort to perform the activity thoroughlyor resulting in missed information.
in this paper we have investigated the automatic generation of one kind of software artifact bug reports to provide developers with the benefits others experience daily in other domains.
we found that existing conversation based extrac tive summary generators can p roduce summaries for reports that are better than a random classifier.
we also found thatan extractive summary generato r trained on bug reports produces the best results.
the human judges we asked to evaluate reports produced by the bug report summary generator agree that the generated extractive summaries contain important points from the original report and are coherent.
this work opens up new possibilities to improve the effec tiveness of existing systems for recommending duplicate bugreports and for recommending similar changes completed in the past for a current software evolution task.
it also opens up the possibility of summarizing other software project ar tifacts to enable developers to make better use of the richknowledge base tracked for most software developments.
9acknowledgments thanks to giuseppe carenini and raymond ng for useful discussions on summarization and to jan ulrich for helpingwith the annotation software.
this research is supported bynserc.
thanks also to the annotators for helping to createthe bug report corpus.
.