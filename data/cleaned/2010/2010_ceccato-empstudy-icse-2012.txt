an empirical study about the effectiveness of debugging when random test cases are used mariano ceccato alessandro marchetto fondazione bruno kessler trento italy fceccato marchetto g fbk.euleonardo mariani university of milano bicocca milano italy mariani disco.unimib.itcu d. nguyen paolo tonella fondazione bruno kessler trento italy fcunduy tonella g fbk.eu abstract automatically generated test cases are usually evaluated in terms of their fault revealing or coverage capability.
beside these two aspects test cases are also the major source of information for fault localization and fixing.
the impact of automatically generated test cases on the debugging activity compared to the use of manually written test cases has never been studied before.
in this paper we report the results obtained from two controlled experiments with human subjects performing debugging tasks using automatically generated or manually written test cases.
we investigate whether the features of the former type of test cases which make them less readable and understandable e.g.
unclear test scenarios meaningless identifiers have an impact on accuracy and efficiency of debugging.
the empirical study is aimed at investigating whether despite the lack of readability in automatically generated test cases subjects can still take advantage of them during debugging.
keywords empirical software engineering debugging automatic test case generation i. i ntroduction automatic test case generation is an important area of software testing.
the scientific community has reserved a great attention to test generation techniques which are now available for many popular frameworks and programming languages.
for instance randoop can automatically generate test cases for java classes by randomly combining method invocations pex can generate test cases that cover all the statements in a .net class by combining concrete and symbolic execution test uses mutation analysis to generate test suites for java programs .
empirical evidence has indicated that those automated solutions are effective in discovering programming faults .
however the data and examples in these studies also show that automatically generated autogen for brevity test cases are generally less readable and intuitive than manually designed test cases.
in fact manually defined test cases usually address a well defined scenario identified by a tester while autogen test cases result from a random or coverage oriented process and do not address a clear scenario.
as a consequence testers might find it difficult to interpret a failure exposed by an autogen test case and thedebugging of such a failure might be expensive compared to debugging from a manually designed test case.
even if a certain lack of readability is intrinsic of autogen test cases there is no empirical evidence that autogen test cases impact debugging negatively.
understanding the costs of test automation including indirect costs such as the cost of debugging is of critical importance for testers who have the responsibility of designing effective testing strategies for their software.
in this paper we present an empirical study that aims at qualitatively and quantitatively evaluating the impact of autogen test cases on debugging.
our study considers debugging of 8faults in the jtopas1andxml security2 applications.
we evaluate the effectiveness of debugging when using a manually designed test suite and a test suite generated by randoop.
the study is based on the activity of14bsc bachelor of science students and 15msc master of science students involved in debugging tasks during two replications of a controlled experiment.
in this study we used randoop because among many different automatic test case generation techniques we find that random test cases produced by randoop are particularly difficult to understand.
thus if autogen test cases introduce inefficiencies in debugging random test cases are likely to expose the issue.
autogen test cases are often short and composed of simple but sometimes unrelated sequences of method calls while manually designed tests address rather complicated but meaningful scenarios.
our study investigates whether debugging tasks driven by autogen test cases are as reasonably accurate and efficient as those driven by manually designed test cases.
the paper is organized as follow.
section ii presents the design of the experiment.
section iii reports the experimental results.
section iv discusses the results.
section v compares our achievements with related work.
section vi provides final remarks and discusses future work.
e xperiment definition and planning this section reports the definition design and settings of the experiments in a structured way following the template and guidelines by wohlin et al.
.
the goal of the study is to investigate the differences between manually written and autogen test cases with the purpose of evaluating how well they support debugging tasks.
the quality focus regards how manually written and autogen test cases affect the capability of developers to correctly and efficiently debug the code.
the results of this experiment are interpreted regarding two perspectives a researcher interested in empirically validating autogen test cases and a quality manager who wants to understand whether the time spent in writing test cases pays off when facing actual debugging tasks.
thecontext of the study is defined as follows the subjects of the study are developers facing debugging tasks while the objects are applications that contain the faults to be fixed.
we collected empirical data from two replications the first one involved msc students of the university of trento attending the course of software analysis and testing .
the second replication involved bsc students of the university of milano bicocca attending the course of software analysis and testing and msc students of the university of milano bicocca attending the course of software quality control .
subjects from both studies have skills in java programming debugging and use of the eclipse ide3.
the first replication with less subjects provided feedback used to fine tune the experimental procedure before running the second larger replication.
for instance we improved the training material and the instructions provided to the subjects before carrying out the second replication.
the applications used in the experiment are two realworld java applications jtopas and xml security .jtopas is a simple tokenizer that can tokenize text files as inputs and allows users to customize the grammar of the input files by specifying the structure of keywords compounds and comments and the case sensitivity.
jtopas consists of classes and meloc method lines of code .
xml security is a library that provides functionalities to sign and verify signatures in xml documents.
it supports many mature digital signature and encryption algorithms on standard xml formats such as xhtml and soap.
it consists of classes for a total of meloc.
both applications are available with a full suite of manually written tests.
a. hypotheses formulation and variable selection based on the study definition reported above we can formulate the following null hypotheses to be tested there is no difference in the accuracy of debugging when debugging is supported either by manually written or autogen test cases.
h02 there is no difference in the efficiency of debugging when debugging is supported either by manually written or autogen test cases.
these two hypotheses are two tailed because there is no a priori knowledge on the expected trend that should favor either manually written or autogen test cases.
on the one hand manual test cases are meaningful for a developer who is determining the position of a fault while automatic tests may contain meaningless statements and code identifiers that may confuse developers.
on the other hand manual tests could be difficult to understand because they may require understanding of complex parts of the application logic while automatic tests are simpler since they are generated without a clear knowledge of the application business logic.
the null hypotheses suggest that we have two dependent variables debugging accuracy anddebugging efficiency .
to measure debugging accuracy and efficiency we asked subjects to fix eight faults in the object application source code.
we defined faults that satisfy the following requirements faults are located in different parts of the applications each fault is revealed by a manual and an autogen test case faults do not interact different faults are revealed by disjoint manual or autogen test cases that is each test case reveals at most one fault and faults are based on the bugs available in the software artifact infrastructure repository sir .
since randoop was not able to reveal some of the sir faults as provided in the repository in order to meet the four requirements we slightly changed some faults to simplify their detection.
however such changes do not modify the nature of the faults they just make the used test suites able to detect them.
for instance according to the sir repository the fault of jtopas is injected into the application by removing the getmessage .. method from the extindexoutofboundsexception class.
we changed the location of fault to the tokenexception class so that it can be revealed by both autogen and manual tests since random tests do not expose this fault if left inside the former class.
when a fault was revealed by multiple test cases we randomly selected one test case to use in the study.
in reality developers often have a larger suite of test cases that reveal the fault or can be used for regression testing.
however providing a full test suite to subjects could represent a too wide source of variability for a controlled in lab experiment as different starting points could be used by different developers based on their own experience and style.
instead we wanted all the subjects to start from the same test and work under exactly the same initial conditions.
our experiments the accuracy of debugging is measured as the number of correctly fixed faults.
we objectively evaluated the correctness of the fixes by running a predefined set of test cases not provided to subjects and checking if they all pass.
the efficiency of debugging is evaluated as the number of correct tasks i.e.
the number of correctly fixed faults divided by the total amount of time spent for these tasks measured in minutes eff corr i time i where corr iis equal to if the i th task is performed correctly otherwise while time iis the time spent to perform the i th task.
in other words efficiency is measured as the number of correctly performed tasks per minute.
the independent variable the main factor of the experiment is the treatment during the execution of debugging tasks.
the two alternative treatments are manually written test cases those distributed as unit tests for the object applications i.e.
we obtained them from the sir repository and test cases automatically generated by randoop .
the understandability of the test cases that reveal faults might affect the debugging performance and may vary substantially between manual and autogen test cases.
since we cannot control this factor in the experiments because it depends on how manual test cases have been defined and how the test case generation algorithm works we measure whether any difference occurs in our experimental setting.
the test case understandability might in fact represent one of the key features in which manual and autogen test cases differ which could possibly explain some of the observed performance differences.
unfortunately there is no easy widely accepted way of measuring the understandability of test cases.
we approximate such measurement by considering two specific factors of understandability namely identifier meaningfulness and complexity of the test code.
for the former factor we manually classify each identifier in a test case as either artificial automatically generated or userdef user defined and count the respective numbers.
in order to measure the test case complexity we consider both static and dynamic metrics which provide an approximation of how complex a test case is from the developer s perspective5.
metrics are computed using the eclipse plugin metrics as static metrics we measure meloc non blank and non comment lines of code inside each method body mccabe cyclomatic complexity of each test method.
as dynamic metrics we consider the amount of code executed by each test case.
we count it at two granularity levels 5although some of the used metrics are actually size metrics we regard them as test case complexity indicators since they reflect the perceived complexity associated with using the test case during debugging.
exec.
methods the number of methods executed by a test case exec.
locs the number of statements executed by a test case.
to determine whether manual and autogen test cases differ according to the identifier meaningfulness and code complexity metrics we introduce two additional derived null hypotheses dh 03there is no difference in the number of artificial user defined identifiers of the manually written and autogen test cases used in the experiment.
dh 04there is no difference in the static dynamic complexity of the manually written and autogen test cases used in the experiment.
experimental support for the alternative hypotheses associated with dh 03anddh 04would provide useful information for the interpretation of the results about the two dependent variables considered in h01andh02 .
b. co factors identification we measured the following co factors that could influence the dependent variables the subjects ability the ability of subjects in performing debugging tasks was measured using a pre test questionnaire and a training session.
the pre test questionnaire included questions about their programming and debugging ability their experience with the development of large applications and their scores in academic courses related to development and testing.
in the training session subjects were asked to complete tasks that consist of answering some codeunderstanding questions and fixing faults in each of the two object applications.
according to the answers given to the pre questionnaire and the results of the training lab we classified the subjects into three categories high ability subjects in the first experiment and in the second one are those who had experience with the development of large applications have an academic score of at least 306and completed correctly at least of the tasks in the training lab medium ability subjects respectively and are those who correctly completed at least of the tasks in the training lab and either had experience with the development of large applications or have an academic score higher than the rest of the subjects are classified as low ability subjects respectively and .
the subjects experience we classified bsc students as low experience subjects and msc students as high experience subjects.
the object system aka application since we adopted a balanced design with two systems subjects could show different performance on different systems.
hence the system is also a co factor.
6in the italian academic grade system a score of corresponds to a b in the ects grade system to an a in the us system.
the experiment session aka lab subjects could spend some effort to familiarize with the experimental environment during the first session.
we measured whether any learning effect occurred between the two labs.
the fault to be fixed as faults are all different the specific features of the fault to be fixed may interact with the main factor.
for each co factor we test if there is any effect on the debugging accuracy and debugging efficiency and we check their interaction with the main factor.
we formulate the following null hypotheses on the co factors h0cithe co factor i i does not significantly interact with the kind of test cases to influence accuracy and efficiency in performing debugging tasks.
these null hypotheses are also two tailed because we do not have any a priori knowledge about the direction in which a co factor could influence accuracy and efficiency.
c. experimental design we adopted a counter balanced design each replication of the experiment consisted of two experimental sessions lab 1andlab with hours allocated for each lab.
subjects have been split into four groups balancing the level of ability and experience in each group.
this design ensures that each subject works on the two applications jtopas and xmlsecurity and with the two different treatments manually written and randomly generated test cases as shown in table i. moreover this design allowed us to study the effect of each co factor using statistical tests i.e.
analysis of variance .
table i experimental design .
r r andoop test cases m manually written test cases .
group1 group2 group3 group4 lab jtopas m xmlsecurity m jtopas r xmlsecurity r lab xmlsecurity r jtopas r xmlsecurity m jtopas m d. experimental procedure and material before the experiment we asked the subjects to fill a pre questionnaire in which we collected information about their ability and experience in programming and testing.
subjects have also been trained with lectures on testing and debugging.
moreover subjects participated in a training laboratory where they were asked to cope with debugging tasks very similar to the experiment on the object applications.
this made us confident that subjects were quite familiar with both the development debugging environment and the source code of the applications to debug.
we wanted to make sure that subjects spend enough time to familiarize with the applications during training so that the time measured during the actual experiment was required mostly to understand the test case and to identify and fixfaults on already familiar applications.
the accuracy in the training tasks has been recorded and used to assess the subjects level of ability.
to perform the experiment subjects used a personal computer with the eclipse development environment already used in the training equipped with a standard java debugger.
we distributed the following material the application code depending on the group either jtopas or xml security.
the code contains four faults four test cases either manually written or autogen depending on the group the subjects belong to as shown in table i. each test case reveals exactly one fault test cases are supposed to be addressed in order and they are sorted according the their difficulty from easier to harder to fix slides describing the experimental procedure.
before the experiment we gave subjects a clear description of the experimental procedure but no reference was made to the study hypotheses.
the experiment has been carried out according to the following procedure import the application code in eclipse for each of the four test cases i mark the start time ii run the test case and use it to debug the application and fix the fault iii mark the stop time create an archive containing the modified source code and send it to the experimenter by email fill a post experiment survey questionnaire.
during the experiment teaching assistants were present in the laboratory to prevent collaboration among subjects and to verify that the experimental procedure was respected in particular that faults were addressed in the right order and time was correctly marked.
after the experiment subjects have been asked to fill a post experiment survey questionnaire devoted to gaining insight about the subjects behavior during the experiment and finding justification for the quantitative observations.
the questionnaire consists of questions expressed in a likert scale related to q1 whether the time given to complete the tasks was enough q2 clarity of tasks q3 difficulties experienced in understanding the source code of the application and the source code of the test cases q5 difficulties in understanding the features under test q6 difficulties in identifying the portion of code to change q7 use and usefulness of the eclipse debugging environment q9 number of executions of the test case q10 percentage of total time spent looking at the code of the test cases and of the application q12 difficulties in using the test cases for debugging q13 whether the bugs have been fixed without fully understanding them relying just on test cases q14 need for inspecting the application code to understand bugs q15 perceived level of redundancy in test cases q16 usefulness of local variables in test casesto understand the test q17 whether they found the test cases misleading.
e. analysis method we used a non parametric statistical test to check the two hypotheses related to the accuracy and efficiency of subjects in performing debugging tasks h01andh02 .
the use of non parametric tests does not impose any constraint on the normal distribution of the population.
since the empirical data is intrinsically paired the same subjects attended both labs thus worked with both randomly generated and manually written test cases we used the wilcoxon two tailed paired test to check the hypotheses.
in order to use this test however we had to make the experimental data paired by removing data points of subjects who did not participate in both experimental sessions few students could not attend both labs .
such a test allows to check whether differences exhibited by the same subjects with different treatments manual and random tests over the two labs are significant.
we assume significance at a confidence level .
so we reject the null hypothesis when p value .
.
the same statistical test was used to address the derived null hypotheses dh 03anddh .
in order to understand whether the test case complexity is a property which characterizes the main treatment manual vs. autogen test cases we measured the performance of the test case complexity metrics as predictors of the treatment.
specifically we computed the confusion matrix where each test case complexity metrics is a possible actual factor and the binary classification between manual and autogen test cases is the predicted factor.
standard classification metrics number of true false positives negatives and derived metrics precision recall accuracy and f measure are then used to assess the degree to which manual and autogen test cases can be separated using the test case complexity as the distinguishing feature.
specifically correct classifications are indicated as tp true positives i.e.
correctly classified as autogen and tn true negatives i.e.
correctly classified as non autogen while errors are of two types fp false positives i.e.
manual test cases classified as autogen and fn false negatives i.e.
autogen test cases classified as nonautogen .
among the several indicators that can be used to assess the quality of a classifier we consider four of the most widely used indicators precision recall accuracy and f measure.
they are defined as follows precision tp tp fp recall tp tp fn accuracy tp tn tp fp tn fn f measure 2precision recall precision recall .
the analysis of co factors that is the test of hypotheses h0c1 h0c2 h0c3 h0c4 h0c5 is performed using a twoway analysis of variance anova and when detected interactions are visualized using interaction plots.
regarding the analysis of survey questionnaires we evaluate questions related to availability of enough time generaldifficulties found by subjects and the use of the debugging environment q1 q8 by verifying that the answers are either strongly agree or agree .
we test medians using a one tailed mann whitney test for the null hypothesis fqx where corresponds to undecided and fqxis the median for question qx.
among these questions for those specific to test cases q4 q8 answers of subjects using manually written tests were compared with answers of subjects using randomly generated tests.
in this case a two tailed mann whitey test is used for the null hypothesis eqrandom eqmanual .
the same comparison is also performed for questions q9 q17.
iii.
r esults a. debugging accuracy figure top shows the box plot of the accuracy of fault fixing.
the figure compares the number of correct answers given by subjects in the first experiment left hand side second experiment middle and overall results righthand side when faults are debugged using either manually written or randomly generated test cases.
figure bottom reports descriptive statistics including the number of subjects who participated in both labs paired analysis mean median and standard deviation together with the p value for the wilcoxon s test.
we can notice that subjects who used autogen tests showed better accuracy i.e.
correctly fixed more faults than subjects who used manually written tests in both experiments.
even if the trend is clearly evident data from the first experiment are not statistically significant.
this could be due to the small number of subjects involved in the first experiment.
data from the second experiment and overall data confirm this trend with significance at confidence level although showing a smaller gap between random and manually written tests.
thus we can reject h01and conclude that subjects perform significantly better when using autogen tests than manually written tests.
b. debugging efficiency the same procedure used with accuracy was also applied to efficiency.
figure top shows the box plot for efficiency with the two treatments.
it compares the efficiency of the subjects in the first left hand side and second experiment middle and overall result right hand side when faults are debugged using either manually written or randomly generated test cases.
figure bottom reports descriptive statistics of the paired data and the p value for the wilcoxon test.
the trend shown for accuracy is confirmed here the efficiency of subjects working with autogen tests is higher than when working with manually written tests.
while data from the first experiment are not significant though the trend is evident data from the second experiment and from the two experiments together confirm themanual random manual random manual random0 4accuracy of correct tasks exp i exp ii allexp n diff.mean diff.median diff.sd p.value i .
.
.
.
ii .
.
.
.
all .
.
.
.
figure .
analysis of accuracy trend observed in the first experiment with a significance at confidence level although showing a smaller gap between random and manually written tests.
thus we can reject h02and conclude that subjects perform significantly faster when using random tests than manually written tests.
manual random manual random manual random0.
.
.
.15efficiency correct time exp i exp ii all exp n diff.mean diff.median diff.sd p.value i .
.
.
.
ii .
.
.
.
all .
.
.
.
figure .
box plots of fixing efficiency.
c. test case understandability comparing the number of artificial and user defined identifiers in the fault revealing test cases of jtopas and xmlsecurity see table ii one can see that no artificially generated identifier is present in any of the manual test cases.
user defined identifiers are of course present also in autogentest cases.
for instance names of classes instantiated or methods called in the test cases.
artificial identifiers may be present in manual test cases as well for instance when code generation tools e.g.
tools for parser generation from grammars are used.
this never happens in our two case studies.
each random test case has on average artificial identifiers more than the corresponding manual test case and such a difference is statistically significant at level .
according to the wilcoxon paired test.
random tests have on average non artificial identifiers less than the corresponding manual tests.
this difference is statistically significant at level .
not at level .
.
in summary the number of meaningless identifiers artificial in random test cases is substantially greater than in manual test cases and the number of meaningful identifiers userdef substantially smaller.
hence we can reject the null hypothesis dh but the alternative hypothesis does not explain the observed difference in accuracy and efficiency which goes in the opposite direction with random tests associated to superior performance.
table ii occurrences of artificial user defined identifiers in the test cases autogen tests manual tests artificial ids userdef ids artificial ids userdef ids jtopas t1 t2 t3 t4 xml security t1 t2 t3 t4 table iii paired analysis of static top and dynamic bottom test case metrics wilcoxon s test .
metric n diff.mean diff.median diff.sd p.value meloc .
.
.
.
mccabe .
.
.
.
methods .
.
.
.
locs .
.
.
.
comparing the values of the complexity metrics for random and manual test cases per fault one can observe see table iii that while static metrics meloc and mccabe exhibit no substantial difference between manual and random test cases dynamic metrics executed methods and executed locs show a major difference when comparing manual and random test cases.
in both applications the number of methods and statements executed by manual test cases is substantially higher than by random tests.
the ratio is the order of two for jtopas while it is even bigger forxml security reaching an order of magnitude when locs are considered .
as we can see from the results in table iii for dynamic metrics the difference between manual and random test cases is statistically significant at confidence level so we can reject the null hypothesis dh with respect to dynamic metrics .
in summary manual test cases are dynamically more complex and this might explain the observed performance degradation exhibited by subjects working with manual test cases despite the presence of more meaningful identifiers in these test cases.
we computed the confusion matrix not shown for lack of space associated with a nearest neighbor classifier that predicts the test case type based on one of the two dynamic complexity metrics either the executed methods or locs .
the predictor classifies each test case by determining the test case having the closest dynamic complexity metrics value and assigning it to the class autogen or manual of such closest test case .
table iv prediction performance metrics for the test case based nearest neighbor classifier on executed methods or loc s. metric precision recall accuracy f.measure exec.
methods .
.
.
.
exec.
locs .
.
.
.
executed locs is a better predictor than executed methods.
the values reported in table iv bottom for this indicator are quite close to showing that in our experiment it is possible to predict the type of a test case from complexity metrics specifically executed locs with high accuracy.
this means that the two types of test cases used in the experiment autogen vs. manual can be characterized with high accuracy as low dynamic complexity vs. high dynamic complexity.
d. analysis of co factors for lack of space we do not report all the anova tables but just the p values.
however all detailed analysis results can be found in the technical report .
for each co factor table v shows the result of the twoway anova of efficiency by treatment and co factor for the first and second experiment and for the overall results.
we first analyze ability high medium orlow and experience bsc or msc student .
for the first experiment we cannot consider experience as a co factor because all subjects involved in the first experiment share the same level i.e msc .
we can notice that both ability and experience have a significant effect.
while ability does not experience does interact with the main factor treatment in a marginally statistically significant way when we consider overall data p value .
.
the interaction plot in figure top shows that experienced subjects are definitely more efficientwhen working with random tests rather than manual tests while less experienced subjects only marginally improve their efficiency when working with random tests.
we can hence see that while for non skilled nonexperienced subjects the difference between manual and autogen tests is small skilled experienced subjects performed better when they worked with random tests than with manual tests.
table v anova ofefficiency by treatment co factor ci co factor exp i exp ii all treatment .
.
.
ability .
.
.
treatment ability .
.
.
treatment .
.
experience .
.
treatment experience .
.
treatment .
.
.
system .
.
.
treatment system .
.
.
treatment .
.
.
lab .
.
.
treatment lab .
.
.
.
.
.
.
.
treatmentmean of efficiency manual random experience msc bsc .
.
.
.
.
treatmentmean of efficiency manual random lab lab2 lab1 figure .
interaction plots between treatment experience top and treatment lab bottom for efficiency both experiments we also analyze whether the particular system used in the experiment jtopas or xml security influenced the results.
the two way anova by treatment and system table v 3rd sector indicates no significant effect of the applications and no interaction with the main factor.
let us now analyze the lab as co factor table v bottom .
we can notice that lab is a significant co factor in the second experiment and overall thus there is a learning effect between the two experimental sessions.
figure bottom shows the interaction plot between the efficiency and the labs.
subjects who used random tests first do not show any learning effect while there is a significant learning effect for subjects who used manually written test cases first.
last we analyze the role of faults as a co factor to see if the faults influenced the result and or interacted with the main factor manual vs. autogen test to influence the result.
we cannot study the impact of the co factor on accuracy and efficiency as they are metrics over all faults and we are interested in each fault individually.
so we resort to corr i andtime ias the dependent variable available for each i th fault.
the analysis is performed separately for the two systems jtopas and xml security because faults are different.
results of the two way anova by treatment and fault oncorrectness are significant only for jtopas overall see table vi but they never interact with the main factor.
results of the two way anova by treatment and fault on time are significant for jtopas on the first and second experiment while for xml security they are significant on the second experiment and overall see table vi but again they never interact with the main factor.
table vi anova ofcorrectness time by treatment f ault correctness time co factor exp i exp ii all exp i exp ii all treatment .
.
.
.
.
.
fault jt .
.
.
.
.
.
treatm flt jt .
.
.
.
.
.
treatm .
.
.
.
.
.
fault xs .
.
.
.
.
.
treatm flt xs .
.
.
.
.
.
e. analysis the post questionnaire survey we used answers to questions from q1 to q8 to gain insights on the subjects activity.
results are summarized in table vii.
considering data over all the experiments answers to questions q1 q2 q4 q7 and q8 produced statistically significant results p value while answers to the other questions are not significant.
subjects found the time slightly insufficient to accomplish all the tasks fq1 i.e.
not certain .
subjects considered the tasks to be clear q2 and overall they had no difficulty in understanding the source code of the test cases q4 .
finally the debugger was used q7 only in the second experimentbut it was judged useful q8 in all the experiments.
during the first experiment even if judged useful subjects did not significantly use the debugger p value q7 .
table vii analysis of post quest q1 q8.
m ann whitney test for the null hypothesis median qx quest.
exp i exp ii all median p.value median p.value median p.value q1 .
.
.
.
.
.
q2 .
.
.
.
.
.
q3 .
.
.
.
.
.
q4 .
.
.
.
.
.
q5 .
.
.
.
.
.
q6 .
.
.
.
.
.
q7 .
.
.
.
.
.
q8 .
.
.
.
.
.
then we compared the answers for questions specific to test cases q4 to q17 to understand if any statistical difference can be observed between subjects who worked with manually written test cases and those who used autogen ones.
the unpaired mann whitney s test never reported statistical significance so we omit the table for lack of space it can be found in the technical report .
we can notice a remarkable difference in the use of the eclipse debugger between the first and the second experiment see q7 in table vii .
in fact subjects involved in the first experiment declared to have used the eclipse debugger less than subjects involved in the second experiment even though both recognize the potential usefulness of the debugger see q8 in table vii .
this might explain the larger gap between manual and random test cases observed in the first experiment compared to the second experiment see figures and for a comparison of accuracy and efficiency respectively .
without the debugger subjects could take advantage only of simple test cases i.e.
those generated by randoop while they could not manage the complexity of most manual test cases resulting in lower performance in the latter case.
on the contrary subjects who used the debugger more extensively were able to take advantage also of the complex test scenarios.
however they also had better performance with the simpler random tests.
the extensive use of the debugger in the second experiment compared to the limited use of the debugger in the first experiment might also explain the learning effect mentioned in the analysis of the lab as co factor.
the learning effect has been observed only for the subjects who worked with manually written tests first.
we can hypothesize that after analyzing manual tests with the debugger subjects gain a knowledge of the debugging environment that can be effectively reused when analyzing random tests.
on the contrary subjects who analyzed random tests first did not gain knowledge that can be used in the analysis of manually written tests due to the simplicity of the executions considered before.iv.
d iscussion a. interpretation of findings the key result of the experiment is that autogen test cases affect positively debugging by improving the accuracy and efficiency of developers conducting fault localization and bug fixing tasks and that in our experiment manually written test cases are more complex than autogen test cases but they contain more meaningful identifiers.
we can summarize our interpretations int of the data collected in our experiment as follows int1 meaningfulness of test case identifiers does not affect debugging accuracy and efficiency .
manual test cases have more non artificial identifiers and substantially less artificially generated identifiers see analysis of identifiers in test cases .
however this does not result in superior debugging performance of subjects using manual test cases.
indeed the opposite is true.
we think the presence of meaningless identifiers in autogen tests is not an influential factor because such identifiers appear only when debugging the top level methods in execution i.e.
the test methods .
below such top level identifiers are perfectly understandable and meaningful.
moreover any difficulty of interpretation of a test method due to its identifiers does not matter as long as the test reveals a fault.
subjects probably did not even attempt to attribute an intent to autogen tests.
int2 test case complexity affects debugging accuracy and efficiency .
manual test cases exercise complex long execution scenarios that require substantial effort to be fully understood.
autogen test cases are simple short linear sequences of method invocations that do not require any dedicated understanding activity see analysis of dynamic test case complexity .
in our experiment this difference may at least partially explain the superior performance of autogen test cases.
the difference in the dynamic metrics between manual and random test cases can be attributed to the different way in which the test cases have been constructed.
manual test cases have been designed to exercise a high level usage scenario which involves application functionalities requiring a complex and extensive code execution.
on the other hand random test cases are focused on implementation functionalities not user requirements and consider a portion of the implementation quite locally with a limited amount of dependencies from other code portions.
int3 ability and experience are key factors affecting the debugging performance .
even when provided with simple test cases inexperienced programmers had a hard time fixing the bugs see analysis of interaction between treatment and experience .
availability of focused simple test cases empowers the fault finding capabilities of developers only if these have enough previous experience and related skills.
debugging is a hard task that requires training and discipline and it cannot be learned in the short time provided by an empirical study like ours.int4 debugging performance improves over time especially if developers are exposed to complex test scenarios .
we observed a learning effect which was particularly relevant when manual hence more complex test cases were used see analysis of interaction between treatment and lab .
this seems to indicate that debugging tasks conducted in the past affect to a major extent the performance expected on a new debugging task.
we think this has probably to do with an incrementally learned capability of fine tuning the debugging strategy depending on the test scenario at hand.
such capability can be learned but it requires training on complex scenarios.
int5 usage of advanced debugging environments is fundamental with complex test scenarios .
only subjects able to effectively use the eclipse debugger could use the more complex manual test cases to fix bugs see analysis of feedback questionnaire .
when the complexity of a test case becomes high automation of the debugging activities is required in order for the tester to be able to effectively and efficiently investigate the execution and locate and fix the fault.
still simpler test cases amplify the benefits of the debugging environment.
we think the collected results have interesting implications on the use of tools for automated test case generation and on the testing process as a whole.
the debugging capabilities of skilled and experienced testers can be amplified by providing them with focused and simple autogen test cases that reveal the fault to be fixed.
such a benefit is not compromised by the use of meaningless identifiers in the test cases.
hence whenever the same bug can be revealed by complex manual test cases but also by simple automated tests the latter are preferable to maximize the debugging performance.
based on the results obtained in our experiment we reconsidered the whole testing process and the potential room for automated test case generation.
we think that our results suggest the following strategy first generate automated test cases and fix any bug possibly revealed by them write consider manual test cases only later since these are less effective for the bugs revealed by both classes of tests.
however since manual test cases tend to be more complex we think they should not be replaced by the automated ones.
they should be just considered later for those bugs that require complex execution scenarios to be exposed.
b. threats to validity the main threats to the validity of this experiment belong to the conclusion internal construct and external validity threat categories.
conclusion validity threats concern the relationship between treatment and outcome.
we used non parametric statistical tests mann whitney and wilcoxon that do not require normal distribution of the experimental data.the only parametric test we performed is anova used only to assess the interactions which is however robust for deviations from normality.
the survey questionnaire was designed using standard scales.
internal validity threats concern external factors that may affect the independent variable.
subjects were not aware of the experimental hypotheses.
subjects were not rewarded for the participation in the experiment and they were not evaluated on their performance in the experiment.
construct validity threats concern the relationship between theory and observation.
they are mainly due to how we measure the accuracy of debugging.
we relied on previously defined test cases to objectively evaluate whether the fixes were correct.
the ability of subjects was estimated during the training phase on similar debugging tasks and using the exam scores.
external validity concerns the generalization of the findings.
we considered test cases generated by randoop.
workin progress aims at experimenting with test cases generated using other techniques such as concolic testing.
even if we considered two different real world systems from different domains and with different complexity more replications are desirable on other applications.
the study was performed in an academic environment which may differ substantially from the industrial one.
however we mitigate this threat by considering ability and experience as a co factor.
v. r elated work multiple strategies can be used to automatically generate unit test cases.
for instance it is possible to combine concrete and symbolic execution use dynamically detected assertions apply mutation analysis and randomly combine method invocations .
these techniques have been reported several times as effective solutions for revealing programming faults .
however no studies are available about the impact of automatically generated test cases on debugging activities.
in this paper we report an experiment that for the first time measures the effectiveness of automatically generated test cases when used as part of debugging tasks.
in particular we compare manually defined test cases with random test cases generated by randoop.
our results indicate that automatically generated test cases represent a valid support for debugging.
even if in this study automatically generated test cases outperformed manually defined test cases in terms of accuracy and efficiency we have to recall that automatically generated tests suffer well known limitations such as the impossibility to generate domain dependent oracles as well as the capability to generate tests of moderate length and complexity only .
most of the literature on empirical studies related to testing and debugging presented experiments that do not involve human subjects.
for instance frankl and weiss compared the capability of revealing faults when test cases satisfy different coverage criteria.
however their work didnot consider the intensive manual activity spent to locate the fault after a test case revealed it.
a few empirical works on software testing and debugging involved human subjects but they considered directions different from the work presented in this paper.
for instance the studies by ricca et al and huang et al focused on the testing process and strategy evaluating the impact of the test first strategy either on the accuracy of change tasks or on the quality of the final code.
a few attempts have been made to investigate the relation between testing and debugging.
fry et al presented an observational study on the accuracy of human subjects in locating faults.
they discovered that certain types of faults are difficult to locate for humans.
for instance extra statement faults seem to be easier to detect than missing statement faults.
however the authors did not investigate the role of test cases.
weiser et al.
empirically evaluated the impact of a slicing tool on debugging.
they did not observe any improvement when developers used a slicing tool to debug small faulty programs.
parnin and orso performed an experiment to investigate how developers use debugging tools and whether these improve performance.
tools are proven to help complete debugging tasks faster.
still this study does not consider the role of test cases.
vi.
c onclusion to the best of our knowledge this is the first humanbased study that evaluates the impact of different kinds of test cases on debugging.
the data we collected from two experiments indicate that automatically generated test cases positively affect debugging improving both the accuracy and the efficiency of developers working on fault localization and bug fixing tasks.
this result demonstrates that even if automatically generated test cases contain less understandable identifiers the simplicity of these tests facilitates the implementation of correct fixes.
in our experiments subjects took advantage of the simple structure of the tests to debug problems faster and better especially when subjects were well experienced.
to corroborate our findings this study needs to be extended in several directions such as considering more applications more types of faults and more test case generation techniques.
nevertheless the results reported in this paper provide useful insights for testers and software engineers and clarify issues related to the yet unexplored interplay between test automation and human factors.