precision reuse for efficient regression verificationy dirk beyer1 stefan l we1 evgeny novikov2 andreas stahlbauer1 and philipp wendler1 1university of passau germany 2institute for system programming isp ras russia abstract continuous testing during development is a well established technique for software quality assurance.
continuous model checking from revision to revision is not yet established as a standard practice because the enormous resource consumption makes its application impractical.
model checkers compute a large number of veri cation facts that are necessary for verifying if a given speci cation holds.
we have identi ed a category of such intermediate results that are easy to store and e cient to reuse abstraction precisions .
the precision of an abstract domain speci es the level of abstraction that the analysis works on.
precisions are thus a precious result of the veri cation e ort and it is a waste of resources to throw them away after each veri cation run.
in particular precisions are reasonably small and thus easy to store they are easy to process and have a large impact on resource consumption.
we experimentally show the impact of precision reuse on industrial veri cation problems created from linux kernel device drivers with revisions.
categories and subject descriptors d. .
software engineering software program veri cation f. .
logics and meanings of programs specifying verifying reasoning about programs general terms veri cation reliability languages keywords formal veri cation regression checking .
introduction reliable software is essential both for convenience and safety in our daily lives and for the revenue in the economy.
producing reliable software is costly and speeding up testing and formal veri cation of software can save huge amounts of time and money.
economic pressure requires companies to come up with innovations more quickly by introducing more features in shorter release cycles software is a key contributor to today s innovations.
however the problem of ya preliminary version appeared as technical report .
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
esec fse august saint petersburg russia copyright acm ... .
.extending software e.g.
by introducing a new feature is that this might break existing features bugs get introduced.
this is known as regression.
to avoid regression developers execute automated tests before a new revision of a piece of software is checked in in the hope that the tests alarm the developer of any new bug.
the quality of the software in terms of correctness depends on the coverage of the regression test set.
regression testing is an established and well investigated technique since many years e.g.
.
the con dence of correctness can be increased by augmenting the development process with formal veri cation i.e.
regression veri cation .
formal veri cation exhaustively checks the program for bugs but at the same time consumes large amounts of computation resources time and memory in particular when applied to industrial size software.
regression veri cation applies formal veri cation techniques to continuously check development revisions in order to identify regressions early.
innovations in this eld pave the road that leads from regression testing to regression veri cation and from simply nding bugs to actual proofs of correctness during the whole software development process.
veri cation tools spend much e ort on computing intermediate results that are needed to check if the speci cation holds.
in most uses of model checking these intermediate results are erased after the veri cation process wasting precious information in failing and succeeding runs .
there are several directions to reuse intermediate results .
conditional model checking outputs partial veri cation results for later re veri cation of the same program by other veri cation approaches.
regression veri cation outputs intermediate results or checks di erences for re veri cation of a changed program by the same veri cation approach.
the contribution of this paper is to reuse precisions as intermediate veri cation results.
in program analysis e.g.
predicate analysis shape analysis or interval analysis the respective abstract domain de nes the kind of abstraction that is used to automatically construct the abstract model.
the precision for an abstract domain de nes the level of abstraction in the abstract model for example which predicates to track in predicate analysis or which pointers to track in shape analysis .
such precisions can be obtained automatically interpolation is an example for a technique that extracts predicate precisions from infeasible error paths.
precisions are a good choice for reuse in regression veri cation because they are technically easy to use and do not require much extra computation e ort before they can be reused they have a small memory footprint and they are as we show rather insensitive to changes in the program source code.
we performed an extensive experimental studypermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse august saint petersburg russia copyright acm ... .
389table veri cation of driver extcon arizona without and with precision reuse time in seconds of cpu usage rev.
commit message result re nements with reuse abstractions with reuse analysis time with reuseresult re nements with reuse abstractions with reuse analysis time with reuse implement button detection support safe unsafe free micdet irq on error during probe safe unsafe x typos in extcon arizona safe unsafe use bypass mode for micvdd safe unsafe merge tag driver core .
of git git.kernel.org .
.
.
safe unsafe 8unlock mutex on error path in arizona micdet safe safe remove use of devexit safe unsafe remove use of devinit safe unsafe remove use of devexit p safe unsafe merge tag pull req20121122 of git git.kernel.org .
.
.
safe safe speci cation spinlocks lock unlock speci cation mutex lock unlock on industrial code in order to show the signi cant impact of precision reuse on regression veri cation in terms of performance gains and increased number of solvable veri cation tasks .
the benchmark veri cation tasks were extracted from the linux kernel which is an important application domain and prepared for veri cation using the linux driver veri cation toolkit ldv .
our study consisted of a total of veri cation runs for veri cation tasks composed from a total of revisions spanning more than years of linux drivers from the linux kernel repository.
verifying large numbers of program revisions often takes several hours or even days.
our approach of precision reuse can speed this up by a factor greater than in terms of cpu time for the analysis on average for predicate analysis.
example.
we consider ten revisions of the linux device driver extcon arizona for which a bug was discovered using formal veri cation by the ldv team1.
table lists the revisions and the corresponding commit messages in bold the commit that xes the above mentioned bug .
we verify two speci cations with a cegar based predicate analysis spinlocks lock unlock and mutex lock unlock .
revisions 3to7and9to11violate speci cation .
tasks that violate the speci cation generally need less re nements and abstraction computations because the analysis terminates as soon as a bug is found.
in cases where the speci cation holds the whole state space of the program has to be analyzed mostly a large number of re nements and expensive abstraction computations have to be performed.
the columns titled with reuse show the results with precision reuse.
for cases where a complete reusable precision from a successful veri cation of a previous revision is not available revision 3for speci cation revisions 3to8for speci cation because the whole state space was not yet analyzed before there is no speedup.
for most cases where a complete state space analysis was done in a previous run with result safe and thus a large precision is available for reuse a speedup of 4can be achieved cpu time for analysis around 1s instead of over 4s .
re nements are eliminated completely because all necessary veri cation facts are already speci ed by the reused precision that is given as input.
contributions.
we make the following novel contributions we identify the abstraction precisions as intermediate results that are valuable for reuse in regression checking.
we de ne a tool independent format for persistent storage and exchange of precisions.
extend the existing software veri cation tool cpachecker in order to support regression veri cation with precision reuse.
we prepare and consolidate a benchmark set for regression veri cation that is based on industrial source code from the linux kernel and consists of thousands of benchmarks.
in an extensive experimental study we show that precision reuse leads to signi cant performance improvements and causes almost no overhead for the veri cation tool as well as for the benchmarking infrastructure and thus no additional barriers in a software development process .
related work.
the desire of constructing e cient tools for incremental formal veri cation exists since more than years .
in the literature there are two main directions to approach the problem of regression veri cation based on analyzing the di erence between the program and another program that was successfully veri ed in a previous veri cation run and based on reuse of intermediate results that were costly computed in previous veri cation runs.
veri cation of di erences.
the rst group of approaches to e cient regression veri cation takes two programs as input and analyzes the di erences in order to verify whether the speci cation is still ful lled.
an input condition can restrict the analysis to certain relevant parts of the state space .
these approaches can be seen as conditional model checking where the input condition instructs the veri er to perform a partial veri cation.
the parts of the program that were identi ed as not being a ected by modi cations can be skipped during the veri cation process.
a technique for proving conditional equivalence of two programs isolates and abstracts the functions of both versions using uninterpreted functions and then proves their equivalence.
checking the equivalence of two programs can be reduced to checking just those parts impacted by changes .
reuse of veri cation results.
the other group of approaches reuses state space graphs constraint solving results function summaries or counterexample traces .
to ensure that the information is valid to be reused those parts of the information that were a ected by changes to the analyzed program or its speci cation have to be re validated.
the check for reusability is done either before the actual veri cation process is started or immediately before certain information should be reused .
extreme model checking is the only existing approach that uses unbounded model checking with lazy abstraction and predicate analysis for390regression veri cation.
evolcheck is based on bounded model checking the reuse is focused on function summaries enhanced by a syntactic analysis of the di erences between the program versions for improved performance.
another way of information reuse is to not store the concrete data but its hash value.
one such approach stores hashes of veri ed models these models are constructed by reducing a program to those parts that are relevant to prove a property.
to be e cient model construction must be less expensive than verifying the model.
for formal regression veri cation of hardware using the ic3 algorithm the reuse of correctness proofs and counterexamples has been proposed .
a more general fashion of reuse is to store and reuse canonicalized constraint solver queries and the corresponding results .
our approach belongs to the second category we reuse abstraction precisions as intermediate results and do not explicitly analyze the di erences in the program code our approach implicitly spends more e ort on changed parts .
this is the rst work that reuses abstraction precisions.
in general it is possible to combine the reuse of di erent kinds of information for example state space graphs abstraction precisions and constraint solver results.
approaches for reusing veri cation results can also be preceded by inexpensive syntactic checks for di erences between program versions for improved performance.
.
background abstract reachability graph.
the class of analyses that we consider in our work is based on creating an abstract model of the program in form of an abstract reachability graph arg .
an example for such an analysis is blast .
the arg is constructed iteratively by unrolling the controlow automaton cfa of the program creating an abstract successor state for the next location whenever the control ow passes through an edge of the cfa.
the creation of abstract successor states is usually over approximating and guided by some form of precision that instructs the analysis which facts should be tracked and which facts should be omitted by abstraction.
the abstract domain determines the characteristics of the precision.
for example if the abstract domain tracks information on program variables explicitly then the set of relevant program variables to consider at a program location is a suitable precision for the analysis .
the precision in use should require the tracking of just enough information to prevent false alarms while at the same time be as concise as possible for an e cient analysis.
counterexample guided abstraction re nement cegar .
cegar is a well established technique for automatically nding a suitable precision that matches the above criteria.
beginning with an initial coarse or even empty precision the arg is created based on this initial precision.
if no state violating the speci cation is found the program is proved safe.
if a violation of the speci cation is found the concrete path of this counterexample is analyzed for feasibility.
if it is feasible the program is unsafe and the analysis terminates.
otherwise the abstract model of the program was too coarse so the precision needs to be re ned to exclude this infeasible counterexample from future explorations.
depending on the abstract domain the facts necessary to rule out this counterexample are extracted from the proof of infeasibility and added to the precision.
then the cegar loop is continued with this newly re ned precision.lazy abstraction.
the e ciency of cegar based analyses can be increased by using lazy abstraction .
instead of always restarting the analysis from scratch after an infeasible counterexample was found the abstract model is re ned in a lazy style.
that is during counterexample analysis the newly learned facts that are extracted from the counterexample are only added where necessary.
then only those parts from the arg that were computed with a too coarse precision are removed and scheduled for re exploration.
the remainder of the arg for example a pre x of the current counterexample path or other paths not related to the current counterexample are kept and are neither removed nor re explored.
this does not only reduce unnecessary recomputations but also reduces computation e ort by lazily applying the new stronger precision only to those states of the arg where it is needed.
states on unrelated paths of the arg are still computed with the old weaker and thus more e cient precision.
a further improvement is to use di erent precisions for each program location in order to track as little information as possible and for example to erase information during path exploration when reaching a location after which the information is not needed any more .
predicate analysis.
one technique which is used widely together with the above concepts is predicate abstraction .
given the set xof program variables and the set pof quanti er free predicates over variables from x the abstract domain here is the set of boolean combinations of predicates fromp.
the precision is a set of predicates from p. when constructing the arg abstract successor states are created by computing either the cartesian or the boolean abstraction of the current state using the predicates from with an smt solver.
using craig interpolation predicates can be generated fully automatically from a proof of unsatis ability for the formula representing a spurious counterexample .
the performance of predicate abstraction can be improved by adjustable block encoding abe .
this technique groups program statements into blocks and computes abstractions only at the end of each block instead of at all program locations.
furthermore if control ow merges within a block paths in the arg are also merged so that sets of paths are considered instead of single program paths.
when using abe loops which encodes loop free parts of the program into blocks abstractions will be computed only at loop head locations.
thus predicates will be relevant only at these locations and no precision is used at all other locations.
explicit value analysis.
another domain that can utilize a precision is explicit value analysis which tracks the current value for each program variable explicitly.
within this analysis an abstract state is represented as an abstract variable assignment x!z f ?g wherexdenotes the set of program variables of a program.
the value represents a variable valuation that is unknown e.g.
due to an uninitialized variable the value ?represents a variable valuation that is impossible.
abstract successor computations are performed by evaluating program operations and assigning the resulting values to the respective program variables in abstract variable assignments explicitly in contrast to modeling them symbolically as in the predicate domain.
the precision for an abstract variable assignment is de ned as a set of variables from x which is used to restrict an abstract variable assignment to variables from that precision .
for example applying the precision fbgto the abstract variable assignment v fa7!
b7!15gwould result in the391abstract variable assignment v fb7!15g.
experiments show that a variable that is relevant for one path is often relevant on similar paths as well and thus it is bene cial to add a newly found relevant variable to the precision for all locations of the function in which it is relevant.
this reduces the number of re nements because similar paths can be eliminated often without further re nements.
.
precision reuse de nitions.
aprecision is the information that an abstraction based analysis uses to guide the abstraction computation for creating abstract states.given an analysis we write for the set of possible precisions and for one element thereof.
the empty precision is the coarsest precision from usually this precision de nes that all information is abstracted .
the union of two precisions from is de ned in the intuitive way.
for example for predicate abstraction a precision is a set of predicates over program variables and the union of two precisions is the union of the two sets of predicates.
in order to use lazy abstraction which supports di erent precisions at di erent program locations we de ne a program precision as a mapping l!
from the set lof program locations to the set of precisions .
the union of two program precisionsp1andp2is the program precision that maps every locationlto the union of p1 l andp2 l .
format for program precision files.
in order to write and read precisions to and from persistent storage we dene a simple text based le format that describes program precisions in a human readable and tool independent way.
a formal de nition of the format is given in a technical report .
the basic structure is the same for all analyses.
the le starts with a header the content depends on the analysis .
after the header an arbitrary number of sections follow each consisting of one line of scope selectors and an analysis dependent precision.
there are three kinds of scope selectors the literal representing all program locations the name of a function in the program representing all locations inside this function and the id of a program location representing this single location .
the precision given in a section is used at all locations represented by the speci ed scope selector.
the e ective precision for any given program location is the union over the precisions from all sections with a scope selector matching that location.
for explicit value analysis the header is empty and each precision is a list of variables that occur in the program.
for predicate analysis each precision is a list of predicates given in the syntax of the smt lib standard a standard for smt solver interfaces supported by state of the art smt solvers the header contains a sequence of term declarations which may be referenced by the formulas in the sections of the precision le.
example.
consider a c program that contains two variables lock and x both of which are relevant for proving the safety of the program.
variable lock may be relevant at all locations whereas variable xmay be relevant only in the functions main andf.
an example program precision le for explicit value analysis that encodes this information is given in fig.
left .
an example for predicate analysis is shown in fig.
right assuming that the model checker encodes these variables as real numbers and the predicates lock andx are relevant.
lock main f x declare fun lock real declare fun x real define fun t1 bool lock define fun t2 bool x assert t1 main f assert t2 figure example program precision les left explicit value analysis right predicate analysis generating program precision files.
in order to enable the reuse of precisions we collect all program precisions that are created during the analysis typically there is one for each re nement step and create the union over all these program precisions.
this resulting program precision is written to a le.
for each program location the precision is written once with the id and once with the function name of the program location as scope selector not written if empty .
precision reuse.
in order to reuse a precision for the subsequent analysis of the same or a similar program an initial program precision for the analysis is created by interpreting the contents of a previously stored program precision le.
there are three possibilities to construct such a precision.
first precisions can be function scoped such that a precision is created for each function of the program by taking the union of all precisions labeled with the function name.
the result is assigned to all locations of the respective function.
note that this will widen the scope of precisions thus potentially leading to a more precise abstraction and also loose precisions if functions are renamed.
this precision assignment is insensitive to changes of the controlow structure within functions of a program.
second precisions can be location scoped such that the location ids in the le are used to identify the locations to which the read precision is assigned in the resulting program precision.
for all program locations that do not occur in the le the empty precision is assigned.
note that location ids may change if the program code was changed and thus precisions get assigned to locations that do not semantically correspond to the original location in the previous program.
third precisions can be global scoped by taking the union of all precisions in the le and assigning the result to all locations of the program.
this will not loose any precision from the previous analysis but might apply precisions to locations where they are not necessary and thus make the analysis more expensive .
in any case precision elements that reference removed program variables or functions and are thus irrelevant can be identi ed and ignored with a quick syntactical check.
after the creation of the initial program precision the analysis is started as usual.
no change to the analysis itself is necessary.
if the provided precision is strong enough to prove the program safe no further re nement e ort will be needed.
if the input precision contains only a part of the necessary precision to be tracked spurious counterexamples will be detected and subsequent re nements will strengthen the precision.
note that even in this case the input precision likely reduces the e ort by decreasing the number of necessary re nements.
this process may be iterated by writing again the program precision that was further re ned by the second392analysis to le and using this as the input for a further analysis possibly on a newer version of the program.
discussion.
one signi cant e ect of reusing precisions from a previous veri cation run is a reduced number of re nements.
these are usually among the most expensive operations executed by a model checker for example involving satis ability checks and interpolation queries over formulas that represent sets of complete program paths from the entry point to the error state .
the second signi cant e ect is that fewer renements lead to a reduced e ort on partial arg pruning and re construction.
this is especially important for analyses that perform expensive operations during this phase for example in predicate analysis which needs smt solver queries to compute abstractions.
while the introduction of large block encoding has reduced the number of such computations by only abstracting at loop head locations and not at every program location the need to use boolean abstraction still makes this costly.
precision reuse is an elegant and conceptually simple approach because it integrates naturally into the techniques that are used by many successful model checkers.
these techniques can be applied as they exist without any change to the rst initial veri cation run when no reusable information is present and also to the subsequent re veri cation runs.
furthermore this makes precision reuse applicable not only for the two presented analyses but also to any analysis and abstract domain that is based on cegar and incorporates an abstraction step that is guided by some form of precision.
for example precision reuse would extend naturally to the abstract domain of shape analysis .
precision reuse is easy to implement in existing model checkers that are based on cegar and abstractions.
only the import and export of precisions before and after the actual analysis needs to be added.
complex algorithms as required for comparing two revisions of a program and detecting similar and changed code are not necessary in our approach.
the format we de ned is easy to parse and write and could be supported by a variety of model checkers thus even enabling the reuse of precisions across di erent tools.
furthermore precision reuse is also user friendly a user that is already familiar with using one model checker will not need to learn how to use new concepts or tools.
exporting precisions as part of the analysis result should be enabled by default in most tools and thus the only necessary action by the user is to supply the previously written program precision le as an additional input to the next veri cation run.
even if the user mistakenly speci es a wrong program precision le as input the analysis will still work correctly the result will be valid and only the performance might degrade slightly.
in order to employ precision reuse it is not necessary to have access to previous program revisions the only information needed is the small generated program precision le.
applicability of precisions.
the precisions from the previous veri cation run can be applied to the program locations of the program s next revision using three strategies which di er in how they widen the scope of the precisions.
a location scoped precision is applied at exactly those locations stated in this precision.
for example consider a precision that is relevant at locations to of a program.
now a change is made to the program and a statement that is unrelated to the safety of the program is introduced right after location .
thus the previous locations 5to10now correspond to the new locations .
the previousprecision is not applied to location 11and the analysis would nd an infeasible error path thus needing at least one renement to rediscover the missing facts.
function scoped precisions are insensitive to such changes.
even changes due to cross cutting concerns that a ect code locally in many functions are expected to be veri able without many further re nements.
changes to the call graph of the program however might still generate a similar need for re nements for example if code that is relevant to the safety of a program is moved to another function.
global scoped precisions reduce this problem further making re nements only necessary if code referenced by the precision is changed directly for example if variables are renamed .
we consider location scoped precisions to be too sensitive to program code revisions.
which of the other two strategies performs better depends on the class of program changes e.g.
whether heavy refactorings changing the functions of the program are common and how expensive an unnecessarily ne precision is for the analysis.
often the latter has less e ect than one would intuitively consider.
for example specifying local variables from a function fin the precision of a function ghas no e ect because the local variables in f are out of scope in g. the policy of most projects is to create small commits with mostly local changes thus we expect function scoped precisions to be most promising in practice.
.
experimental evaluation in order to evaluate the impact of precision reuse on the e ectiveness and e ciency of regression veri cation we performed an extensive experimental evaluation.
we used industrial software for our experiments in total we prepared veri cation tasks from revisions of device drivers from the linux kernel.
we started veri cation runs on all those problems with both an explicit value analysis and a predicate analysis each with and without precision reuse a total of runs .
our tool implementation the c source code of the device drivers and the full benchmark results are available on our supplementary web page dbeyer cpa reuse .
during our experiments we found an actual bug in the linux kernel2.
implementation.
our implementation is based on the open source veri cation framework cpachecker3 which is available under the apache .
license.
cpachecker provides implementations of explicit value analysis and predicate analysis with abe .
both approaches are based on cegar and use a precision to de ne the level of abstraction.
thus we only had to add support for writing the program precision to le after a veri cation run and reading a previously written program precision to be used as initial precision before a veri cation run.
the format for persistent storage of program precisions is described in sect.
.
further changes to the veri cation tool were not necessary in particular the veri cation algorithm and the abstract domains were not changed.
our extension for precision reuse is integrated into the trunk of the project s source code repository4.
veri cation tasks.
a veri cation task is a fully speci ed veri cation input which is referred to by a triple that consists of the name of the driver the speci cation that the driver has to satisfy and the revision number from the repository.
of an industrial benchmark for regression veri cation.
we started our selection process by considering the veri cation tasks from the category devicedrivers64 of the 2nd intl.
competition on software veri cation sv comp which is a benchmark set that consists of veri cation tasks.
from this set of veri cation tasks we selected those device drivers that met the following two criteria cpachecker in revision needed more than s of cpu time to report either safe orunsafe and at least one re nement was necessary to verify the driver.
this selection process helped us to omit trivial tasks and those for which precisions are not needed.
in total device drivers from the sv comp benchmarks ful lled the criteria above.
we extracted the sources for all available revisions of those drivers from the o cial linux kernel repository5.
each of these device drivers consists of several header and source les each having its own revision history.
we considered all commits to all c source les of the device driver in chronological order starting with the revision in which the device driver was added to its directory in the kernel repository if the driver resided in the staging area of the kernel before being accepted into the main area these revisions were not considered .
in order to obtain a linear history of changes we excluded commits that occurred on branches that were created during the development of a driver the merge commits that reintegrated such branches are included and thus no changes are lost .
our oldest revisions date back to the year and the latest revisions to the end of .
in order to obtain veri cation tasks we also need speci cations.
we used as speci cations six di erent rules for correct linux kernel core api usage cf.
table .
we composed each revision of the selected drivers with each speci cation.
the composition was done using the ldv toolkit6 and consisted of adding a main function that simulates calls to the device driver from the linux kernel core weaving in one of the six speci cations reducing the rule based speci cation of the property into a reachability property by instrumenting the driver with a monitor automaton and combining all les that the device driver in the particular revision consists of into a single le using cilpre processing .
the result of this composition process is a veri cation task that consists of a single veri able c le for each revision.
we omitted tasks where the speci cation is trivially satis ed e.g.
speci cation module get put for drivers that do not call the functions try module get and module put .
for evaluating the e ect of our approach we need to consider those veri cation tasks for which the precision needs to be fully discovered and where repeated application of the veri er yields deterministically the same precision.
this is not the case for veri cation tasks with a known speci cation violation because the analysis can terminate as soon as nding a counterexample skipping parts of the state space.
of course precision reuse is applicable in such cases as well witnessed by the bug we found but in our benchmarks the numbers would not be comparable.
therefore we remove from our benchmark set all veri cation tasks with the expected result unsafe .
the resulting benchmark set7for regression veri ca5git git.kernel.org pub scm linux kernel git torvalds linux.git dbeyer cpa reuse regression benchmarks table considered speci cations ldv rules name description 081amodule get put.
for each successful call to trymodule get a corresponding call to module put that unblocks the module must exist.
321mutex lock unlock.
a less accurate implementation of speci cation 7a.
327amutex lock unlock.
a mutex must not be acquired or released twice.
a mutex must not be released without prior acquiring.
finally all mutexes must be released.
397aspinlocks lock unlock.
a spin lock must not be acquired or released twice.
a spin lock must not be released without prior acquiring.
finally all spin locks must be released.
431amemory allocation inside spinlocks.
the ag for atomic allocation operations must be used whenever a memory allocation function call is done while a spin lock is held.
681usb alloc free urb.
for each allocation of an usb request block urb using usb alloc urb a corresponding call to usb freeurb must exist.
tion consists of a total of industrial strength veri cation tasks which allows us to perform a signi cant experimental study.
its high quality and usefulness has been acknowledged by the esec fse artifact evaluation committee.
di erences between veri cation task revisions.
while normally source code changes for the device drivers are rather limited from revision to revision our benchmark set has quite large source code di erences between revisions which is not by design rather as a side e ect good to evaluate insensitivity to changes.
we explain the main three reasons in the following whenever commits occurred in branches we did not include the corresponding revisions along the branch instead we extracted only revisions from the mainline branch.
the revisions after a merge into the mainline branch result from a single generally larger commit.
another reason for a large di erence between revisions is the omission of revisions with a known speci cation violation.
thus the changes from such revisions appear together with the changes of the next commit in the succeeding revision without a speci cation violation.
another cause for large di erences is that we took one snapshot of the code for each revision in which one of the c source les of the device driver was changed.
however in the kernel project there are many other header les that in uence the code of a particular le by being included from the le and by de ning macros types inline functions etc.
which are used in the code.
thus the change between two revisions incorporates not only the changes to the c source les of the device driver but also the changes to all other kernel header les since the last revision.
the latter changes are sometimes even larger in size and e ect than changes to the driver.
for example the introduction of the kernel feature config branch tracer pro ling of unlikely and likely branches in the code by code instrumentation added several lines of auxiliary variables per ifstatement and this additional code appears as new code in the next revision that was made for each driver after the feature was introduced.
our benchmark set of veri cation tasks has an average of changed lines of source code between subsequent revisions.
our results presented in the following show that precision reuse is quite insensitive to such large di erences between revisions.
results for driver dvb usb az6007 using predicate analysis time in seconds of cpu usage n th rev.
di .
lineswithout precision reuse with precision reuse spec.
analysis time re nements abstractions analysis time re nements abstractions 081a32 total 327a32 total 397a32 total setup.
all experiments were performed on machines with a .
ghz quad core cpu intel core i7 and gb of ram.
we used ubuntu .
bit with linux .
and openjdk .
.
we used cpachecker revision .
the predicate analysis uses mathsat .
.
as smt solver.
each veri cation run was limited to minutes of run time and gb of ram the java heap size was limited to gb.
this is a similar environment to the community agreed setting of sv comp .
the analysis time that we report refers to the cpu time of the analysis phase of the veri cation tool excluding startup and program parsing and is given in seconds with two signi cant digits.
the size of code di erences between two revisions of one program is given as the number of di ering lines excluding whitespace changes calculated with diff ignore all space diffstat .
results.
we experiment with the reuse of precisions across a sequence of di erent revisions of a program.
for this we start the veri cation of the rst revision with the empty precision save the generated precision and use it as the initial precision for the veri cation of the second revision.
the nal precision of the second veri cation run is used as input precision for the veri cation of the third revision and so on.
we compare the time needed for this process against the time that is needed for verifying all revisions individually using the empty precision as the input for each run and without generating program precision les .
results for a single driver.
the results for a single driver dvb usb az6007 from the linux kernel are shown in table .
there are ve revisions for this driver and we show the veri cation of three speci cations using predicate analysis.
the column di .
lines shows the number of lines di ering in one revision compared to the previous revision.
the lines total show the sum of the respective values for all revisions with one speci cation.
as expected the run time for verifying the rst revision is not decreased by the reuse of precisions there is no precision to reuse also there is no signi cant overhead for writing the precision to the output le.
for the remainingrevisions the run time results show a clear improvement of performance if the precision from the previous revision is reused.
this is achieved by almost completely eliminating the need for re nements and by lowering the number of costly boolean abstraction computations considerably compared to the veri cation of the same program without precision reuse.
it is interesting to observe the third revision of this driver this revision a ected the program source in a way that made additional predicates necessary for all speci cations witnessed by the increase in the number of re nements .
in such a case the analysis with precision reuse also has to perform re nements because these additional predicates are not yet known.
however those re nements that were necessary to discover predicates for the rst revision are not necessary because the results are read from the precision le.
thus the run time is still much better than without precision reuse.
results for all device drivers and speci cations.
tables and show the results of verifying all revisions of all device drivers against all appropriate speci cations from table with predicate analysis and explicit value analysis respectively.
due to space reasons we restrict these tables to the best and worst cases out of the total driver speci cation pairs sorted by column analysis speedup .
we also show all cases where due to precision reuse the analysis is able to verify more revisions.
the complete tables are available on the supplementary webpage.
the columns analysis cpu time show the accumulated cpu time used by the analysis to verify all revisions of the device driver against the given speci cation excluding revisions for which a timeout or an out of memory occurred .
in addition the columns total cpu time show the total cpu time used by the tool including program startup and parsing time .
the columns 1st rev.
show the time needed for verifying only the rst revision this is the same with and without reuse .
the column solved tasks shows the number of successfully veri ed revisions out of the total number of revisions for this driver the remaining cases were either timeout or out of memory there were no incorrect veri cation results .
if the value in this column is of format n m this means that without precision reuse only n revisions could be veri ed whereas with precision reuse n mrevisions could be veri ed otherwise the number of successfully veri ed revisions is the same.
there were no cases where a revision could be veri ed without reuse but not with precision reuse.
the column analysis speedup gives the average speedup for the task of verifying a single revision of the driver if a precision from a previous revision is reused in relation to the case where no information is reused based on the analysis cpu time without program startup and parsing .
the analysis time of the rst revision of each driver is not taken into account for calculating the speedup in order to make this value independent from the number of revisions per driver otherwise a driver with more revisions would in general show a higher speedup because the cost of the veri cation of the rst revision has less impact on the speedup .
we also excluded from calculating the speedup such revisions that could not be veri ed by the con guration without reuse.
in the last column we report the maximal size in bytes of the nal program precision le that was produced during the veri cation of the revisions of this driver.
note that our le format is purely text based thus this number gives a coarse over approximation of the amount of information that is reused between veri cation395table best and worst results for predicate analysis details for highlighted line in table device driver spec.
tasks avg.
re nements total cpu time analysis cpu time solved analysis max.
di .
no 1st no 1st no tasks speedup size of lines reuse reuse rev.
reuse reuse rev.
reuse reuse prec.
leds bd2802 1a leds bd2802 1a mos7840 7a dp83640 7a farsync 1a i2c algo pca i915 7a i2c algo pca dmx3191d 1a vsxxxaa it87 1a videobuf vmalloc dvb usb vp7045 xilinx uartps 7a mos7840 1a arkfb 7a vsxxxaa i915 1a vsxxxaa 1a spcp8x5 7a i2c algo pca 7a cp210x 7a dvb usb rtl28xxu 7a it87 7a sym53c500 cs 7a misdn core 7a rtc pcf2123 i915 7a dmx3191d 7a uartlite 7a budget patch 7a it87 7a sym53c500 cs cp210x twidjoy 7a mtdoops cp210x i2c matroxfb 7a sym53c500 cs rtc pcf2123 7a i915 wm831x dcdc 7a dvb usb rtl28xxu 7a metro usb 7a dvb usb az6007 397a ar7part spcp8x5 ssu100 7a panasonic laptop metro usb 7a ...for full results cf.
dbeyer cpa reuse predicate.html... cp210x 7a ...... farsync 7a slram 1a cfag12864b 1a i2c algo pca 1a magellan 7a wl12xx sdio 1a mtdoops 1a wl12xx sdio 7a slram 7a mos7840 1a sum average runs.
the highlighted row shows the driver dvb usb az6007 for which further details are available in table the line here corresponds to the line labeled total for speci cation 39 7a in table .
the bottom rows report the sum and the average of the respective values per driver speci cation pair.
precision reuse not only increases the e ciency but also the e ectiveness for ve pairs of driver and speci cation thenumber of successfully solved veri cation tasks was increased by our approach.
this is possible if an early revision of a driver is veri able and a later revision results in a timeout.
with precision reuse the veri cation of the later revision is easier because a large part of the precision is given as input sometimes making it possible to successfully verify tasks that could not be veri ed before.396table best and worst results for explicit value analysis device driver spec.
tasks avg.
re nements total cpu time analysis cpu time solved analysis max.
di .
no 1st no 1st no tasks speedup size of lines reuse reuse rev.
reuse reuse rev.
reuse reuse prec.
cfag12864b 1a cfag12864b 7a cfag12864b com20020 cs 7a misdn core 7a wl12xx sdio 7a slram uartlite 7a slram 1a sil164 7a slram 7a tcmloop 7a slram i2c matroxfb 7a intel vrnor 7a mtdoops 7a dvb usb rtl28xxu 7a panasonic laptop 7a xilinx uartps 7a lms283gf05 7a uiosercos3 7a cfag12864b gpio regulator 7a dvb usb az6007 7a i2oscsi 7a rtc max6902 7a metro usb 7a emsusb 7a keyspan remote 7a cx231xx dvb 7a dvb usb vp7045 7a budget patch 7a pcc cpufreq 7a lms283gf05 7a budget patch 1a dvb usb az6007 7a keyspan remote 1a rtc pcf2123 7a cp210x 7a dp83640 7a misdn core dvb usb vp7045 ssu100 7a mt2266 7a ab8500 usb 7a catc 7a sym53c500 cs 7a dvb usb rtl28xxu 7a it87 7a mtdoops ...for full results cf.
dbeyer cpa reuse explicit.html... abyss 7a twidjoy 7a i915 1a dmx3191d 7a pt 7a farsync 7a abyss 1a i2c algo pca 1a ar7part 1a magellan 7a sum average the maximum analysis speedup is for predicate analysis and for explicit value analysis with average analysis speedups of .
and .
respectively.
the time that the predicate analysis used for successfully verifying verication tasks without precision reuse was s whereas with precision reuse veri cation tasks more were veri ed in only s less than a third of the time.
the time used by the explicit value analysis improved from s to4 s if our technique of precision reuse is appliedto the veri cation tasks.
this gives evidence for the signi cant performance improvement of our approach.
we also list all negative results there are only a few.
the last lines of the tables report the few cases for which the veri cation with precision reuse takes a bit more time than without.
most of these cases have only a low average cpu time per revision of about 1s.
there is only one case for which the time for veri cation with precision reuse is signi cantly higher the last line in table driver mos7840 with397table results for considering all revisions versus considering only every 4th revision analysis revs.
tasks avg.
total cpu time analysis cpu time analysis solved di .
no no speedup tasks lines reuse reuse reuse reuse predicateall .
4th .
explicit valueall .
4th .
speci cation 43 1a .
however note that precision reuse increased the number of successfully solved tasks from to for this case.
we generally consider an increase in the number of solved programs to be more important than a performance di erence.
a detailed analysis of the mos7840 driver code revealed that a single variable is used as loop counter in three di erent loops.
some predicates about this variable are added to the precision and due to the function scoped precisions in our benchmark con guration are now applied to all three loops the analysis becomes more expensive.
the veri cation of the same driver against the other speci cations actually shows nice speedups e.g.
line of table .
size of precision.
the size of the precision that is necessary to be stored between veri cation runs is small usually just a few kbs in our uncompressed plain text format.
the average size for predicate analysis is kb max kb for explicit value analysis it is bytes max kb .
the total amount of precision storage that was necessary for verifying all veri cation tasks was kb for predicate analysis and kb for explicit value analysis which is orders of magnitude less compared to the size of the source code.
scaling with larger changes.
the changes between subsequent revisions in our benchmark set are rather large affecting lines on average compared to typical developer commits.
to nd out how our approach scales with the size of changes per revision change size sensitivity we created veri cation problems with even more changes we consider only every 4th revision per driver speci cation pair as an alternative benchmark set.
thus the di erence between two revisions in this benchmark set combines the di erences of four actual driver revisions into one.
table shows the results for this experiment in the lines that are marked 4th in column revs.
the lines marked all show the results from tables and for comparison .
the average size of di erences between revisions increased from to lines.
as expected the speedup decreases but only from to for predicate analysis and from to for explicit value analysis.
in both cases the speedup decrease is signi cantly smaller than the increase in the size of the code di erences.
this shows that our approach copes well even with massive changes to the analyzed code.
threats to validity.
to obtain signi cant experimental results we created a large benchmark set consisting of veri cation tasks.
for highly credible experimental data and instead of relying on random or arti cial benchmarks our selection of veri cation tasks is based on hundreds of actual source code commits to di erent linux device drivers.
the characteristics of systems software in particular kernel device drivers might be similar and could have an impact on the validity of our experiments but linux driver veri cation is important enough to be representative on its own .
the linux driver veri cation program of the linux veri cation center and microsoft s static driver veri er project dedicate considerable resources to driver veri cation.we used an experimental setup and environment that is identical to the infrastructure for the competition on software veri cation community agreed .
precision reuse has a di erent impact on di erent abstract domains.
we included two very di erent analysis approaches in our experimental evaluation a symbolic and an explicit model checking approach.
our experiments are not based on one particular speci cation but on six di erent real world speci cations with all showing a considerable speedup.
it would be interesting to compare precision reuse to other approaches for regression veri cation.
our implementation does not analyze for syntactical di erences in order to be able to identify the speedup that is achievable by precision reuse in isolation such optimizations are orthogonal and could be used in combination with precision reuse as well .
.
conclusion precisions de ning the abstraction level of an abstract model are costly to compute and represent precious intermediate veri cation results.
we propose to treat these abstraction precisions as reusable veri cation facts because precisions are easy to extract from model checkers that automatically construct an abstract model of the program e.g.
cegar have a small memory footprint are toolindependent and are easy to use for regression veri cation.
the technical insight of our approach is that reusing precisions drastically reduces the number of re nements.
the e ort spent on analyzing spurious counterexamples and reexploring the abstract state space in search for a suitable abstract model is signi cantly reduced.
to con rm the e ectiveness and e ciency of our approach we derived an extensive collection of veri cation tasks from the linux kernel in order to enable a meaningful evaluation of regression veri cation techniques.
our benchmark set consists of single veri cation tasks and is publicly available for download on our supplementary web page.
based on these benchmarks we show that the reuse of precisions has a signi cant e ect on the veri cation process.
the approach drastically improves the performance on most veri cation problems and does not have noticeable negative side e ects.
besides improving the run time we are also able to solve veri cation problems that were not solvable before within the given time and memory limits.
because the information that we reuse does not depend on source code details our approach is less sensitive to changes in the source code compared to other approaches.
precision reuse is applicable to all veri cation approaches that are based on abstraction and automatically computing the precision of the abstract model including e.g.
cegarbased approaches .
both the e ciency and e ectiveness of such approaches can be increased by reusing the precision.
as a result of the experiments for this paper a previously unknown bug in the linux kernel was discovered and a x was submitted to the maintainers by the ldv team2.
.