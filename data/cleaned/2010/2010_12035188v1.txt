arxiv .5188v1 mar to appear in the proceedings of the international conferenc e on software engineering .semi automatically extracting faqs to improve accessibil ity of software development knowledge stefan hen technische universit at darmstadt darmstadt germany stefan.henss gmail.commartin monperrus university of lille inria lille france martin.monperrus univ lille1.frmira mezini technische universit at darmstadt darmstadt germany mezini st.informatik.tu darmstadt.de abstract frequently asked questions faqs are a popular way to document software development knowledge.
as creatin g such documents is expensive this paper presents an approac h for automatically extracting faqs from sources of software development discussion such as mailing lists and internet forums by combining techniques of text mining and natural language processing.
we apply the approach to popular maili ng lists and carry out a survey among software developers to sho w that it is able to extract high quality faqs that may be furth er improved by experts.
i. introduction the term frequently asked questions faq refers to a documentation format which lists questions as they are or might be asked by the target audience and the corresponding expert answers also known as q as .
in software development this format is used by many projects as part of their documentation for example linux1 apache lucene2and eclipse swt3.
even entire books on software development are written in the faq format .
with their typical characteristics faqs are complementary to other kinds of software documentation.
reference manuals are meant to be comprehensive and well structured but are not suited for providing independent pieces of knowledge targeting practical problems.
mailing lists and forums do provide such pieces of knowledge but suffer from the sheer mass of information which makes them difficult to use for novices.
even despite voting and tagging facilities that can be found in advanced social media sites like stackoverflow.com good q a pairs are still lost in myriads of messages and can not easily be found .
furthermore social media channels often cannot replace firsthand documentation especially for commercial software see for instance the faq on microsoft silverlight4 .
as a result there is a place for faqs to fill the gap between traditional documentation and web .
user support.
as with other kinds of software documentation creating high quality faqs is an intellectually challenging and time endeavor especially since it requires foreseei ng potential questions of the reader.
the question we raise in this paper is whether this endeavor can be facilitated by automatically generating parts or even whole faqs.
this relates to other software engineering research aiming at generating software documentation e.g.
but to our knowledge the idea of generating faqs is novel.
our key insight is that one could extract faqs from support channels such as mailing lists or q a communities by identifying popular reoccurring topics e.g.
compile r errors and finding representative q a pairs for them.
to do so we mine topic models using latent dirichlet allocation lda complemented with pre processing and post processing steps specifically designed for softwa re development discussions.
from those topic models we then assemble the final faqs by extracting question answer pairs thatcloselyrelatetotheminedtopicsandfurtherfulfillso me qualityheuristics.the wholeapproachis solely basedon th e discussions and does not require any metadata.
while the faq creation itself is automated the generated faqs are meant to be read by human users and endorsed by software product managers.
hence the generated q as may be validated and edited e.g.
reformulation and clarification before publishing.
to this extent our approach to generating faqs is semi automated.
to evaluate the approach we have generated faq candidates for major open source projects referenced at ohloh.netandinvitedtheirleadsoftwaredeveloperstoass ess them.
among other results this evaluation shows that there is a clear correlation between our relevance and qualityheuristics and the experts approval of our question selections and that of the answers selected by our system are correct.
furthermore we provide quantitative argumen ts on the system s ability to mine meaningful topic models from software discussions.
the results of those evaluation s confirm that faqs can automatically be distilled from raw knowledge available in software development discussions.
the rest of this paper is structured as follows section ii gives an overview of the approach.
section iiiexplains our datamodelandhowwe pre processtherawinputdata.from this data we then mine topic modelsusing lda as describedto appear in the proceedings of the international conferenc e on software engineering .insection iv.theresultingmodelsserveasreferenceswhen assembling the final faqs using the techniques presented in sectionv.
the results of our survey involving open source leadersispresentedinsection vi thequantitativeevaluation of the approach in section vii.
finally we discuss related work in section viiiand conclude in section ix.
ii.
overview our approach consists of three phases.
first conversation entries obtained from mailing lists forums q a communities and alike are prepared to be displayed in a faq.
raw conversations especiallyfromtheinternet generallyco ntain a significant amount of noise due to how they are presented e.g.
html markup to annotationswithin the content e.g.
reply information wrote and to social conte nt e.g.
best regards .
as we will show later this noi se also significantly impacts the performance of our algorithm especially topic mining.
hence to obtain meaningful well shaped results we define severalpre processingheuristic sto remove noise specific to software development discussions.
second for extracting focused faqs latent dirichlet allocation lda is used to identify clusters of related conversations which in the following will be called top ic models or just topics .
lda characterizestopicsthrough a bag of words their terminology and for each conversation lda estimates a probability distribution over the mined topics.
in our context each topic model depending on its granularity as determined by how many topics are to be mined either represents a general category e.g.
configur ation or a specific reoccurring issue e.g.
a bug .
finally in the third phase of the approach the topic models are used to obtain the final faqs.
we define means ofselectingrelevanttopics meansforselectingpreciseq uestions and good answers from the conversations associated with a topic as well as means of ordering the final q as.
note that even if the term frequent in faq suggests the contrary questions included in a faq do not necessarily havetobefrequentlyaskedtobevaluablefordocumentation purposes.
if a q a is well referenced by search engines the question may not be posed many times yet such a q a may be perfectly appropriate for a faq.
as a result the task of creating valuable faqs is to a lesser extent a matter of measuring question frequencies it is rather a matter of identifying good q a pairs that generally relate to the concrete problems that users face.
iii.
data and pre processing a. data model in order to generate faqs from a variety of sources like mailing lists internet forums newsgroups or q a communities ourapproachassumesaverybasicdatamodel.
in general a user starts a new conversation by presenting his issue the faq s question in a first message and all followingrepliestrytoansweritorcontributetoadiscuss ionfaq representation data mining version i think the only way to restore the link is to go into the xml and clean up that clause.restore link xml clean clause figure .
the effect of heuristic illustrated towards the solution.
we assume that the best reply the faq sanswer isasingleresponseofthewholeconversation we do not merge different contents to create an answer .
we pre process this raw data to create two different views the first one aims to be the text appearing in the final faqs it must have the look n feel of faqs the second one is used for topic mining and data analysis and hence aims at minimizing noise.
b. look n feel pre processing heuristics the following heuristics filter out most of the content that is irrelevant for faq readers.
heuristic regular expressions first we use a list5 of regular expressions to remove email headers irrelevant markup e.g.
html and other forms of formatting e.g.
reply quotations such as what is ?
.
also certain figures of regular speech such as greetings and expressions of gratitude are removed.
in the resulting version approx imately of all characters are filtered out by this heuristic .
heuristic determining frequent sentences second we empirically observed that sentences delimited by dot or multiple line breaks reoccurring at least times within all conversations are unlikely to contain question specifi c information e.g.
my problem is as follows .
those sentences unrelatedto a specific issue generallydecrease t he look n feel of resulting q as and also introduce noise to our algorithm.
by applying this heuristic we remove of sentences within all conversations.
c. mining related pre processing heuristics in order to maximize the approach sefficiency we further devised additional heuristics specific to improving the qua lity of the topics mined by lda and their post processing.
however these heuristics remove information which has to be kept in the final faqs as opposed to heuristics and e.g.
codefragments.hence twolinkedversionsofeac h conversation entry are stored one for the data analysis on e for the end user faqs as shown in figure .
heuristic removing stop words first we use a list of most frequent english words that are removed from the conversation as due to their generic use they carry no rele vantinformation alsoknownas stop words .thislist als o includes manually added terms which are frequently used in the analyzed domain but known to be not semantically significant for topic mining.
examples are java or class for discussions about java software their high frequency biases and hides the actual topics of the conversations.
appear in the proceedings of the international conferenc e on software engineering .software related conversations often contain large passages of source code snippets configuration files automati cally created debug output e.g.
stack traces and alike.
th is can as well lead to topics being heavily distorted by the high concentration of special terminology e.g.
java keywords that outnumbers semantically relevant terms.
for example we foundone user submitting largetechnical outputsinclud ing his user name in system paths.
this resulted in one topic beingfocusedonhisusernameandthereforeincludingmost ofthe conversationscontaininghisname despitethem bein g related to separated concerns or even different software.
heuristic removing long paragraphs inourdatabasis of software related conversations very few paragraphs of natural language are longer than characters while many paragraphs of generated text e.g.
debug output or source code are much longer.
filtering those paragraphs we are able to remove of all characters.
heuristic removing paragraphs with too much punctuation additionally we foundthat anyparagraphcontaining more than characters with at least of them being punctuation except dots is very likely to be source code or computer generated e.g.
stack traces which use to contai n many brackets and colons and we therefore filter them out as well.
of all paragraphsare removedby this heuristic.
please note that the size limit not only prevents false positives butalsopreservesshortoutputlikeerrormessa ges which rather support topic mining than introducing noise.
the impact of all heuristics is thoroughly evaluated in sectionvii d. iv.
topicmining we use topic mining to extract topics discussed in software development conversation e.g.
one topic can be related to compilation while another relates to userinterface customization .
topic mining enables us a to group conversations by common characteristics and b to obtain information about the terms that characterize the topics.
the approach is fully automatic i.e.
the faq topic s are not required to be sketched beforehand instead they are extracted from the corpus of conversations.
to mine topics we chose the latent dirichlet allocation model whic h has a mature open source java implementation .
in the following we use the generic term document to refer to entire conversations i.e.
all entries of one conversatio n are merged to a single textual element for lda to be able to analyze conversations as a whole.
a. latent dirichlet allocation lda characterizes each topic ias a probability distribution over words noted i .
for example conversations related to java will likely contain terms like object or class hence they will have a high probability in a related model.
figure 2shows an excerpt of this probability distribution for an actually mined topic related to spamassassin bag of words for topic apache spamassassin spamassassin .
spam .
mail .
rules .
dbg .
score .
email .
spf .
tests .
rule .
figure .
top words of a topic model representing apache sp amassassin.
the numbers indicate a relative importance the sum of all words is hence the word spamassassin is five times more importan t expressed through frequency than rule .
a spam filtering software package.
additionally since each document can refer to several topics e.g.
related to java xml and eclipse lda also defines a probability distribution over topics for each document j noted j .
both distributions the topic s specific language bag of words andthetopicsforeachdocument arenotknownbeforehand.
as mentioned before the number of topics to be mined is a parameter of the model and varies greatly between different data sets and applications.
for example one coul d estimate the amount of major topics and treat each topic model as the basis for a separate faq.
another possibility is to decide on a fixed number of questions to include in the faq to mine an according number of question models and to select the most representative q as for each.
since our evaluation shows that our approach is able to filter out unfocused models the exact number of actual topics is not required.
also as it usually only requires a few minutes to generate faqs several strategies can be tried.
the objectiveof lda is to approximateboth distributions by maximizing the overall probability of the model i.e.
by maximizing the following function.
kis the number of topics mis the number of documents njis the number of word instances also called tokens for document j wj t identifies the word for each instance tin a document j and zj ito which topic this word instance is assigned p k productdisplay i 1p i m productdisplay j 1p j nj productdisplay t 1p zj t j p wj t zj t latentdirichletallocationalso usesdirichletpriors and on and respectively which is out of the scope of this presentation p i andp j are assumed to be .
the approximation of the model parameters is generally done using a technique called gibbs sampling .
for our implementation of the approach we use mallet machine learning for language toolkit a java library developed by mccallum and colleagues at the university of massachusetts amherst .
it contains several text mining tools including an implementation of lda.
an important feature of the mallet implementation of lda is that it automatically adjusts model parameters dirichlet priors and during optimization to further improve the accuracy of the mined topics.to appear in the proceedings of the international conferenc e on software engineering .b.
resulting data as described lda extracts a set of topics each of which is a candidate to be transformed into a faq.
each lda topic model represents a common conversation topic that is frequently discussed in the mailing list.
each document is given a probability distribution over topics i.e.
for e very document lda outputs a relative likelihood percentage of relating to a mined topic.
for example one document might have a likelihood of .
to relate to topic 2and of .
to relate to topic the sum is always .
in this case the document mostly refers to topic 2and is a good candidate to be included in the faq representing this topic.
filter removing associations with low probabilities we associate a document jto a topic ionly if the likelihood j i is higher than a certain threshold.
we set this threshold to .
meaning that we consider only relationships between documents and topics when at least one fourth of the document content relates to the topic.
if a document is not associated to at least one topic j i .25for each i it is definitely discarded i.e.
it appears in no faq at all.
v. faq a ssembly the processof turningmined topic modelsto correspondingfaqs consistsofthreephases.thefirst phaseconstitute s a faq by selecting q a pairs from the conversations associated with a topic.
the second phase discards certain mined topics that seem unfocused.
finally the questions of the remaining faqs are ordered to maximize readability.
a. question and answer selection forselectingquestionsandanswerstoincludeinthefaq we compute a similarity metric between each entry in a conversation and the associated topic model.
the metric corelates the weighted bag of words tgiven by lda see sectioniv a with a word vector erepresenting the entry s normalized term frequencies.
we use cosine similarity for comparing the two vectors which is defined as follows cos t e te bardblt bardbl bardble bardbl summationtextn i 1tiei radicalbig summationtextn i ti radicalbig summationtextn i ei a high cosine value indicates that an entry is closely related to the topic model and thus a good candidate for being a question or answer in the final faq on this topic.
hence we will use it in both question and answer selection.
an entry in a conversation is selected as a good question if it satisfies the following three conditions a it is the fi rst one in the conversation b its cosine value with respect to the topic model the faq theme is relatively high we found .
to be a good threshold see section vii d and c it is short enough.
the rationales for the latter point are as follows.
on the one hand for popular software development faqs we analyzed questions in total the average question length is characters which is rathe rshort.
on the other hand we assume that long questions are either too specific complex scenarios often need many words to be explained or are imprecisely formulated.
we use a threshold of characters which encompasses most questions from existing faqs and gives very good results in practice for automated faq extraction see section vi .
selected questions may still not be part of the final faq if they are not clearly answered i.e.
if we cannot select an entry that meets the answer selection criteria.
in the standard faq format there is just one answer per question.
in internet conversations there is an arbitr ary number of replies of varying relevance and quality to each question.
so the best reply to each question has to be determined.
the key to estimating an answer s relevance is the language that is used.
an expert usually employs a significant amount of domain related terminology for example he might point to several software library feature s or describe a usage scenario in details.
in comparison othe r participating users might not have such understandingof th e domain and therefore use a less specific language.
as the domain sterminologyismodeledinthetopic sbagofwords to select the best answer we use again the cosine similarity between a reply and the related topic s bag of words.
the best reply is expected to be the one with the highest correlation in terminology with respect to the topic model which we measure through cosine similarity.
if the highest correlation is still low we discard the selected question since it misses a precise and thorough answer if the cosine similarity is lower than .
see section vii dfor an explanation on how we determine this .
b. topic selection in this phase faqs constructed from the mined topics are filtered if too few question answer pairs were selected inthepreviousphase.therationaleforthisisthatsometim es topic models have no clear focus which results in lowquality faqs.
for instance if a mailing list discusses thre e major topics e.g.
development maintenance and customer support but lda was set up to mine models one of the models contains all conversations that are off topic with respect to the major topics.
those conversations do not discuss a common topic there is no actual forth topic which induces a more vague and unfocused bag of words compared to the focused topics.
consequently many conversations won t yield question answer pairs surpassi ng our selection filters due to a lack of semantic correlation.
based onthis characterization we definethe followingfilte r. filter removing unfocused topics to removeunfocused topics as a whole we require a minimum threshold on the amount of selected questions q a compared to the number of initial conversations in the topic q a conversations threshold to appear in the proceedings of the international conferenc e on software engineering .c.
faq ordering finally inside an faq the q as themselves are ordered by their relevance to the faq theme as given by the harmonic mean between the question s and the answer s cosinesimilaritywiththe topic.asopposedtothearithmet ic mean the harmonicmean requiresboth question and answer relevance to be high to yield a high value i.e.
it ensures tha t for the top q a pairs both questions and answers are of high quality.
the resulting faqs are meant to be validated by experts before being published.
this means that some q as may be finally discarded but more importantly q as may be slightly reformulated or detailed to improve the quality of the final faq.
vi.
qualitative evaluation the goal of the proposed approach is to automatically create faqs that are meant to be published as official software documentation.
therefore we have constructed an evaluation setup that answers to the question are our generated faqs considered valuable by software project leaders?
.
for popular open source projects we invited the top committers to go through a guided evaluation interface where they were first asked to select relevant question s from an automatically created faq and then to evaluate the respective answers.
from the results we estimate the relevance and correctness of our mined topics automatical ly selected questions and automatically selected answers.
a. data basis and setup to construct a representative set of software development mailing lists we chose of the most popular projects found at ohloh.net an open source software directory.
specifically we chose those projects for which we could retrieve the corresponding mailing lists from the archives at markmail.com .
from these we excluded conversationspublishedbefore2009 a compromisebetween havingonly up to datequestionsand havingenoughdata for making significant observations.
in total our data contain s messages of conversations from lists.
for each selected project we also obtained a list of the most active committers from ohloh those who made most contributions to the source code repository to whom we sent a link to a website based review system for the project s generated faq.
in total invitations were sen t out nearly equally distributed among projects for some less than committerswere contactable .i.e.
all review ers were at least among the most active contributors to their respective project.
of the projectswere not includedi n the expert review because the associated topic models were automatically removed see section v b or in fewer cases no contact data could be found.
for generating the faqs to be reviewed we mined a single topic model per mailing list to get the most frequenttopic and we applied our faq assembly techniques to this topic.
in other terms this evaluation does not assess the importance of the number of topics but rather the impact of processing and faq assembly techniques to select good q as.
b. review process after a short explanation of our approach the reviewer is presented the highest scoring questions that our approac h extracted from the associated mailing list in terms of cosine similarity see section v a and is asked to select all questions he considers to be worth publishing.
since the expert is further asked to review one proposed answer for each selected question we advised him a questions maximum to prevent demotivation and breaking off the review process during the answer validation phase6.
at this point the answers that our system has chosen are not given in order to get the reviewer opinion on the questions only.
additionally the questions are not ordered by cosine similarity but by their length.
first shorter questions a re more motivating to start with.
second this length based ordering avoids a cosine based bias which would inhibit an evaluation of this crucial score.
once the reviewer has completed the question selection phase each selectedquestionis presentedon a separatepag e along with the answer selected by our approach.
there are three options available to the reviewer to validate the answer i.e.
accept the answer as is to modify the answer text and question text if needed and to discard the question because the presented answer does not match his expectations and cannot be corrected effortlessly.
eventually the reviewer can leave a comment and download a xml or html version of the whole faq he has just rolled out.
on faqcluster.com we present the validated faqs that expertsagreedto publish on ourown website.
for the evaluation we only considered those reviews that are completed seriously i.e.
reviews with at least question s selected and all corresponding answers evaluated.
c. survey results tableiprovides the most important results of this evaluation.
the first four rows give general information about the participation and how many projects got reviewed at least once.
the data shows that of all invited reviewers actually visited the review page.
since we did not give much informationaboutthesubjectin theemails we concludetha t they either not participate in surveys in general or the emai l was not read at all many committers finished contributing up to years ago so they may not further be interested in their projects or the email address may not be in use anymore .
when visiting the website however experts took the time to complete the whole review process.
6for questions a review takes minutes on about pag es.to appear in the proceedings of the international conferenc e on software engineering .table i qualitative evaluation results.
category amount relative selected open source projects review invitations sent website visited once reviews completed reviewed projects q a to be reviewed rev.
questions viewed .
rev.
questions selected .
rev.
questions modified mean cosine selected questions .
mean cosine not selected questions .
correct answers answers as is after modification answers discarded wrong answer imprecise answer other selected questions mean length lq selected answers mean length la mean levenshtein dist.
quest.
lq58 mean levenshtein dist.
answ.
la the next section of the table is concerned with the quality of the selected questions and answers how many of them were edited and if answers got discarded for whichreasons.tobeabletoalsocompareselectedquestions with questions intentionally not selected we split the questions into pages of questions and only consider the questions of those pages visited by the reviewer if he she clicked on the link to this page .
for an average of questions looked at were selected which indicates that most of the reviewers were able to find a sufficient amount of questions they surpassed the proposed limit of and at least of all questions selected by our approach are relevant for a faq.
of all questions then were discardedbecause the providedanswerwas insufficient in most cases either wrong or imprecise .
in other terms of answers were of sufficient quality for a faq at least after slight modifications .
finally compari ng selected questions to intentionally skipped ones also allo ws us to measure the correlation between cosine similarity and questions selection.
the average selected question has a cosinevalueof0.
allothersonly0.
i.e.
thehig her the cosine similarity the significantly lower the probabil ity of irrelevant questions .
confidence .
to sum up most reviewers were able to select more than a dozen questions some even significantly more and almost half of all questions selected by our approach were considered relevant.
the evaluation also shows that it isoften challenging to find an appropriate answer only of answers were correct or already sufficiently precise.
thi s shows that having a statistical surface understanding of textisnotprecise enoughinall cases.
thismayalsoindicat e that there is sometimes no complete solution given in one response e.g.
when the answer is distributed across sever al replies of the same discussion.
the last section of the table gives information about message length and editing amount.
for instance the selected questions have an average length of characters and the selected answers characters.
this confirms that expert tend to prefer short questions and that answers are generall y longer as observed in real life faqs.
when reformulated the selected questions have a mean levenshtein distance of characters i.e.
not much effort is required in making them publishable.
the answers validated marked as correct by experts yet reformulated were changed by an average amount of characters i.e.
of changes.
interestingly the experts reformulated the questions much more than the answers.
unfortunately there is no way to know whether this is because of the text length it is easier to reformulat e shorttexts becauseexpertscaremoreaboutquestionclar ity than answer clarity or because answers in mailing lists are generally of a higher standard than question and thus do not need much revision.
the personal comments received in the feedback text furthermore indicate that some open source leaders have a clearinterestinautomaticallygeneratingfaqs.forinsta nce a reviewer said that your project seems quite interesting and automatically generating a meaningful repository of information out of the mess that mailing lists are seems to me like a very interesting development in nl processing technology.
.
to sum up this evaluation shows that our approach is able to generate software development faqs which only require a short review before they can actually be published.
vii.
q uantitative evaluation when selecting q a pairs in section v we rely on topic models specified as bag of words to distinguish between good questions and answers and those which are irrelevant for faqs.
this second evaluation estimates the capabilitie s of lda to provide such models and the performance and importanceof ourpre andpost progressingstepsin improv ing them with respect to our requirements.
in particular we provide evidence showing that a lda can actually make sense of software related discussions using appropriate p reprocessing section vii c b the post processing of the lda s results presented significantly improvesthe quality of the generated faqs section vii b and c the approach s parameter settings are set and optimized in a systematic manner section vii d .to appear in the proceedings of the international conferenc e on software engineering .a.
evaluation setup the evaluation is set up to investigate lda s performance in constructing topic models from software development discussions and to see how those models can be improved by our filters.
to do so we first merge all mailing lists of our evaluation data see section vi a into one single data set leaving no explicit distinction between conversation s from different sources.
second we configure lda to create exactly topic models from the merged conversation set and expect to have each model correlating with exactly one mailing list.
finally we apply several metrics to measure the success in reconstructing the original mailing lists.
the metrics we use for measuring success are precision and recall.
for each mined topic we identify the mailing list from which most of the topic s associated conversation s originateandcallitthe main mailing list .
precisionmeasures thepercentageofthetopic sconversationsthatbelongtot hat main list i.e.
a precision of means that no conversations fromotherlists are included.recall measuresthepercenta ge of the list s conversations being included in the model i.e .
a recall of means that the entire mailing list is included in the topic model.
monitoring both values is important since for example a precision of could be achieved by having only conversation obviously this model would be useless .
we use the approximately same amount of conversations for each subject mailing list to avoid the effects of an unequal distribution of mailing list sizes on the results.
f or instance with a mailing list being too large in proportion ldacouldcreatemultipletopicsfromthislist whilesever al smaller lists would be merged to one model the amount of topics is fixed beforehand .
the smallest mailing list contains conversations from to the largest the median is hence we limit the amount of conversationsused inthe experimentto 600foreachmailing list.
b. evaluation of faq assembly tableiishows precision and recall for certain mined topics.
for each mined topic precision and recall are computed with respect to the closest corresponding mailing lis t given in brackets .
values are given for pp models with pre processing only and faqs with both pre and postprocessing faq assembly switched on.
the rows present best and worst topic models with respect to the highest harmonic mean between precision and recall after faq assembly also known as the f score .
for instance topic initially contains all conversation s of the nhibernate mailing list a recall of .
for this topic the faq question and answer filtering increase the precision to meaning that only conversations from nhibernate remain after filtering .
on the contrary topic correspond s to no mailing lists at all a recall of .
after lda buttable ii reconstructing mailing lists with lda.
t hefaq assembly techniques improve the precision and filter out unfocused topic .
topic model main list recall precision pp faq pp faq best topics nhibernate .
.
.
d bus .
.
.
.
mutt .
.
.
.
hudson .
.
.
.
log4j .
.
.
.
worst topics firebug .
x .
x hudson .
x .
x mediawiki .
x .
x firebug .
x .
x greasemonkey .
x .
x std deviation .
.
.
.
average .
.
.
.
median .
.
.
.
top terms for high score topic mutt mail .
mutt .
message .
messages .
list .
email .
folder .
set .
gmail .
hook .
send .
imap .
attachment .
subject .
mime .
top terms for low score topic code .
write .
dont .
make .
fe ature .
simple .
idea .
add .
api .
sol ution .
implement .
question .
part .
work .
documentation .
figure .
bags of words representing a focused topic mut t and an unfocused one .
the first one contains a very specific termi nology the second one only contains generic software development term s. is filtered out by our topic filter indicated by an x in the faq column .
when comparing the bags of words in figure it is obvious that topic models an email terminology perfectly correspondingto the email client mutt.
topic however does not seem to relate to a specific software package at all buttosoftwaredevelopmentingeneral itisratheracciden tal that firebug is the main mailing list only .
of all conversation originate from the firebug list .
this table shows two interesting points.
first all unfocused topics are filtered out by the post progressing techniques are filtered out including of the worst topic models .
second for the focused topics the precisio n is always increased after faq assembly the average precision jumps from .
to .
.
to sum up if a generated topic model represents one meaningful topic one can be confident that our faq assembly techniques improve the precision i.e.
the focus of extracted q as or otherwise filter them out.to appear in the proceedings of the international conferenc e on software engineering .table iii conformance between mined topics and corresponding mailing lists.
raw columns give results for ldamodels without pre processing pp columns give ldaresults after pre processing faqcolumns give results after all filtering phases .
pre processing improves the performance of ldain both precision and recall .
topic model main mailing list recall precision q as raw pp faq raw pp faq raw pp faq standard deviation .
.
.
.
.
.
average .
.
.
.
.
.
median .
.
.
.
.
.
c. the pre processing heuristics improve lda s models tableiiishows average and median precision recall and total number of q as for all mined topics.
the difference with table iiis that it also shows the precision and recall of using basic lda on raw data.
for instance as shown in the row labeled median lda on raw data has a median recall of .
and a median precision of .
.
lda on top of pre processed data column pp has a precision of .
and a recall of .
respectively.
compared to raw lda this is a significant increase in both precisionandrecall.
in the conversationcolumn onecan se e thatpre processingyieldsmoreconversationsrelatedtot opic models instead of which is due to having more focused topics models resulting in the topic containment probabilities jof a conversation smajor topics see section iv a being generally higher than the threshold defined in sectioniv b. this shows that our techniques to filter noise specific to software development such as stack traces and debug logs do increase lda s capability to grasp the actual semantics of software mailing lists.
the results of faq columns are identical to table ii but in this table they contribute to showing our overall goal to improve the precision of the approach.
in general we prefer having fewer q as of high quality rather than many q as of medium preciseness.
the column precision of table iii exactly shows this we are able to constantly increase the precision first with the pre processing then with the faq assembly whilestill retaininga reasonablenumberofq as at the end q as per mined topic in average to be compared with the input conversations .
d. importance of the approach parameters finally since our approach consists of many heuristics filters and thresholds we are interested in understanding to what extent they contribute to the performance of the system.
we use the same setup as in the previous sections butinsteadofusingonlytheoptimizedconfiguration werun the whole approach with a variety of parameter settings.
for each parameter we select the optimized configuration we obtained from an exploration of the parameter value space with respect to qualitative and quantitative evaluation e.g.
not too much filtering and then change the parameter value to observe the effects on the evaluation metrics.in table iv each line represents one of the approach s parameters being changed from the optimized settings.
the given evaluation metrics refer to the performanceof the sec ond evaluation task i.e.
the reconstruction of mailing li sts including the faq assembly phase.
as already seen turning off the pre processing yields a significant decrease of performance the results indicate that each heuristic contrib utes to a higher precision and recall.
however precision values are still better than for the lda only models from table iii column pp since our faq assembly techniques are able to dampen the negative effects.
note that disabling the lda model threshold also leads to a decrease in precision.
increasing it accordingly leads to a higher precision .
however it has a strong impact on the number of generated faqs then only out of faqs are kept instead of .
changing the cosine threshold on question and answer selection shows the correlation with our metrics the highe r the threshold the higher the precision and the lower the recall.
the same is evident for the faq selection where a higher threshold also means more precision since unfocused faqs are filtered out.
e. limitations of the quantitative evaluation in this section we have shown that our approach is able to recognize topics that are hidden in data.
we know those topicsbecausewe seed themartificiallyby mergingdifferen t mailing lists.
while this works on our particular dataset i t may be the case that those mailing lists are actually very much different with a clearly orthogonal terminology to describethem.
we hopethat replicationand extensionof our work will introduce new datasets to improve the confidence of the topic mining results.
for the sake of creating faqs there may be some topics thatcrosscutdifferentmailinglists.
withthesameevalua tion setup one could use tagged data instead of completely disjoint lists in order to evaluate the emergence of crosscutting topics using our approach.
finally it is clear to us that this automated evaluation does not assess whether the mined questions and their correspondinganswers are good with respect to relevance to the software documentation the clarity the style etc.
th ose quality attributes were evaluated by the user study present ed in section vi.
however in this work we assume that a good precision in reconstructing merged mailing lists is indire ctlyto appear in the proceedings of the international conferenc e on software engineering .table iv the impact of the approach parameters on the metric medians .the first line is the baseline corresponding to the optimized configuration .the other lines read as a comparison against it .
parameter differing from default setting default value new value faq recall faq precision q as optimized setting .
.
heuristic iii b on off .
.
heuristic iii b on off .
.
heuristic iii c on off .
.
heuristic iii c off .
.
heuristic iii c on off .
.
all heuristics iii b iii c on off .
.
lda model threshold iv b .
.
.
.
lda model threshold iv b .
.
.
.
q a cosine threshold v a .
off .
.
q a cosine threshold v a .
.
.
.
q a cosine threshold v a .
.
.
.
faq selection threshold v b .
off .
.
faq selection threshold v b .
.
.
.
faq selection threshold v b .
.
.
.
linked with good q as.
based on this assumption we have calibrated the various thresholds and parameters of th e approach in a fully automated manner before setting up the user study.
the fact that the ratio of correct questions and answers as assessed by experts is high gives support to our core assumption.
viii.
r elatedwork question answering systems have a long tradition in artificial intelligence research.
certain systems are desi gned to find the most appropriate answer in a database of question answer pairs e.g.
.
others use the world wide web as reservoir of answers e.g.
.
certain authors only focus on finding similar questions to those asked by users e.g.
.
compared to this previous research we do not work with a structured set of q as.
in our input data we know neither the questions nor the corresponding answers.
jeon et al.
surdeanu et al.
and others use a variety of approaches for learning to detect questions or correct answers from mailing lists and forums.
their approaches differ from ours as following first they do not try to group q as by topics as we do second they do not address the details specific to software discussions.
th e lexical chasm refers to the fact that an answer contains a terminologythat is missing the question.
interestingly w hen we measure the similarity of the question and the answer to the topic s bag of words directly we assume that the topic contains the terminology of both the question and the answer.
the selected q as with high cosine similarity are veryclosetoit.inotherterms comparedtothisrelatedwor k that finds questions and correct answers to a certainextent we find q as that are also clear and precise if the topic s bag of word is focused and relevant .
gottipati et al.
recently presented an approach to finding answers of software development questions in a manner that is similar to search engines.
first their syste m performs a classification of software forum posts into categories questions answers question answerclarific ation question answer feedback and junk.
second it offers a standard search engine query interface which uses the categorization to improve the search performance and to only display answers as results.
our approach is similar with respect to identifyingrelevant answers.
while they constr uct a search engine we try to infer relevant and precise topics and questions from data which is not in their scope at all.
celikyilmaz et al.
and liu et al.
also use lda in the context of question answering systems.
celikyilmaz et al.
use lda on possible answers to a given question.
on the contrary we use lda on the whole mailing list to identify the frequent topics and the bag of words representing them.
while it makes sense to build an lda model on the web it does not make sense to build a lda model on a small dataset such as the replies to an initial question usuallylessthana dozenreplies .liuet al.
useldato predict the best potential answerers to a new question thei r lda models capture different topics an expert is interested in.
in the domain of software engineering hindle et al.
cluster commit logcommentsand otherscluster source code all with lda.
on the contrary our lda model captures topics of mailing lists.
the concept of applying data mining techniques to faqs has been explored by ng ambi .
his system guesses the next question that the user would ask pre empting user questions .
we use data mining in a different context.to appear in the proceedings of the international conferenc e on software engineering .different authors have explored different ways of automatically inferring software documentation.
the approach es vary in terms of both input information used to generate documentation and the kind of generated documentation artifacts.
buse et al.
analyze method bodies to infer api documentation related to exceptions.
long et al.
also use source code to infer relevant links between api elements to improve developer s awareness.
bruch et al.
extract subclassing directives also from source code to assist developers in correctly extending object orient ed frameworks.
sridhara uses method signatures to infer meaningful natural language sentences to be included in a method s summary.
in contrast our approach infers faqs which is another frequently used kind of software documentation.
furthermore it uses mailing lists and forums as inpu t and not source code.
mailing lists have been used to infer social networks and their relationswith software development.the approac h uses structured email headers e.g.
the from and to headers .
on the contrary we use the content of mailing lists and forums the latter contain a large amount of noise not present in email headers.
ix.
conclusion in this paper we presented an approach to semiautomatically extract faqs from software development forums and mailing lists.
the three main components of the approach are a applying several pre processing heurist ics to remove noise from the raw data and to prepare it for further steps and the final display b using latent dirichlet allocation lda to automatically extract an arbitrary amount of topic models from the pre processed data each of which may serve as the basis for a topic specific faq c using the model information for selecting and ordering most relevant questions selecting the most relevant answe r and estimating the faq quality.
we conducted both a qualitative and quantitative evaluations.
we asked top contributors to major open source projects to review generated faqs corresponding to their project.
the results confirm that the quality of the generate d faqs is promising almost half of all questionswe proposed were considered relevant and of the automatically selected answers were correct.
for the quantitative evaluation we extracted faqs from a set of merged mailing lists to prove our ability to identify the most important topics.
we observed a strong correlation between the hidden structure the mailinglists and the mined topics the faqs .
furthermore we were able to show that our filtering techniquesspecific to software developmentand faq generation could significantly improve the performanceof topic mining compared to basic lda.
the replication data can be found on .
the work presented in this paper lies in a wider area of research dedicatedto improvingthe accessibility of softw aredevelopment data and knowledge.
since we are the first to generate this kind of documentation we are convinced that there is much room for creativity and optimization in order to improvethequalityand thecompletenessofthe generated faqs.
from a wider perspective an important future work consists of checking if not inferring the semantic links b etweendifferentformsofdocumentation apidocumentation faqs tutorials etc.
acknowledgement this work was supportedby the german federal ministry of education and research bmbf within ec spride.