see discussions st ats and author pr ofiles f or this public ation at .researchgate.ne t public ation a cost effectiveness criterion for applying software defect prediction models conf erence paper august .
.
citations 21reads author s hong yu zhang univ ersity of ne wcastle a ustr alia publica tions citations see profile shing chi cheung hong k ong univ ersity of scienc e and t echnolog y publica tions citations see profile all c ontent f ollo wing this p age was uplo aded b y hong yu zhang on june .
the user has r equest ed enhanc ement of the do wnlo aded file.a cost effectiveness criterion for applying software defect prediction models hongyu zhang1 and s.c. cheung3 1tsinghua university 2state key laboratory of computer science institute of software chinese academy of sciences beijing china hongyu tsinghua.edu.cn 3the hong kong university of science and technology hong kong china scc cse.ust.hk abstract ideally software defect prediction models should help organize software quality assu rance sqa resources and reduce cost of finding defects by allowing the modules most likely to contain defects to be inspected first.
in this paper we study the cost effectiveness of applying de fect prediction models in sqa and propose a basic cost effectiveness criterion.
the criterion implies that defect prediction models should be applied with caution.
we also propose a new metric fn fn tn to measure the cost effectiveness of a defect prediction model.
categories and subject descriptors d. .
testing and debugging debugging aids.
general terms measurement reliability keywords defect prediction cost effectiveness evaluation metrics.
.
introduction software quality assura nce sqa is important for the success of a software project.
however it is a resource and time consuming activity which may include manual code inspections technical review meetings and intensive soft ware testing.
modern large and complex systems often consist of hundreds or even thousands of modules methods files components etc .
it is thus desirable to predict which modules are more like ly to contain defects so that project managers can allocate limited sqa resources in a cost effective manner.
software defect prediction has recently attracted immense interest from the software engineering community.
many defect prediction models have been proposed e.g.
.
these models collect historical defects of software m odules as well as various module features such as program complexity structural dependency changes process and organizational factors.
a classifier such as decision tree or na ve bayes is utilized to train a classification model which can then be used to predict the defect proneness of a new module.
to evaluate the accuracy of predictions researchers have adopted many metrics such as recall and precision.
despite recent advances in defect prediction models these models are not able to detect all de fective modules and can identify correct modules as defective.
although the cost of building a defect prediction is usually small as most features can be automatically extracted by mining software repositories we still need to deploy models for defect prediction cautiously.
for example one may be hesitated to deploy defect prediction models to safety critical systems if th e models cannot guarantee a low degree of false negatives.
it is because the deployment cost is not restricted to the effort of insp ecting the reported defective modules.
it also includes the cost of failures arising from the defects that are missed by the prediction models.
therefore studying the cost effectiveness of defect prediction models is important for the practical application of these m odels.
current studies on defect prediction mostly focus on the improvement of data quality the identification of new featur es the design of new classification techniques and th e selection of training projects .
the cost effectiveness of a pplying defect pr ediction models in sqa practices is not well explored.
in this paper we study the cost effectiveness of applying defect prediction models.
we propose a crit erion that is derived from the intuitions that a defect predic tion based sqa strategy should at least cost less than the strategy of inspecting all modules and the strategy of inspecting random ly sampled modules.
we also propose a new metric to measure th e cost effectiveness of defect prediction models.
we use a case study on eclipse project to illustrate the usage of the proposed metric.
.
evaluation metrics for defect prediction models prediction of defective modules can be cast as a classification problem in machine learning give n training samples of modules with labels as defective containing at least one defect or non defective no defects .
a prediction model has four results true positives tp false positives fp true negatives tn and false negatives fn as shown in table .
the total number of actual defective modules is deno ted as pos and the total number of actual non def ective modules is denoted as neg.
recall and precision are often adopted to evaluate defect predication models e.g.
.
these metrics are the accuracy measures widely used in information retrieval area.
they are defined as follows permission to make digital or hard copi es of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
esec fse august saint petersburg russia.
copyright acm ... .
.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse august saint petersburg russia copyright acm ... .
643fn tptpcall re fp tptpecision pr recall defines the rate of true de fective modules in comparison to the total number of defective m odules and precision relates the number of true defective module s to the number of modules predicted as defective.
the values of recall and precision are between and the higher the better.
table .
defect prediction results predicted defective predicted non defective actual defective tp fn pos actual non defective fp tn neg to evaluate the accuracy of a prediction many researchers e.g.
also use the probability of detection pd and probability of false alarm pf metrics which are defined as follows.
fn tptppd tn fpfppf the pd metric is the same as recall.
the pf metric measures how many defect free modules are wrongly classified as defective.
the values of pf are between and the lower the better.
.
analyzing the cost of applying software defect prediction .
cost of defect prediction strategy ideally a defect prediction mode l should be able to correctly identify all defective software modules.
limited qa resources can then be allocated to these modules to make sqa more effective.
however there are two costs incurred by this strategy cost of inspection for the de fective modules selected by defect prediction models.
a defect prediction model returns tp fp defective modules.
assuming the average cost required for inspecting a module is ci the cost of inspecting all predicted defective modules is ci tp fp .
in this study to simplify the analysis we as sume that the inspection is perfect i.e.
after inspecting a module the defects can be successfully revealed.
the value of ci is project specific.
for example ci for a complex project is usually higher than a simple project.
also ci for a project closer to its delivery deadline is usually higher than that for a project at its initial phase.
cost of failures arising from false negative modules.
a defect prediction model often fails to identify all defective modules.
we denote the total cost incurred by false negative modules cfn fn where cfn is the average cost of missing a defective module.
the value of cfn is project specific which could be high for some pr ojects .
for example cfn for a safety critical project is usually higher than that for a web forum project.
in summary the cost of inspecting modules selected by a defect prediction model cp is defined as follows cp ci tp fp c fn fn as an example table shows th e results produced by a defect prediction model.
the precision is .
and the recall is .
.
if applied to sqa the cost cp is 29c i 10c fn.
table .
the accuracy of a defect prediction model predicted defective predicted non defective actual defective actual non defective .
cost of inspecting all strategy a defect prediction model allows the developers to concentrate on the defect prone modules.
the other sqa strategy is to simply inspect all modules without prioritization.
we define the cost of such strategy call as follows call c i n ci tp fp fn tn clearly a cost effective defect pr ediction model should satisfy the following requirement cp c all ci tp fp c fn fn c i tp fp fn tn fn fn tn c i cfn considering the example shown in table the cost call is ci.
fn fn tn is .
.
assuming ci cfn is then fn fn tn c i cfn failing to satisfy the requirement defined in equation .
applying such a def ect prediction model is not more cost effective than the simple stra tegy of inspecting all modules.
.
cost of random sampling strategy another simple sqa strategy is to randomly sample a subset of modules and test them.
a defect prediction model should be able to correctly identify defective m odules.
it should perform better than randomly selecting and in specting the same number of modules.
we define the cost of the random inspecting strategy cran as follows crnd ci tp fp c fn d n tp fp c i tp fp cfn d tn fn where ci tp fp denotes the cost of randomly inspecting the same number of modules as a defect prediction model suggests.
d denotes the density of de fective modules i.e.
the percentage of defective modules in the project .
for a new project the value of d could be estimated based on the experience of similar projects.
the item cfn d tn fn denotes the cost of missing the defective modules.
cl early a cost effective defect prediction should satisfy th e following requirement cp c rnd ci tp fp c fn fn c i tp fp cfn d tn fn fn fn tn d consider the example shown in table d is .
.
the cost crnd is 29c i .96c fn.
fn fn tn is .
which is larger than d. therefore applying such a defect prediction model is not more cost effective than the simple strategy of random sampling and inspection.
.
a criterion for cost effectiveness combining equations and a defect prediction model should at least satisfy the following basic criterion in order to be cost effective fn fn tn min c i cfn d considering the example shown in table the upper bound value of fn fn tn a cost effective defect prediction model should achieve is .
we also propose to use fn fn tn as a metric to measure the cost effectiveness of a defect prediction model complementing 644other metrics for measuring classifi cation accuracy e.g.
precision and recall .
the lower is the fn fn tn value the better is the cost effectiveness of the prediction model.
such a metric can be used for determining the benefit of applying a defect prediction model and for selecting among alternative prediction models.
.
a case study to illustrate the usefulness of the proposed metric for evaluating cost effectiveness of a defect pred iction model we re examine the defect prediction results of a st udy performed by zimmermann et al.
on eclipse .
eclipse is a widely used integrated development platform for creating java c and web applications.
it has been used as an experimental subject by many studies on software defect prediction e.g.
.
note that although our case study uses the eclipse experiment described in the proposed metric is a general metr ic that is independent of the methods and data used for c onstructing prediction models.
in this study we focus on the eclip se .
defect data v2.
at the package level.
the data was collected by mining eclipse s bug databases and version achieves.
there are packages in eclipse .
of them contain post rele ase defects defects reported in the first six months after release .
the authors of build several defect prediction models based on the data collect from eclipse .
and eclipse .
to predict defect prone packages packages contain at least one defect in eclipse .
.
table summarizes their prediction results.
.
evaluation with recall and precision metrics in table for the two prediction models the precision values are .
and .
the recall values are .
and .
respectively.
these results may or may not be satisfactory for some application scenarios.
to evaluate the cost effectiveness of the results we apply the proposed metric as defined in equation .
assume that the c i cfn value is the fn fn tn values are .
and .
for the two prediction models respectively.
these values are all above the ci cfn value.
therefore both models are unsatisfactory when cost effectiveness is considered.
the results suggest that cautions should be taken when these prediction models are applied in practice.
table .
defect prediction results for eclipse .
packages train test precision recall d fn fn tn eclipse .
eclipse .
.
.
.
eclipse .
eclipse .
.
.
.
figure illustrates how the value of fn fn tn changes with different precision and recall values for the eclipse example.
from figure we can see that in order to satisfy the criterion that the value of fn fn tn should be lower than the precision values should be higher than .
if recall is .
higher than .
if recall is .
.
the current precision values are not good enough in terms of cost effect iveness.
figure also shows that if the ci cfn value is given the same recall values the precision values shown in table satisfy the cost effective criterion.
figure illustrates how the values of fn fn tn vary with different d precision p and recall r values.
given the criterion that the value of fn fn tn should be lower than both prediction models given in table are not cost effective when d is .
however if d is less than these two models satisfy the cost effective criterion.
suppose that the ci cfn value becomes the two models satisfy the cost effective criterion only if d is less than .
figure .
fn fn tn under different recall and precision values figure .
fn fn tn under different d values .
evaluation with pd and pf metrics in table the recall and precision metrics are used to measure the classification accuracy of prediction models.
we also examine how the proposed metric works with the pd probability of detection and pf probability of false alarm metrics.
the original paper does not give the evaluation results measured in terms of pd and pf.
however we can derive these values from table directly.
in our prior work we discovered the relationship between the precision and pd pf as follows pd pospf neg tpfp fp tptpecision 11pr clearly the neg pos item in the above equation can be calculated as neg pos d d based on equations and we transform the recall precision values in table to pd and pf values in table .
to evaluate whether the pd and pf values shown in table are good enough we plot a figure figure that illustrates how the value of fn fn tn changes with different pd and pf values for the same eclipse example.
from figure we can see that in order to satisfy the criterion that the value of fn fn tn should be lower than the pf values should be less than .
if pd is .
less than .
if pd is .
.
the current pf values are .
and .
.
.
.
.
.
.
.
.
.
.
.9precision recallfn fn tn .
fn fn tn .50fn fn tn .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.75fn fn tn dp .
r .
p .
r .
ci cf d .
which do not satisfy the co st effectiveness criteria.
figure also shows that if the ci cfn value is given the same pd values the pf values shown in table satisfy the cost effective criterion.
table .
defect prediction results in terms of pd and pf train test pd pf eclipse .
eclipse .
.
.
eclipse .
eclipse .
.
.
figure .
fn fn tn under different pd and pf values .
discussions in this section we answer the que stions specified in the call for paper1 what is the new idea?
we propose a new criterion and metric for evaluating the cost effectiveness of a defect prediction model.
why is it new?
currently defect prediction models are evaluated using conventional accuracy metrics such as precision recall pd and pf.
there is a lack of explicit criterion and metric that can evaluate a prediction model from the cost effectiveness perspective.
what is the single most related paper by the same author s ?
by others?
in our prior work we studied the metrics used for evaluating defect prediction models.
we explore the relationship between the recall precision and pd pf metrics.
our finding reve als a fundamental limit on the ability to improve defect predictors for domains where the defective modules are relatively infrequent.
in this work we further explore the cost effectiveness aspect of the evaluation which is more significant when the defective modules are relatively frequent.
recently other researchers also explored effort aware defect prediction models.
for exampl e mende and koschke discovered that when effort is considered many classifiers perform not significantly better than a random selection of modules.
they considered th e ordering of modules and measure effort using a ce metr ic proposed by arisholm et al.
.
the ce metric is defined in a cumulative lift chart and plots the percentage of identified faults against the examined lines of code.
posnett et al.
also proposed a revised ce metric called aucce.
unlike their work our criterion and metric are based on defect prediction results alone and do not consider ordering and size of defective modules.
what feedback do the authors expect from the forum?
we would like to exchange ideas with other researchers and explore the applicability of th e proposed metric in evaluating defect prediction models.
.
conclusion in this paper we study the cost effectiveness of applying defect prediction models in sqa.
we propose a basic cost effectiveness criterion equation derived from the intuition that a defect prediction based sqa strategy should at least cost less than the strategy of inspecting all modules and the strategy of inspecting randomly sampled modules.
the cr iterion implies that defect prediction models should be app lied with cautions.
besides the conventional metrics such as recal l and precision for measuring the accuracy of a defect pred iction model we propose a new metric fn fn tn to measure the cost effectiveness of the prediction model.
we believe our work can help better understand and apply defect prediction research results in practice.