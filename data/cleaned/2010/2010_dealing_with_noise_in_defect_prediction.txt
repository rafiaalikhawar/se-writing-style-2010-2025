dealing with noise in defect prediction sunghun kim1 hongyu zhang2 rongxin wu2 and liang gong2 department of computer science and engineering hong kong university of science and technology hong kong school of software tsinghua universit y beijing china hunkim cse.ust.hk hongyu tsinghua.edu.cn se.wu.rongxin jacksongl1988 gmail.com abstract many software defect prediction models have been built using historical defect data obtained by mining software repositories msr .
recent stu dies have discovered that data so collected contain noise s because current defect collection practice s are based on optional bug fix keywords or bug report links in change logs.
automatically collected defect data based on the change log s could include noi ses.
this paper proposes approaches to deal with the nois e in defect data.
first we measure the impact of noise on defect prediction models and provide guidelines f or acceptable noise level.
we measure noise resistant ability of two well known defect prediction algorithms and find that in general for large defect datasets adding fp false positive or fn false negative noise s alone does not lead to substantial performance differences.
however the prediction performance decreases signific antly when th e dataset contains of both fp and fn noises .
second we propose a noise detecti on and eliminati on algorithm to address this problem .
our empirical study shows that our algorithm can identify noisy instances with reasonable accuracy .
in addition after eliminating the noise s using our algorithm defect prediction accuracy is improved .
categories and subject descriptors d. .
distribution maintenance and enhancement restructuring reverse engineering and reengineering d. .
metrics product metric s k. .
management of computing and information systems software management software maintenance general terms algorithms measurement experimentation keywords defect prediction noise resistance bug gy changes buggy files data quality.
.
introduction defect prediction is a very active area in software engineering research .
many effective new metrics and algorithms to predict defect proneness have been proposed.
when re searchers evaluate their new algorithms or metrics they often use defect information collected from the change logs in software configuration management scm systems and from the bug reports in bug tracking systems.
unfortunately recent studies have found that extracted defect information from change logs and bug reports are noisy.
for example aranda and venolia et al.
manually inspected ten bug reports in microsoft and interviewed developers related to the reports.
they found lots of important information missi ng in bug reports.
bird et al.
also studied the quality of change logs and bug reports and found that many change logs and bug reports were not linked .
they also found that the noisy defect information could seriously affect the performan ce of a bug prediction algorithm.
these surprising findings challenge the validity of all existing bug prediction algorithms by r aising important questions how could we deal with the noise in the defect data?
are existin g defect prediction algorithms stil l useful if their prediction models are trained by noisy defect data?
how much noise is acceptable for bug prediction algorithms?
how could we detect and eliminate the noise?
this paper addresses these questions.
first we propo se a method which intention ally adds false positive and negative information only in the training data to measure noise resistan ce of a given bug prediction algorithm.
using the proposed method we measure noise resistan ce of two well known bug prediction algorithms change classification and buggy file prediction .
we found that these two algorithms are relatively nois e resistant.
when there are enough buggy instances in the dataset s defect prediction performance measured in terms of f measure does not decrease significantly with the increases of false positive or false negative noises.
we also find that these algorithms are more resistant to false negative noise s. however the prediction performance decreases significantly when the dataset contains of both fp and fn noises .
second we propose an algorithm to detect and eliminate noises in the defect data to address the noisy data problem .
we experimental ly evaluate our algorithm and t he results show that it can identify noisy instances with reasonable accuracy.
in addition after eliminating the noise s using our algorithm the defect prediction accuracy is improved .
overall this paper makes the following contributions noise resistance measuring technique we propose a method to measure noise resistan ce of defect prediction models .
empirical study of measuring noise resistan ce we apply the resistan ce measuring method for two well known prediction algorithms and provide guidelines for acceptable noise level.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies ar e not made or distributed for profit or commercial advantage and that copies bear this notice and the full ci tation on the first page.
to copy otherwise or republish to post on servers or to redistribute to list s requires prior specific permission and or a fee.
icse may waikiki honolulu hi usa.
copyright acm ... .
.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
icse may waikiki honolulu hi usa copyright acm ... .
noise detection technique we propose an accurate noise detection algorithm which also improves defect prediction accuracy.
in the remainder of the paper we start by presenting the background on defect prediction algorithms in section .
in section we discuss the noisy defect data issue.
we propose a noise resis tance measuring method in section and apply it for change classification and buggy file prediction in section .
we present our noise detection algorithm in section .
section discusses the threats to validity .
we r ound off the paper with related work in section and conclusions in section .
.
background .
a general defect prediction process before measuring noise resistance of defect prediction algorithms we describe a common defect prediction process as shown in figure .
then we introduce two well known defect prediction algorithms used in this paper.
instances true training instancesmachine learnerinstance prediction true or falseinstances false features featuressoftware archives labeling true or false feature extraction creating a training corpus building a prediction model prediction evaluation figure .
a general defect prediction process before design ing a prediction model we need to specify the predict ion target.
a prediction model can be used for predicting defect proneness buggy or clean of different software entities such as a component file or a change .
after deciding the prediction target a general defect prediction process figure can be as follow s labeling defect data need to be collected for training a prediction model .
this process typically involves extract ing instances data items from software archives and labeling them as true buggy or false clean .
however s ome recent studies have discovered that data collect ed by mining software repositor ies often contain noise.
n oisy data threat en the validity of prediction models.
we will describe this issue in section .
extracting features and creating training corpus this step extracts features for predict ing the labels of instances.
common features for defect prediction are complexity metrics keywords changes and structural dependenc ies.
by combining labels and features of instances we can create a training corpus to be used by a machine learner to construct a predi ction model .
building prediction models using a training corpus general machine learners such as support vector machines svm or bayes net can be used to build a prediction model.
the model can then take a new instance and predict its label i.e.
true or false.
evaluation to evaluate a prediction model we need a testing data set besides a training set.
we predict the labels of insta nces in the testing set and evaluate the prediction model by comparing th e prediction and real labels.
to separate the t raining and testing sets fold cross validation is widely used.
testing set training set figure .
fold cross validation in fold cross validation the data is divided equally into folds as shown in figure .
then the instances in each fold are in turn used as a testing set and the remaining nine folds are used to train the model.
for example in the first iteration instances in fold2 to fold10 are used as a training set and fold1 as a testing set.
there are four possible outcomes from a prediction model classifying a buggy instance as buggy nb b classifying a buggy instance as clean nb c classifying a clean instance as clean nc c and cla ssifying a clean instance as buggy nc b .
the recall precision and f measures are widely used to evaluate prediction results .
we use these measures to evaluate prediction models as follows precision bug gy nb b nb b nc b this is t he number of correct classifications of the type nb b over the total number of predicted buggy instances .
recall buggy nb b nb b nb c this is t he number of correct classifications of the type nb b over the total number of actual buggy instances .
f measure buggy p b r b p b r b this is a composite measure of precision and recall.
we use the f1 metric that weight s recall and precision equally .
.
software defect prediction algorithms in this section we describe two well known defect prediction models used to measure noise resistance .
.
.
predicting buggy change s change classification cc learns buggy change patterns from history and predicts if a new change introduces bugs or not .
file at rev 3file at rev 1file at rev 2file at rev 4file at rev n 1file at rev nfile at rev n ......change log rev n fixed issue bug introducing change fix change ?
figure .
change history with buggy and fix changes suppose we have a change history of a file as shown in figure .
after learning from buggy and clean change patterns f rom revision to revisio n n cc predicts if the change in revision n introduces bugs.
to learn from history cc extracts features from eac h change.
to label changes as buggy or clean changes first we need to extract changes from project history.
then we identify fix changes using change logs.
to extract change history we use kenyon a system that extracts source code change histories from scm systems such as cvs and subversion.
kenyon automatically checks out the source code of each revision and extracts change 482information such as the change log author change date source code and change deltas.
once a commit has been determined to contain a fix it is possible to trace backward in the revision history to determine when the fixed erroneous code is introduced in the syst em.
we define that as a bug introducing change.
the bug introducing change identification algorithms proposed by liwerski et al.
and kim et al.
are used.
a file change involves two source code revisions an old revision and a new revision and a change delta that records the added code added de lta and the deleted code deleted delta between the two revisions.
a file change has associated metadata including the change log author and commit date.
by mining change histories we can derive features such as co change counts to indicate how many files are changed together in a commit the number of authors of a file and the previous change count of a file.
every term in the source code change delta an d change log texts is used as features.
detailed feature extraction methods o f cc can be found in .
.
.
predicting buggy files another common defect prediction model is identifying b uggy files in advance.
it is widely believed that some inter nal properties of software e.g.
metrics have relationship s with external properties e.g.
defects .
in re cent years many defect prediction models based on software metrics have been proposed e.g.
.
these prediction models identify code features expressed as measurement data learn a classification model from historical defect data a nd use the constructed model to predict defect proneness of a new program module .
many code features can be extracted from software projects to predict defective files.
these features include complexity metri cs such as lines of code cyclomatic complexity number of classes etc.
process metrics such as the number of lines of code ch anges the number of file changes etc.
and resource metrics suc h as developer information etc .
all these metrics or a combination of these metrics can be used to buil d effective software defect prediction models.
.
noise s in defect data both prediction algorithm s described in section require labels buggy or clean to build and evaluate models.
in this section we discuss typical techniques to identify labels and the noise in the labels.
to label a file change as buggy or clean many resea rchers mine the bug database and version achieves for open source systems.
two approaches are widely used searching for keywords such as fixed or bug and searching for refere nces to bug reports like .
we use both techniques in our experiments .
chen et al.
studied open source change log quality .
they checked the correctness of each change log and found almost all logs were correc t. some open source projects have strong guidelines for writing their change logs.
for example of columba s change logs used in our experiment have a tag such as and .
usually eclipse developers lea ve relevant bug report ids in their change logs.
however s ome recent studies such as those reported by bird et al.
discovered that data collected via mining software repositories msr often contain noise.
they found that the number of linked bugs bugs whose change logs and bug reports are linked does not match the number of total fixed bugs the ratio could be even lower than suggesting a high percentage of false negatives in the defect dataset.
this is because developers often do not write specific keywords or leave links for fix revisions.
it is also possible that developers make mi stakes when they write keywords or links in the change logs.
for this reas on automatically collected defect data based on these keywords or links are inevitably noisy.
recent studies have also found that noisy data in training and testing sets affect performance of prediction models .
we also performed a replication study of bird et al.
s experiments.
our results confirm their findings about noisy defect da ta.
for example for the eclipse swt component there are unlinked bugs bugs that do not reflected in cvs logs in eclipse .
and unlinked bugs in eclipse .
.
the existence of the unlinked bugs indicates that the defect data collected via msr is noisy.
we also notice d that the noise level decreased in eclipse .
where .
swt bugs are recorded in cvs logs.
in this paper we measure the effect of noise on two defect prediction models described in section and propose an algorithm to detect the noise in section .
.
experimental setup .
research questions our experiments are designed to address the following research questions rq1 how resistant a defect prediction model is to false negative fn buggy data?
rq2 how resistant a defect predicti on model is to false positive fp buggy data?
rq3 how resistant a defect prediction model is to both false negative fn and false positive fp buggy data?
as bird et al.
found out developers often forget to leave explicit messages or links to indicate buggy changes.
since most automatic buggy change file identification s are based on special keywords and links in the change logs this will lead to false negatives missing some buggy chang es in the automatically identified data.
rq1 measures predictor resistance for this case.
on the other hand it is possible that developers label a change file as buggy by leaving special keywords and bug report links together with some non bug fix changes in one commit .
this behavior leads to false positives identifying non buggy changes files as buggy .
rq2 measures resistance of defect prediction models to false positives in the training data set.
finally rq3 measures the noise resistan t ability of defect prediction models when data has both false positives and false negatives.
.
making noisy data to address the research questions we first need a golden set which contains no fps and fns .
in addition we need noisy data sets.
however it is very hard to get a golden set.
in our approach we carefully select high quality datasets and assume them the golden set s. then to create noise sets we add fps and fns intentionally into the golden sets.
to add fps and fns we 483randomly selects instances in a golden set and artifi cially change their label s from buggy to clean or from clean to buggy inspired by experiments in .
testing set original training setxxxx xxxxxxxxx x x buggy labelled instancebiased training set false negative instancesx xxxx adding buggy labels2removing buggy labels biased training set false positive instancesxxxx xxxxxxx xx x xxxx xxx figure .
creating biased training set to make fn data sets for rq1 we randomly select n buggy labeled instances and change their labels to clean as shown in figure .
similarly to make fp data sets for rq2 we select n of clean labeled instances and change their labels to buggy which adds false buggy changes as shown in figure .
for the fn and fp data sets for rq3 we select random n of instances and change their labels.
for example if a clean labeled instance is selected we change its label to buggy.
if a buggy instance is selected we change its label to clean.
it is very important to note that we add noise only in the traini ng set not in the testing set.
for testing we use the original golden set.
in this way we can measure the accuracy of a defect prediction model which is trained from noisy data sets to predict buggy clean changes in the golden set.
in this paper we use the fold cross validation described in section .
first we group folds to be used as a training set .
then we add noise only in the training set and leav e the testing set unchanged.
for the machine learner we use the bayes net classifier the weka implementation .
bayesian networks have good performance when dealing with a large number of variables with much variance in values .
we also compare performances of other machine learners in section .
.
.
.
dummy predictor an effective defect prediction model should outperfor m at least random guessing guessing a change file as buggy or clean purely at random.
we call a predictor based on random guessing a dummy predictor .
since there are only two labels buggy and clean changes the dummy predictor could also achieve certain prediction accuracy.
for example if there are buggy changes in a project by predicting all changes as buggy the buggy r ecall would be and the precision would be .
.
i t is also possible that the dummy predictor randomly predicts a change as buggy o r clean with .
probability.
in this case the buggy recall would be .
but still the precision is .
.
we use the f measure of the dummy predictor as a reference line when measuring the noise resistance of defect prediction models .
we compute the dummy f measure assuming the dummy predictor randomly predicts as buggy and as clean.
for example for a project with buggy changes the dummy buggy f measure is .
.
.
.
.
.
.
noise resistan ce this section reports our experiments on the impact of noise on two defect prediction algorithms and discusses the results.
.
noise resistance of change classification .
.
subject programs we use columba eclipse jdt.core and scarab as our subjects for this experiment as these projects have high quality change logs and links between changes logs and bug reports.
for the first two projects w e adopt the exact dataset s used in which were also used by other r esearchers .
we assume these datasets as golden set s and use them to measure noise resistance .
.
.
original accuracy first we build a cc prediction model using the orig inal training set and measure the performance of the model using a testing set.
figure shows the buggy recall prediction and f measure.
overall the accuracy results for the first two projects are comparable to those reported i n the small variations in results coming from the use of bayes net instead of svm and the randomness in the fold cross validation.
for columba the buggy precision and recall are around .
to .
.
for eclipse the buggy recall is .
and precision is .
.
we notice that the precision for eclipse reported in is .
which is higher than our precision .
.
however our recall is .
which is much higher than the recall .
reported in .
this happens due to the recall precision tradeoff.
to address this issue we use f measure to measure the noise resistance of cc in this paper.
vhhz 1sfdjtjpo vhhz 3fdbmm vhhz nfbtvsf 4dbsbc pmvncb dmjqtf figure .
defect prediction using the original training set table .
analyzed subject programs for predicting buggy changes project revisions period of clean instances of buggy instanc es of buggy instances of features columba .
eclipse .
scarab .
!
.
.
pmvncb dmjqtf 4dbsbc vnnz gps dmjqtf vnnz gps pmvncb vnnz gps 4dbsbc !
.
pmvncb dmjqtf 4dbsbc vnnz gps pmvncb vnnz gps dmjqtf vnnz gps 4dbsbc !
.
pmvncb dmjqtf 4dbsbc vnnz gps pmvncb vnnz gps dmjqtf vnnz gps 4dbsbc .
.
fn resistance rq1 in this section we meas ure the resistance of cc for false negative fn training sets.
to add fns we randomly select buggy instances in the training set and label them as clean as shown in figure .
in this way we increase the rate of fn by changing buggy labels to clean.
for example suppose we have buggy instances in a training set.
changing labels of buggy instanc es to clean will add fn.
figure a s hows buggy f measure results for columba eclipse and scarab with various fn training sets.
the x axis indicates the fn rates.
the dummy f measures described in section .
are also shown in the figures as the reference lines .
for columba the buggy f measure shows strong resistance against the fn training sets.
the f measure values are relatively stable.
when th e noises reach the f measure just drops .
.
the same can be observed for scarab the buggy f measure is not affected by fn noises significantly .
after false negatives are injected into the training set the f measure is not changed.
when the noi ses reach the f measure only drops less than .
.
for eclipse the buggy f measure is just slightly affected by the fn training sets too.
after adding fn noises to the training set the f measure drops from .
to .
.
when the noises reach the f measure still remains at .
.
a possible explanation of th ese results is that the features characterizing bugs are often common across the buggy changes.
therefore l osing some instances in the training set does not lead to significant performanc e decrease .
.
.
fp resistance rq2 we also observe cc f measures using fp training sets.
we add fps in to the training set s as described in figure and then perform change classifications.
the results are shown in figure b .
for columba and scarab the buggy f measures are not significantly affected by the false positives.
for eclipse buggy f measure s are affected by the fn training sets.
after adding fp noises to the training set the f measure drops from .
to .
.
after having more than fp noises the f measure is close to that of the dummy predictor.
a possible explanation of the sensitivity of the eclipse f measure s is the small number of buggy changes in the dataset .
there are only buggy changes as shown in table .
after adding many fps the features that characterize bugs become less obvious for classifiers to learn.
on the other hand columba and scarab all have more than buggy changes to learn from the features characteriz ing bugs can be still identified and prediction performance is still kept.
.
.
fn and fp resistance rq3 we also examine the prediction performance when the training sets contain both fp and fn noises.
as shown in figure c th e trend of buggy f measures for all projects decline when the noise rate increase s. for columba and scarab their f measures only decrease by .
.
when noise level reaches .
interestingly the f measure of eclipse decreases much faster than that of columba and scarab .
note that columba and scarab have many buggy clean instance s and switching some labels dose not hurt the prediction too much.
however the f measure of eclipse significantly drops when the level of fp and fn noise increases .
after the noises reach the eclipse s f measures are almost the same as the dummy f measures.
figure .
the impact of noises on predicting buggy changes a .
f measure for fn training sets b .
f measure for f p training sets c .
fmeasure for fn fp training sets .
the bayes net machine learner is used.
for columba and scarab the f measures are not affected by the noises significantly.
for eclipse the f measure drops significantly when the noise rate increases.
!
.
.
fcvh vnnz gps fcvh vnnz gps !
.
fcvh vnnz gps fcvh vnnz gps !
.
fcvh vnnz gps fcvh vnnz gps .
buggy file prediction .
.
subject programs to obtain the golden set for building prediction models for buggy files we use the swt and debug projects in eclipse .
.
we collected the defect data by mining the eclipse b ugzilla and cvs repos itories.
we find that both projects have a high percentage of linked bugs bugs whose changes logs and bug reports are linked .
f or swt .
bugs reported in bugzilla are linked to changes .
for debug .
bugs are linked.
therefore we use these two datasets as the golden sets .
table summarizes the datasets used in this study.
the swt dataset contains java source files among which .
files are defective.
the debug dataset contains files among which .
are defective .
we have als o collected the following metrics for each file in the projects.
these metrics capture different aspects of a file and are used as features for constructing our defect prediction model complexity metrics including loc lines of code average cyclomatic complexity measure maximum cyclomatic complexity measure.
object oriented metrics including the wmc cbo noc dit lcom rfc metrics that are proposed by chidamber and kemerer .
change metrics including the number of added and deleted lines of code since the last major revision the number of times the file is changed.
developer metric the number of developers who changed the file.
following the method described in section .
we intentionally make the dataset noisy by randomly select ing a given pe rcentage of instances and changing their class labels buggy or clean thus artificially creating false positives and false negatives.
we again use fold cross validation to evaluate the prediction results.
we first randomly partition the whole dataset into folds.
we use folds as a training set and inject noise into them and then use the remaining unchanged fold as the testing set.
the bayes net classifier is used to construct the prediction model.
table .
the dataset used for predicting buggy f iles project loc programs src files defects defective programs of linked bugs swt 386k .
.
debug 77k .
.
.
.
fn resistance rq1 figure a show s how fn false negative training sets affect prediction performance.
clearly the defect prediction model has strong resistance against the fn training sets.
for swt t he buggy f measure using the original dataset is .
.
with the increases of noises the prediction results are still very stable with f meas ures around .
even when the false negative rate reaches .
simil ar results are found for the debug project which exhibits stable performance until the fn rate reaches .
although some buggy instances are marked as clean the remaining buggy insta nces can capture the program features and can be still used for training prediction models effectively.
.
.
fp resistance rq figure b show s how fp false positive training sets affect prediction performance.
similarly the defect prediction model has figure .
the impact of noises on predicting buggy files a .
f measure for fn training set s b .
f measure for f p training set s c .
fmeasure for fn fp training set s. the bayes net machine learner is used.
the f measures of the debug and swt projects are not affected by the fn or fp noises significantly.
however when fn fp noises reach certain level the f measures drop significantly.
486resistance against the f p training sets.
the prediction results are very stable.
for swt the f measure values are all around .
even the fp rate is .
for debug the f measures are about .
until the fp rate reaches .
the data noise introduced by false positives does not decrease the prediction accuracy significantly.
.
.
fn and fp resistance rq3 a training set may contain both false positives and f alse negatives.
figure c show s how fn and fp noises affect prediction accuracy.
for swt once the fn and fp noise rate reaches prediction accuracy starts decreasing quickly.
for debug prediction accuracy drops after the fn and fp noise rate exceeds .
these results show that fn and fp noises together have larger impact on defect prediction.
.
discu ssions .
.
acceptable noise rate many bug prediction approaches use software history to build prediction models.
it is often very difficult to collect perfect historical dataset s that have no fps and fns.
how much noise is acceptable for prediction approaches?
our experiments show that cc and the buggy file prediction yield reasonably stable accuracy at the presence of noises when the bayes net learner is used.
when the number of buggy instances is large enough increasing fp or fn noises does not affect predi ction performance significantly.
for datasets with both fp and fn noises the prediction performance decreases when the noises increas e. when the number of buggy instances in a dataset is small the prediction performance will be affected by noises signifi cantly.
in defect prediction practices fns are more common as some defects recorded in bug tracking systems are not linked to cvs svn logs .
fps happen when developers leave a message saying he fixed a bug but he actually did not.
chen et al.
studied the correctness of open source change logs and they find that when developers leave a message indicating fixing of bugs it is likely a real fix .
our experimental results show that noises in fn or fp alone do not affect prediction performance si gnific antly.
also up to of fp and fn noise s together usually do not affect the performance significantly either .
obviously our results may not be generalizable to all prediction models but at least th ese can serve as guideline s for cc and the buggy file prediction users.
we suggest that b efore using th ese predictors users can sample their data and manually inspect them to measure fp and fn rates.
based on the rates they can d ecide if their defect data is applicable for these predictors.
.
.
noise resistances of different machine learners in previous sections we obtained our results using the bayes net machine learner.
in this section we use na ve bayes support vector machines svm and bagging learners t o repeat the e xperiments and observe the impact of data noises on prediction accuracy.
figure show s the noise resistance ability of the four machine learners under different false negative rates.
similar to bayes net the na ve bayes learner also has strong noise resistance ab ility when p redicting buggy files.
the f measures do not change significantly when fn rates are increasing.
all bayesian classifiers are based on the bayer s rule.
the classifier is interested in the most probable hypothesis.
therefore even if there is a certain amou nt of noise in the defect dataset which could affect calculation of probability for some hypothesis the bayesian classifiers can still make correct classifi cations when the most probable hypothesis is preserved.
the svm learner performs poorly with noisy data f measures decrease quickly when fns increase until fn rate reaches .
svm performs classification by constructing an n dimensional hyperplane that optimally separates the data into two categories.
the noise in the data could affect the construc tion of the hyperplane considerably.
therefore more noise could lead to more bias in classification.
the bagging bootstrap aggregating classifier is a machine learning algorithm that ensembles meta algorithm s to build models.
in th is experiment we use the multilayer perceptron algorithm as the meta algorithm .
the bagging classifier separates a training set into several new training sets by random sampling and builds models based on the new training sets.
the final classification result is obtained by the voting of each model.
figure shows that bagging can improve the original prediction performance and resist a certain amount of noises.
however when the noise level exceeds the probability of each model making a wrong classification is incre asing causing the quick drop of the performance.
!
.
.
bzft fu b wf bzft .
bhhjoh figure .
swt defect prediction results of different machine learners f measure s for fn training set .
.
handling noises in defect data this section proposes a noise detection algorithm and presents its evaluation.
.
identifying noisy instances we investigate possible method s for identifying noisy instances in defect datasets.
if we can detect no ises in advance it is possible to eliminate them and make the data more suitable for predictors.
we propose a novel noise detection algorithm called closest list noise identification clni .
the pseudo code of the algorithm is given in figure .
the clni algorithm works as follows.
in each iteratio n j for each instance inst i its closest instances are listed we c all it listi.
in listi the instances are sorted in ascending order according to their euclidean distance to insti.
the percentage of top n instances that have different class value s from insti is recorded as .
if is more 487than or equal to a given threshold then insti is highly probable to be a noisy instance and will be include d in noise set aj.
the above process is repeat ed until the similarity between aj and aj is over .
aj will be returned as the identified noise set.
empirical study found that when n is is .
and is .
this algorithm perform s the best .
clni algorithm for each iteration j for each instance inst i for each instance inst k if inst k aj continue else add euclideand istance inst i inst k to listi end end calculate percent age of top n instances in listi whose label is different from inst i as if a j a j inst i end end if aj aj max aj aj break end end return aj figure .
the pseudo code of the clni algorithm the high level idea of clni can be illustrated as in figure .
the bl ue points represent clean instances and the white points represent buggy instances.
when checking if an instance i is noisy clni first lists all instances that are close to i the points included in the circle .
clni then calculates the ratio of instances in the list that have a class label different from that of i the numb er of white points over the total number of points in the circle .
if the ratio reaches a specific threshold we consider instance i to have a high probability to be a nois y instance.
figure .
an illustration of the clni algorithm .
evaluation we evaluate clni using data from the eclipse .
swt and debug projects as described in se ction .
.
these two datasets are considered as the golden sets as most of their bugs are linked bugs.
following the method described in section .
we cre ate the noisy datasets for these two projects by s elect ing random n of instances and artificially changing their labels from buggy to clean and from clean to buggy .
we then apply the clni algorithm to detect noisy instances that we have just injected.
we use precision recall and f measures to evaluate the performance in identifying the noisy instance s. table shows the results when the noise rate is .
the precisions are above .
recall s are above .
and f measures are above .
.
these promising results confirm that the proposed clni algorithm is capable of identifying noisy insta nces.
table .the performance of clni in identifying noisy instances precision recall f measure debug .
.
.
swt .
.
.
figure also shows the performance of clni under di fferent noise levels for the swt component.
when the noise rate is below f measures increase with the increase of the noise rates.
when the noise rate is above clni will have bias toward incorrect instances causing f measures to decrease.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
noise rate precision recall f measure figure .
performance of clni with different noise rates after identifying the noises in the noisy eclipse .
swt and debug datasets using clni we eliminate these noises by flipping their labels.
we then evaluate if the noise removed training set improves prediction accuracy.
the results for the swt component before and af ter removing fn and fp noises are shown in table .
in general after removing the noises the prediction performance f measure im proves for all learners especially for those that do not have strong noise resistance ability.
for example for the svm learner wh en fn fp noises were injected into the swt dataset the f measure was .
.
after identifying and removing the noises the f measure jumped to .
.
these results confirm that th e proposed clni algorithm can improve defect prediction perform ance for noisy datasets.
table .the defect prediction performance f measure after identifying and removing noisy instances swt remove noises ?
noise rate bayes net na ve bayes svm bagging no .
.
.
.
.
.
.
.
.
.
.
.
yes .
.
.
.
.
.
.
.
.
.
.
.
i .
threats to validity we note some threats to the validity of this work.
all datasets used in our experiments are collected f rom open source projects.
the types of noises introduced by open source developers may be different from those introduced by employees in a well managed software organization.
we need to evaluate if commercial projects also exhibit similar noise resistance behavior in defect prediction.
this remains as future work.
the golden set used in this paper may not be perfect .
for example there are still a few percentages of bugs that are not linked to the cvs logs.
even though some files are annotated with bug ids they may not be the files that actually contain the bugs.
it is also possible that a few bugs may not even be recorded in the bug tracking system.
our results may be under threat if the golden sets contain a large number of fps and fns.
the noisy data simulations used in our experiment may not reflect the actual noise patterns in practice.
in our experiments instances to be included as fp fn training sets are randomly selected.
it is possible that in practice occurrences of some noises actually follow certain pat terns for example files developed by a poorly managed team are more likely to contain noisy defect data.
.
related work .
the data quality problem real world data are often noisy which may affect interpretations and models derived from the data.
the data quality problem is well recognized in the data mining area.
some studies show that errors in a large dataset are common and field error rates are typically around or more .
many existing learning algorithms have integrated various approaches to handle noises.
for example the well known decision tree algorithm uses tree pruning methods to avoid over fitting problems introduced by noises in training data .
zhu and wu descried a quantitative study of the impact of noisy data on classification accuracies using the uci machine learning datasets.
they found that although some machine learning algorithms have been designed to accommodate noises noises in class labels can still lower classification accuracies.
they also suggest preprocessing methods such as eliminating instances containing class noise to enhance classification accuracy.
the data quality problem has also been observed by s ome software engineering researchers.
for example mockus noted that in many realistic scenarios the da ta quality is low e.g.
some change data could be missing which could affect the outcome of an empirical study.
he proposed to use multiple imputation methods to mitigate the effects of missing values.
myrtveit et al.
and strike et al.
also no ticed the problem of missing and incomplete data in software effort estimation.
in this paper we address the problem of noisy data in software defect prediction.
.
the quality of software defect data research on software defect prediction has received much attention in recent years as the ability to predict defect proneness of a software module is important for software quality improvement and project management.
many defect prediction models have been proposed e.g.
.
however almost all defect prediction models do not take noise in the data into consideration.
as described in section many current defect predic tion models are built based on data collected by mining software repositories msr .
bird et al.
reported that th e data collected in this manner could introduce a large amount of noises.
although they have noticed the noisy defect data problem they did not empirically measure the impact of different noise levels on defect prediction accuracy or try to eliminate nois e. the noisy data problem does not pertain to data collected by msr only .
it may occur in industrial metric projects as well.
for example khoshgoftaar and seliya performed an extensive study on nasa mdp datasets .
they observed low prediction performan ce and suggested that instead of focusing on searching for another classification technique for improving prediction accuracy the quality of the software measurement data should be addressed .
they also proposed a noise elimination technique based on the kmeans algorithm .
they detected outliers in the d ata and treated them as noisy instances.
the limitation of their method is that mislabel ed instances are often not outliers.
in this paper we present one of the first empirical studies of the impact of noisy data on defect prediction.
we also propose a novel noise detection algorithm which can identify mislabeled instances with good accuracy.
.
conclusions defect data collected based on specific bug fix keywords or bug report links in change logs are c ommonly used to build defect prediction models and to evaluate the models.
since leaving specific keywords or bug report links in change logs is optional automatically collected defect data from change logs inevitably includes noise.
recent studies show that noise in defect data is not negligible and this noise affects prediction performance .
however the issue of dealing with noisy data has not been addressed adequately.
in this paper we have introduced a method to measure noise resistan ce in softwa re defect prediction for predicting buggy files and buggy changes .
by applying the method to two well known defect prediction models we found that in general noises in the defect data do not affect defect prediction performance in a significant manner.
however the prediction performance decreases significantly when the dataset contains of both fp s and fn s. we have also proposed a new method called clni for i dentifying noisy instances in defect data.
our experiment results show that clni can effectively identify noises with reasonable accuracy.
the noise eliminated training sets produced by clni can improve the defect prediction performance especially for the machine learners that do not have strong noise resistant ability.
in future we will further investigate techniques for im proving defect prediction accuracy under noisy environment .
we will also explore if the results ob tained in this paper are applicable to industrial project s. all data used in our experiments are available at e.google.com p hunkim wiki handlingnoise