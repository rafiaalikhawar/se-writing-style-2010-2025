language based replay via data flow cut ming wu fan longyxi wang zhilei xu haoxiang lin xuezheng liu zhenyu guo huayang guoylidong zhou zheng zhang microsoft research asiaytsinghua university mit csail miw v falon haoxlin xueliu zhenyug v huguo lidongz zzhang microsoft.com xi timxu csail.mit.edu abstract a replay tool aiming to reproduce a program s execution interposes itself at an appropriate replay interface between the program and the environment.
during recording it logs all non deterministic side effects passing through the interface from the environment and feeds them back during replay.
the replay interface is critical for correctness and recording overhead of replay tools.
itarget is a novel replay tool that uses programming language techniques to automatically seek a replay interface that both ensures correctness and minimizes recording overhead.
it performs static analysis to extract data flows estimates their recording costs via dynamic profiling computes an optimal replay interface that minimizes the recording overhead and instruments the program accordingly for interposition.
experimental results show that itarget can successfully replay complex c programs including apache web server and berkeley db and that it can reduce the log size by up to two orders of magnitude and slowdown by up to .
categories and subject descriptors d. .
testing and debugging debugging aids d. .
processors debuggers f. .
semantics of programming languages program analysis general terms languages performance reliability keywords data flow graph cut replay instrumentation .
introduction a replay tool aims at reproducing a program s execution which enables cyclic debugging and comprehensive diagnosis techniques such as intrusion analysis predicate checking program slicing model checking and test generation .
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
fse november santa fe new mexico usa.
copyright acm ... .
.re execution of a program could often deviate from the original execution due to non determinism from the environment such as time user input and network i o activities.
a replay tool therefore interposes at an appropriate replay interface between the program and the environment recording in a log all non determinism that arises during execution.
traditional choices of replay interfaces include virtual machines system calls and higherlevel apis .
for correctness at the replay interface the tool must observe all non determinism during recording and eliminate the non deterministic effects during replay e.g.
by feeding back recorded values from the log.
furthermore both interposition and logging introduce performance overhead to a program s execution during recording it is of practical importance for a replay tool to minimize such overhead especially when the program is part of a deployed production system.
this paper proposes itarget a replay tool that makes use of programming language techniques to find a correct and low overhead replay interface.
itarget achieves a replay of a program s execution with respect to a given replay target i.e.
the part of the program to be replayed by ensuring that the behavior of the replay target during replay is identical to that in the original execution.
to this end itarget analyzes the source code and instruments the program during compilation to produce a single binary executable that is able to run in either recording or replay mode.
ensuring correctness while reducing recording overhead is challenging for a replay tool.
consider the apache http server shown in figure consisting of a number of plug in modules that extend its functionality.
the server communicates intensively with the environment such as clients memory mapped files and a database server.
the programmer is developing a plug in module mod x which is loaded into the apache process at runtime.
unfortunately mod x occasionally crashes at run time.
the programmer s debugging goal is to reproduce the execution of replay target mod x using itarget and inspect suspicious control flows.
the first challenge facing itarget is that it must interpose at a complete replay interface that observes all non determinism.
for example the replay target mod x may both issue system calls that return non deterministic results and retrieve the contents of memorymapped files by dereferencing pointers.
to replay mod x itarget thus must capture non determinism that comes from both function calls and direct memory accesses.
an incomplete replay interface such as one composed of only functions would result in a failed replay.
a complete interposition at an instructionlevel replay interface observes all non determinism but it often comes with a prohibitively high interposition overhead because the execution of each memory access instruction is inspected.
another challenge is that itarget should choose a replay interface wisely and prefer one with a low recording overhead.
for example g2 g19 g7 g9 g13 g11 g1 g16 g18 g10 g23 g6 g1 g16 g18 g10 g23 g2 g1 g22 g22 g22 g1 g16 g18 g10 g23 g3 g1 g16 g16 g7 g19 g1 g12 g15 g11 g1 g5 g7 g21 g7 g8 g7 g20 g11 g1 g4 g15 g14 g11 g17 g21 g1figure the apache http server process consisting of several modules communicates with the environment.
the apache module mod x enclosed by the dash line is the replay target.
ifmod x s own logic does not directly involve database communications it should be safe to ignore most of the database input data during recording for replaying mod x .
naively recording all input to the whole process would lead to a huge log size and significant slowdown.
however if mod x is tightly coupled with mod b i.e.
they exchange a large amount of data it is better to replay both modules together rather than mod x alone so as to avoid recording their communications.
itarget addresses these challenges with the benefit of languagebased techniques for replay.
first itarget instruments a program at the granularity of instructions in the form of an intermediate representation used by compilers for the interposition at the replay interface.
such a fine granularity is necessary for correctly replaying programs with sources of non determinism from non function interfaces e.g.
memory mapped files.
in addition itarget models a program s execution as a data flow graph data flows across a replay interface are directly correlated with the amount of data to be recorded.
therefore the problem of finding the replay interface with a minimal recording overhead is reduced to that of finding the minimum cut in the data flow graph.
in so itarget instruments a needed part of the program and records data accordingly which brings down the overhead of both interposition and logging at runtime.
the actual interposition is through compile time instrumentation at the chosen replay interface as the result of static analysis thereby avoiding the executiontime cost of inspecting every instruction execution.
we have implemented itarget on windows x86 and applied it to a wide range of c programs including apache http server berkeley db two http clients neon and wget and a set of spec cint2000 the integer component of spec cpu2000 benchmarks.
experimental results show that itarget can reduce log sizes by up to two orders of magnitude and reduce performance overhead by up to when some logical subset of a program is chosen as a replay target.
even when the whole program is chosen as the replay target itarget s recording performance is still comparable to that of state of the art replay tools.
the contributions of this paper are twofold a data flow model for understanding and optimizing replay tools and a languagebased replay tool that provides both a high correctness assurance and a low overhead.
the rest of the paper is organized as follows.
section presents a replay model.
section describes how itarget computes a replay interface via static analysis.
section describes itarget s runtime for recording and replay.
section discusses the choice of a replay target in practice.
we evaluate itarget in section survey related work in section and then conclude in section .
.
model the foundation of itarget hinges on our replay model which provides a general framework for understanding replay correctness as well as the associated recording overhead.
the model naturally execution cnt1 a1 random cnt2 cnt1 a1 print cnt2 a2 random cnt3 cnt2 a2 print cnt3f cnt g cnt printf d n cnt g cnt printf d n cnt g int p a random p a figure a code snippet and its execution.
inst1 inst2 inst3 inst5cnt1 a1 cnt2 a2 inst6 cnt3 inst7inst4 cut cut figure execution flow graph.
ovals represent operation nodes and rectangles represent value nodes.
of all operation nodes double ovals are target operations for replay while shadow ovals are non deterministic operations.
explains different strategies of existing replay tools and paves the way for itarget s language based replay approach.
both the replay model and different replay strategies with respect to the model are the subject of this section.
.
execution flow graph we first assume single threaded executions multi threading issues will be discussed in section .
.
we use the code listed in figure as a running example where function fcalls function gtwice to increase a counter by a random number.
each variable in the execution is attached with a subscript indicating its version which is bumped every time the variable is assigned a value such as cnt1 3anda1 .
the seven instructions in the execution sequence are labeled as inst .
we model an execution of a program as an execution flow graph that captures data flow as illustrated in figure .
an execution flow graph is a bipartite graph consisting of operation nodes represented by ovals and value nodes represented by rectangles .
an operation node corresponds to an execution of an instruction while the adjacent value nodes serve as its input and output data.
each operation node may have several input and output value nodes connected by read andwrite edges respectively.
for example inst reads from both cnt1anda1 and writes to cnt2.
a value node is identified by a variable with its version number the node may have multiple read edges but only one write edge for which the version number is bumped.
each edge is weighted by the volume of data that flow through it omitted in figure .
an execution flow graph covers the code either written by the programmer or adopted from supporting libraries.
the programmer can choose part of code of her interest as the replay target a replay target corresponds to a subset of operation nodes referred to as target nodes represented by double ovals in an execution flow graph.
for example to replay function fin figure inst 7are set as target nodes.
the goal of replay is to reproduce an identical run of these target nodes defined as follows.definition .a replay with respect to a replay target is a run that reproduces a subgraph containing all target nodes of the execution flow graph as well as their input and output value nodes.
the programmer can also choose a subset of value nodes as the replay target.
since an execution flow graph is bipartite it is equivalent to choose their adjacent operation nodes as the replay target.
we assume that the replay target is a subset of operation nodes in the following discussion.
a naive way to reproduce a subgraph is to record executions of alltarget nodes with their input and output values but this will likely introduce a significant and unnecessary overhead.
one way to cope with this is to take advantage of deterministic operation nodes which can be re executed with the same input values to generate the same output.
for example assignments e.g.
inst and numerical computations e.g.
inst are deterministic.
in contrast non deterministic operation nodes correspond to the execution of instructions that generate random numbers or receive input from the network.
these instructions cannot be re executed during replay because each run may produce a different output even with the same input values.
in figure non deterministic operation nodes are represented by shadow ovals e.g.
inst .
since a non deterministic node cannot be re executed to ensure correctness a replay tool can record either the output of that nondeterministic operation node or the input of any deterministic operation node that is affected by the output of that non deterministic operation node and feed the recorded values back during replay.
to replay target nodes correctly a replay tool must ensure that target nodes are not affected by non deterministic nodes as manifested as a path from a non deterministic operation node to any of the target nodes.
a replay tool can introduce a cutthrough that path like cuts and given in figure .
such a cut is called a replay interface defined as follows.
definition .given an execution flow graph any graph cut that partitions non deterministic operation nodes from target nodes gives a valid replay interface.
a replay interface partitions operation nodes in an execution flow graph into two sets.
the set containing target nodes is called the replay space and the other set containing non deterministic operation nodes is called the non replay space .
during replay only operation nodes in replay space will be re executed.
a replay tool should log the data that flow from non replay space to replay space i.e.
through the cut set edges of the replay interface because the data are non deterministic.
recall that each edge is weighted with the cost of the corresponding read write operation.
to reduce recording overhead an optimal interface can be computed as the minimum cut defined as follows.
definition .given an execution flow graph the minimum log size required to record the execution for replay is the maximum flow of the graph passing from the non deterministic operation nodes to the target nodes the minimum cut gives the corresponding replay interface.
.
cut strategies one simple strategy for finding a replay interface is to cut nondeterminism eagerly whenever any surfaces during execution by recording the output values of that instruction.
take figure as an example.
given non deterministic operation random cut prevents the return values of inst 5from flowing into the rest of the execution.
a replay tool that adopts this strategy will record the values that flow through the edges inst a1 and inst a2 in fcnt1 a1 cnt2 a2 cnt3g1 g2 figure condensed execution flow graph.
the cut corresponds to cut in figure .
this case inst 5are in non replay space while the rest of the nodes are in replay space.
function level cut.
we can impose an additional cut constraint that instructions of the same function will be either re executed or skipped entirely i.e.
a function as a whole belongs to either replay space or non replay space.
a function level cut provides a natural debugging unit for the programmer and avoids switching back and forth between replay and non replay spaces within a function.
we believe that such a function level cut offers better debugging experience in practice even though it may lead to a slightly larger log.
itarget computes a function level cut.
for a function level cut itarget condenses instructions in an execution of a function into a single operation node.
as shown in figure g1 including inst and g2 including inst are two calls to function g which returns a non deterministic value.
the cut in figure corresponds to cut in figure .
note that this cut also employs the eager strategy which tries to cut nondeterminism by recording the output whenever an execution of a function involves non deterministic operation nodes.
a replay tool that adopts the strategy will record the values that flow through the edges g1 cnt2 and g2 cnt3 in this case g1 2anda1 2are in non replay space while the rest of the nodes are in replay space.
neither of the two eager cuts shown in figure and is optimal because some of the returned non deterministic values may never be used by their callers and thus can be stripped during recording previous replay tools generally use similar eager strategies.
another simple cut strategy is to wrap precisely around the replay targets in replay space leaving the rest in non replay space which is usually not optimal either see section .
.
itarget employs a lazy globally optimized strategy based on the minimum cut which can result in smaller recording overhead see section for details .
.
multithreading thread interleaving introduces another source of non determinism that may change from recording to replay.
for example suppose threads t1andt2writes to the same memory address in order in the original run.
a replay tool must enforce the same write order during replay otherwise the value at the memory address may be different and the replay run may diverge from the original one.
to reproduce the same run a replay tool should generally record information of the original run in two kinds of logs a data flow log as defined in our model and a synchronization log with regard to thread interleaving.
we discuss recording strategies for producing the synchronization log as follows.
the first strategy is to record the complete information about how thread scheduling occurs in the original run.
the tool can either serialize the execution so that only one thread is allowed to run in the replay space or track the causal dependence between concur rent threads enforced by synchronization primitives e.g.
locks .
the two methods are standard techniques used by many runtime debugging tools .
note that causal dependence tracking may be used along with a race detector that eliminates unprotected memory accesses beforehand.
the second strategy on the other extreme is to record nothing in the synchronization log assuming a deterministic multithreading model .
in this case the thread scheduler behaves deterministically so that the scheduling order in the replay run will be the same as that in the original run.
the replay tool can then use the data flow log alone to reproduce the replay run.
itarget supports both recording strategies for multithreaded programs.
note that the strategy for producing the synchronization log is orthogonal to the data flow log.
.
replay interface computation although the minimum cut in an execution flow graph precisely defines the replay interface with the minimum log as described in section the cut is only optimal with respect to that specific run and is known only after the run.
itarget instead estimates a general replay interface that approximates the optimal one beforehand and statically .
this section describes how itarget constructs a data flow graph via static analysis and finds a cut as the replay interface.
.
static flow graph to approximate execution flow graphs statically itarget computes a static flow graph of a program via program analysis to estimate the execution flow graphs of all runs.
for example because version information of both value nodes and operation nodes may be only available during run time rather than during compile time cnt1 3in the execution flow graph figure may be projected to a single value node cntin a static flow graph similarly g1and g2in figure may be projected into a single operation node g. the weight of each edge is given via runtime profiling under typical workloads see section .
.
the minimum cut of the resulting static flow graph is computed as the recommended replay interface which is expected to approximate the optimal ones in typical runs.
a static flow graph can be regarded as an approximation of the corresponding execution flow graphs where operation nodes are functions and value nodes are variables.
the approximation should besound a cut in the static flow graph should correspond to a cut in the execution flow graph.
itarget performs static analysis to construct a static flow graph from source code as follows.
first itarget scans the whole program and adds an operation node for each function and a value node for each variable in the ssa form .
secondly itarget interprets each instruction as a series of reads and writes.
for example y x 1can be interpreted as read x andwrite y. every time itarget discovers a function freading from variable x it adds an edge from xtof similarly it adds an edge from ftoyif function fwrites to variable y. finally itarget performs pointer analysis and determines variable pairs that may alias i.e.
they may represent the same memory address and merges such pairs into single value nodes.
specifically itarget uses a classical andersen style pointer analysis .
the analysis is flow and context insensitive which means that it does not consider the order of statements though it uses the ssa form to partially do so nor different calling contexts in a program.
in this way the analysis is both efficient and correct for multithreaded programs.
as a result a static flow graph that itarget constructs can be considered as a projection from an execution flow graph invocationsof the same functions are merged into single operation nodes and variables that may alias are merged into single value nodes.
thus the approximation is sound as a cut in a static flow graph is easily translated to one in a corresponding execution flow graph.
we omit the proof detail for brevity.
.
missing functions the analysis for constructing a static flow graph requires the source code of all functions.
for functions without source code such as low level system and libc calls itarget speculates their side effects as follows.
by default itarget conservatively considers functions without source code as non deterministic i.e.
they are placed in non replay space.
consequently these functions are not re executed during replay so itarget must record their side effects.
itarget assumes that such functions will modify memory addresses reachable from their parameters.
for example for function recv fd buf len ags itarget assumes that recv may modify memory reachable from buf.
as a result itarget would cut at all the read edges that flow from variables affected by bufto the replay space.
the default approach also works smoothly with non deterministic functions involving global variables.
for example the global variableerrno is defined internally in libc and may be modified by a libc function without source code considered as non deterministic when errno is read by the program itarget will place the corresponding value node of errno in non replay space.
thus the replay interface may cut through the read edges of the value node.
itarget can then log its value during recording and feed the value back during replay.
a downside of the default approach is that if a program calls recv once to fill bufand then reads the content ten times itarget would have to record ten copies of buf.
to reduce recording overhead further itarget reuses r2 s annotations on windows api functions to complete a static flow graph.
for example r2 annotates function recv with bufthat will be modified and with the size of the buffer.
itarget exploits the knowledge to fix the static flow graph with a write edge from recv tobuf it may then choose to cut at the write edge and record only one copy of buf.
in addition itarget annotates dozens of popular libc functions as deterministic including math functions e.g.
abs sqrt memory and string operations e.g.
memcpy strcat since they do not interact with the environment.
itarget can place them in replay space to avoid recording their side effects if possible.
it is worth noting that itarget uses function annotations optionally for reducing recording overhead rather than for correctness.
itarget does not require any annotations for higher level functions.
these annotations are also shared across different applications and do not require programmer involvement.
.
minimum cut itarget weights each edge in the static flow graph with the volume of data that pass through it.
unlike edges in a dynamic execution flow graph the weight of each edge in a static flow graph depends on the number of invocations of the corresponding instructions.
itarget estimates the weight via runtime profiling.
given a replay target in a weighted static flow graph itarget computes the minimum cut using dinic s algorithm .
it runs in o jvj2jej time wherejvjandjejare the numbers of vertices and edges in the static flow graph respectively.
profiling can be done in a variety of ways for example by using an instruction level simulator or through sampling.
currently itarget simply builds a profiling version of a program where memory access instructions are all instrumented to count a total size of datatransfers with each of them.
we run this version multiple times on a sample input.
for functions that are never invoked during profiling itarget assigns a minimal weight to their corresponding edges.
generally the recording overhead may depend on the extent to which the profiling run reflects the execution path of the actual recording run.
different types of workload in profiling and recording runs may drive the runs to different execution paths and hence negatively affect the recording performance.
however in our experience the resulting cut tends notto be sensitive to the profiling workload scale see section .
.
thus the programmer can profile the program with smaller workload scale without incurring much extra recording overhead.
.
record replay runtime after computing a desirable replay interface as described in section itarget instruments the target program accordingly during compilation to insert calls that are linked to its runtime for recording and replay.
this section describes itarget s runtime mechanisms that ensure control and data flow memory footprints and thread interleaving do not change from recording to replay.
.
calls reads and writes when computing a replay interface itarget partitions functions operation nodes and variables value nodes in a static flow graph into replay and non replay spaces.
since functions in non replay space will not be executed during replay itarget must record all side effects from non replay space.
first itarget records function calls from non replay space to replay space.
consider a function fin non replay space that calls function gin replay space.
during replay fwill not be executed and is not able to call g which should be executed the itarget runtime does so instead.
specifically for each call site in a function placed in non replay space itarget resolves the callee to see whether it must belong to non replay space if yes itarget does not record anything otherwise itarget logs the call event during recording and issues the call during replay when the callee does belong to replay space.
furthermore itarget instruments necessary instructions to record data that flow from non replay space to replay space.
specifically itarget instruments instructions placed in replay space that read from variables in non replay space and instructions placed in non replay space that write to variables in replay space.
we refer the two kinds of instructions to be instrumented above as read and write respectively.
other instructions remain unchanged so that they can run at native speed.
when executing a read instruction in the original run the runtime records the values being read in the log.
when executing the same instruction during replay the runtime simply feeds back the values from the log rather than letting it read from memory.
it is more complex to record and replay a write instruction.
since the instruction is never executed during replay the runtime has to instead issue the write to memory.
in addition to the values to be written the runtime needs to know where andwhen to do so.
to determine where to issue the writes the runtime records the memory addresses that the instruction is writing to along with the values so that it can write the values back to the recorded memory addresses during replay.
to determine when to issue the writes the runtime further orders writes with calls.
consider a function fin non replay space which writes to variable x makes a call to function gthat is in replay space and then writes to variable y. the runtime records the three events of writing x calling g and writing yin the original run it then issues the three events in the same order during replay.
.
memory management to ensure correctness addresses of variables in replay space should not change from recording to replay.
this is non trivial because functions in the non replay space may allocate memory e.g.
by calling malloc and will not be executed during replay itarget needs to ensure that memory addresses returned by subsequent calls tomalloc in replay space are the same as those during recording.
for values allocated on the heap itarget uses a separate memory pool for the execution in replay space.
any call to malloc in replay space will be redirected to that pool.
since the execution in replay space remains the same from recording to replay so are the memory addresses allocated in the pool.
for values allocated on the stack itarget could run functions in replay and non replay spaces on two separate stacks like jockey and r2 the runtime would switch the stacks when crossing the two spaces which introduces additional overhead.
itarget employs a more lightweight approach.
it uses only one stack and ensures deterministic stack addresses by guaranteeing the value of the stack pointer esp on x86 does not change from recording to replay when the program enters a function in replay space.
to do so the itarget runtime records current esp value before a call from non replay space to replay space in the original run.
during replay the runtime sets esp to the recorded value before issuing the call and restores esp and the stack after that.
if current esp is lower than the recorded one itarget also backs up the data in the overlapped range to avoid overwriting.
.
thread management as we have discussed in section .
itarget supports several multithreading strategies.
itarget runs in the mode of causal dependence tracking by default so the resulting log contains both data flow and synchronization information.
for asynchronous signals itarget uses the standard technique to delay the delivery until safe points.
.
choosing replay targets itarget allows programmers to choose appropriate replay targets and therefore enables target replay .
this added flexibility can translate into significant savings in recording overhead compared to whole program replay.
this section discusses several possible ways for choosing replay targets as well as the resulting replay interfaces.
.
modular programs applications that emphasize a modular design come with a natural boundary for choosing replay targets.
for example apache http server is designed and implemented as a main backbone plus a set of plug in modules as shown in figure .
the programmer developing a plug in module neither has to debug the backbone which is generally stable nor other modules which are largely irrelevant.
she can simply choose all the source code of her own module as the replay target for recording and replay.
a second example berkeley db also uses a modular design for building a replicated database.
on each node running berkeley db there is a replication manager that coordinates with other nodes as well as a traditional storage component that manages ondisk data.
when debugging the replication protocol a programmer can choose the code of the replication manager as the replay target ignoring the mature and irrelevant storage component that may involve massive i o communications.
section .
provides case studies for the two applications.
the replay interface computed by itarget often approximates the boundary between modules.
the insight is that a modular design impliesthat each module mostly uses internal data structures the rest of the program may not be involved much.
a programmer can choose the module of her interest as a replay target itarget s minimum cut can exploit the structure manifested in design and computes a replay interface that results in smaller overhead.
.
monolithic programs for a monolithic program that does not have a clear component boundary such as certain algorithm implementations a programmer can simply choose the entire program as the replay target which falls back to a whole program replay.
even in this case if the program does not directly manipulate all of its input itarget will record only the necessary data and skip the payload.
note that a programmer can still choose a subset of functions as replay target.
the risk is that the replay target may be tightly coupled with the rest of the program exchanging a large amount of data.
it could possibly lead to even higher recording overhead to choose the replay interface naively.
fortunately itarget can avoid such an anomaly through computing the minimum cut as the replay interface.
it is expected that in the worst case itarget will resort to that of whole program replay.
section .
uses spec cint2000 benchmarks to illustrate such cases.
.
crash points in practice when a running program crashes at some program point a programmer may choose code pieces related to that crash point as the replay target.
this can be done by simple heuristics e.g.
picking up all functions in the same source file or by automatic tools e.g.
program slicing or related function investigation .
the topic is beyond the scope of this paper.
.
ev aluation we have implemented itarget for c programs on windows x86.
the analysis and instrumentation components are implemented as plug ins within the phoenix compiler framework .
we have applied itarget to a variety of benchmarks including apache http server .
.
with service modules berkeley db .
.
with the fault tolerant replication service the neon http client .
.
the wget website crawler .
.
and six programs from spec cint2000.
the code sizes of the benchmarks also span a wide range from small spec cint2000 kloc medium neon wget kloc to very large apache berkeley db kloc .
their variety and complexity extensively exercise both the static interface analysis and the recordreplay runtime of itarget leading to a thorough evaluation.
we checked the correctness of replay run by making sure it successfully consumes all the logs generated during recording run.
we also manually spot checked some of the executions through attaching debugger into both recording and replay runs and comparing their internal states in replay space.
the rest of the section answers the following questions.
how effective is target replay in reducing recording overhead?
how well does itarget perform when the replay target is the whole program?
is the effectiveness of itarget sensitive to the data used in the profiling run?
what is the computation cost for finding an efficient replay interface?
.
methodology we categorize the benchmarks into two sets according to program modularity.
one set includes apache and berkeley db both of which contain natural module boundaries.
we apply both target and whole program replay to them for evaluating the effectiveness of target replay.
to reflect the fact that a programmer typicallyworks on individual modules we replay a single module each time during target replay experiments.
the other set consists of network clients and spec cint2000 programs.
these programs are implemented in a monolithic way with no natural module boundary.
we evaluate the performance of itarget in the worst case by applying whole program replay on these benchmarks.
we also evaluate the sensitivity of itarget s effectiveness on profiling runs with different workload scales and present a quantitative computation cost of itarget.
we additionally run two other recent replay tools idna and r2 on all benchmarks for comparison.
idna is built on top of an instruction level simulator and inspects each instruction for recording and replay while r2 interposes at function level and requires developers to manually annotate the replay interface to decide what should be recorded.
in our experiments idna imposes more than five times slowdown and generates significantly larger logs than itarget and r2 for all benchmarks.
it even fails to terminate when recording mcf andvpr after producing gb logs.
therefore we omit its data on the detailed discussion of each benchmark and only compare the overhead of itarget log size and slowdown with those imposed by r2.
our experiments on two server applications apache and berkeley db were conducted on machines with .
ghz intel xeon 8way cpu and gb memory.
other experiments were conducted on machines with .
ghz intel xeon dual core cpu and gb memory.
all of these machines are connected with gb ethernet running windows server .
in all experiments itarget shares the same disk with the application.
.
performance on modular programs we use apache http server and berkeley db to evaluate the performance advantage of target replay on modular programs.
we further investigate how itarget computes an appropriate replay interface to isolate the target module with only slight recording overhead.
our experiments include replaying both whole programs and individual modules.
table lists the modules used for target replay.
note that r2 cannot replay individual modules of these programs so we only present data of whole program replay.
in fact although r2 supports the replay of part of a program it requires the programmer to annotate the side effects of the functions that compose the replay interface.
however its annotation language only supports to specify plain buffers rather than side effects that involve complex pointer usages which pervade apache and berkeley db functions.
besides it would be tedious and error prone to manually annotate hundreds of functions.
apache http server.
apache comes with a flexible modular framework.
the core part libhttpd invokes responsible modules to handle http requests.
we include three commonly used modules in our experiment namely mod alias mod dir and mod deflate listed in table .
specifically mod alias maps request urls to filesystem paths mod dir provides trailing slash redirection for serving directory index files and mod deflate acting as an output filter compresses files before sending them to clients.
in the experiment apache http server runs with all these three modules we use itarget to replay each of them individually.
since each module contains a single source file to replay a module we choose all functions in the corresponding file as the replay target.
we set up the built in apache benchmarking tool ab which starts clients that repetitively fetch files from a server via http requests.
we put an html file index.html sized kb on the server and start eight clients to grab the compressed version of the file via an aliasing name of the directory.
thus for each request allreplay target replay target sources description apache mod alias mod alias.c mapping urls and file paths mod dir mod dir.c serving directory index files mod deflate mod deflate.c compressing http output berkeley db repmgr all files in the repmgr directory replication service table modules used for target replay.
.8m9.4m .4m1015.7m .5m .0m .
.
.
.
.
.
.
.
.
deflate alias dir all r2 baselinelogsize mb a log size .
.
.
.
.
.
deflate alias dir all r2 nativerequests s b throughput figure recording performance of different replay targets in apache.
three modules are executed.
itarget uses requests for profiling in order to assign costs to edges in the static flow graph clients send requests in the experiment.
figure a shows log sizes generated for answering client requests when itarget is replaying each module and the whole apache program respectively where the baseline is the total size of data that apache reads from disk and network.
the log sizes remain small as itarget tries to replay only an individual module.
itarget consumes less than mb log when replaying mod alias andmod dir .
replaying the more complex module mod deflate takes mb which is still substantially smaller than the baseline.
this shows that itarget manages to avoid logging the entire file and network i o and only record the necessary data for replaying a single module.
for example to correctly replay module mod deflate itarget only needs to record the metadata exchanges and skips the entire file content which is manipulated in the underlying third party library zlib .
on the contrary the log sizes of both whole program replay and r2 are close to the baseline gb .
figure b shows the throughput during recording for each replay target.
the throughput decreases as the log size increases.
replaying a single module using itarget inflicts only less than performance slowdown.
however a whole program replay incurs slowdown though the performance is still comparable to r2.
berkeley db.
in the experiment on berkeley db we start two nodes to form a replication group.
one node is elected as the master and randomly inserts key value pairs sized kb each to the replicated database.
we use itarget to replay one node.
repmgr all r2 baseline log size mb a log size repmgr all r2 na1ve throughput req sec b throughput figure log size and throughput of recording berkeley db.
we choose to replay the repmgr module that implements a distributed replication protocol.
it is known to have subtle bugs and hard to debug .
we specify all functions in source files of this module as the replay target.
figure a shows the log sizes of itarget and r2 with different interfaces.
it also shows the baseline log size as the size of all input data from disk and network.
note that the baseline volume to be recorded is much larger than the application s input data because when each node receives log records from the master it may scan its own database log file to find the corresponding records belonging to the same transaction which incurs substantial file reads.
the result shows that the log size of itarget for replaying the repmgr module is only about 3of that of itarget for wholeprogram replay and that of r2.
this is because repmgr does not directly touch all the data read from the database log file.
itarget only needs to record values returned by lower file i o and local database modules thereby substantially reducing the log size.
both the log sizes of itarget for whole program replay and r2 are larger than the baseline.
this is due to the cost of tracking the causal order of synchronization events in the synchronization log.
it turns out that berkeley db heavily uses the interlocked operations leading to the excessive cost.
figure b shows berkeley db s throughput during recording with itarget for different replay targets and with r2.
itarget and r2 incur and slowdown in whole program replay respectively.
this is mostly due to the cost of tracking interlock operations.
however when using target replay on the repmgr component itarget incurs only slowdown thus achieving and throughput improvements compared to the previous two whole program replay cases respectively.
the above experiments demonstrate that itarget can automatically identify a correct replay interface that separates a single module from the surrounding environment.
for modular programs like apache http server and berkeley db itarget enables significant performance improvements through target replay of modules.
.
performance on monolithic programs we evaluate itarget s recording performance of whole program replay on monolithic programs.
for monolithic programs that do not directly manipulate input data e.g.
an http client that parses .
.
.
0100200300400500600log size kb a log size for neon .
.
.
0200040006000800010000log size kb b log size for wget figure log size for recording neon and wget.
the http header and ignores the content of the http body itarget is able to exploit this to reduce recording overhead we use two http clients neon and wget to illustrate this case.
for monolithic programs that implement certain algorithms and perform intensive computations their executions depend heavily on every piece of input data and replay tools need to record all input we use six spec cint2000 programs to illustrate that itarget s performance is still comparable to existing replay tools in this case.
the programmer can specify a subset of functions as the replay target for a monolithic program.
even if the replay target may exchange a large amount of data with the rest of the program itarget can automatically detect the issue and avoid recording the data by finding a lower cost replay interface elsewhere.
it guarantees that the resulting performance is no worse than that of the wholeprogram replay.
we use crafty one of the spec cint2000 programs to illustrate the case.
network clients.
we use http clients neon and wget as benchmarks to evaluate itarget s performance.
for http clients the most essential part is to handle the http protocol which is only related to the header of input data.
in all our experiments for neon and wget the slowdown caused by replay tools is negligible.
this is because the performance bottleneck is mainly the network or the server side.
we therefore present only the log sizes here.
the neon client skips the payload in http responses.
we set up an http server and run neon on another machine to request files repetitively the average size of those files is kb.
figure a shows the log sizes for neon.
the size of the data downloaded from the network is the baseline for replay.
itarget successfully avoids recording the payload data reducing logs to around .
of the original.
itarget records only the http headers that neon inspects while r2 records the entire html files.
wget is a web crawler that crawls a web site and converts its pages for offline browsing.
we set up a mini web site and make wget crawl its pages.
each html file is kb and the total size of the crawled html files is mb.
figure b shows the log sizes for wget.
unlike neon wget parses each downloaded file to find new hyperlinks.
the baseline log size is twice of html files size due to extra disk i o during parsing.
itarget still shows its advantage because wget touches the payload only via libc functions such asstricmp .
itarget then only records their return values to avoid recording the whole file data.
it reduces the logs to only .
kb.
the above experiments show that with the help of the languagebased model itarget can identify the payload in an input file and skip it.
thus for the applications that do not directly manipulate all of their inputs such as neon and wget itarget outperforms previous tools even with whole program replay.
395x 20x gzip .3mvpr .1mmcf .7mcrafty .4kvortex .2mtwolf .5mitarget r2 a normalized log size .
.
.
.
.
gzip .87svpr 2251smcf 125scrafty 132svortex 62stwolf 325sitarget r2 b normalized execution time figure recording cost of whole program replay for spec cint2000 programs.
spec cint2000.
we evaluate the performance on six spec cint2000 applications using standard test suites.
although many of them are actually deterministic given the same input they are good for evaluating the worst cases of itarget where neither target replay nor skipping payload is possible.
we use itarget to replay the whole program of each benchmark and compare the slowdown and the log size of itarget with r2.
figure a shows the log sizes normalized to input data size shown under each label .
it is not surprising that itarget requires to record as much data as the input file.
vprandtwolf read files multiple times thereby causing the large log sizes of itarget and r2.
the input data size of crafty is less than bytes and the log of both itarget and r2 is dominated by auxiliary tags and events e.g.
calls .
this explains the large log size ratio of crafty .
figure b shows the slowdown normalized to the native execution time shown under each label .
we can see that itarget has similar performance to r2 and native in all spec cint2000 benchmarks.
itarget and r2 may run faster than native execution sometimes because they redirect malloc andfree in replay space to a separate memory pool with full fledged memory management functionality which has slightly different performance from the libc counterpart see section .
.
the result shows that for cpu intensive benchmarks both the log size and slowdown of itarget to replay whole programs are comparable to those of r2.
target replay in crafty.
in spec cint2000 programs functions have heavy data dependencies.
only replaying a single function can even result in a worse overhead than whole program replay if done naively.
itarget will automatically choose an appropriate replay interface while a naive cut mentioned in section .
may generate a huge log especially when the replay target repeatedly reads data from other parts of the program as seen in some algorithm implementations.
we use crafty as a test case where we choose searchroot a function that implements the alpha beta search as the replay target.
itarget detects the heavy dependencies between this function and the rest part of crafty the resulting replay interface leads to a log similar to that of a whole program replay in size both kb .
in contrast a naive cut strategy produces a huge log gb .
of req.
size per req.
kb log size mb apache none none .
mod deflate .
.
.
berkeley db none none .
repmgr .
.
.
.
table the log size of apache and berkeley db under different profiling settings.
prog.
kloc op.
val.
edges cut time s mcf gzip vpr neon twolf crafty wget vortex apache bdb table statistics of computation cost of replay interfaces.
op.
val.
and edges list the numbers of operation nodes value nodes and edges in static flow graphs respectively cut time lists the time for computing the minimum cut bdb is short for berkeley db.
the experiments show that itarget is capable of intelligently skipping the unused payload within input during whole program replay of monolithic programs therefore reducing log sizes significantly.
in the worst case where programs depend on all input data and the replay target is tightly coupled with the rest of the program target replay in itarget will automatically fall back to whole program replay.
even in those cases the overhead of itarget remains comparable to that of existing tools.
.
profiling workload and computation cost profiling workload.
we evaluate how different profiling workload scales could affect the resulting replay interfaces.
table shows the log sizes for replaying the module mod deflate in apache http server and the module repmgr in berkeley db under different profiling workload scales.
none represents the recording log size without any profiling run.
in this case we simply assign the same weight to each edge in static flow graph.
the result shows that the profiling run is important for reducing overhead of itarget.
without profiling the log sizes of apache and berkeley db are times and twice larger respectively.
however itarget is not sensitive to different workload scales of a profiling run.
in apache and berkeley db the resulting log sizes do not change much when using different input file sizes or different numbers of requests for profiling.
computation cost.
table shows the statistics of computation cost of replay interfaces.
for each program we report the numbers of operation nodes value nodes and edges in the static flow graph as well as the time it takes to compute the minimum cut.
the pointer analysis can finish within seconds for all the benchmarks so we omit the time.
in general itarget is efficient to find a near optimal replay interface for a target function set.
if a programmer chooses different targets or modifies the code itarget can reconstruct the graph and recompute the replay interface within minutes.
.
summary in terms of performance itarget enjoys two advantages imposing small instrumentation overhead and reducing logging i o that competes with application i o. this makes itarget more lightweight when applied to both cpu and i o intensive applications.
it also shows that even in the worst case of whole program replay with the minimum cut strategy itarget still achieves comparable performance to state of the art replay tools.
the accuracy of profiling is critical for itarget but itarget does not strictly require the same input during profiling and recording.
the user does not always need to re profile the graph if she just increases the testing workload e.g.
in terms of file sizes or numbers of requests in recording phase.
.
related work process level replay.
library based replay tools like jockey liblog recplay and flashback adopt a fixed replay interface at library functions and tend to record and replay an entire program.
r2 is more related to itarget since it also employs language based techniques and is able to replay only a part of a program.
r2 asks programmers to select a set of functions as a replay interface to isolate replay targets and annotate their side effects with a set of keywords.
however it is tedious and error prone for programmers to select manually acomplete interface that isolates all non determinism.
furthermore the expressiveness of r2 keywords is limited it is difficult to annotate functions that access memory other than plain buffers.
itarget automatically computes an efficient replay interface from source code via program analysis to minimize recording overhead.
it interposes itself at the instruction level and requires no programmer annotations.
idna also takes an instruction level replay interface.
it uses an instruction level simulator to track each instruction and to record necessary data for replay such as register states after certain special non deterministic instructions and memory values that are read by instructions during execution.
to avoid recording all memory values read idna maintains a shadow memory internally to cache previous values.
despite the optimization idna still incurs significant performance slowdown due to expensive instruction tracking.
itarget needs to track only memory access instructions at the computed replay interface and hence incurs remarkably less overhead.
there are a number of replay tools focusing on applications using various programming language runtimes such as java mpi and standard ml .
while current itarget implementation works with the c language its language based technique and data flow model are general enough and can be easily applied to other programming languages.
whole system replay.
hardware based and virtual machine based replay tools aim to replay a whole system including both target applications and the underlying operating system.
because these tools intercept a system at a low level interface such as at the processor level it is easy for them to observe all the non determinism from the environment.
special hardware or virtual machine environment is required for those tools to work.
language based security.
swift provides a secure programming language to construct secure web applications and partitions a control flow graph to split programs into a client and a server itarget partitions a program into replay and non replay spaces through a cut on data flow graph to ensure determinism for replay target.
flowcheck is a tool quantitatively estimating leak of secure data by dynamically tracking information flows itarget estimates its flow graph statically.
.
conclusion the beauty of itarget lies in its simple and general model that defines the notion of correct replay precisely.
the model leads to our insight that the problem of finding an optimal replay interface can be reduced to that of finding the minimum cut in a data flow graph.
with this model itarget employs programming language techniques to achieve both correctness and low recording overhead when replaying complex real world programs.