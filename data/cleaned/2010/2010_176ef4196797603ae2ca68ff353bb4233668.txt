scalable product line configuration a straw to break the camel s back abdel salam sayyad joseph ingram tim menzies hany ammar lane department of computer science and electrical engineering west virginia university morgantown wv usa asayyad jingram3 mix.wvu.edu tim menzies.us hany.ammar mail.wvu.edu abstract software product lines are hard to configure.
techniques that work for medium sized product lines fail for much larger product lines such as the linux kernel with features .
this paper presents simple heuristic s that help the indicator based evolutionary algorithm ibea in finding sound and optimum configurations of very large variability models in the presence of competing objectives.
we employ a combination of static and evolutionary learning of model structure in addition to utilizing a pre computed solution used as a seed in the midst of a randomly generated initial population .
the seed sol ution work s like a single straw that is enough to break the camel s back given that it is a feature rich seed.
we show promising results where we can find sound solutions for configuring upward of featu res within minutes .
index terms variabilit y models automated configuration multiobjective optimization evolutionary algorithms smt solvers.
i. introduction a. motivation scalability of search based software engineering sbse methods is of high importance because it can mean the difference between theoretical obscurity and industrial adoption.
the larger and more complex the application examples are the closer they resemble practical applications and the more believable the result will be .
yet the lack of scalability of results is one of the bigge st problems facing software engineers according to harman et al.
.
they state that many approaches that are attractive and elegant in the laboratory turn out to be inapplicable in the field beca use they lack scalability.
a case in point is the subject of this paper the many objective optimum feature selection in software product lines.
many results in the automated analysis of software product lines were validated using feature models published in online feature model repositories such as splot .
examples are pohl et al.
lopez herrejon and egyed johansen et al.
mendonca et al.
and our own previous work .
most of the feature models in splot were produced for academic purposes without representing actual system s. one such model is electronic shopping designed by lau the largest in splot with features.
while it might be a best effort in emulating a real system it does not represent an actual project.
berger et al.
explain in detail the differences in properties between splot feature models and the large feature models that they developed by reverse engineering real systems and published in the lvat linux variability analysis tools repository1.
in short splot models had significantly smaller and less constrained models with lower branching factors but they also had higher ratios of feature groups and deeper leaves than lvat models.
this shows an underlying gap between academic assumptions and actual properties of software product lines.
the only two studies we know that experimented with the lvat feature models were done by johansen et al.
who generated test covering arrays for feature models and henard et al.
who worked on prioritizing t wise test suites .
both experimented with three very large models from the lvat repository linux ecos and freebsd in addition to model s from splot and other sources.
our work is the first to attempt the many objective optimization of product line configuration.
other researchers attempted to prove scalability of their methods using randomly generated feature models that follow ed a set of ass umed characteristics.
examples are white et al.
shi et al.
guo et al.
and mendonca et al.
.
while those randomly generated models can be larger in size than published models they still suffer from the same assumptions that diverge from the properties of real systems.
in this work we seek to show the scalability of objective optimization of software product lines using the indicator based evolutionary algorithm ibea compared with the nondominated sorting genetic algorithm ii nsga ii .
previously we applied ibea and nsga ii among other algorithms to feature models from splot with sizes ranging from to features.
with ibea we were able to achieve objective optimization with a significant amount of fully correct configurations .
other algorithms including nsga ii failed t o learn the model constraints and produced few usable solutions.
in this paper we apply both ibea and nsga ii to large models from lvat.
in out of models we successfully show the superiority of ibea over nsga ii in producing significant amounts of fully correct a nd highly optimal configurations.
towards that end ibea is assisted by variability analysis tools source browse ?repo formulas a static scan to detect features that must have fixed values and those features are excluded from the evolutionary process.
for the largest model in this experiment i .e.
linux kernel the above combination of static and evolutionary learning did not succeed in producing any correct configurations du ring the first minutes.
to address this problem w e resorted to a novel approach in which we pre computed one correct configuration and planted it in the initial population of candidate solutions for the objective optimization.
the result was correct configurations in the first minutes of the objective optimization.
when we planted correct seeds the minute result was the same i.e.
about valid solutions.
therefore the effect of one carefully selected seed was enough to influence a randomly generated population into finding a range of solutions that can be suggested to the user in the initial stage of interactive configuration.
the seeding approach is depicted in fig .
.
fig.
.
depiction of the seeding approach one caveat is that the magic seed has to be feature rich i.e.
it must be a solution in which a sufficiently large number of features are enabled.
the result we obtain points us to the following guideline the proper selection of a seed s in the initial population is a ke y to scalability of product line configuration.
b. contributions of this paper this paper makes the following contributions a proof of scale up of previous results that showed superiority of ibea over other meoas e.g.
nsga ii that used absolute dominance criteria coupled with diversification measures in the fitness assignment.
ibea uses a continuous dominance metric that makes bette r use of the user preferences.
first time attempt to automate the configuration of the very large variability models available from lvat feature model repository which resulted from the works of berger et al.
.
we are not aware of any previous work that used those models for automated configuration .
johansen et al.
experimented with generating test cover ing arrays for three models linux ecos and freebsd but their tools were not able to handle the large sizes of th ese models for most of the purposes of their experiment.
henard et al.
presented a search based technique for prioritizing t wise test suites and applied it to ecos freebsd and linux with good results.
a novel approach that relies on ibea s ability to exploit use r preference knowledge in reaching optimum results but also enlists the help of a pre computed correct solution a seed and a static analysis of the model structure to aid ibea in converging faster to a large number of correct configurations.
the seeding approach was used by fraser and arcuri in the context of sear ch based software testing.
a breakthrough scalability result .
using th e novel approach in above we now show that it is possible to configure product lines as large as features .
note that this is a significant improvement in the state of the art in this ar ea since prior work was shown to configure up to features only .
c. organization of this paper section ii discusses related work in the automated analysis of feature models.
section iii introduces background material on feature modeling meoas and the z3 smt solver.
section iv explains the e xperimental setup and section v presents the results.
we discuss the findings and their impact in section v i. in section v ii we discuss potential th reats to validity and then in section v iii we offer our conclusions and directions for future work .
ii.
related work first we discuss related work in the area of automated product configuration and feature selection.
the idea of extending or augmenting feat ure models with quality attributes was proposed by many among them zhang et al.
.
the following papers used a similar approach and synthetic data to experiment with optimizing feature selection in spls.
soltani et al.
employ ed hierarchical task network htn planning a popular planning technique to automatically select suitable features that satisfy the stakeholders business concerns and resource limitations.
a performance evaluation was provided with three feature models containing and features .
the worst case run time was reported to be seconds which is significant for these small size feature models.
benavides et al.
provide d automated reasoning on extended feature models.
they assign ed extra functionality such as price range or time range to features.
they modeled the problem as a constraint satisfaction problem a nd solved it using csp solvers to return a set of features which satisfy the stakeholders criteria.
white et al.
mapped the feature selection problem to a multidimensional multi choice knapsack pr oblem mmkp .
they apply filtered cartesian f lattening to provide partially optimal feature configuration.
also white et al.
introduced the muscle tool which provided a formal model for multistep configuration and mapped it to constraint satisfaction problems csps .
hence csp solvers were used to determine the path from the start of the configuration to the desired final configuration.
non functional requirements were considered such as cost constraints between two configurations.
a sequence of minimal feature adaptations is calculated to reach from the initial to the desired feature model configurations.
seeder grower pre computes one valid configuration finds valid solutions that are pareto optimum in objectives the limitations of these methods are obvious given the small models that they experimented with.
as spls become larger the problem grows more intractable.
more recently a genetic algorithm was used to tackle this problem .
although the problem is obviously multiobjective the various objectives where aggregated into one and a simple ga was used.
the result is to provide the product manager with only one optimal configuration which is only optimal according to the weights chosen in the objective formula.
also they used a repair oper ator to keep all candidate solutions in line with the feature model all throughout the evolutionary process.
next we discuss other related work in the general area of automated analysis of feature models .
in a large experiment was performed to measure the efficiency of available bdd sat and csp solvers to perform four analysis operations on feature models from the splot repository .
they reported long run times for certain o perations and they cancelled certain runs with the larger feature models when the run time exceeded three hours.
an exponential runtime increase with the number of features for non bdd solvers on the valid operation was also reported.
in a basic search method breadth first search is used to find feature model inconsistencies and suggest fixing sets.
the method was run with feature models from the splot website the largest being features .
they report that computation time increases steadily as the number of features increases wi th features it took sec.
approx.
.
in efficient ordering heuristics are proposed for bdds that represent feature models.
such ordering can dramatically reduce the size of bdds thus allowing fast processing for interactive configuration algorithms.
the proposed heuristics were tested with five realistic feature models in addition to randomly generated feature models with larger sizes.
it was shown that the heuristics produce high quality variable orders that enable the compilation of large feature models wit h up to features .
in it is shown that the task of satisfiability sat solving of realistic models is easy .
in particular the phenomenon of phase transition is not observed for realistic feature models.
the explanation for this is that many real world problems are either over constrained in terms of variability they have no realizable products or under constrained they have many easily identifiable realizations .
for instance consistency checks on randomly generated models with up to features and a large number of cross tree constraints took about .
seconds.
in addition computing valid domains was completed in about seconds for models with features and a fairly large number of cross tree constraints.
in all the works mentioned above the testing was done wi th relatively small feature models published in academic repositories like splot or with large feature model that were randomly generated based on the same characteristics as splot models.
large feature models that represent actual code such as those pub lished in lvat have not yet been used to test out automated analysis and configuration methods.
fig.
.
feature model for mobile phone product line fig.
.
mobile phone feature model as a boolean expression iii.
background a. feature models a feature is an end user visible behavior of a software product that is of interest to some stakeholder.
a feature model represents the information of all possible products of a software product line in terms of features and relat ionships among them.
feature models are a special type of information model widely used in software product line engineering.
a feature model is represented as a hierarchically arranged set of features composed by relationships between a parent feature an d its child features or subfeatures .
cross tree constraints that are typically inclusion or exclusion statements in the form if feature f is included then features a and b must also be included or excluded .
figure adapted from depicts a simplified feature model inspired by the mobile phone industry.
the full set of rules in a feature model can be captured in a boolean expre ssion such as the one in fig .
which shows the expression for the mobile phone feature model.
from it we can conclude that the total number of rules in this feature model is including the following the root feature is mandatory.
every child requires its own parent.
if the chil d is mandatory the parent requires the child.
every group adds a rule about how many members can be chosen.
every cross tree constraint ctc is a rule.
fm mobile phone calls mobile phone screen gps mobile phone media mobile phone screen xor basic color high resolution media camera mp3 camera high resolution gps basic the feature models used in this study were obtained from the linux variability analysis tools lvat feature model repository which resulted from the works of berger et al.
.
the models were reverse engineered from open source code comments and documentation of such projects as the linux kernel ecos and freebsd operating systems and other large projects.
the resu lting feature models had distinctly different properties than models published by academic researchers such as those in splot .
the lvat models are significantly larger in size more constrained and h ave higher branchin g factors than academic models but they also had lower ratios of feature groups and in general shallower leaves.
the lvat models provide an opportunity for testing the scalability of many results in feature modeling for software produ ct lines.
the models downloaded from lvat website had the dimacs format which expresses each model as a formula in the conjunctive normal form cnf .
b. multiobjective optimization many real world problems involve simultaneous optimization of several incommensurable and often competing objectives.
often there is no single optimal solution but rather a set of alternative solutions.
these solutions are optimal in the wider sense that no other solutions in the search space are superior to them whe n all objectives are considered .
formally a vector is said to be dominate d by a vector if and only if u is partially less than v i.e.
the set of all points in the objective space that are not dominated by any other points is called the pareto front .
c. multiobjective evolutionary optimization algorithms meoas many algorithms have been suggested over the past two decades for multiobjective optimization based on evolutionary algorithms that were designed primarily for single objective optimization most notably genetic algorithms.
we have previously experimented with meoas that are implemented in the jmetal framework as applied to splot feature models .
we found a remarkable advantage in performance for the indicator based evolutionary algorithm ibea as compared to pareto based algorithms of which the nondominated sorting genetic algorithm version nsga ii is the best known .
the fundamental difference between these two types of algorithms is in the ranking criterion i.e.
fitness assignment used to determine which individuals have stronger chance to survive to the next generation.
thus we focus on the ranking criteria in both algorithms.
nondominated sorting genetic algorithm version nsga ii the sorting procedure in nsga ii is depicted in fig.
taken from .
it shows how t he combined primary and secondary population gets sorted according to domination where f contains all nondominated solutions f contains all nondominated solutions after excluding f and so on.
when the solutions within f need to be sorted for truncation they are ranked according to crowding distance a value calculated from distances to nearest neighbors in all objective values.
thus diversity preservation is the second criterion after domination to determine fitness f or survival.
indicator based evolutionary algorithm ibea figure provide s an outline of the ibea algorithm.
the details can be found in .
fig.
.
nsga ii sorting procedure fig.
.
outline of ibea input population size n maximum number of generations fitness scaling factor output a pareto set approximation step initialization generate an initial population p of size and an initial mating pool p of size append p to p set the generation counter m to .
step fitness assignment calculate fitness values of individuals in p i.e.
for all x1 p set where i .
is a dominance preserving binary indicator.
step environmental selection iterate the following three steps until the size of population p does not exceed .
choose an individual x p with the smallest fitness value i.e.
f x f x for all x p. .
remove x from the population.
.
update the fitness values of the remaining individuals i.e.
f x f x for all x p. step termination if m n or another stopping criterion is satisfied then set a to the set of decision vectors represented by the nondominated individuals in p. stop.
step mating selection perform binary tournament selection with replacement on p in order to fill the temporary mating pool p .
step variation apply recombination and mutation operators to the mating pool p and add the resulting offspring to p. increment the generation counter m m and go to step .
equation in fig.
shows ibea s fitness assignment.
each solution is given a weight based on i .
a dominance preserving quality indicator thus factoring in more of the optimization objectives of the user .
the authors of ibea zitzler and kunzli designed the algorithm such that preference information of the decision maker can be integr ated into multiobjective search .
it is noticed here that the ranking criteria in ibea place no emphasis on diversity of solutions thus diverging from the conventional trend set by nsga ii and followed by many others.
this difference in ranking criteria causes ibea to outperform nsga ii when the objective space increases in dimension.
in it is experimentally demonstrated with real valued test functions that the performance of nsga ii and spea2 rapidly deteriorates with in creasing dimension and that other algorithms such as ibea cope very well with high dimensional objective spaces .
it is argued that nsga ii tends to increase the distance to the pareto front in the first generations because the diversity based selection criteria favor higher distances between solutions.
special emphasis is given to extremal solutions with values near zero in one or more objectives.
these solutions remain non dominated and the distance cannot be reduced thereafter .
d. z3 smt solver we will use the z3 smt solver as one way to generate a known correct configuration for the large feature models.
this method is fast and straightforward since the models are already expressed in dimacs format which is a direct representation of the model s boolea n formula such as the one in fig.
.
satisfiability modulo theories smt generalizes boolean satisfiability sat by adding equality reasoning arithmetic fixed size bit vectors arrays quantifiers and other useful first order theories.
an smt solver is a tool for deciding the satisfiability i.e.
validity of formulas in these theories.
z3 is an efficient smt solver freely available from microsoft research.
it is used in various software verification and analysis applications.
although the configuration generated by z3 was not useful in this experiment compared to the one generated using ibea we still plan to use z3 in future work to efficiently find a set of correct configurations that would help ibea in converging faster to a larger set of sound and optimal solutions.
iv.
setup a. feature models used in this study the lvat formula repository includes models as of may each represented in two formats boolean and dimacs.
table i lists the models for which the results in this study are reported.
b. feature attributes our research explores alternate methods to explore complex decision spaces.
in our recent literature review we found that most m eoa research in software engineering explores a very simplistic two valued objective space.
table i. models used in this study model version features ref.
toybox .
.
axtls .
.
ecos .
freebsd .
.
fiasco uclinux linux x86 .
.
in our work with users we find that me rely exploring two objectives is insufficient to capture the breadth of their concerns.
therefore when we certify different optimizers we take care to explore problems with up to half a dozen objectives.
to make such rich objective spaces we augment simpler models with a rich set of objectives.
specifically we augmented the feature models with attributes for each feature cost used before and defects .
the values were selected stochastically according to distributions that emulate software projects.
cost takes real values distributed normally between .
a nd .
used before takes boolean values distributed uniformly and defects takes integer values distributed normally between and .
the only dependency among these qualities is if not used before then defects c. problem representat ion the feature models were represented as binary strings where the number of bits is equal to the number of features.
if the bit value is true then the feature is selected otherwise the feature is removed i.e.
deselected .
d. problem formulation defining the optimization objectives in this work we optimize the following objectives correctness i.e.
compliance to the relationships and constraints defined in the feature model.
since jmetal treats all optimization objectives as minimization objectives we seek to minimize rule violations.
richness of features we seek to minimize the number of deselected features.
features that were used before we seek to minimize the number of features tha t weren t used before.
known defects which we seek to minimize.
cost which we seek to minimize.
the second objective i.e.
richness of features counteracts the effects of the other objectives by increasing the number of selected features while minimizin g violations defects and cost.
without it the final pareto front would crowd in the area with minimum features and thus would provide a narrow set of options to the end user.
e. meoa parameters the following parameter values were used after rudimentary runs for parameter tuning.
it is noted that low values for crossover and mutation rates perform better with feature models as we found in previous work .
table ii.
parameter values parameter value populat ion size crossover rate .
mutation rate .
run time minutes independent runs f. run time as stopping criterion in we compared meoas by allowing each to perform a fixed number of fitness function evaluations which is a commonly used approach.
the number of evaluations is proportional to the total run time and the required cpu power.
yet the total run time is affec ted by many other algorithm dependent operations including the fitness ranking of individuals in each generation.
this leads to varying runtimes with the same number of evaluations.
for instance we noticed that ibea took five times longer than nsga ii to perform the same number of evaluations which meant that ibea spent far more time in fitness ranking than nsga ii.
this is expected from our study of fitness ranking criterion in subsection iii.c.
the question here is which criterion shall we fix in orde r to have a fair comparison among algorithms?
we have come to the opinion that each algorithm should be given a fixed amount of time to calculate its best approximation of the pareto front.
a better algorithm should score better on the quality indicators hv correct within that duration of time.
going back to the comparison between ibea and nsga ii if both are given the same duration of time then nsga ii would perform far more evaluations than ibea and thus would be given a better chance to improve it s results.
as we will see in the coming section providing nsga ii with the chance to evolve more generations did not help it to overcome ibea at producing more correct solutions or better hv.
in addition the user should be more concerned with the amount of time it takes to optimize than with the number of evaluations.
cpu power is often available at the user s disposal and the algorithms should utilize that cpu power to produce the best results in the least amount of time regardless of number of evalua tions or number of evolved generations.
therefore in the this paper s experiments we make our comparisons of the results after limiting the amount of time given to each algorithm to minutes regardless of number of evaluations each algorithm were able to perform.
g. quality of pareto front we compare the performance of meoas using the following q uality indicators hypervolume hv defined in is a measure of the size of the space covered underneath the pareto front.
if the objectives are all to be maximized then the preferred pareto front is the one with the highest hypervolume.
in jmetal all objectives are minimized but the pareto front i s inverted before calculating hypervolume thus the higher the hypervolume the closer to optimum the pareto front is.
correct i.e.
the percentage of fully correct solutions which is an indicator particular to this problem.
since correctness is an optimi zation objective that evolves over time there maybe points in the final pareto front that have rule violations.
such points are not likely to be useful to the user.
we are interested in percentage of points within the pareto front that have zero violation s and thus a full correctness score.
tt50 i.e.
time to achieve correct solutions is another problem specific indicator that we added as a measure of the speed of convergence to a large amount of valid solutions.
this is a useful comparison figure when the final correct value is the same since it shows who arrived faster at the milestone.
v. results in the following we run the objective optimization problem that we described in iii.d.
w e first try nsga ii and ibea without adding any knowledge of model constraints and then we add the feature fixing technique which we find to help ibea in optimizing the configuration of large models from lvat within the allocated minutes.
as for the 7th and largest model i.e.
linux kernel ibea requires f urther domain knowledge assistance which we offer in the form of a seed planted in the initial population.
this technique results in finding correct configurations in minutes.
a. static analysis to detect fixed features our original approach to the con figuration of feature models was to start from a population of randomly generated configurations and let the evolutionary process promote those configurations that conform to the feature model .
that approach worked well for the small feature models in splot although with extended run times but it was clear that we needed to guide the evolutionary algorithms to closely respect the structure of feature model s. in the dimacs formulas representin g our feature models certain disjunctions rules only include one feature which means that the feature is either mandatory a commonality which must always be selected or a dead feature which must always be deselected.
also we looked for disjunctions rules that included two features but one of them was fixed in the first round and thus the second one was fixed as well .
once a feature is detected as fixed we fix it in the initial population while all other features are subject to random configurat ion and we restrict the bit mutation operator to only flipping features that are not fixed.
table ii i shows the amount of fixed featur es detected in each model.
it also shows the amount of skipped rules i.e.
the rules that we stop checking in our fitne ss evaluation since they only include fixed features.
we observe that ecos freebsd and the linux x86 models have few fixed features.
table iv shows the results comparing ibea and nsga ii with and without feature fixing.
each algorithm is run times for minutes in each case.
the median values are reported.
we also performed mann whitney tests to assess the statistical significance of the correct indicator.
we highlight the correct in bold if the confidence level exceeds when comparing each metho d to the one to its left .
table iii.
fixed features and skipped rules model total features fixed features total rules skipped rules toybox axtls ecos freebsd fiasco uclinux linux x86 we make the following observations the feature fixing approach is still not enough for the largest model the linux kernel.
there were no valid solutions after minutes for all cases.
ibea outperforms nsga ii in terms of the correct indicator.
feature fixing helps nsga ii achieve better correct and hv but the majority of solutions remain useless due to violations of the model constraints.
this confirms previous findings by the authors regarding the superiority of a continuous measure of domination as in ibea over absolute dominance used in nsga ii see subsection ii i.c .
ibea with feature fixing achieves remarkable results for six models th e numbers highlighted in bold .
for two of these six models toybox and uclinux the percentage of correct solutions is and respectively.
when considering that the final pareto front is composed of individuals corresponds to fully correct solutions and means valid solutions.
this is remarkable as well compared to nsga ii or ibea without feature fixing.
ibea without feature fixing achieved high correct with two models ecos and freebsd .
when feature fixing was used the tt50 i ndicator showed a faster growth of correct configurations while the hv indicator showed an improvement in the overall optimality of solutions.
some cases show a lower hv value when the correct value is improved.
this means that when the number of violat ions is high the other objectives take closer to optimum values which would not be useful because of the rule violations.
b. using a pre computed correct solution as seed to ibea the results in the previous part are encouraging but it s clear that ibea n eeds more assistance to achieve acceptable configurations for the linux model within reasonable time.
our next innovative technique was to pre compute a correct configuration and plant it like a seed in the initial population of the evolutionary algorithms .
the intuition behind this was that the randomly generated members of the initial population are highly likely to violate thousands of feature model rules and be punished for that in the fitness assignments.
when an individual in the initial population st ands out as a fully correct solution then it should be promoted more often than others for crossover with other individuals and would survive through successive generations due to elitism.
thus the seed acts as a role model to the chaotic members of the population.
this technique proved to be useful as we will see next.
first we present two different ways of pre computing a correct solution using the z3 smt solver.
z3 takes the dimacs formula as input and outputs the first correct solutions that it finds.
this technique is fast but it tends to produce correct solutions with a low number of s i.e.
a low number of selected features.
using objective optimization with ibea where one objective is to minimize rule violations while the other object ive is to maximize the number of selected features.
this technique can be time consuming for very large feature models but it produces more selected features.
table v shows the time it took each of these two techniques to generate a correct solution and the number of selected features within that solution.
notice that the ecos model accepts the zero feature solution which definitely is a bug in that formula.
to show the benefit of the seeding technique we apply ibea with feature fixing and seeding to the linux x86 feature model with the full optimization objectives.
three different kinds of seeds are tried separately one feature rich seed generated using objective ibea along with random solutions.
thirty different fully correct seeds generated in a previous run of objective ibea along with random solutions.
one low feature seed generated using z3 smt solver along with random solutions.
table iv.
results for ibea and nsga ii with and without feature fixing optimization objectives model nsga ii without feature fixing nsga ii with feature fixing ibea without feature fixing ibea with feature fixing correct tt50 hv correct tt50 hv correct tt50 hv correct tt50 hv toybox .
n a .
.
n a .
.
n a .
n a .
axtls .
n a .
.
n a .
.
n a .
.
ecos .
n a .
n a .
.
.
freebsd .
n a .
.
n a .
.
.
fiasco .
n a .
n a .
.
n a .
.
uclinux .
n a .
n a .
.
n a n a .
linux n a n a n a n a .
a correct percentage of correct solutions.
tt50 time to achieve correctness in seconds .
hv hypervolume.
b each cell reports the median value for independent runs each run for minutes.
table v. generating a correct solution using methods model total features using z3 using obj ibea time sec selected features time sec selected features toybox .
.
axtls .
.
ecos .
freebsd .
fiasco .
uclinux .
linux x86 .
hours fig.
.
number of valid configurations over time ibea objectives with seeding linux x86 feature model figure shows the growth of the number of correct solutions over time for all three types of seeds.
the results for the one feature rich show t hat correct solutions are available after minutes and such solutions are found after hour.
compare this to the result in table iv which shows zero correct solutions for all runs of minutes each.
for the seeds planted along with ra ndom solutions the population had fully correct configurations at seconds but the number dropped as the objective optimization continued down to valid solutions at minutes and then back up to after hour.
this shows that the outcome o f carefully selected feature rich seed is compatible with that of seeds.
the quality not quantity of seeds had the most influence in the ability to scale up ibea s optimization ability to the feature linux model .
for the low feature seed obta ined with z3 smt solver the result was disappointing no correct solutions were found for the first hour.
we attribute this failure to the scarcity of selected features in the z3 solution features only .
a solution with so small number of s would e asily be overrun by other incorrect individuals in the population through the crossover process.
in addition this correct individual is assigned a low fitness value due to the low number of features which decreases its likelihood of survival.
the feat ure rich seed obtained with objective ibea had selected features and thus was able to influence other individuals and stay ahead in the fitness evaluation.
vi.
discussion a. method innovation i s key to scalability a traditional view about the scalability of evolutionary algorithms is that the technology i.e.
cpu power ram needs to catch up with the algorithms since the population based evolutionary methods require large amounts of ram to store the primary population and the archive resulting from cross over and mutation and cpu power would help finish the computations within reasonable time.
multicore cpus would allow for the parallelization of execution which is an important proper ty of population based methods.
our experience as reported in this paper was that large memory and fast cpus were not enough to handle the size and complexity of the very large linux model features .
it took hours for the objective optimization p rocess to find any valid configurations and more hours to find a significant set of valid solutions that are closer to optimality.
the innovation in method the seeding technique was our key to scalability.
one feature rich valid seed in the midst of a member initial population was enough to generate valid configurations within minutes.
a larger set of seeds did not help in improving the result which hinted that the careful selection of seeds was more effective than increasing their quantity .
one effective seed acted like the proverbial straw that broke the camel s back .
b. impact of the scale up result on interactive configuration configuration of a software package is an interactive process during which the users would make initial choices an d then seek advice from the optimizer and then make more choices and so forth.
the user s choices can be in the decision space e.g.
select an optional feature select an option from a group or they can be in the objective space e.g.
specifying range for cost maximum acceptable risk .
the more choices the user makes the less complex the search space becomes and the fa ster the optimizer can respond.
the advantage of pareto optimal solutions is that they offer a range of options rather than a unique optimal solution.
thus the user would be more informed and enabled in the configuration process.
the breakthrough that we achieved in this paper via the seeding technique enables jump starting the configuration of features by offering valid opti ons within the first minute see the tri angles in fig.
.
those options are not just valid but they dominate a host of other candidates in the pareto sense although they don t represent the absolute optimal pareto front.
the user can choose to begi n making configuration decisions that early in the process.
the optimizer takes the user s input which narrows down the search space and build s on the candidates achieved so far and turn s around with more good candidates that cater to the user s prefere nces.
the seed which is pre computed offline serves as an accelerator to the interactive configuration process.
c. evolutionary learning still rules without the remarkable result we presented in this paper the slow convergence toward correct solutions may tempt us to abandon meoas and go directly to theorem provers find all 10000valid configurations time sec one feature rich seed correct seeds one low feature seedpossible product variants and evaluate them all.
such approach may be feasibl e with small and simple spls but would not be scalable to large and complex ones such as the linux feature model and the run times become prohibitive.
evolutionary methods and especially ibea are still the best way to optimize with many objectives and vast decision spaces.
navigating the decision space with the aid of heuristics and simultaneously evaluating a population of candidates has proven to outperform exhaustive search over many years of research.
this trend should continue to scale up with the help of innovative techniques that inhibit the randomness of exploration and nudge the optimizer towards respecting domain constraints.
the seeding trick is one such helper.
d. be careful with problem formulation many objective optimiz ation is a paradigm shi ft that forces researchers to reformulate traditional problems in order to bring out the various objectives and map out a pareto front.
the same problem can have different formulations according to which parameters the researcher chooses to bring out as in dependent dimension s. in a recent survey we found that most researchers only examined two objective formulations of their problems.
but we also found several examples of the same researchers add ressing different formulations of the same problems by varying the number of objectives .
the most interesting problem formulation would have a maximum number of objectives among which there are minimal correlations.
such formulation would challenge the multiobjective optimizer to find the set of best trade offs among competing objectives.
on the other hand if the formulation separates two objectives among which there s a high correlation then the optimization takes a monolithic direction with the solutions crowding in the same area that tends to optimize both objectives at the same time .
a case in point is our objective formulation of the software configuration problem.
we seek to minimize violations maximize features minimize newly developed features not used before minimize known defects and minimize cost.
some have looked at the second objective and questioned its merit does the user really seek to maximize the number of features in a product?
our answer takes a holis tic look at the goal of our optimization to provide the user with a wide range of pareto optimal solutions that explore as many feature configuration choices as possible and then let the user make their own decisions.
if the feature richness objective is removed the other four objectives would push the solutions toward minimizing the number of features since that area of the decision space tends to decrease violations new features known defects and cost.
such formulation would defeat the overall purpose of offering a diverse set of valid configurations.
e. confirming ibea s advantage in many objective problems the results of this experiment confirm the findings of earlier work by the authors and by wagner et al.
regarding the superiority of ibea over other pareto based algorithms such as nsga ii in high dimensional objective spaces.
this is attributed to ibea s fitness assignment strategy which heavily factors in the user preferences whereas pareto based methods rely on absolute dominance as primary fitness criterion and diversity as secondary c riterion which tend to ignore differences in quality that ibea is able to capture.
f. building on the seeding approach in light of the limited success of the seeding technique w e suggest the use of a pre computed set of seeds a set of correct solutions given that it is diverse in the amount of selected features.
the more selected features the better chance there will be of promoting correct solutions in the many objective optimization problem.
furthermore since the z3 smt solver can arrive at valid solut ions much faster than the objective ibea we will try to create the desired set of seeds using z3.
vii.
threats to validity in the first part of the results i.e.
the feature fixing technique we repeated each algorithm run times for each of the models.
we performed the mann whitney test and found significant improvements in the correct indicator for out of models.
this should be sufficient to eliminate a potential threat to conclusion validity.
as for the second part of the results i.e.
the seeding technique we didn t validate the findings with the same level of repeats and statistical testing.
we plan to do so in future work as we explore the proper characterization of the effective seeds and reduce the time needed to generate them.
a potential th reat to construct validity is the use of synthetic data as attributes of features i.e.
cost defects and used before.
the use of synthetic data is common in software engineering literature.
the difficulty of obtaining real data comes from the fact that such data are usually associated with software components not features.
when available such data is often proprietary and not published.
nevertheless the results we obtained have such a larg e margin of superiority achieved by ibea with feature fixing over other methods which couldn t possibly be biased by the synthetic data.
viii.
conclusion and future work this experiment explored the scalability of optimum 5objective product configuration using ibea for very large feature models .
for models with less than features ibea was able to achieve the goal within reasonable time only with the help of feature fixing.
as for the linux kernel a feature model we were able to achieve valid conf igurations within minutes with the help of an innovative population seeding technique.
one pre computed feature rich solution was enough to influence the rest of the population into learning valid solutions faster.
future work will focus on characterizi ng the quality and quantity of the best seed which would be most influential in helping ibea to converge faster to large amounts of valid solutions.
in addition the z3 smt solver will be utilized to generate the desired set of seeds in shorter times than possible using evolutionary methods.
acknowledgment this research work was funded by the qatar national research fund qnrf under the national priorities research program nprp grant no.
.
additional funding by the national science foundati on nsf grant no.
ccf .