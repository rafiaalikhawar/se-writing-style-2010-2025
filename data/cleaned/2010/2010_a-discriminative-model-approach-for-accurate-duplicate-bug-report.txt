singapor e management univ ersity singapor e management univ ersity institutional k nowledge at singapor e management univ ersity institutional k nowledge at singapor e management univ ersity resear ch collection school of computing and information systems school of computing and information systems a discriminativ e model appr oach for accur ate duplicate bug a discriminativ e model appr oach for accur ate duplicate bug repor t retrie val repor t retrie val chengnian sun david l o singapor e management univ ersity davidlo smu.edu.sg xiao yin w ang siau cheng khoo follow this and additional works at https ink.libr ary.smu.edu.sg sis r esear ch part of the softwar e engineering commons citation citation sun chengnian l o david w ang xiao yin and khoo siau cheng.
a discriminativ e model appr oach for accur ate duplicate bug r epor t retrie val.
.
proceedings of the 32nd acm ieee international conf erence on softwar e engineering cape t own south africa ma y .
.
available at available at https ink.libr ary.smu.edu.sg sis r esear ch this conf erence pr oceeding ar ticle is br ought t o you for fr ee and open access b y the school of computing and information systems at institutional k nowledge at singapor e management univ ersity .
it has been accepted for inclusion in resear ch collection school of computing and information systems b y an authoriz ed administr ator of institutional k nowledge at singapor e management univ ersity .
for mor e information please email cher ylds smu.edu.sg .
see discussions stats and author profiles for this publication at a discriminative model approach for accurate duplicate bug report retrieval conference paper in proceedings international conference on software engineering january .
.
source dblp citations reads authors including some of the authors of this publication are also working on these related projects tark a tool kit to mine linear temporal rules view project recommender systems view project david lo singapore management university publications citations see profile xiaoyin wang university of california berkeley publications citations see profile siau cheng khoo national university of singapore publications citations see profile all content following this page was uploaded by siau cheng khoo on june .
the user has requested enhancement of the downloaded file.a discriminative model approach for accurate duplicate bug report retrieval chengnian sun1 david lo2 xiaoyin wang3 jing jiang2 siau cheng khoo1 1school of computing national university of singapore 2school of information systems singapore management university 3key laboratory of high confidence software t echnologies peking university ministry of education suncn comp.nus.edu.sg davidlo smu.edu.sg wangxy06 sei.pku.edu.cn jingjiang smu.edu.sg khoosc comp.nus.edu.sg abstract bug repositories are usually maintained in software projects.
testers or users submit bug reports to identify various issueswith systems.
sometimes two or more bug reports corre spond to the same defect.
to address the problem with du plicate bug reports a person called a triager needs to manually label these bug reports as duplicates and link them to their master reports for subsequent maintenance work.however in practice there are considerable duplicate bug re ports sent daily requesting triagers to manually label these bugs could be highly time consuming.
to address this issue recently several techniques have be proposed using various similarity based metrics to detect candidate duplicate bug reports for manual verification.
au tomatingtriaginghasbeenprovedchallengingastworeports of the same bug could be written in various ways.
there is still much room for improvement in terms of accuracy of duplicate detection process.
in this paper we leverage recent advances on using discriminative models for information retrieval to detect duplicate bug reports more accurately.
we have validated our approach on three large software bug repositories from firefox eclipse and openoffice.
we show that our technique could result in and relativeimprovementoverstate of the arttechniquesin openoffice firefox and eclipse datasets respectively using commonly available natural language information only.
categories and subject descriptors d. .
distribution maintenance and enhancement general terms management reliability .
introduction permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.due to complexities of systems built software often comes with defects.
software defects have caused billions of dollars lost .
fixing defects is one of the most frequent reasonsfor software maintenance activities which also goes to 70billion us dollars in the united states alone .
in order to help track software defects and build more reliable systems bug tracking tools have been introduced.
bugtracking systems like bugzilla 1enable many users to serve as testers and report their findings in a unified environ ment.
these bug reports are then used to guide software corrective maintenance activities and result in more reliable software systems.
via the bug tracking systems users areable to report new bugs track statuses of bug reports andcomment on existing bug reports.
despite the benefits of a bug reporting system it does cause some challenges.
as bug reporting process is oftenuncoordinated and ad hoc often the same bugs could bereported more than once by different users.
hence there isoftenaneedformanualinspectiontodetectwhetherthebug has been reported before.
if the incoming bug report is not reported before then the bug should be assigned to a developer.
however if other users have reported the bug before then the bug would be classified as being a duplicate and attached to the original first reported master bug report.
this process referred to as triaging often takes much time.
for example for the mozilla programmers it has been reported in that everyday almost bugs appear thatneed triaging.
this is far too much for only the mozilla programmers to handle .
in order to alleviate the heavy burden of triagers there have been recent techniques to automate the triaging process in two ways.
the first one is automatically filteringduplicates to prevent multiple duplicate reports from reaching triagers .
the second is providing a list of similar bug reports to each incoming report under investigation with the help rather than checking against the entire collection of bug reports a triager could first inspect the top k most similar bug reports returned by the systems.
if there is a report in the list that reports about the same de fect as the new one then the one is a duplicate the triagerthen marks it as a duplicate and adds a link between the two duplicates for subsequent maintenance work.
in our paper instead of filtering duplicates we choose the second approach as duplicate bug reports are not necessarily bad.
as stated in one report usually does not carry enough information for developers to dig into the reported defect may cape town south africa.
copyright acm ... .
.
45while duplicate reports can complement one another.
to achieve better automation and thus save triagers time it is important to improve the quality of the ranked list of similar bug reports given a new bug report.
there have been several studies on retrieving similar bug reports.
however the performance of these systems is still relative low makingit hard to apply them in practice.
the low performance ispartly due to the following limitations of the current methods.
first all the three techniques in employ one or two features to describe the similarity between re ports despite the fact that other features are also available foreffectivemeasurementofsimilarity.
second differentfeatures contribute differently towards determining similarities.
for example the feature capturing similarity between summaries of two reports are more effective than that between descriptions as summaries typically carry more concise information.
however as project contexts evolve the relative importance of features might vary.
this can cause the past techniques which are largely based on absolute rating ofimportance to deteriorate in their performance.
more accurate results would mean more automation and less effort by triagers to find duplicate bug reports.
to address this need we propose a discriminative model based approach that further improves accuracy in retrieving duplicate bug reports by up to on real bug report datasets.
different from the previous approaches that rank similar bug reports based on similarity score of vector space representation we develop a discriminative model to retrievesimilar bug reports from a bug repository.
we make useof the recent advances in information retrieval community that uses a classifier to retrieve similar documents from a collection .
we build a model that contrasts duplicatebug reports from non duplicate bug reports and utilize thismodel to extract similar bug reports given a query bug re port under consideration.
we strengthen the effectiveness of bug report retrieval system by introducing many more relevant features to cap ture the similarity between bug reports.
moreover with the adoption of the discriminative model approach the relative importance of each feature will be automatically determined by the model through assignment of an optimum weight.
consequently as bug repository evolves our discriminative model also evolves to guarantee the all the weights remainoptimum at all time.
in this sense our process is more adaptive robust and automated.
we evaluate our discriminative model approach on three large bug report datasets from large programs including firefox anopensourcewebbrowser eclipse apopularopen source integrated development environment and openoffice a well known open source rich text editor.
in termsof the range of types of programs considered for evaluation to the best of our knowledge we are the first to investi gate the applicability of the approach on different types of systems.
we show that our technique could result in and improvement over state of the art techniques in openoffice firefox andeclipse datasets respectively using commonly available natural language information alone.
we summarize our contributions as follows .
we employ in total features to comprehensively evaluate the similarity between two reports.
.
we propose a discriminative model based solution toretrieve similar bug reports from a bug tracking system.
our model can automatically assign optimumweighttoeachfeatureandevolvealongwiththechangesof bug repositories.
.
we are the first to analyze the applicability of duplicate bug report detection techniques across different sizable bug repositories of various large open source programs including openoffice firefox and eclipse.
.
weimprovetheaccuracyofstate of the artautomated duplicate bug detection systems by up to on dif ferent open source datasets.
the paper is organized as follows.
section presents some background information on bug reports informationretrieval and discriminative model construction.
section presents our approach to retrieving similar bug reports for duplicate bug report detection.
section describes our case study on sizable bug repositories of different open source projects and shows the utility of the proposed approach in improving the state of the art detection performance.
sec tion discusses some important consideration about our ap proach.
section discusses related work and finally section concludes and describes some potential future work.
.
background in general duplicate bug report retrieval involves information extraction from and comparison between documents in natural language.
this section covers the necessary back ground and foundation techniques to perform the task in our approach.
.
duplicate bug reports a bug report is a structured record consisting of several fields.
commonly they include summary description project submitter priority and so forth.
each field carries a different type of information.
for example summary is a concise description of the defect problem while description is the detailed outline of what went wrong and how it happened.
both of them are in natural language format.
otherfields such as project priority try to characterize the defect from other perspectives.
in a typical software development process the bug tracki n gs y s t e mi so p e nf o rt e s t e r so re v e nf o ra l le n du s e r s s oi tis unavoidable that two people may submit different reportson the same bug.
this causes the problem of duplicate bug reports.
as mentioned in duplicate reports can be divided into two categories.
one describes the same failure and the other depicts two different failures both originated from the same root cause.
in this paper we only handle the first category.
as an example table shows three pairs of duplicate reports extracted from issue tracker of openoffice.
only the summaries are listed.
usually newbugreportsarecontinuallysubmitted.
when triagers identify that a new report is a duplicate of an old one the new one is marked as duplicate.
as a result given a set of reports on the same defect only the oldest one in the set is not marked as duplicate.
we refer to the oldest one as masterand the others as its duplicates .
abugrepositorycouldbeviewedascontainingtwogroups of reports masters and duplicates.
since each duplicate must have a corresponding master and both reports are on the same defect the defects represented by all the duplicates 46table examples of duplicate bug reports id summary no scrolling of document content by use of the mouse wheel unable to scroll in a note with the mouse wheel 85502alt letter does not work in dialogs 85819alt key no longer works as expected 85487connectivity evoab2 needs to be changed to build against changed api 85496connectivity fails to build evoab2 in m4 in the repository belong to the set of the defects representedby all the masters.
furthermore typically each master re port represents a distinct defect.
.
information retrieval information retrieval ir aims to extract useful information from unstructured documents most of which are expressed in natural language.
ir methods typically treatdocuments as bags of words and subsequently represent them in a high dimensional vector space where each dimen sion corresponds to a unique word or term.
in this paper weuse term and word interchangeably.
the following describes some commonly used strategies to pre process documents and methods to weigh terms.pre processing.
in order to computerize retrieval task a sequence of actions should be taken first to preprocess documents using natural language processing techniques.
usually this sequence comprises tokenization stemming andstop word removal.
a word token is a maximum sequenceof consecutive characters without any delimiters.
a delim iter in turn could be a space punctuation mark etc.
tokenization is the process of parsing a character stream into a sequence of word tokens by splitting the stream by the de limiters.
stemming is the process to reduce words to theirgroundforms.themotiv ationtodosoisthatdifferen tforms of words derived from the same root usually have similarmeanings.
by stemming computers can capture this simi larity via direct string equivalence.
for example a stemmercan reduce both tested and testing to test .
the last action is stop word removal.
stop words are those words carrying little helpful information for information retrieval task.
these include pronouns such as it he and she link verbs such as is am and are etc.
in our stop wordlist in addition to removing common stop words we also drop common abbreviations such as i m that s we ll etc.
term weighting.
tf idf term frequency inverse document frequency is a common term weighting scheme.
it is a statistical approach to evaluating the importance of a term in a corpus.
tf is a localimportance measure.
given a term and a document in general tf corresponds to the numberoftimesthetermappearswithinthedocument.
dif ferent from tf idf is a globalimportance measure most commonly calculated by the formula within the corpus idf term log dall dterm in dallis the number of the documents in the corpus while dtermis the number of documents containing the figure maximum margin hyperplane calculatedby svm in two dimensional space term.
given a term the fewer documents it is contained in the more important it becomes.
in our approach we employanidf based formula to weigh terms.
.
building discriminative models via svm support vector machine svm is an approach to building a discriminative model or classifier based on a set oflabeled vectors.
given a set of vectors some belonging to a positive class and others belonging to a negative class svm tries to build a hyperplane that separates vectors belongingto the positive class from those of the negative class withthe largest margin.
figure shows such kind of a hyperplane built by svm with the maximum margin in a two dimensional space.
the resultant model could then be used to classify other unknown data points in vector representa tion and label them as either positive or negative.
in thisstudy we use libsvm a popular implementation of svm.
.
our approach duplicate bug report retrieval can be viewed as an application of information retrieval ir technique to the domain of software engineering with the objective of improving productivity of software maintenance.
in classical re trieval problem the user gives a query expressing the infor mation he she is looking for.
the ir system would then return a list of documents relevant to the query.
for duplicate report retrieval problem the triager receives a newreport and inputs it to the duplicate report retrieval systemas a query.
the system then returns a list of potential du plicate reports.
the list should be sorted in a descending order of relevance to the queried bug report.
our approach adopts recent development on discriminative models for information retrieval to retrieve duplicate bug reports.
adapted from we consider duplicate bug report retrieval as a binary classification problem that is given a new report the retrieval process is to classify all ex isting reports into two classes duplicate and non duplicate.we compute types of textual similarities between reportsand use them as features for training and classification purpose.
the rest of this section is structured as follows subsection .
gives a bird s eye view of the overall framework.
sub section .
explains how existing bug reports in the repository are organized.
sub section .
elaborates on how a discriminative model is built.
sub section .
describeshow the model is applied for retrieving duplicate bug reports.
finally sub section .
describes how the model is updated when new triaged bug reports arrive.
47figure overall framework to retrieve duplicate bug reports figure bucket structure .
overall framework figure shows the overall framework of our approach.
in general there are three main steps in the system preprocessing training a discriminative model andretrieving duplicate bug reports .
the first step preprocessing follows a standard natural language processing style tokenization stemming and stop words removal described in sub section .
.
the second step training a discriminative model trains a classifier to answer the question how likely are two bug reports dupli cates of each other?
.
the third step retrieving duplicate bug reports makes use of this classifier to retrieve relevant bug reports from the repository.
.
data structure allthereportsintherepositoryareorganizedintoabucket structure.
the bucket structure is a hash map like datastructure.
each bucket contains a master report as the keyand all the duplicates of the master as its value.
as ex plained in sub section .
different masters report differentdefects while a master and its duplicates report the same defect.
therefore each bucket stands for a distinct defect while all the reports in a bucket correspond to the same de fect.
the structure of the bucket is shown diagrammaticallyin figure .
new reports will also be added to the structure after they are labeled as duplicate or non duplicate by triagers.
if a new report is a duplicate it will go to thebucket indexed by its master otherwise a new bucket willbe created to include the new report and it becomes a mas ter.
.
training a discriminative model given a set of bug reports classified into masters and duplicates we would like to build a discriminative model ora classifier that answers the question how likely are two input bug reports duplicate of each other?
.
this question is essential in our retrieval system.
as described in subsec tion .
the answer is a probability describing the likelihoodof these two reports being duplicate of each other.
when a new report comes we ask the question for each pair between figure training a discriminative model the new report and all the existing reports in the repository and then retrieve the duplicate reports based on the probability answers.
to get the answer we follow a multi step approach involving example creation feature extraction anddiscriminative model creation via support vector machines svms .
the steps are shown in figure .
based on the buckets containing masters associated with corresponding dupli cates we extract positive and negative examples.
positive examples correspond to pairs of bug reports that are duplicates of each other.
negative examples correspond to pairs of bug reports that are not duplicates of each other.
next a feature extraction process is employed to extract features from the pairs of bug reports.
these features must be richenough to be able to discriminate between cases where bugreports are duplicate of one another and cases where they are distinct.
these feature vectors corresponding to duplicates and non duplicates are then input to an svm learningalgorithm to build a suitable discriminative model.
the fol lowing sub sections describe each of the steps in more detail.
.
.
creating examples to create positive examples for each bucket we perform the following .
create the pair master duplicate where duplicate is one of the duplicates in the bucket and master is theoriginal report in the bucket.
.
create the pairs duplicate duplicate where the two duplicates belong to the same bucket.
to create negative examples one could pair one report from one bucket with another report from the other bucket.the number of negative examples could be much larger thanthe number of positive examples.
as there are issues relatedto skewed or imbalanced dataset when building classification models c.f.
we choose to under sample the negative examples thus ensure that we have the same number ofpositive and negative examples.
at the end of the process we have two sets of examples one corresponds to examples of pairs of bug reports that are 48duplicates and the other corresponds to examples of pairs of bug reports that are non duplicates.
.
.
feature engineering extraction at times limited features make it hard to differentiate between two contrasting datasets in our case pairs that areduplicates and pairs that are non duplicates.
hence a richenough feature set is needed to make duplicate bug reportretrieval more accurate.
since we are extracting features corresponding to a pair of textual reports various textualsimilarity measures between the two reports are good feature candidates.
in our approach we employ the followingformula as the textual similarity.
sim b b2 summationdisplay w b1 b2idf w in sim b1 b2 returns the similarity between two bags of words b1andb2.
the similarity is the sum of idfvalues of all the shared words between b1andb2.t h eidfvalue for each word is computed based on a corpus formed from allthe reports in the repository which will be detailed furtherbelow.
the rational why the similarity measure does notinvolve tf is that the measure with only idf yields better performance indicated by fisher score which will be detailed in sub section .
and validated by the experiments.
generally each feature in our approach can then be abstracted by the following formula f r r2 sim words from r words from r from a feature is actually the similarity between twobags of words from two reports r 1andr2.
oneobservationisthatabugreportconsistsoftwoimportant fields summary and description.
so we can get three bags of words from one report one bag from summary one from descriptionandonefrom both summary description .
to extract a feature from a pair of bug reports for example one could compute the similarity between the bag of wordsfrom the summary of one report and the words from the description of the other.
alternatively one could use the similarity between the words from both the summary and description of one report and those from the summary of the other.
other combinations are also possible.
furthermore we can compute three types of idf a st h e bug repository can form three distinct corpora.
one corpus is the collection of all the summaries one corpus is the collection of all the descriptions and the other is the collection of all the both summary description .
we denote the three types of idfcomputed within the three corpora by idf sum idfdesc a n didfbothrespectively.
the output of function fdefined in depends on the choice of bag of words for r1 the choice of bag of words for r2and the choice of idf.
considering each of the combinations as a separate feature the total number of differentfeatures would be which is equal to .
figure shows how the features are extracted from a pair of bug reports.
aside from considering words we also consider bigrams two consecutive words.
with bigrams considering different combinations of bag of words coming from and idfcomputed based on summaries d escriptions or both we would have another features which would then bring the number of features extracted to .algorithm calculate candidate reports for q procedure proposecandidatesinput q a new report rep the bug repository n the expected size of candidate list output result al i s to f nmasters of which q is a likely duplicate body candidates an empty min heap of maximum size n for each bucketb buckets rep do similarity predictbucket q b master b .similarity similarity add master b t ocandidates end for sortcandidates in descending order of field similarity returnsortedcandidates algorithm calculate similarity between q and a bucket procedure predictbucketinput q a new report b a bucket output max the maximum similarity between qand each report ofb body max tests f54 q r r reports b for each feature vector t testsdo probability svmpredict t max max max probability end for returnmax .
.
training models before training an extra action is taken to normalize the values of all features in the training set to within the range .
this is to avoid the case that some features in the bigger range dominate those in the smaller range.
we use astandardalgorithmtonormalizethefeaturevectors c.f.
.the same normalization will also be applied when the resul tant model is used for classifying unknown pairs.
we use libsvm to train a discriminative model from the training set.
as suggested by we choose the linearkernel for svm as it is efficient and effective in performingclassification for information retrieval.
a typical classifier would only give binary answers in our case whether two bug reports are duplicate or not.
we are notinterested in binary answers rather we are interested in knowing howlikely two bug reports are duplicates.
to do this we enablethe probability estimation functionality of libsvm to train a discriminative model which is able to produce a probability of two bug reports being duplicates of each other.
.
applying models for duplicate detection when a new report qarrives we can apply the trained model to retrieve a list of candidate reports of which qis likely a duplicate.
the retrieval details are displayed in algorithm and algorithm .
algorithm returns a duplicate candidate list for a new 49figure feature extraction first features report.
in general it iterates over all the buckets in the repository and calculates the similarity between the new report and each bucket.
at last a list of masters whosebuckets have the biggest similarity is returned.
in line buckets rep returns all the buckets in the bug repository andmaster b i nl i n e4i st h em a s t e rr e p o r to fb u c k e t b. the algorithm makes a call to algorithm which computesthe similarity between a report and a bucket of reports.
tomake the algorithm efficient we only keep top ncandidate buckets in memory while the buckets in the repository are being analyzed.
this is achieved by a minimum heap of maximum size n.i ft h es i z eo f candidates is less than n new bucket will be directly added.
when the size equals ton if the new bucket whose similarity is greater than the minimum similarity in candidates it will replace the bucket with the minimum similarity otherwise the requestof adding the bucket will be ignored by candidates.
given a new report qand a bucket b algorithm returns the similarity between qandb.
this algorithm first creates candidate duplicate pairs between qand all the reports in the bucket b line .
each pair is represented by a vector of features which is calculated by the function f54.
the trained discriminative model is used to predict the probability for each candidate pair.
finally the maximum probability between qand the bug reports in bis returned as the similarity between qandb.reports b denotes all the reports in b. the operation f54 q r r e turns a vector of the similarity features from the pair ofbug reports qandr including features based on single words and features based on bigrams described in sub section .
.
.
the procedure svmpredict in line is the invocation to the discriminative model and it returns aprobability in which the pair of reports the feature vector t corresponds to are duplicates of each other.
.
model evolution as time passes new bug reports will come and be triaged.
this new information could be used as new training data toupdate the model.
users could perform this process period ically or every time after a new bug report has been triaged.in general such newly created training data should be use ful.
however we find that such information is not alwaysbeneficial to us for the following reason our retrieval modelis based on lexical similarity rather than semantic similarity of text.
in another word if two bug reports refer to the same defect but use different lexical representations i.e.words then our retrieval model does not give a high similar ity measure to this pair of bug reports.
therefore includinga pair of duplicate bug reports that are not lexically similar in the training data would actually bring noise to our trained classifier.
we therefore consider the following twokinds of update .
light update.
if our retrieval engine fails to retrieve the right master for a new report before the triager marks the report as a duplicate we perform this update.
when the failure happens the new bug reportcould syntactically be very far from the master.
for this case we only perform a light update to the model by updating the idfscores of the training examples and re training the model.
.
regular update.
we perform this update if our retrieval engine is able to retrieve the right master for the new duplicate bug report.
when this happens the new bug report is used to update the idfand to create new training examples.
an updated discriminative model is then trained based on the new idfand training examples.
via our experiments we have empirically validated that theaboveheuristicsworkeffectivelyinimprovingthequality of the model after updating.
.
case studies weapplyourdiscriminativemodelapproachtothreelarge bug repositories of open source projects openoffice firefox and eclipse.
for comparison purpose we also implemented the algorithms in to the best of our knowledge.to evaluate the performance of different approaches we em ployed the notion of recall rate defined in .
recall rate n detected ntotal shows how recall rate is calculated.
ndetectedis the number of duplicate reports whose masters are successfully 50detected while ntotalis the total number of duplicate reports for testing the retrieval process.
given a candidate list size the recall rate can be interpreted as the percentage of duplicates whose masters are successfully retrieved inthe list.
in terms of this measure the result shows that ourapproach can bring remarkable improvement over the otherthree approaches.
.
experimental setup in our experiment we used the bug repositories of three large open source projects the eclipse project the firefox project and the openoffice project.
among the three projects eclipse is a popular open source integrated devel opmentenvironmentwritteninjava firefoxisawell knownopen source web browser written in c c and openoffice is an open source counterpart of microsoft office.
these three projects are from different domains written in different languages and used by different types of users.
thus carrying out the experiment on them helps to general ize our conclusions.
on top of that all of the three projects havelargebugrepositoriessoastoprovideampledataforan evaluation.
weselectedasubsetofeachrepositoryincludingboth defect reports and feature requests within a period oftime to set up an experimental bug set in our study.
specifically we use the bug report set submitted to openoffice in year including bug reports the bug reportset of eclipse in year including bug reports to evaluate our approach.
furthermore to study how our training based approach works in the long run in case the property of bug reports change over time we further evaluated our approach on the whole bug report set of firefox including bug reports submitted since firefox was started in before june as a long run evaluation set.
table gives the details of the three datasets.
we refer to thetimeperiodofthedatasetsas time frame asthesecond column time frame displays.
to run our approach we select the first mreports of which reports are duplicates as training set to train a discriminative model.
the thirdcolumn of training reports in the table shows the ratio of duplicates and all reports in the training set.
besides of serving as a training set in our approach those mreports are also used to simulate the initial bug repository for allexperiment runs.
selecting reports within a time frame introduces a problem.
if a duplicate report r 1is within the time frame while i t sm a s t e ri sn o t t h e n r1is not detectable as its master is not in the dataset which will only decrease the recall rate.for this case we simply re mark r 1as non duplicate.
however there is a more complex case.
suppose there are two ormore duplicate reports on the same defect within the frame while their common master is excluded from the dataset.
if we still simply mark them as non duplicate we will lose twoor more duplicates.
so in this case we first make the oldestreportr oldestbecome the master report and then mark the others as duplicates of roldest.
.
experimental details and result weevaluatetheperformanceofourapproachascompared to previous techniques over the three datasets.
the previousapproaches come with a parameter setting corresponding tothe weight being assigned to the words from the summaryand those from the description of the bug reports.
theyconsider two weights one is equal weight between the sum mary and description second is summary carries doubleweight.
also in the three approaches they consider a non bucketbased retrieval where relevant bug reports are retrieved rather than relevant buckets.
as one bucket contains bug reports corresponding to the same defect we believe it is bestto consider bucket based retrieval.
report based retrievalcould potentially return more than one report referring to the same defect in the list of candidate duplicate bug reports causing redundant effort for the triager.
also in theauthors use both ir and execution trace to detect duplicatereports.
asexecutiontraceishardto get forexistingreportsand especially for our large datasets we only compare based on textual summary and description of the bug reports.
in the three datasets training reports are used as initial bug repository.
they are also used to construct the training set and further to build a discriminative model.
at each experimental run we iterate over the testing reports in chronological order.
once reaching a duplicate report r we apply the corresponding technique to get a top nlist of r s potential master reports.
after each detection is done we record the result whether r s master is detected successfullyforfuture recallrate calculation andthenadd rtothe repository.
after the last iteration is over the recall rates for different top list size are calculated.
figure shows the experiment results of the seven runs on the three datasets.
in the figure the horizontal axis isthe top n list s size and the vertical axis is the recall rate jstands for rstands for and wis .
suffix or represents weighing summaries or .
ocorresponds to our approach.
from the three sub figures we can easily come to the first conclusion that our technique brings evidence of improvement.
we have relative improve ment in openoffice dataset in firefox dataset and35 in eclipse dataset.
the second conclusion is that manually empirically weighing summaries for traditional ir techniques can help improve limited performance as the sixcurves near the bottom of each sub figure are very close toone another.
compared to other six runs the improvement achieved in our run is due to the similarity features to compre hensively measure how similar two reports are and the use of discriminative model to automatically assign weights to each feature to discriminate duplicate reports from nonduplicate ones which is rigorous and has theoretical support from machine learning area.
.
discussion this section will first discuss issues related to runtime overhead followed by the rationale behind the choice of the similarity scores as features rather than other types of similarity scores.
.
runtime overhead there is no free lunch.
with the big increase of recall rates the runtime overhead of our detection algorithm isalso higher than past approaches.
in the largest datasetfirefox in the experiment run of our approach the initialcost to detect a duplicate report is one second.
however as the training set continually grows and the repository gets increasingly large the detection cost also increases over time.when the experiment considers detecting the last duplicate 51table summary of datasets dataset time frame training reports duplicate all testing reports duplicate all openoffice jan dec firefox apr jul eclipse jan dec a openoffice b firefox c eclipse figure recall rates comparison between various techniques with certain top list size bug report the cost becomes oneminute.
the major overhead is due to the fact that we consider different similarity features between reports.
in contrast previous works consider the similarity between sum mary description and summary description and con siders the similarity between summary and summary and the similarity between description and description.
the runtime overhead is higher at the later part of the experiment asthe number of bug report pairs in the training set is larger.consequently svm will need more time to build a discrim inative model.
however a higher runtime overhead does not mean thatthisapproachisnotpractical.
infact itisacceptableforrealworld bug triaging process for two reasons.
first we haveexperimented with real world datasets.
the dataset fromfirefox spans from april to july and contains more than reports in total.
for an active softwareproject consideringreportsinone yearframeisenoughas bug reports are usually received for new software releases.although the eclipse dataset with reports is withinone yeartimeframe itcontainsmultiplesub projects which meansthat it canbesplit into smallerdatasets.
second new bugreportsdonotcomeeveryminute.
inourthreedatasets the average frequency of new report arrival for openofficeis .
reports hour the one for firefox is report hour and the one for eclipse is reports hour.
therefore our system still has enough time to retrain the model before processingan e wb u gr e p o r t .
.
feature selection feature selection is the process to identify a set of features which can bring the best classification performance.
a com monly used metric for feature selection is fisher score defined in the following formula f r summationtextc i 1ni i summationtextci 1ni 2 i whereniis the number of data examples in class i iis the average feature value in class i iis the standard deviation of the feature value in class i a n d is the average feature value in the whole dataset.
assume xijis the attribute value for the jth instance in class i then iand iare defined as summationtext i summationtext jxij summationtext ini i summationtext jxij ni i radicalbig summationtext j xij i ni respectively.
the higher the fisher score the better is the feature for classification.
in our approach we use idf based formulas to calculate similarity features between two bug reports.
one mightask why tfortf idfmeasures are not used in the formulas.
the reason to choose an idf only solution is that this setting yields the best performance during our empirical validation.
to further demonstrate that idfis a good measure we replace the idfmeasure in the features with tfand tf idfmeasure respectively.
we then calculate the fisher score of each of the features using idf tf a n dtf idf.
theresultsoneclipsedatasetshowsthat idf basedformulas outperform tf based and tf idf b a s e df o r m u l a si nt e r m s of fisher score.
this supports our decision in choosing the idf based formulas as feature scores.
.
related work one of the pioneer studies on duplicate bug report detection is by runeson et al.
.
their approach first cleanedthetextualbugreportsvianaturallanguageprocessingtechniques tokenization stemming and stop word removal.
52the remaining words were then modeled as a vector space where each axis corresponded to a unique word.
each re port was represented by a vector in the space.
the value of each element in the vector was computed by the following formula on the tfvalue of the corresponding word.
weight word log tf word after these vectors were formed they used three measures cosine diceandjaccard tocalculatethedistancebetweentwo vectors as the similarity of the two corresponding reports.
given a bug report under investigation their system would return top k similar bug reports based on the similar ities between the new report and the existing reports in therepository.
a case study was performed on defect reports at sony ericsson mobile communications which showed that the tool was able to identify of duplicate bug reports.
it also showed that cosine outperformed the other two similarity measures.
in wang et al.
extended the work by runeson et al.
in two dimensions.
first they considered not only tf but idf.
hence in their work the value of each element in avector corresponded to the following formula weight word tf word idf word second they considered execution information to detect duplicates.
a case study based on cosine similarity measure on a small subset of bug reports from firefox showed that theirappraoch could detect of duplicate bug reports byutilizing both natural language information and executiontraces.
in jalbert and weimer extended the work by runeson et al.
in two dimensions.
first they proposed a newterm weighting scheme for the vector representation weight word log tf word the cosine similarity was adopted to extract top k similar reports.
aside from the above they also adopted clustering and classification techniques to filter duplicates.
similarities and differences.
similar to the above three studies we address the problem of retrieving similar bug reports from repositories for duplicate bug report identification.
similar to the work in we only consider natural language information which is widely available.
the execution information considered in is oftennot available and hard to collect especially for binary programs.
for example in the openoffice firefox and eclipse datasets used in our experiment the percentages of reportshaving execution information are indeed low .
.
and respectively .
also for large bug repositories creation of execution information for legacy reports can be time consuming.
compared to the three approaches there are generally three differences.
first to retrieve top k similar repots they used similarity measure to compute distances between reports while we adopt an approach based on discriminative model.
we train a discriminative model via svms toclassify whether two bug reports are duplicates of one otherwith a probability.
based on this probability score we retrieve and rank candidate duplicate bug reports.
what is more the approach in also employed a classifier trainedby svms but for a different purpose.
their classifier wasused to return a boolean flag of duplicate or new fora new report if the flag was duplicate the new reportwould be filtered and would not reach triagers.
they re ported that of the duplicate reports could be filtered intheir experiment.
the second difference is that we introduce54 features out of which are based on bigrams to better discriminate duplicate bug reports.
then the trained discriminative model can automatically infer optimum weightsfor each feature.
this mechanism enables our approach tobe robust to bad features adaptive to report repository evo lution over time and effective for different software projects.
finally instead of returning similar bug reports we return similar buckets.
as each bucket represents a distinct defect our approach can easily avoid two or more reports in thetop k similar report list referring to the same defect.
from the point of performance view we show that our discriminative model approach outperforms all previous ap proaches using natural language information alone by up to43 on bug report repositories of three large open sourceapplications including firefox eclipse and openoffice.
besides the effort on duplicate bug report detection there has also been effort on bug report mining.
anvik et al.
and cubranic and murphy and lucca all proposedsemi automatic techniques to categorize bug reports.
based on categories of bug reports their approaches helped assign bug reports to suitable developers.
menzies and marcus alsosuggested a classification based approach to predicting theseverity of bug reports .
bettenburg et al.
proposed awork on extracting structural information including stack traces patches and codes from the descriptions of the bug reports .
ko and myers analyzed the linguistic character istics of bug report summaries and proposed a technique todifferentiate between failure reports and feature calls .
they also emphasized on the need of duplicate bug report detection but did not propose a solution.
while the aboveworksmentionedtheneedofduplicatebugdetectionorsomeoftheirtechniquesmaybenefitduplicatebugdetection noneofthemworkeddirectlyontheduplicatebugdetectionproblem.
there have been several statistical studies and surveys of existing bug repositories.
anvik et al.
reported a statistical study on open bug repositories with some interesting results such as the proportion of different resolutions and the number of bug reports that a single reporter submit ted .
sandusky et al.
studied the relationships betweenbug reports and reported some statistic results on duplicatebugs in open bug repositories .
additionally hooimeijer and weimer suggested a statistics based model to predict the quality of bug reports .
after that bettenburg et al.made a survey on the developers of several well known opensource projects eclipse mozilla and apache to study the factors that developers cared most on dealing with bug reports .
bettenburg et al.
also suggested that duplicatebug reports were actually not harmful but useful for thedevelopers .
so the requirement for duplicate bug reportdetection became even stronger because it could not only reduce the waste of developer s time on duplicate bug reports but also helped developers to gather more related informa tion to solve the bug more quickly.
in general none of thesework proposed any approaches to duplicate bug report detection but some of the work pointed out the motivation and effect of detecting duplicate bug reports.
.
conclusion future work in this work we consider a new approach to detecting 53duplicate bug reports by building a discriminative model that answers the question are two bug reports duplicatesof each other?
.
the model would report a score on the probability of a and b being duplicates.
this score is then used to retrieve similar bug reports from a bug report repos itory for user inspection.
we have investigated the utilityof our approach on sizable bug repositories from largeopen source applications including openoffice firefox and eclipse.
the experiment shows that our approach outperforms existing state of the art techniques by a relative im provement of and on openoffice firefox and eclipse dataset respectively.
as a future work we plan to investigate the utility of paraphrases in discriminative models for potential improve ment in accuracy.
we have developed a technique to extract technical paraphrases and is currently investigating their utility in improving detection of duplicate bug reports.
what is more an interesting direction is incorporating response threads to bug reports as further sourcesof information.
the other interesting direction is adoptingpattern based classification to extract richer feature set that enables better discrimination and detection of duplicate bug reports.
.