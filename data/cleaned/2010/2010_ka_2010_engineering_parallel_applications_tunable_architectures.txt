engineering parallel applications with tunable architectures christoph a. schaefer karlsruhe institute of technology ipd am fasanengarten karlsruhe germany cschaefer ipd.uka.devictor pankratius karlsruhe institute of technology ipd am fasanengarten karlsruhe germany pankratius ipd.uka.dewalter f .
tichy karlsruhe institute of technology ipd am fasanengarten karlsruhe germany tichy ipd.uka.de abstract current multicore computers di er in many hardware characteristics.
software developers thus hand tune their parallel programs for a speci c platform to achieve the best performance this is tedious and leads to non portable code.
although the software architecture also requires adaptation to achieve best performance it is rarely modi ed because of the additional implementation e ort.
the tunable architectures approach proposed in this paper automates the architecture adaptation of parallel programs and uses an auto tuner to nd the best performing software architecture for a particular machine.
we introduce a new architecture description language based on parallel patterns and a framework to express architecture variants in a generic way.
several case studies demonstrate signi cant performance improvements due to architecture tuning and show the applicability of our approach to industrial applications.
software developers are exposed to less parallel programming complexity thus making the approach attractive for experts as well as inexperienced parallel programmers.
categories and subject descriptors d. .
concurrent programming parallel programming d. .
software architectures patterns general terms performance design languages .
introduction performance tuning of parallel applications is a challenging task on today s multicore computers because they di er in a variety of ways e.g.
in the number of cores per chip memory bandwidth cache architecture or employed operating systems .
being under pressure to deliver the best parallel program performance many software developers are permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
icse may cape town south africa copyright acm ... .
.forced to hand tune their programs for certain platforms.
this approach is tedious costly and may lead to non portable code that has to be re tuned on new machines .
recent studies have shown that recompiling programs with appropriate instruction level optimizations does not always lead to acceptable performance on new platforms and that the parallel program architecture also needs adaptation.
in addition some situations require reordering of nested parallel components.
yet programmers are hesitant to do such invasive changes on new machines due to the required implementation e ort.
we propose tunable architectures to address these problems for shared memory parallel programs on multicore platforms.
in principle developers make the performance relevant parts of a program architecture con gurable and prepare the program for experiments.
the experiments are done by an auto tuner this is an external program that systematically tests a program with di erent architecture variants on a particular machine to nd the best performing variant.
auto tuning is a feedback directed process consisting of several steps choice of architecture con guration program execution monitoring of execution time and generation of a new con guration based on optimization strategies such as hill climbing or simulated annealing.
in this process the software architecture converges to the best performing architecture on a given target platform.
to realize tunable architectures we propose a novel architecture description language that has operators to express parallel programming patterns such as pipelines producer consumer or fork join.
developers start with atomic software components that contain executable code.
the operators are used to compose a parallel program out of atomic software components.
by default all patterns have an associated set of performance parameters e.g.
number of producers for the producer consumer pattern and an auto tuner is supposed to choose appropriate values out of a user de ned value range.
the language also allows specifying one or more architecture variants that compose a parallel program in di erent ways these are the variants that are tested by an auto tuner.
tunable architectures advance earlier work in two important ways they provide the technical means to express performance relevant architecture variations needed speci cally for parallel programs and they automate the architecture optimization process for parallel programs on contemporary shared memory multicore machines.
our case studies with realistic c programs showthat the approach works and that the language is expressive enough to implement a wide range of parallel programs.
we also achieved signi cant performance improvements while keeping code portable.
at the same time developers had less exposure to the complexity of parallel programming and performance tuning which makes the approach attractive not only for experts but also for less experienced parallel programmers.
the paper is organized as follows section motivates why we need tunable architectures for multicore parallel programs.
section introduces our tunable architecture description language tadl an application of the language is illustrated by examples in section .
in section we describe the implementation concepts for con gurable parallel programs and present a full edged tadl compiler that creates executable multicore applications.
in section four case studies including an industrial application evaluate our approach in di erent contexts.
section discusses related work and we present our conclusions in section .
.
why we need tunable architectures developing parallel applications is di cult when software engineers need to pay attention to low level concurrency details such as explicit thread management or synchronization.
it is well known that this way of parallel programming is error prone.
although it might appear that programmers are able to achieve better performance through a ne grained control of parallelization programmers are overwhelmed in large programs by the amount of performance data and by the details of thread interleaving or locking protocols.
complex parallel programs are di cult to tune because code changes may have unforeseen non local e ects on correctness and performance.
in addition predicting overheads for parallel computations is di cult which forces many programmers to manually experiment with code changes and observe their performance impact.
many parallel programs not only have parallelization potential on low abstraction levels e.g.
at instruction level but also at an architectural level.
such an architecture typically contains several layers of nested parallelism .
for illustration let s think in a top down fashion of a data analysis application processing data packets in a pipeline with several stages.
inside a stage input packets of a particular stage could be processed in a master worker fashion.
workers in turn could be replicated if they are stateless and they could work on a disjoint partition of a packet.
this example shows that we can have di erent types of parallelism such as pipeline parallelism task parallelism and data parallelism in one single application.
the example also shows that there are many opportunities to con gure the program architecture in di erent ways to in uence performance how many pipeline stages are needed on the highest abstraction level?
how many workers should be created?
which load balancing strategy works best?
how many disjoint data partitions lead to best performance?
earlier studies have shown that parallel program architecture can have a major impact on performance and that the software architecture adaptation to a target platform can have a higher performance leverage than pure instruction level optimizations.
these adaptations however are rarely done because they require invasive code changes.programmers are also hesitant to modify complex parallel code being afraid of introducing bugs.
motivated by these observations our tunable architectures approach makes parallel application architecture easily con gurable and exploits architecture inherent parallelism.
the developer implements the program s work items that is what to parallelize and speci es in our architecture description language how these items are processed in parallel.
thread management and synchronization are done implicitly based on the operators used to compose a parallel program out of work items.
we describe next the language constructs for tunable architectures in greater detail.
.
the tunable architecture description language thetunable architecture description language tadl speci es all architecture variants for a parallel program that should be tested by an auto tuner.
we developed a formal grammar speci cation and an antlr parser implementation however we omit the full presentation of the grammar because of its length.
we use examples instead to explain the available constructs syntax and semantics.
we also show how the architecture constructs are related to program code.
we start with the key language constructs used to specify atomic components andconnectors .
.
atomic components anatomic component represents a piece of elementary sequential work i.e.
the component has no internal parallelism .
developers need to think of pieces of work in terms of atomic components rather than threads.
an atomic component is implemented by a method.
although we used c in our case studies our concepts are also realizable with other languages e.g java .
the method code associated to an atomic component manipulates individual data items this eases the exploitation of potential data parallelism.
if methods do not share states it is easy for a run time system to replicate their functionality and create copies working in parallel.
our language provides a keyword to mark such atomic components as replicable.
an atomic component in the architecture description language has an identifying name that also establishes a relation to a c method providing the implementation.
the de nition of an atomic component has the pre x ac followed by the respective method name and an optional replicable attribute ac mymethodname the method associated to an atomic component can contain arbitrary code.
however atomic components are intended to run concurrently.
therefore it is the developer s responsibility to write correct code.
in particular the developer has to take care of potential side e ects.
shared data structures used by two or more atomic components should be synchronized to ensure the expected behavior.
our prototype implementation currently does not check code correctness or the proper use of data structures that are not part of the tunable architecture.
when an application is written from scratch developers need to create all necessary atomic components rst.
we describe next how to glue them together using patterns.
.
connectors a tunable architecture has a tree structure with nodes ofconnectors and leaves of atomic components.
in contrast to the de nition of connectors in architecture description languages where a connector handles the communication between two components a tadl connector de nes an entire processing strategy as well as the corresponding interactions for its child items.
the tadl keyword of a connector encloses a block of child items that can be either atomic components or connectors.
there are ve connector types sequential composition.
thesequential composition connector provides sequential processing semantics for components.
the language construct encloses one or more child items to be executed one after the other.
the sequential composition describes parts of the program that are not executed concurrently.
sequential composition requires all child items to have compatible input types and output types.
tunable alternative.
thetunable alternative connector expresses an exclusive choice between two or more enclosed child items the auto tuner may pick any of them for a particular program execution.
all child items have individual means for input and output.
tunable fork join.
thetunable fork join connector introduces task parallelism the sequential control ow is forked to execute all enclosed child items in parallel and joined back to sequential after all child items are nished.
the child items are not supposed to interact with each other so there are no restrictions on input types and output types.
every child item is assumed to have individual means for input and output.
tunable producer consumer.
thetunable producerconsumer connector has exactly two child items the producer and the consumer .
it is used for synchronization and the developer does not have to care about details e.g.
bu er creation synchronized access signalling because they are handled by our prototype implementation.
this connector has streaming semantics the producer is designed to accept data from an enumerable data source.
after processing a data element the producer passes it on to the consumer.
the semantics require that the consumer s input type must match the producer s output type.
tunable pipeline.
thetunable pipeline connector introduces pipeline parallelism.
it has two or more child items representing chained stages.
the connector also has streaming semantics the rst stage accepts data from a source with enumerable data elements and the last stage passes all data elements to a sink.
a stage that nished its work on a particular data element passes it on to the next stage.
we have carefully selected the aforementioned connector types to cover widely used parallelization strategies and program structures.
with the sequential composition and tunable alternative connector tadl supports the description of component sequences and exclusive component choices respectively.
the connectors for parallelization tunable fork join tunable producer consumer tunable pipeline and the replicable atomic components express data task and pipeline parallelism.all connectors have a similar syntax which we exemplify for a tunable pipeline connector with two stages.
the rst stage consists of an alternative which means that an autotuner can try out a program with a pipeline with ac mymethod1 in the rst stage and ac mymethod3 in the second stage or with ac mymethod2 in the rst stage and ac mymethod3 in the second stage tunablepipeline mytunablepipeline tunablealternative ac mymethod1 ac mymethod2 ac mymethod3 .
.
handling input and output every atomic component can be associated to an input component and an output component.
an input component provides input data e.g.
from a data source or the command line an output component takes data to process e.g.
stores it on disk prints the results on screen or modi es a data structure .
similar to atomic components input and output components have an associated method with code to handle input or output respectively.
input and output components are de ned and matched recursively for child items of a connector.
for sequential composition tunable alternative and tunable fork join connectors input and output components need to be de ned for every child item.
by contrast the tunable producerconsumer and the tunable pipeline connector have only one source and one sink.
input and output components are de ned for child items that are enclosed by one of the aforementioned connectors and the relation between input and output components and corresponding method code is similar to atomic components.
in addition all method names of input and output components are listed in an array the rst method in this array is supposed to handle the input of the rst child item the second method for the second child item and so on.
if child item doesn t need inputs its corresponding array position contains the null keyword.
as an example consider a tunable fork join connector with two child items ac mymethod1 andac mymethod2 that are executed in parallel.
ac mymethod1 has a component for both input and output and ac mymethod2 has a component to produce output but needs no input.
tunableforkjoin input ac inputmethodfor1 null output ac outmethodfor1 ac outmethodfor2 ac mymethod1 ac mymethod2 for tunable producer consumer and tunable pipelines i o routines are not speci ed for every stage but only once for the entire pattern as shown in the previous example with the tunable pipeline.
.
examples for tunable architectures as a proof of concept we fully implemented several parallel c applications that are all running on multicore computers.
we discuss in greater detail the tunable architecture of two of these applications and sketch other applications in section .
.
parallel video processing in a bottom up approach we implement a parallel video processing application that applies a sequence of lters on each bitmap image of a video stream.
the implementation starts with methods for atomic components and input and output i.e.
one method for every lter one method for loading the original video and one method for processing a manipulated image.
listing outlines the method signatures.
public ienumerable bitmap loadvideo f. .
.g public bitmap crop bitmap bmp f. .
.g public bitmap oilpaint bitmap bmp f. .
.g public bitmap resize bitmap bmp f. .
.g public bitmap sharpen bitmap bmp f. .
.g public void consumevideo bitmap bmp f. .
.g listing atomic component methods of the video processing application.
the four lter methods i.e.
crop oilpaint resize and sharpen represent the associated implementation for all our atomic components whereas getvideo and consume represent the associated implementation of an input and and output component i.e.
a source and a sink for a pipeline .
the components are assembled to a parallel program using the tunable architecture description shown in listing .
to process the algorithms in a pipelined fashion we introduce a tunable pipeline connector with source and sink components handling input and output.
the stages reference the respective lter method implementations.
as all lters are stateless that is the processing of a particular bitmap does not depend on any previously processed bitmaps we mark the child items of the tunable pipeline connector with the replicable keyword to potentially exploit data parallelism.
tunablepipeline myvideoprocessing source ac loadvideo sink ac consumevideo f accrop acoilpaint acresize acsharpen g listing architecture description of the parallel video processing application.
this example exploits data parallelism and pipeline parallelism.
we discuss the performance results in section .
.
parallel desktop search we implement a parallel desktop search engine in c that crawls les on a local hard disk creates an inverted indexdata structure that relates all document words to les on disk and which allows user input to query all les containing speci ed words.
the implementation starts with the methods shown in listing public list string crawl f. .
.g public searchresult stringsearch1 string path f. .
.g public searchresult stringsearch2 string path f. .
.g public void updateindex searchresult r f. .
.g public void createindexfile index i f. .
.g public list string query string keywords f. .
.g public string getkeywords f. .
.g public void showresults list string r e s u l t s f. .
.g listing atomic component methods of the parallel video processing application.
the methods implement the following component functionality 2ac crawl crawls the folders and retrieves all le paths.
2ac stringsearch1 splits a text into words using a delimiter character.
2ac stringsearch2 implementation of the knuth morris pratt kmp string search algorithm .
while the rst string search algorithm performs a naive search with moderate overhead the kmp algorithm employs a smarter search but with slightly more overhead e.g.
storing word indices .
as we don t know in advance which of the algorithms performs better on certain platforms we let the auto tuner decide.
2ac updateindex updates the le index data structure in memory.
2ac createindexfile stores index on disk.
2ac query performs a query to nd all les that contain all keywords speci ed by the user.
2ac getkeywords retrieves a list of keywords from command line.
2ac showresults prints the results of a query to the command window.
the tunable architecture shown in listing illustrates how to de ne two architecture variants for this application desktopsearch1 and desktopsearch2 tunablealternative desktopsearchalternatives f sequentialcomposition desktopsearch1 input null ac getkeywords output null ac showresults f tunablepipeline source ac crawl sink ac createindexfile f tunablealternative f acstringsearch1 acstringsearch2 g acupdateindex g acquery g sequentialcomposition desktopsearch2 input null ac getkeywords output null ac showresults f tunableproducerconsumer source ac crawl sink ac createindexfile f tunablealternative f acstringsearch1 acstringsearch2 g acupdateindex g acquery g g listing architecture description of the parallel desktop search application.
to realize a parallel indexing process consisting of a string search algorithm and the index update method we can either use a tunable pipeline or a tunable producer consumer connector.
however we do not know which strategy performs better on a particular hardware platform.
according to the parallel pattern de nitions in the pipeline processes the data elements one by one that is each stage processes one item and then gets the next whereas the producer consumer strategy allows the consumer to fetch more than one item at a time.
consequently a pipeline stage has less waiting time after processing a data element however the consumer can process a batch of data elements without synchronizing after each element.
using a tunable alternative connector we de ne two architectural variants and let the auto tuner decide which one performs better.
in both alternatives we start with a sequential composition connector to ensure the indexing process is nished before the program accepts search queries.
the indexing process in the rst alternative employs a tunable pipeline connector calling the ac crawl source to obtain les to be parsed and the ac createindexfile sink to store processed words.
there are two stages one for searching and one for index updating with ac updateindex .
the searching stage can have one of the two speci ed string search algorithms i.e.
ac stringsearch1 and ac stringsearch2 .
we add ac query as the second child item to the sequential composition connector.
the second architecture alternative is similar to the rst one except that we de ne a tunable producer consumer connector.
we use the same source and sink methods i.e.
ac getkeywords and ac showresults .
this example illustrates data parallelism pipeline parallelism and the de nition of performance relevant architecture variants.
the performance results are presented in section .
we remark that other architecture variants can be de ned to do parallel indexing at di erent granularity levels e.g.
les or chunks of words .
this is important for parallel programs as a varying granularity of processing in uences the program overheads and the performance on di erent machines.
apart from the architecture declared above we can think of other scenarios that provide even more exibility when using the application.
for example we can use a tunable fork join connector instead of a sequential composition connector to allow queries to be performed while indexing is in progress.
this can be useful if indexing takes a long time but the user wants to perform searches as soon as possible on preliminary versions of the index.
however if querying and indexing run concurrently additional synchronization is required that could lead to a slowdown of the indexing process.
to instruct the auto tuner to nd the best variant we would replace one of the sequential composition connectors with a tunable fork join connector.
if the index data structure is synchronized we could even use the same method implementation for ac query .
.
implementation techniques the implementation of a parallel program with tunable architectures needs library and tool support.
we present next the tunable architecture library the tadl compiler and the automatic architecture tuner .
we also illustrate their usage in the software development process.
.
the tunable architecture library thetunable architecture library talib contains modules with implementations for every tadl connector for example the talib contains the code to implement the tunable pipeline connector.
in addition each such module implements a set of prede ned tuning parameters that in uence performance.
table shows all tuning parameters.
the values of these parameters are chosen by an auto tuner out of an associated set of values.
the talib modules also handle exceptions that might have been thrown by atomic components.
this means that a parallel pattern handles the exceptions of its enclosed child items.
each talib module collects all exceptions thrown by its child items and re throws them as one aggregated exception.
.
the tadl compiler thetadl compiler processes scripts written in the tunable architecture description language to generate source code.
compilation consists of two steps .
a preprocessor extracts all atomic component declarations from the tadl script.
it uses re ection to establish the bindings between methods and their atomic component declarations.
.
the compiler analyzes the architecture description translates it to an internal representation checks type consistency for connectors generates the source code and integrates the code into a nal executable program.
.
.
tuning wrappers the tadl compiler generates code organized in tuning wrappers .
a tuning wrapper is implemented as a class that initializes a particular talib module and implements access methods to that module.
the tadl compiler generates a tuning wrapper for each connector in the tadl script.
for example the compiler translates a construct de ning a tunable pipeline connector into a tuning wrapper that handles the access to the talib s tunable pipeline module.
a tuning wrapper also contains code to create an instance of the corresponding talib module e.g.
an instance of a tunable pipeline connector .
the wrapper has elds for the module s tuning parameters e.g.
a fork join s number of worker threads and methods to connect child items of a connector to its talib module implementation.
the wrapper also implements the methods for input and output data handling and exposes a run method executing the wrappedconnector tuningparameters tunable alternative choice of alternative de nes which alternative is executed tunable fork join num worker threads number of threads the component can use tunable producer consumer buffer size size of the central bu er between producer and consumer batch size number of data elements the consumer grabs at once tunable pipeline stage fusion for each supporting stage enables or disables stage fusion replication num instances number of replicated instances of the atomic component load balancing de nes how the data elements are assigned to the instances batch size number of data elements an instance grabs at once table prede ned tuning parameters of tadl connectors.
talib module e.g.
the tunable pipeline .
the entry point for the execution of the tunable architecture is the run method of the tuning wrapper that implements the architecture s root component.
.
.
tuning instructions tuning wrappers just provide interfaces to the variables to tune.
the auto tuner however needs to be able to change variable values and get performance feedback.
to achieve this we instrument the source code with atune il which is apragma based tuning language.
we refer to our previously published work for details .
atune il provides constructs to declare program variables as tuning parameters and to de ne measuring points within the program to return execution time feedback to the autotuner.
in addition atune il o ers block constructs to de ne the scope of tuning parameters this is important for search space reduction.
atune il provides a compact representation of several program variants.
before the auto tuner starts the atune il statements are replaced by appropriate code fragments such as variable assignments and calls to performance libraries.
in a tuning wrapper the tadl compiler instruments the variables declaring tuning parameters sets measuring points at prede ned positions to determine the wrapper s execution time and encloses the wrapper class with a block statement.
.
the automatic architecture tuner theautomatic architecture tuner auto tuner performs an automatic search based optimization and uses atune il instrumentations to steer the process.
we adapted our autotuner from previously published work to support tunable architectures.
auto tuning is a cyclical feedbackdirected process .
the tuner extracts the atune il instrumentations from all tuning wrappers in the program and builds a data structure containing the required tuning information.
in addition the tuner builds up a tuple based multi dimensional search space based on all parameters value ranges.
each tuple represent a particular parameter con guration.
.
starting from the current parameter con guration we use empirical search algorithms to traverse the search space and nd a new parameter con guration.
the selection of parameter con gurations is based on exchangeable search algorithms.
depending on the complexity of the search space we employ adapted versions of hillclimbing swarm optimization and random sampling.
.
a new executable program is generated in which instrumentations are removed.
atune il placeholders are replaced by concrete values.
time measuring points are replaced by calls to an instrumentation run time.
.
the tuner executes the new program variant and monitors it.
the instrumentation run time records aggregates and stores data from all measuring points.
.
the recorded monitoring results are gathered and prepared for further processing.
the tuning cycle steps is repeated until some prede ned termination condition holds this depends on the chosen search algorithm.
.
.
search space reduction one of the main problems of search based auto tuning is the explosion of the search space.
in the worst case an autotuner might try out the cartesian product of all parameter domains which grows exponentially in the number of parameters.
if the search space is very large even a smart search algorithm would require a long time to nd a good parameter con guration.
to tackle this problem the automatic architecture tuner takes advantage of the semantics of the tadl connectors.
for search space partitioning it uses the knowledge about sequential composition and tunable alternative connectors.
both ensure that their child items will never run concurrently.
thus tuning parameters exposed in a particular child item or in its sub tree will never interfere with parameters of other child items.
from a tuning perspective the sub trees of sequential composition and tunable architecture are independent.
we call independent sub trees tuning entities as the auto tuner can optimize these sub trees separately.
considering the tree structure of tunable architectures the search space can be split into smaller parts.
figure conceptually illustrates such a situation.
if an architecture tree contains a sequential composition connector in a particular place we can assign each of its sub trees to a separate tuning entity.
we thus obtain two signi cantly smaller parts of the search space.
instead of tuning the parameters of all components below the sequential composition connector together the auto tuner can optimize each tuning entity separately.
this applies in a similar way for the tunable alternative connector.
for each of the remaining connectors the auto tuner applies a particular tuning heuristic that de nes an optimization process for a connector.
using tuning heuristics the... sub tree sub tree n tuning entity tuning entity nchild connector child connector nsequential compositionfigure exploiting tree structure of tunable architectures to create tuning entities.
auto tuner exploits context knowledge about the connectors parallelization strategies and performs a guided search.
for example let s think of a tunable pipeline connector with replicable child items.
instead of simply testing a subset of all parameter con gurations the auto tuner tries to balance pipeline stages by increasing the number of threads for replicable child items with long executions times or by decreasing the number of threads for short running child items.
we refer to our earlier work for details .
.
how to create an auto tuned parallel application the implementation process using tunable architecture is illustrated in figure along with the sequence of steps a developer has to do.
figure entire process of creating a parallel autotuned application.
.
a developer programs the methods associated to atomic components ac .
.
a tunable architecture is de ned in a tadl script that speci es the composition of atomic components to a complete program.
.
the tadl compiler is invoked with and as inputs.
.
the tadl compiler produces code les containing tuning wrappers the wrappers are instrumented with atune il statements.
each wrapper handles the access to the corresponding talib module.
.
the developer inserts in his or her program a call to the tuning wrapper that implements the architecture s root component.
this call is typically inserted in the program s main method.
.
the completion of step produces an executable but not yet optimized parallel program.
this program version represents an intermediate portable tuning template.
.
the automatic architecture tuner optimizes the parallel program for performance.
it hooks on to the atune il instrumentations that are interfaced in the tuning wrappers.
.
the completion of the previous step produces a parallel program optimized on a target platform.
if the program is migrated to another platform the developer repeats the process starting at step .
.
case studies to evaluate tunable architectures we conducted four case studies with parallel applications written in c one of which is a re engineered industrial application.
we discuss the context and experimental results next.
.
parallel applications .
.
video processing this application applies four di erent lters on each frame of an avi audio video interleave format video.
it uses the tunable architecture described earlier by the tadl script in listing .
the tadl compiler generates the standard tuning parameters for the pipeline as well as for each of the replicable atomic components.
in the script just six lines de ne the entire architecture.
the tadl compiler produces a con gurable parallel program that is ready to execute.
as a performance benchmark we used an avi video consisting of frames with a resolution of 800x600 pixels.
.
.
desktop search this application indexes the words contained in a set of text les and allows queries on the index to return all les that contain all words in given list.
the indexing performs most work and is the most relevant part for parallelization.
the desktop search engine s tunable architecture is the one shown earlier in listing .
it also illustrates the use of alternatives.
we de ned two tunable alternative connectors to instruct the auto tuner to test di erent variants of the architecture as well as of particular algorithms.
this application context demonstrates the exchangeability of the connectors and the de nition of variants on each architecture level.
the performance benchmark for this case study consisted of ascii text les with le sizes ranging between 9kb and kb.
.
.
biological data analysis agilent s metaboliteid mid is a large commercial application with more than .
lines of code for biological data analysis we parallelized this sequential application using tunable architectures.
metaboliteid identi es metabolites in mass spectrograms which is a key method for drug testing.
metabolism is the set of chemical reactionstaking place within cells of a living organism and metaboliteid compares mass spectrograms to identify the metabolites caused by a particular drug.
the metaboliteid application executes several algorithms in sequence abbreviated sc eic adc uv btl ipm pcs fmsc fpm and mfa that identify and extract the metabolite candidates.
compared to the previous case studies mid already existed in a sequential version.
we show in this case study that tunable architectures can be used to re engineer sequential programs to parallel programs and tune the performance of the parallel program more easily.
listing illustrates the tunable architecture for metaboliteid.
we declared the methods implementing the aforementioned algorithms as atomic components.
we then de ned the parallel processing architecture that includes three tunable fork join connectors two of which are nested and the innermost fork join executes two replicable atomic components in parallel.
we had access to documentation and communicated with metaboliteid s developers to ensure that the new order of algorithms still produces valid results.
in the compact description of our program architecture we de ned ve parallel sections and exploited parallelism on three di erent application layers.
each of the tuning wrappers generated by the tadl compiler for the connectors were already instrumented with prede ned tuning parameters they were used by an auto tuner to reduce the search space based on additional information about the nesting structure of the program see section .
.
for example the two tunable fork join connectors within the outer sequential composition connector are tuned separately because they don t in uence each other.
sequentialcomposition midf acrunpreprocessing tunableforkjoin f acrunsc acruneic g tunableforkjoin f acrunadc acrunuv acrunmdf acrunbtl ipm sequentialcomposition f acrunpcs fmsc tunableforkjoin input ac fpminput ac mfainput output ac fpmoutput ac mfaoutput f acrunfpm acrunmfa g g g acrunpostprocessing g listing architecture description of the parallel version of mid.
a manual implementation of a parallel program with the same functionality as ours would have required more effort in particular because nested parallel components require careful attention to thread management synchronization and locking protocols.
the input les for the performance benchmark were gb in size and contained the data representing the mass spectrograms to compare.
.
.
graph rewriting grgen is the currently fastest sequential graph rewriting system .
in this case study we simulated the biological gene expression process on the e.coli bacteria dna .
the model of the dna is represented as an input graph with more than million nodes.
this is the second study to re engineer a sequential application for parallelism using tunable architectures.
grgen has two performance bottlenecks that both provide potential for massive data parallelism.
we declared the respective methods promoterseacrch and rnapoly as replicable atomic components in our tunable architecture.
listing shows the parallel grgen s tadl script.
the two atomic components must be executed one after another so they are enclosed by a sequential composition connector.
we also declared methods to handle input and output.
sequentialcomposition grgen input ac promoterinput ac rnapolyinput output ac promoteroutput ac rnapolyoutput f acpromotersearch acrnapoly g listing architecture description of the parallel version of grgen.
the tunable parallel version of grgen could be de ned with just a few architecture script lines.
the tadl compiler automatically generated tuning wrappers for the replicable atomic components the compiler also generated prede ned tuning parameters for the patterns as well as code instrumentations for feedback to the auto tuner.
.
experimental results we present performance results for each case study application that were obtained by our automatic architecture tuner.
we then explain how programming e ort was reduced and how tunable architectures helped reduce the potential of parallel programming errors.
.
.
performance for performance evaluation we are interested in two performance metrics 2best obtained speedup.
this is the ratio between the execution time of the program s optimized parallel version and the execution time of the sequential version.
it expresses how much faster the parallel program is compared to the sequential one.
2tuning performance gain tpg .
this is the difference between the worst and the best speedup obtained by our auto tuner.
this metric indicates the impact of tuning the parallel program that is how much more the performance of the parallel program could be improved by tunable architectures and autotuning.
all experiments were done on an eight core machine with two intel xeon e5320 quadcore cpus clocked at .
ghz with gb ram and running windows server 32bit.
figure shows the performance results for the worst and the best speedup best spd.
andworst spd.
resp.
andsummarizes the resulting tpg.
the execution times of the sequential versions were seconds for the video application hours and minutes for the desktop search application seconds for metaboliteid and seconds for grgen.
video ds mid grgen worst spd.
best spd.
tpg 0speedup figure performance results of all case studies.
the results show that signi cant speedups could be indeed be achieved by architecture level performance tuning so it pays o for performance to use tunable architectures.
for the desktop search application the auto tuner has nally chosen the best architecture based on the tunable producerconsumer connector and the kmp algorithm.
the tuning performance gain results show that auto tuning is worthwhile.
based on our experience a manual performance optimization leads to results somewhere within the range of the tpg but rarely close to the maximum this is because it is di cult to gure out intuitively what the best parameter con guration is.
.
.
programming effort we compare the approach with tunable architectures with a manual approach without tunable architectures implementing the same functionality.
we compare the programming e ort and analyze the potential for parallel programming errors.
in particular we look at the following metrics 2lines of code loc .
we use the loc metric as a measure correlated to the implementation e ort required to manually re build the functionality we provide in our language compiler and library.
2number of synchronization primitives syncs .
thread synchronization is one of the most di cult tasks in parallel programming.
we interpret the number of saved synchronization primitives as an indication for reduced error potential.
2number of tuning instrumentations.
instrumentations are necessary for automatic performance tuning.
the saved instrumentation statements in our approach indicate how much easier tuning becomes with tunable architectures.
to estimate the average number of loc and synchronization primitives required for the manual implementation weassume average values based on our experience from previous experiments the implementation of a tunable pipeline connector requires loc syncs a replicable component loc syncs a tunable fork join connector loc syncs a tunable producer consumer connector loc syncs and a tunable alternative connector loc.
we do not consider sequential composition connectors as they contain neither parallelism nor tuning options.
figure lists the particular numbers for each of our case study applications.
video proce ssingdesktop searchmid grgen man.
impl.
ta based reduction man.
impl.
ta based reduction man.
impl.
ta based reduction loc synchr onizatio ns1 tuning instrumentation statemen ts includes all synchr onization pr imitiv es such as lock notify wait join etc.
figure reduction of loc synchronization and instrumentation.
the comparison shows that our approach saves a large number of loc this helps reducing the overall application development e ort.
in addition almost no manual thread synchronization and tuning instrumentations are necessary with tunable architectures this reduces the potential for parallel programming errors.
tunable architectures do not overly in ate program code and introduce modest complexity in exchange for automation.
this is supported by the class coupling metric and the total loc of the nal application.
with tunable architectures the average number of class couplings for all our case study applications increases by the number of classes by and the total loc by just compared to the original program version.
.
related work architecture description languages adls have been explored for sequential programs and the recommendations of are also useful in our context.
the adls discussed in such as c2ordarwin support the description of distributed systems but do not o er means to describe adaptable parallelism within shared memory programs.
most related work addresses parallel programming for distributed memory environments e.g.
the web and pays little attention to parallel programming patterns or performance tuning for shared memory computers recently available multicore computers are di erent because they have a shared memory architecture and a di erent programming model.
this is similar for work in autonomous computing architecture adaptation self management and self organization and self optimization .
in the parallel programming domain generative approaches such as co2p3s are used to create program skeletons for distributed memory programs which require man ual extensions by developers.
there is no comparable support for auto tuning on an architectural level.
.
conclusion as multicore computers have arrived on every desktop many software engineers now have to switch from sequential application development to the more complex parallel application development.
the design and performance optimization of parallel programs is more di cult than for sequential programs.
even experts have problems predicting the performance of complex parallel programs as a result experimentation is unavoidable in practice.
the tunable architectures approach proposed in this paper relieves developers from the burden of manual program adaptation and tuning.
it provides a systematic approach that helps avoid common design and implementation pitfalls.
tunable architectures also simplify the exploitation of parallelism on an architecture level rather than just on an instruction level which is a key leverage for performance.
in addition the quality of parallel code is improved hardcoded optimizations are replaced by generic constructs thus making multicore applications easier to port and re tune on new platforms.
multicore processors are de nitely here to stay and we need approaches like this one to make multicore software development more accessible for experts as well as for less experienced parallel programmers.