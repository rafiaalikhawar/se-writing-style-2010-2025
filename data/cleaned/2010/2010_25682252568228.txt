how to make best use of cross company data in software effort estimation?
leandro l. minku and xin y ao cercia school of computer science the university of birmingham edgbaston birmingham b15 2tt uk l.minku x.y ao cs.bham.ac.uk abstract previous works using cross company cc data for making within company wc software e ort estimation see try to use cc data or models directly to provide predictions in the wc context.
so these data or models are only helpful when they match the wc context well.
when they do not a fair amount of wc training data which are usually expensive to acquire are still necessary to achieve good performance.
we investigate how to make best use of cc data so that we can reduce the amount of wc data while maintaining or improving performance in comparison to wc see models.
this is done by proposing a new framework to learn the relationship between cc and wc projects explicitly allowing cc models to be mapped to the wc context.
such mapped models can be useful even when the cc models themselves do not match the wc context directly.
our study shows that a new approach instantiating this framework is able not only to use substantially less wc data than a corresponding wc model but also to achieve similar better performance.
this approach can also be used to provide insight into the behaviour of a company in comparison to others.
categories and subject descriptors d. .
management cost estimation i. .
learning concept learning general terms experimentation algorithms management keywords software e ort estimation cross company learning transfer learning online learning ensembles of learning machines permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for thirdparty components of this work must be honored.
for all other uses contact the owner author.
copyright is held by the owner author s .
icse may june hyderabad india acm .
introduction software e ort estimation see is the process of estimating the e ort required to develop a software project.
it is a task of strategic importance in project management as software e ort is the major contributing factor for software cost.
due to the importance of this task many automated methods for software cost or e ort estimation have been proposed including several machine learning approaches .
automated methods for creating see models rely on a set of training examples data which can be used for training building the models.
the training examples usually must contain information on completed projects in the form of input features and target.
input features can be for example the project s functional size development type language type etc.
the target is the true required e ort of the project.
however collecting within company wc training examples i.e.
information on projects completed by the company for which we are interested in providing predictions takes time.
moreover even though collecting certain input features can be relatively cheap and automated by using some search mechanism in the project database collecting the true e ort can be very expensive and involve a lot of human e ort.
as a result one of the major challenges in see is that wc training sets are typically small and building see models based solely on such small training sets can result in poor performance.
in order to overcome this problem several studies have attempted to use cross company cc training examples from di erent companies to augment existing wc training sets or to avoid their need .
we will use the term cc to refer to other companies than the one for which we are interested in providing estimations.
lately approaches that use cc training examples have also been referred to as transfer learning approaches .
all existing see cc approaches so far try to use cc training examples or models directly to provide predictions in the wc context.
as a result they are only useful when they re ect the wc context well i.e when the e ort required by another company to develop a certain software project would be the same as the e ort required by the company that we are interested in.
when this is not the case which can happen in practice these training examples or models perform poorly in the wc context and a fair amount of wc training examples would still be necessary to achieve good performance.
no see approach so far has tried to map cc training examples or models to the wc context i.e.
to learn a function that transforms the e ort of cc training examples or models to the e ort that would be required in the companypermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for third party components of this work must be honored.
for all other uses contact the owner author.
copyright is held by the author owner s .
icse may june hyderabad india acm that we are interested in.
if such mapping is possible then cc training examples or models that would not be helpful when used directly in the wc context could become helpful after being mapped and less wc training examples would be necessary for building the see model.
with that in mind this paper aims at answering the following question if we map cc models to the wc context can we reduce the amount of wc training examples used in the learning while maintaining or improving the performance obtained by a wc model trained on more wc examples?
in order to answer this question we de ne a cc learning scenario that considers the relationship between cc and wc training examples explicitly.
after that we propose the rst framework for learning this relationship and mapping cc models to the wc context i.e.
a framework designed to make best use of cc data for see.
this framework is then illustrated by a new approach called dynamic crosscompany mapped model learning dycom .
this approach is evaluated on ve databases and is able to substantially reduce the amount of wc projects necessary for the learning while obtaining similar or better performance than a wc model trained on more wc examples.
this demonstrates the success of our proposed framework and the importance of considering the relationship between cc and wc training examples explicitly.
dycom is designed and evaluated considering an online learning scenario where additional wc training examples may become available with time and the chronology of the projects is important.
the reason why it is important to consider chronology is that the company may su er changes with time that can a ect the performance of see models i.e.
see operates in a dynamic as opposed to a static environment.
so even though cc models may be mapped to the wc context using a certain function at a certain moment in time another function may be necessary to make the mapping if when the wc context changes.
.
related work several see works have tried to use cc training examples or models directly in the wc context as an attempt to deal with the problem of small wc training sets.
kitchenham et al.
performed a systematic literature review with the main objective of investigating under what circumstances individual organizations would be able to rely on cc see models.
even though some trends were observed no conclusions could be drawn in terms of under what circumstances cc data would be useful or not.
mcdonell and shepperd also performed a systematic literature review to investigate what evidence is there that cc see models are at least as good as wc estimation models.
they point to a lack of strong evidence to support either cc or wc models.
both systematic reviews mention that the level of heterogeneity of the single company being estimated may vary and in uence wc models performance.
this has in uenced more recent works that use local learning approaches to deal with the heterogeneity of wc and cc data sets .
the works retrieved by the systematic reviews found wc models to be either statistically similar or worse than cc models.
some of the studies used cc training examples to augment their existing wc training sets.
the resulting wc cc training set was then used to build a model to make predictions in the wc context.
for example le eyand shepperd tested this strategy using several learning machines based on the finish database.
in particular they ensured that the training examples were projects completed up to a certain date and that the test examples were projects yet to start by this date.
very few other studies considered chronological splitting .
some other studies recalibrated stepwise models obtained from wc cc data by using only cc training examples .
so the cc model is not independent of the wc data.
for example kitchenham and mendes performed a study on web e ort estimation based on the tukutuku database using this approach.
other studies used solely cc training examples to build the cc predictive models .
for example briand et al.
performed a study using the european space agency software project database using several di erent learning machines to create cc and wc models.
they compared the performances resulting from cross validation using only the wc data set to the performances of cc models trained only on a cc data set but tested on the wc data set.
most of these works assumed the see environment to be static and treated cc training examples as if they were from the same context of the wc training examples.
it is reasonable that when the context of the cc and wc examples is di erent cc models would perform poorly in the wc context.
when the contexts are similar then cc models may perform similarly to wc models.
more recent works tried to eliminate projects from predictions when they were likely to result in poor estimations.
for example kocaguneli et al.
used a ltering mechanism based on binary trees.
this approach obtained very encouraging results as models trained solely with di erent types of projects or projects from di erent centres of a company from the ones we would like to predict obtained overall similar performance to models trained on projects of the same type or from the same centre .
however this approach assumes a static see environment and does not consider that di erent companies may behave considerably di erently from each other.
in addition even though it has been evaluated in the context of di erent centres of a company it is unclear how well it would behave in a strict cc vs wc scenario.
an insightful work in the context of see and software defect prediction sdp clustered wc cc examples and then created prediction rules for each cluster.
given a certain cluster its neighbouring cluster with the lowest required e orts defects was referred to as the envied cluster.
when making predictions for wc projects from a cluster rules created using only the cc examples from the envied cluster were better than rules created using only the wc examples from the envied cluster.
so the authors recommend to cluster wc cc examples but to learn rules using solely the cc examples from the envied cluster.
however even though the paper is written for both see and sdp this particular conclusion was drawn based solely on sdp data.
the wc models are also likely to have been trained on very small subsets of the wc data possibly hindering their performance.
this approach uses the cc examples directly in the wc context and does not consider the dynamic aspects of the see task.
another work compares the performance of local models regression trees trained only on wc and only on cc data throughout time.
it con rms that di erent companies can have di erent contexts and that a certain company may su er changes over time.
these changes can cause it to447behave more less similarly to other companies making cc data bene cial or detrimental depending on the moment in time.
an approach called dynamic cross company learning dcl was designed to identify when cc data is bene cial and then use it to improve performance in comparison to wc models.
dcl was the rst see approach able to use cc models to achieve better performance in comparison to wc models showing that it is helpful to check how relevant cc models are to the current wc context before using them for wc predictions.
however as the cc models are only useful when they match the current wc context reasonably well dcl still requires a fair amount of wc examples to achieve good performance during the periods when the cc models are not useful.
as all existing cc works attempt to use cc training examples or models directly in the wc context there is currently no solution for the problem of small wc training sets when the cc data do not match the wc context well.
as di erent companies are likely to behave di erently such situation is very likely to happen in practice.
our work is the rst one to consider mapping cc models to the wc context so that the mapped models can be useful even when the cc models themselves do not match the wc context.
a successful mapping would reduce the amount of wc training examples required for achieving good performance.
.
cc see learning scenario di erent from previous work we consider for the rst time that there is a relationship between the see context of a certain company and other companies.
we formalise the relationship between two companies caandcbas follows fa x gba fb x wherecais the company in which we are interested cbis another company or a section of this other company fais the true function that provides the required e ort to ca fb is the true function that provides the required e ort to cb gbais a function that maps the e ort from the context of cb to the context of ca and x are the input features of a software project.
as an illustrative example consider the software projects in table .
in this case the true e ort of a project in cais times the e ort that would be required in cb i.e.
fa x gba fb x fb x .
even though the relationship between the e ort in caand cbis linear in this example our cc learning scenario does not restrict gbato linear functions.
note also that faand fbcan be functions of any type.
for instance fa orfb could be composed of sub functions representing di erent clusters of the company s data in order to represent the level of heterogeneity within a company .
in practice it is also likely that there will be some noise in the e orts.
given equation the learning task of creating an see model to a certain company cacan involve the task of learning the relationship between caand other companies i.e.
learning how to map cc training examples or models from the context of other companies to the context of ca.
.
cc model mapping framework based on equation we propose a framework for learning see models for a company cabased on mapping cc models toca s context.
it can be used both when wc and cc training examples arrive continuously online and when they comprise pre existing sets of xed size o ine .
.
mapping one cc see model this section explains the framework to learn a function gbato map e ort estimations given by an see model fb from company cbto the context of a company ca.
the learning process is illustrated in gure .
wc training data assume that training examples from caare made available.
the number of wc examples can be very small and a wc model created based on it may not be accurate.
so we would like to improve see for caby using cc training data.
cc training data assume that training examples from cb or from a section of cb are made available.
the input features and target of these examples must be compatible with the wc training examples.
cc see model training examples from cbare used for building an see model fbfor the context of cb.
ifcb does not wish to provide its raw training training examples toca it could provide fbdirectly toca.
mapping function given the wc training examples fromcaand the cc see model fb the learning task is then to learn a mapping function gbato map sees provided by fbto the context of ca.
in order to do so an appropriate training set needs to be derived from the wc training examples and fb.
this is done as follows.
for each training example x y in the wc training set use fbto provide an e ort estimation fb x .
then create an example fb x y for training gba.
it is hoped that the task of learning gbais not more di cult than the task of learning a whole wc model fabased solely on the wc training examples as this would reduce the amount of wc training examples required for the learning.
mapped see model once the mapping function is learnt then an see for a project from cadescribed by the input features x would be provided by the following function named as mapped model fa x gba fb x .
using more than one cc see model if the see model fband the mapping function gbalearnt using the framework explained in section .
were perfect then the mapped model would be a perfect see model to ca.
however it is very unlikely that an see model would be a perfect especially considering the limited amount of training examples and the di culty of the task.
in order to improve the sees given to ca cc training examples from more than one other company or sections of a company can and should be used.
the framework explained in section .
can be used to learn the mapping function gbiafor the cc model corresponding to each company or section of a company cbi i m .
each of the mmapped models would provide e ort estimations to caas shown in equation .
so for each wc project to be estimated mdi erent estimations can be provided.
an approach implementing this framework would then need to decide which of these estimations should be used and or how these estimations should be combined into a single and more trustful estimation to be given toca.
combining estimations of di erent models has been showing to improve see considerably .
in addition to the mapped models one may also wish to combine a wc model fwatrained onca s training examples.448table example of relationship between company caand company cb.
in this case the true e ort in person hours for a given project in cais1 2times its true e ort in person hours in cb.
id functional size development type language type cb s true e ort ca s true e ort enhancement 3gl re development 4gl new development 4gl new development 3gl cb training datatraining exampleslearning fb function learnt fb e ort estimationsestimation enquiries ca training datatraining examplescreate mapping training examplestraining examplesmapping training datatraining exampleslearning gbafunction learnt gba figure framework for learning function gbato map estimations given by an see model fbfrom company cbinto the context of a company ca.
.
using a mixed cc training set a certain company camay have access to a set of cc training examples from several di erent companies without information on which examples belong to which individual companies among the ones that provided the examples.
this would be the case of a company causing data from the international software benchmarking standards group isbsg for example.
nevertheless it would still be possible to use our framework.
for instance the cc training examples could be split into di erent sets according to their corresponding productivities as done in section or according to some clustering technique based on their input features and targets.
in this way similar cc examples would be grouped together into di erent cbi i m sets where eachcbiis a virtual imaginary company that develops projects with the characteristics of the projects grouped together.
each virtual company would represent a mix of the behaviours of the di erent companies that provided the cc projects.
if appropriate functions fcbiare learnt the context of these virtual companies cbican be mapped to the context ofca turning them into useful information to ca.
.
online learning scenarios as explained in section the cc model mapping framework can be used both for online and o ine learning.
in this paper we propose an instantiation of the framework where new wc projects arrive with time and cc training examples comprise pre de ned sets similarly to .
such situation could occur for example when a company acquires sets of cc training examples without a contract to receive updates of these sets.
another situation would be when a company adopts a cc training set available in the web whose size is not increasing with time.
as companies are evolving entities changes can happen that would a ect the e ort required for them to develop a software project .
examples of changes are train ing being provided to employees new employees being hired new programming languages being introduced the company starting to take new types of projects etc.
so it is important to consider the chronology e.g.
the date of completion of the projects of the company for which we are interested in providing estimations.
additionally if this company su ers changes that a ect the e ort that it would require to develop a software project the model used for providing such estimations should be able to adapt to such changes.
we formulate see as an online learning problem in which a new wc project is completed at each time step point in time .
whenever a new wc project is completed we wish to provide up to date estimations of the e ort for the next ten wc projects to be or being developed based on the latest available see model.
the number of wc projects for which up to date estimations should be provided at each time step depends on the needs of the company being estimated.
investigation of values di erent from ten is left as future work.
as an important aim of the current work is to analyse whether it is possible to reduce the amount of wc training examples required for building an see model we consider two di erent scenarios .each completed wc project contains both information on its input features and on its true required e ort.
.the wc projects that arrive at every p p time steps contain both information on their input features and true required e ort.
all remaining wc projects contain only the information on the input features whereas the true required e ort is missing.
so even though an e ort estimation is required for all wc projects only a few of them are used as training examples once they are completed.
.
dycom in this section we present dynamic cross company mapped model learning dycom an instantiation of the framework presented in section .
dycom can be used with cc449training examples from several di erent companies as described in sections .
and .
and operates under the online learning scenario explained in section .
wc training data similarly to section we will refer to the company for which we are interested in providing predictions as ca.
wc training examples from caare incoming as described in section .
wc see model our framework allows a wc model to be trained if desired.
in dycom whenever a new wc training example arrives it is used to train a model fwa.
cc training data the cc training examples are available beforehand as explained in section .
they are split into sections based on some clustering algorithm or on their productivity or on the size of the projects.
each section cbi is considered as a separate cc training set.
for example if there are ncompanies and the training examples from each company are split into ssections then there will be m n sdi erent cc training sets.
if it is not known from what company each cc training example comes then the whole cc training set is split into di erent sections as ifn .
the reason for the splitting will be explained in the paragraph on mapping functions.
cc see models each of the mcc training sets is used to create a di erent cc model fbi i m .
mapping functions whenever a new wc training example arrives each model fbiis asked to perform an see each see is then used to create a mapping training example fbi x y and the corresponding gbiais trained with it as described in section .
.
dycom assumes that the relationship formalised in equation can be modelled reasonably well by linear functions of the format gbia fbi x fbi x biwhen di erent sections containing relatively more similar cc training examples are considered separately.
this is the reason to split cc training examples into di erent sections as explained previously.
learning this function means learning the factor bi which is done using equation bi if no mapping training example has been received yet y fbi x if fbi x y is the rst mapping training example lr y fbi x lr bi otherwise.
where fbi x y is the mapping training example being learnt lr lr is a pre de ned smoothing factor and the factor biin the right side of the equation represents the latest value of bibefore receiving the current mapping training example.
the mapping function performs a direct mapping bi while no mapping training example is received.
when the rst mapping training example is received biis set to the valuey fbi x .
this gives a perfect mapping for the example being learnt as fbi x bi fbi x y fbi x y. for all other mapping training examples received exponential smoothing with smoothing factor lris used to set bi.
this is the simple weighted average of the value that would provide a perfect mapping for the current mapping example and the previous value of bi which was calculated based on the previous mapping examples.
higher smoothing factor lrwill cause more emphasis on the most recent mapping trainingexamples and higher adaptability to changing environments whereas lower lrwill lead to a more stable mapping function.
so the smoothing function allows learning mapping functions that provide good mappings based on previous mapping examples while allowing adaptability to changes that may a ect a company s required software e orts.
mapped see model as explained in section .
each mapped model gbia fbi and the wc model fwacan provide an see in the wc context when required.
the see given by dycom is the weighted average of these m estimations fa x mx i 1wbi gbia fbi x wwa fwa x where the weights wbiandwwarepresent how much we trust each of the models are positive and sum to one.
so dycom uses an ensemble of mapped and wc see models.
weights the weights are initialised so that they have the same value for all models being used in the ensemble and are updated in a similar way to the weights used in .
whenever a new wc training example is made available the model which provided the lowest absolute error is considered to be the winner and the others are the losers .
the losers have their weights multiplied by a pre de ned parameter and then all weights are normalised so that they sum to one.
algorithm presents dycom s learning process.
dycom rst learns the cc models line .
the weight associated to each cc model is initialised to m so that each model has equal weight and all weights sum to one line .
the mapping functions are initialised to use bi line .
before any wc training example is made available the weight wwacorresponding to the wc model is initialised to zero line because this model has not received any training yet.
in this way the cc models can be used to make predictions while there is no wc training example available.
after that for each new wc training example the weights are updated lines .
if the wc training example is the rst one the weight of the wc model needs to be set line .
then the mapping training examples are created and used to update the corresponding mapping functions lines and .
finally the wc model is updated with the wc training example line .
.
databases this section presents the databases used in the evaluation of dycom.
five di erent databases were used kitchenmax cocnasacoc81 isbsg2000 isbsg2001 and isbsg.
these include both data sets derived from the predictor models in software engineering software promise repository and the international software benchmarking standards group isbsg repository .
each database contains a wc data set and three cc data sets derived based on the projects productivity.
.
kitchenmax the database kitchenmax is composed of kitchenham and maxwell which are two see data sets available from the promise repository.
kitchenham s detailed description can be found in .
it comprises maintenance and development projects undertaken between and by a single software development company.
maxwell s detailed450algorithm dycom parameters dbi i m cc training sets.
factor for decreasing model weights flearn cc base models g foreach cc training set dbido create cc model bbiusingdbi wbi mfinitialise weight.g bi 1finitialise mapping function.
g end for wwa 0finitialise wc model weight.
g foreach new wc training example x y do fupdate weights g foreach model fbiand fwado determine the model s estimation to x. calculate the absolute error aebi oraewa .
end for determine loser models based on their ae.
multiply loser models weights by .
if x y is the rst wc training example then wwa m end if divide each weight by the sum of all weights.
fupdate mapping functions g foreach model fbido create mapping example fbi x y .
use fbi x y to update gbiabased on eq.
.
end for fupdate wc model g update wc model fwausing x y .
end for description can be found in .
it contains projects from one of the biggest commercial banks in finland covering the years to and both in house and outsourced development.
in order to make these data sets compatible a single input feature functional size was used.
still maxwell uses functional size whereas kitchenham uses adjusted functional size.
an appropriate mapping function should be able to overcome this problem.
there were no functional size feature values missing.
the target is the effort in person hours.
kitchenham was considered as the wc data and was sorted according to the actual start date plus the duration.
this sorting corresponds to the exact completion order of the projects.
maxwell was considered as the cc data and was split into three cc sets for use with dycom according to their productivity in terms of e ort divided by functional size.
the ranges used for the di erent cc sets are shown in table and were chosen to provide similar size partitions.
this process could be easily automated in practice.
wc projects were not split.
.
cocnasacoc81 the database cocnasacoc81 is composed of cocomo nasa and cocomo which are two see data sets available from the promise repository.
cocomo nasa contains nasa projects from 1980s 1990s and cocomo consists of the projects analysed by boehm to develop the software cost estimation model cocomo rst published in .
both data sets contain input features cost drivers andtable productivity ranges for cc data sets.
cc data productivity band examples high .
.
maxwell medium .
.
low .
.
high cocomo medium .
.
low .
.
high isbsg2000 medium .
.
low .
.
high isbsg2001 medium .
.
low .
.
high isbsg medium .
.
low .
.
number of lines of code and one target software e ort in person months .
cocomo contains an additional input feature development type not present in cocomo nasa which was thus removed.
these data sets contain no missing values.
cocomo nasa s projects were considered as the wc data and cocomo s projects were considered as the cc data.
cocomo nasa provides no information on whether the projects are sorted in chronological order.
the original order of the cocomo nasa projects was preserved in order to simulate the wc projects chronology.
even though this may not be the true chronological order it is still useful to evaluate whether approaches are able to make use of mapped cc models when if they are bene cial.
the three cc data sets for dycom were created by separating cocomo s projects based on the productivity in terms of e ort divided by the number of lines of code using the ranges in table .
this database has also been used in where it was shown that cocomo s projects can sometimes be useful and sometimes detrimental in predicting cocomo nasa s projects.
.
isbsg databases three see databases were derived from isbsg release which contains software project information from several companies.
information on which projects belong to a single company for composing a wc data set have been provided to us upon request.
the databases are isbsg2000 wc projects implemented after the year and cc projects implemented up to the end of year .
isbsg2001 wc projects implemented after the year and cc projects implemented up to the end of year .
isbsg no date restriction to the wc and cc projects meaning that cc projects with implementation date more recent than wc projects are allowed.
this data set can be used to simulate the case in which it is known that other companies can be more evolved than the single company analysed.
these databases have been previously used in .
information on how they were preprocessed can be found in that paper and is not included here due to space limitations.
four input features development type language type development platform and functional size and one target software e ort in person hours were used.
the wc projects451were sorted based on the implementation date to compose a stream of incoming projects.
dycom further uses a separation of cc projects into di erent training sets.
this was done by splitting projects according to their normalised level productivity rate in hours per functional size unit provided by the repository.
the ranges used for creating the cc sets are shown in table .
it was shown in that regression trees local models trained on each cc set were better similar or worse than regression trees trained on the wc projects depending on the moment in time.
.
evaluation of dycom this section presents the experiments performed with the aim of determining whether mapping cc models to the wc context allows us to reduce the amount of wc training examples used in the learning while maintaining or improving performance in comparison to a wc model trained with more wc examples.
these experiments also work as an evaluation of dycom as is used as the mapping learning approach.
.
experimental setup dycom can be used with any base learner.
in this work regression trees rts were used as the base learners in the experiments.
rts were chosen because they are local approaches in which estimations are based on the projects that are most similar to the project being predicted.
this can help dealing with the heterogeneity within each data set .
rts have been shown to achieve good performance for see in comparison to several other approaches .
we used the rt implementation reptree provided by weka where splits are created so as to minimise the variance of the targets of the training examples in the nodes.
two di erent approaches were compared rt and dycom rt.
rt rts were created to re ect wc online learning.
whenever a new wc training example was provided the current rt was discarded and a new rt was trained on all projects so far including the one received at the current time step .
this rt was then used to predict the next ten wc projects.
this approach considers that all wc completed projects contained known true e ort i.e.
every time step received a wc training example online scenario .
dycom rt dycom was used with rts as the cc and wc models under the online scenario with p i.e.
a new wc training example was provided only at every time steps.
so dycom rt uses only of the wc training examples used by the rt explained in the paragraph above.
the wc rt used by dycom was rebuilt from scratch using all wc training examples so far whenever a new wc example was made available.
both dycom and the wc approach used the same base learner in the experiments ensuring that the comparison is fair.
the study of dycom with other base learning approaches is left as future work.
at each time step the performance on the next ten examples was evaluated based on several di erent measures mean absolute error mae standardised accuracy sa root mean squared error rmse correlation coe cient corr logarithmic standard deviation lsd mean magnitude of the relative error mmre and percentage of predictions within of the actual value pred .
the equations to calculate these measures are the following wheretis the number of examples used for evaluatingthe performance yiis the actual e ort for the example i and yiis the estimated e ort for example i mae tpt i 1j yi yij sa mae mae rguess wheremae is the mae of the approach being evaluated and mae rguess is the mae of runs of random guess.
random guess is de ned here as uniformly randomly sampling the true e ort over all the wc projects received up to the current time step.
it is calculated based on the online scenario rmse qpt i yi yi t corr pt i yi y yi y ppt i yi y 2ppt i yi y where yand yare the average predicted and average actual e orts respectively lsd r pt i ei s2 t wheres2is an estimator of the variance of the residual eiandei lnyi ln yi mmre tpt i 1mre i wheremre i j yi yij yi pred tpt i ifmre i otherwise.
these measures have been chosen because they emphasise a variety of di erent behaviours evaluating see from different angles.
mae has been recommended by for being unbiased towards under or overestimations.
sa is an unbiased measure that allows for interpretability it is viewed as the ratio of how much better an approach is than random guess .
so it can also be used to give a better idea of the magnitude of the di erences in performance.
rmse is a popular measure in the machine learning community which emphasizes large errors more.
corr is widely used in sciences as a measure of the strength of linear dependence between two variables.
in the case of see the two variables are the estimated and the actual e ort .
lsd uses the residual in the log scale which is independent of size i.e.
homoscedastic .
mmre and pred are measures biased towards underestimations and can behave quite differently from other measures .
they are reported here to provide insight into the behaviour of the approaches when analysed together with mae.
in addition to these measures the standard deviation stddev of mae across time steps has also been used.
this is because it is desirable to have see models that are not only accurate but also whose errors do not vary much for di erent projects.
a more stable see model is more reliable because it gives a better idea of the error that is likely to happen when estimating a new project.
the databases used in the experiments are the ones explained in section .
dycom s parameter was set to the default value of .
which has been used previously in the literature for similar weight update mechanisms .
dycom s parameter lrwas set to .
after some preliminary investigation with .
and .
.
the parameters used with each rt were the ones more likely to obtain good results in previous work minimum total weight of for the instances in a leaf and minimum proportion of the variance on all the data that need to be present at a node in order for splitting to be performed .
.
a single execution was performed for each data set as we used deterministic rts.452table overall average performance across time steps.
database approach mae stddev sa rmse corr lsd mmre pred rt .
.
.
.
.
.
.
.
kitchenmax dycom rt .
.
.
.
.
.
.
.
p value .82e .35e .46e .62e .25e .80e .12e rt .
.
.
.
.
.
.
.
cocnasacoc81 dycom rt .
.
.
.
.
.
.
.
p value .04e .40e .95e .12e .82e .37e .78e rt .
.
.
.
.
.
.
.
isbsg2000 dycom rt .
.
.
.
.
.
.
.
p value .72e .01e .83e .73e .27e .90e .60e rt .
.
.
.
.
.
.
.
isbsg2001 dycom rt .
.
.
.
.
.
.
.
p value .21e .16e .88e .29e .24e .76e .26e rt .
.
.
.
.
.
.
.
isbsg dycom rt .
.
.
.
.
.
.
.
p value .56e .54e .18e .90e .99e .99e .59e cells in lime light grey represent better values.
p values of wilcoxon sign rank tests to compare dycom rt against rt for each database are also shown.
for stddev the p values correspond to levene tests for equality of variances.
cells in orange dark grey indicate statistically signi cant di erence when using holm bonferroni corrections at the overall level of signi cance of .
considering the ve databases.
p values for sa are not shown because this measure is an interpretable equivalent of mae.
.
results .
.
overall average performance we rst analyse the overall average performances across time steps .
in order to compare the overall performances between rt and dycom rt we performed a wilcoxon sign rank test for each database.
for stddev we performed levene test for equality of variances.
for each performance measure we used holm bonferroni corrections considering ve comparisons at the overall level of signi cance of .
.
no tests have been done for sa because it is an interpretable equivalent to mae.
in terms of all performance measures but mmre and pred dycom rt obtained always similar or better overall performance than rt.
it is worth noting that even if dycom rt had obtained similar and never better overall performance than rt this would still have represented a strong advantage of dycom rt.
the reason is that dycomrt required much less wc training examples saving the cost of collecting the required e ort for wc projects.
our experiments show that dycom rt not only managed to use ten times less wc training examples but also was able to provide similar or better overall performance for all databases.
in terms of mae and sa rmse and correlation sometimes the di erences were signi cant and sometimes not.
the measure sa allows us to have a better idea of the magnitude of the di erences in performance as it is interpreted as how much better an approach does than random guess.
for cocnasacoc81 and isbsg2001 the difference in sa was considerably large being likely to have large impact in practice.
in terms of lsd there was always statistically signi cant di erence and dycom rt was better for all databases.
in terms of stddev statistically signi cant di erence was found only for cocnasacoc81.
so dycomrt and rt behaved mostly similarly in terms of stability of mae across time steps.
in terms of mmre and pred dycom rt was sometimes better similar and worse than rt.
as explained in section .
these measures are biased towards underestimations.
so combined with the results in terms of mae these results do not imply that dycom rt obtained behaviour sometimes better similar or worse than rts but that dy coms sometimes managed to obtain more improvements in terms of reducing overestimations sometimes balanced improvements and sometimes more improvements in terms of reducing underestimations.
sometimes these improvements pushed the relative errors under the boundary of of the actual e ort and sometimes not.
.
.
performance throughout time in addition to the overall performance across time steps when working with online learning it is also important to verify the performance at each time step.
this allows checking whether a certain approach is better at some time steps but worse at others.
we make this analysis based on mae which is an unbiased measure recommended in .
figure shows the mae over time for all databases.
for the databases where dycom rt obtained statistically better overall mae across time steps than rt kitchenmax cocnasacoc81 and isbsg2001 there were very few time steps when dycom rt performed worse than rt.
however for the databases where dycom rt and rt obtained no statistically signi cant di erence in terms of overall mae isbsg2000 and isbsg dycom rt performed considerably better during some prolonged periods of time and considerably worse during some others.
the reason for the worse performance during these periods of time is likely to be that the single company su ered some change that was not captured for a while due to the lack of wc training examples.
the best interval pfor the collection of true required e ort of wc projects is likely to depend on how often the company su ers considerable changes.
companies that present changes more often would need smaller intervals.
the intervalpmay also a ect the ability of dycom to deal with outlier training examples.
a very interesting area of future research is the investigation of manual or algorithmic approaches to decide when projects should have their required e orts collected.
.
insight provided by dycom the mapping functions learnt by dycom explain the relationship of the e ort required by a certain company cain comparison to sections cbiof other companies throughout45320 time stepmae dycom rt rt a kitchenmax time stepmae dycom rt rt b cocnasacoc81 time stepmae dycom rt rt c isbsg2000 time stepmae dycom rt rt d isbsg2001 time stepmae dycom rt rt e isbsg figure mae throughout time.
time.
so it provides useful insight into the behaviour of a given company in comparison to other companies.
this relationship can be visualised via plots of the factor bito show the need for strategic decision making towards the improvement of productivity as well as to monitor the success of strategies adopted for such improvement.
the mapping function considers the e ort that di erent sections from other companies would require for each given project from caconsidering their input features.
so plots the factor bi consider more information than general plots of the e ort or productivity of a company throughout time which would provide only trends without considering the individual features of the projects being developed.
figure presents the factor bilearnt for each mapping function over time.
in our experiments the cc projects were split into three data sets according to their productivity.
so the cc sections cbi i refer to a cc data set with high productivity medium productivity and low productivity.
for all databases caneeds more e ort than the high productivity cc sections bi and less e ort than the low productivity cc sections bi .
however each single company cabeing analysed has a di erent behaviour with respect to their corresponding cc models especially with respect to the high productivity ones.
for instance kitchenmax s cainitially behaves quite similarly to the medium productivity cc model and then its behaviour gradually changes to become more and more similar to the high productivity cc model.
initially a project incawould require around twice the e ort that this same project would require by the high productivity cc model.
in the end of the period observed this gure improves to time stepfactor bi cb1 low cc productivity cb2 medium cc productivity cb3 high cc productivity a kitchenmax time stepfactor bi cb1 low cc productivity cb2 medium cc productivity cb3 high cc productivity b cocnasacoc81 time stepfactor bi cb3 low cc productivity cb2 medium cc productivity cb1 high cc productivity c isbsg2000 time stepfactor bi cb3 low cc productivity cb2 medium cc productivity cb1 high cc productivity d isbsg2001 time stepfactor bi cb3 low cc productivity cb2 medium cc productivity cb1 high cc productivity e isbsg figure factor biassociated to each cc data set throughout time.
.
times the e ort.
this demonstrates the success of the companycain improving its behaviour.
another example is cocnasacoc81 whose relationship with respect to all cc models remains stable.
carequires around .
times the e ort that would be required by the medium productivity cc model.
so cocnasacoc81 s ca may wish to adopt some strategy to become more productive.
in order to decide on a strategy one would have to ask whycarequires more e ort than the medium productivity cc model or why carequires more e ort than the high productivity cc model.
in order to answer these questions ifcahas access to the training examples from these cc sections it could analyse them to nd out possible reasons forca s lower productivity.
for example cacould analyse the medium productivity cc projects that are most similar e.g.
nearest neighbours to each given project that ca wishes to improve.
or it could analyse all training examples from this cc section if cawishes to improve its overall behaviour.
if the features of the cc projects from a given section are completely di erent from the wc projects then it may be more di cult to interpret these di erences and decide on a strategy to improve ca.
so it may be a better strategy to analyse the examples from the cc set with the most similar productivity rst as these are more likely to be more similar and incur less dramatic changes for a start.
for example table shows that both the medium productivity cc section and cafrequently used sta with high language experience.
however the medium productivity cc section used sta highly experienced in virtual machine more454table number of projects with each feature value for the cc projects from the medium productivity cc section and the rst wc projects.
feature lexp vexp value cc wc cc wc very low low nominal high very high extremely high lexp refers to the feature language experience and vexp refers to the feature virtual machine experience.
often thanca.
it may happen that the di erent companies did not judge whether sta had nominal or high experience in exactly the same way.
however the fact that the medium productivity cc section contains more projects with high virtual machine experience indicates that this feature rather than language experience is more likely to be one of the reasons for ca s lower productivity.
after such analysis cacould then adopt a certain strategy based on the di erences between its project features and the cc project features to improve its productivity.
for instance it could hire sta with more virtual machine experience.
the plots of bi could then be used to analyse how successful the strategies adopted are being with time.
one might nd intriguing that the same caprovided by the isbsg repository behave similarly for isbsg2000 and isbsg2001 but di erently for isbsg.
however this behaviour is understandable because the cc training examples used for isbsg were not restricted to past projects.
they can represent for example companies that are already used to certain technologies that are new to ca.
when such new technologies start being used castarts requiring more e ort than these companies.
.
threats to validity internal validity regards to establishing that a certain observable event was responsible for a change in behaviour.
it is related to the question is there something other than the treatment that could cause the di erence in behaviour?
.
when using machine learning approaches it is important that the approaches being compared use fair parameter choices in comparison to each other .
in this paper both the rts used as wc learners and within dycom used the same parameters which were the ones more likely to obtain good results in the literature .
dycom contains two extra parameters which were set to the same value for all databases used in this study i.e.
they were not ne tuned for each database.
so the results obtained in this paper do not depend on the user ne tuning dycom.
it is natural that a software manager with no specialist knowledge on machine learning would run these approaches with default parameters or parameters that have previously obtained good results in the literature.
so it is reasonable to perform the analysis in this way in the current paper.
future work should investigate whether dycom s results could be improved further by ne tuning parameters.
construct validity regards to accurately naming our measures and manipulations .
we used several di erent performance measures mae stddev sa rmse corr lsd mmre and pred .
wilcoxon statistical tests with holm bonferroni corrections were used to check the statis tical signi cance of the di erences in overall performance and sa was also used to give a better idea of the magnitude of the di erences in performance and the impact that they are likely to have in practice.
external validity regards to generalizing the study s results outside the study to other situations .
besides never using a wc project for training before using it for testing we considered ve databases to handle external validity.
four databases with known wc chronological order were used for evaluating dycom.
even though the wc chronological order is not known for the other database it can still be used to evaluate whether dycom is able to successfully make use of cc models contributing to the generalisation of our results.
obtaining additional databases for evaluating dycom is di cult due to our need for nonproprietary data sets with information on which projects belong to a single company among the projects of a crosscompany data set.
however the data sets used in this study can be made available through promise and isbsg.
so researchers and companies willing to use dycom could use the same cc data sets used in this study.
further investigation should be done into what would happen with dycom when using other base learners than rts and when using other input features for the data sets.
.
conclusions we have introduced a new cc see learning scenario that considers the relationship between the required e ort of wc and cc projects.
we propose the rst framework for learning this relationship and mapping cc models to the wc context.
it is designed to make best use of cc data in see.
the bene t of such a framework is demonstrated by a new approach dycom which was evaluated on ve databases and was able to substantially reduce the amount of required wc training examples while maintaining or improving overall average performance in comparison to a wc model trained on more wc examples.
the research question raised in section was if we map cc models to the wc context can we reduce the amount of wc labelled projects used in the learning and still obtain similar or better performance than a wc model?
the study performed in this paper gives the answer yes to this question.
dycom is an example of approach that successfully achieves that.
dycom can also be used to give insight into monitoring and improving the productivity of a company.
future work includes an investigation of dycom s sensitivity to parameter values base learners input features and techniques for splitting cc projects into di erent sections.
our proposed framework could also be used to design other approaches.
for instance approaches considering di erent productivity sections of wc data could also be considered.
the online learning scenarios should also be studied when predicting di erent numbers of wc projects than ten at each time step and techniques to decide which projects to have their e ort collected should be investigated.
in addition the best type of function to describe the relationship between wc s and cc s e ort should be studied.
.