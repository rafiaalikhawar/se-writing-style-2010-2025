automated documentation inference to explain failed tests sai zhang1cheng zhang2michael d. ernst1 1university of washington2shanghai jiao tong university szhang cs.washington.edu cheng.zhang.stap sjtu.edu.cn mernst cs.washington.edu abstract a failed test reveals a potential bug in the tested code.
developers need to understand which parts of the test are relevant to the failure before they start bug fixing.
this paper presents a fully automated technique and its tool implementation called failuredoc to explain a failed test.
failuredoc augments the failed test with explanatory documentation in the form of code comments.
the comments indicate changes to the test that would cause it to pass helping programmers understand why the test fails.
we evaluated failuredoc on five real world programs.
failuredoc generated meaningful comments for most of the failed tests.
the inferred comments were concise and revealed important debugging clues.
we further conducted a user study.
the results showed that failuredoc is useful in bug diagnosis.
i. i ntroduction a failed unit test indicates a potential bug in the tested code.
a programmer must understand the cause of the failure and confirm its validity before starting bug fixing.
recently many automated test generation techniques have been studied to create tests but few techniques are proposed to explain why a test fails.
understanding why a test fails or even knowing which part of the code should be inspected first in debugging is a non trivial task.
this is a particular problem for automatically generated tests which are often long and have poor readability but it is also relevant for human written tests.
for example figures and show a human written test and an automatically generated test respectively.
the first test in figure is associated with a jdk bug report.
it shows that method arrays.toarray is not type safe.
however when executed this test throws an arraystoreexception at the last statement which is not obviously related to any type safety issues.
to confirm this reported bug a programmer must manually connect the available failure symptom an arraystoreexception with possible failure causes.
the second test shown in figure is also not easy to understand.
this automatically generated test involves classes such as arraylist treeset and collections in the jdk each of which contains hundreds of lines of code.
executing this test also does not give much useful information the assertion simply fails without dumping any stack trace as debugging clues.
furthermore the test has already been minimized if any of the calls is removed and the code is fixed up so that the test compiles again then the bug is not triggered and the test passes.
.
public void test1 f .
arraylist number nums new arraylist number .
integer i new integer .
boolean b0 nums.add i .
long l new long .
boolean b1 nums.add l .
integer is new integer .
this statement throws arraystoreexception .
integer ints nums.toarray is .g fig.
.
a human written failed test.
this test reveals a potential error in the jdk bug id .
.
public void test2 f .
int i .
arraylist l new arraylist i .
object o new object .
boolean b0 l.add o .
treeset t new treeset l .
set s collections.synchronizedset t .
this assertion reflexivity of equals fails .
asserttrue s.equals s .g fig.
.
an automatically generated failed test.
this test reveals an error in jdk version .
.
it shows a short sequence of calls leading up to the creation of an object that is not equal to itself.
good documentation i.e.
code comments can help programmers quickly understand what source code does facilitating program comprehension and software maintenance tasks .
unfortunately a human written test is often poorly documented and few automated test generation tools can adequately comment the code they generate.
even more importantly when a test fails the most useful documentation is relevant to the defect in the tested code and leads the developer to that defect.
later when the test reveals a different defect different documentation would be best.
as a result of the lack of contextually relevant documentation programmers must guess about what parts of the test and the tested code are relevant.
proposed approach.
this paper presents a fully automated approach and its tool implementation called failuredoc to infer documentation for a failed test.
failuredoc is not an automated fault localization tool that pinpoints the exact buggy code.
instead it augments a failed test with debugging clues code comments that provide potentially useful facts about the failure helping programmers fix the bug quickly.
figures and show the inferred documentation for the failed tests of figures and respectively.
in figure the comments above lines and reveal important clues that the test passes if object lis changed to1.
public void test1 f .
arraylist number nums new arraylist number .
integer i new integer .
boolean b0 nums.add i test passes if line is integer l new integer .
long l new long test passes if l is not added to nums .
boolean b1 nums.add l .
integer is new integer .
this statement throws arraystoreexception .
integer ints nums.toarray is .g fig.
.
the failing test of figure with code comments inferred by the failuredoc tool highlighted by underline .
integer type or if lis not added to the nums list.
these clues guide the programmer to discover that the test failure is because the test erroneously adds two type incompatible objects iand linto the list and later casts both of them to integer type.
such information is much more helpful in understanding why a test fails than merely dumping an arraystoreexception .
in figure the comment above line discloses a crucial fact that the test passes if object o implements comparable .
this clue guides the programmer to inspect places where object o is used.
in fact the treeset constructor is buggy it should not accept a list containing a non comparable object but it does.
this indicates the exact cause and a possible bug fix.
to infer useful documentation failuredoc simulates programmers debugging activity.
its design is based on the following common debugging practice given a failed test a programmer often tries to make some minimal edits to make it pass observes the difference between passing and failing executions then generalizes those failure correcting edits to understand the failure cause.
failuredoc automates the above reasoning process summarizing its observations as documentation.
failuredoc works in four phases figure namely value replacement execution observation failure correlation and documentation generation .
in the first phase failuredoc first generates an object pool and then mimics programmers activity in correcting a failed test by repeatedly replacing existing values with possible alternatives.
each replacement creates a slightly mutated test.
in the second phase failuredoc executes the mutated test uses static slicing to prune irrelevant statements and selectively observes its outcomes.
in the third phase failuredoc uses a statistical algorithm to correlate the replaced values with their corresponding outcomes identifying suspicious statements and their failure correcting objects.
in the final phase for each identified suspicious statement failuredoc uses a daikon like technique to summarize properties of the observed failure correcting objects translating them into explanatory code comments.
evaluation.
we implemented the failuredoc prototype and evaluated its effectiveness through an experiment and a user study.
in our experiment we used failuredoc to generate comments for failed tests from five real world programs.
failuredoc successfully generated human readable comments for out of failed tests.
we validated the helpfulness of the inferred comments by sending the documented tests to1.
public void test2 f .
int i .
arraylist l new arraylist i test passes if o implements comparable .
object o new object test passes if o is not added to l .
boolean b0 l.add o .
treeset t new treeset l .
set s collections.synchronizedset t .
this assertion reflexivity of equals fails .
asserttrue s.equals s .g fig.
.
the failing test of figure with code comments inferred by the failuredoc tool highlighted by underline .
the developers.
the developers reaction strongly suggests that failuredoc is useful in practice.
we also conducted a user study to investigate the usefulness of the inferred documentation.
with the generated comments programmers spent less time in understanding the revealed bugs than without those comments and programmers spent .
less time than with the aid of delta debugging .
contributions.
the main contributions of this paper are technique.
a technique to infer descriptive comments to explain failed tests section ii .
tool.
an open source automated tool implementing the proposed technique evaluation.
an experiment and a user study demonstrate the usefulness of the proposed technique section iii .
ii.
t echnique figure gives an overview of failuredoc s architecture.
failuredoc consists of four major modules working in a pipelined manner.
value replacement .
this module takes a failed test as input.
it first uses a randomized algorithm to create an object pool containing instances of all needed classes then mutates the failed test by repeatedly replacing expressions in the test code with possible alternatives from the created object pool to construct a set of slightly mutated tests section ii a .
execution observation .
this module executes the mutated tests to obtain execution traces.
for each mutated test this module uses static slicing to prune all irrelevant statements from the execution trace section ii b .
failure correlation .
this module takes as inputs the replaced values and the observed outcomes.
it uses a statistical algorithm to identify a small set of suspicious statements that have a strong correlation with the test failure and their failurecorrecting objects section ii c .
documentation generation.
this module generalizes properties of the observed failure correcting objects for each suspicious statement then converts the generalized properties into documentation section ii d .
a. value replacement given a failed test the value replacement module repeatedly replaces expressions in the test code with possible alternatives to construct a set of mutated tests.
for example in figure failuredoc can replace int i on line with int i input a f ailed t estoutput a f ailed t est with debugging c luesex ecu tio n o b serv atio nmutated testsv alu e r ep lacemen texecution tracessuspicious statementsfailu re c o rrelatio nd o cu men tatio n g en eratio nfig.
.
the architecture of failuredoc.
it augments a failed test with explanatory documentation debugging clues in four steps.
or replace treeset t new treeset l on line with treeset t new treeset t.put .
to correctly and efficiently implement value replacement two key challenges must be addressed.
first failuredoc must create new values to replace an existing one.
second for the sake of efficiency failuredoc must select among the new values it creates for each existing value.
value generation the failuredoc technique can be instantiated using any value generation technique such as exhaustive generation up to a given size bound .
our current implementation uses an existing random test generation algorithm that of randoop .
we now briefly describe it.
failuredoc parses the failed test to extract all referred to classes.
for example the failed test in figure refers to classes integer object arraylist treeset set and collections .
optionally the user can also provide failuredoc additional classes for value generation but our experiments did not use this capability.
failuredoc next uses an existing random test generation algorithm to create object instances of the referredto classes and keeps all created objects in a value pool.
the algorithm iteratively builds a method call sequence that produces an object value by randomly selecting a method or constructor to invoke using previously computed values as inputs.
failuredoc uses object values in the pool to mutate the failed test.
the mutated tests are never shown to the user.
value selection after generating an object pool a natural question is how to select possible alternatives for each expression.
many of the objects in the pool may be similar.
hence the naive approach of choosing every typecompatible object from the pool to replace the existing value is unnecessary and inefficient.
on the other hand understanding the failure cause requires selecting a group of diverse objects as replacement candidates in order to expose different test behaviors.
randomly selecting a group of objects from the pool may end up with the same test behavior which would not be helpful to infer useful information.
to alleviate this problem failuredoc adaptively selects a diverse set of objects as replacement candidates based on an abstract object profile representation described below .
adaptive selection reduces the likelihood of choosing a group of similar objects in which most of the objects reveal the same test behavior while failing to reveal other behaviors.
for each expression eof type tein the test code failuredoc creates approximately kmutants kis user settable our experiments used the default value k by replacing e by another value from the created pool.
if the pool has kor fewer elements of type te failuredoc uses them all to create mutants.
otherwise failuredoc tries to choose kvalues thatare as different from one another as possible in the following way.
for each object of type tein the pool failuredoc computes its abstract object profile.
let abe the number of distinct abstract object profiles.
failuredoc randomly chooses dk ae values with each abstract object profile.
anabstract object profile is a boolean vector that abstracts the object s concrete state the values of its fields .
each field of the object maps to a distinct set of boolean values in the vector.
a concrete numerical value v of type int float etc.
maps to three abstract values v v and v .
a concrete boolean field value vmaps to two abstract values v true andv false .
a concrete enumeration value vchosen from choices e1 e jmaps to jabstract values v eifor each i j. a concrete object reference value vis mapped to two abstract values v null andv6 null .
a concrete array or collection value vis mapped to three abstract values as follows the same two abstract values as for objects since an array or collection is an object one abstract value for whether vis empty.
for example suppose the treemap class has two fields int size andset entryset which represent the map size and the internal data representation respectively.
the following table summarizes the corresponding abstract object profiles for an empty and a non empty treemap object.
tmeans the property holds.
a abstract object profile treemap size entryset object null6 null is empty?
an empty map f t f f t t a non empty map f f t f t f the above abstract object profile which is used in our implementation looks at the top level of the concrete representation.
the abstract object profile can go as deep in the object as desired.
it is straightforward to extend it to more of the concrete representation as follows enrich the abstraction of a concrete object reference or enumeration value by adding abstract values for each of its fields.
this corresponds to following two field