us and them a study of privacy requirements across north america asia and europe swapneel sheth gail kaiser department of computer science columbia university new y ork ny usa swapneel kaiser cs.columbia.eduwalid maalej department of informatics university of hamburg hamburg germany maalej informatik.uni hamburg.de abstract data privacy when using online systems like facebook and amazon has become an increasingly popular topic in the last few years.
however only a little is known about how users and developers perceive privacy and which concrete measures would mitigate their privacy concerns.
to investigate privacy requirements we conducted an online survey with closed and open questions and collected valid responses.
our results show that users often reduce privacy to security with data sharing and data breaches being their biggest concerns.
users are more concerned about the content of their documents and their personal data such as location than about their interaction data.
unlike users developers clearly prefer technical measures like data anonymization and think that privacy laws and policies are less e ective.
we also observed interesting di erences between people from di erent geographies.
for example people from europe are more concerned about data breaches than people from north america.
people from asia paci c and europe believe that content and metadata are more critical for privacy than people from north america.
our results contribute to developing a user driven privacy framework that is based on empirical evidence in addition to the legal technical and commercial perspectives.
categories and subject descriptors d. .
requirements speci cations k. .
public policy issues privacy general terms human factors keywords human factors in software engineering requirements engineering privacy user developer collaboration interaction data empirical studies permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
icse may june hyderabad india copyright acm ... .
.
.
introduction as systems that collect and use personal data such as facebook and amazon become more pervasive in our daily lives users are starting to worry about their privacy.
there has been a lot of media coverage about data privacy.
one of the earliest articles in the new york times reported how it was possible to break the anonymity of aol s search engine s users .
a more recent article mentions privacy concerns about google glass .
both technical and especially nontechnical users are nding it increasingly hard to navigate this privacy mine eld .
this is further exacerbated by well known systems periodically making changes that breach privacy and not allowing users to opt out a priori .
there is a large body of research on privacy in various research communities.
this ranges from data anonymization techniques in di erent domains to novel approaches to make privacy settings more understandable .
recent studies have shown that there is a discrepancy between users intentions and reality for privacy settings .
the assumption behind most of this work is that privacy is well speci ed and important.
however there is very little evidence about what exactly are the user concerns priorities and trade o s and how users think these concerns can be mitigated.
in particular in the software engineering community there have been no systematic studies to nd out what privacy requirements are and how these requirements should be addressed by developers.
this research aims to understand the privacy expectations and needs for modern software systems.
to this end we conducted an online survey.
we received responses and selected of them as valid.
the responses represented diverse populations including developers and users and people from north america europe and asia.
the results of our study show that the biggest privacy concerns are data sharing and data breaches.
however there is a disagreement on the best approach to address these concerns.
with respect to types of data that are critical for privacy respondents are least concerned about metadata and interaction data and most concerned about their personal data and the content of documents.
most respondents are not willing to accept less privacy in exchange for fewer advertisements and nancial incentives such as discounts on purchases.
the main contribution of this paper is threefold.
first it illustrates and quanti es the general trends on how users understand privacy and on how they assess di erent privacy concerns and measures to address them.
second the paper identi es di erences in privacy expectations between various groups developers versus users and people from di erentpermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
icse may june hyderabad india copyright acm ... .
geographic regions.
finally the paper gives insights into how software developers and managers can identify analyze and address privacy concerns of their users building a rst step towards a software engineering privacy framework.
our analysis for geographic regions for example shows that there is a signi cant di erence between respondents from north america europe and asia paci c. people from europe and asia paci c rate di erent types of data such as metadata content and interaction data being a lotmore critical for privacy than respondents from north america.
people from europe are a lot more concerned about data breaches than data sharing whereas people from north america are equally concerned about the two.
similarly our analysis for developers versus users shows a marked di erence between the two groups.
for example developers believe that privacy laws and policies are lesse ective for reducing privacy concerns than data anonymization.
the rest of the paper is organized as follows.
section describes the design of our study.
sections and highlight its key results.
section discusses the implications of the results and their limitations.
finally section describes related work and section concludes the paper.
.
study design we describe the research questions methods and respondents of our study.
.
research questions there have been many di erent de nitions of privacy over time.
one of the earliest de nitions was the right to be left alone as described by warren and brandeis .
solove claims that privacy is an umbrella term referring to a wide and disparate group of related things .
the author proposes a taxonomy of privacy in the context of harmful activities such as information collection information processing information dissemination and invasion .
according to the merriam webster dictionary privacy is the freedom from unauthorized intrusion .
we are interested speci cally in data privacy and other notions of privacy such as physical privacy are beyond the scope of our work.
the goal of this study is to gather and analyze privacy requirements for modern software systems.
in particular we want to study the perception of di erent groups of people on privacy.
we focused on the following research questions rq what are developers and users perceptions of privacy?
what aspects of privacy are more important and what are the best measures to address them?
section rq does software development experience have any impact on privacy requirements?
section rq does geography have any impact on privacy requirements?
section by perception we mean the subjective understanding and assessment of privacy aspects.
since privacy is a very broad term we are interested in speci c aspects in particular types of concerns measures to mitigate these concerns types of data that are critical to privacy and whether people would give up privacy.
we think these aspects are most related to software and requirements engineering concerns.
.
research method we designed an online survey with questions which took minutes to answer.
out of the questions were closed and respondents had to choose an answer from a list of options.
the survey also had two open ended questions.
this helped us get qualitative insights about privacy and gave an opportunity for respondents to report aspects that were not already included in the closed questions.
we chose a survey instead of observations or interviews for the following reasons.
first surveys are scalable and allow to get a large number and broad cross section of responses.
second we were interested in the subjective opinion of people and this can be di erent from real behavior.
third the closed questions were purely quantitative and allowed us to analyze general trends and correlations.
we did not aim for a representative report of the opinions.
this would have been possible only through a well de ned population and random representative sampling.
instead we were interested in the priority trends and inter relations which can be analyzed through a cross tabulation of the survey answers.
we used semantic scales for the closed questions allowing for the measurement of subjective assessments while giving respondents some exibility of the interpretation .
for example one question was would users be willing to use your system if they are worried about privacy issues?
and the answer options were de nitely yes users don t care about privacy probably yes unsure probably not and de nitely not if there are privacy concerns users will not use this system .
to reduce the complexity of matrix questions which include multiple answer options we used a point scale consisting of yes no and uncertain .
when we observed in the dry runs that higher discriminative powers were needed we used a point scale .
respondents could choose to ll out our survey in two languages english or german.
for each language there were two slightly di erent versions based on whether the respondents had experience in software development or not.
the di erence in the versions was only in the phrasing of the questions in order to reduce confusion.
for example developers were asked would users be willing to use your system if they are worried about privacy issues?
whereas users were asked would you be willing to use the system if you are worried about privacy issues?
to increase the reliability of the study we took the following measures pilot testing we conducted pilot testing in four iterations with a total of ten users that focused on improving the timing and understandability of the questions.
we wanted to reduce ambiguity about the questions and answers and ensure that none of the semantics were lost in translation.
we used the feedback from pilot testing to improve the phrasing and the order of questions for the english and german versions.
random order of answers the answer options for the closed questions were randomly ordered.
this ensures that answer order does not in uence the response .
validation questions to ensure that respondents did not ll out the answers arbitrarily we included two validation questions .
for example one of the validation questions was what is the sum of and ?
respondents who did not answer these correctly were not included in the nal set of valid responses.860table summary of study respondents based on location and software development experience developers users north america europe asia south america africa post sampling we monitored the number of respondents from each category of interest developers users and geographic location.
we conducted post sampling and strati cation to ensure that we got su cient responses for each category and that the ratio of developers to users for each geographic location was roughly similar.
for categories that did not have su cient respondents we targeted those populations by posting the survey in speci c channels.
we stopped data collection when we had a broad spectrum of respondents and su cient representation in all the categories.
finally to corroborate our results we conducted a number of statistical tests.
in particular we used the z test for equality of proportions and welch s two sample t test to check if our results are statistically signi cant.
.
survey respondents we did not have any restrictions on who could ll out the survey.
we wanted in particular people with and without software development experience and people from di erent parts of the world.
we distributed our survey through a variety of channels including various mailing lists social networks like facebook and twitter personal contacts and colleagues.
we circulated the survey across companies with which we are collaborating.
we also asked speci c people with many contacts e.g.
with many followers on twitter to forward the survey.
as an incentive two ipads were ra ed among the respondents.
in total respondents lled out our survey between november and september .
filtering out the incomplete and invalid responses resulted in valid responses .
.
table shows the respondents based on location and software development experience.
the four versions of the survey along with raw data and summary information are available on our website1.
among the respondents have software development experience and do not.
for respondents with development experience have less than one year of experience have years have years and have more than ten years of experience.
respondents live in north america in europe and in asia paci c. are a liated with industry or public sector are in academia and research and are students.
.
privacy perceptions we asked respondents how important is the privacy issue in online systems?
they answered using a point semantic scale ranging from very important to least important .
two thirds of the respondents chose very important .
important and the remaining three options average less important least important combined were chosen by a total of .
of the respondents.
the location of the data storage was a key concern for the respondents.
we asked respondents whether privacy concerns depend on the location of where the data is stored and provided a point semantic scale with options yes maybe yes unsure maybe not and no .
.
of the respondents chose yes .
chose maybe yes while only .
of the respondents chose the remaining three options.
on the other hand there was disagreement about whether users would be willing to use such systems if there were privacy concerns.
the answer options were de nitely yes users don t care about privacy probably yes unsure probably not and de nitely not if there are privacy concerns users will not use this system .
.
of the respondents choose unsure while .
and .
chose probably yes and probably not respectively.
.
factors that increase and reduce privacy concerns we asked respondents if the following factors would increase privacy concerns data aggregation the system discovers additional information about the user by aggregating data over a long period of time.
data distortion the system might misrepresent the data or user intent.
data sharing the collected data might be given to third parties for purposes like advertising.
data breaches malicious users might get access to sensitive data about other users.
for each concern the respondents could answer using a point semantic scale with the options yes uncertain and no .
we also asked respondents if the following would help to reduce concerns about privacy privacy policy license agreements etc.
describing what the system will won t do with the data.
privacy laws describing which national law the system is compliant with e.g.
hipaa in the us european privacy laws .
anonymizing all data ensuring that none of the data has any personal identi ers.
technical details describing the algorithms source code of the system in order to achieve higher trust e.g.
encryption of data .
details on usage describe e.g.
in a table how di erent data are used.
figure shows the overall answers for both questions.
in the gure each answer option is sorted by the number of yes respondents.
most respondents agreed that the biggest privacy concerns are data breaches and data sharing.
there is disagreement about whether data distortion and data aggregation would increase privacy concerns.
to check if these results are statistically signi cant we ran z tests for equality of proportions.
this would help us validate for example if there is a statistically signi cant di erence in the number of respondents who said yes for two di erent861 responsesaggregationdistortionsharingbreachincrease concernincrease concern technical detailspolicylawsusageanonymization 400reduce concernreduce concern no uncertain yesfigure what increases and reduces privacy concerns?
table what increases privacy concerns?
for each privacy concern x y indicates that xis a bigger concern than yfor the respondents.
we used the ztest for equality of proportions and only statistically signi cant results for p 01are shown.
privacy concerns p values sharing aggregation p 231e sharing distortion p 036e breach aggregation p 2e breach distortion p 2e options.
the results for increasing concerns about privacy are shown in table .
for all of these tests the null hypothesis is that a similar fraction of the respondents chose yes for both options.
the results show that the concerns about data breaches and data sharing are signi cantly higher than data aggregation and data distortion p 231e .
hypothesis people are more concerned about the security aspects of privacy in particular about data sharing and data breaches than data distortion and data aggregation.
forreducing concerns respondents consider technical details the least e ective option with p values ranging from 218e 10for comparing to policy to 2e 16for comparing to anonymization .
respondents think that anonymization is the most e ective option for mitigating privacy concerns and signi cantly better than privacy laws p and privacy policy p .
there is however no statistically signi cant di erence between anonymization and providing usage details p .
the remaining three options privacy policy privacy laws and usage details had similar responses and none of their combinations for the z test yielded statistically signi cant results for p .
hypothesis there is less agreement on the best measures for mitigating privacy concerns.
.
qualitative feedback from the respondents we collected comments on our open questions about additional privacy concerns and measures to reduce them.
we analyzed these comments manually in three steps.
first we read each comment andannotated it with a few keywords.
thereby we tried to reuse the keywords whenever possible.
second we uni ed and grouped the keywords into topics making sure that no important comments are lost.
finally we read the comments again and assigned each of them to the identi ed topics.
.
.
additional privacy concerns we collected comments on additional privacy concerns.
comments were useless as they just repeated the standard response options were not understandable or without content e.g.
no comment or nothing more .
the remaining comments gave interesting insights which can be grouped into the following topics authorities and intelligent services respondents mentioned authorities and intelligent services as an additional privacy concern.
one wrote government access is not exactly a data breach but still a privacy concern .
another commented anyway there is prism .
it is important to mention that about half of the responses were collected after the nsa prism scandal .
apis program correctness and viruses nine respondents mentioned concerns related to the program behavior including malicious programs and viruses.
respondents also mentioned that privacy concerns are transmitted through the application programming interfaces of the tools collecting data.
one respondent wrote sharing data over api while others mentioned speci c systems such as google analytics or facebook api.
three respondents speci cally pointed the correctness of privacy implementation as a speci c concern.
unusable and nontransparent policies seven users complained about unusable privacy implementations with unclear nontransparent policies.
these respondents were concerned because most users simply do not know which data is being collected about them and for what purposes.
one respondent wrote companies and software developers shield themselves by making consumers agree on a long convoluted and often a hard to understand hard to read policy.
companies know that people do not read them a tactic on which they are banking .
another gave a more concrete example sometimes sharing sensitive data862is activated by default in applications unaware users would leave it so .
one respondent wrote transparency and letting the user choose make a huge di erence.
maybe not in the beginning and maybe not for all users but de nitely for a speci c user group .
intentional or unintentional misuse at least seven respondents mentioned di erent forms of misusing the data as main concerns.
this includes commercial misuse such as making products of interest more expensive but it could also be misused for social and political purposes.
apart from abusing the data to put pressure on users respondents mentioned using fake data to manipulate public opinions or inferencing sensitive information about groups of people and minorities.
one respondent wrote whenever something happen the media uses their data accessible online to sell this person as good or evil .
lack of control seven respondents mentioned the lack of control and in particular options to delete data collected about them as their main concern.
one wrote if we agree to give the data we are not able anymore to revise this decision and delete the data.
even if the service con rms the deletion we don t have any mean of control .
another respondent explicitly mentioned the case where companies owning their data are bankrupt or sold and in this case the privacy of their data is also lost company a has a decent privacy policy company b acquires the company and in so now has access to company a s data .
combined data sources five respondents explicitly mentioned combining data about users from di erent sources as a main privacy concern.
in most cases this cannot be anticipated when developing or using a single system or a service.
one respondent wrote it s di cult to anticipate or assess the privacy risk in this case .
another claimed continuous monitoring combined with aggregation over multiple channels or sources leads to complex user pro ling.
it s disturbing to know that your life is monitored on so many levels .
collecting and storing data five respondents wrote that collecting and storing data is on its own a privacy concern.
in particular respondents complained about too much data being collected about them and stored for too long time.
one respondent mentioned the sheer amount of cookies that are placed on my computer just by landing on their website .
another claimed collecting the data and storing for a long period of time is seen more critical than just collecting .
other issues three respondents mentioned problems with the legal framework and in particular the compatibility of laws in the developer and user countries.
three respondents said that in some cases there is no real option to not use a system or service e.g.
due to a social pressure as all use facebook or since we depend on technology .
.
.
suggestions for reducing privacy concerns in total respondents answered the open question on additional measures to reduce user concerns about privacy.
ten of these answers either repeated the standard options or were useless.
the remaining comments showed more convergence in the opinion than the comments on the additional concerns possibly because this question was more concrete.the suggestions can be grouped into the following measures easy and ne grained control over the data including access and deletion respondents recommended allowing the users to easily access and control the collected and processed data about them.
in particular respondents mentioned the options of deactivating the collection and deleting the data.
one respondent wrote to alleviate privacy concerns it should be possible to opt out of or disagree with certain terms .
another wrote allow users to access a summary of all the data stored on their behalf and allow them to delete all or part of it if they desire .
the respondents also highlighted that this should be simple and easy to do and embedded into the user interface at the data level.
certi cation from independent trusted organizations respondents suggested introducing a privacy certi cation mechanism by independent trusted authorities.
a few also suggested the continuous conduction of privacy audits similar to other elds such as safety and banking.
respondents also suggested that the results of the checks and audits should be made public to increase the pressure on software vendors.
one respondent even suggested having a privacy police to check on how data is handled .
transparency and risk communication open source respondents mentioned increased transparency about the collection aggregation and sharing of the data.
in particular respondents mentioned that the risks of misusing the data should be also communicated clearly and continuously.
three respondents suggested that making the code open source would be the best approach for transparency.
one wrote tell users maybe in the side bar how they are being tracked.
this would educate the general public and ideally enable them to take control of their own data .
the spectrum of transparency was from the data being collected to physical safety measures of servers and quali cations of people handling data to how long the data is stored.
period and amount of data respondents recommended always limiting and minimizing the amount of data and the period of storage referring to the principle of minimality.
the period of time for storing the data seems to be crucial for users.
one wrote not allowing users data being stored in servers.
just maintaining them in the source .
security and encryption we noted that respondents strongly relate privacy issues to information security.
at least seven suggested security measures mainly complete encryption of data and communication channels.
trust and education seven respondents mentioned building trust in the system and vendor as well as education of users on privacy as e ective means to reduce privacy concerns.
short usable precise and understandable description in the ui at least six respondents mentioned increasing the usability to access data and policy as an important measure to reduce privacy concerns.
one wrote the disclaimer should be directly accessible from the user interface when conducting a function which needs my data .
another respondent wrote short understandable description and no long complicated legal text .
responsesmetadatainteractionpreferenceslocationpersonal datacontent 100metadatainteractionpreferenceslocationpersonal datacontent very critical critical neutral somewhat uncritical uncriticalfigure how critical would you rate the collection of the following data?
.
criticality of different types of data to get a deeper insight into the privacy criticality of di erent types of data we asked respondents to rate the following types of data on a point semantic scale ranging from very critical to uncritical .
content of documents such as email body metadata such as date interaction such as a mouse click to open or send an email user location such as the city from where the email was sent name or personal data such as email address user preferences such as inbox or email settings the results are shown in figure .
respondents chose content as most critical followed by personal data location preferences and interaction and metadata are the least critical as far as privacy is concerned.
we used welch s two sample t test to compare if the di erence among the di erent types of data is statistically signi cant.
the null hypothesis was that the di erence in means was equal to zero.
table summarizes the results.
it shows for example that there is no statistically signi cant di erence between content and personal data.
on the other hand there is a statistically signi cant di erence between content and location for p .
hypothesis people are more concerned about content and personal data than interaction and metadata.
.
giving up privacy we asked respondents if they would accept lessprivacy for the following monetary discounts e.g.
discount on the next purchase intelligent or added functionality of the system such as the amazon recommendations fewer advertisements for each option the respondents could answer using a point semantic scale having options yes uncertain and no .
the results are shown in figure .
responsesadsmoneyfunctionality responses200 responses no uncertain yesfigure would users accept lessprivacy for the following?
.
of the respondents said they would accept less privacy for added functionality of the system while only .
and .
would accept less privacy for monetary discounts and fewer advertisements respectively.
added functionality seems to be the most important reason to accept less privacy.
these results are statistically signi cant using the z test for equality of proportions p 882e 5for monetary discounts andp 854e 9for fewer advertisements .
it is important to note that less than half of the respondents would accept less privacy for added functionality of the system.
previous studies such as the one conducted by acquisti et al.
have shown however that people s economic valuations of privacy vary signi cantly and that people doaccept less privacy for monetary discounts.
this contrast in results might be due to a di erence between people s opinion and their actual behavior.
hypothesis there are di erent groups of opinions about accepting less privacy for certain bene ts.
the largest group of users say that they are not inclined to give up privacy for additional bene ts.
however their actual behavior might be di erent.
.
developer vs user perceptions the results from the previous section describe the broad concerns for all respondents of our survey.
in this section we report on the important results from a di erential analysis between two groups of respondents developers out of versus users of software systems out of .
we used z tests for equality of proportions for the rest of this section unless otherwise noted.864table the signi cance in the di erence between the criticality of collecting di erent data.
p values forp e for p e for p and for p .
the rows and columns are ordered from most to least critical.
for each cell t tests compare if the di erence in criticality is statistically signi cant.
for example the di erence between interaction and content is statistically signi cant for p e .
content personal data location preferences interaction metadata content personal data location preferences interaction metadata .
privacy concerns data distortion .
of developers believe that data distortion is an important privacy concern.
the percentage of users on the other hand is .
.
the di erence between these two groups is statistically signi cant p .
data aggregation .
of developers believe that data aggregation is an important privacy concern.
the percentage of users on the other hand is .
.
the di erence between them is statistically signi cant p .
it seems that developers trust their systems more than users when it comes to wrong interpretation of sensitive data.
data criticality developers believe that name and personal data p and interaction p are more critical for privacy compared to users.
on the other hand for the remaining four categories content location preferences metadata there is no statistically signi cant di erence between the perceptions of developers and users p for all .
we used welch s two sample t test here.
less privacy for added functionality a larger fraction of developers .
would accept less privacy for added or intelligent functionality of the system compared to .
of users p .
hypothesis developers are more concerned about interaction name and personal data whereas users are more concerned about data distortion and data aggregation.
.
measures to reduce concerns developers and reducing concerns a larger fraction of developers .
feel that data anonymization is a better option to reduce privacy concerns as compared to privacy policies or privacy laws both .
p .
.
of developers prefer providing details on data usage for mitigating privacy concerns compared to privacy policies .
p .
similarly .
of developers feel that privacy policies will notreduce privacy concerns whereas only .
feel that providing details on data usage will notbe bene cial p .
users and reducing concerns in contrast for users there is no statistically signi cant di erence between their perception on privacy policies laws anonymization and providing usage details.
pfor all combinations .
hypothesis developers prefer anonymization and providing usage details as measures to reduce privacy concerns.
users on the other hand do not have a strong preference.
.
the role of geography in this section we present the results of the di erential analysis based on the geography of respondents.
we asked respondents to specify with which region of the world they identify themselves.
the options were north america south america europe asia paci c africa and other.
since we have only seven responses from south america and africa combined we focus on the di erences between the others.
we used the z test for equality of proportions for the rest of this section unless otherwise noted.
data criticality we asked respondents to rate the criticality of the di erent types of data for privacy i.e.
content of documents metadata interaction user location name or personal data user preferences using a semantic scale from with being very critical and being uncritical .
there is a statistically signi cant di erence between respondents from north america europe and asia paci c. we used welch s two sample t test to compare the ratings given by respondents.
respondents in north america think that all types of data are less critical overall mean across the six types of data is .
than respondents in europe overall mean is .
for p 144e .
similarly respondents from north america think all items are less critical that those in asia paci c overall mean .
forp .
on the other hand there is no statistically signi cant di erence between respondents in europe and asia paci c p .
less privacy for added functionality a larger fraction of respondents in europe .
claim that they would not give up privacy for added functionality.
in north america on the other hand this fraction is .
.
the di erence between the two regions is statistically signi cant p .
hypothesis people from north america are more willing to give up privacy and feel that di erent types of data are less critical for privacy compared to people from europe.
concerns about data sharing versus data distortion a larger fraction of respondents in north america .
feel that data sharing is a concern compared to .
for data distortion p 093e .
on the other hand there is no statistically signi cant di erence among respondents in asia paci c p .
concerns about data sharing versus data breach in europe a larger fraction of the respondents .
are concerned about data breaches as compared to .
865for data sharing.
the di erence is statistically signi cant p 435e .
on the other hand there is no statistically signi cant di erence among respondents in north america p .
laws versus usage details in europe a larger fraction of respondents .
feel that providing details on how the data is being used will reduce privacy concerns as opposed to .
who feel that privacy laws will be e ective p .
on the other hand there is no statistically signi cant di erence among respondents in north america where the percentage of respondents are .
and .
respectively p .
usage details versus privacy policies a larger fraction of respondents in europe .
feel that providing usage details can mitigate privacy concerns compared to .
for using a privacy policy p .
on the other hand there is no statistically signi cant di erence among respondents in north america p .
hypothesis people from europe feel that providing usage details can be more e ective for mitigating privacy concerns than privacy laws and privacy policies whereas people from north america feel that these three options are equally e ective.
.
discussion we discuss our results potential reasons and the implications for software developers and analysts.
we also re ect on the limitations and threats to validity of our results.
.
privacy interpretation gaps data privacy is often an implicit requirement everyone talks about it but no one speci es what it means and how it should be implemented.
this topic also attracts the interests of di erent stakeholders including users lawyers sales people and security experts which makes it even harder to de ne and implement.
one important result from our study is that while almost all respondents agree about the importance of privacy the understanding of the privacy issues and the measures to reduce privacy concerns are divergent.
this calls for an even more careful and distinguished analysis of privacy when designing and building a system.
our results from sections and show there is a de nite gap in privacy expectations and needs between users and developers and between people from di erent regions of the world.
developers have assumptions about privacy which do not always correspond to what users need.
developers seem to be less concerned about data distortion and aggregation compared to users.
it seems that developers trust their systems more than users when it comes to wrong interpretation of privacy critical data.
unlike users developers prefer anonymization and providing usage details for mitigating privacy concerns.
if the expectations and needs of users do not match those of developers developers might have wrong assumptions and might end up making wrong decisions when designing and building privacy aware software systems.
in addition privacy is not a universal requirement as it appears to have an internationalization aspect to it.
di erent regions seem to have di erent concrete requirements and understanding for privacy.
our results con rm that there exist many cultural di erences between various regions of theworld as far as privacy is concerned.
the recent nsa prism scandal has also brought these di erences into sharp focus.
a majority of americans considered nsa s accessing personal data to prevent terrorist threats more important that privacy concerns .
in contrast there was widespread outrage in europe over these incidents .
it also led to an article in the new york times by malte spitz a member of the german green party s executive committee titled germans loved obama.
now we don t trust him .
these di erences both in terms of laws and people s perceptions should be considered carefully when designing and deploying software systems.
we think that privacy should become an explicit requirement with measurable and testable criteria.
we also think that privacy should also become a main design criteria for developers as software systems are collecting more and more data about their users .
to this end we feel that there is a need to develop a standard survey for privacy that software teams can customize and reuse for their projects and users.
our survey can be reused to conduct additional user studies on privacy for speci c systems.
our results can also serve as a benchmark for comparing the data.
this can help build a body of knowledge and provide guidelines such as best practices.
.
the security dimension of privacy we think that people are more concerned about data breaches and data sharing as there have been many recent instances that have received a lot of media coverage.
to list a few recent examples sony su ered a massive data breach in its playstation network that led to the theft of personal information belonging to million users .
one hundred and sixty million credit card numbers were stolen and sold from various companies including citibank the nasdaq stock exchange and carrefour .
the federal trade commission publicly lent support to the do not track system for advertising .
compared to these high pro le stories we feel that there have been relatively few famous instances of privacy problems caused by data aggregation or data distortion yet.
there is a large body of research that has advanced the state of the art in security encryption and authorization.
one short term implication for engineers and managers is to systematically implement security solutions when designing and deploying systems that collect user data even if it is not a commercially or politically sensitive system.
this would signi cantly and immediately reduce privacy concerns.
for the medium term more research should be conducted for deployable data aggregation and data distortion solutions.
as far as mitigating privacy concerns our results show that there is more disagreement.
we believe that the reason for this is that online privacy concerns are a relatively recent phenomenon.
due to this people are not sure which approach works best and might be bene cial in the long run.
.
privacy framework we feel that an important direction for software and requirements engineering researchers is to develop a universal empirically grounded framework for collecting analyzing and implementing privacy requirements.
this study is the rst building block towards such a framework.
some of the lessons learned from our study can be translated into concrete qualities and features which should be part of such a framework.
this includes anonymization this is perhaps the most well known privacy mitigating technique and seems to be perceived as an important and e ective measure by both users and developers.
developers should therefore use anonymization algorithms and libraries.
data usage although anonymization is perceived as the most e ective measure for addressing privacy concerns this is currently not practical as approaches like di erential privacy are computationally infeasible .
in such situations it is better to provide users with data usage details and make these more transparent and easier to understand.
our ndings show that there is no statistical di erence between anonymization and providing usage details as far as users are concerned.
thus in terms of future research it is perhaps better to focus on improving techniques for providing data usage details rather than or in addition to making anonymization computationally feasible.
default encryption as users are mainly concerned about the loss and abuse of their data systems collecting user data should implement and activate encryption mechanism for storing and transmitting these data.
in facebook e.g.
the default standard communication protocol should be https and not http.
fine grained control over the data users become less concerned about privacy if the system provides a mechanism to control their data.
this includes activating and deactivating the collection at any time the possibility to access and delete the raw and processed data and de ne who should have access to what data.
interaction data rst users have a rather clear preference of the criticality of the di erent types of data collected about them.
therefore software researchers and designers should rst try to implement their systems based on collecting and mining interaction data instead of content of les and documents.
research has advanced a lot in this eld in especially recommender systems .
time and space limited storage the storage of data about users should be limited in time and space.
the location where the data is stored is an important factor for many respondents.
therefore systems should provide options for choosing the location of storing privacy sensitive data.
privacy policies laws and usage details users rated all these options as equally e ective for mitigating their privacy concerns.
therefore developers could utilize any of these options thus giving them better exibility in the design and deployment of software systems.
.
limitations and threats to validity there are several limitations to our study which we discuss in this section.
the rst limitation is a potential selection bias.
respondents who volunteered to ll out our survey were self selected.
such selection bias implies that our results are only applicable to the volunteering population and may not necessarily generalize to other populations.
the summaries have helped us identify certain trends and hypotheses and these should be validated and tested by representative samples e.g.
for certain countries.
in contrast the di erential analysis also called pseudo experimentation conductedwithin our set of respondents enabled us to identify statistically signi cant relationships and correlations.
hence many of our results deliberately focus on correlations and cross tabulations between di erent populations.
as for internal validity we are aware that by lling out a brief survey we can only understand a limited amount of concerns that the respondents have in mind.
similarly the format and questions of the survey might constrain the expressiveness of some of the respondents.
we might have missed certain privacy concerns and measures to reduce concerns by the design of the survey.
we tried to mitigate this risk by providing open ended questions that respondents could use to express additional aspects they had in mind.
moreover we designed the survey in a highly iterative process and tested it in dry runs to ensure that all options are understandable and that we did not miss any obvious option.
as with any online survey there is a possibility that respondents did not fully understand the question or chose the response options arbitrarily.
we conducted several pilot tests gave the option to input comments and the incompletion rate is relatively small.
we included a few validation questions and we only report responses in this paper from respondents who answered these questions correctly.
we also provided two versions of the survey in english and german to make it easier for non native speakers.
in spite of these limitations we managed to get a large and diverse population that lled out our survey.
this gives us con dence about the overall trends reported in this paper.
.
related work there has been a lot of research about privacy and security in di erent research communities.
we summarize the important related work focussing on usability and economic aspects of privacy anonymization techniques and work from the software and requirements engineering community.
many recent studies on online social networks show that there is a typically large discrepancy between users intentions for what their privacy settings should be versus what they actually are.
for example madejski et al.
report in their study of facebook that of their participants n were sharing something they intended to hide and were hiding something that they intended to share.
liu et al.
found that facebook s users privacy settings match their expectations only of the time.
a recent longitudinal study by stutzman et al.
shows how privacy settings for facebook users have evolved over a period of time.
these studies have focused on privacy settings in a speci c online system whereas our study was designed to be agnostic to any modern system collecting user sensitive data.
further the main contribution of these studies is to show that there is a discrepancy between what the settings are and what they should be and how settings evolve over time.
our study aims to gain a deeper understanding of what the requirements are and how they change across geography and depending on software development experience.
fang and lefevre proposed an automated technique for con guring a user s privacy settings in online social networking sites.
paul et al.
present using a color coding scheme for making privacy settings more usable.
squicciarini shehab and paci propose a game theoretic approach for collaborative sharing and control of images in a social network.
toubiana et al.
present a system that automatically applies users privacy settings for photo tagging.867all these papers propose new approaches to make privacy settings better from a user s perspective i.e.
more usable and more visible .
our results help development teams decide when and which of these techniques should be implemented.
we focus more on a broader requirements and engineering perspective of privacy than on a speci c technical perspective.
there has been a lot of recent work on the economic rami cations of privacy.
for example acquisti et al.
and the