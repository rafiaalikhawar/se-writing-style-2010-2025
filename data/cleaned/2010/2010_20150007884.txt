statistical symbolic execution with informed sampling antonio filieri university of stuttgart stuttgart germanycorina s. p as areanu carnegie mellon silicon valley nasa ames moffet field ca usawillem visser and jaco geldenhuys stellenbosch university stellenbosch south africa abstract symbolic execution techniques have been proposed recently for the probabilistic analysis of programs.
these techniques seek to quantify the likelihood of reaching program events of interest e.g.
assert violations.
they have many promising applications but have scalability issues due to high computational demand.
to address this challenge we propose a statistical symbolic execution technique that performs monte carlo sampling of the symbolic program paths and uses the obtained information for bayesian estimation and hypothesis testing with respect to the probability of reaching the target events.
to speed up the convergence of the statistical analysis we propose informed sampling an iterative symbolic execution that first explores the paths that have high statistical significance prunes them from the state space and guides the execution towards less likely paths.
the technique combines bayesian estimation with a partial exact analysis for the pruned paths leading to provably improved convergence of the statistical analysis.
we have implemented statistical symbolic execution with informed sampling in the symbolic pathfinder tool.
we show experimentally that the informed sampling obtains more precise results and converges faster than a purely statistical analysis and may also be more efficient than an exact symbolic analysis.
when the latter does not terminate symbolic execution with informed sampling can give meaningful results under the same time and memory limits.
categories and subject descriptors d. .
software program verification model checking reliability statistical methods .
introduction several techniques have been proposed recently for the probabilistic analysis of programs .
these techniques have multiple applications ranging from program understanding and debugging to computing reliability of software operating in uncertain environments.
for example in previous work we described a bounded symbolic execution of a program that uses a quantification procedure over the collected symbolic constraints to compute the counts permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
copyright 20xx acm x xxxxx xx x xx xx ... .
.of the inputs that follow the explored program paths.
these counts are then used to compute the probability of executing different paths through the program or of violating program assertions under given probabilistic usage profiles.
while promising these exact techniques have scalability issues due to the large number of symbolic paths to be explored.
to address this problem we describe a statistical symbolic execution technique that uses randomized sampling of the symbolic paths.
for deciding termination of sampling we investigate two different criteria bayesian estimation and hypothesis testing .
the first is used to estimate the probability of executing designated program paths while the latter is used to test a given hypothesis about such probability.
unlike in a typical statistical setting where one samples randomly across a concrete input domain our samples are done in the context of symbolic execution according to conditional probabilities computed at each branching point in the program.
this approach is similar to statistical model checking with the difference that we work with code not with models and we sample symbolic paths where the probabilistic information is computed based on the collected symbolic constraints.
when using bayesian estimation the randomized sampling terminates when pre specified confidence and error bounds accuracy have been achieved.
the answer to the analysis problem is not guaranteed to be correct but the probability of a wrong answer can be made arbitrarily small .
however in practice the convergence to an answer might be very slow.
hypothesis testing can be faster but both techniques may require a very large number of sample paths to achieve the desired statistical confidence.
to speed up both methods we propose informed sampling is an iterative technique combining statistical methods with partial exact analysis.
at each iteration is randomly samples a set of execution paths and performs a statistical analysis of the sample.
the probability of sampling each path is proportional to the number of input points following it under the specified usage profile so not to bias the sample.
if the statistical method converged its result is returned.
otherwise the already sampled paths are pruned out from the execution tree and analyzed exactly.
the next iteration will then focus on the analysis of only the remaining part of the execution tree increasing also the chances of selecting low probability paths that might have not been sampled and pruned during the previous iterations.
for pruning the sampling space we propose an efficient procedure that leverages the counts of the inputs associated with each explored symbolic path and subtracts them from the counts of all the prefixes along the path.
the intuition is that at the end of each iteration the counts should keep track of the number of inputs that still need to be explored sampled for the execution to follow that path.
the counts keep decreasing with each iteration and if a counter be comes it means that the sub tree rooted at that node has been fully explored and can be safely pruned from the search space.
for estimating the probability results we propose a combination of exact analysis for the paths that are pruned in previous iterations and bayesian statistical analysis for the paths sampled in the current iteration over the pruned state space .
the analysis terminates when the pre specified confidence and error bounds have been achieved for bayesian estimation or when the hypothesis is confirmed for hypothesis testing .
the analysis may also terminate when all the paths have been explored in which case the results will be the same as for the exact analysis.
is converges faster and requires fewer samples than the purely random sampling techniques since the set of samples is different with each iteration and each pruned path set is analyzed exactly with confidence .
furthermore the probability of finding the target program events increases with each iteration.
the main focus of this work is on computing non functional properties of programs such as the probability of successful termination or conversely the probability of failure under a given usage profile .
however statistical symbolic execution with is can also be used for improving classical non probabilistic symbolic execution in the sense that if symbolic execution runs out of resources time memory the statistical techniques can be used to provide useful information with statistical guarantees.
note also that the statistical techniques provide an any time approach to software analysis the longer they run the better the results.
we make the following contributions statistical symbolic execution with two stopping criteria bayesian estimation and hypothesis testing and implementation within the symbolic pathfinder tool is that converges faster than monte carlo sampling an efficient procedure for pruning the state space for incremental symbolic execution combined statistical and exact information for bayesian estimation and hypothesis testing experimental evidence showing the improvement of is over state of the art statistical approaches.
.
background .
symbolic execution symbolic execution is an extension of normal execution in which the semantics of the basic operators of a language is extended to accept symbolic inputs and to produce symbolic formulas as output .
the behavior of a program pis determined by the values of its inputs and can be described by means of a symbolic execution tree where tree nodes are program states and tree edges are the program transitions as determined by the symbolic execution of program instructions.
the state sof a program is defined by the tuple ip v pc where iprepresents the next instruction to be executed vis a mapping from each program variable vto its symbolic value i.e.
a symbolic expression in terms of the symbolic inputs and pcis apath condition .pcis a conjunction of constraints over the symbolic inputs that characterizes exactly those inputs that follow the path from the program s initial state to state s. the current state sand the next instruction ipdefine the set of transitions from s. without going into the details of every java instruction we informally define these transitions depending on the type of instruction pointed to by ip.assignment .
the execution of an assignment to variable v2v leads to a new state where ipis incremented to point to the next instruction and vis updated to map vto its new symbolic value.
pcdoes not change.
branch .
the execution of an if then else instruction on conditioncintroduces two new transitions.
the first leads to the state s1 where ip1points to the first instruction of the then block and the path condition is updated to pc1 pc c. the second leads to a state s2where ip2points to the first instruction of the elseblock and the path condition is updated to pc2 pc c. if the path condition associated with a branch is not satisfiable the new transition and state are not added to the symbolic execution tree.
loop .
awhile loop is unrolled until its condition evaluates to false or a pre specified exploration depth limit is reached.
analogous transformations are applied to other loop constructs.
the initial state of a program is s0 ip0 v0 pc0 where ip0 points to the first instruction of the main method v0maps the arguments of main if any to fresh symbolic values and pc0 true.
a program may also have one or more terminal states that represent conditions such as the successful termination of the program or an uncaught exception that aborts the program execution abruptly.
although our approach can be customized for any symbolic execution system we focus on symbolic pathfinder spf that works at the java bytecode level.
.
probability theory the possible outcomes of an experiment are called elementary events .
for example the rolling of a sided die may produce the elementary events and .
elementary events have to be atomic i.e.
the occurrence of one of them excludes the occurrence of any other.
the set of all elementary events is called a sample space .
in this paper we consider only finite and countable sample spaces meaning that the underlying set of elementary events is countable and finite.
definition p robability distribution .let be the sample space of an experiment.
a probability distribution on is any function pr p !
rthat satisfies the following conditions probability axioms pr fxg 0for every elementary event x pr pr a b pr a pr b for all events a b with a b the pair pr constitutes a probability space .
definition c onditional probability .let pr be a probability space.
let a and b be events a b and let pr b .
the conditional probability of the event a given that the event b occurs is pr ajb pr a b pr b pr ajb is also referred to as the probability of agiven b. definition l aw of total probability .let pr be a probability space and fbn n gbe a finite partition of .
then for any event a pr a npr ajbi pr bi the law of total probability can also be stated for conditional probabilities pr ajx npr ajx bi pr bijx where biare defined as in definition 3andxdoes not invalidate the assumptions of definition .
.
probabilistic analysis we follow previous work where we defined a symbolic execution framework for computing the probability of successful termination and alternatively the probability of failure for a java software component placed in a stochastic environment.
a failure can be any reachable error such as a failed assertion or an uncaught exception.
for simplicity we assume the satisfaction of target program properties to be characterized by the occurrence of a target event but our work generalizes to bounded ltl properties .
to deal with loops we run spf using bounded symbolic execution i.e.
a bound is set for the exploration depths.
the result of symbolic execution is then a finite set of paths each with a path condition.
some of these paths lead to failure some of them to success termination without failure and some of them lead to neither success nor failure they were interrupted because of the bounded exploration the latter are called grey paths.
the path conditions produced by spf consequently form three sets pcs fpcs pcs pcsmg pcf fpcf pcf pcf pg andpcg fpcg pcg pcg qg according to whether they lead to success failure or were truncated.
note that the path conditions are disjoint and cover the whole input domain.
in other words the three sets form a complete partition of the input domain .
not all input values are equally likely and we employ a usage profile to characterize the interaction of the software and the environment.
it maps each valid combination of inputs to the probability with which it may occur.
in we provide an extensive treatment of usage profiles and how they are used for the probabilistic computations.
for simplicity and without loss of generality we will assume here that the usage profile is embedded in the code.
this can be done with every usage profile where the probabilities piare described by arbitrary precision rational numbers.
more general usage profiles such as markov chains can be embedded as well they are analyzed in a bounded way.
given the output of spf and assuming the constraints from the usage profile have been embedded in the code the probability of success is defined as the probability of executing program pwith an input satisfying any of the successful path conditions recall the path conditions are disjoint prs p ipr pcs i the failure probability prf p and grey probability prg p have analogous definitions it is straightforward to prove that prs p prf p prg p .prg p can be used to quantify the impact of the execution bound on the quality of the analysis prg p .
in this paper we focus on sequential programs with integer inputs.
in other work we provide treatment of multi threading input data structures and floating point inputs see section7 .
.
.
quantification procedure we compute the probabilities of path conditions using a quantification procedure e.g.
for the path conditions.
we use latte to count models for linear integer constraints but our work generalizes straightforwardly to other tools such as qcoral for arbitrary floating point constraints and korat for heap data structures see for details .
given a finite integer domain d model counting allows us to compute the number of elements of dthat satisfy a given constraint c we denote this number by c a finite non negative integer .
by definition pr c is c d where d is the size of the domain implicitly assumed to be greater than zero .
the success probability or failure or grey probability can thens0 true s1 x s2 x s3 x x s4 x x figure partial symbolic execution tree of the example.
be computed using model counting as follows.
prs p ipr pcs i i pcs i d .
example in this section we illustrate the proposed techniques with a simple example.
consider the code in listing .
assume the goal is to estimate the success probability of the method test i.e.
the probability of not reaching line where an exception would be raised.
assume the input variables x y and zrange over the integer domain .
the size of the input domain is points.
in practice the domains can be much larger.
note that the size of the domain does not affect the complexity of the counting procedure which only depends on the number of input variables and the number of constraints .
listing illustrative example 1void test int x int y int z if x do some work else if x y z assert false do more work assuming a uniform usage profile the probability of hitting the failure assert false is since there is only one point in the input domain that can lead to failure.
to illustrate how sampling works consider figure where a part of the symbolic execution tree of listing 1is reported.
for each branching point represented as a node we show both the path condition and the corresponding counter in square brackets.
these counters are initially computed by latte as the paths are explored and stored for re use.
the first time a branch is encountered the counters are used to compute the probability of each alternative and a randomized choice is made accordingly see section .
for example the probability of moving from s2tos3is the number of points satisfying pcats3is 106while the number of those satisfying pcats4is which together sum up to the number of points in pcat the parent state s2.
in our approach a second simulation run would reuse this computation making repeated sampling efficient.
statistical symbolic execution with is starts the first iteration by performing a small number of samples as dictated by the probabilities of the branching conditions.
assume for simplicity that at the end of the first iteration only the path s0!s2!s3has been taken perhaps multiple times this is reasonable since the transitions along this path have significantly higher probabilities than their peers.
the counter for the final pcalong this path is .
this number is then subtracted from all the counters upward alongs0 true s1 x s2 x s3 x x s4 x x figure symbolic execution tree counters updated after one iteration s0 true s1 x s2 x s3 x x s4 x x figure symbolic execution tree counters updated after two iterations only one path left to explore.
the path yielding the updated counters in figure .
a new iteration begins where the sampling is now guided by the updated values of thepccounters.
note that in this iteration the transition from s2to s3can never be taken since its counter is .
at the same time notice that the probability of following the path leading to the subtree containing the error rooted at s4 has increased from 103in the first iteration to in the second iteration.
assuming the more likely path s0!s1is sampled in the second iteration the counters are updated according to the numbers shown in figure .
in this last iteration the only remaining path s0!
s2!s4is taken leading to the exploration of the subtree containing the assert violation.
monte carlo sampling without pruning would miss the path leading to the violation unless an infeasibly large number of samples were taken.
for example after samples the error is still undetected.
after each iteration we also combine the information obtained from an exact analysis of pruned paths and a bayesian inference over sampled paths over the pruned state space to determine if enough evidence was collected about the probabilities of the events of interest.
for simplicity we omit the details for this example but we will describe this at length later in the paper.
.
statistical symbolic execution we first describe statistical symbolic execution which computes an approximate solution to the probability of success or failure of a program based on sampling carefully chosen program paths.
informed sampling will be described in the next section.
the basic idea is to address the probability computation as a statistical inference problem.
first a randomized sampling procedure generates a finite number of simulation runs and classifies each of them as either satisfying or violating a given property e.g.
an assertion in the code .
second suitable statistical methods are applied to either estimate the probability of from an analysis of the samples or to test a hypothesis about this probability.
similar techniques have already been explored in the literature on statistical model checking which typically phrase thestatistical inference problem in the context of formal verification of probabilistic models i.e.
transition systems annotated with probabilities such as markov chains or markov decision processes.
we describe here how we adapted bayesian statistical techniques in the context of symbolic execution of java programs where no model is assumed and the probabilistic information is derived via model counting over the symbolic constraints in combination with the usage profile .
.
monte carlo sampling of symbolic paths typically a monte carlo method defines the solution of the problem as the parameter of a hypothetical population and then generates a random set of samples from which statistical estimates of this parameter can be obtained .
in the context of symbolic execution we define a sample as the simulation of one symbolic path.
whenever a branch is encountered during such simulation the decision to proceed along either of the alternative branches has to be taken according to the probability of satisfying the corresponding branch conditions under the current usage profile.
every time a condition is encountered the simulation has to decide whether to follow the true or the false branch.
in particular let pcbranch be the path condition at the current state and let cbe the branching condition at that state.
the path condition after taking the then branch is pcthen c pcbranch while the path condition after taking the else branch is pcelse c pcbranch .
similar to we associate to each of pcbranch pcthen and pcelsea counter of the number of points in the input domain satisfying the path condition identified by c pcbranch c pcthen and c pcelse respectively.
the first time a path condition pcis encountered its counter is initialized through model counting to the number of points of the input domain that satisfies pc ci pc .
after its initialization the value of each counter is stored and reused through the simulation process.
we can compute the branch probabilities as follows pthen c pcbranch pcbranch c pcthen c pcbranch pelse c pcbranch pcbranch c pcthen c pcbranch from equation it is straightforward to note that pcthen pcelse pcbranch andpthen pelse .
the decision whether to take the then or the elsebranch can now be taken randomly according to their respective probabilities pthen andpelse.
it remains to show that making the sampling choices locally at each branch is equivalent to making the choices over the complete pc i.e.
we do not introduce any statistical bias.
this is implied by the following result theorem .for a path with pc c1 c2 cnand the branching conditions encountered in the given order the path probability given by pr pc is equal to the product of the conditional probabilities at each branch given by pr c1jtrue pr c2jc1 pr c3jc2 c1 pr cnjcn c1 .
proof .from section .
.
we have that pr pc pc d where d is the complete finite domain and from equation 3we can rewrite the product of conditional probabilities as c1 d c1 c2 c1 c1 c2 c3 c1 c2 c1 cn c1 cn which is equal to c1 cn d pr pc .
.
bayesian inference and stopping criteria the samples generated by the monte carlo simulation described in the previous section need to be analyzed to estimate the probability mof the program to satisfy a given property .
bayesian statistical techniques exploit bayes theorem to update the prior information on the probability mafter every observed sample.
the prior is a probability distribution that summarizes all the available information including its lack gathered through sampling .
as explained in a prior for mcan be formalized via the beta distribution b a b see details in .
by setting aand bit is possible to specify the initial assumption about mas follows.
assume the software engineer has an initial guess mabout m for example based on the analysis of previous versions of the software or on the quality of third party components involved.
one way of encoding such knowledge as a prior distribution is a m np b m np where np represents the trust on mas if it was observed on np samples.
if no initial information is available a non informative prior can be used such as b .
the meaning is that we give the same chance to both possible outcomes and we give small trust to it.
we treat grey paths either optimistically or pessimistically meaning that they are considered as either success or failure as desired by the user.
when new samples are gathered they are used to update the prior leading to the construction of the posterior distribution.
in particular if nsamples have been collected with nsof them satisfying the parameters a andb of the posterior distribution will be computed as a a ns b b n ns this information can then be used for statistical estimation or hypothesis testing as explained below.
.
.
bayesian estimation we use bayesian estimation to compute a value that is close to mwith high probability.
more precisely we compute an estimate mbsuch that pr mb e m mb e d where e is the accuracy and d is the confidence the accuracy determines how close the estimate has to be to the real unknown mand the confidence expresses how much this result can be trusted .
recalling that the posterior has a beta distribution with parametersa andb equation can be restated as fb a b mb e fb a b mb e d where fb a b is the cumulative distribution function of the posterior distribution i.e.
it computes the probability for a random variable distributed according to the posterior to assume a value less than or equal to the argument .
from the correctness of bayesian estimation i.e.
it always converges to the real value of mafter enough samples are collected equation can be used as a sequential stopping criterion to decide how many samples are needed to achieve the accuracy and confidence goals.
if the estimation converges with the prescribed accuracy and confidence the estimate mbis defined as the expected value of theposterior distribution mb a a b an estimate on the number of samples that is required to achieve the accuracy and confidence goals is discussed in .
in general this number is highly sensitive to the accuracy parameter while increasing the prescribed confidence has a lower impact on the number of samples.
.
.
bayesian hypothesis testing we use hypothesis testing as an alternative stopping criterion for termination.
hypothesis testing is a statistical method for deciding with enough confidence whether the unknown probabilitymis greater than a given threshold q h0 m q .
alternatively we may want to evaluate the complementary hypothesis h1 m q. similar to estimation hypothesis testing starts from prior knowledge and updates it with the information obtained through sampling until enough evidence is provided in support of either h0or h1.
the procedure aims at estimating the odds for hypothesis h0 versus h1 which can be computed as follows pr h0js pr h1js pr sjh0 pr sjh1 pr h0 pr h1 where sis the set of samples collected and pr h0 andpr h1 are the probability of the hypothesis to be true given the prior knowledge respectively pr h0 fb a b q andpr h1 pr h0 .
the ratio pr sjh0 pr sjh1 is called a bayes factor and can be used as a measure of relative confidence in h0versus h1 i.e.
it quantifies how many times h0is more likely to be true than h1given the evidence collected through sampling.
the valuespr h0js andpr h1js represent the probability of the two hypotheses to be true after samples shave been collected.
since all the information gathered from the samples is embedded in the posterior distribution the latter is used to compute pr h0js fb a b q andpr h1js pr h0js .
thus the bayes factor bcorresponding to the posterior odds for hypothesis h0can be computed from equation after some algebraic simplifications as b pr sjh0 pr sjh1 pr h1 pr h0 fb a b q if no preference among the two hypotheses is provided by the prior e.g.
when a non informative prior is used the initial value of the ratio pr h1 pr h0 is .
equation can be used to define a sequential stopping criterion.
in particular sampling can stop when the odds in favor of one of the hypotheses the bayes factor b is greater than a given threshold t i.e.
when a relative confidence of at least tis obtained from data to support one of the hypotheses.
a precise quantification of the number of samples needed to achieve convergence for bayesian hypothesis testing is discussed in .
in general hypothesis testing is faster than estimation although its performance degrades when qis close to the unknown probability m .
.
informed sampling a weakness of statistical analysis is the large number of paths that may need to be explored and the slow convergence to a result within the desired confidence.
to address this problem we introduce here informed sampling is an iterative technique that combines monte carlo sampling with pruning of already exploredalgorithm statistical symbolic execution with informed sampling 1exploredd 2successd 3repeat numsamples numsuccess successpcs fg exploredpcs fg repeat p montecarlosample letpcbe the path condition of path p numsamples numsamples exploredpcs exploredpcs fpcg ifpj then numsuccess numsuccess successpcs successpcs fpcg end updateprior until stopcombinedest numsamples ni exploredd exploredd exploredpcs successd successd successpcs ifstopcombinedest then return end pruneoutpaths exploredpcs 25until exploredd domainsize 26return paths.
furthermore to obtain a precise estimation of the probability of satisfying property is combines information from two sources the first is based on the exact probabilistic analysis described in section for the pruned paths and the second is based on bayesian inference as described in section .
for the sampled paths.
we describe is in more detail below.
.
algorithm symbolic execution with informed sampling is described at a high level by algorithm .
assume for simplicity that we are interested in the success probability of the program with respect to a property the algorithm can also be applied to failure probability with only minor modifications .
niis a pre specified number of samples per iteration.
assume also that we treat the grey paths optimistically.
the algorithm works through a number of iterations lines .
at each iteration is first tries to tackle the verification problem through bayesian inference.
for this task it takes a pre specified number of monte carlo samples lines as dictated by the conditional probabilities computed from the code.
at each iteration the algorithm keeps track of the following values numsamples counts the number of sampled paths numsuccess counts the number of sampled paths that lead to success exploredpcs stores the pcs of explored paths successpcs stores the pcs of explored paths that lead to success the algorithm also computes exploredd andsuccessd which keep count of total explored inputs and explored inputs that lead to success.
these values are computed using model counting lines and are used in the combined estimators as described below.
as before we use as stopping criteria for sampling eitherbayesian estimation or hypothesis testing high level procedure stopcombinedest in lines and .
however for is we use combined estimators that enhance the bayesian inference with precise information obtained from symbolic paths.
if the combined bayesian estimator converges to the desired confidence or if the combined estimated probability satisfies the hypothesis this result is reported and the analysis stops.
the iterative process can also terminate when the whole domain was analyzed line .
after each iteration the symbolic paths explored so far are pruned out of the execution tree line and analyzed using the exact method section the results are used to build the combined estimator.
this improves the efficiency of the inference procedure because it accounts for all the information obtainable from the path conditions of explored paths.
indeed each sampled path has a path condition which is used in the exact analysis to quantify how many input values from the domain will follow the execution along that path.
for example referring to figure the symbolic path s0!s2!s3accounts for more concrete program paths than the path s0!s1 however this information is ignored by the purely statistical inference which treats symbolic paths as concrete paths.
pruning using counters.
recall that for each path condition pcwe maintain a counter c pc to count the number of solutions.
initially these counters are computed using off the shelf quantification procedures such as latte.
at each iteration is performs sampling as guided by the pc counters see section .
.
for each sampled non duplicate path with final pccounter n is updates all the counters for the prefixes ofpcalong the path to the root of the symbolic execution tree by subtracting n and a new iteration starts with the updated counters .
thus for each pruned pconly a small number of arithmetic operations is required with no significant impact on the overall computation time.
at the end of each iteration the counters keep track of the number of inputs that need to be sampled to follow that path.
if a counter becomes it means that the sub tree rooted at that node has been fully explored and it does not need to be sampled again.
therefore we can safely prune it from the search space.
if the counter of the root node becomes the analysis stops because the whole domain was analyzed exactly.
after each pruning exact information is obtained for a fraction of the input domain.
this fraction needs no longer to be considered for statistical inference allowing the latter to focus on the remaining part of the domain.
furthermore the overall confidence in the result grows since there is no uncertainty about the fraction of the domain analyzed exactly.
estimation with is.
the combined estimator denoted here as m is defined through the mixture of an exact estimator denoted me and a bayesian estimator denoted mb.erefers to the inputs that follow the paths explored in previous iterations of is and can therefore be analyzed exactly while brefers to the inputs that have not been explored yet and therefore can only be used in bayesian estimation .
a hat denotes an approximate value.
for the input points that have already been explored we can compute the exact probability me.
recall that successd denotes the number of input points corresponding to the pruned successful paths and exploredd is the total number of points corresponding to pruned paths.
then me successd exploredd and or the rest of the input domain we have at each iteration just the bayesian estimator mb numsuccess a numsamples a b where for both aandbwe use as default.
by the law of total probability definition we can combine the exact and bayesian estimators m fe mb fe me where feis the fraction of the domain that has been pruned out up to the previous iteration i.e.
exploredd d .
the number of samples to take at each iteration is decided according to a sequential stopping criteria and it is bounded by the maximum value ni provided by the user.
hypothesis testing with is.
for hypothesis testing recall that we base the decision on the posterior odds of the hypothesis h0 pr pj qversus h1 pr pj q. for is we compute the posterior odds based on a combined estimator similar to the one described in equation mh0 fe mh0 b fe mh0 e where mh0 bis the bayesian posterior estimator defined in section .
.
for the probability pr h0js .sis the set of samples taken during the current iteration i.e.
fb a b q where a a numsuccess andb b numsamples numsuccess are the parameters of the posterior beta distribution of the bayesian estimator .
mh0 eis equal to if the result meof the partial exact analysis is greater than or equal to q and equal to otherwise feis the fraction of the domain that has been pruned out up to the previous iteration as described in the previous section.
early termination.
we further enhance the is procedure to check for additional sufficient termination conditions determined by the partial exact analysis of pruned paths.
indeed the actual value of mis by definition in the interval successd d m faild d where faild exploredd successd .
we use these lower and upper bounds to test against the hypothesis and decide early termination of the is procedure.
indeed ifsuccessd d qthe hypothesis is necessarily true while if faild d qthe hypothesis is necessarily false.
in both cases we stop the iterative process and return the result to the user.
this check is performed in stopcombinedest .
.
discussion combined estimators are unbiased and consistent.
the construction of the combined estimator of equation is an application of stratified sampling where the population the input domain is partitioned into disjoint subsets to be analyzed independently the local results are then linearly composed assigning each one a weight proportional to the size of the corresponding subset .
an estimator obtained through stratified sampling is unbiased i.e.
its expected value converges to the measure it estimates and consistent i.e.
its variance converges to when the number of samples goes to if the local estimators used for each subset of the partition are unbiased and consistent .for the portion of the domain analyzed exhaustively meis by definition the actual measure it estimates.
thus it is trivially unbiased and consistent indeed the variance of a number is always zero .
for the portion of the domain subject to statistical estimation we adopt the standard bayesian estimator for the parameter of a bernoulli distribution.
proofs that it is unbiased and consistent can be found for example in .
thus the combined estimator is in turn unbiased and consistent.
termination.
if is explores the whole domain that is exploredd d the process terminates with the same results as for the exact analysis.
since at each iteration the number of samples to collect for bayesian inference is greater than zero is is guaranteed to terminate in the worst case when the whole domain has been analyzed exactly.
note that we assumed the domain is finite.
faster convergence for bayesian estimation.
a benefit of mixing the bayesian estimator mbwithmeis a faster convergence to the criterion of equation .
indeed if an input falls in the portion of the domain analyzed exactly our estimate is perfectly accurate with confidence by definition.
otherwise it will provide confidence d d fe db fe thus to meet the prescribed confidence das a whole the bayesian estimator is required to just satisfy the relaxed confidence db.
db d fe fe during the first iteration when fe dbneeds to satisfy the original convergence criterion of bayesian estimation i.e.
the prescribed d .
however with each iteration feincreases thus relaxing the constraint on db.
faster convergence for hypothesis testing.
as for bayesian hypothesis testing the process terminates as soon as the odds in favor of h0overcome those in favor of h1 by a factor tdecided by the user.
to understand the benefit in terms of the convergence rate provided by the is estimator of equation we need to consider the ratio of the posterior odds pr h0js pr h1js .
ifh0is actually true pr h0js will converge to and consequently pr h1js to the more samples are collected.
the other way around if h0is false pr h0js will converge to and pr h1js to .
the convergence of the estimator mh0can be evaluated again considering fe.
since after each iteration fegrows the room for the uncertainty derived from the use of bayesian estimation is always bounded by a decreasing factor fe.
the more execution paths are pruned out and analyzed exactly the more such uncertainty is reduced usually speeding up the convergence of the combined estimator.
detecting errors with random exploration.
the iterative pruning of the input domain increases the chances of random exploration to detect errors.
to show this let us consider an error path with path condition pcr.
let birepresent the set of the paths targeted by random sampling during iteration i and let bi 1represent the set of paths targeted by sampling during iteration i .
if the error path is not detected at iteration i we will show that the probability of catching pcris higher at iteration i .
let us assume for simplicity that only one path is sampled per iteration the worst case for our proof .
the probability of samplingpcrat iteration iispr pcrjbi .
if it is sampled then the error has been detected.
otherwise a sampled path with condition pciis removed from bi.
since bi bi pciit follows that at iteration i the probability pr pcrjbi of catching pcris higher than in the previous iteration pr pcrjbi pr pcrjpci pr pci pr pcrjbi pr bi note that pr pcrjpci because we assumed that the sampled path with pciwas not the error path with pcr it follows that pr pcrjbi pr pcrjbi pr bi again assuming that pcrhas not been detected yet necessarily bi 1 and thus pr bi .
the example in section 3illustrates this phenomenon the error is very hard to detect with purely random exploration but it can be easily detected with is.
number of samples incremental symbolic execution.
the maximum number of samples to take in each iteration of is allows us to select different operation modes for the algorithm.
if a very large number of samples are allowed during each iteration is reduces to bayesian inference as described in section .
.
on the other hand if ni the impact of the bayesian estimation becomes negligible since it will almost surely not converge after a single sample making is perform an incremental exact analysis by selecting pruning and analyzing one symbolic path per iteration.
thus is can be used to improve on classical symbolic execution by providing for a new kind of incremental analysis where the next path to be analyzed is selected according to the monte carlo sampling described in section .
.
in this way is will likely cover the most probable paths first computing also the fraction of the domain these paths cover.
this results in an any time approach where it is possible to interrupt the execution when enough of the input domain has been covered even if the analysis cannot be exhaustively completed within a reasonable time.
values of nibetween the two extremes trade off the effort bayesian estimation is allowed to take to converge during a single iteration with the number of iterations required to converge.
choosing a good value for nidepends on the specific problem.
we will discuss its choice for several applications in section .
another option is to adapt the value of niwith the number of iterations e.g.
by starting small to quickly prune out paths with high likelihood of execution and gradually increasing the value of nito stress test the parts of the state space that have a small likelihood of execution.
false positives or negatives.
statistical hypothesis testing being a randomized procedure operating on a limited number of samples may produce false negatives or positives i.e.
it may reject a hypothesis that is actually true and vice versa.
this problem can occur especially when the analyzed programs are very large and the probability of success or failure is close to the extremes or .
in the next section we show an instance of the problem.
for bayesian hypothesis testing it has been proved that the probability of obtaining spurious results is bounded by t where tis the threshold set by the user see section .
.
.
for is pruning reduces the possibility of spurious results since it limits the possibility of wrong conclusions to the fraction of the domain analyzed with the bayesian estimator fe .
also note that the sufficient conditions that we added to is for early termination with hypothesis testing do not suffer from incorrect results because they rely on exact methods.
thus they improve the qualityof the overall approach since if is terminates due to the sufficient conditions its results are always correct.
.
experience we implemented the statistical symbolic execution techniques described in this paper in the context of spf an open source toolset.
we plan to make our tool available for download.
sampling is parallelized using a map reduce algorithm.
path counters are shared and reused in subsequent sampling phases.
in this section we compare is with both an exhaustive analysis and a purely statistical approach.
we report on the analysis of the following software artifacts oae the onboard abort executive oae software component manages the crew exploration vehicle s ascent abort handling developed at nasa.
oae has loc input variables ranging over large domains and fairly complicated logic encoding the flight rules a path condition can have approx.
constraints .
we are interested in the probability of the oae not raising a mission abort command.
mer models a component of the flight software for jpl s mars exploration rovers mer it consists of a resource arbiter and two user components competing for five resources.
mer has loc including the polyglot framework .
the software has an error see and is driven by input test sequences.
we analyze two versions mer small for sequence length and mer large for sequence length the latter cannot be analyzed fully with symbolic execution because of the huge number of execution paths.
sorting an implementation of insertion sort.
we calculate the probability of sorting an array of size nin exactly n n comparisons i.e.
the worst case.
a large number of paths need to be analyzed n!
but only path leads to the worst case.
despite being a simple algorithm this example is very challenging for statistical techniques due to the low probability of hitting any failure.
we analyze a version for n .
windy a standard example in the reinforcement learning community that involves a robot moving in a grid from a start to a goal state.
a crosswind can blow the robot off course and an added weight to the robot counter balances that.
we analyze two versions windy small has a grid and solutions limited to moves and windy large has a grid and moves.
the latter cannot be analyzed exhaustively with symbolic execution because of the very large number of paths the robot may follow.
we consider reaching the goal state in the specified number of moves as a success.
oae was analyzed on a red hat linux 64bit machine with 4gb of memory and a .8ghz intel i7 cpu.
the other software was analyzed on an ubuntu server .
.
lts 64bit with 16gb of memory and a .10ghz quad core intel xeon cpu e31220.
estimation.
table 1shows some of our results for the probability estimation problem.
danderepresent the target confidence and accuracy niis the number of samples per iterations iteris the number of iterations completed during analysis estimate is the result computed and time is time consumption in milliseconds.
for all the examples in this table we assume a uniform usage profile for the inputs and we treat grey paths optimistically.
d denotes that is has been used for incremental exact analysis thus computing the actual success probability without uncertainty while ni means that the analysis was purely statistical no is .
there are several observations to make about these numbers for oae eandnido not seem to play a role in the number of iterations required or the time consumption.
this is because after the first iteration even with samples more than of the domain is pruned out and is achieves the required confidencetable estimation results means non convergence means exhaustive analysis oaed e ni iter estimate time .
.
.
.
.
.
.
.
.
.
.
.
.
165mer small d e ni iter estimate time .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
456sortingd e ni iter estimate time .
.
.
.
.
.
.
.
.
.
.
.
.
719windy small d e ni iter estimate time .
.
.
.
.
.
.
.
.
.
.
.
.
quickly.
indeed oae has a few success behavior paths accounting for most of the executions while the abort paths share a small probability of being taken under the uniform profile we will later report on a different mission profile .
thanks to monte carlo sampling the former are very likely to be sampled and then pruned first.
is is dramatically faster than the purely statistical approach and its estimate is also closer to the true value.
mer small has several execution paths occurring for roughly the same number of inputs.
is needs more iterations to prune them out and achieve the high accuracy and confidence goals unlike in the oae case .
however due to the small number of paths after iterations at least of the domain is covered pushing the convergence of the is estimator which outperforms the bayesian estimator.
notice also that the latter does not reach the required confidence for such high accuracy after samples it reaches a confidence of only and for eequal to 3and respectively.
for sorting only nisignificantly influences the number of iterations.
this reflects the fact that initially the paths are equally likely and so we expect the impact of pruning in is to be small.
this scenario is particularly suitable to bayesian estimation which converges for e 3after about samples.
since we limited nito smaller values is was not able to achieve convergence by its statistical component until pruning covered a large portion of the domain.
when the accuracy is raised to e the bayesian estimator is not able to converge within samples final confidence while for is increasing the accuracy does not require higher overhead allowing it to converge faster than bayesian.
for this problem a higher ni would be a reasonable choice especially for low accuracy.
windy is similar to sorting since there are many paths all withcomparable probability and only a few of them are classified as success.
however while for sorting the bayesian estimator quickly converged for accuracy 3without observing any failure in this case the probability of success is high enough to allow sampling both types of path.
this increases the variance of the sample slowing down the statistical estimator.
on the other hand for is thanks to pruning as soon as the few success paths are collected they are pruned out reducing the variance of the samples of subsequent iterations and speeding up convergence.
in summary is is particularly effective for problems where a subset of the execution paths accounts for a large portion of the inputs.
in this case such paths are likely to be pruned out after a few iterations increasing the confidence on the partial result.
also is outperforms statistical methods when high accuracy is required.
finally if an exact analysis is required for a problem that would require too much memory to be analyzed with previous approaches is can analyze them incrementally producing intermediate results with quantified confidence after each iteration though usually taking longer time.
hypothesis testing.
the results for hypothesis testing are shown in table .qandtrepresent the hypothesis h0 pr pj q and the confidence threshold to accept or reject h0 and result is the result computed whether or not the hypothesis holds while the meanings of the other columns are the same as before.
once again we assume a uniform usage profile.
table hypothesis testing results denotes convergence for sufficient exact conditions denotes a false positive negative oaeq t ni iter result time .
true .
true .
true .
true .
true .
true 125mer small q t ni iter result time .
true .
true .
false .
false .
.
false 229sortingq t ni iter result time .
true .
true .
false .
false .
.
false 449windy small q t ni iter result time .
false .
true .
false .
false .
true .
our choices of qare values close to the actual success probabilities obtained by estimation and as given in table .
as expected hypothesis testing is usually faster than estimation.
however when qis very close to the actual probability of success bayesian methods fail to converge within a reasonable amount of time results marked with .
is responds to this situation by requiring more iterations more rounds of sampling pruning .
compare for example the cases with q and q for mer small .
is generally performs better than a pure bayesian testing and for some smaller cases the sampling procedure covered by chance the full domain after a just few iterations producing an ex act result .
interestingly in the first experiment reported for windy small with ni we obtained a false negative result.
in this case the bayesian component of is converged to a false decision after the samples produced by chance failures.
increasing the number of samples niwas enough to avoid this error.
table hypothesis testing results where classical symbolic execution runs out of memory denotes convergence for sufficient exact conditions mer large q t n i iter result time .
true .
true .
false .
true 372windy large q t n i iter result time false false true true true true intractable classic symbolic execution.
table 3shows the results for a second set of experiments where we ran the techniques on the larger examples for which classical symbolic execution is intractable.
we show results for the most efficient technique from the smaller cases i.e.
is for hypothesis testing.
there is was able to converge to a decision within a reasonable amount of time.
nevertheless the large number of execution paths of these cases led for mer large with qclose to the actual success probability to a false positive result for q and ni we know it is a false positive because with ni we obtained termination for a sufficient condition check.
as already discussed a false positive result is possible for statistical testing.
is can mitigate this issue by leveraging its exact analysis component as for the case of ni although in some cases even could be enough to make the bayesian component of is converge to the wrong conclusion and an even smaller value for nimight be required.
usage profiles.
we briefly mention the impact of the usage profiles on the probability of satisfying a target property.
we analyzed oae with a different usage profile where one input variable thrust has a gaussian normal distribution.
the gaussian distribution was approximated by discretizing the domain of thrust of into segments which led to usage scenarios with different probabilities .
under this usage profile the density of inputs following the normal behavior paths is reduced requiring more rounds of pruning for is estimation to converge even accuracy as low as .
this results in longer computation time though still within reasonable ranges.
for example is with bayesian estimation for confidence .
took approx.
ms in iterations with or samples per iteration while for confidence .
it took approx.
ms in iterations.
the source code for all the examples except oae and more experimental data are available from .
.
related work our work is related to statistical model checking smc also formulated as a statistical hypothesis testing problem verified through wald s sequential probability ratio test sprt .
sprt does not fix the required number of samples a priori but uses a sequential approach to decide after each sample whether to stop or continue.
a different hypothesis testing criterion has been proposed in where the size of the sample set is auto matically increased until it allows for satisfying the convergence criteria.
in smc has been formulated as an estimation problem with the number of samples fixed a priori by means of the chernoff and hoeffding bound .
other approaches for deciding the number of samples have been discussed in .
some of these approaches have been implemented in well known probabilistic model checkers .
in our work we combined bayesian inference techniques with exact analysis through the is technique which is shown to provide better performance than the pure bayesian analysis.
a recent approach related to ours provides automated reliability estimation over partial systematic explorations applied to models.
the approach first performs sampling over the model and then applies invariant inference over the samples.
the inferred invariant characterizes a partial model which is then exhaustively explored using exact probabilistic model checking obtaining better results than full model probabilistic and statistical model checking for system models.
the techniques we propose are different.
indeed we focus on the use of symbolic execution to analyze software from its source code while focuses on markov chain models analyzed through probabilistic model checking.
the samples in are used to produce an approximate simplified model to be analyzed while instead we use an iterative process that prunes the execution tree and guides the sampling towards low probability paths.
we proposed several techniques for the probabilistic analysis of programs .
the approaches in can only perform exact analysis that requires all paths to be evaluated.
the work in addresses the approximate analysis of non linear constraints we can apply the techniques described here also in that domain using the quantification procedure from instead of model counting.
another approximate analysis for programs is proposed in that also uses sampling of symbolic paths but no incremental or informed sampling as we do here and gives bounds on the probability of events of interest in a program.
in more recent work we study statistical techniques that target specifically programs that have nondeterminism for example due to concurrency .
the work also uses hypothesis testing a simpler form than here but its main focus is on deriving optimal schedulers with the best technique using reinforcement learning for the most promising scheduler moves.
our work shares similar goals with guided testing techniques which provide heuristics to guide the exploration of a program towards interesting paths to increase coverage or to uncover errors e.g.
and many other works.
however such techniques do not provide statistical guarantees as we do here.
.
conclusions we described statistical symbolic execution for the analysis of software implementations.
the technique uses a randomized sampling of symbolic paths with bayesian estimation and hypothesis testing.
we also proposed informed sampling an iterative approach that first explores the paths with high statistical significance prunes them from the state space and then keeps guiding the execution along less likely paths.
informed sampling combines statistical information from sampling with exact analysis for pruned paths leading to provably improved convergence of the statistical analysis.
the techniques have been implemented in the context of symbolic pathfinder and have been shown to be effective for the analysis of java programs.
in the future we plan to perform further evaluations and to investigate applications in statistical information flow analysis.
we also plan an in depth study on probability computations for programs with structured inputs.
.