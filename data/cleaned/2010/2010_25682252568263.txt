ar miner mining informative reviews for developers from mobile app marketplace ning chen jialiu liny steven c. h. hoi xiaokui xiao boshen zhang nanyang technological university singapore ycarnegie mellon university usa nchen1 chhoi xkxiao bszhang ntu.edu.sg yjialiul cs.cmu.edu abstract with the popularity of smartphones and mobile devices mobile application a.k.a.
app markets have been growing exponentially in terms of number of users and downloads.
app developers spend considerable e ort on collecting and exploiting user feedback to improve user satisfaction but su er from the absence of e ective user review analytics tools.
to facilitate mobile app developers discover the most informative user reviews from a large and rapidly increasing pool of user reviews we present ar miner a novel computational framework for app review mining which performs comprehensive analytics from raw user reviews by i rst extracting informative user reviews by ltering noisy and irrelevant ones ii then grouping the informative reviews automatically using topic modeling iii further prioritizing the informative reviews by an e ective review ranking scheme iv and nally presenting the groups of most informative reviews via an intuitive visualization approach.
we conduct extensive experiments and case studies on four popular android apps to evaluate ar miner from which the encouraging results indicate that ar miner is e ective e cient and promising for app developers.
categories and subject descriptors d. software engineering h. information systems applications miscellaneous general terms algorithm and experimentation keywords user feedback mobile application user reviews data mining .
introduction the proliferation of smartphones attracts more and more software developers to devote to building mobile applications apps .
as the market competition is becoming more permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
icse may june hyderabad india copyright acm ... .
.intense in order to seize the initiative developers tend to employ an iterative process to develop test and improve apps .
therefore timely and constructive feedback from users becomes extremely crucial for developers to x bugs implement new features and improve user experience agilely.
one key challenge to many app developers is how to obtain and digest user feedback in an e ective and e cient manner i.e.
the user feedback extraction task.
one way to extract user feedback is to adopt typical channels used in traditional software development such as i bug change repositories e.g.
bugzilla ii crash reporting systems iii online forums e.g.
swiftkey feedback forum and iv emails .
unlike the traditional channels modern app marketplaces such as apple app store and google play o er a much easier way i.e.
the web based market portal and the market app for users to rate and post app reviews.
these reviews present user feedback on various aspects of apps such as functionality quality performance etc and provide app developers a new and critical channel to extract user feedback.
however comparing with traditional channels there are two outstanding obstacles for app developers to obtain valuable information from this new channel.
first of all the proportion of informative user reviews is relatively low.
in our study see section .
we found that only .
of app reviews contain information that can directly help developers improve their apps.
second for some popular apps the volume of user reviews is simply too large to do manual checking on all of them.
for example facebook app on google play receives more than user reviews per day making it extremely time consuming to do manual checking.
to our best knowledge very few studies were focused on extracting valuable information for developers from user reviews in app marketplace .
this paper formally formulates this as a new research problem.
speci cally to address this challenging problem and tackle the aforementioned two obstacles we propose a novel computational framework named ar miner app review miner for extracting valuable information from raw user review data with minimal human e orts by exploring e ective data mining and ranking techniques.
generally speaking given a bunch of user reviews of an app collected during a certain time interval ar miner rst lters out those noninformative ones by applying a pre trained classi er.
the remaining informative reviews are then put into several groups and prioritized by our proposed novel ranking model.
finally we visualize the ranking results in a concise andpermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
icse may june hyderabad india copyright acm ... .
intuitive way to help app developers spot the key feedback users have.
we validate the e cacy of ar miner by conducting an extensive set of experiments and case studies on user reviews of four android apps released in google play.
in particular we compare the ranking results generated by ar miner against real app developers decisions and also analyze the advantages of ar miner over manual inspection and facilities used in a traditional channel i.e.
online forum .
our empirical results have validated the e ectiveness and e ciency of arminer in helping apps developers.
supplementary materials including datasets additional results etc.
are publicly available at1.
in short this paper makes the following main contributions we formulate a new problem that aims to discover the most informative information from raw user reviews in app markets for developers to improve their apps we present ar miner as a novel analytic framework to tackle this new problem which includes a new exible and e ective ranking scheme to prioritize the informative reviews we evaluate ar miner based on user reviews of four popular android apps in which empirical results show that ar miner is e ective and promising.
the rest of the paper is organized as follows section discusses related work section gives the problem statement section presents the ar miner framework section gives empirical results section discusses limitations and threats to validity nally section concludes this work.
.
related work we group related work into three major categories and survey the literature of each category in detail below.
.
app marketplace analysis with the rocketing development of mobile applications app marketplace has drawn much more attention among researchers within and outside the software engineering community.
in harman et al.
pointed out that app marketplace is a new form of software repository and very di erent from traditional ones.
they also analyzed the technical customer and business aspects of some apps in blackberry world.
in minelli et al.
proposed to combine data extracted from app marketplace with source code to comprehend apps in depth.
linares v asquez et al.
empirically analyzed how the stability and fault proneness of apis used by some free android apps relate to apps lack of success.
chia et al.
analyzed the relationship between permissions and community ratings of some apps.
our work is different from the aforementioned studies mainly because we explore di erent data in app marketplace i.e.
user reviews and formulate a very di erent problem.
thus far there has been little work on mining user reviews in app marketplace.
in pagano and maalej conducted an exploratory study to analyze the user reviews crawled from apple app store.
they studied i the usage and impact of feedback through statistical analysis and ii the content of feedback via manual content analysis and frequent itemset mining.
chandy et al.
presented a latent to detect bogus user reviews in apple app store.
iacob et al.
developed a prototype named mara that uses a list of linguistic rules to automatically retrieve feature requests from online user reviews.
although the nature of data studied in the above three works is similar to ours the techniques used or research goals are totally di erent.
to our best knowledge there are only two previous studies that are closely related to our work.
galvis carre no et al.
adapted the aspect and sentiment uni cation model asum proposed in to automatically extract topics for requirements changes.
general speaking our work differs from their work in three major aspects.
first our work aims to discover not only requirement changes but also other kinds of valuable information for developers see figure in section .
second we focus on the ranking scheme of informative user reviews which is not addressed in their work.
finally our scheme consisting of an e ective ltering step considerably outperforms the direct application of topic models in solving our problem see more details in section .
.
.
fu et al.
presented a system named wiscom to analyze user reviews in app marketplace at three di erent levels.
the so called meso level of wiscom is to uncover the reasons of user dissatisfaction and their evolution over time for an individual app which is more related to arminer however it su ers from two major limitations.
first it cannot discover app speci c topics by using latent dirichlet allocation lda since it links all the user reviews from the same app together as a document.
second it only considers negative reviews thus missing topics with positive ratings.
unlike wiscom ar miner can address both limitations.
moreover we propose a new ranking model to help app developers prioritize the informative user reviews.
.
mining user reviews on the web our work is related to studies that focus on mining and analyzing user reviews in other kinds of marketplaces e.g.
movies commodity goods etc.
and social webs e.g.
news sites .
however their techniques cannot be directly applied to our problem in that the objective of our problem is di erent i.e.
solving a software engineering problem in requirement bug management and the characteristics of user reviews in apps stores are quite di erent e.g.
review styles volumes etc.
next we discuss three most related classes of work and explain speci c di erences.
in the literature there is a great deal of work that applies sentiment analysis to user reviews in di erent marketplaces .
in general these studies aim to determine the semantic orientation of a given user review at the document sentence or feature level whether the opinion expressed is positive or negative.
there are several studies focusing on summarizing visualizing user reviews via identifying product features and opinions .
compared with the aforementioned work our work di ers in that i we classify each user review into either informative or non informative from the software engineering perspective instead of positive or negative from the emotional perspective ii the ultimate goal of our work is di erent that is visualizing the ranking results of informative information.
much work has focused on spam review ltering .
they aim to protect users and honest vendors via detecting and removing bogus user reviews.
our work di ers from them mainly in two points.
first in our work the de nition of non informative spam see section .
class type rule real example informative functional flaw that produces incorrect or unexpected result none of the pictures will load in my news feed.
performance flaw that degrades the performance of apps it lags and doesn t respond to my touch which almost always causes me to run into stuff.
requests to add modify features amazing app although i wish there were more themes t o choose from.
please make it a little easy to get bananas please and mak e more power ups that would be awesome.
requests to remove advertisements notifications so many ads its unplayable!
requests to remove permissions this game is adding for too much unexplained permiss ions.
noninformative pure u ser emotional expression great fun can t put it down!
this is a crap app.
descriptions of apps features actions etc.
i have changed my review from star to star.
too general unclear expression of failures and reque sts bad game this is not working on my phone.
questions and inquiries how can i get more points?
figure di erent types of informative and non informative information for app developers second although the ltering step in ar miner can help remove some types of spam reviews our major objective is to rank the informative user reviews for app developers.
there also exist several pieces of work on ranking reviews on the social web.
for example hsu et al.
applied support vector regression to rank the reviews of a popular news aggregator digg.
dalal et al.
explored multi aspects ranking of reviews of news articles using hodge decomposition.
di erent from both works our work aims to rank the reviews according to their importance not quality to app developers not users from the software engineering perspective.
besides we propose a completely di erent ranking model in solving our problem.
.
mining data in traditional channels our work is also related to studies that apply data mining machine learning techniques on data stored in traditional channels to support developers with the user feedback extraction task.
speci cally the rst category of related work in this eld is to address problems in bug repositories .
for example sun et al.
proposed a discriminative approach to detect duplicate bug reports.
anvik et al.
compared several classi cation algorithms for solving the bug assignment problem.
antoniol et al.
developed a machine learning approach to distinguish bugs from nonbugs.
in addition another category of related work is to solve problems in other traditional channels e.g.
request repositories emails crash reporting systems .
for example cleland huang et al.
proposed a machine learning approach to categorize product level requirements into pre de ned regulatory codes.
dang et al.
developed an approach based on similarity measures to cluster crash reports.
bacchelli et al.
applied a naive bayes classi er to classify email contents at the line level.
compared with the previous studies in this area our work di ers in that we formulate and solve a brand new problem in a new channel with its distinct features.
.
the problem statement the user feedback extraction task is extremely important in bug requirement engineering.
in this paper we formally formulate it as a new research problem which aims to facilitate app developers to nd the most informative information from large and rapidly increasing pool of raw user reviews in app marketplace.
consider an individual app in a time interval t it receives a list of user reviews r with an attribute set a fa1 a2 a kg andri fri a1 ri a2 r i akgis the i th review instance in r .
without loss of generality in this work we choose a ftext rating timestamp g since these are the common attributes supported in all mainstream app marketplaces.
table shows an example of r withtreview instances.
in particular we set the text attribute ofriat the sentence level.
we will explain how to achieve and why we use this ner granularity in section .
.
table example of a list of user reviews r r rating ts timestamp id text r ts r1nice application but lacks some important4 dec features like support to move on sd card.
r2 so i am not giving ve star rating.
dec r3 can t change cover picture.
jan r4i can t view some cover pictures even mine.
jan r5 wish it d go on my sd card.
dec .
.
.
.
.
.
.
.
.
.
.
.
rt .
.
.
.
.
.
.
.
.
in our problem each riinr is either informative or non informative .
generally informative implies ri contains information that app developers are looking to identify and is potentially useful for improving the quality or user experience of apps.
we summarize di erent types of informative as well as non informative information in figure one or two examples for each type .
for example r1 r3 r4andr5shown in table are informative since they report either bugs or feature requests while r2is noninformative as it is a description of some user action and developers cannot get constructive information from it.
remark.
the summarization shown in figure is not absolutely correct since the authors are not app developers.
in fact even for real app developers no two people would have the exact same understanding of informative .
this is an internal threat of validity in our work.
to alleviate this threat we rst studied some online forums e.g.
to identify what kinds of information do real app developers consider as constructive and then derived the summarization shown in figure based on the ndings.
generally given a list of user reviews r of an app e.g.
the one shown in table the goal of our problem is to lter out those non informative reviews e.g.
r2 then i 769group the remaining reviews based on the topics they are talking about e.g.
fr1 r5gare grouped because they both talk about feature request related to sd card and ii identify the relative importance of di erent groups and reviews in the same group e.g.
the relative importance of r1and r5 and nally present an intuitive visualized summarization to app developers.
.
our framework in this section we rst give an overview of our proposed ar miner framework to address the problem stated in section and then present each step of our framework in detail.
.
overview figure presents an overview of ar miner which consists of ve major steps.
the rst step preprocesses the raw user review data into well structured format to facilitate subsequent tasks section .
.
the second step applies a pre trained classi er to lter out non informative reviews inr section .
.
the third step groups the remaining informative reviews in such a way that reviews in the same group are more semantically similar section .
.
the fourth step the focus of this paper sorts i groups and ii reviews in each group according to their level of importance by using our novel ranking model section .
.
in the last step we visualize the ranking results and present an intuitive summary to app developers section .
.
preprocess ing filtering visualizationgrouping rankinguserreviews report1 figure overview of the ar miner framework.
we focus on tackling the ranking step.
.
preprocessing the rst step of ar miner preprocesses the collected raw data by i converting the raw user reviews into sentencelevel review instances and then ii preprocessing the text attribute of the review instances.
the format of raw user reviews varies with di erent app marketplaces.
as mentioned in section in this work we choosea ftext rating timestamp g. figure shows a real example of a raw user review that contains these three attributes.
the text attribute of a raw user review often consists of more than one sentence.
in this work we split text into several sentences via a standard sentence splitter provided by lingpipe .
for each sentence we generate a review instance riwithrating andtimestamp equal to the values of the corresponding raw user review.
for example the raw user review shown in figure is converted into two sentence level review instances shown in table r1 andr2 .
we choose the sentence level granularity because within a raw user review some sentences can be informative e.g.
sentence shown in figure and some sentences are not e.g.
sentence .
thus this ner granularity can help distinguish informative with non informative information more accurately.
dec09 niceapplication butlackssomeimportantfeatureslike supporttomoveonsdcard1.so iamnotgivingfive starrating2.
figure an example raw user review further we preprocess the text attribute of review instances.
we rst tokenize the text and then remove all non alpha numeric symbols convert words to lowercase and eliminate extra whitespace along with stop words rare words.
next the remaining words are stemmed to their root form.
finally we remove review instances that become empty as a result of the above processing.
.
filtering the preprocessing step generates a review database r e.g.
as shown in table .
in this step our goal is to train some classi er that can automatically lter out noninformative reviews from r .
first we introduce the class label set used in our problem.
as described in section we have two unique class labels finformative non informative g where informative implies that the review is constructive helpful to app developers and non informative means that the review contains no information that is useful for improving apps.
we use the rules types summarized in figure to assign class labels to review instances.
in particular we solve some ambiguous cases.
for example we classify too general unclear expression of failures and requests as non informative e.g.
it doesn t work it needs more update and etc.
.
to eliminate non informative review instances we need to apply a machine learning algorithm to build some classier on the historical training data.
in this work we simply adopt a well known and representative semi supervised algorithm in machine learning i.e.
e xpectation m aximization for n aive b ayes emnb .
the most important reason we choose emnb is that it suits our problem well.
in our problem we can get a mass of unlabeled data almost freely but labeling training data is time consuming and labor intensive.
compared with supervised algorithms emnb can use a small amount of labeled data thus less human e ort along with plenty of unlabeled data to train a fairly good classi er see our comparisons in section .
.
.
besides nb often outperforms other machine learning algorithms in text classi cation and has been widely used in other software engineering problems .
finally nb provides a nice posterior probability for the predicated class which is useful in the ranking step see section .
.
.
once the classi er is built it can be applied to lter future unlabeled user reviews.
table shows a possible good result denoted asr n t after lteringr shown in table where non informative review instances are eliminated r2 and informative ones are preserved r1 r3 r4andr5 .
the last column p of table indicates the probability of the review instance belongs to the informative class.
.
grouping this step is to partition the remaining review instances r into several groups such that the text of review instances in a group is more semantically similar to each other than thetext of review instances in other groups.770table r a possible good result after filtering r rating ts timestamp p probability id text r ts p r1nice application but lacks dec .
some important features like support to move on sd card.
r3 can t change cover picture.
jan .
r4i can t view some cover2 jan .
pictures even mine.
r5wish it d go on my sd card.
dec .
.
.
.
.
.
.
.
.
.
.
.
.
rn .
.
.
.
.
.
.
.
.
in general there are two categories of unsupervised techniques that can be applied to the grouping task.
the rst category is clustering e.g.
k means which assumes that each review instance belongs to exactly one single cluster group .
however this assumption may become problematic for review instances even at the sentence level that exhibit multiple topics groups to di erent degrees.
as a result we adopt another category of techniques topic modeling which assigns multiple topics to each review instance.
for example the review just add emojis and more themes.
is modeled as a distribution over two topics emoji theme .
we will discuss the comparison of two algorithms in topic modeling i.e.
latent dirichlet allocation lda and aspect and sentiment uni cation model asum adopted in in our experiments.
in the future we will explore and compare more topic models.
.
ranking given the grouping results we aim to determine the relative importance of i groups and ii review instances in each group.
to ful ll this purpose we propose a novel ranking model presented as follows.
.
.
the overview of our ranking model the general form of our ranking model is shown in algorithm .
the inputs include i a set of groups topics g generated by the grouping step ii two sets of functions fg see section .
.
and fi see section .
.
that measure the importance of various features of groups e.g.
volume and review instances e.g.
rating respectively and iii two weight vectors wg wg i2 pm i 1wg i and wi wi i2 pn i 1wi i correspond to fgandfi respectively.
algorithm computes i the groupscore g for each groupg2g line and ii the instancescore r for each review instance r2g line .
larger groupscore g andinstancescore r indicate higher importance.
finally algorithm outputs the ranking results.
our ranking model is exible since we can obtain ranking results from di erent angles by adjusting the weight vectors ofwgandwi see section .
.
.
we also claim that our ranking model is extensible because it can easily incorporate more features see section for discussion .
.
.
group ranking to measure the importance of di erent groups we use fg ffg v olume fg timeseries fg avgratinggin this work.
next algorithm the ranking model input a set of groups g feature function sets fg ffg fg mgandfi ffi fi ng weight vectors wg wg wg m and wi wi wi n for each group g2gdo computefg g .
.
.
fg m g setgroupscore g pm i wg i fg i g for each review instance r2gdo computefi r .
.
.
fi n r setinstancescore r pn j wi j fi j r end end output groups in decreasing order of groupscore review instances in each group in decreasing order ofinstancescore table per review distribution over groups a review group matrix g1 gm r1pr1g1 pr1gm ............ rnprng1 prngm b an example g1g2g3 r11.
.
.
r21.
.
.
r30.
.
.
r40.
.
.
we describe each feature function in detail.
volume given the remaining nreview instances after ltering r fr1 r ng in the grouping phase we automatically discover mgroups topics denoted as g fg1 g mg. as described in section .
each review instanceri i n is modeled as a distribution over g. the matrix shown in table a presents such distributions where each entry prigj i n j m represents the proportion that review instance riexhibits group gj and for eachri pm j 1prigj .
for example in table b r4 exhibitsg2with and g3with .
the volume of a groupgis de ned as follows fg v olume g nx i 1prig for example in table b fg v olume g1 .
one group with larger volume indicates it is more important.
the reason is that a larger group is more likely to be a class of common bugs requests re ected by many users while a smaller group is more likely to be i a kind of particular bug request reported by only a few users or ii a few users wrong careless operations.
time series pattern given the time interval t t0 t0 t under investigation we divide tintok t tconsecutive time windows with each has length of t. lettkdenote thek th time window thus tk where k k. for eachri2r we denote ri tsas the timestamp when riis posted t0 ri ts t0 tfor all i n. given a time window tk we denote the total number of review instances posted during it as follows v tk jrtkj jfri ri ts2tkgj n kx k 1v tk 771wherejmjdenotes the cardinality of the set m. for a group g we count the volume of review instances posted during the time window tk formally v g tk x rj2rtkprjg then we can construct a time series for the group g represented by pg t t g wherep g k is short for p g tk andp g k v g tk v tk .
figure shows four typical time series patterns.
the patternp1shown in figure a has a rapid rise at a certain time window tk followed by a small decline then towards plateau.
one group that has this kind of pattern is likely to be a class of newly introduced bug request due to some event happened attk e.g.
version update network server error and etc.
.
in addition this problem is not solved at the end oft.p2shown in figure b presents a quick decay at a certain time window tk .
this demonstrates the scenario where an old bug request is xed satis ed at tk.p3shown in figure c uctuates slightly within a range over the entiret.
this indicates the scenario of an existing bug request introduced earlier than t0that is not xed satis ed during t.p4shown in figure d implies that the problem is introduced earlier than t0that is relieved but not addressed duringt.
obviously groups with pattern p1are the most important fresher while groups of pattern p2are the least important older .
to model the importance of time series pattern for a group g we compute fg timeseries g by fg timeseries g kx k 1p g k p g l k wherep g pk k 1p g k andl k is a monotonically increasing function of k the index oftk since we aim to set a higher weight to later tk.
the choice of l k depends on the importance of the freshness in this work we simply set l k k. average rating we denote ri ras the user rating of ri in this work ri r2 f1 5gfor all i n. informative reviews with lower ratings e.g.
tend to express users strong dissatisfaction with certain aspects of apps that need to be addressed immediately e.g.
critical bugs thus are more important.
on the other hand informative reviews with higher ratings e.g.
often describe some kind of users non urgent requirements e.g.
feature improvement thus are less important.
therefore we measure the rating aspect of a group gby fg avgrating g fg v olume g pn i 1prig ri r which is the inverted average rating of a group g. larger fg avgrating g indicates more importance.
.
.
instance ranking regarding the importance of review instances in a particular groupg in this work we use fi ffi proportion fi duplicates fi probability fi rating fi timestampg.
next we describe each feature function in detail.
a pattern p1 b pattern p2 c pattern p3 d pattern p4 figure representative time series patterns for groups proportion for a group g eachri2 r i n exhibitgwith a certain proportion prig.prigequals to meansrionly exhibits g whileprigequals to indicates ri does not exhibit gat all.riwith larger prigvalue is more important in g since it contains more core content of this group.
formally fi proportion r g prg in this work we denote rgas the reviews instances belong to a groupg.rgis constructed by eliminating those riwith prig inr we set thusrg fri prig g. for example in table b rg1 fr1 r2 r3g r4is ignored sincepr4g1 .
duplicates for a group gofrg fr1 r ngg we denoter text as the text of r represented in the vector space .
it is common that di erent texts of review instances refer to the same informative information.
we intend to remove those duplicates from rgand form a set of unique review instancesru g fru ru n0gg wheren0 g ng.
specifically for each unique review instance ru i2ru g rj2r g is considered as a duplicate of ru iif and only if satisfying sim rj text ru i text wheresimis a certain similarity metric e.g.
jaccard similarity used in our work and is a prede ned threshold.
we count the number of duplicates for eachru i2ru g denoted as duplicates ru i g .
the more duplicatesru ihas the more important it is in g. formally fi duplicates r g duplicates r g whereris a shorthand for ru i. note that for a group g we quantify the importance of every unique review instance r2ru g. forrthat has more than one duplicate the rating ofris set as the minimum rating value of duplicates and the proportion probability and timestamp ofrare set as the maximum values of duplicates.
the features in italics will be introduced below shortly.
probability as mentioned in section .
one of the reasons we choose emnb as our classi er is that it can provide772a posterior probability for each predicated review instance denoted as r p .
intuitively the larger probability of r demonstrates that it is more likely to be an informative review thus more important.
formally fi probability r r p rating similar to the average rating of a group lower rating ofrindicates it is more important thus fi rating r r r timestamp similar to the time series pattern of a group more fresher of rindicates it is more important thus fi timestamp r k wherekis the index oftkthat satis es r ts2tk.
.
visualization the last step of ar miner is to visualize the results generated by our ranking model.
figure shows a possible visualization of top ranked results see details in section .
.
the bottom half of figure is a radar chart which depicts thegroupscore value of each group topic along a separate axis which starts in the center of the chart and ends on the outer ring.
each group is labeled by two or three top probability also descriptive words within the group and a data point near the outer ring indicates a higher groupscore value.
intuitively the group more theme which is requesting for more themes into the app has the highest groupscore and thegroupscore s of the remaining groups decrease in the clockwise direction.
to get insight into a group we can click its label to view the reviews instances the detailed information within the group in decreasing order of instancescore .
for example the top half of figure shows the top review instances in the more theme group.
due to space limitation we present the complete version of figure on our website.
.
empirical evaluation to evaluate if ar miner can really help app developers we conduct several experiments and case studies.
speci cally we aim to answer the following questions what is the topic discovering performance of our scheme?
if the top ranked topics generated by ar miner represent the most informative user feedback for real app developers?
what are the advantages of ar miner over purely manual inspection and facilities used in traditional channels e.g.
online forum ?
.
dataset we select android apps from google play i.e.
swiftkey keyboard smart touchscreen keyboard facebook social app temple run parkour game and tap fish casual game as subject apps in our experiments and case studies.
these apps are selected because i they cover di erent app domains and ii they range from large datasets facebook and temple run to relatively small datasets swiftkey keyboard and tap fish .
we collected the raw user reviews of these apps from google play roughly in the period from oct to feb .
table lists some key features of these datasets after converting to sentence level .
for each dataset we divide it into two partitions where reviews in partition i appear before those of partition ii .
.
.
.
1more theme word type restart default keyboard word predict support chinese langauge predict text like keyboard more option space bar swype feature review instance s of topic more theme score also we need more themes!
.
just wish you had more themes or ability to make a custom theme.
.
... ...................................... ..... figure visualization of top ranked results achieved by ar miner swiftkey .
the number in the square bracket denotes the corresponding rank in figure .
in terms of their posting time.
we adopt some data from partition i for training and some data from partition ii for test.
speci cally for partition i we randomly sample reviews as labeled training pool and treat the rest as unlabeled data.
for partition ii we randomly sample reviews for labeling and use as test set for evaluation.
table statistics of our datasets train training pool unlab.
unlabeled set .
dataset swiftkey facebook templerun2 tapfish train unlab.
test info.
.
.
.
.
we collected the ground truth labels of the training pool and test set according to the rules summarized in figure .
each review is labeled by three di erent evaluators some are workers in amazon mechanical turk and the nal label is determined by the majority voting .
the row info.
in table shows the proportion of informative reviews among data with ground truth .
on average .
reviews are informative .
without loss of generality we take informative as positive class and non informative as negative class.
.
performance metrics in this section we introduce the performance metrics used in our evaluation.
the rst set of metrics include precision recall hit rate and f measure which are de ned below precision tp tp fp recall hit rate tp tp fn773f measure precision recall precision recall wheretp fp fn represent the numbers of true positives hits false positives and false negatives misses respectively.
in addition we also adopt the well known n ormalized discounted c umulative g ain ndcg as a measure for evaluating the quality of top k ranking results ndcg k dcg k idcg k where ndcg k2 and the higher value implies greater agreement between the predicted rank order and the ideal rank order.
.
evaluation of grouping performance we conduct two experiments to evaluate the performance of our scheme the rst steps shown in figure for automatically discovering groups topics .
first we qualitatively compare our scheme which contains a ltering step before grouping with the baseline scheme used in which directly applies topic models .
second we explore and compare two di erent settings of our scheme i.e.
i emnblda stanford topic modeling toolbox implementation for lda and ii emnb asum the original implementation with default parameters for asum where asum is proposed in and adopted in .
we select the emnb lter for each dataset shown in table as follows.
for each experimental trial we randomly choose a subset of training pool examples per class as training data and then apply the emnb algorithm the lingpipe implementation to build a classi er on the combination of training data and unlabeled set nally measure the performance on the test set.
we repeat the above experimental trial times and choose the classi er with the best f measure as the lter.
table shows the f measure attained by the four selected lters used in our experiments.
we can see that their performance is fairly good especially thefacebook lter .
.
table the performance of selected filters filter swiftkey facebook templerun2 tapfish f measure .
.
.
.
.
.
qualitative comparison of both schemes the rst experiment qualitatively compares our scheme emnb lda with the baseline scheme lda .
we apply both schemes to the test set of each dataset shown in table after preprocessing.
we vary the number of topics denoted as k and choose the appropriate k values according to i the perplexity scores on held out data should be small and ii the results themselves should be reasonable .
table shows some representative topics found by emnb lda and lda from the test set of swiftkey .
for each topic we list the top weighted words in the vocabulary distribution.
for space reasons we do not present the results for other datasets which are similar .
from the results shown in table we can draw two observations.
first most topics found by emnb lda are informative e.g.
theme chinese jelly bean predict and space shown in table a while lda presents many noninformative or redundant topics such as type purelytable some topics found by emnb lda k and lda k on swiftkey dataset.
the colored words are topic labels.
a emnb lda theme chinese jelly bean predict space more languag bean word space theme chines jelli predict period wish need galaxi text email love wait note complet enter custom user keyboard auto insert like download samsung like automat color support screen pen input star input updat won mark option except android basic address keyboard thai swiftkei automat dont b lda theme chinese jelly bean type worth theme chines predict type worth more languag text make monei like faster bean easi de nit color input jelli learn paid love more time predict penni wish need issu easier price custom switch accur speed download option annoi start accur total pick time browser perfectli cent red write samsung time amaz praise without any advice and worth emotional expression shown in table b in red color.
the reason is straightforward lda does not have a ltering phase.
second although with well tuned k value lda could also nd informative topics discovered by emnb lda some of them have lower quality.
for example the topic jelly bean shown in table b has i lower ranked key words and ii lower purity the word predict ranked high .
in sum we can conclude that our scheme with ltering performs better than the baseline scheme without ltering in solving our problem.
.
.
comparison of two topic models the second experiment is to compare the performance of two topic models lda and asum in our scheme.
for each dataset shown in table we manually identify one appropriate and representative group from informative reviews in the test set as ground truth prior to running our scheme where each review in the group is assigned a proportion score.
the topic column of table shows the labels of the groups.
following the same setup as the rst experiment k we evaluate the performance of emnblda and emnb asum by measuring if they can discover the pre identi ed groups accurately.
table presents the experimental results in terms of f measure averaged over iterations .
some observations can be drawn from the results shown in table .
first for all topics emnb lda performs better than emnb asum in terms of f measure.
one possible reason is asum imposes a constraint that all words in a sentence be generated from one topic .
thus sentence level reviews exhibit several topics are only assigned to one topic which results in information lost.
second by looking into the results the f measure achieved by774table evaluation results k dataset topic emnb emnbasum lda swiftkey theme .
.
facebook status .
.
templerun2 lag .
.
tapfish easier buck .
.
emnb lda is reasonable but not promising e.g.
.
for the theme topic of swiftkey .
the main reason is the unsupervised topic modeling is a hard task.
besides some informative reviews are removed wrongly by the lter while some non informative ones are not ltered out.
.
evaluation of ranking performance in this section we report a comprehensive case study to evaluate the ranking performance of ar miner.
we aim to examine whether the top ranked topics generated by arminer represent the most informative user feedback for real app developers .
we use swiftkey keyboard shown in table as our subject app.
the developers of this app created a nice swiftkey feedback forum to collect user feedback .
it provides users a voting mechanism for every feedback and feedback with high voting is ranked top.
feedback considered to be informative to developers is assigned a status which shows the current progress of this feedback.
therefore we can obtain a comparable ranking list of informative information for real developers of swiftkey keyboard.
speci cally we rst selected all the user feedback in the forums and then removed those feedback without status indicates non informative to developers or assigned status of complete indicates closed before the time interval t from oct 12th to dec 19th we investigated nally we ranked the remaining feedback in the decreasing order of number of votes.
the top ranked results ground truth are shown in figure veri ed around feb 17th .
the user reviews of swiftkey keyboard collected in t contains instances.
we use the lter shown in table and apply both lda and asum algorithms.
finally the top ranked results generated by our ranking model are visualized in figure lda setting k wg wi .
we compare the ranking results attained by ar miner with the ground truth ranking results figure and measure the comparison in terms of hit rate and ndcg scores introduced in section .
.
note that each feedback shown in figure is considered to be corresponding to a topic shown in figure if and only if i the feedback is closely related to the topic and ii the semantically similar feedback can be found in the top review instances of the topic.
table presents the results.
remark.
we use the ranking list shown in figure as the ground truth mainly because it is the choice of real app developers in real scenarios.
we believe it is much more convincing than a ranking list identi ed by others e.g.
the authors .
besides we assume that the greater agreement between the ranking results achieved by ar miner and the ground truth the better performance of ar miner.
specif ically if both hit rate and ndcg equal to we think that ar miner is as e ective as swiftkey feedback forum .
rank votes user feedback status more themes.
more themes.
more themes.
started continuous input glide your fingers across the screen flow started option to disable auto space after punctuation and or prediction under review customizable smileys emoticons under review autotext word substitution macros brb be right back under review traditional chinese started an option to use swiftkey predictions everywhere in every app including a web searches started chinese pinyin started thai started after jelly bean update swiftkey keeps returning to android default keyboard after restart or shutdown under review figure top ranked results attained from swiftkey feedback forum highlighted feedback has corresponding topic shown in figure table ranking results ar miner lda ar miner asum hit rate .
.
ndcg .
.
some observations can be found from table .
first for both metrics lda performs better than asum in our framework.
second by looking into the results ar miner lda achieves .
in terms of hit rate which indicates that ar miner is able to automatically discover the most informative information e ectively and thus can be benecial for app developers especially those who have not established valid channels.
feedback highlighted in figure has corresponding topic in the radar chart shown in figure .
for example the ranked 1sttopic discovered by ar miner shown in figure more theme where the number in square bracket represents the rank in figure corresponds to the ranked 1stuser feedback more themes.
more themes.
more themes.
shown in figure .
.
comparison with manual inspection and traditional channels we conduct two case studies to i compare ar miner with manual inspection in terms of manpower input and ii analyze the advantages of ar miner over facilities used in a traditional channel i.e.
online forum .
.
.
manpower input analysis in the rst case study we apply three schemes i arminer with emnb lter training examples ii arminer with nb lter training examples and iii purely manual inspection respectively to the test set of the facebook dataset shown in table examples .
we recorded the approximate manpower input of the rst author for nding the most informative information by these three schemes2.
for simplicity we ignore the performance di erence between ar miner and manual inspection.
figure a presents the comparison results.
2for purely manual inspection we recorded the e orts spent on sampled data and then estimated the total man hours.
.
.
.
ar miner emnb ar miner nb manual inspection manpower input man hour a manpower comparison b emnb vs. nb figure evaluation results on facebook .
a manpower comparison with manual inspection b comparison between emnb and nb with varied training data.
some observations can be found from the results shown in figure a .
first we nd that ar miner emnb lter .
man hours is much more e cient than purely manual inspection .
man hours .
the reason is ar miner only requires humans to label some training data and can work automatically after the lter has been built.
second ar miner nb needs more human e orts than ar miner emnb since building a nb lter whose performance is comparable to a emnb lter requires manually labeling more training data and examples for nb and emnb respectively in this case study .
we explain it with the results shown in figure b .
following the same setup described in paragraph of section .
figure b shows the average f measure of nb and emnb under varying amounts of training data facebook .
it is obvious that when the f measure score is xed nb always requires more training data human e orts than emnb the results are similar for other datasets check details on our website .
therefore we choose emnb in ar miner to reduce human e orts as much as possible.
.
.
comparison with an online forum following the same setup of swiftkey keyboard described in paragraph of section .
we conduct a case study to analyze the advantages of ar miner over a traditional channel i.e.
online forum swiftkey feedback forum .
first of all from user reviews ar miner has the ability to discover fresh informative information that does not exist in the swiftkey feedback forum .
take the ranked 1st topic more theme shown in figure as an example.
figure a shows more review instances in the top list of more theme .
the top ranked review shown in figure a and the ranked 1st user feedback shown in figure are semantically the same.
moreover we observe that the ranked 10th review .... or support for third party themes is only discovered by ar miner which o ers app developers new suggestions concerning the topic more theme .
this kind of new information is bene cial to developers since it may inspire them to further improve their apps.
second ar miner can provide app developers deep and more insights than swiftkey feedback forum by exibly adjusting the weight vectors of wgandwi.
for example as described in section .
swiftkey feedback forum only support a user voting mechanism like volume inwg to rank the user feedback while ar miner can achieve it from di erent angles.
if setting wg indicates groups are ranked only according to avgrating the ranking of more theme shown in figure drops from to which implies that it s not a kind of critical and urgent problem topic more theme score also we need more themes!
.
just wish you had more themes or ability to make a custom theme.
.
... ........................................... ....... it would also be nice to have more themes or support for third party themes .
.
a instances ranking .
.
.
p g k time window k score .
just wish you had more themes or ability ... ....... support for third party themes .
b pof theme figure unique information o ered by ar miner to users.
if setting wg indicates groups are ranked only according to timeseries the ranking of more theme drops from to .
the time series pattern of more theme in this case can be automatically visualized as shown in figure b which helps app developers easily understand that it s a kind of existing problem.
in sum this case study implies that even for those app developers who have already established some traditional channels ar miner can be a bene cial compliment.
.
limitations and threats to validity despite the encouraging results this work has two potential threats to validity.
first the authors are not professional app developers and thus the de ned category rules of informativeness as summarized in figure might not be always true for real app developers.
in this paper we have attempted to alleviate this threat by i studying what kinds of user feedback are realapp developers concerned with and ii exploiting realapp developers decisions as the ground truth for evaluation.
the second threat relates to the generality of our framework.
we validate our framework on user reviews of four android apps from google play.
it is unclear that if our framework can attain similar good results when being applied to other kinds of android apps e.g.
apps in amazon appstore and apps on other platforms e.g.
ios .
future work will conduct a large scale empirical study to address the threat.
besides another limitation of our work is that we only choose a ftext rating timestamp gas mentioned in section but a real app marketplace may have more features of user reviews e.g.
device name in google play amazon veri ed purchase in amazon appstore .
the impact of these speci c features is unknown but our framework is rather generic and extensible to incorporating more features in future work.
.
conclusion this paper presented ar miner a novel framework for mobile app review mining to facilitate app developers extract the most informative information from raw user reviews in app marketplace with minimal manual e ort.
we found encouraging results from our extensive experiments and case studies which not only validates the e cacy but also shows the potential application prospect of ar miner.
we also discuss some limitations along with threats to validity in this work and plan to address them in the future.
.