improving trace accuracy through data driven configuration and composition of tracing features sugandha lohar sorawit amornborvornwong depaul university chicago il usa sonul.
gmail.com sorambww hotmail.comandrea zisman department of computing the open university milton keynes mk7 6aa uk andrea.zisman open.ac.ukjane cleland huang depaul university systems and requirements engineering center chicago il usa jhuang cs.depaul.edu abstract software traceability is a sought after yet often elusive quality in large software intensive systems primarily because the cost and effort of tracing can be overwhelming.
state of the art solutions address this problem through utilizing trace retrieval techniques to automate the process of creating and maintaining trace links.
however there is no simple onesize fits all solution to trace retrieval.
as this paper will show finding the right combination of tracing techniques can lead to significant improvements in the quality of generated links.
we present a novel approach to trace retrieval in which the underlying infrastructure is configured at runtime to optimize trace quality.
we utilize a machine learning approach to search for the best configuration given an initial training set of validated trace links a set of available tracing techniques specified in a feature model and an architecture capable of instantiating all valid configurations of features.
we evaluate our approach through a series of experiments using project data from the transportation healthcare and space exploration domains and discuss its implementation in an industrial environment.
finally we show how our approach can create a robust baseline against which new tracing techniques can be evaluated.
categories and subject descriptors d. .
management life cycle general terms documentation management keywords trace retrieval configuration trace configuration .
introduction software traceability is an important element of the development process especially in large complex or safety critical software intensive systems .
it is used to capture relationships between requirements design code test cases and other software engineering artifacts and support critical activities such as impact analysis compliance verification test regression selection and safety analysis.
as such traceability is mandated in safety critical domains including the automotive aeronautics and medical device industries.
unfortunately tracing costs can grow excessively high if trace links have to be created and maintained manually by human users and as a result practitioners often fail to establish adequate traceability in a project .
to address these needs numerous researchers have developed or adopted algorithms that semi automate the process of creating trace links.
these algorithms include the vector space model vsm probabilistic approaches latent semantic indexing latent dirichlet allocation lda rule based approaches that identify relationships across project artifacts and approaches that identify artifacts committed as part of the same change to a version control systems or modified consecutively by a single user .
given such a profusion of traceability techniques and the multiple ways in which each technique can be configured or combined with others it is difficult to know which combination of techniques to use for a specific dataset and or project.
the problem of finding the right configuration is particularly pertinent as researchers have not yet been able to provide clear guidelines as to which tracing techniques are most effective on different kinds of datasets.
as the results reported in this paper will show finding the right configuration can lead to very significant improvements in the accuracy of generated trace links in some cases improving traceaccuracybyover100 .
thisisparticularlynotablebecause there is currently no individual tracing technique that has been shown to consistently outperform other techniques across all datasets.
industrial adoption of trace generation methods will be better supported if best of breed techniques can be discovered and used within the context of a specific project.
furthermore research advances will be facilitated if new techniques can be compared against the best combination of existing techniques instead of against a single baseline technique which performs inconsistently across different datasets.
in this paper we therefore present a state of the art approach for configuring a traceability infrastructure in order to improve the achieved accuracy of the generated trace links.
we define traceability infrastructure as the enterprise permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
copyright is held by the author owner s .
publication rights licensed to acm.
esec fse august saint petersburg russia acm 378trace generation dictionary builder american national corpus local idf trace algorithm vsm lsi preprocessor acronym expander stemmer splitter stopper static stopper dynamically stopper normalized cosine jacard dice simple nonnormalized cosine lda ranked trace capture commit log trace extractor prospective trace capture direct query manipulator results orderer intermediate root node.
feature used in experiments.
potential feature not used in experiments.
cardinality link merger voter alpha wt beta wt gamma wt increment count increment size incremental threshold topic count thrd parameters figure a selection of trace algorithms and utilities represented in a feature model.
wide instrumentation needed to support all aspects of the traceability process including gui graphical user interface components plug ins for case tools files for storing traceability data and also trace generation and traceability management tools.
our approach which we refer to as dynamic trace configuration dtc utilizes an initial training set of validated trace links to discover a goodconfiguration of tracing techniques.
we define a good configuration as one that delivers high accuracy of the generated trace links.
furthermore as our approach does not guarantee to find the optimal approach we also introduce the concept of top configuration as the best configuration found by our dtc approach for a given dataset and feature model.
given the potentially large combination of techniques and the slow running time of some of the tracing algorithms we utilize a genetic algorithm ga to intelligently search through the space of viable configurations in order to find the best performing configuration.
we justify our choice of the ga in section .
.
while other researchers have explored different techniques for combining predefined sets of tracing techniques the novel contribution of our work is the dynamic and scalable approach for learning what can be seen as a good composition and configuration of a very broad and potentially expanding set of tracing techniques.
the remainder of the paper is structured as follows.
section describes the dtc infrastructure and processes including the use of feature modeling simulation and intelligent search.
section describes a series of experiments that we conducted to evaluate the efficacy of our approach for finding an effective configuration and for using the identified configuration in an evolving software project.
section discusses the practical applications in industrial projects and research domains.
section discusses threats to validity and section describes related research from areas of traceability and self adaptation environments.
finally section summarizes our findings and discusses future work.
.
dynamic trace configuration our approach takes as input a training set of source and target artifacts and a matrix of human validated trace links which serve as a reference set for improving the accuracy of the generated trace links.
it also imports a feature modelof available tracing techniques and then searches for the combination and configuration of features that increase the accuracy of automatically generated trace links when applied to the reference set.
the dtc includes three primary elements a feature model specifying the available trace algorithms and utilities from now on referred to as features a simulation environment in which any viable configuration of features in the feature model can be instantiated and then used to generate trace links against a given dataset and an intelligent search component capable of directing a search through the space of all viable configurations in order to find a good configuration for a given dataset.
in the following sections we describe each of these in more detail.
.
modeling features and configurations dtc utilizes a feature model to specify the set of available features and their constraints .
while alternate representations are possible feature models have been shown to be expressive and scalable across many different domains and can therefore support a wide variety of features and interactions.
utilizing a feature model not only accommodates the features of the current experiment but can easily scale up to incorporate new features as they are proposed by researchers and become available to practitioners.
furthermore given the widespread interest in feature modeling several different tools are available for generating valid configurations and or checking the correctness of candidate configurations.
in this work we represent the feature model using the textual variability language tvl and associated tools developed by heymans et al.
tvl is a text based feature modeling language that provides a rich syntax and formal semantics.
the tvl tool provides a syntax checker which wasusedtoinitiallyvalidatethewell formednessofthetrace feature model.
it also provides support for querying whether a specific configuration is valid.
for illustrative purposes we depict a small and representative set of popular tracing techniques in figure using a graphical display of a feature model.
this feature model contains several basic features.
preprocessors are used to prepare raw data for tracing.
for ex379splitter stemmer static stopper dynamic stopper acronym expander pre processor dictionary builder similarity computation display local idf anc idf lsi vector space model simple jacard dice ranked incremental metrics computation cosine norm cosine basic figure pipe and filter architecture used to instantiate valid combinations of tracing features.
ample a stopper removes commonly occurring terms such as this and that which are not useful for tracing purposes while an acronym expander expands acronyms such as rbac to their extended forms in this case role based access control .
the dictionary builder constructs an index of terms found in various documents and computes term weightings using algorithms such as tf idf term frequency inverse document frequency based on terms found in the project documents or on terms found in the american national corpus.
core algorithms such as vsm vector space model lsi latent semantic indexing and lda latent dirichlet analysis compute the similarity between pairs of documents.
latent semantic indexing lsi utilizesamathematicaltechniquecalledsingularvaluedecomposition svd to uncover the underlying latent semantic structure of word usage in unstructured text and then uses these latent topics to match queries and documents that are conceptually similar in meaning regardless of whether they share specific words.
the vector space model vsm represents queries and documents as vectors of terms and then applies the standard term frequency inverse document frequency tf idf approach to compute similarity scores between artifacts based upon the frequency at which the term occurs in both the source and target artifact and the inverse frequency of its occurrence across the collection of documents.
generated trace links can be presented to the user in several different ways including a basic ranked approach in which links are presented in the order of the generated similarity scores an incremental approach in which links are presented incrementally to the user and relevance feedback is used to reorder the remaining links and finally the direct query manipulation dqm approach in which a user modifies the query in order to filter out unwanted results .
finally components such as commit parsers rule based link generation techniques and tool based monitors can also be used to generate trace links.
a feature model also specifies the cardinality of features and depicts whether a given feature is mandatory or optional.
inadditiontothe visual representationshowninfigure additional requires andconstrains relationships must also be specified.
examples include vsm requires dictionary builder or requires link merger voter .
while most features are binary in nature i.e.
either present or not present some features such as lsi must be configured prior to use.
the feature model documents the range of allowed values for each parameter.
.
evaluating a configuration our approach requires various candidate configurations to be evaluated by utilizing the configuration to generate tracelinks from source to target artifacts.
the resulting trace linksarethencomparedagainstthereferencetracematrixto determinehowwelltheconfigurationperformed.
themetric of mean average precision map is used for evaluative purposes.
map computes the extent to which correct links are placed at the top of the ranked list of generated trace links.
because our implementation of map examines all correct links it also assumes recall i.e.
the ability to retrieve correct links of .
the use of map as a traceability measure has been advocated in numerous papers .
first the average precision ap of each query is computed ap summationtextn r precision r isrelevant r relevantdocuments whereris the rank of the target artifact in an ordered list of links isrelevant is a binary function assigned if the link is relevant i.e.
marked as correct in the reference set and otherwise p r is the precision computed after truncating the list immediately below that ranked position and nis the total number of documents.
when multiple links are listed for a single similarity score e.g.
at similarity of zero the links are evenly distributed across the space of that score simulating their random distribution.
map is then computed across all queries as follows map summationtextq q 1ap q q where qis a single query and qis the total number of queries.
to determine the efficacy of a configuration for tracing a particular dataset it is necessary to provide an architecture that can accommodate all valid combinations of available features.
for purposes of the experiments described in this paper we designed the architecture shown in figure to accommodate all of the features shaded in figure .
this architecture utilizes a pipe and filter pattern and assumes a fairly rigid sequencing of processes.
components in the pipeline can be turned on or off depending on whether they are required in the configuration.
the architecture provides one possible solution for configuring any valid configuration of features from our feature model.
however alternate and more flexible architectural designs are also possible.
for example a more extensive architecture could accommodate the dynamic sequencing of preprocessors and or allow results from multiple tracing techniques to be merged through voting and or other combinatory techniques.
.
searching for the best configuration utilizing a brute force approach to evaluate every single candidate configuration can be infeasible given that the fea380current configuration evaluate fitness function perform crossover mutation and roulette wheel selection top discovered configuration feature model triggers user data change trace use change environment change.
generate trace links and compare to reference set new population evolve further?
generate initial population randomly check for validity tvl initial valid population no yes chromosome mean avg.
precision data and trace links architecture model c d e f g figure the overall dtc process turemodelshowninfigure1includes1 408validconfigurations without even considering parameterizations of the individual components.
assuming reasonable increments for each parameter i.e.
increments of for values ranging from or to increments of for values ranging from and increments of .
for values ranging from .
.
the number of valid configurations jumps to .033383e .
in our experimentswe observedthatfor large datasetsandcandidate configurations that include slow running algorithms such as lsi it takes approximately two hours to generate a complete set of trace links from source to target artifacts.
we estimate the average time over all candidate configurations for our largest data set see section to be minutes per configuration.
therefore generating traces for every configuration would take approximately five hundred and thirty five million years to run.
this is clearly infeasible and therefore a more intelligent approach is needed for searching through the space of viable configurations.
there are several viable searching techniques however we chose to utilize a genetic algorithm ga because it is a natural fit for our problem and is relatively straightforward to implement.
a ga mimics the evolutionary process found in nature.
it involves the steps of i modeling a candidate solution as a chromosome ii creating an initial population of chromosomes iii computing fitness functions to evaluate the efficacy of a given configuration iv evolving the population to the next generation and v selecting the best known configuration.
figure shows an overview of the main ga steps.
these steps are described below.
.
.
modeling candidate solutions a ga fits our problem because each candidate trace configuration can easily be modeled as a chromosome encoded as a string of bits where each bit represents a different feature that is either present or not present in the configuration.
as shown in figure configurable parameters are represented as sub chromosomes.
for experimental pur poses we included features with four configurable ones.
for example the incremental ranking component is configured by five parameters alpha beta gamma increment count and increment size all of which are represented in the sub chromosome.
.
.
creating an initial population for each run of the ga an initial population of chromosomes is created see step in figure .
as previously explained the feature model is specified using the textual variability language tvl .
chromosomes are passed as queries to the tvl tool to check their validity see step .
invalid configurations are rejected and the process is continued until the complete population is built.
splitter target side stemmer target side stopper target side dynamic stopper target side threshold acronym expander target side splitter query side stemmer query side stopper query side dynamic stopper query side threshold acronym expander query side local idf dictionary target side local idf dictionary query side .
alpha anc dictionary builder .
beta ranked ordering of results .
gamma incremental ordering of results no.
iterations vsm with simple matching size increment vsm with dice vsm with normalized cosine vsm with non normalized cosine vsm with jacard lsi no.
of topics average precision figure the genetic algorithm represents tracing configurations as a chromosome.
.
.
the fitness function the fitness function of each chromosome is computed by generating a trace configuration according to the features specified in the chromosome and then using this configuration to generate trace links for the training set see steps and in figure .
the map score which is computed by comparing the generated links to the reference matrix for each dataset serves as the fitness function for the ga algorithm.
.
.
evolving the population a stochastic process is used to select the best chromosomes to be carried forward as parents into the next generation.
our implementation carries ten chromosomes forward.
one of these is the elitechromosome i.e.
the chromosome with the highest map value from the previous generation.
the remaining nine chromosomes are selected using a standardpracticebasedonthe roulette wheel whichworksonthe premise that chromosomes scoring higher map values have a greater chance of survival than weaker ones.
basically fitter individuals are given proportionally higher chances of being selected during a spin of the roulette wheel than less fit ones.
in our implementation we select only ten parents 381based on an initial series of experiments designed to finetune performance by minimizing runtime and maximizing accuracy of the ga algorithm.
gas typically use two techniques of cross over andmutationto generate offspring chromosomes from the parents in order to create the new generation step .
these two practices emulate the natural process of carrying forward genetic material from the parents while introducing sufficient variation to potentially surpass their parents in terms of fitness.
amutation involves flipping one of the genomes bits in the chromosome i.e.
flipping a to in order to remove a feature or flipping a to in order to add a feature.
based on initial experimentation a mutation rate of .
was established for our experiments.
to execute a mutation a number nfrom is randomly selected to represent the number of bits to be flipped.
nunique bits are then randomly selected and flipped.
if a sub chromosome exists for any of the selected bits one of the sub chromosome s bits are randomly switched to a score within the range of allowed values.
a cross over involves creating two children from the genetic material of two parent chromosomes.
cross overs are executed by selecting two parents choosing a random position in the chromosome using this position to partition each of the chromosomes into head and tail sections and then exchanging the two tails.
.
.
computing fitness functions for purposes of our experiments we established a stopping condition that halts the ga if no improvements are found after successive generations and otherwise halts after the 60th generation.
once the experiment halts the highest performing configuration across all generations is selected.
the visualization tool depicted in figure was developed to support experimentation by displaying the map score of configurations discovered through multiple generations of the ga search.
it depicts results from the first seven generations for a healthcare dataset i trust described in section .
as a sanity check we compared the output of our ga to previously published results which were available for some of the datasets.
these results confirmed that our approach consistently produced either improved or comparative map scores .
x .
.
.
.
.
.
.
.
.
.
.
mean average precision main page itrustreq jspcode2 itrustreq jspcode generations g1 g2 g3 g4 g5 g6 g7 figure visualization of the growth in map scores y axis over progressive generations x axis of the ga search process3.
ev aluating dtc customization while the goal of dtc is to find the optimal trace configuration for a dataset given an available set of tracing features a ga algorithm does not guarantee to do so.
for any form of dynamic adaptation to be worthwhile in a system the benefits gained from changing the system must outweigh the associated costs.
in this section of the paper we evaluate the benefits of customizing the trace infrastructure for a specific project.
in the following section we explore the practicality and processes that might be followed in order to integrate reconfiguration into a real project environment.
.
experimental design dtc was evaluated against six different projects from the transportation healthcare and space domains as depicted in table .
in the table the number of the source and target elements are represented in brackets.
for example industry has unique requirements and unique design artifacts.
four of our datasets were obtained from industry or government sources while the remaining two datasets represent academic projects.
the two transportation sector projects were provided under non disclosure agreements and are therefore simplyreferredtoasindustry 1andindustry2.
alldatasetsexceptfor industry1and2arepubliclyavailable through coest.org.
for each dataset the confirmed trace matrix was provided either by the industrial source or else previously used and validated by several different research groups .
table datasets used in study project source target links industry transport reqs design industry transport reqs design i trust health reqs code cchit health reg.
codes reqs e clinic health use cases test cases cm nasa reqs design all of the experiments were conducted utilizing the subset of features shaded gray in figure .
features were limited to those which were either already available to us as executable components or which we were able to implement within the timeframe of this project.
the architecture shown in figure was utilized as a framework capable of executing any valid combination of features.
the experimental tracing environment was constructed using the tracelab tool .
tracelab is a highly flexible research platform that provides an extensible library of components for importing and preprocessing artifacts for generating trace links using multiple techniques and for reviewing and evaluating results.
these components can be integrated into complex executable workflows using the tracelab plug and play environment.
data is exchanged between components via standard tracelab datatypes using a blackboard architectural style.
our experimental test harness which was created in tracelab was responsible for importing the data to be tested and then utilizing the dtc process to search for the best trace configuration.
.
experiments experiments were conducted to address three specific research questions rq designed to evaluate the benefits and efficacy of dynamic trace configuration.
382table experiment demonstrated that each dataset had a unique top configuration features e clincm 1cchit i trust ind 1ind c1c2c3 c4c5c6 achieved map .
.
.
.
.
.
stem sourcestandardstop targetstandardstop sourcedynstop targetdynstop splitsource splittarget sourceacronym targetacronym tf idfdictbld ancdictbld vectorspmodel lsi cosine basic cosine norm dice jacard simplematch incr.display rank.display sourcedynstop targetdynstop rocchioiteration rocchiotopn rocchiogamma .
.
rocchioalpha .
.
rocchiobeta .
.
lsi k rq1 does each dataset of source and target artifacts have a distinct topperforming trace configuration?
the first research question investigated whether each of the six datasets depicted in table had a unique top configuration.
we utilized dtc to discover the top configuration for each of the datasets.
running time of dtc varied from minutes for smaller datasets e.g.
e clinic to over hours for larger datasets e.g.
industry .
results are reported in table and show that different datasets performed best with very different configurations.
for each data set we report the achieved map score for the highest performing configuration ci marked with a bullet in the corresponding cells of the features in the configuration.
the lower part of table shows the parameter values of the features in the respective configurations.
for example the e clinic dataset returned a healthy map score of approximately .
when using the vsm with local idf.
in contrast the cchit data set performed best using lsi achieving map scores of .
.
finally the industry achieved its highest map scores of .
using vsm combined with the anc dictionary builder.
these results clearly show that different datasets have their own top scoring configurations.
as a general observation it should be noted that larger datasets tend to produce lower map scores due to the higher potential for false positive links.
another interesting observation is that the results were not easily predictable in advance.
in fact general wisdom suggests that lsi outperforms vsm on larger datasets however in this experiment lsi was not selected for the largest dataset i.e.
industry .
furthermore the use of the global tf idf which had previously been discredited led to very significant improvements in the industry dataset.
these results highlight the value of using dtc to experimentallydiscovertheoptimalconfigurationforagivendatasetinstead of just adopting a default configuration.
rq2 is there a single configuration which performs well on all datasets?
the previous experiment identified the topconfiguration for each dataset however in a second experiment we explored the closely related question of whether any of these top configurations would perform well across all six datasets.
the underlying premise here is that if we could identify a single configuration which performed well for all datasets then we should adopt that configuration as the default and avoid the costs associated with dynamic discovery and adaptation of a trace configuration.
in order to investigate these differences we used the top configuration achieved for each of the six datasets in experiment labeled configurations c1 to c6 in table and applied each configuration against all of the other five datasets to obtain their respective map values.
the results of this experiment are reported in figure and show that none of the tested configurations performed well across all datasets.
the best overall configuration was c4 with a mean map of .
over all six datasets.
however this configuration performed particularly poorly on the industry dataset and was outperformed by three other techniques for e clinic.
these results and the others reported in figure clearly show that no single configuration performed well across all datasets.
we hypothesize that individual characteristics of eachdatasetinfluencetheperformanceofthevarioustracing features.
for this reason the one size fits all approach which has been the defacto standard until now does not appear to be effective.
rq3 does each pair of artifact types in a project have a distinct top performing configuration?
in this experiment we evaluated whether the same configuration could be used to trace between different types of artifacts in the same project i.e.
between requirements and regulatory standards or between test cases and requirements.
easy clinic industry and i trust datasets were used for this experiment as each of these had multiple trace matrices as depicted in table .
dtc was run against each pair of artifact types for which a confirmed trace matrix was available.
the experiment followed the same design used for experiment1.
foreachpairofartifactsinaprojectweusedthetop configuration achieved for that pair and applied it against all other pairs of artifacts in the same project to obtain their respective map values.
the results are reported in table and show that each pair of source target artifacts performed best when using its own customized configuration meaning that a configuration identified for one pair of artifacts in a project should not necessarily be used for all other pairs of artifacts in a project.
these results were not entirely unexpected given that different artifact types i.e.
requirements vs. code vs. test cases exhibit very different characteristics in terms of document length and vocabulary used.
.
analysis of results an analysis was performed to determine statistically if the use of the customized trace configurations was effective.
based on the results of experiment and experiment we identified the top configuration for each pair of source target artifacts across all of the datasets.
we refer to these configurations as customized trace configurations .
for example .
.
.
.
.
.
.
.
.
e clinic cm cchit i trust ind ind mean average precision map c1 c2 c3 c4 c5 c6 customized configuration figure mean average precision map obtained using the six winni ng configurations across all datasets the customized trace configuration for easy clinic dataset from table is c1 while all other tested configurations are considered non customized configurations for that dataset.
based on these definitions a paired t test was performed to determine if the use of customized trace configuration is effective.
in this analysis each customized configuration is paired with its relevant non customized configuration.
for example in the easy clinic dataset c1 is paired with its relevant c2 c3 c4 c5 and c6 see table configurations as well as c1.
and c1.
see table .
please note that c1.
is excluded because it is the same as c1 .
for different pairs of source target artifacts across six different datasets there was pair of customized non customized configuration pairs.
we point out that if we could have performed pairwise comparisons against a far larger sample of configurations generated by the ga or randomly selected ones.
we chose not to do this because we wanted our comparison to be against strongly viable configurations and not very low performing configurations which would be unlikely to be used.
a mean improvement in map score of .
on a scale of .
.
was observed for customized configurations with a standard deviation of .
.
this was significantly greater than zero i.e.
t .
one tail p .20e11 and confidence interval of providing evidence that customized configurations are effective for improving trace accuracy.
the .
increase in map score represented a .
improvement realized through using customized versus non customized configurations.
furthermore given concerns over the normal distribution of the data a wilcoxonranked sum test was also conducted.
this test rejected the null hypothesis at z value .
with alpha .
and p .
indicating that there is a significant difference between the two samples.
we also compared the raw distribution of map scores for customized versus non customized configurations and plotted the results in the box and whisker graph shown in figure .
this graph provides an intuitive visualization of the difference in map scores between customized and noncustomized configurations.
for example it shows that mean map scores for customized trace infrastructures is .
versus .
for non customized configurations.
based on these results and the datasets included in our study we concludetable different configurations are needed for different traceability paths within the same project easy clinicuc tc tc code uc code c1.
c1.
c1.
use case test case .
.
.
test case code .
.
.
use case code .
.
.
i trustreq code req jspcode req javacode c2.
c2.
c2.
requirements code .
.
.
requiremnts jsp .
.
.
requirements java .
.
.
industry 2ssrs sdd sdd srs c3.
c3.
system reqs design .
.
design software reqs.
.
.
that customizing trace configurations significantly improves the quality of generated trace links across alldatasets.
.
dtc in practice one of the primary objectives of dtc is to support ongoing customization of the traceability infrastructure in an industrial project in order to improve the effectiveness of trace retrieval methods in practice.
in this phase of the work we experimentally investigated the use of dtc in an industrial setting by simulating the impact of a growing dataset on the trace configuration.
.
experiments two additional experiments were conducted.
the first explored the degree to which configurations were stable over time while the second addressed the critical question of whether a configuration learned on an initial training set of data would be effective for new data as it was created i.e.
new requirements and or new code etc .
rq4 are configurations stable over time?
an experiment was designed to answer the critical question of how stable the trace configuration is over time.
this is an important practical question that addresses the likelihood of thrashing from one configuration to another.
this question 384x x .
.
.
.
.
.
.
.
.
customized non customizedmean average precision map x mean figure difference in map scores obtained across all evaluated datasets for customized versus noncustomized configurations.
maximum minimum mean as well as 1st 2nd median and 3rd quartiles are shown.
wasexploredagainsttheindustry industry andcchit datasets as these were the only datasets of sufficient size to supportthistypeofanalysis.
wecreatedfourversionsofthe source artifacts including and of the data labeled d25 d50 d75 and d100 respectively.
the sub parts were derived following the natural structure of the requirements documents that constituted source artifacts in all three datasets.
this meant that d25 represented the first physical of the requirements appearing in the specification and d50 represented the first of the requirements and so on.
the dtc algorithm was used to identify the top configuration for tracing from each subset of source artifacts i.e.
d25 d50 d75 and d100 to the target artifacts shown in table i.e.
in the case of industry the first trace was executed from the first of requirements to the design elements and so on.
the top configurations are shown in table labeled c25 c50 c75 and c100 respectively.
in the case of the industry dataset we see an initial configuration based on lsifor the d25 subset and then a transition to vsmwith use of the global tf idf computations i.e.
american national corpus of written terms .
the configuration remained relatively stable as the datasize grew over d50 d100.
we see a similar pattern for the cchit dataset.
the initial d25 configuration utilized vsm but this was replaced by lsi at d50 and from that point on the configuration remained relatively stable.
in the final case of industry the configuration was slightly less stable.
we hypothesize that this could have been because the number of source artifacts was significantly smaller than in the other two datasets i.e.
requirements vs. and in industry and cchit respectively and so the configuration did not have time to stabilize.
these results suggest that changing configurations too frequently during initial phases of the project could lead to thrashing however some reconfiguration is necessary as the size of the project grows.
although not investigated here it might be helpful to evaluate configurations over small sized increments of growth and only reconfigure if a configuration is stable over a window of several increments or if it returns a very significant increase in map score.table trace configurations for various sized growth increments of three datasets featuresindustry industry cchitc25 c50 c75 c100 c25 c50 c75 c100 c25 c50 c75 c100.
stem sourcestandardstop targetstandardstop sourcedynamicstop targetdynamicstop splitsource splittarget sourceacronym targetacronym tf idf dict builder anc dict builder vector space model lsi cosine not norm cosine normalized dice jaccard simplematching incremental display ranked display rq5 do reconfigurations of the trace infrastructure lead to better trace quality in future traces?
while previous experimental results clearly demonstrated that trace configurations can be customized according to the current dataset it is important to know whether runtime configuration improves the overall quality of trace links in the project.
this question directly addresses one of the underlyinghypothesesofourwork thatperforminga whatif analysis on past data serves as an effective predictor of the best configuration as the project data continues to grow insize.
inthisscenario abaselinetraceconfigurationisused to generate an initial set of trace links and then dynamically learned configurations are used throughout the remainder of the project.
for experimental purposes we selected an initial default configuration c0 that included a stemmer stopper vector space model with non normalized cosine similarity and ranked ordering of results.
while we could have selected a different default configuration this one was chosen because the selected combination of components is fairly standard across the literature and also performed well in our previous experiments.
the documents d25 d100 and configurations c25 c100 were reused from experiment .
configuration c0 was used to generate trace links and then to compute map scores for d25 c25 ford50 c50 ford75 and finally c75 ford100.
in this way the system was reconfigured after the simulated arrival of each block of of requirements and the new configuration was used for tracing purposes until the next configuration occured.
figure8depictstwoscenariosforeachofthethreedatasets.
in the first scenario the c0 baseline configuration was used for the entire dataset while in the second scenario the trace infrastructure was reconfigured upon arrival of each subsequent block of requirements.
the three graphs show that reconfiguration led to marked improvements in map scores.
the most important observation from this experiment is that in all cases the customized configuration outperformed the static configuration.
the dip in map scores for industry is attributed to the fact that the industry dataset is smaller than the other two data sets i.e.
had a .
.
.
.
.
.
.
.
mean average precision customized configuration standard configuration a industry .
.
.
.
.
.
.
.
mean average precision b industry .
.
.
.
.
.
.
.
mean average precision c cchit figure results of experiment showing the impact of utilizing a default industry standard configuration c0 versus dynamically reconfiguring the trace infrastructure at regular intervals i.e.
arrival of next of requirements across the project total of requirements only and therefore the results were more sensitive to nuances in individual requirements.
.
threats to v alidity there are a number of potential threats to validity.
one threattointernalvalidityisthatthetopconfigurationsidentified in our experiments are based on the selected architectural design the feature model used in the experiments and also the components which we implemented.
different choices in any of these areas might have produced different results.
the issue of component quality was partially mitigated through reusing as many existing tracelab components as possible especially those which had been used successfully in previously published results.
for example the lsi feature was implemented through using an lsi component previously developed by researchers at the college of williamandmary.
moreimportantly theprimarycontribution of this paper is the process for dynamically discovering an optimal configuration given an available set of features and is not primarily designed to comparatively evaluate different algorithms.
a threat to external validity is concerned with the data sets used in the experiments.
we assumed that the initial set of trace links provided for the data sets are correct independent of the way they have been generated.
this issue is mitigated in two ways.
first trace links for industry and industry 2wereprovidedbyourindustrycollaborators.
the provided links are therefore likely to be correct although there is no guarantee of completeness.
i trust links were likewise created by a member of the i trust project.
links for the other three datasets have been created by various research groups and have been used extensively in past research projects.
despite potential inaccuracies in the answer set our approach is designed to search for the best trace configuration in terms of delivered map scores given the known set of trace links and would only be significantly impacted if the quality of the trace links were very low.
this is not the case as each trace matrix has undergone significant review either in an industrial or research setting.
in terms of experimental design it was not possible to explore every single configuration to know whether the ga algorithm discovered the optimal configuration.
given the running time needed to test certain configurations we were unable to repeat execution of the entire dct algorithm multiple times for each experiment in order to perform a more rigorous statistical analysis.
we leave this for future work.
furthermore in the experiments in which we compared the efficacy of different configurations it is possible although somewhat unlikely given the consistency of our results that another undiscovered configuration would have performed well across all datasets.
finally only six different projects were used in this work providing a limited perspective on the traceability problem.
this limitation which is partially due to the significant challenge of acquiring industrial sized datasets means that we cannot make broad generalizations about the relationships between data characteristics and trace configurations.
on the other hand several of the datasets represent large industrial projects taken from different domains artifact types and sizes and therefore provide a solid context for the experiments.
.
related work in related work gethers et al.
analyzed the benefits of combining ir techniques such as vsm and the jensen and shannon model with relational topic modeling to improve traceability recovery accuracy.
similarly chen and grundy combined vsm with regular expressions key phrases and clustering techniques.
dekhtyar et al.
used voting committees composed of three to five trace retrieval techniques to identify mistakes introduced by human analysts when building a trace matrix.
however all of these approaches integrated a fixed set of previously selected techniques.
finally falessi et al.
empirically evaluated whether a limited combination of techniques outperformed a single technique.
they used logistic regression to compute the optimal combination of techniques unlike dtc their 386approach was not designed to support dynamic configuration of a large and potentially growing set of techniques.
dynamicallyadaptableandconfigurablesystemshavebeen the focus of study in several areas of computing such as software engineering robotics control systems programming languages and bioinformatics.
the software engineering community attaches great importance to this topic and has proposed two roadmaps on software engineering for selfadaptive systems .
inrequirementsengineering self adaptivesystemsinclude languages to address uncertainty when specifying the behavior of adaptive systems requirements monitoring frameworks to verify violations of system s properties and changes in requirements due to other requirements .
architecture models have also been used to support adaptation of software systems .
for example dashofy et al.
present a framework for self healing of event based software architectures which uses what if analysis to verify the change impact in architecture descriptions.
genetic algorithms gas have been used in different software engineering activities .
examples of these activities include but are not limited to evaluation of software reliability refactoring software testing identification of valid design pattern transformation for software reusability software clone detection and configuration of topic modelling techniques .
our approach has similarities to ensan et al.
s approach which uses a ga to explore the configuration space of product line feature models in order to identify test suites at low size complexity while balancing error and feature coverage.
in contrast our approach is designed to identify an optimal configuration of the traceability infrastructure.
while both ensan s approach and our approach use a ga to search the configuration space of a feature model the fitness functions are very different.
gas were also used by wang et al.
to identify suitable configurations for clone detection techniques in order to ameliorate the confounding configuration choice problem that is found in several clone detection techniques.
the lda ga approach uses a ga to configure the parameters of lda latent dirichlet allocation topic modelling technique in order to improve the performance of traceability link recovery feature location and software artefacts labelling.
in contrast our work uses a ga to explore configurations and compositions of different tracing techniques of which lda could be one of the possible features.
.
conclusion this paper has presented a novel approach for composing and configuring a trace infrastructure according to the data characteristics of a project.
from an implementation perspective dtc adds little overhead to the human effort involved in the tracing process as all of the functionality can be bundled up and deployed into a single tracing component.
while dtc s benefits can only be realized once an initial training set of confirmed trace links has been established it is primarily during the later phases of development and maintenance that tracing support is most needed.
once realized thebenefitsofdtccanleadtosignificantimprovements in the accuracy of the generated trace links.
dtc also has the potential to significantly impact future research practices in the area of traceability.
currently new techniques are typically compared against a single baseline technique such as vsm or lsi.
the authors of this paperhave also followed this practice in the past.
however a more rigorous analysis would either compare a new technique against the top configurations for a set of datasets or would demonstrably show that the technique generally improves trace results across multiple datasets in comparison to a baseline technique such as vsm or lsi.
finally dtc supports technology transfer.
if an organization has adopted dtc then there is little innate risk in adding a new feature to the mix.
if the feature is effective it will be selected during the configuration process and if it is not effective it will not be used.
this approach follows a low risk in situ approach to data driven adoption.
this paper has described the dtc approach and conducted an initial series of experiments.
however in future work we intend to extend the feature model to incorporate additional features and to explore different searching techniques such as hill climbing.
.