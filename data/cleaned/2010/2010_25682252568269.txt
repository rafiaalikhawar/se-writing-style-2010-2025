comparing static bug finders and statistical prediction foyzur rahman sameer khatri earl t. barr premkumar devanbu department of computer science university of california davis davis ca usa mfrahman sakhatri ptdevanbu ucdavis.edu department of computer science university college london london wc1e 6bt uk e.barr ucl.ac.uk abstract the all important goal of delivering better software at lower cost has led to a vital enduring quest for ways to find and remove defects efficiently and accurately.
to this end two parallel lines of research have emerged over the last years.
static analysis seeks to find defects using algorithms that process well defined semantic abstractions of code.
statistical defect prediction uses historical data to estimate parameters of statistical formulae modeling the phenomena thought to govern defect occurrence and predict where defects are likely to occur.
these two approaches have emerged from distinct intellectual traditions and have largely evolved in dependently in splendid isolation .
in this paper we evaluat e thesetwo largely disparateapproachesonasimilarfooting.
we use historical defect data to apprise the two approaches compare them and seek synergies.
we find that under some accounting principles they provide comparable benefits we also find that in some settings the performance of certain static bug finders can be enhanced using information provided by statistical defect prediction.
categories and subject descriptors d. .
testing and debugging code inspections and walk throughs general terms experimentation measurement reliability verification keywords software quality fault prediction inspection empirical software engineering empirical research .
introduction given the centrality of software in modern life and the fallibility of human programmers better software quality is a never ending quest.
software quality control involves several distinct approaches including testing inspection an d permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
icse may june hyderabad india copyright acm ... .
.formal verification.
in this paper we use historical defect data to comparatively evaluate and search for synergies between two approaches that have of late been of tremendous interest both in academia and industry static bug finding andstatistical defect prediction static bug finding sbf theseapproachesrangefrom simple code pattern matching techniques to rigorous static analyses that process carefully designed semantic abstractions of code all sbftools find and report likely defect locations in code.
these likely locations are reported to programmers typically at coding time.
the pattern matching techniques such as pmd are unsound but scale well and have been effectively employed in industry.
hybrid tools likefindbugs incorporate both static data flow analysis and pattern matching.
tools like esc java and codesonar1areslower buttheiranalyzeshavedesirable soundness properties see section .
.
in practice however reported warnings are infested with both false positives and or false negatives.
false positives can waste a developer s time and false negatives can allow defects to escape and cause customer grief.
developers usually cannot determine the truth or falsity of a given warning without examining code.
defect prediction dp extensiveandwidespreaddatagathering in modern software processes stimulates this approach.
animated by theories of the human and technical factors that durably influence errors researchers have channeled this deluge of data to estimate rich statistical and machine learning models that predictwhere defects can occur.
given the relative immutability of human nature models are expected to be quite stable over relatively long time intervals.
thus it has been believed statistical methods can predict where defects are likely to occur in the future.
empirical evaluations using fairly sophisticated economic models suggest that these methods are indeed likely to be effective in helping to locate defects motivation barriers these approaches have emerged fromparallelanddisparatetraditionsofintellectualthought one driven by algorithm and abstraction over code and otherbystatisticalmethodsoverlargedefectdatasets.
the se differences are perhaps analogous to the chomsky norvig debate in computational linguistics see section .
although the holy grail of sbfis automatically proving a correctness property of a program i.e.certifying the program free of a certain class of bugs undecidability means that in practice sbfproduces warnings that identify lines for developer inspection.
for its part dpidentifies filesfor to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
icse may june hyderabad india copyright acm ... .
inspection.
in practice then these approaches tackle the same problem improving inspection efficiency the problem of finding minimal potentially defective regions in source for careful inspection.
however neither is infallible.
although in practice these two approaches share the same goal their performance is difficult to compare.
we call this the comparability of bugginess identification cbi problem this problem comprises two subproblems zonationandmeasurement .
the zonation problem concerns the granularity at which a tool reports bugginess intra line line statement scope block method file etc.. while most defects are local and contiguous a single defect can be arbitrarily scattered in the sense that fixing it requires changes to lines throughout a codebase.
defect scatter causes the measurement problem given a particular zonation how should one measure bug identification?
choices for a binary score include the reported zone must subsume the defect or it must simply intersect the defect choices for a fractional score include the ratio of the size of intersection to the size of the defect or their jaccard index.
the ieee computer society defines software engineering as the application of a systematic disciplined quantifiable approach to the development operation and maintenance of software that is the application of engineering to software.
the study of approaches as in .
its goal is to find ways to produce better cheaper software faster.
the cbi problem therefore cuts to the core of software engineering neglecting it violates this imperative because it not only leaves unanswered which approach is better than the other under what circumstances but it also forgoes the exploration of their synergies in service of this goal.
both approaches have attracted considerable investment which could be better guided by a good answer to the former question failure to address the latter has stymied cross pollination between the two traditions.
we take the first steps toward solving it and building a bridge between sbfanddp.
the two approaches offer very different trade offs.
dp tools are very easy to implement once adequate history is available programming language build environment platform evolution etc.are immaterial indeed process metadata is very easily extracted from bug repositories and version control has proven to be much more valuable than metrics based on source code content .
on the other hand dpgenerally operates at a coarse granularity typically suggesting entire files for inspection.
sbftools including bugfinders are fine grained suggesting individual lines to inspect but they are language and platform specific they require a variety of compilation thus carrying the baggage of build procedures that vary with language platform and even time.
sbftools can be very difficult to deploy2.
indeed our dataset of warnings from bug finding tools is very hard won and we claim the public availability of our dataset is itself an important stepping stone for future work on comparing the two and searching for synergies.
in current state of the art sbfanddptechniques and tools the cbi problem first manifests itself in differences in how the two traditions have done their evaluation.
in terms of zonation dpworks at file granularity while sbf works at line granularity this zonation mismatch complicates the choice of a bug identification measure.
solving the 2see for vivid testimonials on deployment difficulties.cbi problem and comparing the two approaches is vital given the investments in using them and the high human and economic cost of software defects.
section .
presents the choices we made and the techniques we deployed to overcome these challenges and present a first solution to the cbi problem.
we empirically study the value of three different static bug finding tools findbugs jlint andpmd in conjunction with statistical defect prediction using a very large set of historical defects.
we formulate and present two cost effectiveness based accounting approaches to the comparability of bugginess identification problem cbi .
we find no significant differences in the cost effectiveness of the dpand the two sbftechniques findbugs andpmd we could evaluate.
we find sbfanddpdo sometimes find different defects.
we find the incorporation of metrics derived from sbf does not significantly improve the performance of dp techniques.
wefindthat sbftoolsfrequentlyperformbetterwhen ordering their warnings using priorities produced by dpthan when using their native priorities.
finally we have with a great deal of effort created a multi release repository of warnings from three tools jlint pmd andfindbugs which we shall make publicly available for future research.
these findings are actionable as explained earlier dpare independent of language and build procedures and work very well with process meta data if they could provide comparable benefits this is good news for projects that have abundant process data but have complex multilingual source bases and build procedures that see inhibit the adoption of sbftools.
it is also interesting that they do find different defects in some cases so that if budgets allow it might be worthwhile to use both.
finally under one measurement regime dpappears to be almost always a better way to order sbfwarnings compared to their native priority ordering.
.
background the contrast between sbfanddphas parallels in the chomsky vs norvig debate about statistical vs.first principles approaches to natural language processing3.
chomsky tends to favor first principles approaches to nlp whereas norvig argues that nlp should be based on the statistics of human linguistic behavior.
defects in software likewise arise from the interaction of formal semantics of pl and human behavioural imperfections.
sbfresearchers observe a pattern in defect occurrence use pl semantics to theorize how such defects could be found and then engineer abstractions and algorithms to find these defects.
dpresearchers observe patterns in human behaviors that cause defects and because formal theories of 3norvig vs chomksy and the fight for the future of ai at june .425humans do not exist pursue the ml approach of extracting features and letting learning algorithms do the work of fitting the data.
perhaps because of different intellectual traditions the two have never been properly compared and synthesized.
in this paper we first attack the problem of comparing the two on an equal footing.
prelude defect data most modern software projects follow software processes that require extensive data gathering.
version control tools such as gittrack every change made to the code.
tools such as bugzilla andjiraare widely used to record bug reports track the associated discussion and task allocation and eventually relate the fix to a specific commit in a specific set of files by a specific individual.
it is thus possible to find where which files and line numbers defect repairs occurred.
because of the careful tracking of changes by version control systems such as git it is actually possible to precisely track the provenance of each line of code it is thus possible to track for every line of code that is changed to fix a defect exactly where that line came from.
a large body of research has been animated by the availability of this data and a steady stream of results in defect prediction and the etiologies of defects has been coming out in recent years.
in this paper we use this data to evaluate static analysis tools and statistical defec t prediction and to compare the two and seek synergies.
.
statistical defect prediction statistical defect prediction dp employs historical data on reported and repaired defects to predict the location of previously unknown defects that lurk in the code.
specifically let us assume that we are developing the code for release rn.
when working on rn we are also repairing defects reported on the previous release rn .
in the process of working on rn we also invariably introduce defects someofwhichwillbediscoveredandreportedafterthe official release of rn and presumably fixed during the development of rn .
in modern development one typically gathers not just the details of all defect repairs during r1 r n but also other process and product metrics associated with files or packages or directories such as complexity metrics number of developers number of lines changed number of commits etc.
the task of statistical defect prediction is to learn a per file prediction function bugsof the form defectcount f bugs m1 f m2 f mn f wherem1 m2 are process or product metrics over the filef as of release of rnand the predicted defect count is the number of defects bugspredicts will be discovered in the filef.
in some settings bugsis a binary indicating only whether somedefect is predicted to occur in f. defect prediction is a fairly mature field of research.
previous research indicates that process metrics and organizational metrics are quite effective in predicting the loci of defects.
defect prediction models have been reported to have been used at google .
evaluating defect prediction dptools are intended to be used to focus quality control efforts.
good prediction performance is thus vital to ensure that limited money and time are spent effectively.
several different approaches to evaluating dptool performance have been reported.
ir methods such as precision recall and accuracy are easily calculated but are difficult to interpret in the highly class imbalanced settings of defect prediction a simple guesser that predicts all files to be defect free achieves high levels of accuracy.
in response the non parametric auroc area underthereceiver operatingcharacteristiccurve alsoknown as auc has become increasingly popular.
however as noted by arisholm briand measures of dpperformance should be sensitive to the costof inspecting files in particular larger files are often more costly to inspect.
they proposed aucec area under the cost effectiveness curve a lift chart measure non parametric like auc but different in that it is sensitive to the cost of inspection essentially based on the number of lines inspected.
it is by nowde rigueur for papers on dptools to report a variety of parametric non parametric and cost sensitive measures of performance.
the scholarly literature on well evaluated dptools is extensive as a simple search will reveal we have just presented a few highlights above for lack of space.
.
static bug finders staticbugfinding sbf arguablybeginswithtypechecking built into the compiler.
we focus here on supplementary tools which are typically targeted at specific kinds of coding errors such as buffer overflow race conditions sql injection vulnerabilities or cross site scripting attacks .
static analysis has typically developed oppo rtunistically as researchers discover new classes of defects and seek to invent abstraction techniques and algorithms that can detect these classes of defects efficiently.
analysis tools that can detect various classes of defects includin g memory allocation errors race conditions buffer overflows and taint related errors have been reported in the literature.
the verification imperative is to never falsely certify a program to be free of some class of bugs.
since bugfreeness is undecidable in general verification techniques over approximate program behavior to include infeasible behavior so long as no feasible behavior is lost.
bug freeness is then proven with respect to the over approximation.
a tool is sound i.e.meets the verification imperative when this proof goes through and the over approximation is proven not to lose any feasible behavior.
imprecision is the degree to which an over approximation admits infeasible behavior.
sound static analysis generates false positives when it warns aboutinfeasiblebehaviorintroducedbyover approximation.
undecidability of nontrivial program properties thus forces a hobson s choice sound tools have false positives whereas unsound tools can have false negatives.
and so without foreknowledge of which warnings are false and which are true developers must examine all the lines of code flagged in the warnings and hope to find some actual defects.
evaluating static analysis tools the evaluation criteria here have parallels with those discussed above for defect prediction.
sbftools pay off when warnings lead to true positives viz.
actual defects and waste effort when warnings are false positives they incur potential subsequent cos ts for missed defects that leak out as field defects.
formerly papers on static analysis tools reported successful experiences with finding actual defects with their tools.
when open source developers agreed that reported warnings were actually bugs and agreed to fix them that was considered a success.
often the sample of test subjects is chosen by thesbfdevelopers to illustrate the power of the tool in finding the specific coding errors targeted by the sbftool rather than improving the overall quality of the test subject.426clearly not all coding errors lead to defects that actually get exposed and reported this issue is typically not of concern in these evaluations.
in terms of cbi their zonation is line and their measure developer confirmation of a warning.
in addition these papers typically describe one specific tool and evaluate it rather than comparing the overall power of several different tools in finding defects.
our evaluation here is comparative retrospective and centers onreported defects we are asking if developers had actually carefully inspected the lines warned by sbftools how many of the defects subsequently reported and fixed in the system could potentially have been discovered?
.
we also retrospectively evaluate sbfanddpon an equal footing and explore synergies.
we focus exclusively on pattern matching static bug finding tools for three reasons commercial static analysis tools are encumbered by licenses that prohibit their study unencumbered static analysis tools are prototypes that usually suffer from bit rot and are difficult to acquire build and run in particular running these tools retrospectively on systems with long histories is especially difficult and the fact that static anal ysis tools do not scale well to large systems which would have hampered a fair comparison with prediction models which do scale and in fact are hungry for masses of data.
in particular our work focuses on jlint findbugs andpmd.
closely related work we now briefly review prior work in evaluating sbftools.
kim ernst analyze history to determine which warnings programmers tend to fix to help prioritize future warnings they focused on warnings not associated defects if any .
wagner et al evaluate findbugs pmd and qj pro with small java projects 3k58k ncsl by manually inspecting warnings to determine how many of them were true positives.
our interest is in much larger systems we also focus on field defects and false negatives with respect to field defects.
rutar et al evaluate the overlap between different java bug finding tools without considering their relationship to reported defects.
thunget al actually do retrospectively evaluate with field bugs with widely used open source systems they use a case study methodology manually examining the code to evaluate the precision and recall of static analysis tools.
nandaet al also do a retrospective evaluation with respect to actual defects but their retrospective evaluation is concerned primarily with null pointer defects we consider all kinds of reported field defects.
zheng evaluated the effectiveness of sbftools in live use in an industrial testing evaluating what kinds of errors the tools detected.
marchenko abrahamsson report that warning counts are sometimes correlated and sometimes anti correlated!
with defects.
nagappan ball report that static analysis warning density is well correlated with prerelease defect density weareinterestedtousemoretypicalprocess metricbasedpredictionmodels ratherthanstaticanalysistools per se.
ayewah et alreport the results of a survey at google on how users and projects use findbugs.
bessy offers engaging anecdotes on the experience of running coveritytm at scale.
the defect finding rate is not reported.
none of the above approaches specifically compare or look for synergies between dpandsbf.
our central experimental conceit is evaluation based on actual reported and fixed defects we take the position thatthese defects which after all are the ones that developers actually chose to fix are the most important ones.
while otherdefectsmaylurkundetectedinthesystemforyears we argue that the defects that were actually reported and fixed are the ones most likely to have influenced the perceived quality of the system.
so we assess the potential value of dpandsbfbased on their potential to guide developers towards locating these defects as early as possible.
an immediate consequence of this conceit is that our analysis ispost facto as such we have to retrospectively predicate how the dpandsbftoolsshouldhave been used to locate and remove these defects beforethey were released into the wild and base our analysis on this predication.
we chose a simple approach.
we stipulate that .
the tools either dporsbf are run very close to release date .
lines indicated with warnings by sbfand files predicted as defective by dpare carefully inspected by competent personnel and .all defects associated with those lines are discovered during inspection.
whilethesesimplificationsarenotentirelyrealistic weargue that they are justifiable for a retrospective experiment.
first step above many processes do indicate that code inspections occur fairly close to release candidate status fo r simplicity we run the analysis tools and defect prediction on the code of a released version.
second step above the cost of inspecting the entire system make it quite reasonable to target inspections at those portions of the system that are considered highest risk.
finally step as a first step toward solving the cbi measurement problem we assume that all defects in the inspected code are discovered even ones unrelated to sbfwarnings.
thisefficacy assumption is made in published literature on the evaluation of dptools since dptools indicate the likely locations of defects based on previous patterns rather than on specific etiologies.
however efficacy is typically notassumed in the evaluation of sbftools a line of code that flagged as containing a race condition may not in fact contain a race condition but contain instead an unrelated error.
by convention sbftool researchers typically give thesbftool credit only if the actual error corresponds to the reported error.
to measure the two approaches on equal footing we must adopt either dp s efficacy assumption orsbf s more stringent measure.
we lack an oracle for matching warnings to bugs and are operating at a scale where manually checking warning to bug correspondence is infeasible so we make dp s efficacy assumption.
in other words we assume that a careful code inspector would in fact detect any and all errors in the warned lines.
in conclusion we make a fairness assumption that applies equally to sbfanddp if some code is flagged for inspection regardless of the reason why we assume that a competent and conscientious developer would detect anydefect in that region of code.
.
research questions armed with our solution to cbi which we explicate in section .
we are now ready to compare sbftodp.
first we ask how well do the two approaches perform in terms of effectively identifying code for inspection.427research question how do static bug finding tools compare with statistical defect prediction with partial and full credit?
here partial and full credit refers to our solution to the cbimeasurementproblem againasdescribedinsection3.
.
to our knowledge we are the first to effect this comparison.
having established a means for comparing the two approaches we now turn our attention to the search for synergies between the two approaches.
first we ask research question can statistical defect prediction improve the performance of static bug finding tools?
then vice versa we ask research question can static bug finding tools improvetheperformanceofstatisticaldefectprediction?
.
experimental methodology we study five open source projects from the apache software foundation lucene derby wicket openjpa and qpidtm as shown in table .
they range in size from 68630k ncsl.
all are java projects.
there are varying numbers of releases for each project.
we studied the occurrence of bugs and the performance dpand and sbfapproaches at release level intervals.
specifically as described above we assess how well inspections just prior to a release rn guided by dpand orsbf would have helped developers avoid defects discovered after rnis made available to users.
.
data gathered foreveryproject wegatheredversion controlinformation fromgit all projects also use the jiraissue tracking system.
from the jiradata we identified commits that were bug fixing.
we considered any file associated with a bug fix to be buggy.
we used gitblameto identify the provenance of the lines where defects occurred with options for detecting changes and moves and whitespace insensitivity.
we used theszz approachwiththisdatatoidentifythesourceof the buggy lines.
for each release of each project we collect the warnings reported by findbugs pmd andjlint.sbf tools have different settings that could potentially produce different sets of warnings.
in practice we found that the warnings produced findbugs andjlintshowed no variation with different settings.
for pmdwe used all the java language rulesets except for rules having to do with comments coupling design we felt these were higher level than we wanted to target and api rules that were not relevant to the systems under evaluation.
while pmd being source based did not require a huge effort to run jlintand findbugs required builds of multiple versions of our large subject systems which required six person months of effort to compile all the systems run the tools over the resulting class files and gather warnings.
perhaps because of the need to map warnings from class files to source files we found that jlintwarnings had erroneous line numbers.
upon inspection of a sizable randomly chosen sample we found too many warnings pointed into figure the timeline shows a system version release date dashed vertical line with released versions of all files in a dashed box.
for every bug that is fixed post release we use git blame to identify the lines in the released version of the file we then examine whether dppredictions or sbfwarnings would have indicated at release time that those lines should be inspected.
commented regions of code specifically in the license disclo sure region in the first several lines.
because of poor data quality issues and because our analysis is at the line level wediscarded jlintfrom further consideration .
we gathered a wide range of process and product metrics and used them all for dp.
as reported earlier the precise learning algorithm is not vital for simplicity we just used logistic regression with this collection of metrics to predict defect occurrence likelihood.
since the objective here is to maximize prediction performance we were not concerned with issues such as multi collinearity and just used all available process and product metrics at the prediction problem.
for process metrics we used a large collection including number of committers number of changes number of minor committers experience of owners etc.
for product metrics we used a large collection of complexity size and object oriented metrics.
we have described and used this set of metrics in earlier work .
to classify files as defective or clean we use a logistic regression classifier with these metrics as its input.
the logistic regression classifier uses both code metrics and process metrics as gathered from the jiraissue tracking system.
to consider a file truly defective there must be some bugfixing commit that includes that file.
we train our logistic regression based prediction models on the k th release of a project and we test the model on any successive release.
test data we study five open source projects from the apachesoftwarefoundation lucene derby wicket openjpa and qpidtm.
for each release of each project we collect the warnings reported by findbugs pmd andjlint.
all three tools produce warnings on the functionality of code.jlintis the only one not to report on style.
however findbugs andpmdwarn on style.
for example findbugs has a warning category specifically for style and pmdwarns on empty code.
while pmdandjlintreport warnings at a line level granularity findbugs operates differently.
findbugs reports warnings using a combination of linelevel method level and class level granularity.
for this re ason findbugs reportssignificantlymorelines butfarfewer warnings as compared to pmdandjlint.
defect data for our test to scale we need to map defects to lines so we can then ask when sbfwarned lines or files428table summary data on projects ranges of values are shown in colum ns when applicable from smallest to largest.
extremal values may occur in different releases for different covariates.
project releases files ncslfindbugs warningspmd warningsdefectsfindbugs pmd rec.
prec.
rec.
prec.
lucene .
.4k 178k 31k .
.
.
.
qpid .
.3k 342k 80k .
.
.
.
wicket .
.7k 178k 30k .
.
.
.
derby .9k 630k 192k .
.
.
.
openjpa .
.7k 454k 171k .
.
.
.
identifiedby dpintersectthoselines.
figure1showsoursolution to this problem.
first we identify using jira all the big fixing commits that occurred subsequent to the release date note commit marked with x ed out bug .
the lines that were changed in these commits are considered defective lines.
we use git blame to identify the provenance of the defective lines.
now any blamed lines that were present in the released system shown in the figure could potentially have been identified for inspection by sbfwarnings or by andpprediction.
blamed lines that are actually identified withsbfwarnings or dppredictions are considered true positives or hits to use ir terminology .
.
solving the cbi problem in general we measure the value of both methods on an equal footing using area under the cost effectiveness curve aucec .
thismeasurementistheareaunderalift chart x axis being proportion of sloc and y axis the proportion of defects.
this is a cost sensitive and unlike precision and recall non parametric measure.
for these reasons it has become quite popular recently.
however several complications arise when applying these measures in our setting which we discuss below along with our solution to the cbi problem.
zonation the fact that dppredictions are file granular whilesbfwarns at a line level makes their direct comparison challenging.
a fair comparison dictates that we compare the performance of the two approaches on the same number of lines.
to solve this problem we introduce some terminology.
each warning a sbftool emits warns a possibly empty set of lines.
for each project we sum all of the unique warned lines a sbftool emits.
this sum is our inspection budget for that project.
to measure the hit rate of the warned lines we inspect the emitting tool s native priority order high priority warnings in smaller files first in line order within files and calculate the aucec value.
we call this value aucecl aucec for warning lines .
we then train logistic regression normally using all available data but restrict the files for which we measure its success at defect prediction to be only over a set of files whose line sum is within a small error tolerance of the inspection budget.
we then ask what is logistic regression s pay off given this inspection budget?
.
now the question becomes choosing on which subset of a project s files to spend this inspection budget based on dppredictions.
first we order all the files by defect likelihood as predicted by dp then select as many files as we can given the inspection budget.
most of the time we waste some of the inspection budget because it is insufficient to allow us to buy the next file sometimes this means we return no file.
the aucec measure over this collection of files can then be compared with the aucecl above on an equal footing.
one could argue that this approach is unfair to dp sometimes sbftools warn on as little as .
of the sloc inaproject withsuchalimited line budget acandidate dp might only be allowed to recommend a handful of files.
on the other hand one could argue that when inspecting sbf warnings developers rarelylook atjustthe warned lines and examine surrounding code with the result that sbf s budget is unfairly low.
we acknowledge these arguments but claim our procedure defines a reasonable baseline for a longoverdue comparison we hope that our dataset will enable other types of future analysis of the cbi problem.
important aucec and aucecl are the same measure calculated the same way proportional pay off in defects for proportion of lines inspected .
we use the different spellings to remind the reader of the zonation issue at play here betweenthefile levelfor dpandtheline levelfor sbf.
cbi measurement defective code sometimes is a few consecutive lines in a file sometimes defective code is widel y scattered.
what we have access to in the process metadata is the bug fixing commit and the lines changed to repair the defect.
current best practice in the mining community is the celebrated szz approach which flags the lines changed in the bug fix commit as defective lines.
we adopt this approach while acknowledging its imperfections and consider this code to be defective in our analysis.
theoverlap intersection of defective code with either a file flagged by dp or lines warned by sbfmay be partial or complete.
consider a null pointer warning by an sbftool sayfindbugs .
assume there are mlines warned for this null pointer a file fin release r. let s assume that this same filefafter the same release rhas a bug fix that changes n lines in file f between release randr .
assume further thatllines overlap between the mwarning lines and the n defective lines.
the cbi measurement problem arises here how much credit should be given to findbugs ?
an optimistic view the optimistic credit or credit f view is that if even a singlewarning line overlaps with a defectivelineassociatedwithabugfix i.e.thereisanonempty intersection there is a strong possibility that bug would have been noticed during inspection and caught before release.
this view suggests that with a single line of overlap the warning tool should be given full credit for the bug.
a less optimistic view scaled or partial credit credit p is that if x of the defective lines associated with a bug overlap a warning tool then the warning tool gets credit equivalent tox 100for that bug.
these measures are not perfect.
for example one can argue that credit fwill lower the aucec scores for dp since defective lines occur together in files dpwould get credit for only full defects whereas sbf which can give scattered warnings at different locations in files has a greater chance of hitting defective lines.
one could also criticize credit p as lowering the scores for sbf as when a single warning429table recall between various methods of finding defects.
o l refers to number of defects found in that category that overlap defects found by logistic.
projectfindbugs pmd logistic total o l total o l fb pmd lucene qpid wicket derby openjpa on one line e.g.
a failure to check a return value generates bug fix lines thus giving the warning only .
credit for that bug.
this seems unfairly low if the warning on that one line was sufficient to incite the programmer to fix that bug.
in this work we compare dpandsbfusing both the optimistic and scaled approaches to give two different perspectives on their relative effectiveness.
surely other measures could be defined with other attendant compromises.
in this work we have highlighted the importance of the cbi problem explicated the difficulties of devising an experiment to solve it and present the results of our solution.
we have spent a great deal of effort constructing the dataset we hope that follow on work will leverage our dataset to more readily tackle cbi bringing new measures to bear.
.
results we now present the results of our comparative study of twosbftools pmdandfindbugs anddpbased on a logistic model.
table summarizes our projects.
the number of releasesvaryineachproject fromalowof5forwicketto8for openjpa.
we generally discarded the last current during our analysis release because defect data is incomplete and subject to right censorship.
the systems are of moderate size ranging from 68k lines to 630k lines for derby.
the file count varies from the defect counts per release range from lucene to almost wicket .
once we gathered the warnings we removed comment lines from our partial and full credit calculations.
the remaining lines that are warned in each release are shown table .
the difference between findbugs andpmdis in some cases a couple of orders of magnitude.
the sparsity of warnings from findbugs is particularly noteworthy in qpid and wicket and is in fact troublesome for our purposes with such a limited number of warned lines our approach of selecting files from dpto meet this budget is severely constrained and thus we might reasonably expect very poor performance as we shall see.
the recall and precision numbers are shown in the last column.
these numbers are cumulative over all releases we simply count the total number of defects and see how many are indicated in the warning lines using credit f. while the recall numbers and precision numbers are very low this is not unexpected.
in particular note that the recall numbers are as a proportion of actual field defects it is worthwhile to avoid even a single field defect so finding any at all is a good thing.
the low precision numbers are more troublesome this indicates that the vast majority of the warnings as many as .
in some cases are perhaps false positives that havenobearingondefectsreportedafterrelease.
aspointedout by zhang cheung however sometimes avoinding the high cost of defects might compensate for the cost of inspecting a large number of false positives.
in table we show the defect counts actually found byfindbugs e.g.
total across all releases in lucene and the defects found by logistic regression dpusing the number of lines warned by findbugs a total of and number of lines warned by pmd total of .
this table also shows the overlap between findbugs defects and those found by logistic dp of defects and the overlap between pmd found defects and logistic dpdefects defects .
it s noteworthy that the overlap with dpis generally higher for findbugs than for pmd thus suggesting thatfindbugs and logistic dpare more complementary thanpmdand logistic dp.
certainly dpalso produce a lot of false alarms suggesting files for inspection that have no defects.
we compare the two next.
.
inspection cost comparison figure compares the performance of the findbugs tool against performance of logistic dp for all of our test projects over all releases.
each plot point represents the performance of a given approach for a given project for a given release.
the lines are not really meaningful or indicative of any real trends and are just shown for clarity and aesthetics.
theperformancehereismeasuredusingaucec for logic dp and aucecl for findbugs .
as explained in section .
the measures are fair they allow the two approaches an equal inspection budget and measure their effectiveness in capturing field defects.
we use the different names to emphasize that they arise from different tool granularities.
this plot is made using full credit credit f as discussed in section .
for defects that require multiple line changes even a single line overlapping with an inspected line or file is considered a hit .
the random is the aucecl value in expectation of choosing lines uniformly at random and assume that defects are uniformly at random associated with the lines.
in the case of derby dp outperforms findbugs while in all the other findbugs generally does better except for a couple of early releases in lucene release in qpid and release in openjpa.
all in all in out of releases across projects findbugs outperforms logistic dp.
in the case of pmd the situation is almost reversed figure logistic dpdominates uniformly in qpid derby and openjpa in half the releases in lucene and uniformly loses in wicket.
all in all logistic dpdominates in out of releases.
in the case of partial credit accounting not shown for reasons of space for pmd logistic dpalmost uniformly dominates pmd in out of the releases except for the first and last release in qpid.
turning to findbugs for partial credit accounting logistic dpagain dominates for derby and dominates findbugs for two releases each in openjpa and lucene and one release in qpid overall better in only releases out of .
for of the projects wicket qpid and openjpa only a very small portion of the system is selected for inspection by the warning tools and thus the same line count by dp so the performance of both is very low.
in some cases a few defects are hit by both tools but there is no noticeable consistent difference between the approaches.
the core observationhereisthatthereisnosignificantdiscerniblediffer lucene wicket openjpa qpid derby .
.
.
.
.
0e 005e 051e 0e 001e 042e 043e 0e 001e 042e 043e .
.
.
.
releasesaucec aucecl logistic fb randomfull credit figure comparing sbf findbugs anddp logistic regression prediction using aucec as a performa nce measure fordpand aucecl for warnings.
in general the performance is not sig nificantly different in our dataset.
the two measures are calculated in a commensurate manner using credit f see section .
.
lucene wicket openjpa qpid derby .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
releasesaucec aucecl logistic pmd randomfull credit figure isomorphic to figure but for pmd.
ence in our dataset between the performance of findbugs and logistic dp figure shows the same data as figure but for pmd again we can observe that there is no discernible consistent difference either way.
the above plots are calculated using the full credit accounting methods credit f. we have also generated plots using partial credit method credit p however we see the same lack of a clear consistent difference between the two approaches.
this is a rather unexpected finding.
as noted above sbf toolssuchas findbugs andpmdoperateataline levelgranularity and only require specific lines that are warned to be inspected.
specially given partial credit accounting one might reasonably expect that dptools would have the benefit and thus show a clear advantage over sbf.
subject to the warnings given in section this finding emphasizes the importance of comparing these two important disparate approaches to software inspecting targeting on an equal footing.
it also suggests licensing regimes imposed by certain very successful commercial vendors of sbftools that inhibit thepublicationofsuchevaluationresultsareanunjustifiable barrier to the development of more effective software inspection practices certainly vendors such as grammatech who actively support such evaluations are to be commended.
if this result holds up on replication it s rather cheering.dptools work well with meta data and do not require build integration!
withdptools we do not need to get sbftools for each programming language and integrate with build procedures.
the barriers to large scale use of sbftools are documented in bessey et al however sbf tools that analyze byte codes do not require separate build integration andcanbeeasiertouse.
infact manyorganiza tions becauseofprocessmaturityimperatives haveengaged in substantial metrics gathering and have good databases of bugreportsandversioncontrolrepositories insuchsettings dpcan be readily utilized without need for expensive software licenses or patent royalties in fact we just used the open source r system in our work.
.
enhancing dpwithsbf wenowturntoresearchquestion2 cantheperformance ofdpbe improved by using static analysis results?
see figure .
there is a simple rationale for this pursuit prior work on dppredictions has indicated that process metrics work really well and in general beat measure of properties of the product.
generally speaking product metrics which measure various properties of code such as coupling inheritance etc.
are strongly correlated with size the larger a module the more it is coupled etc.
have been found by others and us .
in a sense one can view static analysis warnings as assessing a novel kind of property of source code perhaps one more strongly allied with defects.
allsbftools are intentionally designed to locate regions of code that appear to have properties that prior experience or theory directlyindicate the presence of defects.
this is different than a property like coupling which is thought to cause defects indirectly perhaps mediated by hci phenomena such as the difficulty of code comprehension.
indeed prior work by nagappan ball suggests that there is a direct positive correlation between static analysis warning density and defect density although marchenko abrahamsson found a negative correlation in some cases .
in conclusion motivated by prior research and experience and our available data we sought to improve the performance of dp using metrics based static analysis warnings.
our metrics are quite simple we simply followed the successful experience of and added warning counts from bothpmdandfindbugs as an additional metrics into logisticdp.
whether the correlation or earnings with defects was positive or negative !
logistic regression would discover the relationship and train a model that can exploit this information.
the results can be seen in figure which uses the same general scheme as the earlier figures so it is compacted to save space.
the y axis shows the performance measure using the aucec measure.
the per lucene wicket openjpa qpid derby .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
releasesaucec process process warningfull credit figure the effect of including sbfwarnings as predictive metrics for dpfor20 ncsl inspection budget.
there appears to be no discernible trend.
this figure is isomorphic to figure hence its compact form here.
formance of baseline process metrics based dp logistic regression in this case are shown in red.
the blue line shows an enhanced dpexploiting a combination of the traditional process metrics and the warnings counts from pmdandfindbugs as an additional metric.
the specific plot here is computed using the optimistic credit accounting method credit f. however accounting under scaled credit gives similar results.
the basic conclusion that can be drawn here is that there is no clear evidence that warning counts improve the performance of logistic dp.
sometimes the enhanced dpperforms better sometimes the baseline performs better this phenomenon does not change under credit paccounting.
other approaches to synergizing dpandsbfmay provide better performance.
for example traditional product metrics thus far have only taken limited advantage of semantic properties of code such as control flow mccabe metrics or data flow certain cohesion metrics .
we believe that more complex properties such as the cardinality of points to sets or a count of the number of times widening was applied during an abstract interpretation that relate semantic properties to potential difficulties in human comprehension or maintainability might yield better results.
.
enhancing sbfwithdp our final research question concerns whether dpcan improve the performance of sbftools.
the intuition here arises from the abundant evidence that human process factors such as ownership organizational structure and geographical distribution influence quality.
thus i t is entirely possible that static bug finders could benefit from paying attention to such factors when prioritizing warnings for developers.
as an example warnings on code recently produced by an inexperienced programmer who is changing code that he is unfamiliar with are probably likely to be associated with defects.
by contrast warnings produced on mature code written by one of the initial creators of a system that has remained unchanged for several releases are unlikely to be of any great concern.
these phenomena are very well accounted for in the typical process metrics used in thedpcommunity in recent years to predict defective files with very good reported results.
there is a zonation problem here dpprioritizes filesfor inspection how is to be used for prioritizing line level warnings?
our approach to enhancing sbfwithdp wasto order the warnings by the dp predicted probability of the files within which the warned lines occur .
this gives us a particular ordering of the warned lines.
using both our optimisticand scaled accounting we can scan these lines in this order assigning credit for any defective lines that are encountered.
this gives us an aucecl score for this enhanced sbfwarning order.
now what should we compare this with?
clearly one good candidate the the aucecl score of a standard priority based ordering that is produced by findbugs that we use in the first research question above.
however there are many possible based orderings of defects in fact programmers may have different preferred orders on how they choose to inspect the warnings.
it would be interesting to get a robust estimate of how well our ordering enhanced by dp and the native ordering produced by the built in prioritization of the tool compares with the entire population of such orderings.
for this purpose we generate random re orderings of the warnings produced by pmdandfindbugs and for each one we calculate the aucecl.
this gives an empirical distribution over a large sample of orderings which then allows us to estimate how well the dp enhanced ordering compares with the overall population of orderings.
figure shows the results under partial credit accounting.
there are separate panels for each projects and each is broken up by release.
for each release we show a box plot of the aucecl values for random orderings a blue triangle for the native priority warning and a red dot for the logistic p val or predicted probability of defects value based ordering.
the lines are shown for visual clarity and have no semantics.
upper row is findbugs and lower row is pmd.
under this accounting after correcting the p values for false discovery using benjami hochberg procedure the logistic p val ordering dominates p .
in of the releases for pmd over all different projects and of the releases forfindbugs .
it also dominates the native priority ordering most cases.
in fact logistic based ordering dominates in virtually every release of virtually every project except for wicket just because wicket has very few warning lines thus constraining the line budget for dp.
thus yielding a simple lesson under partial accounting best to order the warnings using logistic dppredicted probability of defects!
under full accounting which we omit for space reasons p val based ordering doesn t fare well at all.
it significantly beats the random orderings p .
after correction only in releases in openjpa and in qpid .
as explained earlier thisisnotsurprising whenbugsspanmultiplelines all lines tend to occur together in a file and scattershot sbf warnings that happen to hit even a single one of those lines will get full credit for that bug whereas dpwill select full files and get only a single credit for a multi line bug only after the entire set of lines for that file is accounted for.
finally there is a very interesting observation the logistic based ordering frequently outperforms the native prior ity tool based ordering across project and release for bo th tools.
in the case of pmdonly for out of the releases under full credit for the partial the native ordering is better than logistic ordering for findbugs the native ordering dominates only cases under partial credit full .
a two sample wilcoxon test rejects the null hypothesis with alternative hypothesis set to logistic native after correction p .
in all cases except for partial credit under findbugs p .
.
this suggests that one could prefer logistic based ordering to the native ordering.
however one should interpret the p values prudently an abundance of caution suggests that releases are not necessarily indepen 432derby .
.
.
.
.
.
.030fb pmd 6lucene .
.
.
.
.
.
.020fb pmd 6openjpa 0e 001e 052e 053e 054e .
.
.
.
.06fb pmd 8qpid 0e 002e 064e 066e 068e .
.
.
.
.025fb pmd 6wicket 0e 001e 052e 053e 054e 055e .
.
.
.010fb pmd 4partial credit releaseaucecl pval priority figure the effect of ordering sbfwarnings using dppredictions compared to random orderings of sbf warnings.
the boxplot shows the orderings.
the red line with circles is warnings ordered by logistic dp prediction of defect probability p val and the blue trian gles are the native priority ordering.
partial credit scoring is used.
dent samples.
.
threats to v alidity if developers were in fact using sbftools during development then it is possible that warnings and perhaps associated bugs were fixed before release and thus fewer post release defects would be associated with warnings.
in lucene and wicket we found no evidence of systematic sbfuse.
in the case of derby a developer list message reported that contributors from oracle may have been using findbugs.
in the case of qpid we found that a findbugs task was added sept and in openjpa on jun .
however when we examined the history of warning counts in derby wicket and openjpa we found no significant evidence of warning repair nor did we find any significant preferential reduction in high priority warnings or ones more ominous sounding e.g.
malicious code or correctness being .
in addition we found no evidence in the email archives of any these projects suggesting a systematic adoption of sbftools.
these observations mitigate this particular threat to our findings.
our work may not be generalizable .
we have chosen distinct projects of different sizes and application domains.
we however all are java based.
our two tools pmdand findbugs are also therefore java based.
both are heuristic bug finders although findbugs does employ some static analysis.
findbugs requires compiled java code and thus entailed tremendous effort to compile our systems older versions presented special challenges such as finding older java sdk versions.
pmdwas relatively easier as it works on source code.
ideally this experiment should be repeated for more projects in more different languages with other tools.
in this case since we making the defect data publicly available itmayhelpotherresearcherstryotherjava based sbf tools.
although we use one dpmethod logistic regression using primarily process metrics prior reports suggest that this approach a would be difficult to beat and b easy to repeat in a new setting.
our measures may be mis targeted .
as explained we report performance under the full credit and partial credit assumptions.
as discussedin section .
there are arguments against these measures and others could be defined.
in addition aucec per sehas beencriticizedforignoringthecostoffalsenegatives missed defects .
we hope that our making this hard won warnings defect data available will encourage repetition of this trial with other measures.
.
conclusion and future work defect prediction and bug finders target the same problem selecting a subset of source code on which to expend limited quality control budgets.
we are the first to compare their performance.
we address the comparability of bugginess identification problem whose tackling cuts to the core of software engineering knowing when one outperforms the other optimizes resource allocation and promises to guide the search for useful synergies in service of software engineering s core aim to produce better cheaper software faster.
our comparison is based on the aucec cost benefit metric.
we find that statistical defect prediction appears to do better than pmd a widely used tool under both partial and full credit accounting in most cases.
however dpdoes not fare as well against findbugs generally worse until full credit accounting and not as badly under partial credit accounting.
second we find that using sbfwarnings as an additional metric does not significantly improve statistical prediction.
last but not least we find that ordering sbf warnings based on dpappears to improve upon the native sbfpriority levels in a majority of cases.
while this result appears significant on a two sample statistical test we urge caution since releases are not necessarily independent samples.
comparisons such as these are key to promoting engineering discipline in the selection of quality control te chniques and we invite others use our dataset for further experiments.
this material is based upon work supported by the national science fondation under grant no.
.