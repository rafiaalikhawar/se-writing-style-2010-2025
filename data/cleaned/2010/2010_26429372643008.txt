angels and monsters an empirical investigation of potential test effectiveness and efficiency improvement from strongly subsuming higher order mutation mark harman and yue jia crest university college london ukpedro reales mateo and macario polo univ.
of castilla la mancha spain abstract we study the simultaneous test e ectiveness and e ciency improvement achievable by strongly subsuming higher order mutants sshoms constructed from rst order mutants in four java programs.
using sshoms in place of the rst order mutants they subsume yielded a reduction in the number of mutants required while simultaneously improving test e ciency by and e ectiveness by between .
and .
trivial rst order faults often combine to form exceptionally non trivial higher order faults apparently innocuous angels can combine to breed monsters.
nevertheless these same monsters can be recruited to improve automated test e ectiveness and e ciency.
.
introduction mutation testing seeds faults into the program under test to create mutants .
these mutants can be classied as being either first order mutants foms or higher order mutants homs .
foms seed only a single fault while homs are a generalizaion in which at least one fault is seeded.
the space of higher order mutants is exponential but search techniques have proved able to nd many interesting homs thereby avoiding the need for an infeasible full enumeration of the hom space .
there is empirical evidence that homs may also reduce the proportion of mutants that are equivalent to the original program from which they are constructed .
it has also been shown that homs have the potential to reduce test e ort .
higher order mutation has been applied to model based testing and concurrency testing as well as code level mutation as studied in this paper .
a more detailed introduction to the higher order mutation paradigm can be found elsewhere .
strongly subsuming higher order mutants sshoms are a particularly important subclass of homs that are only killed by a subset of the intersection of test cases that kill each of the rst order mutants from which they are constructed .
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
ase september vasteras sweden.
copyright acm ... .
.
construction an sshom is at least as good and possibly superior to any of its constituent foms in the sense that a test case that kills it will be guaranteed to kill all foms but not vice versa whence the moniker strongly subsuming .
although sshoms are theoretically at least as good as the foms they subsume there has been no previous empirical investigation of this e ect.
therefore we have no data in the literature that reports the expected reduction in test e ort than can be achieved if any .
furthermore though the use of sshoms cannot by construction reduce test e ectiveness we have no data on whether any improvement in test e ectiveness can be achieved.
most test optimization techniques for example regression test selection and minimization and selective mutation involve an inherent trade o between e ectiveness and e ciency.
test improvement approaches either make testing more e ective at the expense of some hopefully acceptable reduction in its e ciency or they make testing more e cient with a concomitant reduction in effectiveness.
the use of sshoms in place of foms o ers a more attractive possibility maybe we can simultaneously improve both e ectiveness and e ciency.
adoption of the approach will therefore not require any trade o s between the competing objectives of e ciency and e ectiveness.
more precisely we distinguish two distinct reasons why an sshom study may be of interest to mutation testers depending upon whether or not the test process has a fully automated oracle available fully automated the entire mutation testing process is automated including checking whether a test output is correct the so called oracle problem .
partially automated mutant generation and execution are automated but the determination of whether the output is correct is not fully automated.
there is no fully automated test oracle and so some of the burden of the oracle problem falls on the shoulders of the test engineer or other humans involved in the test process.
in the rst of these two scenarios full automation means that the number of mutants required will have a strong impact on the overall testing time.
therefore in this fully automated scenario both reductions in the numbers of mutants required andreductions in the number of test cases required will be of interest.
in the second of the two scenarios the test e ort will be likely dominated by the human e ort required to provide an oracle so reductions the number of test cases to be examined will outweigh the value of any reductions in the numbers of mutants required.
in this paper we study the reduction in both the number of mutants and the number of test cases that result from the replacement of foms with the sshoms that subsume them.
all current techniques for nding homs including that used in this paper to nd sshoms require all foms to be executed as part of the hom construction process.
therefore any reduction in the number of mutants will only be of interest if we can nd ways of statically predicting the foms that are more likely to combine to form sshoms.
however in the second scenario where test e ort is dominated by the oracle cost any reduction in the number of tests required while maintaining or improving test e ectiveness will be valuable to the tester even using currently available techniques to construct homs .
we asses the e ciency and e ectiveness of sshom based testing but we also want to understand the properties of subsumed foms if subsumed foms are very hard to kill they are monsters then we might expect sshoms constructed from them to also be monsters from monsters shall we expect to construct monsters.
by contrast if we nd that subsumed foms are easy to kill on average they are angels then the sshom construction process transforms angels into monsters it builds hard to kill monster faults from easy to kill angelic faults.
our ndings concern the e ects of using the set of strongly subsuming higher order mutants which we denote sshoms in place of the set of rst order mutants they subsume which we denote foms .
the primary contributions are mutant minimization sshoms reduce the number of mutants to be considered by .
this rises to if sshoms is minimized using greedy test suite minimization to produce a reduced set sshomsred.
e ciency improvement sshoms reduce test e ort by for sshomsred .
e ectiveness improvement sshoms increase test effectiveness by .
to .
similarly for sshomsred .
monsters from angels sshoms are monster faults that are exceptionally hard to detect yet are constructed entirely from angel faults that are easy to detect.
unsubsumed coverage sshoms subsume of all foms.
we therefore need to nd techniques that increase sshom coverage or to use hybrid fom sshom testing .
.
searching for sshoms a strongly subsuming higher order mutant sshom is de ned as follows.
let kill s return the set of test cases which kill all mutants in the set s. the higher order mutant h constructed from a set of rst order mutants f1 f n is an sshom i kill h ikill fi and kill h figure depicts this de nition of sshom in a venn diagram.
the test set denoted tmkills the higher order mutant h while the test sets denoted tikill rst order mutants fi.
ifhis an sshom then tmmust be a non empty subset of the intersection of ti.
our four subjects are monopoly and chess two board games and validator and clifrom the apache project.
they range in size from lines of code loc to loc see table .
we use the java mutation testing tool bacterio and its set of java mutants.
t1t2tntmfigure generalized nthorder sshom scenario table quantitative information about subject systems subject tests statement first order system coverage mutation score monopoly .
.
chess .
.
validator .
.
cli .
.
all applications come with good quality test suites distributed with the applications and designed by third party testers not the experimenters .
we use these tests in our empirical study because they are considered to be complete and adequate by their developers.
quantitative information concerning the test suites are summarised in table .
in the rest of this study we focus on the mutants that are killed by the test suites.
manual inspection of samples of the remaining unkilled mutants indicated that many are equivalent.
in order to ensure high quality test data we manually added test cases to the test suites provided where we found unkilled yet non equivalent mutants.
however as with all studies of equivalent mutants we cannot be sure that we have detected all equivalent mutants even if we were to manually inspect every one which is seldom practical .
finding sshoms among the space of all higher order mutants requires a computational search process such as those associated with the widely studied area known as search based software engineering sbse .
this is because the space of higher order mutants is exponentially large innfornthorder mutants.
we use a simple genetic algorithm to search for sshoms among the homs constructed within each of the methods of the program.
it is recommended that authors of sbse work de ne clearly the representation and tness function used in the search process.
we de ne these two key sbse ingredients in the remainder of this section along with the search operators we use and our termination procedure.
representation we represent a higher order mutant as a vector of pairs of integers.
the length of the vector is the number of rst order mutants that combine to form the hom represented.
each vector element indexes the location of a rst order mutant and the type of mutation to be applied.
a single point crossover is applied to generate o spring .
it rst chooses a cut point.
two higher order mutant parents are then split at the cut point and the resulting four fragments are recombined to create two new children in the standard manner for single point crossover .
figure illustrates this process.
crossover can produce mutants that are not higher order either because of repetition of a lower order mutants or because of repeated mutation at the same location.
we avoid repetition of lower order mutants but make no attempt to control for same location mutations.
398m1m3m5m7m8 m3 m4 m6 m9m1m3m5m7m8 m3 m4 m6 m9cutting point parentsoffspringsfigure crossover operator used in our search m1m2m5m8 m8m1m2 m3genetic mutation 1m1m2m5m8 m8m1m2genetic mutation m5m1m2m5m8 m8m1m2 m4genetic mutation m5 figure genetic mutation operators used in our search such same location mutants will not have increased tness and therefore the genetic algorithm will not favour their selection.
the genetic algorithm uses three genetic mutation operators illustarted in figure .
the rst operator randomly selects a fom and replaces it with another fom.
the second operator randomly removes a selected fom and the third operator adds a new randomly selected fom to the current hom.
fitness function our tness function simply seeks to reduce the number of tests that kill all foms while increasing the number that kill the hom the tness of an sshom hff1 fng with respect to a test suite tis de ned as jtj j kill h ff1 fng j jnt i 1kill f i j. this function targets subsuming homs but there is nothing in the computation that speci cally favours strongly subsuming homs.
in so we are thus sampling randomly from the space of subsuming higher order mutants to nd a set that is strongly subsuming.
because we do not aggressively seek strong subsumption our results can be thought of as a lower bound on technical improvements that can be achieved using strongly subsuming homs.
our goal is not to introduce a new mutation testing technique.
rather we seek to identify a set of sshoms so that we can empirically study them thereby providing results against which future work on sshoms can compare.
future work may focuses on carefully designed tness functions that target strongly subsuming higher order mutants with particular behaviour and properties.
indeed such work is on going.
for example recent work has introduced a tness function that targets strongly subsuming homs that test untested behaviour that which is not tested by any of the constituent foms .
we used a reset feature as a termination condition and also to avoid the search becoming unnecessarily trapped in local optima.
reset simply restarts the search with a new randomly generated population when the variance in tness observed has fallen below of its starting variance.
the overall search terminates when no new sshoms are found after three restarts have been applied.
.
empirical study procedure this section explains our research questions motivating why we ask them how they relate to one another and the experimental and inferential statistical procedures used to answer them.
we search for a set of sshoms for each of the four programs using the generic algorithms described in the previous section section .
from the set sshoms we construct a set sshomsred using greedy minimization .
this algorithm iteratively removes mutants from sshom while there remain mutants that can be removed without a ecting the set of foms subsumed.
we start by establishing a baseline set of data on the numbers of mutants and test cases involved in our study.
such baseline data can be useful for example in replication studies that involve di erent subjects and for which the sets of mutants and test cases may therefore di er.
in particular we are interested to establish baseline data for the sizes of the sshom and sshomsred sets since these are comparatively less well studied than rst order mutants the fom set .
rq1 baseline data we decompose rq1 into two sub research questions one concerning baseline data on mutants and the other concerning baseline data on test suites.
rq1.
baseline data on mutants what are the numbers of foms sshoms sshomsred and the percentage of foms subsumed by the sshoms?
once the sshoms were found mutation adequate tests suites were constructed for each of the three sets of mutants foms sshoms and sshomsred.
the test suite construction process consisted of random selection from the pool of test cases provided with each of the four programs under study.
by using tests from a third party we remove one source of potential bias selection of test cases by the experimenters .
we also hope that the use of real world test suites may make it more likely that our results will be realistic and thereby more relevant to real world testers.
to select from this pool a test suite to kill mutants in the sets test cases were repeatedly randomly chosen until all mutants inswere killed.
this process was repeated times to cater for the stochastic nature of the selection process.
rq1.
baseline data on test suites what are the average sizes of mutation adequate test suites and the standard deviation over the thirty tests suites selected to kill all mutants in the three classes of mutant for each of the four programs?
rq1.
and rq1.
also allow us to assess the degree of test e ort reduction in terms of the numbers of mutants and test cases required that can be achieved using sshoms in place of the foms that they subsume.
having established baseline data for mutants and test cases we move to the two research questions at the heart of our technical investigation.
that is how much harder to kill are strongly subsuming higher order mutants rq2 and how much more e ective are the test suites constructed from them rq3 ?
rq2 higher order potency how much harder to kill are mutants in sshom and sshomsred than those in fom?
note that sshoms are theoretically at least as hard to kill as the foms from which they are constructed by de nition.
therefore we need not ask whether sshoms areharder to kill we know they are at least as hard to kill.
399in our study we seek to measure the signi cance or otherwise and size of this e ect.
in order visualize the di erence in killability of foms and sshoms and sshomsred we draw a graph we term the kill growth graph.
we consider two types of kill growth graph.
a proportional kill growth pkg graph and an absolute kill growth akg graph.
for a given test suite order toand set of mutants m the kill growth graph shows the growth in the absolute number akg or proportion pkg of mutants from mkilled by to.
a point in the pkg graph at x y signi es that after x of the tests in tohave been executed the proportion of mutants from mkilled isy.
a point in the akg graph at x y signi es that after x of the tests in tohave been executed the number of mutants from mkilled isy.
it is reasonable to use a pkg graph when the mutant sets are the same size.
however for di erently sized sets of mutants the akg graph should also be reported because it re ects the growth in the numbers of faults found.
we have sets of test suites one set for each of the three classes of mutant .
we combine all these sets of test suites into a single set of test suites for this visualization.
in so we seek to visualise the typical expected average kill growth trends for each of the three classes of mutants averaged over trials.
we have no information about the speci c order in which a tester might execute test cases.
therefore we report the average mutation score observed over all test suites in the arbitrary order in which they are selected by the random test selection algorithm.
to assess the statistical signi cance of the di erence between rst order mutants and higher order mutants we used a wilcoxon signed rank test with the widely adopted standard signi cance level .
.
this level is reduced to .
using the bonferroni correction to cater for the fact that we perform three wilcoxon tests .
.
.
the bonferroni correction is known to be a conservative correction.
it risks type ii errors incorrectly accepting the null hypothesis in order to reduce the risk of type i errors incorrectly rejecting the null hypothesis .
therefore should we nd that our results force us to reject the null hypothesis after this correction i.e.
there is no possibility of type ii error then we can be con dent that our results are strongly signi cant.
knowing how much harder sshoms are to kill compared to foms is interesting to mutation testing researchers but for testing practitioners the important question is how much more e ective is sshom testing?
this question is addressed by rq3 rq3 higher order test e ectiveness how much more e ective are test suites constructed from sshoms compared to those constructed from foms?
we plot pkg graphs for each of the three types of test suite those selected to kill each of the three types of mutants foms sshoms and sshomsred .
since each test suite is selected to kill a particular class of mutants s there is a natural order in which to execute the test suites unlike the union of all test cases used in answering rq2 we order the test cases in each test suite using the so called additional greedy algorithm .
this chooses the next test to be executed as that which kills the greatest number of remaining unkilled mutants in the set s.this greedy ordering ensures that we consider test cases in the order that respects the design of the test suite tests selected to kill foms for example are ordered so that they favour early achievement of fom coverage.
in total we have selected di erent test suites for each of the three classes of mutants foms sshoms and sshomsred so we now have a total of ordered test suites with which to experiment.
since we have unpaired data we use the kruskal wallis test to analyse the variance of the three sets of data obtained from the test suites selected to kill each of the three kinds of mutants.
the kruskal wallis test is an unpaired and nonparametric test that determines whether a set of samples originate from the same distribution according to the variance of their ranks.
we augment the kruskal wallis test with a post hoc pairwise comparison of the di erent classes of test selected for foms sshoms and sshomsred using dunn s test .
we use dunn s test because we have not only unpaired samples from populations of unknown distribution therefore suggesting the use of a non parametric test but also samples of uneven sizes which means that wilcoxon s unpaired non parametric test is not directly applicable .
dunn s test also includes a generalised version of the bonferroni correction for multiple tests.
therefore it the pvalues we report are adjusted to take account of the number of statistical signi cance tests we perform.
having explored whether sshoms can improve testing by replacing subsumed foms we wish to study whether there is anything more interesting about subsumed foms compared to the unsubsumed foms rq4 characteristics of subsumed foms what are the di erences between subsumed and unsubsumed foms?
we decompose rq2 into three subquestions as follows rq4.
subsumed fom killability what is the killability of subsumed foms compared to the remaining foms?
to answer rq4.
we compare the two box plots constructed from the set of killability values proportion of all test cases that kill a mutant for subsumed and unsubsumed foms.
however a mutant may fail to be killed by a test case because the test case fails to cover the mutant.
mutants that reside in branches that are seldom executed will thus naturally tend to appear harder to kill simply because they are harder to reach it will be harder to execute the mutated code .
such reduced killability due to reduced executability is a property of the program under test rather than any characteristic of the mutants that reside in it.
therefore in order to provide an assessment of mutant killability that takes account of reachability we complement the answer to rq4.
with a repeated experiment in which only test cases that execute the mutant covering test cases are counted in determining a mutant s killability rq4.
subsumed fom killability by covering tests what is the killability of subsumed foms compared to the remaining foms when restricted to covering tests?
to answer rq4.
we compare the two box plots constructed from the set of killability values for subsumed and unsubsumed foms using only test cases that cover the mutants.
finally we examine the distribution of subsumed foms over all foms rq4.
subsumed fom distribution how do the subsumed foms distribute over all mutants in the process of mutation testing?
400table number of sshoms found subject loc foms ss sshoms subsumed homs red foms monopoly chess validator cli total reduction table number of tests derived subject mean number of tests for systems foms sshoms sshomsred monopoly mean std.
dev.
.
.
.
chess mean std.
dev.
.
.
.
validator mean std.
dev.
.
.
.
cli mean std.
dev.
.
.
.
total averaged tests reduction in tests needed we plot a proportional kill growth pkg graph for all mutants and study the distribution of subsumed foms on this graph.
this allows us to see how subsumed mutants distribute over the pkg.
for example we can see whether subsumed mutants tend to be killed earlier later or evenly in the overall mutation testing process.
finally for completeness we ask how many of the unsubsumed foms would have been killed using the test suites selected to kill subsumed foms and their sshoms rq4.
unsumbsumed coverage how many unsubsumed foms do subsumed foms and their sshoms kill?
.
results this section presents the results that answer the four research questions set out in the previous section section .
rq1.
baseline data on mutants table provides baseline data.
it shows the numbers of foms generated from each of the four programs and the numbers of sshoms generated from the foms using the search process de ned in section .
the table also gives the sizes of the reduced set sshomsred produced by minimization.
in the nal column the table reports the number of foms and the percentages of all foms this denotes that are subsumed by the sshom set.
as we can see from table of the rst order mutants are subsumed by sshoms.
also these data allow us to quantify the degree of saving possible using subsuming higher order mutants.
over all programs we found that the saving achievable using sshoms over the foms they subsume is increasing to if we use the reduced set sshomsred.
rq1.
baseline data on test suites table shows the mean number and the standard deviation of the number of test cases selected for each of the four programs and for each of the three classes of mutant.
table shows that the number of test cases selected to achieve fom adequacy is larger than the number required to achieve sshom adequacy on average.this suggests that sshoms can potentially reduce the number of tests required without losing test e ectiveness.
it is also noticeable that the number of tests derived from the complete set of sshoms and the reduced set sshomsred are almost identical each di ering by at most one test case on average which is well within a single standard deviation .
this is to be expected since the reduction the minimization algorithm is designed to reduce the number of mutants without a ecting the e ectiveness of the mutant set.
we should therefore expect a similar number of test cases would be required to kill the minimized set of strongly subsuming higher order mutants as are required to kill the unreduced set.
for the test engineer s point of view it is interesting to note that using tests selected to kill strongly subsuming homs results in a reduction in the number of test cases that need to be selected.
in cases where there is no automated oracle and the test engineer must play this role this may be a welcome reduction in test e ort especially if it is accompanied by an increase in test e ectiveness a question explored in rq3 .
rq2 higher order mutant potency the akg and pkg graphs for each of the three classes of mutants are depicted in figure .
in both akg and pkg graphs the average value of all ninety executions was calculated at each point.
the akg graphs show the number of mutants killed as more test cases are executed while the pkg graph shows the proportion of all mutants in the class fom sshom or sshomsred that are killed.
the dashed line representing the mutation score obtained with foms is always above other lines which indicates foms are killed faster than sshoms independently of the test cases used.
from table we know that the average saving in numbers of mutants needed when sshoms are used in place of the foms is for sshoms rising to for the reduced set of sshoms.
however though fewer sshoms are needed they are also harder to kill.
these growth graphs provide a visualization this reduced killability.
as can be seen from figure foms are killed at a greater rate than sshoms in absolute terms.
there are also proportionately more foms killed than sshoms at every stage of the test process until the very nal stages of testing when almost all test cases have been executed.
as a result we can be reasonably sure that were testing to cease at any stage a greater number and proportion of foms would be killed than sshoms.
to examine the observation in more detail statistical analyses were carried out as explained in section .
table shows the results of a paired wilcoxon signed rank test.
in table the pvalues obtained are all lower than .
indicating strong signi cance at the .
level after accounting for multiple testing with the bonferroni correction .
each statistical test involves sets of paired data points.
there is one set of paired points for each set of test data.
each paired data element is the mutation score reached by the two mutation techniques being compared after the same number of test cases has been executed.
to interpret the cohen s de ect size measurement we use a standard range of e ect sizes from small to large .
the e ect size is medium between foms and both sshoms and the reduced set sshomsred.
35number of killed mutants testsmean foms mean sshoms mean sshomsred 200number of killed mutants testsmean foms mean sshoms mean sshomsredmonopoly chess 80number of killed mutants testsmean foms mean sshoms mean sshomsred 40number of killed mutants testsmean foms mean sshoms mean sshomsred validator cli absolute kill growth akg graphs .
.
.
.
35mutation score testsmean foms mean sshoms mean sshomsred .
.
.
.
200mutation score testsmean foms mean sshoms mean sshomsred monopoly chess .
.
.
.
80mutation score testsmean foms mean sshoms mean sshomsred .
.
.
.
40mutation score testsmean foms mean sshoms mean sshomsred validator cli proportional kill growth pkg graphs figure the potency of strongly subsuming higher order mutants absolute proportional kill growth fa pgkg graphs showing the fabsolute proportional gnumbers of mutants killed by all test cases for each of the three classes of mutants first order mutants foms strongly subsuming higher order mutants sshoms and reduced sshoms sshomsred .
the graphs reveal that the rate at which sshoms are killed is much lower than for foms in both absolute terms and also proportionally relative to the numbers of mutants in each class .
we also observe a noticeable further saving in the absolute numbers of mutants using the reduced set of sshoms for three out of the four programs studied.
402table wilcoxon signed rank test results comparing foms with sshomscomparing foms with sshomsredcomparing sshoms with sshomsred z .
.
.
sig.
.
.
.
e ect size .
.
.
the reduction in mutants achieved by mimimizing the sshoms set to sshomsred is also signi cant and the effect though not as large as the e ect size for foms is still appreciable.
in this case the e ect size is typically regarded as between small and medium .
we conclude that using strongly subsuming homs in place of the foms they subsume signi cantly reduces the number of mutants required and that this is a statistically medium sized e ect.
rq3 higher order test e ectiveness from rq2 we know how much harder it is to kill higher order mutants compared to rst order mutants.
furthermore from rq1 we know that the test suites selected to kill sshoms contain fewer tests than those selected to kill foms yet they kill all foms and sshoms .
this is interesting to a mutation tester but for a software engineer interested in testing in general the more important question will be whether the test suites selected to achieve coverage of higher order mutants are in some sense better not merely smaller than those test suites selected for the more traditional rst order mutation criteria.
as with all testing techniques the determination of which technique is better can vary depending on context.
however in mutation testing there is a way to order two test suites aandbaccording to their mutation score on some set of mutantsm.
suppose test suite tsshom is selected to kill mutants in sshom and test suite tfom is selected to kill mutants from the set fom then we can compare each test suite to see how good the suite is at killing mutants used to select the other kind of mutants.
however the test suites may have di erent size so a direct comparison of scores is not appropriate.
rather we compare the growth in mutant scores as we order the test cases as before thereby catering for di ering sizes of test suite.
figure shows the results of comparing the mutation score achieved by the tests selected to kill the mutants in foms sshoms sshomsred.
as we can see from this gure for all four subjects the results show that though test suites derived from sshoms contain fewer test cases they nevertheless achieve both higher mutation score overall and more importantly do so faster then test suites derived from foms.
we also observe in all cases that the test suites selected to kill foms fail to reach mutant score.
this is because there are homs sshoms that are not killed by anyof the test cases select to kill foms over all thirty test suites selected to kill all foms .
.
and .
of the homs remain unkilled on average for the programs monopoly chess validator and cli respectively.
this provides additional evidence that con rms the observation that test suites selected to kill foms are inferior to test suites selected to kill sshoms.
it also reveals that for all programs studied some of the sshoms are indeed very stubborn hard to kill yet not equivalent .table kruskal wallis test results test set selected mean to kill n rank foms sshoms sshomsred p .
total table dunn s method results kind of mutants from z sig.
adj.
e ect which tests are selected sig.
size foms sshoms .
.
.
.
foms sshomsred .
.
.
.
sshoms sshomsred .
.
.
.
table presents the results of the kruskal wallis test.
in this tablenis the total number of test executions tests mutants required to achieve mutation score for each of the three criteria over thirty repeated samples of test cases .
since these test suites are unpaired their sizes differ more test cases were executed according to the fom criteria.
the result sig.
.
indicates a strongly signi cant di erence exists between the three test suite executions they are very unlikely to be sampled from the same population distribution.
the kruskal wallis test was run to determine whether there were signi cant di erences between tests generated from the set of foms sshoms and sshomsred.
pairwise comparisons were performed using dunn s method as explained in section .
this post hoc analysis the results of which are shown in table reveals statistically signi cant di erences in mutation score between the tests selected to kill foms and those selected to kill sshoms p and also between the tests selected to kill foms and those selected to kill sshomsred p .
however there was no statistical di erence between the tests selected to kill sshoms and those selected to kill sshomsred p .
the e ect size between the fom tests and the sshom tests and fom tests and the sshomsred tests was assessed as small according to the cohen s dmeasurement.
we conclude that a small but signi cant reduction in the numbers of test cases required for mutation adequacy can be obtained using strongly subsuming higher order mutants in place of the rst order mutants that they cover.
furthermore though we found in rq2 that minimization had a signi cant e ect on the numbers of mutants in the sshom sets it has no signi cant e ect on the number of test cases required to achieve mutation adequacy.
overall we nd a .
average improvement in test e ectiveness using tests selected to kill sshoms in place of test selected to kill foms.
that is it requires .
fewer tests on average to kill all mutants foms and homs using tests selected to kill the sshoms compared to those selected to kill the foms.
this gure might downplay the potential improvement in test e ectiveness because it is computed as an average over all points in the test process thereby including cases such as those near the end of the process when almost all tests have been executed in any case.
if we consider instead the maximum e ectiveness improvement achieved over the test process this gives a contrasting upper bound on the e ectiveness improvement that can be expected.
.
.
.
.
35mutation score testsmean tests foms mean tests sshoms mean tests sshomsred .
.
.
.
200mutation score testsmean tests foms mean tests sshoms mean tests sshomsredmonopoly chess .
.
.
.
80mutation score testsmean tests foms mean tests sshoms mean tests sshomsred .
.
.
.
40mutation score testsmean tests foms mean tests sshoms mean tests sshomsred validator cli figure the quality of tests selected to kill strongly subsuming higher order mutants the four gures show for each program studied the growth in mutants killed over all mutants subsumed foms and the homs that strongly subsume them for each of the three kinds of test suite.
the graphs visualise the increased test quality in terms of mutant score for test suites selected to kill strongly subsuming mutants.
the maximum saving achieved using tests selected to kill sshoms was .
.
.
and .
for the programs monopoly chess validator and cli respectively.
these improvements in e ectiveness are obtained while simultaneously reducing the number of test cases required by and the number of mutants by .
we also found that mimization of sshom test suites has only a very small impact on e ectiveness.
though as reported in rq2 minimization of the set sshoms to sshomsred improves the reduction in mutants from to it also decreases the improvement in test e ectiveness that can be achieved by a small amount.
that is though we found an average improvement in effectiveness of .
with sshom this was reduced by only .
to .
when using sshomsred.
similarly we found only a .
reduction in the maximum e ectiveness improvement overall all four programs .
this is to be expected since minimization seeks to remove only redundant test cases .
rq4 characteristics of subsumed foms rqs and show that subsumed foms can be combined to make sshoms that reduce test e ort by approximately while simultaneously improving test e ectiveness by .
.
we next investigate whether there are any special properties of the subsumed foms we have been able to nd compared to the unsubsumed foms.
rq4.
subsumed fom killability figure shows the killability of subsumed and unsubsumed mutant sets for each of the four programs studied.
.
.
.
.
.
monopoly us monopoly s chess us chess s validator us validator s cli us cli s fom killabilty all tests figure killability distributions for subsumed s and unsubsumed us mutants over all test cases as can be seen the mutants of programs chess and validator are noticeably harder to kill in general than those of the other two programs.
the average killability of the subsumed foms appears to be similar or higher than that of the unsubsumed foms in all cases.
in all cases except chess this di erence is statistically signi cant according to a wilcoxon test but the e ect size is small in all cases cohen sdis .
.
and .
for monopoly validator and cli respectively .
rq4.
subsumed fom killability by covering tests figure shows the killability of subsumed and unsubsumed mutant sets for each of the four programs studied where killability is restricted to test cases the cover the mutant in question.
that is a mutant is only deemed to be unkilled when a test case reaches and therefore executes it but fails to kill it.
this factors out the e ects of programs that contain branches that are hard to reach.
.
.
.
.
monopoly us monopoly s chess us chess s validator us validator s cli us cli s fom killabilty covered tests figure killability distributions for subsumed s and unsubsumed us mutants with tests that reach cover them table unsubsumed foms killed by test selected to kill subsumed foms and their sshoms subject tests to kill age unsubsumed foms killed chess foms sshoms sshomsred cli foms sshoms sshomsred monopoly foms sshoms sshomsred validator foms sshoms sshomsred in this case comparing figures and we can see immediately that the apparent reduced killability of the mutants for programs chess and validator was largely a product of these programs containing mutants that are seldom executed.
as can be seen for two programs monopoly and chess subsumed mutants are on average a little easier to kill by tests that execute them while in the other two cases validator and cli the subsumed mutants are a little harder to kill.
the di erences are statistically signi cant according to the wilcoxon test but once again the e ect size is small according to cohen s dmeasure .
.
.
and .
for monopoly chess validator and cli respectively .
rq4.
subsumed fom distribution over all foms the proportional kill growth pkg graphs for all mutants are depicted in figure .
the pkg graphs are drawn as a faint solid line on which we superimpose an indication with short lines and whiskers of the proportion of subsumed foms that are killed.
the pkg graphs are averaged over all test suites and so the locations of the whisker plots give an indication of the expected proportion of subsumed foms that would be killed as testing progresses.
as can be seen there are di erent subsumed fom distribution patterns for each of the programs but overall subsumed foms are certainly no harder to kill than that other foms.
rq4.
unsumbsumed coverage how many unsubsumed foms do subsumed foms and their sshoms kill?
table shows the percentage of unsubsumed foms killed by tests selected to kill each of subsumed foms sshoms and the reduced set of sshoms.
as can be seen even though subsumed foms are only of all mutants the test suites that kill them can kill to of the unsubsumed foms to of all foms .
overall in answer to rq4 we nd that in all programs a large number of subsumed foms are easy to kill.
from a software engineer s point of view they are angel faults easy to detect.
.
.
.
.
80monopoly .
.
.
.
chess .
.
.
.
validator .
.
.
.
cli figure distributions of killed subsumed foms over all foms killed as testing progresses however we also found that there exist higher order combinations of these angels that are from a software engineer s point of view monsters the entire test suite cannot detect them.
nevertheless scary though they may rst appear we also know from the answers to rqs and that we can exploit these monsters to help simultaneously improve test e ort and e ectiveness.
.
threats to validity and future work our study has the typical threats to validity that impinge on research investigations reporting on a set of studied programs we cannot be sure that our results will generalise to wider classes of programs and our results are obviously speci c to java programs.
we sought to take care of construct threats that may otherwise have emerged due to multiple inferential statistical testing and uncertainties about the distributions from which we sampled.
however another important potential threat arises because of the need to use hand constructed test suites.
405we wanted the pool of test cases from which we selected test cases to be both realistic and to exhibit high coverage so that we would not observe spurious e ects that might have otherwise arisen by limiting our study to only those mutants that can be easily killed.
though there have been advances on automated test case generation tools remain unable to achieve high coverage in all cases .
therefore we used the human designed test suites that are provided with the four programs we studied .
more work is require to assess the impact of sshom testing on for example tests constructed by automated test generation tools or those that are considered to be of poor quality or low coverage by their developers.
our results are primarily of interest to mutation testing researchers and designers of mutation testing systems1.
for example if further research can identify procedures that nd more sshoms and thereby subsume a larger set of foms then the e ectiveness and e ciency improvements we report would have a signi cant impact reducing the oracle cost and increasing test e ectiveness.
future work may also consider whether static analyses can be adapted to predict the combinations of foms that would likely lead to sshoms.
if so then the reduction in the numbers of mutants required would have a signi cant e ect on the cost of mutation testing even when oracle testing can be fully automated .
.
related work mutation testing has been used to assess test quality for a wide range of systems to compare test adequacy criteria and to generate tests and test oracle .
it has also be used to guide test generation and selection using non mutation based techniques .
there are several publicly available tools for mutation testing and mutation faults have been demonstrated to be reasonable simulations of real faults .
a recent comprehensive survey of mutation testing is provided by jia and harman .
there has recent interest in mutant subsumption as a means of improving mutation e ectiveness by removing subsumed mutants from consideration for rst order mutation testing .
the idea of subsuming higher order mutation was rst proposed by jia and harman .
they introduced the concept of subsuming and strongly subsuming homs and suggested to combine higher order mutants and non trivial rst order mutants together for mutation testing.
the present paper is the st to report results strongly subsuming mutants and their relationship to the rst order mutants they subsume.
interest in the higher order mutation testing paradigm has grown in recent years with many results .
we summarise these results and their relationship to our work in the remainder of this section.
polo et al.
investigated second order mutants.
they proposed di erent algorithms to combine rst order mutants to generate second order mutants.
by applying the second order mutants test e ort was reduced by without great loss of test e ectiveness.
however polo et al.
did not use search based optimization and so they were limited to small number of lower order mutants.
1all results can be found at the paper s companion website et al.
extended this work with additional combination strategies for second order mutants.
neither of these papers used strong subsumption so they achieve greater e ciency improvements than those reported in the present paper but with a small decrease in e ectiveness rather than a small increase in e ectiveness.
jia and harman and omar et al.
studied the way in which a hom could combine foms in subtle ways.
while jia and harman generated subtle homs for the c language omar et al.
extended this to java and aspectj.
fault subtlety comes from the way in which the hom s constituent foms are mutually partially masking.
langdon et al.
found homs using a grammar based bi objective strongly typed genetic programming system.
their two objectives are killability and diversity which found realistic homs that are harder to kill than any of the foms in four c subject systems.
however they neither investigated nor reported on strong subsumption.
higher order mutation may also reduce the e ects of the equivalent mutant problem in which a mutant is syntactically di erent yet semantically identical to the original program from which it is constructed.
o utt was the rst to suggest that homs might be less likely to be equivalent than foms.
this observation was also con rmed in the subsequent work of papadakis and malevris on second order mutation .
more recently kintis et al.
also showed that impact analysis with second order mutants can be used to detect rst order equivalent mutants .
in the present paper we are concerned only with non equivalent mutants since we wish to study strongly subsuming higher order mutants and the rst order mutants that they subsume.
.
conclusions and future work this paper studied the sshoms found in four real world java systems.
we found that using sshoms in place of the foms they subsume results in a requirement for many fewer mutants up to fewer and that test e ciency can be improved by approximately while test e ectiveness is simultaneously improved by between .
and .
we also found that sshoms can be exceptionally hard to detect.
these sshoms are monster faults that remain undetected by the entire test suite selected to kill all foms yet their constituent foms are angelic faults that are easy to kill and therefore o er little resistance to testing.
nevertheless despite their potency the sshoms we found do still need to be combined with some unsubsumed foms in order to achieve full mutation adequacy.
our tness function merely samples strongly subsuming homs from a set of subsuming homs.
we do not specifically search for strong subsumption merely subsumption so our results form a lower bound on the improvement in e ectiveness and e ciency that might accrue from strong subsumption.
though we have demonstrated that monster faults can be constructed from angelic faults we have surely not found the best construction algorithm.
we hope and believe that future work will see the bene ts of homs migrating from research to practice with wider incorporation of higher order mutation in testing tools.
we also expect future research will nd better targeted tness functions that seek out various forms of strong subsumption.
.