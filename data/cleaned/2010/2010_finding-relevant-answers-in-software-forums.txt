finding relevant answers in software forumsswapna gottipati david lo and jing jiangschool of information systemssingapore management university swapnag.
davidlo jingjiang smu.edu.sgabstract online software forums provide a huge amountof valuable content.
developers and users often ask questionsand receive answers from such forums.
the availability of avast amount of thread discussions in forums provides ampleopportunities for knowledge acquisition and summarization.
fora given search query current search engines use traditionalinformation retrieval approach to extract webpages containingrelevant keywords.
however in software forums often there aremany threads containing similar keywords where each threadcould contain a lot of posts as many as or more.
manuallyfinding relevant answers from these long threads is a painstakingtask to the users.
finding relevant answers is particularly hard insoftware forums as complexities of software systems cause a hugevariety of issues often expressed in similar technical jargons andsoftware forum users are often expert internet users who oftenposts answers in multiple venues creating many duplicate posts often without satisfying answers in the world wide web.to address this problem this paper provides a semantic searchengine framework to process software threads and recover rele vant answers according to user queries.
different from standardinformation retrieval engine our framework infer semantic tagsof posts in the software forum threads and utilize these tags torecover relevant answer posts.
in our case study we analyze 068posts from three software forums.
in terms of accuracy of ourinferred tags we could achieve on average an overall precision recall and f measure of and respectively.
toempirically study the benefit of our overall framework we alsoconduct a user assisted study which shows that as comparedto a standard information retrieval approach our proposedframework could increase mean average precision from to in retrieving relevant answers to various queries andachieve a normalized discounted cumulative gain ndcg 1score of .
and ndcg score of .
.i.
introductionduring software development and maintenance activities users and developers often face issues and questions to besolved.
addressing these questions fast would make mainte nance activities cheaper to perform.
fortunately often ques tions faced by one developer or user have been faced bymany others before.
these questions along with the associatedconversations i.e.
answers contexts etc are often stored inthe many online software forums.an online forum is a web application for holding discus sions.
users post questions usually related to some specificproblems and rely on others to provide potential answers.within a forum there are many threads.
and in each thread there are many posts.
software forums contain a wealth ofknowledge related to discussions and solutions to variousproblems and needs posed by various developers and users.therefore mining such content is desirable and valuable.we investigated over software forums and found thatall of them contain question answer knowledge.
the threadscontain posts ranging anywhere from to .
it is apainstaking process for users to manually search throughmany posts in various threads.
this is true especially forlong threads that contains hundreds or even thousands ofposts.
when the user scans through the posts he she oftenlands up finding a variety of replies to various questions some of these might not be of interest to him her.
even afterperforming an exhaustive search it may turn out that eitherthere are no replies to the question or correct answer has beennot be provided.forum thread usually consists of an initiating post and anumber of reply posts.
the initiating post usually has severalquestions and the reply posts usually contain answers to thequestions and perhaps new questions or clarifying informationor some kind of feedback.
the threads at times grow longeither because the issue is hard to solve or new independentinitiating questions are posted in the same thread.
the newinitiating questions will have more replies and the processcontinues.
we refer to a sequence of posts related to a particu lar initiating question post as aquestion answer conversation.within a thread there could be multiple independent question answer conversations each starting with a different initiatingquestion post.another interesting observation is that the majority ofsoftware forum threads contain questions and answers ratherthan junks or irrelevant contents.
it is different from socialnetworking site like twitter .
in a thread the first postis likely a question and the following posts may containrelated information like clarifying posts or potential solutionsand feedbacks.
thus software discussion boards contain richinformation similar to organized qa services like yahoo!answers that are designed specifically for question answeringpurpose.
unfortunately this wealth of information is oftenhidden in many threads of posts some of which are very long.to automatically search for relevant answers typically de velopers search via a standard search engine e.g.
google or specialized search engines in software forums.
the formerapproach will return many webpages many of which areoften not relevant to answering questions.
the later approachwould return specialized pages from software forums thatoften contain answers to various question.
however even inthe second approach returned answers could be numerous.consider searching answers for the following question howto get values from arraylist?
.
as shown in the figure .
c ieee ase lawrence ks usa323searching this query in oracle forum would return threadswith some threads as large as posts.
it is the user s job tofilter the answers manually across all the threads which is verytime consuming.
fig.
.
search results from oracle forum for query how toget values from arraylist?
to leverage the wealth of information in software forums we propose a framework to find relevant answers from soft ware forums.
our framework consists of two main steps.first we propose an engine that automatically classifies andtags posts in software forums as answers clarifying answers clarifying questions feedbacks both positive and negative and junk e.g.
today i am happy .
users could then focuson reading only the questions answers including those burieddeep inside long threads rather than the entire posts.
somequestions with correct answers based on the positive ornegative feedbacks could also be identified.
second we builda semantic search engine framework that uses the inferredsemantic tags to recover the relevant answers from the threads.we collected a dataset of and posts fromoracle softwaretipsandtricks and dzone software forumsrespectively.
using this dataset we test our tag inferenceengine and show that we could infer tags with precision recall and f measure.
to evaluate our overallsearch engine framework based on the same dataset weconduct experiments where users are tasked to label returnedanswers to technical software related queries expressed innatural language returned by a standard information retrievaltoolkit and our proposed framework.
we show that wecould increase the mean average precision from to after the tags are utilized.
we show that we could achievendcg of .
and ndcg of .
on these queries with automatically generated inferred tags.the contributions of this work are as follows .we propose an engine that infers a comprehensive set oftags of posts in software forums questions answers clar ifying questions clarifying answers positive feedback negative feedback and junk.
.to the best of our knowledge we are the first to propose asemantic search engine to find relevant answers to queriesby leveraging automatically inferred tags.
.we have experimented our solution on real softwareforums analyzing a total of posts which shows ourframework s reasonable overall accuracy in inferring tags.
.we have run a user assisted study to evaluate the qualityof our proposed framework in returning relevant answersfrom software forums with encouraging result.this paper is organized as follows.
section ii describes therelated work.
section iii presents the details of the forum dataextraction process and some of the properties of the forum datathat we consider in this study.
section iv describes tag infer ence engine.
section v presents our semantic search engineframework built upon the tag inference engine.
section vielaborates our experimental settings evaluation approaches results and analysis.
section vii discusses some interestingissues and future work.
finally we conclude in section viii.ii.
relatedworkrecently there has been an active interest to mine orextract information from the mass of available software data.some work propose extraction of information from code execution traces softwarerepositories like svn or cvs etc.
in this work we are also extracting information from the mass of availablesoftware data.
different from many of the above work we areinterested in the inference of tags of posts in software forumsand the utilization of those tags to effectively retrieve relevantanswers.there are a number of work on extracting software knowl edge from the web or via natural language processing tech niques .
similar to us thummalapentaand xie also extract information from the web .
however different from their work we process textual information insoftware forums rather than code from google code.
wanget al.
use natural language and execution trace information inbug reports to detect duplicate bug reports .
similar towang et al.
we also analyze textual information.
however we focus on the retrieval of relevant answers in softwareforums rather than detecting duplicate bug reports.
zhonget al.
mine for software specifications from textual softwaredocumentations .
different from their work we infer tagsfrom software forums rather than specifications from softwaredocumentations.there have been a number of recent studies that analyzelogged communication among users and developers to aidsoftware engineering activities.
one of the early work is thework by birdet al.in that extracts a social networkfrom developers communication via email.
they find that thelevel of email activity strongly correlates with the level ofactivity in the source code.
rigby and hassan perform apsychometric text analysis on oss mailing list .
theyfind interesting patterns that relate a particular sentimentwith a particular release.
wolf et al.
investigate the use ofsocial network and developer communication information topredict for failures .
recently storey propose the use offeeds to help reverse engineer in leveraging resources fromthe community i.e.
crowdsourcing .
dit and marcusinvestigate the readability of defect reports .
breu et al.suggest ways how a bug tracking system could be improvedwith user participation .
in this study we add to the varietyof work that analyzes logged communication in particularsoftware forums to aid both developers and users of softwareproducts.ibrahim et al.
analyze forums to find discussion threadsthat developers could contribute in in effect answering thequestion should i contribute to this discussion?
.
in thisstudy we extend their work in analyzing software forums by324fig.
.long threads in software forums java top posts in 8pages multimedia bottom posts in pages proposing an approach to retrieve relevant answers to naturallanguage queries which are embedded in the mass of posts insoftware forums via a tag inference approach.in this study as the first step of our framework we proposean engine to automatically infer tags for posts in softwareforums.
our system could be used to aid other works requiringthe tagging of software forum posts.
for example hou etal.
extracted questions from news groups and tagged themmanually .
our system could potentially be used to providethe tags automatically.
treude and storey shows the usefulnessof tagging in software engineering activities to link technicaland social aspects in managing work items .
al kofahi etal.
infer tags from software artifacts in ibm jazz .
duanet al.
propose methods to extract the questions semanticallyclose to a queried question and jeon at al.
retrieve similarquestions .
different from the above studies we infertags from software forums and utilize the inferred tags toeffectively retrieve relevant answers to user queries expressedin natural language.in the data mining and information retrieval communities there have been several recent works on the extraction ofquestion and answer sentences from online forums .
we extend their study by building a semantic searchengine that leverages a tag inference engine to return relevantanswers from software forums.iii.
datasourcein this section we describe software forums how data areextracted from them and some properties of software forumdataset.a.
software forumssoftware forums usually consists of many threads.
somethreads are organized in a hierarchical fashion.
each node ina hierarchy corresponds to a particular domain of interest.questions posted are varied some ask about device driversneeded libraries needed to accomplished a particular program ming task help to solve a particular bug and many more.
anexample of a software forum with long threads is shown infigure .we crawl the webpages corresponding to the threads inthe forums to collect our dataset.
we make use of winht track to crawl.
the crawled pages contains raw contentand they need to be cleaned.
we process them by pruning thehtml tags and keep the most important part of the posts.this helps to reduce the noise generated by such html tagswhich will not be useful for tagging posts.b.
data properties and challengesin our observations the posts can be classified into varioustypes like questions clarifying questions answers clarifyinganswers feedbacks and some junks.
the feedback could beclassified as positive or negative.
i still get the same error isan example of a negative feedback.
table shows an exampleof each of the post types that are used in our study.we have observed many peculiar behaviors in the threadsdifferent from a typical one shown in table i. for example aquestion can follow another question and there is no necessitythat the question conversation is completed.
an example ofan interwoven question is shown in table ii.
this makes thetask of tagging the posts harder.
to address this we use theauthor s name along with the textual content of the posts.
weobserve that a forum often has authors that frequently askclarifying questions and give answers whereas questions areposted mostly by new authors.
another challenge here is tofind the independent questions in the same thread.we also observe that the same question could be askedmultiple times.
for these some users might have the patienceto answer or they might just reply back saying this questionhas already been answered.
please read old posts.
.
thechallenge is to tag such post as junk.furthermore a question might not be answered by anysubsequent posts or could be answered by multiple users.some answers might not work and the conversation continueswith more clarifying questions answers feedbacks and so on.the general conversation behavior is represented in figure .two typical forum conversation chains are shown in figure .questionclarifying questionclarifying answeranswerpositive negativefeedbackjunkfig.
.question conversation graphiv.
proposedtaginferenceenginein this section we present our engine for tagging softwareforum posts which could be used to help in building aneffective search engine.our engine is based on a text classification approach.as with any classification problem we first need to clearlydefine our class tags.
we defined tags including question answer clarifying question clarifying answer positive feed back negative feedback and junk.
we believe these tags wellcharacterize the different types of post in software forums.the relationships among posts having the seven class tags areillustrated in figure and some example posts belonging toeach class are shown in table i.325table iatypical conversationpost typeauthorpost contentquestionlisai have a jtable with columns.
the first column is a jcheckbox which is working fine.
the other three are jcomboboxes.they work fine when you click them and select from the list but our end users only want to navigate with the keyboard.below is the code for my table.
any help .. code snippet..clarifying questionschristiaanwhen you tab to the component does it become active .....clarifying answerlisawhen i tab to the combobox it doesn t seem to have focus on the component unless i hit the f2 key.answerchristiaanhi lisa i think your best bet is to add keylistener to the table addkeylistener keylistener l and listen to certain keys andnext call jtable.editcellat int row int column to put the cell in edit mode...clarifying questionlisabut since i have four columns the first being a checkbox which is working fine and the next three being comboboxes wouldi need to check which column that is selected ....answerchristiaanhi i think the following code should help you some code lines....positive feedbacklisathank you thank you thank you!!!!!!!!!!!!!!!!!
it works great.
you are a lifesaver.
lisajunklindai m a newbie from kansas.
really like this forum so far and am looking forward to contributing!table iiexample of interwoven questionspost typeauthorpost contentquestionbluoke i have the same problem doesn t recognize the multimedia audio controller.
before i installed a fresh windows xp onmy pc i runned sandra something like belarc and it said under multimedia audio controller intel 82801eb ich5 ac 97questionspinki have the same problem after formatting my hard drive.
i ll post the information i think is needed.
operatingsystem windowsxp professional service pack build system model medionpcanswerbwget the driver here and installanswerbwintel it s no on the list install and then restart your comp.questionclarifying questionclarifying answeranswerpositive feedbackjunkquestionanswernegative feedbackpositive feedbackjunkfig.
.sample forum conversation chainsall classification algorithms rely on a sufficient amount oflabeled taggedtraining data.
for our task it means for eachof the post type that we have defined we need to identify a setof posts that belongs to this type to be used as training data.to this end we manually annotate the posts for each softwareforum we study according to our classification scheme.once tagged training data is created the next step is to ex tract features from these examples.
for most text classificationproblems usually features simply include individual wordsfrom each piece of text to be classified.
this is often referred toas a bag of words vector space model in information retrieval.additionally we also use author information as an additionalfeature because we observe that some authors can be highlycorrelated with certain types of posts.
auto labeling theauthors according to expertise levels and use it as featuresmay further benefit the model which we keep for the futurework.after the training posts are transformed into feature vectorsbased on the features we choose to use we can then apply aclassification algorithm to train a classifier.figure shows the structure of our tag inference engine.
ascan be seen at the training stage the posts are manually taggedand processed into feature vectors.
the learning algorithmmakes use of these tagged feature vectors to train a classifier.this classifier is later used in the classification stage on unseennew posts.feature extractorfeature vectorsclassifiertraining corpusmanual annotationtrue labelstrain classifiertraining phasefeature extractorfeature vectorsclassificationpredicted outputtesting corpus testing deployment phase data processclassifierfig.
.proposed tag inference enginein the following sub sections we describe some componentsof our tag inference engine in more detail.
we first describeour feature extractor component followed by our classificationmodel.a.
feature extractormost classification algorithms require data to be representedas feature vectors.
we use the following steps to convert eachpost into a feature vector.stopword removal stopwords are non descriptive wordssuch as prepositions e.g.of in and pronouns e.g.he it andare usually removed.
we use a standard stopword list obtainedfrom that contains along list of stopwords.stemming stemming is another commonly used techniquein information retrieval.
it normalizes words with the sameroot by removing certain suffixes of the words.
for example computing computerandcomputecan all be normalized intothe stemcomput.
we used porter s stemming algorithm to process our text.author information each post is associated to one author.some authors tend to provide answers while others questions and yet others provide junk like advertisements or irrelevantinformation.
hence considering author information for taggingof posts would be beneficial.326post content for text classification usually individual wordsare used as features.
we use the word term frequency i.e.
thenumber of times the word appear in the post as the weightof each feature.in this study we are interested on different featureextractor configurations stop only the textual content of the posts with stopwords being removed are considered as features.
stopa both the textual content of the posts with stopwords being removed and author information are usedas features.
stema both the textual content of the posts withstemming being performed and author information areused as features.
ssa both the textual content of the posts with stopwords being removed and stemming being performed and author information are used as features.b.
classification modelswe consider two classification models independent postclassification and context dependent post classification.
wedescribe each of the classification model that supports our taginference engine in the following paragraphs.
independent post classification in independent postclassification we treat each post independently as shown infigure .tirepresents a thread in the software forum.p0 p1andp2represent observed posts withinti.l0 l1andl2represent the hidden labels for these posts.
using independentpost classification each post is classified based on its contentalone.
since our classification scheme has more than twoclasses we perform multi class classification for classifyingthe posts into classes.
fig.
.independent post classification2 context dependent post classification while a straight forward approach is to classify the posts separately for ourtask we observed that there is a potential dependence betweenthe class tags of consecutive posts.
for example a questionis likely to be followed by either a clarifying question or ananswer rather than a feedback message.
this kind of depen dence can be naturally modeled by a hidden markov modelas shown in figure .
in this approach we use the contenttogether with the context to classify the posts.
the contextof a post is its k immediate preceding posts.
our intuition isthat this approach could have better accuracy than independentpost classification.
we empirically investigate whether this isthe case in our experiments described in section vi.
fig.
.context dependent post classification.
context dependentpost classification model utilizes two pieces of information fortagging correlation between the contents of the observed posts andspecific tags shown in the figure by dashed circles and correlationbetween neighboring tags shown in the figure by dotted circles v. proposedsearchengineframeworkin this section we describe how we could utilize inferredtags to help the retrieval of relevant answer posts from softwareforums.
we first describe a typical search engine framework.next we describe how we could embed our tag inferenceengine to form a semantic search engine framework.a standard search engine framework as shown in figure follows the following steps for processing a natural languagequery and retrieving relevant documents pre processing the pre processor takes in a set of rawdocuments and extracts a set of relevant features fromthem.
the features are in the form of words existing inthe document.
indexing the indexing tool builds an index from acollection of documents.
this index would be usedduring the retrieval process to quickly locate relevantdocuments.
query processing and retrieval the retrieval toolprocesses user queries and retrieve relevant documentsusing the index.
top k matching documents are returnedalong with their respective relevance scores.our proposed search engine model is shown in the fig ure .
this model consists of additional two components thatretrieves the relevant answers to the query.
tagger we embed our tag inference engine into thestandard search engine framework.
as described in theprevious sections our tag inference engine or tagger would tag each document with various tags question answer etc.
filterer this additional component filters the top kmatching documents from the retrieval tool based ontheir inferred tags.
with the filterer relevant solutionposts are kept while irrelevant posts e.g.
junk arefiltered out.vi.
experimentsin this section we want to answer the following researchquestions rq1how accurate are the inferred tags?rq2what is the best feature combination to be used?rq3what is the best classification model to be used?rq4is our proposed semantic search engine frameworkleveraging inferred tags effective in retrieving rele vant answers for users ?327fig.
.semantic search engine framework.pre c i s i o n tptp fp recall tptp fnf measure pre c i s i o n recallpre c i s i o n recallfig.
.precision recall f measure.
tp fp and fn corresponds totrue positives false positives and false negatives respectively.we answer the first three research questions in section .3and the last one in section .
.
we first describe the datasetsthat we use to evaluate our algorithm and our evaluationcriteria.
we next describe our tag inference experimental studyand the results of employing various configurations of ourproposed approach in obtaining accurate tags.
finally wedescribe our search engine experimental study that shows theeffectiveness of our proposed semantic search framework inthe retrieval of relevant answer posts for question queriesexpressed in natural language.a.
experiment settingsour dataset is constructed by crawling webpages corre sponding to posts from several software forums.
we recur sively followed the chain of replies to recover the posts fromthe threads in the forums.
after the webpages have beencrawled we clean it and retrieve various information includingthe posts message content and author.
we analyze threedifferent forums tips tips forum we investigate the longer threads withat least posts each.
we focus on the multimedia groupof threads.
tips forum consists of posts that involve morehardware and software related questions whereas the other twoforums are programming related posts.
dzone forum containsadvertisement posts e.g.
pat niemeyer author of learningjava by o reilly lays out best practices with a modern fossenterprise java stack.
.
oracle forum consists of lengthy postsoften with lengthy code snippets.we perform two experiments tag inference and searchengine enhancement.
for the earlier we evaluate the qualityof our inferred tags as compared to manually labeled tags.
forthe latter we evaluate the benefit of incorporating inferred tagsto a standard document retrieval tool to improve the qualityof the returned answers.
we describe the settings for the twoexperiments below.for the tag inference experiment we manually tagged 6068posts from the software forums and fromsoftwaretipsandtricks dzone and oracle respectively .
weuse approximately half of the posts for training i.e.
posts from each of the datasets are used respectively and the rest for testing.search engine experiment is conducted on the same sets ofposts to answer a set of software queries.
the completelist of queries is shown in the table vii.
the results froma standard information retrieval toolkit1and our semanticsearch engine framework are consolidated for each queryand are annotated by a team of five human annotators.
theannotator would annotate for irrelevant answer forpartially answer and for the definite answer.
the annotatorsfor our experiments include three phd students and twoworking professionals with years of software developmentexperience.b.
evaluation criteria1 precision recall and f measure we evaluate our taginference approach based on precision recall and f measure.these measures have been commonly used to evaluate theaccuracy of various retrieval classification and mining algo rithms.
precision refers to the proportion of true positives overthe sum of the true and false positives.
recall refers to theproportion of true positives over the sum of the true positivesand false negatives.
f measure combines both precision andrecall.
this is useful as there is a tradeoff between precisionand recall.
the formulas for the evaluation criteria are shownin table .
mean average precision we evaluate the quality of asearch engine based on mean average precision map .a search engine returns an ordered list of documents for agiven query.
map gives a single numerical value to representthe quality of the ordering for relevance or or irrele vance .the average precision value for a single query is calculatedby taking the mean of the precision scores after each retrieveddocument is manually analyzed one by one for relevance.map is then the mean of the average precision scores overasetof queries.
the formula for map is shown in the figure .
normalized discount cumulative gain ndcg note thatmap can only handle cases with binary judgment relevant or irrelevant .
another evaluation measure called normalizeddiscount cumulative gain ndcg has been proposed which can handle multiple levels of relevance judgments.while evaluating a ranking list ndcg follows two rules first highly relevant documents are more valuable than marginally1 iiioverallprecision recall andf measureresults in classifiersmeasurehssahstemahstopahstopmssamstemamstopamstopprec6766626250505049recall7170676557575653f measure6968646353545350avgpi integraldisplaynj 1p j pos j number of positive instancesp j number of positive instances in topjpositionsjmapm avgpimfig.
.average precision and mean average precision.avgpiisthe average precision of queryqi jis the rank nis the number ofinstances retrieved pos j is either or where represents relevantdocument and represents irrelevant document.
p j is the precisionat the given cut off rankj.mapmis the mean average precision ofthe total number of queriesm.relevant document and second the lower ranking position adocument of any relevance level has the less valuable it isfor the user because it is less likely to be examined by theuser.
according to these rules the ndcg value of a rankinglist at position n is calculated as follow ndcgp idcgp integraldisplaypi braceleftbigg2reli i 12reli 1log2 i i 1wherereliis the rating of the i th document in the rankinglist and the normalization constant ideal idcg idcgpischosen so that the perfect list gets a ndcg score of .
inorder to calculate ndcg we use three ratings0 fromannotation.c.
tag inference experimental resultswe evaluate all four different feature set extractor config urations mentioned in section iv.
in the first configuration we perform stopword removal stop .
in the second con figuration we perform stopword removal and consider theauthors of the posts stopa .
in the third configuration we perform stemming and consider the author of the posts stema .
in the fourth configuration we perform stopwordremoval stemming and consider the author as well ssa .we evaluate the two classification models independent postclassification and context dependent post classification.
forthe concrete implementation of the model we use multi class support vector machine m for the first modeland hidden markov support vector machine h for thesecond model.
thus we have different combinations mssa mstema mstopa mstop hssa hstema hstopa and hstop.the overall result over all the datasets is shown in thetable iii.
the result broken down to each of the datasetsis shown in table iv.
the result broken down to each tagtype i.e.
question answer clarifying question etc is shownin table v. the following paragraphs describe the results inmore detail.from the overall result shown in table iii it could benoted that the classification accuracy ranges from to69 in terms of f measure .
mstop performs the worstwhile hssa performs the best.
in general considering authorinformation improves accuracy.
also stemming seems to bemore effective than stop word removal in improving theclassification accuracy.
the best combination is to considerauthor information removing the stop words and performingstemming i.e.
ssa .
in general for the same feature extractorconfiguration hidden markov support vector machine svm performs better than multi class svm.
hidden markov svmtakes into consideration the context of a particular post.from the per dataset result shown in table iv it couldbe noted that the f measure result ranges from to for hssa method.
the accuracy values are relatively stableacross datasets.
the result for dzone has lower f measureas compared to the other datasets.
the poor performance isattributed to the fact that about of the threads in thisdataset contains merely junk e.g.
advertisements rather thanquestions and answers.from the per tag result shown in table v it could be notedthat our approach works best in tagging questions and answers which are the most important tags .
using hssa we couldachieve up to accuracy in terms of f measure in taggingthe questions.
we could achieve up to accuracyin tagging the answers using hssa and hstopa .
it isinteresting to note that for tagging feedback the performanceof hidden markov svm far outperforms that of multi classsvm.
this is the case as the detection of feedbacks often relieson information from the surrounding context of a particularpost.
in general clarifying question could be detected betterthan clarifying answer.
we could also identify the junks well with an accuracy of up to in terms of f measure usinghidden markov svm .confusion matrix.table vi shows detailed information onthe performance of hssa in the form of a confusion ma trix .
the rows correspond to the actual tag class basedon manual annotation and the columns correspond to thepredicted tag class.
the values in the cells of the confusionmatrix indicates the proportion of posts of a particular tag thatare predicted classified as of a particular tag.
for example thecell at row column with value means that ofthe negative feedback posts row was wrongly classified asquestion posts column .the percentages of correct classifications for a particulartag lie in the diagonal of the confusion matrix.
the cell atrow column with value indicates that of thequestions were correctly classified.
these values represent thetrue positives or correct predictions and all other off diagonalvalues represent wrong predictions either false positives or329table ivprecision recall andf measureresults for softwaretipsandtricks d zone andoracle in classifiersdatasethssahstemahstopahstopmssamstemamstopamstopsoftwaretipsandtricksprec7372767363636158recall7170767353535650f measure7271767357575853dzoneprec6865494846464242recall6160424435373332f measure6462464640413736oracleprec7170676557575653recall6766626250505049f measure6968646353545350table vprecision recall andf measureresults for questions q answers a clarifyingquestions cq clarifyinganswers ca positivefeedback pf negativefeedback nf junks j in classifiersdatasethssahstemahstopahstopmssamstemamstopamstopqprec8987888890908887recall8283827468687064f measure8585858077777874aprec8078746868676960recall7170806063635840f measure7574766465656348cqprec646465654457184recall6355706857577155f measure645967675057298caprec28404848913154recall4036453913132233f measure323746431013187pfprec545154420013recall656956580000f measure595955480000nfprec65733210023recall5746596200370f measure115142310040jprec7776767967677076recall6563606053546051f measure7069676859606561table viconfusionmatrixpredicted tag classactual tag classqacqcapfnfjq90513101a580221011cq320644009ca231717283110pf10190354113nf314814366j613014077false negatives .
the best predicted tags are question answer and junk.
clarifying question and positive feedback couldalso be classified reasonably well.
the worst tags in termsof classification accuracy are negative feedback and clarifyinganswer.negative feedbacks are often predicted wrongly as questions or answers as forum users often add moreinformation to their negative feedback posts to clarify whatwent wrong when they tried the proposed answer.
also oftensome questions are added too in these negative feedback posts.for example hi!thanks your idea is interesting exec .. but it only works in shell but not in a java aplication evenusing exec do you have any other ideas?xxdd is a negativefeedback containing a question.
the accuracy of taggingclarifying answers is quite bad as they could be mis tagged asquestions answers clarifying questions or junk .
again this is attributed to the fact that userstend to add questions in clarifying answers.
for example fora clarifying question is ftp installed on the machine?
the reply is ftp is installed on the server and we are usingjre .
... on all client systems.
using ftp manually through acommand prompt works fine.
i m wondering if i need a newerversion of jre which is a clarifying answer together with aclarifying question.summary.as an answer to rq1 we show that we could ac hieve an f score of up to in inferring tags.
as an answerto rq2 our experiment shows that the best feature extractorconfiguration is ssa.
ssa improves other configuration optionby .
as an answer to rq3 our experiment shows thatcontext dependent post classification approach performs betterthan independent post classification by .d.
search engine experimental resultsto evaluate our overall search engine framework we con duct a user assisted experiments where users are tasked tolabel returned answers to technical software related queriesshown in the table vii that are expressed in natural languagereturned by a standard information retrieval toolkit and ourproposed framework.
we use the best configuration identifiedin our tag inference experiments the hssa configuration forour tag inference engine.we develop three search engines sys0 sys1 and sys2.
sys0is created by treating all posts as individual documents in acollection.
sys1 is created with each thread treated as a singledocument.
sys2 is created in the same way as sys1 but wealso add theinferredtags for each post.
we illustrate the setupfor the three search engines in figure .
we build sys0 sys1 and sys2 on top of lemur a language model toolkit that330table viiqueries to ev aluate search engine performancewhat is javax telephony invalidargumentexception?how to calculate pi?how to change this clock object to display hour time instead of ?how to execute internet explore or windows word in java program?what are the differences between a hash map and a hash table?what is an invalid argument?how to access cvs files?how to set a combo box with different fonts?how to generate random numbers or random booleans?how to read files in java?how to generate prime numbers?how to remove duplicates in an arraylist?how to get values from an arraylist?how to calculate the difference between two dates?how to create a two dimensional array or 2d vector?how to encrypt?how to read a pdf file or pdf document?table viiindcgat position p comparison for17queries.searchenginesndcg 1ndcg 2ndcg 3ndcg 4ndcg 5sys00.
.
.
.
.371sys10.
.
.
.
.315sys20.
.
.
.
.540processes a query q and retrieves relevant documents from aset of documents.
for sys1 and sys2 we convert the threadsretrieved by lemur back to posts.
sys1 retains all posts whilesys2 only retains posts that are auto tagged as answers.
fig.
.system setup for search engine experiments.
sys0 sys1 andsys2 are the search engines under test.
the labels for sys2 are thetags automaticallyinferredby our inference engine.we fed the queries to all the engines and retrieved thetop posts for each query using lemur.
finally we usedmap and ndcg to evaluate the results from each of the threeengines against human annotations.
the comparisons of thethree systems in terms of map is shown in the figure .
itcan be observed that sys performs better than sys1 as manyposts in a single thread might be junks or feedbacks ratherthan relevant answers.
sys2 with tagged posts outperforms theother two.
the tagging helps us to filter out the uninterestedposts corresponding to feedbacks junks and so on fromthe retrieved results.
with sys2 we can display the relevantanswers at the top of the returned list of posts which wouldbenefit the users.while the results for ndcg as shown in the table viii shows that sys2 outperforms both sys0 and sys1 for allndcg ndcg ndcg ndcg and ndcg .summary.as an answer to rq4 we show that our frameworkcould increase map from to and achieve ndcg 1of .
.
fig.
.map comparison for queries.e.
threats to validitysimilar to other empirical studies there are several threatsto validity in interpreting the results.threats to construct validity corresponds to the appropri ateness of our evaluation metrics.
we use standard metrics precision recall f measure and mean average precision.thus we believe there is little threat to construct validity.threats to internal validity correspond to the ability ofour experiments to link the independent variable i.e.
inputvariable to be varied during the experiment and the dependentvariable i.e.
target variable .
since the real tags of the postsare not available we need to tag the posts manually.
ourmanual tagging process might be prone to error.
we havetried to minimize this error by performing some checks tothe manually created tags.threats to external validity correspond to the ability togeneralize our results.
in this study we have experimentedwith sets of posts from different forums.
a total of 6068posts are investigated.
we admit that this dataset is in no waynear the number of all posts available in the various forums inthe internet.
in the future we plan to extend our case study toinclude more posts more forums and evaluate se performancewith questions on code.
we also hope there would be moreresearchers interested in analyzing software forums and couldhelp us in collecting and tagging more datasets.vii.
discussion andfutureworksome forums come with search utilities to help users inidentifying relevant posts.
it might seem that the search func tionalities provided in these forums would suffice in helpingusers navigate through the mass of posts.
unfortunately oftenthere are many posts of various types questions answers junks etc having similar keywords.
different keywords couldalso be used to convey the same meaning .
in this work westudy an orthogonal approach by inferring tags of the variousposts and utilizing these tags to find relevant answers.we provide not only question and answer tags but alsoclarifying question clarifying answer positive feedback neg ative feedback and junk tags.
these expressive and intuitivetags are useful.
the tags could be used to extract answerswhich have been given positive feedback.
answers with onlynegative feedbacks can be pruned.
questions with no answersor answers receiving positive feedback can also be detected331and sent to experts.
clarifying questions and answers couldbe used to improve the search experience e.g.
by providingsome help for users to refine his her query.
we leave thesepossible extensions for future work.posts are often highly ungrammatical and filled withspelling errors.
in the future we would look into the spellingvariations to handle the noise in human written communica tion.
the level of noise in software forum could at times behigh.
we also plan to explicitly incorporate technical terms api code questions and jargons.
we tested our model with50 training data but we would like to further explore themodel accuracy with smaller training set to reduce the manualtagging task as our future work.furthermore it is interesting to combine our approach thatworks on macro or post level to latest research in informationretrieval that works on micro or sentence level.
it wouldalso be interesting to develop an automated approach thatcould automatically arrange or cluster the forum posts in ahierarchical fashion to help users in finding the right answerto his her question.in this study we only investigate software forums.
in thefuture we want to extend this study further to investigate yetmore forums to further validate the utility of our approach.viii.
conclusionin this paper we present a new approach to find relevantanswers from software forums by leveraging an engine thatautomatically infers tags of posts in software forum threads.this tag inference engine automatically assigns differenttags to posts question answer clarifying question clarifyinganswer positive feedback negative feedback and junk.
webuild a semantic search engine by leveraging the inferredtags to find relevant answers.
our experiments shows thatour tag inference engine could achieve up to precision recall and f measure.
the best result is obtainedwhen using the ssa stop stem author feature extractorand context dependent classification model implemented byhidden markov svm.
our user assisted study shows that ascompared to a standard information retrieval approach ourproposed semantic search engine framework could increasemean average precision from to in retrieving relevantanswers to various queries.
we could achieve a normalizeddiscounted cumulative gain ndcg score of .
anda ndcg score of .
on these queries.ackowledgementswe would like to thank swetha gottipati qui minghui diao qiming karthik thirugnanam and wang shaowei fortheir help in the manual annotation process.