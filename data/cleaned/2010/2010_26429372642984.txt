using visual dataflow programming for interactive model comparison rainer lutz and stephan diehl computer science department university of trier trier germany lutzr diehl uni trier.de abstract in software engineering the comparison of graph based models is a well known problem.
although di erent comparison metrics have been proposed there are situations in which automatic or pre con gured approaches do not provide reasonable results.
especially when models contain semantic similarities or di erences additional human knowledge is often required.
however only few approaches tackle the problem of how to support humans when comparing models.
in this paper we propose a tool for interactive model comparison.
its design was informed by a set of guidelines that we identi ed in previous work.
in particular our prototype leverages visual data ow programming to allow users to implement custom comparison strategies.
to this end they can combine metrics and graph operations to compute similarities and di erences and use color coding to visualize the gained results.
additionally a qualitative user study allowed to assess whether and how our tool facilitates iterative exploration of similarities and di erences between models.
categories and subject descriptors d. .
design tools and techniques keywords graphs models comparison human centered visual data ow programming tools .
introduction the comparison of graphs or graph based models is of importance for di erent application scenarios.
in software engineering where models are often used during early phases of the development process or to provide an overview of an existing system there is also a need for graph comparison.
software designers for example would like to nd similar components of a system that might be replaced or even permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
ase september vasteras sweden.
copyright acm ... .
.
into a single one.
research in this area often focuses on the algorithmic challenge posed by sophisticated comparison metrics which are supposed to compute a reasonable set of corresponding model elements.
however there are situations when automatic or pre con gured approaches do not provide satisfying results such that human interaction is required.
for instance when semantic aspects of models are compared it becomes increasingly important to involve developers instead of entirely relying on the results of an automatic approach.
while some works present visualizations to show similarities or di erences between models only few cover the issue of how those may support developers when manually comparing software models.
especially in the eld of model driven software development several authors cf.
discuss the need for suitable visual representations of similarities or di erences and the evaluation of such visualizations.
also there seems to be a trend towards adaptable comparison metrics cf.
.
when designing interactive software it is of particular importance to project the work habits of potential users to the graphical user interface and the corresponding interactions.
in order to learn more about practices and strategies for model comparison in a recent study we observed humans comparing uml class diagrams in the context of model merging.
in summary we found the entire process of comparing and merging these models to be iterative and exploratory.
our participants were often adjusting or alternating their comparison strategies in order to nd different types of similarities which could for instance help them to verify a previous observation.
based on the results of our study we formulated design guidelines for tools that are intended to support users when comparing uml class diagrams with respect to a subsequent merging.
in this paper we present a tool that was informed by these guidelines.
thus we formulate the following contributions we provide a tool that follows a visual data ow programming approach in order to allow users to visually con gure and combine model comparison strategies.
in addition users may explore the results of their comparison strategies by using di erent visualizations.
by conducting a qualitative user study we were able to nd out if and how our tool facilitates iterative exploration of similarities and di erences between two uml class diagrams and to assess whether it conforms to the guidelines from our previous work.
in the subsequent section we summarize our previous study where humans compared uml class diagrams without tool653 table guidelines for tool design in the context of model comparison.
g1 support individual work ow in we found that there have been many individual di erences in the way humans compare uml class diagrams.
therefore a tool that allows users to interactively compare two models must not restrict users to a certain work ow.
in particular it should not only support them when comparing models it might also enable to explore individual ideas by for instance adapting available comparison metrics.
g2 allow model extensions in the rst place extending models becomes increasingly important to produce valid results during merging e.g.
resolving inheritance cycles .
however changing the layout of a model may also be considered as an extension and moreover can support the comparison process e.g.
moving similar model elements next to each other .
thus a tool should at least allow to modify the layout of the compared models or even provide a model editor that temporarily accepts invalid models.
g3 support annotations during our study we discovered that humans may resort to a variety of di erent visualizations and annotation techniques which should be preserved by a tool for model comparison.
for instance they used color coding to distinguish between several matches but also to mark di erent types of similarities.
furthermore a tool should allow to write additional comments such that users are able to capture ideas e.g.
hints on how they found a certain similarity for later use.
g4 support grouping according to grouping is an adequate means for di erent reasons.
for instance grouping may reduce the complexity of uml class diagrams and enables users to work more e ciently .
it also allows humans to nd model elements that share a certain property e.g.
have been matched by the same metric .
thus grouping does not necessarily mean that all elements a rendered in a single box.
in a broader sense elements that share the same color may also be seen as a group.
g5 raise awareness especially when humans tended to focus on one of the models they sometimes overlooked or ignored important di erences.
therefore in order to raise their awareness for possible alternatives a tool should not only allow users to access the best matches but also those of lower similarity.
g6 provide algorithmic support our participants often applied straightforward comparison strategies e.g.
name or layout matching to identify similarities between the uml class diagrams.
thus a repertory of comprehensible comparison metrics could already help to simplify the problem.
also we found that humans tend to explore and combine such simple metrics often in an iterative process from a coarse overview of all similarities to a ne grained level di erences between a particular match .
g7 help to keep track during our study the participants used various gestures and annotations to keep track of the current state of their analysis.
although they often followed a certain strategy this was not always a strict process.
thus a tool should besides allowing users to browse an interaction history also enable them to interrupt their work ow and explore alternative ways for comparing both models.
support and introduce the resulting guidelines.
in section we describe our prototype implementation in detail and show how users can create and execute comparison strategies.
section analyzes how the participants of our study perceived and applied our tool while section summarizes important related work.
section concludes this paper.
.
requirements when developing interactive software systems it is crucial to involve potential users to gather requirements for the design of such systems.
common strategies for requirements elicitation are interviews and questionnaires brainstorming sessions use cases and role playing or user observation.
to nd requirements for an interactive tool that allows to compare models and especially uml class diagrams for a subsequent merging we resorted to the last method user observation.
to this end we explored how humans accomplish such a task without any tool support by conducting a think aloud study following the approach of boren and ramey .
in particular we recruited participants and asked them to manually compare and merge two pairs of printed uml class diagrams by using only colored pens and a sheet of paper.
as the participants verbalized their thoughts we could videotape each session for later analysis.
in a subsequent step the grounded theory method as described by strauss and corbin allowed us to systematically analyze the videotaped experiments in an iterative procedure.
the result of this analysis comprises a network of six top level categories and about subcategories their descriptions and dependencies among each other.
details on this network can be retrieved from .
in general it re ects both the behavior of the participants as well as the strategies they developed when manually comparing and merging uml class diagrams e.g.
dividing a problem into sev eral subproblems or handling con icts .
although the entire information captured by this network provides a detailed source for user de ned comparison metrics visualizations or interactions which might be included into a prototype implementation these details are di cult to communicate in the rst place.
thus in order to provide important ideas and observations from our study in we formulated seven guidelines for tool design.
however as these are related to observations described in our previous work one may not entirely understand them without this information.
to overcome this problem we rephrased the guidelines in the context of model comparison such that readers are able to understand them without knowing our previous work in detail.
table presents these adapted guidelines.
although they provide important insights on the problem of manual model comparison not every single design decision can be motivated by them.
in other words when developing our tool we also included results from the more ne grained network of categories and their descriptions.
.
tigam based on the guidelines presented above we were able to develop a rst prototype which is introduced in this section in detail.
tigam a tool for interactive graph analysis and matching allows to iteratively discover similarities and di erences between two graphs by applying user de ned comparison strategies and related visualizations.
moreover it supports users when merging both graphs into a single one.
in this paper however we focus on the comparison of two graphs with user de ned strategies and visualizations which in the following are referred to as analysis programs .
besides uml class diagrams our tool also allows to import other types of graphs like mindmaps or social networks.
for the sake of simplicity we use the term model for the re654mode tabs synchronize zooming or panning choose predefined analysis programsfigure the compare mode allows to load two source models which can be compared with a prede ned analysis programs.
both models are read only such that users may always view their initial layout.
mainder of the paper.
also a model element may be either a node or an edge.
finally we denote the models that are supposed to be compared as source models and the result of an analysis as result model .
in we observed that the process of manually comparing and merging two models involves several di erent tasks like getting an overview and navigating through the models comparing them by developing adapted comparison strategies and nally merging them into a single model while solving possible con icts.
in tigam we support these top level tasks by introducing di erent modes that are accessible via the tabs at the top of the main component cf.
fig.
.
we distinguish four modes which are brie y described below edit mode in this mode a single model can be created or modi ed.
basically it resembles a full featured graph editor due to the underlying jgraphx library.
in particular it allows to add or remove nodes and edges alternate their appearances apply layout algorithms to automatically place nodes and edges of the model on the screen and also provides an undo history.
we extended the jgraphx library such that users are able to create uml class diagrams.
the nished model can be compared with another model in a subsequent step.
compare mode as shown in figure this mode allows to load two source models in a side by side view in order to compare them.
to this end both subviews may be panned and zoomed individually or even synchronized.
however it is not possible to modify the models in this mode because in we observed that it can be important to examine the content and the layoutof the initial source models which is why both of them should stay untouched.
the compare mode also allows to choose from a list of prede ned or previously saved analysis programs that may be used for a quick comparison.
a simple description of the chosen program can be obtained by opening the panel at the right.
analyze mode this mode allows to develop custom comparison strategies and their visualizations.
in order to facilitate the creation of such a strategy we chose a visual programming approach in particular this mode is based on the idea of data ow programming .
users may choose from a variety of high level programming blocks that enable them to compare the elements or an arbitrary subset of both models with each other and visualize the computed result.
figure shows an example of such an analysis program.
a detailed description of this program is given in section .
.
merge mode as depicted in figure this mode shows the results of an analysis program.
in particular when a valid program is executed copies of both source models are transfered to the merge mode and placed next to each other on a single canvas.
this allows users to rearrange nodes and edges freely such that the layouts of both models may also intersect and are not restricted to a certain area.
all visualizations included in such a program are displayed some of them only on demand and may help users to explore similarities and di erences of the compared models.
moreover this mode allows to combine both source models into a single one.
therefore it provides semi automatic features 655different types of prog ramming blocks settings of upper uml metric block settings of lower uml metric block execute analysis progra m splits a similarity map i nto two element sets one from the first and one from the second model.
port descriptions displa y which model elements are currently connected .
this particular port e.g .
provides only nodes of the first graph.
figure the analyze mode allows to develop custom comparison strategies and visualizations by combining di erent visual programming blocks.
this class has been colored three times results of the upper uml metric block are colored green results of the lower uml metric block are colored blueresults of both uml metric blocks are colored red currently selected class similarity edges show similar classes on dema nd browse similarities figure the merge mode displays the results of a custom comparison strategy allows to browse the discovered similarities and enables users to merge both models into a single one.
656to merge overlapping parts of both models and to resolve common con icts that might occur during that process.
for more complex merge operations or extensions to the result model users may resort to common graph editor features as described earlier.
in the following we focus on the last two modes as these contain the most interesting and novel features of our tool.
to this end we assume that two source models have already been opened like shown in figure .
as a next step an appropriate comparison metric is required.
this is where theanalyze mode comes into play.
.
creating analysis programs the analyze mode allows to create custom analysis programs.
therefore users may switch to this mode which initially displays an empty canvas and create a new program from scratch.
as an alternative they can also load an existing analysis program use it as a starting point or even integrate it into another one.
in any case users can access a repertory of visual programming blocks through the menu beneath the tab bar and add them to their program.
a single programming block usually consists of a short name a number of input and output ports which allow to connect blocks to each other and depending on its purpose a list of settings that may be used to adjust the outcome of a block.
several types of blocks are available which are listed in table and introduced below.
directed links denote a data ow from a source to a target block.
in general we distinguish two types of data that ow along these links model based data and con guration data.
the rst type comprises element sets and similarity maps.
element sets on the one hand ow along thick yellow links cf.
fig.
and may contain any combination of nodes and edges from a source model.
similarity maps on the other hand ow along thick green links and store discovered similarities between two element sets.
in particular for each element from one set which is stored in a hash map a separate priority queue collects matches from the other set ordered by their similarity.
the latter type con guration data is used to modify the settings of a particular block and ows along thin blue or purple links like depicted for thevalue block in figure .
cycles in the data ow are not allowed and thus rejected by our tool.
table available visual programming blocks category blocks input graph normalized value metric string metric uml metric node degree similarity coverage operator set operator split set find neighbors split map merge maps filter map string normalizer string tokenizer output colorize hide colorize map similarity edges graph layout comparison layout text output the visual programming blocks included in our tool are organized into four categories input metrics operators and output.
due to space limitations we cannot provide particular examples for all blocks.
however table gives an overview of all currently available blocks.
input blocks which are also called sources in visual programming are supposed to provide certain data from theunderlying data model.
with graph blocks for instance users may access di erent aspects of a chosen source model.
in figure the orange graph blocks and their connected nodes output ports indicate that the user decided to consider only nodes for her custom analysis program.
metric blocks one can also see that both graph blocks are connected to gray uml metric blocks which allow to compare model elements from the provided element sets.
furthermore the settings of each uml metric block may be adjusted such that di erent aspects of the source models can be observed cf.
fig.
.
tigam also provides other metric blocks e.g.
for label based comparison of other graph types like mindmaps or a simple structural analysis.
all of these metric blocks have in common that they accept two element sets and produce a similarity map that captures all discovered similarities between the given sets.
individual settings allow users to nd di erent types of similarities and may help them to make a decision on how to combine similar parts of both source models cf.
.
operator blocks a variety of di erent operator blocks allow to split combine lter or preprocess arbitrary element sets.
in figure for example the resulting similarity maps of both metric blocks have been split such that the matched nodes from each source model are separately available.
in a next step the computed node sets are pairwise intersected by using two set operator blocks in order to get only those nodes that where found by both uml metric blocks.
other blocks for instance enable to combine similarity maps inspect the neighborhood of nodes or allow to preprocess labels by removing blanks or special characters.
output blocks in order to view the results of custom comparison strategies in merge mode users have to include visualization blocks into their analysis program.
figure shows the usage of so called colorize blocks which allow to style incoming model elements by changing their colors or adjusting their opacity.
besides element sets these blocks can also handle similarity maps.
in this case all elements contained in a given similarity map are colored.
although this visualization seems to be rather simple it may be used in several situations.
for instance to color both models di erently such that they can be told apart during a subsequent analysis.
or like in the program depicted in figure to highlight several types of similarity with separate colors.
the result of this program is shown in figure .
in general visualization blocks are often useful to get an overview of which model elements have been found to be similar by a certain metric block or still remain after a certain ltering.
however such a visualization does not tell us which elements in the rst model correspond to one or more elements in the second one or vice versa.
to this end we included two additional blocks that allow to visualize matched elements.
while the colorize map block assigns equal colors to matched elements of a connected similarity map the similarity edges block enables to display additional lines between similar model elements on demand such that users may browse the discovered matches one by one.
the latter block also allows to select an arbitrary number of elements inmerge mode and examine their matches like shown in figure for the guest class on the left side.
finally we also want to explore whether certain layouts facilitate the comparison of two models.
to this end users may include graph layout blocks into their analysis program.
for instance we implemented a layout that bene657 ts the comparison of mindmaps which in general follow a tree like structure by facing their leave nodes to each other.
in we used a similar approach to compare directory structures.
furthermore it could also be interesting to investigate whether it is suitable to contrast class or even package hierarchies in uml class diagrams.
.
executing analysis programs in we observed that two uml class diagrams are often compared iteratively i.e.
new insights about the similarity of two diagrams are collected step by step.
such an iterative analysis is also supported by tigam as it allows users to modify and re execute their analysis programs at any time.
after the execution of an analysis program the merge mode displays those visualizations that have been integrated into this program.
for instance nodes or edges contained in a set that has been plugged into a colorize block are rendered in the chosen color while the remaining model elements keep their default style cf.
fig .
if two or more colors have been assigned to an element it primarily shows that color which has been applied last.
in other words the visualization block with the highest topological order provides the main color for a node.
here we simply assume that users consider this visualization block the most important one.
the remaining colors however are not discarded they are displayed in a small overlay at the bottom right of a model element.
users may click it in order to swap the main color of a node.
in addition users are able to browse and examine the discovered matches one by one via the single arrow buttons beneath the tab bar or the assigned keyboard shortcuts.
in order to highlight the currently selected match tigam draws temporary lines between the particular elements which we call similarity edges .
figure includes two of those edges.
there one can also see that their labels show the similarity that has been computed by a particular metric block.
if more than a single metric block is used in an analysis program users may directly switch between their results similarity maps by clicking the double arrow buttons.
moreover when users browse corresponding uml classes matched elds and methods are also highlighted in order to indicate which parts of the classes are similar.
in addition to browsing the discovered similarities one by one users may also view all of them at once or restrict the displayed similarities to the currently selected elements.
this way users can focus on a certain part of a model.
.
example the analysis program shown in figure was extracted from a program created by a participant of our user study.
in general it was designed to explore di erent types of similarities between the source models and was developed in an iterative manner.
first a program using only a single uml metric has been created and executed.
then an additional metric block with di erent settings was added and a second execution was triggered in order to uncover new similarities.
finally the user wanted to observe which matches have been detected by both metrics as this may suggest more reliable candidates added a few more programming blocks and executed the analysis program a third time.
in particular the nal program uses two separate graph blocks to access the nodes of each particular source model.
these node sets are then connected with two uml metricblocks such that each metric block compares the nodes of the rst model with those of the second one.
by con guring theuml metric blocks individually cf.
fig.
di erent similarities between both models can be detected.
the rst metric block only searches for nodes that share the exact same class name.
the second one allows to nd elds that have been named like classes i.e.
for two classes the metric compares the class name of the rst one to all eld names of the second and vice versa.
in addition the ignore case option produces better results as classes at least in java typically begin with a capital letter.
the similarity maps of both metric blocks are then plugged into separate colorize blocks such that the found matches can be distinguish in merge mode .
furthermore both maps are also split into two sets of nodes one from each model.
while the upper set contains those nodes that only belong to the rst model and have been matched by the particularuml metric block the lower set only includes matched nodes from the second model.
this way one can create subsets which may be analyzed in subsequent steps.
in figure this is done by using intersections in order to get only such nodes that where found by both uml metric blocks.
to this end the resulting sets are intersected by connecting them with two set operator blocks one for each model.
in order to distinguish nodes that are produced by this intersection the gained sets are plugged into a third colorize block.
figure depicts the visualized similarities after executing the last iteration of this analysis program.
there nodes with a green color share the same class name.
blue nodes on the other hand show a match between a class name and a eld.
finally nodes that have been detected by both metrics are colored red.
the red guest class from the left diagram for instance was matched with two classes of the right diagram.
in order to access this information the user selected the red class and demanded to display all similarity edges.
the upper edge shows that there exists a class with the name guest in the right diagram.
the lower one indicates that a eld named mood was matched with the mood class of the right diagram.
as the ignore case checkbox was selected for the metric block cf.
fig.
even this edge depicts a similarity of .
in addition the eld itself and the class name are highlighted with the same color such that the user is directly able to recall the type of the similarity.
although this example shows how to distinguish di erent types of similarities out tool is not limited to this visualization.
for instance it is also possible to use individual colors for each particular match cf.
sec.
.
.
.
ev aluation in order to nd out whether our prototype implementation is suitable for an iterative analysis and comparison of two source models and whether it complies with the guidelines presented in section we conducted a qualitative user study.
also we wanted to observe how people work with tigam i.e.
which features they use which work ow they follow and where they could imagine improvements and extensions to our tool.
.
experimental design for our study we chose the hallway testing method where you randomly invite four to six non expert participants in order to nd usability aws.
nielson and landauer showed that this number of subjects is su cient to discover most of 658these aws.
thus we asked two students s two academic researchers r and two software developers d to participate in our study.
detailed information about our subjects is shown in table including their general experience and the time that they have spent working with our tool.
we tested the version of tigam that is described in section on a windows computer with a inch widescreen monitor and full hd resolution.
besides audio and screen capturing we also logged all user interactions with our prototype.
table participants occupation see above experience none to expert and time spent with our tool for brie ng training and free exploration.
p1 p2 p3 p4 p5 p6 avg occupation s s r r d dexp erienceobje ct orientation .
uml modeling textual di tools .
graph or tree comp.
.
tigam .
time spent in min brie ng training free exploration after a short brie ng where we explained the general procedure of our experiment and introduced basic features of our tool the training phase began.
there the participants were asked to solve six well de ned analysis tasks in which they learned common controls of tigam and were introduced to the most important visual programming blocks.
due to time constraints and as we wanted to allow the participants to explore the possibilities of our tool we did not include all programming blocks into this training phase.
we also decided to use non uml models during the rst phase of the experiment which represented a small portion of two partly similar social networks.
this way the participants could rather focus on their analysis programs than on the complexity of the graphs.
moreover the programs created during the training phase are not directly applicable to uml class diagrams such that the participants were required to develop their own analysis programs.
when all six tasks had been solved we moved to the second phase where our participants got a short introduction to the uml speci c features of our tool.
then we provided two uml class diagrams excerpts of them are shown in figure and together with the task to implement custom comparison strategies and visualizations that would allow them to merge both diagrams in a later step.
otherwise our participants were free to explore the tool and combine visual programming blocks to obtain information on the similarity of the provided uml class diagrams.
after each experiment we asked our participants to answer a questionnaire which was designed to assess the overall usability of our tool to discover missing features and opportunities for improvement and to evaluate whether and how our tool conforms to the guidelines presented in section .
.
findings when analyzing the results of our study we had to handle three di erent types of data sources screen captures along with audio recordings logged interactions with ourprototype and the post study questionnaire.
as the rst two data sources are strong interrelated we discuss both of them in the following subsection.
the results of the questionnaire are described afterwards.
.
.
interactions and strategies by analyzing the screen captures and audio recordings as well as the logged interactions with our tool we wanted to uncover how users work with our prototype which strategies they follow and which features they use and appreciate.
to this end we watched all screen captures and transcribed important activities and statements to text les along with their individual timestamps.
these text les allowed us to quickly browse the according videos in case we wanted to inspect a certain activity in detail.
moreover we used the recorded log les to generate an overview of the interactions with tigam for each particular experiment.
these visualizations are shown in figure and allowed us to search for interesting patterns on a higher level of abstraction.
in a next step we went back to the screen captures in order to inspect these particular patterns in detail.
before generating the log le visualizations shown in figure we classi ed all possible interactions into ve categories.
next we provided these categories to our data analysis tool which divided each experiment into time spans of seconds1and counted the number of interactions for each category during a given time span.
while blue bars show the amount of modi cations to an analysis program red ones occur every time a participant executed it.
the green category comprises interactions that helped our participants to compare nodes like browsing the discovered matches or displaying similarity edges for a certain selection of nodes.
yellow bars show those situations in which our participants modi ed the result model e.g.
when they changed its layout or even applied a merging operation .
finally the navigation category includes selection as well as zooming and panning interactions.
for referencing purposes we numbered the time spans and assigned a letter to each category such that e.g.
p2.c4 refers to the second participant and there to the fourth time span of the comparison category while p2.
and p2.e refer to the entire fourth time span and the entire execution category respectively.
in general figure shows that although all participants got the same brie ng solved the same tasks during the training phase and were asked to compare the same uml class diagrams they developed individual comparison strategies and freely explored the possibilities of our tool.
for instance nding an initial metric was accomplished di erently.
while some participants customized their metric already before the rst execution of their analysis program e.g.
p1.p0 to p1.p7 others simply kept the default settings e.g.
p4.p0 top4.p1 assuming that these have been chosen well.
another example is the frequency of the program executions.
although all participants compared both uml class diagrams in a iterative process which can be seen in figure by alternating blue and red bars some of them executed their program less frequently p2.e p3.e or p4.e .
sometimes due to a more detailed inspection of the obtained results e.g.
p3.c14 to p3.c20 or even initial merging operations e.g.
p4.g5 to p4.g17 .
others like p2.p12 to p2.p23 spent their time on creating a more complex analysis program.
1we experimented with di erent time spans but found that a length of sec.
produces the most comprehensible results.
659nevertheless we could also identify similarities between the work ows of our participants.
for all of them we observed the visual information seeking mantra introduced by shneiderman when examining the result model for similarities and di erences overview rst zoom and lter then details on demand.
overview after executing an analysis program our participants rst examined the result model from a zoomed out perspective.
through di erent color codings they were able to distinguish multiple metrics.
in addition some participants quickly browsed the discovered matches e.g.
fig.
p1.c47 or displayed similarity edges once in order to get a rst idea of how both models are related to each other.
zoom and filter in a subsequent step our participants focused on a certain part of the model by zooming and panning the viewport or displaying particular similarity edges to view only a subset of the discovered matches.
these interactions were often closely related to each other which can be seen in figure where green and gray bars have a similar distribution.
three of the participants also used hide blocks or the opacity settings of colorize blocks in order to temporarily lter from uninteresting classes.
for instance classes for which an exact match was found seemed to be less interesting than those that are only similar but not equal.
details on demand finally the participants investigated interesting classes in detail for instance by exploring alternative matches comparing their elds and methods or inspecting the similarity value of a speci c similarity edge.
during this process some participants also found weak matches or mismatched classes that helped them to re ne or improve on their comparison strategy.
also one participant tended to spatially separate matched classes from their neighbors before inspecting them in detail.
.
.
questionnaire after having worked with our tool for about an hour each participant was asked to answer a questionnaire which was designed to gain insights on the usability of tigam.
our main goal was to nd out whether our prototype conforms to the guidelines shown in table .
as these provide a highlevel view on the requirements of our tool they were suitable to derive questions for our survey.
in particular we asked each participant if and why our tool corresponds to these guidelines.
to this end we formulated statements that should be rated on a point likert scale strongly disagree to strongly agree .
the statement for g1 e.g.
reads as follows i have not been restricted in my individual work ow .
in addition we allowed the participants to comment on these statements.
in case of g1 they should explain where they felt restricted in their work ow.
table provides the scores for all guidelines.
one can quickly see that all of their means are above average which suggests that our tool generally conforms to the guidelines presented in table .
however there are some guidelines namely g1 g2 and g7 that got lower scores than others.
by browsing the comments related to the guidelines as well as the screen captures of each experiment we were able to nd explanations for these scores.
in particular participants and stated with respect to g1 that a certain training with our tool is required in order to manage all of its features and that new users can be slowed down in their individual work ow.
participant added that this might also be due to his lack of experience in comparingtable participants opinion on the conformance of our prototype to the guidelines from table .
p1 p2 p3 p4 p5 p6 avg g1 support individual work ow .
g2 allow model extensions .
g3 support annotations .
g4 support grouping .
g5 raise awareness .
g6 provide algorithmic support .
g7 help to keep track .
uml class diagrams.
as our tool also allows to merge two models participant would have normally integrated such activities into his work ow.
as extensions to the uml class diagrams were not explicitly mentioned in our task description our participants did not have the intention to improve the result model.
this is why most scores for g2 are around average.
solely participant explicitly investigated at the end of his session how uml class diagrams can be modi ed and extended in merge mode cf.
fig.
p5.g36 to p5.g38 and found these features to be of decent quality.
scores for the seventh guideline are twofold while some participants reported that they lost the overview of the considered classes at times others actively tried to keep the merge mode organized.
this was done by either using additional hide blocks or lowering the opacity of a speci c colorize block in order to lter from already observed classes.
in addition to the questions about the guidelines we also asked our participants to assess whether a stepwise exploration facilitated the process of comparing the given uml class diagrams.
four out of six participants answered this question and stated that an iterative or stepwise exploration is a crucial technique when comparing uml class diagrams.
it allowed them to begin on a coarse grained level using simple comparison strategies and re ne them iteratively.
also one participant stated that a stepwise exploration helped him to keep track of those classes he had already visited.
moreover we wanted to know whether our participants could imagine further scenarios in which tigam may be applied.
the participants suggestions show that the application of tigam is not limited to uml class diagrams.
for instance due to his background in code clone detection participant found that our tool could be used to nd and especially present code clones to developers.
participant on the other hand could imagine to use an adapted version of our tool in his every day work ow to compare xml les or hierarchical directory structures.
other ideas were to compare business process models or to detect plagiarism in student exercises.
.
.
ideas for improvement during the course of our user study as well as by evaluating our questionnaire we found several interesting suggestions to improve tigam.
inanalyze mode for instance our tool could allow users to de ne and reuse their own programming blocks by grouping existing ones.
also participant suggested to introduce switches such that di erent subprograms can be turned on or o .
although most of our participants have been satis ed with the metrics provided by our tool cf.
table g6 we 660program editing p program execution e comparison c graph editing g navigation n p1p2p3p4p5p6figure visualization of logged interactions with our prototype for each participant p1 p6 .
plan to add more sophisticated ones which allow to discover elaborate structural or semantic matches.
however a challenge is to nd intuitive metrics that can be parameterized or customized to be applicable in di erent scenarios while they are still comprehensible.
inmerge mode participant reported that it sometimes became tedious to compare such nodes in detail that are spatially distributed as a lot of panning and zooming might be required.
thus we plan to extend the similarity browsing feature such that users may instruct tigam to pan and zoom to the next corresponding node automatically.
furthermore participant suggested that in merge mode users should be able to manually de ne nodes that are treated as matches or that will not be matched independently of which result a particular metric computes.
such manually de ned data could also be made available in analyze mode via a speci c input block.
.
.
validity and limitations the qualitative nature of our evaluation allowed us to explore which strategies users develop when working with our prototype.
however due to the low number of participants we cannot draw any quanti able conclusions from our experiments.
moreover the interpretation of the observed user behavior could be biased by the authors as well as the results of our previous work.
also people that work in the eld of model driven software development could have perceived our tool di erently.
nevertheless the exploratory character of our evaluation allowed us to investigate di erent aspects of computer supported model comparison and provides preliminary evidence of the usability of our tool.
.
related work in general works related to our approach are originated in two areas of research graph comparison and visual programming.
in the following we present a brief overview of these areas.
however we do not attempt to cover all existing works but rather want to show a broad eld of application.
.
graph comparison the comparison of graphs or graph based models is of importance for di erent application scenarios.
in information visualization for example several approaches have been proposed .
for instance the evolution of software is visualized by computing di erences between consecutive versions of the system or genomic sequences are aligned next to each other.
also the comparison of social networks seems to be of increasing importance.
users probably want to know which friends they share among several networks in order to store them as a single contact.
in ley describes similar problems on how to nd homonyms and synonyms for authors included in the dblp computer science