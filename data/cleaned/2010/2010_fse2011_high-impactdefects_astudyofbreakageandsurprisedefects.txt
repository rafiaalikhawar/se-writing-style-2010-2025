high impact defects a study of breakage and surprise defects emad shihab software analysis and intelligence lab sail queen s university canada emads cs.queensu.caaudris mockus avaya labs research mt airy rd basking ridge nj audris avaya.comy asutaka kamei bram adams and ahmed e. hassan software analysis and intelligence lab sail queen s university canada kamei bram ahmed cs.queensu.ca abstract the relationship between various software related phenomena e.g.
code complexity and post release software defects has been thoroughly examined.
however to date these predictions have a limited adoption in practice.
the most commonly cited reason is that the prediction identifies too much code to review without distinguishing the impact of these defects.
our aim is to address this drawback by focusing on high impact defects for customers and practitioners.
customers are highly impacted by defects that break pre existing functionality breakage defects whereas practitionersare caught off guard by defects in files that had relatively few pre release changes surprise defects .
the large commercial software system that we study already had an established concept of breakages as the highest impact defects however the concept of surprises is novel and not as well established.
we find that surprise defects are related to incomplete requirements and that the common assumption that a fix is caused by a previous change does not hold in this project.
we then fit prediction models that are effective at identifying files containing breakages and surprises.
the number of pre release defects and file size are good indicators of breakages whereas the number of co changed files and the amount of time between the latest pre release change and the release date are good indicators of surprises.
although our prediction models are effective at identifying files that have breakages and surprises we learn that the prediction should also identify the nature or type of defects with each type being specific enough to be easily identified and repaired.
categories and subject descriptors d. .
metrics complexity measures performance measures general terms software quality assurance keywords high impact process metrics defect prediction permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
esec fse september szeged hungary.
copyright acm ... .
.
.
introduction work on defect prediction aims to assist practitioners in prioritizing software quality assurance efforts .
most of this work uses code measures e.g.
process measures e.g.
and social structure measures e.g.
to predict source code areas i.e.
files where the post release defects are most likely to be found.
the number of pre release changes or defects and the size of an artifact are commonly found to be the best indicators of a file s post release defect potential.
even though some studies showed promising performance results in terms of predictive power the adoption of defect prediction models in practice remains low .
one of the main reasons is that the amount of code predicted to be defect prone far exceeds the resources of the development teams considering inspection of that code.
for example ostrand et al.
found that of the defects were in of the files.
however these of the files accounted for of the source code lines.
at the same time all defects are considered to have the same negative impact which is not realistic because for example documentation defects tend to be far less impacting than security defects.
at the same time model based prediction tends to indicate the largest and the most changed files as defect prone areas already known to practitioners to be the most problematic.
in this work we address the problem of predicting too many defects by focusing the prediction on the limited subset of highimpact defects .
since impact has a different meaning for different stakeholders we consider two of the possible definitions in this paper breakages andsurprises .
breakages are a commonly accepted concept in industry that refer to defects that break functionality delivered in earlier releases of the product on which customers heavily rely in their daily operations.
such defects are more disruptive because customers are more sensitive to defects that occur in functionality that they are used to than to defects in a new feature and breakages are more likely to hurt the quality image of a producer thus directly affecting its business.
to ensure that we focus only on the highest impact defects we only consider breakages that have severities critical the product is rendered non operational for a period of time and high the product operation has significant limitations negatively impacting the customer s business .
whereas breakages have a high impact on customers surprise defects are a novel concept representing a kind of defects that highly impact practitioners.
surprises are defects that appear in unexpected locations or locations that have a high ratio of post to pre release defects catching practitioners off guard disrupting their already tight quality assurance schedules.
for example post releasedefects in files that are heavily changed or have many defects prior to the release are expected and scheduled for.
however when a defect appears in an unexpected file the workflow of developers is disrupted causing them to vacate their current work and shift focus to addressing these surprises.
the investigated project has used the concept of breakages for several decades but surprise defects are a new concept.
hence we start by comparing the properties of breakage and surprise defects and qualifying the impact of surprises.
we then build prediction models for breakages and surprise defects rq1 identify which factors are the best predictors for each type of defect rq2 and quantify the relative effect of each factor on the breakage and surprise proneness rq3 .
finally we calculate the effort savings of using specialized defect prediction models and perform a qualitative evaluation of the usability of the prediction models in practice and propose ways to make such prediction more relevant.
we make the following contributions identify and explore breakage and surprise defects.
we find that breakage and surprise defects are approximately one fifth of the post release defects.
only of the files have both types of defects.
for example breakages tend to occur in locations that have experienced more defect fixing changes in the past and contain functionality that was implemented less recently than the functionality in locations with surprise defects.
develop effective prediction models for breakage and surprise defects.
our models can identify future breakage and surprise files with more than recall and a two to three fold increase of precision over random prediction.
identify and quantify the major factors predicting breakage and surprise defects.
traditional defect prediction factors i.e.
pre release defects and size have a strong positive effect on the likelihood of a file containing a breakage whereas the co changed files and time related factors have a negative effect on the likelihood of a file containing a surprise defect.
measure the effort savings of specialized prediction models.our custom models reduce the amount of inspected files by which represents a reduction in the number of inspected lines of code.
propose areas and methods to make defect prediction more practical.
a qualitative study suggests that an important barrier to the use of prediction in practice is lack of indications about the nature of the problem or the ways to solve it.
the method to detect surprise defects may be able to highlight areas of the code that have incorrect requirements.
we propose that an essential part of defect prediction should include prediction of the nature of the defect or ways to fix it.
the rest of the paper is organized as follows.
section highlights the related work.
section compares the properties of breakages and surprise defects.
section outlines the case study setup and section presents our case study results.
section discusses the effort savings provided by the specialized models built in our study.
section highlights the limitations of the study.
section reflects on the lessons learned about the practicality of defect prediction and details our future work.
we conclude the paper in section .
.
related work the majority of the related work comes from the area of defect prediction.
previous work typically builds multivariate logistic regression models to predict defect prone locations e.g.
files or directories .
a large number of previous studies use complexity metrics e.g.
mccabe s cyclomatic complexity metric and chidamber and kemerer ck metrics suite to predict defect prone locations .
however graves et al.
leszak et al.
and herraiz et al.
showed that complexity metrics highly correlate with the much simpler lines of code loc measure.
graves et al.
argued that change data is a better predictor of defects than code metrics in general and showed that the number of prior changes to a file is a good predictor of defects.
a number of other studies supported the finding that prior changes are a good defect predictor and additionally showed that prior defect is also a good predictor of post release defects .
to sum up this previous work showed that complexity metrics and hence size measured in loc prior change and prior defects are a good predictor of defect proneness.
the focus of the aforementioned work was to improve the prediction performance by enriching the set of metrics used in the prediction model.
in this work we incorporate the findings of previous work by using traditional defect prediction metrics factors in addition to other more specialized factors related to co change and time properties to predict the location of highly impacting defects.
however we would like to note that our focus here is capturing high impact defects rather than adding metrics to improve the performance of defect prediction models in general.
although it has been shown that defect prediction can yield benefit in practice its adoption remains low .
as ostrand et al.
showed of the files with the highest number of predicted defects contain between of the defects however these of the files make up of the code.
to make defect prediction more appealing to practice recent work examined the performance of software prediction models when the effort required to address the identified defect is considered i.e.
effortaware defect prediction .
although effort is taken into account these models still predict the entire set of post release defects giving each defect equal impact potential.
other work focused on reducing the set of predicted defects based on the semantics of the defect.
for example shin et al.
and zimmermann et al.
focused on predicting software vulnerabilities since they have high priority.
instead of narrowing down the set of defects vertically based on the semantics of the defects we narrow down the defects horizontally across domains.
for example our models can predict high impact defects across many domains whereas a model focused on vulnerability defects is only useful for one domain.
.
breakage and surprise defects in this section we provide background of the software project under study and define breakages and surprise defects.
we then characterize and compare breakage and surprise defects.
.
background software project the data used in our study comes from a wellestablished telephony system with many tens of thousands of customers that was in active development for almost years and thus has highly mature development and quality assurance procedures.
the present size of the system is approximately seven million non comment loc primarily in c and c .
the data used in our study covers five different releases of the software system.change data there are two primary sources of data used.
sablime a configuration management system is used to track modification requests mr which we use to identify and measure the size of a software release and the number of defects pre release and postrelease .
when a change to the software is needed a work item mr is created.
mrs are created for any modification to the code new features enhancements and fixes.
the project uses internally developed tools on top of the source code control system sccs to keep track of changes to the code.
every change to the code has to have an associated mr and a separate mr is created for different tasks.
we call an individual modification to a single file a delta.
each mr may have zero or more deltas associated with it.
since the development culture is very mature these norms are strictly enforced by peers.
for each mr we extracted a number of attributes from sablime and the sccs the files the mr touches the release in which the mr was discovered the date the mr was reported the software build where the code was submitted the resolution date i.e.
when the mr was fixed implemented resolution status for each release the mr was submitted to the severity and priority of the mr the mr type e.g.
enhancement or problem and the general availability date of the release that includes the mr. for each release we classify all mrs into two types pre release changes or defects if they are type problem and post release defects.
mrs that are submitted to a release before the general availability date we refer to it as ga are considered to be pre release defects.
fixes reported for a particular release after the ga date are considered to be post release defects.
.
defining breakages and surprise defects breakage defects defects are introduced into the product because the source code is modified to add new features or to fix existing defects.
when such defects break i.e.
cause a fault or change existing functionality that has been introduced in prior releases we call these defects breakages.
the concept of breakages typically is familiar in most companies.
for example in the project studied in this paper all severity one and two defects of established functionality are carefully investigated by a small team of experts.
in addition to a root cause analysis and suggestions to improve quality assurance efforts the team also determines the originating mr that introduced the breakage.
while other companies may use a different terminology than breakages most investigate such highimpact problems just as carefully therefore similar data is likely to be available in other projects with mature quality practices.
surprise defects previous research has shown that the number of pre release defects is a good predictor of post release defects e.g.
.
therefore it is a common practice for software practitioners to thoroughly test files with a large number of prerelease defects.
however in some cases files that rarely change also have post release defects.
such defects catch the software practitioners off guard disrupting their already tight schedules.
to the best of our knowledge surprise defects have not been studied yet and therefore are not recorded in issue tracking systems.
hence for the purpose of our study we use one possible definition of surprise.
we define the degree to which a file contains surprise defects as surprise file no of post release defects file no of pre release defects file whereas in files without pre release defects we define it as surprise file no of post release defects file because our definition of surprise is given in terms of the ratio of post to pre release defects we need to determine from whatthreshold the ratio should be considered significant.
for example if a file has one post release defect and pre release defects i.e.
the defined surprise value is1 should this file be flagged as being a surprise?
to calculate the surprise threshold we examine all the files that had a defect reported within one month of the ga date of the previous release.
the intuition behind this rule is that high impact defects are likely to be reported immediately after the software is released.
hence we calculate the median of the ratio of all post release defects and all pre release defects for all files changed within one month of the previous ga date then use this value as the surprise threshold for the next release.
for example the surprise threshold for release .
is determined by the median ratio of post to pre release defects of all files that had a post release defect reported against them within a month after release .
.
.
occurrence of breakage and surprise defects before analyzing the characteristics of breakage and surprise defects we examine the percentage of files that have breakages and surprise defects.
to put things in context we also show the percentage of files with one or more post release defects.
table shows that on average only of the files have breakages or surprise defects.
that is approximately one fifth of the files that have postrelease defects.
having a small percentage of files does not necessarily mean less code since some files are larger than others.
therefore we also examine the amount of loc that these files represent.
table shows that on average the amount of loc that these breakage and surprise files make up is .
and .
respectively.
this is approximately one fourth the loc of files with the post release defects.
the reduction is both promising because it narrows down the set of files to be flagged for review and challenging because predicting such unusual files is much harder.
table also compares the percentages of mrs representing postrelease breakage and surprise defects.
on average the percentage of breakage and surprise mrs is much smaller than that of postrelease mrs. since we use the surprise threshold from the previous release to determine the surprise threshold we are not able to calculate surprise defects for the first release r1.
.
breakage and surprise defects are difficult to pinpoint since they only appear in of the files.
.
breakages vs surprise defects as mentioned earlier breakage defects are high severity defects that break existing functionality.
they are a common concept in industry and their impact is understood to be quite high.
however surprise defects are a concept that we defined based on our own industrial experience.
in this section we would like to learn more about the characteristics of surprise defects and verify our assumption that surprise defects highly impact the development organization by addressing the following questions are surprise defects different than breakage defects?
if so what are the differences?
are surprise defects impactful?
such verification helps us appreciate the value of studying surprise defects and to understand the implications of our findings for future research in the field.table percentage of files loc and mrs containing post release breakage and surprise defects.
release post release breakage surprise files loc mrs files loc mrs files loc mrs r1.
.
.
.
.
.
r2.
.
.
.
.
.
.
.
.
.
r3.
.
.
.
.
.
.
.
.
.
r4.
.
.
.
.
.
.
.
r4.
.
.
.
.
.
.
.
.
.
average .
.
.
.
.
.
.
.
.
r2.
r3.
r4.
r4.
of files figure percentage of files containing both surprise and breakage defects are surprise defects different than breakage defects?
if so what are the differences?
first we look at the locations where breakage and surprise defects occur.
if we for example find that breakage and surprise defects occur in the same location then we can assume that the prediction models for breakage prone files will suffice for surprise defect prediction.
to quantify the percentage of files that contain both types of defects we divide the files into two sets files with breakages and files with surprise defects.
then we measure the intersection of the files in the two sets divided by the union of the files in the two sets breakages surprise breakages surprise figure shows the percentage of files that have both breakages and surprise defects.
at most of the breakage and surprise files overlap.
this low percentage of overlap shows that breakages and surprises are two very different types of defects.
however further examination of their specific characteristics is needed to better understand the potential overlap.
therefore we investigate the characteristics of breakage and surprise defects along the different defect prediction dimensions.
for example previous work showed that the amount of activity i.e.
number of pre release changes is a good indicator of defect proneness therefore we look at the activity of breakage and surprise defects to compare and contrast.
we summarize our findings which are statistically significant with a p value .
as follows activity.
comparing the activity measured in number of mrs of breakage and surprise files to non breakage and non surprise files shows that breakage and surprise files have less activity.
this could indicate that perhaps breakage and surprise files were inspected less.
mr size.
breakage and surprise files are modified by larger mrs. on average a breakage mr touches files as compared to files touched for a non breakage mrs. surprise mrs touch files on average as compared to files touched by non surprise mrs. time.
the start times of mrs that touch breakage and surprise files show that for a particular release breakage files are modified earlier than average whereas surprise defect files are on average worked on closer to the release date.
this suggests that considerably less time was available for the development and testing of surprise files.
functionality.
by definition breakage files are involved with legacy features e.g.
core features of the communication software whereas surprise defect files are involved with more recent features e.g.
the porting of the studied software system to work in virtual environments .
maintenance efforts.
breakage and surprise defect files are involved in more code addition mrs than the average non breakage or non surprise files.
however breakage files were involved in more fixes than surprise defect files.
are surprise defects impactful?
the above comparisons show that breakage and surprise defects are different.
to assess the impact of surprise defects we selected the five files with the highest surprise ratios in the last studied release of the software for an in depth evaluation.
to evaluate the five files with the highest surprise score we selected the latest changes prior to ga and the changes defects within one month after the ga for each of these files and investigated the nature and the origin of these defects by reading defect descriptions resolution history and notes inspection notes as well as changes to the code.
the five files were created eight six five and for two files one year s prior to ga. there were seven fixes within one month after the ga in these five files.
five defects had high severity and only two had medium severity indicating that all of them were important defects.
at least one fix in each of the five files was related to incomplete or inadequate requirements i.e.
introduced at the time of file creation or during the last enhancement of functionality.
for each post ga defect in these five files we traced back through changes to determine the first change that introduced the defect.
our intuition was that if the prediction can point to the change introducing the defect it would limit the scope of the developers inspecting the flagged area to the kind of functionality changed and to the particular lines of code thus simplifying the task of determining the nature of the potential defect or perhaps even highlighting the avenues for fixing it.the results were surprising.
in none of the cases the last pre ga change could have been the cause of the defect.
in the five defects related to inadequate requirements we had to trace all the way to the initial implementation of the file.
in the case of the two more recent files at least we found a relationship between the pre ga change that was fixing inadequate requirements and the post ga fix that was fixing another shortcoming of the same requirements that was not addressed by the prior fix.
for the two remaining defects introduced during coding or design phases we had to trace back at least three changes to find the cause.
this qualitative investigation supports our intuition about the nature of surprise defects and our findings of the differences between breakages and surprise defects.
the surprise defects appear to be an important kind of unexpected defect that appears to lurk in the code for long periods of time before surfacing.
it also suggests a possible mechanism by which the prediction of surprise defects might work.
as the usage profile or the intensity of usage changes over the years early warnings are given by customers wanting to adjust a feature one of the defects or hard to reproduce defects are starting to surface.
two of the defects appear to be caused by the increased use of multicore architecture that resulted in hard toreproduce timer errors and interactions with the system clock.
at some point a major fault surprise is discovered system restarting because of full message buffer or because of data corruption and the shortcomings of the requirements are finally addressed by the fix to the surprise defect.
breakage and surprise defects are unique and different.
surprise defects also have high severity and appear to indicate problems in the requirements.
.
case study setup now that we have analyzed the characteristics of breakage and surprise defects we want to examine the effectiveness of predicting the locations of breakage and surprise defects.
we address the following research questions rq1.
can we effectively predict which files will have breakage surprise defects?
rq2.
which factors are important for the breakage surprise defect prediction models?
rq3.
what effect does each factor have on the likelihood of finding a breakage surprise defect in a file?
in this section we outline and discuss the case study setup.
first we present the various factors used to predict breakage and surprise defects.
then we outline the prediction modeling technique used.
finally we outline how we evaluate the performance of our prediction models.
.
factors used to predict breakage and surprise defects the factors that we use to predict files with breakage or surprise defects belong to three different categories traditional factors found in previous defect prediction work factors associated to co changed files and time related factors i.e.
the age of a file and the time since the last change to the file .
these factors are based on previous post release defect prediction work and on our findings in section .
.
traditional factors previous work shows that the number of previous changes the number of pre release defects and the size of a file are good indicators of post release defects .
sincebreakage and surprise defects are a special case of post release defects we believe that they can help us predict breakage and surprise defects as well.
co change factors by definition breakage and surprise defects are not expected to happen.
one possible reason for their occurrence could be hidden dependencies e.g.
logical coupling .
the intuition here is that a change to a co changed file may cause a defect.
since more recent changes are more relevant we also calculate for each factor its value in the three months prior to release labeled as recent .
time factors to take into account the fact that breakage and surprise defects start earlier and later than average respectively we consider factors related to how close to a release a file is changed as well as the file s age.
each category comprises several factors as listed in table .
for each factor we provide a description motivation and any related work.
.
prediction models in this work we are interested in predicting whether or not a file has a breakage or surprise defect.
similar to previous work on defect prediction we use a logistic regression model.
a logistic regression model correlates the independent variables in table with the dependent variable probability of the file containing a breakage or surprise defect .
initially we built the logistic regression model using all of the factors.
however to avoid collinearity problems and to assure that all of the variables in the model are statistically significant we removed highly correlated variables i.e.
any variables with correlation higher than .
.
in the end we were left with five factors that all had a variance inflation factor vif below .
as recommended by previous work .
to test for statistical significance we measure the p value of the independent variables in the model to make sure that this is less than .
.
altogether we extracted a total of factors.
after removing the highly correlated variables five factors were left that covered all three categories i.e.
pre defects file size num co changed files modification size co changed files and latest change before release.
.
performance evaluation of the prediction models after building the logistic regression model we verify its performance using two criteria explanatory power and predictive power.
these measures are widely used to measure the performance of logistic regression models in defect prediction .
explanatory power .
ranges between and quantifies the variability in the data explained by the model.
we also report and compare the variability explained by each independent variable in the model.
examining the explained variability of each independent variable allows us to quantify the relative importance of the independent variables in the model.
predictive power .
measures the accuracy of the model in predicting the files that have one or more breakage surprise defects.
the accuracy measures that we use precision and recall are based on the classification results in the confusion matrix shown in table .
.precision the percentage of correctly classified breakage surprise files over all of the files classified as having breakage surprise defects precision tp tp fp.
.recall the percentage of correctly classified breakage surprise files relative to all of the files that actually have breakage surprise defects recall tp tp fn.table list of factors used to predict breakage and surprise defects.
factors in italics are factors used in the final prediction models i.e.
after removing highly correlated factors.
category factor description rationale related work traditional factorspre defectsnumber of prerelease defectstraditionally performs well for post release defect prediction.prior defects are a good indicator of future defects .
pre changesnumber of prerelease changestraditionally performs well for post release defect prediction.the number of prior modifications to a file is a good predictor of future defects .
file sizetotal number of lines in the filetraditionally performs well for post release defect prediction.the lines of code metric correlates well with most complexity metrics e.g.
mccabe complexity .
co change factors recent num cochanged filesnumber of files a file co changed withthe higher the number of files a file cochanges with the higher the chance of missing to propagate a change.the number of files touched by a change is a good indicator of its risk to introduce a defect .
we apply this factor to the number of files co changing with a file.
recent size cochanged filescumulative size of the co changed filesthe larger the cochanged files are the harder they are to maintain and understand.the simple lines of code metric correlates well with most complexity metrics e.g.
mccabe complexity .
we apply this factor to co changing files.
recent modification size cochanged filesthe number of lines changed in the cochanged filesthe larger the changes are to the co changed files the larger the chance of introducing a defect.larger changes have a higher risk of introducing a defect .
we apply this factor to a file s co changing files.
recent num changes cochanged filesnumber of changes to the co changed filesthe higher the number of changes to the cochanged file the higher the chance of introducing defects.the number of prior modifications to a file is a good predictor of its future defects .
we apply this factor to a file s cochanging files.
recent pre defects cochanged filesnumber of prerelease defects in co changed filesthe higher the number of pre release defects in the co changed files the higher the chance of a defect.prior defects are a good indicator of future defects .
we apply this factor to a file s co changing files.
time factorslatest change before releasethe time from the latest change to the release in days changes made close to the release date do not have time to get tested properly.more recent changes contribute more defects than older changes .
agethe age of the file from first change until the release ga date the older the file the harder it becomes to change and maintain.code becomes harder to change over time .
table confusion matrix true class classified as breakage no breakage breakage tp fp no breakage fn tn a precision value of would indicate that every file we classify as having a breakage surprise defect actually has a breakage surprise defect.
a recall value of would indicate that every file that actually has a breakage was classified as having a breakage surprise.
we employ fold cross validation .
the data set is divided into two parts a testing data set that contains of the originaldata set and a training data set that contains of the original data set.
the model is trained using the training data and its accuracy is tested using the testing data.
we repeat the fold cross validation times by randomly changing the fold.
we report the average of the runs.
determining the logistic regression model threshold value the output of a logistic regression model is a probability between and of the likelihood that a file belongs to the true class e.g.
a file is buggy .
then it is up to the user of the output of the logistic regression model to determine a threshold at which she he will consider a file as belonging to the true class.
generally speaking a threshold of .
is used.
for example if a file has a likelihood of .
or higher then it is considered buggy otherwise it is not.
however the threshold is different for different data sets and the value of the threshold affects the precision and recall values of theprediction models.
in this paper we determine the threshold for each model using an approach that examines the tradeoff between type i and type ii errors .
type i errors are files that are identified as belonging to the true class while they are not.
having a low logistic regression threshold e.g.
.
increases type i errors a higher fraction of identified files will not belong to the true class.
a high type i error leads to a waste of resources since many nonfaulty files may be wrongly classified.
on the other hand type ii error is the fraction of files in the true class that are not identified as being true when they should be.
having a high threshold can lead to large type ii errors and thus missing many files that may be defective.
to determine the optimal threshold for our models we perform a cost benefit analysis between the type i and type ii errors.
similar to previous work we vary the threshold value between to and use the threshold where the type i and type ii errors are equal.
.
measuring the effect of factors on the predicted probability in addition to evaluating the accuracy and explanatory power of our prediction models we need to understand the effect of a factor on the likelihood of finding a breakage or surprise defect.
quantifying this effect helps practitioners gain an in depth understanding of how the various factors relate to breakage and surprise defects.
to quantify this effect we set all of the factors to their median value and record the predicted probabilities which we call the standard median model smm .
then to measure the individual effect of each factor we set all of the factors to their median value except for the factor whose effect we want to measure.
we double the median value of that factor and re calculate the predicted values which we call the doubled median model dmm .
we then subtract the predicted probability of the smm from the predicted output of the dmm and divide by the predicted probability of the smm.
so provides us with a way to quantify the effect a factor has on the likelihood of a file containing a breakage or surprise defect.
the effect of a factor can be positive or negative.
a positive effect means that a higher value of the factor increases the likelihood whereas a negative effect value means that a higher value of the factor decreases the likelihood of a file containing a breakage surprise defect.
.
case study results in this section we address the research questions posted earlier.
first we examine the accuracy in terms of predictive and explanatory power of our prediction models.
then we examine the contribution of each factor on the models in terms of explanatory power.
lastly we examine the effect of the factors on the breakage and surprise proneness.
rq1.
can we effectively predict which files will have breakage surprise defects?
using the extracted factors we build logistic regression models that aim to predict whether or not a file will have a breakage or surprise defect.
the prediction was performed for five different releases of the large commercial software system.
to measure predictive power we present the precision recall and the threshold th.
value used in the logistic regression model for each release.
the last row in the tables presents the average across all releases.
predictive power tables and show the results of the breakage and surprise defect prediction models respectively.
on average the precision for breakage and surprise defects is low i.e.
.
for breakages and .
for surprise defects.table performance of breakage prediction models predictive power explanatory power release precision recall th.
pred deviance explained r1.
.
.
.
.
r2.
.
.
.
.
r3 .
.
.
.
r4 .
.
.
.
r4.
.
.
.
.
average .
.
.
.
table performance of surprise prediction models predictive power explanatory power release precision recall th.
pred th.
sup deviance explained r1.
r2.
.
.
.
.
.
r3 .
.
.
.
.
r4 .
.
.
.
.
r4.
.
.
.
.
.
average .
.
.
.
.
it is important to note that the low precision value is due to the low percentage of breakage and surprise defects in the data set as shown earlier in table .
as noted by menzies et al.
in cases where the number of instances of an occurrence is so low i.e.
achieving a high precision is extremely difficult yet not that important.
in fact a random prediction would be correct .
of the time on average whereas our prediction model more than doubles that precision for breakages and more than triples that for surprise defects.
the more important measure of the prediction model s performance is recall which on average is .
for breakage defects and .
for surprise defects.
explanatory power the explanatory power of the models ranges between .
average of .
for breakage defects and between .
.
average of .
for surprise defects.
the values that we achieve here are comparable to those achieved in previous work predicting post release defects .
the explanatory power may be improved if more or better factors are used in the prediction model.
we view the factors used in our study as a starting point for breakage and surprise defect prediction and plan to and encourage others to further investigate in order to improve the explanatory power of these models.
other considerations for each prediction model we explicitly report the threshold for the logistic regression models as th.
pred in tables and .
in addition the surprise threshold which we use to identify files that had surprise defects is given under the th.
sup column in table .
since the number of pre release defects is used in the dependent variable of the surprise model surprise file post defects pre defects we did not use it as part of the independent variables in the prediction model.
this makes our results even more significant since previous work showed that pre release defects traditionally are the largest contributor to post release defect prediction models.
our prediction models predict breakage and surprise defects with a precision that is at least double that of a random prediction and a recall above .
rq2.
which factors are important for the breakage surprise defect prediction models?
in addition to knowing the predictive and explanatory power of our models we would like to know which factors contribute the most to these predictions.
to answer this question we perform an anov a analysis to determine the contribution of the three factor categories.
we summarize the findings in tables and as follows traditional defect prediction factors are major contributors for the breakage defect prediction models however they only have a small contribution in predicting surprise defects.
co change factors provide a small contribution to breakage defect prediction models however they make a major contribution to predicting surprise defects.
time factors provide a minor contribution to breakage defect prediction models however they make a major contribution in predicting surprise defects.
although there are exceptions to the above mentioned observations e.g.
traditional defect prediction factors make a major contribution to the surprise defect model in r2.
our observations are based on the trends observed in the majority of the releases.
as mentioned earlier in section .
breakage files were mainly involved with defect fixing efforts and are by definition defects that are found in the field.
perhaps these characteristics of breakage defects help explain why the traditional post release defect prediction factors perform well in predicting breakages.
furthermore earlier observations in section .
showed that surprise defect files were worked on later than other files i.e.
mrs that touched surprise defect files were started later than other mrs .
our findings show that being changed close to a release is one of the best indicators of whether or not a file will have a surprise defect.
traditional defect prediction factors are good indicators of breakage defects.
the factors related to co changed files and time related factors are good indicators of surprise defects.
rq3.
what effect does each factor have on the likelihood of finding a breakage surprise defect in a file?
thus far we examined the prediction accuracy and the importance of the factors to these prediction models.
now we study the effect of each factor on the likelihood of finding a breakage or surprise defect in a file.
in addition to measuring the effect we also consider stability and explanatory impact.
if an effect has the same sign direction for all releases i.e.
positive or negative effect in all releases then we label it as highly stable.
if the effect of a factor has the same sign in all releases except for one then we label it as being mainly stable.
a factor having a different sign in more than two of the five releases is labeled as being unstable.
the explanatory impact column is derived from the values in tables and .
if a factor belongs to a category that had a strong explanatory power then we label it as having high impact otherwise we consider it as having low impact.we use the stability and impact measure to help explain the strength of our findings.
for example if we find that a factor has a positive effect and has high impact to the explanatory power of the model then we believe this effect to be strong.
breakage defects table shows the effect of the different factors on the likelihood of predicting a breakage defect in a file.
we observe that in all releases pre release defects have a positive effect on the likelihood of a breakage defect existing in a file i.e.
highly stable .
in addition table showed that the traditional defect prediction factors contributed the most to the explanatory power of the model i.e.
high impact .
file size and the number of co changed files generally have a positive and negative effect respectively.
however the impact of the number of co changed files and recent modification size co changed files is low as shown in table .
similarly the latest change before release factor had low impact.
as stated earlier our manual examination of the breakage files showed that breakage files were especially involved with fixing efforts.
therefore the fact that pre release defects and size have a strong positive effect was expected since these factors are positively correlated with post release defects .
in fact we found that the average file size of breakage files is larger than non breakage files.
the rest of the factors had low impact on the explanatory power of the prediction model therefore we cannot conclude any meaningful results from their effect.
release r4 in our results seems to be an outlier.
for example contrary to the other releases file size shows a negative effect in r4.
after closer examination we found that r4 was a large major release that added a large amount of new functionality.
this could be the reason why the effect values for r4 are so different from the effect results of the remaining releases.
surprise defects table shows the effect values for the surprise defect prediction model.
in this model file size has a large stable positive effect however as shown earlier in table this factor category has very little contribution to the explanatory power of the model i.e.
low impact .
the number of co changed files has a negative effect that is highly stable and highly impacting.
this is counter intuitive especially since manual inspection showed that mrs that contained surprise files are larger than average.
after closer examination we found that although surprise files were part of large mrs the number of surprise files in those mrs was low.
in fact the percentage of surprise files in those large mrs was .
in r2.
and in the remaining releases .
to sum up we find that surprise defect files exist in large mrs however these surprise files make up only a small portion of those mrs. we find that the recent modification size of co changed files has a negative effect that is mainly stable and highly impacting.
furthermore as expected making a change last minute increases the likelihood of a surprise defect i.e.
the negative effect of latest change before release .
as shown in section .
in contrast to breakages surprise defect files were worked on later than usual.
we conjecture that starting late means less time for testing hence the much higher effect of these late changes on surprise defect files compared to the breakage files.
pre release defects and file size have a positive effect on the likelihood of a file containing a breakage defect.
the number of co changed files the modification size of recently co changed files and the time since last change before release have a negative effect on the likelihood of a file having a surprise defect.table contribution of factor categories to the explanatory power of breakage prediction models r1.
r2.
r3 r4 r4.
traditional defect prediction factors .
.
.
.
.
co change factors .
.
.
.
.
time factors .
.
.
.
.
overall deviance explained .
.
.
.
.
table contribution of factor categories to the explanatory power of surprise prediction models r1.
r2.
r3 r4 r4.
traditional defect prediction factors .
.
.
.
co change factors .
.
.
.
time factors .
.
.
.
overall deviance explained .
.
.
.
table effect of factors on the likelihood of predicting a file with a breakage defect.
effect is measured by setting a factor to double its median value if the median is while the rest of the factors are set to their median value.
r1.
r2.
r3 r4 r4.
stability explanatory impact pre defects highly stable high impact file size mainly stable high impact num co changed files per mr mainly stable low impact recent modification size co changed files mainly stable low impact latest change before release not stable low impact table effect of factors on the likelihood of predicting a file with a surprise defect.
effect is measured by setting a factor to double its median value if the median is while the rest of the factors are set to their median value.
r1.
r2.
r3 r4 r4.
stability explanatory impact file size highly stable low impact num co changed files per mr highly stable high impact recent modification size co changed files mainly stable high impact latest change before release highly stable high impact .
effort sa vings by focusing on surprise and breakage files thus far we have shown that we are able to build prediction models to predict files that contain breakage and surprise defects.
however one question still lingers what if we used the traditional post release defect prediction model to predict breakage and surprise defects?
is it really worth the effort to build these specialized models?
to investigate whether building specialized prediction models for breakage and surprise defects is beneficial we use defect prediction models that are trained to predict post release defects to predict files with breakages and surprise defects.
due to the fact that post release defects are much more common than breakages or surprise defects post release defect models are more likely to say that most files have breakages or surprise defects.
that will lead to a large amount of unnecessary work.
therefore we use the number of false negatives to compare the performance of the post release models and the specialized models.
to make a meaningful comparison of effort which is related to type i error we fix type ii error to be the same in both models.
breakage defects.
table shows the results of the specialized prediction model breakage breakage and the post release prediction model post breakage for release .
.
both of thesetable breakages in release .
breakage breakage post breakage predicted predicted actual actual models predict files that have breakage defects.
the false positives are highlighted in grey in table .
we observe that the specialized prediction model has approximately i.e.
less false positives than the post release model.
this means that using the specialized model would reduce the inspection effort of files by .
we also convert this effort saving into loc which is approximately of the total loc.
surprise defects.
table shows the results of the specialized model surprise surprise and the post release prediction model post surprise for files that have surprise defects.
in this case the specialized models lead to approximately i.e.
effort savings i.e.
less false positives .
comparing the savings in terms of loc we find that using the specialized prediction modeltable surprise in r4.
surprise surprise post surprise predicted predicted actual actual leads to approximately effort savings compared to using a traditional post release defect prediction model.
this is a considerable amount of effort savings and shows the benefits of building a specialized prediction model of files with surprise defects.
using our custom prediction models reduces the amount of files inspected by practitioners by for breakages and for surprise defects.
.
limitations threats to construct validity consider the relationship between theory and observation in case the measured variables do not measure the actual factors.
breakage mrs were manually identified in our data by project experts.
although this manual linking was done by these project experts some mrs may have been missed or incorrectly linked.
we used files in defects reported within one month after release to determine the surprise defect threshold.
the assumption here is that defects reported within one month involve important functionality that is widely used after release.
defects that affect important functionality may be reported later than one month however.
threats to external validity consider the generalization of our findings.
the studied project was a commercial project written mainly in c c therefore our results may not generalize to other commercial or open source projects.
.
lessons learned and future work after performing our study we asked the opinions of the highly experienced quality manager in the project about the prediction results.
the manager has a theory about the reported effect of our last change before release factor.
the theory is that the so called late fix frenzies that go on in organizations to bring down the number of open defects in a software system before the release might have compromised the quality of inspections and other quality assurance activities.
this suggests that prediction may help to quantify and confirm intuition about the relationships between the aspects of the development process and the high impact defects.
however when the quality manager considered the merits of our prediction by their utility for system verification she argued that identifying locations of defects is of limited use to system testers because they test system features or behaviors not individual files.
in addition she doubted that the predicted location could be helpful even to developers inspections or unit tests without the additional information about the nature of the problem or how it should be fixed.
despite the positive findings related to prediction quality narrowing the scope and effort savings we still appear to be far from the state where the prediction results could be used in practice.
based on our quantitative and qualitative findings and experience we hypothesize that for defect prediction to become a practical tool each predicted location has to also contain a clear suggestion on why the defect might be there and how it may be fixed.to achieve this we propose a procedure similar to the one we conducted to identify surprise defects to classify defects into a variety of classes according to their nature the ways they may have been introduced and to the ways they may need to be fixed.
we expect that each type of high impact defect would have a different prediction signature which in turn can be used to provide developers with a recommendation on where the defect may be what nature it may have and how it may be fixed.
we can see an example of such a classification in the static analysis tools that not only provide a warning but also give a reason why a particular pattern might be a defect and a clear suggestion on how the potential defect can be fixed.
we are not aware of any similar patterns for defect prediction.
for example our investigation of surprise defects could be used to provide a warning of the kind this file might contain a defect that has been introduced a while ago perhaps because of incorrect requirements.
change patterns to this file suggest that the usage profile might have changed recently and the requirements may need to be reviewed to make sure they are accurate and complete.
obviously a more extensive investigation may be needed to provide more specific recommendations and we believe that the defect prediction methods should be tailored not simply to predict defect locations but like basic static analysis tools such as lint should also detect patterns of changes that are suggestive of a particular type of defect and recommend appropriate remedies.
.
conclusion the majority of defect prediction work focuses on predicting post release defects yet the adoption of this work in practice remains relatively low .
one of the main reasons for this is that defect prediction techniques generally identify too many files as having post release defects requiring significant inspection effort from developers.
instead of considering all defects as equal this paper focuses on predicting a small subset of defects that are highly impacting.
since there are many different interpretations of high impact we focus on one interpretation from the perspective of customers breakage defects and one from the perspective of practitioners surprise defects .
we find that both kinds of defects are different and that surprise defects similar to the more established concept of breakage defects have a high impact.
specialized defect prediction models can predict breakage and surprise defects effectively yielding sizeable effort savings over using simple post release defect prediction models.
traditional defect prediction factors i.e.
the number of prerelease defects and file size are good predictors of breakage defects whereas the number of co changed files the size of recently co changed files and the time since the last change are good predictors of surprise defects.
our findings suggest that building specialized prediction models is valuable to bring defect prediction techniques closer to adoption in practice.
however our qualitative analysis of surprise defects and the feedback from the quality assurance manager clearly indicate that further work is needed to develop defect prediction into a practical tool.
in particular we found support for the idea of building specialized models that identify not only a defect s location but also its nature thus greatly simplifying the process of determining what the defect is and how it needs to be fixed.
.