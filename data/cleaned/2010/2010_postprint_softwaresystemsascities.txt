software systems as cities a controlled experiment richard wettel and michele lanza reveal faculty of informatics university of lugano richard.wettel michele.lanza usi.chromain robbes pleiad dcc university of chile rrobbes dcc.uchile.cl abstract software visualization is a popular program comprehension technique used in the context of software maintenance reverse engineering and software evolution analysis.
while there is a broad range of software visualization approaches only few have been empirically evaluated.
this is detrimental to the acceptance of software visualization in both the academic and the industrial world.
we present a controlled experiment for the empirical evaluation of a 3d software visualization approach based on a city metaphor and implemented in a tool called codecity.
the goal is to provide experimental evidence of the viability of our approach in the context of program comprehension by having subjects perform tasks related to program comprehension.
we designed our experiment based on lessons extracted from the current body of research.
we conducted the experiment in four locations across three countries involving participants from both academia and industry.
the experiment shows that codecity leads to a statistically significant increase in terms of task correctness and decrease in task completion time.
we detail the experiment we performed discuss its results and reflect on the many lessons learned.
categories and subject descriptors d. .
restructuring reverse engineering and reengineering general terms experimentation human factors measurement keywords software visualization empirical validation .
introduction software visualization is defined by stasko et al.
as the use of the crafts of typography graphic design animation and cinematography with modern human computer interaction and computer graphics technology to facilitate both the human understanding and effective use of computer software .
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
icse may honolulu hawaii copyright acm ... .
.it has earned a reputation as an effective program comprehension technique and is widely used in the context of software maintenance reverse engineering and reengineering .
the last decade has witnessed a proliferation of software visualization approaches aimed at supporting a broad range of software engineering activities.
as a consequence there is a growing need for objective assessments of visualization approaches to demonstrate their effectiveness.
unfortunately only few software visualization approaches have been empirically validated so far which is detrimental to the development of the field .
one of the reasons behind the shortage of empirical evaluation lies in the difficulty of performing a controlled experiment for software visualization.
on the one hand the variety of software visualization approaches makes it almost impossible to reuse the design of an experiment from the current body of research.
on the other hand organizing and conducting a proper controlled experiment is in itself a difficult endeavor which can fail in many ways data which does not support a true hypothesis conclusions which are not statistically significant due to insufficient data points etc.
we present a controlled experiment for the empirical evaluation of a software visualization approach based on a city metaphor .
the aim is to show that our approach implemented in a tool called codecity is at least as effective and efficient as the state of the practice at supporting reverse engineering and program comprehension activities.
we conceived a set of tasks and measured both the correctness of the task solutions and the task completion time.
we conducted the experiment in four locations across three countries with participants from both industry and academia with a broad range of experience.
in this paper we make the following two major contributions .we detail the experiment design and its operation reporting on a number of lessons learned regarding the many pitfalls that this type of experiment entails and .we discuss the results of the experiment which show that our approach is a viable alternative to existing non visual techniques.
structure of the paper.
in section we describe our software visualization approach.
in section we present the related work followed by a list of desiderata in section extracted from an extensive study of the existing body of research.
in section we describe the design of our experiment and in section we detail the experiment s operational phase.
in section we present how we collected the data on which we performed the analysis presented in section .
in section we present the results of the experiment followed by a presentation of the threats to validity in section and the conclusions in section .
.
codecity in a nutshell our approach uses a city metaphor to depict software systems as three dimensional cities see figure where classes are buildings and packages are districts.
figure representation of a software system in codecity similar to the 2d polymetric view approach we map a set of software metrics on the visual properties of artifacts the number of methods is mapped on the height of the buildings the number of attributes on the base size and the number of lines of code on the color of the buildings from dark gray low to intense blue high .
we also extended our approach to address the visualization of design problems inspired by disease maps we assign vivid colors to design problems and color the affected artifacts according to the design problems that characterize them.
the resulting visualization is called disharmony map see figure and relies on design problem data computed using marinescu s detection strategies .
componentjava.awtnom 280noa inputeventjava.awt.eventnom 14noa 21stringjava.langnom 81noa 7calendarjava.utilsnom 71noa figure example of disharmony map a part of jdk .
codecity also supports software evolution analysis activities which for reasons we explain later we did not evaluate.
tool support.
our approach is implemented in codecity a freely available1standalone tool programmed in smalltalk and depicted in figure .
apart from the main visualization panel there is an information panel providing contextual data on the selected city artifact e.g.
name metric values etc.
.
codecity supports a broad range of facilities to interact such as inspection or querying and navigate the visualization.
although not integrated in an ide codecity features unidirectional linking between the visualization and the source code i.e.
the source code of the software element represented by a city artifact can be accessed in an editor from the visualization.
codecity is customizable the user can compose a new view by choosing from a set of metrics and artifact representations.
related work there is a rich body of research on empirical evaluation of information visualization by means of controlled experiments.
to identify both good practices and commonly occurring mistakes we conducted an extensive study of the literature.
the study extends far beyond the scope of this paper we limit the discussion to controlled experiments for the evaluation of software visualization.
storey and m ller performed a controlled experiment to compare the support of shrimp and rigi in solving a number of program comprehension tasks both to each other and to a baseline i.e.
sniff .
marcus et al.
performed a study to test the support provided by the sv3d visualization tool in answering a set of program comprehension questions .
the authors compared the performances obtained by using sv3d to the ones obtained by exploring the source code in an ide and a text file containing metrics values.
lange et al.
performed a controlled experiment in which they evaluated the usefulness of their enriched uml views by comparing them with traditional uml diagrams .
the baseline of the experiment was composed of a uml tool and a metric analysis tool.
quante performed a controlled experiment for the evaluation of dynamic object process graphs in supporting program understanding using not one but two subject systems.
a statistically significant improvement of the experimental group could only be detected in the case of one of the systems which shows that relying on solely one system is unsound.
cornelissen et al.
performed a controlled experiment for the evaluation of extra vis an execution trace visualization tool.
the purpose of the experiment was to evaluate how the availability of extra vis influences the correctness and the time spent by the participants in solving a number of program comprehension tasks.
from the perspective of the experiment s design there are two major differences between our experiment and the related work summarized in table .
experimentsubjects object system s academia industry classes kloc storey et al.
marcus et al.
lange et al.
?
?
quante cornelissen et al.
wettel et al.
table comparing to the related work the first is that in our experiment we have an equal split between subjects from academia i.e.
and industry i.e.
while the rest of the experiments have only subjects from academia with the exception of cornelissen et al.
who had one single industry practitioner.
while students are convenient subjects a sample made up entirely of students might not adequately represent the intended user population .
the second issue regards the fact that the systems used by most of the experiments are rather small and thus not representative for a realistic work setting.
in our experiment the largest of the two systems has almost half a million lines of code.
moreover learning from quante s experiment we do not rely on one system only but on two.
.
experimental design wish list we conducted a very exhaustive survey of research works dealing with experimental validation of software engineering information visualization and software visualization approaches.
the survey which included more than articles and various books is detailed in a technical report .
from the survey we distilled the following experimental design wish list which we kept in mind as we designed and conducted the experiment .choose a fair baseline for comparison.
if one wants to provide evidence for the value of an approach the question of what one compares it to has to be answered unambiguously.
.involve participants from industry .
any approach devised to support practitioners in their work is best evaluated using a subject sample with a fair share of software practitioners.
.take into account the range of experience level of the participants.
experience can greatly influence the outcome of the experiment.
.provide a tutorial of the experimental tool to the participants .
the experimental group would require intensive training to even come close to the skills of the control group acquired in years of operation.
therefore a minimal exercising of the experimental group in using the features required to solve the tasks is paramount.
.find a set of relevant tasks .
the tasks should be close in scope and complexity to real tasks performed by practitioners.
.include tasks which may not advantage the tool being evaluated.
this allows the experimenters to actually learn something during the experiment including shortcomings of their approach.
.limit the time allowed for solving each task .
allowing unbounded time for a task to avoid time pressure may lead participants to spend the entire time allotted for the experiment on solving a single task as quante s experiment showed.
.choose real world systems.
many experiments in the literature use small systems making it hard to generalize the results for real world systems.
.include more than one subject system in the experimental design .
quante showed that performing the same experiment on two different systems can lead to significantly different results.
.provide the same data to all participants .
the observed effect of the experiment is more likely attributable to the independent variables if this guideline is followed.
.report results on individual tasks .
this allows for a more precise and in depth analysis of the strengths and weaknesses of an approach.
.provide all the details needed to make the experiment replicable .
di penta et al.
discuss the benefits of replicability and present a set of guidelines that enable the replication of the experiment .
.
experimental design the purpose of our experiment is to quantitatively evaluate the effectiveness and efficiency of our approach when compared to state of the practice exploration approaches.
.
research questions hypotheses the research questions underlying our experiment are q1 does the use of codecity increase the correctness of the solutions to program comprehension tasks compared to nonvisual exploration tools regardless of the object system s size?
q2 does the use of codecity reduce the time needed to solve program comprehension tasks compared to non visual exploration tools regardless of the object system s size?
q3 which are the task types for which using codecity over nonvisual exploration tools makes a difference in either correctness orcompletion time ?
q4 do the potential benefits of using codecity in terms of correctness andtime depend on the user s background i.e.
academic versus industry practitioner ?
q5 do the potential benefits of using codecity in terms of correctness andtime depend on the user s experience level i.e.
novice versus advanced ?
the null hypotheses and alternative hypotheses corresponding to the first two research questions are synthesized in table .
null hypothesis alternative hypothesis h10 the tool does not impact the correctness of the solutions to program comprehension tasks.h1 the tool impacts the correctness of the solutions to program comprehension tasks.
h20 thetool does not impact the time required to complete program comprehension tasks.h2 thetool impacts the time required to complete program comprehension tasks.
table null and alternative hypotheses for the third question we perform a separate analysis of correctness and completion time for each of the tasks.
for the last two questions we analyze the data within blocks which we however do not discuss in this paper due to space reasons.
we refer the interested reader to our detailed technical report .
.
dependent and independent variables the purpose of the experiment is to show whether codecity s 3d visualizations provide better support to software practitioners in solving program comprehension tasks than state of the practice exploration tools.
additionally we are interested in how well codecity performs compared to the baseline when analyzing systems of different magnitudes.
consequently our experiment has two independent variables the tool used to solve the tasks and the object system size.
the tool variable has two levels i.e.
codecity and a baseline chosen based on the criteria described in section .
.
.
the object system size has two levels i.e.
medium and large because visualization starts to become useful only when the analyzed system has a reasonable size.
the object systems chosen to represent these two treatments are presented in section .
.
.
similarly to other empirical evaluations of software visualization approaches the dependent variables of our experiment arecorrectness of the task solution i.e.
a measure of effectiveness andcompletion time i.e.
measure of efficiency .
the design of our experiment is a between subjects design i.e.
a subject is part of either the control group or of the experimental group.
.
.
finding a baseline there is a subtle interdependency between the baseline and the set of tasks for the experiment.
in an ideal world we would have devised tasks for each of the three contexts in which we applied our approach software understanding evolution analysis and design quality assessment.
instead we had to settle for a reasonable compromise.
we looked for two characteristics in an appropriate baseline data feature compatibility with codecity and recognition from the community i.e.
a state of the practice tool .
unfortunately we could not find a single tool satisfying both criteria.
to allow a fair comparison without having to limit the task range we opted to build a baseline from several tools.
the baseline needed to provide exploration and querying functionality support for presenting at least the most important software metrics support for design problems exploration and if possible support for evolutionary analysis.
in spite of the many existing software analysis and visualization approaches software understanding is still mainly performed at the source code level.
since the most common source code exploration tools are integrated development environments ides we chose eclipse a popular ide in both academia and industry which represents the current state of the practice.
the next step was finding support for exploring meta data such as software metrics and design problem data since they were not available in eclipse.
we looked for a convenient eclipse plugin for metrics or even an external metrics tool but could not find one that fit our requirements including support for user defined metrics .
since we did not want to confer an unfair data advantage to the subjects in the experimental group we chose to provide the control group with tables containing the metrics and design problem data and the popular excel spreadsheet application for exploring the data.
finally due to eclipse s lack of support for multiple versions we decided to exclude the evolution analysis from our evaluation although we consider it one of the strong points of our approach.
providing the users with several projects in eclipse representing different versions of the same system with no relation among them or even worse with just a versioning repository would have been unfair.
.
.
object systems we chose two java systems both large enough to potentially benefit from visualization yet of different size so that we can reason about this independent variable.
the smaller of the two systems is findbugs2 a tool using static analysis to find bugs in java code developed as an academic project at the university of maryland while the larger system is azureus3 a popular p2p file sharing client and one of the most active open source projects hosted at sourceforge.
in table we present the main characteristics of the two systems related to the tasks of the experiment.
system size medium large name findbugs azureus lines of code packages classes god classes brain classes data classes table the two object systems controlled variables we identified two factors that could have an influence on the subjects performance i.e.
their background and experience level.
the background represents the working context of a subject which we divided into industry andacademy .
the second factor is experience level which represents the domain expertise gained by each of the participants divided into beginner andadvanced .
for academia subjects below a phd were considered beginners while researchers i.e.
phd students post docs and professors were considered advanced.
for industry we considered that participants with up to three years of experience were beginners and the rest advanced.
we used a randomized block design with background andexperience level as blocking factors.
we assigned each participant based on personal information collected before the experiment to one of the four categories i.e.
academy beginner academy advanced industry beginner and industry advanced .
we then randomly assigned one of the four treatments i.e.
combinations of tool and system size to the participants in each category.
.
treatments by combining the two levels of each of the two independent variables we obtain four treatments illustrated in table .
tool azureus findbugs treatment description codecity t1 t2 codecity installation with a loaded model of the system to be analyzed and the source code of the object system accessible from the visualizations.
ecl exl t3 t4 eclipse installation with default development tools an eclipse workspace containing a project with the entire source code of the object system an excel installation and a sheet containing all the metrics and design problem data required for solving the tasks and available to the experimental groups.
table treatments we provided the treatments as virtual images for virtualbox4 which was the only piece of software required to be installed by the participants.
each virtual image contained only the necessary pieces of software i.e.
either codecity or eclipse excel installed on a windows xp sp2 operating system.
.
tasks our approach provides aid in comprehension tasks supporting adaptive and perfective maintenance.
we tried to use the previouslydefined maintenance task definition frameworks by pacione et al.
and by sillito et al.
to design the tasks of our evaluation.
however both frameworks proved ill suited.
since codecity relies exclusively on static information extracted from the source code it was not realistic to map our tasks over pacione s model which is biased towards dynamic information visualization.
sillito s wellknown set of questions asked by developers although partially compatible with our tasks refers to developers exploring source code only.
our approach supports software architects designers quality assurance engineers and project managers in addition to developers.
these additional roles assess systems at higher levels of abstraction not covered by sillito s framework.
we decided to use these works as inspirations and defined our own set of tasks detailed in table dealing with various maintenance concerns and split into program comprehension a and design quality assessment b .
task concern a1 description.
locate all the unit test classes of the system and identify the convention or lack therof used by the developers to organize the tests.
rationale.
test classes are typically defined in packages according to a project specific convention.
before integrating their work in the system developers need to understand how the test classes are organized.
software architects design the high level structure of the system which may include the convention by which test classes are organized while quality assurance engineers monitor the consistency of applying these rules in the system.structural understanding a2.
description.
look for the term t1in the names of classes and their attributes and methods and describe the spread of these classes in the system.
rationale.
assessing how domain knowledge is encapsulated in source code is important in several scenarios.
to understand a system they are not familiar with developers often start by locating familiar domain concepts in the source code.
maintainers use concept location on terms extracted from change requests to identify where changes need to be performed in the system.
software architects want to maintain a consistent mapping between the static structure and the domain knowledge.
each of these tasks starts with locating a term or set of terms in the system and assess its dispersion.concept location a2.
description.
look for the term t2in the names of classes and their attributes and methods and describe the spread of these classes in the system.
rationale.
same as for task a2.
.
however the term t2was chosen such that it had a different type of spread than t1.concept location a3 description.
evaluate the change impact of class cby considering its caller classes classes invoking any of its methods .
the assessment is done in terms of both intensity number of potentially affected classes and dispersion how these classes are distributed in the package structure .
rationale.
impact analysis allows one to estimate how a change to a part of the system impacts the rest of the system.
although extensively used in maintenance activities impact analysis may also be performed by developers when estimating the effort needed to perform a change.
it also gives an idea of the quality of the system a part of the system which requires a large effort to change may be a good candidate for refactoring.
a4.
description.
find the three classes with the highest number of methods nom in the system.
rationale.
classes in object oriented systems ideally encapsulate one single responsibility.
since methods are the class s unit of functionality the number of methods metric is a measure of the amount of functionality of a class.
classes with an exceptionally large number of methods make good candidates for refactoring e.g.
split class and therefore are of interest to practitioners involved in either maintenance activities or quality assurance .metricbased analysis a4.
description.
find the three classes with the highest average number of lines of code per method in the system.
rationale.
it is difficult to prioritize candidates for refactoring from a list of large classes.
in the absence of other criteria the number and complexity of methods can be used as a measure of the amount of functionality for solving this problem related to maintenance andquality assurance .metricbased analysis b1.
description.
identify the package with the highest percentage of god classes in the system.
rationale.
god classes are classes that tend to incorporate an overly large amount of intelligence.
their size and complexity often make them a maintainer s nightmare.
keeping these potentially problematic classes under control is important.
by maintaining the ratio of god classes in packages to a minimum the quality assurance engineer keeps this problem manageable.
for a project manager in the context of the software process packages represent work units assigned to the developers.
assessing the magnitude of this problem allows him to take informed decisions in assigning resources.focused design assessment b1.
description.
identify the god class containing the largest number of methods in the system.
rationale.
it is difficult to prioritize candidates for refactoring from a list of god classes.
in the absence of other criteria e.g.
the stability of a god class over its evolution the number of methods can be used as a measure of the amount of functionality for solving this problem related to maintenance and quality assurance .focused design assessment b2.
description.
identify the dominant class level design problem the design problem that affects the largest number of classes in the system.
rationale.
god class is only one of the design problems that can affect a class.
a similar design problem is the brain class which accumulates an excessive amount of intelligence usually in the form of brain methods i.e.
methods that tend to centralize the intelligence of their containing class .
finally data classes are just dumb data holders without complex functionality but with other classes strongly relying on them.
gaining a big picture of the design problems in the system would benefit maintainers quality assurance engineers and project managers .holistic design assessment b2.
description.
write an overview of the class level design problems in the system.
describe your most interesting or unexpected observations.
rationale.
the rationale and targeted user roles are the same as for task b2.
.
however while the previous one gives an overview of design problems in figures this task provides qualitative details and has the potential to reveal the types of additional insights obtained with visualization over raw data.holistic design assessment table tasks .
operation pilotexperiment20092010februaryjanuarydecembernovember1428182224252921281824251luganobolognaantwerpbernapril14.. 2611111rrr 416rtraining sessionnn subjects with codecity experimental legend followed by an experimental session withm subjects with eclipse excel control remote self controlled sessionrm figure the timeline of the experiment the operation covered the period nov apr and was divided in two phases the pilot and the experiment.
figure shows the timeline of the operation.
the operation is composed of a series of experimental runs each run consists of a one hour training session followed by one or more experimental sessions of up to two hours.
a training session consists of a talk in which the experimenter presents the approach concluded with a codecity demonstration.during the experimental session s the subjects solve the tasks with the assigned tool on the assigned object system under the experimenter s observation.
the numbers in figure reflect only the participants whose data points were taken into account during the analysis and not those excluded from it which we discuss later.
.
the pilot study we conducted a pilot study with master students of the university of lugano enrolled in a course on software design improving the questionnaire and solving problems as they emerged required several iterations.
we organized a joint training session followed by three experimental sessions each one week apart.
before the first session we conducted the experiment with a researcher from our group with extensive experience with eclipse to make sure the tasks for the control group were doable in the allotted time.
we did not include any of these data points in the analysis.
.
experimental runs at this point we were confident enough to start our experiment.
we performed the following experimental runs bologna i. professionals with years of experience.
bologna ii.
professionals with years of experience.
lugano i. researcher development leader of a small company.
lugano ii iii.
practioners with years of experience.
antwerp.
ph.d. and msc students.
bern.
consultants professor ph.d. and msc student.
.
data collection we collected data continuously before during and after the experiment.
.
personal information before the experiment we collected both personal information e.g.
gender age nationality and professional data e.g.
job position experience with object oriented programming java eclipse and reverse engineering by means of an online questionnaire to be found in .
.
timing data to time participants accurately we developed our own timing web application in smalltalk.
during the experimental sessions the timing application would run on the experimenter s computer and project the current time.
the displayed time was used as common reference by the participants whenever they were required in the questionnaire to log the time.
in addition the application displayed for each participant the name current task and the remaining time allotted for the task.
the subjects were asked to announce to the experimenter every time they logged the time so that the experimenter could reset their personal timer by clicking on the hyperlink marked with the name of the subject.
whenever a subject was unable to finish a task in the allotted time minutes for each task the application would display the message overtime beside the name and the experimenter would ask the subject to immediately pass to the next task before resetting the timer.
.
correctness data to convert the collected task solutions into quantitative information we needed an oracle which would provide both the set of correct answers and the grading scheme for each task.
having two object systems and two data sources source code for the control group and a model of the system for the experimental group led to four oracle sets.
to build them two of the authors and a third experimenter solved the tasks with each treatment and designed the grading scheme for each task.
in addition for the two experimental treatments we computed the results using queries on the model of the object system to make sure that we were not missing any detail because of particularities of the visual presentation e.g.
occlusion too small buildings etc.
.
eventually we discussed the divergences and merged our solutions.
finally we needed to grade the solution of the subjects.
we employed blind marking to rule out bias when grading a solution the experimenter does not know whether the subject that provided the solution has used an experimental or a control treatment.
for this one of the authors created four code names for the groups and created a mapping between groups and code names known only to him.
then he provided the other two experimenters with the encoded data along with the encoded corresponding oracle which allowed them to perform blind grading.
in addition the author who encoded the data performed his grading unblinded.
eventually the authors discussed the differences and converged towards the final grading.
the minimum grade was while the maximum was .
.
participants feedback the questionnaire handout ends with a debriefing section in which the participants are asked to assess the level of difficulty for each task and the overall time pressure to give us feedback which could potentially help us improve the experiment and optionally to share with us any interesting insights they encountered during the analysis.
.
data analysis .
preliminary data analysis we observed an exceptional condition related to task a4.
which had the highest discrepancy in time and correctness between control and experimental groups.
the data points showed that the experimental group was not able to solve this task while the control group was quite successful at solving it.
the experimental group had an average of .
with null scores out of and most subjects used up the entire allotted time i.e.
ceiling effect .
the control group had an average of .
with perfect scores out of and most subjects finished in roughly half the allotted time.
the subjects perceived its difficulty accordingly in the debriefing questionnaire experimental subjects graded task a4.
as impossible while the control group described it as simple as shown in figure .
difficultya1a2.1a2.2a3a4.1a4.2b1.1b1.2b2.1b2.2trivialsimpleintermediatedifficultimpossible77512024817101211702119434351136536401418911800000102002 036912a1a2.1a2.2a3a4.1a4.2b1.1b1.2b2.1b2.2frequencytasktrivialsimpleintermediatedifficultimpossible difficultya1a2.1a2.2a3a4.1a4.2b1.1b1.2b2.1b2.2trivialsimpleintermediatedifficultimpossible31119415106108279766088107345810620050240180004002015036912a1a2.1a2.2a3a4.1a4.2b1.1b1.2b2.1b2.2frequencytasktrivialsimpleintermediatedifficultimpossibletool codecity tool ecl excldifficultya1a2.1a2.2a3a4.1a4.2b1.1b1.2b2.1b2.2trivialsimpleintermediatedifficultimpossible77512024817101211702119434351136536401418911800000102002 036912a1a2.1a2.2a3a4.1a4.2b1.1b1.2b2.1b2.2frequencytasktrivialsimpleintermediatedifficultimpossible difficultya1a2.1a2.2a3a4.1a4.2b1.1b1.2b2.1b2.2trivialsimpleintermediatedifficultimpossible31119415106108279766088107345810620050240180004002015036912a1a2.1a2.2a3a4.1a4.2b1.1b1.2b2.1b2.2frequencytasktrivialsimpleintermediatedifficultimpossibletool codecity tool ecl excldifficultya1a2.1a2.2a3a4.1a4.2b1.1b1.2b2.1b2.2trivialsimpleintermediatedifficultimpossible77512024817101211702119434351136536401418911800000102002 036912a1a2.1a2.2a3a4.1a4.2b1.1b1.2b2.1b2.2frequencytasktrivialsimpleintermediatedifficultimpossible difficultya1a2.1a2.2a3a4.1a4.2b1.1b1.2b2.1b2.2trivialsimpleintermediatedifficultimpossible31119415106108279766088107345810620050240180004002015036912a1a2.1a2.2a3a4.1a4.2b1.1b1.2b2.1b2.2frequencytasktrivialsimpleintermediatedifficultimpossibletool codecity tool ecl excllegend ecl exl codecity figure histograms of perceived difficulty per task the reason is that we underestimated the knowledge of codecity required to perform this task.
solving this task with our tool implies a deep knowledge of codecity s customization features and of the underlying smalltalk programming language the only subject who managed to solve the task in the experimental group is a smalltalk expert.
these were unreasonable requirements to expect from the experimental subjects who were not trained to use such features.
to eliminate this unusually large discrepancy between the two groups we excluded the task from the analysis.
.
outlier analysis before performing our statistical test we followed the suggestion of wohlin et al.
regarding the removal of outliers caused by exceptional conditions to allow us to draw valid conclusions from the data.
during the bologna i experimental run one of the participants assigned with an experimental treatment experienced serious performance slowdowns due to the low performance of his computer.
although this participant was not the only one reporting performance slowdowns he was by far the slowest as measured by the completion time and since this represented an exceptional condition we excluded his data from the analysis.
another participant got assigned to an ecl exl treatment although he did not have any experience with eclipse but with another ide.
for this reason this subject took more time in the first tasks than the others because of his lack of experience with eclipse.
since we did not want to compromise the analysis by disfavoring any of the groups i.e.
this data point provided the highest completion time and would have biased the analysis by disadvantaging the control groups we excluded also this data point from the analysis.
during the bologna ii run two participants had compatibility problems with the virtualization software installed on their machines.
eventually they were borrowed our two replacement machines but due to the meeting room s tight schedule we were not able to wait for them to finish the experiment.
we decided to exclude these two data points from our analysis.
.
subject analysis after the pilot study involving nine participants we conducted the experiment with a total of participants in several runs.
after removing four data points during the outlier analysis we were left with subjects of which industry practitioners all advanced and from academia of which beginners and advanced .
for each of the treatments we have data points and a fair subject distribution within the remaining three blocks as shown in table .
treatment t1 t2 t3 t4 total blockacademiabeginner advanced industrybeginner advanced total table subject distribution moreover the random assignments of treatment within blocks led to a balanced distribution of the subjects expertise among treatments as we see in figure .
experience level codecityoopjavaeclipsereverse engineeringnonebeginnerknowledgeableadvancedexpert0010035105410791264830103691215oopjavaeclipsereverse engineeringfrequencyexpertise fieldnonebeginnerknowledgeableadvancedexpert experience level eclipse exceloopjavaeclipsereverse engineeringnonebeginnerknowledgeableadvancedexpert0000000722871213104541103691215oopjavaeclipsereverse engineeringfrequencyexpertise fieldnonebeginnerknowledgeableadvancedexperttool codecity tool ecl excllegend legend experience level codecityoopjavaeclipsereverse engineeringnonebeginnerknowledgeableadvancedexpert0010035105410791264830103691215oopjavaeclipsereverse engineeringfrequencyexpertise fieldnonebeginnerknowledgeableadvancedexpert experience level eclipse exceloopjavaeclipsereverse engineeringnonebeginnerknowledgeableadvancedexpert0000000722871213104541103691215oopjavaeclipsereverse engineeringfrequencyexpertise fieldnonebeginnerknowledgeableadvancedexperttool codecity tool ecl excllegend legend codecityecl exl figure subjects expertise while some of the subjects assigned to codecity have little or no experience with eclipse every subject assigned to ecl exl is at least knowledgeable in using this ide.
.
results based on the design of our experiment i.e.
a between subjects unbalanced i.e.
implying unequal sample sizes design with two independent variables tool and system size the suitable parametric test for hypothesis testing is a two way analysis of variance anov a .
we performed the test for correctness andcompletion time using the spss statistical package.
before the analysis we ensured that our data met the test s assumptions .independence of observations .
this assumption is implicitly met through the choice of a between subjects design.
.homogeneity of variances of the dependent variables .
we tested our data for homogeneity of both correctness andcompletion time using levene s test and in both cases the assumption was met.
.normality of the dependent variable across levels of the independent variables .
we tested the normality of correctness andcompletion time across the two levels of toolandobject system size using the shapiro wilk test for normality and also this assumption was met in all the cases.
we chose a significance level of which corresponds to a95 confidence interval.
the statistics related to correctness and completion time are presented in table .
.
analysis results on correctness interaction effect between tool and system size on correctness.
it is important that there is no interaction between the two factors which could have affected the correctness.
the interaction effect of tool and system size on correctness was not significant f p .
the data shows no evidence that the variation in correctness between codecity and ecl exl depends on the size of the system which strengthens any observed effect of the tool factor on the correctness.
the effect of tool on correctness.
there was a significant main effect of the tool on the correctness of the solutions f p indicating that the mean correctness score obtained by codecity users was significantly higher than the one for ecl exl users regardless of the size of the object system.
overall there was an increase in correctness of .
for codecity users m sd over ecl exl users m sd .
in the case of the medium size system there was a .
increase in correctness of codecity users m sd over ecl exl users m sd while in the case of the large size system the increase in correctness was .
for codecity users m sd over ecl exl users m sd .
the data shows that the increase in correctness for codecity over ecl exl was higher for the larger system.
the effect of system size on correctness.
although not the object of the experiment an expected significant main effect of system size on the correctness of the solutions was observed f p indicating that the correctness score was significantly higher for users performing the analysis on the medium size system than for users performing the analysis on the large size system regardless of the tool they used to solve the tasks.
the main effect of both tool and object system size on correctness and the lack of the effect of interaction between tool and object system size on correctness are illustrated in figure as well as the correctness box plots for the four treatments.
object system sizelargemediumcorrectness7.
.
.
.
.
.
.
.
.
.
.
.
codecityecl exltool time page object system sizelargemediumcorrectness8.
.
.
.
.
.
.00codecityecl exltool examine variables time by syssize by tool plot boxplot statistics none nototal.explore page figure means and box plots for correctness .
analysis results on completion time interaction effect between tool and system size on completion time.
similarly it is important that there is no interaction between the two factors which could have affected the completion time.
the interaction effect of tool and system size on completion time was not significant f p .
the data shows no evidence that the variation in completion time between codecity and ecl exl depends on the size of the system which strengthens any observed effect of the tool factor on the completion time.dep.
var.
correctness points completion time minutes system size medium large overall medium large overall tool ecl exl codecity ecl exl codecity ecl exl codecity ecl exl codecity ecl exl codecity ecl exl codecity mean .
.
.
.
.
.
.
.
.
.
.
.
difference .
.
.
.
.
.
min .
.
.
.
.
.
.
.
.
.
.
.
max .
.
.
.
.
.
.
.
.
.
.
.
median .
.
.
.
.
.
.
.
.
.
.
.
stdev .
.
.
.
.
.
.
.
.
.
.
.
table descriptive statistics related to correctness and completion time the effect of tool on completion time.
there was a significant main effect of the tool on the completion time f p indicating that the mean completion time was significantly lower for codecity users than for ecl exl users.
overall there was a decrease in completion time of .
for codecity users m sd over ecl exl users m sd .
in the case of the medium size system there was a .
decrease in completion time of codecity users m sd over ecl exl users m sd while in the case of the large size system there is a .
decrease in completion time for codecity users m sd over ecl exl users m sd .
the data shows that the time decrease for codecity users over ecl exl users is slightly lower in the case of the large system compared to that obtained for the medium system.
the effect of system size on completion time.
while not the goal of the experiment a significant effect of system size on the completion time was observed f p indicating that completion time was significantly lower for users analyzing the medium system than for users analyzing the large system.
the main effect of both tool and object system size on completion time and the lack of the effect of interaction between tool and object system size on completion time are illustrated in figure as well as the completion time box plots for the four treatments.
object system sizelargemediumcompletion time50.
.
.
.
.
.
.
.
.
.
codecityecl exltool examine variables correctness by syssize plot boxplot statistics none nototal id tool.
examine variables correctness by syssize by tool plot boxplot statistics none nototal.
explore page object system sizelargemediumcompletion time60.
.
.
.
.00codecityecl exltool page figure means and box plots for completion time .
result summary correctness.
the data allows us to reject the first null hypothesis h10in favor of the alternative hypothesis h1 which states that the tool impacts the correctness of the solutions to program comprehension tasks.
overall codecity enabled an increase in correctness of .
over ecl exl.
this result is statistically significant.
completion time.
we can also reject the second null hypothesis h20in favor of the alternative hypothesis h2 which states that the tool impacts the time required to complete program comprehension tasks.
overall codecity enabled a completion time reduction of .
over ecl exl.
this result is statistically significant.
.
task analysis a secondary goal of our experiment was to identify the types of tasks for which codecity provides an advantage over the baseline.
to this end for each task described in section .
we compared the performances in terms of correctness and time of the two tools and reasoned about the potential causes behind the differences.
see figure for a graphical overview supporting our task analysis5.
a1 identifying the convention used to organize unit tests relatively to the tested classes.
while eclipse performed consistently codecity outperformed it on the medium system and underperformed it on the large system.
the difference in performance is partially owed to fact that in spite of the existence of a number of classes named test there are no unit tests in the large system.
only a small number of codecity users examined the inheritance relations while the majority relied only on name matching.
the time is slightly better for the codecity subjects who benefited from the visual overview of the system while eclipse required scrolling up and down through the package structure.
a2.
determining the spread of a term among the classes.
codecity performed marginally better than eclipse in both correctness and completion time.
in codecity once the term search is completed the spread can be visually assessed.
in eclipse the term search produces a list of class names.
for this task the list showed classes in many packages belonging to different hierarchies and therefore a dispersed spread may represent a safe guess.
a2.
determining the spread of a term among the classes.
although the task is similar to the previous the results in correctness are quite different codecity significantly outperformed eclipse by .
the list of classes and packages in eclipse without the context provided by an overview i.e.
how many other packages are there in the system?
deceived some of the control subjects into believing that the spread of the term was dispersed while the codecity users took advantage of the big picture and better identified the localized spread of this term.
a3 estimating impact.
codecity significantly outperformed eclipse in correctness by and was slightly faster than eclipse.
finding the caller classes of a given class in eclipse is not straightforward.
moreover the resulting list provides no overview.
a4.
identifying the classes with the highest number of methods.
in terms of correctness codecity was on a par with excel for the medium system and slightly better for the large system.
in terms of completion time excel was slightly faster than codecity.
while codecity is faster at building an approximate overview of systems a spreadsheet is faster at finding precise answers in large data sets.
b1.
identifying the package with the highest percentage of god classes.
in both correctness and completion time codecity slightly outperformed excel.
the low scores in correctness of both tools shows that none of them is good enough to solve this problem alone.
instead codecity s visual presentation and excel s precision could complement each other well.
5due to space concerns we do not discuss task b2.2correctness points completion time minutes medium object system correctnessa1a2.1a2.2a3a4.1a4.2b1.1b1.2b2.1t1 codecity larget2 codecity mediumt3 ecl excl larget4 ecl excl medium0.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
a1a2.1a2.2a3a4.1a4.2b1.1b1.2b2.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.55correctness points tool ecl excltool codecity .
.
.
.
.
a1a2.1a2.2a3a4.1a4.2b1.1b1.2b2.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.50correctness points tool ecl excltool codecitytimea1a2.1a2.2a3a4.1a4.2b1.1b1.2b2.1t1 codecity larget2 codecity mediumt3 ecl excl larget4 ecl excl medium7.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
a1a2.1a2.2a3a4.1a4.2b1.1b1.2b2.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.39time minutes tool ecl excltool codecity .
.
.
.
.
a1a2.1a2.2a3a4.1a4.2b1.1b1.2b2.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.29time minutes tool ecl excltool codecity system size largesystem size mediumsystem size medium system size largelegend correctnessa1a2.1a2.2a3a4.1a4.2b1.1b1.2b2.1t1 codecity larget2 codecity mediumt3 ecl excl larget4 ecl excl medium0.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
a1a2.1a2.2a3a4.1a4.2b1.1b1.2b2.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.55correctness points tool ecl excltool codecity .
.
.
.
.
a1a2.1a2.2a3a4.1a4.2b1.1b1.2b2.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.50correctness points tool ecl excltool codecitytimea1a2.1a2.2a3a4.1a4.2b1.1b1.2b2.1t1 codecity larget2 codecity mediumt3 ecl excl larget4 ecl excl medium7.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
a1a2.1a2.2a3a4.1a4.2b1.1b1.2b2.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.39time minutes tool ecl excltool codecity .
.
.
.
.
a1a2.1a2.2a3a4.1a4.2b1.1b1.2b2.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.29time minutes tool ecl excltool codecity system size largesystem size mediumsystem size medium system size largelegend large object system correctnessa1a2.1a2.2a3a4.1a4.2b1.1b1.2b2.1t1 codecity larget2 codecity mediumt3 ecl excl larget4 ecl excl medium0.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
a1a2.1a2.2a3a4.1a4.2b1.1b1.2b2.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.55correctness points tool ecl excltool codecity .
.
.
.
.
a1a2.1a2.2a3a4.1a4.2b1.1b1.2b2.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.50correctness points tool ecl excltool codecitytimea1a2.1a2.2a3a4.1a4.2b1.1b1.2b2.1t1 codecity larget2 codecity mediumt3 ecl excl larget4 ecl excl medium7.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
a1a2.1a2.2a3a4.1a4.2b1.1b1.2b2.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.39time minutes tool ecl excltool codecity .
.
.
.
.
a1a2.1a2.2a3a4.1a4.2b1.1b1.2b2.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.29time minutes tool ecl excltool codecity system size largesystem size mediumsystem size medium system size largelegend correctnessa1a2.1a2.2a3a4.1a4.2b1.1b1.2b2.1t1 codecity larget2 codecity mediumt3 ecl excl larget4 ecl excl medium0.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
a1a2.1a2.2a3a4.1a4.2b1.1b1.2b2.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.55correctness points tool ecl excltool codecity .
.
.
.
.
a1a2.1a2.2a3a4.1a4.2b1.1b1.2b2.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.50correctness points tool ecl excltool codecitytimea1a2.1a2.2a3a4.1a4.2b1.1b1.2b2.1t1 codecity larget2 codecity mediumt3 ecl excl larget4 ecl excl medium7.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
a1a2.1a2.2a3a4.1a4.2b1.1b1.2b2.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.39time minutes tool ecl excltool codecity .
.
.
.
.
a1a2.1a2.2a3a4.1a4.2b1.1b1.2b2.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.29time minutes tool ecl excltool codecity system size largesystem size mediumsystem size medium system size largelegend correctnessa1a2.1a2.2a3a4.1a4.2b1.1b1.2b2.1t1 codecity larget2 codecity mediumt3 ecl excl larget4 ecl excl medium0.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
a1a2.1a2.2a3a4.1a4.2b1.1b1.2b2.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.55correctness points tool ecl excltool codecity .
.
.
.
.
a1a2.1a2.2a3a4.1a4.2b1.1b1.2b2.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.50correctness points tool ecl excltool codecitytimea1a2.1a2.2a3a4.1a4.2b1.1b1.2b2.1t1 codecity larget2 codecity mediumt3 ecl excl larget4 ecl excl medium7.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
a1a2.1a2.2a3a4.1a4.2b1.1b1.2b2.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.39time minutes tool ecl excltool codecity .
.
.
.
.
a1a2.1a2.2a3a4.1a4.2b1.1b1.2b2.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.29time minutes tool ecl excltool codecity system size largesystem size mediumsystem size medium system size largelegend figure average correctness and completion time per task b1.
identifying the god class with the highest number of methods.
both tools obtained good correctness scores i.e.
over .
excel was slightly better in the case of the medium size system while codecity outperformed excel in the case of the large system.
while codecity s performance was consistent across systems with different sizes excel s support was slightly more error prone in the case of a larger system which implies the handling of more data.
b2.
identifying the dominant class level design.
in terms of correctness codecity outperformed excel regardless of system size.
the aggregated data found in codecity s disharmony map was less error prone than counting rows in excel.
in terms of completion time codecity significantly outperformed excel and the difference was probably caused by the lack of overview in excel.
summary.
as expected at focused tasks e.g.
a4.
b1.
codecity did not perform better than the baseline because excel is very efficient in finding precise answers e.g.
the largest the topn .
however it is surprising that in most of these tasks codecity managed to be on par with excel.
at tasks that benefit from an overview e.g.
a2.
a3 orb1.
codecity constantly outperformed the baseline in particular in terms of correctness mainly because the overview enabled the experimental group to produce a faster and more confident solution compared to the control group.
.
threats to v alidity .
internal validity the internal validity refers to uncontrolled factors that may influence the effect of the treatments on the dependent variables.
subjects.
to reduce the threat that the subjects may not have been competent enough we ensured they had expertise in relevant fields using an online questionnaire.
second to mitigate the threat that the subjects expertise may not have been fairly distributed across the control and experimental groups we used randomization and blocking to assign treatments to subjects.
tasks.
the choice of tasks may have been biased to the advantage of codecity.
we alleviate this threat by presenting the tasks in context with a rationale described in and left out here due to space constraints and the targeted user roles.
moreover we included tasks which disadvantage codecity e.g.
tasks focused on precision rather than on locality .
second the tasks may have been too difficult.
third the allotted time per task may have been insufficient.
to alleviate these two threats we performed a pilot study and collected feedback about the perceived task difficulty and time pressure.
moreover we excluded the only task whose difficulty was discordantly perceived by the two groups.
in addition this task was the only one that showed a ceiling effect i.e.
most subjects used up the entire time for the affected group.baseline.
we compared codecity with a baseline composed of two different tools and this might have affected the performance of the control group.
we attenuate this threat by designing the task set such that no task requires the use of both tools.
moreover all the tasks that were to be solved with eclipse were grouped in the first part of the experiment while all the tasks that were to be solved with excel were grouped in the second part of the experiment.
this allowed us to minimize the effect of switching between tools to only one time between tasks a3anda4.
.
the good scores obtained by the ecl exl subjects on task a4.
in both correctness and time provide no indication of such a negative effect.
data differences.
codecity relies on models of the systems while eclipse works with the source code.
the data differences might have had an effect on the results of the two groups.
to alleviate this threat we accurately produced the answer model based on the available data source i.e.
source code or model and made sure that the slight differences did not lead to incompatible answers.
session differences.
there were several runs and the differences among them may have influenced the result.
to mitigate this threat we performed four different sessions with nine subjects in total during the pilot phase and obtained a stable and reliable experimental setup e.g.
instrumentation questionnaires experimental kit .
moreover there were four industry practitioners who performed the experiment remotely controlled merely by their conscience.
given the value of data points from these practitioners and the reliability of these particular persons i.e.
one of the experimenters knew them personally we trusted them without reservation.
training.
we only trained the subjects with the experimental treatment and this may have influenced the result of the experiment.
we afforded to do so because we chose a baseline tool set composed of two state of the practice tools and we made sure that the control subjects had a minimum of knowledge with eclipse.
although many of the control subjects remarked the fact that we should have included excel among the assessed competencies they scored well on the tasks solved with excel due to the rather simple operations i.e.
sorting arithmetic operations between columns required to solve them.
as many of the codecity subjects observed one hour of demonstration of a new and mostly unknown tool will never leverage years of use even if sparse of popular tools such as eclipse or excel.
.
external validity this refers to the generalizability of the experiment s results.
subjects.
to mitigate the threat of the representativeness of the subjects we categorized our subjects in four categories along two axes i.e.
background and experience level and strived to cover all categories.
we obtained a balanced mix of academics beginners and advanced and industry practitioners only advanced .tasks.
our choice of tasks may not reflect real reverse engineering situations.
we could not match our analysis with any of the existing frameworks because they do not support design problem assessment and in addition are either too low level e.g.
the questions asked by practitioners during a programming change task by sillito et al.
or biased towards dynamic analysis tools e.g.
pacione s framework .
to alleviate this threat we complemented our tasks with usage scenarios and targeted users.
object systems.
the representativeness of our object systems is another threat.
we chose to perform the experiment with two different object systems in spite of the added complexity in organizing the experiment and analyzing the data introduced by a second independent variable.
the two object systems we chose are well known open source systems of different realistic sizes and of orthogonal application domains.
it is not known how appropriate they are for the reverse engineering tasks we designed but the variation in the solutions to the same task shows that the systems are quite different.
experimenter effect.
one of the experimenters is also the author of the approach and of the tool.
this may have influenced any subjective aspect of the experiment.
one instance of this threat is that the task solutions may not have been graded correctly.
to mitigate this threat the three authors built a model of the answers and a grading scheme and then reached consensus.
moreover the grading was performed in a similar manner and two of the three experimenters graded the solutions blinded i.e.
without knowing the treatments e.g.
tool used to obtain the solutions.
even if we tried to mitigate this threat extensively we cannot exclude all the possible influences of this factor on the results of the experiment.
.
conclusion we presented a controlled experiment aimed at evaluating our software visualization approach based on a city metaphor.
we designed our experiment from a list of desiderata built during an extensive study of the current body of research.
we recruited large samples of our target population with subjects from both academia and industry with a broad range of experience.
the main result of the experiment is that our approach leads to an improvement in both correctness and completion time over the state of the practice exploration tools.
this result is statistically significant.
we believe this is due to both the visualization as such as well as the metaphor used by codecity but we can not measure the exact contribution of each factor.
apart from an aggregated analysis we performed a detailed analysis of each task which provided a number of insights on the type of tasks that our approach best supports.
unsurprisingly in the case of focused tasks i.e.
tasks which require very precise answers codecity did not perform better than eclipse excel.
surprisingly for most of these tasks our approach managed to be on a par with excel.
as for the tasks that benefit from an overview of the system codecity constantly outperformed the baseline.
finally we designed our experiment with repeatability in mind.
in we provided the complete raw and processed data of this experiment i.e.
the pre experiment in experiment and debriefing questionnaires solution oracles and grading systems correction scores and measured completion time to allow reviewers to better evaluate the experiment s design and results and fellow researchers to repeat the experiment or reuse its design as a base for their own.