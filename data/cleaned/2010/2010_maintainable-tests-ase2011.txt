scaling up automated test generation automatically generating maintainable regression unit tests for programs brian robinson abb corporate research brian.p.robinson us.abb.commichael d. ernst university of washington mernst cs.washington.edujeff h. perkins mit csail jhp csail.mit.eduvinay augustine abb corporate research vinay.augustine us.abb.comnuo li abb robotics nuo.li cn.abb.com abstract this paper presents an automatic technique for generating maintainable regression unit tests for programs.
we found previous test generation techniques inadequate for two main reasons.
first.
they were designed for and evaluated upon libraries rather than applications.
second they were designed to find bugs rather than to create maintainable regression test suites the test suites that they generated were brittle and hard to understand.
this paper presents a suite of techniques that address these problems by enhancing an existing unit test generation system.
in experiments using an industrial system the generated tests achieved good coverage and mutation kill score were readable by the product s developers and required few edits as the system under test evolved.
while our evaluation is in the context of one test generator we are aware of many research systems that suffer similar limitations so our approach and observations are more generally relevant.
i. i ntroduction the benefits of unit and regression testing are widely recognized.
these tests reveal defects that would be difficult and expensive to find and fix in later development phases verify that changes to the code do not break existing functionality and are integral to practices such as test driven development and continuous integration.
a key obstacle to adopting unit testing is the costs to develop and to maintain a unit test suite.
the cost to manually develop a regression unit test suite for an existing codebase most of which have no such tests is usually prohibitive.
in addition if the original developers are not involved in creating the tests the team creating the tests must determine the correct behavior of the code on their own.
and developers are often more interested in and better rewarded for developing functionality than test code.
an alternative to creating an entire regression test suite is to incrementally create unit tests for new code as it is developed.
this fails to address the bulk of the existing codebase and regression defects often make up a significant percentage of the defects detected in a new release.
a recent internal study of a product at abb found that of post release defects over the last three years were either latent defects injected many releases previously or regression defects due to changes in the code.
for this product creating unit tests for only new code would provide very little perceived benefit.
the cost to maintain a regression unit test suite is also a problem.
as the software evolves over time existing testsmust be updated to reflect changes in behavior or in the product s specification.
there is a tradeoff between sensitivity and brittleness a test suite that detects many erroneous changes is also likely to issue false positive failures for many desired changes.1a developer must update or remove each failing test and if updating is difficult or tedious the temptation to remove it becomes too great to resist.
ten years ago a product development team at abb focused on creating a good automated unit test suite.
at substantial cost they created a suite that achieved statement coverage.
but maintaining that suite was too costly due to brittle and difficult to understand test cases and it decayed over time.
today it has statement coverage and is rarely run.
this example shows that the coverage of a test suite alone does not make it successful but rather the balance between coverage and maintainability.
the costs of developing and maintaining tests are not perceived to justify the benefits of unit testing.
as a result adequate adoption of unit testing by large industrial software development organizations remains low .
our goal in this work is to tip the balance of the cost benefit tradeoff by automatically generating maintainable regression tests forreal software programs .
existing test generation techniques such as random or concolic testing have limitations that prevent them from fulfilling either part of the goal generate maintainable regression tests orreal software programs .
for example existing tools focus on detecting crashes or exceptions.
while this is helpful in finding new defects a maintainable regression suite needs test oracles that determine if a new version of the software behaves differently than the previous version.
in addition existing tools have been evaluated on library code rather than real software programs .
as we further describe in section iii libraries are easier for tools to handle and do not contain many of the complexities that real software programs have.
for example real software programs contain persistent state and make use of external code which prevent existing concolic testing techniques from working.
finally to be industrially practical a test generation technique should 1an example false positive failure is requiring a new version s output to be identical to the previous version s output even though the program s specification permits multiple behaviors such as allowing iteration through a set in an arbitrary order.work without human interaction and without assuming a formal specification or test oracle.
since existing techniques do not meet our goal we decided to extend a test generation technique.
we chose randoop an implementation of feedback directed random testing as the most practical available test generation tool.
we did not use symbolic concolic techniques such as java pathfinder because of their limitations when applied to real systems .
our ideas would be equally applicable to any attempt to scale up such techniques to generate regression tests for real programs.
this paper makes two main contributions.
we present a suite of enhancements to the feedbackdirected random testing approach and to the randoop tool.
these enhancements address the challenge of creating effective maintainable regression tests for real software programs.
the enhancements are publicly available at each enhancement is conceptually simple but is sometimes subtle in its application and their combination is novel.
we evaluated our extensions in the context of a large mature industrial software system.
the generated tests have good code coverage and mutation score better than the manually written tests that the development team runs each week.
although the generated tests cover much of the code and are sensitive to many faults and even exposed a previously unknown bug they are easy for a developer to maintain requiring just edits to accommodate years and public versions of changes.
the rest of this paper is organized as follows.
section ii reviews the feedback directed random test generation technique and section iii describes how we extended it.
section iv explains our experimental methodology and section v presents the results.
section vi compares our research to related work and section vii states our conclusions and identifies future work.
ii.
r andoop our work extends the randoop tool which automatically generates unit tests.
randoop implements a technique called feedback directed random testing .
in feedback directed random test generation a test is built up iteratively.
each iteration randomly selects a method or constructor to invoke using previously computed values as inputs.
the technique uses feedback obtained from executing the sequence as it is being constructed in order to guide the search toward sequences that yield new andlegal object states.
the key idea of feedback directed testing is to execute each test as soon as it is generated and to use information gathered from that execution to bias the test generation process as it creates further tests.
the bias makes the test generator less likely to generate illegal tests and less likely to generate redundant tests.
in particular inputs that create redundant or illegal states are never extended which has the effect of pruning the search space.
an object oriented unit test consists of a sequence of method calls that set up state such as creating and mutating objects public class a public a ... public b m1 a a1 ... public class b public b int i ... public void m2 b b a a ... pool of previously constructed sequences b b1 new b b b2 new b a a1 new a b b3 a1.m1 a1 possible extensions b b1 new b a a1 new a b b3 a1.m1 a1 b1.m2 b1 a1 a a1 new a b b3 a1.m1 a1 b b1 new b b1.m2 b1 a1 b b1 new b b b2 new b b1.m2 b2 null fig.
.
example code and method call sequences that could be used in the body of a unit test for the code.
the bottom three sequences show three possible extensions if randoop chooses to extend sequences in the pool so as to make a sequence that ends with m2.
adapted from .
and at least one assertion about the result of the final call.
randoop creates such sequences iteratively it creates test cases out of short sequences then extends the sequences to create larger test cases.
to create a new test case randoop needs to choose a method to test as well as arguments and assertions.
see figure for an illustration.
to choose the method randoop uses random choice among the public methods of the classes under test.
to aid in choosing the arguments randoop maintains a pool of all values that have been generated by any test case so far along with the sequence of calls that created the object.
randoop initializes the pool with a set of values such as null a true etc.
for each argument to the chosen method randoop selects a random value of the proper type from the pool.
randoop creates assertions that detect errors.
randoop can optionally create a regression test that detects deviations from previous behavior by adding assertions about the final state of the object after the test completes.
to detect errors randoop uses the following built in set of assertions.
no method should throw assertionerror .
no method should throw nullpointerexception if none of its arguments was null.
some methods such as equals hashcode and tostring should never throw an error.
for any object o.equals o should return true.
a user may extend these built in assertions with domain specific properties but this is not required.
iii.
e nhancements to randoop randoop was originally targeted toward detecting existing bugs in data structure libraries libraries such as the jdk s java.util .
instead we want to extend randoop to generate maintainable regression tests for complex industrial software systems.
data structure libraries tend to be easier for tools to handle in several ways.
most library methods are deterministic.
nondeterministic results tend to fall into simple patterns such as the default return value of tostring whichprints a unique identifier similar to a hash code.
there is little or no global state global state can give the appearance of non determinism.
most utility library methods do not make changes to the underlying system or the outside world such as databases .
as a result the methods can be called in any order and most classes do not have to be initialized before they can be used.
most utility library methods do not interact with the external environment.
by contrast a program s code may query the user for information exit the program etc.
however this behavior is not acceptable in an automated regression test.
furthermore libraries undergo few code changes and even fewer behavior changes to existing functionality.
by contrast the test suite of an evolving program may need to be updated frequently during development.
because the library is written generically simple arguments can exercise all of its behavior and that behavior depends on only simple properties of the arguments such as equality and ordering.
by contrast a program manipulates much more complex data and correctness properties may depend on part or all of that data.
a library is less likely to be in need of a regression test suite.
libraries rarely change and a library is likely to already have some tests.
test generation is used chiefly to find bugs as opposed to creating regression tests possibly using a partial specification of permitted behavior.
only failing tests are shown to the user while succeeding tests can be discarded without examination.
readability maintainability and redundancy of tests generated for a library are secondary concerns.
general purpose libraries are less likely than programs to have specific magic constants that must be used in order to exercise certain behaviors.
the following six sections describe enhancements we made to extend randoop to to create maintainable regression tests for real software programs.
the first two changes are needed by any technique to create robust behavior preserving test oracles.
the next two are changes needed by any technique to support testing real software programs.
the final two changes are enhancements specific to randoop that improve its generated tests no matter what context randoop is used it.
all of these enhancements are publicly available and documented at http randoop.googlecode.com .
a. remove non deterministic observations a method can return different results on different invocations.
this can happen because of nondeterminism concurrency the current time dependence on hash codes the memory management system etc.
but is more often caused by dependence on program state that is exposed by a different ordering of calls.
randoop needs to ensure that its tests do not depend on the results of apparently non deterministic methods because such a test would produce many false positive failures.a concrete example from our case study is a static variable of type vector randoop calls vector methods such as add remove and size on its value and the order of such calls affects their return values.
this is not just a problem in randoop generated tests manually written test suites also have data dependences that make results change if tests are run in a different order .
during test generation randoop executes each statement in a test and records certain outcomes as assertions.
these assertions may fail when the test suit is run.
as described in section ii randoop discards possible method sequences as it is running that lead to redundant object states exceptions also discussed in section iv a or other error conditions.
these sequences may have modified the global state in ways that other generation time observations depended upon.
randoop previously addressed the problem of test outcome nondeterminism using a very simplistic approach it discarded did not place in the pool string values that contained the pattern typical of object.tostring .
this was sufficient for certain libraries on which randoop had been evaluated but not for real programs.
we developed a new approach.
now after generating an entire test suite randoop runs it before outputting it.
if any test fails randoop assumes the failing test is nondeterministic and retains the test in the suite but disables each failing assertion.
alternative approach an alternative approach we considered would remove any non deterministic method call from the test suite.
this approach would yield a smaller test suite since the suite does not contain calls whose results are ignored.
this simple approach does not work because removing a method call in one test can cause a subsequent putatively independent test to fail.
this is because tests can interact through shared variables side effects in one test cause differences to the behavior of subsequent tests.
even changing the order of tests in a suite or equivalently performing test case selection or prioritization on the suite can cause other tests to fail.
for example if an array is smaller than it was on a previous test execution a test that previously succeeded could now throw a index out of bounds exception.
in conclusion apparently non deterministic behavior cannot be removed from a suite as its removal could easily lead to other changes in the global state and further spuriously failing tests.
b. prevent changes to the underlying system application code may depend on and make changes to the underlying system e.g.
add delete or modify files in the file system .
putting aside for the moment the fact that this may lead to nondeterministic behavior automatically created unit tests may perform unexpected or even catastrophic changes to the computer system.
to prevent this problem randoop now uses a java security manager.
a security manager can prevent certain operations and is useful in contexts beyond preventing security breaches.
in our case study the security manager gives permissions to specific properties and the socket used to connect to the database.a second related change is that randoop now provides an option for its user to specify setup code that should be performed at the beginning of testing or of running a test.
this was necessary in our case study as the program connects to a database and performs other initialization activities.
c. modify inappropriate calls application code may interact with the user such as creating a dialog box in a variety of different situations.
in our case study which targeted the logic connected to a gui this was often only to confirm a particular action.
if such calls occur during either test generation or test execution the tests cannot be run without manual intervention.
we added a feature to randoop that allows a user to specify a mapping from current method calls to a replacement call.
for example the javax.swing.joptionpane .showmessagedialog method which usually presents a dialog box can be replaced with a call that simply prints out the message and returns.
the feature works via bytecode rewriting.
in the experiments we used this feature to remove dialog boxes that require a response.
we also removed calls to system.exit .
we found another compelling use for this feature.
during test maintenance it can be used to understand regression test errors that result from behavioral changes.
a developer can selectively on a per method basis revert application behavior back to the behavior in the previous software release in order to reproduce old behavior cause a failing regression test to succeed again and thereby verify exactly what changes caused a regression test failure.
for example in our case study we discovered that between versions the developers changed the behavior of a method so that it created two columns in a database rather than one.
this simple change caused a relatively large number of failures in the automatically generated regression unit tests.
we mapped each of these calls to a wrapper routine that converted to the old arguments and results.
the complete code required was lines long.
once the regression tests have been verified on a new release including any possible changes to the new version due to errors that were uncovered a tester has two choices.
the tester can update the suite or the tester can simply discard it and use randoop to build a new regression suite for the new version.
the wrapper routines are not a permanent fixture of the tests but are only used when debugging failing tests on a newer software version.
d. observe pure methods as described in section ii randoop tests that a result has the expected value.
for example assert result.equals expected using an existing equals method may be too strict because equals checks undesired fields or data or even implements reference equality or too lenient because equals skips important information .we modified randoop to apply a set of user defined observer methods to the value and check their results.
an observer method is a method with no side effects.
thus instead of having a single assertion at the end of a generated test there may be many assertions at the end one for each applicable observer method.
for example assert result.f1 .equals expected.f1 assert result.f2 .equals expected.f2 assert result.f3 .equals expected.f3 we define an observer method as a pure nullary non void method that is it has no side effects it takes no arguments except the receiver and it returns a value.
for instance a getter method such as point.getx is an observer method.
randoop takes as input a set of observer methods a user could mark these manually or use an automated analysis to compute them.
randoop utilizes observer methods in a second way to avoid making no op method calls.
randoop conservatively assumes that any method may side effect any of its arguments.
without this assumption randoop would never call a void method!
another way of saying this is that randoop treats each method as having multiple outputs its return value and the final values of its non primitive arguments.
however this can lead to useless calls in the middle of a sequence if the call has no side effect.
randoop can use knowledge about lack of side effects to ignore some of these possible outputs.
randoop s equality testing can often detect lack of side effects but our approach is more direct and efficient.
e. filter lexically redundant tests randoop builds larger tests out of smaller ones.
this means that every time randoop adds a test the new test lexically subsumes at least one and perhaps many shorter tests.
for example suppose that methods a through d perform side effects.
randoop might output these four tests a a b a b c a b c d the first three tests are lexically redundant.
as a heuristic we enhanced randoop to remove them from the test suite that it generates.
if randoop is run with a time limit then removing lexically redundant tests creates smaller and easier to understand suites.
if randoop is run with a size goal number of tests to output then removing lexically redundant tests rather than outputting them has three effects.
generation time increases slightly.
the test run time also increases marginally since the tests tend to be slightly larger on average the smaller ones were removed .
most importantly test suite quality improves because the resulting suite has more diversity than it would have had otherwise.
removing lexically redundant tests is a heuristic because the removed tests are not necessarily semantically redundant.
in the example above it is possible that a has internal state and that a fault occurs only after calling it four times also see section iii a .
in this case removing the first three testswould reduce the fault detection capability of the overall test suite.
we have never observed this effect in practice.
even though the redundant tests are removed from the generated test suite they are kept in the pool to permit creating for example a b e .
we implemented this feature as a response to abb developers objections to the original randoop generated tests.
when industrial developers examined the tests they identified and objected to lexical redundancies in the tests.
they thought the redundancies might negatively impact maintainability by making some tests needlessly longer and by increasing the number of redundant test failures when the software evolves.
f .
use source code literals as arguments a constant that appears in the source program may be relevant to its behavior.
for example a program may have different behavior when a data structure is larger vs. smaller than a given threshold.
a parser may expect a string to start with a given sequence of characters the parser may not exhibit interesting behavior for malformed inputs.
test code may define a special user and password without which certain functionality cannot be accessed.
an integer may be used as an enumeration to control behavior.
randoop previously used only a small set of constant values for numbers and for characters and a and for strings and hi!
.
this may be adequate to exercise much of a utility library but is inadequate to exercise a real program.
we enhanced randoop to add to the initial pool any constants that appear in the source code under test.
suppose a constant appears in class package.class .
as a user option randoop can use the constant only when generating tests within class package.class for any tests in package package or anywhere.
our experiments use the package choice.
using the source code literals indiscriminately was not effective because a realistic program has many literals and the pool would be so large that the likelihood of choosing the right one for any particular method would be vanishingly small.
using the class granularity was suboptimal as well we saw cases in which a literal in one class needed to be supplied as as argument to a method in a different class that might eventually call the first one .
iv.
s tudy design we conducted a study with two main goals.
.
determine the impact of our randoop enhancements section iii on the quality of the generated tests.
to determine the improvement due to each added feature we generated test suites with and without the added features enabled.
as a measure of quality we used statement coverage and mutation kill score see section iv d .
these two metrics are believed to be correlated with defect detection.
.
determine how maintainable the randoop generated regression test suite is.
to measure this we used randoop to create a test suite for one version of a system ran the test suite on a subsequent version of the system and determinedhow much effort was required to return the suite to the passing state.
when generating tests we used all of randoop s defaults including its maximum of seconds of test generation time per class.
sections iv a and iv b expand on our methodology for assessing quality and maintainability respectively.
a. measuring quality we refined the quality goal into these research questions can randoop create an effective regression suite for a non library program?
does using string literals from the program improve the effectiveness of the generated tests?
does removing lexically redundant tests improve the effectiveness of the generated test suites?
does using observer methods improve the effectiveness of the tests generated?
which has greater impact on test effectiveness larger test suites or longer test cases?
the results for these research questions are shown in section v a. we evaluated the first research question by measuring the quality of randoop generated tests and by comparing them to good quality human generated tests.
we evaluated the next three research questions by running randoop to generate two test suites one with and one without the given feature and comparing the suites.
we used a test suite size of tests per class beyond this size additional tests have negligible effect.
we evaluated the fifth research question by generating and comparing test suites of different sizes.
we evaluated each pair of suites based on their statement coverage andmutation kill score .
we also computed branch coverage but the results were similar so this paper omits those measurements for brevity.
we computed statement coverage for all classes and computed mutation scores for classes see section iv d for justification .
it would be better to directly measure defect detection the number of previously unknown errors that each test suite reveals.
this is infeasible to compute for most real software systems the lack of a formal specification makes it difficult to know whether a given test execution exposes a failure.
for example most exceptions thrown by randomly generated tests are valid behavior caused by illegal inputs.
to avoid a flood of false positives we ran randoop in a mode where it ignored any test that throws an exception.
randoop thus created regression tests that ensured the software s non exceptional behavior did not change.
b. measuring maintainability we refined the maintainability goal into these research questions how much editing is required to maintain the generated tests through a major version change?
does removing lexically redundant tests improve the maintainability of the generated test suites?
what is developers opinion of the readability and maintainability of the generated tests?
the results for these research questions are shown in section v b. we evaluated test maintainability by using the generated unit test suite as a regression test suite that is we ran it on a later version of the software.
any failures on this later version are due to a behavioral change in the software either a regression defect or an intended change.
a test failure due to intended behavioral changes is seen as a false positive by developers.
there is a tradeoff between the sensitivity of a test suite and its brittleness.
ideally a test suite should detect many defects but should issue few false positives when the code changes over time.
a suite that misses too many defects is useless for testing and a suite that issues too many hard tointerpret false positives will be too hard to maintain and will be abandoned by developers.
similar to many other test generation strategies randoop has many parameters that affect the test suites that it generates.
we measured the effect of three parameters that we believed based on our previous experience would be important test suite size test case size and test suite run time.
in addition to a quantitative analysis of test suite maintenance we asked the developers to evaluate the generated test suite qualitatively as the acceptance of the test suite by the development teams is critical.
c. industrial system studied our experiments use a subject program that we call rata.
rata is a mature real time monitoring and control product developed at abb.
rata contains two parts of approximately equal size.
the control algorithms and connectivity components in the system are written in c. the business logic database connectivity and ui are written in java and consist of total classes.
of these classes only are testable as classes are interfaces and are gui components.
the testable classes are the focus of this study and contain 127k lines of code.
rata has above average quality when compared to other abb products with regards to field defects.
the product does not currently have a unit level regression test suite.
release testing currently finds the majority of the defects.
the development team would like to have a unit test suite but there is no business case for manually writing one.
the rata development team does have a black box regression suite they run manually each week.
we call this the manual suite .
d. mutation analysis the mutation kill score is computed by mutation analysis.
mutation analysis constructs many variants of a program each of which differs from the original in some small respect such as by replacing one instance of by .
the variants are called mutants and the code changes are called mutations .
given a set of mutants a test suite s mutation score or kill score is the fraction of mutants that are detected by the test suite.
a mutation score is computed by the following equation killed mutants total mutants.
a killed mutant is a mutation that is detected by a test in the suite.
the main purpose of mutation is comparing two test suites the suite with the larger score is better.
creating mutants for all classes of rata and evaluating all of the generated test suites against them would be computationally infeasible.
therefore we asked the rata developers to identify the package they see as the most critical.
we performed mutation analysis on only this package.
randoop works on all of rata and our other results use all testable classes.
we created mutants using mujava version and the muclipse plugin for the eclipse ide.
however mujava does not work on classes that contain gui components.
it created mutants for of the classes in the package.
therefore our mutation results in this paper are for those classes.
we now describe our configuration of mujava.
for consistency with other research papers we used only mujava s method code level mutation operators not its class level mutation operators.
furthermore we reduced the number of equivalent mutants by disabling mujava s aois mutation.
mujava created mutants from the mutated classes.
we evaluated these mutants against four test suites from classes resulting in over test executions.
executing all of these tests on all of the mutants took a quad core server executing continuously over one month of calendar time.
e. experimental setup to evaluate test suite size we ran randoop to generate three different test suites containing and tests per class .
we ran randoop with an arbitrary but consistent random seed so for a given class a smaller suite is a subset of a larger suite.
each suite includes some non consistent tests non deterministic observations that are disabled as discussed in section iii a .
thus the number of tests that are run is about smaller than the number of tests in the suite.
in these suites test case size is conflated with test suite size larger suites contain on average larger tests.
the reason is that to create a new test randoop extends an existing test.
therefore randoop tends to add larger and larger test cases over time.
to control for this factor we created an additional test suite of size by choosing the last500 tests that were added to the test suite.
it contains the same number of tests as the test suite but each test is larger on average.
we call this suite the last suite.
f .
threats to validity internal validity involves influences that can affect the results without the researcher s knowledge.
because of computational cost the mutation analysis was only performed on the package selected by the rata developers.
other packages might provide different results.
threats to external validity are conditions that limit the generalization of the results.
the primary threat here involves the use of only one industry program which will not be50 verage by class500 last 501statement coverage by class classes500 last manualfig.
.
coverage is nearly the same for each of the generated test suites and is much better than the manual suite which covers only a few classes.
y class500 last500 1mutation kill score by class classes500 last manual fig.
.
mutation coverage is similar for the generated test suites and is much better than for the manual suite.
representative of all industrial programs.
to address this threat we included jhotdraw an open source program in the maintainability analysis.
future work will be needed to verify that the technique provides similar results on other industrial software systems.
threats to construct validity arise when measurement instruments do not properly capture what they are intended to capture.
we addressed this threat in a few ways.
first the end to end process of generating the tests executing them and calculating the results was automated providing a consistent process for each of the dependent variables in the study.
second we ran randomly selected tests outside the automated process to verify the results were correct and consistent.
finally some values and measures between separate test runs are not affected by the dependent variables and we checked them to verify consistency between test runs.
v. e xperimental results this section presents the results of the studies described in section iv.
a. test suite quality randoop for non library programs to address the first research question we present the results of the best test suite that our enhanced randoop tool generated.
this suite was generated by randoop with string literals and removing lexically redundant tests enabled and observer functions disabled.
the best test suite size when consideringtest generation time execution time mutation scores statement coverage and maintainability is the last tests suite.
this suite retains over of the coverage and mutation scores of the full test suite while executing more quickly and reporting fewer redundant errors when valid behavioral changes are made to the software over time.
this suite averages statement coverage and mutation kill score.
figures and show the coverage and mutation data at the class level sorted from highest to lowest.
these results answer the first research question by showing that randoop can provide the rata development team an automatically generated regression test suite that covers more than half the code and is able to detect a large number of potential defects.
the remaining subsections present the answers for the other research questions listed in section iv a .
these answers also provide the rationale for the randoop settings used in this section.
effectiveness of string literals we calculated coverage and mutation scores for test suites generated with and without string literals support.
for the vast majority of the classes in rata the impact of string literals on statement coverage was quite small.
there are only six classes where the statement coverage was improved by more than .
by contrast string literals have a large impact on mutation kill scores.
across all of the classes of rata the mutation kill scores are approximately better.
six classes had mutation scores go from to and five other classes improved by over .
on the other hand two classes had a loss of around for their mutation kill scores.
these two classes only check strings for null which randoop now uses less frequently due to its larger pool of string values.
effectiveness of removing lexically redundant tests we created a test suite with randoop configured to remove lexically redundant sequences before outputting a suite of the specified size and compared it to a second set of tests created without removing redundant sequences.
the difference in statement coverage between the two suites is quite small only three classes had any difference.
the mutation score on the other hand increases slightly for eight classes while the remaining classes have no difference.
these results make sense as removing lexically redundant tests is supposed to reduce test suite size without materially affecting test effectiveness.
effectiveness of observers to determine the effectiveness of specifying observer functions to randoop we randomly selected classes from the mutated package and manually determined which of their methods were observers.
for a description of observer methods see section iii d .
we then used randoop to generate test suites of all four suite sizes both with and without the observers specified.
specifying observers to randoop has very little impact on statement coverage which is very high to start with.
this makes sense as the observers do not heavily impact test sequences only the values randoop uses for the test oracle.
observers have a larger impact on mutation score at smaller test suite sizes.
the results show that the test suite has a noticeabledifference in mutation score while the test suite shows a smaller difference and finally the and last test suites show a negligible difference.
these graphs are omitted due to space limitations.
specifying observers to randoop for small test suites can triple the number of assert statements each test contains.
these additional asserts at the small test suite level are responsible for the increase in mutation score when observers are used.
in a larger suite randoop is more likely to have called all observers via its ordinary selection of random methods.
there is little benefit from the explicit observer calls that randoop adds when the user specifies observer methods.
effectiveness of test suite size and test case length this study involved evaluating the four different sized randoop test suites as well as the manually written test suite on rata and computing the statement coverage and mutation scores.
the results are shown in figures and .
test suite size only slightly impacts statement coverage.
for a given class the average difference in statement coverage is only .
the last tests provide the same statement coverage as the entire test suite.
this shows that test case length is more important for statement coverage than test suite size.
the mutation kill scores are influenced more by test suite size.
the scores increase the most between and tests and then the improvement decreases sharply for the test suite.
for the majority of the classes the last suite has the same mutation score as the test suite showing that test case length is also more important than test suite size for mutation kill score.
the generated tests perform significantly better than the manually written tests for both coverage and mutation score.
the randoop test suites also execute significantly faster than the manual suite since the manual suite is manually executed by clicking elements in the ui.
overall the randoop test suite will be a much better regression test suite for the rata developers.
b. test suite maintenance in order to determine how much effort is required to maintain the generated test suites we conducted experiments on rata and on jhotdraw an open source program commonly used in empirical studies in software engineering.
we evaluated the maintainability of these tests in a few ways.
first we determined the effort required to maintain a randoop test suite on both rata and jhotdraw section v b1 .
next we determined the impact that removing lexically redundant tests has on maintainability section v b2 in rata.
finally we asked the rata developers to examine the tests by hand to offer a qualitative analysis section v b3 .
maintainability of the regression suite for this evaluation we ran the best randoop regression suite described in section v a1 on rata version xand version x .
using the same parameters we created a test suite for jhotdraw version 1and ran it against version 2and5 .
since the test suite is generated from a base version of the project each subsequenttest tests behavior false program tests failures failed defects changes positives rata x .
jhotdraw .
.
jhotdraw .
fig.
.
maintenance analysis.
we ran randoop on rata version xand jhotdraw .
to generate tests.
the table reports the number of root causes for the test failures when running those tests on subsequent versions.
in rata version x 3and in jhotdraw .
there were only root causes for all the failures.
version represents a new set of changes that need regression testing.
for rata these two releases represent two years of development effort and two intermediate releases .
the rata files we tested suffered .
code churn that is .
of the source lines were modified between the two releases.
these changes included both bug fixes and new functionality.
for jhotdraw each version was released approximately one year apart.
the code churn from .
to .
was and from .
to .
was .
there are three reasons a regression test may fail on a later version.
a regression defect was introduced during development.
in this case the developer must change the source code to fix the bug.
a desired behavior change causes the software to conform to a different specification but the tests check for the old specification.
in this case the developer must change the test case to accommodate the new behavior.
the software is changed in such a way that it still satisfies the old specification but the test was too strict and checked for the old implementation specific behavior.
this is a false positive and the developer must generalize the test case to accommodate all permitted behaviors.
developers will resent any work required for case and the work for case should be kept to a minimum.
the maintenance burden of a large number of failed tests caused by valid changes may lead developers to stop updating them and they may abandon the regression suite over time.
figure shows the results of running the regression test suites on the later versions of the two programs.
for rata our best randoop test suite had .
of tests failed.
we believe this is a small enough number not to discourage a developer especially because just a few edits can correct all the test failures.
there are just three underlying causes for the failures found in version x .
furthermore in practice regression tests would be run at least daily not once every two years so defects and behavioral changes would be noticed quickly when their impact is small and the code changes are fresh in the developers minds.
once they had investigated the failures developers would regenerate the test suite from the new version of the code.
this causes tests to be created for new functionality while also automatically fixing tests that failed due to changed behavior.
for jhotdraw version 2had .
failures which is close to the results for rata.
version .
was different.
in this case there were failures.
jhotdraw 3was a major change its development site notes that projects created in previous versions may not work in the new version and that they brought together tests behavior false failures failed defects changes positives redundant sequences .
no redundant sequences .
fig.
.
the effect of lexically redundant tests on regression test failures.
three different development versions that forked off of .
the vast majority of the failures were due to removed api functions that are easy to debug.
even with this extreme case of the tests still succeeded.
maintainability of removing redundant tests for this evaluation we ran randoop both with and without elimination of redundant tests as described in section v a3.
we ran the resulting test suites on rata version xand version x .
the results are shown in figure .
when redundant tests are not eliminated they are responsible for failure of .
of the tests.
human assessment of maintainability we showed the randoop generated tests from the test suite without use of observers and with lexically redundant tests to rata developers.
while these developers are familiar with junit and unit testing and would like to have a unit test suite as described in section iv c there is no business case for writing one by hand.
overall the developers found the generated tests to be understandable with acceptable style code format and coverage.
the developers were surprised by the size of each test in the suite the tests were longer than they expected.
the developers may have expected a unit test to consist of a few lines of setup code followed by an assertion or two.
while tutorial examples often look like this much larger tests are required to test complicated behavior.
once they understood the behavior of the tests the developers found the test length acceptable.
when we showed the developers the overall results from our experiments they became very excited.
they are now in the process of incorporating randoop and its tests into their development process.
vi.
r elated work our work builds on the feedback directed random testing technique and on the randoop tool described in section ii.
feedback directed random testing was first proposed and evaluated in the context of the eclat tool which was itself inspired by the operational abstraction approach to measuring test quality.
eclat s main goal is to classify test results in the absence of an oracle or specification so that a human can be directed to examine the most promising ones.
eclat prunes sequences that appear to be illegal because they make the program behave differently than a set of correct training runs.
eclat s test generation differs from randoop in that eclat makes no assumption about a specification or oracle but randoop builds in known tests contract checking such as for the behavior of the equals method.
randoop also has more heuristics for directing the random search and has been ported to c and extensively evaluated finding important errors in a variety of libraries.
our work differs in being focused on regression testing maintainability and realprograms domains for which randoop was not previously suited.
automatic test input generation is an active research area with a rich literature.
we focus on input generation techniques that create method sequences.
random testing random testing has been used to find errors in many applications a partial list includes unix utilities windows gui applications haskell programs and java programs .
jcrasher creates test inputs by using a parameter graph to find method calls whose return values can serve as input parameters.
randoop does not explicitly create a parameter graph instead it uses a component set of previously created sequences to find input parameters.
randoop creates fewer redundant and illegal inputs because it discards component sequences that create redundant objects or throw exceptions.
jcrasher creates every input from scratch and does not use execution feedback so in practice it creates many invalid tests that throw an exception because of illegal input rather than because of a bug in the code under test.
systematic testing many techniques have been proposed to systematically explore method sequences .
bounded exhaustive generation has been implemented in tools like rostra and jpf .
jpf and rostra share the use of state matching on objects that are receivers of a method call and prune sequences that create a redundant receiver.
randoop performs state matching on values other than the receiver and introduces permits a sequence to create some redundant and some nonredundant objects.
only sequences that create nothing but redundant objects are discarded.
rostra and jpf do not favor repetition or use contracts during generation to prune illegal sequences or create oracles.
randoop is scalable but these tools are not rostra was evaluated on a set of small programs loc and jpf s sequence generation techniques were evaluated on data structures neither rostra nor jpf found errors in the tested programs.
an alternative to bounded exhaustive exploration is symbolic execution implemented in tools like symstra xrt jpf dart and jcute .
symbolic execution executes method sequences with symbolic input parameters builds path constraints on the parameters and solves the constraints to create actual test inputs with concrete parameters.
some of these like dart and jcute even integrate random input generation into their symbolic execution approach an idea investigated earlier by ferguson and korel .
randoop is closer to the other side of the random systematic spectrum it is primarily a random input generator but uses techniques that impose some systematization in the search to make it more effective.
check n crash creates abstract constraints over inputs that cause exceptional behavior and uses a constraint solver to derive concrete test inputs that exhibit the behavior.
dsd augments check n crash with a dynamic analysis to filter out illegal input parameters.comparing random and systematic theoretical studies have shown that random testing is as effective as more systematic techniques such as partition testing .
however the literature contains relatively few empirical comparisons of random testing and systematic testing.
ferguson and korel compared basic block coverage achieved by inputs generated using their chaining technique versus randomly generated inputs .
marinov et al.
compared mutant killing rate achieved by a set of exhaustively generated test inputs with a randomly selected subset of inputs.
visser et al.
compared basic block and a form of predicate coverage achieved by model checking symbolic execution and random testing.
in all three studies undirected random testing achieved less coverage or killed fewer mutants than the systematic techniques.
however undirected random testing is a strawman randoop has been found to outperforms systematic techniques.
vii.
c onclusion we have presented an effective fully automatic technique for creating a sensitive yet maintainable regression test suite.
it is applicable to large real world software systems.
it builds on the previously known technique of feedback directed random testing and extends the randoop implementation.
it extends both the technique and the implementation to overcome a variety of limitations that we encountered while using randoop.
our implementation is publicly available under a permissive license at experiments with an industrial software system demonstrated that randoop now creates test suites that have high code coverage and mutation kill score that are comprehensible to developers and that are easy to maintain even when years of changes are applied to the system under test.
we also investigated the effects of various choices for randoop s parameters including test suite size use of observer methods and elimination of lexically redundant tests.
we believe that the result is highly encouraging for the continued use of random testing and feedback directed random testing in particular as one tool in the tester s toolbox.