software process evaluation a machine learningapproachning chen steven c. h. hoi xiaokui xiaoschool of computer engineeringnanyang technological universitysingapore nchen1 chhoi xkxiao ntu.edu.sgabstract software process evaluation is essential to improvesoftware development and the quality of software products inan organization.
conventional approaches based on manualqualitative evaluations e.g.
artifacts inspection are deficientin the sense that i they are time consuming ii they sufferfrom the authority constraints and iii they are often subjective.to overcome these limitations this paper presents a novelsemi automated approach to software process evaluation usingmachine learning techniques.
in particular we formulate theproblem as a sequence classification task which is solved byapplying machine learning algorithms.
based on the framework we define a new quantitative indicator to objectively evaluatethe quality and performance of a software process.
to validatethe efficacy of our approach we apply it to evaluate the defectmanagement process performed in four real industrial softwareprojects.
our empirical results show that our approach iseffective and promising in providing an objective and quantitativemeasurement for software process evaluation.keywords software process defect management process se quence classification machine learningi.
introductiona software process is a coherent set of policies orga nizational structures technologies procedures and artifactsthat are needed to conceive develop deploy and maintaina software product .
most software organizations adoptcertain software process methodologies to access support andimprove their development activities because empirical studieshave shown that the quality of software processes are directlyrelated to the productivity of an organization and the quality ofsoftware products .
one key challenge faced by manyorganizations is how to evaluate a software process performedin a specified scope ranging from a project to a departmentor an entire organization .
this is termed as the softwareprocess evaluation task which is usually done by conductingquestionnaire based assessment or interviewing project teammembers or randomly checking artifacts generated by formersoftware projects e.g.
checking some source codes storedin software configuration management systems or checkingsome bug reports in test management systems etc.
.
theconventional methods are based on subjective and qualitativeinvestigations and have been adopted by the capability matu rity model integration cmmi assessment .despite being widely used the existing software processevaluation methods often suffer from numerous limitations.first they require manual evaluation which is rather usu allytime consuming especially for a software developmentorganization with a large number of projects.
for example itis inefficient to manually examine all the artifacts producedby former projects as the number of such artifacts could beenormous.
although one may circumvent this issue by examin ing only a random sample set of the artifacts the inaccuracyincurred by sampling may cause misjudgments.
second theexisting methods suffer from theauthority constraints.
forexample if an external audit agency or a consultation agencyis asked to evaluate the software processes of an organization it may not be allowed to access every artifact directly dueto privacy or security concerns.
this may lead to inadequatechecking of artifacts which result in inaccurate evaluationresults.
third the existing approaches are often based onsubjective evaluation and hence they suffer from biasedevaluation results due to human factors in the evaluationprocess.
besides the conventional methods usually have a highrequirement of experienced evaluation experts to understandthe specific software processes and design various questionsfor conducting a qualitative evaluation task.to address the above limitations this paper presents a novelsemi automated approach to software process evaluation usingmachine learning techniques.
since software processes canbe broadly referred to a variety of concepts we restrict thediscussion of a software process as a systematic approach tothe accomplishment of some software development tasks inthis paper.
the key idea is to model the series of activities orartifacts states in a software process as sequential instances and then formulate the evaluation task as a binary classificationof the sequential instances into either normal or abnormal .based on this framework we present a new quantitative mea sure referred to as the process execution qualification rate as an objective evaluation of the quality and performance ofa software process.our approach aims to overcome the limitations of con ventional subjective evaluation methods.
first our approachis semi automated which i is more efficient than the con ventional manual methods that requires a significant amountof human efforts and ii alleviates the need of highly ex perienced evaluation experts on process evaluation.
second our approach requires only the process execution history dataand does not access artifacts directly which alleviates the978 .
c ieee ase lawrence ks usa333authority constraints as the process execution history datausually do not contain private contents.
third by adoptingthe process execution qualification rate our approach avoidsmaking subjective judgements.
to validate the efficacy ofour approach we apply our technique to evaluate the defectmanagement process performed in four projects from a largesoftware development center of a commercial bank in china.our empirical results show that the proposed approach iseffective.in summary this paper makes the following contributions we propose a novel machine learning approach that mayhelp practitioners to evaluate their software processes.
a new quantitative indicator referred to as process execu tion qualification rate is proposed to evaluate the qualityand performance of software processes objectively.
we compare and explore different kinds of sequenceclassification algorithms for solving this problem.the rest of the paper is organized as follows.
section iidiscusses the related work.
section iii presents the problemstatement.
section iv introduces the proposed machine learn ing approach to software process evaluation tasks.
section vpresents the experimental results and discusses the limitationsof validation.
finally section vi concludes this paper.ii.
relatedworkour work is closely related to previous works on softwareprocess research.
in particular there exist a variety of softwareprocess methodologies and models which contain a set ofguidelines and practices for assessing and improving thegeneral software process capability of large and small softwaredevelopment organizations .
the previous works mainlyfocus on high level comprehensive and general frameworksfor software process evaluation tasks while our work aimsto present a concrete method for an important subtask ofsoftware process evaluation which could be integrated intothose methodologies and models.
formerly qualitative evalu ation methods such as questionnaire based assessment interviewing and artifacts studies are widely adopted bysome process models e.g.
the capability maturity modelintegration cmmi .
cmmi was proposed by softwareengineering institute sei 1and holds a very important po sition in software process research.
it defines a series ofbest practices to guide software process assessment and im provement in software development organizations.
practicescover topics that include techniques management supportand more.
this model has been broadly used in variousindustrial organizations all over the world and turns out tobe especially effective for large organizations.
the standardcmmi appraisal method for process improvement scampi defines a formal framework to provide evaluation resultsrelative to cmmi models.
all the traditional qualitative meth ods mentioned above are used by scampi.
however thesemethods have the critical drawbacks described in section i which motivates the study of our work.
in this paper we1 a new quantitative machine learning approach whichcould be a strong complementarity for traditional softwareprocess evaluation methods.
in software process validation cook et al.
proposed a method for detecting differencebetween a formal process model and a process execution.
intheir work the correspondence between an execution eventstream produced by process executing and a model eventstream induced by the formal process model is measuredusing string distance metrics.
two metrics referred to assimple string distance ssd and nonlinear string distance nsd were developed.
since only one model event stream isderived from the process model in their approach it may leadto big mistakes for process validation results when i two ormore formal and reasonable process models exist for a samesoftware process ii process models are changing rapidlywithout updating the model definition in the documents iii there are a number of non trivial process branches.
differentfrom their work our proposed machine learning approachcan address these limitations.
moreover we propose a novelquantitative indicator.
similarly moor et al.
presentedan approach to software validation that employed conceptualgraph theory to compare differences between defined processmodels and actual process models.
in summary our work dif fers from the aforementioned software validation approachesin that we aim to learn some dynamic prediction models froma collection of updated process executions by machine learningtechniques.in addition our work is related to software process studiesthat mine event logs stored in some software repositories touncover the software process models.
for instance rubin etal.
gave an overview of applications of process miningtechniques to discover software development processes.
sama likova et al.
made use of a heuristic mining algorithmto construct the real ccb change control board processmodel from event logs stored in software configuration man agement systems.
they compared this mined real processmodel with the official process model and gave feedbacksto the development team.
although the nature of data studiedin their work is similar to ours the techniques used aredifferent.
in their approach the key technique is applyingan off the shelf process mining algorithm offered by promframework to construct an explicit actual processmodel from event logs while our approach adopts machinelearning techniques to learn a model to classify processexecutions into either normal or abnormal .
the researchpurpose of samalikova et al.
s work is to use the analysisresults of mined objective process models to improve thesoftware processes in the project teams.
they did case studiesin a large industrial company in the netherlands.
differentfrom their research purpose we are aiming at developing aquantitative approach to software process evaluation tasks.extensive experiments and case studies are conducted inour work to validate the effectiveness and efficiency of ourproposed approach.
in summary research on software processmining mainly focuses on the construction of explicit and real graph based representation of software process models334using process mining algorithms from event logs while ourwork aims at learning dynamic prediction models from eventlogs to evaluate the quality and performance of a softwareprocess objectively by using a new quantitative indicator.last but not least our work is in general also related to theemerging studies that apply data mining and machine learningtechniques in the literature of software engineering .specifically the first category of related work in this fieldapplies sequence mining algorithms to support software engi neering tasks.
for example el ramly et al.
formulatedthe problem of recovering user scenarios from user interaction traces as an instance of the sequential pattern mining prob lem where an interaction trace is initially represented as asequence of screen ids and then a mining algorithm namedipm2 was developed to mine the qualified patterns.
lo etal.
proposed an efficient algorithm to mine a closed set ofsoftware iterative patterns from program execution traces.
inaddition another category of related work employs classifica tion algorithms to support and improve software engineeringactivities.
for example clenland huang et al.
proposedthe none functional requirements nfr classifier which isused to classify none functional requirements from both struc tured and un structured documents.
anvik et al.
exploredvarious classification algorithms in solving bug assignmenttasks where bug reports text documents are classified intodifferent categories defined by the names of developers whoare appropriate to resolve some specific types of reports.compared with the previous studies in this area our work iscommon in that we all apply machine learning and data miningtechniques to solve software engineering problems but differsin that we present a machine learning approach to address adifferent problem in software engineering.iii.
problemstatementin general software process evaluationaims to assessthe quality of some software processes performed in somespecified scope ranging from a project to a department or anentire organization .
typically each organization hassoftwareprocess specifications which provide explicit representationand definition of different kinds of software processes usingformalism models such as petri nets finite state machines or flow diagrams associated with detailed text descriptions.different aspects of processes are addressed in the speci fications such as artifacts states of the artifacts activitiesand responsible persons.
different software organizations mayadopt different representation methods for software processspecifications which usually can be found in the qualitymanuals of an organization.in our approach the first step towards software processevaluation is to obtain raw data recording the detailed ac tivities or artifacts state of a software process.
most soft ware development organizations are equipped with commercialor open source computer aided software engineering case tools for the whole software development life cycle sdlc .for instance software configuration management systems aredesigned to monitor and control changes in the softwaredevelopment requirement management systems are used totrace manage and control user requirements etc.
as animportant part of case tools software repositoriesstore thedata generated by all activities via sdlc.
in our work weobtain raw data contain software process execution historyextracted from software repositories.in software repositories a key element isprocess execution which is a sequence of snapshots of the actions executed bydifferent roles or artifacts states of the observed softwareprocess.
as mentioned in section i software processes canbe broadly defined as various concepts to simplify our dis cussion we restrict the discussion of a software process asa collection of process executions in this work.
accordingly the goal of a software process evaluation task in our study isto assess the quality of some collection of process executionsspecified in some particular scope.
for example in this study our goal is to evaluate a software defect management processfrom some collection of process executions stored in defectmanagement repositories.
by using process executions weintend to apply machine learning techniques together with aquantitative indicator to evaluate the quality and performanceof a software process objectively.iv.
approachin this section we first give an overview of the proposedmachine learning approach to software process evaluation andthen discuss each step of our approach in detail.a.
overviewgiven a collection of process executions the key idea of ourmachine learning approach is to employ supervised sequenceclassification techniques to classify each process executioninto normal or abnormal .
here normal means that aprocess execution is qualified or is able to meet the organiza tion s standards criteria.
we also propose a novel quantitativeindicator namedprocess execution qualification rate whichis calculated to evaluate the quality and performance of thesoftware process.
fig.
illustrates the proposed framework ofour machine learning approach to software process evaluation.in a nutshell the proposed approach consists of four majorsteps.
the first step extracts raw data stored in softwarerepositories.
the raw data that we are interested in containprocess execution history e.g.
defect state change history .the second step preprocesses the data by transforming relevantdata to a set of sequences.
the third step builds differentsequence classifiers through a set of labeled training data andconducts classifier evaluation.
the last step applies the bestsequence classifier to classify all the rest sequences into either normal or abnormal as a result the process executionqualification rate can be calculated to evaluate the quality andperformance of the specified software process.b.
collecting dataas mentioned before there are various kinds of data storedin software repositories.
for example we can find technicaldocuments in software configuration management systems 335datapreprocess softwarerepositories sample label trainingsetraw dataunlabelledsequences build differentclassifiers evaluate theclassifiers apply theclassifiertraining set measurequantitativeindicatorbest classifierpredictedsequencesstep step 2step 3step 4classificationalgorithmsclassifiersmetricsfig.
.
the framework of the proposed machine learning approach to software process evaluationbug reports can be extracted from test management systems requirement change reports can be collected from requirementmanagement systems etc.
we refer to these data that have notbeen processed as raw data which can be either structuredor non structured and usually can be exported through somefunctionality provided by case tools.newassessedassignedclosedresolvedverifiedfig.
.
example of a simple requirement change processrequirement change rc00001 closed details status log attachment id status owner when description new tom assessed neil assigned neil please fix it.
resolved jerry verified tom closed tom done.
fig.
.
example of some raw data from a software repositorythe first step of our approach extracts the raw data fromrelated software repositories according to the characteristicsof the software process that we intend to evaluate.
in thiswork we are only interested in raw data that contain processexecution history.
the process execution history captures theevolutions of artifacts or activities of various tasks in theprocess.
to better describe our approach in an intuitive way fig.
shows an example of a simple requirement changeprocess we intend to evaluate.
this process represents thestate flow of requirement changes rc each node in the flowdiagram represents a status of rc.
to evaluate this processin a specified scope in our approach we should collect allthe raw data needed from requirement management systems.fig.
shows an instance of raw data i.e.
an rc extractedfrom a certain requirement management system that containsprocess execution history see the status log column .
ingeneral for some particular software processes we only needto extract raw data from one software repository.
for someother types of processes we may need to extract raw datafrom multiple different software repositories.c.
data preprocessingthis step preprocesses the collected raw data into well structured format to facilitate subsequent machine learningtasks.
specifically the idea is to convert every process execu tion from the raw data into a sequence based instance.
for easeof representation we introduce an alphabet that consists ofa set of symbolic values each of which represents a status ofsome artifact or an action of some task in the software process.such an alphabet comprises all the possible values states thatcan appear in the process executions.
thus each processexecution can be represented as a sequence of symbolic valuesfrom the alphabet.
consider the example of the requirementchange process shown in fig.
we can determine an alphabetas n s a r v c where each letter of n s a r v c represents new assessed assigned resolved verified and closed respectively.table iexample of a sequence databaserc idsequence idsequences1s1 angbracketleftn s a c angbracketr g t2s2 angbracketleftn s a r v c angbracketr g t.........nsn...once the alphabet is chosen the next is to convert eachprocess execution from the raw data into a sequence ofsymbolic values.
consider the example in fig.
the processexecution history are contained in the status log field.for this example we first extract a sequence of statuses336 an bracketle tnew assessed assigned resolved verified closed an bracketr taccording to the temporal order and then convertit to a sequence of symbolic values an bracketle tn s a r v c an bracketr tby following the alphabet.
after processing all the processexecutions we obtain a sequence database which contains aset of unlabeled sequences s s1 s2...sn where eachsequencesiis an ordered list of symbolic values from thealphabet.
table i shows an example of sequence database withnsequences.d.
building sequence classifiersthe data preprocessing step generates a sequence databasethat consists ofnunlabeled sequences whose class labels areunknown.
our goal is to learn some classification model bymachine learning algorithms to automatically predict the classlabels of these sequences.
sequence classification has beenextensively studied in the literature of machine learning anddata mining .
in general we must address three keyissues of a sequence classification task i create a trainingset of labeled sequences ii represent each sequence ofvaried length into a fixed dimension vector and iii choose amachine learning algorithm to build a classifier on the trainingdata.
we discuss our approach to addressing each of theseissues in detail below.
sampling and labeling a training data set this taskis to form a training data set by sampling a subset of unlabeledsequences from the entire database and then manually labelthe sampled sequences.
there are several issues in this task.first of all we need to decide a class label set to labelthe sequences.
since we formulate this problem as a binaryclassification task we have two unique class labels normal abnormal where normal implies that the process exe cution represented by this sequence is reasonable or meetsthe organization s standards and abnormal implies that theprocess execution represented by this sequence is irrational orviolates the organization s standards and could cause potentialnegative impacts.the second issue of this task is to choose an appropriatesize of the training data set denoted asntr which is smallerthan the database sizen.
ifntris too large it would betime consuming for performing the manual labeling task onthe other hand ifntris too small the training data may notbe enough to build a good classifier.
typically this size isdetermined empirically.
we will discuss this issue further inour empirical study.the third issue is to assign an appropriate class labelto an unlabeled sequence manually.
one common approachis to simply request an evaluation staff to examine if theprocess execution represented by the sequence has correctlyfollowed the software process specifications given by theorganization and adhered to the principles of the processmethodology adopted by the organization.
however it is oftennot enough.
one reason has to do with some ambiguous casesin the software process specifications.
another reason is that software processes in an organization are usually changingrapidly therefore the given software process specificationsmay be outdated.
besides typically in a large organization some departments or project teams may not always adoptthe standard processes of the organization due to variousreasons these specific processes are usually not recordedin the organizational software process specifications.
due tothe above reasons sometimes it is not easy to assign a labelclearly.
for such cases it would be necessary for the evaluationstaff to consult and discuss with some experienced domainexperts in the organization.
feature representation classical classification algo rithms in machine learning and data mining often work on datarepresented in vector space while the sequence examples inthe database are not represented in vector space in which theyhave varied lengths and the order concern of their elements.to apply the existing classification algorithms we need tofind some appropriate feature representation technique to mapeach of the sequence example into an example in some vectorspace of fixed dimensions.
to this purpose we adopt the well knownk gramsfeature technique in our approach which hasbeen widely used in other domains .specifically given a long sequence a short sequence seg ment of anykconsecutive symbols is called ak gram which isselected as a feature in order to keep the order of the elementsin the sequence.
after determining a set of k grams eachsequence can be represented as a fixed dimension vector ofthe presence and the absence of the k grams or as a vector ofthe frequencies of the k grams appeared in the sequence .
letus illustrate the idea by the example in fig.
.
the alphabetrepresenting the statuses is n s a r v c i.e.
thenumber of unique symbolsm .
assuming thatk 2is chosen for thek grams model we have62 36uniquek grams.
thesek grams include nn ns .
.
.
cv cc .
nowconsider a sequencesi an bracketle tn s a r v c an bracketr t. we slide a windowof sizek 2across the sequence to obtain the set of all theshort sequences of lengthk i.e.
ns sa ar rv vc asa result we get the feature vector corresponding to sequencesi as shown in table ii.table iifeature vector of a sequencesi angbracketleftn s a r v c angbracketr g tbased on a2 gram mapping methodnnns...sa...ar...rv...vc...si010101010103 training classifiers by machine learning algorithms after obtaining a training data set ofntrexamples in a vectorspace the next step is to apply a machine learning algorithm tobuild classifiers on the training data.
in our approach we adoptthree well known and representative classification algorithmsin machine learning i.e.
decision trees c4.
naivebayes nb algorithm and support vector machines svm all of which have been widely used in a varietyof real world classification tasks.
we will discuss the detailedcomparison of these three classification algorithms in ourexperiments.337e.
evaluating different classifiersafter training a set of classifiers by different machine learn ing algorithms the next step is to evaluate the performance ofthese classifiers in order to find the best classifier.
to achievethis purpose we first need to choose some performance met rics for comparisons.
the first set of metrics include precision recall and f measure which are defined as follows precision tptp fp recall tptp fnf measure precision recallprecision recallwheretp fp fnrepresent the numbers of true positives false positives and false negatives respectively.
in addition we also adopt the well known area under roc curve auc as another metric for evaluation.
finally we alsoadopt the root mean square error rmse a widely usedmeasure of the differences between values predicated and theactual truth values.f .
a new quantitative indicatorbased on the proposed classification framework we pro pose a new quantitative indicator named asprocess executionqualification rateas an objective measure for software processevaluation which is able to overcome the limitations ofconventional evaluation methods.specifically in the above classification scheme we build aset of classifiers on a training data set and choose the bestclassifier of the highest classification performance to label theremaining unlabeled sequences in the database.
after obtainingthe predicted class labels of all the sequences in the database we define theprocess execution qualification rate denoted asp as follows p predicted normal sequencestotal sequences precisionrecall the above definition indicates thatprepresents the percentageof actual normal sequences among all the sequences in thedatabase.
the larger the value ofp the better the quality andperformance of a software process.
this quantitative indicatorprovides an objective measure to evaluate the quality andperformance of a software process.remark.in the above definition it is important to note thatboth true precision and recall on the test set are unknownduring the software process evaluation phase for a real worldapplication unless we manually label all the test sequences .in order to calculate an estimated process execution qualifi cation rate without manually labeling the test sequences weadopt the precision and recall values on the training set as theestimated precision and recall values to calculate the value ofp.
in our experiments we will evaluate the difference betweenthe estimated qualification rate and the true qualification rateunder different classification settings.v.
experimentsto evaluate the effectiveness of the proposed approach weconduct an extensive set of experiments by applying our tech nique to evaluate the defect management process performedin four real life software projects.
first we briefly describethe profiles of these four projects and introduce the defectmanagement process to be evaluated.
we then describe theexperimental setup followed by presenting our experimentalresults and discussions as well as some case studies in detail.a.
experimental testbedwe adopt an experimental testbed that consists of foursoftware projects of a large software development centerof a commercial bank in china.
these four projects weredeveloped by different project teams from the same departmentof the organization.
the organization is assessed at cmmilevel which means this organization has a certain level inprocess management.
besides we intend to evaluate the defectmanagement process conducted in this department and in eachindividual project.
table iii summarizes the characteristics ofthese four projects where the last column shows the numberof bugs defects found in each project.fig.
shows the defect management process flow definedin the defect management process specification used in thisorganization.
for simplicity we only show the state flow ofdefect management process by different roles and omit somedetailed descriptions and explanations due to space limitation.as shown in this flow diagram there are two kinds of nodes status node which represents the possible status of defectand the responsible role and decision node which representsthe possible decisions that can affect the state change of adefect.
the status of a defect starts from new and evolutesalong with different decisions.table iiisummary of the four projects in our experimentsproject iddescriptionno.
of defects1electronic commercial draft system10192wealth management system phase1 4783wealth management system phase2 6654financial leasing system phase2 460b.
experimental setupto perform the evaluation of the defect management processof the projects shown in table iii we collect the raw dataof the defect status change history from the repository oftest management system adopted by this organization whichwas based on the hp quality center hqc version .
.
fig.
shows an example of the raw data of the defect historylog collected in our experiments from which we can extractthe sequence of statuses an bracketle tnew open assigned fixed closed an bracketr tto represent this particular defect process execution.further we generate an alphabet based on the process flowdiagram as shown in fig.
which consists of a total of 10338new assigned postponed fixed rejected open finished canceled closed reopen decision1 decision2 decision3status nodedecision nodetestertestertester testertesterdeveloperdeveloperdeveloperdeveloperdeveloper fig.
.
the defect management process flow extracted from a software process specification detailed explanations were omitted .defect id summary error project name electronic commercial draft system history log field name date of update modificator old value new value defect status fixed closed closed data song defect status wang assigned fixed defect status lin open assigned developer lin wang defect status song new open developer song lin fig.
.
example of a defect history log from an industrial project in our empirical study.symbols.
based on this alphabet we then convert the sequenceof statuses into a sequence of symbols.
further we adoptthek grams model for simplicity we choosek in ourexperiments.
we will discuss the problem of appropriatekvalues in section v.g as described in section iv to representeach sequence by a100 dimensional vector.
as a result we form a sequence database of unlabeled examplesrepresented in a100 dimensional vector space.
to facilitatethe classification experiments we collect the ground truthlabels by manually labeling these unlabeled sequences into normal or abnormal based on the standards adopted bythe organization sequences are labeled as normal and637 sequences are labeled as abnormal in which we hadresolved some ambiguous cases by consulting the experts fromthis organization for example we label sequences contain reopen more than2times as abnormal in this context .in all the experiments we take normal as positive class and abnormal as negative class.based on the above processed data and the ground truth la bels we conduct extensive experiments to evaluate the efficacyof our technique.
in particular we aim to answer the followingquestions what would be the best classification algorithmfor sequence classification in our problem?
how does theamount of training data affect the classification performancein our task?
what is the accuracy of the estimated processexecution qualification rate based on the proposed framework?next we discuss each of our experiments to address theseissues in detail.table iveva l uat i o n r e s u lt sprecisionrecallf measureaucrmsec4.
.
.
.
.
.284nb0.
.
.
.
.291svm0.
.
.
.
.143c.
comparison of classification algorithmsthe first experiment is to compare the performance of threedifferent classification algorithms for our defect managementprocess evaluation task including decision tree naive bayes nb classifier and support vector machine svm .
we adoptthe c4.
implementation for decision tree and employ a linearkernel for svm with the penalty parameterc .
we adoptthe standard fold cross validation setting on the wholedatabase of sequences to evaluate the performance ofdifferent classifiers.
table iv presents the experimental resultsof comparison with respect to the metrics of precision recall f measure auc and root mean square error rmse .from the results in table iv we can see that among thethree algorithms svm achieves the best performance for allthe performance metrics while c4.
and nb perform diversely3390.
.
.
.
.
.
.
.
.
average f measuresvmc4.5nbamount of training datafig.
.
effect of training data size on f measure .
.
.
.
.
.
.
average aucsvmc4.5nbamount of training datafig.
.
effect of training data size on aucunder different metrics c4.
performs slightly better in termsof f measure and rmse while nb performs better in terms ofauc score .
by looking into the results achieved by svm wefind that the f measure and auc values attained98.
and96.
respectively which indicates that the proposed schemeis able to learn an effective classifier for the classificationtask.
the very positive result is partly derived from the natureof the data a large portion of the sequences are repetitive.we further informally investigate the classification result forunusual sequences sequences that appear less than times inthe database and find that the classification performance isalso fairly good.d.
classification performance with varying amount of train ing datathe second experiment is to examine how the amount oftraining data affects the classification performance.
to thispurpose for each experimental trial we randomly sample30 sequences from the database as the candidate training data and the remaining70 sequences as the test set among thecandidate training examples we randomly choose a subset oftraining data to build the classifiers by varying the percentageof training data from5 to30 .
we repeat the above exper imental trial20times and calculate the average performanceover these20trials.
fig.
and fig.
summarize the resultsof average f measure and auc under varying amounts oftraining data respectively.from the experimental results we can draw some observa tions.
first among the three algorithms similar to the previousresults svm achieves significantly better f measure perfor mance than the other two algorithm while nb achieves thebetter auc performance than both svm and c4.
especiallyunder small amount of training data.
when the size of trainingdata is large enough svm can achieve a comparable or betterauc performance than that of nb.
second we found that for all the three classifiers increasing the size of trainingdata generally leads to an improvement of the classificationperformance.
but the improvement becomes minor when thesize of training data is larger than some threshold e.g.
.since it is often time consuming and expensive to label thetraining data manually in practice we should empiricallydetermine an appropriate amount of training data to trade offbetween the labeling cost and the classification performance.e.
estimated indicator with varying amount of training datathe third experiment is to evaluate the accuracy of thequantitative indicator of the estimated process execution qual ification rate i.e.
thepindicator.
following the same setupas the second experiment we evaluate the accuracy of theestimatedpvalues by randomly choosing a subset of can didate training data ranging from5 to30 to build theclassifiers.pis calculated according to formula defined insection iv where the estimated precision and recall values areobtained on the training set.
we adopt the root mean squareerror rmse to measure the difference between the actualp values and the estimatedpvalues of the process executionqualification rate.
fig.
shows the experimental results.some observations can be drawn from the results.
firstof all similar to the previous experiment for all the threealgorithms increasing the amount of training data generallyleads to the decrease of the rmse value indicating a moreaccurate estimation of thepvalue.
second among the threeclassification algorithms svm achieves considerably betterperformance than the other two algorithms especially whenthe amount of training data is small.
finally by examiningthe results achieved by svm we found that the proposedclassification based indicator is able to estimate a fairlyaccurate value ofp i.e.
with a very small rmse value when the amount of training data is sufficient.
.
.
.
.
.
.
root mean square error of psvmnbc4.5amount of training datafig.
.
effect of training size on predicted pf .
case studieswe conduct case studies to examine the efficacy of theproposed indicator by examining if the estimated valueof the indicator is close to the true value and exploring340if the proposed indicator is able to differentiate the quality ofprocess among different projects in the same organization.first we show some case studies to examine the differ ence between the estimatedpand the actualp values byinspecting different amounts of training data in detail.
inparticular we study two cases of different settings i caseof small sized training data sampling10 of all data fortraining and ii case of medium sized training data sampling20 of all data for training .
for each of the two cases we adopt svm as the classification algorithm repeat eachexperiment times and summarize the average results intable v where sdenotes the total number of sequences tndenotes the actual number of normal sequences p denotes the actual value of process execution qualification rate pn1denotes the predicted number of normal sequencesin case and p1 denotes the predicted process executionqualification rate in case while pn2andp2denotethe predicted number of normal sequences and the predictedprocess execution qualification rate in case respectively.table vexperimental results for our case studyproject1project2project3project4department s10194786654602622 tn9132954992781985p .
.
.
.
.
pn19583335342852110 .
.
.
.
.2p188.
.
.
.
.
.
.
.
.
.
pn29413075092822039 .
.
.
.
.5p289.
.
.
.
.
.
.
.
.
.
from the results we found that when providing sufficientamount of training data e.g.
we can achieve a highlyaccurate prediction of the process execution qualification rate.for example considering the whole department the predictedp2 .
is very close to the actualp .
valuecomputed from the ground truth.
even for the case of usingonly small sized training data we still can achieve a satisfiedprediction e.g.
for the whole department p2 .
is stillclose to the actualp .
.
these promising results againvalidate the efficacy of the proposed technique for softwareprocess evaluation.further according to the predicted values of the proposedquantitative indicator we can also compare the quality of thedefect management process performed in different projectsof the same department.
from table v we can concludethat the defect management process performed in project 1is significantly better than that performed by the other threeprojects.
specifically we found that project and project4 have relatively low process execution qualification rates indicating that these two projects have a high chance ofexecuting irrational processes or violating some organizationrules standards.
to verify this result we conduct some furtheranalysis by checking the labeled sequences of these twoprojects and found that there indeed exist at least three seriousproblems in both project and project i defects were notclosed or closed without fixed ii defects were reopened formany times iii defects were canceled without analysis.g.
limitations of validationthere exist four primary limitations in this work.
first we validate our approach based on the defect managementprocess performed in four software projects from a softwaredevelopment center of a commercial bank in china.
despitethe promising classification and prediction performance it isunknown if the approach can also achieve the similar goodperformance when being applied to other more complicatedsoftware process evaluation tasks where much longer andcomplex sequences may appear in different organizations.furthermore in our validation an informal experiment showsthat thekvalue in feature representation has little effecton the classification performance.
however for much longerand complex sequences the problem of appropriatekvaluemay arise.
future work will examine the applications of ourapproach to other more complicated and challenging softwareprocesses from various organizations.
besides we will studythe relation betweenkvalue and classification performance ingreat detail.the second limitation relates to the question of classificationperformance for unusual sequences.
as mentioned in sectionv.c the very positive result is partly result from the nature ofthe data.
the classification performance for unusual sequencesis not systematically analyzed in our validation.
we willconduct more experiments to address this question in ourfuture work.third as emphasized in the remark on formula insection iv.f for real world applications in order to calculatethe estimated process execution qualification rate we use the precision and recall values on the training set to approxi mate both values on the test set.
although the experiments andcase study show that we can estimate a fairly accurate processexecution qualification rate using formula this approximateassumption may not hold in some real world situations.
thiswill be left primarily to our future detailed empirical studies.last different organizations adopt different kinds of casetools which are either open source or commercial.
however there is no standard for the formats and contents of processexecution histories stored in different software repositories ofcase tools.
many potential problems may occur during thedata preprocess phase which may affect the efficacy ofour approach to some extent.
future work will be needed toempirically study the impact of different kinds of case tools.vi.
conclusionthis paper proposed a semi automated approach to soft ware process evaluation using machine learning techniques in which we formulate a software process evaluation taskas a sequence classification problem that can be resolved by341machine learning techniques.
based on the proposed frame work we present a new quantitative indicator i.e.
processexecution qualification rate as an objective evaluation of thequality of software process.
we found encouraging resultsfrom our preliminary empirical studies which validated theempirical efficacy of our technique for software process eval uation.
compared with the conventional subjective evaluationmethods the proposed machine learning approach has severaladvantages and potentially opens a new direction of applyingmachine learning techniques to facilitate automated softwareprocess evaluation in software engineering.
our future workwill apply our technique to more complicated software processevaluation tasks and investigate more sophisticated machinelearning techniques.
additionally we will conduct a com prehensive comparative study to analyze the benefits anddrawbacks of prior art traditional approaches and our proposedmachine learning approach to software process evaluationtasks.acknowledgmentthis work was supported by nanyang technological uni versity under sug grant m58020016 and acrf tier grantrg .
special thanks to quanxi mi for sharing the rawdata sets.
we also thank the anonymous reviewers for theirgreatly helpful comments.