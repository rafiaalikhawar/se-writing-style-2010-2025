accelerated test execution using gpus ajitha rajan school of informatics university of edinburgh u.k. arajan inf.ed.ac.uk subodh sharma peter schrammel and daniel kroening department of computer science university of oxford u.k. subodh.sharma peter.schrammel kroening cs.ox.ac.uk abstract as product life cycles become shorter and the scale and complexity of systems increase accelerating the execution of large test suites gains importance.
existing research has primarily focussed on techniques that reduce the size of the test suite.
by contrast we propose a technique that accelerates test execution allowing test suites to run in a fraction of the original time by parallel execution with a graphics processing unit gpu .
program testing which is in essence execution of the same program with multiple sets of test data naturally exhibits the kind of data parallelism that can be exploited with gpus.
our approach simultaneously executes the program with one test case per gpu thread.
gpus have severe limitations and we discuss these in the context of our approach and define the scope of our applications.
we observe speed ups up to a factor of compared to single core execution on conventional cpus with embedded systems benchmark programs.
categories and subject descriptors d. software program verification .
introduction the number of tests needed to effectively validate any non trivial software is extremely large.
for instance yoo et al.
state that for an ibm middleware product used in their study it takes a total of seven weeks to execute all the test cases making overnight builds impossible.
much of the research in software testing over the last few decades has focussed on test suite reduction techniques and criteria such as coverage that help in identifying the effective tests to retain.
this trend is particularly seen in regression testing and black box testing where numerous optimisation techniques test case selection test suite reduction and test case prioritisation have been proposed to reduce testing time .
even after applying these optimisations test suites remain large and their execution is typically very time consuming.
this puts an enormous strain on software development schedules.
permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for third party components of this work must be honored.
for all other uses contact the owner author s .
copyright is held by the author owner s .
ase september vasteras sweden.
copyright acm .
present an approach with the potential of executing test suites in a fraction of the original time and explore its feasibility on embedded systems benchmark programs.
our approach leverages the speedup offered by graphics processing units gpus .
gpus aremassively parallel processors featuring multi threaded performance unmatched by high end cpus.
the single chip peak performance of state of the art gpu architectures exceeds gflops which compares to around gflops for a traditional processor at its best .
a further advantage of gpus is that they are more energy efficient than their multi core cpu counterparts .
finally in terms of cost per performance gpus are more affordable than multiple pcs.
in particular the management cost of one computer with gpu is much smaller than that of a corresponding cluster of pcs.
test execution using gpus.
general purpose computing on gpus gpgpu has been successfully applied in a broad range of domains .
gpus use a single instruction multiple thread simt architecture to exploit data level parallelism.
we believe software testing will benefit greatly from gpus and the most compelling reason for this is the fact that program testing i.e.
running the same program with multiple sets of test data naturally exhibits data parallelism that can be exploited with gpus.
there has been no work in the past exploring this possibility and this paper paves the way in leveraging the acceleration provided by gpus for software testing.
existing literature on gpgpu investigates techniques to transform cpu versions of a program to run on the gpu.
the key problem here is the identification of opportunities to parallelise.
our approach is dramatically different we leave the program and its logic untouched and only paralellise the running of multiple test cases on the program.
our approach simultaneously executes the program with one test case per gpu thread.
we instrument the program with gpu device management code that acts as a wrapper for the original code.
we store the test suite in the gpu device memory and launch one gpu thread for each test input with the original program.
the threads require no synchronisation or coordination since the executions of different test cases are completely independent.
no program transformation from cpu program to gpu program is required in our proposed technique and thus we still test the original program logic and obtain results that are the same as those obtained with a cpu.
however current gpus have severe limitations and impose restrictions on the class of programs we can test.
we discuss these limitations in section .
.
our approach for accelerated test execution using gpus provides the following potential benefits a significant reduction in test testcase1 load instructionsprogram execute input testcase0 ... testcasen copy input to gpu device memory launch n gpu threads threadidtestcasen testcase2.
.
.
load dataload dataload data copy output from device memory to cpuoutput program input n 2n 1figure our approach for test execution on a gpu suite execution time and as a result huge savings in testing costs and for the same allotted test time allows more test cases to be executed potentially increasing the likelihood of fault detection and better energy efficiency than testing using multicore cpus or pc clusters .
related work.
gpus can be exploited for non graphical tasks using general purpose computing on graphical processing units gpgpu .
gpgpu has been successfully applied in a broad range of applications.
there is growing interest in the software engineering community to use the massive performance advantage offered by gpus.
recenty yu et al.
explored the use of gpus for test case generation .
bardsley et al.
have developed a static verfication tool gpuverify for analysing gpu kernels.
li at al.
and yoo et al.
have adapted multi objective evolutionary algorithms for test suite optimization to execute on gpus.
nevertheless we are not aware of any existing study that has explored the use of gpus to accelerate test suite execution which is the goal of our work.
our contributions.
to validate our hypothesis on test acceleration with gpus we make the following contributions in this paper .
we describe and implement our approach for executing test cases in parallel on a gpu using the cuda programming model.
.
we evaluate our approach experimentally using example programs and test suites and discuss the achieved speedup.
we use programs from the embedded systems domain in particular programs from the embassy benchmark suite .
.
we discuss the limitations of our approach i.e.
the testing activities it can be applied to and the application domain.
.
background the success of gpus in the past few years has been due to the ease of programming using the cuda and opencl parallel programming models which abstract away details of the architecture.
in these programming models the developer uses a c like programming language to implement algorithms.
the parallelism in those algorithms has to be exposed explicitly.
the gpu simt architecture can deliver extreme speedups if the threads executed have no data dependencies.
we now present a brief overview of the core concepts of cuda.
the highest level of the cuda thread programming model is a parallel kernel.
a kernel is a function that is invoked by a program running on the host cpu but is executed on the gpu.
kernelfunctions are identified by means of the keyword global .
kernel functions can only access the memory on the gpu any data required by it has to be copied to gpu memory before invoking the kernel.
the kernel is executed as a grid ofblocks ofthreads.
in other words threads are grouped into blocks and blocks are grouped into a grid.
grid and block dimensions can be specified when launching the kernel.
grids and blocks can be multidimensional and along each dimension there is a hardware defined limit on the number of threads and blocks that can be created.
the memory system is organized in three levels of hierarchy.
closest to the core are the registers which have the lowest latency and are private to each thread.
next is shared memory which is again small and has low access latency.
shared memory is available to the threads within a block.
all threads can access the large global memory which is comparatively slow cycles .
finally the execution model in cuda requires threads in blocks to be executed in groups called warps.
a warp is a group of threads in a block that are launched and executed together.
all threads in a warp execute the same instruction but on different data.
this is often referred to as lock step execution semantics.
when a conditional instruction is encountered with control flow divergence among the threads within a warp gpus resort to predication which can improve instruction scheduling and cache behavior of programs.
each thread block is mapped to one or more warps.
as a result we choose the thread block dimension as multiple of the warp size.
warps within a thread block can execute independently.
the example in figure gives the definition of a kernel function which is invoked as follows compute num tests device inputs here specifies the grid dimension and num tests specifies the block dimension.
a pointer to a block of gpu memory is passed as an argument to the function.
note that such memory is allocated and initialised by the host code using the cudamalloc and cudamemcpy functions.
a unique thread id within a thread block is given by the system variable threadidx.x along a single dimension and the block id is denoted by blockidx.x.
thus a thread id that is unique in the grid can be obtained by calculating blockidx x blockdim threadidx x. .
our approach we believe that the execution of a large test suite is a natural match for this architecture since it requires executing the same program multiple times with different inputs.
running a program with a test case on a cpu typically involves loading the program and the test case into memory and executing the instructions with the loaded data.
this is repeated for every test case in the test suite.
each test case run is completely independent of other test case runs.
also all the executions are over the same program albeit possibly not the same instructions depending on the control logic in the program.
on the gpu our approach will launch as many gpu threads as there are test cases where each thread executes the same sequential program with a different test case.
since executions of different test cases have no data dependencies there is no need for any thread synchronisation in our approach.
the key points of our approach figure are .
we vectorise the test inputs so that their dimension is the number of test cases in the test suite.
.
we copy the vectorised inputs from the host memory to the gpu device memory.
.
we then launch the kernel with the program functionality on the requisite number of gpu threads ideally as many as there are test cases .
each gpu thread will operate on the define array size void quicksort int int int intpartition int int int intmain void sample input array as test case inta quicksort a array size return figure harness for testing quicksort with one test input define array size define num tests device void quicksort int int int device int partition int int int gpu function global void compute int tests the thread id identifies the test case inttest case threadidx.x array size quicksort tests test case array size intmain void inthost inputs .... int device inputs cudamalloc void device inputs sizeof host inputs cudamemcpy device inputs host inputs sizeof host inputs cudamemcpyhosttodevice number of blocks is and number of threads per block is .
compute num tests device inputs return figure cuda test harness for quicksort with tests same program but with different test data using the unique thread id to identify the test case inputs to execute over.
.
we copy the program output from the gpu back to the cpu.
developers using our approach only need to understand the program interface i.e.
inputs and outputs.
the rest of the program is used as a black box.
.
an example to better understand our approach consider figure which gives a harness for testing a quicksort program.
we omit the code for the quicksort implementation which is unchanged in our approach.
the original quicksort program is available at .
one test input is provided as an integer array and the quicksort function is called given the array and its lower and upper array index bounds.
figure gives a cuda harness for testing the quicksort procedure.
the cuda kernel has four functions compute main quicksort andpartition .
the main function now features a test suite with 256tests which are stored in the host inputs variable.
the test cases in host inputs are copied to device inputs which resides on the gpu device memory using cudamemcpy .
finally the function compute is called by the kernel which launches one block of 256threads in parallel.
the function compute runs on the gpu and is identified as a kernel using the keyword global .
a local variable arr inputs is assigned a test case from the suite of tests in test inputs parameter using the unique thread id and the size of the array.
the function quicksort is called using the array input as before.
the functionsquicksort andpartition run on the gpu which is indicated using the device keyword.
the bodies of the functions remain unchanged and are not shown.
the main code changes required for this example using our approach stem from i transferring the test suite from the host to the device ii adding a kernel function compute to be called on all threads iii reading the test case from the test suite using the thread id in the kernel function.
the rest of the code largely remains the same.
in particular note that that the quicksort and partition functions remain unchanged.
as a result we still test the original functions with our approach.
compilers for gpu programs are highly specialised and do not support all features used in c c programs.
the limitations of gpus and their implications are discussed in the next section.
.
limitations of gpus and implications gpus can offer considerable acceleration.
however in turn there are severe limitations l1gpu programs have to copy data back and forth from the host memory to perform i o or when gpu memory is exhausted.
data transfer between the gpu and host memory is slow due to the high latency of the interface.
furthermore the typical bandwidth of accesses to gpu memory is two to three orders of magnitude higher than the bandwidth of transfers to host memory over pcie.
l2the different programming model cuda or opencl often requires heavy non trivial changes to existing source code to leverage gpu performance.
l3control flow branching in source code using control structures like if then else statements penalises gpu performance.
gpus execute groups of threads in lock step.
all threads that belong to the same group execute the same instruction but use different data.
lock step execution is violated if the branch instruction diverges.
this can impact performance negatively .
l4while the typical memory bandwidth of gpus is about five times higher than that of cpus gpus are restricted by the fact that their bandwidth is shared among thousands of threads .
this is not a problem in applications like graphics where threads share large data sets that can be retrieved from the shared memory in blocks.
in applications that do not share data data transfers from the device global memory to each of the several thousand threads will be a bottleneck.
l5the compiler for cuda source files nvcc processes them according to c syntax rules.
as a result some valid c but invalid c code fails to compile.
full c is supported for the host code.
however only a subset of c is supported for the device code as described in cuda programming guide .
we will now discuss these limitations in the context of our approach.
limitations l1 and l4 are believed to be less of an issue in next generation gpus.
a recent keynote from the ceo of nvidia predicts the next generation of nvidia gpus pascal to be released in to bring larger memory size and bandwidth using stacked memory faster data transfer between cpu and gpu to times more using nvlink and smaller more energyefficient chips .
in our approach we would ideally want to do a data transfer once from the cpu to the gpu and once the other way.
however system calls and other program features that cuda opencl cannot handle require data to be transferred more frequently.
handling system calls effectively on the gpu is an active area of research .
limitation l2 is not an issue for our approach since we are not transforming the program to run on the gpu.
instead we only need to write a test wrapper in cuda or opencl that launches for each 99test case a copy of the program on a thread.
little or no knowledge of the program logic is needed and the transformation is typically straightforward.
we plan to automate the insertion of this test wrapper in the future.
limitation l3 restricts the programs we can test with our approach.
we hypothesise that for programs with heavy branching our approach will not produce a significant speedup.
we have applied our approach to programs with different degrees of control flow divergence to test this hypothesis.
it is expected that the impact of this limitation will reduce in future generation gpus which will feature more sophisticated branch prediction logic.
limitation l5 restricts the program features cuda opencl can support on the gpu device.
unsupported features can be re implemented for cuda but this requires program transformations which may affect the correctness of the test execution.
cuda and opencl are evolving and future releases will support more features of c c .
however this limitation is currently the primary constraint for the scope of our approach.
.
scope of our approach our approach is best suited to c programs that can be compiled for the gpu.
limitation l5 drives the set of programs that satisfy this constraint.
c c programs with limited system calls.
c c programs with limited control flow branching.
c c programs that do not exceed gpu memory size.
future generation gpus and cuda opencl compilers will potentially allow a wider application scope.
in this paper we use c programs from the embedded systems domain that satisfy the constraints mentioned above.
.
ev aluation we check the feasibility of our approach on c programs from the embedded systems domain.
we evaluate the hypothesis that test execution on gpus is faster than on cpus on the example c programs and test suites.
we also show that our approach does not alter the program functionality and that the test case outputs on the gpus and cpus remain the same.
finally we discuss the overhead of data transfer between host and device.
in our experiments the cpu we use is an intel xeon processor with cores at .
ghz and gb ram.
the gpu we use features the gtx kepler architecture cores at .
ghz and gb device memory.
.
benchmarks we use four benchmarks in our evaluation .
image decompression using inverse discrete cosine transform idctrn01 .
fast fourier transform processing in the automotive area aifftr01 .
inverse fast fourier transform processing aiifft01 .
brake by wire system bbw loc the first three programs are from the embedded microprocessor benchmark consortium eembc which provides a diverse suite of processor benchmarks organised into categories that span numerous real world applications namely automotive digital media networking office automation and signal processing among others .
the three benchmark programs that we have chosen are from the automotive category.
the benchmark programs have test inputs associated with them.
all the inputs are stored in a large data structure.
the program only reads a small fraction of the test inputs in the data structure for one execution iteration.
however the program is executed iteratively several times each time reading inputs from a different location in the data structure.
the output valuesspeedup tests idctrn01 aifftr01 aiifft01 bbw table speedup cpu time gpu time rounded for the programs with gpu block dimension of from all executions are captured for both cpu and gpu executions and later compared to determine correctness of test execution using our approach.
the fourth benchmark is a brake by wire bbw system provided by v olvo technology ab designed in simulink .
c code was generated from it using simulink coder.
the system consists of five components four wheel brake controllers for sensing and actuating and a main controller responsible for computing the braking torque.
we generated random test vectors over the input ranges of the five inputs rotations per minute for each wheel and state of the brake pedal.
the values of the four brake torque outputs were captured for cpu and gpu executions and compared.
similar to the example illustrated in section .
we added gpu device management code to run the program with one test case on each gpu thread.
we did not make any changes to the code that implements the program functionality.
the modifications on all programs were straightforward and easy to implement.
.
experimental results we collect the following data .
execution time on the cpu .
execution time on the gpu for different grid and block dimensions .
test outputs on the cpu and gpu .
device from to host data transfer time for the gpu executions.
table gives the results obtained from test execution on the cpu and gpu.
column benchmark contains the name of the benchmark used.
column tests is the number of tests run on the program.
block dim andgrid dim are the number of threads in a block and number of blocks in a grid respectively.
gpu time andcpu time are times taken on the gpu and cpu respectively to execute all the tests on the program.
device host time column represents the time spent on data transfers between cpu host and gpu device .
finally the outputs match column indicates whether test outputs from the cpu run and the gpu run match.
table gives the speedup achieved by executing tests on the gpu compared to the cpu for the different benchmarks.
speedup is computed by dividing the cpu time column by the gpu time column in table .
.
discussion speedup achieved.
as seen in table the speedup achieved with our approach is to times depending on the benchmark and test suite size.
speedups achieved for the eembc benchmarks idctrn01 aifftr01 andaiifft01 are high to times .
a possible explanation for this is that control flow in all of these benchmarks is induced by forstatements rather than if else statements.
recall that in section .
we mentioned that control flow divergence reduces gpu performance since groups of threads execute in lock step.
lockstep execution is impossible if branches diverge.
in our examples theforstatements cause only very limited divergence in control flow and hence a high speedup is observed.
100benchmark tests block dim.
grid dim.
gpu time ms cpu time ms device host time outputs match?
idctrn01 .
.
.
yes idctrn01 .
.
.
yes idctrn01 .
.
.
yes idctrn01 .
.
.
yes idctrn01 .
.
.
yes aifftr01 .
.
.
yes aifftr01 .
.
.
yes aifftr01 .
.
.
yes aifftr01 .
.
.
yes aifftr01 .
.
.
yes aiifft01 .
.
.
yes aiifft01 .
.
.
yes aiifft01 .
.
.
yes aiifft01 .
.
.
yes aiifft01 .
.
.
yes bbw .
.
.
yes bbw .
.
.
yes bbw .
.
.
yes bbw .
.
.
yes bbw .
.
.
yes table results on both the cpu and gpu from running the benchmarks with different test suite sizes on the other hand the speedup achieved for the bbw example is only two times regardless of test suite size.
the bbw code contains heavy control flow branching with if else statements.
diverging control flow and lock step semantics cause instructions on different branches to wait and synchronise which leads to higher gpu execution times and lower speedups.
the cuda version of the bbw benchmark contains a large set of thread local variables.
nvidia s kepler architecture evaluations were performed on the card having this architecture allows bit registers per thread to be allocated for thread local variables.
excess variables are spilled over to the global memory.
we confirmed that when bbw benchmark was evaluated on threads a spill of bytes was observed.
accessing global memory is known to be at least an order of magnitude slower.
notice that for all four programs the speedup achieved remains the same beyond tests for grid dimensions and .
the likely reason for this is that the number of blocks and hence warps for a very large number of tests exceeds the maximum number of warps that can be scheduled on a streaming multiprocessor.
it might also be that for larger number of tests in our examples there are not enough resources available to run all the test cases in parallel.
as a result some of the threads will have to wait and be scheduled later.
for all the benchmarks we saved and compared the test outputs from the cpu and gpu for different numbers of tests.
we found that in all the cases listed in table the test outputs from our approach matched the test outputs from cpu.
although this is not a proof of correctness of our approach it does serve as an initial evidence of feasibility of test execution with gpus.
effect of block and grid dimensions.
the user supplied dimensions for grid and block play a crucial role in the runtime of a kernel.
a larger block size will result in frequent context switching of warps in a block.
while thread and warp context switching in gpus is relatively lightweight the effects become tangible when kernel execution is long.
current gpu cards allow a maximum of threads in a single dimensional block a hardware restriction .
the log plot in figure quantifies the effect of the block size on the benchmarks.
for this plot we fixed the number of test runs to .
notice that as the block size approaches the limit of the hardware the execution times worsen.
figure effect of block size on kernel runtime all the benchmarks show an optimal execution time when the block size ranges between and .
a straightforward reason for this is that a warp is executed as a group of threads.
thus block sizes in the aforementioned range will require minimal or no context switches.
when a block contains a single thread then for most benchmarks the execution time remains close to optimal except for the bbw benchmark.
a block of size one implies that none of threads are executing in lock step.
if a benchmark is frequently accessing global memory then due to the absence of lock step execution the kernel runtime will increase.
the bbw benchmark has such a memory access pattern.
for each thread iteration in bbw frequently accessed inputs to the thread are located in global memory.
one may refactor the code to move inputs to the thread local memory but gpu cards only offer a very small amount of thread local memory few kbs .
thus such code refactorings become nontrivial when the input data structures are large.
data transfer between host and device.
a key component of our approach is to copy the inputs for a test suite from the cpu memory to the gpu memory.
these transfers start to gain importance when the benchmark requires large inputs.
notice in table that the memory transfers between the cpu and the gpu take more time than kernel execution for large test runs on aiifft01 and aifftr01.
while the issue pertaining to gpu cpu memory transfer has not posed a problem for our benchmarks it 101becomes more serious for benchmarks where the complete input may not fit in the gpu memory.
in such cases either gpu cards with larger memory have to be procured or the gpu code will involve nontrivial refactoring where the kernel operates on partial inputs at a time and synchronises with the cpu before operating on the rest.
consequently transfer latency and bandwidth limitations of the pci express link will become more apparent.
the recent road maps of companies developing gpu cards indicate that the next generation gpus try to address the problem of cpu gpu memory transfers.
some of the solutions have already been released including unified virtual memory and the fabrication of the gpu chip on the same die as the cpu kaveri .
we believe that with advances in gpu technology test acceleration via gpus will become only even more attractive.
system calls.
currently there are few abstractions available that allow gpu code to perform system calls such as brk file i o and callbacks.
with current state of the art gpu technology it is still nontrivial to run benchmarks that frequently perform system calls on gpus.
this is an active area of research .
our benchmarks notably did not have system calls.
.
threats to validity the first threat to validity is the small number of programs used in our experiments.
we have only used four programs even if they are all industry standard programs.
the second threat arises from the fact that we only use programs from the automotive domain.
programs from other domains were not used in our experiments.
we plan to do a more extensive evaluation using programs from different application domains in the future.
.
conclusion in this paper we proposed an approach to accelerate test execution using gpus and explored its feasibility.
our approach inserts gpu device management code in the program interface to launch a gpu thread for each test case.
the program functionality is not modified in our approach.
we evaluated our approach using programs in the embedded systems domain benchmark programs from eembc and one brake by wire system from v olvo.
we ran the programs on test suites with sizes that range from to tests.
our approach using gpus achieved speedups in the range of to times for the eembc benchmarks and a speedup of for the brake by wire system.
the extent of control flow divergence in a program affects the speedup achieved with gpus.
we verified for the benchmark programs that our approach generated the same test case outputs over all the tests as the cpu.
finally we also discussed limitations in gpus and the restrictions they impose in the context of our approach.