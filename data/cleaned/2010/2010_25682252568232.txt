performance regression testing target prioritization via performance risk analysis peng huang xiao ma dongcai shen and yuanyuan zhou computer science and engineering computer science univ.
of california at san diego univ.
of illinois at urbana champaign la jolla ca usa urbana il usa ryanhuang x1ma doshen yyzhou cs.ucsd.edu abstract as software evolves problematic changes can significantly degrade software performance i.e.
introducing performance regr ession.
performance regression testing is an effective way to reveal su ch issues in early stages.
yet because of its high overhead this activ ity is usually performed infrequently.
consequently when performa nce regression issue is spotted at a certain point multiple commi ts might have been merged since last testing.
developers have to spen d extra time and efforts narrowing down which commit caused the prob lem.
existing efforts try to improve performance regressio n testing efficiency through test case reduction or prioritization.
in this paper we propose a new lightweight and white box approach performance risk analysis pra to improve performance regression testing efficiency via testing target prioritization.
the analysis statically evaluates a given source code commit s risk in introducing performance regression.
performance regress ion testing can leverage the analysis result to test commits with hig h risks first while delaying or skipping testing on low risk commits .
to validate this idea s feasibility we conduct a study on realworld performance regression issues from three widely used opensource software.
guided by insights from the study we desig npra and build a tool perfscope.
evaluation on the examined prob lematic commits shows our tool can successfully alarm of the m. moreover on randomly picked new commits from six largescale software with our tool developers just need to test o nly of the commits and will still be able to alert of the commits with performance regression.
categories and subject descriptors d. .
testing and debugging testing tools general terms performance experimentation keywords performance regression performance risk analysis cost m odeling permission to make digital or hard copies of all or part of thi s work for personal or classroom use is granted without fee provided th at copies are not made or distributed for profit or commercial advantage an d that copies bear this notice and the full citation on the first page.
to cop y otherwise to republish to post on servers or to redistribute to lists re quires prior specific permission and or a fee.
icse may june hyderabad india copyright acm ... .
.int bstream rd db catalogue ... do if bcat add item cat ti.base.base !
bstream ok return bstream error while ret bstream ok int bcat add item ... switch item type case bstream it privilege image info dbobj it1 info add db object ... backup image info dbobj backup info add db object ... the new block calls an expensive function.
when indirectly executed inside a loop it can incur times slowdown if type bstream it trigger obs obj tbl obj obs find table for trigger ... figure a real world performance regression times is sue in mysql the change causes an expensive function call find table for trigger to be executed many times.
.
introduction .
performance regression testing performance is a vital quality metric for software system.
i t can directly affect user experience and job efficiency.
for e xample a ms latency increase could cause traffic loss for google .
as another example the colorado benefits manag ement system is designed to make social welfare accessible.
b ut it runs so slowly that the system is virtually unable to accept a ssistance applications .
on the other hand software today is evolving rapidly.
code commits1for feature enhancement bug fixing or refactoring are frequently pushed to the code repository.
some of these comm its while preserving the software s functionality may signifi cantly degrade performance i.e.
introducing performance regression .
figure shows a real world performance regression issue fr om mysql.
the added code commit was written without much consideration for performance.
when the code gets indirectly e xecuted inside a loop it can cause mysql s backup operation to be times slower.
after this problem was reported developers o ptimized the added code and released a patch.
to provide motivating evidence figure shows the releases containing performance regression issue s in a snapshot of th e evo1we use commit revision and changeset interchangeably.permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
icse may june hyderabad india copyright acm ... .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.963mysql popular database system chrome popular web browser ... ... ... ... ... ... ... version containing performance regression ... ... ... ... ... figure performance regression in modern popular softwar e. data from performance regression bug reports studied in .
table typical running cost for popular benchmarks.
category benchmark per run cost web server autobench web polygraph specweb min hr database pgbench sysbench dbt2 min hrs compiler cp2k polyhedron spec cpu hr hrs os lmbench phoronix test suite hrs hrs lution history for two popular performance critical ope n source software.
these regression issues were user reported ones i.e.
post release.
we can see that regressing releases are preva lent across the evolution history of these two software packages.
performance regression often damages software s usabilit y considerably.
in one example switching from mysql .
to .
in a production e commerce website caused the loading time o f the same web page to increase from second to seconds.
this made the website almost unusable.
the reporter complained t hat mysql is no good for production until this bug is fixed .
as another example after upgrading gcc from .
to .
mozill a developers experienced an up to performance regression w hich forced them to reconsider a complete switchover .
performance regression has been a known problem for mature and performance conscious software projects.
for example in many software issue tracking systems there is a special categor y to annotate issues related to performance.
in the case of mysql a se parate severity level s5 is used to mark performance relate d reports.
interestingly among a set of randomly sampled mysql perf ormance regression issues almost half of them were also t agged as s1 critical or s2 serious .
.
performance regression testing challenges an effective way to combat performance regression is to employ systematic continuous performance regression testi ng.
this is widely advocated in academia open source community and industry .
ideally the testing should be carried out as comprehensive ly and intensively as possible i.e.
on every source commit basis .
this can eliminate the last minute surprise wherein performa nce issues are exposed too late to be fixed before release .
more importantly this would avoid the tedious and lengthy diagnos is process to figure out which commit is responsible for the observe d performance regression.
unfortunately despite the obvious benefits real practice s often cannot afford performance regression testing for every com mit due to the combination of two factors the high performance test ing overhead and the rapid software evolution pace.
performance testing is supposed to measure systems under re presentative and comprehensive workloads.
it can take hours to daystable estimated commit and performance testing frequenc y in popular software.
softwareavg.
rev.regular perf.
testingper day mysql every release chrome every rev.
linux every week to complete even with a number of dedicated machines .
table lists the typical per run cost of several popular benchmarks.
a comprehensive performance testing often includes not only these benchmarks but also load and stress testing suite s. in a leading public us data warehousing company that we collabo rate with anonymized as required some internal test cases can take almost one week to just load the data.
what s more performance testing results are subject to ext ernal factors such as caching and testing environment load.
to min imize experimental errors performance testing should be carrie d out in a clean environment for a period long enough and repeated sev eral times until the performance becomes stable .
c onsequently performance testing guide often advocates to n ever believe any test that runs for only a few seconds .
in thi s sense performance testing is by nature time and resource consumin g. on the other hand the increasingly favored rapid developme nt methodologies such as agile development are catalyzin g software revision speed.
as table shows chrome and linux have more than revisions per day merged into the code base.
wit h the high revision rate and high testing cost it is almost imp ractical to run comprehensive performance testing on every commit.
.
current practices the above factors often cause performance regression testi ng frequency to be compromised.
testing is carried out on daily or per release basis .
when performance regression is exp osed developers have to spend extra efforts bisecting which commit among the recently committed changes causes the problem.
fo r example in diagnosing a chrome performance regression iss ue revealed during testing there were six revisions included since last testing.
the developers had to conduct bisection to narrow d own the problematic commit which already took hours.
with performance test case prioritization a technique as u sed in feature regression testing the testing frequen cy can be increased but at the cost of reduced comprehensiveness.
in t his scheme test cases are divided into multiple levels based on their overhead and ability to catch performance issues.
then ligh tweight test cases with high detection rate are run more frequently w hile costly tests are run infrequently.
our industry collaborat or adopted this practice.
such prioritization is effective to capture easy totrigger performance regression.
but the detection of compl icated issues that are manifested only under the comprehensive tes t cases are delayed because of reduced comprehensiveness.
.
our contributions given that performance testing frequency and the commit spe ed are not in synchrony it is important to fully utilize the tes ting cost.
to this end it is desired to devote testing on exactly the reg ressing commits and skip non regressing commits.
but existing prac tices as above all treat the testing target code commits as black box ignoring the valuable information in commit content that ma y be exploited to direct performance testing on the right target .
consequently the testing is carried out blindly even for commits that are unlikely to introduce performance regression.61table studied software.
software description loc of issues mysql dbms .2m postgresql dbms 651k chrome web browser .0m our work takes this complementary approach by leveraging th e information in each code commit to help prioritize performa nce regression testing on risky commits.
in particular by cond ucting a lightweight static performance risk analysis abbr.
pra hereafter on source code change we estimate the risk of this rev ision in introducing performance regression issues.
based on such e stimation we can prioritize performance testing to conduct more comprehensive tests for high risk revisions while lowering th e testing cost for low risk ones.
for example by statically analyzing the added code in figur e we know it can be executed indirectly inside a loop.
with stat ic cost estimation and profile information our pra will recommend this commit to be tested heavily.
on the other hand for commits li ke adding few arithmetic operations in a cold path we could sug gest skipping testing or testing it lightly.
the major contributions of this paper are we conduct an empirical study on real world performance regression issues from three widely used software to gain insights on the feasibility and approaches for testing target prioritization with commit performance risk analysis.
to the best of our knowledge we are the first to propose a white box approach on prioritizing performance testing targets with code change performance risk analysis pra to make performance regression testing more efficient.
we implement a tool and release it in open source.
evaluation on the studied commits as well as randomly selected newcommits shows that based on the recommendation of our tool developers just need to test only of the code commits and will be able to catch of all performance risky commits.
.
understanding real world performance regression issues to validate the feasibility of performance regression test ing target prioritization and design an effective pra we conduct an empirical study on randomly selected real world performa nce regression issues from three widely used open source softwar e. the study focuses on understanding what kind of code changes wou ld cause performance regression and how they impact performan ce.
designing in this bottom up manner may suffer from the overfitting problem.
to address this concern we also evaluate ou r proposed pra on randomly selected new code commits that are not used in the study described here.
.
software studied table lists the details of studied software.
we choose thes e software packages because they are performance critical t op rated widely used and industry backed.
chrome is developed by goo gle mysql is now owned by oracle and postgresql s development team consists of employees from red hat.
besides their long evolution history and well maintained issue tracking systems provide us with many real world performance regression issues.
.
issue collection methodology for each software we first query the tracking system or maili ng list with a set of broad performance related keywords like perfor mance hit drop slow slower on resolved issues .
after getting this initial collection of issues we manually go ov er each to prune out those issues unrelated to performance regressi on.
finally we randomly sample issues of which not only the fixe s can be found but also the responsible change set can be track ed.
.
threats to validity construct validity our study has high construct validity because all the issues we collect are indeed related to performance r egression issues based on issue symptom description from the repo rter and confirmation of issue fix from the developer.
internal validity there is potential selection bias in our study.
we try to minimize it by first covering diverse categories and re presentative software.
except for chrome which is landed in t he other two software projects have more than years of histor y. additionally the issues we study for each software are randomly sampled without favoring or ignoring particular type of perfor mance regression issues.
for each issue we write a diagnosis repo rt and have at least inspectors agree on the understanding.
another potential threat is unreported performance regres sion issues.
it is difficult to measure quantitatively them.
howeve r we believe that at least the reported issues are of importance.
external validity although we believe our study provides interesting findings on performance regression issues in studied representative software they may not be generalized to other sof tware projects beyond the specific scope this study was conducted.
thus they should be taken with limitations in mind.
most importantly we want to emphasize the primary purpose of this study is for ourselves to gain insights for designing pra instead of drawing any general conclusions.
therefore the potential threats to validity in the study will only impair the effi cacy of our design.
nevertheless as evaluation demonstrates the pra guided by this study is indeed effective even for new code commits that are not examined in our empirical study.
.
code change categorization to analyze what kind of code changes are likely to introduce p erformance regression we develop a taxonomy.
the key rationa le is that program performance depends on how expensive an operat ion is and how many times the operation gets executed.
therefore performance regression could be introduced by either more expe nsive operations or regular operations but executed many times in a critical path.
as a starting point we need to look into what andwhere are the code changes in their execution context.
moreover w e need to consider code changes that indirectly impact expensive operations or critical path.
for example a code change that modifi es a variable controlling the loop iteration count in a critical path may have a big impact on performance.
we further zoom into each of the three perspective and divide it into subcategories.
the subcategories may not exclusive.
for example in the where perspective a change could lie in both a loop and a primitive function.
we ch oose the first applicable subcategory in top to bottom order.
in our study we consider only code changes that are the culpr its for performance regression ignoring innocent changes in t he same commit.
also we focus on changes that impact the performance of existing functionalities ignoring changes for indepen dent new functionalities that do not interact with existing ones.
th e result based on this categorization methodology is presented in ta ble .
.
.
where a change takes place program scopes such as loop or primitive function are perfor mance sensitive places.
it is because these places can be exe cuted or called many times and magnify any overhead added inside th em.62table categorization of issues examined from three persp ectives.
categorysoftware mysql postgresql chrome where the change takes place api can be called a lot of times by external program primitive utility function can be called a lot of times internally e.g.
mutex spin wait routine function will definitely be called under various input e.g.
mysqlparse loop can have multiple iterations others what the change modifies expensive function call e.g.
figure performance sensitive condition e.g.
figure performance critical variable e.g.
figure others e.g.
overhaul how the change impacts performance direct e.g.
figure indirect latentthrough function return value e.g.
figure through function referential parameter e.g.
figure through class member through global variable others total hence we first categorize all the problematic changes based o n the scopes that they lie in.
we also need to be context sensitive and consider call paths.
for example in figure although the added expensive functi on callfind table for trigger is not inside loop in the static context if the expensive call path is executed it will be dy namically executed in a tight loop loop that iterates many times .
therefore when we design pra we need to be context sensitive and examine possible call paths.
table shows more than half of the problematic changes are located in performance sensitive scopes.
however using ch ange scope as the sole indicator of performance regression can mi ss some performance killers that can influence critical paths via da ta and control flow.
that is why there are a significant number of regr ession issues in the others subcategory in table .
for this reason we further categorize the change content.
implication pra should pay attention to common performance critical places such as loop and primitive function.
b ut using only place as criteria can incur high analysis inaccur acy.
.
.
what a change modifies intuitively for a change set to introduce performance regr ession it needs to add high overhead computation or i o .
such influ ence can happen in two ways one case is that the change directly ad ds significant cost into hot path another case is that the chang e modifies some control variables which result in an expensive c ode region to iterate more times or take a longer execution path.
the former case is straightforward especially if our pra can be context sensitive whereas the latter would require us to c onsider some special variables and their control and data dependenc ies.
we refer to the variable of which different values can result in dramatically different performance as performance critical variable e.g.
loop iteration count database table index variable as in fi gure and the condition that controls whether a critical path will be taken in a branch as performance sensitive condition e.g.
the branch condition in function create sort index in figure .
in the study we obtain information regarding whether opera tions are expensive or performance critical by reading the bug rep orts and consulting with developers.
in our implementation of pra we obtain such information from static analysis and profiling.as table shows expensive function calls is the most common problematic change content.
but other types of changes such as modifying the performance critical variable and performan ce sensitive condition also cause a significant number of performa nce regression issues in the dataset.
implication pra should also account for the code change content to identify costly instructions and performance cr itical variables or sensitive conditions.
.
.
how a change impacts performance as explained above not all problematic changes affect perf ormance in a direct way.
a change may propagate via data and control flow that eventually causes a performance problem in its forward slice.
for example in figure although the change its elf is not expensive it modifies the function return value whic h later determines whether an expensive filesort is called.
thus we divide all the perilous changes into two categories b ased on how they impact performance changes that directly de grade performance in its execution scope changes that i ndirectly cause performance problem later in other code region s. table shows the majority of the issues belong to the first cat egory.
but there are also a significant number of issues impac ting performance indirectly.
for these issues the table also li sts the program constructs through which the change propagates its eff ect to critical paths and thereby leads to performance degradatio n. most of them are through function return values.
implication pra should follow the control and data flow to factor in the indirect effect of a change.
the analysis shoul d be inter procedural and context sensitive.
.
case studies we now go through three performance regression issues from mysql as case studies.
the issue in figure is introduced when patching a functiona lity bug in mysql cluster.
this commit changes the system call clock gettime argument to use clock monotonic instead of real time clock to prevent potential hang in certain scen arios.
but in some platform like solaris clock gettime call is expensive when using clock monotonic .
this extra overhead is further amplified when it is called in a tight loop.
if table s primary key !
max key table file primary key is clustered tab index table s primary key else the new logic prefers clustered primary index over secondary ones.
it degrades performance for certain workloads.uint make join readinfo join join ulonglong options for i join const tables i join tables i join tab tab join join tab i tab index find shortest key table ... int join read first join tab tab if !table file inited table file ha index init tab index tab sorted figure mysql performance regression issue ndb ticks ndbtick currentmillisecond void struct timespec tick time return tick time.tv sec millisec per sec tick time.tv nsec millisec per nanosec int waitclusterstatus const char addr ... while allinstate false ... time now ndbtick currentmillisecond replacing realtime clock with monotonic for clock g ettime sys call has performance loss in solaris.
when put in loop this loss can be up to clock gettime clock monotonic tick time clock gettime clock realtime tick time figure mysql performance regression issue in figure function test if skip sort order decides whether it can leverage database table index to perform the o rder by query or an actual file sort is necessary.
the new optimiza tion rule prefers file sort with a join buffer over an index scan if possible.
in such case it returns .
however when an index is clus tered index scan can actually be much faster than the preferred file sort.
as a result this premature optimization code change causes performance slowdowns for these types of data layout.
similarly the new optimization in figure prefers cluster ed primary key over secondary key.
while this can speed up disk bou nd workloads it slows down cases when the data is in disk cache.
.
performance risk analysis .
overview the objective of pra is to examine source code commit content and determine whether the commit is likely to introduce perf ormance regression.
therefore pra is a white box approach.
but pra is not meant to replace performance regression testing.
on the contrary its main consumer is performance regression t esting.
it recommends risky commits to performance regression test ing to test comprehensively and suggests skipping low risk commi ts.
this role relaxes the safety and soundness requirement for pra.
in the extreme case if pra recommends every commit to be tested it is the same situation as testing without pra ifpra recommends to skip a commit that is actually risky it is similar as condu cting infrequent such as per release testing.
that said pra should aimbool test if skip sort order ... if select limit table records dbug return int create sort index if order !
join group list ... test if skip sort order ... dbug return table sort.found records filesort thd table join sortorder ... filesort and join cache are usually faster than reading in index order and not using join cache if tab type jt all ... dbug return the new control flow can change the function return value which later affects whether an expensive path with firesort call will be taken or not figure mysql performance regression issue to flag the commits risk accurately to be able to truly improv e performance regression testing efficiency.
.
pra design pra is essentially static performance risk estimation of code changes.
the challenge is how to reason about performance im pact without actually running the software.
guided by the real wo rld issue study the performance impact of a code change depends on the cost of change operations whether it is expensive or not and the frequency of its execution whether it lies in hot path or not .
therefore we design pra as follows.
first we use a cost model to estimate the expensiveness of a change instruction.
if th e change touches performance sensitive conditional expression it is also considered expensive.
then we estimate the frequency of the ch ange instruction.
with the two estimations we index them into a r isk matrix to assess the risk level.
the risk levels for the entir e patch can then be aggregated into a single risk score.
.
.
cost modeling to establish a systematic and efficient estimation regardin g the expensiveness information a static cost model for differe nt operations is necessary.
the cost modeling will determine wheth er a change instruction is expensive or not.
since the purpose of the modeling emphasizes on relative cos t rather than absolute cost we express cost in abstract unit denoted as instead of actual cpu cycles.
with simple architecture mod el and heuristics we build a basic cost table for different ins tructions based on its type and operands.
for example add instruction has cost multiply has cost 4 andcall instruction has the cost equal to calling convention overhead plus the callee s cost .
a basic block s cost is a sum of the cost of those instructions that live inside the basic block.
for control flows we adopt worst case analysis.
for example the cost of a function or a loop body is the maximum cost among the static paths.
next we assume if an operation has cost and it s executed times the aggregated cost is 10 .
while this assumption doesn t account for factors such as compiler optimization it is a go od start for the purpose of risk assessment.
then for a loop if its trip count maximum number of iterations can be statically inferred we multiply the loop body s cost by the trip count as the loop s cost .
otherwise the loop is considered to be potentially expensive.64table risk matrix of a change s expensiveness and frequen cy frequencyexpensivenessfrequent normal rare expensive extreme high moderate normal high moderate low minor moderate low low with this model we can obtain cost in terms of whereas pra needs to gauge whether the cost is expensive or not.
we use thr esholds to convert the cost into level.
such thresholds can be co nfigured or computed automatically by running the cost model on t he whole program to obtain cost distribution for functions and basic blocks.
then the cost of a change is ranked in the distributio n to convert to expensiveness.
besides this static model we als o allow users to add dynamic profile information or domain knowledge to make pra more accurate.
but this is only optional for pra.
the above modeling mainly deals with addcode change type.
code change can also be deleting or replacing statements.
fo r delete type changes we can offset their direct cost from the total cost.
in current implementation we do not want to make pra aggressive that miss potential performance regression issue s due to inaccurate offsetting.
therefore the cost of a delete chan ge is by default .
the theoretical cost for a replace type change would be new.cost old.cost .
but for similar reason the cost of a replace change is the cost of new program elements.
the exception is if a delete or replace change touches performance sensitive va riables or conditions the change cost is directly assigned to be exp ensive.
.
.
performance sensitive condition variable condition expressions need special attention.
conditiona l branch instruction itself is rarely costly.
but as seen from the rea l world study a condition can significantly influence whether an exp ensive path will be taken or not.
we define a branch condition to be performance sensitive if its intra procedural paths su ccessors have dramatic cost difference which is estimated using abo ve cost model.
for example the branch condition for the first ifstatement in function create sort index in figure is a performance sensitive condition.
change affecting such performance se nsitive condition is considered expensive.
for performance critical variable we currently only consi ders variable that can affect loop termination condition.
we lea ve systematically identifying performance critical variable su ch as the table index variable in future work.
.
.
frequency estimation in addition to estimating the cost of change content we shou ld also analyze whether the change lies in hot path.
to do this w e first analyze the intra procedural scope that a change lies i n. if the change is enclosed in any loop and the trip count of this loop i ncluding all its parent loops can be statically determined t he execution frequency of this change instruction is estimated by th e product of these trip counts.
otherwise if any of enclosing loop has non determined trip count it is considered to be possibly e xecuted frequently.
similarly code change that lies in recursive fu nctions is also assessed to be potentially frequently executed.
next we examine the call path backward that could potential ly reach the function where the change is located and perform si milar frequency estimation for each call site.
in implementation the level of call path length is bounded.
given the estimation we conc lude if a change lies in the context that may be frequently execute d using a ranking of the frequency count.mapper filter origin new patch patch parser performance risk analyzer performance regression testing profile db code repository figure architecture of our pra implementation perfscope .
.
risk matrix combining the above two pieces of information we use a risk matrix to assess the risk.
such matrix can be extend ed to express more expensiveness frequency categories and risk levels.
the output of pra is therefore the risk level distributions.
we also calculate a simple risk score based on the distribution risk score nextreme nhigh nmoderate nlow practitioners can then define testing target selection crit eria against the risk level distribution or summary score.
for example c ommits withnextreme nhigh or whose risk score exceeds require comprehensive testing.
like in choosing the cri teria in performance testing to judge regression for different s oftware these criteria may also require some initial tuning.
.
.
indirect risk as section shows in addition to direct performance impact there are also a number of risk commits that indirectly affec t performance via data flow and control flow to their forward slice the program subset that may be affected by a given program point.
the refore we extend the basic pra with the ability to analyze such cases named pra slicing .
we use static program slicing techniques to compute a forward slice of the change set.
then we check if a ny element in the slice is performance sensitive or not using th e logic as described in .
.
.
the final risk level is only assigned on ce to the change instruction instead of its slice.
the slicing i s interprocedural and bounded to a limited depth.
while pra slicing can catch more complicated risky commits it may also recommend excessive commits due to the imprecisions in the slicing.
more over computing precise slice can be expensive.
consequently praslicing remains an extension to the main pra analysis and is disabled by default.
.
implementation we develop a tool called perfscope that implements the proposed pra on top of the llvm compiler infrastructure .
the tool is released in open source at .
.
architecture we first briefly describe the architecture of perfscope.
it co nsists of five components figure .
parser parses patch information regarding the changed files lines and types add delete change .
for the change type unified diff65file contains addanddelete but no replace .
the parser will pair delete with addand reduce them to replace .
mapper extracts debug information of a program build search trees and map given lines to the program constructs if any w hich are fed to the filter.
filter prunes out insignificant change such as stylish change or renaming.
if all changes in the entire commit are filtered th e commit will be considered trivial and not fed to the analyzer.
profile database optional allows users to incorporate profile information and domain knowledge that may make pra more accurate.
such profile can be practically incorporated becaus e for large software the cost of many functions doesn t change ve ry frequently.
therefore the profile does not need frequent updat e. but note that pra already provides a cost model .
.
.
the profile information is only optional.
interestingly we obt ained the profiles for several software using a popular low overhead systemwide profiler oprofile and found the expensive function list has a large portion of overlap with the list computed by pra.
performance risk analyzer runs pra or pra slicing on the language constructs remained after filtering.
it computes t he risk level e.g.
extreme high moderate of a change using a risk matrix like in table .
the output of the analyzer is a risk leve l distribution from all changes in the commit and a summary risk sc ore. the two results can be used by performance testing practitio ners to determine the performance testing strategy for the commit.
.
challenges there are two key challenges in implementing perfscope.
mapping raw information in patch files produced by standard tools likediff are language agnostic and in line granularity.
but pra works on language constructs.
for example it needs to know the change on line corresponds to an ifstatement.
llvm provides debugging information such as line number fil e path attached in instructions.
we use this information to bu ild a search tree.
we first find the compile unit for the changed sour ce files then match the top level constructs such as functions i nside that unit and finally the corresponding instruction s .
filtering there are changes on non source files such as the documentations test cases.
it is unlikely for them to introduc e performance regression.
we predefine a set of source code suffixes e .g.
.c .cpp to filter non source changes.
we also prune changes on source files that essentially do not alter the program e.g.
add comments rename variables .
we only perform safe simple chec king instead of trying to determine general equivalence of two pr ograms which is undecidable.
algorithm shows the filtering logic.
.
ev aluation this section evaluates the effectiveness and efficiency of o ur pra implementation perfscope .
it consists of six parts.
first we test whether perfscope is able to report these problematic c ommits from our real world issue study.
second to address the over fitting concern we also evaluate pra using new commits from both studied software and unstudied ones.
third we evaluat epraslicing extension and also compare our design with random test target selection.
fourth we estimate the practical testing co st savings with pra.
fifth we show the sensitivity of parameters used in the experiment.
the last part shows the overhead of perfscope.
.
subject software six large scale up to millions of lines of code popular op ensource software are used as our subjects.
they range from dat abase system mysql postgresql compiler gcc web caching server apache squid and javascript engine v8 used in ch rome .algorithm determine if a change is trivial input change c source code suffix set suffixes before revision program oldp after revision program newp output true if cis trivial false otherwise ifc.file.suffix suffixes then return true end if map get instructions at a line in a program new instrs map c.new line newp old instrs map c.old line oldp ifnew instrs.empty andold instrs.empty then change only comments spaces etc.
return true end if ifnew instrs.size negationslash old instrs.size then change added or deleted instructions return false end if fori 1tonew instrs.size do diff compare two instructions opcodes and operands ifdiff old instrs new instrs true then return false end if end for return true table subject software.
software loc studied?
mysql .2m yes postgresql 651k yes apache httpd 220k no squid 751k no gcc .6m no v8 680k no among them gcc apache squid and v8 are notused in our realworld study.
table summarizes the details.
.
methodology table benchmarks and regression thresholds used for subject software.
.
.
describes how we obtain the threshold s. software benchmarks threshold mysql dbt2 sysbench sql bench postgresql dbt2 sysbench pgbench apache httpd specweb2005 autobench ab squid web polygraph autobench gcc cp2k spec cpu2006 v8 octane sunspider .
.
ground truth for new commits for the studied commits we already know they caused performance regressions.
but for the new commits since they are ta ken from recent code repository few feedback on them exists.
we need to get the ground truth for each commit with respect to whether it may introduce performance regression.
therefore we run mu ltiple standard moderately intensive performance benchmarks on each compiled revision of subject software.
these benchmarks ar e often used internally and in user reported regression cases.
eac h benchmarking is run multiple times.66table coverage of studied problematic commits.
softwarebuggy pra rev.
ratio mysql postgresql total with the benchmarking results we can compute the performan ce deviation of a commit from the previous commit.
then if the de viation exceeds certain threshold the commit introduces perf ormance regression.
therefore what thresholds to use for judging pe rformance regression is crucial.
however there is inevitable b ias in choosing the criteria because performance regression is a fundamentally subjective definition.
different practitioners and software use different judgment.
for example in our real world stud y the user reported performance regression issues contain a var iety of criteria even for the same software.
to reduce the inevitable selection bias we refer to the comm on practices.
in particular we sample each software s issue t racker code commit comment and developer posts to see what degree of performance deviation starts to attract practitioners at tention.
we then choose the minimum as our criteria.
table lists the benchmarks and thresholds we use for each software.
sometimes the commit causing repeatable regression i.e.
the regression is observed consistently when the testing is exe rcised several times may not be a performance bug but rather an expe cted performance behavior e.g.
a patch adding authentication step to existing work flow introduces performance overhead .
it is u p to developers to decide whether a repeatable regression is exp ected or not.
pra isnota performance bug detector.
it mainly serves performance testing and therefore only target on repeatabl e regression rather than performance bug.
therefore if pra recommends a commit that can indeed manifest repeatable regression in p erformance testing the recommendation is considered to be usefu l. .
.
setup the benchmarking is carried out on three dedicated machines each with intel i7 quad core cpu .
ghz 16gb ram.
the performance overhead measurement is run on the same machines.
a dditionally as pra outputs the distribution of the change set s risk levels and a score we need a criterion for recommending perf ormance regression testing.
we set it to be if risk score equat ion is larger than .
.
evaluates the sensitivity of this cr iterion.
.
evaluation on studied commits table presents the coverage on the studied problematic com mits.
commits from mysql issue study are omitted because these changes are for source languages other than c c whi ch our tool currently supports.
chrome is not evaluated becaus e the compiler our analysis bases on llvm clang cannot reliably compile chrome into whole program llvm bitcode files for analys is rather than because of our analysis.
as the table shows pra can report majority of these regressing commits.
although the basic pra does not implement slicing it still can capture cases in the indirect catego ry in table .
this is because the categorization only focuses on th eroot cause .
it can be the case that a change impacts performance indirectly but happens to lie in a loop.
a few cases are missed by pra either because of long propagation impact or complicated domain knowledge.
listing is su ch an example.
more detailed profile information and deeper ana lysis are needed to be able to alert on them.listing commit not alarmed by pra voidinit read record read record info thd thd the patch sets mmap flag which later causes a function pointer to be changed to mmap.
if table s tmp table tmp table ... void table file extra ha extra mmap .
evaluation on new commits since our pra design is guided by a real world issue study it might be tailored for these examined buggy commits.
therefo re we also evaluate the tool on new commits from both studied and unstudied software.
table presents the result.
theafter filtering column is the number of commits remaining after pruning.
the filtered commits by our tool either only change non source files or only have insignificant changes on source files.
interestingly filtering already reduces a significant numb er of commits not worth consideration for performance regression te sting.
for example in apache more than one third of the commits are just updating documentation files or code styles.
we manually che cked the filtered commits are indeed trivial.
our tool perfscope can successfully reduce at least of the testing candidates if no filtering is conducted in ex isting testing and alarm of the new risky commits.
in other words with our tool developers only now need to test of the orig inal commits and still be able to alert of the risky commits .
this means our pra design can significantly reduce performance testing overhead while preserving relatively high coverag e. from the table we can also read the number of commits that are reported by perfscope but not confirmed by our benchmarki ng from rec.
commits risky commits miss .
however they should notbe interpreted false alarms for two reasons.
first perfscope is not a bug detection tool but only to reduce testi ng overhead.
these additionally recommended testing targe ts need to be tested anyway in the original performance testing sche me without using pra.
second the risky commits in the table are lower bounds because of the limitation of our benchmarking.
therefore some of these additional commits might turn out to be in deed risky if tested more comprehensively.
.
extension and alternative solution perfscope also implements an extension to pra pra slicing .
.
.
in addition to the basic analysis pra slicing also performs forward slicing of each change in the commit and checks if any element in the slice is performance sensitive or not.
table also shows the evaluation of pra slicing .pra slicing performs deeper analysis and as a result has higher coverage but at the cost of lower reduction percentage .
a simple alternative to pra is random test target selection.
the probability for this approach to achieve the same or better r esult as our tool in table is only .
calculated using script2 .
.
practical cost saving the objective of pra is to reduce the testing target set to the risky commits for performance regression testing.
previou s sections mainly evaluate the number of reduced testing target.
how much does the reduction translate to actual testing cost sav ing?
it depends on how comprehensive the original testing is carrie d out.
evaluation of perfscope on new commits.
the filter ing is done automatically by our tool with algorithm .
for re duction rate and testing saving larger number is based on the com mits smaller number is based on the commits.
softwaretest risky after pra pra slicing commits commits filtering rec.
commits miss testing rec.
commits miss testing reduction coverage savings hrs.
reduction coverage savings hrs.
mysql postgresql gcc v8 apache squid total risk score thresholdpercentage reduction coverage figure sensitivity of perfscope s reduction rate and cov erage rate of the new commits evaluation to the recommendation criterion risk score threshold .
in our experiment we run the benchmarks using recommended s ettings from popular performance testing guides or the tools.
therefore we calculate the expected testing cost saving ac cording to our setup.
as we run multiple benchmarks for each softw are we use the average running cost as the per iteration testing c ost.
the per commit cost is calculated by multiplying the per iter ation cost with iterations as used in our experiment.
in specifics for mysql and postgresql the per commit testing cost is hours for gcc it is hours for v8 it is minutes for apache it i s .
hours for squid it is hours table lists the result.
the lower saving is calculated assu ming the original testing already skips all non essential commi ts.
the higher saving assumes no filtering is conducted.
perfscope i s most useful for software whose original performance regression testing cost is big.
for example for gcc the saving can be almost two months.
however for v8 the saving is limited because the be nchmark we use only takes minutes to test one revision.
admitted ly for software whose performance testing overhead is very sma ll perfscope does not save much.
directly running performance testing on every commit is then a better option.
one possible usag e scenario for this type of software is to run perfscope as cro ss reference with the performance testing result.
.
sensitivity since the output of pra is risk level distribution and a summary score a criterion based on the output is necessary to decide whether to recommend the given code commit to be tested.
the evaluati on in previous sections used the risk score threshold as the criteria.
figure measures the reduction and coverage sensitivity to the risk score threshold.
in general reduction rate increases as the risktable perfscope running time in seconds and breakdown.
the numbers in parentheses are for pra slicing .
softwareloading analysis total module pra slicing pra slicing mysql postgresql gcc v8 apache squid score criteria increases because higher threshold would re sult in fewer number of commits to be recommended and thus achieving higher reduction in contrast coverage decreases as the cr iteria increases.
such variety means like performance regression t esting result judgment criteria the selection criteria requires initial tuning.
but as seen from the figure the sensitivity in certain th reshold ranges is is small because problematic and regular commits u sually have quite different risk level distribution.
.
performance as the primary goal of pra is to reduce performance testing overhead through testing target selection the analysis it self shouldn t become a new bottleneck.
table shows that in the evaluated software perfscope s a verage execution time is within minutes for pra and minutes for pra slicing .
the loading module breakdown is the time to load llvm object files for analysis using llvm api parseirfile .
it is a constant cost and occupies a large portion of the total time for both pra andpra slicing .
.
related work performance regression testing there are case studies and research effort on performance regression testing in softwar e systems .
to name a few details the linux kernel performance testing project to catch kernel performanc e regression issues.
shares the experience in automating regre ssion benchmarking for the mono project.
proposes a model ba sed performance testing framework to generate appropriate wor kloads.
offers a learning based performance testing framewor k that automatically selects test input data based on learned rule s. uses symbolic execution to generate load test suites that ex pose program s diverse resource consumption behaviors.
these efforts focus building better performance regressio n testing infrastructure and test cases.
our work assumes the exis tence of good performance testing infrastructure and test cases and improves the testing efficiency by prioritizing testing targe t. performance bug detection and analysis a wealth of literature exists on performance analysis with regard to performance d ebug 68ging performance bug detection performance regression testing result analysis .
similar to these work the ultimate goal of our work is to help uncover performance problems.
but we do notattempt to detect performance regression bugs or provide forensic diagnosis but target on recommending risky commit for performance regression te sting.
for the performance regression testing result analysi s work we complements the work by improving performance testing ef ficiency by better utilizing the testing resources on risky co mmit.
regression testing efficiency much work has been done to reduce functional regression testing cost by test case selection test suite reduction and test cases prio ritization .
test case selection realizes this through s electing a subset of test cases in the test suite based on test case prope rty and code modification information.
test suite reduction works o n removing redundancy in test suite.
test case prioritization o rders test case execution in a way to meet time constraints in hope to det ect fault faster.
different from these work our goal is to reduce performance regression testing overhead via testing target prioritizati on.
in functional regression testing work that also analyzes code mod ifications focuses on code coverage.
but in the context of perform ance more important is information such as whether an operation i s expensive or lies in critical path.
the analysis we propose is s pecifically for assessing commits performance risk.
there are also practices on improving performance regressi on testing efficiency mainly through hierarchical test case or ganization.
we differ from them in that we take a white box approach to prioritize test target by analyzing commit content.
impact analysis there is fruitful work on software change impact analysis techniques to compute the set of program elements t hat may be affected by the change impact set .
they can be broadly divided into three categories s tatic analysis based dynamic execution based and history based .
our proposed method is inspired by these work.
the key difference is that we focus on the performance risk implication of change instead of the impact set .pra assesses the risk of a code change to introduce performance regression in addition to t he impact set.
additionally many impact analysis work focuses on functio n level.
but pra needs to examine more fine grained statement level for detailed analysis.
this not only poses challenges in the analysis but also on mapping from textual changes to the correspon ding programming constructs .
.
worst case execution time analysis analyzing the worst case execution time wcet of a task to be executed on a spe cific hardware is a necessary process for reliable real time system because of its stringent timing constraints.
our pra is similar as the static approach in wcet analysis.
for example both pra and wcet analysis needs to use control flow information and bound calculation to determine the worst case execution path.
however wcet analysis mainly applies to real time systems as they have restricted form of programming e.g.
no recursio n allowed .
pra works on regular performance critical software written in standard c c that supports generic programming con structs.
the biggest size of tasks analyzed by wcet analysis tool is around 50k loc .
pra can scale to millions of loc for regular software.
more importantly pra does not aim to predict the absolute performance bound for the entire program.
instead pra calculates the relative performance riskintroduced by given code change to reduce testing target set.
the analysis is focused in code ch angescopes.
this makes pra light weight enough to fit in the performance testing cycle.
in contrast wcet analysis needs to obt ain a safe bound as a worst case guarantee for the entire program .
it therefore requires careful modeling of underlying archite ctural behaviors e.g.
branch prediction and often requires user a nnotations e.g.
loop bounds flow facts which is very expensive to pe rform on a per commit basis to be used by performance testing.
.
limitations and discussions there are limitations in the current implementation that we are considering for future work.
first our pra is designed to recommend straightforward performance regression issues.
while this makes the analysis ligh tweight the analysis may not accurately assess the risk of sophistic ated performance regression issues such as resource contention ca ching effect.
second although our cost model is generic for both c omputation and i o detailed modeling and profiling are needed if i o behavior is of particular interest.
therefore our current m odeling has limited applicability to software like os kernel.
also o ur model does not apply to networked software.
third since for a comm it that is considered to be potentially risky pra knows the program points that are risky.
with the risky program points and test cases coverage information we are extending the analysis to not o nly select testing target but also recommend which test case may potentially expose the performance issue in the risky version .
.
conclusions and future work in this paper we propose a new approach performance risk an alysis pra to improve performance regression testing efficiency through testing target prioritization.
it analyzes the ris k of a given code commit in introducing performance regression.
to gain deep understanding of perilous commits code characteristics we conduct a study on randomly sampled real world performance regression issues from three large popular software.
based on the insights from the study we propose a pra design and implement a tool perfscope.
evaluation on the studied problematic com mits shows perfscope can successfully recommend of them for testing.
we also evaluate the tool on newcommits that are not studied.
perfscope significantly reduces the testing overh ead by recommending only of the commits and is still able to cover of the risky commits.
experiment demonstrates the analysis is lightweight.
the source code of perfscope is rel eased at .
.