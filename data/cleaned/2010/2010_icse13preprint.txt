assisting developers of big data analytics applications when deploying on hadoop clouds weiyi shangy zhen ming jiangy hadi hemmatiy bram adamsz ahmed e. hassany patrick martinx ysoftware analysis and intelligence lab sail school of computing queen s university kingston canada zd epartement de g enie informatique et g enie logiciel polytechnique montr eal montr eal qu ebec canada xdatabase systems laboratory school of computing queen s university kingston canada fswy zmjiang hhemmati martin ahmed g cs.queensu.ca bram.adams polymtl.ca abstract big data analytics is the process of examining large amounts of data big data in an effort to uncover hidden patterns or unknown correlations.
big data analytics applications bda apps are a new type of software applications which analyze big data using massive parallel processing frameworks e.g.
hadoop .
developers of such applications typically develop them using a small sample of data in a pseudo cloud environment.
afterwards they deploy the applications in a large scale cloud environment with considerably more processing power and larger input data reminiscent of the mainframe days .
working with bda app developers in industry over the past three years we noticed that the runtime analysis and debugging of such applications in the deployment phase cannot be easily addressed by traditional monitoring and debugging approaches.
in this paper as a first step in assisting developers of bda apps for cloud deployments we propose a lightweight approach for uncovering differences between pseudo and large scale cloud deployments.
our approach makes use of the readily available yet rarely used execution logs from these platforms.
our approach abstracts the execution logs recovers the execution sequences and compares the sequences between the pseudo and cloud deployments.
through a case study on three representative hadoop based bda apps we show that our approach can rapidly direct the attention of bda app developers to the major differences between the two deployments.
knowledge of such differences is essential in verifying bda apps when analyzing big data in the cloud.
using injected deployment faults we show that our approach not only significantly reduces the deployment verification effort but also provides very few false positives when identifying deployment failures.
index terms big data analytics application cloud computing monitoring and debugging log analysis hadoop i. i ntroduction big data analytics applications bda apps are a new category of software applications that leverage large scale data which is typically too large to fit in memory or even on one hard drive to uncover actionable knowledge using largescale parallel processing infrastructures .
the big data can come from sources such as runtime information about traffic tweets during the olympic games stock market updates usage information of an online game or the data from any other rapidly growing data intensive software system.
for instance ebay1has deployed bda apps to optimize the search of products by analyzing over pbs data using more than cpu cores .
last checked feburary .over the past three years we have been working closely with bda app developers in industry.
we noted and found that developing bda apps brings many new challenges compared to traditional programming and testing practices.
among all challenges in different phases of bda app development the deployment phase introduces unique challenges related to verifying and debugging the bda executions as bda app developers want to know if their bda app will function correctly once deployed.
similar observations were recently noted in an interview of professional bda app developers at microsoft .
in practice the deployment of bda apps in the cloud follows these three steps developers implement and test the bda app in a small or pseudo cloud using virtual or physical machines environment using a small data sample developers deploy the application on a larger cloud with a considerably larger data set and processing power to test the application in a real life setting and developers verify the execution of the application to make sure all data are processed and all jobs are successful.
the traditional approach for deployment verification is to simply search for known error keywords related to unusual executions.
however such verification approaches are very ineffective in large cloud deployments.
for instance a common basic approach for identifying deployment problems is searching for killed jobs in the generated execution logs the output of the internal instrumentation of the underlying platform hosting the deployed application .
however a simple keyword search would lead to false positive results since a platform such as hadoop may intervene in the execution of a job kill it and restart it elsewhere to achieve better performance or it might start and kill speculative jobs .
considering the large amount of data and logs such false positives rapidly overwhelm the developer of bda apps.
in this paper we propose an approach for verifying the runtime execution of bda apps after deployment.
the approach abstracts the platform s execution logs from both the small pseudo and large scale cloud deployments groups the related abstracted log lines into execution sequences for both deployments then examines and reports the differences between the two sets of execution sequences.
ideally these two sets should be identical for a successful deployment.
however due to framework configurations and data size differences the978 c ieee icse san francisco ca usa accepted for publication by ieee.
c ieee.
personal use of this material is permitted.
permission from ieee must be obtained for all other uses in any current or future media including reprinting republishing this material for advertising or promotional purposes creating new collective works for resale or redistribution to servers or lists or reuse of any copyrighted component of this work in other works.402underlying platform may execute the applications differently.
among the delta sets of execution sequences between these two sets we filter out sequences that are due to well known platform related in our case study hadoop differences.
the remaining sets of sequences are potential deployment failures anomalies that should be reported and carefully examined.
we have implemented our approach as a prototype tool and performed a case study on three representative hadoop bda apps.
the choice of hadoop is due to it being one of the most used platforms for big data analytics in industry today.
however our general idea of using the underlying platform s logs as a means for bda app monitoring in the cloud is easily extensible to other platforms such as microsoft dryad .
the case study results show that our log abstraction and clustering into execution sequences not only significantly reduces the amount of logs by between to that should be verified but it also provides much higher precision for identifying deployment failures anomalies compared to a traditional keyword search approach commonly used in practice today .
in addition practitioners who have used our approach in practice have noted that the reporting of the abstracted execution sequences rather than raw log lines provides a summarized context that dramatically improves their efficiency in identifying and investigating failure anomaly.
the rest of this paper is organized as follows.
we present a motivating example in section ii.
we present hadoop the platform that we studied in section iii.
we present our approach to summarize logs into execution log sequences in section iv.
we present the setup for our case studies in sections v. we present the results of our case study in section vi.
we discuss other features of our approach in section vii and discuss the limitations of our approach in section viii.
we present prior work related to our approach in section ix.
finally we conclude the paper in section x. ii.
a m otivating example we now present a hypothetical but realistic motivating example to better explain the challenges of deploying bda apps in a cloud environment.
assume developer ian developed a bda app that analyzes the user information from a large scale social network.
ian has thoroughly tested the app on an in house small scale cloud environment with a small sample of testing data.
before officially releasing the app ian needs to deploy the app in a large scale cloud environment and run the app with reallife large scale data.
after the test run of the app in the real cloud setup ian needs to verify whether the app behaves as expected or not in the testing environment.
ian followed a traditional approach to examine the behaviour of the app in the cloud environment.
he leveraged the logs from the underlying platform e.g.
hadoop to find whether there are any problematic log lines.
after downloading all the logs from the cloud environment ian found that the logs are of enormous size because the cloud environment contains thousands of nodes and the processed real life data is in pb scale which makes the manual inspection of the logsimpossible.
therefore ian performed a simple keyword search on the logs.
the keywords are based on his own experience of developing bda apps.
however the keyword search still returns thousands of problematic log lines.
by manually exploring the problematic log lines ian found that a large portion of the log lines do not indicate any problematic executions i.e.
false positives .
for example the run time scheduler of the underlying platform often kills remote processes and restarts them locally to achieve better performance.
however such kill operations lead to seemingly problematic logs that are retrieved by his keyword search.
moreover for each log line ian must trace through the log files across multiple nodes to gain some context about the generated log files and in many instances he discovers that such log lines are expected and are not problematic ones .
in short identifying deployment problems of the bda app is excessively difficult and time consuming.
moreover this difficulty increases considerably as the size of the analyzed data grows and the size of the cloud increases.
from the above example we observe that verifying the deployment of bda apps in a cloud environment with largescale data is challenging.
although today developers primarily use grep to locate possible troublesome instrumentation logs uncovering the related context of the troublesome logs is still challenging with enormously large data as noted in recent interviews of bda app developers .
in the following sections we present our approach which summarizes the large amount of platform logs and presents them in tables where developers can easily note troublesome events and where they are able to easily view such events in the context of their execution since the table shows summarized execution sequences .
iii.
l arge scale data analysis platforms hadoop hadoop is one of the most widely used platforms for the development of bda apps in practice today.
we briefly present the programming model of hadoop then present the hadoop logs that we use in our case studies.
a. the mapreduce programming model hadoop is an open source distributed platform that is supported by yahoo!
and is used by amazon aol and a number of other companies.
to achieve parallel execution hadoop implements a programming model named mapreduce.
this programming model is implemented by many other cloud platforms as well .
mapreduce is a distributed divide and conquer programming model that consists of two phases a massively parallel map phase followed by an aggregating reduce phase.
the input data of mapreduce is broken down into a list of key value pairs.
mappers processes assigned to the map phase accept the incoming pairs process them in parallel and generate intermediate key value pairs.
all intermediate pairs having the same key are then passed to a specific reducer process assigned to the reduce phase .
each reducer performs computations to reduce the data to one403single key value pair.
the output of all reducers is the final result of a mapreduce run.
to illustrate mapreduce we consider an example mapreduce process that counts the frequency of word lengths in a book.
mappers take each single word from the book and generate a key value pair of the form word length dummy value .
for example a mapper generates a key value pair of hello from the input word hello .
afterwards the key value pairs with the same key are grouped and sent to reducers.
each reducer receives the list of all key values pairs for a particular word length and hence can simply output the size of this list.
if a reducer receives a list with key for example it will count the number of all the words with length .
if the size is n it generates an output pair n which means there are nwords with length in the book.
b. components of hadoop hadoop has three types of execution components.
each component has logging enabled in it.
such platform logging tracks the operation of the platform itself i.e.
how the platform is orchestrating the mapreduce processing .
today such logging is enabled in all deployed hadoop clusters and it provides a glimpse into the inner working mechanism of the platform itself.
such inner working mechanism is impacted by any problems in the cloud on which the platform is executing.
the three execution components and a brief example of the logs generated by them are as follows job.
a hadoop program consists of one or multiple mapreduce steps running as a pipeline.
each mapreduce step is a job in hadoop.
a jobtracker is a process initialized by the hadoop platform to track the status of the jobs.
the information tracked by the jobtracker includes the overall information of the job e.g.
input data size and the high level information of the execution of the job.
the high level information of the job s execution corresponds to the executions of map and reduce.
for example a job log may say that the job is split into map tasks and map taskid id is finished at time t1 .
task.
the execution of a job is divided into multiple tasks based on the mapreduce programming model.
therefore a task can be either a map task that corresponds to the map in the mapreduce programming model or a reduce task.
the hadoop platform groups a set of map or reduce executions together to create a task.
therefore each task contains more than one execution of map or reduce.
similar to the jobtracker the tasktracker monitors the execution of a task.
for example a task log may say received commit of task id id .
attempt.
to support fault tolerance the hadoop platform allows each task to have multiple trials of execution.
each execution is an attempt.
typically only when an attempt of a task has failed another attempt of the same task will start.
this restart process continues until the task is successfully completed or the number of failed attempts is larger than a threshold.
however there areexceptions such as speculative execution which we discuss later in this paper.
the attempt is also monitored by the tasktracker and the detailed execution information of the attempt such as reading data for map task with taskid id is recorded in the attempt logs.
the job task and attempt logs form the source of information used by our approach.
we use the former kinds of logs instead of application level logs since such logs provide information about the inner working of the platform itself and not the application which is assumed to be correctly implemented for our purposes.
in particular the platform logs provide us with information about any deployment problems.
iv.
a pproach the basic idea behind our approach is to cluster the platform logs to improve their comprehensibility and to help understand and flag differences in the run time behaviour.
as mentioned before our approach is based on the analysis of platform logs of bda apps.
these logs are generated by the statements embedded by the platform developers because they consider the information to be particularly important.
containing rich knowledge but not fully explored platform logs typically consist of the major system activities and their associated contexts e.g.
operation ids .
the log is a valuable resource for studying the run time behaviour of a software system since they are generated by the internal instrumentations and are readily available.
however previous research shows that logs are continuously changing and evolving .
therefore ad hoc approaches based on keyword search may not always work.
thus we propose an approach that does not rely on particular phrases or format of logs.
figure shows an overview of our approach.
our approach compares the run time behaviour of the underlying platform of bda apps in testing environment with a small testing data sample to the cloud environment with large scale data.
to overcome the enormous amount of logs generated by a bda platform and to provide useful context for the developers looking at our results we recover the execution sequences of the logs.
a. execution sequence recovery in this step we recover sequences of the execution logs.
the log sequence clustering includes three phases.
log abstraction log files typically do not follow strict formats but instead contain significant unstructured data.
for example log lines may contain the task type the execution time stamp and a free form making it hard to extract any structured information from them.
in addition to being in free form log lines contain static and dynamic information.
the static information is specific to each particular event while the dynamic values of the logs describe the event context.
we use a technique proposed by jiang et al to abstract logs.
this technique is designed to be generalizable as it does not rely on any log formats.
using the technique we first identify the static and dynamic values of the logs based on a small sample of logs.
then we apply the identified static and dynamic parts404executionlogslog linkinglogabstractionsimplifyingsequencesexecution sequence reporttesting run withsmall data run with large data in cloudexecution sequence recoveryexecution sequencedeltaexecutionlogslog linkinglogabstractionsimplifyingsequencesexecution sequence reportexecution sequence recoveryfig.
.
overview of our approach.
log abstraction log linking simplify sequences a example of log lines consists of execution time stamps task types and task identifiers.
b execution events consists of normalized execution time stamps task types and normalized task identifiers.
c execution log sequences representing a sequence of system executions with the same taskid d final execution sequences after simplifying sequences.
the final sequences are without repetition and permutation.
figure process of our problem detection approach.
figure an example of our approach.the log abstraction phase abstracts log lines into execu tion events.
in this phase we choose dynamic values suchas id to link the log lines into a sequence.
the linking isbased on the heuristic on the name of the dynamic values.for our example taskid will be used for event linking sincetaskid contains string id .
therefore line and line inthe input data in figure a can be linked together sincethey contain the same taskid.figure c shows the result sequence after abstracting thelogs and linking them into sequences using the taskid val ues.
in the event linking result in figure c events e1 e2 e3 e5 and e6 are linked together note that event e3 hasbeen executed twice and event e1 e2 e4 e6 are linkedtogether since the same taskid values are shared.
.
.
eliminating repetitionsthere can be event repetitions in the existing sequencescaused by loops.
for example for sequences about readingdata from a remote node there would be repeated eventsabout keeping fetching the data.
similar log sequences thatinclude di erent times of the same events are considereddi erent sequences although they indicate the same sys tem behaviour in essence.
these repeated events need to besuppressed to ease the analysis.
we use regular expressiontechniques to detect and suppress the repetitions.
for theexample shown in figure the sequence e1 e2 e3 e3 e5e6 our technique would detect the repetition of e3 andsuppress this sequence into e1 e2 e3 e5 e6 .table example of log lines log line1time task trying to launch taskid 01a2time task trying to launch taskid 0773time task jvm taskid 01a4time task reduce taskid 01a5time task jvm taskid 0776time task reduce taskid 01a7time task reduce taskid 01a8time task progress taskid 0779time task done taskid 07710time task commit pending taskid 01a11time task done taskid 01aafter eliminating looping the final log sequences are shownin figure d.table execution sequencetaskidevent sequence01ae1 e2 e3 e3 e3 e5 e6077e1 e2 e4 e63.
failure detectionintuitively if any failure exists the cloud computing plat table execution eventseventevent template e1time t task trying to launch taskid id1 2e2time t task jvm taskid id3 5e3time t task reduce taskid id4 7e4time t task progress taskid id8e5time t task commit pending taskid id10e6time t task done taskid id9 11table execution sequence after eliminating loop ingtaskidevent sequence01ae1 e2 e3 e5 e6077e1 e2 e4 e6form would generate extra logs.
the extra logs containevent sequences indicating the process of error message andfault recovery.
therefore di erent event sequences whichreflect di erent system behaviours should be recovered be tween di erent runs of an application with and without fail ures.
several approaches that identify the di erent eventsequences in logs can be used to identify system failures.
.
.
sequence counts variancesequence counts variance scv is the coe cient of vari ance of the number of sequences among multiple runs of thesame application.
it is defined as the ratio of the standarddeviation to the mean scv intuitively the bigger the variance is higher the proba bility that a failure exists.
before running an applicationperiodically on a cloud computing platform a certain num ber of runs of the program without problems are requiredto setup a baseline of the scv.
after deploying the pro gram any scv among the number of consecutive runs thatis larger than the baseline would be considered a warning offailure.for example if we setup our baseline based on three failure free runs and recover and log sequences respectively the baseline scv would be .
.
if a following run gen erates log sequences we would calculate the scv basedon the number and number and the first num ber are from the previous two runs .
the scv wouldbe .
which is smaller than the baseline .
andwould indicate that the new run is failure free.
if anotherrun has failure and generates log sequence the scv cal culated by the number and would be .
whichis larger than the baseline.
an alert would be sent to theadministrator for further inspection.
.
.
sequence distributiondi erent runs of an application with and without failuresintuitively have di erent sequence distributions.
therefore statistical methods to detect di erent distributions can beused for automatic failure detection.
in practice we havebaseline runs for each application without failure and wecarry out an unpaired sided t test mann whitney be tween the two runs to setup a baseline p value.
the lowerthe p value the higher probability that the new run hasfailure.
therefore every new run will be tested with theprevious failure free run to calculate the p value.
a p valuethat is larger than the baseline would indicate failures.for example we start two failure free baseline run withthe sequences distribution of and .
thep value of the t test would be .
.
if another run hasthe sequence distribution of we perform a t testbetween and the sequence distributionof the previous failure free run .
the p value is which islarger than the baseline .
and the new run is consid ered failure free.
if next run with recovered failure has thesequence distribution of the calculated p valuewould be .
which is smaller than the baseline.
there fore the administrator would receive an alert.in the following two sections we present our three case stud ies on a widely used cloud computing platform and detectthree injected problems with the approach presented in thissection.
.
case study setupwe present the cloud computing platform that we chose the subject programs the experimental environment and theinput data.
.
cloud computing platform hadoopthis sub section introduces hadoop a widely used cloudcomputing platform that we choose for our case studies.
webriefly present the programming model and the hadoop logthat we use in our case studies.
.
.
programming modelhadoop is an open source cloud computing platform that is supported by yahoo!
and is used by amazon aoland a number of other companies as their cloud computingplatform.
to achieve parallel execution hadoop implementsa programming model named mapreduce.mapreduce is a distributed divide and conquer program ming model.
the programming model consists of two phases a massively parallel map phase followed by an aggregating reduce phase.
the input data for mapreduce is brokendown into a list of key value pairs.
mappers processes as signed to the map phase accept the incoming pairs pro cess them in parallel and generate intermediate key valuepairs.
all intermediate pairs having the same key are thenpassed to a specific reducer process assigned to the re duce phase .
each reducer performs computations to re duce the data to one single key value pair.
the output ofall reducers is the final result of a mapreduce run.to illustrate mapreduce we consider an example mapre duce process which counts the frequency of word lengths infigure process of our problem detection approach.
figure an example of our approach.the log abstraction phase abstracts log lines into execu tion events.
in this phase we choose dynamic values suchas id to link the log lines into a sequence.
the linking isbased on the heuristic on the name of the dynamic values.for our example taskid will be used for event linking sincetaskid contains string id .
therefore line and line inthe input data in figure a can be linked together sincethey contain the same taskid.figure c shows the result sequence after abstracting thelogs and linking them into sequences using the taskid val ues.
in the event linking result in figure c events e1 e2 e3 e5 and e6 are linked together note that event e3 hasbeen executed twice and event e1 e2 e4 e6 are linkedtogether since the same taskid values are shared.
.
.
eliminating repetitionsthere can be event repetitions in the existing sequencescaused by loops.
for example for sequences about readingdata from a remote node there would be repeated eventsabout keeping fetching the data.
similar log sequences thatinclude di erent times of the same events are considereddi erent sequences although they indicate the same sys tem behaviour in essence.
these repeated events need to besuppressed to ease the analysis.
we use regular expressiontechniques to detect and suppress the repetitions.
for theexample shown in figure the sequence e1 e2 e3 e3 e5e6 our technique would detect the repetition of e3 andsuppress this sequence into e1 e2 e3 e5 e6 .table example of log lines log line1time task trying to launch taskid 01a2time task trying to launch taskid 0773time task jvm taskid 01a4time task reduce taskid 01a5time task jvm taskid 0776time task reduce taskid 01a7time task reduce taskid 01a8time task progress taskid 0779time task done taskid 07710time task commit pending taskid 01a11time task done taskid 01aafter eliminating looping the final log sequences are shownin figure d.table execution sequencetaskidevent sequence01ae1 e2 e3 e3 e3 e5 e6077e1 e2 e4 e63.
failure detectionintuitively if any failure exists the cloud computing plat table execution eventseventevent template e1time t task trying to launch taskid id1 2e2time t task jvm taskid id3 5e3time t task reduce taskid id4 7e4time t task progress taskid id8e5time t task commit pending taskid id10e6time t task done taskid id9 11table execution sequence after eliminating loop ingtaskidevent sequence01ae1 e2 e3 e5 e6077e1 e2 e4 e6form would generate extra logs.
the extra logs containevent sequences indicating the process of error message andfault recovery.
therefore di erent event sequences whichreflect di erent system behaviours should be recovered be tween di erent runs of an application with and without fail ures.
several approaches that identify the di erent eventsequences in logs can be used to identify system failures.
.
.
sequence counts variancesequence counts variance scv is the coe cient of vari ance of the number of sequences among multiple runs of thesame application.
it is defined as the ratio of the standarddeviation to the mean scv intuitively the bigger the variance is higher the proba bility that a failure exists.
before running an applicationperiodically on a cloud computing platform a certain num ber of runs of the program without problems are requiredto setup a baseline of the scv.
after deploying the pro gram any scv among the number of consecutive runs thatis larger than the baseline would be considered a warning offailure.for example if we setup our baseline based on three failure free runs and recover and log sequences respectively the baseline scv would be .
.
if a following run gen erates log sequences we would calculate the scv basedon the number and number and the first num ber are from the previous two runs .
the scv wouldbe .
which is smaller than the baseline .
andwould indicate that the new run is failure free.
if anotherrun has failure and generates log sequence the scv cal culated by the number and would be .
whichis larger than the baseline.
an alert would be sent to theadministrator for further inspection.
.
.
sequence distributiondi erent runs of an application with and without failuresintuitively have di erent sequence distributions.
therefore statistical methods to detect di erent distributions can beused for automatic failure detection.
in practice we havebaseline runs for each application without failure and wecarry out an unpaired sided t test mann whitney be tween the two runs to setup a baseline p value.
the lowerthe p value the higher probability that the new run hasfailure.
therefore every new run will be tested with theprevious failure free run to calculate the p value.
a p valuethat is larger than the baseline would indicate failures.for example we start two failure free baseline run withthe sequences distribution of and .
thep value of the t test would be .
.
if another run hasthe sequence distribution of we perform a t testbetween and the sequence distributionof the previous failure free run .
the p value is which islarger than the baseline .
and the new run is consid ered failure free.
if next run with recovered failure has thesequence distribution of the calculated p valuewould be .
which is smaller than the baseline.
there fore the administrator would receive an alert.in the following two sections we present our three case stud ies on a widely used cloud computing platform and detectthree injected problems with the approach presented in thissection.
.
case study setupwe present the cloud computing platform that we chose the subject programs the experimental environment and theinput data.
.
cloud computing platform hadoopthis sub section introduces hadoop a widely used cloudcomputing platform that we choose for our case studies.
webriefly present the programming model and the hadoop logthat we use in our case studies.
.
.
programming modelhadoop is an open source cloud computing platform that is supported by yahoo!
and is used by amazon aoland a number of other companies as their cloud computingplatform.
to achieve parallel execution hadoop implementsa programming model named mapreduce.mapreduce is a distributed divide and conquer program ming model.
the programming model consists of two phases a massively parallel map phase followed by an aggregating reduce phase.
the input data for mapreduce is brokendown into a list of key value pairs.
mappers processes as signed to the map phase accept the incoming pairs pro cess them in parallel and generate intermediate key valuepairs.
all intermediate pairs having the same key are thenpassed to a specific reducer process assigned to the re duce phase .
each reducer performs computations to re duce the data to one single key value pair.
the output ofall reducers is the final result of a mapreduce run.to illustrate mapreduce we consider an example mapre duce process which counts the frequency of word lengths in fig.
.
an example of our approach for summarizing the run time behaviour of bda apps.
on full logs to abstract the logs.
figure shows an example of a log file with log lines and how we process it.
each log line contains the execution time stamp the task type and the task id.
the log lines are abstracted into six different system events as shown in figure b. the id and t identifiers indicate two dynamic values.
log linking this phase uses dynamic values such as id to link log lines into a sequence.
the linking heuristic is based on the dynamic values.
in our example taskid is used for log linking since taskid represents some kind of session id .
therefore line and line in the input data in figure 2a can be linked together since they contain the same taskid.
similar to log abstraction we also identify the linkage among a few ids based on a small sample of data then apply the linking on full data.
figure c shows the resulting sequences after abstracting the logs and linking them into sequences using the taskid values.
events e1 e2 e3 e5 and e6 are linked together notethat event e3 has been executed three times and events e1 e2 e4 e6 are linked together since the same taskid values are shared among them.
simplifying sequences an example of repetition is sequence caused by loops.
for example for sequences about reading data from a remote node there would be repeated events about fetching the data.
without this step similar log sequences that include different occurrences of the same event are considered different sequences although they indicate the same system behaviour in essence.
these repeated events need to be suppressed to improve the readability of the generated summaries.
therefore we use regular expression techniques to detect and suppress the repetitions.
for the example shown in figure our technique detects the repetition of e3 in the sequence e1 e2 e3 e3 e3 e5 e6 and reduces this sequence to e1 e2 e3 e5 e6 .
the second step of simplifying sequences is dealing with permutations of sequences.
the reason why permutations oc 405cur is that sometimes the events execute asynchronously on the distributed computing platforms although the corresponding sequences result in the same system behaviour.
we group the permutations of a sequence together to simplify the sequences.
for example if we recovered two sequences e1 e2 e3 e4 and e1 e3 e2 e4 we would group these two sequences together in this step.
after simplifying sequences we obtain the final log sequences in figure d. b. generating reports we generate a log sequence report in html format.
figure shows an example report.
the report consists of two parts an overview of the number of execution log sequences and a list of sample log lines.
to ease the comparison of different reports each event is represented by the same unique number across the reports.
an exemplar sequence from the analyzed logs is shown in each row of the report to provide developers an actual example with a realistic context.
v. c ase study in this section we present the design of the case study that we performed to evaluate our approach.
a. subject applications we use three bda apps as subjects for our study.
two out of the three applications are chosen to be representative of industrial bda apps.
in addition to avoid potential bias from the development of the applications we chose one application that is developed from scratch and another application that is re engineered by migrating a set of perl scripts to the hadoop platform.
in addition to ease the replication of our approach by others we chose a third application from hadoop s official example package.
the overview of the three bda apps is shown in table i. wordcount.
wordcount is an application that is released with hadoop as one of the examples of mapreduce programming.
the wordcount application analyzes the input files and counts the number of occurrences of each word in the input files.
pagerank.
pagerank is a program used by the google internet search engine for rating web pages.
we implemented the pagerank algorithm on hadoop .
jack.
jack is an industrial application that uses data mining techniques to identify problems in load tests .
this tool is used in practice on a daily basis.
we migrated jack to the hadoop platform.
note that none of the three above bda apps have their own logs and the logs that our approach uses are the platform hadoop logs generated during the execution of these applications.
b. the experiment s environment setting as input data for wordcount andjack we use two groups of execution log files of a large enterprise application.
the input data for the pagerank application however comes fromtable i overview of the three subject bda a pps.
wordcount pagerank jack source hadoop google rim official developed migrated example from scratch from perl domain file social log processing network analysis injected machine missing supporting lack of problem failure library disk space table ii overview of the bda a pp s input data size .
wordcount pagerank jack large data 69gb 08gb small data 597mb 7mb two social network datasets from the stanford large network dataset collection2.
table ii summarizes the overall size of the input data for the studied applications.
to have a proof of concept experiment setting we performed our experiments on an in house small cloud a cluster with cores across five machines .
each machine has intel xeon e5540 .53ghz with cores gb memory a gigabit network adaptor and sata hard drives.
the operating system of the machines is ubuntu .
vi.
c ase study results in this section we present our research questions and the results of our case study.
for each research question we present the motivation of the question our approach to answer the question and the results.
a. rq1 how much effort reduction does our approach provide when verifying the deployment of bda apps in the cloud?
motivation developers often use simple text search techniques to identify troublesome events when deploying bda apps.
for example keywords such as kill and fail are often used to find problematic tasks in hadoop.
due to the decision by underlying platform e.g.
algorithms that hadoop uses for assigning tasks to machines a problematic event might be caused by some other reasons than an actual deployment failure.
two commonly seen examples of such reasons on hadoop platform are task exceptions and speculative execution task exceptions.
when there is an exception during the execution of a hadoop task the task will be killed and restarted on another cloud node.
therefore a keyword search for kill on the log lines would flag such hadoop decisions as a sign of failure even though this is supposed to be transparent from the developer.
speculative execution.
the overall performance of a hadoop job may slow down because of some slowrunning tasks.
to optimize the performance hadoop last check aug .406total grouped sequencescountgrouped sequence1058 1show sample sequences total grouped sequencescountgrouped sequence 1sample sequence8 info org.apache.hadoop.mapred.jobinprogress tip task 201106261526 0001 m 000047 has split on node default rack sail215.cs.queensu.ca82011 info org.apache.hadoop.mapred.jobinprogress tip task 201106261526 0001 m 000047 has split on node default rack sail217.cs.queensu.ca82011 info org.apache.hadoop.mapred.jobinprogress tip task 201106261526 0001 m 000047 has split on node default rack sail213.cs.queensu.ca102011 info org.apache.hadoop.mapred.jobtracker adding task attempt 201106261526 0001 m 000047 0 to tip task 201106261526 0001 m 000047 for tracker tracker sail215.cs.queensu.ca localhost .
.
.
info org.apache.hadoop.mapred.jobinprogress choosing data local task task 201106261526 0001 m 00004702011 debug org.apache.hadoop.mapred.jobtracker tracker sail215.cs.queensu.ca localhost .
.
.
launchtask attempt 201106261526 0001 m 000047 052011 info org.apache.hadoop.mapred.jobinprogress task attempt 201106261526 0001 m 000047 0 has completedtask 201106261526 0001 m 000047 successfully.
debug org.apache.hadoop.mapred.jobtracker marked attempt 201106261526 0001 m 000047 0 from tracker sail215.cs.queensu.ca localhost .
.
.
debug org.apache.hadoop.mapred.jobtracker removing task attempt 201106261526 0001 m 000047 0 info org.apache.hadoop.mapred.jobtracker removed completedtask attempt 201106261526 0001 m 000047 0 from tracker sail215.cs.queensu.ca localhost .
.
.
1fig.
.
an example of our log sequences report.
replicates the unfinished tasks on idle machines.
when one of the replicated tasks or the original task is finished hadoop commits the results from the task and kills other replicas.
this mechanism is similar to the backup task in google s mapreduce platform .
the replication and killing are decided at run time and are not signs of deployment failures.
however again a keyword search would flag them as a problem to be verified.
therefore a simple text search for such keywords may result in a very large set of irrelevant log lines for manual verification.
in this research question we investigate whether our approach saves any effort in the cloud deployment verification process.
approach to evaluate our approach in terms of effort reduction we use the amount of log lines that must be examined as a basic approximation of the amount of effort.
we first use the traditional most often used in practice today e.g.
approach of searching for keywords in the raw log lines as a baseline of comparison.
the keywords that we use in this experiment are common basic keywords kill error fail exception and died that are usually a sign of failure in a log line.
we applied this search on all three bda apps.
we measure the number of log lines with these keywords as the baseline effort.
to apply our approach for deployment verification we first recover execution sequences of the three bda apps when deployed on a cloud environment.
we then compare the two sets of log sequences small scale environment and large cloud and identify the delta set the execution sequencestable iii effort required to verify the cloud deployment using our approach versus the traditional keyword search .
using our approach using keyword search execution unique log line sequences log events with keyword wordcount pagerank jack table iv repeated execution sequences between running the bda a pps once twice and three times .
once and twice twice and three times wordcount .
.
pagerank .
.
jack .
.
without exact match .
the last step involves searching for the same keywords as the traditional approach to measure the number of execution sequences and log events that are required to examine.
results the results from table iii show that with our approach the number of execution log sequences and their corresponding number of log events to verify is and for wordcount pagerank and jack respectively.
however the number of raw log lines to verify after the keyword search i.e.
the traditional approach is and for wordcount pagerank and jack respectively.
therefore our approach provides and effort reduction over407table v number of log lines generated by running bda a pps once twice and three times .
once twice three times wordcount k k k pagerank k k k jack k k k the traditional approach ignoring the fact that verifying a log line may require more effort than verifying a log event.
indeed verifying a log line requires checking the other log lines to get a context of the failure whereas the log events are already shown in the context i.e.
the execution sequences .
also notice that our approach does not incur any instrumentation overhead since the platform logs are already available.
another interesting point is that when the input data grows several new execution sequences and log lines appear.
that is due to the fact that the behaviour is not present with the smaller runs.
however when moving to bigger runs the execution sequences will not increase dramatically.
the reason is that most of the runs in the abstract level are identical.
therefore the size of the final sequences to verify will show very minor increases.
however the log lines to be verified using traditional approach always increase proportional to the data.
table iv shows the number of repeated execution sequences when running the same bda app once twice and three times.
in table v we report the number of log lines to be verified using the traditional approach for the same bda app executions.
the large portion of repeated execution sequences in the number of execution sequences vs. the rapid growth in the number of log lines emphasizes the effectiveness of our approach in terms of effort reduction during verification of the deployment of a bda app in the cloud.
our approach reduces the verification effort by between to when verifying the cloud deployment of bda apps.
b. rq2 how precise and informative is our approach when verifying cloud deployments?
motivation as discussed in rq1 a flagged sequence using our approach or a flagged log line using the traditional approach might be caused by some other reasons than an actual deployment failure.
we consider such flagged sequences or log events e.g.
those that are related to task exception and speculative execution as false positive results that affect the precision of the approaches.
therefore in this question we compare the two approaches in terms of precision.
we also discuss how our approach facilitates the verification of the flagged execution sequences.
approach in this research question we categorize the flagged logs execution sequences by the traditional our approach into two classes actual failures true positives and platform relatedevents false positives .
to get the instances of the actual failure we intentionally injected three different failures which are commonly observed by bda app developers into our experimental environment during the execution of the three subject programs.
these three failures are all encountered often in our real life experience of using hadoop in the large industrial clouds.
the three injected failures are as follows machine failure.
machine failure is one of the common system errors in distributed computing.
to inject this failure we manually turn off one machine in the cluster.
missing supporting library.
a cluster administrator may decide to expand the size of the cluster.
however the new machines in the cluster may miss supporting libraries or the versions of the supporting libraries may be outdated.
we inject this failure by removing one required library of the analysis.
lack of disk space.
disks often run out of space while a bda app is running on the platform due to the large amount of generated intermediate data.
we manually fill up one of the machine s disks to inject this failure.
next we manually analyzed the log sequences in the html reports and identified any false positive instances.
results table vi summarizes the number of false positives total number of flagged sequences log lines and the precision of both approaches.
the precision of our approach is and for wordcount pagerank and jack respectively.
the range of the number of false positive sequences to verify is to sequences log events .
however the precision of the traditional approach is and for wordcount pagerank and jack respectively while the range of the number of false positive log lines to verify is to log lines.
a more detailed analysis of the results shows that the only case where the precision of our approach is outperformed by traditional approach is with jack bda app where one single exception is appearing in almost every log line.
though the traditional approach is of higher precision however the same exception produces log lines that must be examined each by hand to determine their context and decided whether they are problematic or not.
unfortunately since a keyword approach does not provide any abstraction all log lines would need to be examined carefully even though they all are instances of the same abstract problem.
note that the recall for both approaches is since all instances of log lines and execution sequences related to the failures are identified by the keyword search.
however there are cases that deployment failure might not be possible to catch by a keyword search.
for example a temporary network congestion may cause the pending queue to be very long but logs may record that the pending queue is too long without making use of an error like string in the log line.
in some cases a node in the cloud may even fail without recording any error message .
in such situations our approach is even superior since the traditional approach simply would not work as no error log lines are produced and the developer would miss such problems all together unless he or she examines408table vi number of false positives true positives and the precision of both our approach and the traditional keyword search .
using our approach using keyword search false positive true positive precision false positive true positive precision wordcount pagerank jack each log line.
however our recovered execution sequences still would work since it only depends on finding the delta set of sequences when switching from the small to large cloud.
another interesting aspect of our approach which was the initial motivation of this work is the extra information context of a log line that our approach provides for deployment verification.
even after all the reduction that is performed by the keyword search approach to log lines should be verified manually.
as discussed earlier there are false positives in the flagged log lines.
distinguishing them from true positives requires knowledge about the context of each line otherwise both categories contain the failure related keyword .
our approach provides such context by grouping the log events in one execution sequence which speeds up the understanding and verification of the event.
the precision of our approach for assisting deployment verification of bda apps in the cloud is comparable with the precision of the traditional approach.
however our approach provides additional context information execution sequences that is essential in speeding up the manual investigation of flagged problems.
vii.
d iscussion in this section we discuss other possible features of our approach.
in particular one feature is to support developers to understand the runtime differences when migrating bda apps from one platform to another.
to find the most optimal and economical platform for bda apps a bda app may need to be migrated from one big data analytics platform to another .
this type of redeployments requires similar verifications as discussed in the research questions.
therefore developers need an approach to help them in identifying any run time behaviour change caused by the migration.
identifying the differences between the execution of the bda app in the two environments help verifying the new deployment and flag any potential failure or anomalies.
to assess the ability of our approach for identifying the potential redeployment problems in the cloud we migrated thepagerank program from hadoop to pig platform.
pig is a hadoop based platform designed for analysis of massive amounts of data.
to reduce the effort of coding in the mapreduce paradigm pig provides a high level data processing language called pig latin .
using pig latin developers can improve their productivity by focusing on theprocess of data analysis instead of writing the boiler plating mapreduce coding .
we ran pagerank three times on hadoop and three times on pig.
after examining the sequence reports we could note the following differences between both platforms .
hadoop based pagerank has more mapreduce steps than pig based pagerank in our implementation the hadoop based pagerank consists of four mapreduce steps while the pig based pagerank has eight lines of pig scripts each line is one step in the pipeline of the data analysis .
however in the real life execution the pig platform groups the eight steps of data process into three mapreduce steps.
since the hadoop platform does not have such a feature of grouping mapreduce steps the execution of hadoop based pagerank has four mapreduce steps as it is written.
.
hadoop based pagerank has more tasks than pig based pagerank we examine the distribution of sequences in both implementations of pagerank.
the results show that the total number of log sequences from the hadoop based pagerank is much larger than the pig based pagerank.
for example one run of the hadoop based pagerank generates in total over execution log sequences in the task log while this number by the pig based pagerank is less than .
this result indicates that the pig based pagerank splits the execution into a significantly smaller number of tasks than the hadoop based one.
the reason is that based on the hadoop instructions the number of map tasks should be set to a relatively large number.
therefore in our case study the number of map tasks is configured to .
however pig optimizes the platform configurations at run time and reduces the number of map tasks to a smaller number to get better performance.
identifying such differences would be extremely difficult by only looking at the raw log lines.
thus our approach not only assists developers of bda apps with the first deployment in the cloud but also helps them with any redeployment.
we do note that our approach only works when the new platform is a derivative of the older platform e.g.
in the case of pig it provides a high level abstraction to create programs that eventually still run on mapreduce.
hence we can compare the eventual mapreduce execution for the pig program against the old mapreduce execution .
viii.
l imitations and threats to validity we present the limitations and threats to validity for our approach in this section.409a.
external validity as a proof of concept we illustrate the use of our approach to address the challenges encountered in our experience.
however there are still other challenges of developing and testing bda apps such as choosing an architecture that optimizes for cost and performance .
our approach may not be able to address other challenges.
additional case studies are needed to better understand the strengths and limitations of our approach.
we only demonstrate the use of our approach on hadoop one of the most widely adopted underlying frameworks for bda apps with three injected failures.
in practice we have tried our approach on several commercial bda apps on other underlying platforms.
the only effort for adapting our approach to other platforms of bda apps is to determine the parameters for abstracting and linking the platform logs.
additional studies on other open source and commercial platforms with other types of failures are needed to study the generalizability of our approach.
all our experiments are carried out on a small scale private experimental cluster which mimics the large cloud with cores.
however a typical environment for bda apps has more than cores such as amazon ec2 .
the logs of such large scale clouds do lead to considerably more logs and more sequences.
from our experiences using our approach in practice on such large clouds we have found that our approach performs even better than grep since the abstraction and sequencing leads to a drastic reduction in the amount of data.
such observation was noted in our case study as well.
one interesting note is to support such large clouds we needed to re implement our own approach to run in a cloud setting using the hadoop platform since we needed to process a very large amount of logs and summarize them into a small number of event sequences .
since our log linking is designed to be localized for example linking hadoop task logs only needs the logs from one task our approach is parallelizable with minimal effort.
b. construct validity we use abstracted execution logs to perform log clustering and to learn the runtime behaviour.
however the execution logs may not contain all the information of the runtime behaviour.
other types of dynamic information such as execution tracing may have more details about the execution of the bda apps.
we use the execution logs rather than other more detailed dynamic information in this work because execution logs are readily available and are widely used in practice leading to no performance overhead .
we leverage the logs from the underlying platform of the bda apps e.g.
hadoop instead of the logs from the apps themselves.
the purpose of this work is not to identify the bugs in the bda apps but rather assist in reducing the effort in deploying bda apps in a cloud environment.
therefore the platform logs provide more and better information than application logs.
identifying the re occurrences of sub sequences can also be used in our approach to reduce the event sequences similar to our method of eliminating repetitions in section iv.
inour experience we performed sub sequence detection on the recovered event sequences and found that it did not suppress execution log sequences as good as our repetition elimination approach.
in addition the process of sub sequence detection is very time consuming and slows down the overall analysis.
therefore we did not use the sub sequence detection in practice.
for other bda apps and other distributed platforms sub sequence detection may be effective in reducing the log sequences.
ix.
r elated work in this section we discuss the related work in two areas.
a. dynamic software understanding fischer et al instrument the source code and produce different kinds of visualizations to track and to understand the evolution of software at the module level.
kothari et al propose a technique to evaluate the efficiency of software feature development by studying the evolution of call graphs generated by execution traces.
r othlisberger et al implement an ide called hermion which captures run time information from an application under development.
the run time information is used to understand the navigation and browsing of source code in an ide.
recent work by beschastnikh et al designed an automated tool that infers execution models from logs.
the models can be used by developers to verify and diagnose bugs.
our techniques aim to provide context of logs when deploying bda apps in cloud.
in addition cornelissen et al perform a systematic survey of using dynamic analysis to assist in program understanding and comprehension.
field is a development environment created by reiss et al that contains the features to dynamically understand the execution of a program.
however the environment is rather designed for traditional application development and not for the cloud deployment of bda apps.
b. hadoop log analysis hadoop typically runs on large scale clusters of machines with hundreds or even thousands of nodes.
as a result large amounts of log data are generated by hadoop.
to collect and analyze the large amounts of log data from hadoop boulon et al built chukwa .
this framework monitors hadoop clusters in real time and stores the log data in hadoop s distributed file system hdfs .
by leveraging hadoop s infrastructure chukwa can scale to thousands of nodes in both collection and analysis.
however chukwa focuses more on collecting logs without the ability to perform complex analysis.
tanet al introduced salsa an approach to automatically analyze hadoop logs to construct state machine views of the platform s execution .
the derived state machines are used to trace the data flow and control flow executions.
salsa computes the histograms of the durations of each state and uses these histograms to estimate the probability density functions pdfs of the distributions of the durations.
salsa uses410the difference between the pdfs across machines to detect anomalies.
tan et al also compare the duration of a state in a particular node with its past pdf to determine if the duration exceeds a determined threshold and can be flagged as an anomaly.
another related work to this paper is the approach of xu et al in which uses the source code to understand the structure of the logs.
they create features based on the constant and variable parts of the log messages and apply the principal component analysis pca to detect the abnormal behaviour.
all the above approaches are all designed for system administrators in managing their large clusters.
our approach on the other hand aims to assist developers in comparing the deployed system on such large clusters against the development cloud.
x. c onclusion developers of bda apps typically first develop their application with a small sample of data in a pseudo cloud then deploy the application in a large scale cloud environment.
however the larger data and more complex environments lead to unexpected executions of the underlying platform.
such unexpected executions and their context cannot be easily uncovered by traditional approaches.
in this paper we propose an approach to uncover the different behaviour of the underlying platforms for bda apps between runs with small testing data and large real life data in a cloud environment.
to evaluate our approach we perform a case study on hadoop a widely used platform with three bda apps.
the case study results show the strength of our approach in two aspects our approach drastically reduces the verification effort by when verifying the deployment of bda apps in the cloud.
the precision of our approach is comparable with the traditional keyword search approach.
however the problematic logs reported by our approach are much fewer than using keyword search which makes it possible to manually explore the problematic logs.
in addition our approach provides additional context information execution sequences .
based on the context information developers can explore the execution sequences of the logs to rapidly understand the cause of problematic log lines.