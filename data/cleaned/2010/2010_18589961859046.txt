towards mining replacement queries for hard to retrieve traces marek gibiec adam czauderna and jane cleland huang systems and requirements engineering center sarec depaul university chicago usa mgibiec aczauderna gmail.com jhuang cs.depaul.edu abstract automated trace retrieval methods can significantly reduce the cost and effort needed to create and maintain requirements traces.
however the set of generated traces is generally quite imprecise and must be manually evaluated by analysts.
in applied settings when the retrieval algorithm is unable to find the relevant links for a given query a human user can improve the trace results by manually adding additional search terms and filter ing out unhelpful ones.
however the effectiveness of this approach is largely dependent upon the knowledge of the user.
in this paper we present an automated technique for replacing the original query with a new set of query terms.
these query terms are learned through seeding a web based search with the original query and then processing the results to identify a set of domain specific terms.
the query mining algorithm was evaluated and fine tuned using security regulations from the usa government s health insurance privacy and portability act hipaa traced against ten healthcare related requirements specifications.
categories and subject descriptors d. .
requirements specifications h. .
information storage and retrieval information search and retri eval.
general terms documentation verification.
keywords requirements traceability trace retrieval.
.
introduction requirements traceability is a critical software engineering activity that provides support for numerous tasks such as impact analysis compliance verification and coverage analysis .
it has been defined as the ability to track the life of a requirement back to its source documents and forward to the downstream work products in which it is realized .
traceability links are gene rally created and maintained by project stakeholders using spreadsheets databases or specialized trace features of commercial requirements management tools.
however in non trivial projects the number of traceability links can grow very large and as a result the manual effort required to establish and maintain such traces is often inhibitive.
for example in a recent transportation sector project the federally required cost of tracing requirements to over sets of regulatory codes was estimate d at a staggering us million .
because of this excessive effort unless mandated by law or enforced by organizational standards many projects have no systematic traceability process in place and software engineers and analysts are forced to manually construct traces between artifacts on an as needed basis.
several different researchers have attempted to address these problems by using information retrieval ir methods to dynamically generate traceability links.
the most popular approaches have utili zed either the vector space model vsm probabilistic approaches or latent semantic indexing lsi .
the large body of research in this area has been applied across a variety of domains including safety and business critical projects as w ell as both large and small sized projects.
results from these studies have demonstrated that although trace retrieval methods can significantly reduce the traceability effort in most projects the generated traces are imprecise and require an analyst to s pend time evaluating the results in order to find the correct set of links .
a closer analysis of these earlier results shows that the precision problems in many of the projects are caused by only a small percentage of stubborn traces which pull down the overall quality of the generated traces while the majority of trace queries perform quite well.
figure illustrates this state of affairs for four previously baselined datasets.
it highlights the percentage of trace queries th at returned low precision results.
for example the arema and gascode datasets previously used in an industrial case study had only and of queries returning average precision values lower than .
similarly cm1 which is a rather comple x nasa dataset also had of these low performing queries while the hipaa datase t used in this paper had in this range.
stubborn traces occur primarily when language in the target do cument neither match es the language of the source document nor matches project level synonyms defined in a thesaurus .
as this is a common occurrence o ur traceability tool poirot includes a feature to help human user s improve stubborn traces through adding additional search terms and filtering out unwanted terms .
this is illustrated in figure which depicts a cloud of terms for a query from an industrial case study that involved tracing regulatory codes for the canadian gas regulations gascodes to contractual requirements .
in this example the user added the term flange and filtered out several terms before re permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permis sion and or a fee.
ase september antwerp belgium.
copyright acm ... .
.
running the query to generate an updated list of candidate links.
modifying the trace query in this way has been shown to improve the recall and precision of certain stubborn queries.
unfortunately the effectiveness of this approach is largely dependent upon the expertise and knowledge of the person performing the trace .
to address th ese shortcoming s we developed a web mining a pproach designed to discover a new set of query terms that can be used to replace or augment the original query.
this work builds on a technique introduced in our prior work in which we compared the effectiveness of two machine learning methods for improving trace results.
one method utilized a manually co nstructed answer set to train a trace classifier while the second approach attempted to replace this training set with terms mined from performing related web searches.
this current paper a dvances the second web searching technique in several important ways.
first the original method was only partially automated and required a human to evaluate and select appropriate documents returned as a result of the web search.
this prevented the technique from being practically applied in an industrial traceability setting.
it also severely limited the number of web based doc uments that could be processed.
second the prior method was very coarse grained and processed entire documents during the mining process as opposed to identifying and using only more relevant chunks of text.
the work in this paper addresses th ese problem s through automating the web mining process and implementing and validating a technique for identifying appropriate sections of text from within the retrieved documents .
furthermore in the prior work numerous decisions and parameters were established in a relatively subjective way.
although this was acceptable for a proof of concept study it is necessary to fine tune the algorithm so that it delivers optimal results.
this paper also describes a series of experiments which were conducted to empirically di scover the correct way to parameterize and execute the algorithm in order to maximize the achievable trace results.
.
trace retrieval the experiments reported in thi s paper were conducted using a probabilistic network pn model which has been shown through prior experimentation to perform equivalently to other trace r etrieval models.
its use involves three primary tracing activities of pre processing text computing link probabilities and analyzi ng results.
.
the probabilistic network model to prepare text for tracing a pre parsing phase is conducted to remove very common words known as stop words and to stem the remaining words to their root forms.
the basic retrieval alg orithm uses a prob abilistic network model to evaluate the relevance of a document to a specific query.
the probability model is d efined over a concept space u called the universe of discourse in which terms t t2 ... t k represent the singleton concepts and each documen t d i and each query q j are random variables that represent propositions within the concept space.
the probability pr d of a proposition is interpreted as the degree of coverage of u by the knowledge contained in proposition d. the probability pr d j q of a given document being linked or relevant to a speci fic query is defined as the degree of coverage for document d j provided by q. a conditional probability value p d q is computed for each query q and each document d defined as a function of the fr equency of terms co occurring in both q and d and computed as where is defined as p q t i is defined as and finally p q is defined as where freq d t i represents the frequency of term ti in a document d and ni is the number of documents in the searchable document collection d1 d2 ... d n that contain term ti.
the first component p d t i represents the relative frequency of term ti in document d and increases with the number of occurrence s of ti in d. idf is used to reduce the contribution of common terms to the overall probability value between the query and the document.
any query docum ent pair receiving a probability score over a threshold value is treated as a candidate traceability link.
these threshold values can be learned automatically based on the distribution of similarity scores .
figure .
stubborn trace queries exhibiting average precision scores in four datasets figure .
a poirot screenshot showing a phrase cloud for a query .
traceability metrics the experiments in this paper are reported using three metrics of recall precision and average precision.
recall measures the fraction of relevant links that ar e retrieved and is computed as while precision measures the fraction of retrieved links that are relevant and is computed as recall and precision generally trade off against each other so that as one increases the other decreases.
for experimental purposes a threshold score is established such that all links above or at the threshold are retrieved and all links below the threshold are r ejected.
because it is not always feasible to ach ieve identical recall values across every trace query it can be difficult to compare recall and precision results across experiments .
furthermore these metrics are unable to differentiate between results in which rel evant links are closer to the top of t he ranked listing of results versus those in which relevant links are lower down in the list.
an additional metric average precision favors results that return relevant links nearer to the top of the list and furthermore does not depend on the creation of an arbitrary threshold value.
average precision is computed as where r is the rank n is the number of retrieved documents relevant is a binary function assigned if the rank is relevant and otherwise and p r is the precision computed after trunca ting the list immediately below that ranked position.
in cases where all relevant links are retrieved at the top of the list average precision values will be .
.
prior trace retrieval results there are numerous reported studies in which information retrie val methods have been used to generate traceability links.
hayes et al.
built a tool named retro requirements tra cing on target that implements vsm.
they used retro to evaluate the vsm traceability model against the modis and cm1 datasets.
their baseline study without use of any enhancement methods returned maximum recall values of approximately .
at prec ision levels of approximately .
for modis results are appro ximate as they are read from graphs and not tables and recall of approximately .
at precision of only .
for cm1.
cleland huang et al conducted several different experiments using their tool poirot to generate and retrieve links between regulatory codes requirements business goals uml classes java code and test cases .
they reported achieving precision of to at recall levels of close to for various datasets.
delucia et al implemented the lsi algorithm in the trace retrieval feature of their advanced artifact management system adams and provided an empirical comparison of two different recovery processes.
maletic and marcus also utilized lsi to recover links be tween source code and documentation and showed that it outperformed vsm.
in contrast hayes et al reported on a study in which lsi performed similarly to vsm for one dataset but returned significantly worse results for anot her one .
as lsi is general ly accepted to perform better on larger datasets the results may be at least partially related to the size of the datasets being traced.
r esearchers have also investigated traceability enhancement techniques.
for example hayes showed that using a user defined thesaurus improved recall and precision results.
cleland huang et al augmented term weights for terms and phrases found in a project glossary while zou et al used natural language processing methods to weight shared phrases more highly than simp le terms .
each of these approaches led to some improvements in recall and precision for certain datasets but improvements were small and inconsistent across datasets.
.
data sets the work described in this paper is based on a dataset containing technical requirements for the security rule of the usa s health insurance portability and accountability act ten health care related software requirements specifications srs depicted in table and a traceability matrix showing which requirements contribute towards satisfying each of the regulations.
the ten relevant hipaa regulations addressed concerns related to access control ac audit aud automated logoff al emergency access procedures eap integrity i personal authentication pa sto rage encryption and decryption sed transmission encryption and decryption ted transmission security ts integrity controls ic and unique user identification uui all of which were extract ed directly from the hipaa regulations.
as an example of a hipaa regulation the uui regulation states assign a unique name and or number for identifying and trac king user identity .
the associated srss were collected by depaul researchers during the summer of from a variety of sources including open so urce products it healthcare standards requir ements exemplars and feature descriptions for commercial pro ducts.
the data sets are more fully described in our prior work .
.
dealing with stubborn traces as previously stated t his paper describes an automated approach for addressing the problem of stubborn traces through replacing trace queries with key terms mined from documents on the world wide web .
we first examine related work in query expansion and then present our proposed m ethod.
several researchers have previously explored the use of inform ation retrieval methods to augment web based search queries.
table .
hipaa related requirements used in this study description hipaa related requirements tot ac au d al ea p i pa se d te d ts ic uu i care2x open source hospital info.
system cchit health info exchange ehr cert.
clearhealth open source her.
physician information exchange.
itrust open source her trial implement ations nat l coord.
for health info.
tech.
patientos source healthcare info.
sys.
practiceone a suite of healthcare info.systems.
lauesen sample her reqs spec.
worldvista usa veteran admin.
totals 247however their work generally assumes a typical web query containing between .
and .
terms individual users who dynam ically compose queries in real time and a large community of users who often repeat similar queries.
in contrast trace queries are much longer in nature typically involving requirements co ntaining terms.
furthermore they often describe relatively obscure concepts are composed in advance by business analysts requirements engineers or even lawyers and are used infrequen tly by a small group of trace users.
despite these differences there is a large body of work in the area of query expansion that is relevant to the work we propose in this paper.
for example hu et al used wikipedia to augment users web based queries however based on our initial analysis of the traceability problem websites such as wikipedia do not currently provide sufficient information to augment a rich and diverse set of domain specific traceability queries.
other researchers have used more gen eral web knowledge to augment queries however their techniques are designed with the primary intent of expanding a short query with additional and potentially disambiguating terms.
in contrast our approach is designed to add and remove terms from the original queries.
other classes of query augmentation methods are quite interactive in natu re.
for example yurekli et al.
used collaborative reco mmender systems to recommend query refinements to users based on other users similar queries .
this approach shows some promise in the requ irements domain however it is impeded by the fact that trace users are often not the people who create the trace query in the first place and therefore may not have the knowledge needed to accept and reject recommended expansion terms.
finally other rese archers have explored techniques that augment queries with different types of knowledge such as folksonomy tags or terms from taxonomies.
for example gabrilovich et al use initial search results to classify queries according to an extensive taxonomy and then to improve search results based on prior history of queries in the designated categories .
however these approaches generally depend on either very large groups of interactive users or large numbers of search queries neither of which hold true for tracing in most software engineering projects.
.
proposed approach our proposed approach is illustrated in figure .
the text from a stubborn trace query in our experiment we only use the hipaa goal title is used to seed a series of web searches using one or more standard search engines.
the retrieved documents are then filtered to remove documents which are difficult to parse because they are primarily graphical in nature or which contain primarily adverti sements.
the remaining documents are then partitioned into chunks of text see section .
.
and a similarity score is computed between each chunk of text and the original query using the vector space model vsm .
vsm is a standard approach which compute s the cosine similarity between a query and doc ument each of which is represented as a vector of weighted terms.
a more complete explanation is provided in most introductory information retrieval textbooks .
the most relevant chunk i.e.
the chunk exhi biting highest similarity to the trace query is then selected.
all identified chunks are then processed using an alg orithm we previously developed for extracting project glossary items from requirements specifications .
this algorithm ou tputs a set of candidate terms and computes domain term fr equency dtf domain specificity ds and concept generality cg metrics which are then used to select a small set of query terms to replace the original trace query .
several components of this algorithm are now discussed in further detail.
.
.
document chunking each of the retrieved documents is partitioned into smaller section s through splitting the document into overlapping chunks of length chunklength .
chunks overlap to ensure that unfortunate partitioning decisions do no t prevent the algorithm from recognizing the most relevant section of text .
the degree of overlapping is determined by the chunkoffset which defines the distance in characters between the starting point of the previous chunk an d the starting point of the current chunk.
chun ksize and chunkoffset were both measured in terms of characters in order to speed up the running time of the chunking process.
the impact of various chunking parameters is experimentally ev aluated in section .
of this paper.
.
.
term and phrase extraction our algorithm is designed to extract nouns and noun phrases from the retrieved documents.
it utilizes a freely available parser based part of speech pos tagger named qtag to identify nouns and noun phrases .
qtag uses a dictionary to identify the syntactic category of each token in the text and outputs a series of pos tags that represent grammatical classes of words such as nouns verbs and adjectives for each token in the input text.
this phase of the process can produce a large number of terms and phrases numbering from to for a typical trace query.
sample terms generated for hipaa s audit control regulation are depicted in table .
figure .
query modification technique 248candidate terms are then filtered by establishin g threshold values for the following three metrics.
domain term frequency dtf computes normalized term frequency information for term t across multiple documents as where freq t d is the total number of occurances of term t in a given document d and d is the length of that document expressed as the total number of all terms in d .
the normalized occurrences from all retrieved documents d are then summed .
domain specificity ds measures the extent to which a term or term phrase is specific to the domain document as opposed to occurring frequently across a broad spectrum of t opics.
for our experiments domain specificity ds t d of term t in document d was estimated by comparing the relative frequency of the term within a domain specific document versus its relative frequency in a general corpus of documents .
it is calculated as follows where the first component is the normalized number of occurrences of term t in the domain specific document d and the second component is the normalized number of occurrences of t in the general corpus of documents.
dom ain specificity ds is then calculated as the average value of all domain specificit ies from each document from the collection additionally when a term is not found in the general cor pus of documents it is considered as highly domain specific and is assigned a ds value of .
concept generality cg computes the fraction of domain specific documents in which a specific term occurs.
concept generality differentiates between terms that occur in multiple domain specific documents versus those that occur in only a few.
the concept generality of term t cg t is computed as the number of documents containing term t dt over the total number of documents .
once these metrics are computed they are used to filter out non useful terms.
the remaining terms are then ranked in descending order of term frequency.
section .
of this paper describes a series of experiments we conducted to explore the optimal threshold values for each metric.
.
results from prior experiment in our previously reported experiments t hese threshold values were set according to our informal observations of the initial results.
remaining phrases were sorted in descending order a ccording to their domain term frequency and the top terms were then formed into a new query used for tracing.
results from this earlier exp eriment showed that for the five hipaa regulations of type al eap sed ted and ts the basic poirot algorithms performed very well retrieving at least of the relevant requirements at precision values of .
neither the manually trained nor the w eb trained algorithms were able to improve on these results and in fact both machine learning techniques had an overall negative effect on the results.
on the other hand the web mining approach returned improved traces for the pa aud ac and i hipaa re gulations.
a further analysis of the results suggested that the basic poirot algorithm performed well for traces with low fan out values i.e.
for which a hipaa regulation traced to only one or two requirements in the target dataset but tended to perform s ignificantly worse for cases with higher fan out.
this is explained by the fact that traces exhibiting high fan out use a broader set of terms to represent the set of traceable concepts and that the language in the regulation is often insufficient for ret rieving all of the relevant documents.
query expansion techniques can potentially address this problem by discovering a broader set of terms than those found in the original query.
.
open issues although our previous results demonstrated that this approach could be useful the technique was not fully automated and neither was it fine tuned to produce optimal results.
the new work d escribed in this paper therefore fully automates the approach and includes tec hniques for dispatching queries to the search engine selecting relevant documents parsing those documents to part ition the text into smaller chunks and finally selecting the most relevant chunk.
it further includes a series of experiments that were cond ucted to evaluate and fine tune the query augmentation process and related algorithms.
as our approach relies quite si gnificantly upon the results returned from standard search engines the first experiment evaluated the impact of search engine sele ction o n the trace results.
experiments were also conducted to evaluate the optimal number of documents to retrieve and parse as it is important to discover whether retrieving a smaller number of documents such as the ten documents used in our proof of concept study or a larger number of documents provides the best table .
a small sample of the total candidate terms generated for hipaa s audit regulation 249results.
furthermore our previous study used all the text from the retrieved document whereas it is likely that a single document may cover multiple topics and results might be improved by identif ying one or more chunks of more relevant data from within each document.
experiments were conducted to evaluate the impact of chunk size and offset values on the trace results.
finally the set of candidate terms and term phrases are filtered using the met rics of domain term frequency domain specificity and concept generality and a series of experiments were conducted to empirically explore these relationships in order to identify the optimal values for the datasets studied .
.
experimental evaluation this section of the paper describes the experiments that were conducted to comparatively evaluate different approaches .
recall precision and average precision values were computed for each hipaa goal traced against all ten of the datasets.
.
use of different search engines the first experiment was designed to comparative ly evaluate the impact of using three popular search engines namely bing google and yahoo as well as a combination of the three .
for each of these experiments documents were retrieved per trace query.
each of the retrieved documents was automatical ly part itioned into chunks of characters and the chunk that exh ibited greatest relevance to the query was mined for query expa nsion terms.
the results are depi cted in figure and show that different search engines performed well on different queries .
as a result we decided to utilize the combination of all three search engines as the default setting for future experiments.
.
number of documents retrieved a series of experiments were conducted to evaluate the impact of selecting and documents from each search engine .
as three search engines were used the actual number of unique documents retrieved averaged and documents for each experiment .
in many cases documents could not be read our tool only supports html doc and some pdf files or had almost no textual content so fewer documents were actually parsed .
furthermore as the google search engine api only supports results in sets of up to a maximum of distinct links the number of web docum ents retrieved from google results were and respectively.
results are depicted in figure and varied somewhat by query depending on how many relevant high quality documents were available .
.
chunk size experiments were also con ducted to evaluate the impact of chunk size on the trace query results .
for these experiments doc uments were retrieved from each of the search engines.
chunk sizes were evaluated at and characters.
as in previous experi ments queries were issued for each hipaa goal and recall precision and average precision results were computed.
these r esults are reported in figure and show that in six of the ten trace queries results were improved with smaller chunks while in tw o queries results were improved with larger chunks.
in the final two queries no definite trend was identified based on chunk size.
these results suggest that chunk size should not be fixed in length but should be customized a ccording to the characteristics of each query and document.
inve stigating this conjecture is left for future work.
.
threshold values for term selection the final experiment focused on determining appropriate thr eshold values for cg ds and dtf metrics .
candidate list s of terms and phrases were generated using a combination of the three search engines documents per query chunk sizes of and chunk offset values of .
as there were three param eters that are all quite closely interdependent a series of experiments were run to evaluate various combinations of each par ameter across the following ranges concept generality ranged from .
.
domai n specificity from and term fr equency from .
.
.
a total of different combinations were investi gated using an inductive experimental style which tested combinations of low medium and high values for each metric and then further explored areas of high performing combinations .
average precision values were recorded for all experiments .
results are shown in figure .
domain specificity exhibited best results at threshold values of to .
setting the ds threshold too high caused useful terms to be excluded.
similarly domain term frequency performed best at values be tween .
and .
and caused a gradual decline in average precision values when it was set to higher values.
the concept generality threshold did not appear to have a significant impact on the quality of the trace a. recall precision pr ec and average precision a vg prec b. average precision of trace results figure .
impact of search engine selection 250results .
the top scoring results are shown in table and show that maximum average precision was achieved for this dataset at following thresholds cg .
ds and dtf .
or .
.
domain specificity appears to be the dominant factor in impro ving trace resu lts.
setting it too high appears to exclude some useful terms while setting it too low introduces terms that are too a. recall precision pr ec and average precision a vg prec b. average precision of trace results figure .
impact of retrieved document count a. recall rec precision pr and average precision av.
prec b. average precision by chunk size.
figure .
impact of chunk size on trace query results 251general in nature.
term frequency serves as a second internal discriminator.
optimal results were achieved by lowering this threshold in or der to include terms that occur less frequently but which are representative of the domain.
.
tool support for experimental purposes we developed a java based tool named watson web augmented traces on demand that imports two xml files containing source and target artifacts and three text files containing stop words correct trace information for evaluative purposes and program settings .
for each input query watson issues queries to google bing and yahoo collects and filters the re sults extracts the most relevant chunks of text from each result computes candidate terms and term phrases and then s elects the ten terms which exhibit appropriate levels of term fr equency domain specificity and concept generality.
watson outputs a new xml file containing the reconstituted queries which are passed to a report generator .
this generator utilizes libraries from poiro t to generate traces and compute standard metrics.
watson is currently being integrated into poirot to provide analysts with support for stubborn trace queries.
.
stubborn traces watson was evaluated against the stubborn traces of the hipaa project.
unlike the previous experiments which traced each hipaa regulation to requirements from all ten datasets this experiment explore d the quality of each individual trace query.
it therefore exposed the details of individual trace queries providing visibility into successful and stubborn traces.
the ten hipaa regulations and ten srs documents created a potential for differe nt trace queries.
however several srs documents did not have any relevant traces for certain hipaa regulations and so the actual number of queries with relevant traces was .
each of these queries was executed using both the basic pn algorithm and the query expansion algorithm.
.
hipaa query expansion to illustrate the effect of query expansion on the hipaa regul atory codes figure shows stemmed terms from both the original regulation and the replacement query generated by watson and shows that query terms can be retained removed or added by the algorithm.
for example in the ac regulation terms such as access and control were retained terms such as perm iss and ident were added and finally terms such as technic and program were r emoved.
the integrity regulation is worth mentioning because it is the only regulation for which watson did not return a reasonable set of replacement terms.
in fact this regulat ion was not clearly defined in the original hipaa regulations and created problems throughout the experiment for both the automated techniques and for human analysts who had to manually create traceability links.
although not reported in this paper we also conducted a series of experiments that combined new and original queries however early results did not show improvements with this approach and further investigation is left for future work.
.
hipaa query expansions results are reported in figure displayed in ascending order according to the average precision of the basic poirot results.
in other words queries that were more stubborn to trace using the basic poirot algorithm appear on the left hand side of the graph while those that returned high average precision values are shown on the right hand side.
the darker bars on the left hand side that table .
achieved average precision results average precision cg ds dtf top results .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
selectio n of lower ranked results samp led every 100th result from ranked list .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
figure .
maximum average precision achieved at various threshold values 252rise above the gray bars represent the significant improvements achieved through using query modification techniques on the stubborn que ries that did not perform well in the initial trace.
there are several interesting observations that can be made.
first a pearson correlation analysis shows only weak correlation of .
between the average precision results returned by the two methods.
this suggests that the augmented query technique is not simp ly returning good results for the same traces as the base method.
in fact the graph shows that there are several traces that performed well using the base method that did not perform well under the augmented query method.
on the other hand there are sever al traces that performed very poorly using the base method which were significantly improved by the augmented query approach.
if stubborn queries are defined as those which returned average precision scores of .
or lower from the base algorithm then th e experimental results show that of the traces in this category augmented queries performed significantly better than the baseline performed marginally better or equivalently while performed marginally worse.
furthermore out of the total traces the query augmentation technique improved and matched the quality of an additional .
it is equally clear from these results that this approach does not uniformly improve every trace query.
the augmented query a p proach returned only perfect sco res i.e.
all relevant traces were found in the topmost positions of the candidate links for of the queries exhibiting perfect results with the basic method.
in fact in three of these cases average precision dropped below including two cases i n which it dropped all the way down to .
on the other hand watson returned perfect scores for queries that did not perform so well using the basic trace method.
watson is not designed to replace the basic approach but instead to provide the user with a realistic option for improving stubborn traces.
this is especially useful for tracing regulatory codes such as hipaa regulations as this is an inherently labor intensive task in which the analyst has to first determine whether the regulation is relevant and if it is found to be relevant to demonstrate that the requirements sufficiently satisfy it.
in this type of scenario the analyst could attempt the base trace first and if results are not satisfactory could then generate a replacement query.
.
c onclusions this paper has described a fully automated approach for modif ying the terms found in trace queries and then re issuing the query using the new set of terms.
the experimental results have clearly shown that in the hipaa dataset a significant portio n of previou sly stubborn trace queries were improved through use of this m ethod.
although our current algorithm cannot predict which queries could benefit from the use of this method the approach provides reg original query modified query ac access allow control electron grant health implement inform maintain person polici procedure program protect right softwar specifi technic access control door reader permiss control system com role lock ident kei al automat electron implement inact logoff predetermin procedur session termin time logoff post shutdown com auto logon inact forum password login aud activ audit contain control electron examin hardwar health implement inform mechan procedur protect record software audit risk auditor procedur review complianc govern depart board audit control eap access electron emerg establish health implement inform necessari need obtain procedur protect emerg procedur health plan implement depart emerg access fire offic build ic control detect dispos electron ensur health implement improperli inform integr measur modif integr pa access authent claim electron entiti health implement inform person procedur protect seek verifi authent entiti health procedur ident implement certif identif password kei sed decrypt electron encrypt health implement inform mechan protect encrypt kei algorithm password disk bit drive certif cipher decrypt ted appropri deem electron encrypt health implement inform mechan protect encrypt decrypt kei algorithm password cipher string byte bit ts access commun electron guard health implement inform measure network protrect secur techni transmiss secur encrypt transmiss kei health risk integr em ail data com uui assign ident identif identifi number track uniqu user identif password authent kei login mail card com figure .
key terms in original vs. modified queries figure .
the effect of query modification on stubborn trace queries 253analysts with a potentially useful option to help them improve results for unsatisfactory traces.
the experiments reported in this paper represent an initial expl oration of the use of query expansion techniques to support r equirements traceability.
however there are several threats to validity that must be considered.
the most obvious threat is that the experiments were all conducted in the healthcare domain and focused on tracing hipaa regulations.
we therefore cannot claim generality of the approach.
this is especially true because info rmation retrieva l methods are quite sensitive to the nuances of various contexts and so could behave quite differently in different domains.
secondly our approach makes the assumption that documents can be found on the web to support each of the search queries.
however this may not always be the case in which case no expansion terms will be recommended.
finally the exper iments involved exploring the combination of many different factors and although we approached this in a systematic way it is possible that some impo rtant combinations went unexplored.
nevertheless this work represent a first step in focusing on the problem of stubborn traces and makes a unique contribution to traceability research through demonstrating that query modific ation can be used effectively to improve many of these traces.
the results from these experiments introduce further questions that will be explored in future work.
for example we need to develop techniques for more dynamically sizing chunks in the retrieved documents according to the relevance of the retrieved data however over constraining these chunks to sections of text that closely match the initial query might undermine the goal of finding alternate query terms.
finally in order to draw general conclusions we need to conduct a f urther series of experiments to evaluate the query augmentation technique against a much broa der set of datasets representing a variety of domains.
.