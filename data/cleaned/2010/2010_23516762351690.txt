automatic query performance assessment during the retrieval of software artifacts sonia haiduc wayne state univ.
detroit mi usa gabriele bavota univ.
of salerno fisciano sa italy rocco oliveto univ.
of molise pesche is italy andrea de lucia univ.
of salerno fisciano sa italy andrian marcus wayne state univ.
detroit mi usa sonja wayne.edu gbavota unisa.it rocco.oliveto unimol.it adelucia unisa.it amarcus wayne.edu abstract text based search and retrieval is used by developers in the context of many se tasks such as concept location traceability link retrieval reuse impact analysis etc.
solutions for software text search range from regular expression matching to complex techniques using text retrieval.
in all cases the results of a search depend on the query formulated by the developer.
a developer needs to run a query and look at the results before realizing that it needs reformulating.
our aim is to automatically assess the performance of a query before it is executed.
we introduce an automatic query performance assessment approach for software artifact retrieval which uses measures from the field of text retrieval.
we evaluate the approach in the context of concept location in source code.
the evaluation shows that our approach is able to predict the performance of queries with accuracy using very little training data.
categories and subject descriptors d. .
distribution maintenance and enhancement corrections enhancement.
general terms measurement experimentation.
keywords query performance text retrieval concept location .
introduction text based search and retrieval are frequently employed by developers when looking for software artifacts that might be helpful for their task at hand.
for more than two decades text retrieval tr based search is being successfully applied to a multitude of software engineering se tasks including concept location impact analysis code reuse traceability link recovery bug triage requirements analysis etc.
approaches using tr usually require formulating a query and return a list of ranked software artifacts.
the developer examines the list of artifacts and for each of them decides if it is relevant to the current task or not.
the performance of any tr based search technique used in se depends strongly on the text query and its relationship with the text contained in the software artifacts.
writing good queries is not easy especially when searching for source code.
one of the causes is the vocabulary mismatch problem i.e.
developers often use different language to describe the code and its behavior than they use to implement it.
in addition most tr techniques rely on complex statistics which are rather opaque to the user i.e.
it is not always clear to the user why a document is ranked high low with respect to a query.
the performance of a query reflects how well the query retrieves the desired documents when executed by a tr approach.
a high performing query retrieves the relevant documents on top of the results list.
conversely a low performing query either retrieves the desired documents in the bottom part of the list of results or it does not retrieve them at all.
when low performing queries are executed the developer will spend time and effort analyzing irrelevant search results before she decides to reformulate the query.
our goal is to overcome this problem by estimating the performance of a query before it is executed.
we want to identify the queries that are likely to perform poorly immediately after they are written and notify the developer that a reformulation of the query is likely needed.
in order to solve this problem we get our inspiration from the field of natural language nl document retrieval.
in that field query performance prediction has been actively researched over the past decade.
we found that while similar the query performance problems for nl documents and software documents exhibit some essential and important differences.
in se the relevance of a result to the query is much more sensitive to the context i.e.
the se task at hand than in nl document retrieval.
for example during concept location in source code it is more important to improve the rank of one relevant artifact i.e.
find something relevant quickly rather than improving the rank of all the relevant artifacts as it is common in nl document retrieval .
at the same time solutions in nl are usually based on measuring different properties of the query and of the nl documents which are not always applicable to se artifacts e.g.
the text extracted from the source code is not always correct english .
thus a careful selection of measures that can be applied to software artifacts is needed.
moreover many solutions in nl require the execution of the query in order to make an assessment of its performance i.e.
post retrieval techniques .
since our goal is to offer quick feedback to developers about their queries we focus only on assessments performed before retrieval i.e.
pre retrieval techniques .
most pre retrieval techniques rely on measuring some properties of the query e.g.
coherence and their relationship with the corpus e.g.
the similarity between the query and the entire document collection .
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
ase september essen germany.
copyright acm ... .
.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
ase september essen germany copyright acm ... .
we propose a solution to the problem of query performance assessment in se by adapting nl inspired solutions and using them on software data.
our approach uses selected measures which assess four different aspects that can influence the performance of a query specificity similarity coherency and term relatedness.
our technique uses classification trees to learn rules constructed using some of the measures which classify queries as having high or low performance.
we evaluated the proposed approach in the context of tr based concept location in source code.
the results on five software systems indicate that our approach is able to correctly assess the performance of queries for the task of concept location with an accuracy of i.e.
it classifies correctly of the queries in average .
the technique performs better than expected as the results are better than those achieved by state of the art approaches in nl document retrieval .
the paper is a premiere in se as no prior work addressed query performance prediction on software corpora.
considering the good performance of our technique we anticipate that it can be used to help developers reformulate their queries faster hence saving effort and time while solving their se tasks.
.
related work the need for an in depth analysis of query performance surfaced in the text retrieval conference1 trec community as the high variance in performance across different queries became evident for the tr approaches participating in the trec competitions.
in consequence a special conference track was created between i.e.
the robust track where a new challenge was introduced that required predicting the performance of the participating tr approaches on each of the queries in the competition .
the query performance assessments were done based on different measures that captured various properties of the queries document collections and list of search results.
the prediction power of each measure was determined by correlating its values with the average precision ap values achieved by the queries after execution.
a high correlation would indicate that the measure is able to assess the performance of a query in terms of ap.
the correlations obtained were however very low and even negative in some cases.
this outcome of the robust track indicated that assessing the performance of queries is a challenging problem and sprouted the research on this topic.
since then numerous approaches for assessing the performance of a query have been proposed in the nl document retrieval field but the main goal has remained the same predicting the ap of a query based on measures that correlate with it.
a few papers in the field of nl document retrieval have investigated the query performance assessment problem from the perspective of classifying incoming queries into easy to answer high performing and hard to answer low performing queries .
in these works several classification approaches have been used for this purpose and in each case decision trees were found to be the most adequate for solving this problem.
while we take inspiration from this work in using classifiers and in particular decision trees for predicting if a query exhibits low of high performance our work is significantly different in terms of the measures used to train the classifiers and the timing of the prediction i.e.
pre retrieval vs. post retrieval .
in terms of measures the approaches in nl document retrieval usually use a combination of pre retrieval measures based on the length and some linguistic properties of the query and post retrieval measures collected after the query is executed.
while we use pre retrieval measures in our approach we chose a different set of measures for two reasons.
first the measures based on query length were proved to have little to no bearing on the query performance .
second the rest of the pre retrieval measures used were all based on linguistic properties of the query and made use of word relationships as defined in a nl context.
software artifacts e.g.
source code do not always follow the rules of discourse found in nl documents .
thus the linguistic measures are not generally applicable to software and in consequence we do not use them in our approach.
within se predicting the performance of queries has not been tackled.
the somewhat related work in se deals with the manual or automatic query reformulation and refinement mostly based on relevance feedback mechanisms.
also some studies have investigated the results of formulating different queries for the same information need which highlighted the strong dependence of the retrieval performance on the query and motivates our work.
.
proposed approach we propose an approach for automatically assessing the performance of queries before they are executed in the context of se tasks.
while this bares clear resemblances to the analogous problem in nl there are some aspects that make se tasks unique.
as mentioned before in many se tasks such as concept location it is more important that one relevant artifact is found as soon as possible rather than retrieving all relevant ones.
even when the retrieval of all relevant software artifacts is needed software data offers additional means for retrieving the complete set of the relevant artifacts once the first one is identified e.g.
dependencies in the code .
for example in source code a call graph can be used to locate the other relevant artifacts starting from the first one identified .
query performance assessment for certain se tasks would therefore benefit more from determining if the query leads to quickly identifying the first relevant document than from predicting the ap i.e.
average precision .
however there are se tasks such as traceability link recovery between software artifacts that may gain more by reducing the ap of the retrieval.
we focus on the former category of tasks in this work which require rethinking and adapting existing work from nl document retrieval.
our goal is to offer a clear and pragmatic answer to developers indicating if a query is worth pursuing or requires reformulation.
the proposed approach offers such an answer by classifying queries in two categories i.e.
high performing or low performing where the latter are queries that require reformulation.
the term query performance refers here to the ability of the query to retrieve the relevant software artifacts to the task at hand in such a way that they are easily accessible by developers i.e.
they are placed close to the top of the result list .
a query that achieves this is considered a high performing query as opposed to low performing queries which either fail to retrieve the relevant documents altogether or they place them at high ranks in the list of results making them hard to reach by developers.
note that the definition of high performing and low performing queries may need to be reformulated based on the current se task.
for example in some applications a high performing query may be considered one that retrieves the highest ranking relevant artifact 91within the top whereas in other cases this threshold could be set to .
our proposed approach uses classification trees in order to assign queries to one of the two categories.
decision tree learning has been previously applied successfully to query performance prediction in nl and also to analyzing se data i.e.
for defect prediction .
in order to train the classification trees for assessing query performance we make use of a set of carefully selected state of the art pre retrieval performance prediction measures from the field of tr for nl documents.
we selected such measures which refer to four aspects that can influence the performance of a query specificity similarity coherency and term relatedness.
the measures were selected such that they can be applied to any type of software artifacts.
for example measures making use of word relationships based on wordnet2 were not selected due to the fact that the lexical relationships in software data have a different nature than the ones in english .
the rest of this section is organized as follows.
subsection .
presents an overview of the process followed in our approach followed by subsection .
which presents the measures used and subsection .
which contains details about the classification approach.
.
process our approach is applicable to any se task that makes use of tr based search and it consists of several steps described below.
.
.
collect training data the first step in our approach deals with collecting the data needed for training the classification trees.
this data consists of sets made of tuples each containing the query the pre retrieval measures for the query and the class of the query i.e.
high performing or low performing .
the queries may be collected from different sources depending on the se task.
for example in the case of traceability link recovery the queries may be requirements documents or fragments of requirements source code design documents etc.
in the case of concept location the queries are either formulated by developers or are automatically extracted from bug and feature repositories.
once the queries are collected the performance measures are computed based on the text of the query and on the software artifacts in the collection.
last the queries are executed using a tr engine and based on the results their class is determined i.e.
high performance or low performance .
.
.
build the classification tree at this point the classification tree can be trained using the data collected.
after this initial training the classifier rules are built and they can be used to assess the performance of new queries.
there are two different approaches that can be used for training the classifier.
the first is based on training the classifier independently for each new software system thus using only the training data from one system at the time.
while this approach might be able to assess the performance of queries better for a particular system as it adapts to its specific features it may not be applicable to new systems.
the second approach is based on training the classifier based on data from a set of systems with the purpose of creating a general model that can be applied to a variety of software systems.
we evaluate both approaches for the task of concept location in our study presented in section .
more details about the classifier used by our approach can be found in section .
.
.
.
assess the performance of new queries once the classifier is built it can be applied for assessing the performance of new queries based only on computing the subset of the measures of query performance which are included in the rules of the classification tree.
thus the classifier may be able to determine if a query needs reformulation immediately after it was formulated by the developer.
.
query performance aspects and measures this section presents the pre retrieval query performance assessment measures used by our approach.
more details including the formulas used to compute each of the measures can be found in and and are summarized in our online appendix3.
.
.
specificity specificity refers to the ability of the query to represent the current information need and discriminate it from others.
a query composed of non specific terms commonly used in the collection of documents is considered having low specificity as it is hard to differentiate the relevant documents from non relevant ones based on its terms.
for example when searching source code the query initialize members could have low specificity if a comment containing this text would be found in most class constructors in a system.
specificity measures are usually based on the query terms distribution over the collection of documents.
for our approach we considered eight specificity measures from the text retrieval literature namely average inverse document frequency avgidf maximum inverse document frequency maxidf standard deviation of the inverse document frequency devidf average inverse collection term frequency avgictf maximum inverse collection term frequency maxictf standard deviation of inverse collection term frequency devictf query scope qs and simplified clarity score scs .
the first six measures are based on using the inverse document frequency metric idf which is the inverse of the number of documents in the collection in which a term appears and the inverse collection term frequency ictf which is the inverse of the number of occurrences of a term in the entire document collection.
the assumption is that the more documents a term appears in and the highest its frequency over the entire collection is the more difficult it is to discriminate the relevant documents based on it.
thus query terms should have high idf and ictf values and a high performance query should have a high avgidf and avgictf which are the average idf and ictf among the query terms.
maxidf and maxictf which represent the maximum idf and ictf values across all query terms respectively are popular variations of the average and are also expected to assume high values in the case of high performance queries.
devidf and devictf are the standard deviations of the idf and ictf values over the query terms and assume that low variance reflects the lack of dominant discriminative terms in the query which may prevent the retrieval of relevant documents.
in consequence devidf and devictf are expected to have high values for high performance discriminative queries.
92qs query scope measures the percentage of documents in the collection containing at least one of the query terms.
a high qs value indicates that there are many candidates for retrieval thus separating relevant documents from irrelevant ones might be difficult.
a query should therefore aim at having a low qs.
the last specificity measure we considered is scs which measures the divergence of the query language model from the collection language model as an indicator of query specificity.
the measure considers that a query is not specific if the language used in it is similar to the language used in the entire collection of documents which indicates a large number of documents that could potentially be retrieved.
a high scs indicating a significant divergence of the two language models is thus desirable.
in addition to the metrics existent in the field of tr we considered four new metrics based on using information entropy in order to identify discriminative high performance queries.
in a preliminary study we have shown that entropy is a better indicator of query specificity for se tasks than the leading specificity measures from text retrieval.
therefore we defined four query specificity measures using entropy avgentropy which is the average entropy value among the query terms medentropy and maxentropy which represent the median and the maximum entropy values across the terms in the query and deventropy which is the standard deviation of the entropy across all query terms.
as low entropy indicates high information content the desirable values for a high performance query are low for the first three entropy based measures.
for deventropy high values are wanted.
.
.
similarity the similarity between the query and the entire document collection is considered as being another indicator of query performance.
the argument behind this type of measures is that it is easier to retrieve relevant documents for a query that is similar to the collection since high similarity potentially indicates the existence of many relevant documents to retrieve from.
the existing similarity approaches for query performance in the field of text retrieval make use of a metric called collection query similarity scq .
this is computed for each query term and is a combination of the collection frequency of a term ctf and its idf in the corpus.
three measures of a query s performance were defined based on it namely sumscq which is the sum of the scq values over all query terms avgscq which is the average scq across all query terms and maxscq which represents the maximum of the query term scq values.
in the case of every scq based measure a high value is expected for high performance queries.
.
.
coherency another performance indicator for queries is their coherency which measures how focused a query is on a particular topic.
the coherency of a query is usually measured as the level of inter similarity between the documents in the collection containing the query terms.
the more similar the documents are the more coherent the query is.
the coherence score cs of a term is one of the measures used for this performance aspect and it reflects the average pairwise similarity between all pairs of documents in the collection that contain that particular term.
the cs of the query is then computed as the average cs over all its query terms and it is expected to be high in the case of high performance queries.
a second approach for measuring the query coherency is based on measuring the variance var of the query term weights over the documents containing the terms in the collection.
the weight of a term in a document indicates the importance or relevance of the term for that document and it can be computed in various ways.
one of the most frequent ways to compute it which we also adopt in our implementation is tf idf i.e.
a combination between the frequency of a term in the document tf and the term s idf value over the document collection.
the intuition behind measuring the variance of the query term weights is that if the variance is low then the retrieval system will be less able to differentiate between highly relevant documents and less relevant ones making the query harder to answer.
three measures based on var have been defined i.e.
sumvar which is the sum of the variances for all query terms avgvar computed as the average var value across all query terms and maxvar which is the maximum var value among the query terms.
as in the case of cs high values are expected for high performance queries.
.
.
term relatedness term relatedness measures make use of term co occurrence statistics in order to assess the performance of a query.
the terms in a query are assumed to be related to the same topic and are thus expected to occur together frequently in the document collection.
we use two measures of term relatedness previously used in text retrieval both using the pointwise mutual information pmi metric which is based on the probability of two terms appearing together in the corpus.
the two pmi based metrics are avgpmi and maxpmi which compute the average and the maximum pmi values across all query terms.
.
the classifier as mentioned before we use a classification tree in order to determine rules that can predict if queries are high or low performing.
a classification tree is a prediction model that can be represented as a decision tree .
such a prediction model is suitable to solve classification type problems where the goal is to predict values of a categorical variable from one or more continuous and or categorical predictor variables.
in our work the categorical dependent variable is represented by the query performance high or low while the query performance measures represent the predictor variables.
training data with pre assigned values for the dependent variables are used to build the classification tree.
this set of data is used by the classification tree to automatically select the predictor variables and their interactions that are most important in determining the outcome variable to be explained.
the constructed classification tree is represented by a set of yes no questions that splits the training sample into gradually smaller partitions that group together cohesive sets of data i.e.
those having the same value for the dependent variable.
an example of classification tree based on two pre retrieval measures can be found in figure .
classification trees have some additional benefits.
first the classification rules they produce are easy to understand by humans which is not true for other more complex models.
second they offer automatic feature selection.
this is a very important property as it allows using as input a large set of measures that might capture a phenomenon without having to determine beforehand the ones that represent the phenomenon the best.
this allows us to use all performance measures as input as our classification tree will determine automatically the 93measures needed for the classification.
one last advantage of using classification trees is that the classification is performed very fast compared to using other slower techniques.
in our study presented in section we use the cart classification and regression tree implementation of classification trees provided in the statistical platform r4.
figure .
example of classification tree .
evaluation we evaluated our approach for concept or feature location in source code as many existing concept location techniques use tr based solutions .
in the context of software change concept location is concerned with identifying a point of change e.g.
a class or a method given a change request.
.
query performance assessment for concept location in source code in order to collect the queries needed for the case study we used an approach frequently adopted in concept location empirical studies.
the approach is based on change reenactment and user simulation i.e.
automatically extracting queries and the changed code from bug reports found in online bug tracking systems.
we collected queries for five open source object oriented oo systems from different problem domains implemented in java and c which are summarized in table .
adempiere5 is a common based peer production of open source enterprise resource planning applications.
atunes6 is a full featured media player and manager.
filezilla7 is a graphical ftp ftps and sftp client jedit8 is a programming editor and winmerge9 is a document differencing and merging tool.
table .
the systems used in the study and their properties system version language kloc methods queries adempiere .
.
java atunes .
.
java filezilla .
.
c jedit .
java winmerge .
.
c total for each system we built the source code corpus used by the tr search by considering every method in the system as a separate document.
for each method we extracted the terms found in its source code identifiers and comments.
we then normalized the text using identifier splitting we also kept the original identifiers stop words removal i.e.
we removed common english words and programming keywords and stemming we used the porter stemmer .
the corpus was indexed by lucene10 a popular implementation of the vector space model.
we then identified for each system a set bug reports that correspond to bugs that are present in the version of the software system used in our study but fixed in a later version.
we also determined the set of methods that were modified in order to fix each bug based on the patches attached to the bug reports in the online issue tracking systems.
this set of methods represents the oracle for concept location.
we will refer to these methods as the target methods.
for each change request we created two queries extracted from the online issue tracking systems.
the first query was obtained from the title of the bug report i.e.
the short description while the second query was represented by the description of the bug i.e.
the long description .
as usually done for concept location any trace information or log files contained in these descriptions were eliminated prior to the extraction.
also the normalization techniques applied on the corpus were applied on the extracted queries as well.
table reports the number of queries we selected for each system.
for example from bug of adempiere we obtained the following two queries after extraction and normalization in parenthesis is the original text extracted from the bug reports before the normalization .
print invoic process draft select print invoices process draft selection .
us garden world select date rang in todai all invoic select regardless document statu client bad print post custom us email option draft potenti cancel invoic sent using garden world if you select a date range from somewhere in to today then all invoices are selected regardless of document status or client!!!
not so bad if you are printing them and posting them to customers but if you use the email option then drafted and potentially cancelled invoices are sent too!
while fixing this bug the target method changed by the developers was found in the process package invoiceprint.java file and invoiceprint class.
the document corresponding to this method is the one that the queries are supposed to retrieve.
during concept location it is important that developers find their target method i.e.
the method where they have to start the change as fast as possible.
other methods that will change are identified during impact analysis.
when reenacting concept location the success criterion is translated into the rankings of the target methods as opposed to many other tr applications where recall and precision are considered .
in other words if any of the target methods ranks in among the top retrieved results we consider it a successful retrieval.
a rule used in concept location applications is that finding a target method among the top ranked results is considered a good result based on the assumption that most developers would look at no more than methods before reformulating their query.
hence we define a query as high performing if any of the target methods is retrieved low performing query low performing query high performing query avgidf .
maxvar .
true false false true 94in the top results.
otherwise we consider the query as low performing.
in the above example if a query returns the target method in top then it is considered high performing.
the rank i.e.
the position of the target method in the ranked list of results retrieved by the two queries in the previous example as well as the classification of the two queries and the values of the measures of query performance are presented in table .
for all measures besides avgentropy maxentropy medentropy and qs a higher value indicates a better query.
table .
the pre retrieval measures of the short and long description queries for bug in adempiere measure short long measure short long avgidf .
.
scs .
.
maxidf .
.
avgvar .
.
devidf .
.
maxvar .
.
avgictf .
.
sumvar .
.
maxictf .
.
cs .
.
devictf .
.
avgscq .
.
avgentropy .
.
maxscq .
.
medentropy .
.
sumscq .
.
maxentropy .
.
avgpmi .
.
deventropy .
.
maxpmi .
.
qs .
.
rank class low high we classify in this way all the queries used in our evaluation.
table reports the number of high and low performing queries for each system.
table .
high and low performing queries system high performing queries low performing queries adempiere atunes filezilla jedit winmerge total .
.
validation method in order to evaluate the ability of the proposed methodology in predicting the query performance we performed two types of validation single system and cross system validation.
for the single system validation the classification model was trained on each system individually and a fold cross validation was performed.
building the classification tree took less than second per system using a macbook pro with a 2ghz dual core intel core duo processor and 4gb ram.
the process for the single system validation is composed of five steps i randomly divide the set of queries for a system into approximately equal subsets ii set aside one query subset as a test set and build the classification model with the queries in the remaining subsets i.e.
the training set iii classify the queries in the test set using the classification model built on the query training set and store the accuracy of the classification iv repeat this process setting aside each query subset in turn v compute the overall average accuracy of the model.
the misclassification rate of the model has been evaluated in terms of type i and type ii classification errors.
a type i error occurs when the model wrongly classifies a high performing query as low performing while a type ii error is when the model wrongly classifies a low performing query as high performing.
for the cross system validation we performed the same fold cross validation process this time considering all the queries from the different object systems as a single dataset.
when dividing the dataset into approximately equal subsets we ensured that in both training and test sets there was the same percentage of queries belonging to the different object systems.
the two types of validation i.e.
single system and cross system were both needed in order to derive guidelines on how to use historical data to build the classifier.
in particular we aimed at analyzing whether a specialized model is required for each system or it is possible to define a generic model that could be applied on several systems.
.
.
baselines in the context of our study we compared our approach based on classification trees with four baseline approaches logistic regression a random classifier and two variants of a constant classifier pessimistic and optimistic .
the random classifier randomly selects a prediction from the possible values i.e.
high or low.
the two values have the same probability to be selected.
the constant classifier always predicts a specific value disregarding the instance.
in particular the pessimistic constant classifier always classifies a query as low while the optimistic constant classifier works in the opposite way always classifying a query as high performing.
it is worth noting that a classifier is useful only if it outperforms a random or constant classifier.
logistic regression is used for the prediction of the occurrence probability of an event by fitting data to a logistic function .
it is one of the most commonly used classification techniques and it has been applied to software engineering problems as well as other experimental fields.
for this reason we decided to use it as an additional baseline for comparison in our study.
given the novelty of our work there is no prior state of the art technique to compare our results with.
formally the multivariate logistic regression model is based on the formula x!
x!
... x!
e!!!
!
!
!
!!
!
!!
!
!
e!!!
!
!
!
!!
!
!!
!
!
where x!
are the independent variables i.e.
the pre retrieval measures and is a value on the logistic regression curve.
in a logistic regression model the dependent variable is commonly a dichotomous variable and thus assumes only two values i.e.
it states whether a query is high or low performing.
in our study we used the weka11 tool for the definition of a logistic model.
before applying logistic regression to a dataset it is a common approach to perform feature selection in order to determine which features should be considered when building the logistic model.
we performed feature selection among the pre retrieval measures using the gain ratio technique implemented in weka.
.
quantitative results figure and table report the results achieved in the single system evaluation.
in particular table shows the number of type i and type ii misclassifications performed by the experimented classifiers.
the total number of errors performed by the classification tree cart is type i type ii compared to for the logistic regression type i type ii for the optimistic constant model all of type ii for the 95pessimistic constant model all of type i and for the random predictor of type i of type ii .
this indicates that the model built using the classification tree significantly outperforms all the baseline classifiers by correctly classifying of the examined queries i.e.
out of see figure .
in comparison the model built using the logistic regression correctly classified only queries the optimistic and pessimistic constant predictors classified correctly and respectively and the random classifier was able to correctly assess queries .
in addition to the significantly better results obtained by the classification tree based predictor over the baseline approaches our results are better than even state of the art results from the nl document retrieval field.
by comparison the best approaches in nl document retrieval correctly classify on average between and of the queries.
figure .
accuracy achieved in the single system evaluation it is also worth noting that the accuracy achieved by the proposed classifier was obtained using very small training samples the average dimension of the employed training samples is queries.
thus the proposed approach is able to provide excellent results with relatively little training.
such results emphasize the applicability of the proposed approach as it does not require a large training set that might be not available for some software projects.
the classification tree is the most accurate predictor on all the systems except filezilla.
on this system cart is able to correctly classify only of the queries whereas the pessimistic constant predictor achieves a correct classification on of the queries.
these results are explained by the fact that in filezilla most of the queries are low performing out of as shown in table .
this has the following consequences which affect our results i the classification tree faces an increased difficulty in identifying the characteristics of the high performing queries given that only a small number of such queries are available in the training set and ii the pessimistic constant predictor obtains a very good performance as it always classifies queries as low performance and is thus correct in classifying all the low performing queries which represent the majority of the data.
the classification tree built on the adempiere software system is reported in figure .
based on the rule of this classification tree we can see that the short description query for bug in adempiere is correctly classified as a low performing query as its sumscq is .
which is smaller than .
see table .
at the same time the long description query of the same bug is also correctly classified but as a high performing query having the sumscq equal to .
thus greater than .
.
figure .
a classification tree for adempiere in our single system evaluation the decision tree always selected one measure to discriminate between high performing and low performing queries in the example reported in figure the measure sumscq was selected .
however the measure used to build the classification tree was often different among the software systems and sometimes even among the different training samples used in the fold validation on the same system.
table shows the measures selected in each run of the single system evaluation for each system.
table .
predictor selected by the classification tree in the single system evaluation system 1st fold 2nd fold 3rd fold 4th fold adempiere sumscq sumscq sumscq sumscq atunes devidf medentropy devidf devidf jedit devidf medentropy devidf medentropy filezilla maxscq devidf avgidf avgidf winmerge avgentropy avgentropy avgentropy avgentropy the fact that the measure selected for building the classification tree is generally different among the object systems highlights the fact that different software corpora having different characteristics e.g.
verbosity vocabulary dimension etc.
may require different classifiers to estimate the performance of a query.
this is confirmed also by the cross system evaluation whose results are presented in table .
low performing query high performing query sumscq .
false true table .
type i and type ii errors in the single system evaluation.
system cart logistic regression optimistic constant pessimistic constant random type i type ii type i type ii type i type ii type i type ii type i type ii adempire atunes filezilla jedit winmerge 96the results illustrate that the cross system classification tree performs poorly as it correctly classifies only of the queries.
its results are comparable to the results of the baseline techniques which never achieve a correct classification rate higher than .
this indicates that the assessment of query performance is strongly dependent on the system.
in consequence training needs to be performed on each system independently in order obtain a correct classification of the performance of incoming queries for the same system .
table .
type i and type ii errors in the cross system evaluation method type i type ii type i ii correct cart logistic regression optimistic constant pessimistic constant random .
qualitative analysis of misclassifications among the errors performed by our classifier out of are type i errors i.e.
a high performing query is classified as a low performing one while out of are type ii errors occurring when a low performing query is classified as a high performing one.
we manually analyzed all type i errors and we were not able to identify any pattern that could help understand why the proposed approach misclassifies the queries.
one of the challenges cause by using bug descriptions and change data for such studies is that in some cases the bug descriptions capture the observed behavior of the system whereas the bug is fixed in a part of a code more with different vocabulary characteristics.
we believe that if we use more data both in terms of queries and systems we may get a richer set of type i errors which will reveal some patterns.
figure .
bug report in filezilla on the other hand we were able to make interesting observations from the analysis of the type ii errors.
we analyzed the two different aspects involved in it i.e.
why the queries performed poorly and why our classifier considers these queries as high performing.
in order to explain our findings we use an example of type ii error from the filezilla software system.
in filezilla the bug reports figure and figure both have the onmenudelete wxcommandevent method from the cremotetree class as the target method.
both bug reports describe problems occurring during the deletion of a folder.
as described in our experimental design we extracted four different queries from these two bug reports i.e.
two from their titles and two from their descriptions.
the two queries for bug report retrieve the target method on positions and respectively while those for bug report recover the target method on positions and .
thus all four queries are low performing.
however two of these queries those retrieving the target method on position and are considered high performing by our classifier resulting in two type ii errors.
when investigating the origin of these errors we first examined the reasons for which the two queries were classified as high performing.
we found that the queries contain some highly discriminating terms i.e.
rmd cwd which appear in few documents in the corpus thus justifying the decision of the classifier to consider them as high performing.
figure .
bug report in filezilla since we considered that the classifier performed properly in this case we further focused on understanding why none of the queries composed for the two bug reports were able to retrieve the target method in the top results.
we first investigated whether it is possible to retrieve the target method among the top results by using a query based on the terms in the bug report.
we thus investigated the performance of each individual term in the bug reports when run as a query as well as that of all the possible combinations of terms from the two bug reports.
in other words for each bug report we first executed all the queries composed of one of the terms present in it then all the queries composed of pairs of terms and so on.
we formulated and ran a total of queries for bug report and for bug report .
the highest performing query was delete remote which was able to retrieve the target document on position .
therefore no query based on terms from the bug reports was able to retrieve the target method in the top results.
we then investigated the target method in order to understand what the queries based on bug report terms were missing.
the target method contained the following terms in parenthesis the number of times they appear in the method 97path menu item context event remote state tree view rename wx command idl server empty delete the first thing we noticed was that the terms contained in the method were mostly generic terms which were found in many other methods in the system path present in methods menu in item in context in event in remote in state in tree in view in rename in wx in command in idl in server in empty in delete in thus retrieving this method in the first results would be difficult even for a query containing terms from the method as there are many other candidates for retrieval i.e.
the other methods containing these terms.
this is also an example where the vocabulary mismatch between the bug description i.e.
system behavior and the target method implementation is quite strong.
we conjecture that in such cases type ii misclassifications are more likely to occur.
however we need more data to confirm this hypothesis which we plan to do in the future.
in conclusion in cases like the above example it is most likely that misclassification would occur due to the fact that the target method is generally hard to retrieve resulting in a low performing query.
in these situations other approaches may be needed in order to complement tr during concept location such as static or dynamic analysis.
.
threats to validity this section discusses the main threats to validity that could affect our results.
construct validity threats concern the relationship between theory and observation.
we evaluated the proposed predictor through two metrics i.e.
type i and type ii errors that are widely used to evaluate predictor models .
in addition we analyze and compare the overall classification accuracy of the proposed approach taking into account the number of queries correctly and wrongly classified and also perform a qualitative analysis of the errors.
concerning the internal validity in our experimentation we automatically extracted the set of queries from the online bug tracking system of the object systems.
in particular we extracted two different queries one derived from the title of the bug report and one from the description of the bug.
this choice could affect the results of our study since such queries are approximations of actual user queries.
however developers are often faced with unfamiliar systems in which cases they must rely on outside sources of information such as issue reports in order to formulate queries during tr based concept location.
therefore we believe that the approach used in our experimentation resembles real usage scenarios.
nevertheless empirical studies conducted with users are required to evaluate our predictor in a real usage scenario and we plan to perform such studies in the near future.
the external validity refers to the generalization of our findings.
in order to address this threat we selected a set of five software systems from diverse domains implemented in two programming languages i.e.
java and c .
a larger set of queries and more systems would clearly strengthen the results from this perspective.
one threat to the external validity of our results is the fact that we used the results of only one tr engine in order to classify the queries as high performing or low performing.
more precisely we used the rank of the first target method retrieved by a query submitted to the lucene tr engine which is an implementation of the vsm technique.
since several other tr methods have been previously used to support concept location further experimentation is needed to analyze whether the proposed predictor works well also with other tr methods.
the last threat to external validity is related to the fact that we only evaluated the proposed approach for the task of tr based concept location.
thus we cannot and do not generalize the results to other se tasks.
we plan to evaluate the proposed predictor in other contexts such as traceability link recovery.
finally conclusion validity refers to the degree to which conclusions reached about relationships between variables are justified.
in our case study we only draw conclusions referring to the use of different classifiers which we support with evidence in the form of classification correctness and type i and ii errors.
.
conclusions and future work in this paper we proposed an approach that estimates the performance of a text query before it is executed in the context of tr based concept location.
the proposed approach can be used for other se tasks and it allows the identification of queries that are likely to perform poorly immediately after they are written.
this can save the developer time and effort as she can be notified right away when a query is unlikely to lead to satisfactory results and would likely need reformulation.
the proposed approach is based on using classification trees and pre retrieval query performance measures selected from the field of text retrieval.
our empirical evaluation showed that the classification trees built using very small training samples are able to correctly classify of queries in average strongly outperforming several baseline approaches as well as state of the art results from the field of tr.
in our future work we plan to perform a more extensive experimentation by evaluating several different classifiers e.g.
bayesian neural networks random forests etc.
using more tr techniques to classify the queries as high and low performing e.g.
lsi lda etc.
and applying our approach to other tr based se tasks e.g.
traceability recovery code reuse etc.
.
another direction we plan to pursue is the tool supported reformulation of low performing queries.
in particular once a low performing query is identified we plan to provide support to the developer to reformulate the query suggesting terms that can improve its performance.
such an approach can help developers find helpful software artifacts faster and finish their tasks sooner.
.