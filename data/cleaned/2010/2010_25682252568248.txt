unit test virtualization with vmvm jonathan bell columbia university west 120th st mc new y ork ny usa jbell cs.columbia.edugail kaiser columbia university west 120th st mc new y ork ny usa kaiser cs.columbia.edu abstract testing large software packages can become very time intensive.
to address this problem researchers have investigated techniques such as test suite minimization.
test suite minimization reduces the number of tests in a suite by removing tests that appear redundant at the risk of a reduction in fault nding ability since it can be di cult to identify which tests are truly redundant.
we take a completely di erent approach to solving the same problem of long running test suites by instead reducing the time needed to execute each test an approach that we call unit test virtualization.
with unit test virtualization we reduce the overhead of isolating each unit test with a lightweight virtualization container.
we describe the empirical analysis that grounds our approach and provide an implementation of unit test virtualization targeting java applications.
we evaluated our implementation vmvm using real world java applications and found that it reduces test suite execution time by up to on average when compared to traditional unit test execution.
we also compared vmvm to a well known test suite minimization technique nding the reduction provided by vmvm to be four times greater while still executing every test with no loss of fault nding ability.
categories and subject descriptors d. .
testing and debugging testing tools general terms reliability performance keywords testing test suite minimization unit test virtualization .
introduction as developers x bugs they often create regression tests to ensure that should those bugs recur they will be detected permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
icse june hyderabad india copyright acm ... .
.by the test suite.
these tests are added to existing unit test suites and in an ideal continuous integration environment executed regularly e.g.
upon code check ins or nightly .
because developers are often creating new tests as software grows in size and complexity its test suite frequently grows similarly.
software can reach a point where its test suite has gotten so large that it takes too long to regularly execute previous work has reported test suites in industry taking several weeks to execute fully .
to cope with long running test suites testers might turn to test suite minimization or test suite prioritization .
test suite minimization techniques such as seek to reduce the total number of tests to execute by approximating redundant tests.
however identifying which tests are truly redundant is hard and test suite minimization approaches typically rely on coverage measures to identify redundancy which may not be completely accurate leading to a potential loss in fault nding ability.
furthermore test suite minimization is an npcomplete problem and therefore existing algorithms rely on heuristics.
test suite prioritization techniques such as re order test cases for example so that given the set of changes to the application since the last test execution the most relevant tests are executed rst.
this technique is useful for prioritizing test cases to identify faults earlier in the testing cycle but does not actually reduce the total time necessary to execute the entire suite.
rather than focus our approach on reducing the number of tests executed in a suite we have set our goal broadly on minimizing the total amount of time necessary to execute the test suite as a whole while still executing all tests and without risking loss in fault nding ability.
we conducted a study on approximately large and open source java applications to identify bottlenecks in the unit testing process.
we found that for most large applications each test executes in its own process rather than executing multiple tests in the same process.
we discovered that this is done to isolate the state based side e ects of each test from skewing the results for future tests.
the upper half of figure shows an example of a typical test suite execution loop before each test is executed the application is initialized and after each test the application terminates.
in our study we found that these initialization steps add an overhead to testing time of up to of the total testing time on average .
at rst it may seem that the time spent running tests could be trivially reduced by removing the initialization step from the loop performing initialization only at the beginning of the test suite.
in this way that initialized applicationpermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
icse may june hyderabad india copyright acm ... .
begin testinitialize applicationrun testterminate applicationend test continue to nextbegintest suite begin testinitialize applicationrun testreset applicationend test continue to nextbegintest suitetraditional unit testing optimized unit testingfigure the test execution loop in traditional unit testing the application under test is restarted for each test.
in optimized unit testing the application is started only once then each test runs within the same process which risks in memory side e ects from each test case.
could be reused for all tests illustrated in the bottom half of figure cutting out this high overhead.
in some cases this is exactly what testers do writing pre test methods to bring the system under test into the correct state and post test methods to return the system to the starting state.
in practice these setup and teardown methods can be difcult to implement correctly developers may make explicit assumptions about how their code will run such as permissible in memory side e ects.
as we found in our study of real world java applications described further in x2 developers often sacri ce performance for correctness by isolating each test in its own process rather than risk that these side e ects result in false positives or false negatives.
our key insight is that in the case of memory managed languages such as java it is not actually necessary to reinitialize the entire application being tested between each test in order to maintain this isolation.
instead it is feasible to analyze the software to nd all potential side e ect causing code and automatically reinitialize only the parts necessary when needed in a just in time manner.
in this paper we introduce unit test virtualization a technique whereby the side e ects of each unit test are efciently isolated from other tests eliminating the need to restart the system under test with every new test.
with a hybrid static dynamic analysis unit test virtualization automatically identi es the code segments that may create side e ects and isolates them in a container similar to a lightweight virtual machine.
each unit test in a suite executes in its own container that isolates all in memory side e ects to contain them to a ect only that suite exactly mimicking the isolation e ect of executing each test in its own process but with much lower overhead.
this approach is relevant to any situation where a suite of tests is executed and must be isolated such as regression testing continuous integration or test driven development.
we implemented unit test virtualization for java creating our tool vmvm pronounced vroom vroom which transforms application byte code directly without requiringmodi cation to the jvm or access to application source code.
we have integrated it directly with popular java testing and build automation tools junit ant and maven and it is available for download via github .
we evaluated vmvm to determine the performance bene ts that it can provide and show that it does not a ect fault nding ability.
in our study of applications we found that the test suites for most large applications isolate each unit test into its own process and that in a sample of these applications vmvm provides up to a performance gain when executing tests.
we compared vmvm with a well known test suite minimization process and found that the performance bene ts of vmvm exceed those of the minimization technique without sacri cing fault nding ability.
the primary contributions of this paper are .
a categorical study of the test suites of open source projects showing that developers isolate tests .
a presentation of unit test virtualization a technique to e ciently isolate test cases that is language agnostic among memory managed languages .
an implementation of our technique for java vmvm released freely via github evaluated to show its e cacy in reducing test suite runtime and maintaining fault nding properties .
motiv ation this work would be unnecessary if we could safely execute all of an application s tests in the same process.
were that the case then the performance overhead of isolating test cases to individual processes could be trivially removed by running each test in the same process.
we have discovered however that developers rely on process separation to ensure that their tests are isolated and execute correctly.
in this section we answer the following three motivation questions to underscore the need for this work.
mq1 do developers isolate their unit tests?
mq2 why do developers isolate their unit tests?
mq3 what is the overhead of the isolation technique that developers use?
.
mq1 do developers isolate their tests?
to answer mq1 we analyzed the largest open source java projects listed by ohloh a website that indexes open source software .
at time of writing ohloh indexed over individual sources such as github sourceforge and google code comprising over projects and over billion lines of code .
we restricted ourselves to java projects in this study due to the widespread adoption of test automation tools for java allowing us to easily parse con guration les to determine if the project isolates its test cases a process described further below .
using the ohloh api we identi ed the largest open source java projects ranked by number of active committers in the preceding months.
table statistics for subjects retrieved from ohloh min max avg std dev loc .14k .40k .48k active devs .
.
.
.
age years .
.
.
.24551table projects creating a process per test grouped by tests per project and by lines of code per project of tests in project of projects creating new processes per testlines of code in project of projects creating new processes per test 10k 10k 100k 100k 1m 1m all projects all projects from the projects we downloaded the source code for repositories each project may have several repositories to track di erent versions or to track dependencies .
we captured this data between august and august .
basic statistics as calculated by ohloh for these projects appear in table showing the aggregate minimum maximum average and standard deviation for lines of code active developers and age in years.
a complete description of the entire dataset is available in the technical report that accompanies this paper .
the two most popular build automation systems for java are ant and maven .
these systems allow developers to write build scripts in xml with the build system managing dependencies and automatically executing pre deployment tasks such as running tests.
both systems can be con gured to either run all tests in the same process or to create a new process for each test to execute in.
from our projects we parsed these xml les to identify those that use junit as part of their build process and of those how many direct junit to isolate each test in its own process.
then we parsed the source les for each of the projects that use junit to determine the number of tests in each of these projects.
next we broke down the projects both by the number of tests per project and by the number of lines of code per project.
table shows the result of this study.
we found that of those projects with over tests create a new process for each test when executing it only do not isolate their tests in separate processes.
when grouping by lines of code of projects with over one million lines of code create new processes for each test case.
overall of those projects in our sample that use junit create separate processes for each test.
with these ndings we are con dent in our claim that it is common practice particularly among large applications which may have the longest running test suites to isolate each test case into its own process.
.
mq2 why isolate tests?
understanding now that it is common practice for developers to isolate unit tests into separate processes we next sought to answer mq2 why developers isolate tests.
perhaps in the ideal unit testing environment each unit test could be executed in the same application process with pre test and post test methods ensuring that the application under test is in a clean state for the next test.
however handwritten pre test and post test teardown methods can place a burden on developers to write and may not always be correct.
when these pre test and post test methods are not correct tests may produce false negatives missing bugs that should be caught or false positives incorrectly raising an exception when the failure is in the test case not in the application begin tested.for example mu slu et al.
discuss a bug in the apache commons cli library that took approximately four years from initial report to reach a con rmed x. this bug could be detected by running the application s existing tests independently of each other but when running on the same instance of the application using only the developer provided pre and post test methods to reset the application it did not present because it was masked by a hidden dependency between tests that was not automatically reset.
there can be many confounding factors that create such hidden dependencies between tests.
for instance methods may have side e ects that are undocumented.
in a complex codebase with hundreds of thousands of lines of code it may be very di cult to identify all potential side e ects of an action.
when a tester writes the test case for a method they will be unable to properly reset the system state if they are unaware of that method s implicit side e ects.
to avoid this sort of confusion testers may decide to simply execute each test in a separate process introducing signi cant runtime overhead to their test suite.
in the remainder of this subsection we describe these dependencies as they appear in the java programming language and show a real world example of one such dependency.
although some terminology is speci c to java these concepts apply similarly to other languages.
consider the following real java code snippet from the apache tomcat project shown in listing .
this single line of code is taken from the cookiesupport class which denes a series of con guration constants.
in this example the eld allow equals in value is de ned with the modi ers static final .static signi es that it can be referenced by any object regardless of position in the object graph.
the final modi er indicates that its value is constant once it is set it can never be changed.
the value that it is assigned on the right hand side of the expression is derived from a system property a java feature that mirrors environmental variables .
this initializer is executed only once in the application when the class containing it is initialized.
if a test case depends on the value of this eld then it must set the appropriate system property before the class containing the eld is initialized.
imagine the following test execution rst a test executes and sets the system property to false.
then the public s t a t i c f i n a l boolean allow equals invalue boolean .
valueof system .
getproperty grave.ts1 grave.ts1org .
apache .
tomcat .
u t i l .
http .
servercookie .
allow equals invalue grave.ts1 grave.ts1f a l s e .
booleanvalue listing cookiesupport.java an example of java code that breaks test independence552initializer runs setting the eld allow equals in value to false.
then the next test executes setting the system property to true expecting that allow equals in value will be set to true when the eld is initialized.
however because the value has already been set it will remain as it is false causing the second test to fail unexpectedly.
this scenario is exactly what occurs in the tomcat test suite and in fact in the source code for several tests that rely on this property the following comment appears note because of the use of static nal constants in cookies each of these tests must be executed in a new jvm instance .
although the above example was from a java application the sort of leakage that occurred could happen in practically any language provided that the developers follow a similar pattern.
in any situation where a program reads in some con guration from a le and stores it in memory there is the potential for such leakage.
there are certainly other potential sources of leakage between test executions.
for instance in java the system property interface mentioned above allows developers to set properties that are persisted for the entire execution of that process.
there are also various forms of registries provided by the java api to allow developers to register services and lookup environments these too provide avenues through which data could be leaked between executions.
while in some cases it is possible although perhaps complicated and time consuming to write post test methods to e ciently reset system state take note that our example thestatic final eld can not be manually reset.
the only option left to developers is to re architect their codebase to make testing easier for example by removing such elds at the cost of the time to re architect it and potential defects introduced by the new implementation or to isolate each test to a separate process.
.
mq3 the overhead of test isolation to gauge the overhead of test isolation we compared the execution time of several application test suites running in isolation with the execution time running without isolation.
from the set of approximately projects that include build scripts with junit tests that executed without modi cation or con guration on our test machine we selected projects for this study with the aim of including a mix of both widely used and recognizable projects e.g.
the apache tomcat project a popular jsp server with commits and recent contributors overall and smaller projects as well e.g.
jtor an alpha quality tor implementation in java with only commits and contributors overall .
details about each project including a direct link to download the application used can be found on ohloh and are archived in our accompanying technical report .
modifying each project s build scripts we ran the test suite for each project twice once with all tests executing in the same process and once with one process per test.
then we calculated the overhead of executing each test in a separate process as tn to to where tnis the absolute time to execute all of the tests in their own process and to is the absolute time to execute all of the tests in the same process.
we performed this study on our commodity server running ubuntu .
.
lts and java .
.
with a core .66ghz xeon processor and 8gb of ram.
table shows the results of this study.
for each project studied we have included the total lines of code in thetable overhead of isolating tests in new processes.
bolded applications normally isolate each test case.
additional descriptions of each subject appear in table .
project loc in k test classesoverhead apache ivy .
apache nutch .
apache river .
apache tomcat .
betterform .
bristlecone .
btrace .
closure compiler .
commons codec .
commons io .
commons validator .
freerapid downloader .
gedcom4j .
jaxx .
jetty .
jtor .
mkgmap .
open re .
trove for java .
upm .
average .30k .
project as counted by ohloh the overhead of isolating each test in its own process and an indicator as to whether that project executes each test in its own process by default.
on average the overhead of executing each test in its own process is stunningly high on average.
we investigated further the subjects bristlecone and upm the subjects with the lowest and highest overhead respectively.
we observed that bristlecone had a low number of tests total only four test classes in total with each test taking on average approximately seconds.
meanwhile in the upm subject there were test classes total and each test took on average approximately .
seconds.
in general in test suites that have very fast tests such as upm the testing time can be easily dominated by setup and teardown time to create new processes.
on the other hand for test suites with longer running tests such as bristlecone the setup and teardown time is masked by the long duration of the tests themselves.
.
approach our key insight that enables unit test virtualization is that it is often unnecessary to completely reinitialize an application in order to isolate its test cases.
as shown in figure unit test virtualization ts into a traditional unit testing process.
during each test execution unit test virtualization determines what parts of the application will need to be reset during future executions.
then during future executions the a ected memory is reset just before it is accessed.
this section describes how we determine which parts of the application need to be reset and how we reset just those components.
unit test virtualization relies on both static and dynamic analyses to detect what memory areas need to be reset after each test execution.
this approach leverages the runtime553begin test processexecute testnormal test setupnormal test teardownunit test virtualization reinitialize contaminated areasunit test virtualization log accesses that may cause contaminationsystem is clean contaminated systemfigure unit test virtualization at the high level performance bene ts of static analysis i.e.
that the analysis is precomputed with the precision of dynamic analysis.
before test execution a static analysis pass occurs placing each addressed memory region into one of two categories ms safe and mu unknown .
memory areas that are inmscan be guaranteed to never be shared between test executions and therefore do not need to be reset.
an area might be in msbecause we can determine statically that it is never accessed or that it is always reset to its starting condition at the conclusion of a test.
this static analysis can be cached at the module level only needing to be recomputed when the module code changes.
all stack memory can be placed in msbecause we assume that the test suite runner which calls each individual test does not pass a pointer to the same stack memory to more than one test we also assume that code can only access the current stack frame and no others .
we nd this reasonable as it only places a burden on developers of test suite runners not developers of actual tests which are reusable and often standardized.
memory areas that are placed in muare left to a runtime checker to identify those which are written to and not cleared.
as each test case executes memory allocations and accesses are tracked speci cally tracking each allocation that occurs in mu.
during future executions we ensure that accesses to that same location in muare treated as if the location hadn t been accessed before.
this is a general approach and indeed is left somewhat vague as the details of exactly how msis built and how muis checked at runtime will vary from language to language.
further detail for the implementation of unit test virtualization as applied to java programs is provided in the implementation section that follows.
.
implementation to evaluate the performance of unit test virtualization we created a fully functioning implementation of it for java.
we call our implementation vmvm named after its technique of building a virtual machine like container within the java virtual machine.
vmvm pronounced vroomvroom is released under an mit license and is available on github .
vmvm is compatible with any java bytecode but the runtime depends on newer language features requiring a jre version or newer.
we integrated vmvm with the test case 1test case 2object aobject bobject cobject dstatic fieldstest runnerobject graphfigure a leaked reference between two tests.
notice that the only link between both test cases is through a static eld reference.
popular test utility junit and two common build systems ant and maven to reset the test environment between automated test executions with no intervention.
vmvm requires no modi cation to the host machine or jvm running in a completely unmodi ed environment.
this detail is key in that vmvm is portable to di erent jvms running on di erent platforms.
vmvm requires no access to source code an important feature when testing applications that use third party libraries for which the source may not be available .
architecturally vmvm consists of a static bytecode instrumenter implemented with the asm instrumentation library and a dynamic runtime.
the static analyzer and instrumenter identify locations that may require reinitializing and insert code to reinitialize if necessary at runtime.
the dynamic runtime tracks what actually needs to be reset and performs this reinitialization between each junit test.
these components are shown at a high level in figure .
.
java background before describing the implementation details for vmvm we rst brie y provide some short background on memory management in java.
in a managed memory model such as in java machine instructions can not build pointers to arbitrary locations in memory.
without pointer manipulation the set of accessible memory sto a code region rin java is constrained to all regions to which rhas a pointer plus all pointers that may be contained in that region.
in an object oriented language this is referred to as an object graph each object is a node and if there is a reference from object ato object b then we say that there exists an edge fromatob.
an object can only access other objects which are children in its object graph with the exception of objects that are referred to by elds declared with the static modi er.
the static keyword indicates that rather than a eld belonging to the instances of some object of some class there is only one instance of that eld for the class and therefore can be referenced directly without already having a reference to an object of that class.
it is easy to see how to systematically avoid leaking data between two tests through non static