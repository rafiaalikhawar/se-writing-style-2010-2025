automating test automation suresh thummalapenta saurabh sinha nimit singhania and satish chandra ibm research india ibm t. j. watson research center abstract mention test case and it conjures up the image of a script or a program that exercises a system under test.
in industrial practice however test cases often start out as steps described in natural language.
these are essentially directions a human tester needs to follow to interact with an application exercising a given scenario.
since tests need to be executed repeatedly such manual tests then have to go through test automation to create scripts or programs out of them.
test automation can be expensive in programmer time.
we describe a technique to automate test automation.
the input to our technique is a sequence of steps written in natural language and the output is a sequence of procedure calls with accompanying parameters that can drive the application without human intervention.
the technique is based on looking at the natural language test steps as consisting of segments that describe actions on targets except that there can be ambiguity in identifying segments in identifying the action in a segment as well as in the specification of the target of the action.
the technique resolves this ambiguity by backtracking until it can synthesize a successful sequence of calls.
we present an evaluation of our technique on professionally created manual test cases for two open source web applications as well as a proprietary enterprise application.
our technique could automate over of the steps contained in these test cases with no human intervention indicating that the technique can reduce the cost of test automation quite effectively.
i. i ntroduction a. what is test automation?
developers create unit test cases in the form of small programs typically in a framework such as junit .
a suite of unit tests can be run as often as desired without any manual intervention.
the story for system tests which are end to end tests for an application is different.
by application here we mean a web application that resides on a server and is accessed via a web browser.
system tests are typically written by testers not the developers of the application in fact system testing is often carried out in a different organization altogether.
system testing starts with identification of test scenarios from requirements and use cases and creation of manual test cases from those scenarios.
a manual test case is a sequence of test steps written in natural language.
figure shows a sample manual test case written by a professional tester.
manual test cases are intended to be used by humans.
unlike unit tests they do not have the benefit of efficiently repeatable test execution.
therefore executable test scripts or programs need to be created from manual test cases so that the suite of system tests can be run efficiently.
launch the application through the link enter the intended book search name as mysql at the title edit field and select category as all by drop down list and then click search button select a title from the list of all search results displayed and then click either on the image of the book or on name of the book enter login guest and password guest and click login enter the quantity and click on add to shopping cart button verify whether order and total are displayed figure .
an example manual test case for an online bookstore application written by a professional tester.
test automation is the task of creating a mechanically interpretable representation of a manual test case.
such a representation could be a program in a general purpose programming or scripting language e.g.
java or vbscript .
alternatively automation could be realized by formulating a test case as a sequence of keywords or calls to executable subroutines along with the relevant arguments.
this sequence can be interpreted automatically by a driver program.
figure shows this keyword form for the test case of figure .
the second row for example is equivalent to a call locatetextbox title .enter mysql where enter applies to the text box identified by the first call.
the last two rows in the table are verification steps which ensure that the state of the browser is as expected.
yet another alternative is to record a tester s gui interaction while the tester performs a manual test into a script that can be replayed later.
these approaches entail different degrees of control over interaction with the application and require different degrees of technical skill in carrying out test automation.
writing in java is more complex albeit more general than using keyword driven automation.
record replay requires no programming skill but offers least control moreover recorded scripts tend to be brittle they can easily break in the presence of even minor application gui changes.
the cost of test automation has tremendous implications on the cost of testing and the quality of the application being developed.
the number of manual test cases for an enterprise grade application typically is in the thousands.
given this scale the cost of test automation adds up.
note that even keyword based automation takes time and expertise.
for example the target column as shown in figure requires careful specification of the location of a gui element in the document object model dom as presented in a browser.
moreover the tester has to compensate for imperfections in the manual test case.
it is also the case978 .
c ieee icse zurich switzerland 881ui element action target data type goto store default.jsp text box enter title mysql combo box select category all button click search link select mysql php from scratch text box enter login guest text box enter password guest button click login text box enter quantity button click add to shopping cart label exists order label exists total figure .
a keyword based script consisting of a sequence of triples where each triple specifies an action the target of the action and any required data.
that after application changes some of the automated tests needs to be fixed.
consequently only a fraction sometimes less than of manual tests end up being automated as projects are unwilling to pay for the cost of test automation.
typically there is insufficient time to perform manually all the manual test cases in between releases of an application.
this compromises on the quality of the application which the testing phase was supposed to ensure.
therefore it is important to try to reduce the cost of test automation.
b. automating test automation in this paper we address the problem of creating automated test scripts from manual test cases specifically in the context of keyword driven automation.
we have built a tool ata that takes as input a manual test case of the form shown in figure and outputs a script as shown in figure which is mechanically interpretable by a driver also included with the tool.
thus our tool enables execution of test cases written in natural language and creates a representation suitable for automated execution.
since natural language understanding is still challenging for computers our technique relies on basic parsing capability only.
rather we rely on the observation that the style in which testers write manual steps have a predictable structure consisting of segments where a segment is an instruction for the tester to perform an action on a user interface.
examination of over a thousand test steps taken from manual tests of real applications supports this observation.
moreover the tests have a restricted vocabulary e.g.
consisting of nouns that are pertinent for the application being tested and verbs that specify actions on user interface elements this is also borne out by our experiments see section iv c .
consider figure which illustrates the segments in the second step in the manual test case of figure .
column shows the test step with conjunction words highlighted.
column shows preliminary segments obtained by breaking up the sentence at conjunction words.
in each segment we look for a verb to specify the action a noun tagged as a direct object of the verb to specify the target of the action and another noun if present to specify the associated data.test step preliminary segments candidate tuples enter the intended book search name as mysql at the title edit field and select category as all by drop down list and then click search buttonenter the intended enter name book book search name enter name mysql as mysql at the enter name title title edit field .
.
.
select category as all select category all drop down list drop down list drop list down then click search button1 click search button click search click button search click button figure .
illustration of the segmentation of step of the sample test case.
this can be achieved by parts of speech pos tagging a basic operation in a natural language parser such as the stanford natural language parser .
verbs have been highlighted in the preliminary segments shown in column .
these are preliminary segments because there is a significant amount of ambiguity in making precise instructions out of them.
there are many sources of ambiguity.
a naive decomposition of a sentence on the basis of conjunctions may not always give valid segments e.g.
enter login guest and password guest cannot be broken into enter login guest and password guest .
within a segment there may be multiple parses possible making it difficult to identify the action and the targets with certainty.
column shows a partial list of the possibilities that must be considered corresponding to each of the four preliminary segments where each possibility is a candidate tuple of action target and the associated data.
a human would resolve these ambiguities intuitively but a machine cannot.
even if the correct segment is selected from these many possibilities further difficulties lie ahead in mechanical interpretation.
segments may be spurious in that they correspond to no operation that needs to be performed.
for example the segment drop down list in figure is in fact spurious the desired work has already been done in the previous segment.
also the target of an action on a web page may itself be ambiguous e.g.
if there are multiple buttons labeled okon the page .
our main idea is to model a sequence of ambiguous test steps by a non deterministic program in which all possible alternative meanings of each test step have been captured.
some determinisation of this non deterministic program is the correct sequence of desired test steps.
the problem then is reduced to one of finding the correct determinisation of the non deterministic program.
our approach accomplishes this by backtracking exploration while trying to interpret the test steps against the application.
the application acts as the test oracle.
we proceed by exploring an alternative until either the entire test case can be interpreted or the interpretation reaches a point beyond which it cannot proceed.
such an event happens if for example the requested action cannot be performed on the application in its current state .
in the latter case the interpretation backtracks to a previous decision point and explores a different alternative.
in this manner the approach explores different determinisations882of the non deterministic program until a successful one is found.
if our tool terminates successfully it outputs a sequence of action target data tuples that correspond to the correct determinisation.
the sequence of these tuples is the automatically executable test script for that test case.
we can look at the problem of automating test automation as an instance of program synthesis where the goal is to discover a program that realizes a given user intent.
in our case the user intent is stated in stylized natural language as a sequence of manual test steps and the goal is to synthesize a mechanically interpretable test script.
our work is also an instance of end user programming where the intent is to bridge the gap between natural language and programming language expressions and lower the barriers to programming for end users and novice programmers.
as a practical matter ata has to be prepared for incompleteness and errors in manual test steps.
when it encounters a situation that it cannot resolve automatically it stops and waits for human input.
in practice this occurs infrequently and moreover ataremembers the human feedback and does not require intervention if a similar situation arises later.
c. overview of results we implemented the approach and conducted three studies.
our first study measures the success of the approach in creating with minimal human intervention the keywordbased representation as in figure for a suite of end toend manual test cases for two open source web applications and a proprietary enterprise application.
these tests were written by professional testers who were unfamiliar with our approach and the tool that implements the approach.
overall our approach was able to automate with no human intervention of the test steps in these test cases the remaining steps required some user feedback.
in the second study we evaluated the effectiveness of three optimization techniques that we designed to make the backtracking exploration more efficient.
our results indicate that the optimizations are highly effective in reducing the amount of backtracking performed and the time required to automate tests.
in the final study we studied the success of segmentation on a large proprietary test corpus containing over a thousand test steps.
our results show that in real test corpses about half of the individual test steps contained multiple segments and across all these segments the tool was able to reconstruct them with precision and recall supporting our hypothesis that real world tests steps are written in a stylized manner.
d. contributions the key contributions of the work are the following a novel and effective approach based on backtracking and exploration of alternative flows for automating manual tests.
three optimization techniques that significantly improve the efficiency of the technique without compromising on its effectiveness.
an evaluation of our approach on two publicly available web applications and three proprietary applications on manual tests created by professional testers.
ii.
o urapproach our goal is to infer from a manual test case a sequence of automatically interpretable action target data atd tuples where each tuple is a triple a t d consisting of an action a target user interface ui element and a data value.
the key intuition underlying our technique is that the presence of ambiguities gives rise to alternative interpretations of a test step that have to be explored.
to achieve this our technique converts the manual test case into a nondeterministic program that encodes all alternative interpretations of all test steps.
this program is represented in a specialized control flow graph called a test flow graph that contains non deterministic decision nodes.
thus our technique transforms the automation problem into a synthesis problem of searching for a feasible path from the root node of the test flow graph to a leaf node along which the entire test can be interpreted .
we identify such a feasible path by exhaustively exploring the graph in a depth first traversal and backtracking to the last decision point when the analysis reaches a node beyond which it cannot proceed.
while exploring a new alternative at a decision node our technique has to restore the state of the application to the previous state at that decision node.
for web applications the state includes the web page of the application the values of the ui elements on that web page the values of the session variables and the persistent state of the backend database.
performing state checkpoint and restoration at each decision point can be expensive.
the alternative is to start each backtracking exploration from the beginning like existing web crawling techniques do e.g.
our current tool adopts this approach.
a naive approach of exhaustive exploration may be unduly expensive.
therefore our technique uses different optimizations which enable it to reduce the search space.
next we introduce some terminology.
then we present the exploration algorithm and the search optimizations.
a. terminology amanual test case is a sequence of test steps where each test step can be interpreted as a list of segments or a segment list .
a test step can have multiple segment lists.
we use conjunction words such as and to split a test step into segments.
for instance consider step in figure .
splitting on the first occurrence of andresults in the following segment list segment enter login guest segment password guest and click login883algorithm the algorithm for generating a test script from a test case.
input testcase tcase output testscript tscript ornull 1while true do invoke explorepath algorithm ifsuccessful then collect generated test script tscript return tscript choose a new path in the test flow graph get the last decision point iflast decision point has more alternatives then move to next alternative and goto step ifnot last decision point is root node then set previous decision point as last decision point goto step explored all paths in the test flow graph return null there are four possible segment lists for this test step.
next a tagged segment is a segment in which some words are annotated with pos tags.
an example tagged segment for segment is enter login noun guest where login is tagged as a noun.
segment tagging helps address the ambiguity with english words where a word can have different pos tags depending on the context of its use.
for example in the following the first step uses login as a verb whereas the second step uses it as a noun.
login as administrator enter login guest there can be multiple tagged segments for a segment because for example a verb may refer to the label of a ui element.
a disambiguated tuple is one in which the target tis associated with a ui element of the application.
due to ambiguities in specifying target or data there can be multiple disambiguated tuples for a tuple.
atest flow graph consists of different types of nodes to represent test steps segments tagged segments disambiguated tuples and decision points.
the goal of our approach is to discover a feasible path in the test flow graph by identifying the correct alternative at each decision point.
b. algorithms we illustrate our algorithms using illustrative examples shown in figure based on step of the manual test shown in figure .
algorithm invokes explorepath ep in line to explore a single path in the test flow graph from the root node.
if ep is not successful lines choose the next alternative for the last decision point.
algorithm repeats these two steps until either a feasible path is identified or all paths are explored in the test flow graph.
we next describe the exploration of a path algorithm .
compute segment lists line for each test step ep identifies all segment lists by splitting the test step based onalgorithm algorithm explorepath ep for exploring a path in the test flow graph.
input testcase tcase output success or fail 1foreach test step tstep tcase do compute segment lists for tstep if not already computed get current segment list of tstep foreach segment seg segment list do generate tagged segments for segment seg get current tagged segment tseg ofseg generate tuple list tuplist fortseg iftuplist list is empty then return fail foreach tuple tle tuplist do identify ui element list for target toftle ifelement list is not empty then get current element uelem oftle execute action aoftleonuelem ifaction ais not successful then return fail ifat least one tuple is successful then declare segment seg as successful ifno segment is successful then return fail 21return success conjunction words.
currently the implementation uses and from by then under and onas conjunction words.
if the test step includes nsuch phrases the algorithm generates 2nsegment lists representing all possible combinations of segments.
for example consider step in figure .
figures a and b show test flow graphs before and after the generation of segment lists for step respectively.
our algorithm constructs the test flow graph dynamically while interpreting the test steps.
because this test step has multiple segment lists the algorithm introduces a decision node and create a new node for each segment list.
a decision point records the current alternative being explored whose initial value is one.
figure b also shows the four segment lists sl sl sl and sl generated for step .
the currently explored path is highlighted with thick edges.
for the sake of brevity we do not show the details of sl sl and sl .
generate tagged segments line the algorithm analyzes each segment in the segment list and generates a tagged segment for each segment.
in particular it identifies words that can be either nouns or verbs and annotates those words.
for example consider seg enter login guest .
the algorithm generates two tagged segments ts as follows ts enter login verb guest ts enter login noun guest here login is tagged as a verb in the first tagged segment and as a noun in the second tagged segment.
figure c enter login guest and password guest and click lo gin seg enter login guest ts enter login verb guest ts enter login noun guest ui login barb2righttextbox ui login barb2rightbutton seg password guest seg click login sl sl sl sl ... a b root 4root seg seg seg 3sl c root seg seg 3ts ts ... sl d root seg seg 3ts ts ... sl e root seg 2ts ui ui figure .
illustrative examples for explaining the algorithm steps.
shows the test flow graph after the tagged segments for seg have been generated.
as shown the segment node is replaced with a decision node with two successors one for each tagged segment.
generate tuples lines next the algorithm uses natural language processing to generate tuples from each tagged segment.
in particular the algorithm considers the verb in a tagged segment as an action and nouns as targets or data.
section iii a provides more details on tuple generation.
if no tuples are generated which indicates that the algorithm had selected either a wrong tagged segment alternative or a wrong segment list alternative at a preceding decision point the algorithm backtracks to choose a different alternative.
for our illustrative example the algorithm generates no tuples for ts which includes two verbs and no nouns and therefore does not satisfy the criteria for valid segments.
therefore the algorithm backtracks to the next alternative.
figure d shows the test flow graph after backtracking where the current alternative of the decision point is changed from one to two.
because ts is valid the algorithm successfully generates the tuple enter login guest and proceeds forward.
a tagged segment can result in multiple tuples which the algorithm processes.
disambiguate targets lines the algorithm next generates disambiguated tuples by associating the target 1a segment is considered as valid if it includes only oneverb because each segment maps to one action performed on the application.in each tuple with ui elements in the application.
if there exist multiple ui elements for a target t the algorithm introduces a new decision point and creates a new node for each disambiguated tuple.
then it executes each disambiguated tuple by performing the action aon the identified ui element.
the action can be either one that changes the state of the application or a verification action that checks whether the application state is correct and does not update the state .
if the action fails the algorithm backtracks to explore other alternatives.
figure e shows the two disambiguated tuples ui and ui for the tuple enter login guest .
in the login page of the bookstore application login is used as the label of a text box and a button.
the algorithm next performs the action and based on the outcome of the action either backtracks or proceeds forward.
backtrack scenarios as shown in alg the technique backtracks in three different scenarios segment list backtrack btseg which occurs when none of the segments in a segment list is successful line .
note that the algorithm does not backtrack if at least one segment in a segment list executes successfully because the remaining segments may be irrelevant and could be ignored.
tagged segment backtrack bttag which occurs when an invalid tagged segment is encountered line .
disambiguation backtrack btdis which occurs when the algorithm fails to perform an action which may be a verification step on the ui element of a disambiguated tuple line .
choose next alternative finally when our technique backtracks lines of algorithm select the next alternative to explore in the test flow graph.
selecting an alternative involves identifying the last decision point and checking whether an unexplored alternative exists at that point.
to perform this the algorithm starts from the failing test step and iteratively checks for alternatives among disambiguated tuples tagged segments and segment lists.
if no unexplored alternative exists the technique moves to the preceding test step and repeats the process.
if it finds an unexplored alternative it invokes ep otherwise it terminates.
c. optimizations as mentioned earlier our approach uses optimizations to reduce the search space in the test flow graph.
specifically we use three optimizations lookahead static checking lsc local backtracking lb and active learning al .
each of these optimizations is safe in the sense that it can improve efficiency without compromising effectiveness i.e.
an optimization cannot cause the technique to not interpret a step that it could interpret without the optimization.
lookahead static checking lsc when our technique chooses a segment list alternative for a test step lsc statically analyzes the segment list and checks whether the885list includes an invalid segment.
if so lsc backtracks immediately to another alternative.
the idea behind lsc is that choosing such a segment list would anyway result in a failure subsequently when the invalid segment is encountered.
therefore lsc saves the effort that would be spent unnecessarily in exploring the segments that precede the invalid segment in the segment list.
local backtracking lb recall that during backtracking our technique starts exploration from the root node of the test flow graph to address the issue of state restoration.
however at a decision point if the application state is not modified after an alternative is chosen it is safe to continue exploration of another alternative from that point itself starting again from the root node is unnecessary.
to achieve this lb monitors whether the application state is modified after an alternative is explored.
lb conservatively considers the state as unmodified only when no action after a decision point succeeds.
active learning al the key idea of al is to learn from test cases that are automated successfully and reuse that knowledge for other test cases.
to learn from successful test cases al identifies the alternatives chosen at each decision point of the test flow graph of that test case.
in future during the exploration of another test case if al encounters a decision point with the same set of alternatives it assigns a higher priority to those alternatives that resulted in successful interpretation of test cases previously.
note that al does not add or delete any alternative rather it just assigns priorities to the alternatives thereby improving the efficiency without compromising effectiveness.
iii.
i mplementation we implemented our test automation technique in a tool called ata.
in addition to the implementation of the backtracking algorithm ata consists of three main components a component for natural language processing a component for runtime interpretation and an interface for user interaction.
a. natural language processing nlp the nlp component processes a tagged segment and generates atd tuples.
first the component annotates any unannotated word in a tagged segment using a pos tagger .
then it uses the a natural language parser to compute dependences among the annotated words.
these dependences represent grammatical relationships between the words in the tagged segments.
for example given a tagged segment enter verb login noun in which enter andlogin are annotated as a verb and a noun respectively the parser produces the dependence dobj enter login where dobj represents direct object dependence .
based on this dependence the nlp component generates atd tuple enter login .
the ata nlp component includes two alternative open source natural language parsers the stanford parser and the link grammar parser .b.
runtime interpretation ri the ri component generates disambiguated tuples from an atd tuple.
given a tuple a t d ri identifies the ui elements that are candidate matches for t. to compute the candidate matches ri extracts the textual information associated with each ui element and constructs an element map which links a ui element to a set of text labels.
ri extracts different textual information for different types of ui elements.
for example for a button ri extracts the name of the button whereas for a text box ri extracts the name of the text box and the label preceding the text box.
the reason for associating a text box with its preceding label is that manual test cases often refer to text boxes by their preceding labels.
using the element map ri identifies all ui elements that are potential matches for t. in cases where tis not precisely mentioned in a test step and therefore tdoes not match the text labels of any ui element in the element map ri uses the levenshtein edit distance to identify labels that are similar tot.
the ri component also executes the disambiguated tuples.
currently ata includes three versions of the ri component built using different testing tools two open source tools htmlunit and selenium and a commercial tool rational functional tester rft .
c. human computer interaction hci our technique can handle ambiguities in test steps but it requires the test steps to be complete .
in our context completeness means that a test step should include all necessary information for mechanically interpreting the step and that a test case should have no missing steps.
however we observed that in practice manual tests are often incomplete.
for example the test step login as guest into the application does not include all necessary information such as password or the button to click for signing into the application.
to address the completeness issue ata includes an hci component which lets a human to provide feedback for incomplete or missing steps.
ata highlights the failing step and also displays the relevant web page in the application.
the human can fix the test step insert a new step or enter tuples for the test step.
ata reuses the human provided feedback for resolving similar test steps in other test cases as well thereby reducing human intervention over time.
iv.
e mpirical evaluation to evaluate our technique we conducted three empirical studies.
in the first study we evaluated the effectiveness of ata in executing manual tests with no human intervention.
in the second study we evaluated the efficiency achieved by the three optimization techniques in reducing the time taken and the number of backtracks performed.
in the886table i effectiveness of automated interpretation of manual tests .
test steps successful steps modified steps inserted reused steps tests action verification action verification action verification steps action verification bookstore bugtracker app1 total third study we evaluated the effectiveness of segmentation.
following the description of the studies we briefly present our deployment experience with ata and then conclude this section with a discussion of the current limitations of our technique.
a. effectiveness of automated interpretation goals and method the goal of the first study is to evaluate the effectiveness of ata in automatically interpreting natural language test cases with minimal human intervention.
we considered the interpretation of a manual test to be successful only if ata could execute all steps including verification steps .
we also manually verified that the automated script indeed conformed to the behavior specified in the manual test.
for this study we used two popular open source web applications bookstore andbugtracker and an ibminternal enterprise application.
for confidentiality we refer to the enterprise application as app1 .
to create manual tests for the open source applications we identified a few scenarios that cover various independent flows in bookstore andbugtracker and that also include manipulation of persistent data.
we requested two professional testers from ibm s testing services to create manual tests for those scenarios.
the testers were unaware of our tool and technique and the goal of the study i.e.
the evaluation of a testautomation tool .
the only instructions they were given were to be elaborate in writing the tests and mention the test data wherever needed.
they were given no other instructions such as on segmentation and different ambiguities.
thus we eliminated bias to the extent possible in the creation of the tests.
in total the testers wrote 23manual tests forbookstore and 12forbugtracker which together consist of steps.
for app1 we used 15manual tests consisting of 90steps from the existing test suite of the application.
results and analysis table i presents the results of the study.
overall ata automated of the tests for bookstore of the tests for bugtracker and of the tests for app1 .
table i shows data for only the tests that could be automated.
later in this section we discuss the reasons why some tests could not be automated.
columns and show the number of action and verification steps respectively in each subject.
columns and show 2the complete dataset including the manual tests and the generated scripts is available at also available on request from the authors the number of steps that are automatically interpreted by ata with no human intervention .
columns and show the number of steps that are interpreted automatically but by reusing human feedback obtained for other steps.
together these columns illustrate the extent of automated interpretation achieved by ata.
for the action steps could be automatically interpreted by ata with no human intervention and via feedback reuse.
for the verification steps the success rate was lower of the verification steps could be automatically interpreted.
over both action and verification steps automated interpretation could be achieved for steps.
in terms of the required human intervention of the action steps and of the verification steps required human input in order to be interpreted.
at the granularity of tests all the tests required at least minimal feedback for the login step.
but of the tests were automated via reused feedback only with no human intervention.
next we present insights into the nature of the human feedback that ata required.
action steps among the failing action steps there are two main categories.
first some steps do not describe the target in the application correctly.
for instance consider the step click on the bug tracking tab .
the specified target bug tracking tab is actually an image in the application.
therefore ata could not identify any target for this step.
second some steps do not explicitly mention the data or refer to data created in previous steps.
for example consider the step click on the employee link of the employee that has been updated .
currently ata does not perform any analysis to identify such dependences therefore it cannot execute such steps.
indeed in bugtracker of the steps that required modifications column occur in one test case only.
verification steps we have found that in general verification steps are written in a more informal manner with less structure than action steps.
therefore verification steps pose a greater challenge for automated interpretation as illustrated by our empirical data of the verification steps in bugtracker required modifications.
among the failing verification steps there are two main categories.
first some steps do not describe precisely what has to be verified.
for example consider the following verification step from bookstore user should be able to cancel the updation to the user profiles .
this step does not clearly state what has to be checked.
if the step8870200 manual test time sec none a l s sl sa la sla figure .
time taken for automation using optimizations.
were to say what page should be shown after the action ata would have automated the step.
second some verification steps require logic to be coded.
for instance consider the step the bug names in the bug column should be rearranged to be displayed in sorted order .
such verification steps require the coding of custom logic which cannot be automatically synthesized.
to address such cases ata has a feature which lets users specify the name of a custom function as the action of an atd tuple the specified function is automatically invoked by ata during test interpretation.
more generally users can create a library of custom functions possibly using a testing tool such as rft and link the library to ata.
inserted steps among the missing steps the most common case is related to signing into the application.
for example many tests required the insertion of the following step enter login guest and password guest and click login .
column of table i shows the number of distinct steps that were added to the manual tests.
our feedback mechanism has indeed a valuable side benefit while performing automation testers can also improve the manual tests and keep them up to date.
forbookstore andbugtracker three tests could not be automated due to lack of support for state restoration of persistent database in our current prototype.
section iv e discusses this issue further.
for app1 two tests could not be automated one of which required looping over test steps.
b. benefits of the optimization techniques goals and method the goal of the second study was to evaluate the efficiency of the three optimization techniques in reducing the time taken for automation and the number of backtracks performed.
we use s l and a to refer to lookahead static checking local backtracking and active learning optimizations respectively.
for this study we automated the successful manual tests of bookstore using 8different configurations with all optimizations disabled configuration with each optimization enabled individually configurations with pairs of optimizations enabled configurations and with all optimizations enabled configuration .
in each configuration we ran ata for a maximum time of 20minutes for each manual test.
if ata does not complete automation in 20minutes it recordstable ii evaluation results for metrics btseg bttag andbtdis.
btsegbttagbtdis s sl sa sla table iii accuracy of segmentation .
test test test steps with as subject cases steps multiple segments asrs rs precision recall app2 app3 total the result as a failure and proceeds to the next manual test.
we used four measures in our study time taken number of btseg bttag andbtdis section ii b .
all experiments were conducted on an intel core duo cpu machine with .
ghz and gb ram.
results and analysis figures shows the time taken in seconds using the eight configurations.
the result shows that without static checking the other configurations l a and la could not finish within the allotted time which illustrates that static checking is a very important optimization.
among the other configurations local backtracking helped sla perform better than safor the initial two manual tests.
however after ata gained knowledge through active learning of the right alternatives at different decision points sla andsatook similar time.
table ii shows the backtracking data for configurations s sl sa and sla .
we do not show the results of none l a and laconfigurations because they could not complete within the allotted time.
overall the data show that active learning significantly helped reduce the number of backtracks.
in summary static checking helps improve efficiency significantly by reducing the time that would be spent unnecessarily in exploring the segments that precede an invalid segment in a segment list.
local backtracking helps when ata encounters new decision points whereas active learning helps to choose intelligently the right alternatives at decision points based on the knowledge gained from automating the previous manual tests.
c. accuracy of segmentation goals and method the goal of the final study was to perform a more extensive evaluation over a much larger corpus of manual tests the effectiveness of segmentation on which the backtracking technique is based.
this study also provides evidence that in practice test steps are often composed of multiple segments.
for this study we used two enterprise applications maintained by the ibm testing services practice we refer to the applications as app2 and app3 .
we used a suite of the manual tests written for these applications.
for lack of space we present this study briefly here the detailed results are available in reference .888to evaluate the accuracy of segmentation we measure precision andrecall of retrieving segments from the set of manual steps.
results and analysis table iii shows our results.
here as andrs represent the segments that can be potentially derived and the segments retrieved by ata respectively.
the results show that a large fraction inapp2 and in app3 of manual test steps require to be split into segments before an automation task becomes meaningful.
on average of the steps that needed segmentation are composed of just two segments but there are steps that contain as many as six segments.
for app2 both precision and recall are very high.
the cases for app2 are complex many sentences are longwinding with a wide variety of constructions of clauses.
consequently many spurious segments are identified and precision drops to even as recall remains high at .
d. benefits observed in a production environment we have also evaluated ata in a production environment to assess it usefulness in improving tester productivity in automation.
we deployed ata in an ibm internal project in a pilot evaluation conducted over an eight week period involving four testers.
prior to using ata this team had automated about manual tests over a period of months via writing java code in rft with the average automation rate of tests per day per tester.
using ata the testers automated close to tests in eight weeks with the average of 10tests per day per tester thus ata doubled the tester productivity in automating manual tests.
in addition we found that ata is quite easy to use and does not require a high level of expertise in programming or proficiency in automation tools such as rft.
e. discussion our results indicate that the backtracking based approach is quite effective in being able to interpret manual tests even in the presence of different types of ambiguities.
however there are limitations of the current technique which we discuss here.
multiple feasible executions it is possible that there could be multiple feasible paths in the test flow graph.
currently ata computes the automated script using the first feasible path that it finds.
however ata could be easily adapted to generate all feasible paths by not terminating but simply continuing further after the first feasible path is found and leaving the resolution of the correct interpretation among the candidate feasible interpretations to human judgment.
state restoration while exploring a new alternative during backtracking atarestores the state of the application to the previous state at each decision node.
currently ata supports only application level state restoration it cannot restore the persistent state of the back end database.
inour evaluation ata could not automate three manual tests one in bookstore and two bugtracker because of this limitation.
in future work we plan to address this limitation by using database level checkpoint and rollback to restore the persistent state.
correctness of the ata generated script atagenerates a test script only if all verification steps pass which gives high confidence in the correctness of the generated script.
however we recommend the user of ata to verify the final script by playing back the script to guard against the rare probability that ata chooses an incorrect feasible path that also passes all verification steps.
v. r elated work conventional approaches for test automation include record replay and keyword driven automation discussed in the introduction.
record replay which is a feature available in many commercial and open source tools e.g.
rft and selenium requires a human to perform the manual test steps on the application user interface.
keyword driven automation involves the creation of a library of reusable subroutines or keywords .
a manual test case is translated by a human into a sequence of keywords a driver program interprets such sequences by invoking appropriate subroutines.
both these techniques require human interpretation of english language tests whereas our approach attempts to eliminate this and can work in a more unattended manner.
there is a large body of work on synthesizing programs via mechanical interpretation of natural language phrases which is inspired by the ideal of bringing programming to end users and bridging the gap between human readable natural languages and mechanically interpretable programming languages .
a recent symposium on the future of software engineering research discussed the automated generation of formal software engineering artifacts such as models and test cases from natural language descriptions as one of the important research directions .
the most notable effort in the direction of end user programming with a long line of research is programming by demonstration which expounds the principle that a program is synthesized automatically by observing the manual actions performed by the user.
other efforts at bridging the gap between natural languages and programming languages include the development natural language interfaces for programming languages e.g.
providing semantics to natural language interfaces e.g.
and the development of structured editors .
automated program synthesis has been studied more broadly as the problem of discovering a program that realizes a user intent the intent could be stated in different forms such as natural language input output examples logical relations between inputs and outputs and demonstrations the users could range from algorithm designers and programmers to students and end users .
gulwani describes889three dimensions of the synthesis problem the form of user intent the search space of programs and the search technique.
our work can be formulated within this framework.
we target intent stated as natural language imperativeform commands.
the search space consists of tuples of keywords from the keyword domain of a specific testing framework and target elements from the domain of labels of user interface elements used in the application under test .
the search technique is based on natural language parsing combined with exploration of multiple flows via backtracking.
recent approaches in automated program synthesis consider user intent specified in different forms such as inputoutput mappings e.g.
logical relations between inputs and outputs e.g.
and traces e.g.
.
closer to our work techniques for automatically interpreting user intent stated in stylized natural language expressions have been developed in the contexts of test scripting synthesizing code from informal keyword expressions automating web based processes and guiding users in following how to instructions .
the cotester system uses an english like testscripting language called clearscript which can be automatically interpreted.
cotester provides a record replay feature similar to that available in testing tools with the difference that the recorded scripts are in the stylized english form of clearscript and therefore easily readable even by non programmers.
although clearscript test steps are expressed in restricted english language each step must correspond precisely to a segment.
thus the burden of segmentation is on the user.
our approach removes this restriction.
in essence programming in clearscript is at the level of keyword driven approaches albeit with a more user friendly natural language syntax the higher level of informality is more in the syntax and not in degree of precision in specifying test steps.
moreover the targetambiguity problem is not addressed in cotester.
the test scripting language of cotester clearscript was developed in earlier work in the context of automating web based processes e.g.
expense reimbursement in enterprises .
the clearscript specifications require accurately segmented and ordered process steps.
recently lau and colleagues have developed techniques for automated interpretation of how to instructions to provide guidance to end users in accomplishing webbased tasks such as making an online purchase or creating a web based email account.
they formalize the interpretation problem as the inference of a tuple a v t consisting of an action a data value and a target element the inference of such a specification is our goal too.
their approach targets instructions written by end users which can have a greater degree of informality and ambiguity than what would typically be present in the manual tests targeted by our approach.
however they assume that the instructionshave been properly segmented and therefore do not handle segmentation ambiguity.
they also do not handle target ambiguity.
little and miller present a technique for automatically translating keywords i.e.
informal plain text expressions to the api of a particular system.
the goal of their work is to lower the barriers for end users in leveraging the scripting interfaces provided by applications such as microsoft word.
the users can specify a task such as formatting a document using plain text keywords which the system attempts to translate to the application api.
in this approach an expression corresponds roughly to a segmented action.
this approach is similar in principle to api discovery techniques such as jungloid mining but requires less formal expressions that are more appropriate for end users.
in related work researchers have presented techniques for analyzing natural language descriptions of use cases to extract models measure quality detect defects and generate test cases e.g.
.
the cucumber tool lets functional descriptions or scenarios to be written in a language called gherkin a business readable domain specific language which is englishlike in flavor but with an underlying syntax.
such scenarios can be automatically executed by writing the glue code for a particular testing tool framework e.g.
selenium or junit .
in contrast our approach interprets manual tests written in unconstrained natural language with no underlying syntax and attempts to resolve different types of ambiguities.
vi.
c onclusion we presented a technique using which manual tests written in english can be almost automatically converted to a form ready for mechanical interpretation.
the technique uses backtracking based search to resolve ambiguities inherent in the tester s instructions.
the importance of this work is that it can drastically reduce the cost of test automation.
test automation is a necessary cost in testing practice because manual tests cannot be executed repeatedly in an efficient manner they have to be converted to executable scripts.
our work automates the conversion to script form.
the technique accomplishes this with remarkably high accuracy automating of 293steps in our corpus of test cases.
in future work we will further evaluate the technique in production environments add support for persistent state restoration and investigate whether our technique can be combined with text2test to enable automation from natural language use cases to executable test scripts.