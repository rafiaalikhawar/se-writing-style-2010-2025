safe asynchronous multicore memory operations matko botin can mike dodds university of cambridge fmatko.botincan mike.dodds g cl.cam.ac.ukalastair f. donaldson imperial college london afd doc.ic.ac.ukmatthew j. parkinson microsoft research cambridge mattpark microsoft.com abstract asynchronous memory operations provide a means for coping with the memory wall problem in multicore processors and are available in many platforms and languages e.g.
the cell broadband engine cuda and opencl.
reasoning about the correct usage of such operations involves complex analysis of memory accesses to check for races.
we present a method and tool for proving memory safety and race freedom of multicore programs that use asynchronous memory operations.
our approach uses separation logic with permissions and our tool automates this method targeting a c like core language.
we describe our solutions to several challenges that arose in the course of this research.
these include syntactic reasoning about permissions and arrays integration of numerical abstract domains and utilization of an smt solver.
we demonstrate the feasibility of our approach experimentally by checking absence of dma races on a set of programs drawn from the ibm cell sdk.
index terms software verification concurrent programs abstract interpretation automated theorem proving i. i ntroduction asynchronous memory operations are an important feature of modern multicore systems.
they provide a means for coping with the high cost of shared memory access the socalled memory wall problem .
using asynchronous operations cores can delegate data movement to dedicated hardware and continue processing on private memory to which they have fast contention free access.
these operations are widely available e.g.
direct memory access dma operations in the cell broadband engine asynchronous copying in opencl and one sided operations in mpi .
the high performance permitted by asynchronous memory operations comes at a price increased programming complexity.
erroneous synchronisation within a thread can lead to data races for example when copying a section of memory by an asynchronous operation and then writing to or deallocating it before the operation completes.
the deallocation of the memory can happen implicitly when a function returns or when a thread is joined.
these problems are compounded in a multithreaded setting incorrectly managed asynchronous operations can lead to inter core data races where a thread running on core iissues an operation to copy data to the local memory of core j while a thread running on core j simultaneously accesses this memory.
asynchronous operations run concurrently with other threads and have undefined behaviour over written memory until synchronised.
consequently incorrectly managed asynchronous copying can lead to highly nondeterministic bugs that are extremely difficult to diagnose through simulationand testing.
while data races in shared variable concurrent programs are often benign or intentional this is virtually never the case for races involving asynchronous operations and we regard such races as bugs.
since buggy programs may behave entirely correctly on some implementations while failing drastically on others there is an urgent need for formal verification techniques in this area.
in this paper we present a method and a tool for proving safety of multicore programs with fork join thread spawning and asynchronous memory operations.
our deductive proofbased method uses the following intuition to successfully perform an asynchronous memory operation to move data from atob a thread must have permission to read from a and permission to write to b and upon issuing such an operation the thread loses these permissions until it issues a corresponding synchronisation operation that waits until the transfer has completed at which point permissions for aand bare restored.
we present program logic rules for asynchronous memory operations that formalise these intuitions.
if it is possible to derive a proof for a multicore program using our proof system then the program is guaranteed to be memory safe and race free.
we have developed techniques for automating our verification method which involves complex flow path and contextsensitive analysis.
our approach hinges on symbolic execution over an abstract domain of separation logic assertions with permissions and array predicates.
using the rules of our proof system for entailment checking and abstraction symbolic execution is guided towards a fix point yielding a safety proof if the program is correct.
if the program is buggy the proof attempt will fail providing feedback which can be used to locate the source of the bug.
we have implemented our method for a c like language in a prototype tool asyncstar built on corestar a modular back end for separation logic analysis and verification.
standard separation logic abstraction techniques are used to automatically infer loop invariants describing heap resources.
to handle numeric constraints relevant to array manipulation we extend corestar with a novel technique allowing integration of arbitrary numerical domains with separation logic abstraction.
we present an experimental evaluation demonstrating the feasibility of the abstraction approach on a number of handcrafted examples and of the whole method using a set of benchmark programs drawn from the ibm cell broadband engine sdk .in summary the contributions of the paper are a program logic for multicore programs with threads and asynchronous memory operations.
solutions to a number of challenges arising when automating the reasoning with such a logic including syntactic reasoning about permissions and arrays integration of numerical abstract domains and utilization of an smt solver.
asyncstar a prototype implementation of our method evaluated on a set of industrial benchmarks.
asynchronous memory operations are increasingly widely used for example in languages such as cuda and opencl.
while we have focussed in this paper on the cell be architecture the techniques we propose could form the basis of a correctness analysis for any similar domain.
ii.
b ackground we now introduce asynchronous memory operations via a running example and recap the definition of a data race in a concurrent program.
we then motivate the need for techniques capable of analysing data races caused by asynchronous memory operations.
we use our running example to demonstrate the subtle nature of bugs that can arise due to this type of race and argue that data races caused by asynchronous operations are almost never benign.
asynchronous memory operations.
our target memory architecture closely resembles the ibm cell be.
we assume a single central cpu core responsible for coordinating and distributing tasks the master or host and a number of subsidiary cores that can be used to perform asynchronous computations slave oraccelerator cores .
each core has its own local memory disjoint from other cores.
the host core has regular access to the system s main memory which we call host memory while each accelerator core has its own scratchpad memory also referred to as local memory .
an accelerator core can access its local memory in a fast contention free manner using standard load and store instructions.
the local memory of one core is not directly visible to any other cores including the host.
accelerator cores cannot access host memory using standard load and store instructions.
instead the accelerator cores access host memory using special get and put operations which copy a segment of memory from and to from the core s local memory respectively.
get and put areasynchronous operations they each initiate a memory transfer but they do not wait for the transfer to complete.
in order to be sure that a transfer has finished a thread must call wait .
each get andput is associated with atag.
a wait call takes a tag as an argument and blocks until all operations associated with that tag have completed.
in the cell architecture asynchronous memory operations are implemented using direct memory access dma .
1in the case of cell be a wait operation only blocks until all operations associated with the tag andissued by the core that calls wait have completed.
this can be realised in our setting by equipping each accelerator core with a distinct set of tags.to see how this works in practice consider the doublebuffering algorithm dub bufshown in fig.
which is to be executed by an accelerator core and is spawned by a host thread master .
the algorithm reads an array of l mvalues from host memory processes the values locally and copies the result to an output array in host memory.
the algorithm reads chunks from the input array using get processes them in local memory we do not show the actual processing code assuming that it does not issue any asynchronous memory operations then copies the result to the output array using put.
the arguments to getandput are a local address x a host address y the number of bytes sof data to be copied and a tag t. two internal buffers of size lare used to store copied chunks of the shared array.
at any point during execution of thewhile loop one buffer is being filled with data via a get operation while the other buffer is being processed in place or its contents are being transferred back to shared memory via a put operation.
because local computations and asynchronous operations execute in parallel the algorithm achieves efficiency by overlapping computation with communication.
data races.
we follow the definition of a data race given in .
a program has a data race if the program can be executed on a multiprocessor such that two memory accesses are performed simultaneously and the accessed regions of memory overlap at least one the accesses is a write the accesses are not both for synchronisation purposes in general concurrent programs data races often lead to subtle bugs which occur nondeterministically depending on particular thread or process interleavings.
data races can often be benign.
for instance it is common for threads to race when updating shared variables used for logging .
distinguishing between dangerous and benign races is a difficult problem.
however we will argue that in an asynchronous setting dataraces are almost never benign.
asynchronous memory operations and data races.
in the context of asynchronous memory operations a getoperation issued by core ican race with a regular read or write by core ito the associated region of local memory agetorput issued by core i accessing an overlapping region of local memory a regular write access by the host core to the associated region of host memory aputissued by a core j possibly equal to i accessing an overlapping region of host memory the race scenarios for a put operation are analogous.
high latency asynchronous operations are typically used to transfer large chunks of data between memory spaces.
thus in contrast to data races in shared variable concurrent programs races caused by asynchronous memory operations are almost never benign.
furthermore the asynchronous nature of get andput can lead to particularly subtle bugs.
for example if define n ... num threads.
define m ... num chunks of data to be processed per thread.
define l ... size of chunk in bytes.
master char src char dst f master runs on host.
tid t int i for i i n i f t fork dub buf i src i m l dst i m l g for i i n i f join t g gdub buf int t char ihead char ohead f char buf two buffers each of size l bytes.
int cur indices recording which buffers int nxt are being used for input output.
int i char in ihead pointers to data in host memory.
char out ohead get buf in l t in l while i m f wait t nxt wait for previous put to complete.
get buf in l t nxt get data to process next iteration.
in l wait t cur wait for requested data to arrive.
hprocess datai put buf out l t cur put results back to host memory.
out l cur cur 1 nxt nxt 1 switch buffers and i increase chunk count.
g wait t cur wait t nxt hprocess datai put buf out l t cur put results back to host memory.
wait cur g fig.
.
double buffering algorithm adapted from the cell sdk .
aget operation targeting a region of the stack allocated by a function fis not properly synchronised the operation may still be pending after freturns.
this can lead to corruption of the stack during subsequent calls.
asynchronous operations may complete quickly enough that stack corruption is unlikely occurring only rarely e.g.
if bus traffic is sufficiently high.
this means that bugs can be extremely difficult to reliably reproduce.
patterns of copying can be complex even for simple examples making mistakes hard to avoid.
consider again the double buffering algorithm.
the algorithm uses binary variables curandnxtto record which buffer is incoming and which is outgoing.
at each loop iteration the variables swap values.
furthermore at the start of each loop iteration data from one local buffer is being copied back to the input buffer while data in adjacent cells in the input buffer is being copied into the other local buffer.
at the end of the loop one local buffer is the target of a copy from the input buffer while the other is the source of a copy to the output buffer.
the potential for confusion is enormous the authors of this paper found avoiding bugs rather tricky when adapting the algorithm for presentation here .
to avoid undefined behaviour caused by races with asynchronous memory operations we must be able to ensure that the source of an asynchronous operation is not written to and the target neither written to nor read from until the operation is explicitly synchronised via a wait call.
for instance if there was no call to wait at the first line in the while loop of dub bufthen the example would exhibit a race which we can observe by logging first five asynchronous operations get buf in l t cur get buf in l t nxt i nxt wait t cur i put buf out l t cur i cur get buf in l t nxt i cur the operation put at is in race with the operation getat because both of them are accessing the same portion of the internal buffer.
existing techniques for dynamic race detection could be readily adapted to apply in the context of asynchronous memory operations and runtime monitoring and system simulation have been used to find races in cell be programs.
however such techniques can only detect data races.
in the remainder of the paper we describe a method which can be used to prove absence of data races due to asynchronous memory operations in concurrent programs.
iii.
r easoning about asynchronous operations we now use the double buffering example of fig.
to provide an overview of our technique for reasoning about race freedom of asynchronous memory operations in concurrent programs.
for reasons of space we do not present full formal details of our approach and justify its soundness in an informal manner.
by careful analysis of the dub buffunction of fig.
we can determine precisely the pattern of copying between thewhile i m f wait t nxt buf buf i lllin i.l in lin m i lm.lout i l cur get buf in l t nxt in l buf buf i lllin i l in 2lin lin m i lm.lout i l curnxt wait t cur buf buf i.lllin i l in lin m i lm.lout i l nxt put buf out l t cur out l cur cur 1 nxt nxt 1 i buf buf i lllin i.l in lin m i l i lout i l curoutout lnxt m i l g fig.
.
sequence of memory reads and writes in the main loop of the double buffering algorithm.
input and output arrays and the two buffers local to each thread.
this pattern of behaviour is illustrated in fig.
.
in this diagram boxes denote contiguous chunks of memory.
labelled arrows inside boxes are used to indicate the size of each chunk while external labels indicate addresses.
greyfilled boxes denote chunks that are subject to pending memorytransfer operations while arrows between grey boxes indicatethe direction of copying.
labels on these arrows denote the tag of the operation.
reasoning using separation logic.
diagrams such as fig.
would be laborious and difficult to annotate correctly by hand.
however precisely capturing information about shifting patterns of reading and writing is essential in establishing correctness of an algorithm involving asynchronous memory operations.
we use a program logic based on separation logic to capture precisely the information presented intuitively in fig.
.
separation logic is a hoare style logic for verifying programs.
reasoning in it depends on a new logical connective the separating conjunction .
formula p1 p2asserts that memory can be split into two disjoint parts one satisfying p1and the other p2.
reasoning in separation logic is local meaning that a specification must express all the resources a program needs to execute without faulting.
for example the specification fpgcfqg says that if the program cexecutes to termination starting with a resource satisfying p the result will be a resource satisfying q and that all resources used by care either specified by por acquired through explicit resource transfer.
locality means that a program can be verified within a small resource and then substituted into a larger context.
this property is expressed by the frame andparallel rules2 framefpgcfqg fp fgcfq fg parallelfp1gc1fq1g f p2gc2fq2g fp1 p2gc1kc2fq1 q2g the frame rule allows any specification fpgcfqgto be extended by an arbitrary frame fthat is unchanged by the c. the parallel rule allows the specifications for two threads to be combined provided they access disjoint resources.
reasoning in separation logic is performed in terms of a single thread at once the local thread .
reasoning is therefore said to be thread local .
we can think of the pre and postcondition as denoting the portion of the state of which the local thread has ownership .
other running threads may own other portions of the state but the parallel rule ensures that the global pattern of ownership is consistent.
note that ownership does not preclude sharing two threads can share a resource as long as both only read from it.
separation logic is an appropriate approach to verifying asynchronous memory copying operations because it deals gracefully with ownership and with resource transfer between threads.
separation logic also has a proven history in verifying complex algorithms with dynamic patterns of ownership and good tool support for symbolic execution.
representing arrays and pending operations.
to reason about examples such as the double buffering algorithm we 2below we also give rules for thread manipulation using fork andjoin.must be able to precisely capture patterns of resource ownership and transfer.
to do this we extend separation logic with two new assertions arrandpend .
arrc x s p vs denotes an array of sbytes starting at address x. parameter pis a permission giving the level of access the local thread holds over the array.
the final parameter records that the array holds values vs. subscript crecords where the array is stored either hfor host memory or for local scratchpad memory.
pend t o denotes a setoof pending asynchronous memory operations with associated tag t. each element ofois a tuple recording a particular operation.
as well as recording the existence of a set of pending operations a thread owning this predicate can access the associated resources after the operations complete.
using these predicates and the operator we can represent complex states in resource transferring algorithms.
permissions.
permissions are used in separation logic to support race free sharing between threads .
they can be represented by fractions3in the interval .
a permission denotes that the thread has exclusive write access to the array while a permission p2 records that it has non exclusive read access.
soundness is ensured by the guarantee that the sum of all permissions is never more than .
the permission can be split into read permissions which may themselves be split further and split permissions may be joined back together.
permissions in array predicates are split and joined according to the following rule p q arrc x s q vs arrc x s p vs arrc x s q p vs in addition to splitting and joining of permissions we can also split and join array predicates with respect to length x y x s arrc x s p vs 9vs1 vs2 vs vs1 vs2 arrc x y x p vs arrc y x s y p vs a we use to denote concatenation of array values.
reasoning about get put and wait .we reason about get andput by giving them specifications based on the arrand pend predicates.
the specification for getis as follows.
arr x s xs arrh y s p ys pend t o get x y s t pend t fhyh x s p ysig o before calling get the thread must have read access to the host array and write access to the local array.
after getcompletes the thread loses the permissions it held for both arrays it cannot safely write to x as it may be in an inconsistent state.
itcancontinue to safely read from y as long as it holds an additional read permission.
3in order to avoid reasoning about fractional arithmetic in our tool we represent permissions by trees.
see xiv b for details.
arr x s xs arr z s zs arrh y s ys pend t get x y s t arr z s zs arrh y s ys pend t fhyh x s ysig get z y s t pend t fhyh x s ysi hyh z s ysig fig.
.
an example proof outline illustrating splitting of permissions and framing.
intuitively the arrays are held by the memory controller until the operation completes.
the pend predicate is updated with a tuple recording the new operation the source address the target address length the permission on the source and the value of the source the subscripts andhindicate in which memory spaces the arrays are stored .
the specification for put is analogous to that for get arr x s p xs arrh y s ys pend t o put x y s t pend t fhx yh s p xsig o in fig.
we illustrate how splitting of permissions and framing works when used with specifications of getandput.
suppose that we want to transfer sbytes from the host array yto local arrays xandz.
to do this we need a write permission for arrays xandzand a read permission for array y which can be any fraction greater than 0and less than or equal to .
assume that we start with the permission1 2given to the array y. to issue the first get transferring from ytoxwe can split the fraction1 2into quarters and then frame with arr z s zs andarrh y s ys in order to apply the specification.
to apply the specification for the second getwe frame with the predicate pend t fhyh x s ysig .
the wait function ensures that all memory operations associated with a particular tag thave completed.
all the arrays that were sources or targets of these pending operations can be safely accessed once the memory operations have completed.
consequently the specification of wait returns the arrays held by the memory controller to the thread.
fpend t o g wait t pend t hxc yc0 s p vsi2o arrc x s p vs arrc0 y s vs in the post condition of wait the set of pending operations for the tag tis empty and for each pending operation recorded in the precondition a pair of arrays has been returned to the thread.
note that for each pair of host and scratchpad arrays the values in the arrays are identical as the copying operation has completed.
we use the iterated separating conjunction 4to say that the thread holds a pair of arrays for each pending operation represented in the precondition.
4that is i2fx y z g p p p p verifying the double buffering algorithm.
the dub buf function can be specified as follows using predicates from our assertion language arrh ihead m l p is arrh ohead m l pend tag pend tag dub buf tag ihead ohead arrh ihead m l p is arrh ohead m l pend tag pend tag here and elsewhere we use underscore to denote a fresh existentially quantified variable that is used only once.
the pre and post conditions both assert that a pair of arrays exist at addresses ihead andohead each of length m l. the sets of pending operations on tags tagandtag 1are empty.
theohead array must have a permission argument meaning the algorithm must be able to write to this array.
however the ihead array can have an arbitrary permission p2 as its argument meaning that the thread only requires the ability to read.
the separating conjunction is essential in assigning the right meaning to this specification.
conjoining the array predicates with ensures that they occupy disjoint portions of memory meaning that the ohead array can be modified without affecting the ihead array .
the assertions arrandpend can now be used to represent invariants of the double buffering algorithm.
fig.
shows the invariants for the main loop of the double buffering algorithm.
these invariants correspond directly to the intuitive reading of the algorithm given by the diagrams in fig.
.
furthermore these invariants can be generated automatically by our tool.
the following assertion the first invariant in fig.
corresponds to the first diagram in fig.
buf !bc buf !bn1 arr bn l arrh in i l i l p is arrh in m i l p is pend t cur fhinh l bc l p is pig pend t nxt arrh out i l m l is1 isp is2 is line asserts that buf andbuf point to addresses bcandbn.
variables curandnxtare used to choose between elements of an array of two buffers where each buffer has sizel.
line denotes the unused buffer array at bn.
lines and denote the inandoutarrays.
the inarray has a chunk of size lmissing this is held by the asynchronous copying operation and will be returned when the operation completes.
line asserts that the t curtag is being used to copy from the host inarray to the local array at bc but that the t nxttag is unused it is associated with an empty set of operations .
line asserts that the concatenation of the two input arrays and the values in the pending copy results in the original contents of the input array.while i m f wait t nxt buf !bc buf !bn arr bn l arrh in i l i l p is arrh in m i l p is pend t cur fhinh l bc l p is pig pend t nxt arrh out i l m l is1 isp is2 is9 get buf in l t nxt in l buf !bc buf !bn arrh in i l i l p is arrh in m i l p is pend t cur fhinh 2l bc l p is pig pend t nxt fhinh l bn l p is0 pig arrh out i l m l is1 isp is0 p is2 is9 wait t cur buf !bc buf !bn arr bc l arrh in i l i l p is arrh in m i l p is pend t cur pend t nxt fhinh l bn l p is pig arrh out i l m l is1 isp is2 is9 put buf out l t cur out l cur cur 1 nxt nxt 1 i buf !bc buf !bn arrh in i l i l p is arrh in m i l p is pend t nxt fhbn outh l l ig pend t cur fhinh l bc l p is pig arrh out i l i l os arrh out m i l os is1 isp is2 is9 g fig.
.
invariants in the main loop of the double buffering algorithm.
reasoning about fork and join.
verifying master requires rules for fork andjoin.
f x fpgfqg2 fp gt fork f e fthr t f e g f x fpgfqg2 fthr t f e gv join t fq g the rules for fork andjoin essentially extend the p arallel rule to handle threads dynamically.
is an environment associating functions with their specifications.
upon forking a new thread the parent thread obtains the assertion thr that stores information about passed arguments for program variables and gives up ownership of the precondition of thefunction.
joining requires that the executing thread owns the thread handle which it then exchanges for the function s postcondition.
soundness.
we have defined a formal semantics for multicore programs with asynchronous memory operations and used this to establish the soundness of our method.
in particular if it is possible to derive a proof for a multicore program using our system then the program is guaranteed to be free of data races and memory faults.
for reasons of space we defer a full formal presentation of this result to future work.
iv.
a utomation we have built a prototype tool asyncstar automating our approach.
asyncstar can check proofs written in our logic and if supplied with pre and post conditions it can synthesise proofs automatically for many examples including the doublebuffering algorithm of fig.
.
approach.
theasyncstar tool uses symbolic execution combined with shape analysis for separation logic .
states in a program are represented symbolically by disjunctions of separation logic assertions.
statements are then executed symbolically over these assertions that is assertions are updated to reflect the abstract effect of the statement.
the analysis executes until it reaches a fix point where no new symbolic states can be reached.
symbolic execution alone does not converge in many cases.
to ensure termination symbolic states are abstracted at the heads of loops.
in asyncstar abstraction of separation logic assertions is based on syntactic summarisation of predicates.
for example a linked list of five nodes might be abstracted by a predicate representing a list of unknown length.
this approach however is not flexible enough to abstract away numeric constraints arising from loop iterations and array manipulation.
to deal with such properties we have developed a novel technique that allows integration of standard abstract interpretation tools.
a. tool architecture asyncstar is built on top of corestar a languageindependent verification tool for separation logic consisting of a symbolic execution engine and a separation logic theorem prover.
as input asyncstar takes a program written invmc verified multicore c a fragment of c enriched with user supplied pre and post conditions for functions and optionally loop invariants.
the input program is translated into the corestar intermediate language corestaril .asyncstar invokes corestar s core engine which symbolically executes the generated corestaril program indicates whether verification succeeded and returns any inferred invariants.
the overall architecture of asyncstar is illustrated in fig.
.
corestar has a language agnostic internal representation.
support for new languages and abstract domains can be added by providing logic rules and abstraction rules through text input files.
unlike most tools for shape analysis corestar does not just deal with heap data.
rather its analysis allows automatic reasoning about abstract objects such as threads pending abstraction theorem prover symbolic execution corestar logic rules abstraction rules vmc frontend vmc program pre post conditions loop invariants corestaril program smt solver apron fig.
.
architecture of the asyncstar tool memory operations and so on.
by building on corestar we were able to develop and test asyncstar rapidly avoiding redevelopment of existing features.
to implement our approach using corestar we developed a front end which accommodates vmc programs and our logic for reasoning about asynchronous memory operations and extended corestar s abstraction engine.
developing the frontend involved three major efforts translating vmc programs into corestar s intermediate representation time consuming but straightforward developing a syntactic encoding of permissions seexiv b and writing logic rules for reasoning about arrays with permissions and pending asynchronous memory operations technically involved and thus omitted in this paper .
extending corestar s core engine required two major pieces of work extending corestar s symbolic execution with abstraction for first order domains such as arithmetic see xiv c and adding support for external smt solvers see xiv d .
b. reasoning syntactically about permissions permissions are fundamental to our approach allowing control over reading and writing for memory shared between threads and pending operations.
in xiii we represented permissions by fractions from the interval and permission joining by addition.
however arithmetic reasoning is expensive in corestar requiring calls to an smt solver see xiv d .
hence in our implementation we actually use the binary tree share model .
permission splitting and joining can be reasoned about syntactically using corestar s rewrite engine without recourse to arithmetic.
in permissions are represented by binary trees with boolean valued leaves and unlabelled internal nodes.
maximum permission in the numerical model corresponding to write permission is represented by a single true valued leaf.
splitting maximum permission results in a pair of two leaf trees representing read permissions.
one tree has a true left and false right leaf while the other is the other way round and further splits result in larger trees.
minimum permission is represented by a single false valued leaf.
we have encoded these binary trees using corestar s expression language.
for example we might have the following expression representing a tree with three leaves branch leaft branch leaft leaff note that branch leaft andleaff have no semantics in corestar they are treated syntactically as uninterpreted functions.
c. abstraction in pure domains inasyncstar we extend corestar s syntactic abstraction with support for abstraction over numerical properties.
our approach uses external tools to implement standard abstract interpretation in which consecutive abstract assertions are joined and if necessary widened .
approach to abstraction in corestar .incorestar the symbolic execution is performed on the control flow graph of the input program.
each node in a program s control flow graph is associated with a set of separation logic assertions.
each set represents an abstract state consisting of the disjunction of its elements.
by symbolically executing an iteration of a loop starting from the current abstract state corestar constructs a new set of candidate assertions.
corestar checks whether each candidate is contained within any of the existing assertions and if not adds the candidate assertion to the set.
containment is defined via separation logic entailment an assertion cis contained within an assertion aif c a. abstraction is applied to candidate assertions before checking for containment.
prior to asyncstar the abstraction in corestar was entirely based on syntactic rewriting of predicates.
for example suppose we had a predicate node x y representing a node at address xwith next pointer y and a predicate lseg x y representing a linked list starting at yand ending with a pointer to y. we might write the following rules for abstraction node x x0 node x0 nil lseg x nil lseg x x0 node x0 nil lseg x nil lseg x x0 lseg x0 nil lseg x nil by applying these rules the infinite family of assertions representing lists of finite length will be collapsed into a single assertion.
this approach works well for reasoning about heap resources but often fails for concrete values in the first order orpure part of the assertion.
for example consider the following code fragment which sets all the elements of an array aof size nto0.
int i while i n f a i i g 5such queries are decided by the separation logic theorem prover.after the k th iteration of the loop symbolic execution will generate the candidate assertion kdef 9vs i k arr a n vs the prior abstract state will consist of the set of assertions f k 1g.
the analysis will not terminate because there exists no j k such that k j. achieving convergence requires abstraction of numerical properties.
pure abstraction.
inasyncstar we handle such numerical abstraction by using abstract interpretation.
as in abstract fix point calculation the pure parts of the existing and the candidate assertion are joined and optionally employed to widen the existing pure assertion.
the obtained abstraction is then used to replace the original pure parts of the assertions.
in our example joining the pure parts of jand k forj k in the polygonal domain yields j i k. after widening and intersecting with the upper bound we obtain j i n. after replacing the original pure parts of jand kthe following entailment holds and thus the analysis converges k i n arr a n vs j i n arr a n vs our approach applies to abstracting the pure part of separation logic assertions in anyabstract domain.
the join and widening operators are assumed to be provided by an external tool for abstract interpretation.
asyncstar has a modular interface allowing the calling of such tools during the fixed point calculation.
in our experiments we used the apron tool .
d. smt utilisation corestar s theorem prover was not designed to reason about pure domains such as arithmetic.
it reasons purely syntactically knowing nothing about the semantics of interpreted symbols such as and thus cannot establish even simple facts like a b b a .
for arithmetic reasoning and to reason about arrays we call an external smt solver following the approach proposed in .
calling the smt solver is expensive as we transfer the entire proof state to the solver using the smt lib2 format .
we only call the solver in three scenarios when only pure assertions remain to be proved in which case they are sent to the solver when proof search is stuck in which case the solver is invoked to establish new equalities between terms when we wish to check whether proof rules with pure guards apply.
queries are memoised to further reduce the expense of solver calls.
v. e xperiments we evaluated asyncstar using a set of benchmarks drawn from the cell broadband engine sdk which represent prototypical patterns used in cell programs.
we also considered a selection of hand crafted programs.
the cell benchmarks comprise euler integration based particle simulation particlesim array processing algorithms using single double and triple buffering with shared buffers for input and output as in the example ofxii buffer and with separate buffers buffer io .
all benchmarks consist of a master threadrunning on the host that divides array processing among slave threads running on accelerator cores.
the hand crafted benchmarks do notexhibit asynchronous memory operations and are instead designed to assess the effectiveness of our abstraction framework in isolation.
benchmarks with prefix array perform an operation on elements of an input array.
we consider left to right countup and rightto left countdown processing of the array and where elements to be processed are contiguous suffix or separated bynbytes suffix n .
benchmarks with prefix control flow exhibit the same control flow as the buffering benchmarks but do not contain transfers or processing.
figure shows experimental results obtained on a personal laptop with .8ghz intel core2 duo cpu and 4gb ram under the windows operating system.
we used the smt solver z3 and the polygonal abstract domain from the apron numerical abstract library.
for each benchmark we give the number of symbolic states the total execution time in seconds and the percentage of execution time spent on computations by the abstract domain library ai and the smt solver smt .
for cell benchmarks we also apply asyncstar to buggy versions obtained by removing a wait operation.
asyncstar fails to find a proof for these examples and prints details of the failed proof attempt from which the bug can be recovered.
for all benchmarks the user is only required to specify function pre and post conditions loop invariants and intermediate assertions are synthesised automatically.
if the analysis succeeds it proves memory safety and race freedom.
in the failing case the user can examine the proof state and look for information exposing the source of the problem.
the results demonstrate that our technique can prove or refute our benchmark set within reasonable time bounds.
we observe that spatial reasoning is not a bottleneck per se nor is numerical abstraction but the arithmetical constraints arising from proof rules guards and arithmetic in the code discharged to the smt solver are.
we anticipate that in future the expense of smt utilisation can be significantly reduced by optimisations in our implementation such as better caching of queries.
vi.
r elated work in prior work we presented a preliminary outline of our verification technique .
asyncstar is built on corestar which inherits its approach from jstar and a series of prior tools syntactic reasoning in separation logic was pioneered in smallfoot corestar s generation of loop invariants uses on shape analysis based on separation logic .
our approach to pure abstraction is similar to that of .
these prior tools achieved performance by hard coding their domain into the tool.
in contrast corestar s core is designed as a modular backend intended for use in experiments with verification.
this approach is validated by our success in quickly building a prototype on top of corestar .benchmarkcorrect symbolic total ai smtstates time particle sim buffer buffer io buffer buffer io buffer buffer io benchmarkbuggy symbolic total ai smtstates time particle sim buffer buffer io buffer buffer io buffer buffer io benchmarkcorrect symbolic total ai smtstates time array countup .
array countup n .
array countdown .
array countdown n .
control flow sb .
control flow db .
control flow tb .
fig.
.
experimental results obtained using asyncstar to analyse correct and buggy benchmarks an approach to checking dma races for cell be using bounded model checking bmc and k induction has been proposed in and implemented as the s cratch tool .
the technique has been extended with abstract interpretation to automatically infer loop invariants .
unlike our method this technique can only be applied to sequential software safety of dma races is checked with respect to one thread.
extending bmc to concurrent programs would not scale well as it would involve thread interleaving at a global scope.
in contrast our technique verifies threads in a modular fashion and can be used to prove safety in the presence of dynamic thread creation.
general purpose techniques for race detection e.g.
could be adapted to handle asynchronous memory operations and runtime monitoring and system simulation have been applied directly in this setting.
however none of these techniques can prove absence of races which is the contribution of our work.
in session types are used for synthesis of low level data movement code allowing the generation asynchronous memory operations that are race free by construction.
we view this approach as complimentary to our approach.vii.
c onclusions and future work we have presented a novel method and tool for verifying the memory safety of multicore programs that use asynchronous memory operations.
our approach allows full verification of industrial benchmarks from the ibm cell sdk which is beyond the reach of current techniques based on model checking.
we plan to extend asyncstar to take into account alignment restrictions associated with data transfers.
in the cell be architecture dma operations must operate on byte aligned pointers otherwise behaviour is undefined.
misalignment can lead to subtle errors which are an ideal candidate for verification.
we also plan to evaluate our technique on benchmarks from other domains where asynchronous memory operations are important.
we are exploring the use of abduction to reduce the need to annotate functions with specifications.
our analysis uses frame inference to divide resources into operation preconditions and frames unaffected by the operation.
abduction generates antiframes consisting of the minimal extra resource needed for the operation to execute without error.
antiframes can be pushed to the start of a function automatically generating function preconditions.
our preliminary experiments using this technique can discover a full specification for the single buffer benchmark.