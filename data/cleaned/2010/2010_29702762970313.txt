automated model based android gui testing using multi level gui comparison criteria y oung min baek doo hwan bae korea advanced institute of science and technology kaist daejeon republic of korea ymbaek bae se.kaist.ac.kr abstract automated graphical user interface gui testing is one of the most widely used techniques to detect faults in mobile applications apps and to test functionality and usability.
gui testing exercises behaviors of an application under test aut by executing events on guis and checking whether the app behaves correctly.
in particular because android leads in market share of mobile os platforms a lot of research on automated android gui testing techniques has been performed.
among various techniques we focus on model based android gui testing that utilizes a gui model for systematic test generation and e ective debugging support.
since test inputs are generated based on the underlying model accurate gui modeling of an aut is the most crucial factor in order to generate e ective test inputs.
however most modern android apps contain a number of dynamically constructed guis that make accurate behavior modeling more challenging.
to address this problem we propose a set of multi level gui comparison criteria guicc that provides the selection of multiple abstraction levels for gui model generation.
by using multilevel guicc we conducted empirical experiments to identify the in uence of guicc on testing e ectiveness.
results show that our approach which performs model based testing with multi level guicc achieved higher e ectiveness than activity based gui model generation.
we also found that multi level guicc can alleviate the inherent state explosion problems of existing a single level guicc for behavior modeling of real world android apps by exibly manipulating guicc .
ccs concepts software and its engineering !software testing and debugging empirical software validation permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
ase september singapore singapore c acm.
isbn .
.
.
.
gui testing android application testing gui model generation gui comparison criteria model based test input generation .
introduction one of the most widely used methodologies to test mobile applications apps is graphical user interface testing gui testing .
gui testing is usually conducted as a validation technique for gui driven software such as mobile apps and it mainly performs functional tests for an application under test aut .
by running test inputs e.g.
click button a on guis of an aut gui testing examines behaviors of the app and checks whether it behaves correctly .
since manual gui testing is costly time consuming and not sophisticated at nding real faults automated gui testing techniques and tools for mobile apps have been actively researched to perform cost e ective testing .
in particular as the android platform has dominated the market share among mobile os platforms automated gui testing for android apps has been getting the spotlight from the researchers and practitioners .
as a means of gui test automation for android apps current studies that deal with automated test input generation for android apps can be classi ed into several approaches .
the simplest methodology is random test input generation which generates random events as test inputs to explore behaviors of auts and checks runtime errors .
android monkey is the best known random tool for robustness testing and dynodroid is an advanced random testing tool that guides random testing to generate only relevant events.
although these random testing techniques or tools could explore the behavior space of an aut in simple ways not only do they generate redundant tests due to the nature of randomness but tracing suspicious paths is also more di cult than systematic approaches .
as a solution model based gui testing techniques have been researched to facilitate the systematic exploration of a behavior space and support e ective debugging .
the model based technique builds a gui model that represents a behavior space of an aut and generates test inputs based on the model .
compared to the random techniques model based approaches can generate a nite set of e ective test inputs by analyzing the exploration context based on gui models.
furthermore they have advantages in systematic debugging by enabling their models to provide concrete execution paths related to the detected errors.
aided by these advantages the model based testing techniques bepermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
ase september singapore singapore c acm.
... .
came major approaches for automated android gui testing and there has been a great deal of research to improve the e ectiveness of their techniques .
for e ective model based gui testing re ecting a behavior space of an aut on a gui model is the most crucial ability to generate e ective test inputs .
however as many real world android apps contain a lot of dynamic or unpredictable guis to provide convenient user experience existing model based gui testing techniques for android apps have encountered di culties in accurate modeling.
more speci cally a gui model that contains only a small range of possible behavior space can have poor the testing e ectiveness and dynamic behaviors in guis can cause inconsistent model generation or state explosion problems due to the non deterministically changing guis.
in order to tackle the behavior modeling problems of realworld apps model based techniques have to specify the level of behavior abstraction by clearly de ning a gui comparison criterion .gui comparison criteria guicc are the information related to the gui to distinguish gui states e.g.
name of activity number of enabled widgets whether two guis have an equivalent state or di erent gui states.
for behavior modeling and abstraction previous studies have de ned their own criteria to distinguish gui states to generate models an activity name a set of event handlers and observable graphical changes .
however they overlooked the volatile behaviors of real world android apps and they have not been delved deeply into considering the in uence of the guicc on the testing e ectiveness.
as simple examples of guicc figure illustrates two types of comparison criteria for gui model generation.
figure a shows a case where activity names are used as a comparison criterion which was used in .
the activity based gui model can be easily made with tools however an activity level gui model is too abstract to model dynamically constructed guis in recent android apps.
as illustrated in figure a activity based comparison could miss a lot of information needed to adequately explore the behavior space because the three screens are not considered as distinct states.
on the other hand if the gui comparison criterion is too sensitive a testing tool could not complete gui model generation due to the endlessly increasing behavior space.
figure b shows a case where a testing tool distinguishes gui states with every observable change in a calculator app.
since every single change in the displayed calculated value makes new gui states the gui model generation may have to store redundant gui states unnecessarily.
therefore model based gui testing techniques should carefully consider the guicc for model generation of realworld android apps that contain complicated behaviors.
in this paper we focus on the gui model generation technique especially on the gui comparison criteria for model generation.
this study de nes a multi level gui comparison criteria multi level guicc by suggesting a set of abstraction levels for automated modelbased android gui testing.
we also develop an automated model based gui testing framework to actually perform automated gui testing for real world android apps and we evaluate the in uence of guicc on the testing results.
in an evaluation on open source and commercial android apps we analyze the experimental results in terms of the modeling of behaviors the code coverage and the error detection.
figure behavior modeling of android guis based on gui comparison criteria this paper is organized as follows.
chapter provides background information of model based gui testing for android apps.
chapter introduces our main approach multilevel guicc and chapter presents an automated modelbased gui testing framework that contains multi level guicc .
chapter provides our research questions and experimental setup and chapter discusses the results of the experiments.
chapter provides related work and chapter concludes this paper with future research directions.
.
background .
gui model in model based gui testing a key element that enables systematic test input generation is the underlying gui model.
the gui models are usually in the form of finite state machines fsms and a gui model represents a behavior space of the target aut with gui states and event driven transitions between gui states .
in this study we call a gui model a gui graph and it is de ned as follows.
de nition.
agui graph gis a directed graph that represents distinct gui states distinguished by a speci c gui comparison criterion guicc asscreennodes and represents transitions between gui states which are triggered by events as eventedges .
simply a gui graph gis de ned as g s e where sis a set of screennodes s fs1 s2 ... sng and eis a set of eventedges e fe1 e2 ... eng .
ascreennode represents a gui state that contains abstracted gui information of an execution screen of an aut and screennodes are distinct from each other based on a guicc .
agui comparison criterion guicc represents a speci c type of gui information to determine equivalence di erence between gui states.
aneventedge represents a single event that can be triggered both by a user e.g.
click and by a system e.g.
sms phone calls and each eventedge links between screennodes .
239in remaining chapters we construct gui graphs with screennodes and eventedges to model event driven behaviors of an aut and generate test inputs.
therefore the gui graph should represent as much behavior space i.e.
gui states and event interactions as possible in order to generate adequate test inputs to explore the behavior space.
.
gui comparison criteria guicc as mentioned in section .
a guicc designates speci c gui information to distinguish the equivalence or difference between multiple gui states.
when a model based testing tool generates or updates a gui graph while exploring the behavior space guicc distinguishes a gui of the current screen on android device after a test input is executed.
by comparing guis with guicc the testing tool determines how to update the gui graph.
if a testing tool uses a strong sensitive guicc e.g.
observable graphical change the abstraction level of generated gui models decreases because more gui states are regarded as distinct ones and vice versa.
in other words a guicc determines the granularity of screennodes in a gui graph which can a ect test input generation.
for this reason the guicc could signi cantly a ect the behavior modeling.
more speci cally a model based testing tool repeatedly executes test inputs on a running aut and analyzes the execution results to generate and update the gui model.
after an event eis executed on a certain screen sc the gui graph should be updated to re ect the currently exercised behavior i.e.
a behavior triggered by e from the observed results i.e.
a state after event execution .
the update process is illustrated in figure and it displays two cases the gui state changes to an existing state or stays the same the gui state changes to a new state that has a di erent gui from other screennodes in the gui graph.
in case if a gui state after event execution s is judged to be the same gui state as a certain visited screen siof the gui graph i.e.
s si then only a new eventedge is added into the graph and it links two screennodes scandsi.
otherwise as in case if the event execution causes a change to a new gui state then a new screennode s5 is added into the gui graph and a new eventedge is also added into the graph to link scands5.
during the update process the guicc determines whether a screen after event execution s has a new gui state which di ers from other gui states of visited screennodes .
for the comparison of gui information between screennodes a model generation module inserts speci c gui information into each screennode based on the chosen guicc when a new node is added into the graph.
for example if a testing tool utilizes an activity name as a guicc then the activity information such as com.example.mainactivity is included inscreennodes .
.
methodology .
multi level gui comparison criteria in the previous sections we emphasized the role and the importance of guicc in model based gui testing and we mentioned some previous studies that did not de ne the criteria clearly or used improper ones.
to come up with a clear de nition of guicc for the modeling of real world android apps we propose a set of multi level gui comparison crifigure gui graph update according to the comparison result teria multi level guicc for android apps based on the empirical investigation of real world apps.
.
.
investigation of comparison criteria the multi level gui comparison technique was designed based on a semi automated investigation with real world android commercial apps registered in google play .
the investigation was conducted as follows.
app selection.
in order to obtain the characteristics of android guis in commercial android apps we selected real world apps that were registered in google play .
first we collected the most popular apps in categories total apps and then we excluded the apps in the game category because most of them were not implemented as native apps.
we selected out of categories and we excluded apps that were downloaded fewer than times.
finally we chose target apps apps in the book category in business in communication in medical in music in shopping in social in transportation and in weather .
manual exploration.
after selecting the apps we investigated composed widgets of the apps by manual exploration.
for the exploration we visited main screens of the target apps in an end user s view.
not just the functionality was exercised the constituent widgets in guis were examined to extract gui related information.
in order to obtain the information we used the uiautomator tool in android devices and we generated uidump.xml les to analyze the guis of the screens.
by automatically parsing theuidump.xml les we analyzed the structure of widgets and their properties and values.
in addition the system information was extracted through dumpsys system diagnostics classi cation of gui information.
after extracting and collecting the gui information from the selected apps we classi ed the gui information.
using the extracted gui information we could nd hierarchical relationships among several types of gui information.
for example a package includes multiple activities and an activity includes multiple widget structures.
at the same time we ltered out redundant gui information that highly depends on the device or the execution environment such as coordinates or screenshot images.
in addition we merged some gui information into a single property.
for instance event related properties clickable long clickable checkable and scrollable were merged into an executable property.
240figure a gui comparison model using multi level gui comparison criteria guicc for android apps de nition of guicc model.
based on the classication of extracted gui information we nally designed amulti level guicc model that contains hierarchical relationships among the information.
the comparison model is shown in figure .
the comparison model has comparison levels c lv and it has types of outputs according to the comparison result t terminated s same or n new .
the default comparison level is set to c lv5 but the maximum comparison level max c lv can be exibly modi ed if a tester wants to adjust the abstraction level of a gui model.
finally these multi level guicc were embedded in graphgenerator oftestingengine of our testing framework and the details of the framework are explained in section .
.
.
gui comparison criteria for android apps asc lv increases from c lv1 toc lv5 more concrete i.e.
ne grained gui information is used to distinguish gui states.
following are the detailed comparison techniques for each c lv .
c lv1 compare package names .
first of all our comparison model compares the package name of the currently running screen to visited screennodes of the gui graph.
in order to e ciently explore a behavior space i.e.
explore only the relevant space a testing engine must be able to clearly distinguish the boundary of the space of an aut.
to avoid exploration outside of the app boundary the testing engine analyzes which app package is currently being focused and run by the android device.
we used dumpsys again to dump the status of the android system and extracted the package name of the currently focused app.
by parsing mfocusedapp information analyzed bydumpsys the testing engine identi es which package is running in the foreground.
if the device does not focus on the package of the target aut the current status of the target app is considered as a terminated state t i.e.
an exit state .
this exit case can be caused by transitions to a third party app shutdown or crashes by an executed event.
c lv2 compare activity names .
after the comparison in c lv1 is passed the comparison of activity names is performed.
the activity name of the currently running screen is compared with the names of activities of other screennodes in the gui graph.
if not all screennodes in the gui graph have the same activity name the current screen is considered as a newly discovered gui state.
on the contrary if one or more screennodes that have the same activity name are discovered in the gui graph the next level of comparison c lv3 is taken.
because each activity of android apps is independently implemented in di erent source code les e.g.
mainactivity.java andsecondactivity.java and the life cycle of each activity is managed individually the activity name can be used to distinguish the physical di erence in gui states.
similar to the package name extraction in c lv1 the name of the currently running activity in the foreground can be obtained by parsing mcurrentfocus information from dumpsys system diagnostics.
c lv3 c lv4 compare widget composition .
even though the activity is the basic and simplest information to distinguish gui states of android the activity name still could not distinguish detailed gui states in realworld apps.
in particular modern android apps show dynamic and complicated user interfaces in a single activity to provide better experience as well as good modularity.
for example many commercial apps utilize the viewpager widget which contains multiple di erent pages to show.
if a testing tool cannot distinguish each page then a generated gui graph cannot exercise some events in abstracted pages.
therefore it is imperative to compare gui states with more detailed information at the widget level because guis differ depending on the widget composition even though they are running on the same activity see figure a .
in other words gui graphs based on activities can be inadequate for the e ective gui model generation of real world apps.
in order to compare two guis based on the composition of widgets a widget hierarchy extracted by uiautomator a ui testing support tool of android is used.
uiautomator automatically analyzes a widget hierarchy from the screen of an android device and constructs a widget hierarchy tree as illustrated in figure .
every widget node has parentschildren or sibling relationships with each other and the relationships are encoded in an index property.
index represents the order of child widget nodes for example if the value of an index of a certain widget wiis wiis the rst child of its parent widget node.
in addition by using index values each widget node x can be speci ed as an index sequence that accumulates the indices from the root node to the target node.
for instance a widget node m in figure has its index sequence that accumulates the indices from node a tonode m .
by using these index sequences our comparison model obtains the composition of speci c types of widgets.
table shows two types of widget composition extracted from the widget tree of figure composition of layout widgets and executable widgets .
from the widget tree we refer to nonleaf widget nodes as layout widgets and leaf widget nodes whose event properties e.g.
clickable have at least one true value as executable widgets .
if a non leaf widget node has an executable property e.g.
if a listview isclickable while its children are not executable its child leaf 241figure a widget hierarchy tree analyzed by uiautomator table widget composition of a gui of figure widget type nodes cis layout a b c f executable d g l m nodes are considered as executable widgets .
in order to utilize the extracted index sequences as the widget information we store cumulative index sequences cis into newly added screennodes .
for the above example two ciss are included in a screennode to represent the composition of layout widgetsand executable widgets .
atc lv3 the layout cisis used as guicc ofscreennodes.
if a screen has the same layout cis asciss of other visited screennodes in a gui graph then the executable cis is compared at c lv4 in succession.
otherwise the screen is regarded as a new screennode because a screen that has di erent widget composition is likely to have a di erent gui state.
comparison of executable widgets at c lv4 is similar to the existing comparison technique that compares a set of event handlers between gui states .
since di erence in composition of executable widgets indicates a di erent set of executable events a target screen that has a di erent executable cis must be regarded as a new gui state.
c lv5 compare contents .
the last comparison step is performed to detect a gui that can be distinguished only by the contents information such as text.
in the case that multiple screens have separated context even if there is no di erence in their widget compositions of two screens modeling guis for each screen separately is necessary to represent behaviors that can act di erently in di erent contexts.
using the uiautomator again the contents information is obtained by extracting contents values such as text text and description of contents content desc .
for the abstract representation of the text values and fast comparison we simply use the length of the textual information i.e.
if the value of text is mainactivity and the value of content desc is click here then we use .
there is an additional need to distinguish guis before and after the execution of scroll events e.g.
swipe down swipe up .c lv5 also performs the comparison that examines the rst item of listview android.widget.listview in guis.
because the rst item of listview is changed by scroll events if visible items of the listview are changed after the scrolling the comparison of the visible rst list item is intuitively reasonable.
by distinguishing a new gui state after scroll events our testing tool is able to obtain some possible events to be executed beneath the visible area of the de vice screen.
this comparison step can be applied to all other list widgets such as gridview android.widget.gridview .
however this comparison step c lv5 does not always work as expected for every gui in real world apps.
as explained in chapter comparison of the detailed information can lead to the state explosion problem during the exploration.
if a tester runs gui testing with c lv5 for the calculator app shown in figure b the model generator will add new screennodes in nitely because the text on the screens can be endlessly updated by user inputs.
therefore a tester has to carefully select a proper comparison level to generate a gui model e ciently.
also gui information used in c lv5 can be altered to examine speci c information according to the type of apps.
for example a gallery app can utilize image recognition or comparison between screens to compare contents in c lv5 .
.
.
multi level gui graph generation to summarize the ve levels of hierarchical gui comparison criteria were implemented as a multi level gui comparison model ofmodelgenerator .
a tester or testing tool who performs model based gui testing from a black box view can adjust the level of testing thoroughness by simply adjusting the maximum level of guicc max c lv .
after the testing engine is con gured with a given level the modelgenerator automatically compares gui states based on the comparison criteria of the designated c lv while exploring the behavior space.
as a result the selection of a clvnot only impacts the behavior modeling by a gui graph but also determines the feasibility of gui graph generation.
therefore a tester should carefully consider an adequate criterion that is proper to the target aut.
.
testing framework .
framework overview in this study we mainly refer to the recent researches that describe a generic concept of model based techniques in test input generation aspects .
because most android apps do not have their own gui models beforehand we use the term model based android gui testing as a model learning gui testing technique that dynamically constructs a gui model through reverse engineering i.e.
gui ripping .
these model based techniques do not only perform testing but also build gui models while learning a model from runtime behaviors of an aut at the same time.
the ultimate goal of model based android gui testing is to detect runtime errors using systematically generated test inputs from a gui model .
keeping the basics in the model based testing approaches we develop an automated model based android gui testing framework with multilevel gui comparison criteria multi level guicc as illustrated in figure .
this framework receives two inputs an installation le .apk le on the android side and a selected level ofguicc max c lv to con gure the abstraction level of a gui graph.
after the testing engine sets a target aut it starts a reverse engineering process from a black box view to generate a gui graph of the target app without source code.
during the model generation process the engine automatically explores the behavior space of an aut by executing events sequentially on the android device this process is generally called online test input generation in .
as 242figure model based gui testing framework for android apps using multi level guicc outputs of testing our testing engine nally produces a set of event sequences that were executed on the aut a gui graph representing the explored behavior space and executed tests and an error report if the engine detects any error during the exploration.
.
modules of testing framework as figure displays our testing framework consists of three modules testingengine running on a desktop pc eventagent executed on an android device and a communication layer between them.
from section .
.
to .
.
we provide detailed information about each module of the framework.
.
.
communication layer first of all to generate a gui graph dynamically a means of online communication between a desktop pc and an android device must be constructed.
commands of test inputs events generated by testingengine are sent to the aut running on the android device through the communication layer.
after eventagent on the android device receives the commands and executes them eventagent returns the execution results i.e.
gui information of the current screen of the android device and the testingengine analyzes the results sent by the device.
the testingengine also updates the gui graph based on the execution results.
as shown in figure the android debug bridge adb is used to facilitate the communication between two di erent environments a desktop and an android device.
adb a tool that supports command line communication between a server android device or emulator and a client pc is used to deliver both event commands and the execution result les.
in addition tcp socket communication is also used to send or receive de activation messages between them.
every time the testingengine generates a test input eventagent is activated by a wake up message sent through a tcp socket.
after the eventagent completes the test execution it generates several result les and returns them to the testingengine using adbcommands.
.
.
testingengine basically by repeating test input generation using a gui graph!
transmission of test inputs to eventagent!
event execution on aut !
transmissionof execution results to pc !
result analysis!
gui graph update our testingengine automatically explores the behavior space of a running aut.
details of submodules are explained below.
testexecutor.
the rst submodule of testingengine is testexecutor which generates test inputs and sends command messages to the connected android device to explore the behavior space.
based on the generated gui graph at a certain time testexecutor manages a set of executable events extracted from the visited screennodes and generates event sequences as next test inputs.
the current version of testexecutor only supports encoding of the ui events such as click long click swipe and text inputs it does not deal with system events.
the automated exploration algorithm is another crucial technique to e ciently generate a gui graph.
thus various kinds of exploration techniques have been researched and they are usually called crawling algorithms.
our testing framework implemented a breadth rst search bfs algorithm using an event queue called working list for the gui crawling in testexecutor .
the working list which contains test inputs to be executed determines the order of test selection and execution.
graphgenerator.
the graphgenerator of our testing framework generates and manages a gui graph of an aut while the testexecutor is exploring the behavior space.
the main goal of the graphgenerator is to update the gui graph accurately with the received information execution results from the android device.
in other words the graphgenerator determines how to update the graph by analyzing the execution results and it suggests the next test input to the testexecutor based on the current gui graph.
in order to obtain a set of executable events for test input generation testingengine dynamically analyzes gui states to extract executable events.
previous studies call the dynamic extraction of a set of executable or reable events from guis action deduction or action inference .
in this study we also call the process of extracting events from guis event inference and uiautomator s api is used again to perform the event inference dynamically.
from theuidump.xml le the testingengine can get the structural information of gui components on the screen of the android device and also acquire the detailed properties and 243values related to the events.
uiautomator extracts information from each widget node as follows.
node bounds selected false password false long clickable false scrollable false focused false focusable true enabled true clickable true checked false checkable false content desc package com.se.toyapp class android.widget.
button resource id text a button index by analyzing the values of properties related to user events clickable long clickable checkable scrollable executable events can be inferred from the widgets shown in a gui.
in the case of the above example the widget is a button named a button that is located in of the device screen this button is clickable but neither long clickable norscrollable .
the extracted property information is used as attributes to represent a gui state of each screennode and the inferred events are stored in the working list of the testexecutor to generate subsequent test inputs.
after every event execution the graphgenerator checks whether or not the current screen of the android device is displaying a newly discovered gui state using multi level guicc .
according to the comparison result the graphgenerator updates its gui graph by adding only an eventedge or both an eventedge and a screennode .
when adding a new screennode to the graph graphgenerator stores the gui information of the node to the disk with a screenshot image and its uidump.xml le.
moreover if it receives an error report from the errorchecker the graphgenerator highlights corresponding screennodes as problematic ones and creates an error log le.
after all if the testingengine nishes the testing process a new gui graph that stores all information about visited screennodes and executed eventedges is saved to an xml le by graphgenerator .
errorchecker.
while analyzing the result les given by the device the graphgenerator continually receives a log le made by android logcat .
the errorchecker analyzes the le to detect suspicious execution traces or logs using error lters .
basically the errorchecker is equipped with error lters that catch e androidruntime d crashanrdetector and f logs.
in other words the errorchecker detects runtime errors anr application not responding reports and fatal errors in the android system.
if the errorchecker nds erroneous logs after the event execution the detected error is reported by a text le.
.
.
eventagent eventagent is a tool that runs on an android device to receive commands of test inputs from the testingengine testexecutor .
the eventagent was implemented as uiautomatortestcase it transforms test commands into corresponding method calls.
in order to wait to receive a test command theeventagent waits until a command message has come from testingengine .
for the wake up process of eventagent the tcp socket communication was used see .
.
.
after the eventagent receives the message it is translated to a sequence of corresponding method calls using apis of uiautomatortestcase .
to avoid the waste of the memory resource in an android device the eventagent is only activated to receive and execute test inputs for a while and it is automatically deactivated after the test execution.
.
experiments .
research questions the goal of our experiments is to observe the e ects of the gui comparison criteria guicc on the testing e ectiveness by performing actual automated model based android gui testing.
in our experiments we mainly evaluated the results based on the generated gui graphs according to the target level of guicc max c lv while keeping the test case generation algorithm.
to this end we investigated the following questions rq1.
how does the guicc a ect the behavior modeling of android apps?
rq2.
does the guicc a ect the achieved code coverage in model based gui testing?
rq3.
does the guicc a ect the error detection ability in model based gui testing?
rq1 is an elementary question to evaluate the relationship between the comparison level c lv and the behavior modeling by generated gui graphs.
by identifying the correlation between c lv and modeled gui graphs we answer the e ect of the guicc on behavior modeling of subject android apps.
only for rq1 we conducted the graph generation not only on the open source apps but also on the real world commercial apps to discover the practical application of our testing framework.
in particular we analyze the di erence between activity based gui graphs c lv2 and other gui graphs with the higher level of guicc to gure out the limitations of model based testing techniques that use activity based gui models.
rq2 investigates the code coverage achieved by the modelbased gui testing with our framework.
since the code coverage is the most frequently used metric to assess the e ectiveness of testing we evaluated how much behavior is covered at a speci c level of guicc by measuring code coverage.
we used the emma1code coverage measurement tool which is o cially supported by the android platform.
for the measurement of the code coverage the whole project les with the source code les of the target apps are required.
thus we only measured the coverage of open source apps to answer rq2.
lastly for rq3 we investigated the detected errors during the testing since the error detection ability can vary with theguicc .
more speci cally we observed which level of theguicc can detect runtime errors and failures with our error lters explained in .
.
.
by analyzing the detected errors considering the c lv we expect to point out the limitations of existing activity based gui models and gure out the importance of the selection of the guicc in modelbased gui testing for android apps.
.
experimental setup .
.
experimental environment we built a testing environment on both a pc and an android device or genymotion emulator2 and the experimental environment is described in table .
communication layer and every other module in our testing framework was implemented in java testingengine and eventagent .
1emma java code coverage measurement tool 2genymotion 244table experimental environment of the desktop pc and the android device oswindows enterprise k sp1 bit processorintel r core tm i5 cpu .67ghz desktop memory .0gb emulatorgenymotion .
.
with oracle virtualbox .
.
samsung galaxy s4 android .
api processorsandroid deviceandroid virtual device3 mb memory .
.
benchmark applications in order to evaluate the in uence of the guicc on the gui graph generation and testing e ectiveness we needed a set of benchmark android apps.
we investigated several earlier papers that performed their experiments with open source apps for the evaluation.
most studies used the distributed open source android apps provided by fdroid3 and shauvik s study combined them to evaluate the strengths and weaknesses of the existing techniques.
among the benchmark apps used in we randomly selected medium sized loc apps for our experiments.
also we collected another real world commercial apps that were registered in google play .
the collected open source apps and their sizes are listed in table and commercial benchmark apps with their download download and recommendation recomm.
counts in table .
.
results once the testing framework is ready with the max c lv and a given installation apk le of an application under test a speci c level of gui graph is automatically generated4.
by analyzing the generated gui graphs and related result les we answered our research questions.
note that for assessing the in uence of guicc the same test case generation algorithm was used independently of the chosen c lv .
.
rq1.
gui graph generation by guicc in order to identify the e ect of the guicc on the gui graph generation for rq1 we manipulated the max c lv from to for every benchmark app and generated multiple gui graphs.
after we generated the graphs we compared the number of screennodes sn and eventedges ee among them.
since the rst comparison level c lv1 package name comparison is only used to check the boundary of the behavior space c lv1 was only used for the detection of app termination or runtime errors and not included in the results.
the other remaining four comparison levels c lv2 c lv5 were used to analyze the e ect of the guicc but we omitted the result graphs of c lv3 due to lack of space.
table and table show the size of the generated gui graphs of open source apps and commercial apps respectively.
unsurprisingly for every open source benchmark app the number of screennodes and eventedges tended to increase as the c lv increased.
for some apps a generated gui 3f droid 4the generated graphs are uploaded to our website open source benchmark android apps no application package loc of classes org.jtb.alogcat com.example.anycut com.evancharlton.mileage cri.sanity org.jessies.dalvikexplorer i4nc4mp.mylock com.bwx.bequick com.nloko.android.syncmypix net.mandaria.tippytipper de.freewarepoint.whohasmystu table commercial benchmark android apps no application name download recomm.
google translate advanced task killer alarm clock xtreme free gps status toolbox music folder player free wi matic vnc viewer uni ed remote clipper life time alarm clock graph with c lv5 contained more than three times as many nodes and edges as a gui graph with c lv2 .
because the number of eventedges ee indicates the number of executed tests graphs with more edges are more likely to have better ability to infer the test events.
in addition the results showed that the activity based gui graphs c lv2 could not exercise a lot of behaviors.
the guicc based on the activity name was too abstract to model the behaviors adequately.
on the other hand the graphs using concrete guicc could re ect more behaviors than the activity based graphs.
from the results we found that the higher level of the ne grained guicc that we had de ned were generally more e ective to adequately model the behavior space than the lower level of the guicc .
additionally we found some unusual and notable results during the graph generation.
first the graphs of sanity cri.sanity showed a dramatic change between c lv4 andc lv5 .
this result indicates that di erentiating the guicc can a ect the ability of exploration of fully automated model based gui testing.
second the generated gui graphs of dalvikexplorer org.jessies.dalvikexplorer showed that the gui graph with c lv5 caused state explosion s e due to the in nite exploration.
since dalvikexplorer has a continuously changing textview that monitors the status of the dalvik virtual machine in an android device the comparison of text contents produced an in nite number of gui states while exploring the behavior space.
we regarded the graph generation as a state explosion if it took more than three hours and no sign to nish the exploration.
these unexpected results imply that an optimal or adequate guicc can vary depending on the behaviors of an aut so guicc must be carefully selected considering the target app.
furthermore higher level guicc do not always perform better because the comparison of detailed information can be disruptive for e cient testing.
for the gui graph generation of commercial benchmark apps we discovered similar results to the experiment with open source apps.
the higher levels of guicc usually made larger gui graphs including more di erent gui states and transitions screennodes and eventedges to represent the behavior space.
for some commercial apps advanced task killer alarm clock xtreme free on the other hand c245table result gui graphs of open source apps s e stands for the state explosion c lv2 c lv4 c lv5noapp name sn ee sn ee sn ee 1alogcat 2anycut 3mileage 4sanity 5dalvikexplorer s e s e 6mylock 7bequick 8syncmypix 9tippytipper 10whohasmystu table result gui graphs of commercial apps c lv2 c lv3 c lv5noapp name sn ee c lv sn ee 1google translate 2advanced task killer 3alarm clock xtreme free 4gps status toolbox 5music folder player free 6wi matic 7vnc viewer 8uni ed remote 9clipper 10lifetime alarm clock lv4 andc lv5 led to state explosion due to the dynamically changing contents e.g.
the list of running processes the current time .
the answer to rq1 can be summarized as follows the results of behavior modeling in model based gui testing can be signi cantly in uenced by the selection of the guicc but higher levels of the guicc do not always mean more optimized solutions.
.
rq2.
code coverage by guicc we answered rq1 to investigate how guicc a ects the behavior modeling.
however a gui graph with more screennodes and eventedges does not guarantee better coverage to explore the behaviors.
if some screennodes have di erent guis but the same set of executable events the increasing number of eventedges does not improve the testing e ectiveness.
duplicated gui states and redundant tests can even lower the testing e ciency because the testing tool unnecessarily repeats the execution of the same test inputs.
in order to gure out how much the behavior implemented in source code is covered we measured the code coverage.
since higher code coverage has a better potential to detect faults in the source code the code coverage is widely used as a criterion to evaluate the testing e ectiveness.
the four types of code coverage were measured by emma and we listed the results of method coverage and statement coverage in table .
the sanity app unfortunately had to be excluded due to its internal errors while instrumenting the coverage measurement classes.
the column labelled a shows measured coverage values that achieved by activitybased c lv2 gui graphs.
column mshows maximum coverage values that achieved by the graphs with the higher levels of guicc c lv3 c lv5 .
column c lv shows the minimum comparison level c lv to achieve the maximum coverage m. for example if mis80 andc lv is3 ittable achieved code coverage foropen source apps method coverage statement coveragenopackage nameac lv m ac lv m 1alogcat 2anycut 3mileage 4sanity n a 5dalvikexplorer 6mylock 7bequick 8syncmypix 9tippytipper 10whohasmystu average a activity based c lv2 m maximum coverage means that the lowest c lv that achieved the maximum coverage was c lv3 and the highest coverage achieved was .
the table shows that the level of guicc could also a ect the code coverage such as the method and statement coverage.
also increasing the c lv improved the achieved code coverage for all apps and some apps anycut syncmypix showed substantial di erence between aandm.
in the table the mileage mylock and syncmypix apps showed relatively low coverage values.
here are the reasons for the disappointing results.
mileage contained a lot of behaviors that were dependent on the sophisticated text input and this app also required a license le to access the core features.
mylock performed a lot of tasks through services running in the background of the android system but our testing framework does not support such service based apps.
lastly the main functionalities of syncmypix were started only after a picture le was selected but our framework did not support interoperability testing with 3rd party apps.
the answer to rq2 can be summarized as follows the achieved maximum code coverage was also a ected by the level of guicc and existing activity based gui models could not cover a lot of behaviors in the source code.
.
rq3.
error detection ability by guicc the ultimate goal of the automated gui testing is to detect errors while exploring the behavior space of the target aut by exercising test inputs .
during our experiments the framework had detected four reproducible runtime errors in open source benchmark apps they are listed in table .
mileage was crashed by fatal errors sigsev fatal signal logged by f libc and sanity was terminated by runtime exceptions nullpointerexception logged by e androidruntime .
as expected the results show that the runtime errors were only revealed in the ne grained gui graphs with c lv4 andc lv5 but the lower level of guicc could not detect them.
in other words the ability to detect faults was also a ected by the guicc and existing guicc lower than c lv4 including activity based graphs could not detect the listed runtime errors we found.
this error detection result shows that determining the level of behavior abstraction affects not only the achieved code coverage but also the error detection ability.
also an improperly selected coverage criterion lowers the behavior space to be explored and the ability to detect errors depends on the model as a result.
246table detected runtime errors by our testing framework no c lv app name error type mileagefatal error fatal signal f libc sanityfatal exception runtimeexception e androidruntime 3c lv5 sanityfatal exception runtimeexception e androidruntime 4c lv4 mileagefatal error fatal signal f libc .
related work model based android gui testing.
modeling the guis of android apps can be challenging because the precision of an approximation technique that abstracts the program ow determines the completeness of a testing framework .
in order to tackle the modeling issues a great deal of model based gui testing techniques have been actively researched .
they commonly generate nite state machine based gui models to generate test inputs .
among various approaches the most well known reverseengineering techniques for gui testing are gui crawling and gui ripping .
a testing tool developed by d. amal tano called androidripper realized and utilized those model based techniques for android apps and it became the root of our work.
existing model based gui testing techniques focused on the functional correctness and they have struggled to improve the testing e ectiveness.
for example in the study by r. mahmood et al.
they developed a testing framework to generate e ective test cases using gui based models call graph model utilizing a white box analysis.
for another example w. yang et al.
improved the accuracy of the gui model generation using a gray box approach that performs the static analysis of source code to assist both gui crawling and gui model generation .
also several studies use symbolic execution and concolic testing for model based testing.
although they improved the fault error detection ability and revealed errors that other tools could not discover those frameworks require relatively complicated data or analysis techniques such as static analysis using the complete source code.
meanwhile our testing framework performs fully automated testing with no additional data to be analyzed only an apk le is required.
in order to address the complexity of white box or gray box approaches some crawling based black box gui testing approaches such as mobiguitar have paved the way for e cient model based android gui testing.
gui comparison criteria.
in this study we considered the guicc as an essential requisite for e ective modelbased gui testing.
as mentioned in previous sections existing model based gui testing techniques or tools have their own guicc even though the guicc are not explicitly speci ed.
among the available tools the activitybased guicc have frequently been used to distinguish gui states .
however as our experimental results implied in section the activity based modeling can lead to a low testing e ectiveness i.e.
the code coverage or the fault detection ability since the majority of behaviors can be missed due to the complicated guis in real world android apps.
the recent work by azim and neamtiu for exam ple modeled android apps into activity transition graphs which are almost the same as our gui graphs with c lv2 .
however the gui graphs with c lv2 turned out to show lower e ectiveness than other graphs using a more adequate guicc through our evaluation.
on the contrary guicc of some other techniques are too sensitive to e ciently cope with dynamically changing guis of real world apps.
those volatile guis can cause a state explosion problem during the model based testing but they only provide xed singlelevel criteria e.g.
every observable change in guis .
since an adequate guicc of one app is not always the best one for other apps providing a exible con guration of guicc is needed.
in that sense our model based testing technique could improve testing e ciency exibility and practicality as well as e ectiveness.
.
conclusion most existing techniques for model based android gui testing have not pointed out the limitations for practical application.
they have overlooked dynamic and volatile behaviors of real world android apps that make gui model generation challenging or infeasible.
however the in uence of the behavior abstraction has not previously been evaluated.
in this study we focused on gui model generation which is a key feature of model based testing and we introduced an automated testing framework with multi level gui comparison criteria multi level guicc for android apps.
based on our proposed approach we conducted empirical experiments to identify the in uence of guicc on testing e ectiveness of model based gui testing for android apps.
as a result of the experiments that performed automated model based gui testing with proposed multi level guicc we observed the visible in uences of guicc on generated gui models achieved code coverage and error detection.
model based testing with multi level guicc could generate more e ective gui models than activity based gui models in terms of code coverage and error detection ability.
moreover our testing framework with multi level guicc could support more exible model based gui testing by manipulating comparison levels.
the results will shed light on the behavior modeling of automated model based gui testing for both open source and real world android apps.
in our future research we expect to develop an automatic guicc selection technique for automated model based gui testing that analyzes the inherent characteristics of the target app and selects adequate criteria.
we have been investigating speci c behaviors and widget compositions of realworld commercial android apps and this information is expected to be utilized in the guicc selection technique.
as a short term goal of our research we have a plan to open our testing framework to the public as soon as possible to share our knowledge and to contribute to mobile app testing researchers and practitioners.
.