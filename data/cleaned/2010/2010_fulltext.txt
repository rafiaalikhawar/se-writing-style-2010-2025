automatic generation of load testspingyu zhang sebastian elbaum and matthew b. dwyeruniversity of nebraska lincoln pzhang elbaum dwyer cse.unl.eduabstract load tests aim to validate whether system per formance is acceptable under peak conditions.
existing testgeneration techniques induce load by increasing the size orrate of the input.
ignoring the particular input values however may lead to test suites that grossly mischaracterize a system sperformance.
to address this limitation we introduce a mixedsymbolic execution based approach that is unique in how it favors program paths associated with a performance measureof interest operates in an iterative deepening beam searchfashion to discard paths that are unlikely to lead to high loadtests and generates a test suite of a given size and levelof diversity.
an assessment of the approach shows it generatestest suites that induce program response times and memoryconsumption several times worse than the compared alternatives it scales to large and complex inputs and it exposes a diversityof resource consuming program behavior.keywords load testing symbolic executioni.
introductionload tests aim to validate whether a system s performance e.g.
response time resource utilization is acceptable underproduction projected or extreme loads.
consider for example an sql server that accepts queries specified in the standardquery language to create delete or update tables in a database.functional tests would validate whether a query results inappropriate database changes.
load tests however would berequired to assess whether for example for a given set ofqueries the server responds within an expected time.existing approaches to generate load tests induce load byincreasing the input size e.g.
a larger query or number ofqueries or the rate at which input is provided e.g.
morequery requests per unit of time .
consider again the sqlserver.
given a set of tester supplied queries a load testingtool might replicate the queries send them to the sql serverat certain intervals and measure the response time.
when themeasured response time differs from the user s expectations which might be expressed as some upper bound determined bythe size or complexity of the queries or underlying database a performance fault is said to be detected.current approaches to load testing suffer from four limi tations.
first their cost effectiveness ishighly dependent onthe particular valuesthat are used yet there is no support forchoosing those values.
for example in the context of a sqlserver we studied a simple selection query operating on apredefined database can have response times that vary by anorder of magnitude depending on the specific values in theselect statements.
clearly a poor choice of values could leadto underestimating system response time thereby missing anopportunity to detect a performance fault.second increasing the input size may be a costly means toload a system.
for example in the context of a compressionapplication we studied we found that to increase the responsetime of the application by seconds one could use a 75mbfile filled with random values or a 10mb file if the inputs arechosen carefully.
this is particularly problematic if increasingthe input size requires additional expensive resources in orderexecute the test e.g.
additional disk space bandwidth .third increasing the input size may just force the systemto performmore of the samecomputation.
in the worst case this would fail to reveal performance faults and if a fault isdetected then further scaling is likely to repeatedly reveal thesame fault.
in functional testing diversity in the test suite isdesirable to achieve greater coverage of the system behavior.load suites thatcover behaviors with different performancecharacteristicsare not a focus of current tools and techniques.finally while most load testing focuses on response time orsystem throughput there are many otherresource consumptionmeasuresthat are of interest to developers.
for example mobile device platforms place a limit on the maximum amountof memory or energy that an application can use.in this paper we present an approach for the automatedgeneration of load test suites that starts addressing theselimitations.
it generates load test suites that a induce load byusing carefully selected input values instead of just increasinginput size b expose diversity of resource consuming programbehavior and c target a range of consumption measures.the approach leverages recent advances in symbolic execu tion to perform a directed incremental exploration of the pos sible program paths.
the approach isdirectedby a specifiedresource consumption measure to search for inputs of a givensize or structure that maximize consumption of that resource e.g.
memory execution time .
the technique isincrementalin that it considers program paths in phases.
within each phaseit performs an exhaustive exploration.
at the end of eachphase the paths are grouped based on similarity and the mostpromising path from each group relative to the consumptionmeasure is selected to explore in the next phase.we have implemented the approach in the jpf infrastructure and assessed its effectiveness onjzlib sat4j andtinysql.
the results indicate that the proposed approachcan produce suites that induce response times several timesgreater than random input selection and can scale to millionsof inputs increase memory consumption between and400 when compared with standard benchmarks and exposedifferent ways to induce high consumption of resources.
.
c ieee ase lawrence ks usa432011 26th ieee acm international conference on automated software engineering ase .
ase.
.
ii.
background andoverviewtest engineers have at their disposal many black box ap proaches to support load test suite generation which are basedprimarily on the manipulation of the input size.
overlookingthe program s implementation details however means a lossof insights into how the structure of the input affects programperformance which can obfuscate the assessment.
in thissection we provide an overview of our approach to developload tests and begin with background on techniques that weuse to realize that approach.a.
mixed symbolic executionsymbolic execution simulates the execution of a pro gram usingsymbolic valuesinstead of actual values as inputs.symbolic values are unknown and invariant any computationwith them must interpret them as any legal value and that valuecannot change throughout the computation.
for example thesymbolic execution of y x if y then y return y would introduce a symbolic variablexto denote the valueof variablexon entry to the code fragment.
it then performswhat amounts to a depth first search dfs of the executablepaths through the program.
for this fragment there are twosuch paths whenx 0the valuex 1is returned and when x the valuexis returned.
we note thatthe dfs nature of symbolic execution arises when symbolicvalues are tested in branch conditions.
the extension of thepath along the true branch will be symbolically executed thenbacktracking will occur which will cause the extension alongthe false branch to be executed.paths are uniquely defined by a conjunction of constraints.each constraint encodes a branch decision made along thepath and is defined as a boolean expression over concreteand symbolic values.
the conjunction is called thepathcondition pc .
the pcs for paths and arex 0and x respectively.
eachsymbolic statein a symbolicexecution defines a pc and a set of possible values definedas expressions over concrete and symbolic values for eachprogram variable.the simplicity of this example belies the true complexity ofperforming symbolic execution on real programs.
researchershave extended the basic symbolic execution approach whichwas defined for integer variables to treat heap allocated dataand reference types .
while this allowed the application ofsymbolic execution to a wider range of programs controllingthe cost of symbolic execution has required mixing symbolicand concrete execution e.g.
.
in this approach someinputs are given symbolic values and the rest are given actualdata values.
this significantly speeds up processing since thenon symbolic data values cause no increase in the space ofpaths that are explored i.e.
branches dependent only on non symbolic values do not require backtracking.in spite of the successes of mixed symbolic execution togenerate functional tests its application to the generation ofselect symfld symfld from symtab join symtab on symcond where symcond or symcond fig.
.
sql query template fig.
.
histogram of response time for tinysqlload tests is not cost effective.
to illustrate this point weapply mixed symbolic execution to assess the response timeoftinysql an sql server that we study in some depthlater in section v. assume that the goal is to validate whetherthe server can consistently provide response times below 90seconds for a common query structure like the one in figure1 operating on a benchmark database.
under this setting thequery structure is fixed and the database has concrete data butthe query s parameters are marked as symbolic.
even thoughthis is rather a simple query after hours a mixed symbolicexecution for test generation will still be running.
it will havegenerated tests by solving the paths conditions associatedwith an equal number of paths.figure shows a snapshot of the tests generated as ahistogram where the x axis representstinysqlresponsetimes in seconds and the y axis represents the number of teststhat caused that response time.
we note that most tests causea system response time of less than seconds but there issignificant variability across tests ranging from seconds toover minutes.
of all these tests we are interested in theones on the right of the figure the ones causing the largestresponse times and most likely to reveal performance faults.the question we address is how to direct path exploration toobtain just those tests.b.
proposed approach to induce loadrather than performing a complete symbolic execution weperform anincrementalsymbolic execution to more quicklydiscard paths that do not seem promising anddirectthe searchtowards paths that are distinct yet likely to induce high loadas defined by a performance measure.the approach requires for the tester to mark the vari ables to treat as symbolic just like in standard mixed symbolicexecution specify the number of tests to be generated denoted withtestsuitesize and select the performancemeasure of interest denoted withmeasure from a predefinedand extendable set of measures.
in addition to control the waythat the symbolic execution partitions the space of program44paths intophasesan additional parameter lookahead isneeded.lookaheadrepresents the number of branches thatthe symbolic execution looks ahead in assessing whether pathsare diverse enough and of high load.
we discuss the selectionof values forlookaheadin the next section .given these developer supplied parameters the approachperforms an iterative deepening mixed symbolic execution thatchecks after a depth oflookaheadbranches whether therearetestsuitesizediverse groups of program paths and if so itthen chooses the most promising path from each group basedonmeasure.promoting diversity.after everylookaheadbranches diversity is assessed on the paths reaching the next depth shortpaths are deemed as having less load generating potentialand are hence ignored .
the approach evaluates diversityby grouping the paths intotestsuitesizeclusters based ontheir common prefixes.
a set of clusters is judged to haveenough diversity when no pair of paths from different clustershave a common prefix whose length in terms of branchestaken is withinlookahead testsuitesizeof either path inthe pair.
the intuition is that forcing clusters to diverge bymore thanlookahead testsuitesitebranches will drive testgeneration to explore different behaviors that incur high load.if the diversity among clusters is insufficient then explorationcontinues for anotherlookaheadbranches.
otherwise theapproach selects the most promising path from each clusterfor further exploration in the subsequent symbolic executionphase and the rest of the paths are discarded.favoring paths according to a performance measure.theselection of paths at the end of each phase is performed ac cording to the chosen performance measure.
so for example if the user is interested in response time then paths traversingthe most time consuming code are favored.
if the user isinterested in memory consumption then the paths containingmemory allocation actions are favored.
independent of thechosen measure what is important is that each path has anassociated cost performance summary that acts as a proxy forthe performance measure chosen by the user.
this summary isupdated as a path is traversed and it is used to select promisingpaths for load testing.illustration.figure illustrates this approach withtestsuitesize 2andlookahead .
these valuesmean that the common prefix of any pair of paths from twoclusters must differ in more than branches for the clustersto be considered diverse enough to enable path selection.the approach starts by performing an exhaustive symbolicexecution up to depth .
then it clusters the paths reachingthat depth into two clusters the grayed boxes labelled c1and c2 at depth in the figure.
these clusters however aredeemed insufficiently diverse since a common prefix whoseend is marked with a triangle is found that has a branchlength of .
hence the symbolic execution continues untildepth where the clustering process is repeated.
in this casethe diversity check is successful because the common prefix whose end is marked with a square differs in brancheswhich is more thanlookahead testsuitesize.
fig.
.
iterative deepening beam symbolic executionthe approach in practice.continuing withtinysqlandits query template when the approach is configured to findvalues for the query template to generate tests to use alookahead and to target the maximization of responsetime our approach takes minutes to generate a set of tests of the cost of the hours of symbolic execution .as depicted by annotationst1...t5in figure the testsidentified by the approach do reside on the right hand size ofthe figure.
two of the generated tests are in the bin labelled120 seconds this bin includes all tests that cause responsetimes of seconds or more are in the seconds bin and is in the seconds bin.
each of the generated teststakes at least twice the time of the original test from whichthe template was derived which took less than seconds .in addition as revealed later in the study table iii thegenerated tests are diverse.
these five tests use many differentfields tables and filtering clauses.
furthermore they representthree types of database join operations each of which imposeheavy load on the system in a different way.this section has illustrated how the approach works andits potential to generate load tests that induce high responsetimes.
in the next section we provide a detailed description ofthe approach and its parameterization capabilities to address abroad range of load testing scenarios.iii.
approachthis section presents our approach for load test generation discussing its components parameters and application.a.
symbolic generation of load testsalgorithm symbolicloadgeneration slg details ourload test generation approach.
conceptually the algorithm re peatedly performs a bounded symbolic execution that producesa set offrontiersymbolic states based on the branch lookahead clustersthose states based on the desired number oftests and then if the clusters are sufficiently diverse selects45the mostpromisingstate from each cluster for further explo ration1.
like other mixed symbolic execution techniques e.g.
ours represents states as a combination of a symbolicvariables and their associated constraints for a selected subsetof program inputs in the form of a path condition and b concrete values for the remaining program inputs.
in ouralgorithms we measure the size of a path condition in termsof the number of constraints or clauses which corresponds tothe number of branch decisions required along the path.algorithm takes parameters init the states fromwhich the search commences testsuitesize the numberof tests a tester wants to generate lookahead the increasein path condition size that an individual symbolic executionmay explore maxsize the size of the path condition thatmay be explored by the technique as a whole and measure the cost measure used to evaluate promising states.each of the individual symbolic executions is referred toas aleveland each level is bounded to explore states thatadd at mostlookaheadconstraints to the path condition of apreviously generated state.
this is achieved by incrementingcurrentsizebylookaheadand using the result to bound thesymbolic execution.
if thecurrentsizeexceeds themaxsize then the algorithm restricts the bound to produce states withpath conditions of at mostmaxsize.the first level begins from a set of states init whichforms the initial set of promising states promising.
a levelof the search which corresponds to a call toboundedse attempts to extend states frompromisingto paths of sizecurrentsize.
as that process proceedsmeasureis used toupdate an associated performance estimate with each state.each time afrontieris formed the functionfrontierclusteringis called to cluster the frontier statesintotestsuitesizeclusters.
the details of the clusteringprocess are described below.the goal of clustering is to identify sets of states that arebehaviorally diverse we measure diversity by differences inthe branch decisions required to reach a state.
if the clusters ofstates on the frontier are not sufficiently diverse then we con tinue with another level of symbolic execution that attempts toextend the entire frontier anotherlookaheadbranches.
shouldthe currentclusterpass thediversitycheck the functionselectstates selects the state from each cluster with themaximum accumulated value for the performance measure.we discuss several such measures in section iv.the iterative deepening search terminates if no promisingstate has a path condition whose size islookaheadgreater thanthe previous level s states largestsize returns the largestpath condition found inpromising .when the search terminates the path conditions associatedwith thepromisingstates can be solved with the support ofa constraint solver to generate tests as is done by existingautomated generation approaches for functional tests.1we describe our algorithm here in terms of symbolic states but it isunderstood that each such state defines the end of the prefix of a path exploredby symbolic execution.algorithm 1symbolicloadgeneration init testsuitesize lookahead maxsize measure currentsize 0promising initsearch truewhilesearchdocurrentsize currentsize lookaheadifcurrentsize maxsizethencurrentsize maxsizesearch falseend iffrontier boundedse promising currentsize measure cluster frontierclustering frontier testsuitesize ifdiversitycheck cluster searchthenpromising selectstates cluster measure elsepromising frontierend ififlargestsize promising currentsizethensearch falseend ifend whilereturnpromisingb.
parameterizing slgin defininginita test engineer selects the portion of aprogram s input that is treated symbolically.
depending onthe program one might for example fix the size or structureof the input and let the values be symbolic.
an example ofthe former case is load testing of a program that processesinputs of uniform type but of varying size such as thejzlibcompression program.
an example of the latter case is loadtesting of a program that processes structured input such asthe sql query engine we study.
for such a program thestructure may be fixed e.g.
the number of columns to select number of where clauses etc in the query but the columnnames and where clause expressions are symbolic.
in general treating more inputs symbolically will generate more diverseand higher quality load test suites but such test generationmay also incur greater cost.
we expect that in practice testengineers will start with a small set of inputs as symbolic andthen explore larger sized inputs to confirm the observationsmade on smaller inputs.thetestsuitesizeparameter determines how many testsare to be generated.
varying this parameter helps to producea more comprehensive non functional test suite for the appli cation under test.
regardless of the size of the test suite slgalways attempts to maximize diversity among tests.
exactlyhow many tests are required to perform a thorough testingon the non functional aspect of interest of the application however is a harder question which cannot be assessed withtraditional test adequacy measures such as code coverage.
inpractice selecting a test suite size will likely be an iterativeprocess where test suite size is increased until a point ofdiminishing returns is reached where additional testslack either diversity or high cost.bounding the depth of symbolic execution is a commontechnique to control test generation cost themaxsizeparameter achieves this in our approach.
the parameter46fig.
.
quality of load tests as a function oflookaheadlookahead however is particular to an iterative deepeningsearch as it regulates how much distance the search advancesin one iteration.
the larger thelookahead the more slgresembles a full symbolic execution.
normally a smaller valueforlookaheadis desired because a finer granularity wouldprovide more opportunity for state pruning which is key tothe efficiency of our approach.
ultimately state pruning isdecided by the diversity among the frontier of states so asmallerlookaheadalone cannot lead to ill informed pruning.settinglookaheadtoo small may cause efficiency issues a value of will degrade the search to breadth first.
figure 4provides support for this intuition by plotting the quality oftests generated by our approach usinglookaheadvalues of1 and minutes of test generationtime.
the original tests for these programs execute an averageof branches the selectedlookaheadvalues allowmultiple iterations.
the triangle plots show results for responsetime tests in seconds on the left axis the black triangle plotsare for thetinysqlprogram and the white triangle forjzlib.
the square plots show results for maximum memoryutilization tests in megabytes on the right axis the black squareis for thetinysqlprogram and the gray square forsat4j.in each plot the quality of the test rises as thelookaheadincreases to and then drops off after .
the reason forsuch a trend is that whenlookaheadis smaller than theapproach works less efficiently due to inserting many diversitychecks prematurely and whenlookaheadis larger than much effort is wasted in exploring states that are going tobe pruned.
we use these insights to support the selection ofreasonablelookaheadvalues in the technique evaluations insection v. in practice a similar process could be performedautomatically to select appropriate values for each systemunder test.the last parameter measure defines how the approachshould bias the search to favor paths that are morecostlyinterms of the non functional measure of interest.
the detailsof how the cost for a path is accumulated as the path istraversed and associated with the end state of that path whichis implemented withinboundedse and then used to selectpromising states which is implemented inselectstates are abstracted in algorithm .
in general our approach canaccommodate many measures by simply associating differentcost schemes with the code structures associated with apath.
in our studies we explore response time and maximummemory consumption measures through the following costcomputations a response time cost.this is a cumulative cost so themaximal value occurs at the end of the path.
we estimateresponse time by accumulating the cost of each bytecode.
weuse a very simple and configurable platform independent costmodel that assigns different weights to bytecodes based ontheir associated cost.b maximal memory usage cost.it attempts to record thelargest amount of memory used at any point during a pro gram execution by tracking operations that allocate deallocatememory and increment decrement a memory usage value by aquantity that reflects the object footprint of the allocated type.the maximal memory value is only updated if the currentusage value exceeds it.
as with response time we find thatthis simple platform independent approach strongly correlatesto actual memory consumption.independent of the chosen performance measure and cost our approach assumes that the measure constitutes part of aperformance test oracle.
we note that there are a large numberof measures explored in the literature and that they are oftenspecified in the context of stochastic and queueing models extended process algebras programmatic extensions or concerted and early engineering efforts focused on soft ware performance .
since such performance specificationsare generally still hard to find we take a more pragmaticapproach by leveraging the concept of adifferential testoracle.
the idea behind differential oracles is to compare thesystem performance across successive versions of an evolvingsystem to detect performance regressions across competingsystems performing the same functionality to detect potentialanomalies or disadvantageous settings or across a series ofinputs considered equivalent to assess a systems s sensitivityto variations in common situations.c.
clustering the frontier and diversity checksa convenient choice of clustering would be to use theclassick meansalgorithm and define the number of uniqueclauses between two pcs as theclustering distance.
however this would require comparing across all pairs of pcs of frontierstates and quickly runs into scalability issues.
we devised anapproximate algorithm that is linear in the number of frontierstates.
it makes use of the intuition that for frontier statesresulting from a depth first search a pair of neighboring statesare more likely to resemble each other than a pair of distantstates.
algorithm details the process.
it takesfrontierandthe size of resulting clusterkas input first computes the gap in terms of the number of unique clauses between each pair ofpc si andpc si then sorts the resulting gap vector to find47algorithm 2frontierclustering frontier k cluster n frontier for allsi2frontier i2 n dogap diff pc si pc si end forsortedgap descentsort gap largestgap sortedgap for allsi2frontier i2 n ifcluster.putincurrentpartition si end forreturnclusteralgorithm 3constraintlimitedloadgeneration init test suitesize lookahead maxsize measure maxsolvercon straints currentsize maxsolverconstraintswhilecurrentsize maxsizedopromising symbolicloadgen init testsuitesize looka head maxsolverconstraints measure init fors2promising solve pc s init init stateafterreplay inputs pc s end forcurrentsize currentsize maxsolverconstraintsend whileoutputtests promising the largestk 1gaps.
the algorithm then uses the positionof thesek 1largest gaps to partition thefrontierintokclusters of various sizes.the diversity check ensures that the gaps used to partitionthefrontierare of sufficient size to promote paths that havenon trivial behavioral differences.
the threshold for such agap could be defined in any number of ways.
we use theheuristicthminpartitiongap lookahead cluster since it balancesthe fact that in general largerlookaheadgenerates greaterdiversity while larger values of cluster tends to reduce thedifference between groups of states.
this threshold is enforcedby checking against the least largest gap that is used to defineclusters.d.
dealing with solver limitationswhen algorithm returnspromisinga typical test gener ation technique would simply send the path conditions associ ated with those states to a constraint solver to obtain the testcases inputs.
because path conditions for candidate load testsoften contain many tens of thousands of constraints this basicapproach will quickly expose the performance limitations ofexisting satisfiability solvers.we address this by defining an outer search algorithm that wraps calls to algorithm in such a way that the max imum number of symbolic constraints considered within anyone invocation of algorithm is bounded.constraintlim itedloadgeneration cllg takes the same parameters asslg plus an additional parametermaxsolverconstraints.this parameter can be configured based on the scalability ofthe constraint solver used to implement symbolic execution.algorithm is invoked withmaxsolverconstraintsasthemaxsizeparameter that governs the overall size of theiterative deepening symbolic execution.
whenpromisingisreturned algorithm solves the path constraints of each stateinpromisingto obtain the input values necessary to reachthose states.
then it replays the program using those concreteinputs and when the program has traversed all the predicatesin the path condition the program state is captured and addedtoinit.
the algorithm terminates when the sum of sizes ofthe path conditions explored in each of the invocations ofsymbolicloadgen exceeds themaxsize.in essence algorithm increases the scalability of ourapproach by chaining partial solutions together through theuse of concrete input values.
while this may sacrifice thequality of generated tests it can help overcome the limitationsof satisfiability solvers and allow greater scalability for loadtest generation.
we explore this tradeoff further in section v.finally we note that whenmaxsolverconstraints maxsize constrainedloadgeneration runs a single in stance ofsymbolicloadgeneration .
consequently we usethe former as the entry point for our technique.iv.
implementationwe have implemented the approach by adapting jpf and itssymbc extension which supports symbolic execution.
ourmodifications enable us to associate a performance measurewith a cost for each path to record the path conditions leadingto a state and to compute diversity all of which are necessaryto implement algorithms and .path performance estimators.we have implemented twoperformance cost schemes one aiming to account for responsetime and one for maximum memory consumption.
for re sponse time we use a simple weighted bytecode count schemethat assigns a weight of toinvokebytecodes and a weightof assigned to all others bytecodes.
the implementationallows for the addition of more fine grain schemes e.g.
.the memory consumption costing scheme takes advantage ofjpf built in object life cycle listener mechanism to track theheap size of each path and associate this value with eachpath.
neither scheme takes into account the jit behavior orarchitectural details of the native cpu.
in our experience sofar these simple schemes has been quite effective but this is anarea that we have just begun to explore.
so we have designedour implementation for other estimators to be easily added.
forinstance a cost estimator related to the number of open filesin the system can be easily added by tracking open and closecalls on file apis and multiple measures can be combined byusing a weighted sum of individual measures.bounded symbolic execution.we run jpf configured toperform mixed symbolic execution.
when a phase of symbolicexecution finishes theselectstate function is invoked tocompute a subset of the frontier states for further exploration.in the current implementation the branch choices made alongthe paths leading to those states are externalized to a file.48then jpf is restarted using the recorded branch conditionsto guide execution up to the frontier and then resume itssearch towards until the next frontier.
this solution is efficientbecause the satisfiability of the path condition prefix is knownfrom the previous symbolic execution call thus jpf is simplydirected to execute a series of branches and no backtrackingis needed for those branches.
we note that we have exploredalternative mechanisms to avoid recording and replaying pathprefixes stored in files and to avoid restarting jpf.
we foundthat none was as efficient in scaling to large programs as thereplay approach described above.
this approach also has thedistinct advantage of allowing the exploration of the recordedpath prefixes to proceed in parallel which is a strategywe plan to pursue in future work.
we also use the replaymechanism to implement thestateafterreplay functionused in algorithm .diversity clustering.algorithm takes advantage of thejpf backtracking support to efficiently compute thegapvectoron the fly since each gap between two statessiandsi 1equals the number of branchessineeds to backtrack before itcan continue on a new path that leads tosi .test instantiation.generating tests from a path conditionrequires the ability to both solve and produce a model for agiven formula.
we have explored the use of several constraintand smt solvers including choco cvc3 and yices .
our integration of yices into jpf s symbc extension hasgreatly improved the scalability of symbc in general and ofour technique in particular.
the data reported in our studyuses this yices based implementation.v.
eva l uat i o nour assessment takes on multiple dimensions resources and artifacts.first we assess the approach configured to induce largeresponse times and compared it against random test generation.then we explore the scalability of various instantiations ofthe approach.
we do this in the context of jzlib 5633loc a java re implementation of the popular zlib package.this program is well suited for the study because we caneasily and incrementally increase the input size from 1kbto 100mb to investigate the scalability of the approach andresponse time is one of its two key performance evaluationcriteria.
moreover we can easily generate what the spec2000benchmark documentation defines as the worst load testinputs for jzlib random values to increase response timeand use those to compare against our approach.
last we canuse the zlib package as the differential oracle.second we assess an instantiation of the approach togenerate a suite of tests that follow a prescribed inputstructure with the goal of inducing high memory consumption.we conduct this assessment with sat4j loc which is well suited for the study in that memory consumptionis often a concern for such kind of applications.
furthermore the sat benchmarks that already aim to stress this typeof applications offer a high baseline against which to assessour approach.third we assess an instantiation of the approach to generatethree test suites one that causes large response times one thatcauses large memory consumption and one that strives for acompromise.
we use tinysql loc and its samplequeries and database as described earlier.
since the tests forthis artifact can be easily interpreted they are sql queries we perform a qualitative comparison on their diversity.throughout the study we use the same platform for allprograms a .
ghz intel core duo machine with macos x .
.
and 4gb memory.
we executed our tool onthe jvm .
with 2gb of memory.
we configure it to uselookahead 50as per the description in section iv andmaxpcsize 30000in order to keep within the capabilitiesof the various constraint solvers we used2.a.
revealing response time issuesour study considers two load test case generation techniquesas treatments cllg and random.
each test suite consistsof tests.
cllg is configured to target response time.we imposed a cap of hours for all the test generationstrategies.
strategies requiring more time were terminated andnot reported as part of the series.
for random test generationwe used the whole allocation of hours to simplify the studydesign which is conservative in that it may overestimate theeffectiveness of the random test suites in cases were thecllg suites for the same input size took less than hoursto generate.
for the random treatment we use a random bytegenerator to create the file streams to be expanded.
anotherconsideration with random is that unlike our approach withits built in selection mechanism it does not include a processto distinguish the best10load tests.
this is important asrandom can quickly generate millions of tests from whichonly a small percentage may be worth retaining as part of aload test suite.
to enable the identification of such tests wesimply run each test generated by random once and record itsexecution time.
this execution time although generally brief is included in the overall test case generation time.we first compare the response times of jzlib caused by testsgenerated with our two treatments.
we use zlib a programwith a similar functionality as part of a differential oraclethat defines a performance failure as responsetimejzlib responsetimezlib .
figure describes for input sizesranging from 100kb to 1mb the ratio in response timesbetween jzlib and zlib when a randomly generated suite isused light bars versus when inputs generated by cllg areused dark bars .
the values shown are averaged across theten tests .
we see that cllg generates inputs that revealgreater performance differences more than twice as large asrandom with the same level of effort.
it is also evident that depending on the choice of our approach could increasefault detection.
for example if sec then using randominputs will not reveal a performance fault with the input sizes2this number could be adjusted depending on the time available andthe selected constraint solvers.
as noted earlier however all solvers wouldeventually struggle to process an increasing number of constraints so selectingthis bound is necessary.49fig.
.
revealing performance issues response time differences of jzlib vszlib when using testing suites generated by cllg vs randombeing considered while the tests generate by cllg wouldreveal the fault with an input of .4mb or greater.scalability.to better understand the scalability of the ap proach and the impact of themaxsolverconstraintsboundon the effectiveness of cllg we increased the input size upto 100mb and usedmaxsolverconstraintsvalues of and .
we again imposed a cap of hours for all thetest generation strategies.
strategies requiring more time wereterminated and not reported as part of the series.the scalability results are presented in figure which plotsthe response times averaged across the tests in each suite.
thetrends confirms the previous observations.
the response timeof jzlib is several times greater for the test suites generatedwith cllg strategies than with those generated by random.this is more noticeable for cllg test suites with greatermaxsolverconstraints.
for example for an input of 10mb the suite generated with cllg had an average responsetime that was approximately five times larger than randomand two times larger than that with cllg .
however thisstrength came at the cost of scalability as the former strategycould not scale beyond 15mb.
we note similar trends for theother test suites generated with the cllg strategies whereeach eventually reached an input size that required more thanthe hour cap to generate the test cases.
only the cllg is able to scale to the more demanding inputs of up to100mb.
still the response time of jzlib under the test suitegenerated with this strategy is more than times greater thanthe one caused by the random test suite for approximatelythe same generation cost.
furthermore to generate a responsetime of seconds a randomly generated test would requireon average an input of more than 100mb while cllg 100would require a file smaller than 25mb.
more importantly thisfigure offers evidence of the approach configurability throughthemaxsolvercontraintsparameters to scale and yet itcan outperform an alternative technique that for this particularartifact is considered the worst case .b.
revealing memory consumption issueswe now proceed to assess test suites of tests generatedby our approach to induce high memory consumption insat4j.
we randomly chose benchmarks from the sat fig.
.
scaling response times of jzlib with test suites generated by cllgand randomtable iloadtestgeneration formemoryconsumptionprogramsdescriptionmemory mb variablesclauseshardnessorig.gen.aloul chnl11 132861742medium622cmu bmc longmult15780724351easy3592eq atree braun unsat6842300medium1135rbcl xits unsat98047620hard3845unif k3 r4.254802040medium617competition suite to get a pool of potential values forthe number of variables number of clauses and the numberof variables within each clause and we declare the variablesthemselves as symbolic.
columns to of table i showsome of the attributes of the selected benchmarks includingthe number of variables number of clauses and hardnesslevel assigned based on the results of past competitions withinstances solved within seconds by all solvers labelled as easy not solved by any solver within the timeout of theevent labelled as hard and medium for the rest.
table i shows the memory consumption results in the lasttwo columns.
column orig.
shows memory consumptionof sat4j when these benchmark programs are provided asinputs and column gen. shows the same measure for theaverage of the ten tests generated with our approach.
overall the approach was effective in increasing the memory load forsat4j compared with the original benchmarks.
however thegains are not uniform across instances.
the most dramaticgain achieved by our approach almost a 4x increase inmemory consumption comes from selecting values foraloulwhich is of medium hardness.
not surprisingly the leastsignificant gain comes fromrbcl xits unsatwhichis classified as hard .
but even for this input with a very largeand challenging space of over clauses the approachleads to the generation of a test suite that on average consumes20 more memory.c.
load inducing tests across resources and test diversitywe now assess the approach in the presence of differentperformances measures and in terms of the diversity of thetest suite it generates.we generate three test suites with tests each for tinysql.50table iiresponse time and memory consumption for test suitesdesigned to increase those performance measures in isolation ts rtandts mem and jointly ts rt mem .test suiteresponse timememory consumptionseconds over bas.mb over bas.baseline451006.6100ts rt10523413196ts mem9821715219ts rt mem9621313201table iiiqueries illustrating test suite diversityselect musictracks.trackname musictracks.trackidfrom musictracksjoin musiccollection onmusictracks.trackid musiccollection.trackidwhere musictracks.trackname null ormusictracks.trackid 0select musicartists.artstid musicartists.artstnamefrom musicartistsjoin musicevents onmusicartists.artstid nullwhere musicartists.artstname null ormusicartists.artstid 0select musicartists.artstname musicartists.artstcountryfrom musicartistsjoin musicartists onmusicartists.artstname musicartists.artstnamewhere musicartists.artstid ormusicartists.artstcountry nullthe first ts rt favors response time.
the second ts mem favors memory consumption.
the third ts rt mem was generated with an equally weighted sum of the cost forresponse time and memory consumption.
table ii shows theperformance caused by each test suite averaged across thefive tests.
we use as baseline the original test from which thetest template was derived.
we report both response time andmemory consumption along with their respective effectivenessover the baseline for each suite.the results show that all three test suites are effectiveat increasing their respective measures.
on average ts rtforces response times to rise over the baseline ts mem causes a increase over the baseline in terms ofmemory usage ts rt mem increases both response time by213 and memory by over the baseline.
by looking atts rt and ts mem is clear that favoring response time ormemory consumption has an effect on their counterpart.
asexpected the ts rt mem suite does in between the othertwo suites in terms of memory consumption.
what was abit surprising is that ts rt mem average response time waslower than ts mem.
although the difference is less than this indicates that when using combinations of performancemeasures special care must be taken to integrate the differentcosts schemes to account for potential interactions.test suite diversity.we now turn our attention to the issueof test suite diversity.
we note that there are no identicalqueries tinysql tests within each of the generatedtest suites.
furthermore all queries complete in differenttimes differences in tenths of seconds and consume differ ent memory differences in kb .
we illustrate some of thedifferences with the sample queries in table iii.
because ofspace constraints we only include of the generated querieswhich like all others have various degrees of difference.some obvious differences are in the fields selected tablesretrieved and the type of where clauses specifying the filteringconditions.
others are more subtle but still fundamental.
forexample while the first query in the figure is an inner join it will return rows with values from the two tables withmatching trackid the second one is a cross join it willreturn rows that combine each row from the first table witheach row from the second table and the third one is a self join joining content from rows in just one table.
although this isjust a preliminary qualitative evaluation it provides evidencethat the path diversity pursued by the approach translates intobehaviorally diverse tests.vi.
relatedworkthere are a large number of tools for supporting loadtesting activities some of which offer capabilities todefine an input specification e.g.
input ranges recorded inputsession values and to use those specifications to generate loadtests .
a common trait among these tools is that theyprovide limited support for selecting load inducing inputs asthey all treat the program as a black box.
the program isnot analyzed to determine what inputs would lead to higherloads.
similar trends appear in load testing techniques andprocesses in general as they use other sources of information e.g.
user profiles adaptive resource models to decidehow to induce a given load but still operating from a blackbox perspective.
one interesting exception proposed by yanget al.
.
conceptually the approach aims to assign loadsensitivity indexes to software modules based on their potentialto allocate memory and use that information to drive loadtesting.
our approach also considers program structure but akey difference in that instead of having to come up with staticindexes our approach explores the program systematically withthe support of symbolic execution to identify promising pathsthat we later instantiate as tests.a second thread of related work targets improvements tothe performance scalability and applicability of symbolicexecution more specifically many efforts have studied howto reduce guide and enrich the space explored by symbolicexecution .
our approach takes advantage of some of theseadvances particularly on the use of mixed symbolic execution but is unique in its application to load testing which requiresdifferent search heuristics and exposes different tradeoffs andscalability issues.our work is also related to research efforts aimed atcharacterizing the computational complexity of a programas load testing often tries to expose worst case scenarios.goldsmidth et al.
propose an approach that given a setof loads executes the program under those loads groupscode blocks of similar performance and applies various fitfunctions to the resulting data to summarize the complexity .
critical to the performance of this approach is the userprovided workloads.
gulwani et al.
take a static approach .their approach instruments a program with counters uses an51invariant generator to compute bounds on these counters andcomposes individual bounds together to estimate the upperbound of loop iterations.
this approach relies on the powerof the invariant generator and the user input of quantitativefunctions to bound any type of data structures.the last piece of related work is the closest to our approachin that it uses symbolic execution to identify a worst casescenario .
our approach is different in two significantrespects.
first our goal is develop a suite of diverse tests notjust identifying the worst case.
this requires the incorporationof additional mechanisms and criteria to capture distinct pathsthat contribute to a diverse test suite and of a family ofperformance estimators that can be associated with programpaths.
second the approach is different.
burmin s approachutilizes full symbolic execution on small data sizes and thenattempts to generalize the worst case complexity from thosesmall scenarios.
this works well when the user can provide abranch policy indicating what branches or branch sequencesshould be taken or not in order to generalize the worst casefrom small examples and the authors show sample programswhere that is the case.
however for programs like the ones westudied defining even reasonable branch policies that merelyapproximate the worst case scenario much less a test suite itwould require an extremely good understanding of the programbehavior and even then it would be challenging.
our approachis different in that we perform an incremental symbolicalexecution favoring the deeper exploration of a subset ofthe paths associated with code structures.
this removes therequirement for a user provided generator.vii.
conclusionload testing can assist in the detection of performanceanomalies and many tools and approaches exist to supportthe test engineer in load test suite development.
most of thoseefforts treat the program as a black box focusing on increasingload by providing larger input sizes.
as we have shown however size is not all that matters.
the selection of the rightcombination of input values can deliver an equivalent loadwith smaller input sizes which can reduce testing infrastructurerequirements can provide a more accurate characterization ofscenarios where the system behaves poorly can cover a rangeof resources and may be helpful to identify anomalies thatare exposed when traversing different execution paths.yet identifying such inputs can be extremely challengingsince it requires an understanding of the program internals.to address this challenge for smarter input selection wedeveloped slg an approach that performs a focused formof symbolic execution utilizing iterative deepening and beamsearch strategies on portions of the system with the aim ofdiscovering execution paths that contribute to high programloads while ensuring path diversity.
our implementation3andassessment of slg shows that it can induce program loadsacross different types of resources that are significantly betterthan alternative approaches randomly generated tests in the3source code is available at study a standard benchmark in the second study andthe default suite in the third study .
furthermore we provideevidence that the approach scales to inputs of large size andcomplexity and produces functionally diverse test suites.acknowledgmentsthis material is based in part upon work supported by nsfaward ccf and by afosr award .any opinions findings and conclusions or recommendationsexpressed in this material are those of the author s and donot necessarily reflect the views of afosr or nsf.
we thankw.
visser and v. cortellessa for providing feedback on earlierversions of this work.