measuring the structural complexity of feature models richard pohl vanessa stricker and klaus pohl paluno the ruhr institute for software technology university of duisburg essen gerlingstr.
essen germany richard.pohl vanessa.stricker klaus.pohl paluno.uni due.de abstract the automated analysis of feature models fm is based on sat bdd and csp known np complete problems.
therefore the analysis could have an exponential worst case execution time.
however for many practical relevant analysis cases state of the art sota analysis tools quite successfully master the problem of exponential worst case execution time based on heuristics.
so far however very little is known about the structure of fms that cause the cases in which the execution time hardness for analyzing a given fm increases unpredictably for sota analysis tools.
in this paper we propose to use width measures from graph theory to characterize the structural complexity of fms as a basis for an estimation of the hardness of analysis operations on fms with sota analysis tools.
we present an experiment that we use to analyze the reasonability of graph width measures as metric for the structural complexity of fms and the hardness of fm analysis.
such a complexity metric can be used as a basis for a unified method to systematically improve sota analysis tools.
index terms software product line feature model automated analysis performance measurement i. i ntroduction software product line engineering sple leverages systematic and proactive reuse across several similar software products.
feature models fm are a common way to document commonality and variability between products of software product lines spl .
the automated analysis of fms is concerned with extracting information and detecting inconsistencies of these fms.
examples for analysis operations on fms are determining whether valid products can be configured from an fm or finding dead features .
a. problem statement the complexity of analyzing fms for validity is generally known as an np complete problem .
current analysis techniques are mostly based on reducing the problem of automated analysis of fms to a more general computational problem e.g.
sat or csp.
this implies exponential worst case execution time that is however mastered quite successfully for many practical relevant analysis cases by state of the art sota analysis tools based on the application of heuristics.
current research focuses on maturing and improving these heuristics .
since heuristics used to solve np hard problems are highly dependent on the problem structure this calls for an understanding of the relation between the modeling constructs of fms and the complexity of analysis operations on them.despite a constant evolution of the heuristics experiments have shown that for some models the execution time of the analysis particularly increases unpredictably on some models i.e.
there are models that are hard to analyze.
while hardness can have different causes in practice e.g.
memory consumption required user interactions we limit our work in this paper to hardness in terms of execution time.
even with this restriction apart from knowledge about the theoretical worst case complexity little is known about the structures and the respective modeling constructs in fms that cause a model to be hard to analyze for sota analysis tools .
thus improving the efficiency of fm analysis is an open research issue .
related work on the hardness of fm analysis operations addresses the relation between the structural complexity of the models and the time needed for the analysis with two different approaches.
on the one hand theoretical considerations reveal more approximate estimations of the worst case time complexity for certain structural characteristics.
these theoretical considerations are made for general sat and csp problems but also on the level of fm analysis.
it is for example known that an fm containing only mandatory and optional features is always satisfiable and thus can be searched in linear time.
on the other hand several empirical experiments have been conducted collecting data about the performance of sota analysis tools beyond the knowledge of the worst case complexity.
these studies analyze the execution time with respect to different complexity measures.
these measures all have in common that they consider only one characteristic or a subset of characteristics of the analyzed models e.g.
the size of the fm or the ratio of fm constraints.
these measures however are not able to cover all cases in which the execution time of a solver increases unpredictably.
instead the complexity of the analysis of fms depends on the combination of all modeling concepts of the analyzed fm i.e.
considering the full semantics of the structure.
thus the current situation is lacking a comprehensive theory of the structural complexity of fms that allows drawing conclusions about the hardness of analysis operations.
filling this gap would contribute to build heuristics that achieve a reliable tool performance.
a solid understanding of the structure causing the hardness called the structural complexity of fms in the following will allow developing improved heuristics.
.
c ieee ase palo alto usa454 b. contribution of this paper in this paper we propose to use width measures from graph theory as metrics for the structural complexity of and thus for the hardness of analysis operations on fms.
graph width measures were originally defined with the goal of determining the hardness of analysis operations on graphs.
while some successful general applications of graph width measures to sat and csp exist currently there is no application for the special case of fm analysis.
graph width measures usually provide an upper bound for the worst case time complexity of a graph operation.
thus while they are generally reasonable for sat and csp there are particular problem instances that are much easier than indicated by the measure.
moreover there are sets of models for which a graph width measure does not differentiate i.e.
it delivers the same value for all problem instances.
according to this the primary objective of this paper is to analyze whether graph width measures are reasonable metrics to characterize the structural complexity of fms.
this boils down to the following research question rq are graph width measures reasonable metrics to differentiate easy from hard fm analysis cases?
c. approach according to the defined problem we aim at analyzing the reasonability of graph width measures as metrics for the complexity of the automated analysis of fms.
in general a complexity metric can be considered to be reasonable if it differentiates hard from easy cases .
we define two quality criteria in order to evaluate the reasonability as the graph width measure is an upper bound for the complexity of the analysis the metric should differentiate between models of different hardness classes i.e.
it does not deliver an equal value for all models to be analyzed.
the metric should not deliver values that are unreasonably above the actual complexity for many models i.e.
the majority of models is not significantly easier than predicted by the metric.
we prove criterion by computing the measures on a test set of models from the betty generator .
criterion is addressed by an experiment that aims at finding significant correlations between the execution time of sota analysis tools for the valid operation and the structural complexity metric.
we consider the validity check or valid operation as the most fundamental analysis operation.
it checks whether at least one product can be configured according to a feature model without violating any feature model constraints.
a variety of other operations on fms can be implemented by decomposing them into several calls to the valid operation see e.g.
.
the structure of this paper is as follows.
section ii presents related work on the complexity of fm analysis.
section iii summarizes the foundations of the automated analysis of feature models graph width measures and their relation.
in section iv an experimental study of applying graph width measures to fm analysis is described.
section v presents conclusions from our work.ii.
r elated work the related work on the complexity of fm analysis based on their structure can be divided into theoretical and experimental studies.
theoretical complexity analysis a survey on different representations of fms including theoretical complexity results is presented by schobbens et al.
.
they reveal that checking fm validity in general is np complete.
mendon a et al.
map fm constructs to their representation in pl in order to evaluate the complexity of solving the sat instance.
they conduct an experimental analysis of the complexity of the solving process based on published realistic fms.
despite potential hardness they found that sat based analysis is easy on most models that occur in practice.
gil et al.
analyze the performance of several analysis operations on tree like fms i.e.
models without cross tree constraints showing that these models are comparably easy to analyze.
for the rare hard cases that were revealed on realistic fms however the origin of hardness was not yet traced back to concrete structural elements of the model.
tuikys and dama evi cius propose three structural complexity measures for fms based on model complexity research.
their aim is to use the complexity in order to improve the quality of the design and the productivity of the design process.
their measures cover the cognitive complexity of an fm that is the complexity of understanding the model from a human perspective and the structural complexity as a high level measure for the complexity of verifying a model without stating analysis operations to which this measure applies.
bagheri and gasevic propose the use of structural complexity measures for assessing the maintainability of feature models.
their measures are based on properties of feature models e.g.
percentage of features involved in constraints and do not necessarily reflect the hardness of analysis operations.
experimental performance studies benavides et al.
present a comparison between java based sat bdd and csp solvers focusing on two sample operations.
their experiment used three different solvers and was run on randomly generated models with up to features.
they conclude that the bddbased solver shows a lower execution time increase on larger models on the more complex operation.
furthermore they revealed an exponential growth in memory usage for the bddbased solver on larger models.
in all products of an fm are computed using a csp solver.
the authors describe an experiment with models containing up to features.
the results show an exponential growth in time in particular on highly variable models i.e.
models with few constraints.
mendon a examines the execution time of sat bdd and csp based tools on basic operations and proposes heuristics to reduce bdd size based on randomly generated models and models from literature.
the studies are designed to ensure that the models have certain structural properties e.g.
a fixed ratio of constraints and features .
the result is that sat solvers handle fm problems easily while bdd based solvers suffer from intractability issues on large models.
segura et al.
455present betty a framework for benchmarking and testing in the automated analysis of fms.
their framework provides a generator for fms that can be driven either by random or by an evolutionary algorithm that maximizes properties of the analysis e.g.
execution time.
thus deliberately hard models can be generated.
moreover test data for functional tests of analysis tools can be generated.
in their later work segura et al.
propose the use of hardness metrics for the underlying solvers to measure the complexity of feature models.
these metrics include e.g.
the number of backtracks of a csp solver or the number of decisions of a sat solver.
while these metrics are deterministic and eliminate threats to validity induced by time measurements they currently do not consider structural properties of fms.
pohl et al.
describe an experiment on fms from splot.
an analysis on the effect of the choice of the solver and operation on the execution time is presented proving statistically significant differences between different solvers operations models with a different number of products in the modeled product line and models with a different number of features.
an analysis of correlations between graph parameters e.g.
cyclomatic number and the execution time of the analysis is described.
while the work prior to was performed on java solvers pohl et al.
introduce c solvers and show that there is a considerable difference between java and c solvers regarding their execution time.
these are closer to the current state of the practice in solver technology as shown by their participation and awards in solver competitions.
sayyad et al.
discuss using multi objective optimization techniques for the automated analysis of feature models.
this allows choosing correct models that are optimal with respect to specific goals e.g.
minimum number of features .
this adds more complexity to the original problem of finding correct models.
their paper describes an experimental performance study that compares several techniques for multi objective optimization.
the multi objective scenario in their work is a generalization of the consistency check analyzed in this paper so it could be basically used to find consistent solutions although the techniques used there are not optimized to perform this task.
iii.
f eature model analysis and graph width we briefly outline the foundations of the automated analysis of feature models and present graph width measures from graph theory.
we propose using graph width measures as metrics for the structural complexity of fms.
the measures are based on structural characteristics of graphs .
a. automated analysis of feature models technically the automated analysis of fms follows a twostep approach.
figure shows the dependencies between both steps on the example of our experiment described in section iv.
in the first step the fm is encoded in a formal representation.
to ease automated reasoning general purpose formal encodings e.g.
cnf or csp are used .
in the second step tools for reasoning on this formal encoding e.g.
a sat solver are step a b c libtw parallel binary counter encoding csp direct encoding cnf cnf csp twg 1 twg 2 twg 3 twi 1 twi 2 twi 3 lingeling clasp sat4j picosat buddy cudd javabdd choco cream jacop mistral order encoding step graph based complexity analysis result tool output tree width measures fm tool input bdd sat csp fig.
.
overview of the experiment used to compute properties of the fm.
thus different analysis alternatives can be used by combining the different formalisms for encoding fms and tools for reasoning on them.
depending on the structure of the model and the property to be computed the alternatives differ in the efficiency of the computation.
to realize the first step features are usually represented as variables over a two valued domain so that one value of the domain corresponds to feature selection and the other value to feature exclusion.
constraints on these variables are defined in a way that they are satisfied only for all valid products.
two most common techniques are the encoding of an fm as a formula in propositional logic pl for all fm constructs except for cardinality constraints over feature groups a fixed set of clauses in conjunctive normal form cnf can be used to encode the modeling construct.
cnf facilitates automated processing since most tools expect their input in cnf .
encoding group cardinalities in pl is not trivial .
the size of the formula for a cardinality constraint depends on the minimum and maximum values.
the propositional formula can be used to form an instance of the boolean satisfiability problem sat .
the encoding of an fm using constraint programming cp over integer domains integer domains facilitate the encoding of cardinality constraints over feature groups.
minimum and maximum values of the cardinality constraint can be encoded in a straightforward way.
constraints over the sum of variables corresponding to the features in the group are defined.
the set of constraints456corresponding to the fm can be used to form an instance of the constraint satisfaction problem csp .
solving the problems sat and csp realizes step of an implementation of the valid operation on an fm.
for this a variety of tools can be used.
examples for these include sat and csp solvers but also bdd based tools or satisfiability modulo theory smt solvers.
smt solvers are extended sat solvers that can be used to solve problems in pl and problems with integer variables.
for the experiment in this paper we focus on the most common tools for feature model analysis which are bdd sat and csp based tools .
while smt solvers could show a superior performance to other tools to achieve their optimal performance on feature models featurespecific theories would have to be defined and evaluated first.
despite the difficulty of encoding cardinality constraints the first step is tractable .
the complexity of the second step depends on the choice of analysis operations.
b. application of graph width measures to sat and csp several studies propose the use of graph width measures to measure the complexity of sat or csp instances.
they discuss the reasonability on a theoretical level by proving that problems with a bounded value in these measures can be solved efficiently while other problems remain potentially hard.
because of their general applicability we suggest to apply graph width measures to fm formalisms.
tree width clique width and rank width provide appropriate measures for fms since sat csp and the problem of counting the number of satisfying assignments to a sat instance sat are tractable on bounded measures .
the counting version of csp csp is also tractable on bounded tree width .
while clique width and rank width are definitely promising from a theoretical point of view tree width is the only measure for which current sota algorithms are sufficiently efficient for practical use.
thus for our experimental study we are limited to using tree width.
informally tree width is a metric to describe how far the structure of a graph resembles a tree.
it is applied to many computationally hard e.g.
np complete problems that are tractable on graphs with a bounded tree width.
tree width is a measure for the complexity of analysis operations on undirected graphs.
it has applications to the most basic and generic graph problems that are problems in searching and traversing a graph .
the widest class of efficiently solvable problems on graphs with bounded tree width is given by expressions in monadic second order logic over vertices sets of vertices edges and sets of edges mso expressions .
c. application of graph width measures to fms a feature model encoded as a sat instance in conjunctive normal form cnf or as a csp can be represented as a graph.
more specifically the formal encodings cnf and csp of an fm have a hypergraph representation.
as graph width measures were originally defined for graphs instead of hypergraphs this hypergraph needs to be represented as a graph.
we use two common forms to represent the hypergraph of the fmformalization i.e.
cnf or csp as a graph that are called the gaifman graph and the incidence graph see e.g.
for a definition of this hypergraph.
this way of applying tree width defines a structural complexity metric for an fm depending on the encoding produced in step of the analysis process .
the idea of using these graph representations to extend the application of tree width for hypergraphs is not new.
marx et al.
refer to hypertree width as the tree width of the gaifman graph of a hypergraph.
they also describe applications of hypertree width to the structure of sat and csp instances.
according to the results from the work in the sat and csp communities graph width measures in general can be applied to cnf and csp hypergraphs.
this way we can apply graph width measures to fms.
to describe our experiments on tree width we use the following short notation twgdenotes the tree width of the gaifman graph hypertree width twidenotes the tree width of the incidence graph.
for a feature model m we use p m for its pl cnf formalization and c m for its csp formalization.
thus for example twi c m denotes the tree width of the incidence graph of the cnf formalization of an fm m. iv.
e xperimental study we propose a systematic experimental study on tools that can be used for the automated analysis of fm.
for this purpose we implemented our comparison framework for feature model analysis performance coffemap1 that provides these sota analysis tools with a unified infrastructure for standard tasks like model parsing displaying the results etc.
as input for the experiment we use fms generated from the betty generator created by segura et al.
.
these samples constitute models with constant size but different structural complexity in terms of the proposed tree width measures.
according to the research question we define the following hypothesis.
h there is a statistically significant dependency between the structural complexity in terms of tree width of a formally encoded fm and the time needed to analyze the model for validity.
figure gives an overview of the experimental setup that is described in the following subsections in detail.
a. tool selection we selected the following representative set of encodings and analysis tools for the experiment.
step formal encoding of feature models benavides et al.
make a straightforward proposal for a direct cp encoding of fms that is used in our framework.
the cnf case is more difficult because expressing cardinality constraints within pl is not trivial.
in the literature different encodings are proposed with different implications .
some encodings produce cnf instances that are easy to solve that is at the cost of potentially making these instances very large.
other encodings produce small cnf instances but at the cost of potential computational hardness.
as both space consumption computational hardness can lead to intractability we chose two cnf encodings for our experimental study.
first we consider the order encoding known from the sugar solver.
while we use the order encoding for cardinality constraints the order encoding can also be used to encode general csp instances to cnf.
in this context it has proven to preserve many structural properties of tractable problems see for details .
moreover in connection with a sat solver either picosat or minisat it won about half of the categories in several years of the cpai csp competitions2.
thus we consider it to produce easy to solve problem instances.
second we consider theparallel binary counter ltn k par encoding as it has proven to produce cnf instances of minimal size .
while we provide the direct csp encoding and the parallel binary counter encoding as part of our framework we use an existing implementation for the order encoding from the sugar solver.
this implementation is fed with a cp representation of the fm and encodes it to cnf.
consequently we actually conduct two encoding steps in sequence to realize the order encoding.
as all of the encodings we use run in linear time we decide not to measure the time needed for the encoding of the model so the use of several encodings in sequence has no influence on the measured execution time.
step solving computational problems this step is conducted using several sota solvers.
to find the best performing solver implementations we referred to the last years of the sat competition and cpai csp competition3.
for practical reasons we include solvers from the first three ranks of at least one track of the competition that provide either a java or a c c interface.
moreover we also include the solvers from the fama framework4for the analysis of fms because of their common use in the spl community.
table i gives an overview of all solvers.
this leads to combinations of encodings and analysis tools in our experiment the csp solvers can only be used on the direct csp encoding while the sat solvers can be used on the ltn k par encoding and on the order encoding.
as sugar was designed for the use with sat solvers we do not include the theoretically possible combination of sugar with bdd solvers in the experiment.
as indicators for the solving performance of the valid operation we measure the execution time of the solver step of the analysis process .
figure shows the basic experimental setup with the encoders and solvers used.
for our data analysis we implemented an application to compute the tree width of the formal encodings of an fm based on libtw5.
due to the computational hardness of determining an exact tree width value we used approximation algorithms that delivered lower and upper bounds for each tree width measure.
3competitions from and were not reviewed because experiments were started in i overview of solvers for step 2of the experiment name reason for inclusion buddy1bdd literature cudd2bdd literature javabdd3fama lingeling4sat comp.
clasp5sat comp.
sat4j6fama picosat7sat comp.
cpai comp.
sugar choco8fama cream9cpai comp.
sugar jacop10fama mistral11cpai comp.
b. experiment design according to the research question the goal of the experiment is to reveal dependencies between the different tree width measures of an fm and the time needed to analyze the fm for validity.
since it is known that for the valid operation there is a relationship between the model size number of features and the execution time of the analysis we keep the number of features in all models constant.
our previous experiments ran on models of up to about features.
these previous experiments showed that these models were easy to check for validity.
furthermore industry scale models can easily contain several thousand features .
due to these two reasons we decided to increase the number of features and use models with and features.
the models generated by betty contain only groups with and cardinalities.
this is a common practice in existing performance studies on feature model analysis .
we modified the set of models by introducing random cardinalities of type that are established in current fm languages e.g.
omg s common variability language cvl .
the independent variable that is varied throughout the experiment is the fm structure measured in terms of graph width measures on the fm encodings.
more precisely these are the metrics twg pp m twi pp m twg po m twi po m twg c m and twi c m where ppdenotes the parallel binary counter cnf encoding and podenotes the order encoding.
the dependent variable we measure that represents the performance of the analysis is time consumed by the solver .
we conducted three experiments using different sets of models in the following configurations of features per model jfj groups with cardinalities jfj groups with and cardinalities jfj groups with cardinalities these configurations allow us to reveal differences between different model sizes but also between models with different types of cardinality constraints.
c. experiment execution our experimental environment included four state of the art computers with an intel core i7 cpu and gb ram.
as an operating system we used ubuntu linux .
lts bit.
to minimize confounding factors and to ease replication with the same environment on all computers we used a live cd to boot the pcs.
we set a timeout of seconds and a memory limit of gb.
to avoid slowing down computations of the java applications by extensive garbage collection before running out of memory we set the jvm heap size to .
times the memory limit.
the application including the jvm restarted for each tool combination and model.
when reaching either the memory limit or the timeout the type of error was saved.
otherwise on completion the time for solving the fm and the peak memory were saved.
d. threats to validity concerning construct validity there is a risk that better measures than the graph width measures might exist.
although this would impair the quality of our results the results of the current experiment provide a contribution to the community as it is the first to consider the full semantics of fms in a structural complexity measure.
correlations between graph width measures and execution time with no true causal relationship could furthermore threaten theinternal validity .
in particular confounders that could cause such a false conclusion are all factors that increase the measured execution time beyond the time needed by a sota analysis tool for solving the fm.
most critical is the decision of using sugar as an encoder since the tools using sugar contain several steps that increase the execution time due to its implementation.
first a cnf file in dimacs format is written and one of the participating sat solvers is called for solving the cnf.
the time for solving the cnf is measured.
the other encoders are directly linked to our framework allowing to call the solver directly via the corresponding c or java api and pass them a data structure containing the formalized fm as parameter.
obviously writing and reading a file as well as the initialization of the new process demand additional computational resources.
moreover sugar uses solvers that are implemented in c that are known to perform significantly faster than comparable java implementations .
generally these factors infer the ability to trace back a possible correlation to the solution procedure rather than to implementation aspects.
nevertheless we are confident that the confounders are not too strong since sugar participated in the cpai competitions in the same configuration with verygood results outperforming solvers with a more traditional configuration in several categories.
to avoid differences between the used computers and random performance deviations we distributed equal numbers of models within a specific range of tree width of the formalizations across the different machines and executed them in random order.
our sample includes several fms with approximately equal graph width that further minimizes the effect of outliers on the results.
concerning conclusion validity in particular the time measurement measurements were done before and after calls to solvers.
we measured wall clock time that is common for time measurement e.g.
in the sat competition .
as discussed by luque et al.
wall clock time increases with the number of tasks in a system.
we addressed this issue by using a minimal configuration of the operating system that keeps the number of additional tasks run by the system constant and as low as possible.
for the time measurement we used an own implementation as part of our framework instead of solverinternal time measurements to ensure that the measurement is done equally for all solvers.
as far as our sample of models is concerned we used artificially generated models that source from betty a generator that is in common use in research work in the area.
we used the original models as well as an enhanced version of these models that contain random cardinality constraints to represent realistic models.
another aspect that needs to be discussed carefully is the external validity .
for easy integration within our framework we call the participating bdd solvers in the same way as the sat solvers neglecting feature model specific advances like the bdd heuristics from mendon a .
this lead to out of memory problems with our solvers.
thus our results are not necessarily generalizable for the bdd based analysis of fm.
despite this we cover a wide variety of models and a wide set of sota analysis tools.
thus further evaluation studies in various environments have to be conducted to determine their effect in different practical situations e.g.
in applications that use the automated analysis of fms in modeling environments in the context of industrial projects.
e. data analysis quality criterion from section i c is fulfilled by the distribution of the tree width measures in our experiments as shown in tables ii iv.
the tables show the number of models with a tree width value in a given interval.
for example table ii shows in row that for twg pp short for the upper bound of the gaifman tree width of the parallel binary counter encoding pp there are models for which the computed value is within column models with a tree width in etc.
because the lower and upper bound approximations delivered by the tree width approximation algorithm differ slightly we computed all correlations for the set of lower and the set of upper bounds separately.
we use x to refer to the lower bound x to refer to the upper bound.
because of the complexity of computing the incidence graph for the models with 1000459table ii distribution of twgmeasures over jfj models twg pp twg pp twg po twg po twg c twg c table iii distribution of twimeasures over jfj models twi pp twi pp twi po twi po twi c twi c features we were restricted to conduct the analysis on the gaifman graph for these models.
to analyze quality criterion we used the solver specific execution times in addition to the tree width measures to conduct a spearman s rank correlation analysis see for details .
we applied a post hoc significance test to the resulting correlation coefficient.
spearman s rank correlation is nonparametric and thus we do not need to make assumptions on the distribution of the base data e.g.
normality .
a general observation on the solver execution times of the bdd based solvers is that these solvers were incapable of processing models with cardinality constraints as they ran out of memory.
this might be due to our decision to connect bdd solvers directly to the ltn k par encoder without using bdd specific heuristics as described in the previous section.
tables v xiii describe significant correlations between graph width measures and solver execution time revealed by the post hoc significance test at the significance level.
in the four columns correlations are shown for the lower and upper bound heuristics of the tree width applied to the gaifman twg and the incidence graph twi .
the results of each experiment are grouped according to the analyzed encoding.
we describe the correlation between the tree width of the analyzed encoding and the execution time of the solvers that are run on this particular type of encoding.
these are highlighted grey in the following tables.
in addition for each encoding we describe correlations between the tree width of the encoding and the execution time of the solvers on the according models that are not being run on the encoding but that were run on the other encodings .
this allows us to evaluate the suitability of measures to approximate the structural complexity of more than one type of encoding.
experiment jfj cardinalities a results for the parallel binary counter pp encoding table v the correlation for twgis highest between .
and .
for all solvers that process the ltn k par encoding but is also within the same interval for cream.
for twi lowertable iv distribution of twgmeasures over jfj models twg pp twg pp twg po twg po twg c twg c and upper bounds differ slightly on some solvers and are at about 7for bdd based solvers and for cream.
b results for the order po encoding table vi fortwg only the lower bound shows useful correlations.
the correlation is .
or higher on 3of all solvers.
for lingeling and cream the correlation is above .
fortwi the correlations on the lower bound are higher than on the upper bound with of all solvers above .
and 3above .
c results for the cp c encoding table vii fortwg the lower bound shows stronger correlations.
the correlation is .
or higher on 3of all solvers.
for lingeling and cream the correlation is above .
for twi the correlations on the lower bound are higher than on the upper bound with of all solvers above .
and 3above .
experiment jfj cardinalities a results for the parallel binary counter encoding pp table viii noticeable differences to the first experiment are that on some solvers the correlations for most solvers are weaker in particular for twi.
b results for the order po encoding table ix it is noticeable that while tw po m shows relatively strong correlations on both graph representations in the first experiment for choco cream and jacop the correlations for these solvers decrease in the second experiment.
c results for the cp c encoding table x it is noticeable that the correlations for most solvers remain at the same level or slightly decrease compared to the first experiment.
however the correlations for cream and jacop show a noticeable drop.
experiment jfj cardinalities tables showing the results of this experiment show only the two columns with correlations for tw gandtw g because as mentioned before the incidence graph twiwas not computed on thejfj models.
a results for the parallel binary counter pp encoding table xi the correlations are at approximately the same level as in the first experiment with a slight decrease on some solvers in particular on the lower bound of twg.
for some solvers in particular picosat there is a noticeable decrease of the correlation compared to the first experiment.
b results for the order po encoding table xii while for buddy cudd javabdd and sat4j there are only slight differences the correlation for the other solvers is noticeably weaker on the third experiment.
c results for the cp c encoding table xiii it is noticeable that correlations drop compared to the first460table v significant correlations for experiment encoding pp m corr tw g t corr tw g t corr tw i t corr tw i t buddy .
.
.
.
cudd .
.
.
.
javabdd .
.
.
.
clasp lt par n k enc.
.
.
.
.
lingeling ltpar n k enc.
.
.
.
.
picosat ltpar n k enc.
.
.
.
.
sat4j ltpar n k enc.
.
.
.
.
clasp order enc.
.
.
.
.
lingeling order enc.
.
.
.
.
picosat order enc.
.
.
.
.
sat4j order enc.
.
.
.
.
choco .
.
.
.
cream .
.
.
.
jacop .
.
.
.
mistral .
.
.
table vi significant correlations for experiment encoding po m corr tw g t corr tw g t corr tw i t corr tw i t buddy .
.
.
cudd .
.
.
javabdd .
.
.
clasp lt par n k enc.
.
.
.
lingeling ltpar n k enc.
.
.
.
picosat ltpar n k enc.
.
.
.
.
sat4j ltpar n k enc.
.
.
.
clasp order enc.
.
.
.
.
lingeling order enc.
.
.
.
.
picosat order enc.
.
.
.
.
sat4j order enc.
.
.
.
.
choco .
.
.
cream .
.
.
jacop .
.
.
mistral .
.
.
.
table vii significant correlations for experiment encoding c m corr tw g t corr tw g t corr tw i t corr tw i t buddy .
.
.
.
cudd .
.
.
.
javabdd .
.
.
.
clasp lt par n k enc.
.
.
lingeling ltpar n k enc.
.
.
.
.
picosat ltpar n k enc.
.
.
.
sat4j ltpar n k enc.
.
.
.
.
clasp order enc.
.
.
.
lingeling order enc.
.
.
.
.
picosat order enc.
.
.
.
sat4j order enc.
.
.
choco .
.
cream .
.
.
.
jacop .
.
.
.
mistral .
.
table viii significant correlations for experiment encoding pp m corr tw g t corr tw g t corr tw i t corr tw i t buddy .
.
.
.
cudd .
.
.
.
javabdd .
.
.
.
clasp lt par n k enc.
.
.
.
.
lingeling ltpar n k enc.
.
.
.
.
picosat ltpar n k enc.
.
.
.
.
sat4j ltpar n k enc.
.
.
.
.
clasp order enc.
.
.
.
.
lingeling order enc.
.
.
.
.
picosat order enc.
.
.
.
.
sat4j order enc.
.
.
.
.
choco .
.
cream .
.
.
.
jacop .
.
mistral table ix significant correlations for experiment encoding po m corr tw g t corr tw g t corr tw i t corr tw i t buddy .
.
.
cudd .
.
.
javabdd .
.
.
clasp lt par n k enc.
.
.
.
lingeling ltpar n k enc.
.
.
.
picosat ltpar n k enc.
.
.
.
.
sat4j ltpar n k enc.
.
.
.
clasp order enc.
.
.
.
.
lingeling order enc.
.
.
.
.
picosat order enc.
.
.
.
.
sat4j order enc.
.
.
.
.
choco .
.
.
cream .
.
.
jacop .
.
.
mistral .
.
.
.
table x significant correlations for experiment encoding c m corr tw g t corr tw g t corr tw i t corr tw i t buddy .
.
.
.
cudd .
.
.
.
javabdd .
.
.
.
clasp lt par n k enc.
.
.
.
.
lingeling ltpar n k enc.
.
.
.
picosat ltpar n k enc.
.
.
.
sat4j ltpar n k enc.
.
.
.
.
clasp order enc.
.
.
lingeling order enc.
.
.
.
.
picosat order enc.
.
.
.
sat4j order enc.
.
.
choco .
.
.
cream .
.
.
.
jacop .
.
.
mistral .
table xi significant correlations for experiment encoding pp m corr tw g t corr tw g t buddy .
.
cudd .
.
javabdd .
.
clasp lt par n k enc.
.
.
lingeling ltpar n k enc.
.
.
picosat ltpar n k enc.
.
.
sat4j ltpar n k enc.
.
.
clasp order enc.
.
.
lingeling order enc.
.
.
picosat order enc.
.
.
sat4j order enc.
.
.
choco cream .
.
jacop .
.
mistral .
table xii significant correlations for experiment encoding po m corr tw g t corr tw g t buddy .
cudd .
javabdd .
clasp lt par n k enc.
.
lingeling ltpar n k enc.
.
.
picosat ltpar n k enc.
.
sat4j ltpar n k enc.
.
.
clasp order enc.
.
lingeling order enc.
.
picosat order enc.
.
sat4j order enc.
.
choco .
.
cream .
.
jacop .
mistral .
.249461table xiii significant correlations for experiment encoding c m corr tw g t corr tw g t buddy .
.
cudd .
.
javabdd .
.
clasp lt par n k enc.
.
.
lingeling ltpar n k enc.
.
.
picosat ltpar n k enc.
.
.
sat4j ltpar n k enc.
.
.
clasp order enc.
.
.
lingeling order enc.
.
.
picosat order enc.
.
.
sat4j order enc.
.
.
choco cream .
.
jacop .
.
mistral .
experiment for clasp lingeling picosat and jacop.
for the first two the drop is especially on the upper bound.
f .
interpretation summarizing the results of experiment there is a significant correlation between at least one tree width measure twgor twi and execution time in all cases.
unexpectedly for tools processing the ltn k par encoding thus actually belonging to the pp m category the correlation for tw po m referring to both twgandtwi is higher than the correlation for tw pp m on some solvers.
however the correlation for the sugar based solvers and tw pp m is not significant.
comparing the correlations for the tree width of the encodings that were used on a particular solver in the experiment highlighted values in tables v xiii with the tree width of the encodings that were not used on this solver the latter are in most cases weaker than the former.
the difference between these correlations is in most cases higher on twithan on twg.
there are some cases see e.g.
table vi where only correlations for twishow this difference.
from this it can be concluded that using the incidence graph instead of the gaifman graph is more useful for analyzing the correlation for the encoding that is particular to the analysis tool that was used e.g.
the correlation for tw pp m onltn k parbased solvers compared to using it as a general complexity metric.
this conforms to graph theory as the incidence graph is a more structure preserving representation of the original hypergraph than the gaifman graph.
this is because the gaifman graph abstracts from the hyperedges leading to more similar hypergraphs for the different encodings.
another noticeable observation is the great difference between the correlations for tw po m andtw po m see tables vi ix and xii.
compared to this the difference between the lower and upper bound values for pp m and c m see other tables is very small.
in particular while twi po m produces significant correlations for ltn k parbased tools see tables vi where the correlations for lingeling and picosat are above .
but also tables ix and xii the correlations for tw po m are much weaker in all of the cases.
the difference in the correlations is caused by deviations in the computed tree width values due to the computationalhardness of computing tw po .
interestingly the revealed correlations indicate that in particular the lower bound approximation for twg po m constitutes a good measure for structural complexity.
the correlations for twg po m are strong and significant for most solvers.
fortwg po m see e.g.
the value for cream in table vi that is .
and twi po m significant correlations can be observed on all cp based solvers.
however these correlations are exceptionally low for mistral while cream shows significant correlations on all experiments.
there are correlations for cream and jacop on the twg c m measure.
the jacop solver shows correlations for twg po m andtwi c m in the first experiment that disappears on the larger and the cardinality restricted models.
the mistral solver does not show any significant correlations for these measures.
on experiment with only and cardinality constraints the distribution of the correlations for ltn k par and sugar based solvers is similar to the first experiment.
for jacop and choco the correlations on twg c m and twg po m drop from between 526and0 89in the first experiment to at most .
the correlation for mistral even drops below significance.
moreover for sugar based solvers the correlations for twi pp m drop while correlations for twi po m increase.
in general experiment indicates that there are some differences between arbitrary and restricted cardinalities on at least some solvers.
experiment with jfj and cardinality constraints confirms the previous findings.
the correlations for the sugar encoding are the strongest on some ltn k parbased solvers.
the highest correlation for sugar based solvers can be observed for tw po m .
a noticeable difference to other experiments is that the values of the correlations dropped significantly or even disappeared for picosat choco and jacop.
the correlations for jacop on twicould not be verified since twicould not be computed on the size models.
in summary the results of the experiments confirm the hypothesis as there are significant and strong correlations for all cnf based and the majority of cp based solvers.
throughout all experiments as the gaifman graph is concerned our experiment suggests twg po m as the best structural complexity measure.
the experiments indicate that it can satisfy our quality criteria for a metric for the hardness of the analysis see section i c on the participating cnf based solvers and on cp based solvers since the correlations between solver execution time what we consider as hardness and twg po m are present throughout all experiments.
g. tree width based tool performance characterization the measures revealed by the experiment allow us to characterize the performance of sota analysis tools based on the structure of the analyzed models.
for evaluation we repeated our experiment on all models from splot7 a common benchmark for the fm community.
figure shows an example of a tree width based performance characterization for twg c m fig.
.
example of applying tree width to splot the cream solver operating on the cp encoding.
it is based on the lower bound of the tree width of the gaifman graph of the cp encoding.
the example shows that the solution procedure is fast low execution time and reliable low standard deviation on models with low gaifman tree width while models with higher tree width are increasingly hard to solve for cream.
there are few complex models in splot.
to improve the visualization of these models in the figure we group models with a tree width value within the shown intervals for all models withtwg .
the number of models with twg 7per single twgvalue in splot is very low 7models per value and it remains even relatively low for the depicted interval groups.
accordingly conclusions drawn from the last three boxplots might not be valid.
the correlation for most solvers on splot models using tw gis significant.
in the example it is0 on the raw data without the grouping .
the full data from our experiments and the complete splot example is available for further analysis from our website8.
this type of performance characterization as well as the basic experiments can be used for evaluation and further improvement of tools running on a particular formalization by comparing the results on models with a formalization of particular structural complexity.
though this is not intended to build an automated procedure based on tree width to select tools dynamically in particular verification scenarios.
for this one would have to compute the tree width value for the encoding used by several tools before tool selection which would probably not be efficient.
v. c onclusion in this paper we contribute to measuring the complexity of analysis operations on fm by proposing and evaluating the use of graph width measures as metrics for the structural complexity of fms.
answering our research question we can state that our experiment shows the general reasonability of these metrics and reveals two interesting conclusions.
first we can conclude that the incidence graph of the formalization is most useful for the structural complexity of a model with respect to a particular formalization e.g.
an order encoded cnf .
a possible explanation for this is that the preservation of the hypergraph structure by the gaifman graph is not sufficient for evaluating differences between individual encodings.
second based on our correlation analysis we are able to propose the lower bound heuristic of the tree width of the order encoding twg po as a reasonable metric for the hardness of the analysis run on most sota analysis tools even for those that do not apply order encoding.
such a complexity metric can be used as a basis for a unified method to systematically improve sota analysis tools.
moreover we implemented coffemap a framework to conduct systematic experiments with measures that can be used to characterize the performance of solvers.
results from these experiments can be used to reveal classes of models that are easy to solve using a particular tool.
future work can use this as a basis to systematically examine the impact of tool improvements on the analysis of particular models.
our longterm vision is to encourage the collection of knowledge about the complexity of fm analysis apart from worst case time complexity.
accordingly we expect our presented framework to contribute to overcoming the lack of knowledge about the behavior of different automated fm analysis tools.
in the future we plan a rigorous analysis including the different heuristics within one tool as it was done in in order to reveal their performance in analyzing fms with a constant structural complexity.
this analysis should include existing case studies e.g.
the linux kernel feature model .
moreover we plan to extend our analyses conducted on generated models to realistic handcrafted models e.g.
from industrial case studies in order to improve the validity of our results.
in particular it is still questionable why in the splot case models are more easy to process for analysis tools than generated models.
models from industrial case studies might help to reveal the reason for this difference whether it lies in the nature of splot that consists mostly of academic models or in the possible artificial hardness of our generated models.
we further envision considering more tools that use advanced fm analysis techniques e.g.
the bdd heuristics from mendon a and smt solvers to cover the full state ofthe art in fm analysis.
applying the presented systematic characterization to this wide range of sota analysis tools will yield new opportunities for the construction of a new generation of reliable and scalable fm analysis tools.
in a second step to evaluate this knowledge we would like to extend our experiments to reflect several typical usage scenarios of fm validation tools that will reveal knowledge on more complex analysis operations than the validity check.
acknowledgment we would like to thank sergio segura and ana b. s nchez for providing us with models from betty as well as helpful comments on this paper.
this work has been funded by the dfg under grant po kopi.463references e. bagheri and d. gasevic.
assessing the maintainability of software product line feature models using structural metrics.
software quality journal .
y .
ben haim a. ivrii o. margalit and a. matsliah.
perfect hashing and cnf encodings of cardinality constraints.
in proc.
15th int.
conf.
on theory and application of satisfiability testing pages .
springer .
d. benavides s. segura and a. ruiz cort s. automated analysis of feature models years later a literature review.
inf.
syst.
.
d. benavides s. segura p. trinidad and a. ruiz cort s. a first step towards a framework for the automated analysis of feature models.
inmanaging variability for software product lines working with variability mechanisms .
d. benavides p. trinidad and a. ruiz cort s. automated reasoning on feature models.
in proc.
17th int.
conf.
on advanced information systems engineering pages .
springer .
a. biere m. heule h. van maaren and t. walsh.
handbook of satisfiability volume frontiers in artificial intelligence and applications .
ios press amsterdam the netherlands the netherlands .
h. l. bodlaender.
treewidth characterizations applications and computations.
in proc.
32nd int.
ws on graph theoretic concepts in computer science pages .
springer .
b. courcelle and s. olariu.
upper bounds to the clique width of graphs.
discrete appl.
math.
.
k. czarnecki p. gr nbacher r. rabiser k. schmid and a. wasowski.
cool features and tough decisions a comparison of variability modeling approaches.
in proc.
6th int.
ws on variability modeling of softwareintensive systems pages new york ny usa .
acm.
a. favier s. de givry and p. j gou.
exploiting problem structure for solution counting.
in proc.
15th int.
conf.
on principles and practice of constraint programming pages berlin heidelberg .
springer.
r. ganian p. hlin en and j. obdr lek.
better algorithms for satisfiability problems for formulas of bounded rank width.
arxiv eprints .
r. ganian p. hlin en j. kneis d. meister j. obdr lek p. rossmanith and s. sikdar.
are there any good digraph width measures?
in proc.
5th int.
ws on parameterized and exact computation pages .
springer .
y .
gil s. kremer davidson and i. maman.
sans constraints?
feature diagrams vs. feature models.
in proc.
14th int.
conf.
on software product lines pages berlin heidelberg .
springer.
m. grohe.
the structure of tractable constraint satisfaction problems.
in proc.
31st int.
conf.
on mathematical foundations of computer science pages berlin heidelberg .
springer.
p. hlin en s. i. oum d. seese and g. gottlob.
width parameters beyond tree width and their applications.
computer journal .
g. janssen.
a consumer report on bdd packages.
in proc.
16th symp.
on integrated circuits and systems design pages .
k. kang s. cohen j. hess w. novak and a. s. peterson.
featureoriented domain analysis foda feasibility study.
tech.
rep. cmu software engineering institute .
h. katebi k. a. sakallah and j. a. p. marques silva.
empirical study of the anatomy of modern sat solvers.
in proc.
14th int.
conf.
on theory and application of satisfiability testing pages berlin heidelberg .
springer verlag.
c. luque m. moreto f. cazorla r. gioiosa a. buyuktosunoglu and m. valero.
cpu accounting for multicore processors.
ieee transactions on computers .
d. marx.
approximating fractional hypertree width.
in proc.
annual acm siam symposium on discrete algorithms pages .
m. mendonca a. wasowski and k. czarnecki.
sat based analysis of feature models is easy.
in proceedings of the 13th int.
software product line conf.
splc pages pittsburgh pa usa .
carnegie mellon university.
m. mendon a. efficient reasoning techniques for large scale feature models .
phd thesis university of waterloo .
s. i. oum.
rank width and vertex minors.
j. comb.
theory ser.
b .
j. petke and p. jeavons.
the order encoding from tractable csp to tractable sat.
in proc.
14th int.
conf.
on theory and application of satisfiability testing pages .
springer .
k. pohl g. b ckle and f. j. v. d. linden.
software product line engineering foundations principles and techniques .
springer new york inc. secaucus nj usa .
r. pohl k. lauenroth and k. pohl.
a performance comparison of contemporary algorithmic approaches for automated analysis operations on feature models.
in proc.
26th ieee acm int.
conf.
on automated software engineering pages washington dc usa .
ieee computer society.
n. robertson and p. seymour.
graph minors.
iii.
planar tree width.
j. combinatorial theory series b .
m. samer and s. szeider.
constraint satisfaction with bounded treewidth revisited.
j. computer and system sciences .
a. s. sayyad t. menzies and h. ammar.
on the value of user preferences in search based software engineering a case study in software product lines.
in proceedings of the international conference on software engineering icse pages piscataway nj usa .
ieee press.
p. y .
schobbens p. heymans and j. c. trigaux.
feature diagrams a survey and a formal semantics.
in proc.
14th ieee int.
conf.
on requirements engineering pages .
s. segura j. galindo d. benavides j. parejo and a. ruiz cort s. betty benchmarking and testing on the automated analysis of feature models.
in u. eisenecker s. apel and s. gnesi editors sixth international ws on variability modelling of software intensive systems vamos page leipzig germany .
acm acm.
s. segura j. parejo r. hierons and a. ruiz cort s. ethom an evolutionary algorithm for optimized feature models generation v. .
.
technical report .
s. she r. lotufo t. berger a. wasowski and k. czarnecki.
variability model of the linux kernel.
in fourth international workshop on variability modeling of software intensive systems vamos linz austria .
c. sinz.
towards an optimal cnf encoding of boolean cardinality constraints.
in proc.
11th int.
conf.
on principles and practice of constraint programming pages .
springer .
n. tamura a. taga s. kitagawa and m. banbara.
compiling finite linear csp into sat.
constraints .
v .
tuikys and r. dama evi cius.
measuring complexity of domain models represented by feature diagrams.
information technology and control .
c. wohlin p. runeson m. h st m. c. ohlsson b. regnell and a. wessl n. experimentation in software engineering an introduction .
kluwer academic publishers norwell ma usa .