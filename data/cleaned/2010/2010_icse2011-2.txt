reverse engineering feature models steven she university of waterloo shshe gsd.uwaterloo.carafael lotufo university of waterloo rlotufo gsd.uwaterloo.cathorsten berger university of leipzig berger informatik.uni leipzig.de andrzej w asowski it university of copenhagen wasowski itu.dkkrzysztof czarnecki university of waterloo kczarnec gsd.uwaterloo.ca abstract feature models describe the common and variable characteristics of a product line.
their advantages are well recognized in product line methods.
unfortunately creating a feature model for an existing project is time consuming and requires substantial e ort from a modeler.
we present procedures for reverse engineering feature models based on a crucial heuristic for identifying parents the major challenge of this task.
we also automatically recover constructs such as feature groups mandatory features and implies excludes edges.
we evaluate the technique on two large scale software product lines with existing reference feature models the linux and ecos kernels and freebsd a project without a feature model.
our heuristic is e ective across all three projects by ranking the correct parent among the top results for a vast majority of features.
the procedures e ectively reduce the information a modeler has to consider from thousands of choices to typically ve or less.
categories and subject descriptors d. .
design tools and techniques d. .
distribution maintenance and enhancement restructuring reverse engineering and reengineering general terms design languages keywords feature models feature similarity variability modeling .
introduction software product lines spl enable e ective development of a range of related products with di ering sets of features.
the spl paradigm is centered around a number of practices leading to systematic code reuse .
large scale spls permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
icse may waikiki honolulu hi usa copyright acm ... .
.
powersavecpu hotplugcpu freq performanceacpipm acpi systempowersave acpi!cpu hotplug figure power management feature model such as the linux ecos and freebsd operating system kernels contain thousands of features and many dependencies among them.
these dependencies pose a challenge for both developers and users.
for developers adding or removing features or dependencies requires understanding the impact of such changes.
for users that instantiate a product from the product line intricate dependencies between features lead to errors during the con guration process.
some projects such as linux and ecos address these di culties by providing feature models to describe their product line .
a feature model describes features the common or variable characteristics of the products in a spl as a visual hierarchy with additional constraints between features .
feature models o er a range of bene ts from enabling automated analysis for verifying and resolving product line consistency to generating graphical con gurators that guide users through the con guration process .
figure inspired by the linux kernel model shows a feature model of a power management sub system.
features are represented as rectangles and may be optional denoted by an empty circle or mandatory denoted by a lled circle.
an edge from one feature to another denotes a dependency a solid line denotes a child parent edge of the feature tree where the child implies the parent a dashed line with an arrowhead represents a cross tree implies edge and a dotted line with x s is an excludes edge .
features may also belong to a group.
in our example performance and powersave are in an xor group meaning that one and only one may be selected.
the feature tree the group constraints and the cross tree edges form a feature diagram .
a feature model consists of a feature diagram and possibly of a cross tree formula an additional cross tree constraint expressing more complex dependencies.
other projects such as freebsd do not have a feature model.
these projects describe features and dependencies in an ad hoc manner features are scattered in documentationand dependencies are hidden in code.
such projects would bene t from having an explicit feature model instead.
unfortunately constructing a model is both time and cost intensive.
building the feature hierarchy in particular requires substantial e ort from a modeler.
this task requires the modeler to review feature descriptions and dependencies to determine which dependencies to model in the hierarchy.
freebsd has features constructing a feature model for this project would require tremendous time and e ort.
furthermore the di culty is compounded when the modeler lacks a complete set of dependencies.
in this case the dependencies might need to be uncovered by comparing feature descriptions.
this may require the modeler to sift through hundreds of features in order to determine a parent for a single feature.
even if the modeler is given the complete set of dependencies selecting the right parent for a feature is still challenging a single feature may depend on over a hundred others as we have observed for linux and ecos.
we present a tool supported approach for reverse engineering feature models.
the key challenge in this task is the construction of the feature diagram which reduces to the selection of a parent for each feature.
we present heuristics for identifying the likely parent candidates for a given feature.
our heuristics signi cantly decrease the number of features that a user has to consider from potentially thousands to only a handful typically ve or less as shown by our experiments.
we also provide automated procedures for nding feature groups implies and excludes edges.
the nal feature model is correct with respect to the input dependencies.
our procedures require a list of feature names descriptions and a propositional formula specifying dependencies.
feature names and descriptions can be extracted from documentation preprocessor symbols or code comments.
for the freebsd kernel we extracted input data for our procedures by analyzing make les preprocessor declarations and documentation using a combination of generic and custom extraction tools.
due to the complexity size and nature of most software projects it is likely that the extracted feature dependencies and descriptions are incomplete.
our heuristics accommodate this incompleteness by leveraging two sources of data that complement one another when dependencies are incomplete the feature descriptions are used to identify parent candidates and vice versa.
we evaluate the e ectiveness of our procedures by comparing the results of our heuristics to the reference feature models of the linux ecos and freebsd kernels.
linux and ecos both have an existing reference feature model .
the input data for the procedures was extracted from the reference models themselves.
for freebsd we manually constructed a reference feature model for a subset of features after domain analysis.
the evaluations show that for of features in linux and in ecos the correct parent is in the top ve parent candidates returned by our heuristics.
in contrast to linux and ecos the input set of dependencies for freebsd is incomplete and thus we consider two separate results for freebsd for of the features whose parent dependency is in the set the correct parent is in the top two candidates for of the remaining features the correct parent is in the top or of all features.
finally our procedure automatically recovers all feature groups as presented in the reference models for linux and ecos provided that the modeler settled on the same hierarchies as these acpi!acpi system pm acpi system!acpi cpu freq!pm cpu freq!powersave performance cpu hotplug!powersave cpu hotplug!
performance cpu hotplug!acpi cpu freq powersave!
performance powersave!cpu freq performance!cpu freq powersave acpi!cpu hotplug a dependencies pmpower management cpu and acpi options acpi advanced con guration and power interface support acpi system enable your system to shut down using acpi cpu freq cpu frequency scaling cpu hotplug allows turning cpu on and o powersave this cpu governor uses the lowest frequency performance this cpu governor uses the highest frequency b features and descriptions figure example input models.
with the incomplete dependencies of freebsd we were still able to retrieve one of the three feature groups.
the contribution of this work is twofold.
on the practical side we present heuristics and procedures for reverse engineering feature models.
although reverse engineering feature models from logic formulas and descriptions were considered before in separation the main novelty of our approach is that it combines both sources of information together.
this combination is desirable since as our evaluation shows the two sources are complementary.
also the procedures of and are not complete in the sense that the former cannot recover parents which are not direct dependencies while the latter suggests only a single hierarchy that is unlikely the desired one.
moreover in contrast to previous work we evaluate reverse engineering of feature models on large scale real world systems showing that our approach and procedures scale.
on the theoretical front we expand our understanding of feature models by showing how both con guration semantics and ontological semantics relate to feature hierarchy.
we proceed as follows.
section gives an overview of the procedure.
section provides the background supporting the de nition of the procedure section and the evaluation section .
we discuss threats to validity relate to existing works and conclude in sections and accordingly.
.
overview in this section we demonstrate how our procedures assist the user in reverse engineering a feature model.
figure shows a set of dependencies given as a formula feature names and descriptions that we use as input data.
our procedure reduces the reverse engineering process of building the feature hierarchy nding feature groups and inserting implies and excludes edges in a sound and complete manner to just the rst step building the feature hierarchy.ranked implied featur es ranked all feat uresselected cpu hotplug .
.
.
.
.powersave acpi acpi system cpu freq pmcpu frequency scaling.
.
.
.
.
.cpu freq powersave performance acpi acpi system ...figure parent candidates the remaining steps are automated.
crucially we support the user in building the hierarchy itself by providing tailored suggestions of parents avoiding the need to sift through a multitude of candidates.
the key challenge of building the feature hierarchy is the selection of a parent for each feature.
it requires understanding the meaning of the feature and its relationships to all other features.
as a result the hierarchy building process is inherently interactive and requires a domain expert modeler to review alternative choices for a feature s parent and to select the most suitable one.
our procedures present two lists of parent candidates to suggest the most appropriate parents for every feature thus signi cantly reducing the total number of features to review at each step.
the rst list is the ranked implied features rifs a sorted list of features that a given feature implies.
implied features are the primary criteria when deciding a parent the semantics of feature models state that a child implies its parent.
however a feature may imply more than one other feature.
this is where a ranking heuristic is applied to sort the implied features by their similarity to the selected feature placing the most likely candidates at the very top.
the second list is the ranked all features rafs all features sorted by their similarity to a given feature.
the rafs is a complete ranking but is typically less accurate than the rifs.
it can be reviewed in the case the input dependencies are incomplete and the user cannot nd an appropriate parent in the rifs.
in this case the rafs is useful for identifying potential parents where an implication from the selected feature may be missing due to incompleteness of available dependency information.
we describe the details of the hierarchy building procedure in section .
as an example assume that the user is selecting a parent forcpu hotplug .
we would present its parent candidates as in figure .
there are ve features here that are implied features of cpu hotplug with powersave at the top position and the actual parent cpu freqis at the fourth position.
if the dependencies are incomplete and the user cannot nd an appropriate parent in the left list the right list can be reviewed.
later we show that the best candidates for parents are typically highly ranked in both lists.
furthermore once the feature hierarchy is decided feature groups are detected.
the user reviews the feature groups and select the ones that should be retained in the feature diagram.
any feature groups not retained in the diagram are kept as part of the cross tree formula.
for example if the hierarchy in figure is chosen our tooling will detect two feature groups a mutex group between cpu hotplug and performance and an xor group between performance andpowersave .
the user selects one of the groups to keep in the diagram relegating the other to the cross tree formula.
finally mandatory features implies and excludes edges are automatically discovered and added to the feature diagram.
for example assume that the user decides on the hierarchy in figure and implication is omitted from the dependencies in figure .
our procedure can still detect an implies edge from cpu hotplug!powersave since it can be derived from implications and .
now that the diagram is nished further constraints are added to the cross tree formula to make the resulting feature model sound all legal con gurations of the feature model are legal con gurations with respect to the input dependencies.
all these steps relies on sat based reasoners and thus are independent from the syntactic structure of the dependency constraint.
furthermore if the user assumes the dependencies are complete then only the rifs needs to be reviewed by the user.
the rafs are no longer needed because all possible alternatives for parents are contained in the rifs.
our procedures can be integrated into existing feature model editors in the style of to equip them with reverse engineering capabilities and to allow modelers to make parent and group decisions in a graphical representation.
however we do not advocate any speci c user interface design at this point.
we leave this to future work.
.
background .
feature modeling we rst de ne the abstract syntax of a feature diagram.
def.
.afeature diagram is a tuple fd f e i x g c wherefis a nite set of features e f f is a set of directed child parent edges i f f is a set of implies edges x f f is a set of excludes edges g 2eare non overlapping sets of edges participating in feature groups.
the nal component c g!n0 n0is a mapping from a group to a pair denoting the cardinality of the group.
the following well formedness constraints hold in fd i f e is a rooted tree ii all edges in a group share the same parent so if g2gand f1 f2 f3 f4 2gthenf2 f4and iii m n 2range c m n. amutex group is de ned as a group mwith cardinality c m and an xor groupxis one where c x .
furthermore with def.
mandatory features are syntactic sugar they are represented by an implies edge from parent to child.
features not having such an edge are optional.
def.
.a feature model fm fd where fdis the feature diagram and is a propositional formula over f. the primary meaning of a feature model known as its con guration semantics is a set of legal con gurations sets of selected features that respect the dependencies entailed by the diagram and the cross tree constraints.
the con guration semantics can be speci ed via translation to logic .
take the feature model in figure for example the formula in figure 2a de nes its legal con gurations.
in general the function p de ned below translates a feature diagram or a feature model to propositional logic interpreting features as variable names.
for brevity we use a boolean predicate choice m n in its de nition.
given boolean variables f1 fkand m n k linux net staging dst linux net staging dst a b figure same con gurations di erent ontological semantics choice m n f1 fk holds i at least mand at most nof f1 f kare true.
def.
.for a diagram fd f e i x g c de ne p fd c p 2e c!p f k 2i f!k f k 2x f!
k g2g m n 2c g g f f1 f fk f g f!choice m n f1 f k given fm fd de ne p fm p fd .
while the con guration semantics is most commonly associated with feature modeling there exist other meanings of feature models.
for example figure depicts two feature models with identical con guration semantics yet di erent hierarchies and meaning.
the feature dststands for distributed storage a driver that allows accessing remote storage as a local device.
the parent of dsta ects its meaning.
in a dst is nested under netmeaning the feature is a product quality device.
in the diagram b dstis under staging implying the feature is experimental and not ready for mainstream use.
we call such semantics the model s ontological semantics re ected in the meaning of the features and the hierarchy.
.
linux ecos and freebsd we evaluate our procedures on linux ecos and freebsd kernels.
linux is an open source general purpose os created in the early s. it has a feature model speci ed in the kcon g language for con guring features in its kernel.
ecos is a real time operating systems designed for embedded devices.
it uses its own modeling language called the component de nition language cdl to specify its model.
while kcon g and cdl were developed independently from feature modeling both models so closely resemble feature models that they can be interpreted as one .
freebsd is also an open source os.
unlike linux and ecos freebsd does not have a feature model but only a at list of features.
the linux feature model has over features while ecos and freebsd have over features.
we use the data from linux to train and empirically build our procedures and ecos and freebsd as tests subjects to evaluate our procedures.
.
the procedures the procedures assumes three kinds of inputs a set of feature namesf feature descriptions d and feature dependencies .
the ordering heuristics ignore the order of words in the descriptions so dis de ned as a mapping assigning a multiset of words to each feature in f. amultiset is a pair x c wherexis a set and c x!n1.
given a feature f we writed1 f to refer to the set of words in f s description and for a word w2d f d2 f w denotes the number of pm cpu freq powersaveperformanceacpi acpi system cpu hotplugfigure implication graph where the transitively reduced subgraph is marked with thick edges occurrences of winf s description.
finally dependencies are speci ed as a propositional formula over f. the heuristics assume feature descriptions contain words from both the feature name itself and from any associated text.
often times the feature names provides signi cant clues to its relation with other features.
for example cpu hotplug and cpu freqshare the common word cpu.
we apply a simple tokenization where we split the feature name by the underscore character and retain only alphanumeric words.
we further assume that the descriptions have been stemmed have stop words removed and are case insensitive.
for example using the descriptions in figure with stop words marked in gray d cpu freq fcpu cpu freq frequency scaleg.
.
building feature hierarchy implication graph.
first we de ne several pre requisites for the procedures.
we assume that none of the input features are dead features that no valid con guration can include.
dead features are automatically detected and removed.
next a feature implication graph is a pair v e where vis a set of features and e v vis a directed edges such that s t 2ewhenever entails the implication s!t.
figure is the graph constructed from dependencies in figure 2a.
an implication graph is transitively closed due to the transitivity of implications.
if w!uandu!vthen w!v.
the transitive reduction of an implication graph is the subgraph not containing the aforementioned transitive implications except for cliques.
the transitive reduction can be computed using known algorithms .
we denote the transitive reduction of a graph gbygrand edges remaining ingrasdirect implications .
figure shows the transitively reduced subgraph in thick edges.
next e g denotes the set of edges in a graph g. for a featurefwrite if g to denote features implied by f so heads of edges outgoing from f .
then if gr gives the directly implied features of fing.
identifying parents.
in essence our parent ranking heuristics leverages two complementing forms of data dependencies and descriptions.
the dependencies describe the con guration semantics of our feature model and the descriptions are used to approximate its ontological semantics.
given a parent candidate pand the selected feature s we de ne the similarity function p s to return the sum of theinverse document frequency idf of the words shared between the descriptions of pands weighted by the number of occurrences of each shared word in p s description equa tion .
the parent candidates whose descriptions share many words with feature sand with shared words often repeated are then ranked highly similar to the selected feature.
furthermore we use the idf to give less weight to common domain words such as linux ecos choose or select .
these words are not stop words according to standard natural language processing tools but they do not contribute to nding commonalities between two feature descriptions.
p s x w2d1 p d1 s idf w d2 p w where idf w logjfj jff w2d f gj we use the similarity function to induce a ranking order on features.
given a selected feature swe de ne two strict partial orders sand p s. in the rst s features are ranked strictly by their description similarity to s a sbi a s b s the second partial order p sprioritizes directly implied features ofsover all other implied features.
this prioritization is based on our observation that in linux of parents in the model are directly implied features.
the partial order is de ned as a p sbi a2is gr b 2is gr or a2is gr b2is gr a sbor a 2is gr b 2is gr a sb finally we can de ne the two lists that make the parent candidates the rifs ranks only the implied features of f using the prioritizing order while the rafs ranks all features using the non prioritizing order.
def.
.given a feature sand an implication graph g rif s is the list created by sorting features in is g in decreasing order with respect to p s largest rank rst .
raf s is the list of features in fsorted in decreasing order with respect to s. the orders are made total any ties broken by applying alphabetical ordering to ease browsing.
as an example consider determining the rifs of cpu hotplug .
feature cpu hotplug has ve implied features three of which are directly implied figure .
examining their descriptions in figure 2b we see that cpu hotplug shares the word cpu with cpu freq performance pmand powersave .
the rifs of cpu hotplug are rif cpu hotplug hpowersave acpi acpi system cpu freq pmi the rafs of cpu hotplug uses the set of all features instead raf cpu hotplug hcpu freq performance pm i the user chooses parents for every feature by examining rifs and rafs forming a set of directed child parent edges e f f .
these edges may not form a single tree when there is no common top level ancestor.
in this scenario we insert an additional root feature to join together the forest to form a single tree.
.
groups and cross tree constraints after the hierarchy is built we detect the remaining components of a feature diagram namely feature groups and implies and excludes edges.a mutex graph is used to detect feature groups and excludes edges.
a mutex graph is an undirected graph consisting of vertices being features and edges denoting a mutual exclusion between two features uandvsuch that entailsu!
v. unlike in the implication graphs edges in the mutex graph are not transitive.
the mutex graph constructed from the dependencies in figure 2a consists of two edges cpu hotplug excludes performance and performance excludes powersave .
feature groups.
amutex group de nes a cardinality among its members.
mutex groups are recovered by nding all maximal cliques in the mutex graph mamong sets of children features.
given a hierarchy econsisting of child parent edges c p e returns the children of pine cp e fcj c p 2eg we now de ne gmas the mutex groups represented as sets of child parent edges given the hierarchy e gm p2f g fpgjg2max cliques subgraph m cp e where subgraph g v returns the subgraph containing the verticesvand any edges between elements of ving and max cliques g returns the set of maximal cliques in the undirected graph g ignoring trivial cliques of size one since they do not contribute to feature groups.
in our example we get two overlapping mutex groups f cpu hotplug cpu freq performance cpu freq gand f performance cpu freq powersave cpu freq g. anxor group de nes a cardinality among its members and thus impose a stronger constraint than mutex groups.
xor groups can be recovered by checking an additional condition on a mutex group.
taking the set of mutex groups we check each if given the presence of its parent at least one element in the group must also be present.
thexor groupsgxis de ned gx f f1 p fk p g2gmj j p!
f1 fk now letgm0 gm gxbe groups that are strictly mutex and not xor.
given a group g2gm0 gx the cardinality mapping cis de ned c g ifg2gm0 ifg2gx gm0andgxare maximal no group subsumed by other and complete none missing with respect to the constructed hierarchy and input dependencies.
gm0contains all maximal mutex groups since it uses the maximal cliques algorithm.
gxinherits the maximality of mutex groups but further constrains it by enforcing a lower bound.
thus gxcontains all possible xor groups a feature cannot be added or removed without violating the cardinality constraint.
however the maximality and completeness of gm0and gxmay cause groups to overlap i.e.
a feature may belong to one or more feature groups a property disallowed by the well formedness rules of a feature diagram.
similar to the hierarchy building we present users with all detected groups so that they can decide which to maintain in the feature diagram.
groups that are not overlapping are kept in the diagram.
any groups that are not kept will remain as excludes edges or cross tree constraints.implies and excludes edges.
the nal components of the feature diagram implies and excludes edges describe the remaining implications and exclusions that are not represented in the feature hierarchy or as a feature group.
the implies edges can be thought of as the edges from child to parent candidates that were not selected to be in the hierarchy.
let gbe the implication graph and ebe the constructed hierarchy the implies edges iarei e g e. similarly excludes edges are the edges that were not chosen to be represented as a mutex orxor group.
we de ne members to return the members of a group.
for example letg f f1 p fk p gthen members g ff1 fkg.
given a mutex graph m feature groups g the excludes edges xis de ned x e m s g2gmembers g members g .
the cross tree formula.
using the procedures described in this section yields a feature model that is complete all valid con gurations in the input dependencies are valid con gurations of our feature model.
however it may still allow some con gurations that were disallowed by initial dependencies.
to address this we can add our input dependencies as a cross tree formula to make our feature model sound.
in practice to reduce redundancy in the cross tree formula we add only the clauses in that are not already entailed by the diagram.
incomplete dependencies.
amutex group with members f1 fkis detected if there exists a clique between f1 fkin the mutex graph.
a clique in the mutex graph requires each member to have an exclusion to every other member.
if any exclusions between the members are missing in the case the dependencies are incomplete a mutex group with kmembers will be detected as one with less than kmembers.
xor groups on the other hand contain two dependencies.
first requirement is that an underlying mutex group exists between the group members.
second the xor group requires an implication from the group s parent to its members equation .
if either the rst dependency is incomplete or if the second dependency is missing then the xor group will be detected simply as a mutex group of equal or lesser size.
.
implementation a prototype implementation is available as an open source project1.
computation of the implication and mutex graphs which can be done o ine took about and hours for linux and ecos respectively on a .40ghz intel core2 duo machine.
the computation of rifs and rafs needed for user interaction is instantaneous.
the graph algorithms use sat4j for checking implications and exclusions.
we used the bron kerbosch algorithm for nding all maximal cliques in the mutex graph.
.
evaluation we evaluate our procedures on input from linux ecos and freebsd.
for linux and ecos we extract our input data from their existing reference feature models which gives us two samples with complete dependencies and extensive descriptions.
freebsd on the other hand does not have a reference feature model.
for this project we evaluate our 1gsd.uwaterloo.ca reverse engineering feature modelsprocedures against data extracted from the freebsd codebase giving us a sample with incomplete dependencies and partial descriptions.
we believe that freebsd is representative of the projects that will require our procedures for reverse engineering.
since freebsd lacks a reference model we created one manually for a subset of features.
our evaluation criterion is to check if for every feature its parent in the reference feature model is one of the top parent candidates in the rifs and rafs.
we consider the parent child relations in the reference feature models to be the best choices possible because these models were built over ten years by their respective development communities.
we also measure the e ect of incomplete dependencies and descriptions by progressively removing dependencies and descriptions from the linux and ecos data.
we see that prioritizing direct implications has a signi cant impact on the e ectiveness of our procedures with incomplete descriptions.
finally we evaluated our procedure for recovering feature groups in the presence of complete and incomplete data.
.
experiment input data characteristics we construct our input data using the reference models of the x86linux .
.
.
and the i386pc feature model for ecos .
.
dependencies were extracted by applying a translation of their formal semantics to propositional formulas .
feature names and descriptions were extracted directly from the reference models themselves.
we have placed the translation tools online as open source projects2 .
figure characterizes the inputs for linux ecos and freebsd.
linux has features while ecos has and as shown in figure the distribution of number of words are only slightly di erent.
the majority of features in ecos have to words.
linux on the other hand has a large number of features with no descriptions while the rest have roughly to words.
the distributions of direct and transitive implications are signi cantly di erent.
almost all features in linux have from to transitive implications while in ecos the variation is much larger.
for direct implications linux s features have from to implications and again ecos has a larger variety.
in section .
we will see if and how the distributions a ect the results of our procedures.
unlike linux and ecos that have a con guration tool conguring the freebsd s kernel involves creating a large text le that lists selected features such as devices and cpu options and their values.
various boilerplate templates are available to the user as default con gurations.
there is no explicit description of legal combinations of features.
we extracted the input for our procedure from the freebsd .
.
codebase.
we included the codebase for all nine supported hardware architectures unlike for linux and ecos where we used architecture speci c models .
the features were mined by hand crafted parsers from lint templates maximal con gurations that contain all options supported by the kernel.
since these templates often contain feature descriptions in a semi structured way we also created heuristics based fuzzy parsers to extract them.
dependencies were extracted from various sources.
we manually derived around dependencies that were present in feature descriptions but the majority of dependencies were extracted from source code.
we examined the codebase to 2code.google.com p linux variability analysis tools 3code.google.com p variability wiki cdltoolsnumber of wordsnumber of features050100150200 number of implications01020304050 100ecos freebsd freebsd ref linuxfigure characterization of descriptions transitive implications dark grey and direct implications white for ecos freebsd the freebsd reference model and linux nd statements relevant for extracting feature dependencies.
for example dependencies between device drivers were speci ed by freebsd speci c preprocessor macros making such dependencies easy to extract.
however most constraints had to be extracted by using a more comprehensive static analysis infrastructure.
this infrastructure is one of our ongoing projects but we use its early results for this work.
the analysis infrastructure derives constraints by analyzing c source les and exploiting three types of information error preprocessor macros build error analysis the location of feature