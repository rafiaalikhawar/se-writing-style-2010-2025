data debugging with continuous testing k van mu slu yuriy brun alexandra meliou computer science engineering school of computer science university of washington university of massachusetts seattle wa usa amherst ma usa kivanc cs.washington.edu brun ameli cs.umass.edu abstract today systems rely as heavily on data as on the software that manipulates those data.
errors in these systems are incredibly costly annually resulting in multi billion dollar losses and on multiple occasions in death.
while software debugging and testing have received heavy research attention less effort has been devoted to data debugging discovering system errors caused by well formed but incorrect data.
in this paper we propose continuous data testing using otherwise idle cpu cycles to run test queries in the background as a user or database administrator modifies a database.
this technique notifies the user or administrator about a data bug as quickly as possible after that bug is introduced leading to at least three benefits the bug is discovered quickly and can be fixed before it is likely to cause a problem.
the bug is discovered while the relevant change is fresh in the user s or administrator s mind increasing the chance that the underlying cause of the bug as opposed to only the discovered side effect is fixed.
when poor documentation or company policies contribute to bugs discovering the bug quickly is likely to identify these contributing factors facilitating updating documentation and policies to prevent similar bugs in the future.
we describe the problem space and potential benefits of continuous data testing our vision for the technique challenges we encountered and our prototype implementation for postgresql.
the prototype s low overhead shows promise that continuous data testing can address the important problem of data debugging.
categories and subject descriptors d. .
testing and debugging h. .
database administration general terms design keywords database testing continuous testing data debugging .
motiv ation today s software systems rely heavily on data and have a profound effect on our everyday lives.
defects in these systems are common and extremely costly having caused for example gas pipeline and spacecraft explosions loss of life and at least twice a near start of a nuclear war .
however despite the prevalence of data errors while softwarepermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
esec fse august saint petersburg russia copyright acm ... .
.logic defects have received ample research attention until recently e.g.
detecting and correcting system errors caused by well formed but incorrect data has received far less.
data errors can arise in a variety of ways including data entry errors e.g.
typographical errors and transcription errors from illegible text measurement errors e.g.
the data source may be faulty or corrupted and data integration errors .
these errors can be costly errors in spreadsheet data have led to million dollar losses and poor data quality has been estimated to cost the us economy more than billion per year .
data bugs have caused insurance companies to wrongly deny claims and fail to notify customers of policy changes agencies to miscalculate their budgets medical professionals to deliver incorrect medications to patients resulting in at least deaths in the us in and nasa to mistakenly ignore via erroneous data cleaning from until the earth s largest ozone hole over antarctica .
in this paper we aim to address a particular kind of data errors that are introduced into existing databases.
consider the following motivating example company inc. maintains a database of its employees their personal data salaries benefits etc.
figure a shows a sample view of the data and figure b shows part of the documentation company inc. maintains to describe how the data is stored to assist those using and maintaining the data.
company inc. is facing tough times and negotiates a reduction in the salaries of all employees.
while updating the employee database an administrator makes a mistake and instead of reducing the salary data reduces all compensation data by which includes salary health benefits retirement benefits and other forms of compensation.
after a couple of months of the mistake going unnoticed alonzo church finally realizes that his paycheck stub indicates reduced benefits.
he complains to the human resources office who verify that alonzo s benefits should be higher and ask the database administrator to fix alonzo s benefits data.
the administrator who has made hundreds of updates to the database since making the mistake doesn t think twice about the problem and updates alonzo s data without realizing the underlying erroneous change that caused alonzo s and other data errors.
in a tragic turn of events a month later alan turing accidentally ingests cyanide and is hospitalized.
with modern technology the hospital quickly detects the poison and is able to save alan s life.
unfortunately the insurance company denies alan s claims because his employer has been paying smaller premiums than was negotiated for alan s policy.
alan sues company inc. for the damages.
the mistake is finally discovered too late to save the company.
our example scenario while hypothetical is not much different from real world scenarios caused by data errors .
humans and applications modify data and often inadvertently introduce errors.
while integrity constraints guard against predictablepermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse august saint petersburg russia copyright acm ... .
631fname mname lname dob eid compensation salary hbenefit rbenefit alan mathison turing jun alonzo church jun tim john berners lee jun dennis macalistair ritchie sep marissa ann mayer may william henry gates oct a company inc. s employee database table the employee database table contains a row for every company inc. s employee.
for each employee there is a first middle and last name date of birth employee id salary health benefits and retirement benefits.
all employees must appear in this table even after they retire or pass away.
the employee id must be unique for each employee.
the dob is stored in a day month year format where month is the first three letters of the month capitalized and year is four digits.
b employee table documentation figure company inc. keeps a database of its current and past employees.
a sample view of the database a includes employee information with the database s attributes represented by the bold data descriptors .
company inc. also maintains a description b of their employee database as documentation for database users administrations and maintainers.
erroneous updates many careless unintentional errors still make it through these barriers.
cleaning tools attempt to purge datasets of discrepancies before the data can be used but many errors still go undetected and get propagated further through queries and other transformations.
company inc. illustrates three challenges with maintaining database systems.
these challenges cannot be easily addressed by existing techniques such as integrity constraints.
first a mistake of changing the data for the wrong attribute or set of attributes is hard to detect when the change is within a reasonable range.
for example when the data s valid domain spans multiple orders of magnitude detecting an error caused by a forgotten period during data entry e.g.
instead of .
is impossible automatically since both entries satisfy the integrity constraints and too onerous manually.
detecting these errors requires a method that is aware of the data semantics and of how the data are used.
second when errors go undetected for a long period of time they become obscure and they may cause additional errors.
if the discovered error is a side effect of a deeper faulty change discovering the error quickly after making the change helps identify and correct the underlying cause as opposed to only the discovered side effect.
correcting an error becomes costlier the longer the error remains undetected as decisions based on the error will need to be rolled back.
third poor documentation and company policies often contribute to mistakes.
for example company inc. s data description document does not describe what the compensation attribute is how it is derived and whether changing it will trigger automated changes to other attributes.
further the company has a single point of failure in its one administrator.
having not discovered the cause of the mistake the documentation remains incomplete and policies unaffected.
by contrast discovering quickly the cause of the mistake can expose outdated documentation and poor policies early improving the database system integrity and reducing documentation drift.
to address these challenges we propose continuous data testing a new technique that is complementary to integrity constraints and existing efforts in data cleaning to guard against erroneous updates and reduce the time between the moment the error is introduced and the moment it gets detected.
continuous data testing uses otherwiseidle cpu cycles to execute test queries over a database as a user modifies that database to discover erroneous edits quickly.
if the user changes the database in a way that breaks a test the user sees the test failure right away.
this results in at least three benefits .the bug is discovered quickly and can be fixed before it is likely to cause a problem.
.the bug is discovered while the relevant change is fresh in the administrator s mind increasing the chance that the underlying cause of the bug as opposed to only the discovered side effect is fixed.
.when poor documentation or company policies contribute to bugs discovering the bug quickly is likely to identify these contributing factors facilitating updating documentation and policies to prevent similar future bugs.
continuous data testing can discover multiple kinds of errors including correctness errors and performance degrading errors.
certain changes to the database may not affect correctness but could compromise performance e.g.
the choice of indexes causing queries and applications to run slower.
continuous data testing can alert administrators to changes that affect the running time as well as the memory footprint and other performance metrics.
accordingly continuous data testing can aid in database tuning .
our prototype implementation s low overhead shows promise that continuous data testing can address the important problem of data debugging.
the rest of this paper is organized as follows.
section defines continuous data testing and describes our prototype optimizations and challenges.
section places our work in the context of related research.
finally section summarizes our contributions.
.
continuous data testing while we cannot expect humans to avoid making mistakes altogether the drama at company inc. could have been avoided if the database administrator became aware of the error soon after introducing it.
unfortunately the error was not detected until months later and by that time its impact was irreversible.
our goal is not to stop errors from being introduced but to shorten the time to detection as much as possible.
early detection will prevent errors from propagating and having practical impact and will simplify correction as the user or administrator can more easily associate the occurrence of the error with recent updates.
we achieve this goal through continuous data testing which continuously executes tests black box computations over a database using otherwise idle cpu cycles alerting the user when the test results change.
the test execution continues as long as the user is mak 632db application test1 test2 ... continuous data testing alerts figure continuous data testing architecture.
while users and programs interact with and make changes to the database continuous data testing non intrusively executes test queries to identify potentially harmful changes.
ing changes.
in this paper we assume that each test is a sql query however our framework can handle more general test computations.
a well known approach to data error prevention is integrity constraints.
such constraints verify that updates retain the integrity of the data.
by contrast the errors continuous data testing targets cannot be easily detected by static constraints.
instead semantic queries identify changes in the meaning of the data.
continuous data testing is not meant to replace constraint based techniques but rather to complement them and to work in conjunction with them.
testing is suitable for enforcing complex conditions not captured by integrity constraints.
testing can also capture errors that are introduced to the database outside of regular database updates.
further integrity constraints need to be checked during each data update can hinder performance and are only suitable for enforcing simple restrictions e.g.
value ranges .
while continuous data testing is also computation intensive it does not hinder performance because it only runs on otherwise idle cpu cycles.
we have built a prototype continuous data testing implementation for the postgresql database management system to evaluate the feasibility of our approach.
continuous data testing poses several research challenges that we aim to address in our research test generation generating appropriate tests for applications is challenging and has been explored extensively in the contexts of software testing e.g.
and database testing e.g.
.
continuous data testing can rely on human written tests or tests generated by an automated tool or by a hybrid approach.
for example our prototype implementation uses human written and templategenerated query inputs and generates query outputs by running the query on the unchanged database thus creating regression tests.
critically since tests are application dependent they should be guided by the database workload and should be representative of the expected database queries and capture the data semantics .
continuous data testing can directly benefit from complementary future advances on automated data test generation.
when to test while executing tests continuously ignoring concurrent database activity and without targeting idle cycles reduces the time before a user or administrator discovers a behavioral change caused by an update it does not minimize the notification delay because the test executions are not prioritized based on the updates.
still our prototype continuous data testing implementation showed this approach experienced only a overhead depending on the query workload in database interactions.
while reducing this overhead is important this presents a reasonable starting point.
a more efficient continuous data testing implementation executes tests only when data updates occur.
our prototype uses databasetriggers to accomplish this by performing static analysis on the test queries to decide which triggers to add to which tables to trigger continuous data testing execution.
our preliminary experiments with this approach show a improvement in the overhead.
however both the static analysis and incorporating more intricate database functionality remain future work.
for example in some scenarios query results may change without data updates e.g.
when a new clustered index is added to a database.
what to test a na ve continuous data testing implementation can execute every test in every iteration.
however each data change will only affect a subset of the tests executing the tests not affected by the change wastes resources and delays the notification time.
a more efficient continuous data testing implementation uses static analysis on the updates to determine and run only the tests that could be affected by each update.
this optimization to our prototype decreased the overhead by up to over the na ve implementation.
future work on test query prioritization and even guided test query generation can reduce the notification delay even further.
how to test test queries can be complex executing them on large datasets may be slow and consume system resources.
future research will include incremental query computation using the change to the data as an input to the query computation.
for example a test query that computes a sum does not need to be recomputed from scratch when a datum is updated rather the previous sum can be adjusted based on the value of the update.
previous work on incremental view maintenance will guide our research.
in other domains incremental computation has been shown to greatly speed up data structures and code compilation .
user interface reporting a test failure is not trivial.
our current continuous data testing prototype only indicates which test has failed the user has to manually investigate whether the failure indicates an error.
this can be a tedious process because the test query may be complex and the failure can be non descriptive and hard to analyze.
instead descriptive failure summaries could include information on which and how many tuples are effected and similarities between the affected tuples.
since the goal is to help users interpret failures by using explanations in the same spirit as deriving explanations for query results using causal analysis descriptive failure summaries can use causality theory to analyze the contributions of each update to a given failure.
debugging performance one of the goals of continuous data testing is to detect erroneous data quickly.
however changes in the data may not only affect the system output but also performance.
changes to the data or an introduction of an index could make a test query slower or faster or could affect the execution s memory footprint.
by monitoring the tests performance continuous data testing can become a useful tool for database tuning quickly providing feedback on whether a change has a negative positive or no effect on performance.
.
related work it is possible to prevent data errors from being introduced but this requires devising integrity constraints that anticipate all possible erroneous updates.
chen et al.
follow a more interactive approach asking the user a challenge question to verify an update by comparing the answer with the post update query execution.
this approach is similar to tests but requires manual effort and interferes with normal system use.
in contrast continuous data testing focuses on detecting errors rather than preventing them and has a minimal impact on normal execution.
database testing research has focused on generating tests discovering application logic errors and debugging performance but not detecting data errors.
meanwhile extensive work automati 633cally generating regression tests e.g.
has neither focused on data testing nor query generation.
in the spreadsheet domain data bugs can be discovered by finding outliers in the relative impact each datum has on formulae by detecting and analyzing data region clones and by identifying certain patterns called smells .
in contrast the continuous data testing approach is system specific uses tests that encode the system semantics and of course applies to systems that use databases.
in writing software systems running tests continuously and notifying developers of test failures as soon as possible helps write better code faster .
reducing the notification time for compilation errors eases fixing the compilation errors .
continuous execution of programs even data driven programs such as spreadsheets can inform developers of the programs behavior as the programs are being developed .
continuous integration and merging can notify developers about merge conflicts quickly after they are created .
and speculative analysis can inform developers about errors they have not yet created but are likely to soon .
likely tests can also be predicted and executed to discover unexpected behavioral changes .
an in sync copy of the system or data can allow for impure tests .
overall notifying developers sooner of problems appears to make it easier to resolve those problems which is the primary goal of continuous data testing.
.
contributions continuous data testing is a novel technique for discovering system errors caused by well formed but incorrect data.
the technique is complementary to integrity constraints and existing efforts in data cleaning because it targets semantic data errors.
we have described the problem space and potential benefits of continuous data testing outlined our vision for the technique identified research challenges and discussed preliminary performance measurements based on our prototype implementation that support the feasibility of continuous data testing.
our early research into continuous data testing is encouraging and suggests it can be used to improve dataintensive system quality making continuous data testing a promising technique for addressing the important problem of data debugging.
.