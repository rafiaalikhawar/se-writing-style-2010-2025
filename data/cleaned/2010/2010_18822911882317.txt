instant code clone search mu woong lee jong won roh seung won hwang sunghun kim pohang university of science and technology postech hong kong university of republic of korea science and technology hkust sigliel nbanoh swhwang postech.edu hunkim cse.ust.hk abstract in thispaper wepropose ascalable instantcodeclone search engine for large scale software repositories.
while there arecommercial code search engines available they treat soft ware as text and often fail to find semantically related code.
meanwhile existing tools for semantic code clone searches take a post mortem approach involving the detection of clones after the code developmentis completed and hence fail toreturn the resultsinstantly.
inclear contrast we com bine the strength of these two lines of existing research by supporting instant code clone detection.
to achieve this goal we propose scalable indexing structures on vector ab stractions of code.
our proposed algorithms allow develop ers to detect clones of a given code segment among the .7million code segments from open source projects in subsecond response times without compromising the accuracy obtained by a state of the art tool.
categories and subject descriptors d. .
distribution maintenance and enhancement restructuring reverse engineering and reengineering general terms design management keywords clone detection code search .
introduction clone detection helps software development and maintenance tasks as unmanaged code clones make program maintenancedifficultandmaycause inconsistent clone changes .
therefore clone detection research has been an active area for decades and many practical techniques have beenproposed and widely used .
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted w ithout fee provided that copies are not made or distributed for profit or c ommercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise torepublish to post on servers or to redist ribute to lists requires prior specific permission and or a fee.fse november santa fe new mexico usa.
copyright acm ... .
.existing code clone detection tools usually take a postmortem approach by detecting the clones after code development is completed as they mainly focus on the code refactoring scenario.
in such scenario developers for example may run a code clone detector once per month and based on the gathered clone information they perform any necessary maintenance work such as refactoring or fixing inconsistent clone changes.
in clear contrast we focus on developing a preventive way of finding clones during development process by enabling an instant and scalable search for the clones of a given code segment.
this type of instant clone detection encourages to refer to well tested existing code rather than reinvents code clones.
in this paper we propose an instant clone search engine that is scalable to the size of code repositories e.g.
d e t e c t ing the clones of a given code segment from open source projects million loc .
million code segments in a sub second response time.
our detector considers structural similarities between code segments as in a post mortemclone detection tool deckard .
our key technical contribution is balancing the dual goals of instant response andresult quality by exploiting a multidimensional indexingstructure r tree and proposing dimensionality reduction and i o optimization techniques.
this type of instant detector would enable many interesting applications.
for example when developers work on one section of a very large project instant clone search wouldallow them to easily find and reference other similar codepieces.
conversely commercial code search engines such as koders 1or google code search2 may fail to suggest related code because they treat software as text .
using a post mortem clone detector is simply overkill for their pur pose because post mortem detectors usually focus on find ing all of the clone pairs and it involves an unavoidable and expensive computational cost.
in summary instant clone search is a helpful approach to support rapid and evolving software development.
we summarize our key contributions as follows we address the problem of how to support instant clonesearch.
wedevelopcloneindexingtechniquestoachievesub second responsetimesfor large scale real life software repositories without compromising clone search accuracy.
for applications where the loss of some accuracy is accept able we also propose an approximation scheme achievinga further speedup by trading off some accuracy.
the restof thepaperis organized as follows.
section discusses the preliminaries based on which section proposes our algorithms.
section reports our evaluation results.
section surveys related work and section concludes thispaper.
.
preliminaries this section discusses the preliminaries in code clone detection section .
and multidimensional indexing sec tion .
.
building on thesepreliminaries we formally defineour problem in section .
.
.
code clone detection there have been many code clone detection tools proposed recently including some tree based techniques whichabstract code segments as their corresponding parse trees orabstract syntax tree ast .
building on this abstrac tion clone detection is essentially tree similarity matching which is known to be inherently expensive e.g.
u s i n g tree edit distance as a similarity notion.
to overcome this inherent complexity deckard approximated such similarity notion by representing an ast as multi resolution numerical vectors known as characteristic vectors .
in other words each tree node is represented as a vector representing the frequency of the syntactic ele ments in the code segment represented by its subtree.
withthis representation an expensive tree match can be approx imated as inexpensive vector matches.
we adopt such ab straction as proposed in since its simplicity makes it possible to develop sophisticated techniquesto improvescalability.
.
multidimensional indexing characteristic vectors representing a code are often highdimensional e.g.
therearemorethantwohundredsdifferent syntactic element types in javaasts.
it is thus non trivial to find matching vectors efficiently.
to achieve sub second response times we index the code repository using a multidimensional indexing structure i.e.
a n r tree which have been widely adopted in database literature.
.
.
naive adoption intuitively characteristic vectors can be mappedinto multidimensionalpoints whichcanthenbeindexedusingamultidimensional index structure such as an r tree.aq u e r y also represented as a vector corresponds to another point such that finding clones corresponds to findingthe k nearest neighbors knns of the query point which has been actively studied in the database community .
anr treeis a height balanced tree data structure where each node contains a variable number of entries up to some pre definedmaximum i.e.
node capacity .
fornon leafnodes each entry contains two pieces of data a pointer to a childnode and the minimum bounding rectangle mbr of all entries within this child node.
for leaf nodes each entry has a pointer to a raw record stored on disks and the mbr of this raw record.
to briefly illustrate how this structure can be used to support a knn query figure shows an example of an r treefor dimensional data points.
to find knns of the query point q t h i s r treecan be traversed in a best first search manner by initially storing the root node s entries in the queue and iteratively retrieving the closest entry inxy r1r2 r3r4r5r6 r7r8r9 r10r11 r12p1p2p3p4 p5p6p7 p8p9p10p11 p12p13p14 q p15 p16p17p18 p19 p20p21 r1r2r3 r4r5r6 r7r8r9 r10r11r12 p19p20p21 p18 p17 p16 p15 p14 p13 p12 p11 p10p9p8 p7p6 p5p4 p3p2p1 figure a knn query on an r treefor 2dimensional points the queue with respect to euclidean distance and enqueuing its child node s entries.
this type of search can termi nate when the top kclosest entries are all pointing raw data points e.g.
p 5andp8whenk .
.
.
adoption with dimensionality reduction though adopting an index discussed above enables to find knn with a tree traversal and avoid a sequential scan such traversal can be more costly than a scan when the dimensionality is high.
this problem is known as the dimensionality curse problem .
to avoid this problem dimensionality reduction techniques are typically used to select only a few important features and reduce the overall dimensionality.
however in a dimension reduced space a knn search result may differ from that in the original space.
to illustratethis new challenge with example data in figure supposewe simply reduce the dimensionality of the dataset to one by projecting points onto the x axis.
the closest point in this reduced space is then p i.e.
false positive while the actual closest point in the original space is p5.
due to this challenge in order to get the knearest results in the original space from the reduced space we first need to identifyk primecandidates that are guaranteed to contain all the correctkresults k prime k .
for instance in our projection example retrieving k nearest point in the reduced space i.e.
based on the projection on x axis will retrieve false positive p8 but retrieving k prime is guaranteed to include the correct answer p5.t og u a r a n t e et h a t k primecandidates do not exclude any correct result i.e.
no false negatives the following lower bounding property should hold.
definition lower bounding property .
given a dimensionality reduction function f and two data vectors v1andv2 distance function df in the reduced space and distance function do in the original space should satisfy df f v1 f v2 do v1 v2 if the dimensionality reduction function f satisfies definition we can obtain the k primecandidates as follows first we find the knns off q on the reduced space.
we then sort these knns by their real distances to qon the original space and choose the kthnearestvector vkfrom these knns.
finally we identify all points vsatisfying df f q f v do q v k .
this can be done through a simple range search 168on the reduced space with do q v k and these candidates are guaranteed to have all top kresults as formally proved in .
wethen rank these candidatesby their real distances to find the correct knearest results.
.
problem definition this section formally defines the top kcode clone search problem.
we define the code segments and the characteristic vectors in definition and .
definition then defines the distances between vectors.
based on definition defi nition defines top kcode clones of a query code segment.
definition code segments .
given a code s i t s astt and a threshold mint i fas u b t r e e t ioftcontains at least mint nodes then ti s corresponding part in sis a code segment.
definition characteristic vectors .
given a code segment siand the ast tiofsi the characteristic vector vi angbracketleftci ci ci d angbracketrightofsiconsists of occurrence countersci j of syntactic elements in ti.
definition distanc es between vectors .
given twod dimensional vectors v1andv2 the distance bardblv1 v2 bardbl between v1andv2is theirl2 norm bardblv1 v2 bardbl radicalbig summationtextdi c1 i c2 i .
definition top kcode clones .
given a set vof characteristic vectors a query vector q and the retrieval size k top kclonestc k q v is a set of vectors tc k q v1 v2 vm w h e r e viis theithclosest vector from q m k a n d bardblq v i bardbl bardblq v k bardblfor isatisfying k i m. at o p kcode clone search query qretrieves a set tc k q andtc kis used as its shorthand.
for notational simplicity we usetc kto represent both code clones and their corresponding vectors interchangeably.
.
instant code clone detection this section proposes indexing structures and algorithms to solve the top kcode clone search problem.
as a baseline section .
discusses a sequential scan algorithm scan.
section .
then proposes a sub linear algorithm frtcd u s ing an r treeindex with dimensionality reduction.
though frtcddemonstrates reasonable scalability in medium scale datasets it still incurs prohibitive i o costs.
we thus study i o optimization techniques to further improve the overall performance and build an enhanced algorithm intcd u p o n the optimized r trees in section .
.
lastly section .
discusses how to further boost performance for scenarios where compromising some accuracy can be tolerated.
.
baseline sequential scan one naive solution to find the clones of a given query code segment would be adopting an existing clone detector identifying clusters of clones as their results.
from these results we can identify the cluster to which the given query code belongs and consider other codes in the same cluster as its clones.
however considering we only need one suchcluster finding all clu sters is an overkill.
alternative solution is to adopt locality sensitive hashing lsh as used by deckard to efficiently find nearneighbors similar vectors of eachcharacteristic vector.however lsh is not an exact nearest neighbor algorithm as we will empirically show later in section .
.
we thus adopt a straightforward baseline approach for the exact computation using a sequential scan called scan which simply reads the entire repository sequentially and updates tc k as algorithm illustrates.
algorithm scan q k input q u e r yv e c t o r q r e t r i e v a ls i z e k output s e ttc kof vectors of top kclones initialize tc k foreachv vdo updateclones tc k k q v returntc k specifically scansequentiallytestseachcharacteristicvectorv v.f o re a c h v scantests that vis not farther than a n yv e c t o ri nt h ec u r r e n t l yk n o w nt o p klisttc k.i fvis not farther scanupdates the list tc k as algorithm illustrates.
algorithm updateclones tc k k q v input s e ttc k r e t r i e v a ls i z e k q u e r yq v e c t o rv tci tc kdenotes the ithnearest vector in tc k fromq if t c k kthentc k tc k v else if t c k kand bardblq v bardbl bardblq tc k bardblthen tc k tc k v remove tci tc kfarther than tckfromq .
filtering then ranking clone detection this section proposes a filtering then ranking top kcode clone search algorithm called frtcd u s i n ga n r treeindex.
however naively adopting this type of index structure incurs undesirablehigher cost compared to a simple sequential scan as discussed in section .
.
to overcome this challenge we first discuss a dimensionality reduction technique in section .
.
.
we then discuss how to build an index on this reduced space section .
.
a n dt h e ne x e c u t et o p kclone queries section .
.
.
.
.
dimensionality reduction for a given set vofd dimensional ncharacteristic vectors v1 v2 vn our goal in dimensionality reduction is to generate lower dimensional vectors v prime v prime v prime v prime n whichsatisfy the lower bounding property definition1 and make our algorithms efficient.
as discussed in section it is important to preserve the lower bounding property to ensure that we can retrieve candidates including all of the correct kresults by searching the reduced space only.
formally for all viandvj v and their corresponding reduced vectors v prime iandv prime j the distances measured in the original space and the reduced space should satisfy bardblv prime i v prime j bardbl bardblvi vj bardbl.
we can trivially show that selecting any d prime dimensional subspace of the original d dimensional space ensures the lower bounding property.
however not all such subspaces are equally effective.
a desirable subspace should reflect the original distances between vectors or more formally minimize the sum of 169differences i j summationdisplay i j i negationslash j i j summationdisplay i j i negationslash j bardblvi vj bardbl bardblv prime i v prime j bardbl between two distances measured at the original space and the subspace respectively.
finding such subspace is knownto be np hard which motivates us to develop its ap proximation schemes.
a straightforward approximation would be a greedy strategy that iteratively picks the dimension that reduces the most.
however this strategy requires recomputing forall remaining dimensions at each iteration.
to avoid suchrecomputations we propose to compute the variances of all dimensions once and select the d primedimensions with the highest variances.
to demonstrate that this variance based approach is not only more efficient but also as effective as the greedy strategy discussed above we briefly compare dimensional subspaces selected from dimensional characteristic vectors obtained from real life javasource codes files in table .
the results in the table indicate that both approaches produce nearly identical results while the variance basedmethod incurs a significantly lower cost.
table top selected dimensions variance based greedy strategy identifier identifier 2i d tk id tk unary expression unary expression multiplicative expression multiplicative expression additive expression additive expression relational expression shift expression shift expression relational expression equality expression equality expression conditional expression conditional expression assignment expression assignment expression .
.
index building wenowdiscusshowwecanbuildan r treeinthedimensionreduced space.
a naive way to create an index is to insert one vector at a time which incurs an expensive update on the index tree per each vector insertion.
in contrast a bulk loadingapproach amortizes the updatecost by inserting the entire dataset at once .
existing bottom up bulk loading algorithms first partition the entire dataset and build each partition as a leaf node.
then to build non leaf nodes they iteratively applythesamepartitioningprocesstotheresultingnodes untilwehave only one partition including the entire dataset which corresponds to the root node of the r tree.
bulk loading tightly packs the index structure to enable fast lookups and it is reported to boost the buildingperformance by hundred folds .
inparticular wereviseastate of the artbottom up r tree bulk loading algorithm str to apply to our problem.
str partitions the given dataset into mbrs by recursively subdividing each dimension into the same number of slices.straightforwardly adoptingthispartitioningpolicyisnotde sirable for the dimension reduced characteristic vectors asthe variance differs significantly over dimensions.
that is inone dimension points are highly clustered in a small range while in another points are well scattered.
for such data partitioning each dimension into the same number of sliceswould render non square rectangles with one side significantly larger than the other which incurs higher i o costthan squared blocks for the l 2distance function used in our work.
xy xy a str packing b our packing figure str partitions the data into mbrs while our packing algorithm partitions the data into8 mbrs.
their utilizations are .
and .
respectively.
toillustrate consider30pointsinatwo dimensionalspace where the node capacity cof the tree is i.e.
e a c ht r e e node may hold at most entries and the xvalues are much more scattered than yvalues figure .
at thebottom level of the tree this dataset should be partitioned into or more mbrs.
to do this str partitions the space by dividing each dimension into an equal number of slices i.e.
slices containing the same number of data points to render mbrs figure a .
in contrast we divide the datasetinto mbrs by as illustrated in figure b to significantly enhance the node utilization of str i.e.
.
into .
.
formally fora d dimensionaldatasetcontaining npoints wesubdividethe ithdimensioninto si ri r slices where riis the value range computed as the difference between the maximum and minimum values of the ithdimension.
in other words a dimension with a high riis highly scattered.
assuming points are uniformly scattered rcan be computed as producttextd i 1ri r n c. algorithm formally describes the data partitioning process.
algorithm datapartitioning e c rrr input e n t r i e s e e1 en node capacity c value ranges rrr angbracketleftr1 rd angbracketright output partitioned entries e r d radicalbig c n producttextd i 1ri si ri r f o r i d repeat2 foreachsido si si 1ifit does not incur an overflow until any decrease incurs an overflow slice e angbracketlefts1 sd angbracketright foragivensetofentries eanditsvalueranges angbracketleftr1 rd angbracketright algorithm first computes rand angbracketlefts1 sd angbracketright line .
then algorithm tries to decrease sivalues lines to further enhance the node utilization.
in our observation assitakes ceiling and thus overestimates the right number of subdivisions in each dimension the total number of partitions producttextdi 1si may become too large i.e.
incurring the low node utilization.
to address this problem algorithm de creases each s iby one until any decrease incurs an overflow i.e.
with node utilization higher than .
170algorithm slice e i sss input e n t r i e s e dimension index i sss angbracketlefts1 sd angbracketright sorteaccording to the ithdimension subdivide einto e prime e prime si ifi dthen foreache prime idoslice e prime i i angbracketlefts1 sd angbracketright once the tightest sivalues are determined we recursively subdivide each dimension as described in algorithm .
algorithm first sorts the set of entries ein the ascending order of the ithdimension then divides eintosisubdivisions namely e1 a n desi.
we make sure the first si subdivisions to contain e si where e denotes the number of entries in e.esicontains the remaining entries.
to build an r tree we first partition the reduced vectors using algorithm .
the resulting partitions become leaf nodes.
then we perform algorithm again on the mbrs of the leaf nodes.
in this case we sort them using their centers as representative points of the mbrs.
we repeatthis partitioning process until we have only one partition which corresponds to the root of the tree.
.
.
two phase query processing thissectionproposesthefiltering then rankingtop kcode clone search algorithm frtcd to evaluate top kcode clone queries.
basically frtcdworks in two phases of filteringand ranking as algorithm formally states.
algorithm frtcd q k t input q u e r yv e c t o r q retrieval size k r treet output s e ttc kof vectors of top kclones q prime the reduced vector of q n theknearest neighbors of q prime from t v prime k thekthnearest vector in n by the distances on the original space bardblq v k bardbl c v prime bardblq prime v prime bardbl from t initialize tc k foreachvcorresponding to v prime cdo updateclones tc k k q v returntc k in the filtering phase lines for a given query vector q frtcdidentifies candidate vectors c using a combination ofknn and range search on the reduced space.
more precisely frtcdfinds the knns n o fq prime by traversing the r treein a best first search manner line and computes their distances to qin the original space to choose the kth nearest vector v prime kamongn line .
bardblq v k bardbldenotes the actual distance between the given query qand the characteristic vector vk vcorresponding to v prime k. frtcdthen performs an range search where bardblq v k bardbl to find all the reduced vectors cwithin the distance bardblq v k bardbl fromq prime line .
as bardblq v k bardbl bardblq prime v prime k bardbl cis guaranteed to contain correct kresults.
we can thus prune out the remaining vectors v prime c as they are farther away from q compared to vk.
in the ranking phase lines frtcdcomputes the distance of each characteristic vector vcorresponding to v prime c fromq.
observe that unlike scanthat computes the dis tance for all objects frtcdcomputes the distance of a very small subset cof raw data records n c i.e.
a sub linear algorithm.
after frtcdcomputes the distances of these candidates to q we can finalize the list of top kcode clones.
.
interleaved clone detection thoughweempirically observethat frtcdalready achieves an acceptable efficiency for medium scale datasets as wewill later show with our extensive evaluation results we can also observe that frtcdhas room for further improvements as much i o costs are wasted on random accesses on data records according to our experimental results .
ofthe overall response time of frtcd .
seconds to find the top code clones in the repository of .
million vectors corresponds to the random access cost.
based on this observation we study two i o optimization techniques reducing the number of random accesses and reducing the cost of each access.
.
.
vector packing we first study how we reduce the number of random accesses by packing a group of records to be accessed together i.e.
those with similar values.
this type of packing allows us to reach multiple records with a single random access followed by cheaper sorted accesses which incurs a significantly lower cost than performing a random access per each record.
for one dimensional data this packing can beimplemented straightforwardly by storing raw data records in the same order as the index key.
however for multidimensional data it is non trivial to identify an effectiveone dimensional sorted order to store records.
figure illustrates a scenario reading data records within fromqrequires accesses.
however if these points are packed into two blocks b 1andb2 w eo n l yn e e dt w o random accesses to get to the beginnings of the blocks and the remaining records can be retrieved by cheaper sequential accesses.
while this strategy may incur the overhead of retrieving few more false positive records the overall i ocost is greatly reduced as we also empirically validate insection .
q b1b2 figure a range query example for this packing we apply the same scheme proposed for bulk loading in section .
.
and store these blocks in sev eral files.
after the packing is done we simply build anr treeon the mbrs of the blocks.
each mbr is computed in thereduced d prime dimensionalsubspace and thedata blocks contain the original d dimensional vectors.
as we use the same scheme in section .
.
for this packing data insertion and deletions can be handled in a similar way that the r trees do without the packing.
.
.
single phase query processing this section proposes intcd adopting the vector packing scheme discussed in the previous section.
specifically we implement intcdto further reduce the number of ran171dom accesses to index nodes by interleaving the knn search and the range search in the filtering phase of frtcd a n d reduce the cost of random accesses.
interleaved index traversal recall that frtcdperforms a best first search to find the knns in the reduced space then performs a range search to find candidates.
after this candidate selection frtcdreads the raw records of the candidates to compute the real distances from the query.
in contrast intcdinterleaves these two steps by concurrently accessing raw records during the index traversal.
basically intcdtraverses the index in the reduced space in the same manner as frtcd.
however during the traversal when a leaf entry is reached intcdaccesses the raw data blockpointedbytheleafentry withoutwaiting fortheindextraversal to complete as the cost of reading few extra rawdata is much affordable now with vector packing.
wheneverdata records are accessed from the leaf entry intcdupdates a sorted list tc k of the current known top kclones and we denote the current kth nn in the list as tck.
our key observation is that tckcan be used as a pruning boundary as we can safely prune out both non leaf and leaf entries that are farther than tck.
as more data records are accessed tc kconverges to the actual top kresults.
in this interleaved traversal each index node is accessed at most once which enables to outperform frtcdaccessing some index nodes twice during the knn and the range search respectively.
algorithm formally describes this process of intcd.
algorithm intcd q k t input q u e r yv e c t o r q retrieval size k r treet output s e ttc kof vectors of top kclones tci tc kdenotes the ithnearest vector in tc k fromq q prime the reduced vector of q tc k q h entries within the root of t whilehis not empty do e h.pop if t c k k ormindist q prime e bardblq tc k bardblthen ifeis not a leaf thenh.push children of e else q.push e if q wthen e pop block pointers from q foreachv ab l o c ko f edo updateclones tc k k q v whileqis not empty do e pop block pointers from q foreachv ab l o c ko f edo updateclones tc k k q v returntc k specifically toimplementthissingle scanbest firstsearch am i nh e a p hofeis maintained in the ascending order of mindist q prime e where q primedenotes the reduced vector of query q eis an entry of the r treeindex and mindist q prime e d e notes the shortest distance between q primeande figure .
at the beginning the entries within the root of tare pushed into h line .
then iteratively the entry ein hwith the minimal mindist q prime e is processed.
if the mindist q prime e is no farther than the distances of the cur eq3d1 d2q1 q2 figure mindist qi e in a dimensional space.
mindist q1 e d1 mindist q2 e d2 a n d mindist q3 e .
renttcktoq we continue the iterations.
otherwise we can safely ignore e line .
ifmindist q prime e bardblq tc k bardbl w et e s ti f eis a leaf entry or not.
ifeis not a leaf then the entries within its child node are pushed into h line .
otherwise we process the raw data block pointed by e. when processing raw data a naive approach would be reading each raw data block right away which incurs high random seek costs.
instead to reduce the cost of random accesses we devise an effective scheduling technique called delayed loading by adopting the idea of circular scan disk scheduling .
delayed loading specifically we propose a delayed loading scheme which delays the reading the block of euntil we collect multiple blocks to read as the name itself suggests.
figure illustrates how this scheme works in a scenario involving the reading of four blocks b1 b2 b3 a n d b4.
they are stored on the disk in a sorted order and the mindistsoftheircorrespondingentries e1 e2 e3 a n de4satisfymindist q prime e2 mindist q prime e4 mindist q prime e3 mindist q prime e1 .
b1 b2 b3 b4without delayed loading with delayed loadingsequantial access random access figure effectiveness of delayed loading a naive solution would be reading each block one at a time which incurs four expensive seeks but instead we can read these four blocks at once in the forward direction as infigure which incurs cheaper seeks followed by sequentialscan.
in algorithm we maintain a delayed loading queue q which contains block pointers ein the ascending order of mindist q prime e .
whenever we have a raw data block to read i.e.
w h e nal e a fe n t r y eis reached we push it into q line .
we then delay reading these blocks until we collect a sufficient number of entries in q determined by the given threshold w which we set as in our experiment line .
when this happens intcdthen reads these blocks in batch lines .
once the traversal terminates weprocess the remaining blocks in q lines .
.
approximate clone detection for applications where some accuracy compromise can be tolerated approximation is a good strategy to trade accur a c yf o ra ne v e nh i g h e rp e r f o r m a n c e .
i nt h i ss e c t i o n w e discuss how we study an approximation scheme empirically 172achieving a times speedup against intcd while compromising no more than of the accuracy.
specifically we propose an approximate top kcode clone search algorithm aptcd which efficiently identifies approximateanswerswithoutreadingorprocessinghigh dimensional data records v. instead we read further dimensionality reduced records v primeand issue a dimensionality reduced query q prime.
intuitively as dimensionality of v primeincreases efficiency decreases but accuracy increases.
to balance this trade off we need to effectively select the subspace v prime.
this goal is similar to that of the dimensionality reduction we discussed for exact algorithms section .
.
our approach should be different as the reduction this time is applied upon already dimensionality reduced space.
as a result applyingthe same technology would be redundant and ineffective.
from the reduction results reported in table obtained by variance based ranking we observed that features with similar variancestendtobecorrelatedfrom oneanother.
for example features identifier and id tkwith the same variance were also perfectly correlated with each other which suggests that these two features can be reduced into one feature without any loss of accuracy.
there have been many reduction techniques studied for aggregating correlated features such as principal component analysis pca and piecewise aggregate approximation paa .
in this research which aims at scalability we consider paa with higher scalability.
figure illustrates how paa reduces the dimensionality.
in the figure elements in vare sorted in a descending order based on the variances of their corresponding dimensions.
sorted vector 67 4low variance high variance v figure paa example to illustrate consider a scenario of reducing v vinto a dimensional vector v prime.
our variance based dimension reduction method for frtcdand intcdsimply chooses v prime angbracketleft71 angbracketright.
paa uses a fixed size disjoint window to dividev then aggregates each wi ndow.
subsequently if v prime angbracketleft142 angbracketright.
similarly if v prime angbracketleft201 angbracketright.
once we reduce the dimensionality to evaluate approximate top kqueries we simply build an r treeover these reducedvectors v prime asshown insection3.
.
.
wecanthenfind the approximate results by traversing the tree in a best first manner algorithm similarly to our exact algorithms.
as we later discuss in section our experimental results indicate that applying paa is effective for achieving even higher performances e.g.
times speedup while compromising of accuracy against intcdwhend 2a n d .
.
experimental ev aluation thissectionempirically evaluatesourproposedalgorithms.
first we describe how we generate datasets and queries in section .
.
second we evaluate the efficiency and scalability of our algorithms in section .
.
third we validate the effectiveness of our approximate query processing scheme algorithm aptcd q k t input q u e r yv e c t o r q r e t r i e v a ls i z e k r treet output s e t tildewidertc kof approximate top kclones q prime the reduced vector of q tildewidertc k h root oft whilehis not empty do e h.pop ifmindist q prime e bardblq prime tildewidetck bardblthen ifeis not a leaf thenh.push children of e else updateclones tildewidertc k k q prime e return tildewidertc k in section .
.
lastly we compare our proposed indexing structure with locality sensitive hashing lsh that is used by a state of the art post mortem clone detector deckard section4.
.
all experimentswere carried outon a machine with a pentium iv .2ghz processor with 1gb of memory running linux.
.
experimental setup we generate characteristic vectors in the same way described in for deckard from two real life javacode repositories.
the first repository contains javafiles from jdk .
.
update consisting of lines of code total and the second repository contains java files lines from java open source projects hosted on sourceforge tigris.org and googlecode.
from these repositories we generated six vector datasets two from the jdk code set denoted as jdk and four fromtheopensourceprojectcodeset denotedas osp by varying the parameter mintofdeckard .
the dimensionality of each vector is .
table summarizes the sizes of the resulting vector datasets and the mintsetting.
once these vector sets were generated we randomly chose one hundred vectors from each dataset to use as our queries.
.
efficiency scalability to evaluate the efficiency and scalability this section reports index building time and query execution time for varying retrieval sizes kand dataset sizes v .f o r t h i s s e t o f experiments we empirically chose as the index dimensionality.
we increase kup to to test scalability though we may use very small kin practice.
table index building time for varying v dataset mint v building time s frtcd intcd jdk .
.
jdk .
.
osp .
.
osp .
.
osp .
.
osp .
.
table summarizes index building time of intcdand frtcd.
this table only shows the building time excluding the data processing time for vector extraction and dimensionality reduction.
observe from the table that intcdtakes 173a relatively longer time than frtcdto accomplish the vector packing process as vectors packed into blocks need to be stored causing extra i os.
however owing to this packing scheme intcdperforms better than frtcdin the later experiments.
both the indexes for frtcdand intcdcan be built in minutes which is acceptable considering that index creation is a one time and offline process.
the block sizes for these two datasets were tuned empirically.
for jdkdatasets the block size was set as 8kb and each file contained at most blocks as the performancewas optimal with such setting.
similarly for ospdatasets we set the block size and file size as 384kb and blocks respectively.
.
.
v millions querying time s scan frtcd intcd v t c millions .
.
.
.
figure querying time for varying v jdk datasets k log scaled .
the table lists the average number of clones in tc .
figure shows the average querying time for varying v using jdkdatasets and thetableshows theaverage number of vectors in tc k. this number can be larger than kdue to the ties in the results.
both frtcdand intcdare at least times faster than scan.
though in this mediumscale dataset the performance gain of intcdover frtcdis less significant this type of performance gap significantly increases as the scale of the data increases as we later showwith the larger datasets in figure .
.
.
.
.
v millions querying time s scan frtcd intcd v t c millions .
.
.
.
.
.
.
.
figure querying time for varying v osp datasets k log scaled .
figure shows the average querying time for varying v using ospdatasets.
compared to figure intcdachieves higherspeed upover frtcd i.e.
.
times faster than scan which suggests that our proposed i o optimization techniques are more effective in larger scale datasets and play a crucial role in enhancing the overall efficiency.
figure shows the querying time over varying k u s i n g osp 3dataset.
as scanreads the entire data once regardless ofk its performance is constant over varying k.i n c l e a r contrast intcdand frtcdare and .
times faster than scanwhenk .
considering that kis typically much smaller than the data size in general search scenarios this10 kquerying time s scan frtcd intcd k t c k .
.
.
.
.
figure querying time over varying k osp dataset v 697k .
progressive behavior of our proposed algorithms incurring smaller cost for smaller k is highly desirable.
.
effectiveness of the approximation to validate the proposed approximate query processing algorithm called aptcd this section reports its performance and approximation quality for varying approximation set tings compared to intcd.
figure shows the performance and quality of our approximation algorithm for varying approximation settings.
we varied the aggregation window size f r o m1t o5 t h e dimensionality dof the index from to and kwas set to .
.
.
.
.
window sizequerying time s d d d d d d .
.
.
.
.
.
window sizef score d d d d d d a querying time b f1score figure performance and quality of aptcdfor varying settings osp 3dataset k .
in figure a it is clear that the lower dimensional indexes perform better than higher ones in general.
figure b reports the approximation accuracy measured us ing the balanced f scores f 1scores f1score precision recall precision recall precision tildewidertc k tc k tildewidertc k recall tildewidertc k tc k tc k wheretc kand tildewidertc kdenotethequeryresultsetsusing intcd and aptcd respectively.
note that t c k and tildewidertc k are not always equal to k because they may have ties.
observe from the figure that our proposed reduction using paa enables a high speed up without compromising theaccuracy much when d 2a n d or .
we now compare aptcdwith intcdusing the following two settings.
first we chose the setting where aptcdis most accurate i.e.
d 2a n d .
second we chose amoderate setting where d 4a n d .
b yu s i n g these two settings accurate andmoderate we compared our approximation scheme with the exact querying algorithm proposed intcd.
.
.
.
.
v millions querying time s scan intcd accurate moderate kquerying time s scan intcd accurate moderate a effect of v k b e ff e c to f k osp figure approximation performance using osp figure and table respectively summarizes the performances and accuracies of our approximation scheme using the above two settings.
in figure our approximation scheme in both settings significantly outperformed intcd and the gap increased as the size of the dataset increases.
for osp 3dataset our approximation is and times faster than intcdformoderate and accurate respectively.
meanwhile the accuracy was not compromised much asthe precision and recall results in the accurate setting show e.g.
constantly higher than .
.
table approximation quality using ospdatasets v kaccurate moderate millions precision recall precision recall .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
comparison with lsh lastly we evaluate the proposed indexing structure used inintcd compared to the lsh implementation used by deckard figure .
experiment setting for the given r range query our r treebased index returns the exact answers while lsh returns approximate results with probability guarantee p. more precisely lsh requires two parameters pandrfor building the index.
for the given query point q l s hr e turns all points psuch that bardblp q bardbl rwith probability of por higher.
due to this nature technically lsh structure needs to be re built as rchanges to ensure the probabilistic guarantee unlike our r treebased approach building one single structure and reusing for arbitrary range r. to accommodate such difference we use a favorable setting for lsh of not considering the rebuilding cost of lsh to show our approach outperforms even in such unfair set ting in figure a .
for queries we randomly selected vectors from osp a n dv a r i e d rfrom to .
querying time observe from figure a that even in unfavorable settings our index outperforms lsh in terms of query execution time.
this experiment also shows that asrincreases the performance gap also increases which suggests our approach is more scalable for large r.f o r example when r which means selecting all data points1 rquerying time s lsh p .
lsh p .
lsh p .
r tree based rrecall p .
p .
.
.
.
.
.
.
.
a performance b lsh accuracy figure range query performance log scaled osp 7dataset v 784k .
with inl2distance from the query our index is about times faster than lsh with p .
and when r ours is times faster than lsh.
memory use meanwhile infigure12 a somedatapoints for lsh are missing which are the cases when lsh could not return results due to memory shortage.
alternatively we can use a disk based implementation of lsh not to be constrained by memory size.
however its performance will be worse than that of memory based implementation reported in the figure which is already outperformed by our approach.
deckard goes around this problem by dividing a problem into sub problems and apply lsh for each sub problem.
accuracy in terms of accuracy we compare the precision and recall of our approach and lsh.
in figure b as the precision of lsh was perfect in all settings we only reportits recall when p .
and .
respectively.
observe that as rincreases e.g.
r or in the figure lsh is more likely to miss some vectors within the range r.i n clear contrast our indexing scheme guarantees the perfect precision and recall in all cases.
summing up for instant clone search our r treebased indexing is more suitable than lsh by ensuring higher performance effective memory usage and perfectaccuracy.
.
related work this section surveys existing research on code clone detection code example recommendation and code search.
clone detection as already briefly surveyed in section there have been many clone detection techniques proposed that abstract codes as parsing trees and apply hashing orcharacteristic vector comparison for clone detection.
ina similar way wahler et al.
abstracted codes as xml trees and adopted the concept of frequent itemsets to find clones that share frequent tree patterns.
recently a distributedcodecloneanalysisalgorithm calledd ccfinder wasproposed whichimprovesthescalability ofccfinder by leveraging multi cluster machines.
however these tools are still not scalable enough to achieve online detection in large scale repositories.
code recommendation meanwhile there have been alternative lines of research taking place to find code examples that share similar usage patterns or structuralsimilarity.
i nt h efi r s tl i n eo fw o r k x i e et al.
proposed an ap175proach to abstract codes as api call sequences and identify examples that share similar sequences.
li et al.
used the frequency of operation calls to define similarity and then clustered similar examples into a few representative usagetypes.
in the second line of work holmes et al.
proposed some heuristic structural matching techniques to find relevant example codes.
however the former line of work cannot be used for structural similarity search and the latter has limited scalability.
code search engine there are commercial code search engines including kodersand google code search that abstract codes as text and support simple and regular expression keyword matches.
however these engines which treat codes as simple text do not support structural matches.
sourcerer stores some structural information on codes in relational tables and provides rankedmatching butdoes notfocus on optimizing search performance.
.
conclusion in this paper we introduced scalable and instant code clone search techniques.
these techniques open doors to many interesting unexplored applications such as interleaving clone detection with editing sessions during code development.
we evaluated the accuracy and efficiency of our approach with large scale real life software repositories.
in addition to exact code clone search we also developed an approximation algorithm for scenarios wher e some accuracy compromise can be tolerated which performs nearly a thousand times faster than the baseline approach.
both the exact and ap proximation algorithms achieved sub second response timesfor large scale real life repositories of .
million code segments.
as future work we are considering the following more features in addition to the characteristic vectors we will consider more features such as the structural rela tionship between vectors or runtime semantics to enablemore precise matching.
industry scale detection to build commercial engines e.g.
to achieve google s scalability over billions of documents we need to deal with unexplored issues such as parallelization.
.