convergentcontemporar ysoftware peerreviewpractices peterc.rigby concordia university montreal qc canada peter .rigby concordia.cachristian bird microsoft research redmond wa usa cbird microsoft.com abstra ct softwarepeerreviewispracticedonadiversesetofsoftwareprojectsthathavedrasticallydifferentsettings cultures incentivesystems andtimepressures.inanefforttocharacterizeand understandthesedifferencesweexaminetwo google ledprojects androidandchromiumos threemicrosoftprojects bing office andms sql and projects internaltoamd.wecontrastourfindingswithdatataken fromtraditionalsoftwareinspectionconductedonalucent projectandfromopensourcesoftwarepeerreviewonsix projects includingapache linux andkde.ourmeasures ofinterestincludethereviewinterval thenumberofdevelopersinvolvedinreview and proxymeasuresforthenumberof defectsfound duringreview.wefindthatdespitedifferences amongprojects manyofthecharacteristicsofthereviewprocesshaveindependentlyconvergedtosimilarvalueswhichwe thinkindicategeneralprinciplesofcodereviewpractice.we alsointroduceameasureofthedegreetowhichknowledge issharedduringreview.thisisanaspectofreviewpractice thathastraditionallyonlyhadexperientialsupport.our knowledgesharingmeasureshowsthatconductingpeerreviewincreasesthenumberofdistinctfilesadeveloperknows aboutby66 to depending ontheproject.thispaper isoneofthefirststudiesofcontemporaryreviewinsoftware firmsandthemostdiversestudyofpeerreviewtodate.
categories andsubject descriptors d. .
k. .
software management general terms management measurement keywords peercodereview empiricalsoftwareengineering inspection softwarefirms opensourcesoftware permission tomakedigital orhard copies ofallorpart ofthisworkfor personal orclassroom useisgranted without feeprovided thatcopies are notmade ordistrib uted forprofit orcommercial advantage andthatcopies bear thisnotice andthefullcitation onthefirstpage.
tocopyotherwise to republish topost onserversortoredistrib utetolists requires prior specific permission and or afee.
esec fse august saint petersb urg russia copyright acm978 ... .
.
.intr oduction softwarepeerreview inwhichanindependentevaluator examinessoftwareartifactsforproblems hasbeenanengineeringbestpracticeforover35years .whileeffective inidentifyingdefects therigidityoftraditionalformalreviewpracticeshasbeenshowntolimitadoptionandreview efficiency .incontrast contemporaryormodern peerreviewencompassesaseriesoflessrigidpractices .theselightweightpracticesallowpeerreviewtobe adaptedtofit theneedsofthedevelopmentteam.forexample peerreviewiswidelypracticedonopensourcesoftware oss projects.rigbyetal.
describedaminimalistoss processthatefficientlyfit thedevelopment team.however therewasalackoftraceabilityandtoolstosupportreview thatmadeitdifficult toexternallymonitortheprogressand qualityofanosssystem.despitealargebodyofresearch on peerreviewinthesoftwareengineeringliterature little workfocusesoncontemporarypeerreviewinsoftwarefirms.
therearepractitionerreports but theseareexperiential orbiasedbyacommercialinterestinthereviewtool beingexamined .todate practitionershavedriventhe developmentofcontemporarypeerreviewandthetoolsthat supportit .theproliferationofreviewingtools e.g.
codecollaborator reviewboard gerrit crucible andthe growingnumberofcompaniesusinglightweightreviewindicatessuccessintermsofadoption e.g.
google cisco microsoft butthereisnosystematicexaminationofthe efficacyofcontemporarypeerreviewinsoftwarefirms.
wepositthatcontemporarypeerreview reviewpracticed todaybymanycommercialandoss projects evolvedfrom themoretraditionalpracticeofformal inspectionsofadecade ormoreago.inthispaper wepresentanexplorationof aspectsofcontemporarypeerreviewinsoftwareprojects thatspanvaryingdomains organizations and development processesinanattempt to aggregateandsynthesizemore generalresults.ourprimaryconjectureisthatifthepeer reviewpracticesandcharacteristicsinmultipledisparate projects seetable2 havebecomesimilarastheyhave naturallyororganicallyevolved thensuchcharacteristics maybeindicativeofconvergentpracticesthatrepresent generallysuccessfulandefficientmethodsofreview.assuch thesecan beprescriptiveto otherprojectschoosingto add peerreviewtotheirdevelopmentprocess.
ouroverarchingresearchquestionishowdotheparametersofpeerreviewdifferinmultipledisparateprojects?we operationalizethisquestionforeach parameterofreview permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
copyright is held by the author owner s .
publication rights licensed to acm.
esec fse august saint petersburg russia acm 202table1 projectdatasets thetimeperiodwe examinedinyearsandthenumberofreviews project periodyearsreviews lucent .
apache .
.9k subversion .
.9k linux .528k freebsd .
k kde .
k gnome .88k amd .
k msbing .
k ms sqlserver2011 .
k msoffice2013 .
96k android .016k chromeos .1k39k .whatpeerreviewprocess e.g.
faganinspectionvs commit then review doestheprojectuse?
.howlongdoreviewstakeand howoftenare reviews performed?
.whatisthesizeofartifactunderreview?
.howmanypeopleareinvolvedinreview?
.howeffectiveisreviewintermsofproblemsdiscussed?
.doesreviewspreadknowledgeaboutthesystemacross thedevelopmentteam?
withthe exceptionofthelastquestion theseparameters ofreviewhavebeenstudiedinmanyexperimentsoverthe last35years .ourcontributionistocomparea largediversesetofprojectsontheseparameters.
thispaperisorganizedasfollows.in section2 weprovide abriefoverviewofthesoftwarepeerreviewliteratureand describethereviewpracticesoftheprojectswestudyin thispaper.in section3 wedescribethedatathatwemine andourmultiplecasestudymethodology.in section4 wepresentourcasestudyfindingsand describeconvergent and divergentpractices.in section5 weprovideafirst measurementoftheimpactofpeerreviewonknowledge sharinginadevelopmentteam.whilewediscussthreatsto validitythroughout thepaper weprovideafullerdiscussion ofthemin section6.in section7 weconcludethepaper.
.backgr ound and project information inthissectionweintroducethreetypesofpeer review traditionalinspection ossemail basedpeerreview and lightweight toolsupportedreview.wealsodescribethe projectsand datawehaveforeachreviewtype.thenovel datainthispapercomesfromadvancedmicrodevices amd microsoft andgoogle led projects.table2isintendedtoshowthetimeperiodsandsizeofdatasetwehave foreach project andisnotintendedforcomparisonsamong projects.intheremainderofthispaper wenormalizeand converttherawdatatoperformmeaningfulcomparisons.
.
softwar einspection softwareinspectionsarethemostformaltypeofreview.
theyareconductedafterasoftwareartifactmeetspredefined exitcriteria e.g.
aparticularrequirementisimplemented .
theprocess originallydefined byfagan involvessome variationofthefollowingsteps planning overview preparation inspection reworking andfollow up.inthefirst threesteps theauthorcreatesaninspection package i.e.
determineswhatistobeinspected rolesareassigned e.g.
moderator meetingsarescheduled andtheinspectorsexaminetheinspection package.theinspectionisconducted and defectsarerecordedbutnotfixed.inthefinalsteps theauthorfixesthedefectsandthemediatorensuresthat thefixesareappropriate.althoughtherearemany variationsonformal inspections theirsimilaritiesoutweightheir differences .
comparisondata weusedatathatporteretal.collected ininspectionexperimentsatlucent tocompareour findingsforcontemporaryreviewwithtraditionalsoftware inspection.theirstudywasconductedinasemi controlled industrialsetting.eachconditionintheirstudywasdesignedtoemulateaparticularvariationininspection process.
however theyfoundthatvariationininspection processes accountedforverylittlevariationinthenumberofdefects found duringreview.peopleand productmeasures suchas theexpertiseofthereview accountedformuchmoreofthe variance.
.
open sour cesoftwar epeerreview peerreviewisanaturalwayfoross developers who rarelymeetinperson toensurethat thecommunityagrees onwhatconstitutesa goodcodecontribution.mostlarge successfuloss projects seepeerreviewasoneoftheirmost importantqualityassurancepractices .onoss projects areviewbeginswithadevelopercreating apatch.
apatchisadevelopmentartifact usuallycode that the developerfeelswilladdvaluetotheproject.althoughthe levelofformalityofthereviewprocessesvariesamongoss projects thegeneralstepsareconsistentacrossmostprojects theauthorsubmitsacontributionbyemailingittothe developermailinglistorpostingtothebug orreviewtracking system oneormorepeoplereviewthecontribution itis modified untilitreachesthestandardsofthecommunity and itiscommittedtothecodebase.manycontributionsare ignoredorrejectedand nevermakeitintothecodebase .
thisstyleofreviewiscalledreview then commit rtc .in contrast tortc someprojectsallowtrusted developersto commitcontributions i.e.addtheircontributionstothe sharedcoderepository beforetheyarereviewed.themain orcoredevelopersfortheprojectarethenexpectedtoreview allcommits.thisstyleofreviewiscalledcommit thenreview ctr .allprojectsusertc butsomealsousectr depending onthestatusofthecommitterandthenatureof thepatch .
comparisondata wehavedatafromsixlarge successful oss projects whichwewillrefertoasthe oss projects in thispaper1 theapachehttpdserver thesubversionversion controlsystem thelinuxoperatingsystem thefreebsd 1althoughchromeandandroid haveopensourcelicenses theyareledbygoogle.203operatingsystem kdedesktopenvironment andgnome desktopenvironment.theresultsfromtheseprojectshave beenpublishedbyrigbyetal.
andareusedhere only forcomparison purposes.
.
peerreview atmicr osoft microsofthas slowlybeenevolvingitscodereviewprocess.
whilecodereviewhasbeenanintegralpracticeatmicrosoft formany years themethod usedhasshiftedfromsending patchesand discussionthemonlargeemail liststousing centralizedtools.
afewyearsago microsoftdevelopedaninternaltool codeflow to aidinthereviewprocess.inmanyprojects includingthemicrosoftprojectsthatweexaminedforthis study codereviewisprimarilyaccomplishedviacodeflow andoccursonceadeveloperhascompletedachange but priortocheckinintotheversioncontrolsystem.adeveloper willcreateareviewbyindicatingwhichchangedfilesshould beincluded providing adescriptionofthechange similarto acommitmessage andspecifyingwhoshould beincluded inthereview.thoseincludedreceiveemailnotifications andthenopenthereviewtoolwhich displaysthechangesto thefilesandallowsthereviewerstoannotatethechanges withtheirowncommentsandquestions.theauthorcan respondtothecommentswithinthereviewandcanalso submitanewsetofchangesthataddressesissuesthat the reviewershavebroughtup.onceareviewerissatisfiedwith thechanges hecan signoff onthereviewincodeflow.
whilemanyteamshavepoliciesregardingcodereviewsign off thereisnoexplicitconnection betweenthereviewsystem andversioncontrolthatdisablescheckin untilareviewhas beensignedoff.teams reviewpolicymay varyinmany ways.forexample somerequirejustonereviewertosign offwhileothersrequiremore somespecifywhoshouldsign offforchangesin differentcomponentsandsomeleaveitup tothedeveloper etc.formoredetails wereferthereader toanearlierempiricalstudy inwhichweinvestigated thepurposesforcodereview e.g.
findingdefects sharing knowledge alongwiththeactualoutcomes e.g.
creating awarenessandgainingcodeunderstanding atmicrosoft.
newdata inthispaper wepresenttheresultsofanalyzingreviewdatadrawnfromthreelargeprojectsatmicrosoft thatusecodeflowastheprimarymechanismforcodereview andthatdifferintheirdomainand developmentmethodology bing microsoftoffice2013 andmicrosoftsqlserver.
bingisaninternetsearchengine itiscontinuouslybeing developedand deployedand undergoesconstantdevelopment.
officeisasuiteofbusinessapplicationsthatshipsasaboxed productandfollowsamoretraditionalprocesswith phases forplanning implementation stabilization andshipping.
sqlserverisadatabasemanagementand businessintelligenceapplicationthatfollowsadevelopmentcyclesimilar tooffice.wepresentdetailsofdata gatheringforthese projectsinsubsection3.
.
.
google based gerrit peerreview whentheandroid projectwasreleasedasoss thegoogle engineersworking onandroidwantedtocontinueusingthe internalmondriancodereviewtoolusedatgoogle .
gerritisanoss gitspecificimplementationofthecode reviewpracticesusedinternallyatgoogle createdbygoogleengineers .gerritcentralizesgitacting asabarrier betweenadeveloper sprivaterepositoryandtheshared centralizedrepository.developersmakelocalchangesin theirprivategitrepositoriesandthensubmit thesechanges forreview.reviewersmakecommentsviathegerritweb interface.forachangetobemergedintothecentralized sourcetree itmustbeapprovedandverifiedbyanother developer.thereviewprocesshasthefollowingstages .
verified beforeareviewbeings someonemustverify that thechangemergeswiththecurrentmasterbranch and doesnotbreakthebuild.inmanycases thisstep isdoneautomatically.
.
approved whileanyonecancommentonthechange someonewithappropriateprivilegesandexpertisemust approvethechange.
.
submitted merged oncethechangehasbeenapproveditismergedintogoogle smasterbranchso thatotherdeveloperscanget thelatestversionofthe system.
newdata inthispaper wepresentresultsfromtwo google led oss projectsthatusethegerritpeerreviewtool androidandchromiumos.androidisanoperatingsystem developedformobileandtabletdevices.itisopensource softwareandwasinitiated bya groupofcompaniesknown asopenhandsetalliance whichisledbygoogle.2google chromiumos referredtoaschrome isanoperatingsystem whichrunsonlywebappsandrevolvesaroundthechromium webbrowser.3wepresentdetailsofdata gatheringforthese projectsinsubsection3.
.
.
amd andcodecollaborator ratcliffe presentedapractitionersreportontheadoptionofacodecollaboratorbasedpeerreviewpracticeonan internalamdproject whichservedasthemodelforother projectsatamd.thepracticeusedatamdinvolvesthe followingsteps theauthoruploadsthesoftwareartifacts forreviewinthewebinterface reviewersareassignedto thereview areviewdiscussionoccursand problemsare fixed onceareviewisapproveditiscommitted.the codecollaboratortoolallowsforassignmentofrulesandthe specificationandenforcementofbusinessrules e.g.
areview mustbeapprovedby2reviewersbeforeitcan becommitted .
whileasynchronousdiscussioncanoccur achatinterface canalsobeused.cohen thefounderofthecompanythat sellscodecollaborator performedadetailedevaluationof thetoolatcisco .
newdata ratcliffe spractitionersreportwasmainlyqualitative.inthiswork wepresent thequantitativeresultsfrom theuseofcodecollaboratoratamd.theamddataset islimited soweindicatebelow whenweareunablepresent resultsforamd.
.
contemporary peerreview process comparingtheabovereviewprocesses wefindthatcontemporarypeerreviewischaracterizedbybeinglightweight aversioncontrol repositorythatmanydevelopersdepend upon e.g.
themasterbranch .thisprocesscontrastssharplywithtraditional softwareinspectionwherelargecompletedartifactsare reviewedinco locatedmeetingwithrigidlydefinedgoalsand participantroles.contemporaryossreviewislightweight and fitsthedevelopment team butwhenitisconducted onamailinglistitisdifficulttotrack.someoss projects andallthesoftwarefirmsweexamineuseareviewtool whichmakestheprocesstraceablethought thecollection ofreviewmetrics.contemporaryreviewsaretypicallyconductedasynchronouslyandmeasuresofreviewarerecorded automatically.
convergent practice1 contemporarypeerreviewfollowsalightweight flexibleprocess ingeneral contemporaryreviewinvolvesthefollowing steps.
.theauthorcreatesachangeandsubmitsitforreview.
.developersdiscussthechangeandsuggestfixes.the changecan bere submittedmultipletimestodealwith thesuggestedchanges.
.oneormorereviewersapprovethechangeanditis addedtothe main versioncontrolrepository.the changemayalsoberejected.
.methodology and da ta weuseyin smultiplecasesstudymethodology .case studyfindings generalize oraretransferablethroughanalyticalgeneralizations.unlikestatisticalgeneralization which derivessamplesfromandgeneralizesto adefined population analyticgeneralizationrequiresresearcherstodevelop atheoryorframeworkoffindingsrelatedto aparticular phenomenon.weusetheoreticalsamplingtoselectadiversesetofcasesandthencontrastourfindingsdeveloping aframeworkthatdescribestheconvergentand divergent practicesofcontemporarypeerreview.
webegan bycollectingdata onmicrosoftreviewpractices andweresurprisedtoseeconvergenceswiththepracticesobservedbyrigbyonoss projects .thesepracticestended tocoincidewiththoseseenatamd andcisco .we collected data onthegoogle ledoss projects chromiumos andandroid tounderstandthepracticesofhybridprojects.
wealsohavedata onthetraditional inspection practicesat lucent thatweuseforcomparison purposes.
wequantifyhowlightweight tool supportedreviewis conducted.since eachcasestudyhasdifferentdatapoints andmeasures afurthercontributionofthisworkisthe conversionofrawandsummarydatafrompastandcurrent casestoreportcomparablemeasures.wecontributeaunified setoffindingsacrossalarge diversesampleofprojects.
inthis section wegiveanoverviewofthedatawehavefor each project.ineachsubsequentsection wediscussin detail thepertinentdataandmeasures.wealsodiscusslimitations inourdata andconstructvalidityissues.
.
data extraction thedataextractionforthefollowingprojectsisdescribed inotherwork lucent oss projects andamd .
theformertwodatasetsareusedforcomparison purposes whiletheamddatahadnotbeenquantitativelyreportedin previouswork.in previouswork wedescribedtheextraction processandresultingdataforgooglechromeandandroid thedataisalsoavailableforotherresearchers .this workdid notinvolveanalysisofthedata.intheremainder ofthissection wediscusswhatconstitutesareviewforeach projectand brieflydescribehow we extractedpeerreview data.
microsoft themicrosoftdataforthisstudywascollected fromthecodeflowtool.thistoolstoresalldataregarding codereviewsinacentral location.webuiltaservicetomine theinformationfromthislocationandkeepadatabaseup todatefortoolstoleverageandforempiricalanalysis.for eachreview werecordinformationincludingwhocreatedthe review whatfilesweremodified howmanysetsofchanges weresubmitted thecommentsthatreviewersadded and whosignedoff.
onedifficultywiththisdataisknowingwhenareview iscomplete.thereareanumberofstatesthatareview can bein oneofwhichis closed .however tobeinthe closed state someonemustexplicitlyset thereviewtothat state.weobservedthatinpractice adevelopermaycheck in hischangesoncereviewershadsignedoffwithoutfirst changingthereviewto closed .inothercases therewas evidencethatamemberofaprojectclosedreviewsasaform ofmaintenance onepersonclosedthousandsofreviewsina matterofminutes .todealwiththis weusetheheuristic thatareviewisconsideredcompletedatthetimeofthelast activitybyaparticipantinthereview i.e.thedateofthe lastcommentorthelastsignoff whicheverislater .for allthecasesstudiesinthiswork reviewswith nocomments orsignoffswereexcludedfromthedatasetasnoreview discussionoccurs.
googlechromeandandroid weconsiderreviewsinthe mergedandabandonedstates openreviewsarenotconsideredinthiswork.reviewsmustalsohaveonecommentfrom ahumanreviewerwhoisnot theauthor verificationsby botsareremoved .tocollectpeer reviewdatafromthese projects wereverse engineeredthegerritjson apiand queriedthegerritserversfordataregardingeachreviewfor both projects gatheringinformationsuchastheauthor s andreviewers activity fileschanged commentsmade and datesofsubmissionandcompletion.westoredthisinformationonpeerreviewsinadatabaseforfurtheranalysis.the extracteddataand detailsofourtechniqueareavailableto otherresearchers .
amd weattainedasummaryofthedatadumpfromthe codecollaboratortool .unfortunately thisdatasetdoes nothavealltheparametersofreviewwewishtomeasure suchasthenumberofcommentsperreview.inthisdata set weonlyincludereviewdiscussionsthathaveatleastone reviewer.
lucent siyattendedinspectionmeetingsandcollected self reportdatafromreviewersonacompilerprojectat lucent .therolesand numberofparticipantswere205specifiedinadvance.sincethisiscomparisondata we discussdifferences butdonotpresent thisdatain figures.
ossproject rigbyetal.
sworkconsideredsixossproject apache subversion linux freebsd kde andgnome .thereviewdatawasextractedfromdevelopermailinglists.forareviewtobeconsideredvalidithadtocontain thefollowing achangeor diff and2 oneormoreemails fromreviewers i.e.not theauthorofthechange .both acceptedandrejectedchangesthatwerereviewedareinthe dataset.likethelucentdata wedonotreportthisdata inourfigures.
plottingthedata weusetwotypesofplots beanplots and boxplots.beanplotsshowthedistributiondensityfor multiplesamplesalongthey axis ratherthanmorecommonlyalongthex axis toenableeasy visualcomparison andinthisworkcontainahorizontal linethatrepresents themedian .beanplotsarebestforalargerangeof non normaldata astheyshowthe entiredistribution they essentiallyshowthefulldistribution drawnvertically and showwhethertherearepeaksandvalleysinadistribution whileboxplotsarebetterforsmallerranges.whenwehave countdatathatishighlyconcentrated weuseaboxplot.
foralltheboxplotsinthiswork thebottomandtopof theboxrepresent thefirstandthirdquartiles respectively.
eachwhiskerextends1.5timestheinterquartilerange.the medianisrepresentedbytheboldlineinsidethebox.since ourdataarenotnormallydistributed regardlessofthestyle ofplot wereportand discussmedianvalues.
.mul tiple case stud yfindings inthis section wepresentourconvergentand divergent findingsinthecontextofiterativedevelopment reviewers selection practices reviewdiscussionsand defects andknowledgesharingthroughreview.foreach finding weplaceitin thecontextofthesoftwareengineeringliteratureonpeer review summarizeitina convergentpractice box and thendiscusstheevidencethatwehaveforeach practice.
.
iterati vedevelopment theconceptofiterativedevelopmentisnotnewandcan betraced backtothemanysuccessfulprojectsintheearly daysofsoftwaredevelopment .however progressive generationsofsoftwaredevelopershaveworkedinshorter andshorterintervals.forexample continuousbuilds and releaseearly releaseoften .peerreviewisno exception.
anoriginalgoalofsoftwareinspectionwastofindsoftware defectsbyexposing artifactstocriticismearlyinthedevelopmentcycle.forexample faganinspectionintroducedearly andregularcheckpoints e.g.
afterfinishing amajorcomponent thatwould find defectsbeforethesoftware srelease.
however thetimefromwhenthereviewstartedtowhenthe discussionended i.e.thereviewinterval wasontheorderof weeks .in1998 porter reportedinspectionintervals atlucent tohaveamedianof10days.oss projectslike apacheandlinuxhavereviewintervalsontheorderofa fewhoursto aday .android chrome amd bing office sql projectsinterval in da ys log min hr .
figure1 firstresponseonleft wedo nothave firstresponsedataforamd andfull intervalon right convergent practice2 reviewshappenearly beforea changeiscommitted quickly andfrequently amd microsoft andthegoogle ledprojectsexemplify theconvergentpracticeoffrequentreviews figure2 that happenquickly figure1.thereviewsarealwaysdone early i.e.beforethecodeischeckedintotheversioncontrol system .
amd amdhasshortreviewintervals withthemedian reviewtaking .5hours.thenumberofreviewspermonth isalsohighandincreasesfromafewreviewspermonthwhen thetooland practicewasintroduced to over500reviews permonth.
microsoft bing sql andofficealsoshowshortintervals forreviewswithmediancompletiontimesof14.
.
and .9hoursrespectively.intermsofreviewspermonth all threeprojectsareveryactive butshowdifferent trends.
sqlhasamedianof3739reviewspermonthandisfairly consistentmonthtomonth.incontrast binghasamedian of2290 buthasshownasteadyincreaseovertimesinceits initialadoptionofcodeflow.officehasthehighestmedian at4384 anditfollowsatypicalreleasecyclewithaninitial rampupofreviewsandafall offnearrelease.
googlechromeandandroid themedianfrequencyis 1576and310forchromeandandroid respectively.the mediancompletiontimeis15.7and20.8hours forchrome andandroid respectively.
projectcomparisons thereviewinterval whichison theorderofhoursandwithamedianaroundaday shows2061 android chrome amd bing office sql projectsreviews per month log figure2 thenumberofreviewspermonth remarkableconsistencyacrossallprojects.figure1 also showstheamountoftimeittakesforthefirstresponseto areview.wecanseeforallprojectsthatmostreviewsare pickedupwithanfewhours indicatingthatreviewersare regularlywatching and performingreview.
thenumberofreviewspermonth orthereviewfrequency isveryhighincomparisontotraditionalinspection practices but tendstovarywith thestage developmentstyle and sizeoftheproject adivergentfinding .infigure2 wecan seethreedistinct typesofprojects adoption e.g.
bing cyclic e.g.
office andstable e.g.
chrome .thelongtails ineach beanplotshowthatadoptiontookplace andwith amdandbingtheamountofreviewisstill increasingwith eachmonth.thistrendcan beseeninfigure3 which plots bingdata asatimeseries.incontrasttothismonotonic trend cyclicprojects likeandroid freebsd officeshowan irregularconeshape withgradualfluctuationsintheamount ofdevelopmentandreview seeofficeinfigure2 .finally chromeand sqlshowarelativelystablenumberofreviews.
linuxandkdeexhibitsimilartrends.
convergent practice3 changesizesaresmall having ashortintervalcannotbeachievedwithoutchanges to otheraspectsofsoftwaredevelopment.bycreatingsmaller changes developerscanworkinshorterintervals.forexample mockusetal.notedthatapacheandmozillahadmuch smallerchangesizesthantheindustrialprojectstheyused forcomparison butdidnotunderstandwhy .on theoss projects studiedbyrigbyetal.
themedianchange onoss projectsvariesfrom11to 32lineschanged.they arguedthat thesmallchangeonoss projectsfacilitates frequentreviewofsmall independentchanges.reviews per month bing office monthreviews figure3 numberofreviewspermonthinbingand office.wewererequestedtokeeprawnumbersand datesconfidential butthisplotshowsthetrendsin codereviewasatoolisadopted bing andoverthe courseofareleasecycle office .
fromfigure4 bothandroidandamdhaveamedian changesizeof44lines.thismedianchangesizeislargerthan apache 25lines andlinux 32lines butmuchsmallerthan lucentwherethenumberofnon commentlineschangedis 263lines.bing office sql andchromehavelargermedian changesthantheotherprojectsexamined butarestillmuch smallerthanlucent.forexample chrome smedianchange is78linesandincludes5files.however forchrome only ofchangesarethesamesizeorlargerthanamedian lucentchange.furthermore thedistributionofchanges ongoogle ledandtheotheross projectareleftskewed indicatingthat themajorityofchangesaresmall.whilethe distributionforthecommercialfirmsisalsoleftskewed itis almostlognormal.
.
selecting reviewers traditionally developersareassignedtoreviewanartifact.
onoss projects developersselectthechangesthat theyare interestedinreviewing and noreviewsareassigned.many reviewtoolsallowforassignmentaswellasself selection incorporating apositivemixofbothtechniques .
theself selection usedinreviewtools isaccomplishedby adding a group e.g.
amailinglist tothereviewerlist then individualsfromthisgroupcan findthereview .in thissection wediscusstheoptimalnumberofreviewersas wellasdifferentreviewerselectiontechniques.
theoptimalnumberofinspectorsinvolvedinameeting haslongbeencontentious e.g.
.reviewsareexpensivebecausetheyrequirereviewerstoread understand andcritiqueanartifact.anyreductioninthenumberof reviewersthatdoesnotleadto areductioninthenumber ofdefectsfoundwillresultincostsavings.buck found nodifferenceinthenumberofdefectsfound byteamsof three four and fiveindividuals.bisantandlyle proposed twopersoninspectionteamsthateliminatedthemoderator.
inexaminingthesourcesofvariationin inspections porter207android chrome amd bing office sql projectslines changed log figure4 churn linesaddedandremoved.note wedo notshowproportionofchangesover10 lines.
etal.
foundthat tworeviewersdiscoveredasmanydefectsasfourreviewers.theconsensusseemstobethat two inspectorsfindanoptimalnumberofdefects .inoss review themedian numberofreviewersistwo however sincethepatchandreviewdiscussionsarebroadcastto a largegroupofstakeholders thereisthepotentialtoinvolve alargenumberofinterestedreviewersifnecessary .
convergent practice4 tworeviewersfindanoptimal numberofdefects atamdthemedian numberofreviewsis2.whilereviewersaretypicallyinvited ratcliffedescribeshowcodecollaboratorallowsinvitestobebroadcastto a groupofdevelopers .hefurtherdescribeshowcodecollaboratorsuggests potentialreviewersbasedonwhohasworkedonthefilein therecentpast.
forgooglechromeandandroid thereisamedianof tworeviewers seefigure5.gerritallowsdevelopersto subscribetonotificationswhenareviewincludeschangesto aparticularpartofthesystem .reviewerscanalsobe invitedwhentheauthorincludestheiremailaddressinthe reviewsubmissionsenttogerrit.
atmicrosoftthemedian numberofreviewersinvitedto eachreviewinbing office and sqlrespectivelyare3 and4.asfigure5shows themedian numberofpeoplethat actuallytakepartinareview otherthantheauthor is2.
interestingly wefoundthattherewasonlyaminimalincrease inthenumberofcommentsabout thechangewhenmore reviewerswereactiveandtherewasnoincreaseinthenumber ofchangesetssubmitted i.e.
thesamenumberof rounds ofreviewing .wealsoinvestigated bothqualitativelyandandroid chrome amd bing office sql1 projectsreviewers per re view log figure5 tworeviewersinvolvedinreviewinthe mediancase quantitatively reviewsthathadmanymorereviewersthan themedianandfoundthat theauthororthereviewerswill inviteadditionaldevelopersafteraroundofreviewinghas takenplace.thiscanbetheresultofadeveloperrealizing thatsomeoneelseisbetterfit toexaminethechangeor concludingthat thechangecarriesahighriskandshouldbe reviewedby moreeyes .thegeneralpracticeappearsto involveinvitingthreetofourreviewersandthenlettingthe reviewtakeitscoursewhichmayleadtoinvolving additional participants.
.
defects vsdiscussion therigidtimeconstraintsofsynchronousreviewmeetingsforcedtraditional inspectionstofocusexclusivelyon findingdefects discussionsofothertopics suchas solutions tothedefect werestrictlyforbidden .inspection used explicitroles suchasreaderandsecretary toensurethat defectswereaccuratelyrecordedandthatdeveloperswere notdistractedfromfindingsdefects .atlucentthereisa medianof3truedefectsfound perreview .anadditional 4defectsperreviewwerefoundtobefalsepositives.inspectionsalsofoundalargenumberofsoftmaintenanceissues median13perreview whichincludedcodingconventions andtheadditionofcomments.thistypeofsoftmaintenance codeimprovementswasalso observedatmicrosoftandin ossreview .incontrasttosoftwareinspection asynchronousreviewshavelessrigidtimeconstraintsallowing fordetaileddiscussionsofsoftwareartifacts.forexample onoss projects thediscoveryofthedefectisnot thefocal point.instead developersdiscusspotentialdefectsandsolutionstothesedefects.theseteamdiscussionsmeanthat theauthorisnolongerisolatedfromhisorherpeerswhen208android chrome bing office sqlcomments per re view log figure6 numberofcommentsperreview fixingthedefectsfound duringreview .inthissection wealsoprovideproxymeasuresforthenumberofdefects foundandshowthat theyarecomparabletothosefound duringinspection.
convergent practice5 reviewhaschangedfroma defectfinding activitytoa group problemsolving activity examiningcontemporarypracticesinsoftwarefirms we findconvergencewithoss defectsarenotexplicitlyrecorded.
amdusescodecollaborator which hasafieldtorecordthe numberofdefectsfound however ofreviewshaveno recordeddefects andonly7 havetwo ormoredefectsfound.
measuresofreviewactivityindicateamedianoftwoparticipantsperreviewandqualitativeanalysisbyratcliffe foundthatdiscussionsdidoccurbutfocusedon fixingdefects insteadofrecordingtheexistenceofadefect.thedisconnect betweenexplicitlyrecordeddefectsandactivityonareview indicatesthatreviewersareexaminingthesystem butthat developersarenotrecordingthenumberofdefectsfound.
microsoft scodeflowreviewtoolprovidesfurtherevidence itdoesnotprovideawayfordeveloperstorecordthe defectsfound duringreview.thisdesign decisionresults fromthewaythatcodereviewispracticedatmicrosoft.
whenanauthorsubmitsachangeforreview theauthor andotherreviewershaveajointgoalofhelpingthecode reachasatisfactorylevelbeforeitischeckedin.wehave observedthatreviewerswillcommentonstyle adherence toconventions documentation defects missedcornercases andwillalsoaskquestionsabout thechanges inaneffort tohelptheauthormakethecodeacceptable.itisunclear whichoftheserepresentdefectsandwhich donot e.g.wouldandroid chrome bing office sql1 projectsnumber of resubmissions per re view log figure7 numberofsubmissionsperreview thecomment areyousureyoudon tneedtocheckagainst nullhere?
beadefect?
.inaddition recordingthedefects found duringreviewwouldnotaid intheaforementionedgoal.
codeflowdoesprovidetheabilityforanauthortomark anythreadofconversationwithastatusof open ignored resolved or closed enablingparticipantstotrackthe variousdiscussionswithinthechanges.forourpurposes the closestartifact to adefectisathreadofdiscussionthathas beenmarkedasresolved asaproblemfoundwithinthecode wouldneedtoberesolvedbytheauthorpriortocheckin.
thecaveatisthatareviewermightmakecommentsorask questionsthatleadtodiscussionandareeventuallymarked asresolved but thatdon trepresentadefectfoundorresult inanycodebeingchanged.
onthegooglechromeandandroidprojects thegerrit reviewtooldoesnotprovideanyfieldtoexplicitlyrecord thenumberofdefectsfound.however aswediscussedin section2 reviewspassthroughthreestages verifiedto notbreakthebuild reviewed andmerged.thegoalof thesestagesisnot tosimplyidentifydefects buttoremove anydefectsbeforemergingthecodeintoacentral shared repository.aswecanseefromfigure6 thereisamedian of4 and3commentsperreviewforchromeandandroid respectively discussionoccursontheseprojectsatsimilar levelsto otheross projects.ontheindustrialside the mediansarethesame with4 and3commentsforbing office and sqlrespectively.
youcan tcontr olwhat youcan tmeasur e thecontemporarysoftwareprojectswestudieddonotrecord thenumberofdefectsfound duringreview in partbecauseit distractsdevelopersfromtheirprimarytaskofimmediately fixingthedefectsfoundinreview.however without this209table2 descriptivestatisticsforthenumberof comments threadsofdiscussionandthreadsmarked asresolvedinbing office andsql.
project commentsthreadsresolved 1stquartile bingmedian 3rdquartile 1stquartile officemedian 3rdquartile 1stquartile sql median 3rdquartile measurecansoftwareprojectsimprovetheirprocessand productinthelongterm?aretherealternativemeasuresof revieweffectiveness?
wesuggestthreealternativemeasuresthatwhentaken togetherprovideanapproximationofrevieweffectiveness.
first thenumberofcommentsduringreviewisan upper boundonthenumberofdefectsfound perreview seefigure6 .theunderlying assumptionisthateachcomment representsadistinctdefect.thisassumptionisofteninvalidasmanycommentswillberelatedtothediscussionof asingledefect.inourmanualanalyses wefoundthatit wasextremelyrareforacomment toincludemorethanone substantivedefect however trivialformattingissueswere oftenreportedinasinglecomment.second abetterestimateisthenumberofcomment threads seetable2 .the assumptionisthateachthreadcontainsasingledefect however sometimesacommentthreadwillcontaindiscussions ofmultiplerelateddefects othertimesitwillcontainfalse positives suchasdeveloperquestionsthatdonotuncover adefect.third alowerboundonthenumberofdefects foundinareviewisthenumberofartifactresubmissions seefigure7 .fornon trivialdefects arevisedartifactmay besubmittedforre review.however arevisionwillcover allthefixeddefectsidentifiedduring areviewsession.since codeflowistheonlytoolthat tracksthreadsofconversation wereportthesummarystatisticsofthenumberofcomments threads andthreadsmarkedasresolvedintable2.
thesesmeasuresprovidenon intrusivetechniques i.e.the dataisimplicitlyrecordedduringtheprimaryactivityof discussingthesoftware to approximaterevieweffectiveness.
wedonotwant tomakestrongclaimsaboutrevieweffectivenessbecausethesemeasuresareproxiesofthenumber ofdefectsfoundandartifactsizestendtobesmallerthan intraditional inspection.however wefeelthat thelevelof discussion duringreviewand patchresubmissions suggests thatcontemporaryreviewdoesfind defectsatacomparable leveltotraditionalinspection.thesemeasuresandreview practicesoncontemporaryprojectsraisealargerphilosophicalquestionthatdeservesfuturework isitmoreimportant tohaveadiscussionabout thesystemortofindandreport defects?
.sharing kno wledge thr ough review thenumberofdefectsfound duringreviewisknowntobe alimitedmeasureofrevieweffectivenessbecauseitignores manyoftheotherbenefitsofreview suchasthesharing of knowledgeamongdevelopers .someofthebenefitsof spreadingknowledgeacrossthedevelopmentteaminclude havingco developerswhocan doeachother sworkifa developerleavesaprojectandinvolvingnewdevelopersin reviewstofamiliarizethemwithaproject scodebase.while qualitative evidencefrompractitionersindicatesthatreview doesindeedspreadknowledgeacrossthedevelopmentteam weareunawareofanyempiricalstudiesthatmeasure thisphenomenon.
toprovideapreliminarymeasurementoftheknowledge spreadingeffectofpeerreview weextendtheexpertise measuredeveloped bymockusandherbsleb .theymeasuredthenumberoffilesadeveloperhasmodified submitted changesto .wealsomeasurethenumberoffilesadeveloper hasreviewedandthetotalnumberoffilesheknowsabout submitted reviewed .figure8showthat thenumberof filesadeveloperhasmodified ontheleft comparedtothe totalnumberoffilesheorsheknowsabout ontheright .
forexample inthemediancase agooglechromedeveloper submitschangesto24distinctfiles reviews38distinctfiles andknowsaboutatotalof43distinctfiles.withoutreview inthemediancase achromedeveloperwouldknowabout 19fewerfiles adecreaseof44 .similarly inthemedian caseforbing office and sql reviewincreasesthenumber offilesadeveloperknowsaboutby100 and150 respectively.
bothgooglechromeandandroidappeartohavealarger numberofdeveloperswhohavesubmittedto andreviewed fewfiles.oss projectareknowntohave whatoneintervieweecalled drive by developers whosubmitasingle change e.g.
abugfixthateffectsthedeveloper .figure8showsthatthiseffectisespeciallypronouncedon theandroidprojectwhere54 ofdevelopershavemodified fewerthan fivefiles.theincreaseinthenumberoffiles seen throughreviewisalsolowerforandroid a increasein themediancase.ifwe excludedeveloperswhohavemodified fiveorfewerfiles weseethemediannumberoffilesmodified jumpsfrom6to16 andthetotalnumberoffilesgoesfrom 10to .
ourmeasureofknowledgesharingthough peerreview hasshownasubstantial increaseinthenumberoffilesa developerknowsaboutexclusivelybyconductingreviews.
thismeasuredeservesfuturestudy.enhancementstothe measurecouldalsobeusedto gaugethediversityofthe knowledgeofdevelopersassignedto areview.ifareview hasdevelopersfromdiversepartsofthesystemreviewing thecodeand discussingit itislesslikelythattherewillbe downstreamintegrationproblems.
4weconservativelyexcludesubmissionsandreviewsthat containmorethan10files.210android chrome bing office sql projectsfiles seen log figure8 onleft thenumberoffilessubmittedfor review onright thetotalnumberoffileseither submittedforrevieworreviewed .threa tstovalidity westudiedalarge diversesampleofprojects however each projecthasdifferenttools processes incentives etc sothedatawecollectedisnotascontrolledandfreefrom confoundsasitwould beinanexperimentalsetting.wehave attemptedtocleanandreportthedatausingsimilarmeasures andmethodsand havediscussedlimitationsthroughout the paper.
whenafindingwasunusualwewouldreadtheassociated anomalousreviewsand discussthemwith developers.for example wehaveremovedreviewsthatreceivedno activity fromthemicrosoftdatasets reviewsthathad nocomments nosignoffs andonlyonesubmittedchangeset .upon initiallyfindingtheseinourdataset weinquiredofthe developerswhocreatedthereviews.theyindicatedthat sometimesreviewsaremoreforawareness e.g.
alerting amanagerto afeaturebeingimplementedorshowing a testercodethat theyshouldwriteatestfor thanactually reviewingthechangesandthat thecompletelackofactivity representsreviewsthatareusedforawarenesspurposesonly.
itispossiblethataportionofthesereviewswereactually intendedasactualcodereviewsratherthanforawareness andsimplyreceivedno attentionfromtheinvitedreviewers.
thistypeofreviewhasalsobeenremovedfromthegoogleled projectsandfromtheoss projects studiedbyrigby etal.
and usedforcomparison purposesinthispaper.
insomecases were ranmeasuresonrigby srawdatasets.
theamdandlucentdatasetspresentedfurtherdifficulties becausewehavesummaryinsteadofrawdata.wehave convertedtherawdatatomakeitcomparablewiththeother datawecollected however weareoftenmissingtherequired data.
.conclusion thevariationsintraditional formalsoftwareinspections werefoundtohavelittleimpactonitseffectivenessasa peerreviewprocess withproductand processfactorsbeing betterpredictorsofthenumberofdefectsfoundinreview .furthermore aswiegerspointsinhispracticalguideto peerreview the similarities outweightheirdifferences .
contemporarypeerreviewrepresentsalightweight strippeddown versionofsoftwareinspectionthatremovestherigidity oftheformalinspection processes whileleavingtheeffective defectfindingtechniqueofhaving anexpertpeerexamine softwareartifactsbeforetheyareaddedtothesharedversion controlrepository.
contemporarypeerreviewhasevolvedfromtheneedsof practitionersandthesepractitionershavedriventhedevelopmentofreviewtools .thelargebodyofliteratureon softwareinspection haslargelyignoredthesecontemporary practices.inthispaper wehavepresentedfindingsonthe peerreviewpracticesusedonthreeprojectsrepresentativeof developmentatmicrosoft amdprojects andtwogoogleledoss projects.wehavecomparedparametersofreview suchasreviewintervalandthenumberofcommentsinreviewdiscussions ofthesesixprojectswiththedatafrom rigby sstudyofsixoss projects .wealsousedata frominspectionatlucentasacontrast.wefoundthat whilethereweresomeminordivergencesincontemporary practice theirsimilaritiesoutweighedtheirdifferences i.e.
thefindingsconverged.
theconvergentcontemporarypeerreviewpracticescan can bedescribedasthefollowing.
.contemporaryreviewisperformedregularlyandquickly justbeforethecodeiscommittedinsteadofwhena largerworkproductiscompleteasininspection.
.contemporaryreviewusuallyinvolvestworeviewers.
however thenumberofreviewersisnotfixedand canvaryto accommodateotherfactors suchasthe complexityofachange.
.contemporaryreviewersprefersdiscussionand fixing codeoverreportingdefects.
.toolsupportedreviewprovidesthebenefitsoftraceability whencomparedtoemailbasedreview and canrecordimplicitmeasures whencomparedtotraditionalinspection.theriseinadoptionofreviewtools providesanindicatorofsuccess.
afinalcontributionofthispaperwasanovelmeasureof thedegreetowhichreviewsspreadsknowledgeacrossthe development team.thismeasureprovidesaquantificationof knowledgespreadthathaspreviouslyonlyhadexperiential support.wefindthatreviewincreasesthenumberofdistinct filesadeveloperknowsaboutby66 to depending ontheproject.wefeelthatfutureworkisnecessaryto determinewhetherlightweightmeasures suchasthediversity andamountofdiscussionduringreview can beusedasnew releasequalitymeasures.insteadofcountingthenumber ofdefectsfoundinamodule amanagermightask have developerswithsufficientlydiversebackgroundsdiscussed thisnewsectionofcodeenoughforittobereleased?
.