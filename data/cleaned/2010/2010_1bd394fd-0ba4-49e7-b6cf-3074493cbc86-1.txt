bugcache for inspections hit or miss?
foyzur rahman daryl posnett abram hindle earl barr premkumar devanbu department of computer science university of california davis davis ca.
usa mfrahman dpposnett ajhindle etbarr ptdevanbu ucdavis.edu abstract inspection is a highly e ective but costly technique for quality control.
most companies do not have the resources to inspect all the code thus accurate defect prediction can help focus available inspection resources.
bugcache is a simple elegant award winning prediction scheme that caches les that are likely to contain defects .
in this paper we evaluate the utility of bugcache as a tool for focusing inspection we examine the assumptions underlying bugcache with the aim of improving it and nally we compare it with a simple standard bug prediction technique.
we nd that bugcache is in fact useful for focusing inspection e ort but surprisingly we nd that its performance when used for inspections is not much better than a naive prediction model viz.
a model that orders les in the system by their count of closed bugs and chooses enough les to capture of the lines in the system.
categories and subject descriptors d. .
testing and debugging code inspections and walk throughs general terms experimentation measurement reliability veri cation keywords empirical software engineering fault prediction inspection .
introduction the later a bug is discovered the more expensive and difcult it is to resolve .
deployed bugs are the worst of all requiring costly embarrassing and labor intensive software updates in the eld and or re deployments.
thus stakeholders such as managers and quality assurance people worry most about faults that escape into released and deployed products.
quality control methods such as inspection and permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
esec fse september szeged hungary.
copyright acm ... .
.testing aim to detect faults prior to release.
unfortunately code inspection and testing are costly in terms of time and manpower so managers seek to optimize their e ectiveness.
bug prediction has been suggested as a means to this end.
if one could predict which entities modules les or lines of code are bug prone quality control teams might focus their e ort on inspecting those entities.
in their award winning paper on bugcache kim et al.
hypothesized that past knowledge of fault occurrence can optimize testing and inspection e ort.
their proposed cacheinspired models depend on the exploitable locality of faults.
they hypothesized three types of locality.
first faults are more likely to occur in recently added changed entities churn locality .
second if an entity contains a fault then it is likely to contain more faults temporal locality .
finally entities that are logically coupled by co changes with other faulty entities are more likely to contain faults spatial locality .
to exploit fault locality kim et al.
uses two types of changes bug introducing changes and bug xing changes along with change history to identify a subset of entities that are highly likely to contain defects.
they implemented two types of caches bugcache a theoretical model and fixcache a practical prediction model.
bugcache contains faulty entities and is updated when a bug or fault is introduced into a le.
in reality bug introductions usually occur unnoticed and hopefully discovered later!
so bugcache is a theoretical model that can t be used until the full project history is known.
fixcache on the other hand is a practical realization of bugcache that doesn t need oracular knowledge.
the evaluation of fixcache drew our attention we wanted to replicate it from a di erent perspective with an emphasis on inspection.
fixcache was originally evaluated using the measure of hit rate which is the cumulative percentage of the successful cache probes through a project history.
for regression testing this measure is e ective if fixcache accurately identi es the defect prone les one can run tests only on defect prone modi ed les.
however our concern is inspection e ciency .
kim et al.
wrote a manager then can use the list for quality assurance for example she can test or review the entities in the bug cache with increased priority our italics .
thus we evaluate fixcache from the point of view of a stakeholder involved in code inspection how much e ort does fixcache actually save inspectors?
code inspectors would prefer to focus their e ort on buggiest of entities in order to achieve a greatest possible payo of bug detection for a given investment in inspection time.
suppose hypothetically that fixcache simply loaded the largest les into the cache.
in this case a high hit rate would not necessarily translate into an e ective inspection process a great many lines of code might have to be inspected to nd bugs.
likewise loading smaller les might lead to fewer hits but might result in e cient inspection that discovers more bugs per unit of work.
to this end we rstevaluate fixcache not against hit rate cache hits but against defect density and the cost e ectiveness of inspection.
second we address the questions what makes fixcache work?
and of the di erent cache update policies it uses which is most e ective?
third we investigate whether certain modi cations to these cache update policies would improve fixcache s performance with respect to the inspection cost e ectiveness.
finally we study whether fixcache outperforms in terms of inspection cost e ectiveness a very simple standard bug prediction model.
this paper mounts a replicated study of fixcache and makes the following contributions we evaluate fixcache using defect density.
we nd that in fact fixcache does indeed nd more bugdense entities.
we study how the di erent update and replacement policies applicable contribute to fixcache s bug prediction and inspection e ciency using an inspection cost e ectiveness measure.
we nd that there is not much di erence in the performance of di erent update and replacement policies.
we compare fixcache with a simple prediction model using the same inspection measure and nd that there is not much di erence .
.
theory fixcache is easily described.
to start the cache is primed with a set of les typically the largest les.
then the fixcache algorithm monitors the commits to the system and updates its cache using a set of policies.
a hitoccurs when a le in the cache undergoes a bug x a miss occurs when a le outside the cache receives a bug x. the cache s contents at any one point in time contains the target set of buggy les for testing or inspection.
fixcache uses update policies.
when a bug x is committed fixcache immediately adds the modi ed les to its cache this is the temporal locality policy.
next it retroactively nds and adds the le s from the commit s in which the bug was introduced as well as the les that were most often committed together with these les at that time.
this is the spatial locality policy.
finally fixcache adds recently added changed les to the cache.
we call this the churn locality policy.
all these les are added to the cache if they are not already present when a bug x is observed in the anticipatory belief that they will require xes soon.
kim et al.
empirically evaluated fixcache using various cache sizes most often of the entities in the system and found it very accurate at identifying faulty les.
they also evaluated fixcache s accuracy at method granularity and found its accuracy to fall within .
in this paper we only evaluate fixcache at le granularity.
several papers have reported re implementationsof fixcache for various applications our implementation achieved performance levels similar to those reported in these earlier papers roughly hit rate at le granularity as we discuss below.
.
cost effectiveness evidently if we could inspect just of the les in a project and nd most of the project s defects that would be terri c for the software industry.
unfortunately unlike a hardware cache that contains xed size cache lines fixcache contains source code entities such as les that can vary greatly in size.
thus if the les in the fixcache tend to be large and contain a disproportionately large portion of the project code then focusing on the les in fixcache may not be very helpful for code inspection.
so rst we ask what proportion of the lines of code are actually contained in the les that reside in the cache.
research question what proportion of the total lines of code are in the fixcache ?
even if the entities in fixcache include a very large proportion of the lines of code in the project they might still be well worth inspecting if those lines contain proportionately more bugs viz.
higher defect density than the lines not in the cache.
when this happens prioritizing the cached les increases inspection e ectiveness.
if however cached les turn out to have lower defect density than les that are not cached then prioritizing les in fixcache for inspection is unwarranted.
this leads us to our next research question.
research question do the cached les have higher defect density defects per lines of code than les not in the cache?
answering this question will allow us to measure the efcacy of fixcache .
while this gives us a general idea of the possible bene ts of using fixcache it does not tell us why it actually works if it does .
a good understanding of the mechanics of fixcache may enable us to customize or enhance it.
thus arises the imperative to desconstruct the various locality based update policies used by fixcache and evaluate their merits.
.
what makes fixcache tick?
the operation of fixcache is predicated upon several update policies.
how do these update policies a ect the performance of fixcache ?
there are four sources of cache updates .
initial pre fetch files added to the cache during cache initialization following kim et al.
.
.
temporal locality files added to the cache because they were changed to x bugs.
.
spatial locality files co changed with a buggy le viz.
les that co occur in many transactions where the buggy le was changed.
.
churn locality files that have recently been changed or added.of these four we consider the rst two to re ect core aspects of any cache including fixcache .
first we must initialize the cache with some les or else the initial hit rate would be low.
second temporal locality adds a le to the cache if it is not already present when a defect is found in it.
the other two sources spatial locality and churn locality provide additional policies for including les into the cache it is these two policies that we evaluate empirically.
research question how do temporal and spatial locality policies a ect fixcache performance?
with a detailed understanding of the relative in uence of di erent fixcache update policies future research may explore customizing fixcache to attain a di erent set of performance goals.
for example while kim et al.
used the hit rate measure to evaluate performance one might as we do choose other measures such as density or cost e ectiveness a measure developed by arisholm briand johannessen .
research question can we improve fixcache to achieve better defect density?
this question raises additional interesting questions.
what is the right way to optimize cache performance?
surely it will depend on a project s requirements and resource availability.
kim et al.
measured fixcache performance in terms of predictive accuracy.
is this su cient?
.
evaluating prediction performance kim et al.
use the measure hit rate to evaluate fixcache .
hit rate is the cumulative percentage of successful cache probes at the end of a run through a project s history.
at any point in time the cache may contain both defective and non defective les.
only les that actually contain defects score hits so the number of defective les in the cache bears some relation to the hit rate.
fixcache considers les in the cache to be defective and those not in the cache to be non defective.
consequently the cache contains both true positives tp les that actually contain defects and false positives fp .
the les not in the cache comprise the true negatives tn non defective les and false negatives fn defective les that are incorrectly classi ed.
recall measures the ratio of retrieved to relevant les recall tp tp fn tp defective no matter how many false positives are in the cache recall remains una ected.
in fact we can achieve perfect recall simply by enlarging the cache to hold all les.
recall however only tells half the story of a successful prediction algorithm.
we are also interested in the ratio of relevant les to those retrieved.
if only of the les are defective then retrieving of the les which would naively yield perfect recall is an ine cient way to identify the defective les and demonstrates the limitations of using recall in isolation.
thus recall is usually paired with another measure that penalizes an algorithm for falsely predicting too many entities as defective.
precision is the ratio of correct predictions to the total predicted as defective precision tp tp fp tp incache when applied in this setting however both precision and recall treat all les equally.
e ectively they are indi erent to the number of lines of code loc in the les being agged as defective.
when using a prediction tool as a guide for inspection loc is critical smaller buggy les may well have higher defect density and consequently may yield a better return on inspection investment.
when comparing di erent cache update policies or di erent prediction models that select di erent numbers of les and varying amounts of code for inspection we adopt a cost e ectiveness measure speci cally the area under the cost e ectiveness curve or aucec .readers unfamiliar with this rather subtle measure may wish to read section .
below before returning here.
we apply these concepts to the evaluation of fixcache in order to build a more complete picture of its performance in an inspection setting.
the extent to which fixcache exceeds the performance of standard models could suggest approaches for improving standard prediction models intended for use in inspection perhaps an update policy such as spatial locality can be fruitfully incorporated into a traditional model.
for this reason and to asses its e cacy we use aucec to compare fixcache against a vanilla prediction model.
thefixcache algorithm imposes only a gross ordering of the les to be inspected viz.
those les within the cache are to be inspected prior to those without.
the calculation of cost e ectiveness requires that we impose an order on the les within the cache so that we do not arti cially penalize thefixcache algorithm in our evaluation.
to evaluate fixcache using aucec we impose the same order on the les within the cache and those not in the cache rst by decreasing number of closed bugs then by increasing size.
this means that the smaller les with many closed bugs bubble up to the top.
this approach evaluates the cost e ectiveness of fixcache fairly with our naive algorithms we found that this ordering provided the best performance for fixcache in every setting when compared with other simple orderings e.g.density le size or previous defects.
research question does fixcache outperform standard simple prediction models when used for inspection?
.
cost effectiveness curve here we brie y de ne cost e ectiveness then the area under the ce curve aucec measures which we use to investigate rq5.
if you are familiar with these measures please feel free to skip this section.
suppose defects are uniformly distributed throughout the source code.
then if an inspection budget allows inspection of10 of the source code we might choose of the lines at random and reasonably expect to nd about of the defects.
this scheme requires no work or expertise in data gathering and defect modeling.
it is therefore reasonable to expect that any useful defect prediction method should be able to improve on this result.
this is the basis for the cost e ectiveness ce metric de ned by arisholm et al.
to investigate defects in telecommunications software.
suppose that source code is selected for inspection using a prediction algorithm that orders code entities e.g.
les in terms of predicted defectiveness.
if the algorithm is good and the system has only a few defects or well clustered defects 100755025100755025percent of locpercent of bugs foundopr 755025100755025percent of locpercent of bugs found10020 p1p2rfigure cost e ectiveness curve.
on the left o is the optimal r is random and p is a possible practical predictor model.
on the right we have two di erent models p1 and p2 with the same overall performance but p2 is better when inspecting of the lines or less.
.
this procedure would allow us to inspect just a few lines of code to nd most of the defects.
if the algorithm performs poorly and or the defects are uniformly distributed in the code we would expect to inspect most of the lines before we nd all the defects.
the ce curve see figure left side is a harsh but meticulous judge of prediction algorithms it plots the proportion of identi ed faults a number between and found against the proportion of the lines of code also between and inspected.
it is a way to evaluate a proposed inspection ordering.
with a random ordering the curve labeled ron the left side of figure and or defects uniformly scattered throughout the code the ce curve is the diagonal y xline.
at le granularity the optimal ordering labeled oin the figure left places the smallest most defective les rst and the curve climbs rapidly quickly rising well above the y xline.
a practical algorithm has a ce curve labeled pin the figure left that falls below the optimal ordering but remains usefully above the y xline.
the ce curve represents a variety of operating choices one can choose an operating point along this curve inspect more or fewer lines of code and nd more or fewer defects.
thus to jointly capture the entire set of choices a orded by a particular prediction algorithm one typically uses the area under the ce curve oraucec which is also a number between 0and1.
an imaginary utterly fantastical prediction algorithm will have an area very close to viz.
by ordering thelines so that one can discover all the defects by inspecting a single line a superb algorithm will have a aucec value close to the optimal.
values of aucec below indicate a poor algorithm.
thus useful values of aucec lie between and the optimum.
consider two di erent prediction models with nearly identicalaucec .
on the right side of figure the two curves labeled p1 and p2 have very similar aucec values.
however if one were inspecting of the lines or less p2 o ers a better set of operating points.
this line budget is indeed quite realistic inspecting of the loc in the system is de nitely more realistic than inspecting all of it.
thus the aucec cut o at is a useful measure of the cost e ectiveness of a prediction model this is what we use.
arisholm et al.
refer to a similar notion of aucec conditioned on choosing of the system.
as it turns out the preferred fixcache cache size setting of of the overall number of les in the system typically results in acache that contains close to of the loc in the system .
so we adopt aucec at20 of the loc which we refer to asaucec .
.
experimental framework in this section we de ne some terminology and present our experimental setup.
.
revision source code management scm systems provide a rich version history of software projects.
this history includes all commits to every le.
these commits have various attributes such as a timestamp authorship change content and commit log message.
in our study a commit or a revision consists of an author a timestamp and a set of les changed.
we chose gitas our version control system because of its excellent ability to track changes and nd the origin of each line of code.
.
bug fixing revision bugs are discovered and usually recorded into an issuetracking system such as bugzilla and subsequently rejected or xed by the developers.
each bug report records its opening date the date of the bug x a free form textual bug description and the nal triaged bugzilla severity.
for this research we consider any severity other than an enhancement to be a bug.
our study begins with links between bugzilla bugs and the speci c revision that xes the bug we call this a bug xing revision .
we employed various heuristics to derive our data.
we scan for keywords such as bug xed etc.
in the scm commit log to ag bug xing revisions .
also numerical bug identi ers mentioned in the commit log are linked back to the issue tracking system s identi ers .
then these identi ers are crosschecked against the issue tracking system to see whether such an issue identi er exists and whether its status changed after the bug xing commit.
finally we manually inspect the links to remove as many spurious links as possible.
each remaining linked bug has a bug xing revision.
we gratefully acknowledge bug data we obtained from bachmann et al.
.
.
bug introducing change we call the lines of code that are associated with the changes that trigger a bug x x inducing code following sliwerski et al.
also see as it is the code that needed repair.
for example if strcpy str2 str1 were changed to strncpy str2 str1 n then the original line is considered x inducing code.
new lines may also be added in the bug xing revision but we do not consider these implicated since they are part of the treatment not the symptom.
kim et al.
uses the term bug introducing change to describe such x inducing code.
we use the popular sliwerski zimmerman zeller szz approach to identify bug introducing changes.
we start with data that links a bug to the revision where that bug was xed.
if a bug x is linked to revision n then n the immediately preceding revision contains the relevant buggy code.
the diff of revision nandn 1of each le changed in revision n 1gives us the potentially buggy code.
we call these lines the x inducing code.
we then use thegit blame command on the x inducing code git blame produces accurate provenance annotations author date revision number where they were last changed for each line.
with this information we use the szz approach to rule out lines that could not have been involved in the defect because of their time of introduction.
for the rest this approach identi es the commit that introduced the x inducing line.
.
defect density we calculate two types of defect density defect density based on open bugs and defect density based on closed bugs.
at time t we identify number of closed bugs of a le up to time tand divide that by the size of the le to calculate closed bug density .
at time t we identify open bugs as those reported before tand closed after t.open bug density is the count of open bugs divided by le size.
note we use only the bugs that are linked to a le since we cannot in general identify the culpable le of an unlinked bug.
by using this density measure we are assuming that les with higher open bug density are more pro table to inspect.
.5fixcache we faithfully re implemented fixcache as described in .
following earlier evaluation studies offixcache we implemented le level fixcache i.e.our implementation aims to predict defect prone les.
before running fixcache for e ciency we rst identify all the bug xing revisions and chronologically order them.
for each of these bug xing revisions we also use the szz technique to identify the revision of the bug introducing change.
now we are ready to run fixcache .
we prime the cache with the largest les of a project during the initial pre fetch then scan its history.
at every bug x revision we probe the cache to see if the le being xed is in the cache if so we score a hit otherwise we score a miss.
if a probe results in a cache miss we fetch that buggy le into the cache and additionally determine all the spatially related les les that were modi ed frequently with the buggy le in question .
as described by kim et al.
we go back in time to when a bug was introduced using szz and determine spatially local les les that were co committed with the buggy le being probed at that time.
we limit the fetch size of spatially local les to the block size parameter prioritized by the frequency of co commit more often cocommitted les get higher priority .
regardless of cache hit or miss we also fetch the recently changed added les les that were changed added between two bug x revisions subject to the pre fetch size parameter .
as new les are added into the cache we may need to evict old les.
following kim et al.
we used two eviction policies least recently used lru to retain les recently involved in a bug x and closed bugs to retain les with the highest number of closed bugs.
we did not implement the change eviction policy as kim et al.
reported that it performs poorly.
the original fixcache presented results for cache size.
prior evaluations offixcache also rely on cache size.
to ensure the sensitivity of our analysis we experimented with several di erent cache sizes.
we worked with 18and25 cache size.
for these cache sizes we set the block size to 9and13 respectively and pre fetch size to and respectively.
.
snapshot as we described earlier after each bug xing revision we update the cached les.
after updating the cache we determine the state of the system in particular the cached and uncached les.
we also capture other system details such as the cache update source the last hit time etc.for each cached le we determine its number of closed bugs its size etc.we call the overall system state after each bug xing revision a snapshot of the system.
.
the data sets we chose di erent medium to large sized open source projects for our study.
all have long development histories but are otherwise quite di erent.
apache httpd is a widely used open source web server.
gimp is a popular open source image manipulation program.
nautilus is the default le manager for the gnome desktop.
evolution is the default email client for the gnome desktop with support for integrated mail address book and calender functionality.
lucene is the most popular text search engine library.
except lucene all of our projects are written in c. lucene is written in java.
we converted the apache subversion repository to gitand used the other projects gitrepositories directly.
these systems span varied application domains clients and servers gui applications a le browsers and libraries for text searching.
all are of substantial size and complexity.
table summarizes some descriptive statistics for each project studied.
project size ranges from 191k lines to about 800k lines.
the table also presents the study period the number of buggy les which represents the number of cache probes made the average number of source les .c .cpp .h or .java les in bug xing revisions the average lines of code as measured by wc l unix command in bug xing revisions the number of commits made during the study period the number of bug xing commit made during this period and the number of bug introducing commits commits that introduced changed some code that was changed deleted later to x some bug .
we determine all linked bugs and their associated bugxing revisions for all the subject projects.
the corresponding bug xing change is found by using diff between a bugxing revision and its immediate preceding revision.
after collecting all bug xing changes we use git blame on each line of the bug xing change to identify its lastmodi cation time.
this gives us all of the bug introducing changes.
.
ev aluation we now present our results per each research question presented in section .
we only present plots from apache and lucene.
data from other projects is discussed in the text.
rq1 what is the line coverage of fixcache ?
does fixcache achieve good hit rates by simply caching a disproportionate fraction of the total loc in the system?
to check this we plot the cached les total loc count in figure for cache size and lru based replacement policy.
the time series line plot shows that the fixcache caches a higher proportion of total line count when compared to the proportion of total le count within the cache.
we found similar results for all projects.
moreover we ran a sensitivity analysis by choosing the cache size to be 18start end number of avg.
avg.
number number number of date date buggy number number of of fix fix inducing name files of files of loc commits commits commits apache gimp nautilus evolution lucene table summary of study subjects apache fixtime oftotalfilesize lucene fixtime oftotalfilesize figure loc in cached les for apache and lucene.
if fixcache does not favor any particular size of le then the dashed at line represents the expected fraction of total project line count in the cache for a cache size.
time varying line is the fraction of total project line count in the cache and25 of the overall les and using both lru and closed bugs replacement policy.
in all cases the fixcache retained a signi cantly greater double to triple proportion of lines in comparison to the proportion of the total number of les held in the cache.
this fact could undermine the e cacy of fixcache when used to prioritize les for inspection.
these results suggest that the performance of fixcache when used as an aid for targeting inspections is not necessarily as good as suggested by the original hit rate measure.
for inspection use we need to consider le size to build a clearer picture of fixcache performance.
we do this by comparing the defect density of cached les with that of the un cached les.
cached0.
.
.
.006apachedefectdensity uncached cached uncached0.
.
.
.
.020lucenedefectdensity figure defect density boxplot in cached and uncached les cache size lru replacement for apache and lucene.
rq2a do the cached les have higher defect density than les not in cache?
figure shows the boxplot of defect density in cached .
.
.
.
.
apache fixtimedefectdensity cachedfile uncachedfile .
.
.
.
lucene fixtimedefectdensity cachedfile uncachedfilefigure defect density in cached and un cached les cache size lru replacement for apache and lucene.
and un cached les for cache size and lru replacement policy.
clearly les in the fixcache have higher defect density.
we also ran an one sided paired wilcoxon signed rank test with a null hypothesis defect density in cached les is no greater than defect density in un cached les.
we found that the fixcache does indeed contain les with greater density all the p values were very small .
we also did a sensitivity analysis for di erent cache sizes.
we found that the above result holds for all projects for all cache sizes.
moreover we found that the e ect size varies little over all cache sizes for a given project.
figure shows the time series line plot of density in cached and un cached les again for cache size and lru replacement policy .
this gure highlights the rightcensoring e ect in our data.
we use bug x information to determine opened closed bugs for a given sets of les.
we use our a posteriori knowledge of bug xes to calculate the density of open bugs in the code at any given point in time.
as we near the end of our data collection phase we have fewer xes this gradually reduces the number of known open bugs to zero.
the cache consequently su ers from a diminishing defect density at the end of the run which is really an artifact of right censoring not an indication that the performance of fixcache is getting worse.
there is a noteworthy sudden spike in fixcache defectdensity and hit rate in july for apache.
we did a case study to understand this event.
it turned out that on july 15distinct les were linked to 7distinct bugs resulting in a total of cache probes.
this rapid succession of defects in the same les induced a a dramatic boost in the hit rate.
moreover since each of these les had a large number of opened bugs we also observed a higher cache defect density at that point.
we observed similar though less pronounced sudden hit rate spikes for other projects as well.
this suggests that fixcache may yield a higherreturn in response to certain events while performing less well at other times.
though les resident in fixcache have higher density one might argue that inspecting the entire set of les in the cache during each bug x commit would induce redundant inspection e ort since some les would be inspected repeatedly.
rather we could prefer to inspect les that are added or changed we refer to such les as churned les since the last inspection the immediate prior bug x .
thus we can ask how well does fixcache work if we only inspect the churned les?
this immediately raises the question of fixcache e cacy when considering only churned les.
we therefore compare defect density of churned cached les against the defect density of churned un cached les.
cached uncached0.
.
.
.
.
.010apachedefectdensity cached uncached0.
.
.
.
.04lucenedefectdensity figure defect density boxplot in cached and uncached churned newly added or changed les cache size lru replacement for apache and lucene.
rq2b do the churned les in the cache have higher defect density than churned les not in the cache?
figure shows the boxplot of the calculated defect density of cached and un cached churned les at cache size for the lru replacement policy.
not only the e ect size dramatically diminished it actually ipped for some projects under some parameter settings.
the evolution project shows a lower defect density of cached les even at the typical cache size.
moreover nautilus also shows lower defect density of cached les at all but the and cache size.
e ect size is very small across the board even so some projects for some parameter settings show a statistically signi cant greater defect density for churned les.
next we evaluate the e ect of each cache replacement policy on the defect density achieved in fixcache .
preload churned spatial temporal0.
.
.
.006apachedefectdensity preload churned spatial temporal0.
.
.
.
.020lucenedefectdensity figure defect density by loading reason in cached and un cached les for apache and lucene.
rq3 do di erent sources of cache updates have di erent levels of in uence on cache performance?figure shows the defect density as contributed by di erent sources of cache updates.
as is apparent from the gure temporal locality is the highest contributor of defect density.
we disregard the initial preloading of large les as we observe that across all projects they are evicted from the cache early in the run.
wilcoxon signed rank tests were used to check whether temporal locality consistently yields greater defect density across all cache sizes.
we did nd that temporal locality for various cache sizes and across all projects exhibits higher defect density than spatial locality with only a few exceptions apache at and and nautilus at cache sizes.
when compared to churn locality however temporal locality may not yield better performance.
this immediately raises a question are temporally local entities are competing for cache space with spatially local or churn local entities?
in terms of defect density can we do better if we selectively enable disable the other sources of cache updates?
this is a challenging question to answer as enabling and disabling di erent policies will change both the number of les and the number of lines in the cache.
consequently we have to take into account the total number of lines to be inspected as well as the open bug density.
we use the aforementioned cost e ectiveness measure aucec to consider this question.
to use aucec we require a prediction model to provide an ordering in which the les should be inspected.
as specied the fixcache algorithm provides only a gross ordering of les i.e.
les in the cache should be inspected prior to les not in the cache.
to compare fixcache to other algorithms we tried various orderings to nd the one most favorable to fixcache but that only used data available to practitioners at the time of prediction.
speci cally we order the les within the cache and those not in the cache independently using the same ordering order les rst by decreasing number of closed bugs then by increasing size.
we found that this ordering provided the best performance forfixcache in every setting.
given this ordering we can then pick o les from the top of the ordering until we reach just over of the overall line count in the system.
dsdc dsec esdc esec0.
.
.
.
.
.25apache cost effectiveness dsdc dsec esdc esec0.
.
.
.
.
.25lucenecost effectiveness figure aucec 20cost e ectiveness after enabling disabling spatial changed le locality cache size lru replacement of lines inspected for apache and lucene.
ec dc refers to enable disable churn locality es ds refers to enable disable spatial locality.
dsdc means disable both all other combinations are labeled similarlyrq4a do spatial and churn locality update policies improve the cost e ectiveness of fixcache ?
figure shows the aucec 20measure after enabling and disabling spatial and churn locality.
in figure we use ds es to denote disable enable spatial locality and dc ec for disable enable churn locality .
dsdc denotes both disabled while dsec denotes disable spatial enable churn .
this gure makes it abundantly clear that from the perspective of aucec 20cost e ectiveness adding spatial and churn locality to the fixcache update policies provides little bene t. we want to show that there is little to no practical di erence in cost e ectiveness across the di erent locality settings.
the kruskal wallis test is a non parametric test which assess whether nindependent samples are drawn from the same population.
it is the non parametric counterpart to a oneway anova .
this test shows that for every project there is no signi cant di erence in cost e ectiveness observed when enabling or disabling churn and or spatial locality.
thus although figure suggests that churn and spatial locality potentially have some defect density to contribute and may in fact cache additional les they do not provide any bene t for inspection when inspecting or fewer lines as peraucec .
this clearly indicates that temporal locality which caches les when defects are found underlies most of the predictive power in fixcache .
finally we focus on di erent cache replacement policies infixcache and compare their performance.
lru density clo sed bugs naive0.
.
.
.
.
.
.30apachecost effectiveness lru density clo sed bugs naive0.
.
.
.
.
.
.30lucenecost effectiveness figure aucec 20values for apache and lucene.
the left are fixcache with di erent replacement policies and right most is a naive prediction model which simply order les by closed bug count.
only in apache of all are any of the fixcache version statistically better than the naive model and even then the e ect size is quite small rq4b can we improve fixcache aucec 20performance by using a di erent cache replacement policy?
the original fixcache used two di erent policies.
the rst is an lru based replacement policy which keeps frequently hit les in the cache.
the principle here is that les that haven t had bug xes in a while are probably unlikely to have latent bugs.
the second was a closed bugs policy which replaced les with the least number of closed bugs rst.
for our evaluation we added a third policy based on defect density which replaced the le with the least density of closed bugs.
the latter two policies are based on the theory that les with fewer bugs or lower defect density are likely to have fewer latent bugs.
all of these policies prima facie appear to have merit so it is reasonable to compare them to see which policies yield better aucec 20performance.the results are shown as the left most boxplots in the two plots in figure for this question we ran fixcache with cache size set to and25 of the total le count and consistently obtained results with all projects similar to those seen in figure .
the closed bugs policy generally showed the best performance while the density policy was consistently the worst performer.
this nding again is strongly indicative that the raw count of closed bugs in a le is an excellent indicator of future latent bugs.
our nal question is in some sense the most critical evaluation of fixcache .
does fixcache perform better than a naive prediction model?
rq5 when used for inspections is fixcache better than a naive model that simply picks previously buggy les?
for this question we chose a very simple naive prediction model the model simply orders all the les in the system by the number of closed bugs and chooses enough les in order to capture of the total line count.
the results are shown as the rightmost boxplot in figure .
testing the hypothesis that the best fixcache replacement policy vizclosed bugs is better than the naive model we nd that after benjamini hochberg correction only the apache project data rejects the null hypothesis.
however as can be seen in figure even in this case the e ect size is fairly small.
this leads to a rather surprising conclusion the naive model is actually about the same utility for inspections under the aucec 20measure as the best possible setting for fixcache .
.
threats to v alidity perry et al.
identify three forms of validity that must be addressed in research studies.
we now examine threats to each form of validity in our study and the methods used to mitigate these threats where possible.
construct validity refers to the degree to which the measurements are actually related to the concepts they are believed to represent.
the main threat to construct validity come from bias in bug reporting.
the bugs that are being reported might not be representative of actual bugs encountered in the system.
linking bias is certainly extant in the datasets and can a ect the performance of fixcache and a ect our evaluations of fixcache and the merits of different update policies rq1 .
however it is less likely that the result of comparing fixcache and the simple standard prediction model rq5 is a ected by bias.
internal validity is the ability of a study to establish a causal link between independent and dependent variables regardless of what the variables are believed to represent.
our study is focused on examining a link between variables such as closed bugs per le the fixcache prediction and the actually open bugs in a le.
threats to internal validity come from our choice of evaluation.
we attempted to address this threat by using di erent speci cally tailored measures for di erent rqs lines for rq1 density for rqs 2a 2b and and the very demanding aucec for the rest.
we chose aucec at of the lines because it closely approximates the linecount of the les in fixcache running with a cache size of .external validity refers to how these results generalize.
our study includes multiple projects belonging to di erent software domains and we nd similar results among all projects studied.
however the sample size is only ve projects.
while this gives evidence of the ability of our results to generalize further study on more projects will increase our con dence in these ndings as answers to the research questions on a broad level.
in particular we are concerned about projects that do not follow development processes similar to those of the projects that we studied.
.
related work kim et al.
took their inspiration from hassan et al.
who proposed the cache metaphor drawing on operating systems research .
bugcache principally di ers from hassan et al.
s top ten list in granularity the top ten list contains modules while bugcache contains les.
bugcache performed better even at the ner granularity.
our work mirrors this di erence we empirically evaluate the performance of fixcache at line granularity for prioritizing entities for inspection.
in the course of this investigation we compare fixcache to simple prediction models with an eye toward improving fixcache.
thus there are two strands of work related to ours fixcache evaluations and prediction models.
fixcache evaluations wikrstrand et al.
used fixcache to dynamically select regression tests that run over modi ed fault prone in cache les.
they found that in an embedded industrial system the fixcache reached a weekly hit rate of .
the study found that fixcache pre population did not a ect its hit rate and observed that daily updates quickly negate any pre population e ects.
engstr omet al.
follow up wikrstrand et al.
s and evaluate the e ectiveness of wikrstrand et al.
s algorithm at selecting regression tests.
they found the x cache approach to be more e cient than its predecessor at selecting regression tests in four case studies .
their work principally di ers from ours in their focus on regression testing.
sadowski et al.
investigated how fixcache s prediction performance varies over time and how much the set of buggy les changes .
they report that the hit rate is relatively stable and that apache httpd the initial exception stabilizes as the number of les approached the current project size.
they nd that most les that remain in the cache are correlated with a high hit rate but that there is room for improvement.
this evaluation di ers from ours in the questions posed and our focus on cost e ectiveness which is line granular.
prediction models a large body of research into prediction models exits.
catal and diri have published a current survey.
the predictor models in software engineering promise series of events at icse are a well known venue in this sub discipline.
the core idea is to build a statistical model usually some type of linear logistic or count model with either defect proneness binary response or number of defects count or continuous response as the dependent variable.
predictors are measures such as size number of authors churn social network measures complexity and a range of other measures.
a full survey of this proli c sub discipline would consume many pages for our purposes here it is su cient to note that we chose a simple model as a straw man for comparison one that simply ordered les by number of closed defects found in that le.
.
conclusion in this paper we evaluated the usefulness of fixcache as a tool for focusing code inspection e ort.
we found that fixcache tends to pull larger les into the cache however the bug density of les in fixcache is generally higher.
we found in addition that of all the update policies in fixcache temporal locality recently closed bugs contributed the most to increase bug density in the cache indeed when measured in terms of the demanding cost e ectiveness measureaucec the other two policies spatial and churn locality did not contribute much.
furthermore we compared three di erent cache replacement policies viz.
based on lru closed bug density and closed bugs and found that closed bugs based replacement policy generally performs better in terms of cost e ectiveness.
perhaps most surprisingly we found that a very simple scheme using the number of closed defects in a le to order the les provides a aucec 20performance level that is in a statistical dead heat with the best performing con guration of fixcache .
while a priori this might seem like a discouraging nding it is in fact quite informative this suggests that the most important aspect of fixcache if one were to use it for inspection is in fact its temporal locality.
thus the major conclusion of our paper is this when used for inspection fixcache gets most of its power by predicting that les that recently experienced a bug x do in fact contain additional latent bugs.
finally we emphasize that our evaluation targeted the utility of fixcache for inspections .
while the aucec measures the e ectiveness of fixcache or other ordering algorithms when choosing of the lines to inspect it does not really say much about other possible uses of fixcache for inspections static analysis formal veri cation etc.we believe these are fruitful directions for further research.