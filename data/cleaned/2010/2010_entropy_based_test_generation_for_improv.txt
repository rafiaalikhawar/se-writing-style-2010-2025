entropy based test generation for improved fault localization jos e campos rui abreu gordon fraser marcelo d amorim faculty of engineering of university of porto portugal jose.carlos.campos fe.up.pt rui computer.org university of sheffield united kindom gordon.fraser sheffield.ac.uk federal university of pernambuco brazil damorim cin.ufpe.br abstract spectrum based bayesian reasoning can effectively rank candidate fault locations based on passing failing test cases but the diagnostic quality highly depends on the size and diversity of the underlying test suite.
as test suites in practice often do not exhibit the necessary properties we present a technique to extend existing test suites with new test cases that optimize the diagnostic quality.
we apply probability theory concepts to guide test case generation using entropy such that the amount of uncertainty in the diagnostic ranking is minimized.
our entbugprototype extends the search based test generation tool evosuite to use entropy in the fitness function of its underlying genetic algorithm and we applied it to seven real faults.
empirical results show that our approach reduces the entropy of the diagnostic ranking by on average compared to using the original test suite leading to a average reduction of diagnosis candidates needed to inspect to find the true faulty one.
index terms fault localization test case generation.
i. i ntroduction programmers make mistakes therefore testing and debugging are inevitable parts of software development.
spectrumbased fault localization is a popular automated approach to assist programmers in debugging .
it takes as input code coverage information for a given test suite and produces a list of statements ranked in order of fault suspiciousness.
unfortunately the technique is fundamentally imprecise it is possible that a tester will need to inspect several suspicious statements in the rank to find the faulty one.
in fact a recent study involving developers failed to establish the usefulness of the approach as it only showed improvement for experienced programmers on simple code.
considering this result is spectrum based fault localization a dead end avenue of research?
we conjecture that the answer is no.
research in spectrumbased fault localization has continuously advanced over the past few years.
many of its notorious initial limitations are no longer a problem it can identify multiple faults it can aggregate faults scattered across the code it can quantify confidence of the diagnosis .c subject trianglet new tests t1t2t3t4t5t6t7t8t9 class triangle ... static int type int a int b int c c1int type scalene c2if a b b c c3type equilateral c4else if a a b b c c c5type right c6else if a b b a fault c7type isosceles c8return type static double area int a int b int c c9double s a b c .
zero.noslash c10return math.sqrt s s a s b s c ... test case outcome pass fail number of fault candidates figure triangle class adapted from with tests and coverage matrix type classifies triangles based on the side lengths and area calculates the area of the triangle.
test suite tproduces an unusable ranking but adding new test cases t7 t8 t9gradually narrows down the candidate statements until only c6remains.
despite all these advances there remains one fundamental limitation which we argue is one of the main confounding factors for the usefulness of spectrum based fault localization the dependency on the quality of the existing test suite .
figure shows a variation of the well known triangle example.
there is a fault at statement c6 method type declares the predicate b abut the correct condition should be b c. assume we have received a bug report for the triangle class.
using b arinel results in a ranking with a very low confidence score and the tester would need to inspect six statements c3 c5 c6 c7 c9 c10 prior to locating the fault.
note that none of the existing tests t1 t6 fails even with a failing test the confidence score might be low and thus the fault localization is unreliable and not helpful for the developer.
to overcome this limitation this paper proposes e ntbug a novel technique to generate test cases specifically guided by diagnostic accuracy.
based on our knowledge of how the existing test cases cover the statements of the triangle class entbuggenerates a new test case t7 which is the test case that improves the confidence in the ranking the most over the existing test suite test case t7fails.
adding t7to the original c circlecopyrt2013 ieee ase palo alto usa accepted for publication by ieee.
c circlecopyrt2013 ieee.
personal use of this material is permitted.
permission from ieee must be obtained for all other uses in any current or future media including reprinting republishing this material for advertising or promotional purposes creating new collective works for resale or redistribution to servers or lists or reuse of any copyrighted component of this wor k in other works.257suite reduces the average number of statements for inspection to four.
generating test case t8reduces the average number of statements for inspection to two and finally adding test case t9precisely pinpoints the faulty statement c6in the top of the rank with high confidence.
to produce these new test cases automatically our technique borrows ideas from probability theory.
given a diagnostic report ranked by probability of diagnosis candidates being the true fault explanation the uncertainty in the ranking can be quantified using entropy a measure of uncertainty in a random variable .
to increase the quality of the ranking we need test cases that maximize the diagnostic precision of the underlying fault localization algorithm this way decreasing the entropy in the ranking .
in order to achieve this e ntbuguses a search based technique to automatically generate tests that reduce uncertainty in rank reports.
this paper makes the following contributions an entropy based strategy to optimize the quality of ranking reports a fitness function based on entropy to guide search based test generation a prototype implementation of the described approach on top of the e vosuite test generation tool an evaluation of the approach using seven real faults to the best of our knowledge this is the first attempt to generate tests based on the entropy of a test suite.
e ntbug was able to pinpoint the faulty statement in all real world bugs we have considered in our benchmark.
compared to the original test suite we observed an average reduction of on the uncertainty of the diagnostic ranking delivering an expressive reduction on the number of candidates that one needs to inspect before finding the fault by on average.
comparing with random approach e ntbugreduces the uncertainty of the diagnostic ranking in .
more times and .
times the number of candidates to explore until finding the faulty one.
ii.
b ackground entbugis based on a spectrum based reasoning approach to multiple fault localization .
in this section we summarize the background information of spectrum based reasoning and automated test generation.
a. spectrum based reasoning spectrum based reasoning is an approach to fault localization founded on probability theory.
the main principles underlying the technique are based on model based diagnosis mbd which uses logical reasoning to find faults.
acomponent is a program statement.
a fault candidate is a set of components that together explain a fault.
let the symbolcdenote the set of program components and let the symbol ddenote a set of fault candidates.
for instance d an bracketle t c1 c3 c4 an bracketri htindicates that components c1 c3 andc4are simultaneously at fault and no other.
the approach sorts the faulty candidates in dby the probability of each candidate toexplain fault.
spectrum based reasoning is comprised of two phases candidate generation and candidate ranking.
candidate generation the problem of finding fault candidates can be defined in terms of the widely known minimal hitting set mhs problem .
the precise computation of mhs is highly demanding restricting its direct usage for diagnosis.
however in practice previous research has found that precise computation of dis not necessary .
staccato is a low cost heuristic for computing a relevant set of multiple fault candidates.
to illustrate s taccato consider the triangle class from figure and the test suite t. as all test cases in tpass s taccato yields a diagnostic report containing all components in the program i.e.
d an bracketle t c1 c2 c3 c4 c5 c6 c7 c8 c9 c10 an bracketri ht.
but if we consider taugmented with the failing test t7 staccato reports the following diagnostic report d an bracketle t c1 c2 c4 c5 c6 c8 c9 c10 an bracketri ht.
note that componentsc3andc7are not considered valid candidates because they have not been covered by any failing tests.
for detailed information on s taccato interested readers may refer to .
candidate ranking the candidate generation phase may result in an extensive list of diagnosis candidates.
as not all candidates have the same probability of being the true fault explanation techniques have been devised to assign a probability to each diagnosis candidate dk.
each candidate dkis a subset of the system components that when at fault can explain the faulty behavior.
the probability a diagnosis candidate being the true fault explanation pr dk obs given a number of observations obs is computed using bayesian probability updates.
an observation obsis a tuple ai ei .
executing each test case tifrom the test suite t the probability of each candidate is updated following bayes rule pr dk obs pr dk productdisplay obsi obspr obsi dk pr obsi pr dk is the a priori probability of the candidate i.e.
the probability before any test is executed defined as pr dk p dk p m dk wherepis the a priori probability of a component being faulty.
the prior probability given no observations is an approximation to fault for every lines of code loc .
.
pr obsi dk represents the conditional probability of the observed outcome eiproduced by a test ti obsi assuming that candidate dkis the actual diagnosis pr obsi dk 0ifei dkare inconsistent 1ifeiis unique to dk otherwise.
where is defined as productdisplay j dk aij 1hj ifei productdisplay j dk aij 1hjifei 258andaijrepresents the coverage of the component jwhen the testiis executed.
as this information is typically not available the values for hj are determined by maximizing pr obs dk using maximum likelihood estimation .
to solve the maximization problem a simple gradient ascent procedure bounded within the domain hj is applied.
pr obsi represents the probability of the observed outcome independently of which diagnostic explanation is the correct one and thus needs not be computed directly.
the value of pr obsi is a normalizing factor given by pr obsi summationdisplay dk dpr obsi dk pr dk the b arinel framework is used in our approach to compute the probabilities of each diagnosis candidate dk.
further information about the framework and underlying technique can be found in .
when compared to other spectrum based approaches to fault localization b arinel yields more information rich diagnostic reports due to the fact that it also reasons in terms of multiple faults.
to illustrate this approach the following probabilities are computed for the example in figure i after executing test suite t for detailed information about the probabilities see table i d1 c1 pr obs d1 h3 .
... d10 c10 pr obs d10 h10 .
after computing the probabilities for each dk d the candidates are ranked and shown to the user in descending order of probability to be the true fault explanation see table i .
b. diagnostic report entropy while debugging developers can resort to the diagnostic ranking of diagnosis candidates yielded by b arinel to pinpoint the root cause of observed failures quickly.
this can be done by inspecting the ranked candidates in a descending order according to the diagnostic probabilities.
as the diagnostic ranking assigns probabilities to the diagnosis candidates one may compute an entropy based score quantifying the reliability of the ranking this score conveys how confident one can be that the ranking helps finding the fault.
the entropy h d intuitively serves to quantify the capability to distinguish candidates in the set d. for example the value of h d is very high for the set d .
.
.
.
because we have several elements in the set with the same value.
therefore it is difficult to distinguish which element is more relevant i.e.
which of the candidates in the example with probability .
of being faulty can explain the fault better.
the minimum value i.e.
approximate ideal forhis zero in which case all elements in the set can be distinguished from one another.
in our context the maximum value islog2 m wheremis the number of components.
so h d can be defined as h d summationdisplay dk dpr dk log2 pr dk h log2 m in the triangle example we have components c1 ... c and to represent the whole set c in theory we need2mstates 2x m x log2 m x log2 x .
so the best value of his the minimum value zero and the worst value is the maximum log2 h h d .
.
therefore the entropy for the example is h d parenleftbig .
log2 .
... .
log2 .
parenrightbig .
which corresponds to the maximum value.
this means that the ranking suffers considerably from uncertainty and we cannot distinguish which components are most probably at fault.
therefore we are not able to diagnose the faulty program.
c. automated test generation the basis for a diagnosis is a test suite and often the existing test suite is not optimized for producing high quality diagnostic reports.
hence is important to generate tests to improve diagnosis.
there are many available test generation techniques search based software testing sbst is well suited for our scenario as it has the distinct ability to optimize test cases and test suites based on custom nonfunctional properties.
for example sbst can generate test suites optimized for coverage and size at the same time and sbst is also well suited to address test generation in a diagnosis context .
sbst uses meta heuristic algorithms to generate test cases for user informed objectives.
global search algorithms such as genetic algorithms gas are popular for domains where the neighbourhood of a candidate solution is very large such as for example for unit tests represented as sequences of method calls.
the e vosuite tool uses ga to generate test suites for java classes with respect to a given coverage criterion.
the ga starts with an initial population of randomly generated candidate solutions.
a fitness function determines for each individual of the population a numerical value estimating its distance to the optimal solution.
individuals are selected from the population those with better fitness value have higher probability of being selected.
selected individuals evolve according to pre defined operators and form a new population.
this procedure continues until it either finds an optimal solution or runs out of resources e.g.
it reaches timeout .
debugging is usually seen as a complementary activity to software testing typically done afterwards.
however there are some test generation techniques that aim to improve debugging .
in addition test generation in particular search based testing can also be helpful for failure reproduction as well as debugging .
259iii.
e ntropy based testgeneration with entbug this section presents e ntbug a novel approach to improve fault diagnosis.
e ntbugreceives a test suite as input and produces additional test cases for that test suite such that the entropy in the diagnosis is reduced.
a. calculating entropy spectrum based fault localization heavily relies on the diversity of coverage information across passing and failing test cases.
the variety and number of test cases are two major factors to determine uncertainty in the ranking.
previous work has shown that variety andsizeare directly related to the density of the coverage matrix and that this metric can be used as a proxy for entropy.
the density of a coverage matrix is the average percentage of components covered by test cases.
it is defined as follows n n summationdisplay i 1 ti where ti refers to the coverage density of a test case ti ti j aij j m m wherenandmdenote the number of test cases and the number of components respectively.
low values of mean that test cases exercise small parts of the program sparse matrices whereas high values mean that test cases tend to involve most components of the program dense matrices .
intuitively neither too low nor too high values of are positive.
considering the example from figure we have t1 ... t6 .
consequently the coverage density is .
... .
.
i.e.
the test cases yield a coverage matrix density of .
.
b. optimal coverage matrix density our aim is to reduce the entropy of a test suite and the drop in entropy is known as information gain .
the information gain that a new test case provides is determined by the reduction of the size of the top ranked suspect set.
assuming there are d top ranked suspects a test tiwith coverage density ti reduces the top ranked set to d ti components if tifails and to d ti iftipasses.
under these conditions it has been previously demonstrated that the information gain can be modeled as follows ig log2 log2 for our running example the value of igis equal to .
log2 .
.
log2 .
.
.
the value ofigis optimal for .
.
hence a technique that is able to generate test cases such that the coverage density of the matrix is .
provided there is a variety of test cases will have the capability of reducing the diagnostic ranking entropy and consequently improve the diagnostic quality of spectrum based reasoning.
our approach augments the existing test suite with additional test cases with the goal of balancing the density of the coverage matrix.algorithm entbugtest generation input program test suite t search budget t stopping condition c output extended test suite t t t d .
density t getfitness function t while cdo tc evosuite t ifd .
density t t then t t tc d .
density t getfitness function t end if end while returnt c. generating tests guided by the coverage matrix density the coverage matrix density gives us a measurable goal to guide test generation.
as we can measure the effect but cannot construct suitable test cases systematically this is an ideal application for search based software testing sbst .
in sbst an optimization goal is formulated as a fitness function and then efficient meta heuristic search algorithms are guided by the fitness function to generate tests.
a fitness function takes as input a candidate solution and calculates a numerical value representing the quality of the candidate such that there is a strict ordering.
in our case the optimal solution is a test case that leads to .
consequently our fitness function for test case tfor a given test suite tis fitness t .
t t this fitness function turns the problem into a minimization problem i.e.
the optimization aims to achieve a fitness value of0 which is the case if a solution is found such that .
.
d. entropy based test suite extension algorithm illustrates e ntbug s test generation procedure.
the goal of the algorithm is to extend a potentially empty test suite with test cases for improving the diagnosis.
it takes as input the program the original test suite t the search budget tone wants to invest in generating each individual test and boolean condition cwhich evaluates to true once the process should stop e.g.
timeout fixed number of test cases etc.
.
it produces an extension of tas output.
the density entailed by t which is used to guide the test generation process is calculated using the d ensity function line and the difference to .
is stored in dto later compare if a generated test improves .
then e vosuite is called using the fitness function to generate a test case using tas the search budget line .
e vosuite returns the test case that gives the maximum diagnostic information i.e.
the one that minimizes the fitness function line .
however it may be the case that e vosuite fails to find a solution that improves and in that case tcis ignored.
if tcimproves within an accepted error margin then it is added to the test suite t a new fitness function is created and the value of dis updated lines .
these steps are repeated until stopping condition cholds.
e. revisiting the triangle example to measure the success of a diagnosis technique we use thediagnostic quality cd which is a numerical value that estimates the number of components the tester needs to inspect to find the fault .
note that cdcannot be computed prior to computing the ranking one does not know the actual position of true fault candidates in the ranking beforehand.
because multiple explanations can be assigned with the same probability the value of cdfor the real fault d is the average of the ranks that have the same probability dk pr dk pr d k m dk pr dk pr d k m cd mf a value of for cdindicates an ideal diagnostic report where allmffaulty components appear on top of the ranking i.e.
there is no wasted effort in inspecting other components.
on the other hand cd m mfindicates that the user needs to test all m mfhealthy components until reaching the mffaulty ones this is the worst case outcome.
for the triangle example the value of cdis4.
after executing test suite t. this means that on average one needs to inspect components to find the fault.
table i shows the progress of the diagnostic report when new tests are added to the test suite using e ntbug.
each column shows a different stage in the augmentation process.
numbers in bold font face under column pr dk indicate components which appear at the same or higher position as the reaul faulty component d .
the final diagnostic report obtained with the suite t t7 t8 t9 is i.e.
best .
at the bottom of the figure one can observe three metrics that as explained before strongly correlate with cd ig andh.
whilecdis dependent on d which is unknown prior to finding the faults the other metrics are independent.
they are very important for two reasons i they provide a confidence value on the quality of the diagnostic report which is critical for the user to build trust on the report and ii they provide a means to automate test generation.
in particular we observed that it is possible to derive from the definition of a generation.
iv.
e valuation we have conducted an empirical study to evaluate the extent to which e ntbugis capable of improving diagnostic quality compared to a baseline technique that to the best of our knowledge incorporates all recent advances in spectrumbased fault localization .
as a sanity check we have also compared e ntbugwith random test generation.table i impact of additional test cases on fault localization for the triangle example.
rankingt t t7 t t7 t8 t t7 t8 t9 d pr dk d pr dk d pr dk d pr dk c3 .
c5 .
c5 .
c6 .
c5 .
c6 .
c6 .
c4 .
c7 .
c9 .
c4 .
c9 .
c6 .
c10 .
c9 .
c10 .
c9 .
c4 .
c10 .
c1 .
c10 .
c1 .
c1 .
c2 .
c4 .
c2 .
c2 .
c8 .
c1 .
c8 .
c8 .
c2 .
c8 .
cd .
.
.
an bracketri ht an bracketri ht0.
an bracketle t an bracketle t .
.
.
.
ig .
.
.
.
h .
.
.
.
a. experimental setup we have implemented e ntbugusing e vosuite to drive test generation and evaluated it on a set of real faults.
a particular difficulty to address in our evaluation setup is the need to create test oracles the test generation procedure needs to decide whether a test it generates passes or not.
the automated generation of test oracles is challenging .
this has to do with the fact that the behaviour of the software has to be known so that the right oracles are added to the test cases.
as the oracle problem is orthogonal to this paper we mitigated this problem by using the version of the subject after the fix was applied.
let pto be the faulty program and p its fixed version a test case tpasses ifp t p t itfails otherwise.
e vosuite adds regression oracles to the tests that make it possible to make this comparison automatically.
to investigate how the diagnosis improves over time we configured the ga in e vosuite to run for seconds per attempt see tfrom algorithm and measured the diagnostic accuracy of the evolving test suite at five time intervals and minutes.
to account for the randomness of the test generation we repeated all experiments for times and all values reported are averaged.
as a sanity check we also evaluated whether the test cases produced by e ntbugimprove the diagnosis over the same number of random test cases.
to do this we used e vosuite to produce a fixed number of random tests.
for example if an original test suite is composed by test cases and at the end of minute e ntbuggenerated additional test cases we also generated random test cases and compared the impact on the diagnosis.
b. subjects the requirements for choosing the subjects used in our evaluation are as follows the programs should be developed in java the fault must be documented and the fix should be available to validate if e ntbugis able to identify the exact place of the fault.
we chose the vending machine example used in previous work and selected six new faults from four large opensource programs.
for each subject we analyzed recent bug 261table ii details of subject programs.
subject revision classes locs original test suite test cases usedbug id affected class vending machine vendingmachine apache commons codec base64 apache commons compress tarutils apache commons math fraction apache commons math line apache commons math covariance joda time basicdayofyeardatetimefield reports and selected those reports where the fix represents a change in only one statement single fault programs .
we use the fixed version p of a faulty version pto evaluate whether or not e ntbugpinpoints the exact location of the bug in the report i.e.
we check if e ntbugeffectively isolates the faulty candidate on the top of the ranking.
note that the same subjects have also been used in previous studies e.g.
but we use different faults to demonstrate that e ntbugworks regardless of whether the fault causes an undeclared exception or a wrong output.
table ii provides details about our experimental subjects.
for each subject we only used those test cases that executed the class containing the fault therefore table ii shows both the total number of test cases in the original test suite1and the number of test cases actually used.
it has also the bug identifier of each bug report used.
c. summary of results empirical results indicate that e ntbugimproves the ability of a test suite to diagnose a problem.
in fact e ntbugwas able to pinpoint the location of every bug we considered in our study.
figure shows the evolution of cdentbug andcdrandom over time during test generation.
the y axis at the left hand side denotes the domain of while the y axis at the righthand side denotes the domain of cd.
the x axis represents the number of test cases from the original test suite plus the test cases generated on each time interval.
recall that the ideal scenario for diagnosis is one where approximates to .5and cdapproximates to .
figure 3a shows the increase in number of tests for each of the subjects over time and figure 3b demonstrates how e ntbugis able to reduce entropy during evolution.
in the following we discuss each subject in detail show how e ntbughelps to locate each corresponding fault and compare this result with random test generation.
d. vending machine vending machine is a small proof of concept example comprised of two classes and one test case only.
its purpose is to check whether there is enough credit that allows a user to buy a product.
if needed the user may chip more money in.
1the original test suite of each subject does not contain the test cases which were submitted when a patch was committed.listing vending machine fix.
class vendingmachine.vendingmachine public void vend throwsexception ... this.currvalue cost if this.currvalue zero.noslash if this.currvalue cost zero.noslash this.enabled false ... vending machine fails when the user credit reaches a negative value and the branch was not taken the enabled variable stays true and the user has permission to buy more products even with a negative credit .
using the original test suite and despite the fact that an observed failure is documented the spectrum based reasoning approach to fault localization leads to of the code to be inspected to find the fault.
this has to do with the fact that the original test suite has a high value as it only contains one passing test.
consequently the cost cdto diagnose this program is relatively high as figure 2a indicates.
augmenting the test suite with new test cases during minute reduces the number of statements to inspect to find the fault on average as well as reduces the value of from .
to .
.
during this time interval e ntbugincreased the size of the original test suite from to .
figure 3a shows the increase in the number of test cases.
at the same time the information gain ig obtained with the new test cases increases and the entropy decreases from .
to .
.
after minutes and test cases generated e ntbug reduces the statements that need to be inspected to just on average .
after minutes e ntbugobtains a test suite that produces a value of .
.
away from the optimal value and an igvalue of .
very close to the ideal value of .
.
in turn the entropy of the ranking is .398representing a reduction of .
compared to that of the original test suite which has an entropy of .
.
see figure 3b.
entbugpinpoints as faulty the statements at lines and .
as one can see from listing the fault localization technique is accurate when singling the statement out as faulty.
the statement also appears in the top of the ranking because it is always executed with statement the underlying technique cannot distinguish between components that have the same execution pattern.
comparing the results with random test generation e ntbugwas twice as precise in terms of the number of statements that need to be explored until finding the faulty one.
.
.
.
.
.
.
.
.
.
22 cd suite size cdentbug cdrandom a vending machine.
.
.
.
.
.
.
.
.
.
60 cd suite size cdentbug cdrandom b apache commons codec .
.
.
.
.
.
.
.
.
.
40 cd suite size cdentbug cdrandom c apache commons compress .
.
.
.
.
.
.
.
.
.
60 cd suite size cdentbug cdrandom d apache commonsmath .
.
.
.
.
.
.
.
.
.
20 cd suite size cdentbug cdrandom e apache commons math .
.
.
.
.
.
.
.
.
.
60 cd suite size cdentbug cdrandom f apache commons math .
.
.
.
.
.
.
.
.
.
20 cd suite size cdentbug cdrandom g joda time.
figure evolution of andcdfor every subject.
30size t minutes vending machine apache commons codec apache commons compres apache commons math apache commons math apache commons math joda time a test suite size evolution.
h d t minutes vending machine apache commons codec apache commons compress apache commons math apache commons math apache commons math joda time b entropy value evolution.
figure evolution of the test suites size and hfor every subject.
e. apache commons codec apache commons codec2provides an api of common encoders and decoders such as base64 hex and urls.
as described in the major bug the method encodebase64string of the class base64 fails because it chunks the parameter binarydata .
this means that the second parameter of the method newstringutf8 called on method encodebase64string should be false and nottrue .
listing apache commons codec fix for bug .
org.apache.commons.codec.binary.base64 public static string encodebase64string byte binarydata returnstringutils.newstringutf8 encodebase64 binarydata true 2apache commons codec project homepage proper commons codec .
3apache commons codec bug codec returnstringutils.newstringutf8 encodebase64 binarydata false ... the original test suite requires a considerable effort to find the fault statements on average when using spectrumbased reasoning.
after generating new test cases minute we are able to reduce cdto statements.
as can be seen in figure 2b this represents a reduction of .
on the diagnostic effort.
applying e ntbugfor an extra minute results in more test cases see figure 3a and we are able to pinpoint exactly the faulty line described in listing .
this corresponds as expected to a value of .
very close to the optimal value a total information of .
and that the faulty statement appears in top of the ranking as the most likely faulty component in the program.
note that after minutes there is a slight decrease in the value of .
recall that e ntbug s test generation uses an error263margin to calculate fitness improvement see algorithm .
as such it can happen that a test case can be admitted for inclusion in the test suite even without improving fitness.
although random test cases achieve a minimum that represents a reduction of .
when comparing to the original test suite the diagnostic effort is times as high as on the result achieved by e ntbug see figure 2b .
f .
apache commons compress the apache commons compress4library defines an api for working with the most popular compressed archives such as ar cpio unix dump tar zip gzip xz pack200 and bzip2.
listing apache commons compress fix for bug .
org.apache.commons.compress.archivers.tar.tarutils for inti offset i end i ... result.append char buffer result.append char b zero.noslashxff allow for sign extension ... the reported major bug 1145explains that the project apache commons compress fails when the class tarutils receive as input a tarfile which contains files with special characters.
a simple fix to resolve the encoding problem is to guarantee that the name of the files are treated as unsigned.
the original test suite of apache commons compress project has an igclose to the perfect value .
this means a value of .
.
however the cost to diagnose this project in the beginning is statements on average as seen in figure 2c .
this is explained by the diversity of the test suite although being able to entail a near to the optimal value the tests are not diverse.
after the first minute of test generation the size of the original test suite increases from test cases to test cases see figure 3a and the value of cdis reduced to only statements on average.
compared to the initial effort to diagnose the program this represents a reduction of only two statement on average.
if we continue generating new test cases for more minute new test cases the effort to find the faulty statement is 27statements on average.
in this period as we can see in figure 2c random generation has a slightly better result.
at the interval of minutes we have an effort of only 7statement and at minutes we pinpoint with precision of cd this means a reduction of .
the faulty statements described in listing with a best value of .
.
at the same time random generation only achieved a reduction in terms of cdaround .
g. apache commons math the apache commons math6is a library that provides selfcontained mathematics and statistics functions for java.
4apache commons compress project homepage proper commons compress .
5apache commons compress bug compress .
6apache commons math project homepage proper commons math .listing apache commons math fix for bug .
org.apache.commons.math3.fraction.fraction public double percentagevalue returnmultiply zero.noslash zero.noslash .doublevalue return1 zero.noslash zero.noslash doublevalue ... the bug 8357reports a failure when the percentagevalue method of the fraction class multiplies a fraction value by and then converts the result to a double.
this causes an overflow when the numerator is greater than integer.max value zero.noslash zero.noslash and even when the value of a fraction is far below this value.
a change in the order of multiplication i.e.
first convert a fraction value to a double and then multiply that value by resolved the overflow problem.
the cost to diagnose the apache commons math project with the original test suite for bug is on average statements as we can see in figure 2d .
for the bug which starts with a coverage density of .
entbugtakes more time to achieve the perfect value.
after minutes of random generation the number of candidates to inspect was reduced from to whereas entbugreduced the diagnostic effort in the same time by .
which represents only remaining components on average.
but only after minutes of generation we achieve acdvalue of almost perfect and a value of0.365for bug listing .
in this period of time the test suite increases its size see figure 3a from number of original test cases that touch the fault class to test cases.
we conjecture that the slow performance is because e vosuite produces large numbers which are important for this scenario with only a low probability.
for this particular subject as we can see in figure 3b the value of entropy decreases in the first two minutes of generation .
.
however at minutes where we are able to reduce the cost to diagnose in .
the entropy that characterizes the ranking increases from .
to5.
.
the reason why this happened is that at the first two minutes almost every out of candidate in the ranking has the same probability .
at minutes the probabilities still are the same but higher .
.
for these two intervals the number of candidates in the ranking is equal in total.
h. apache commons math the major bug 9388explains that the method revert from the class line only maintains digits of precision for the field direction.
this becomes a bug when the line s position is evaluated far from the origin.
a possible fix is creating a new instance of line and then reverting its direction.
using the original test suite the cost incurred to diagnose the apache commons math project for bug is statements.
7apache commons math bug math .
8apache commons math bug project homepage org jira browse math .
264listing apache commons math fix for bug .
org.apache.commons.math3.geometry.euclidean.threed.line publicline revert return new line zero zero.subtract direction finalline reverted newline this reverted.direction reverted.direction.negate returnreverted ... figure 2e shows that the cost increase as more tests are added rather than decreasing even though improves.
this is because the generated test cases were all passing and therefore there is no evidence for exonerating blaming components .
after minutes random generation achieves a close to perfect cd taking advantage of the fact that random generated test cases failed too.
however after generating test cases for minutes and with sufficient pass fail test cases in the suite cdreaches the minimum value of of statement to inspect on average and .
.
this is a improvement over the cost when using the random approach.
i. apache commons math listing apache commons math fix for bug .
org.apache.commons.math3.stat.correlation.covariance private void checksufficientdata finalrealmatrix matrix throwsmathillegalargumentexception intnrows matrix.getrowdimension intncols matrix.getcolumndimension if nrows ncols if nrows ncols throw new mathillegalargumentexception localizedformats.insufficient rows and columns nrows ncols ... the specification of the class covariance states that it only takes a single column matrix i.e.
n dimensional random variable with n as argument and returns a by 1covariance matrix.
however the method checksufficientdata throws an illegalargumentexception see major bug for detailed information when the constructor of the class receives a by mmatrix.
the cost to diagnose the apache commons math project for the bug is on average statements with the original test suite as we can see in figure 2f .
to pinpoint the faulty location for bug described in listing e ntbugneeds minutes.
this represents a value of0.
a gain of .
in terms of ranking entropies and new test cases.
on the other hand as with the previous subjects random test generation performed .
times worse when comparing to the cdvalue returned by e ntbug.
for this subject e ntbugmay not achieve the perfect value of cd 0because of the structure of the class covariance .
the checksufficientdata function more properly the line responsible for the bug is a private function.
this function is only executed by the constructor of the class covariance .
9apache commons math bug math .therefore those components that appears at the top of the ranking are the statements of the covariance constructor and thechecksufficientdata function.
j. joda time joda time10is a library for advanced date andtime functionalities for the java language.
listing joda time fix.
org.joda.time.chrono.basicdayofyeardatetimefield zero.noslash zero.noslash protected int getmaximumvalueforset longinstant intvalue intmaxlessone ichronology.getdaysinyearmax returnvalue maxlessone ?
getmaximumvalue instant maxlessone return value maxlessone value ?
getmaximumvalue instant maxlessone ... the class basicdayofyeardatetimefield provides methods to perform time calculations for a day of a year.
joda time bug11was related to the method getmaximumvalueforset which returns an incorrect value.
the fix of this bug consists of validating if the value of the variable value is between the maximum and the minimum value of the range or not.
due to the nature of the joda time project e.g.
the source code is structured in large hierarchies of classes consisting of many one line methods the original test suite has a coverage density of only .
.
with the original test suite the cost to diagnose the fault described in listing is statements on average.
however already after two minutes and new test cases see figure 2g for more details the value of cddrops to only one statement.
at the same time the ranking entropy was reduced by .
.
after half an hour of generating new test cases the value of increases to .
.
for this subject and considering the minutes of experiments random generation achieved values of cdbetween to statements on average.
so considering the total of time minutes this means that entbugperformed times better than random generation.
k. threats to validity construct validity the performance of e ntbughas been evaluated using the cdmetric which measures diagnostic effort in terms of the position of the fault in the diagnostic report.
this metric assumes that developers traverse the ranking but that may not be the case in practice .
however we argue that developers are more likely to traverse the ranking if the precision is increased.
external validity despite being real large and open source software systems we have only considered five subjects with seven single faults in our empirical experiment.
therefore it is possible that the results for a different set of programs with different characteristics and even with multiplefaults may produce different results.
10joda time project homepage .
11joda time bug corrected at joda time trunk .
265internal validity eventual faults in the implementation of entbugor in the underlying test case generation e vosuite may invalidate the results.
to mitigate this threat we have not only thoroughly tested our scripts but also manually checked a large set of results.
furthermore all experiments were repeated multiple times to account for the randomness of the test generation and we verified the results between runs for consistency.
v. r elated work although there is a large body of work on automated test generation and debugging in general there have only been few attempts at using test generation in the context of debugging.
baudry et.
al proposed an approach to improve diagnostic accuracy using a bacteriological algorithm similar to a ga to select test cases from a test suite.
the criterion for test selection they proposed estimates the quality of a test for diagnosis.
their selection procedure attempts to find an optimal balance between the size of a test suite and its contribution to diagnosis.
the goal of their work is similar to ours but our contributions are complementary one could use e ntbugfor test generation and the algorithm they proposed for test selection.
it remains to be evaluated if such a combination would improve the diagnostic report s accuracy.
artzi et al.
use a specialized concrete symbolic execution to improve fault localization.
the principle of their customized algorithm is highly similar to that of the nearest neighbors queries algorithm proposed by renieris and reiss.
however instead of selecting tests that are similar to a given failing test artzi et al.
generate such tests.
one important difference between our work and theirs is in the assumptions while we make no assumption about the previous test suite they assume there is at least one faultrevealing test to seed the search.
however in practice it is possible that no fault revealing test exists in the test suite.
an important technical difference between our approaches is that their algorithm uses as input a single fault revealing test and generates passing tests that minimizes observed differences for that particular test.
however it has been shown that multiple fault revealing tests can help improve diagnostic accuracy .
r o ler et.
al introduced a search based approach to identify fault candidates.
similar to the work of artzi et al.
their b ugex tool takes a failing test case as a starting point determines a set of facts e.g.
executed statements branches program states etc.
and then systematically tries to generate variations of the failing test which differ in individual facts.
if a passing test differs in only one fact to the failing test then that fact is assumed to be relevant for diagnosis if the differing test also fails then the fact is irrelevant.
b ugex is also implemented using e vosuite but our approach differs in several aspects first we do not assume the existence of a single failing test we can optimize a test suite also in the presence of no faults or in the presence of multiple faults.
second b ugex uses a white box testing approach to minimize facts about structural aspects of a program.
incontrast our approach is only guided by entropy which means it is applicable to any testing domain.
to the best of our knowledge the use of entropy to guide test generation is novel.
however entropy has been used to prioritize test cases to improve fault localization .
vi.
c onclusions and future work this paper presents an approach to generate test cases automatically based on their ability to produce a refined diagnosis rather than finding a failure as early as possible.
the results observed in our empirical evaluation using our entbugprototype show that the proposed approach can localize the faults with superior accuracy in all seven considered scenarios.
compared to the original test suites we observed an average reduction of on the uncertainty of the diagnostic ranking while also reducing the number of candidates that one needs to inspect before finding the fault by on average.
comparison to random tests shows that this improvement does not simply come from the increased test suite size but that entropy provides guidance to the test generator.
these are encouraging results yet whether these improvements are sufficient to convince programmers to use spectrumbased fault localization needs to be empirically studied.
we therefore plan to perform more experimentation and user studies to assess the usefulness of the results for developers.
besides studying its practical value the presented approach offers several avenues for future research.
first our evaluation so far only considered single faults and we plan to evaluate the performance of our approach in presence of multiple faults.
second our approach is currently only driven by the observed entropy and does not take information about the source code into account.
potentially the efficiency could be increased further by considering structural aspects during test generation for example by explicitly considering the coverage of existing test cases in the fitness function e.g.
.
third any approach aiming to improve diagnostic accuracy by generating new tests is limited by the oracle problem future work will therefore have to consider how to apply the presented approach in practice and how to best interact with the developer.
finally our current usage scenario is that of an existing test suite that needs to be improved.
we plan to apply and evaluate the ideas presented in this paper also for generating test suites with high diagnostic accuracy in the first place.
more information on e ntbugis available at