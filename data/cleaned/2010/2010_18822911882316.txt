leveraging usage similarity for effective retrieval of examples in code repositories sushil k bajracharya joel ossher and cristina v lopes bren school of information and computer sciences university of california irvine irvine ca usa sbajrach ics.uci.edu jossher ics.uci.edu lopes ics.uci.edu abstract developers often learn to use apis application programming interfaces by looking at existing examples of api usage.
code repositories contain many instances of such usage of apis.
however conventional information retrieval techniques fail to perform well in retrieving api usage examples from code repositories.
this paper presents structural semantic indexing ssi a technique to associate words to source code entities based on similarities of api usage.
the heuristic behind this technique is that entities classes methods etc.
that show similar uses of apis are semantically related because they do similar things.
we evaluate the effectiveness of ssi in code retrieval by comparing three ssi based retrieval schemes with two conventional baseline schemes.
we evaluate the performance of the retrieval schemes by running a set of candidate queries against a repository containing source code entities from jars belonging to the eclipse framework.
the results of the evaluation show that ssi is effective in improving the retrieval of examples in code repositories.
categories and subject descriptors d. .
coding tools and techniques d. .
programming environments h. .
information search and retrieval general terms algorithms design experimentation measurement keywords api usage code search ssi software information retrieval structural semantic indexing this work has been supported in part by the national science foundation s grants ccf and ccf permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
fse november santa fe new mexico usa.
copyright acm ... .
.
.
introduction the large availability of software on the web is having a fundamental impact on software development in at least two ways.
first web search engines have enabled the retrieval of software that would otherwise be undiscoverable.
second thanks to the wide availability of all sorts of libraries developers often prefer to reuse components than to write something from scratch.
these days it is possible to find libraries that implement virtually every well known piece of functionality both because those libraries exist and because they are findable via search engines.
for example if we develop a program that needs to perform singular value decomposition svd we need only to type java singular value decomposition in google or any other search engine.
google returns as second hit a link to jama a basic linear algebra package for java.
it is then the developer s task to import that package and call the appropriate application programming interfaces apis in it the knowledge of how svd works is abstracted in the api.
this allows developers to focus on the core of what they are developing for example an indexer without having to become experts in topics like linear algebra and without having to implement that piece of expertise.
indeed a significant part of programming activity now revolves around finding and using the right apis.
some research studies have confirmed that.
hoffmann et al.
showed that when developers search the web the majority of their queries are about apis.
other studies of developers needs and practices have shown that developers seek and use code examples to learn apis .
finding the right packages is easy however it is only one half of the problem.
the other half has to do with learning how to use the apis of those packages.
for example in the case of jama the developer needs to find out what classes to instantiate and what methods to call in order to get an svd of a given matrix.
in order to do this the developer might look into the documentation if it exists in any form javadoc tutorials examples etc.
on the web site where the package is hosted.
often it doesn t exist or it is too brief or too long.
existing approaches that try to tackle this second part of the problem work by harvesting words from documents on the web that describe api usage.
while these approaches improve the process of using packages that have tutorials or other documents available elsewhere on the web they fail to leverage a potentially large and rich source of information highly relevant to what the developers are looking for the collection of projects that are also available on the web and that usethose packages.
in these projects there is a variety of concrete examples of how to use the apis of interest.
moreover in many cases the developer s needs include combinations of packages seldom documented in tutorials retrieving api usage examples from actual projects can serve this need because these combinations are bound to exist out there.
retrieving useful code fragments from source code alone can be very difficult.
compared to natural language text source code has a scarcity of words that describe the entities defined and used in the code.
this makes it more prone to the vocabulary mismatch problem which states that the likelihood of two people choosing the same keyword for a familiar concept is only between .
in the context of finding api usage information the vocabulary mismatch problem manifests itself as the gap between the situation and solution models .
api designers implement the apis using words that describe the solution models while the developers seeking to use apis might be using words that come from their situation model.
this mismatch hinders the very first step of locating the right apis.
in general purpose web search engines this mismatch is mitigated partly due to the abundance of linguistically rich documents.
sophisticated information retrieval models that work well on natural language text tend to be infeasible and not scalable for source code because source code is linguistically sparse.
however unlike natural language documents source code is rich in structure.
therefore a retrieval technique that leverages this property of source code is likely to address the problem of location better.
this paper presents a technique that we call structural semantic indexing ssi .
the key idea is the following code entities that share common usage of apis are functionally similar and can share the terms used to define each other.
simply put if both aandb use a common set of apis in a similar manner then aandbare semantically related even if their names are different.
as such a query for ashould retrieve band vice versa.
in ssi we build an index taking these relations into account.
this way we are able to mitigate the scarcity of words in source code and retrieve more and more relevant code fragments.
to illustrate this idea with an example consider a use case where a developer needs to write some java code to extract contents of a zipped archive.
the standard way to do this is to use some apis from the standard java library namely the classes zipinputstream fileinputstream zipentry and the respective methods in those classes.
using sourcererdb an aggregated database of usage relations in large open source java projects we found out that some of the methods that use these apis are named as follows extract unzip unpackzip explodejarfile decompress unpackarchive etc.
these methods with similar functionality are given different names.
however the terms that can be extracted from one method name are seemingly relevant in describing the implementation in another.
this example suggests that using api usage similarity we can harvest similar words to describe code entities.
the primary hypothesis of this paper is that a source code retrieval scheme that uses ssi can be more effective.
a retrieval scheme defines how a user s query is processed and matched with the documents in a corpus to retrieve a ranked list of relevant results.
to validate our hypothesis we introduce three new retrieval schemes based on ssi that incorporate different measures of usage similarity.
we compare these schemes with two baseline schemes that do not use similarity information.
the first baseline scheme uses a combination of three heuristics based on code structure that we introduced in our earlier work .
the second baseline adds a heuristic based on using api calls to improve retrieval in code search.
this heuristic has been used recently in various code search systems .
the three ssi based retrieval schemes we introduce in this paper add the heuristic of usage similarity on top of structure and usage.
by comparing these schemes with two basefigure code snippets as examples to teach api usage line schemes using existing heuristics to enhance code retrieval we are able to properly validate our hypothesis.
we implemented all the retrieval schemes in an example retrieval system.
the retrieval system consists of a code search tool that takes a keyword query and hands it over to a retrieval scheme.
the retrieval scheme returns a ranked list of code entities as a result.
the search tool extracts a code snippet from the returned entities.
the retrieval schemes were evaluated on the basis of their ability to retrieve code entities that produced code fragments showing the apis relevant to the given query.
our results show that the ssi based retrieval schemes improve code retrieval.
in summary the paper makes the following contributions it introduces structural semantic indexing a novel approach to tackle the problem of source code retrieval by harvesting words for a code entity based on api usage similarity.
it provides empirical evidence that ssi can enhance the effectiveness of retrieving relevant code snippets.
it introduces a technique to extract code snippets showing api usage information from search results of a code search engine.
the rest of the paper is organized as follows.
section introduces code snippets as api usage example and gives an overview of the example retrieval system we implemented.
section presents the heuristics to facilitate retrieval of api usage examples and the retrieval schemes that we devised.
section elaborates the idea of usage based similarity and discusses its implementation in our retrieval system.
section provides details on the evaluation methodology and the results.
remaining sections discuss the validity of our work related work and conclusion.
.
retrieving api usage examples the de facto method of explaining the usage of apis is to show a code snippet where the apis are used.
all books and tutorials on teaching apis follow this style.
figure shows such a snippet.
the snippet is taken from the java developer s guide to eclipse one of the standard books that teaches using eclipse apis.
it demonstrates the task of programmatically writing to the eclipse workbench s log.
using this snippet as an example the book shows that the primary api to achieve this task is to call the logmethod available in the interface org.eclipse.core.runtime.ilog .logtakes a parameter of type org.eclipse.core.runtime.istatus and an instance of ilog can be obtained by calling the getdefault method available in a plugin instance.
for the purpose of this paper an api usage example is defined as a code snippet that shows relevant apis being used in the code.
the search tool we implemented extracts code snippets that look like the one shown in figure .
the extracted snippets include additional comments showing the fully qualified names of the used api entities.
such a snippet is shown in figure that corresponds to the same task of writing to eclipse workbench s log.
158with this definition of api usage example the performance of a retrieval scheme to locate api usage examples can be measured in terms of its ability to retrieve a relevant code snippet from a code repository.
retrieval system overview we implemented an example retrieval system on top of sourcerer an infrastructure for large scale indexing and analysis of open source code .
sourcerer was designed by our research group to easily support the creation of next generation code search tools.
we will briefly describe how the retrieval system was implemented.
further details on the sourcerer infrastructure can be found in our previous publications.
for the retrieval system we first create a repository of files consisting of java binaries and source code for those binaries.
the sourcerer feature extractor is used to process the files in the repository building a relational model of their structure and reference information.
this model captures entities such as classes and methods and the relations between them such as inheritance or method calls.
once extraction is completed the results are imported into sourcererdb.
next we produce a similarity index that is used to perform the similarity computation.
for each entity this index contains the fully qualified name of that entity as well as the fully qualified names of each entity that it uses.
this similarity calculation as discussed in section uses both the index and sourcererdb to compute for each entity a set of similar code entities.
once computed this information is stored back in sourcererdb.
lastly a search index is created based on the updated sourcererdb that is used to support the retrieval system itself.
the retrieval schemes themselves are implemented on top of this final search index.
a search tool is used to invoke these retrieval schemes using standard keyword queries.
each scheme defines exactly how a standard keyword query is matched against the search index to retrieve a ranked list of results.
the ranked list of results returned by a retrieval scheme is processed by the search tool to produce a list of code snippets which are returned as the final result of the search.
one such code snippet extracted by the retrieval system is shown in figure .
.
retrieval schemes a retrieval scheme is a mechanism for processing a keyword query in order to retrieve relevant documents from a document corpus.
table lists the five retrieval schemes we tested along with the heuristics that each scheme uses.
the first column of the table contains the identifier of the scheme and the remaining columns list the heuristics.
a check mark x indicates that a retrieval scheme uses the corresponding heuristic.
schemes b1andb2are baseline schemes indicating levels of performance without any usage similarity comparisons.
schemes s1 s2 and s2are the code retrieval schemes that implement ssi using three different usage similarity measures.
.
heuristics in this section we will describe five different heuristics that can be used to improve code retrieval.
these heuristics capture certain characteristics of source code artifacts.
the first three heuristics leverage the structural and textual content available in source code while the fourth instead focuses on the relations e.g.
api calls between entities.
these four heuristics capture some of the well known techniques for enhancing code retrieval.
the first three id code as text focusing on names specificity usage usage similarity tf idf usage similarity hamming distance usage similarity tanimoto coefficient b1 x x x b2 x x x x s1 x x x x x s2 x x x x x s3 x x x x x table retrieval schemes used in the evaluation comes from our earlier work in while the fourth has been used in various other code search tools .
the fifth heuristic embodies ssi the primary contribution of this work and is based on the idea of usage similarity introduced in this paper.
code as text one of the most well known and successful methods for ranking generic text documents is term frequency inverse document frequency tf idf .
tf idf s heuristic defines the relevance of a document with respect to a search term as a function of the number of occurrences of the search term in that document tf multiplied by inverse document frequency idf .
this is usually computed as logn d f where nis the total documents in the collection and d fis the document frequency the number of documents in the collection that contains the term.
focus on names developers often give meaningful names to the entities they define in the code.
this suggests that one should focus the search on the names of packages classes and methods giving them higher weight compared to words taken from other parts of the code.
to capture this concept of meaningful names we use the fully qualified names fqns for all entities in the code.
specificity source code is structured such that there is a strict containment relation between packages classes and methods which developers use this to organize their thoughts designs.
therefore a result due to a matching package name is likely less significant than a result due to a matching class name.
similarly a class based match is likely less valuable than a method based one.
this leads to a heuristic that treats the terms extracted found in the right most part of the fqn as more significant.
for example in the fqn java.lang.stringbuffer terms string and buffer extracted from the simple name stringbuffer is considered more relevant than terms lang or java extracted from the left parts of the fqn.
usage this heuristic emphasizes the usage relations that exist in source code.
an entity s usage of apis or other external entities can provide important insight into the functionality of the entity itself.
this heuristic seeds an entity with extra terms extracted from the fqns and the javadoc documentation of all the api elements classes methods and interfaces that it uses.
usage similarity in a large code repository there are many instances of similar api usage.
this heuristic captures the idea that source code entities sharing common api usage patterns are functionally similar.
therefore the terms associated with one entity can be shared with the other.
this heuristic seeds a code entity with the terms extracted from the simple names of other code entities that have similar usage profiles.
we use three different measures to compute usage similarity similarity based on tf idf of the fqns of used entities similarity based on the hamming distance measure and the tanimoto coefficient measure.
these measures are discussed in detail in section .
uses org .
e c l i p s e .
c o r e .
r u n t i m e .
i s t a t u s instantiates org .
e c l i p s e .
c o r e .
r u n t i m e .
s t a t u s .
i n i t i n t j a v a .
l a n g .
s t r i n g i n t j a v a .
l a n g .
s t r i n g j a v a .
l a n g .
throwable catch b a c k i n g s t o r e e x c e p t i o n e i s t a t u s s t a t u s new s t a t u s i s t a t u s .error uiplugin .
g e t d e f a u l t .
g e t b u n d l e .
getsymbolicname i s t a t u s .error e .
g e t l o c a l i z e d m e s s a g e e calls org .
e c l i p s e .
c o r e .
r u n t i m e .
ilog .
l o g org .
e c l i p s e .
c o r e .
r u n t i m e .
i s t a t u s calls org .
e c l i p s e .
c o r e .
r u n t i m e .
p l u g i n .
getlog uiplugin .
g e t d e f a u l t .
getlog .
l o g s t a t u s figure annotated api usage example for the task of programmatically writing to eclipse workbench s log .
incorporating heuristics in search index to incorporate the heuristics into the retrieval schemes we created a full text search index based on lucene.
lucene is an efficient information retrieval engine .
the search index stores code entities as a collection of documents.
each document consists of a number of fields.
these fields store terms that are either extracted from various parts of a code entity or harvested from other related code entities.
to populate the fields various tokenizers were implemented that extract meaningful terms from the fqns and the textual content of the code entities in the repository.
common practices in naming such as the use of camel case and the use of special characters eg were used to split the names into these terms.
table lists fields that exist in the search index.
table shows the relationship between the index fields and the heuristics described above.
to capture the heuristic code as text the search index contains an index field that stores the terms extracted from the full text of a code entity.
this field is populated by retrieving the source code from the sourcerer repository.
to capture focusing on names the index contains a field that stores the terms extracted from the fqn of a code entity and to capture specificity it contains a field that stores the terms extracted from the simple name of a code entity.
to populate these two fields the fqn of a code entity is retrieved from sourcererdb which is then processed using the tokenizers.
to capture the heuristic usage the search index contains five different fields.
these fields store the terms extracted from the fqns and documentation of used entities.
to populate these fields for an entity e a request is sent to sourcererdb to list all of its dependencies on the jsl java standard library and non jsl entities.
the returned fqns are then tokenized and stored in the respective fields.
for class entities the following relations are considered as dependencies extends inheritance implements interface implementation and uses type reference .
the following relations are considered as dependencies for methods and constructors calls method call instantiates object creation and uses type reference .
to capture the heuristic usage similarity the search index contains four different fields.
these fields store the terms that are extracted from the simple names of other entities that have similar usage profile.
.
implementation a retrieval scheme converts a simple keyword query to a more complex boolean query that matches the terms in the query with the terms stored in various index fields.
matches in some fields are regarded as more significant than others.
each field in the index is assigned a boost value according to its significance.
the specific fields that are searched depend on the heuristics that are included in a given retrieval scheme.
to illustrate this with an example consider a keyword query such as restart workbench .
in the context of learning about eclipse apis this query might express the information need of finding a id stores the terms extracted from the .
.
.
f1 full text of the code and comments of an entity f2 fqn of an entity f3 simple name of an entity f4 fqns of the used jsl java standard library entities f5 fqns of used non non jsl entities f6 simple name of the used jsl entities f7 simple names of the used non jsl entities f8 javadoc of the used non jsl entities f9 simple names of other entities that have similar usage of jsl entities similarity based on the vector space model f10 simple names of other entities that have similar usage of non jsl entities similarity based on the vector space model f11 simple names of other entities that have similar usage similarity based on hamming distance f12 simple names of other entities that have similar usage similarity based on the tanimoto coefficient table search index fields heuristics captured index fields searched code as text f1 focusing on names f2 specificity f3 usage f4 f5 f6 f7 f8 usage similarity tf idf f9 f10 usage similarity hamming distance f11 usage similarity tanimoto coeffcient f12 table index fields used to capture the heuristics code example for restarting the eclipse workbench.
the retrieval scheme b1includes three heuristics.
to capture the code as text heuristic it searches an index field that stores the terms extracted from the full text of a code entity f1 to capture focusing on names it searches an index field that stores the terms extracted from the fqn of a code entity f2 and to capture specificity it searches a field that stores the terms extracted from the simple name of a code entity f3 .
to apply the specificity heuristic a match in the field f3 is weighted twice more than matches in f1 and f2.
to retrieve code entities from the index scheme b1transforms and executes the term query restart workbench as the following lucene query f1 r e s t a r t .
or f2 r e s t a r t .
or f3 r e s t a r t .
and f1 workbench .
or f2 workbench .
or f3 workbench .
this query searches for code entities that contain both the terms restart and workbench in any one of its fields f1 f2 and f3.
code entities that contain the query terms in their simple names f3 are ranked higher see the boost value for field f3 compared to those where the terms appear in the full text field f1 with boost value or other parts of the fqn field f2 with boost value .
the general procedure can be described as follows.
given a query made up of n terms t1 ... tn a retrieval scheme that searches in m fields f1 .. fm each with the boost values of 160bv fs1 .. bv fm respectively returns the results of running a lucene query that is generated with the following form f1 t1 bv f1 or f2 t1 bv f2 .
.
or fm t1 bv fm and f1 t2 bv f1 or f2 t2 bv f2 .
.
or fm t2 bv fm .
.
and f1 tn bv f1 or f2 tn bv f2 .
.
or fm tn bv fm table lists the boost values that are assigned to each of the index fields.
these values were chosen after some experimentation to see how changing these values affected the results.
we can fully understand how each retrieval scheme searches the search index using four tables table shows the list of heuristics for each retrieval scheme table explains what information is stored in each index field table shows what index fields are used by each heuristic and table shows the boost values assigned to each index field.
ranking the retrieval schemes rank the retrieved entities based on the scores of their similarity to the query.
the retrieval schemes use the scoring that is built into lucene.
lucene uses a combination of the boolean model bm and vector space model vsm of information retrieval for scoring.
lucene first uses the bm to narrow down the documents using the boolean logic in the query specification and then uses the vsm to compute the similarity score based on the cosine distance measure .
the cosine distance measure incorporates the tf idf values computed from the full text of the retrieved entities.
full details on this scoring can be obtained from .
.
rationale for the schemes scheme b1searches the code index using only the information available in the code entity itself.
it retrieves a code entity if all of the terms in a query match the terms extracted from its full text and the fqn.
while so it gives priority to those entities where a match is made on the simple name.
secondary priority is given to matches on other portions of the fqn and the full text of the entity.
this combines the first three heuristics code as text focusing on names and specificity .
in our past evaluation of code search engines for finding implementations these three heuristics proved to provide good results .
thus for this paper we use their combination as scheme b1and consider it to be the first baseline with which other schemes can be compared.
scheme b2searches in all fields that b1searches in.
in addition it also searches in fields that store the terms extracted from the used entities.
this scheme incorporates the heuristic usage in addition to the others included in b1.
the contents from jsl entities have minimum boost and contents from simple name of non jsl entities have the maximum boost.
this is done to rank entities using eclipse apis more highly compared to those using jsl apis.
this scheme therefore retrieves code entities based not only on the content found in a code entity itself but also found in portions of entities that it uses.
it further prioritizes those results where a term in a query matches the contents from simple name of a used entity.
usage information in particular api calls and javadoc documentation have been used to improve search engines to find software applications and retrieve useful code snippets .
leveraging usage information is a well known technique in improving software retrieval systems therefore we add the usage heuristic in b2and use it as a second baseline.
schemes s1 s2 and s3are three different variants of retrieval schemes based on ssi.
these schemes retrieve all the code entities thatb2does.
but these schemes also match the query terms to additional fields that store the terms imported from simple names of other entities with similar usage profiles as the code entity being indexed.
this scheme applies the heuristic usage similarity on top of others.
since there is more than one way to measure usageid f1 f2 f3 f4 f5 f6 f7 f8 f9 f10 f11 f12 bv table boost values bv applied to fields id field id similarity we introduce three schemes that incorporate three different usage similarity measures.
on one hand comparing s1 s2 ands3with the baselines help validate our hypothesis about the effectiveness of ssi.
on the other hand comparing these schemes with each other will tell whether there is a difference in picking a particular usage similarity measure to be used for ssi.
in terms of standard information retrieval measures of precision and recall ssi based schemes s1 s2 and s3 are designed to provide better recall values than b2 and b2is designed to provide better recall than b1.
the improvement in recall is expected because these schemes harvest more terms to describe a code entity either from used entities or similar entities.
it is expected that precision and ranking of relevant results are not compromised while increasing recall.
.
usage based similarity a notion of similarity can be defined for code entities based on their common usage profile.
three different measures of similarity based on common usage are used to incorporate the heuristic usage similarity in the code search index.
.
tf idf based similarity this measure of similarity is based on the vector space model representation of code entities.
each code entity is considered to be a document and fqns of other entities used by the code entity is considered to be the words in the document.
the similarity of an entity with other entities is computed based on a tf idf measure over the vector space as defined by the fqns.
input eid entity id f field storing used apis output sim list of similar entities using same apis as the entity with eid begin q a priority queue stores terms sorted by their weight forall term t in f of entity with entity id eid do w idf t df t w weight of term t idf inverse document frequency from lucene index df document frequency stored in term vector insert t with weight w in q new query top terms from q get top results from running the query f new query in the lucene index sim tophitsfromlucene f new query end algorithm mechanics of computing similar entities using lucene s mlt query our implementation of this similarity measure is based on lucene.
for this purpose we created a similarity index for entities containing a set of documents.
each document has three fields storing the following information i a unique id for an entity feid ii full fqns of all jsl java standard library entities used by the entity fs1 and iii full fqns of all non jsl entities used by the entity fs2 .
for a given entity with entity id entity id entities similar to it can be computed by issuing a more like this mlt query that lucene supports.
the underlying mechanism of how mlt query works for computing similar entities is shown in algorithm .
in the algorithm choosing one of the following fields fs1orfs2 as a value for the input f field returns a list of entities that have similar usage of jsl and non jsl entities respectively.
.
similarity using feature vectors the remaining two measures of similarity are based on a similarity model commonly used in collaborative recommendation .
a usage profile for a code entity is represented as a binary feature vector whose entries denote use of other entities.
the information about relations stored in sourcererdb is used to compute the feature vectors for each entity that exists in our corpus.
for each entity the feature vector is calculated by querying sourcererdb for the following relations calls method call extends inheritance implements interface implementation instantiates object instantiation and uses type reference .
following the same approach as used by bruch et al in usage information for each code entity is encoded as a binary vector.
after obtaining this binary feature vector for each code entity a neighborhood of similar users is obtained using two similarity measures for binary vectors i based on a modified version of hamming distance adapted from and ii the tanimoto coefficient measure.
the fqns of top entities from such a neighborhood are extracted and stored back in sourcererdb for each code entity.
hamming distance for two given bit vectors is defined as the number of bits in which they differ.
bruch et al modified this measure by calculating bit difference on a partial feature space and showed the modified measure to be effective in their work of api recommendation .
we use the same technique of using only the partial feature space while computing similarity.
our implementation differs from theirs in terms of the relations and the types of entities that are used in constructing the bit vector.
we use the following distance measure distance hd e1 e2 pe1 jfe1 fe2j where distance hd e1 e2 is the modified hamming distance between e1ande2 feis the bit vector for entity e andpeis the number of true bits in fe.
similarity measure based on this distance is computed as follows similarity hd distance hd tanimoto coefficient also known as the extended jaccard coefficient can be used as a similarity measure for two bit vectors.
it is given as follows similarity tc e1 e2 jfe1 fe2j jfe1j jfe2j jfe1 fe2j .
evaluation the goal of our evaluation is to validate our hypothesis that ssi enhances the effectiveness of code retrieval by leveraging usage based similarity.
we also seek to compare the performance of the retrieval schemes to observe the effect of using structure and similarity based heuristics in code retrieval.
the effectiveness of a retrieval scheme is measured based on its ability to produce relevant code snippets for a given set of candidate queries.
we first discuss the snippet extraction technique and then present the details of the evaluation.
.
snippet extraction a retrieval scheme takes a keyword query and returns a ranked list of code entities as search result.
this ranked list of entities is called hits and each entry in the list is called a hit.
the retrieval scheme also returns the total number of entities in the index that match the query.
for each hit the corresponding entity id a unique identifier for a code entity is available.
further details about the code entity can be queried from sourcererdb using the entity id .
the search tool uses the information returned by a retrieval scheme to extract a corresponding code snippet for each hit entity in the list.
snippet extraction proceeds in two steps.
first given a set of hits a list of top used entities is generated.
this process is shown in algorithm .
the maximum of or of the total number of hits is processed to generate the list of top apis.
the searchinput hits top n hits returned as search results where n max of of total hits output top used list of top used entities begin list eid all entity ids from hits gettopapis .. selects top non jsl java standard library entities of each type interface method constructor classes from sourcererdb such that they are used by at least entities in the hits top used gettopapis hits end algorithm getting the list of top used entities tool queries sourcererdb for the top non java standard library entities that are used by the entities in the list.
for each entity top interfaces methods constructors and classes are selected.
among all these used entities in the list only those entities that are used by at least different entities are returned as the top used entities.
input eid entity id top used top used entities output snip an annotated code snippet begin snip empty string getusedpositions .. looks up sourcererdb and returns all positions in the code where top used entities are used.
positions are mapped to a list of used entities used pos map getusedpositions top used eid forall position in used pos map do rationale empty string forall used entity in used pos map do below append a b returns a new string by appending string b to a .createrationale .. selects relation type and fqn of used entity and creates a rationale as a comment rationale append rationale createrationale used entity eid extractfragment .. extracts the surrounding expression in a code entity from position snip fragment extractfragment eid position appendsnip .. works same as append .. and returns true if rationale and snip fragment do not already exist in snip ifappendsnip rationale snip fragment 2snipthen snip appendsnip snip rationale snip appendsnip snip snip fragment end algorithm snippet extraction second given the list of top used entities and the entity id a code snippet is extracted from a code entity.
the algorithm for this process is shown in algorithm .
the procedure first queries sourcererdb to locate all the positions in the source of an entity where any of the top apis are used.
few surrounding lines of code are extracted from each such starting position.
on top of these extracted lines comments indicating the fqns and relation types of the top used entities are inserted.
finally a sequence of these commented code fragments is returned as an example code snippet.
a sample code snippet generated using a hit returned for a query write to workbench error log is shown in figure .
.
evaluation methodology the evaluation methodology consists of building a corpus to test the retrieval schemes creating a set of candidate queries executing the queries using all retrieval schemes to generate code snippets assessing the relevancy of each code snippet and finally selecting a set of metrics to compare the effectiveness of the retrieval schemes.
this makes our methodology similar to the widely used cranfield style evaluation of information retrieval systems .
corpus the corpus for the evaluation consists of i a file repository consisting of jars from the eclipse framework.
of these jar files of them were found to contain source code.
as eclipse separates source code and binaries class files into dif162id query q1 copy paste data from clipboard q2 open url inhtml browser q3 track mouse hover q4 write toworkbench error log q5 track job status change q6 open file inexternal editor q7 batch workspace changes insingle operation q8 remove problem marker from resource q9 highlight text range ineditor q10 update status line q11 prompt user toselect directory q12 use shared image from workbench q13 open dialog andask yes noquestion q14 parse source string ast node q15 extract return type from method declaration inast node q16 fill table background thread q17 platform debug tracing forplugin q18 get display created incurrent thread q19 run job inui thread q20 open external file table candidate queries used in the evaluation ferent jar files this indicates that there were different plugins jars of which did not have associated source code.
these jar files were taken from the plugins directory of the bit linux version of eclipse galileo v3.
.
.
ii sourcererdb storing all entities relations and similarity calculation results extracted from the source code and binaries from the jars in the file repository.
to populate sourcererdb a full extraction entities and relations among the entities was performed on plugins with source code.
for the plugins without source we extracted all the entities and the relations to them from the entities extracted from the plugins with source.
the database consisted of code entities out of which had source.
it stored relations among the entities.
iii a code search index that is used to run the retrieval schemes.
the code index is built using the information stored in the sourcererdb and the file repository.
the code index prepared for evaluation consisted of all methods classes and constructors from the plugins that had source.
this resulted in an index with methods classes and constructors.
candidate queries a set of candidate queries was prepared referring to various sources that teach using eclipse apis.
table shows the list of the queries.
queries q11 q18 are created resembling tasks that are described in the eclipse faq web site and the book official eclipse faqs .
queries and are based on tasks described in queries are based on tasks used to evaluate strathcona a recommender tool that suggests api examples and queries are created based on examples found in sites .
the queries are formulated as a list of natural language terms that describe the tasks to be solved.
as such they represent possible queries a developer who is unfamiliar with the apis but understands the task to be completed might formulate.
the italicized terms in the queries are stop words and are not included in the query during retrieval.
we show them here to clarify the intent of the queries.
ranked results we followed the pooling approach to evaluate the retrieval schemes.
in pooling approach a set of documents is collected in a pool by retrieving top k results from each retrieval scheme that is to be evaluated.
there are two primary reasons for adopting this approach i a large retrieval system contains numerous documents all of which is impossible to check for relevancy and ii in search engines usually the top hits are of importance.
we set k to be in our evaluation considering the fact that most search engines present top results in theirfirst result page.
the search tool in our retrieval system fetches the top results for a candidate query using all retrieval schemes.
it then extracts code snippets from the ranked results returned by the retrieval schemes and puts them in a pool.
finally a list of ranked snippets is produced for every candidate query using all five retrieval schemes.
relevancy judgement the original sources used to create the queries also contain solutions to solve the tasks that the queries represent.
these solutions are code snippets along with the list of apis to be used.
this list of suggested apis and code snippets is used as an oracle to refer to evaluate the snippets returned by the evaluation tool.
one of the authors went through all the query snippet pairs in the pool to mark them as either relevant or not relevant using the oracle based on the following two criteria i its comments show the right apis as mentioned in the original solution and ii the code in the snippet resembles the pattern of api usage as shown in the original solution.
for example figure is a code snippet returned by a retrieval scheme and was marked as relevant after consulting the oracle that contained the original solution given in .
performance metrics the lists of ranked results and judgements are sufficient to compute various measures of effectiveness for the retrieval schemes.
we look at such measures i coverage ii precision iii recall and iv normalized discounted cumulative gain ndcg .
coverage is defined as the percentage of total queries used in evaluation for which a retrieval scheme is able to generate a relevant result.
this measures how good a scheme is across a range of queries.
ndcg measures the ranking performance of a retrieval scheme by measuring the cumulative usefulness of search results based on their position in the list of results.
relevant documents appearing higher in the list get higher score whereas relevant documents appearing lower are penalized.
exact formula to compute ndcg and further details on the metric is available in .
precision and recall in evaluation by pooling approach is computed as follows.
p recision s jretrieved s relevant poolj jretrieved sj jrelevant sj jretrieved sj recall s jretrieved s relevant poolj jrelevant poolj jrelevant sj jrelevant poolj where retrieved sis the set of results retrieved by a scheme s that exists in the pooled results relevant pool is the set of relevant results in the pool and relevant sis the set of all relevant results retrieved by a scheme s in top k results.
the five retrieval schemes are compared for precision and recall for top results.
precision measures the ability of a retrieval scheme to filter noise in the hits that it retrieves.
recall can be used to see which retrieval scheme dominates in being able to fetch more relevant results regardless of noise .
.
results table lists the aggregate measures of performance for all the retrieval schemes.
the computed values for coverage show that ssi based schemes s1 s2 s3 have the highest coverage.
schemes s3 ands1are able to retrieve a relevant results for of the total candidate queries almost as twice as the baseline scheme b1which retrieves relevant results for only of the queries.
b2does slightly better than b1on coverage.
this indicates that applying the heuristic of usage can increase the coverage of a retrieval scheme coverage b2 coverage b1 and using usage similarity increases coverage even more coverage s1 s2 s3 coverage b2 .
ndcg values in table indicate that adding the heuristic of usage on top of the baseline b1 deteriorates the ranking.
however adding the usage similarity heuristic on top of usage 163figure precision and recall improves the ranking compared to the baseline scheme b1.
this is only true for the cases when either tf idf based or tanimoto coefficient is used as the similarity metric for the heuristic of usage similarity ndcg s3 s1 ndcg b1 ndcg b2 .
using the hamming distance as a measure of similarity seems to make ranking worse compared to baseline b1 .
the different values of coverage and ndcg for the three ssi based schemes s1 s2 s3 show that the choice of a similarity measure used in ssi can have different effect on its coverage and ndcg measures.
figure plots the precision and recall values obtained for each schemes.
each box plot shows the distribution of the computed measure precision recall for all queries and a particular scheme.
the thick horizontal line inside a boxplot corresponds to the median value of a measure the circles scattered vertically represents an individual measure for a query a dark diamond shape marks the mean of the measure and the dark line drawn across the diamonds show how the mean of the measure changes over schemes.
the recall plot is obtained by plotting measures of recall for all queries.
since precision is not defined when a retrieval scheme fails to retrieve any result i.e.
retrieved sis the precision plot is obtained based on the values for queries that had non zero values forretrieved sin all schemes.
the plot for precision indicates that precision increases with the inclusion of usage heuristic precision b2 precision b1 .
including usage similarity heuristic seem to slightly decrease the average precision achieved by using the usage heuristic alone average precision s3 s1 average precision b2 .
however the median scores for precision seem to be increasing with usage similarity .
to summarize improvement in recall due to ssi seems to be more significant than the corresponding deterioration in precision.
statistical tests evaluation of precision recall and ndcg for each query over the five different schemes resembles a one way repeated measure experiment where precision and recall are measured repeatedly for each scheme using a same query.
shapirowilk normality test shows that the distributions of precision recall and ndcg values are non normal for some schemes.
therefore we choose friedman test a non parametric one way repeated measure analysis of variance to test the statistical significance of these measures.
friedman test on recall measures shows some evidence that significant difference exists in the measures of recall across different retrieval schemes df p value .
.
posthoc analysis using the wilcoxon nemenyi mcdonald thompson test shows significant difference exists between recall values of schemes s3 and b1 p value .
and schemes s1 and b1 pvalue .
.
friedman test on precision measures shows no evidence that precision measures are significantly different across schemes df p value .
.
friedman test on ndcgmeasure b1 b2 s1 s2 s3 coverage ndcg .
.
.
.
.
table performance measures for various schemes values shows no significant differences in ndcg values across the schemes df p value .
.
the above results indicate that the increments in recall across the various schemes are significant for two of the ssi based schemes s1 and s3 while the changes in precision and ndcg are not.
therefore we can conclude that while there is a significant improvement in recall with ssi it does not have an adverse effect on precision and ndcg ranking .
for two choices of similarity measures ssi increases coverage ranking precision and recall compared to the baselines.
ssi performs best with tanimoto coefficient as the similarity measure.
.
implicationss the poor coverage of scheme b1 shows that additional heuristics are necessary to retrieve relevant api usage examples across a broad range of queries.
a non significant decrease in precision but a significant increase in recall could mean that the use of ssi or adding both the heuristics of usage and usage similarity might overall be beneficial across a wide range of queries.
this is especially important when a more precise retrieval scheme fails to retrieve any relevant result for most of the queries.
b1has a low coverage because it fails to retrieve any result at all for most of the queries.
since the ssi based schemes also seem to improve ranking irrelevant results introduced by adding these heuristics would mostly appear lower in the hits.
this knowledge about the performance behavior of the various retrieval schemes can also be used to devise adaptive retrieval schemes when a stricter but more precise scheme could be used at first and if retrieval fails a less restrictive but slightly imprecise scheme could be used.
.
validity certain assumptions and choices made in the evaluation could affect the validity of the work presented here.
corpus the repository used in this paper is relatively small since it consists of entities from a single big project.
for the usage and usage similarity heuristics to work well it is assumed that the underlying software repository is sufficiently large as to contain many instances of usage of apis.
we plan to rerun our evaluation on the larger sourcerer repository to investigate how size impacts the heuristics performance.
types of entities retrieved during evaluation we limited the retrieval to method and constructor entities.
when the results contained class entities we observed two things.
first very few of the top entities were classes.
second when class entities were found in the results they often produced duplicate snippets as methods within the class were also included in the results.
this created confusion regarding whether the duplicate snippets should be considered the same result as they come from the same method or two different results.
we found it easier and of less impact to the evaluation to limit the retrieval to method and constructor entities only.
although this simplified our evaluation we believe properly handing containment and hierarchic structure in code retrieval seems to be an important topic that needs more study.
queries another major factor that influences the evaluation results is the choice of queries and terms used for the queries.
we created the candidate queries based on tasks related to eclipse apis.
these tasks were taken from various standard resources on the web and from published materials.
we therefore believe they represent 164a wide range of tasks from which representative candidate queries can be created.
however we deliberately chose the queries to contain only natural terms omitting names of apis in the queries in order to capture the scenario of developers writing queries about unfamiliar apis.
our queries are relatively long with about average terms per query ignoring the stop words in italics .
experienced developers who are somewhat familiar with the apis might mix api names with natural terms or write shorter and more precise queries.
the queries for a same task can also vary among different individuals.
therefore our results could have been biased by the choice of words we used or by our decision to give them a more natural form.
one way to mitigate this problem could be to obtain real queries from developers to accommodate variations in query terms and forms and use them in evaluation.
we are considering to take this approach when we rerun our evaluation in a bigger repository.
relevancy judgement when judging the relevancy of a snippet we had to make some decisions regarding what to classify as relevant.
if we confirmed that a snippet used valid apis not found in the oracle solution we still marked it as relevant.
however if a snippet used an api marked as internal we marked it as not relevant.
changing these criteria might have an effect on the results.
other parameters there are many variables that can alter the relevancy judgements.
for example boost values for the fields the number of search results processed to get the list of mostly used entities the types of entities chosen to generate the snippets the types of relations considered while computing usage profile for similarity etc.
we fixed most of these parameters to be same in all retrieval schemes after some trial and error with few queries.
as future work we plan to investigate the influence of these values on the sensitivity of the results.
comparing similarity measures a thorough investigation of various similarity measures to be used for the usage similarity heuristic is out of scope of the work presented here.
the similarity measure based on modified hamming distance has a different effect on retrieval performance compared to the other two.
more analysis is required to understand why such a difference exists.
all of the necessary resources to repeat our experiments are available online.
the materials used in this work include the candidate queries the oracle with solutions and the evaluation results.
they are available at .
the entire implementation starting from the sourcerer infrastructure to the search tool with snippet extraction has been open sourced and is available for others to use or extend.
further details about obtaining these resources are available at .
.
related work the work presented in this paper is an extension of our existing work in evaluating ranking schemes for internet scale code search .
the first three heuristics are borrowed from that work.
as mentioned earlier the heuristics usage represents recent approaches of using api calls relations in building software retrieval systems .
the similarity measures used in the usage similarity heuristic is similar to the methods often used in recommendation systems for software .
xfinder is another tool that uses a combination of structural similarity and existing documentation to help developers find examples of framework extensions .
like most of these work we leverage structural similarity but with the goal of attacking the problem of location in code retrieval.
to the best of our knowledge our is the first work to use usage similarity for this purpose.
existing approaches to tackle the vocabulary problem in code search either use complex information retrieval models such as latent semantic indexing lsi or instead rely on information sourceson the web that contain textually richer descriptions of code snippets.
codebroker is an example of the first kind.
mica and assieme are two recent tools that follow the second approach of relying on information on the web describing code snippets and apis used those snippets.
unlike these approaches ssi does not rely on existing documentation and works solely with source code.
extracting code snippets based on popular apis in the search results bear some resemblance with the snippet generation techniques used in sniff .
while sniff s goal is to synthesize a single code snippet from fragments extracted from many entities our approach is to be able to locate useful api usage in the original files.
since usage heuristic essentially capture the approach that sniff uses to retrieve code entities our result that usage similarity improves code retrieval imply that tools such as sniff can benefit by adopting our approach.
our use of popular apis in generating the code snippets is similar to the use of popularity information about apis to generate documentation in jadeite and the work of using api hotspots to detect interesting parts of frameworks .
a similar technique to list relevant apis and code snippet exist in the code search tool mica .
commenting code snippets with used apis is motivated by the rationale view in strathcona that shows similar information about the code example it returns .
there are a number of tools that assist developers with problems other than location in working with api examples.
for example various tools aid comprehension the step where developers need to understand protocols and right patterns of using apis .
a basic requirement of these tools is that they require an initial seed in the form of names of apis to retrieve useful examples.
thus they are of less help to developers who are unfamiliar with apis or those who do not remember them.
locating apis being the very first step our solution to the location problem is largely complementary to the work behind these tools.
furthermore most of these other tools also work solely with the information available in the source code.
combining our approach in code retrieval with these other tools will give a better end toend solution for working with apis that is entirely driven by the information that can be extracted from the source code.
recent commercial work is bringing the capabilities of web search engines into code search.
koders krugle and google code search are the few prominent ones.
since the underlying repository of these code search engines can be extremely large billions of lines of code the retrieval schemes for these systems need to be scalable and automatic.
older approaches to improve retrieval in code repositories do not meet this requirement.
for example codefinder one of the earlier code retrieval system pioneered the idea of adaptive indexing.
however the repository construction process is semi manual requiring human input to create a good index and the matching scheme is computationally expensive that slows down exponentially as the repository grows .
our approach in this paper is simple and scalable.
it uses a semi structure full text index based on the vector space model all computations to build the repository can be done offline and is parallelizable.
we implemented the core retrieval engine using lucene which actually drives most of the commercial code search engines mentioned earlier.
this makes our approach highly suitable to be adapted for commercial code search engines.
there are alternative approaches that try to fulfill the developers information need about apis.
most prominent ones are those that rely on active participation and social interaction among developers .
these approaches are promising and can be successful with enough participation and interest among developers to use the 165tools and approaches they propose.
we provide an alternative solution that is more lightweight and harvests information about api usage from the primary source code that use apis where most of the information about real api usage exists.
.
conclusion in this paper we presented structural semantic indexing ssi a novel approach to tackle the problem of source code retrieval by leveraging api usage information extracted from code in large repositories.
by sharing terms between entities with similar usage profiles ssi partially ameliorates the vocabulary mismatch problem that plagues source code search.
our goal with ssi was to be able to build an effective code retrieval scheme that uses no documents other than source code.
large source code repositories such as sourceforge and github serve both as motivation and target of our approach can we use them and them alone to find the code we need?
our evaluation demonstrates that ssi is indeed a feasible and effective approach.
furthermore we presented a technique for dynamically generating api usage snippets which has the potential to zoom in on exactly the information that the developers want a variety of concrete examples.
we also discussed some open questions section that warrants further exploration.