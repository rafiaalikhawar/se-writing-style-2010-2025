localization of concurrency bugs using shared memor y access pairs wenwen wang1 zhenjiang wang1 chenggang wu1 pen chung y ew3 xipeng shen4 xiang yuan1 jianjun li1 xiaobing feng1 y ong guan5 1skl computer architecture ict cas 2university of chinese academy of sciences 3university of minnesota at twin cities 4college of william and mary 5capital normal university wangwenwen wangzhenjiang wucg yuanxiang lijianjun fxb ict.ac.cn 3yew cs.umn.edu 4xshen cs.wm.edu 5guanyong mail.cnu.edu.cn abstract we propose an effective approach to automatically localize buggy shared memory accesses that trigger concurrency bugs.
compared to existing approaches our approach has two advantages.
first as long as enough successful runs of a concurrent program are collected our approach can localize buggy shared memory accesses even with only one single failed run captured as opposed to the requirement of capturing multiple failed runs in existing approaches.
this is a significant advantage because it is more difficult to capture the elusive failed runs than the successful runs in practice.
second our approach exhibits more precise bug localization results because it also captures buggy shared memory accesses in those failed runs that terminate prematurely which are often neglected in existing approaches.
based on this proposed approach we also implement a prototype named locon.
evaluation results on common concurrency bugs show that all buggy shared memory accesses that trigger these bugs can be precisely localized by locon with only one failed run captured.
categories and subject descriptors d. .
testing and debugging keywords concurrencybug localization sharedmemoryaccesspair .
introduction it is very difficult to debug concurrent programs due to their congenital non determinism .
first concurrency bugs are triggered only under particular thread interleavings.
second even after a concurrency bug has been exposed in a failed program run it still takes a tremendous amount of time and effort to localize and fix the bug .
previous work shows that it takes nearly days on to whomcorrespondence should be addressed.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
ase september vasteras sweden.
copyright acm ... .
.
to fix a concurrency bug and a survey in microsoft shows that over half of the respondents suffer from concurrency bugs at least once a month.
concurrency bugs exhibit most frequently as order violations ov and atomicity violations av including singlevariable sav and multi variable mav accounting for of common concurrency bugs .
some of them are caused by data races in programs while others occur even in data race free programs.
ov is a violation of the desired happen before order between two accesses to the same shared variable.
av is a violation of a code region s atomicity alsoknownas serializability whichoccurswhen the code region is unintentionally interleaved with memory accesses from another thread.
a large amount of work has been proposed to detect concurrency bugs e.g.
data races and av bugs by leveraging static and or dynamic program analysis techniques.
unfortunately due to the large amount of false positives programmers still have to sift through a large volume of results to localize the bugs.
prior work findsthat90 ofthedetecteddataracesareactuallybenign.
meanwhile someotherwork triestoexposeconcurrencybugsbytriggeringthemviasystematicorrandomlyorchestrated thread interleavings.
but even after strenuous testing unexposed bugs could still be hidden in programs and their manifestation could be catastrophic .
during testing concurrent programs developers have to manually localize a concurrency bug after it incidentally shows up in a failed run.
however it is very time and effort consuming to pinpoint the exact cause or source of a concurrency bug i.e.
identifying the exact memory accesses that cause the bug due to their large volume.
to relieve the debugging burden from programmers some approaches try to analyze the logged data and automatically localize the exact causes of the bugs.
the aim of these approaches is not to trigger or detect the bugs but tolocalize the causes of the bugs after they show up in failed runs a so called fault localization problem.
they collect program predicates at runtime in multiple runs which may complete successfully or fail.
such predicates can be branch outcomes bug patterns or cache coherence events which exhibit different behavior in successful and failed runs.
statistical models such as the jaccard index the harmonic mean of the occurrence frequency or well defined scores are then used to calculate the scores for those predicates.
predicates with higher values are more likely to be the true causes.
the score of each predicate is calculated based on the relativ e numbers of the failed and successful runs in which this predicate shows up.
to allow the true causes to have distinctly higher scores a sufficient number of successful and failed runs are required.
however it is not easy to capture a large number of failed runs e.g.
it took about hours tens of thousands of successful runs for a real world sav bug in the apache server to manifest in a failed run .
more likely a programmer will capture one failed run after many more successful runs in reality.
in this paper we propose a more practical approach to address the above limitation.
our approach through a systematic examination of the common buggy shared memory accesses identifies a list of shared memory access pairs that behave distinctively in failed and successful runs.
three scenarios that summarize the manifestation differences of the buggy memory access pairs triggering ov or av also sav or mav bugs in successful and failed runs are thus identified.
this inspires us to develop three test procedures each of which checks the distinctive memory access pairs in one of the three scenarios.
experiments on common concurrency bugs show that our approach can localize these bugs even with only one failed run captured and has significantly fewer false positives than the state of art methods.
this paper makes the following contributions first we identify an exhaustive list of scenarios that indicate the manifestation differences of the buggy access pairs triggering ov or av bugs in successful and failed runs.
to the best of our knowledge this is the first attempt to compile all possible scenarios that cover ov and av bugs.
second we propose comprehensive test procedures to automatically localize buggy memory access pairs by matching them to all identified scenarios.
the test procedures are designed elaborately to eliminate potential false positives.
third we implement a prototype named locon based on the proposed test procedures and evaluate it on common concurrency bugs.
the results show that locon is capable of localizing these bugs precisely with only one failed runcaptured i.e.
itisnotrequiredtocapturemultiplefailed runs to pinpoint a bug as in existing approaches.
the rest of this paper is organized as follows.
section describes the main idea of this paper.
section and respectively illustrate scenarios and test procedures.
section and present and evaluate locon.
section discusses related work.
section concludes the paper.
.
overview we firstly define some notations and then leverage an example to motivate the main idea of our approach.
.
notations a thread is denoted as ti where i ... indicates a unique thread id.
ma xrepresents an access to a shared variable x where mindicates the type of the access either r read orw write ais the instruction or statement in the source text that issues the access.
ashared memory access pair or an access pair for short represents an immediate dependency r w w r orw w between two memory accesses in two threads ma x nb x wherexis the shared variable accessed and at leastone ofm andnis a write.
the access pair indicates that ma xhappenst1 session.c tr handle tr sessioninitfull ... ... h tr new0 tr handle h peermgr tr peermgrnew h h bandwidth tr bandwidthnew h null ... t2 bandwidth.c void allocatebandwidth ... ... assert h bandwidth ... correctorder buggyorder figure an o v bug in transmission .
.
program executionsmanualbug localizationprogrambinary programinputfailedsuccessful bug fixbug cause accesspair collectiontestprocedures based onscenariosmanualtesting and localizationprocess automatic localization basedonourapproachaccess pairs figure automat ic bug localization based on our approach.
beforenb x.ma xrefers to the head and nb xrefers to the tail of the access pair.
besides ma x nb xandnb x ma xare respectively called the reversed access pair of each other.
asuccessful run is a program execution that produces the expected output of the programmer.
otherwise the executionisconsideredasa failedrun.
inthispaper weonly handle with failed runs caused by ov and av bugs.
also failed and successful runs have the sameprogram input.
typically the manifestation of concurrency bugs shows up in the form of wrong values being read from the violated shared variables.
these readsare referred to as etps errortriggering points of the concurrency bugs in this paper.
after the occurrence of an etp the program may continue to run but produce incorrect results or may be terminated prematurely due to faults.
therefore the tail of the buggy access pair may not show up after the etp.
such buggy access pair is often not included in the final report of most existing approaches because its tail is not actually executed.
our approach pays special attention to these buggy access pairs and results in more precise localization results.
.
a motivating example we use the example illustrated in figure to describe the main idea of our approach.
this is a real world ov bug from transmission .
which is a multi threaded bittorrent download client.
in this example h bandwidth is a shared variable initialized by the statement h bandwidth trbandwidthnew h null insession.c i.e.wh bandwidth the statement is omitted for simplicity .
the statement assert h bandwidth inbandwidth.c readsh bandwidth i.e.
rh bandwidth which ought to occur after wh bandwidth .
unfortunately due to the lack of proper synchronization rh bandwidth is likely to be executed before wh bandwidth andreadsanuninitializedvalue causinganassertionfailure.
thus rh bandwidth is theetpof this bug.
thebugisnotoftentriggeredevenwiththebug triggering input.
actually we cannot come across this bug until tens of thousands of successful runs have been tested.
after studied the program we found that the function allocatebandwidth is called from an event callback function that is often called about milliseconds after h peermgr is created.
this is why the bug occurs rarely in practice.
but if the execution environment is changed e.g.
the underlying schedule policy the bug may manifest and cause the program to crash.
idillegalfailed runsscenario successful runsdescriptioninterleavings type run run ov1a xwb xrwa x rb x irb x wa x unexpected value is read due to the premature write.
2a xrb xwra x wb x iwb x ra x unexpected value is read due to the hysteretic write.
ii buggy access pair is missing due to the etp.
3a xwb xwwa x wb x iwb x wa x the first write is covered by the second.
sav4a xwb xwc xrwa x wb x wb x rc xirc x wb xwb x wa xthe first write is covered by the second.
5a xrb xwc xwra x wb x wb x wc xiwc x wb xwb x ra xthe first write is covered by the second.
6a xrb xwc xrra x wb x wb x rc xirc x wb xwb x ra xinconsistent values are read due to improper write.
7a xwb xrc xwwa x rb x rb x wc xiwc x rb xrb x wa xunexpected intermediate value is read between the writes.
wa x rb x i unexpected intermediate value is read after the first write.
mav8a xrb xw d ywc yrra x wb x rc y wd yiiira x wb xwb x ra xinconsistent results of multiple variables due to improper accesses.
wb x ra x wd y rc yiiiwd y rc yrc y wd yinconsistent results of multiple variables due to improper accesses.
ra x wb x ii buggy access pairs are partially missing due to the etp.
9a xrb yr d ywc xwra x wc x rb y wd yiiira x wc xrb y wd yinconsistent results of multiple variables due to improper accesses.
wd y rb ywc x ra x 10a xwb xr d ywc yrwa x rb x rc y wd yiiiwa x rb xrb x wa xinconsistent values of multiple variables are read.
rb x wa x wd y rc yiiiwd y rc yrc y wd yinconsistent values of multiple variables are read.
wa x rb x ii buggy access pairs are partially missing due to the etp.
11a xwb xw d ywc yrwa x wb x rc y wd yiiiwa x wb xwb x wa xinconsistent results of multiple variables due to improper accesses.
wb x wa x wd y rc yiiiwd y rc yrc y wd yinconsistent results of multiple variables due to improper writes.
wa x wb x ii buggy access pairs are partially missing due to the etp.
12a xwb xw d ywc ywwa x wb x wc y wd yiiiwa x wb xwb x wa xinconsistent results of multiple variables due to improper writes.
wb x wa x wd y wc yiiiwd y wc ywc y wd yinconsistent results of multiple variables due to improper writes.
13a xwb yr d ywc xrwa x rc x rb y wd yiiiwa x rc xrb y wd yinconsistent results of multiple variables due to improper reads.
wa x rc x iiwd y rb yrc x wa xbuggy access pairs are partially missing due to the etp.
14a xwb yr d ywc xwwa x wc x rb y wd yiiiwa x wc xrb y wd yinconsistent results of multiple variables due to improper accesses.
wd y rb ywc x wa x 15a xwb yw d ywc xwwa x wc x wb y wd yiiiwa x wc xwb y wd yinconsistent results of multiple variables due to improper writes.
wd y wb ywc x wa x table differen t scenarios for ov and av bugs.
r possible etptriggered by a read there is no access pair in the failed run.
figure shows a common process of testing and debugging a concurrent program which iteratively executes the program binary with a given input.
if the program run is successful the testing process iterates again.
otherwise if failed programmers have to manually find the bug which is often time consuming.
our approach collects access pairs in program runs and when a failed run is encountered test procedures based on the manifestation scenarios analyze the collected access pairs and report buggy access pairs that trigger the bug.
for the bug in figure if the assertion happens to fail after some successful runs our approach will reportrh bandwidth wh bandwidth as the buggy access pair after applying test procedure ii see section .
.
note the program crashes after the execution of the etp of the bug i.e.
rh bandwidth andwh bandwidth is not actually executed in failed runs.
most existing pattern based approaches fail to localize this bug since they only consider access patterns gathered in failed runs .
for instance falcon only ranks access patterns collected in failed runs and therefore may fail to localize this bug.
instead our approach can successfully report the buggy access pair as the bug cause.
our advantage comes from the exhaustive list of manifestation scenarios presented in section which is the basis of our approach.
.
manifestation scenarios we present the scenarios based on two hypotheses .
first most of concurrency bugs can be triggered with a small number of thread preemptions i.e.
the well known small scope hypothesis .
we leverage this observation to bound the number of access pairs in scenarios to two.
second a majority of concurrency bugs can be triggered when the buggy access pairs show up in program runs regardless of the data value of the shared variables involved in the access pairs i.e.
the value independence hypothesis .
besides we also assume that properly synchronized concurrent programs adhere to the data race free model .
with this model the underlying hardware appears to besequential consistent even though it may be implemented using a weaker consistency model.
based on these assumptions table demonstrates a exhaustive list of scenarios for ov and av bugs based on two threads t1andt2 omitted due to space limitation and at most two shared variables x andy.
we explain each column as follows.
the column shows bug types.
the column gives each type of bug an id number.
for ov bugs there are only three possible violations of shared memory access orders r w w r andw w. thus there are three types of ov bugs id id .
for sav bugs each one involves three 613shared memory accesses .
the combinatorial number ofrandwforthe threeaccessesis23 butonlyfourofthem are unserializable i.e.
av bugs.
thus there are four types of sav bugs id id .
similarly we can conclude that there are eight types of mav bugs id id .
the column lists one possible illegal interleaving for each type of bug where the accesses in the left and right sides are respectively executed by t1andt2 the accesses in the same thread are executed from top to bottom and the arrow between two accesses denotes their happen before order i.e.
the source happens before the target.
the bug is triggered under the illegal interleaving leading to a failed run in the column .
for example id is an ov bug where the intended happen before order between the two accesses isrb xhappens before wa x. however the illegal interleaving wa x rb xviolates this order and causes the failed run in the column where wa x rb xshows up.
some types of bugs may have more than one possible illegal interleaving.
for example id is an mav bug where shared variables x andyshould be written wa xandwd y and read rb xand rc y atomically.
there are two possible illegal interleavings i.e.wa x rb x rc y wd yandrb x wa x wd y rc y which respectively violate the atomicity between wa xandwd y and rb xand rc y .
we only show the first one in the column due to space limitation but list all possible buggy access pairs that show up under each illegal interleaving in the column i.e.
the first two cases in the column .
the third case wa x rb xis listedwithadescriptionofthecauseinthecolumn8 i.e.
one buggy access pair is missing due to the etpcaused by rc y. note wa x rb x wd y rc yandrb x wa x rc y wd yare not illegal interleavings for id because the execution results of these two interleavings are respectively equivalent to wa x wd y rb xrc yandrb x rc y wa x wd y which means they are serializable.
the illegal interleavings listed in the column represents all possible anomalous interleavings triggering ov and av includingsavandmav bugs.
theseinterleavingpatterns are more comprehensive and general than the problematic access patterns used in prior work for specific classes of concurrency bugs .
the columns and illustrate access pairs manifested in possible successful runs.
for each ov bug there is only one possible case in successful run where the desired happenbefore order of the involved accesses is followed.
for each sav or mav bug there are two possible cases and each of them is a serialized execution of the involved atomic regions and the atomicity semantics are satisfied.
as shown in the column there are three possible manifestation scenarios denoted as scenario i ii and iii.
the intuitionbehindthesescenariosistosummarizethemanifestation differences of the buggy access pairs in the failed and successful runs.
these differences serve as the basis for our test procedures for concurrency bug localization.
actually developers also pay more attention to abnormal program behaviors in failed runs compared with successful runs when debugging a program.
we discuss each scenario as follows.
.
scenario i in this scenario buggy access pairs only occur in failed runs but notin successful runs.
take id an sav bug as an example.
in the two successful runs the columns and the atomicity between wa xandrc xis enforced and two access pairs respectively show up in each successful run rc x wb xandwb x wa x. but in the failed run the column the atomicity is violated under the illegal interleaving wa x wb x rc x. comparing with the former two access pairs we can easily see that buggy access pairs i.e.
wa x wb xandwb x rc x only occur in the failed run.
.
scenario ii buggyaccesspairsmanifestin neithersuccessful norfailed runs in this scenario.
take id an ov bug as an example.
the intended happen before order between wb xandra xis enforced by wb x ra xin the successful run shown in the column .
because ra xis anetpthat may terminate program execution due to the fault triggered by this violation there are two possible cases in the failed runs.
one is to have the buggy access pair ra x wb x which does not present in the successful runs but another does not form any access pair because wb xis not executed after ra xtriggers an etp.
we use an to represent this case in the column .
anotherexampleofthisscenarioisid anmavbug in whichrc yis a possible etp.
there are three possible cases in failed runs.
the first two will be discussed in scenario iii.
in the third one there exists only one buggy access pairwa x rb x and another buggy access pair rc y wd yis missing because wd yis not executed after the etptriggered byrc y. in fact this bug can only be triggered when the two buggy access pairs show up together in the same run i.e.
it is an mav bug involving two shared variables xandy .
we call such two buggy access pairs coupled buggy access pairs.
if one of them is missing in a failed run we call them partially missing coupled buggy access pairs.
note in the second case of the failed runs in id the bug can be triggered by wa x rb xalone without rb x wc x as in the first case.
however in the third case of the failed runs in id the bug cannot be triggered by wa x rb x alone.
it needs an illegal interleaving with y. this is why the former is in scenario i while the latter in scenario ii.
.
scenario iii buggy access pairs show up in bothfailed and successful runs in this scenario.
take id again as an example.
as mentioned above there are three possible cases in its failed runs.
in the first one the second one is similar there is a coupled buggy access pairs wa x rb xandrc y wd y each of which also shows up in successful runs the columns and .
but they do not show up together in any successful run.
although table only lists mav bugs triggered by only two access pairs other unlisted mav bugs can also fall into this scenario.
suppose an mav bug that can be triggered by at least naccess pairs where n h1 t1 h2 t2 ... hn tn which may involve more than two shared variables.
if the happen before order between hiandti where i n is reversed in a program run the bug will not be triggered and the following access pairs will be collected h1 t1 h2 t2 ... ti hi ... hn tn.
therefore there exist at least nsuccessful runs each of which only reverses one ofnaccess pairs.
thus the scenario of the nbuggy access pairs satisfies the conditions of scenario iii each one of them shows up in both failed and successful runs but they never show up together in any successful run.
aspreviouslydiscussedinthesmallscopehypothesis concurrency bugs triggered by more than two access pairs are very rare in real practice .
hence even if it is very simple we will not extend our test procedures to localize such bugs because it will increase the localization over614head significantly without much benefit.
to the best of our know ledge none of the existing concurrency bug localization methods considered such bugs due to their complicated trigger conditions and rarity in real practice.
.
test procedures assumepis a concurrent program and iis a concurrencybug triggering input of p.prunandfrunrespectively denote a set of access pairs that can be collected in a successful and failed run of punderi.psetis the set of pruns.
givenpsetand anfrun the fault localization problem is to identify buggy access pairs triggering the concurrency bug infrun.
in practice it is much easier to come across prunthanfrundue to the special trigger conditions of concurrency bugs.
moreover after a few number of pruns encountered almost all of possible access pairs in punderi can be collected.
for simplicity of explanation the following description first assumes that we have collected all possible access pairs that can appear in prunsin thepset.
in section .
we will show that the assumption is not required in practice.
besides in section .
we will also study the sensitivity of localization results on the number of pruns.
to solve the localization problem we design test procedures to localize buggy access pairs manifested in scenarios discussed in section .
we describe them as follows.
.
test procedure i the first test procedure aims to uncover buggy access pairs manifested with scenario i where buggy access pairs show up in frun but not in pruns.
algorithm shows the details of this test procedure.
in step1 lines we collect access pairs that show up in frun but not in pruns denoted as apset.
however access pairs in apset are not all buggy.
this is one major cause of false positives in other existing approaches.
we identify those false positives in the following steps and filter them out.
one type of false positives is caused by the predictable access pairs.
an access pair is predictable by another if and only if the presence of the former can be inferred by the presence of the latter but not vice versa.
this means if the latter presents in a program run the former will be also found in this run.
figure shows an ov bug which is triggered by w3 y r2 y i.e.t1dereferences a null pointer .
in this example w1 x r4 xis predictable by r2 y w3 y andw3 y r2 yis predictable by r4 x w1 x. if we process frun2 using algorithm note we can process frun1 andfrun2 in figure independently as mentioned in section we can obtain w3 y r2 yandr4 x w1 xinapset afterstep1 wherer4 x w1 xis a false positive because the bug is not triggered by it but by w3 y r2 y. we filter out this type of false positives in step2of algorithm lines by identifying predictable access pairs inapset.
suppose two access pairs ap1andap2inapset whereap1is predictable by ap2.
thenap2is identified as a false positive.
the reason is as follows.
assume ap2is not a false positive but the buggy access pair that triggers the bug.
then its reversed access pair denoted as rap2 will not be buggy according to ov and sav bugs shown in table .
thus there exists a prunthat contains both ap1 andrap2.
this violates the precondition that ap1is in apset i.e.
ap1does not show up in any prun.
another type of false positives is caused by the execution of a rarely executed code region after an etpin the failedalgorithm test procedure i input failed run fr un successful run set pset output buggy access pairs manifested with scenario i step1 get access pairs that show up only in frun 1apset frun 2foraccess pair ap apsetdo 3forsuc cessful run prun psetdo ifap pr unthenapset apset ap break 5end 6end step2 filter out predictable access pairs 7fortwo access pairs api apj apsetdo 8ifapiis pr edictable by apjthenapset apset ap j 9else if apjis predictable by apithen apset apset ap i 10end step3 sort access pairs according to occurrence order 11aplist sortaccesspairs apset 12return aplist t1 x local1 y local2 t2 y null local3 x intx int y malloc t1t2 prun1 xw xr3 yw2 yrt1t2 frun11 xw xr3 yw yrt1t2 frun21 xw4 xr3 yw yr figure false positive introduced by predictable access pairs.
run e.g.
an embedded debugging routine in the program.
in this case all access pairs in this code region will appear only infrunbut not in pruns.
they are certainly not the causes of the concurrency bug.
to eliminate such false positives access pairs in apsetare sorted according to their occurrence order in step3of algorithm line as such false positives always occur afterbuggy access pairs.
the reason is that the formation of these false positive access pairs results from the triggered concurrency bugs.
.
test procedure ii test procedure ii is to localize buggy access pairs manifested with scenario ii where buggy access pairs show up in neither prunsnorfrunbecause they are missing or partially missing in frundue toetps.
thus they cannot be directly localized as those manifested with scenario i. we localize these buggy access pairs by observing the manifestation of their reversed access pairs in prunsandfrun.
for missing buggy access pairs their reversed access pairs will always show up in pruns but not in frun.
for example in the second case of failed runs of id in table ra x wb xis missing in both successful and failed runs.
but its reversed access pair i.e.
wb x ra x always show up in successful runs due to the intended happen before order between wb xandra x. for partially missing coupled buggy access pairs the reversed access pairs of the missing buggy access pairs show up inprunstogether with their counterparts which are also buggy access pairs in the coupled buggy access pairs.
take the third case of failed runs of id in table as an example.
wa x rb xandrc y wd yare partially missing coupled buggy access pairs where rc y wd yis missing.
the reversed access pair of rc y wd y i.e.wd y rc y which is also missing in the failed run always shows up together with its counterpart i.e.
wa x rb x in successful runs due to the required atomicity of accessing to both xandy.
we leverage above observations on reversed access pairs to localize buggy access pairs manifested with scenario ii.
615algorithm test procedure ii input failed run fr un successful run set pset output buggy access pairs manifested with scenario ii step1.
get access pairs that show up in all pruns 1apset capset 2forsuccessful run prun psetdo 3ifap set thenapset prun 4else foracc ess pair ap apsetdo ifap p runthenapset apset ap end 8end 9end step1.
get coupled access pairs that always show up together in pruns 10forsuccessful run prun m psetdo forapi apj p runmandapi apj apsetand api apjaccess different shared variables do fla g true forsuccessful run prun n psetdo if api p runnandapj prun n or api prun nandapj prun n thenflag false break end ifflag true thencapset capset api apj end 18end step2.
check access pairs from step1.
in frun 19rapset 20foraccess pair ap apsetdo ifap f runthen rapset rapset getreversedap ap 22end step2.
check coupled access pairs from step1.
in frun 23forcoupled access pairs api apj capset do ifapi fr unandapj frunthen rap set rapset getreversedap ap j else if api frunandapj frunthen rap set rapset getreversedap ap i end 29end step3 sort access pairs according to occurrence order 30raplist sortoriginalaccesspairs rapset 31return raplist algorithm shows the details.
in ste p1.
lines we collect access pairs that show up in all pruns.
in step2.
lines we check each access pair collected instep1.
whether it show up in frun.
if not its reversed access pair returned by getreversedap is considered as one of the missing buggy access pairs.
similarly step1.
lines and step2.
lines can localize partially missing coupled buggy access pairs in frun.
however non buggy access pairs may also be missing in frundue toetps.
for example code regions always executedinsuccessfulrunsmaynotbeexecutedinthefailedrun after an etpoccurs so reversed access pairs of the access pairs in these code regions can also be identified as buggy after the above steps.
to eliminate such false positives we also sort the original access pairs of those obtained by above two steps according to their occurrence order step3 in line as the similar reason to that in test procedure i. now apply this test procedure to the example in figure .
after step1.
we can find that wh bandwidth rh bandwidth shows up in all pruns which means the read access to h bandwidth should happen after the initialization.
in step2.
we can further find that this access pair do not show up in frun.
thus its reversed access pair rh bandwidth wh bandwidth is reported as the buggy access pair after sorting in step3.algorithm test procedure iii input failed run fr un successful run set pset output buggy access pairs manifested with scenario iii step1 get access pairs that show up in pruns and frun 1apset 2foraccess pair ap frundo 3forsuc cessful run prun psetdo ifap pr unthenapset apset ap break 5end 6end step2 check for coupled buggy access pairs 7capset 8forapi apj apsetandapi apjaccess different memory locations andapi apjare from two threads do 9flag true forsu ccessful run prun psetdo if p candcheckpc ap i apj prun or tidand checktid ap i apj prun or loopand checkloop ap i apj prun thenflag false break end ifflag true thencapset capset api apj 14end step3 filter out predictable access pairs 15fortwo coupled access pairs api apj apm apn capset do ifapiorapjis pr edictable by apmorapnthen cap set capset apm apn else if apmorapnis predictable by apiorapjthen cap set capset api apj end 21end 22return capset .
test procedure iii this t est procedure aims to localize buggy access pairs manifested with scenario iii.
algorithm shows the details of this test procedure.
in step1 lines we collect access pairs that show up in both prunsandfrun denoted as apset.
in step2 lines for any two access pairs inapsetthat access different shared variables and are from two threads we check whether they show up together inprunsor not.
if they do not show up together in any prun they are considered as coupled buggy access pairs denoted as capset.
as in test procedure i false positives caused by predictable access pairs are filtered out in step3 lines .
to find out whether coupled access pairs show up together in anprun we need to check whether the heads and the tails of the two access pairs also form the same coupled access pairs in this prun.
the problem is how to represent theheadsandthetails whicharedynamicmemoryaccesses.
generally they can be represented by the addresses of the instructions issuing them .
this is usually enough for test procedures i and ii as they mainly aim to localize ov and sav bugs which only involve a single shared variable.
however it is insufficient for test procedure iii.
figure explains the reason.
this is an mav bug where t1writes shared variables xandyin a loop.
in successful runs prun prun3 t2can read consistent values of xandyas the atomicity is satisfied.
if test procedure iii processes frunonly using instruction addresses to identify dynamic memory accesses no buggy access pair can be localized because any two of the three access pairs in frun can be found show up together in prun2 with the same instruction addresses of the heads and the tails.
dynamic instances of the same instruction executed in different threads or calling contexts can also impede our attempt to localize buggy access pairs.
but concurrency bugs 616t2 var1 x var2 y if var1!
var2 printerror t1 for i i i x i y i intx y t1t2 prun12 xw6 yr5 xr yw xw ywt1t2 prun2t1t2 prun3t1t2 frun2 xw yw xw yw6 yr5 xr yr5 xr6 yr5 xr2 xw yw xw yw2 xw yw xw yw figure an ma v bug example to illustrate the insufficiency of instruction addresses for test procedure iii.
involving different calling contexts are very rare in real applications soweomitsuchcasesforefficiencyreasons.
besides the instruction address denoted as pc we append additional information on each shared memory access to eliminate above confusion.
the information include the tid of the thread executing the access denoted as tid and the loop information of the access denoted as loop which indicates the entry and iteration of the loop if the access is executed in a loop.
with these additional information the check in step2is performed at progressive levels of preciseness pc tid and loop.
each gives more preciseness but also more offline analysis overhead.
we use the example in figure again to describe how the check in step2is performed at each preciseness level.
afterstep1in algorithm apset r5 x w2 x w3 y r6 y r6 y w3 y .
instep2 there are two sets of coupled access pairs to be checked r5 x w2 x w3 y r6 y and r5 x w2 x r6 y w3 y .
first at the pc level both of them pass the check as discussed before and no buggy access pair is localized.
second at the tid level the two sets of coupled access pairs respectively have the following forms of tids in frun t2 t1 t1 t2 and t t1 t2 t1 .
as shown in the column of table there is no mav bug with the second form.
thus only the first coupled access pairs are checked and the second is discarded.
in prun2 we can also find that these two access pairs have the same forms of tids thus no buggy access pair is localized.
third at the loop level which is based on the observation that programmers generally either make a whole loop atomic or create atomic regions within the loop body but rarely across iteration boundaries we can find that w2 x andw3 yin r5 x w2 x w3 y r6 y are in the same loop iteration in frun but they are in different loop iterations inprun2.
thus this coupled access pairs cannot pass the check in step2and are reported as buggy access pairs.
.
is the complete pset necessary?
asstatedabove eachtestprocedurerequiresthecomplete pset which is the set that contains allfeasible pruns.
however it is usually not available in practice.
thus is it really necessary to have a truly complete psetwhen we apply the test procedures to localize buggy access pairs?
to answer this question we conduct an experiment on very diverse real concurrent applications shown in table .
in this experiment each application is executed many times with the same workload in two environments nativeand random.
access pairs showing up in each run are collected.
the only difference between native and random environ applications klocdescription workload fft fft transformationdefault input radix integer radix sort default input pbzip2 file compressor compress a regular file transmission bittorrent client open a torrent file apache web server concurrent http requests mysql database server mysql test suite table concu rrent open source applications for the experiment.
kloc program size in thousands of lines of code.
figure accum ulated access pairs in each program run.
n native r random.
ments is that a random number of microsecond delays are inserted at the points of shared memory accesses in the random environment to explore more thread interleavings.
figure5showstheaccumulatednumberofdifferentaccess pairs in each run.
when the number of test runs reaches a certain amount the increase in the number of different access pairs diminishes quickly or remains the same which means that the number of different access pairs in concurrent programs is limited.
in fact the occurrence of a new access pair depends on the explicit synchronizations used in a program and the distance1between the head and the tail of the access pair.
statistically if these two factors are kept unchanged all feasible access pairs can be collected.
if a non buggy access pair is very difficult to be collected it usually means this access pair has a very low probability to appear in both successful and failed runs.
thus although it may introduce false positives if it shows up only in failed runs it can be ignored due to the very low probability.
hence there is really no need to run a large number of prunsto collect a complete psetbecause there is only a limited number of access pairs in prunsfor most programs.
.
implementation this section describes locon a prototype based on the proposed test procedures.
locon has two components an online profiler which takes a concurrent program binary and its input as inputs executes the program binary with the given input and collects access pairs during the executions and an offline analyzer which takes collected access pairs as inputs and applies test procedures to localize buggy access pairs when a concurrency bug is triggered.
1thedetailed definition of the distance can be found in .
617the online profiler is implemented using pin .
to record the head and the tail of an access pair we need to collect the following information of the two memory accesses memory address instruction addresses thread ids and loop information.
to obtain the loop information locon detects loops via dominators in the control flow graph .
the detection is offline and introduces no additional overhead.
loconuses a stack to maintain the loop information for each thread.
when a thread enters a loop the entry address and current iteration number of this loop are pushed onto the stack.
when this thread exits a loop the corresponding loop information is popped from the stack.
for a nested loop when the thread exits the outer loop from the inner loop information of the two loops needs to be popped together from the stack.
when a concurrency bug is triggered the offline analyzer applies test procedures to localize the bug.
users are free to apply any of the test procedures via arguments of locon.
by default test procedures are applied in the order of i ii and then iii in the order of pc tid and then loop until the bug is localized.
.
empirical studies as shown in table we use commonly used concurrency bug benchmarks in prior work to evaluate locon .
they include extracted bugs i.e.
group e and real world bugs from real applications i.e.
group r. these bugs are written in c c and represent different failure symptoms of concurrency bugs.
for each bug we collect successful runs but only failed run.
the test procedures and preciseness levels are applied to the failed run in the default order until the buggy access pairs are localized.
the evaluation is conducted on an intel xeon machine with .87ghz cores and debian operating system.
we also compare locon with a state of the art tool based on a statistical approach falcon which is most relevant and similar to locon.
due to the unavailability of the source code we have to implement falcon based on our best knowledge.
falcon uses a fixed size window to collect access patterns on each shared memory location.
each pattern is assigned a suspicious score and patterns with higher scores are considered more likely to be bugs.
our implementation uses the same window size as that mentioned in the falcon paper.
other arguments are the same in l ocon.
.
study effectiveness as shown in table and bugs are respectively localized by test procedure i ii and iii.
as expected the localization ability of locon is not limited to bugs involving only one or two shared variables.
for example bug is an mav bug involving three shared variables and it is successfully localized by test procedure iii at the tid level.
an interesting example is bug which is an mav bug but localized by test procedure i. figure shows the source of this bug.
r2 logfilenameandr3 curlogint1 w8 lo gfilename andw9 curlogint2resp ectively have atomicity semantics.
even if this bug involves two shared variables logfilename andcurlog it can be triggered by w9 curlog r3 curlog alone whichmeans t1readstherightvalueof logfilename but the wr ong value of curlog.
thus it can be localized by tes t procedure i. this provides a good proof that each test procedure is not limited to localize concurrency bugs manifested in the corresponding scenario.concurrent programs locconcurrency bugs type symptom e1 scounter sav assertion failure baccount sav wrong result clist mav inconsistent results sbuffer sav assertion failure lprocsweep sav segmentation fault mysql e mav inconsistent results mysql e mav abnormal exit r8 pb zip2 2k ov segmentation fault transmission 95k ov assertion failure fft 1k ov wrong program output aget .5k mav inconsistent results apache 290k sav redundant free crash apache 271k sav confusion in log file apache 340k mav assertion failure mysql 681k sav miss in log file mysql 696k sav segmentation fault table eva luated bugs.
loc program size in lines of code.
t2 intqueue event ... ... log file name sql cur log x43 ... t1 log event next event ... if log file name mysql ... if cur log !
binlog magic gotoerr definebinlog magic xfe x62 x69 x6e log file name mysql cur log binlog magic figure descriptio n of bug .
compared with falcon the localization results of locon have much fewer false positives.
as shown in table for the extracted bugs i.e.
the first bugs falcon can precisely identify buggy patterns.
however for the real world programs it is less precise and with more false positives.
although falcon can identify all buggy patterns with high ranks the column rank some non buggy patterns can also be ranked with high scores the column rank1 .
for example in bug the number of patterns with the highestscoreis282.
thereasonisthatfalconrequirestocapture a sufficient number of failed runs to gain higher scores for buggy patterns.
furthermore falcon failed to localize of bugs mainly due to the following three reasons.
first concurrency bugs are triggered by buggy access pairsmanifestedinscenarioii includingbug 9andbug .
due toetps tails of those buggy access pairs are not actually executed in failed runs i.e.
they terminate prematurely.
hence falconfailstocollectthosebuggypatterns.
however loconsuccessfully localizes them with test procedure ii.
second concurrency bugs involve multiple shared variables.
this contributes to bug bug bug and bug .
although falcon also tries to extract patterns from bugs involving two shared variables it is very difficult to extract long patterns from complex bugs involving more than two shared variables such as bug .
in contrast loconis based on access pairs with only memory accesses each rather than long access patterns involved many memory accesses in each pattern hence has no such limitation.
third falcon sometimes fails to gather a buggy access pattern due to its fixed window mechanism for efficiency.
this impedes the localization of bug which is an sav bug with the buggy pattern of r w r similar to id in table .
if the last access recorded in a window is a wand 618concurrency bugs svlocon f alcon ap bap tp i tp iitp iiitime s p rank rank1 time s pc tid loop scounter check .
.
baccount check .
.
clist check .
n .
sbuffer check .
n .
lprocsweep check .
.
mysql e check .
n .
mysql e check .
.
pbzip2 check .
.
transmission check .
n .
fft check .
n .
aget check n .
apache check .
.
apache check .
.
apache check .
n .
mysql check .
.
mysql check .
.
table local ization results.
sv number of shared variables involved ap number of access pairs in failed runs bap number of buggy access pairs tp test procedure p number of patterns in failed runs rank the rank of the buggy patterns rank1 number of patterns with the highest score check m n buggy access pairs are localized by this test procedure mis the number of access pairs that pass the check of this test procedure nis the rank of buggy access pairs n failed to localize the bug.
applications pin loco nfalcon pbzip2 .77x .96x .85x transm ission .05x .2x .17x fft .36x .08x .01x aget .35x .34x .52x apache .25x .75x .25x mysql .31x .81x .13x table normalize d runtime overhead of the online profiler.
the current access to the same memory location is a rfrom the same thread falcon discards the r. this causes a loss of the first rin the collected buggy access pattern.
.
study efficiency table5showstheoverheadoftheonlineprofilerinlocon.
the overhead of pin is also included which is the overhead of executing an application under pin without any instrumentation.
all runtime results are normalized to the native execution time.
note that the original falcon was implemented in java and was compared with the runtime of java byte code.
our implementation is in c c and is compared with the runtime of native binaries.
in our implementation locon introduces comparable overhead in each run to that of falcon on average.
we believe the overhead can be further reduced which is left as part of our future work.
the two columns time s in table respectively shows the time consumed by the offline analyzer in locon and falcon.
in most cases locon is efficient.
but for some bugs e.g.
bug and bug the offline analysis time of loconcould be somewhat long.
to understand this phenomenon theprocessofofflineanalysisisfurtherpartitioned into two steps.
first the analyzer reads access pairs from the available successful runs.
second the analyzer applies test procedures to localize the bugs.
the time consumed by each step is shown in figure .
for most bugs i.e.
bug bug bug bug and bug the reading step consumes most of the analysis time because the analyzer needs to read all available successful runs before the localization.
different from these bugs most of the analysis time for bug is in the localizing step.
the reason is that this bug is localized at the loop level which needs more complex checks for the local percentage concurrency bugsreadinglocalizing others figure over heads of the offline analyzer ization.
however bug consumes almost all of the time in the localizing step.
after studying this bug we find that the loop information recorded in the failed run of this bug is more complex than that of bug which made the localizing step more complex and time consuming.
.
study sensitivity we also study the sensitivity of localization results on the number of successful runs by observing the change of localization results using different numbers of successful runs.
figure shows analysis results.
for test procedures i and ii the increased number of successful runs makes the localization results more precise.
for test procedure iii there are some differences.
for example in c list the number of coupled access pairs in the localization results increases with the number of successful runs.
after studying the localization results we find that the reason is that test procedure iii requires access pairs to show up in both successful and failed runs.
if the number of successful runs is very small some access pairs may not show up in those successful runs.
from figure we can conclude that the localization results of each test procedure are reasonably precise even with a small number of successful runs collected which is for most of the bugs.
even for mysql and transmission the number of successful runs needed is only and respectively.
this demonstrates the practicability of locon for bug localization because there is no need to gather a huge number of successful and failed runs.
100s counter b account l proc sweep mysql 2 e pbzip2 apache apache mysql mysql a test procedure i050100150200 100transmission fft b test procedure ii05101520 100c list s buffer mysql 1 e aget apache c test procedure iii figure sensitivit y analysis on the number of successful runs.
x axis is the number of available successful runs and y axis is the number of access pairs for test procedures i and ii or coupled access pairs for test procedure iii in localization results.
.
threats to validity there are several threats to validity of our empirical studies.
threats to internal validity stem from the empirical setup.
we assume programmers debug and fix the program by checking shared memory access pairs until the bug is localized.
although it may be different with the real debugging process this approach has been adopted by many statistical bug localization research.
threats to external validity constraint the extent to which our approach can be used to other types of program bugs which may introduce potential false positives or false negatives.
due to the lack of widely accepted bug benchmark suites related work also suffer from this threat.
to ease this threat our bug suit has covered a majority of bugs used in related work.
.
related work concurrency bug localization statistical analysis is an effective method for bug localization in sequential programs .
recently it is also applied to concurrency bugs.
falcon localizes concurrency bugs by pinpointing faulty interleaving patterns.
the patterns are extracted from common types of concurrency bugs that include unserializableandconflictinginterleavingpatterns.
falcongathers patternsinsuccessfulandfailedrunsviaonlinepatternidentification.
each pattern gathered in failed runs is assigned a suspicious score according to the jaccard index .
the patterns with higher suspicious scores are more likely to be concurrency bugs.
cci applies the statistical debugging technique for bug isolations.
cci dynamically monitors three types of interleaving related predicates on program states and behavior to diagnose program failures caused by concurrency bugs.
cci identifies failure predicates via statistical models.
based on the statistical model used by cci pbi is proposed to diagnose concurrency bugs in software production runs.
the predicates used by pbi called mesi predicates are cache coherence events that can be monitored by hardware performance counters.
the commonality of above approaches is that they are all based on some empirical statistical models which is the main difference between them and locon.
to precisely localize a concurrency bug these models require a sufficient number of successful and failed runs.
unfortunately some concurrency bugs are not easy to trigger in practice .
therefore it is quite challenging to gather enough failed runs for such models.
in contrast our approach has no such limitation since it leverages different test procedures to localize buggy access pairs manifested with different scenarios.even with only one single failed run captured our approach can still precisely localize the bug.
furthermore the localization results of our approach have fewer false positives due to the consideration of shared memory accesses missing in failed runs.
concurrency bug detection there are many work on concurrency bug detection including data races and av bugs .
different with these work loconaims to localize bugs after they are triggered.
concurrency bug testing some methods use different schedule or preemption policies to effectively expose concurrency bugs .
these methods can complement loconin gathering successful runs for the bug localization.
other work some other tools try to automatically fix concurrency bugs .
locon is also helpful to these tools since they only fix bugs that have been localized.
.
conclusion this paper presents an exhaustive list of scenarios that indicate the manifestation differences of buggy access pairs triggering ov or av bugs in successful and failed runs.
based on these scenarios a concurrency bug localization framework is proposed.
the framework consists of test procedures to localize buggy shared memory access pairs manifested in those scenarios.
each test procedure is designed with the consideration of minimizing false positives during the fault localization.
a bug localization prototype called locon is also implemented based on the test procedures.
empirical results on commonly used bugs show that locon is quite precise and practical for concurrency bug localization.
compared with existing methods which require a sufficient number of failed runs locon needs only one failed run.
besides there is also no need to gather a huge number of successful runs for locon due to the limited number of access pairs in concurrent programs.
.