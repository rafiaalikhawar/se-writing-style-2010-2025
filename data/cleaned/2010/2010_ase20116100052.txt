automated extraction of architecture level performance models of distributed component based systems fabian brosig karlsruhe institute of technology kit karlsruhe germany email fabian.brosig kit.edunikolaus huber karlsruhe institute of technology kit karlsruhe germany email nikolaus.huber kit.edusamuel kounev karlsruhe institute of technology kit karlsruhe germany email kounev kit.edu abstract modern enterprise applications have to satisfy increasingly stringent quality of service requirements.
to ensure that a system meets its performance requirements the ability to predict its performance under different configurations and workloads is essential.
architecture level performance models describe performance relevant aspects of software architectures and execution environments allowing to evaluate different usage profiles as well as system deployment and configuration options.
however building performance models manually requires a lot of time and effort.
in this paper we present a novel automated method for the extraction of architecture level performance models of distributed component based systems based on monitoring data collected at run time.
the method is validated in a case study with the industry standard specjenterprise2010 enterprise java benchmark a representative software system executed in a realistic environment.
the obtained performance predictions match the measurements on the real system within an error margin of mostly percent.
i. i ntroduction modern enterprise applications have to satisfy increasingly stringent requirements for performance scalability and efficiency.
the ability to provide performance guarantees under certain resource efficiency constraints is gaining in importance.
to avoid the pitfalls of inadequate quality of service qos the expected performance characteristics of systems need to be evaluated during all phases of their life cycle.
estimating the level of performance a system can achieve is normally done by means of system performance models that are used to predict the performance and resource utilization of the system under specific workloads.
during system development such models can be exploited to evaluate the performance of early stage prototypes.
during system deployment capacity planning can be conducted without the need for expensive load testing.
during operation a performance model can be used as a basis for continuous performance aware resource management over time .
in all of these contexts typically the following questions arise what will be the mean service response times and the utilization of system resources under a given workload?
what will be the impact of a changing workload e.g.
transaction mix and or workload intensity ?
what are the maximum load levels that the system will be able to handle and which resources would become bottlenecks under load?
how would the system performance change if resources are added or removed e.g.
nodes in an application server cluster are added or removed?
how much resources are required to satisfy performance and availability requirements while ensuring efficient resource usage?
in order to answer such questions using a performance model the model needs to be designed to reflect the abstract system structure and capture its performance relevant aspects.
we distinguish between predictive performance models and descriptive architecture level performance models .
the former capture the temporal system behavior and can be used directly for performance prediction by means of analytical or simulation techniques e.g.
queueing networks .
the latter describe performance relevant aspects of the system s software architecture execution environment and usage profile e.g.
uml models augmented with performance annotations .
they can normally be transformed into predictive performance models by means of automatic model to model transformations .
by modeling the system s performance influencing factors explicitly architecture level models allow to predict the impact of changes in the software components resource allocations and or system workloads.
over the past decade a number of architecture level performance meta models have been developed by the performance engineering community.
proposed meta models are e.g.
the uml spt and marte profiles csm pcm and klaper .
however building models that accurately capture the different aspects of system behavior is a challenging task and requires a lot of time when applied manually to large and complex real world systems .
given the costs of building performance models techniques for automated model extraction based on observation of the system at runtime are highly desirable.
current performance analysis tools used in industry mostly focus on profiling and monitoring transaction response times and resource consumption.
they often provide large amounts of low level data while important978 .
c ieee ase lawrence ks usa183 information about the end to end performance behavior is missing e.g.
service control flow and resource demands .
in this paper we propose a novel method for automated extraction of architecture level performance models for distributed component based systems.
the method uses run time monitoring data to extract the model structure based on tracing information and to extract model parameters based on easily obtainable performance metrics such as throughput resource utilization and response times.
the method supports pointto point asynchronous communication and includes an approach for modeling probabilistic parameter dependencies.
we implemented the proposed extraction method and evaluated its effectiveness by applying it to a system of a representative size and complexity a deployment of the industrystandard specjenterprise2010 benchmark1.
the benchmark is deployed in a complex execution environment consisting of a cluster of application servers and a back end database server for persistence.
we extracted a performance model of the system and evaluated its predictive power in scenarios of increasing complexity.
the presented method builds on our approach sketched in where we described a preliminary implementation of some parts of our extraction method suffering from many limitations and restrictions.
we studied a small part of a prerelease version of specjenterprise2010 deployed in a small testing environment which was not representative of real life deployments.
while the results were encouraging it was not clear if our approach was applicable to realistic applications and what accuracy and scalability it provides.
given the many limitations of our original method a larger part of the specjenterprise2010 application e.g.
java servlets java server pages java message service message driven beans could not be considered in the extraction.
the work presented here addresses the above challenges allowing to extract a model of the entire specjenterprise2010 application.
in addition it includes a detailed validation and evaluation of our approach.
in summary the contributions of this paper are i a method for automated extraction of architecture level performance models of component based systems including an approach to characterize parametric dependencies probabilistically ii a comprehensive evaluation of our approach in the context of a case study of a representative system deployed in a realistic environment.
to the best of our knowledge no similar studies exist in the literature considering systems of the size representativeness and complexity of the one considered here.
we demonstrate that the automated extraction of architecturelevel performance models using measurement data obtained with common monitoring tools is feasible and provides a solid basis for capacity planning.
the latter can be exploited for continuous performance aware resource management improv1specjenterprise2010 is a trademark of the standard performance evaluation corp. spec .
the specjenterprise2010 results or findings in this publication have not been reviewed or accepted by spec therefore no comparison nor performance inference can be made against any published spec result.
the official web site for specjenterprise2010 is located at extract effective architectureextract model parameters refine performance modelcontrol flow statisticsresource demandsprobabilistic parametric dependenciescomponent connectionscomponent internal performance relevant control flowacceptable accuracy?yesnofig.
.
model extraction process.
ing the system s efficiency and thus lowering its total cost ofownership.
the remainder of this paper is organized as follows.
in section ii we describe the method for the automated extraction of architecture level performance models.
we then describe a proof of concept implementation in section iii followed by a case study in section iv.
we review related work in section v and conclude the paper in section vi.
ii.
a utomated performance model extraction our goal is to automate the process of building performance models by observing the system behavior at run time.
based on monitoring data performance relevant abstractions and parameters should be automatically extracted.
the process we employ to extract architecture level performance models includes three main steps depicted in figure .
first we extract the system s effective architecture i.e.
the set of components and connections between components that are effectively used at run time.
second we extract performance model parameters characterizing the system s control flow and resource demands.
third the resulting model is iteratively refined until it provides the required accuracy.
a. extraction of effective architecture given a component based software system extracting its architecture requires identifying its building blocks and the connections between them.
in the context of performance model extraction also the components performance relevant internal structure needs to be extracted.
note that as mentioned above we consider only those parts of the architecture that are effectively used at run time.
component connections componentization is the process of breaking down the considered software system into components.
it is part of the mature research field of software architecture reconstruction .
component boundaries can be obtained in different ways e.g.
specified manually by the system architect or extracted automatically through static code analysis e.g.
.
the granularity of the identified components determines the granularity of the work units whose performance needs to be characterized and hence the184granularity of the resulting performance model.
in our context a component boundary is specified as a set of software building blocks considered as a single entity from the point of view of the system s architect.
for instance this can be a set of classes or a set of enterprise java beans and servlets.
once the component boundaries have been determined the connections between the components can be automatically identified based on monitoring data.
we determine the control flow between the identified components using call path tracing.
we analyze monitoring data consisting of event records obtained through instrumentation.
an event record represents an entry or exit of a software building block.
in order to trace individual system requests the list of gathered event records has to be grouped and ordered.
the set of groups is equal the set of equivalence classes raccording to the following equivalence relation r let a bbe event records obtained through instrumentation.
then arelates to b a rb if and only if aandbwere triggered by the same system request.
this is well defined because an event record is triggered by exactly one system request.
in the following equivalence classes are denoted as call path event record sets .
ordering the elements of a call path event record set in chronological order results in a call path event record list .
from this list a call path can be obtained.
we refer to where call path event record lists are transformed to uml sequence diagrams.
given a list of call paths and the knowledge about component boundaries the list of effectively used components as well as their actual entries provided services and exits required services can be determined.
furthermore for each component s provided service one can determine the list of external services that are called.
when obtaining monitoring data obviously only those paths that are exercised can be captured.
thus a representative usage profile is a prerequisite for a dynamic control flow analysis.
component internal performance relev.
control flow after extracting the components and the connections between them the performance relevant control flow inside the components is examined.
in order to characterize the performance of a component s provided service we need to know about its internal resource demanding behavior on the one hand and how it makes use of external service calls on the other hand.
obviously it makes a difference if an external service is called once or e.g.
ten times within a loop.
furthermore the ordering of external service calls and internal computations of the provided service may have an influence on the component s performance.
the performance relevant control flow we aim to extract is an abstraction of the actual control flow.
performance relevant actions are internal computation tasks and external service calls hence also loops and branches where external services are called.
the set of call paths derived in the previous substep provides information on how a provided component service relates to external service calls of the component s required services.
formally let xc xc ... xc nc be the set of provided services of component cand yc yc ... yc mcthe set of its required services.
for com externalcallaction imfgsession.findassembly internalaction loopaction externalcallaction imfgsession.getinventory internalaction fig.
.
example performance relevant component service control flow.
pactness we omit the index cfrom now on.
then the call paths constitute a relation g x s x x x s x s with s l1 ... l k k n li y i i k where sxrepresents the set of sequences of observed external service calls for provided service x. for instance if there is a provided service xso that sx x s x g sx y1 sx y2holds then one could assume that service xhas a control flow where either y1ory2is called i.e.
that there is a branch.
multiple approaches exist for determining the performance relevant control flow constructs e.g.
.
by instrumenting the performance relevant control flow constructs inside the component the effective component internal control flow can be directly extracted from the obtained call paths.
an example of a performance relevant service control flow is depicted in figure .
b. extraction of model parameters having extracted the performance relevant control flow structure in the previous step the model then needs to be parameterized with statistical information about the control flow including resource demand quantifications.
control flow statistics as control flow statistics we extract branch probabilities and loop iteration numbers.
both parameters can be extracted from the call paths obtained in the previous step.
while it is sufficient to characterize a branch probability with a mean value for a loop iteration number the call paths allow deriving a probability mass function pmf .
for instance the call paths might reveal that a loop is executed either two times or ten times each with a probability of .
in that case the average value would not be a proper approximation as a loop iteration number.
resource demands to characterize the resource demanding behavior of a component s service the resource demands of its internal computations need to be quantified.
determining resource demands involves identification of the resources used by the service and quantification of the amount of time spent using these resources.
the resource demand of a service is its total processing time at the considered resource not including any time spent waiting for the resource to be made available.
referring to figure the internal actions need to be annotated with demands for the used resources e.g.
cpu hdd .
resource demand estimation is an established research area.
typically resource demands are estimated based on measured response times or resource utilization and throughput data .
most approaches focus on cpus and i o resources i.e.
memory accesses are normally not considered explicitly.
probabilistic parametric dependencies the estimates of the model parameters described in the previous two sections are averaged over the observed call paths.
in case the behavior of the considered service depends on one or more input parameters passed when invoking the service such parametric dependencies can be captured in the extracted model.
for instance a branch probability might heavily depend on the value of an input parameter and in such cases it is desirable to be able to quantify such dependencies.
by monitoring service input parameters and co relating observed parameter values with the observed service control flow we can derive probabilistic models of parametric dependencies.
however if no a priori information about the existence of parametric dependencies is available their automatic discovery based on monitoring data alone is a challenging problem that requires the use of complex correlation analysis or machine learning techniques .
an automatic detection method has to cope with high degrees of freedom each control flow construct might depend on multiple input parameters and the possible dependencies are not restricted to parameter values other parameter characterizations such as the length of an array might also have an influence.
for these reasons we consider the automatic discovery of parametric dependencies at run time based on monitoring data alone to be impractical.
we assume that information about the existence of potential performance relevant parametric dependencies is available a priori e.g.
from the system developer or from an automatic detection method based on static code analysis.
with the knowledge of which parametric dependencies to observe we then use monitoring data to quantify the dependencies probabilistically.
we now discuss our approach to quantifying parametric dependencies based on monitoring data.
for compactness of the presentation we assume that each control flow action may depend on at most one input parameter.
for each service execution and for each parameter dependent action a tuple can be obtained that consists of the observed parameter characterization e.g.
the parameter value or its size and information about which control flow path was taken for a branch action with bt1 ... b t mbranch transitions the monitored control flow path is characterized by the index i i mof the observed transition.
for a loop action the monitored control flow path is characterized by the observed loop iteration number i i .
let vbe a monitored parameter characterization then ta v i is such an observed tuple for the considered parameter dependent action a. considering multiple service executions for each parameter dependent action a a set of observed tuples ta ta ... t a n can be extracted.
in the following we fix the action aso that we can omit the index i.e.
ta t t1 ... t n .
considering the taken control flow path for a given parameter characterization vas a discrete random variable we can approximate its probability mass function pmf based on the observed tuple list.
let v vk tk vk ik t be the set of observed parametercharacterizations.
let c ik tk vk ik t be the set of observed control flow paths taken.
then for each v v the pmf of the taken control flow path can be approximated asfv c where fv i tk tk vk ik t vk v ik i tk tk vk ik t vk v if the set v v1 ... v n is large distinguishing the branch transition probabilities loop iteration numbers or resource demands for each individual viis impractical.
in this case we partition the set vand work with partitions instead of individual values.
the partitioning can be done using e.g.
equidistant or equiprobable bins.
the distribution of the branch transition probabilities loop iteration numbers and resource demands is then approximated partition wise.
the pmf representing one partition can be computed by aggregation let pr vr ... v r l vbe such a partition.
then the pmf fr c representing the partition pris defined as the normalized weighted sum of the pmfs fvr ... f vr l fr c f r i l j 1p vr j fvr j i l j 1p vr j where p c p v tk tk vk ik t vk v v c. performance model calibration calibrating a performance model means comparing model predictions with measurements on the real system with the aim to adjust the model improving its accuracy.
given an observed deviation between predictions and measurements in general there are two ways of adjusting the performance model i one possibility is to increase the model s granularity by adding details on the modeled system behavior i.e.
refining the model.
the other possibility ii is to adjust the model parameters e.g.
branch probabilities or resource demands .
in that case the granularity of the model does not change.
in our approach we propose two ways of calibrations the resource demands are calibrated by a factor that represents the measurement overhead during resource demand extraction.
furthermore given that component based systems typically run on a complex middleware stack system requests may get delayed until they enter the system boundaries where the measurement sensors are injected.
these delays should be accounted for.
d. overhead control when applying the model extraction the monitoring overhead has to be kept low.
particularly when extracting resource demand parameters as mentioned in section ii b2 the overhead has to be low since otherwise the parameter extraction then may be biased.
to reduce the overhead of monitoring system requests in general there exist two orthogonal concepts i quantitative throttling throttling the number of requests that are actually186monitored ii qualitative throttling throttling the level of detail requests are monitored at.
existing work on i is presented e.g.
in .
the authors propose an adaptive time slot scheduling for the monitoring process.
the monitoring frequency depends on the load of the system.
in phases of high load the monitoring frequency is throttled.
concerning ii the monitoring approach presented in allows an adaptive monitoring of requests i.e.
monitoring probes can be enabled or disabled depending on what information about the requests should be monitored.
iii.
p roof of concept in this section we describe an implementation of our approach presented in the previous section in the context of a representative platform for distributed component based applications.
in section iv we then present a case study of a representative application demonstrating the effectiveness and practicality of our approach.
a. context we apply our automated performance model extraction approach in the context of the java platform enterprise edition java ee one of today s major technology platforms for building enterprise systems.
besides providing a framework for building distributed web applications it includes a serverside framework for component based applications namely the enterprise java beans ejb architecture.
we use the palladio component model pcm as an architecture level performance modeling formalism to describe extracted models of running java ee applications.
pcm is a domain specific modeling language for describing performance relevant aspects of component based software architectures .
it provides modeling constructs to capture the influence of the following four factors on the system performance i the implementation of software components ii the external services used by components iii the component execution environment and iv the system usage profile.
these performance influencing factors are reflected in an explicit context model and a parameterized component specification.
recent surveys show that the clear separation of these factors is one of the key benefits of pcm compared to other architecture level performance models such as the uml spt and marte profiles csm or klaper .
pcm models are divided into five sub models the repository model consists of interface and component specifications.
a component specification defines which interfaces the component provides and requires.
for each provided service the component specification contains a high level description of the service s internal behavior.
the description is provided in the form of a so called resource demanding service effect specification rdseff .
the system model describes how component instances from the repository are assembled to build a specific system.
the resource environment model specifies the execution environment in which a system is deployed.
the allocation model describes the mapping of components from the system model to resources defined inthe resource environment model.
the usage model describes the user behavior.
it captures the services that are called at runtime the frequency workload intensity and order in which they are invoked and the input parameters passed to them.
as an application server implementing the java ee specifications we employ oracle weblogic server wls .
wls comes with an integrated monitoring platform namely the weblogic diagnostics framework wldf .
wldf is a monitoring and diagnostics framework that enables collecting and analyzing diagnostic data for a running wls instance.
the two main wldf features that we make use of are the data harvester and the instrumentation engine.
the data harvester can be configured to collect detailed diagnostic information about a running wls and the applications deployed thereon.
the instrumentation engine allows injecting diagnostic actions in the server or application code at defined locations.
in short a location can be the beginning or end of a method or before or after a method call.
depending on the configured diagnostic actions each time a specific location is reached during processing an event record is generated.
besides information about e.g.
the time when or the location where the event occurred an event record contains adiagnostic context id which uniquely identifies the request that generated the event and allows to trace individual requests as they traverse the system.
b. implementation the implementation of the extraction process is based on our preliminary work in which included a very basic and limited implementation of a subset of the extraction algorithm.
while in we only considered ejbs our new implementation supports also servlets and java server pages jsps as well as message driven beans mdbs for asynchronous point to point communication using the java message service jms .
in the following we briefly describe the implementation of the extraction process.
for the extraction of the component connections according to section ii a1 the component boundaries can be specified as groups of ejbs servlets and jsps.
thus wldf is configured to monitor entries exits of ejb business methods servlet services including also jsps and jms send receive methods.
as depicted in figure the wldf diagnostic context id uniquely identifies a request but forked threads receive the same context id.
hence to separate the different call paths from each other the context id is not sufficient.
in those cases we make use of the transaction id.
the ordering of the event records is done via the event record id.
based on the set of observed call paths the effective connections among components can be determined i.e.
required interfaces of components can be bound to components providing the respective services.
for the extraction of the component internal performancerelevant control flow according to ii a2 we follow the approach described in .
performance relevant actions are made explicit by method refactorings.
this is because of the lack of tool support for in method instrumentation.
current187 c b asynchcall asynchcall asynchcallsynchcallsame diagnostic context idtransaction id 1transaction id transaction id 3fig.
.
diagnostic context id and transaction ids during asynchronous messaging.
instrumentation tools including wldf support only methodlevel instrumentation.
they do not support instrumentation at custom locations other than method entries and exits.
the sending of asynchronous jms messages is modeled as fork action.
control flow statistics are collected in parallel to the extraction of the abstract component internal control flow.
for the resource demand estimation of individual internal actions we apply two approaches i in phases of low load we approximate resource demands with measured response times ii in phases of medium to high load we estimate resource demands based on measured utilization and throughput data with weighted response time ratios .
concerning the extraction of probabilistic parametric dependencies the current wldf version limits the type of parameters that can be examined.
while wldf allows injecting monitors providing information about method input parameters it provides only string representations of the monitored parameters.
for complex types only the object name is provided.
thus complex parameter types currently cannot be monitored appropriately.
an example of an extracted pcm rdseff is shown in figure .
in includes internal actions external call actions and a loop action.
the loop action is annotated with a loop iteration number specified as a pmf.
with a probability of the loop body is executed times with a probability of it is executed times and with a probability of it is executed times.
notice that both internal actions of the rdseff are enriched with a cpu demand annotation.
while pcm supports generic work units in the context of this paper a demand of cpu unit is understood as a demand of millisecond cpu time.
iv.
c asestudy modeling spec jenterprise to evaluate the implemented model extraction method we applied it to a case study of a representative java ee application.
the application we consider is the specjenterprise2010 fig.
.
example rdseff for the provided service scheduleworkorder .
fig.
.
specjenterprise2010 architecture .
benchmark.
we start with an overview of the benchmark followed by a description of our experimental environment.
a. specjenterprise2010 specjenterprise2010 is a java ee benchmark developed by spec s java subcommittee for measuring the performance and scalability of java ee based application servers.
the benchmark workload is generated by an application that is modeled after an automobile manufacturer.
as business scenarios the application comprises customer relationship management crm manufacturing and supply chain management scm .
the business logic is divided into three domains orders domain manufacturing domain and supplier domain.
figure depicts the architecture of the benchmark as described in the benchmark documentation.
the application logic in the three domains is implemented using ejbs which are deployed on the considered java ee application server.
the domains interact with a database server via java database connectivity jdbc using the java persistence api jpa .
the communication between the domains is asynchronous and implemented using point to point messaging provided by the java message service jms .
the workload of the orders domain is triggered by dealerships whereas the workload of the manufacturing domain is triggered by manufacturing sites.
gbit ......benchmarkdriver agentssupplieremulatorbenchmarkdriver masterdns loadbalancer gbitgbit switchup to application serversfig.
.
experimental environment.
both dealerships and manufacturing sites are emulated by the benchmark driver a separate supplier emulator is used to emulate external suppliers.
the communication with the suppliers is implemented using web services.
while the orders domain is accessed through java servlets the manufacturing domain can be accessed either through web services or ejb calls i.e.
remote method invocation rmi .
the benchmark driver executes five benchmark operations.
a dealer may browse through the catalog of cars purchase cars or manage his dealership inventory i.e.
sell cars or cancel orders.
in the manufacturing domain work orders for manufacturing vehicles are placed triggered either per webservice or rmi calls createvehiclews orcreatevehicleejb .
b. experimental environment we installed the benchmark in the system environment depicted in figure .
the benchmark application is deployed in an oracle weblogic server wls .
.
cluster of up to eight physical nodes.
each wls instance runs on a core intel cpu with opensuse .
.
as a database server dbs we used oracle database 11g running on a dell poweredge r904 with four core amd cpus gb of main memory and 8x146 gb sas disk drives.
the benchmark driver master multiple driver agents the supplier emulator and the dns load balancer were running on separate virtualized blade servers.
the machines are connected by a gbit lan the dbs is connected with x gbit ports.
c. evaluation performance model while in the focus was on modeling the manufacturing domain of a pre version of the benchmark the study presented here considers the entire benchmark application i.e.
including supplier domain dealer domain the web tier and the asynchronous communication between the three domains.
we had to deal with ejbs including mdbs for asynchronous point to point communication web services servlets and jsps.
figure presents a high level overview of the basic structure of the extracted pcm performance model.
the system model configuration itembrowser sessionbeancomponentsuppliersessionbeancomponent purchaseorder mdbcomponent largeordersender sessionbeancomponent specapp servletcomponent itembrowser sessionbeancomponentsuppliersessionbeancomponent purchaseorder mdbcomponent largeordersender sessionbeancomponent specapp servletcomponent itembrowser sessionbeancomponentsuppliersessionbeancomponent purchaseorder mdbcomponent largeordersender sessionbeancomponent specapp servletcomponent databaseemulatorload balancersje instance 1sje instance n system modelcomponent repository model itembrowser sessionbeancomponentsuppliersessionbeancomponent purchaseorder mdbcomponent largeordersender sessionbeancomponent specapp servletcomponent specjenterprisecomposite component composite ....fig.
.
specjenterprise2010 pcm model structure.
shows a load balancer which distributes incoming requests to replicas of the specjenterprise2010 benchmark application which themselves need an emulator instance and a database instance.
a benchmark application instance refers to a composite component which is located in the component repository.
the composite component in turn consists of component instances e.g.
a specappservlet component or a purchaseordermdb component.
these components reside in the repository as well.
the performance model of the benchmark application consists of components whose services are described by rdseffs.
in total internal actions branch actions and four loop actions have been modeled.
the resources we considered were the cpus of the wls instances wls cpu and the cpus of the database server dbs cpu .
the network and hard disk i o load could be neglected.
note that we configured load balancing via dns.
we extracted a pcm model whose resource demands were estimated based on utilization and throughput data.
for the apportioning of resource demands among wls cpu and dbs cpu see .
to keep the overhead low we separated the extraction step in which call paths are monitored from the extraction step in which resource demands are estimated.
both steps were conducted with one single wls instance.
the resource demands were extracted during a benchmark run with a steady state time of sec and a wls cpu utilization of about .
the same benchmark run was then executed without any instrumentation in order to quantify the instrumentation overhead factor and calibrate the estimates of the wls cpu resource demands.
furthermore we measured the delay for establishing a connection to the wls instance which is dependent on the system load.
with the knowledge of the number of connections the individual benchmark operations trigger the load dependent delay is estimated and taken into account in the predicted response times.
for the solution of the extracted pcm model we used the queueing network based simulator simucom .
usagescenario dealerdriver.manage systemcallaction showinventory systemcallaction showinventory systemcallaction home branchaction branchtransition probability .
branchtransition probability .
systemcallaction cancelorder branchaction branchtransition probability .
branchtransition probability .
loopaction loop iteration number systemcallaction sellinventoryfig.
.
usage scenario for operation manage .
evaluation results the extracted pcm models are validated by comparing the model predictions with measurements on the real system.
the usage model representing the benchmark workload has been provided manually.
the five benchmark operations are modeled as individual usage scenarios.
figure shows the usage scenario of benchmark operation manage in a notation similar to uml activity diagrams.
it consists of several system calls two branches with corresponding transition probabilities and a loop action.
the loop iteration number is given as a pmf that was derived from monitoring data.
the usage scenarios of the remaining four benchmark operations are of similar complexity.
in the investigated scenarios we vary the throughputs of the dealer and manufacturing driver as well as the deployment configuration.
note that we extracted the performance model on a single wls instance whereas for validation wls clusters of different sizes were configured.
as performance metrics we considered the average response times of the five benchmark operations as well as the average utilization of the wls cpu and the dbs cpu.
in clustered scenarios where several wls instances are involved we considered the average utilization of all wls cpus.
note that the response times of the benchmark operations are measured at the driver agents i.e.
the wls instances run without any instrumentation.
for each scenario we first predicted the performance metrics for low load conditions wls cpu utilization medium load conditions high load conditions and very high load conditions and then compared them with steady state measurements on the real system.
scenario cluster of two application server instances.
for the first validation scenario we configured an application server cluster consisting of two wls instances and injected different workloads.
the measured and predicted server utilization for the different load levels are depicted in figure a .
the utilization varies from to .
the utilization predictions fit the measurements very well both for the wls instances as well as for the dbs.
figure b shows the response times of the benchmark operations for the four load levels.
the response times vary from 10ms to 70ms.
as expected the higher the load the faster the response times grow.
compared to the other benchmark operations browseworkloadcpu utilization0.
.
.
.
.
.
lowmediumhighvery higha utilization measurements and predictionslegendwls utilization measuredwls utilization predicteddbs utilization measureddbs utilization predicted102030405060 workloadmeasured response time lowmediumhighvery high b response times of benchmark operations legendbrowsepurchasemanagecreatevehicleejbcreatevehiclews01020304050 workloadrelative resp.
time prediction error in lowmediumhighvery high legendbrowsepurchasemanagecreatevehicleejbcreatevehiclewsc relative error of response time predictions fig.
.
scenario measurements prediction results and prediction errors.
andpurchase have lower response times while createvehicleejb andcreatevehiclews take most time.
in figure c the relative error of the response time predictions is shown.
the error is mostly below only browse has a higher error but still lower than .
the prediction accuracy of the latter increases with the load.
this is because browse has a rather small resource demand but includes a high number of round trips to the server translating in connection delays on average .
scenario cluster of four application server instances.
in scenario we considered a four node wls cluster again at four different load levels.
figure a shows the measurements and predictions of the server utilization.
again the predictions are accurate.
however one can identify a small deviation of the dbs cpu utilization that is growing with the load.
figure b depicts the relative response time prediction error for scenario .
again the relative error is mostly below .
for the same reasons already mentioned in scenario operation browse stands out a little.
however its prediction error is still below .
scenario cluster of eight application server instances.
for scenario we deployed the benchmark in a cluster of eight wls instances.
as shown in figure the server utilization predictions are very accurate.
while the dbs utilization prediction exhibits an error that grows with the injected load it does not exceed .
in cases of low to medium load the accuracy of the predicted response times is comparable to scenarios and 2190workloadcpu utilization0.
.
.
.
.
.
lowmediumhighvery highlegendwls utilization measuredwls utilization predicteddbs utilization measureddbs utilization predicteda utilization measurements and predictions01020304050 workloadrelative resp.
time prediction error in lowmediumhighvery high browsepurchasemanagecreatevehicleejbcreatevehiclewsb relative error of response time predictions fig.
.
scenario utilization measurements and relative errors.
workloadcpu utilization0.
.
.
.
.
.
lowmediumhighvery highlegendwls utilization measuredwls utilization predicteddbs utilization measureddbs utilization predicteda utilization measurements and predictions01020304050 workloadrelative resp.
time prediction error in lowmediumhighvery high legendbrowsepurchasemanagecreatevehicleejbcreatevehiclewsb relative error of response time predictions fig.
.
scenario utilization measurements and predictions and relative error of response time predictions.
figure b .
however in cases of high load the prediction error grows by an order of magnitude.
this is because under this load level some of the wls cluster nodes were overloaded i.e.
the cluster was not load balanced anymore.
the overloaded wls instances then lead to biased response time averages.
we assume that the cluster is unbalanced due to dns caching effects that are not reflected in the performance model.
summary we validated the performance model that was extracted with the automated method as described in sections ii and iii in various realistically sized deployment environments under different workload mixes and load intensity.
concerning the cpu utilization the observed prediction error for the wls application server was below .
for the database server the cpu utilization prediction error was mostly below .
the response time predictions of the benchmark operations mostly had an error of to .in the case of the eight node application server cluster the response time predictions were inaccurate for higher loads.
this is because the cluster was not load balanced anymore.
v. r elated work each part of the approach presented in this paper is related to multiple extraction approaches a trace based approaches b resource demand estimation c run time monitoring d benchmark based approaches and e extraction of pcm instances.
trace based approaches.
call path tracing is a form of dynamic analysis which a number of approaches apply to gain reliable data on the actual execution of an application.
hrischuk et al.
use such an approach to extract performance models in the scope of trace based load characterization .
israr et al.
use pattern matching on trace data to differentiate between asynchronous blocking synchronous and forwarding communication.
both approaches use layered queueing networks lqns as the target performance model and are hence limited to lqn structures.
thus features such as stochastic characterizations of loop iteration numbers or branch probabilities are not supported.
briand et al.
extract uml sequence diagrams from trace data which is obtained by aspect based instrumentation.
resource demand estimation.
approaches to resource demand estimation can be found in .
while in applies the service demand law directly for single workload classes linear regression approaches to partition resource demand among multiple workload classes can be found in .
in utilization and throughput data is used to build a kalman filter estimator.
run time monitoring.
approaches such as use systematic measurements to build mathematical models or models obtained with genetic optimization.
however the models serve as interpolation of the measurements and a representation of the system architecture is not extracted.
tools for automatic and adaptive monitoring are presented in all focused on monitoring and collecting performance data of java applications but they are not explicitly focused on component based architectures.
benchmark based approaches.
denaro et al.
and chen et al.
identify the middleware as a key factor for performance problems in component based applications.
to allow performance prediction denaro develops application specific performance test cases to be executed on available middleware platforms.
chen et al.
propose a simple benchmark that processes typical transaction operations to extract a performance profile of the underlying component based middleware and to construct a generic performance model.
pcm extraction.
several approaches are concerned with the extraction of pcm instances from code.
the tool java2pcm extracts component level control flow from java code.
krogmann et al.
extract behavior models via static and dynamic analysis but do not focus on extracting timing values during system operation and their approach relies on manual instrumentations.191vi.
c oncluding remarks in this paper we presented a method to automate the extraction of architecture level performance models of distributed component based systems using run time monitoring data.
we combined existing techniques such as call path tracing and resource demand estimation to an end to end model extraction method.
as a proof of concept we implemented the method with state of the art component of the shelf monitoring tools.
we validated it with the representative industry standard specjenterprise2010 benchmark application in realistically scaled deployment environments.
we extracted a performance model and derived performance predictions for various validation scenarios.
resource utilizations are predicted with a relative error of mostly .
response times which are typically much harder to predict accurately are predicted with a relative error of about to .
we could show that an automated extraction of architecturelevel performance models using measurement data obtained with common monitoring tools is a feasible way to extract performance models for distributed component based systems.
the obstacle of building a performance model manually can be overcome.
performance engineers can use the automated extraction process to obtain performance models allowing to answer sizing resource efficiency and performance questions in an effective way.
however further investigations concerning the extraction of internal service behavior and the characterization of probabilistic parametric dependencies are of concern.
in addition we are interested in studies with performance predictions for highly configurable dynamic virtualized environments.
the latter will reveal new challenges how appropriate architecturelevel performance model representations should look like in the context of virtualized environments.